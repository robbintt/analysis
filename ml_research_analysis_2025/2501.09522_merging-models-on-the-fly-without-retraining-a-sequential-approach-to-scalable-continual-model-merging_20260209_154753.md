---
ver: rpa2
title: 'Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable
  Continual Model Merging'
arxiv_id: '2501.09522'
source_url: https://arxiv.org/abs/2501.09522
tags:
- merging
- task
- tasks
- merged
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a sequential model merging method that processes
  fine-tuned models as they become available, using orthogonal projections to minimize
  interference between tasks while maintaining constant memory complexity. The method
  employs adaptive scaling mechanisms to preserve previously learned knowledge and
  integrate new task-specific information efficiently.
---

# Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging

## Quick Facts
- arXiv ID: 2501.09522
- Source URL: https://arxiv.org/abs/2501.09522
- Authors: Anke Tang; Enneng Yang; Li Shen; Yong Luo; Han Hu; Bo Du; Dacheng Tao
- Reference count: 40
- Primary result: Sequential model merging method achieves 5-8% average accuracy improvements over baselines while maintaining constant memory complexity

## Executive Summary
This paper introduces Orthogonal Projection-based Continual Merging (OPCM), a method for sequentially merging fine-tuned models as they become available without retraining. The approach uses orthogonal projections to minimize interference between tasks while maintaining constant memory complexity regardless of task count. By dynamically scaling the contribution of each merged task, OPCM preserves previously learned knowledge and integrates new task-specific information efficiently. Experimental results on CLIP-ViT models across 8, 14, and 20 classification tasks demonstrate significant improvements over baseline methods while maintaining robust performance regardless of task ordering.

## Method Summary
OPCM processes fine-tuned models sequentially through orthogonal projections and adaptive scaling mechanisms. For each incoming model, the method computes task vectors as the difference from the pre-trained base, projects them onto subspaces orthogonal to previously merged parameters using SVD, and applies dynamic scaling to maintain parameter stability. The approach maintains only the current merged model and base model in memory (O(|θ|) complexity), enabling continual merging without storing all historical models. The projection threshold α=0.5 and adaptive scaling factor λ(t) ensure minimal interference between tasks while preserving task-specific information.

## Key Results
- 5-8% average accuracy improvements over baseline methods on CLIP-ViT models
- Constant memory complexity (O(|θ|)) independent of task count
- Robust performance across different task orderings with low variance
- Effective mitigation of catastrophic forgetting compared to conventional merging techniques
- Sequential processing maintains scalability while achieving competitive multi-task performance

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Projection Minimizes Task Interference
Projecting incoming task vectors onto subspaces orthogonal to previously merged parameters reduces destructive interference between sequential tasks. At each step, SVD extracts orthonormal bases from the current merged delta, and the new task vector is projected onto the orthogonal complement. This preserves previously learned features while allowing non-conflicting new information to be added. The approach assumes task vectors have low mutual cosine similarity, which is empirically observed in vision models.

### Mechanism 2: Adaptive Time-Varying Scaling Maintains Parameter Stability
Dynamically adjusting each merged task's contribution via time-varying scaling factor λ(t) prevents parameter drift from the pre-trained model's loss basin. λ(t) is computed as the ratio of combined projected task vector norm to average individual task vector norm, ensuring ||θ(t)_merged - θ(0)||_2 remains approximately constant as more tasks are merged. This prevents accumulated parameter magnitudes from destabilizing the model.

### Mechanism 3: Sequential Processing with Constant Memory Enables Scalability
Maintaining only the current merged model and base model allows continual merging with O(|θ|) memory, independent of task count. The method stores θ(0) (pre-trained base) and θ(t-1)_merged (current merged model), computing orthogonal updates without needing to store all historical models individually. This enables practical deployment in scenarios with streaming model updates.

## Foundational Learning

- **Concept: Task Arithmetic / Weight Interpolation**
  - Why needed here: OPCM generalizes Task Arithmetic by adding projection and scaling. Understanding the baseline θ_merged = θ(0) + λ·Σ(θ(i) - θ(0)) is essential to see how orthogonal projection modifies the update.
  - Quick check question: Can you write the update rule for Task Arithmetic and identify why it fails in sequential settings?

- **Concept: Singular Value Decomposition (SVD) and Orthogonal Subspaces**
  - Why needed here: The projection operator P(t-1)_α uses SVD to extract orthonormal bases (U, V) from the merged delta. Understanding how SVD defines orthogonal subspaces is critical to implementing the projection.
  - Quick check question: Given a matrix A = UΣV^T, how would you project a vector b onto the subspace orthogonal to A's column space?

- **Concept: Catastrophic Forgetting in Continual Learning**
  - Why needed here: The paper frames sequential model merging as a continual learning problem. Understanding why neural networks forget previous tasks when fine-tuned sequentially clarifies the motivation for orthogonal projection and scaling.
  - Quick check question: Why does standard sequential fine-tuning on task B after task A typically degrade performance on task A?

## Architecture Onboarding

- **Component map:**
  1. Input: Pre-trained model θ(0), sequence of fine-tuned models {θ(1), ..., θ(T)} arriving sequentially
  2. Delta Computation: Compute task vector ΔW(t) = θ(t) - θ(0) for weight matrices; Δp(t) for other parameters
  3. SVD Module: Perform full SVD on ΔW(t-1)_merged to get U(t-1), Σ(t-1), V(t-1). Identify effective rank r_α based on projection threshold
  4. Projection Operator: Apply P(t-1)_α to ΔW(t), zeroing components aligned with ΔW(t-1)_merged's singular vectors (excluding diagonal)
  5. Scaling Module: Compute λ(t) adaptively using norms of combined and average task vectors
  6. Merge Update: θ(t)_merged = θ(0) + (λ(t-1)·Δθ(t-1)_merged + Δθ(t)_proj) / λ(t)
  7. Output: Final merged model θ(T)_merged handling all tasks

- **Critical path:**
  1. Initialize θ(1)_merged = θ(1), compute initial average norm n = ||Δθ(1)||_2, set λ(1) = 1
  2. For each subsequent model, compute deltas → SVD → projection → adaptive scaling → merge
  3. The SVD and projection for weight matrices is the core computational step; other parameters use simple averaging

- **Design tradeoffs:**
  - **Projection threshold α**: Lower α (e.g., 0.4) preserves more new task information but may increase interference; higher α (e.g., 0.6) enforces stricter orthogonality but may discard useful signal. Paper suggests α ≈ 0.5 as robust default
  - **Adaptive vs. fixed λ(t)**: Adaptive scaling handles varied task vector magnitudes but adds compute. Fixed √t is simpler but assumes orthogonality holds well
  - **Full SVD vs. truncated**: Paper uses full SVD for precision; truncated could speed up large models but risks approximation errors

- **Failure signatures:**
  - Rapid accuracy drop after first few tasks: May indicate task vectors are not orthogonal; consider checking cosine similarities or adjusting α
  - High variance across task orderings: Suggests projection or scaling isn't stabilizing knowledge; verify λ(t) computation and SVD implementation
  - Memory growth beyond O(|θ|): Bug in implementation storing historical models; ensure only θ(0) and θ(t-1)_merged are retained

- **First 3 experiments:**
  1. Reproduce 8-task ViT-B/32 result: Fine-tune CLIP-ViT-B/32 on 8 image classification tasks, implement OPCM with α=0.5, compare average accuracy and BWT against Task Arithmetic baseline. Target: ~5% improvement per paper
  2. Ablation on projection threshold α: Run OPCM on 8 tasks with α ∈ {0.2, 0.4, 0.5, 0.6, 0.8}. Plot accuracy vs. α to verify inverted U-shape and identify optimal range
  3. Task ordering robustness test: Shuffle task order 10 times, run OPCM, report mean and std of average accuracy. Verify std remains low (<0.5% per paper) compared to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the orthogonal projection-based continual merging method perform when extended to large language models and multi-modal architectures?
- Basis in paper: [explicit] The conclusion states: "Future work could explore extensions to language models, multi-modal architectures, and other scenarios where sequential model merging is beneficial."
- Why unresolved: All experiments were conducted exclusively on CLIP-ViT vision models across image classification tasks. The method's applicability to fundamentally different architectures (e.g., decoder-only LLMs with attention mechanisms, or multi-modal models with cross-modal alignment requirements) remains untested.
- What evidence would resolve it: Systematic evaluation on language models (e.g., fine-tuned LLaMA variants on different NLP tasks) and multi-modal models (e.g., fine-tuned CLIP with both vision and language encoders), comparing against domain-specific baselines.

### Open Question 2
- Question: What happens to merging performance when task vectors exhibit higher mutual similarity rather than approximate orthogonality?
- Basis in paper: [inferred] The paper empirically validates that task vectors are "approximately orthogonal" (Figures 4, 9-11), and this orthogonality justifies using λ(t) = √t. However, the method's behavior when this assumption is violated (e.g., highly related tasks with similar fine-tuning data) is not analyzed.
- Why unresolved: The theoretical analysis (Theorems 5.1, 5.2) assumes the projection can effectively separate task information, but if incoming task vectors lie largely within the span of previously merged vectors, the orthogonal projection may discard substantial task-specific information.
- What evidence would resolve it: Controlled experiments merging models fine-tuned on progressively more similar tasks (e.g., different subsets of the same dataset, or tasks with known task vector similarity), measuring information loss and accuracy degradation.

### Open Question 3
- Question: Can the projection threshold α be adapted dynamically during merging rather than requiring manual tuning?
- Basis in paper: [inferred] The ablation study (Figure 6, 13, 15) shows optimal α varies between 0.4-0.6 across settings, and the paper acknowledges this as a hyperparameter requiring tuning. The fixed value limits practical deployment.
- Why unresolved: The current approach requires selecting α via grid search, which may not be feasible in true continual settings where validation data is unavailable or task characteristics are unknown a priori.
- What evidence would resolve it: Development and evaluation of adaptive α selection methods (e.g., based on the norm ratio of projected to original task vectors, or gradient-based meta-learning), showing comparable performance to oracle-tuned values without requiring validation data.

## Limitations

- The orthogonal projection mechanism assumes task vectors have low mutual similarity, but the paper doesn't fully explore failure modes when this assumption breaks
- The adaptive scaling factor's theoretical justification (√t behavior) relies on orthogonality that may not hold in practice
- Memory complexity claims assume the projection captures all previous knowledge without revisiting stored models, which hasn't been rigorously validated for non-orthogonal task distributions

## Confidence

- **High confidence:** Sequential processing maintains O(|θ|) memory (directly verified in pseudocode and experiments)
- **Medium confidence:** Orthogonal projection reduces interference (theoretically proven but empirical robustness across diverse task pairs unverified)
- **Medium confidence:** 5-8% average accuracy improvements over baselines (reproduced on CLIP-ViT but may not generalize to other architectures)

## Next Checks

1. **Task vector similarity analysis:** Systematically measure cosine similarities between task vectors across all task pairs in the 8/14/20-task experiments. Identify correlation between similarity scores and accuracy degradation to quantify the orthogonality assumption's practical limits.

2. **Non-vision architecture testing:** Implement OPCM on transformer language models (e.g., BERT fine-tuned on GLUE tasks) to verify scalability claims beyond vision tasks and CLIP-ViT architecture.

3. **Projection threshold sensitivity sweep:** Run comprehensive ablation studies varying α from 0.1 to 0.9 across all task counts, measuring not just accuracy but also parameter norm drift and interference metrics to characterize the trade-off surface.