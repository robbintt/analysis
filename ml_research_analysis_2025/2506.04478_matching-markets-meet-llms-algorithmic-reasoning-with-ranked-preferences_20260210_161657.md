---
ver: rpa2
title: 'Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences'
arxiv_id: '2506.04478'
source_url: https://arxiv.org/abs/2506.04478
tags:
- stable
- matching
- reasoning
- preference
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically benchmarks large language models on preference-based
  reasoning tasks in matching markets, revealing that even advanced reasoning models
  struggle with larger instances and often fail to execute structured algorithms iteratively.
  Models with reasoning capabilities show strong performance on small problems but
  dramatically drop on larger ones, while fine-tuning with synthetic reasoning traces
  significantly improves results for small instances but not for large ones.
---

# Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences

## Quick Facts
- arXiv ID: 2506.04478
- Source URL: https://arxiv.org/abs/2506.04478
- Authors: Hadi Hosseini; Samarth Khanna; Ronak Singh
- Reference count: 40
- Primary result: LLMs struggle with algorithmic reasoning over ranked preferences, especially at scale

## Executive Summary
This study systematically benchmarks large language models on preference-based reasoning tasks in matching markets, revealing that even advanced reasoning models struggle with larger instances and often fail to execute structured algorithms iteratively. Models with reasoning capabilities show strong performance on small problems but dramatically drop on larger ones, while fine-tuning with synthetic reasoning traces significantly improves results for small instances but not for large ones. Models frequently hallucinate blocking pairs or introduce new instabilities when correcting solutions, and their performance is sensitive to preference structure (impartial culture vs. master-list). Preference comprehension is strong at basic levels but errors compound in multi-step reasoning tasks.

## Method Summary
The study evaluates LLMs on four matching market tasks using synthetic preference profiles across three difficulty levels (Easy n=10, Medium n=20, Hard n=50) and two preference distributions (Impartial Culture and Master-list). Models include basic (Llama-3.3-70B, Gemini-2.0-Flash), reasoning (Qwen-QwQ-32B, DeepSeek-70B), and advanced reasoning (o3-mini, DeepSeek-R1, Gemini-2.5-Pro). Performance is measured via stability rates, instability rates, and optimality rates, with additional fine-tuning experiments using LoRA on synthetic reasoning traces. The evaluation uses 300 synthetic profiles generating 2,850 questions total.

## Key Results
- Even advanced reasoning models achieve only 30-60% stable solutions on Hard instances (n=50)
- Fine-tuning with synthetic reasoning traces improves small-instance performance (100% stable on Easy/Medium) but fails on large instances
- Models hallucinate blocking pairs in ~20% of cases on basic models and introduce new instabilities when correcting solutions
- Performance is significantly better on Master-list profiles (single stable solution) versus Impartial Culture profiles (exponentially many solutions)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with synthetic reasoning traces can improve LLM performance on small-scale preference reasoning tasks. Supervised learning on programmatically generated chain-of-thought traces (from DA algorithm execution) teaches models the stepwise proposal-rejection logic, reducing invalid outputs and improving stability rates. This works because the synthetic traces accurately reflect the desired algorithmic procedure, but does not transfer to large instances where context-length limits dominate.

### Mechanism 2
Error compounding undermines multi-step algorithmic execution over ranked preferences. Small errors in preference retrieval or comparison (e.g., misreading a rank) propagate through iterative steps, leading to incorrect blocking pair identification or unstable matchings. This occurs because the task requires precise, sequential state tracking that models lack explicit working memory for, causing cascading failures in iterative algorithms.

### Mechanism 3
Performance is sensitive to preference profile structure (IC vs ML), with ML being easier due to unique stable solution. Master-list profiles have a single stable solution and simpler O(n) algorithm, reducing combinatorial complexity; IC profiles have exponentially many solutions, increasing search difficulty. Models implicitly leverage structural regularities, but even ML profiles become intractable at Hard scale (n=50), suggesting context-length limits dominate structural benefits.

## Foundational Learning

- Concept: **Two-Sided Matching Markets**
  - Why needed here: This is the core formal framework; all tasks (generation, detection, resolution) are defined within it.
  - Quick check question: Given two disjoint sets M and W, can you explain what a "matching" µ means and why "stability" is the key solution concept?

- Concept: **Deferred Acceptance (DA) Algorithm**
  - Why needed here: The canonical algorithm for computing stable matchings; LLMs are implicitly evaluated on their ability to execute it or its reasoning patterns.
  - Quick check question: Describe one iteration of the proposal phase and rejection phase in DA; what guarantees does the algorithm provide?

- Concept: **Blocking Pairs and Instability**
  - Why needed here: Central to detection and resolution tasks; models must identify when a pair (m,w) mutually prefer each other over current partners.
  - Quick check question: For a given matching µ, define a blocking pair and explain why its absence defines stability.

## Architecture Onboarding

- Component map: Preference profiles (IC/ML) → LLM inference → Parse output → Verify stability/optimality → Compute metrics
- Critical path: Preference profile → LLM inference → Parse output → Verify stability/optimality → Compute metrics. Key failure points are output formatting (invalid JSON) and logical correctness (hallucinated blocking pairs).
- Design tradeoffs: Model size vs. reasoning capability (QwQ-32B outperforms larger DeepSeek-70B), fine-tuning vs. prompting (LoRA beats advanced models on small instances but requires data generation), profile structure vs. difficulty (ML simplifies reasoning but doesn't solve scalability).
- Failure signatures: Hallucination (fabricating blocking pairs or introducing new instabilities), invalid outputs (non-one-to-one matchings or failure to return any solution), context overflow (invalid/failed outputs on Hard instances).
- First 3 experiments: 1) Replicate small-scale generation task (n=10) with basic vs. reasoning models to establish baseline, 2) Ablate fine-tuning: Train LoRA adapter on synthetic DA traces and evaluate generalization, 3) Profile structure analysis: Run detection task on One-BP vs. Random matchings across models to quantify hallucination rates.

## Open Questions the Paper Calls Out

- Can full-parameter fine-tuning or reinforcement-learning methods (e.g., GRPO) overcome the scaling limitations of LoRA fine-tuning for matching markets with large instances (n ≥ 50)?
- At which specific state-transition steps do LLMs fail when executing iterative algorithms like deferred acceptance over ranked preferences?
- How do LLMs perform on preference structures with ties, incompleteness, and capacity constraints—settings where stable solutions may not exist or require more complex algorithms?

## Limitations
- Fine-tuning approach fails to scale beyond small instances despite strong performance on Easy/Medium
- Error compounding mechanism not well understood, with no clear mitigation strategy
- Real-world applicability limited by synthetic data generation and structural biases in IC/ML profiles

## Confidence

- High confidence: Basic preference comprehension tasks, error compounding mechanism, structural sensitivity (IC vs ML)
- Medium confidence: Fine-tuning effectiveness on small instances, hallucination rates in basic models, performance drop at scale
- Low confidence: Transferability of findings to real-world matching markets, scalability solutions beyond synthetic fine-tuning, long-term stability of LoRA improvements

## Next Checks
1. Validate model performance on actual preference data from real matching markets (school choice, kidney exchange) rather than synthetic distributions.
2. Systematically vary LoRA rank, training data size, and instance complexity to identify fundamental limits of the fine-tuning approach.
3. Design controlled experiments to measure how specific preference retrieval errors propagate through DA iterations, identifying critical failure points in the reasoning chain.