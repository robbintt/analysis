---
ver: rpa2
title: Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from
  Synthetic Data Validation
arxiv_id: '2504.12450'
source_url: https://arxiv.org/abs/2504.12450
tags:
- spatial
- moran
- eigenvectors
- learning
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Moran Eigenvectors (ME) improve
  machine learning model performance in spatial data analysis. It compares machine
  learning models using either location coordinates or ME as spatial features across
  synthetic datasets with known spatially varying and nonlinear effects.
---

# Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from Synthetic Data Validation

## Quick Facts
- arXiv ID: 2504.12450
- Source URL: https://arxiv.org/abs/2504.12450
- Reference count: 0
- Primary result: Models using coordinates consistently outperform those using Moran Eigenvectors for spatial prediction in positive autocorrelation scenarios.

## Executive Summary
This paper investigates whether Moran Eigenvectors (ME) improve machine learning model performance in spatial data analysis. Through synthetic data validation with known spatially varying and nonlinear effects, the study compares machine learning models using either location coordinates or ME as spatial features. The research finds that models using only coordinates consistently outperform those using ME, achieving higher cross-validated R² values across multiple geometries and model types. This is attributed to machine learning models' ability to transform coordinates into nonlinear space that captures spatial patterns effectively. While ME are useful in linear models, they may introduce unnecessary complexity in nonlinear ML models.

## Method Summary
The study generates synthetic datasets with known data-generating processes featuring spatially varying coefficients as Gaussian Random Fields. Two spatial feature engineering approaches are compared: using raw coordinates (x,y) or Moran Eigenvectors derived from spatial weights matrices. For ME, the top 200 eigenvectors are computed and pre-selected using LASSO with either MSE or BIC criteria. Four ML model types (Random Forest, LightGBM, XGBoost, TabNet) are trained with 5-fold cross-validation. GeoShapley is used to extract spatial effects from ML models, enabling parameter-level comparisons against true data-generating processes. The primary metric is cross-validated out-of-sample R².

## Key Results
- Models using only coordinates consistently outperform those using Moran Eigenvectors, achieving higher cross-validated R² values across multiple geometries and model types.
- TabNet and tree-based models can effectively approximate spatial patterns directly from coordinate inputs, eliminating the need for explicit spatial basis functions in positive autocorrelation scenarios.
- GeoShapley successfully extracts spatial effects from ML models, enabling validation against known data-generating processes and revealing spatially varying coefficient interactions.

## Why This Works (Mechanism)

### Mechanism 1: Universal Approximation of Spatial Functions from Coordinates
Tree-based and neural network models can approximate spatially varying effects directly from coordinate inputs, eliminating the need for explicit spatial basis functions in scenarios with positive spatial autocorrelation. Decision trees recursively partition coordinate space through axis-aligned splits; increasing tree depth enables finer spatial resolution. Neural networks learn implicit spatial embeddings through nonlinear activations. Both architectures function as universal approximators capable of representing location-dependent functions. Core assumption: The spatial process exhibits smooth, positive autocorrelation where nearby locations have similar values. Break condition: Fails when spatial processes exhibit negative autocorrelation or when network topology—not geographic proximity—governs interactions.

### Mechanism 2: Feature Dimensionality Penalty from Moran Eigenvectors
Pre-computed Moran Eigenvectors add 60-200+ features to the model, increasing optimization complexity and feature selection burden without proportional accuracy gains for standard positive-autocorrelation spatial processes. ML models must learn which eigenvector combinations matter; LASSO-based selection helps but introduces hyperparameter tuning overhead. Coordinates add only 2 features with minimal computational cost. Core assumption: Model capacity is not infinite; feature selection becomes harder as candidate feature count increases. Break condition: When eigenvector pre-selection is highly optimized and the spatial process has known structure matching specific eigenvector patterns.

### Mechanism 3: GeoShapley Decomposes Spatial from Non-Spatial Effects
GeoShapley can extract interpretable spatial effects (ϕGEO) and spatially varying coefficient interactions (ϕ(GEO,j)) from any ML model, enabling validation against known data-generating processes. Treats spatial features as a "joint player" in Shapley value computation; combines Joint Shapley values and Shapley interaction values to separate intrinsic location effects from feature-location interactions. Post-hoc smoothing via GWR yields spatially varying coefficients. Core assumption: The model has learned spatial patterns that can be attributed to the spatial features. Break condition: When model predictions rely on features highly correlated with location but not explicitly included as spatial inputs.

## Foundational Learning

- Concept: **Spatial Autocorrelation (Positive vs. Negative)**
  - Why needed here: The paper's central finding applies only to positive spatial autocorrelation; negative autocorrelation requires explicit eigenvector features.
  - Quick check question: Does nearby proximity imply similar values (positive) or dissimilar values (negative)?

- Concept: **Eigendecomposition of Spatial Weights Matrices**
  - Why needed here: Moran Eigenvectors are derived from eigen decomposition of centered spatial weights; each eigenvector represents a distinct spatial pattern with scale determined by its eigenvalue.
  - Quick check question: Given a Queen contiguity matrix, what does the first eigenvector (largest positive eigenvalue) represent spatially?

- Concept: **Shapley Values and Feature Attribution**
  - Why needed here: GeoShapley extends SHAP to spatial contexts; understanding base Shapley concepts is prerequisite for interpreting ϕGEO and ϕ(GEO,j).
  - Quick check question: How does a Shapley value differ from a regression coefficient in interpretation?

## Architecture Onboarding

- Component map: Data Generation -> Spatial Feature Engineering -> Eigenvector Selection -> Model Training -> Interpretation -> Validation
- Critical path: Define spatial weights matrix (Queen or Exponential kernel) -> Compute top 200 Moran Eigenvectors via Nystrom approximation -> Run LASSO-BIC selection to reduce to 60-120 eigenvectors -> Train ML model with coordinates OR selected eigenvectors -> Apply GeoShapley to extract spatial effects -> Compare R² and visual pattern match to ground truth
- Design tradeoffs:
  - Coordinates vs. ME: Coordinates are simpler (2 features) and often sufficient for positive autocorrelation; ME required for network/negative autocorrelation
  - Queen vs. Exponential kernel: Exponential kernel consistently outperforms Queen contiguity in experiments
  - LASSO-MSE vs. LASSO-BIC: BIC yields sparser models (fewer eigenvectors) and better accuracy
  - Tree-based vs. TabNet: Tree models are more robust to feature dimensionality; TabNet benefits most from pre-selection
- Failure signatures:
  - Coordinate-only models show artifacts: Suggests negative autocorrelation or network effects; switch to ME or graph-based methods
  - ME models with low R² despite many eigenvectors: Likely overfitting or selection failure; try LASSO-BIC with stricter threshold
  - GeoShapley ϕGEO shows no spatial pattern: Model may not have learned spatial effects; increase model capacity or check spatial feature encoding
  - TabNet R² near 0.5 with all 200 ME: Feature selection failure; pre-select with LASSO-BIC
- First 3 experiments:
  1. Baseline comparison: Train XGBoost with (a) no spatial features, (b) coordinates only, (c) all 200 ME, (d) LASSO-BIC selected ME. Report cross-validated R². Expected: coords > LASSO-BIC ME > all ME > no spatial.
  2. GeoShapley validation: On best-performing model, extract ϕ(GEO, X₁) and compare spatial pattern against true β₁ field using visual inspection and correlation. Expected: TabNet/XGBoost with coordinates should show high pattern correspondence.
  3. Negative autocorrelation test: Generate synthetic data with mixed positive/negative autocorrelation; compare coordinate-only vs. ME-inclusive models. Expected: Coordinates fail on negative component; ME with negative eigenvectors succeeds.

## Open Questions the Paper Calls Out

### Open Question 1
How can machine learning models effectively capture negative spatial autocorrelation without relying solely on coordinate features? The authors state this would be an interesting area for future research and demonstrate that coordinates fail to capture negative autocorrelation components. This remains unresolved because the paper only examined positive spatial autocorrelation scenarios.

### Open Question 2
Are graph neural networks more effective than Moran Eigenvector approaches for modeling network autocorrelation in spatial interaction data? The authors note that a graph-based learning framework would be more appropriate for network autocorrelation scenarios where coordinates alone cannot capture network topology. This remains unresolved because the paper focused on areal unit data with positive autocorrelation.

### Open Question 3
How well do these findings generalize to empirical real-world datasets with unknown data-generating processes? The study relies entirely on synthetic data with known ground truth, enabling parameter-level validation but not testing on real datasets where ground truth is unavailable. This remains unresolved because real-world spatial data may contain complex mixtures of processes not represented in the synthetic experimental design.

## Limitations
- Study relies entirely on synthetic data with known ground truth, limiting generalizability to real-world scenarios with unknown spatial processes.
- Findings apply specifically to continuous spatial processes with positive autocorrelation and may not extend to network-based interactions or processes with strong negative autocorrelation.
- Comparison uses only four ML model types, excluding other potentially relevant approaches like spatial autoregressive models or graph neural networks.

## Confidence
- **High Confidence**: ML models using coordinates outperform those using Moran Eigenvectors for typical positive-autocorrelation spatial processes. Consistently supported across multiple geometries, model types, and evaluation metrics.
- **Medium Confidence**: GeoShapley effectively extracts spatial effects from ML models and enables parameter-level comparison with true DGP. Methodology is sound but validation is limited to synthetic scenarios.
- **Medium Confidence**: Moran Eigenvectors remain valuable for network autocorrelation and negative spatial autocorrelation scenarios. Supported by theoretical reasoning but lacks direct experimental validation.

## Next Checks
1. Test coordinate vs. ME performance on real-world spatial datasets with documented spatial autocorrelation patterns to assess generalizability beyond synthetic data.
2. Generate synthetic data specifically designed to exhibit network effects or negative autocorrelation, then systematically compare coordinate-only models against ME-inclusive models.
3. Compare LASSO-based eigenvector selection against alternative approaches (stepwise selection, elastic net, or domain-informed selection) to assess robustness of observed performance advantages.