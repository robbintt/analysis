---
ver: rpa2
title: 'TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon'
arxiv_id: '2509.07381'
source_url: https://arxiv.org/abs/2509.07381
tags:
- uni00000013
- control
- uni00000011
- uni00000014
- transmpc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational complexity challenge in
  traditional online Model Predictive Control (MPC) by introducing TransMPC, a novel
  Transformer-based explicit MPC framework. TransMPC formulates the MPC policy as
  an encoder-only Transformer network that leverages bidirectional self-attention
  to generate entire control sequences in a single forward pass, naturally accommodating
  variable prediction horizons.
---

# TransMPC: Transformer-based Explicit MPC with Variable Prediction Horizon

## Quick Facts
- arXiv ID: 2509.07381
- Source URL: https://arxiv.org/abs/2509.07381
- Authors: Sichao Wu; Jiang Wu; Xingyu Cao; Fawang Zhang; Guangyuan Yu; Junjie Zhao; Yue Qu; Fei Ma; Jingliang Duan
- Reference count: 32
- One-line primary result: TransMPC achieves 81.97-516.74% faster computation than competing methods while maintaining superior solution accuracy for vehicle trajectory tracking across variable prediction horizons.

## Executive Summary
This paper addresses the computational complexity challenge in traditional online Model Predictive Control (MPC) by introducing TransMPC, a novel Transformer-based explicit MPC framework. TransMPC formulates the MPC policy as an encoder-only Transformer network that leverages bidirectional self-attention to generate entire control sequences in a single forward pass, naturally accommodating variable prediction horizons. Unlike imitation-based approaches dependent on precomputed optimal trajectories, TransMPC directly optimizes the true finite-horizon cost through automatic differentiation, using random horizon sampling and a replay buffer to ensure robust generalization across states and horizon lengths.

Extensive evaluations on vehicle trajectory tracking tasks demonstrate that TransMPC achieves significant computational speed-ups (81.97-516.74% faster than competing methods) while maintaining superior solution accuracy across varying prediction horizons (1-20 steps). Real-world experiments on an autonomous mobile robot validate TransMPC's effectiveness in trajectory tracking and obstacle avoidance scenarios. The method achieves higher control accuracy than prior explicit MPC approaches based on MLP or RNN architectures while maintaining stable computation efficiency regardless of horizon length, making it suitable for practical deployment in complex dynamic systems.

## Method Summary
TransMPC is an encoder-only Transformer architecture that directly optimizes the MPC policy by minimizing the true finite-horizon cost through automatic differentiation. The method takes as input the current state and reference trajectory, embeds them separately with positional encodings, processes them through Transformer encoder layers with self-attention, and outputs a control sequence via a linear action decoder. Training alternates between a sampling phase (executing first actions, storing transitions in a replay buffer) and a learning phase (sampling random horizons from the buffer, unrolling differentiable dynamics, backpropagating the accumulated cost). The variable horizon is handled by random sampling from U[1, N_max] during training, forcing the attention mechanism to learn scale-invariant representations.

## Key Results
- TransMPC achieves 81.97-516.74% faster computation than competing methods (MLP, RNN, ZipMPC) while maintaining superior solution accuracy
- The method successfully generalizes across variable prediction horizons (1-20 steps) without retraining
- Real-world experiments on an autonomous mobile robot demonstrate effective trajectory tracking and obstacle avoidance capabilities
- TransMPC outperforms prior explicit MPC approaches in control accuracy while maintaining stable computation efficiency regardless of horizon length

## Why This Works (Mechanism)

### Mechanism 1: Parallel Sequence Generation via Bidirectional Attention
- **Claim:** TransMPC appears to resolve the latency-accuracy trade-off in explicit MPC by reformulating policy inference as a parallel sequence-to-sequence mapping rather than a sequential rollout.
- **Mechanism:** The architecture employs an encoder-only Transformer where bidirectional self-attention allows every control token in the sequence to attend to all future reference tokens simultaneously. This contrasts with Recurrent Neural Networks (RNNs) or unidirectional approaches that must process temporal dependencies step-by-step. By framing the control sequence as a unified "image" of the horizon, the model likely captures global temporal constraints in $O(1)$ inference time relative to the horizon length $N$.
- **Core assumption:** The optimal control input $u_t$ depends non-locally on future reference states, and the parallel non-autoregressive generation does not significantly degrade the solution quality compared to autoregressive methods.
- **Evidence anchors:**
  - [abstract]: "...formulate the MPC policy as an encoder-only Transformer... enabling simultaneous inference of entire control sequences in a single forward pass."
  - [section III.A]: "Encoder layers expose this global context at every depth... the encoder yields the entire action vector in a single, parallel pass, providing O(1) inference latency..."
- **Break condition:** If the control problem requires strictly causal, step-wise error correction that depends on the specific realization of the previous step (which is bypassed in a single-pass generation), accuracy may degrade.

### Mechanism 2: Gradient-Based Policy Extraction (Differentiable Optimization)
- **Claim:** TransMPC likely achieves higher fidelity than imitation-based explicit MPC by optimizing the policy directly against the ground-truth cost function rather than matching expert trajectories.
- **Mechanism:** Instead of Behavioral Cloning (supervised learning on pre-computed optimal trajectories), TransMPC minimizes the expected finite-horizon cost $J(\theta)$ using automatic differentiation. It unrolls the dynamics model $f(x_t, u_t)$ inside the training loop, calculates the cumulative cost $V$, and backpropagates gradients through the entire unrolled horizon to update the Transformer weights. This bypasses the "compounding error" issue common in imitation learning.
- **Core assumption:** The system dynamics $f(x_t, u_t)$ are differentiable and sufficiently smooth to provide meaningful gradients for long-horizon backpropagation.
- **Evidence anchors:**
  - [abstract]: "...directly optimizes the true finite-horizon cost through automatic differentiation..."
  - [section III.B]: "...bypasses optimal control sequence altogether and minimizes the true finite-horizon cost... by gradient descent."
  - [corpus]: Contrasts with "ZipMPC" (corpus neighbor) which relies on imitation learning, suggesting a divergence in methodology for handling context-dependent costs.
- **Break condition:** If the cost function is non-differentiable or the dynamics model is highly inaccurate, the gradient signal will be noisy or misleading, leading to unstable convergence.

### Mechanism 3: Horizon-Robust Generalization via Random Sampling
- **Claim:** The architecture is hypothesized to generalize across variable prediction horizons by treating the horizon length $N$ as a random training variable rather than a fixed structural dimension.
- **Mechanism:** The training loop alternates between sampling and learning. Crucially, it samples a random horizon $N \sim U[1, N_{max}]$ for each batch. Combined with a replay buffer, this forces the Transformer's self-attention mechanism to learn scale-invariant representations of the control problem, rather than overfitting to a specific temporal depth.
- **Core assumption:** The underlying control strategy is smooth with respect to horizon length, and the attention mechanism can extrapolate to unseen lengths within the $N_{max}$ bounds.
- **Evidence anchors:**
  - [abstract]: "Random horizon sampling... ensuring robust generalization across varying states and horizon lengths."
  - [section III.B]: Equation (8) defines the loss expectation over random $N$; Algorithm 1 explicitly samples $N$ before policy inference.
- **Break condition:** Performance may collapse if deployed on horizons significantly exceeding $N_{max}$ (extrapolation failure) or if the state distribution shifts drastically from the replay buffer content.

## Foundational Learning

- **Concept: Model Predictive Control (MPC) Basics**
  - **Why needed here:** TransMPC is an *approximation* of MPC. To debug it, you must understand the "Gold Standard" it tries to emulateâ€”specifically, how the receding horizon optimizes a trade-off between tracking error and control effort.
  - **Quick check question:** If the prediction horizon $N$ is reduced from 20 to 5, how should the controller's aggressiveness theoretically change to maintain stability?

- **Concept: Transformers (Self-Attention & Positional Encoding)**
  - **Why needed here:** The core engine is an encoder-only Transformer. You need to understand that positional encoding is what allows the network to distinguish "step 1" from "step 20" in a parallel sequence, which is vital for temporal control.
  - **Quick check question:** Why does the paper use an *encoder* (bidirectional attention) rather than a *decoder* (causal masking) for predicting future controls?

- **Concept: Differentiable Programming / Automatic Differentiation**
  - **Why needed here:** The paper claims to optimize the MPC cost directly. This requires "unrolling" the physics simulation inside the computation graph.
  - **Quick check question:** What happens to the gradient signal if the dynamics model $f(x,u)$ contains a non-differentiable jump (e.g., a hard collision constraint)?

## Architecture Onboarding

- **Component map:** Current State/Ref Embed -> Transformer Encoder (4-head self-attention) -> Action Decoder (Linear) -> Differentiable Rollout -> Cost Calculation

- **Critical path:** Data Sampler -> [State/Ref Embed] -> [Transformer Encoder] -> [Action Head] -> **[Differentiable Rollout]** -> Cost Calculation -> Backprop

- **Design tradeoffs:**
  - *Encoder vs. Decoder:* The paper chooses an Encoder (bidirectional) to allow $u_t$ to "see" $x_{t+N}$. The tradeoff is the loss of strict causality, but they argue global context improves MPC accuracy.
  - *Direct Opt vs. Imitation:* Direct optimization is computationally cheaper offline (no need to run IPOPT to generate datasets) but requires a differentiable model.

- **Failure signatures:**
  - **Oscillatory Control:** Likely caused by exploding gradients during the long-horizon backprop; check gradient clipping or learning rates.
  - **Horizon Myopia:** The model ignores the tail of the horizon; check if Positional Encodings are functioning correctly or if the cost weighting drops off too steeply.
  - **Latency Spikes:** If inference time grows with $N$, the parallelization logic (batching) is broken.

- **First 3 experiments:**
  1. **Sanity Check (Fixed Horizon):** Train TransMPC and a standard MLP on $N=10$. Verify TransMPC converges faster or to a lower cost due to better temporal inductive bias.
  2. **Variable Horizon Stress Test:** Train with $N \in [1, 20]$. At inference, test $N=15$ (interpolation) and $N=25$ (extrapolation) to verify the attention mechanism's ability to generalize sequence length.
  3. **Ablation on Cost Learning:** Compare the "Direct Optimization" (TransMPC) against a supervised "Imitation Learning" baseline (train on IPOPT solutions) using the same network architecture. Quantify the gap in tracking error and computation time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can TransMPC be adapted to optimize non-differentiable cost functions while retaining its direct policy optimization framework?
- Basis in paper: [explicit] The conclusion states, "Future work will focus on extending TransMPC to handle non-differentiable cost functions."
- Why unresolved: The current training pipeline relies entirely on automatic differentiation (Eq. 6) to propagate gradients through the cost $V$, which requires the loss function to be smooth.
- What evidence would resolve it: A modified TransMPC framework successfully optimizing a cost function with non-smooth or discrete components (e.g., $L_0$ regularization) without relying on smooth surrogate losses.

### Open Question 2
- Question: Can TransMPC guarantee strict satisfaction of hard state constraints, given the current formulation is restricted to unconstrained state spaces?
- Basis in paper: [inferred] Section II.A explicitly defines the MPC problem "without state constraints," relying instead on soft penalties (Eq. 9, 11) for regulation and safety in the experiments.
- Why unresolved: Neural network policy approximations struggle to provide formal guarantees on constraint satisfaction compared to traditional region-based explicit MPC, potentially risking constraint violation in safety-critical tasks.
- What evidence would resolve it: Theoretical analysis or empirical validation showing the policy keeps states within defined feasible sets $\mathcal{X}$ under disturbance, rather than just minimizing a penalty term.

### Open Question 3
- Question: Does TransMPC maintain its computational efficiency advantage over recurrent baselines when scaled to significantly longer prediction horizons (e.g., $N > 50$)?
- Basis in paper: [inferred] The experiments limit prediction horizons to $N=20$, yet the architecture employs self-attention, which typically scales quadratically $O(N^2)$ with sequence length compared to the linear scaling of RNNs.
- Why unresolved: While the paper claims "stable computation efficiency," the inherent quadratic complexity of the attention mechanism may negate the speed benefits for the long-horizon tasks required in high-speed autonomous driving.
- What evidence would resolve it: Benchmarks comparing TransMPC and biRNN inference times on horizons significantly exceeding the tested maximum of 20 steps.

## Limitations

- The evaluation focuses primarily on fixed-speed trajectory tracking scenarios, leaving unclear how the method performs under varying dynamics, environmental disturbances, or multi-objective optimization scenarios
- The reliance on differentiable dynamics models limits applicability to systems with discontinuous or non-smooth dynamics
- The random horizon sampling strategy may create training inefficiencies when the optimal control policy changes dramatically between different horizon lengths

## Confidence

- **High Confidence:** The computational efficiency gains (81.97-516.74% faster) and maintained accuracy relative to IPOPT are well-supported by the presented ablation studies and comparisons with MLP/RNN baselines
- **Medium Confidence:** The generalization across variable horizons is demonstrated within the tested range (1-20 steps), but extrapolation beyond N_max remains an open question that could affect real-world deployment reliability
- **Medium Confidence:** The superiority over imitation-based approaches is established, but the comparison is limited to specific MLP/RNN baselines rather than the full spectrum of explicit MPC methods

## Next Checks

1. **Robustness Testing:** Evaluate TransMPC on scenarios with varying speeds, external disturbances, and non-smooth dynamics to assess performance degradation patterns
2. **Horizon Extrapolation Study:** Systematically test performance on horizons extending beyond N_max (e.g., N=25, N=30) to quantify the attention mechanism's extrapolation capabilities
3. **Cross-Domain Transfer:** Apply TransMPC to a fundamentally different control task (e.g., quadrotor maneuvering or robotic arm control) to verify the method's generalizability beyond vehicle trajectory tracking