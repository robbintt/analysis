---
ver: rpa2
title: 'Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs.
  ImageNet Transfer Learning for Dermatological Diagnosis'
arxiv_id: '2505.16773'
source_url: https://arxiv.org/abs/2505.16773
tags:
- learning
- dermatological
- imagenet
- self-supervised
- overfitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares self-supervised pretraining (using a Variational
  Autoencoder trained from scratch on a proprietary dermatological dataset) with ImageNet
  transfer learning for dermatological image classification. While the ImageNet-pretrained
  model converges faster and achieves higher initial accuracy, it exhibits signs of
  overfitting and stagnation in validation performance.
---

# Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis

## Quick Facts
- arXiv ID: 2505.16773
- Source URL: https://arxiv.org/abs/2505.16773
- Reference count: 10
- Primary result: Self-supervised VAE pretraining on dermatological data shows better generalization and less overfitting than ImageNet transfer learning, despite lower peak accuracy

## Executive Summary
This study compares self-supervised pretraining using a Variational Autoencoder (VAE) trained from scratch on a proprietary dermatological dataset against ImageNet transfer learning for dermatological image classification. While the ImageNet-pretrained model converges faster and achieves higher initial accuracy, it exhibits signs of overfitting and stagnation in validation performance. In contrast, the self-supervised model shows steady improvements without overfitting, achieving strong generalization. The VAE model maintained a near-zero overfitting gap (-0.005) compared to the ImageNet model's increasing gap to +0.060, suggesting domain-specific pretraining may offer superior adaptability for medical imaging tasks.

## Method Summary
The study employed a two-stage approach: (1) VAE pretraining using ConvNeXt-Tiny encoder with random initialization, 256-dimensional latent space, and ELBO loss with KL warm-up for 300 epochs on 120,000 dermatoscopic images, and (2) classification using a frozen encoder with a classifier head (768→256→3 classes with ReLU and 50% dropout) trained for 30 epochs with Focal Loss. The VAE approach was compared against ImageNet transfer learning using the same ConvNeXt-Tiny architecture. Both models were evaluated on a 3-class priority-based triage system (Priority 1: melanoma/SCC/BCC variants, Priority 2: nevus variants/actinic keratosis, Priority 3: benign lesions).

## Key Results
- VAE pretraining achieved validation loss of 0.110 (-33.33% reduction) vs ImageNet's plateau at 0.100 (-16.67%)
- Self-supervised model accuracy improved from 45% to 65% (+44.44%) with minimal overfitting gap, while ImageNet model showed 87% training accuracy (+50.00%) vs 75% validation (+19.05%) with increasing overfitting gap to +0.060
- The VAE-based model showed steady improvements without overfitting, while the ImageNet model exhibited pronounced overfitting tendencies despite faster initial convergence

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Feature Relevance
Self-supervised pretraining on in-domain data captures clinically relevant features (e.g., lesion texture, borders) that generic pretraining misses. A VAE trained on dermatoscopic images learns to reconstruct domain-specific structures by minimizing reconstruction error, forcing the encoder to prioritize features that explain the majority of variance in skin lesions. The features required for dermatological classification are distinct from the edge/shape primitives optimized for ImageNet, and the unlabeled target data is sufficient to learn them.

### Mechanism 2: Overfitting Dynamics in Transfer Learning
The rapid convergence of ImageNet-pretrained models can mask a tendency to overfit to non-clinical correlations (e.g., image artifacts, background), leading to plateauing validation performance. Pretrained weights provide a strong optimization starting point, allowing the model to quickly fit the training data. However, if the source and target domains are misaligned, the model may lock onto spurious features present in the fine-tuning set rather than robust clinical markers, widening the train-validation gap.

### Mechanism 3: Latent Space Regularization via VAE Reconstruction
The reconstruction objective in VAEs acts as a regularizer, forcing the model to learn a smoother, more continuous latent space that generalizes better than discriminative-only features. By optimizing the Evidence Lower Bound (ELBO) with a KL-divergence term, the VAE prevents the encoder from memorizing specific training samples and encourages the learning of dense, meaningful feature clusters.

## Foundational Learning

- **Concept: Domain Shift (Covariate Shift)**
  - Why needed here: The central thesis relies on the premise that natural images (ImageNet) and dermatoscopic images have different distributions. Understanding this shift is required to diagnose why ImageNet features might act as a negative bias.
  - Quick check question: Can you explain why a feature optimized to detect "fur texture" on ImageNet might hinder the detection of "pigment network" in skin lesions?

- **Concept: Variational Autoencoder (VAE) Mechanics**
  - Why needed here: The paper utilizes a VAE rather than a standard Autoencoder or Masked Autoencoder (MAE). One must understand the role of the KL-divergence term in shaping the latent space to prevent overfitting.
  - Quick check question: What happens to the latent space of a standard Autoencoder compared to a VAE, and how does that affect the smoothness of the feature representation?

- **Concept: Overfitting Gap Analysis**
  - Why needed here: The paper concludes that self-supervised learning is superior not solely based on accuracy, but on the stability of the validation loss and the gap between training/validation metrics.
  - Quick check question: If Model A has 65% accuracy and Model B has 75% accuracy, why might Model A still be considered "better" or "safer" for clinical deployment in this context?

## Architecture Onboarding

- **Component map:**
  - ConvNeXt-Tiny Encoder -> 256-dim Latent Space -> Symmetric Decoder (VAE)
  - ConvNeXt-Tiny Encoder -> Classifier Head (768→256→3) (Downstream)

- **Critical path:**
  1. Data Curation: Filter 200k images → dermatoscopic only → map to 3 priority classes
  2. Pretraining: Train VAE (300 epochs) to minimize Reconstruction + KL loss
  3. Freezing: Extract the Encoder and freeze weights
  4. Downstream: Train the Classifier Head (30 epochs) on top of the frozen encoder

- **Design tradeoffs:**
  - Speed vs. Stability: ImageNet pretraining offers fast convergence (slope -0.0047) but risks instability/overfitting. VAE pretraining is slower (slope -0.0003) but offers steady generalization
  - Label Efficiency: The VAE leverages the full unlabeled dataset (120k images) for feature learning, whereas the supervised fine-tuning relies on the labeled subset

- **Failure signatures:**
  - Posterior Collapse: VAE decoder ignores the latent code (loss drops but features are useless)
  - Source Confusion: Model classifies based on "hospital source" rather than "lesion type"
  - Plateauing: Validation loss stagnates (0.100) while training loss continues to drop (0.040)

- **First 3 experiments:**
  1. Baseline Verification: Train the classifier on the frozen ImageNet backbone to replicate the +0.060 overfitting gap
  2. Latent Dimensionality Sweep: Retrain the VAE with 64, 128, and 256 latent dimensions to verify 256 is necessary for reconstruction quality
  3. Encoder Fine-Tuning: Unfreeze the last block of the VAE encoder during classification to see if domain-specific adaptation improves accuracy without reintroducing overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the self-supervised VAE-pretrained model match or exceed ImageNet pretraining validation accuracy with extended training or classifier refinements?
- Basis in paper: The VAE-based model retains the potential for further enhancement, offering room for improvement through additional training stages and classifier refinements... could potentially match or surpass the performance of the ImageNet model
- Why unresolved: The self-supervised model achieved 65% validation accuracy vs. ImageNet's 75%, though with better generalization trends. The study stopped at 70 epochs for classification.

### Open Question 2
- Question: How does the total computational cost of self-supervised VAE pretraining (300 epochs) compare to ImageNet transfer learning when accounting for training time and resource requirements?
- Basis in paper: The paper describes 300 epochs for VAE pretraining but does not report computational costs, despite noting that "computational expense" is a key motivation for transfer learning.

### Open Question 3
- Question: How would alternative self-supervised methods (e.g., contrastive learning, masked image modeling) compare to VAE-based pretraining for dermatological image classification?
- Basis in paper: Only VAE was tested as the self-supervised approach; no comparison to other self-supervised paradigms was conducted.

### Open Question 4
- Question: Would allowing encoder fine-tuning during classification change the relative overfitting behavior between self-supervised and ImageNet-pretrained models?
- Basis in paper: Both encoders were frozen during classification, but the paper does not justify this design choice or explore alternatives.

## Limitations
- The proprietary dataset composition and exact class mapping from 15 original categories to 3 priority groups remain unspecified, limiting reproducibility
- The comparison is between self-supervised pretraining from scratch versus ImageNet transfer learning, but not against alternatives like SimCLR, MAE, or supervised pretraining on other medical datasets
- The study does not evaluate computational efficiency or inference latency, which may be clinically relevant

## Confidence

- **High Confidence:** The overfitting dynamics observed in the ImageNet-pretrained model (87% training vs 75% validation accuracy, +0.060 gap) are clearly demonstrated and mechanistically explained
- **Medium Confidence:** The claim that self-supervised pretraining provides superior generalization is well-supported, but the magnitude of improvement (65% vs 75% validation accuracy) may not justify the additional computational cost in all clinical settings
- **Low Confidence:** The assertion that VAE pretraining specifically learns "clinically relevant features" (e.g., lesion borders) is inferred from performance metrics rather than explicitly validated through feature visualization or ablation studies

## Next Checks
1. **Feature Relevance Validation:** Conduct a qualitative analysis by visualizing the learned latent representations and correlating them with clinically annotated features (e.g., pigment networks, symmetry) to confirm the VAE captures diagnostically relevant patterns
2. **Cross-Domain Generalization Test:** Evaluate both models on an external, independent dermatological dataset (e.g., Derm7pt or PAD-UFES-20) to assess whether the self-supervised model's superior generalization holds across different data distributions
3. **Alternative SSL Method Comparison:** Replace the VAE with a more modern self-supervised method (e.g., SimCLR or MAE) pretrained on the same dermatological data to determine if the observed benefits are specific to VAEs or generalize to other SSL approaches