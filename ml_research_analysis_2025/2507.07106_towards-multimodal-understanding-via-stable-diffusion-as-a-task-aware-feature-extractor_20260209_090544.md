---
ver: rpa2
title: Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature
  Extractor
arxiv_id: '2507.07106'
source_url: https://arxiv.org/abs/2507.07106
tags:
- features
- diffusion
- timesteps
- maps
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores using diffusion models as task-aware visual
  encoders for multimodal understanding. It finds that diffusion features, especially
  from early timesteps and cross-attention layers, capture fine-grained visual semantics
  and align well with text.
---

# Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor

## Quick Facts
- arXiv ID: 2507.07106
- Source URL: https://arxiv.org/abs/2507.07106
- Authors: Vatsal Agarwal; Matthew Gwilliam; Gefen Kohavi; Eshan Verma; Daniel Ulbricht; Abhinav Shrivastava
- Reference count: 40
- Primary result: Conditional diffusion features outperform CLIP on spatial reasoning, with a simple fusion improving multimodal benchmarks by +6%.

## Executive Summary
This work explores using diffusion models as task-aware visual encoders for multimodal understanding. It finds that diffusion features, especially from early timesteps and cross-attention layers, capture fine-grained visual semantics and align well with text. These features outperform CLIP on vision-centric tasks like spatial reasoning. Text conditioning modulates feature representations to focus on query-relevant regions, and cross-attention maps encode strong image-text alignment. However, amplifying text guidance can cause prompt leakage into the LLM, which can be mitigated via caption dropout during training. A simple fusion of CLIP and conditional diffusion features improves performance on multimodal benchmarks, particularly for spatial and compositional reasoning tasks.

## Method Summary
The method freezes Stable Diffusion v2.1-base and extracts features from up-stage cross-attention (U-L1-R1-B0-Cross-Q) at timestep T=50. These 16×16 features are projected to LLM dimension via a 2-layer MLP, then fed to Vicuna-7B (finetuned on LLaVA-Mix-665k). Text conditioning is applied with guidance scale s=4 to modulate features for task-awareness. A caption dropout strategy (randomly omitting text during pretraining) prevents leakage where the LLM recovers the prompt from features. Optional fusion with CLIP uses either concatenation or cross-attention (CLIP queries, diffusion keys/values).

## Key Results
- Diffusion features from early timesteps and cross-attention layers capture fine-grained visual semantics and outperform CLIP on spatial reasoning
- Text conditioning with guidance scale s=4 improves feature alignment with queries and enables task-aware visual representations
- Simple fusion of CLIP and conditional diffusion features yields +6% improvement on MMVP, especially for spatial and compositional reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion features from early timesteps and cross-attention layers capture fine-grained visual semantics that CLIP misses, particularly for spatial and compositional reasoning.
- Mechanism: The denoising process in diffusion models preserves hierarchical visual information—higher timesteps encode coarse layout while lower timesteps (T=10-100) retain fine-grained structure. Cross-attention query features (cross-q) directly interact with text embeddings, yielding better semantic alignment than output features.
- Core assumption: The discriminative utility of diffusion features correlates with their generative training objective; features learned to reconstruct images also encode task-relevant visual structure.
- Evidence anchors:
  - [abstract] "diffusion features, especially from early timesteps and cross-attention layers, capture fine-grained visual semantics and align well with text"
  - [Section 3.1] "higher timesteps capture coarse spatial structure... while intermediate timesteps increasingly highlight finer details"
  - [corpus] Weak direct corpus support; neighbor papers focus on diffusion for navigation/reconstruction, not discriminative feature extraction.
- Break condition: Performance degrades at very high timesteps (T=500+) where noise overwhelms structure, and at down-stage layers where diffusion noise is prevalent.

### Mechanism 2
- Claim: Text conditioning modulates spatial features to focus on query-relevant regions through cross-attention, enabling task-aware visual representations.
- Mechanism: Conditioning the diffusion model on text (questions/captions) shifts intermediate feature representations. The difference between conditional and unconditional features (Δcond) highlights regions semantically related to the text. Amplifying guidance scale (s=4) increases this modulation effect.
- Core assumption: The cross-attention mechanism that grounds text-to-image generation also provides localization signals discriminative for understanding tasks.
- Evidence anchors:
  - [abstract] "Text conditioning modulates feature representations to focus on query-relevant regions"
  - [Section 4.2, Figure 5] "the difference between conditional and unconditional features can highlight key attributes"
  - [corpus] No direct corpus evidence for this specific modulation mechanism in MLLMs.
- Break condition: Excessive guidance (s≥4 without dropout) triggers prompt leakage where the LLM recovers the conditioning text directly from features rather than reasoning from visual content.

### Mechanism 3
- Claim: Fusing CLIP and conditional diffusion features yields complementary coverage—CLIP for global semantics and instruction-following, diffusion for fine-grained visual reasoning.
- Mechanism: CLIP excels at coarse global alignment but misses fine details; diffusion features capture structural/compositional information but underperform on knowledge-heavy tasks. Cross-attention fusion (CLIP queries, diffusion keys/values) or simple concatenation aggregates both information streams.
- Core assumption: The failure modes of CLIP and diffusion features are partially non-overlapping, enabling互补 complementarity.
- Evidence anchors:
  - [abstract] "A simple fusion of CLIP and conditional diffusion features improves performance... +6% improvement on MMVP"
  - [Section 5.2, Table 2] Cross-attention fusion at s=4 yields +7 points on MMVP over LLaVA baseline
  - [corpus] Related work (Prismer, VCoder) combines multiple visual encoders but lacks task-aware conditioning.
- Break condition: Fusion gains diminish if diffusion features are extracted from suboptimal layers (down-stage) or extreme timesteps; guidance must be calibrated to avoid leakage.

## Foundational Learning

- Concept: **Diffusion U-Net architecture and timestep-dependent representations**
  - Why needed here: Feature quality varies dramatically across timesteps and layers; extracting from wrong locations yields noisy or over-smoothed features.
  - Quick check question: At which timestep range do fine-grained details peak versus coarse structure? (Answer: T=10-100 for details, T=250+ for coarse)

- Concept: **Classifier-free guidance (CFG) and guidance scale**
  - Why needed here: Guidance scale controls the strength of text conditioning; understanding this enables intentional modulation and leakage diagnosis.
  - Quick check question: What happens to feature-text alignment when guidance scale increases from 0 to 7? (Answer: Greater text modulation, but risk of leakage at high values)

- Concept: **Cross-attention mechanics in transformer-based diffusion models**
  - Why needed here: Cross-attention maps provide localized text-image alignment; selecting cross-q vs. out features changes semantic properties.
  - Quick check question: Why do cross-q features outperform out features for VQA tasks? (Answer: Cross-q directly interacts with text embeddings, yielding better alignment)

## Architecture Onboarding

- Component map:
  - **Stable Diffusion v2.1-base** (frozen) -> **U-L1-R1-B0-Cross-Q feature extraction** (T=50) -> **2-layer MLP projection** -> **Vicuna-7B LLM** -> **Response generation**

- Critical path:
  1. Extract diffusion features at T=50 from up-stage cross-attention (U-L1-R1-B0-Cross-Q)
  2. Apply text conditioning (question or null) with controlled guidance scale
  3. Project features to LLM dimension via trained MLP
  4. If fusing, combine with CLIP features before LLM input
  5. Generate response via Vicuna-7B

- Design tradeoffs:
  - **Timestep selection**: Lower T (10-50) favors fine details; higher T (100-250) favors global structure. Default T=50 balances both.
  - **Guidance scale**: s=0 (unconditional) for pure visual features; s=4 for strong task-awareness but requires caption dropout to prevent leakage.
  - **Fusion strategy**: Concatenation is simpler; cross-attention (CLIP→Q, Diffusion→K,V) yields better MMVP performance but adds complexity.

- Failure signatures:
  - **Leakage**: LLM regenerates the conditioning prompt verbatim; caused by high guidance without dropout. Mitigation: 30% caption dropout during pretraining.
  - **Noisy features**: Degraded performance on all tasks; likely using down-stage (D-L2) features. Fix: Switch to up-stage (U-L1).
  - **Knowledge gap**: Poor instruction-following compared to CLIP baseline; diffusion features lack global semantics. Fix: Add CLIP fusion.

- First 3 experiments:
  1. **Baseline feature extraction**: Train projection head only with unconditional features (s=0) at T=50 from U-L1-R1-B0-Cross-Q; evaluate on MMVP and BLINK-val to verify diffusion feature quality.
  2. **Text conditioning ablation**: Compare unconditional (s=0) vs. question-conditioned (s=4) features with/without caption dropout; measure MMVP gains and leakage via mismatched image-text pairs.
  3. **Fusion validation**: Implement cross-attention fusion of CLIP and diffusion features (s=4); benchmark against CLIP-only and diffusion-only baselines on MMVP, GQA, and NaturalBench.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an optimal ensembling strategy that leverages features from multiple guidance scales simultaneously achieve better performance than using a single guidance scale or CLIP alone?
- Basis in paper: [explicit] The discussion section states: "we envision that an ideal ensembling strategy for leveraging each guidance scale's features could achieve better performance than CLIP and be explored as future work."
- Why unresolved: The paper only experiments with fixed guidance scales (s=0, s=4) and simple concatenation or cross-attention fusion, not multi-scale ensembling.
- What evidence would resolve it: Systematic experiments combining features across multiple guidance scales with learned weighting or attention-based selection, evaluated on MMVP, BLINK-val, and NaturalBench.

### Open Question 2
- Question: Can more sophisticated fusion architectures beyond simple concatenation and cross-attention better exploit the complementary strengths of CLIP and diffusion features?
- Basis in paper: [inferred] The paper uses only two basic fusion strategies (concatenation and cross-attention with CLIP as queries), achieving +6% on MMVP but leaving potential gains unexplored.
- Why unresolved: The paper acknowledges these are "simple" strategies without exploring transformer-based fusion, gated mechanisms, or learned feature selection.
- What evidence would resolve it: Comparing multiple fusion architectures (attention-based, gating networks, multi-layer feature pyramids) on the same benchmarks with consistent training protocols.

### Open Question 3
- Question: Does the leakage phenomenon persist with more powerful diffusion models (e.g., SD3, Flux) or different training objectives, and can architectural modifications prevent it entirely?
- Basis in paper: [explicit] The paper uncovers that "the LLM can inadvertently recover information from the original diffusion prompt" and proposes caption dropout as mitigation, but this is a training workaround rather than a fundamental solution.
- Why unresolved: The leakage is characterized and partially mitigated, but its root causes in the diffusion architecture and whether it generalizes across model families remain unclear.
- What evidence would resolve it: Probing experiments across different diffusion architectures measuring leakage rates, and analysis of which architectural components (cross-attention layers, text encoder connections) most contribute to information leakage.

## Limitations

- Performance improvements are modest (+6-7 points) on limited benchmarks (MMVP, NaturalBench)
- Diffusion model training corpus differs from CLIP, confounding performance comparisons
- Caption dropout is heuristic mitigation for leakage without systematic validation across scales
- Feature extraction protocol optimized for this architecture may not generalize to other diffusion models

## Confidence

- **High Confidence**: Diffusion features from cross-attention layers outperform unconditional output features for text-image alignment
- **Medium Confidence**: Text conditioning with guidance scale s=4 improves spatial reasoning performance
- **Medium Confidence**: CLIP+diffusion fusion provides complementary coverage

## Next Checks

1. **Cross-model generalization test**: Extract features from Stable Diffusion v1.5 and DeepFloyd IF using the same T=50, U-L1-R1-B0-Cross-Q protocol; compare performance on MMVP to isolate architecture effects from training corpus effects.

2. **Guidance scale sweep with leakage measurement**: Systematically evaluate guidance scales s∈{0,2,4,6,8} with and without caption dropout; measure both task performance and direct prompt reconstruction likelihood to establish the exact leakage threshold.

3. **Layer/timestep ablation study**: Compare performance across timesteps T∈{10,25,50,100,250} and layers (U-L1 vs D-L2) on MMVP; generate feature similarity heatmaps to validate the claim that higher timesteps capture coarse structure versus fine details.