---
ver: rpa2
title: 'AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with
  Cross-Attention and Squeezeformer for Speech Enhancement'
arxiv_id: '2510.05295'
source_url: https://arxiv.org/abs/2510.05295
tags:
- speech
- audio
- enhancement
- visual
- audio-visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement

## Quick Facts
- arXiv ID: 2510.05295
- Source URL: https://arxiv.org/abs/2510.05295
- Reference count: 0
- Primary result: SOTA on AVSE4 dataset (PESQ: 1.325, STOI: 0.514, SI-SDR: -4.312 dB)

## Executive Summary
AUREXA-SE introduces a novel audio-visual speech enhancement architecture that leverages bidirectional cross-attention and raw waveform processing to achieve state-of-the-art performance on the AVSE4 dataset. The model processes raw 16kHz audio through a 1D U-Net encoder while extracting visual features from 25 FPS video using a Swin Transformer V2, then fuses these modalities through iterative cross-attention. A Squeezeformer module provides efficient temporal modeling before a U-Net decoder reconstructs the enhanced waveform. The architecture addresses phase fidelity issues inherent in spectrogram-based approaches and achieves superior performance through unified representation exchange between audio and visual modalities.

## Method Summary
The architecture processes raw audio waveforms through a 1D convolutional U-Net encoder and video frames through a Swin Transformer V2, then fuses them via bidirectional cross-attention where audio and video features query each other's representations. The fused embeddings pass through stacked Squeezeformer blocks for efficient temporal modeling, followed by a U-Net decoder with skip connections to reconstruct the enhanced waveform. The model is trained end-to-end using MSE loss on the AVSE4 dataset with 34,524 training scenes, processing 3-second clips at 16kHz audio and 25 FPS video.

## Key Results
- Achieves state-of-the-art PESQ score of 1.325 on AVSE4 test set
- Demonstrates superior STOI performance at 0.514 compared to existing methods
- Shows SI-SDR improvement to -4.312 dB with 54.2M parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bidirectional cross-attention creates robust speech representations by anchoring noisy audio features to noise-agnostic visual cues.
- **Mechanism:** The architecture employs a mutual querying process where audio features act as queries against video keys/values and vice versa. This allows the model to filter acoustic information that lacks corresponding visual evidence (e.g., mouth movement), effectively separating speech from masking noise.
- **Core assumption:** Visual cues (lip movements) are temporally aligned with the audio signal and remain intelligible despite acoustic noise levels.
- **Evidence anchors:**
  - [Page 3, Section 3.4]: "This bi-directional interaction ensures each modality is updated with relevant context from the other... stabilising learning."
  - [Page 4, Section 3.5.2]: "Operating on post-fusion embeddings enables the model to learn joint sequential patterns, capturing synchronisation between visual speech cues and acoustic events."
  - [Corpus]: Related work (e.g., *Router-Gated Cross-Modal Feature Fusion*) suggests that dynamic modality reweighting is critical in noisy environments, supporting the utility of attention-based fusion.
- **Break condition:** Performance degrades if the video input is corrupted, occluded, or if the frame rate is insufficient to capture phoneme-level visual transitions (McGurk effect dependency).

### Mechanism 2
- **Claim:** Raw waveform processing preserves phase and temporal fidelity better than Time-Frequency (TF) masking approaches.
- **Mechanism:** By bypassing the STFT transformation, the 1D U-Net encoder avoids the phase estimation problem inherent in spectrogram reconstruction. The model learns implicit feature extraction directly from amplitude samples, preserving fine-grained temporal structures required for intelligible speech.
- **Core assumption:** The 1D convolutional kernel sizes and strides are sufficient to capture the necessary frequency bands without explicit frequency-domain decomposition.
- **Evidence anchors:**
  - [Page 2, Section 2]: "...spectrogram-based inputs... can compromise fine temporal resolution and phase fidelity, ultimately affecting the precision of speech reconstruction."
  - [Page 3, Section 3.2]: "This hierarchical structure enables the model to extract multi-scale temporal features... retaining essential speech patterns."
  - [Corpus]: Evidence is supported by general trends in the corpus (e.g., *Real-Time Audio-Visual Speech Enhancement*), where end-to-end learning is increasingly favored for robustness, though specific phase-metric comparisons are not detailed in the provided text.
- **Break condition:** The mechanism may struggle with generalized noise types if the training data diversity does not cover the acoustic space, as the model learns filters from scratch rather than relying on fixed basis functions (like FFTs).

### Mechanism 3
- **Claim:** The Squeezeformer module balances computational efficiency with long-range temporal dependency modeling.
- **Mechanism:** By integrating a "squeeze" operation for temporal downsampling within the attention blocks, the model reduces the sequence length before applying self-attention. This lowers complexity from quadratic $O(T^2)$ to $O(T \log T)$, allowing the network to process long audio sequences (37,830 samples) while maintaining a global receptive field.
- **Core assumption:** The aggressive temporal subsampling in the squeeze operation does not discard critical transient information needed for speech clarity.
- **Evidence anchors:**
  - [Page 3, Section 3.5.1]: "...reduces computational complexity from O(T^2) to O(T log(T))... making it suitable for processing raw audio waveforms."
  - [Page 4, Section 3.5.2]: Describes the block as capturing "global dependencies" via attention and "local patterns" via depth-wise convolutions.
  - [Corpus]: Weak direct evidence in the provided corpus summaries regarding Squeezeformer specifically, but *Scalable Frameworks for Real-World AVSR* emphasizes efficiency in real-world deployment.
- **Break condition:** Rapid speech or high-frequency phonetic transitions may be "squeezed out" if the downsampling factor is too aggressive for the 16kHz signal.

## Foundational Learning

- **Concept: Cross-Attention (Query-Key-Value)**
  - **Why needed here:** This is the engine of the "Unified Representation Exchange." You must understand how one modality (Audio) "queries" the other (Video) to retrieve relevant context (Keys/Values).
  - **Quick check question:** If the video feature corresponding to a silence frame is passed, what should the audio-to-video attention weight ideally approach?

- **Concept: U-Net Skip Connections**
  - **Why needed here:** The decoder relies on these connections to recover high-resolution waveform details lost during encoding.
  - **Quick check question:** Why would a simple linear bottleneck fail to reconstruct high-fidelity speech compared to a U-Net with skip connections?

- **Concept: Raw Waveform vs. Spectrogram**
  - **Why needed here:** The authors explicitly reject spectrograms to solve phase estimation issues.
  - **Quick check question:** What specific piece of information is typically lost or difficult to recover when converting a waveform to a magnitude spectrogram?

## Architecture Onboarding

- **Component map:** Raw Audio → 1D U-Net Encoder → Cross-Attention → Squeezeformer → Cross-Attention → U-Net Decoder → Waveform; Raw Video → Swin Transformer V2 → Cross-Attention → Squeezeformer → Cross-Attention

- **Critical path:** The temporal alignment before the Cross-Attention block. The audio is downsampled, and video is interpolated. If this alignment is off by even a few ms, the cross-attention mechanism learns misaligned correlations (hearing "ba" but seeing "ga").

- **Design tradeoffs:**
  - **Efficiency vs. Quality:** The model uses 54.2M parameters (heavy) and has a 40-min inference time (vs. 25-min baseline) to achieve SOTA quality.
  - **Receptive Field vs. Resolution:** The Squeezeformer trades temporal resolution for a wider receptive field and lower compute cost.

- **Failure signatures:**
  - **Lip-Sync Drift:** Output speech sounds robotic or slurred; usually indicates the Video-Audio alignment in the fusion layer is failing.
  - **Muted Output:** Over-smoothing by the Squeezeformer or excessive clamping in the decoder.
  - **Phase Artifacts:** Harsh, metallic sounds in the output, suggesting the raw waveform convolution filters have not converged effectively.

- **First 3 experiments:**
  1. **Modality Ablation:** Feed zero-tensors for video input to quantify the gain specifically from the visual stream vs. the audio-only path.
  2. **Attention Visualization:** Visualize the audio-to-video attention maps to verify if the model focuses on the mouth region during voiced segments and ignores it during silence.
  3. **Inference Latency Profiling:** Profile the Squeezeformer vs. the Cross-Attention block to identify the actual bottleneck causing the 40-minute inference time.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does AUREXA-SE performance degrade when exposed to noise types or acoustic environments significantly different from the AVSE4 training distribution?
- **Basis in paper:** [explicit] The authors state, "Ongoing future work aims to address current limitations of the proposed model, specifically its robustness to unseen noise types."
- **Why unresolved:** The current evaluation is restricted to the AVSE4 dataset (both development and test sets), which may not fully represent the diversity of real-world acoustic conditions or "unseen" noise profiles mentioned as a limitation.
- **What evidence would resolve it:** Evaluation results on out-of-domain datasets or specific noise types excluded from the AVSE4 training mixture.

### Open Question 2
- **Question:** Can the current bidirectional cross-attention and Squeezeformer architecture be extended to handle multi-speaker separation without a significant increase in computational load?
- **Basis in paper:** [explicit] The conclusion lists plans to "extend the architecture to support multi-speaker separation."
- **Why unresolved:** The current framework processes a single target speaker using a specific audio-visual fusion pipeline; it is unclear if the fusion mechanism can disentangle multiple overlapping speech signals simultaneously.
- **What evidence would resolve it:** Modification of the output layer and loss function to support multiple source estimation, followed by benchmarking on multi-speaker datasets.

### Open Question 3
- **Question:** What architectural modifications are necessary to achieve real-time speech enhancement capabilities given the current model's high parameter count (54.2M) and reported inference latency?
- **Basis in paper:** [explicit] The authors acknowledge a trade-off where the model requires "40 minutes [inference time] versus the baseline's 25" and explicitly plan to "enhance AUREXA-SE with real-time capabilities."
- **Why unresolved:** The current Squeezeformer and Swin Transformer V2 components, while efficient relative to dense transformers, still impose a computational burden that challenges real-time streaming deployment.
- **What evidence would resolve it:** A reported Real-Time Factor (RTF) of less than 1.0 on standard CPU/GPU hardware or a pruned/quantized version of the model maintaining similar metric scores.

### Open Question 4
- **Question:** How can the visual encoding pipeline be stabilized to handle real-world video artifacts such as partial occlusion of the lips or extreme lighting variations?
- **Basis in paper:** [explicit] The authors note plans to include "robust visual encoding to better handle real-world challenges."
- **Why unresolved:** The paper utilizes standard preprocessing (resizing, normalization) on the AVSE4 dataset but does not evaluate the model's robustness against visual noise or missing facial landmarks common in unconstrained settings.
- **What evidence would resolve it:** Ablation studies injecting visual noise (e.g., masking, blur) into the test set and reporting the resulting degradation in PESQ/STOI.

## Limitations

- **Temporal alignment sensitivity:** The bidirectional cross-attention mechanism assumes perfect synchronization between visual and audio streams, but no explicit alignment mechanism is described beyond linear interpolation.
- **Computational overhead:** The model's 54.2M parameters and 40-minute inference time suggest significant computational burden compared to baseline systems, though efficiency comparisons are not detailed.
- **Generalization uncertainty:** The model's performance on out-of-domain noise types, severe visual occlusions, or non-face speakers is not evaluated, limiting confidence in real-world applicability.

## Confidence

- **High Confidence** (Mechanistic understanding): The core architectural components (U-Net, cross-attention, Squeezeformer) are well-established in the literature, and the described mechanisms align with known principles of multi-modal fusion and temporal modeling.
- **Medium Confidence** (Performance claims): While the reported metrics (PESQ 1.325, STOI 0.514, SI-SDR -4.312 dB) appear competitive, the lack of baseline comparisons and detailed hyperparameter information makes it difficult to assess relative performance improvements.
- **Low Confidence** (Generalization claims): The model's performance on out-of-domain noise types, severe visual occlusions, or non-face speakers is not evaluated, limiting confidence in real-world applicability.

## Next Checks

1. **Temporal Alignment Verification:** Measure the impact of audio-video misalignment by introducing controlled temporal offsets (±50ms) and evaluating degradation in cross-attention weight patterns and overall speech quality metrics.

2. **Modality Dependency Analysis:** Conduct systematic ablation studies removing visual input entirely, then progressively reducing visual quality (resolution, frame rate, occlusion) to quantify the contribution of each visual component to overall performance.

3. **Phase Reconstruction Assessment:** Compare phase-related metrics (e.g., overall phase error, log-spectral distance) between the raw waveform approach and traditional spectrogram-based methods on the same test set to validate the claimed advantage of avoiding STFT transformation.