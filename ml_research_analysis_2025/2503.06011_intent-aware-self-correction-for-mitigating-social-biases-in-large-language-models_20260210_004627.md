---
ver: rpa2
title: Intent-Aware Self-Correction for Mitigating Social Biases in Large Language
  Models
arxiv_id: '2503.06011'
source_url: https://arxiv.org/abs/2503.06011
tags:
- cross-model
- feedback
- response
- self-corr
- debiasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an intent-aware Self-Correction framework
  for mitigating social biases in Large Language Models (LLMs). The core idea is to
  clarify intentions at three key components: instruction, response, and feedback.'
---

# Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models

## Quick Facts
- arXiv ID: 2503.06011
- Source URL: https://arxiv.org/abs/2503.06.011
- Reference count: 24
- Primary result: Intent-aware Self-Correction framework achieves more consistent and robust debiasing compared to baselines across nine bias categories in BBQ benchmark

## Executive Summary
This paper introduces an intent-aware Self-Correction framework to mitigate social biases in Large Language Models (LLMs). The framework operates through three clarification components: a debiasing prompt for instructions, Chain-of-Thought for response reasoning, and multi-aspect critiques for feedback. By making intentions explicit at each stage, the framework addresses a key limitation of existing self-correction approaches where bias mitigation goals may be unclear. The method is evaluated on the Bias Benchmark for QA (BBQ) across nine bias categories using GPT-3.5, GPT-4o-mini, and LLaMA-3, demonstrating superior debiasing performance compared to baselines.

## Method Summary
The intent-aware Self-Correction framework enhances self-correction by explicitly clarifying intentions at three key components: instruction, response, and feedback. For the instruction component, a debiasing prompt is appended to the original instruction to convey the bias mitigation goal. For the response component, Chain-of-Thought (CoT) is employed to make the reasoning process explicit and transparent. For the feedback component, multi-aspect critiques (Coherent, Comprehensive, Objective) and scoring are defined to provide clear, structured feedback on the response. The framework uses a two-stage process: first generating a response, then generating feedback based on the response, and finally refining the response using the feedback.

## Key Results
- Intent-aware Self-Correction framework achieves more consistent and robust debiasing compared to baselines on BBQ across nine bias categories
- Cross-model correction (using different model for feedback) outperforms same-model correction, though same-model is sufficient for low-bias models
- Framework effectiveness is highly sensitive to feedback quality and source, with feedback quality being the key factor influencing refined response quality

## Why This Works (Mechanism)
The framework addresses the fundamental problem that existing self-correction methods often lack explicit communication of bias mitigation intentions. By making intentions clear at the instruction (debiasing prompt), response (CoT), and feedback (multi-aspect scoring) stages, the framework creates a more structured and transparent correction process. This explicitness helps the model understand and follow the bias mitigation goal more effectively, leading to improved debiasing performance.

## Foundational Learning
- **Bias Benchmark for QA (BBQ)**: Dataset specifically designed to evaluate social biases in question answering systems; why needed - provides standardized evaluation across multiple bias categories; quick check - examine BBQ dataset documentation for bias category definitions
- **Chain-of-Thought (CoT) reasoning**: Method of prompting models to show their reasoning process step-by-step; why needed - makes implicit reasoning explicit for better feedback; quick check - verify CoT implementation in response generation
- **Cross-model correction**: Using different model for feedback generation than the one being corrected; why needed - prevents feedback from favoring original response; quick check - compare performance metrics between cross-model and same-model settings
- **Multi-aspect critique framework**: Structured evaluation criteria (Coherent, Comprehensive, Objective) for feedback; why needed - provides consistent, interpretable feedback; quick check - review critique generation prompt for completeness
- **Self-correction refinement**: Iterative process of improving responses based on feedback; why needed - enables progressive bias mitigation; quick check - verify refinement loop implementation

## Architecture Onboarding

**Component Map**: Instruction -> Response Generator -> Feedback Generator -> Refinement Module

**Critical Path**: Instruction (with debiasing prompt) → Response Generation (with CoT) → Feedback Generation (multi-aspect critique) → Response Refinement

**Design Tradeoffs**: Cross-model correction provides better bias mitigation but requires additional computational resources; same-model correction is more efficient but less effective. The choice between them depends on available resources and required bias mitigation level.

**Failure Signatures**: Low RF scores indicate either poor feedback quality or response generator's inability to follow instructions; bias may persist if debiasing prompt is ineffective; feedback may be too lenient if same-model correction is used without careful prompt engineering.

**First Experiments**:
1. Test debiasing prompt effectiveness by comparing responses with and without the debiasing prompt
2. Evaluate feedback quality by having human raters assess coherence, comprehensiveness, and objectivity
3. Compare cross-model versus same-model correction performance across different bias categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the intent-aware self-correction framework maintain its debiasing efficacy when applied to tasks other than question answering (e.g., Natural Language Inference) and in non-English languages?
- Basis: The authors explicitly state in the Limitations section (Section 7) that extending the validation to "other formats of instructions, evaluation in more task formats, or non-English language settings" is an "essential next step."
- Why unresolved: The current study exclusively evaluates the framework using the Bias Benchmark for QA (BBQ) in English, leaving its generalizability across different linguistic structures and task formats unproven.
- What evidence would resolve it: Experimental results showing the framework's performance on benchmarks like JBBQ (Japanese), CBBQ (Chinese), or Natural Language Inference tasks.

### Open Question 2
- Question: Can automated prompt optimization techniques yield superior feedback generation instructions compared to the manually designed prompts used in this study?
- Basis: In Section 7, the authors note that their feedback generation prompt was "manually designed by the authors" and suggest applying "prompt optimization technique to search for more optimal prompts" as a potential next step.
- Why unresolved: It is currently unknown if the specific evaluation aspects (Coherent, Comprehensive, Objective) defined by the authors are optimal or if machine-discovered prompts could elicit more constructive feedback for debiasing.
- What evidence would resolve it: A comparison of debiasing performance (accuracy and diff-bias score) using feedback generated from automated prompts versus the manual baseline.

### Open Question 3
- Question: How can evaluation metrics be refined to disentangle the cause of refinement failure, specifically distinguishing between poor feedback quality and the response generator's inability to follow instructions?
- Basis: The authors acknowledge in Section 7 that their current rule-based metrics cannot "exclusively distinguish if a low RF score indicates poor feedback instruction-following capability... or poor quality of the feedback."
- Why unresolved: A low Refinement (RF) score currently conflates two distinct failure modes, making it difficult to isolate which component (the feedback generator or the response generator) needs improvement.
- What evidence would resolve it: The development and validation of a semantic-level evaluation metric that correlates with human judgment on the utility of feedback versus the compliance of the response.

### Open Question 4
- Question: To what extent does the superior performance of cross-model correction depend on the feedback generator's lower bias levels versus the separation of conversation contexts?
- Basis: The paper notes that cross-model settings "do not share the conversation contexts" (Section 2) and that same-model correction often results in feedback that "favors the response" (Section 4). It is unclear if the benefit comes from the model's intrinsic low bias or simply the lack of shared context history.
- Why unresolved: Understanding this distinction is necessary to determine if same-model correction can be made as effective as cross-model correction without the computational cost of running multiple models.
- What evidence would resolve it: An ablation study where cross-model correction is performed with shared context versus isolated context, controlling for model bias levels.

## Limitations
- Framework effectiveness is highly sensitive to feedback quality and source, raising concerns about reproducibility across different implementations
- Results primarily measure bias mitigation through automated scoring with limited discussion of potential trade-offs with other aspects of response quality
- Study focuses on nine bias categories from BBQ dataset, with unclear generalizability to other bias types or real-world applications

## Confidence
- **High confidence**: Claims regarding framework's effectiveness compared to baselines within tested conditions, particularly cross-model correction outperforming same-model correction
- **Medium confidence**: Claims about sufficiency of same-model correction for low-bias models (context-dependent), assertion that feedback quality is key factor influencing refined response quality
- **Low confidence**: Not applicable - no low confidence claims identified

## Next Checks
1. Test the framework's effectiveness across additional bias benchmarks beyond BBQ to assess generalizability
2. Conduct human evaluation studies to validate automated scoring and assess potential trade-offs with response quality metrics beyond bias mitigation
3. Experiment with different feedback generator configurations and quality metrics to establish robustness across implementations