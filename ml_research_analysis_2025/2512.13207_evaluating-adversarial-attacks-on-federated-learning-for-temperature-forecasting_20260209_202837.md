---
ver: rpa2
title: Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting
arxiv_id: '2512.13207'
source_url: https://arxiv.org/abs/2512.13207
tags:
- data
- attack
- attacks
- learning
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates adversarial attacks on federated learning
  for temperature forecasting using the CERRA dataset. We demonstrate that data poisoning
  attacks, even from a small fraction of compromised clients, can significantly degrade
  model performance or introduce systematic biases.
---

# Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting

## Quick Facts
- **arXiv ID:** 2512.13207
- **Source URL:** https://arxiv.org/abs/2512.13207
- **Reference count:** 40
- **Primary result:** Adversarial attacks on federated learning for temperature forecasting can significantly degrade model performance or introduce systematic biases, with defense mechanisms showing limited effectiveness against spatially correlated data.

## Executive Summary
This study investigates adversarial attacks on federated learning for temperature forecasting using the CERRA dataset. We demonstrate that data poisoning attacks, even from a small fraction of compromised clients, can significantly degrade model performance or introduce systematic biases. A global temperature bias attack from a single client can shift predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce regional anomalies exceeding +3.5 K. We evaluate trimmed mean aggregation as a defense mechanism, finding it successfully defends against global bias attacks (2-13% degradation) but fails against patch attacks (281-603% amplification), revealing limitations of outlier-based defenses for spatially correlated meteorological data.

## Method Summary
The research uses the CERRA dataset (European reanalysis) partitioned into a 3×3 grid (9 clients) with each client receiving a 64×64 spatial tile. A residual U-Net model performs image-to-image temperature forecasting (t₀, t₋₁ → t₊₁) using FedAvg with 10 rounds and 10 local epochs per round. Two attack types are implemented: Global Temperature Bias Attack (GTBA) that shifts all targets by -2K on malicious clients, and Patch Attack that injects a 20×20 pixel anomaly in the top-left corner. Trimmed mean aggregation (trim_ratio=0.2) serves as the defense mechanism. The study evaluates attacks with 1, 3, and 5 malicious clients starting at rounds 0 and 5.

## Key Results
- A single compromised client using GTBA can introduce a systematic global bias shifting predictions by up to -1.7 K
- Coordinated patch attacks from 3-5 clients more than triple the mean squared error (from 4.12 to 33.35) and create regional anomalies exceeding +3.5 K
- Trimmed mean aggregation successfully defends against global bias attacks (2-13% degradation) but fails against patch attacks (281-603% amplification)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single compromised client can introduce a systematic global bias by persistently modifying training targets, which aggregates into the global model through averaging.
- **Mechanism:** The Global Temperature Bias Attack (GTBA) modifies the loss function on malicious clients to minimize the distance to a shifted target ($y - \beta$) rather than ground truth. Because Federated Averaging (FedAvg) aggregates parameters without inspecting raw data, the biased gradient updates are averaged into the global weights, shifting predictions for all clients.
- **Core assumption:** The central aggregation server utilizes a standard averaging rule (FedAvg) that weights contributions equally or by dataset size, without validating physical plausibility or directional consistency of the updates.
- **Evidence anchors:**
  - [abstract] "A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K."
  - [section 3.3.2] "Malicious clients minimize the biased loss: $L_{GTBA}(\theta, x, y) = \|\hat{y}(\theta, x) - (y - \beta)\|^2$."
  - [corpus] "Poisoning Attacks and Defenses to Federated Unlearning" supports the general vulnerability of distributed FL to malicious local updates.
- **Break condition:** This mechanism fails if the aggregation rule filters updates based on statistical outliers (e.g., Trimmed Mean) or directional consistency, as GTBA creates distinct deviations in parameter space.

### Mechanism 2
- **Claim:** Localized patch attacks evade statistical outlier defenses because the adversarial updates resemble the natural spatial heterogeneity (non-IID) of meteorological data.
- **Mechanism:** A Patch Attack injects a localized anomaly (e.g., a hot spot) into the training data of a few clients. Because meteorological data is inherently non-IID (different regions have different climates), the resulting model updates fall within the natural variance of the federation. "Robust" aggregation methods like Trimmed Mean incorrectly classify these adversarial updates as legitimate regional variations while potentially discarding honest outliers.
- **Core assumption:** The defense mechanism relies solely on statistical distribution of weights and lacks domain-specific constraints (e.g., spatial continuity or physical limits) to distinguish malicious heat anomalies from legitimate micro-climates.
- **Evidence anchors:**
  - [abstract] "...trimmed mean aggregation... fails against patch attacks (281-603% amplification), exposing limitations of outlier-based defenses for spatially correlated data."
  - [section 6] "Patch attacks... exploit the inherent heterogeneity of meteorological data... [adversarial contributions] fall within this natural variability rather than outside it."
  - [corpus] "FedNIA" and "Hybrid Reputation Aggregation" propose alternative defenses, but do not specifically address the camouflaging effect of spatial non-IID data described here.
- **Break condition:** This evasion fails if the defense mechanism incorporates spatial consistency checks or physical constraints that flag static, high-intensity patches as physically implausible over time.

### Mechanism 3
- **Claim:** Spatial correlations in convolutional models allow localized perturbations to propagate and degrade forecast accuracy in neighboring regions.
- **Mechanism:** The U-Net architecture uses convolutional layers with shared weights and skip connections. When a client trains on a poisoned patch, the learned weights adjust to accommodate the anomaly. During global aggregation, these corrupted weights are distributed to all clients, causing the model to hallucinate anomalies or lose accuracy in spatially connected areas that share features with the poisoned region.
- **Core assumption:** The model architecture uses shared weights across spatial dimensions (CNNs/U-Net), implying that learning in one spatial region modifies parameters used for inference in other regions.
- **Evidence anchors:**
  - [abstract] "These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation."
  - [section 1] "Poisoned observations contaminate the global model and propagate through spatial correlations, distorting predictions across neighboring regions."
  - [corpus] Weak direct evidence for the specific spatial propagation mechanism in the provided corpus, which focuses more on general FL security.
- **Break condition:** Propagation is limited if the model architecture uses strictly local parameters (e.g., locally connected layers without weight sharing) or if aggregation strictly isolates regional model components.

## Foundational Learning

- **Concept: Non-IID Data (Non-Independent and Identically Distributed)**
  - **Why needed here:** The paper emphasizes that weather data is geographically distinct. This heterogeneity is the primary reason standard defenses fail—they cannot distinguish between "suspicious" data and "different but legitimate" regional weather patterns.
  - **Quick check question:** If you plot the temperature distributions of two geographically separated clients (e.g., Scandinavia vs. Mediterranean), would they overlap perfectly? (Answer: No, they are non-IID).

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** This is the baseline aggregation method. Understanding that it performs a weighted average of model weights is crucial to seeing why a persistent bias (GTBA) eventually "averages down" the global model's accuracy.
  - **Quick check question:** Does FedAvg verify the content of the training data before averaging the weights? (Answer: No, it only aggregates the resulting model parameters).

- **Concept: Trimmed Mean Aggregation**
  - **Why needed here:** This is the defense evaluated in the paper. It removes the highest and lowest parameter values before averaging to filter outliers. Understanding this helps explain why it stops global shifts (outliers) but fails at localized patches (which look like inliers).
  - **Quick check question:** If 40% of clients send manipulated weights that are numerically similar, will Trimmed Mean remove them? (Answer: Likely not, if they cluster together and are not the extreme tails).

## Architecture Onboarding

- **Component map:** CERRA dataset (2m temperature, 3-hourly, Jan 2015–Dec 2020) -> Spatially partitioned into 3×3 grid (9 clients) -> Residual U-Net model (64×64 spatial tile, t₋₁, t₀ → t₊₁) -> Federated Averaging server (FedAvg or Trimmed Mean aggregation) -> Attack modules (GTBA or Patch Attack)

- **Critical path:**
  1. **Data Loading:** Client receives local $64 \times 64$ spatial tile
  2. **Poisoning (Conditional):** If compromised, apply $-2K$ bias or patch to target variable
  3. **Local Training:** U-Net trains for 5 local epochs (MSE loss)
  4. **Aggregation:** Server receives 9 weight updates. Applies Trimmed Mean (discards top/bottom 20%) or FedAvg
  5. **Evaluation:** Test on full spatial domain to check for global bias or localized anomalies

- **Design tradeoffs:**
  - **FedAvg vs. Trimmed Mean:** FedAvg is computationally cheaper and converges faster on clean data, but is fully vulnerable to bias. Trimmed Mean adds robustness to global bias but severely amplifies localized patch attacks by discarding honest "outlier" updates
  - **Model Complexity:** The paper uses a relatively shallow U-Net to ensure stability across federated clients, trading off potential accuracy for reduced communication overhead and compatibility

- **Failure signatures:**
  - **GTBA Success:** Uniform negative shift in global temperature ($\approx -1.7K$ to $-2.0K$), high "Mean Bias" metric
  - **Patch Success:** High MSE (> 20.0) concentrated in specific regions, visual artifacts in the error maps
  - **Defense Backfire:** When using Trimmed Mean on Patch attacks, MSE skyrockets (e.g., 33.35 vs 4.12 baseline), indicating the defense is filtering legitimate updates as noise

- **First 3 experiments:**
  1. **Establish Baseline:** Run clean federation with FedAvg. Verify MAE $\approx 1.5K$ and ensure spatial error maps are uniform
  2. **Validate Vulnerability (GTBA):** Compromise exactly 1 client with GTBA. Measure global mean bias. Confirm it approximates the theoretical shift ($-2K$)
  3. **Test Defense Failure Mode:** Run Patch Attack with 3 clients against Trimmed Mean defense. Confirm that performance degrades *worse* than the no-defense baseline (MSE amplification)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can defense mechanisms that leverage physical constraints, such as spatial continuity or temporal smoothness, effectively mitigate patch attacks without degrading model utility?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that "Future work should investigate defense mechanisms that leverage physical constraints specific to the forecasting domain, such as spatial continuity or temporal smoothness."
- Why unresolved: The study only evaluated trimmed mean aggregation, which failed against patch attacks by amplifying errors.
- What evidence would resolve it: Experimental results showing that a physics-informed aggregation rule reduces the Mean Squared Error (MSE) during patch attacks compared to trimmed mean.

### Open Question 2
- Question: What is the computational overhead and latency impact of implementing physics-aware defense mechanisms in operational federated meteorological systems?
- Basis in paper: [explicit] The conclusion highlights the need to "evaluate their computational overhead in realistic operational settings" for future defense mechanisms.
- Why unresolved: The current study focused on attack efficacy and defense success rates but did not measure the resource costs of potential defense strategies.
- What evidence would resolve it: Benchmarking analysis comparing the training time, memory usage, and communication rounds of physics-aware defenses against standard FedAvg.

### Open Question 3
- Question: Do other standard Byzantine-robust aggregation methods (e.g., Krum, Coordinate-wise Median) fail similarly to Trimmed Mean when defending against patch attacks on spatially correlated data?
- Basis in paper: [inferred] The authors note that for existing robust methods, "effectiveness... remains unclear" and they only evaluated Trimmed Mean, finding it amplified attacks by 281-603%.
- Why unresolved: It is undetermined if the observed failure is specific to Trimmed Mean's statistical properties or a general limitation of outlier-based defenses on non-IID meteorological data.
- What evidence would resolve it: A comparative study evaluating Krum and Median aggregation under the same patch attack configurations used in the paper.

## Limitations
- The specific spatial configuration (20×20 patch in top-left corner) may not represent optimal adversarial strategies for patch attacks
- The fixed 3×9 client grid may not capture the full complexity of European weather patterns despite the physically justified non-IID assumption
- Only two aggregation methods (FedAvg and Trimmed Mean) were evaluated, limiting generalizability to other Byzantine-robust aggregation techniques

## Confidence

**High Confidence:** GTBA effectiveness (global bias shift up to -1.7K) and trimmed mean defense success against global attacks (2-13% degradation)

**Medium Confidence:** Patch attack results and trimmed mean failure mode (281-603% amplification), given the specific spatial configuration

**Low Confidence:** Generalizability to other aggregation rules beyond trimmed mean and FedAvg, as only these two were evaluated

## Next Checks

1. **Spatial Attack Optimization:** Test variable patch locations and sizes to determine if the observed defense failure is robust across different adversarial strategies

2. **Alternative Defense Mechanisms:** Evaluate other defenses like Krum, coordinate-wise median, or domain-aware aggregation to compare effectiveness against patch attacks

3. **Cross-Dataset Validation:** Reproduce experiments on different meteorological datasets (e.g., ERA5) to assess robustness of findings beyond the CERRA dataset