---
ver: rpa2
title: Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative
  Language Models
arxiv_id: '2505.01539'
source_url: https://arxiv.org/abs/2505.01539
tags:
- prompts
- arguments
- attack
- graphs
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a parameterized benchmark framework for\
  \ evaluating generative language models\u2019 reasoning capabilities using dynamically\
  \ generated argumentation-based tasks. By creating attack graphs of varying complexity\
  \ and translating them into natural language prompts about witness testimony, the\
  \ authors assess models\u2019 ability to reason through layered arguments."
---

# Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models

## Quick Facts
- arXiv ID: 2505.01539
- Source URL: https://arxiv.org/abs/2505.01539
- Reference count: 16
- Primary result: Even state-of-the-art generative models struggle with reasoning through layered argument attack graphs, showing significant performance drops as complexity increases.

## Executive Summary
This study introduces a parameterized benchmark framework for evaluating generative language models' reasoning capabilities using dynamically generated argumentation-based tasks. By creating attack graphs of varying complexity and translating them into natural language prompts about witness testimony, the authors assess models' ability to reason through layered arguments. Testing seven commercial models, including GPT-4o, Claude, Gemini, and o1-preview, on linear and non-linear attack graph prompts, the study reveals that even state-of-the-art models struggle with reasoning tasks, especially when prompts involve multiple attack paths or require rejecting main arguments. Performance drops significantly with increased complexity, and models often exhibit inconsistent reasoning patterns. The findings highlight the brittleness of current generative models' reasoning abilities, underscoring the need for robust evaluation methods in legal and evidence-based domains.

## Method Summary
The benchmark generates argument attack graphs where nodes represent witness statements and edges represent "lying" relationships. Linear graphs form single chains of n arguments, while non-linear graphs have the root argument attacked by multiple disjoint paths. Ground truth answers follow formal argumentation semantics: linear graphs yield "yes" for odd-length paths and "no" for even-length paths (based on who has "the last word"), while non-linear graphs require all attacking paths to be even-length for "yes". Prompts are generated by mapping graph nodes to randomly selected names and statements from a database of 474 names and 90 neutral facts, then formatted as witness testimony about other witnesses lying. Models must output "Answer: yes" or "Answer: no" based on whether the root argument should be believed.

## Key Results
- GPT-4o and similar models show significantly degraded performance on even-numbered linear attack graphs, incorrectly accepting the main argument despite being attacked
- Non-linear reasoning tasks prove substantially harder than linear ones, with models often ignoring shorter attacking paths and focusing only on longest chains
- The o1-preview model achieves highest accuracy but still fails on complex non-linear prompts, with accuracy dropping from 72% on linear tasks to 53% on non-linear tasks
- Performance drops dramatically with increased complexity, with MCC scores approaching zero for higher complexity non-linear graphs

## Why This Works (Mechanism)

### Mechanism 1: Complexity Scaling via Argument Attack Depth
Increasing arguments in attack graphs linearly degrades model performance by exceeding the model's capacity for recursive tracking of defeat relationships. The benchmark constructs directed attack graphs where node A_{i+1} attacks A_i. The correct answer depends on the parity of the path length (odd/even). As n increases, the model must maintain the state of "defeated" vs. "reinstated" over a longer chain. If models possessed robust logical state machines, performance would remain stable regardless of n.

### Mechanism 2: Ontology-Based Surface Variation
Randomizing non-semantic elements (names, specific statements) via an ontology isolates reasoning failures from pattern matching or memorization. The system maps graph nodes to a database of names and statements, generating thousands of unique prompts for the same topological structure to prevent the model from leveraging specific token sequences as shortcuts. If the model solves all variations of a specific graph structure consistently, it has likely learned the abstract rule rather than surface patterns.

### Mechanism 3: Order Perturbation (Shuffling)
Shuffling the order of arguments in the prompt reveals a "positional bias" where models default to accepting the first presented argument as true. In standard prompts, the main argument (A_1) appears first. Models like GPT-4o tend to "presuppose" this argument. Shuffling arguments disrupts this heuristic, forcing the model to build the graph structure internally rather than relying on presentation order. If shuffling degrades performance uniformly for all models, it suggests the task relies on sequential parsing capabilities that are disrupted.

## Foundational Learning

- **Concept: Abstract Argumentation Frameworks (Dung's Framework)**
  - Why needed here: The entire benchmark relies on "attack graphs" where arguments are nodes and conflicts are edges. You must understand "Grounded Semantics" (skeptical acceptance) to verify the ground truth labels.
  - Quick check question: In a linear chain A ← B ← C, is argument A accepted or rejected? (Answer: Accepted, because C attacks B, defending A).

- **Concept: Matthew's Correlation Coefficient (MCC)**
  - Why needed here: The paper highlights that "Accuracy" is misleading because the dataset is imbalanced (most non-linear graphs result in "No"). MCC accounts for true/false positives and negatives, providing a reliable score for binary classification on skewed data.
  - Quick check question: Why would a model achieve 80% accuracy while having an MCC near 0? (Answer: Class imbalance; the model likely guessed the majority class "No" for almost everything).

- **Concept: Data Contamination in LLMs**
  - Why needed here: The motivation for this paper is that static benchmarks (like LegalBench) may appear in training data, inflating scores. Understanding this explains why the "dynamic/parameterized" nature of this specific benchmark is critical for valid evaluation.
  - Quick check question: Why is generating a puzzle on-the-fly better than pulling one from a fixed 2023 dataset? (Answer: The model cannot have memorized the specific question/answer pair during training).

## Architecture Onboarding

- **Component map:** Graph Generator → Ontology Database → Prompt Translator → Evaluator
- **Critical path:** Graph Definition → Prompt Generation → Model Inference → Binary Parsing → Metric Calculation (MCC)
- **Design tradeoffs:**
  - Cost vs. Coverage: Testing o1-preview on all prompts is cost-prohibitive. The design requires a "Hard Prompt" filtering strategy (selecting prompts where GPT-4o failed) to stress-test reasoning models efficiently.
  - Linearity vs. Non-Linearity: Linear graphs are easier to generate but produce predictable parity patterns (odd/even). Non-linear graphs offer exponential structural variety but are harder for error analysis.
- **Failure signatures:**
  - "First Argument" Bias: The model justifies rejecting an attacker because there is "no evidence they are lying," ignoring that their role is to be the evidence of lying.
  - Longest-Path Heuristic: In non-linear graphs, models may ignore short branches (e.g., a single attacker) and only reason about the longest chain.
  - Pattern Recognition: o1-preview identifies the odd/even pattern explicitly. While this yields correct answers, it risks "gaming" the benchmark rather than simulating legal reasoning.
- **First 3 experiments:**
  1. Reproduce the "Even-Number" Failure: Generate linear graphs with n=2, 4, 6, 8 for a mid-tier model (e.g., GPT-4o-mini). Verify if accuracy drops specifically on even numbers, confirming the "accept first" bias.
  2. Shuffle Ablation: Take a set of 50 non-linear graphs. Run them once in standard order and once shuffled. Calculate the delta in MCC. If positive, the model relies on positional heuristics.
  3. Intervention Analysis: Modify the prompt to include a "scratchpad" or explicit instruction: "Draw the attack graph first." Test if this closes the performance gap between linear and non-linear tasks.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the internal reasoning patterns of generative models be analyzed to determine if correct outputs are derived from sound logic rather than spurious correlations?
  - Basis in paper: [explicit] The authors state that investigating the "exact reasoning patterns that lead to a given output" is an "important next step" not covered in the current study.
  - Why unresolved: The current evaluation method relies solely on comparing the final binary output (yes/no) to the ground truth, which fails to detect cases where models reach correct conclusions using flawed reasoning.
  - What evidence would resolve it: A methodology that maps chain-of-thought outputs or attention mechanisms against formal argumentation semantics to verify the validity of the deduction steps.

- **Open Question 2:** How does the inclusion of support relationships (in addition to attacks) in argument graphs affect the reasoning performance of generative language models?
  - Basis in paper: [explicit] The authors explicitly propose to "expand the ontology such that we can generate different attack and support relationships" in future research.
  - Why unresolved: The current benchmarks are limited to "attack" relationships (witnesses claiming others are lying), whereas real-world legal reasoning requires handling both support and conflict.
  - What evidence would resolve it: Results from a modified benchmark suite that generates structured argument graphs containing both attack and support edges, comparing model accuracy against the current attack-only baseline.

- **Open Question 3:** To what extent do specific semantic elements (names or statements) trigger reasoning failures in models that otherwise solve structurally identical problems?
  - Basis in paper: [inferred] The paper observes that models often produce different answers for the same graph structure when ontological elements (names/statements) are varied, indicating "brittleness" that the authors could not fully explain.
  - Why unresolved: It is unclear if specific tokens act as distractors or if the models rely on superficial semantic associations rather than the abstract graph structure.
  - What evidence would resolve it: An ablation study measuring performance variance across thousands of ontological variations while holding the argument graph structure constant.

## Limitations

- The exact ontology dataset (90 statements, 474 names) is referenced but not fully disclosed, preventing exact replication of all prompt variations
- The study does not specify inference parameters like temperature, though this likely has minor impact on results
- The "shuffling" intervention analysis lacks strong corpus support for its theoretical basis
- The "Hard Prompt" filtering strategy for cost control introduces selection bias that isn't fully characterized

## Confidence

- Core findings (models struggle with reasoning complexity, positional bias exists): **High**
- Specific intervention mechanisms (ontology randomization, shuffling effectiveness): **Medium**
- Theoretical basis for failure modes: **Medium**

## Next Checks

1. **Exact Ontology Replication**: Request and test with the complete 90-statement and 474-name ontology to verify if performance patterns hold identically across all prompt variations.

2. **Shuffling Mechanism Validation**: Systematically test models on shuffled prompts across different graph complexities to quantify whether performance gains are consistent or model-specific.

3. **Intervention Prompt Testing**: Test whether explicit instructions to "draw the attack graph first" or similar scaffolding improves performance on non-linear reasoning tasks, validating whether the issue is capability or prompt format.