---
ver: rpa2
title: 'Reasoning While Asking: Transforming Reasoning Large Language Models from
  Passive Solvers to Proactive Inquirers'
arxiv_id: '2601.22139'
source_url: https://arxiv.org/abs/2601.22139
tags:
- reasoning
- user
- interaction
- interactive
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Proactive Interactive Reasoning (PIR), a framework
  that transforms reasoning LLMs from passive solvers into proactive inquirers by
  enabling them to detect uncertainty and actively seek clarification from users.
  PIR employs uncertainty-aware supervised fine-tuning to train models when and how
  to ask questions, followed by a user-simulator-based reinforcement learning stage
  that aligns reasoning with user intent using a composite reward function.
---

# Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers

## Quick Facts
- **arXiv ID:** 2601.22139
- **Source URL:** https://arxiv.org/abs/2601.22139
- **Reference count:** 40
- **Key outcome:** Achieves up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU improvement across reasoning tasks while reducing computation by nearly half.

## Executive Summary
The paper introduces Proactive Interactive Reasoning (PIR), a framework that transforms reasoning LLMs from passive solvers into proactive inquirers by enabling them to detect uncertainty and actively seek clarification from users. PIR employs uncertainty-aware supervised fine-tuning to train models when and how to ask questions, followed by a user-simulator-based reinforcement learning stage that aligns reasoning with user intent using a composite reward function. Extensive experiments show that PIR achieves up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU improvement across mathematical reasoning, code generation, and document editing tasks, while reducing reasoning computation by nearly half and cutting unnecessary interaction turns by half. The approach also demonstrates strong generalization and robustness on factual knowledge, question answering, and missing-premise scenarios.

## Method Summary
PIR transforms reasoning LLMs through a two-phase training approach. Phase I uses uncertainty-aware supervised fine-tuning where the model segments reasoning traces into steps, computes Predictive Entropy for each step, and injects clarification-response pairs at high-uncertainty points, creating "think-ask-respond" training trajectories. Phase II employs user-simulator-based Group Relative Policy Optimization (US-GRPO) where an LLM simulates user responses conditioned on intent, and the policy is updated using a composite reward that balances output accuracy, reasoning quality, helpfulness, and efficiency. The framework is trained on DeepSeek-R1-Distill-Qwen-7B with 4k SFT samples and 5 epochs of GRPO, achieving significant improvements across multiple reasoning tasks.

## Key Results
- Achieves up to 32.70% higher accuracy on MATH reasoning tasks
- Improves pass rate by 22.90% on BigCodeBench code generation
- Delivers 41.36 BLEU improvement on MediumDocEdit document editing
- Reduces reasoning computation by nearly half compared to DeepSeek-R1
- Cuts unnecessary interaction turns by half while maintaining helpfulness

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Aware Data Augmentation
Converting high-uncertainty reasoning points into clarification opportunities creates direct training signals for when-to-ask behavior. The model segments reasoning traces into steps, computes normalized Predictive Entropy for each step, and selects top-k% high-PE points as clarification candidates. At these points, an instruction-following LLM injects clarification-response pairs, transforming monologic CoT into interleaved "think-ask-respond" trajectories. Core assumption: uncertainty spikes correlate with genuine information gaps that clarification can resolve.

### Mechanism 2: User Simulator-Based Policy Optimization (US-GRPO)
A controllable simulated user environment enables systematic optimization of interactive reasoning without real-user costs. An LLM prompted as a user simulator conditions on intent I and generates responses consistent with that intent. The policy generates reasoning steps and questions; the simulator responds. GRPO samples groups of trajectories and computes group-relative advantages for policy updates, avoiding a separate value-function critic. Core assumption: the simulator's response distribution approximates real-user behavior well enough for transfer.

### Mechanism 3: Composite Reward with Conditional Activation
Rewarding reasoning quality only when the answer is correct prevents rewarding hallucinated justifications; balancing helpfulness and efficiency prevents over-asking and premature termination. Total reward R(y) = R_output + R_reason. R_reason activates only when o=g (correct answer). Within R_reason, I_ask provides baseline for asking at least once; E(r) rewards fewer turns; H_LLM(r) rewards question helpfulness. Both normalized to [0,1]. Core assumption: helpful questions that reduce turns on correct solutions generalize to real scenarios.

## Foundational Learning

- **Predictive Entropy (PE)**
  - Why needed here: Identifies when the model is genuinely uncertain during reasoning, serving as the signal for clarification insertion.
  - Quick check question: Can you compute token-level log-probabilities from your model's forward pass and aggregate them into step-level uncertainty scores?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Enables RL optimization without training a separate value critic, reducing computational overhead for multi-turn interaction rollouts.
  - Quick check question: Do you understand how group-relative advantages (comparing within sampled trajectory groups) replace value-function baselines?

- **Composite Reward Design Trade-offs**
  - Why needed here: Balancing multiple objectives (accuracy, efficiency, helpfulness) requires understanding how each component shapes behavior.
  - Quick check question: If you observe the model asking too many questions, which reward component would you adjust?

## Architecture Onboarding

- **Component map:** Input Query → Base Reasoning LLM → Phase I: Uncertainty-Aware SFT → Phase II: US-GRPO → User Simulator → Policy Updates → Output Final answer after </tool_call>

- **Critical path:**
  1. Dataset quality for Phase I (4k samples sufficient per Appendix A.1)
  2. Simulator fidelity in Phase II (stronger simulator = better transfer)
  3. Reward balance (helpfulness vs. efficiency trade-off)

- **Design tradeoffs:**
  - Stronger simulator → higher accuracy but higher cost (Table 5: $111.2 vs $15.8)
  - More interaction turns → better resolution but reduced efficiency
  - PE threshold (top-k%) → more insertions = more training data but potentially lower precision

- **Failure signatures:**
  - Model never asks questions → I_ask reward not activating or SFT insufficient
  - Model asks too many low-value questions → H_LLM evaluator miscalibrated or efficiency reward too weak
  - Performance degrades in real user tests → simulator distribution shift

- **First 3 experiments:**
  1. Validate PE-uncertainty correlation on your domain by plotting PE distributions at reasoning steps where humans identify ambiguity.
  2. Ablate simulator strength: train with weak vs. strong simulator, evaluate on held-out interactive benchmarks.
  3. Sweep reward weights: systematically vary α in R = R_output + α·R_reason and observe TTR/helpfulness/accuracy trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the reliance on a user simulator biased toward majority interaction patterns impact the performance and fairness of PIR when deployed for minority user groups?
- **Basis in paper:** "Limitations: The Diversity of User Simulator... likely biases towards majority interaction patterns, potentially failing to represent the diverse behaviors of minority user groups."
- **Why unresolved:** The training and evaluation were conducted using general instruction-following models (e.g., Llama-3.1, GPT-4o) which may implicitly standardize interaction styles, leaving the performance on underrepresented linguistic patterns unverified.
- **What evidence would resolve it:** Evaluation results on a dataset of interactions specifically curated from diverse demographic user groups to measure accuracy and interaction efficiency across different cohorts.

### Open Question 2
- **Question:** Does the proactive inquiry capability introduce new safety vulnerabilities or interfere with existing safety alignment when handling sensitive topics such as self-harm or violence?
- **Basis in paper:** "Limitations: Lack of Safety Alignment... The PIR model has not been screened for its handling of sensitive topics such as violence, sexual content, self-harm, or hate speech."
- **Why unresolved:** The study focused exclusively on reasoning accuracy and efficiency for mathematical, coding, and editing tasks, omitting safety benchmarks that are standard for deployable dialogue systems.
- **What evidence would resolve it:** Red-teaming evaluation results on the PIR model across standard safety benchmarks (e.g., refusal rates for harmful requests) compared to a non-interactive baseline.

### Open Question 3
- **Question:** How robust is the US-GRPO policy when facing the "sim-to-real" gap, specifically regarding the linguistic noise and rapid intent shifts typical of real human users?
- **Basis in paper:** "Limitations: The Diversity of User Simulator... may not fully capture the linguistic noise and dynamic intent of real-world human interactions."
- **Why unresolved:** The RL training environment utilizes instruction-following LLMs as simulators, which tend to be more coherent and grammatical than real users, potentially limiting the model's robustness to messy, real-world input.
- **What evidence would resolve it:** A comparative study of model performance (Pass Rate, TTR) in a human-in-the-loop setting versus the simulator-based environment to quantify the degradation caused by natural noise.

## Limitations
- Reliance on user simulator may not fully capture real human interaction patterns and linguistic noise
- No safety alignment testing for sensitive topics like self-harm, violence, or hate speech
- Potential bias toward majority interaction patterns that may not generalize to minority user groups

## Confidence
- **High Confidence:** The two-phase training architecture is clearly specified and reproducible; quantitative improvements over baselines are well-documented across three diverse task domains; PE mechanism for identifying uncertainty is mathematically sound.
- **Medium Confidence:** The claim of reducing reasoning computation by nearly half relies on comparison with DeepSeek-R1 but computational accounting methodology is not fully detailed; 41.36 BLEU improvement is impressive but task definition could benefit from additional clarity.
- **Low Confidence:** Generalization claims to factual knowledge and missing-premise scenarios are based on supplementary experiments with limited detail; "strong" robustness assertions lack quantitative benchmarks or confidence intervals.

## Next Checks
1. **Simulator-to-Human Transfer Test:** Deploy PIR on a held-out interactive task with actual human users and compare performance degradation against simulator-only evaluations. Measure distribution shift in user response patterns.

2. **Uncertainty-Label Correlation Study:** Have human annotators mark reasoning steps that genuinely require clarification, then compute the precision and recall of PE-based selection against these ground-truth labels across multiple domains.

3. **Reward Weight Sensitivity Analysis:** Systematically sweep the weighting parameters in the composite reward function (α for reasoning reward, β for efficiency vs. helpfulness) and measure performance stability across at least five different task domains to establish robustness bounds.