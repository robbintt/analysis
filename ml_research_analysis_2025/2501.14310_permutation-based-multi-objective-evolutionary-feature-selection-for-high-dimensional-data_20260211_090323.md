---
ver: rpa2
title: Permutation-based multi-objective evolutionary feature selection for high-dimensional
  data
arxiv_id: '2501.14310'
source_url: https://arxiv.org/abs/2501.14310
tags:
- attr
- feature
- methods
- features
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PSEFS-MOEA, a permutation-based multi-objective
  evolutionary feature selection method for high-dimensional data. The method evaluates
  subsets of attributes rather than individual features, using a multi-objective evolutionary
  algorithm to maximize performance degradation when selected features are shuffled
  and minimize subset cardinality.
---

# Permutation-based multi-objective evolutionary feature selection for high-dimensional data

## Quick Facts
- **arXiv ID**: 2501.14310
- **Source URL**: https://arxiv.org/abs/2501.14310
- **Reference count**: 40
- **Primary result**: Proposed method outperforms nine established feature selection methods on 24 high-dimensional datasets

## Executive Summary
This paper introduces PSEFS-MOEA, a permutation-based multi-objective evolutionary feature selection method designed specifically for high-dimensional data. The approach evaluates subsets of attributes rather than individual features, using a multi-objective evolutionary algorithm that simultaneously maximizes performance degradation when selected features are shuffled and minimizes subset cardinality. The method demonstrates superior performance compared to conventional feature selection techniques while maintaining computational efficiency.

## Method Summary
PSEFS-MOEA operates by treating feature subsets as permutations rather than individual feature selections. The algorithm evaluates each subset by measuring classification or regression performance degradation when selected features are randomly shuffled, combined with a cardinality minimization objective. This dual-objective optimization balances predictive power with feature subset compactness. The evolutionary algorithm evolves populations of feature subsets through selection, crossover, and mutation operations, ultimately producing Pareto-optimal solutions that represent optimal tradeoffs between accuracy and feature count.

## Key Results
- PSEFS-MOEA outperformed nine established feature selection methods on 24 high-dimensional datasets
- Demonstrated strong generalization capabilities with low overfitting across classification and regression tasks
- Achieved state-of-the-art performance in balancing accuracy and computational efficiency
- Successfully handled datasets with up to 10,000 features

## Why This Works (Mechanism)
The method leverages permutation-based feature importance evaluation, which provides more robust and reliable importance scores compared to traditional methods. By evaluating feature subsets through performance degradation when features are shuffled, the approach captures feature interactions and dependencies that individual feature importance methods miss. The multi-objective framework naturally balances competing objectives of maximizing predictive power while minimizing feature subset size, leading to more interpretable and generalizable models.

## Foundational Learning
1. **Permutation Feature Importance** - Measures feature importance by randomly shuffling feature values and observing performance degradation. Why needed: Provides robust feature importance estimation that captures feature interactions. Quick check: Verify that shuffling actually degrades performance as expected.

2. **Multi-Objective Evolutionary Algorithms** - Optimization algorithms that handle multiple conflicting objectives simultaneously. Why needed: Balances predictive accuracy with feature subset compactness. Quick check: Confirm Pareto front convergence across generations.

3. **High-Dimensional Feature Selection** - The challenge of selecting relevant features from datasets with thousands of variables. Why needed: Traditional methods often fail or become computationally prohibitive. Quick check: Monitor runtime scalability with increasing feature dimensions.

## Architecture Onboarding

**Component Map**: Data -> Evolutionary Algorithm -> Fitness Evaluation -> Pareto Front -> Selected Features

**Critical Path**: The evolutionary algorithm's fitness evaluation stage is critical, as it determines both objectives (performance degradation and cardinality). Each generation requires computing model performance with shuffled features, which dominates computational cost.

**Design Tradeoffs**: The method trades increased computational complexity per generation for better feature subset quality and reduced overfitting risk. The permutation-based evaluation provides more reliable importance scores but requires multiple model evaluations per candidate solution.

**Failure Signatures**: Poor performance indicates either insufficient evolutionary pressure, inappropriate permutation patterns, or inadequate model training. If feature subsets become too large, it suggests the performance degradation objective is not effectively weighted against cardinality.

**3 First Experiments**:
1. Verify permutation-based importance by comparing shuffled vs original performance on a simple dataset
2. Test single-objective vs multi-objective performance to validate the dual-objective approach
3. Evaluate scalability by running on increasingly high-dimensional synthetic datasets

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, though the limitations section suggests areas for future research regarding computational scalability beyond 10,000 features and parameter sensitivity analysis.

## Limitations
- Computational scalability beyond 10,000 features remains unverified
- Lack of comprehensive parameter sensitivity analysis for population size and mutation rates
- Limited discussion of permutation pattern sensitivity and sample size effects

## Confidence
- Performance claims: Medium (strong results but limited parameter sensitivity analysis)
- Scalability claims: Low (limited testing beyond 10,000 features)
- Overfitting assessment: Medium (validation present but details sparse)

## Next Checks
1. Conduct scalability tests on datasets exceeding 50,000 features to verify computational efficiency claims
2. Perform comprehensive parameter sensitivity analysis across different population sizes and mutation rates
3. Implement nested cross-validation to independently verify overfitting claims and generalization performance