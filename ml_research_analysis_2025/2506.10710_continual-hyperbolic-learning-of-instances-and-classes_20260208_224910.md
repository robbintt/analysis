---
ver: rpa2
title: Continual Hyperbolic Learning of Instances and Classes
arxiv_id: '2506.10710'
source_url: https://arxiv.org/abs/2506.10710
tags:
- learning
- hyperbolic
- continual
- hyperclic
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual learning of instances
  and classes simultaneously in dynamic environments like robotics and self-driving
  cars. The authors propose HyperCLIC, a hyperbolic continual learning algorithm that
  leverages hyperbolic space to model the inherent hierarchical structure of instances
  and classes.
---

# Continual Hyperbolic Learning of Instances and Classes

## Quick Facts
- arXiv ID: 2506.10710
- Source URL: https://arxiv.org/abs/2506.10710
- Reference count: 38
- Key outcome: HyperCLIC achieves significantly higher instance (41.76%), class (45.91%), and superclass (48.04%) accuracy compared to baselines like iCaRL (20.05%, 21.39%, 22.24%) and EWC (0.22%, 0.66%, 1.42%)

## Executive Summary
This paper addresses the challenge of continual learning of instances and classes simultaneously in dynamic environments like robotics and self-driving cars. The authors propose HyperCLIC, a hyperbolic continual learning algorithm that leverages hyperbolic space to model the inherent hierarchical structure of instances and classes. The method uses hyperbolic classification and distillation objectives to enable incremental learning while preserving hierarchical relationships. Experiments on the EgoObjects dataset show that HyperCLIC achieves significantly higher accuracy across all granularity levels compared to baselines, while also demonstrating lower hierarchical error and forgetting rates.

## Method Summary
HyperCLIC is a two-stage method for joint instance- and class-level continual learning. Stage 1 generates hyperbolic prototypes by training Poincaré embeddings (150 epochs), entailment cones (50 epochs), and separation loss (500 epochs) on the class-instance hierarchy extracted from WordNet. Stage 2 uses these prototypes for classification: features from a ResNet34 backbone are projected to hyperbolic space via exponential map, and classification uses negative hyperbolic distance to prototypes as logits. The model is trained incrementally with classification loss plus hyperbolic distillation loss to prevent catastrophic forgetting, using a memory buffer of 3500 exemplars selected via herding strategy.

## Key Results
- HyperCLIC achieves 41.76% instance-level accuracy versus 20.05% for iCaRL and 0.22% for EWC
- HyperCLIC reaches 45.91% class-level accuracy versus 21.39% for iCaRL and 0.66% for EWC
- HyperCLIC demonstrates LCA distance of 4.93 and forgetting rate of 4.17, significantly better than baselines
- Method consistently outperforms baselines across various pretrained backbones and task configurations

## Why This Works (Mechanism)

### Mechanism 1: Hyperbolic Geometry for Hierarchical Embedding
Hyperbolic space preserves tree-structured relationships with lower distortion than Euclidean space. The Poincaré ball model's negative curvature and exponential volume growth naturally accommodate branching hierarchies. Prototypes are positioned so hyperbolic distance d_B(p_i, p_j) ∝ graph distance d_T(v_i, v_j) in the class-instance tree. If the hierarchy is noisy, incomplete, or poorly aligned with visual similarity, prototype positions become misleading anchors.

### Mechanism 2: Hierarchical Prototype-Based Classification
Distance to hierarchical prototypes in hyperbolic space yields predictions that respect granularity levels. Features φ(x; θ) are mapped to hyperbolic space via exponential map. Classification uses negative hyperbolic distance as logits. If an instance prototype is distant, its parent class prototype may still be relatively close, yielding hierarchically consistent errors. If backbone features are not well-aligned with the hierarchy, the exponential map cannot recover structure.

### Mechanism 3: Hyperbolic Distillation for Catastrophic Forgetting Mitigation
Distilling hyperbolic logits preserves hierarchical relationships across tasks better than Euclidean distillation. Cross-entropy between probability distributions over old classes. Since logits encode hierarchical distances, distillation implicitly preserves relative positions in the tree—not just class identities. If exemplar memory is too small or biased, distillation anchors to unrepresentative samples, degrading both stability and plasticity.

## Foundational Learning

- Concept: **Poincaré Ball Model and Hyperbolic Distance**
  - Why needed here: All prototype learning and classification operate in this manifold; understanding curvature c and Möbius addition is essential for debugging
  - Quick check question: Can you explain why hyperbolic distance grows exponentially toward the ball's boundary?

- Concept: **Exponential and Logarithmic Maps**
  - Why needed here: The bridge between Euclidean backbone features and hyperbolic prototypes. Misunderstanding here causes implementation errors
  - Quick check question: Given a Euclidean vector, what does exp_0(x) produce, and what happens as ||x|| increases?

- Concept: **Knowledge Distillation in Continual Learning**
  - Why needed here: The balance between L_cls and L_distil (λ=0.5) is critical; understanding why unweighted sum works prevents over-tuning
  - Quick check question: Why does distillation use soft logits rather than hard labels?

## Architecture Onboarding

- Component map: Hierarchy Processor -> Prototype Learner -> Backbone + Projection -> Continual Trainer -> Evaluation Metrics
- Critical path: Hierarchy extraction → Prototype optimization → Backbone training with hyperbolic losses. Errors in Stage 1 propagate irrecoverably to Stage 2
- Design tradeoffs:
  - Temperature τ=0.1: Lower makes distribution peaked (better separation, worse generalization); higher smooths (opposite). Paper finds 0.1 optimal but sensitive in from-scratch training
  - λ=0.5 (balanced losses): Prioritizing distillation (λ>0.5) reduces forgetting but harms new task learning; prioritizing classification (λ<0.5) increases catastrophic forgetting
  - Fine-tuning depth: HyperCLIC benefits from fine-tuning all layers (learns hierarchical representations); iCaRL degrades with more fine-tuning (loses pretrained features)
- Failure signatures:
  - High LCA distance (>6) with low instance accuracy: Hierarchy-prototype misalignment; recheck WordNet extraction or increase separation loss epochs
  - Negative forgetting rate with low accuracy: Over-regularization (KL divergence); switch to cross-entropy distillation
  - Large accuracy gap between instance and class levels: Backbone not learning fine-grained features; consider stronger backbone or longer training
  - Pretrained model underperforms from-scratch: Insufficient fine-tuning layers; increase trainable layers
- First 3 experiments:
  1. Prototype quality check: Visualize pairwise hyperbolic distances between prototypes; confirm block-diagonal structure matching hierarchy. If prototypes cluster randomly, increase Poincaré epochs or check hierarchy extraction
  2. Ablation on τ and λ: Run grid search τ∈{0.05, 0.1, 0.3} and λ∈{0.3, 0.5, 0.7} on validation split; confirm τ=0.1, λ=0.5 is optimal for your dataset
  3. Baseline comparison with statistical testing: Compare against iCaRL and DER with 5 random seeds; compute p-values for all metrics. Report mean±std to establish significance

## Open Questions the Paper Calls Out

### Open Question 1
Can HyperCLIC be adapted to function effectively without relying on stored exemplars? The authors state, "We believe that the geometric properties of hyperbolic space can benefit exemplar-free approaches... we see this as an intriguing future research direction." The current implementation relies on a memory buffer and herding strategy (exemplars) for classification and distillation, unlike strictly exemplar-free methods. Demonstrating that hyperbolic prototype alignment alone is sufficient to prevent catastrophic forgetting without replaying raw data samples would resolve this.

### Open Question 2
How does the method perform when the class-instance hierarchy is dynamic or incomplete rather than fixed a priori? The methodology assumes a fixed tree derived from WordNet, but the introduction emphasizes "dynamic real-world environments" (robotics) where taxonomies may evolve or be unknown. The Poincaré embeddings are initialized based on the full hierarchy structure before training begins; it is unclear how the model handles new hierarchical branches or relationships added during the continual learning stream. Experiments where the hierarchical structure expands or shifts between tasks would resolve this.

### Open Question 3
How do recent state-of-the-art instance-level continual learning architectures perform when generalized to the joint instance-and-class task? "We believe that generalizing recent instance-level approaches to our task is an interesting future research direction." The study primarily benchmarks against general class-incremental methods and leaves newer, specialized instance-recognition architectures untested within this hierarchical framework. Integrating hyperbolic distillation into modern instance-specific continual learning architectures and evaluating on the proposed hierarchical metrics would resolve this.

## Limitations
- Evaluation limited to single dataset (EgoObjects) with WordNet-based hierarchy extraction, may not generalize to noisier or more dynamic scenarios
- Method assumes complete class-instance relationships before training begins, limiting applicability to truly online settings
- Prototype generation stage is offline and doesn't address task boundaries during testing, critical for practical deployment
- Ablation shows high sensitivity to temperature τ and distillation weighting λ, suggesting method may not be robust across datasets without extensive hyperparameter tuning

## Confidence
- **High Confidence**: Hyperbolic geometry benefits for hierarchical embedding (supported by mathematical properties and consistent improvements in LCA distance and accuracy metrics)
- **Medium Confidence**: Hyperbolic distillation superiority over Euclidean alternatives (novel claim with ablation support but no external validation)
- **Medium Confidence**: Two-stage approach effectiveness (consistent gains over baselines but untested on other datasets)
- **Low Confidence**: Generalizability to arbitrary hierarchical structures (only tested on WordNet-extracted hierarchies)

## Next Checks
1. Cross-dataset validation: Evaluate HyperCLIC on iNat2021 or tieredImageNet with different hierarchy extraction methods to test generalizability beyond WordNet
2. Memory efficiency analysis: Compare exemplar memory requirements and computational overhead against iCaRL and DER during both training and inference
3. Ablation on hierarchy noise: Systematically corrupt the WordNet hierarchy (add/remove edges) and measure degradation in accuracy and LCA distance to quantify sensitivity to hierarchy quality