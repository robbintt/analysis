---
ver: rpa2
title: Can Large Language Models Understand Preferences in Personalized Recommendation?
arxiv_id: '2501.13391'
source_url: https://arxiv.org/abs/2501.13391
tags:
- user
- item
- rating
- query
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PERRECBENCH, a novel benchmark designed to
  evaluate Large Language Models (LLMs) for personalized recommendation by focusing
  on user preferences rather than rating prediction accuracy. The benchmark addresses
  the limitations of traditional metrics like MAE and RMSE, which can be influenced
  by user rating bias and item quality.
---

# Can Large Language Models Understand Preferences in Personalized Recommendation?

## Quick Facts
- arXiv ID: 2501.13391
- Source URL: https://arxiv.org/abs/2501.13391
- Reference count: 40
- Primary result: Introduces PERRECBENCH benchmark showing current LLMs struggle with personalized recommendation (best model achieves only moderate correlation with ground truth rankings)

## Executive Summary
This paper introduces PERRECBENCH, a novel benchmark designed to evaluate Large Language Models (LLMs) for personalized recommendation by focusing on user preferences rather than rating prediction accuracy. The benchmark addresses the limitations of traditional metrics like MAE and RMSE, which can be influenced by user rating bias and item quality. PERRECBENCH uses a grouped ranking framework, where users are ranked based on their preferences for a shared query item, and evaluates performance using pointwise, pairwise, and listwise ranking methods. Experiments with 19 LLMs reveal that current models struggle with personalized recommendation, with the best-performing model achieving only a moderate correlation with ground truth rankings. The results highlight the superiority of pairwise and listwise ranking approaches, the importance of user profiles, and the role of pretraining data distributions.

## Method Summary
PERRECBENCH evaluates LLMs using relative ratings (actual rating minus user's average rating) to control for user bias and item quality. The benchmark groups users who purchased the same item within a short time window and ranks them based on their relative preferences. Three ranking methods are employed: pointwise (isolated user prediction), pairwise (2-user comparison), and listwise (N-user ranking). Kendall's tau correlation measures the alignment between predicted and ground truth rankings. The evaluation uses zero-shot prompting with user profiles and retrieved history items, while fine-tuning experiments employ LoRA (rank 16) for 2 epochs with batch size 32 and learning rate 1e-5.

## Key Results
- Traditional rating prediction metrics (MAE/RMSE) show weak correlation with personalization capability (Kendall's tau correlation of -0.26 and -0.01 respectively)
- Pairwise and listwise ranking methods significantly outperform pointwise approaches (0.38 vs 0.19 Kendall's tau average)
- Larger models generally perform better, but scaling laws do not consistently hold for personalization tasks
- Weight merging from single-format training shows promise but improving LLMs' understanding of user preferences remains an open challenge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grouped ranking evaluation isolates personalization capability by eliminating confounding factors (user rating bias and item quality).
- **Mechanism:** PERRECBENCH groups users who purchased the same item within a short time window, then ranks them by relative preference. Relative rating = actual rating minus user's average rating. This removes the user's tendency to rate high/low overall and controls for item quality since all users evaluated the same item.
- **Core assumption:** Users' relative preferences (deviations from their personal baselines) represent "true" personalization signals independent of systematic biases.
- **Evidence anchors:**
  - [Section 2.1] Defines relative rating ỹ = y - ȳ and temporal co-purchase criteria
  - [Section 4] Shows MAE/RMSE have weak correlation with Kendall's tau (r=-0.26, r=-0.01)
  - [Corpus] Neighbor papers address preference elicitation but don't systematically control for these biases
- **Break condition:** If user preferences are largely determined by item quality rather than individual taste, relative ratings would show insufficient variance within groups.

### Mechanism 2
- **Claim:** Pairwise and listwise ranking methods outperform pointwise because comparative reasoning elicits stronger preference signals.
- **Mechanism:** Pointwise methods evaluate users in isolation, making subtle preference differences harder to detect. Pairwise/listwise methods force the model to compare users directly, leveraging contrastive information within the same prompt context.
- **Core assumption:** LLMs have stronger comparative reasoning capabilities than absolute judgment capabilities for preference estimation.
- **Evidence anchors:**
  - [Table 1] Average Kendall's tau: pointwise=0.19, pairwise=0.38, listwise=0.35
  - [Section 4] "Pairwise and listwise methods allow the model to leverage comparative reasoning, capturing nuanced differences"
  - [Corpus] "Personalized Recommendations via Active Utility-based Pairwise Sampling" supports pairwise effectiveness, but for different reasons (utility sampling vs. comparative context)
- **Break condition:** If token budget or context window constraints prevent meaningful comparison, listwise methods should degrade faster than pairwise with larger group sizes.

### Mechanism 3
- **Claim:** Weight merging from single-format training improves personalization by combining specialized capabilities without negative transfer.
- **Mechanism:** Single-task training on pointwise/pairwise/listwise separately improves both target task and shows moderate cross-task transfer. Merging weights (α=β=1/3) preserves these gains while avoiding interference that occurs in joint multi-task training.
- **Core assumption:** Personalization requires multiple complementary reasoning modes that can be learned independently and combined linearly.
- **Evidence anchors:**
  - [Table 3] Weight merging achieves best average performance (0.13 for Llama-8B, 0.13 for Mistral-12B)
  - [Section 6] "Multi-task training often underperforms compared to single-task training, indicating potential negative task transfer"
  - [Corpus] No direct corpus evidence on weight merging for personalization; this appears novel
- **Break condition:** If task-specific representations are non-linearly coupled, weight averaging would fail to preserve individual task capabilities.

## Foundational Learning

- **Concept: Relative Rating (Preference Deviation)**
  - Why needed here: Understands that raw ratings conflate user leniency with true preference intensity
  - Quick check question: If User A rates everything 4-5 and User B rates everything 1-2, who prefers a 4-star item more?

- **Concept: Kendall's Tau Correlation**
  - Why needed here: Interprets whether model rankings match ground truth; range [-1, 1] where 0.02-0.18 indicates weak positive correlation
  - Quick check question: A model achieves τ=0.15—does this mean it captures personalization well?

- **Concept: Ranking Paradigms (Pointwise vs. Pairwise vs. Listwise)**
  - Why needed here: Chooses appropriate evaluation/training method; pairwise/listwise require comparative context
  - Quick check question: Which paradigm evaluates users in isolation vs. jointly?

## Architecture Onboarding

- **Component map:**
  - User Group Selector: Filters users by co-purchase window, active history (≥20 items), relative rating difference (>0.6)
  - History Retriever (BM25): Selects top-k relevant items from user history
  - Profile Generator (LLM): Converts behavior history to textual profile
  - Ranking Module: Pointwise (isolated prediction), Pairwise (2-user comparison), Listwise (N-user ranking)
  - Evaluator: Kendall's tau between predicted and ground-truth rankings

- **Critical path:**
  1. User group selection (quality control for evaluation validity)
  2. Profile + history retrieval construction (input representation)
  3. Ranking method selection (determines comparative vs. isolated evaluation)
  4. Correlation computation (final metric)

- **Design tradeoffs:**
  - Group size vs. difficulty: 2-user groups easier (single comparison) but less discriminative; 4+ users harder (τ drops from 0.20 to 0.06)
  - Retrieved history k: Higher k provides more context but may introduce noise; optimal around k=4-8
  - Few-shot demonstrations: Help when k is small, hurt when k is large (pattern mismatch with actual user behavior)

- **Failure signatures:**
  - Random or near-zero τ across all methods → model lacks personalization capability entirely
  - High MAE/RMSE but low τ → model predicts ratings well without capturing individual preferences
  - Pointwise outperforming pairwise → potential prompt formatting issues or position bias not mitigated
  - Performance degrades with group size but not across domains → ranking complexity issue, not domain knowledge gap

- **First 3 experiments:**
  1. **Baseline verification:** Run 3 ranking methods on 20 user groups with k=4, zero-shot, no profile. Confirm pairwise > pointwise pattern holds.
  2. **Profile ablation:** Add textual user profiles, expect ~28% improvement. Isolate whether gains come from profile content or reduced prompt complexity.
  3. **Domain analysis:** Test on books vs. electronics domains. Check if pretraining data frequency (via n-gram lookup in Dolma corpus) correlates with performance差异.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be fundamentally enhanced to robustly understand user preferences when current scaling laws and supervised fine-tuning strategies prove insufficient?
- Basis in paper: [explicit] The authors conclude that "improving LLMs' understanding of user preferences remains an open research problem" and note that even weight merging strategies fail to provide universal improvement.
- Why unresolved: The study shows that simply increasing model size does not guarantee better personalization, and current SFT methods like weight merging do not consistently outperform direct prompting across all scenarios.
- What evidence would resolve it: Novel training paradigms or architectures that consistently yield high Personalization Tau Correlation (PTC) scores across all difficulty levels and domains, surpassing the current 0.18 ceiling.

### Open Question 2
- Question: Can encoding user histories directly into personalized parameter-efficient fine-tuning (PEFT) parameters overcome the limitations of current in-context learning approaches?
- Basis in paper: [explicit] The "Limitations" section suggests this as a specific direction: "Future work could explore encoding user histories into personalized PEFT parameters... which may offer a promising direction."
- Why unresolved: The paper focuses on prompting and basic SFT/weight merging, leaving the specific efficacy of embedding user history into the model weights as an unexplored but promising avenue.
- What evidence would resolve it: Experiments implementing personalized PEFT (e.g., adapting LoRA weights per user) that demonstrate superior performance on PERRECBENCH compared to the weight merging baselines.

### Open Question 3
- Question: Does full-parameter fine-tuning offer significant advantages over LoRA-based adaptation for personalized recommendation tasks?
- Basis in paper: [explicit] The authors note in the "Limitations" section: "our experiments employ LoRA for SFT... While efficient, this approach might impact results compared to full-parameter fine-tuning."
- Why unresolved: The study relied on LoRA for efficiency, leaving the performance upper bound of fully fine-tuned models on this specific benchmark undetermined.
- What evidence would resolve it: A comparison of performance metrics (Kendall's tau) between fully fine-tuned models and LoRA-adapted models on the PERRECBENCH dataset.

## Limitations
- Benchmark relies on relative ratings as proxy for true user preference, which may not capture genuine personalization signals if users' rating distributions are influenced by factors beyond their control
- Temporal co-purchase windows may not capture all meaningful preference relationships, particularly for items with long consideration cycles
- Weight merging results showing superior performance compared to multi-task training lack robustness validation with limited model sizes tested

## Confidence

**High confidence:** The empirical finding that traditional rating prediction metrics (MAE/RMSE) poorly correlate with personalization capability (Kendall's tau correlation of -0.26 and -0.01 respectively). The methodological framework for controlling confounding factors through relative ratings is well-founded.

**Medium confidence:** The superiority of pairwise and listwise ranking methods over pointwise approaches. While the performance gap is consistent (0.38 vs 0.19 Kendall's tau), this may reflect prompt engineering effectiveness rather than inherent model capability differences.

**Low confidence:** The weight merging results showing superior performance compared to multi-task training. With only two model sizes tested (Llama-8B and Mistral-12B) and no ablation studies on the merging weights (α=β=1/3), the robustness of this finding remains uncertain.

## Next Checks

1. **Relative rating validity check:** Analyze the variance distribution of relative ratings within user groups. If variance is consistently low (<0.5) across most groups, this would suggest the relative rating approach may not capture sufficient preference differentiation to serve as a meaningful personalization signal.

2. **Temporal window sensitivity analysis:** Re-run the benchmark with varying temporal co-purchase windows (e.g., 1 day, 1 week, 1 month) to determine whether the 7-day window is optimal or whether results are sensitive to this parameter choice.

3. **Cross-domain pretraining frequency correlation:** For each domain (books, electronics, etc.), measure the frequency of domain-specific n-grams in the pretraining corpus (Dolma) and correlate this with model performance. This would validate whether the observed performance differences stem from pretraining data exposure rather than model architecture effects.