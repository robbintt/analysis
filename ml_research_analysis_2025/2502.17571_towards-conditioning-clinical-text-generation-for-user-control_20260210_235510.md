---
ver: rpa2
title: Towards Conditioning Clinical Text Generation for User Control
arxiv_id: '2502.17571'
source_url: https://arxiv.org/abs/2502.17571
tags:
- text
- generation
- clinical
- topic
- discharge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying clinical text generation
  systems by conditioning large language models for user control. It introduces dataset
  augmentation using LLMs as human proxies to add topic segmentation and authoring
  guidelines, enabling structured, controlled generation without increasing clinician
  workload.
---

# Towards Conditioning Clinical Text Generation for User Control

## Quick Facts
- arXiv ID: 2502.17571
- Source URL: https://arxiv.org/abs/2502.17571
- Reference count: 40
- Primary result: 34% relative improvement on BioNLP ACL'24 Discharge Me! Shared Task with dataset augmentation

## Executive Summary
This paper addresses the challenge of deploying clinical text generation systems by conditioning large language models for user control. It introduces dataset augmentation using LLMs as human proxies to add topic segmentation and authoring guidelines, enabling structured, controlled generation without increasing clinician workload. Evaluated on the BioNLP ACL'24 Discharge Me! Shared Task, the approach achieves new state-of-the-art results, with a 9% relative improvement without augmentation and up to 34% with it. Human evaluation supports its effectiveness, demonstrating improved relevance, accuracy, and factual consistency.

## Method Summary
The approach uses a two-stage pipeline: first, a 70B LLM generates synthetic topic segmentations and authoring guidelines from raw clinical text; second, an 8B student model is fine-tuned using LoRA on this augmented dataset. The segmentation follows Question Under Discussion (QUD) theory, structuring text into XML blocks with Topic, Question, and Text components. The fine-tuning uses instruction tuning with masked loss calculation, training only on the assistant output while preserving the prompt structure.

## Key Results
- 34% relative improvement in ROUGE-2 F1 with dataset augmentation
- 16% improvement when using "Writing Instructions" guidelines vs. 10% for "Style Guidelines"
- Fact-based metrics (AlignScore, MEDCON) improved by 7-8% with style guidelines
- Human evaluation shows 88-92% accuracy for generated headings and questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating stylistic and content-related requirements improves generation control
- Mechanism: By explicitly injecting "Authoring Guidelines" into the prompt, the model conditions on user intent rather than inferring style from spurious correlations
- Core assumption: Current metrics are sensitive to stylistic variances, and explicit style rules align the model better
- Evidence anchors:
  - Section 3.1 states this introduces "clear separation of concerns"
  - Table 2 and Section 5.3 show w/STYLE and w/INSTR outperforming baseline
  - Related work supports context-augmentation for personalized control

### Mechanism 2
- Claim: Fine-grained topic segmentation guided by QUD theory reduces hallucinations
- Mechanism: Structuring text as sequential XML blocks forces the model to justify content via a question, grounding generation in specific subtasks
- Core assumption: Clinical documents possess an implicit structure that can be recovered
- Evidence anchors:
  - Abstract mentions topic segmentation enabling structured, controlled generation
  - Section 3.2 describes utilizing QUD theory to generate questions
  - Expert-guided augmentation papers suggest structured guidance improves robustness

### Mechanism 3
- Claim: Synthetic data generation using LLMs scales training of controllable models without human labeling
- Mechanism: A stronger model (Llama 3.1 70B Instruct) generates "ground truth" guidelines and segmentations
- Core assumption: The teacher model's output approximates human-level guidelines sufficiently
- Evidence anchors:
  - Abstract states it introduces dataset augmentation using LLMs as human proxies
  - Section 5.4 human evaluation validates 88-92% accuracy for generated headings/questions
  - Verification of synthetic data quality in clinical domains remains an active research area

## Foundational Learning

**Question Under Discussion (QUD)**
- Why needed: This is the theoretical framework used to decompose clinical notes into structured blocks
- Quick check: Can you explain how turning a text segment into a question helps reduce factual inconsistency?

**Instruction Tuning (Completions Only)**
- Why needed: The paper fine-tunes models on `prompt(c, g)` where only the assistant output is trained
- Quick check: Why is it important to mask the user prompt during the loss calculation for instruction tuning?

**Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
- Why needed: The architecture uses Rank-stabilized LoRA and PISSA to train the 8B model efficiently
- Quick check: How does initializing LoRA adapters with principal singular values (PISSA) differ from standard random initialization?

## Architecture Onboarding

**Component map:** Data Augmentation Pipeline -> Post-Processor -> Student Model -> Inference Engine

**Critical path:** The Data Augmentation Pipeline is the bottleneck. If the synthetic guidelines are low quality, the student model will not learn control.

**Design tradeoffs:**
- Writing Instructions vs. Style Guidelines: Instructions are longer (1.5x tokens) but yield higher performance (+16% vs +10%)
- Segmentation Post-processing: The system restores original text if generated segment introduces "minor alterations," filtering out ~20% of BHC data to ensure safety

**Failure signatures:**
- Segment Drift: The LLM alters clinical details during topic segmentation
- Metric Sensitivity: Fact-based metrics improving due to style guidelines suggests confounding variable

**First 3 experiments:**
1. Replicate Augmentation: Run the provided prompt on a small set of clinical notes to generate topic segmentations and verify XML structure
2. Ablation on Control: Train a baseline model without guidelines vs. one with "Writing Instructions" on a single GPU
3. Inference Simulation: Manually intervene in the XML generation loop to see if the model recovers when a user edits a topic heading

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Do improvements in fact-based metrics (AlignScore, MEDCON) reflect true factual consistency, or are these metrics merely sensitive to stylistic variances?
- Basis: Page 7 discusses surprising improvement in fact-based metrics when using style guidelines
- Why unresolved: Automated metrics conflate stylistic alignment with factual correctness
- What evidence would resolve it: Targeted ablation study combining automated metrics with fine-grained human expert evaluation

**Open Question 2**
- Question: Which specific components of the generated authoring guidelines are most responsible for the observed performance gains?
- Basis: Page 8 states a systematic study is needed to identify which components contribute most
- Why unresolved: Current approach treats authoring guidelines as a composite input
- What evidence would resolve it: Component-level ablation study with systematic removal of specific guideline sections

**Open Question 3**
- Question: Does interactive, user-controlled generation significantly reduce or increase clinician cognitive workload?
- Basis: Page 9 notes the study does not yet evaluate how interactive clinician involvement impacts cognitive workload
- Why unresolved: While aiming to reduce verification burden, interactive refinement introduces new user effort
- What evidence would resolve it: Clinical user study measuring time-to-completion and cognitive load scores

## Limitations
- The paper relies heavily on synthetic data augmentation via a 70B LLM acting as a human proxy, with no external validation of full synthetic clinical text quality
- The 34% relative improvement conflates multiple interventions (topic segmentation, style guidelines, and authoring instructions)
- Evaluation is limited to the BioNLP ACL'24 Discharge Me! Shared Task dataset, raising generalization concerns

## Confidence

**High Confidence:** The mechanism of separating stylistic and content requirements through explicit guidelines is well-supported by ablation results showing consistent improvements across metrics

**Medium Confidence:** The claim that QUD-based topic segmentation reduces hallucinations is supported by improved fact-based metrics, but evidence is indirect and lacks comprehensive empirical validation

**Low Confidence:** The scalability claim that LLMs can reliably replace human annotation for clinical guidelines is the weakest link, with limited evidence that the 70B teacher model's output approximates human-level quality across clinical scenarios

## Next Checks

1. **Synthetic Data Quality Audit:** Sample 100 synthetic segmentations and have clinical experts rate factual accuracy, completeness, and appropriateness of topic boundaries. Compare against human-annotated segmentations to quantify quality gap.

2. **Cross-Document Type Transfer:** Apply the fine-tuned model to generate clinical text for document types not in the original training set (e.g., pathology reports, clinical trial protocols). Measure performance degradation and identify which components fail to generalize.

3. **Long-Term Stability Analysis:** Track the model's output quality over extended use periods with continuous user feedback. Measure hallucination rates, guideline adherence consistency, and user satisfaction across 1000+ generations to identify degradation patterns.