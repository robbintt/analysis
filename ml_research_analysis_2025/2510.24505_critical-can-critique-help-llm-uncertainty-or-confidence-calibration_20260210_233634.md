---
ver: rpa2
title: 'CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?'
arxiv_id: '2510.24505'
source_url: https://arxiv.org/abs/2510.24505
tags:
- confidence
- uncertainty
- calibration
- critical
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CritiCal, a novel method for improving Large
  Language Model (LLM) confidence calibration through natural language critiques.
  The method addresses the challenge of accurate confidence calibration, which is
  critical for safe LLM deployment in high-stakes domains.
---

# CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?

## Quick Facts
- arXiv ID: 2510.24505
- Source URL: https://arxiv.org/abs/2510.24505
- Reference count: 40
- Key outcome: CritiCal improves LLM confidence calibration through natural language critiques, outperforming baselines and even surpassing teacher model performance on complex reasoning tasks

## Executive Summary
This paper introduces CritiCal, a novel method for improving Large Language Model (LLM) confidence calibration through natural language critiques. The method addresses the challenge of accurate confidence calibration, which is critical for safe LLM deployment in high-stakes domains. CritiCal uses a supervised fine-tuning approach that leverages teacher model critiques to refine LLM confidence expressions, moving beyond traditional direct numerical optimization.

The approach significantly outperforms competitive baselines, including Self-Critique and other fine-tuning methods, and even surpasses the calibration performance of its teacher model (GPT-4o) on complex reasoning tasks. The method shows strong generalization, with models trained on critique-suited multi-hop reasoning data achieving robust calibration improvements in out-of-distribution settings. Additionally, the study finds that uncertainty is better suited for open-ended tasks while confidence excels in multiple-choice scenarios, providing guidance for calibration strategies.

## Method Summary
CritiCal employs a supervised fine-tuning approach that trains LLMs to generate calibrated confidence expressions by learning from teacher model critiques. Rather than optimizing numerical confidence scores directly, the method leverages natural language critiques to provide richer, more interpretable guidance for calibration. The training process involves teacher models (GPT-4o) generating critiques of model responses, which are then used to refine the confidence calibration of student models through fine-tuning.

## Key Results
- CritiCal significantly outperforms competitive baselines including Self-Critique and other fine-tuning methods
- The approach surpasses GPT-4o calibration performance on complex reasoning tasks
- Models trained on multi-hop reasoning data show strong generalization to out-of-distribution settings
- Uncertainty calibration performs better for open-ended tasks while confidence calibration excels in multiple-choice scenarios

## Why This Works (Mechanism)
CritiCal works by leveraging the rich, contextual information contained in natural language critiques to guide confidence calibration. Unlike direct numerical optimization approaches, critiques provide nuanced feedback about reasoning quality, error types, and confidence appropriateness. This allows the fine-tuned models to develop more sophisticated calibration strategies that generalize better across task types and domains.

## Foundational Learning
- **LLM confidence calibration**: Understanding how to align model confidence expressions with actual prediction accuracy is critical for safe deployment in high-stakes applications. Quick check: Verify calibration metrics (ECE, MCE) are computed correctly.
- **Natural language critique generation**: Teacher models must generate meaningful, task-relevant critiques that capture both reasoning quality and confidence appropriateness. Quick check: Assess critique quality through human evaluation or automated metrics.
- **Supervised fine-tuning for calibration**: Adapting pre-trained models to improve calibration through targeted training data. Quick check: Monitor calibration performance during fine-tuning to ensure improvement.

## Architecture Onboarding
- **Component map**: Teacher model (GPT-4o) -> Critique generation -> Student model fine-tuning -> Calibrated confidence output
- **Critical path**: The key sequence is generating high-quality critiques, using them as training signals, and fine-tuning student models to produce calibrated confidence expressions
- **Design tradeoffs**: Natural language critiques provide richer guidance than numerical targets but require more computational overhead and depend on critique quality
- **Failure signatures**: Poor critique quality leads to degraded calibration; overfitting to critique patterns may reduce generalization; computational cost may limit practical deployment
- **3 first experiments**: 1) Evaluate critique quality from different teacher models, 2) Test calibration performance on held-out reasoning tasks, 3) Compare natural language versus numerical calibration approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on teacher model critiques introduces potential brittleness and propagation of systematic errors
- Computational overhead of generating and incorporating natural language critiques may limit practical deployment
- Generalization claims need validation across more diverse task domains beyond reasoning tasks

## Confidence
- High confidence: Core methodology effectiveness and experimental results on reasoning tasks
- Medium confidence: Generalization claims and superiority over GPT-4o on complex reasoning tasks
- Low confidence: Performance on non-reasoning domains and computational efficiency for practical deployment

## Next Checks
1. Test CritiCal's performance on non-reasoning domains (e.g., medical diagnosis, code generation, creative writing) to assess true cross-domain generalization
2. Conduct ablation studies removing the critique component to quantify its specific contribution versus other aspects of the fine-tuning approach
3. Evaluate the method's robustness when using teacher models of varying quality/capabilities to understand the sensitivity to critique quality