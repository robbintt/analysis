---
ver: rpa2
title: 'NFQ2.0: The CartPole Benchmark Revisited'
arxiv_id: '2511.12644'
source_url: https://arxiv.org/abs/2511.12644
tags:
- learning
- cost
- policy
- policies
- pole
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the 20-year-old neural fitted Q-iteration (NFQ)
  algorithm, proposing a modernized version NFQ2.0 and evaluating it on a real-world
  CartPole system. The authors highlight the algorithm's simplicity and stability
  advantages from its batch learning approach, while addressing reproducibility issues
  of the original variant.
---

# NFQ2.0: The CartPole Benchmark Revisited

## Quick Facts
- **arXiv ID:** 2511.12644
- **Source URL:** https://arxiv.org/abs/2511.12644
- **Reference count:** 25
- **Primary result:** NFQ2.0, a modernized version of the 20-year-old NFQ algorithm, demonstrates competitive performance on a real-world CartPole system, finding good policies within 120 episodes.

## Executive Summary
This paper revisits the neural fitted Q-iteration (NFQ) algorithm, proposing a modernized version NFQ2.0 and evaluating it on a real-world CartPole system. The authors highlight the algorithm's simplicity and stability advantages from its batch learning approach, while addressing reproducibility issues of the original variant. Through extensive ablation studies, they identify key design decisions and hyperparameters that improve performance, including larger networks, ReLU activations, Adam optimization, and stacking for handling system latencies. NFQ2.0 demonstrates competitive performance, finding good policies within 120 episodes. The paper also demonstrates offline learning techniques, policy improvement through action space expansion, and cost function modifications, showing how these can be applied to enhance industrial control systems. The findings suggest that NFQ2.0, with modern deep learning practices, remains a viable approach for applying deep reinforcement learning in industrial contexts.

## Method Summary
NFQ2.0 is a batch Q-learning algorithm that uses neural networks to approximate the Q-function for continuous state spaces. It alternates between collecting episode data (with ε-greedy exploration) and training the network on Bellman-updated Q-targets computed over the entire dataset. The method uses a growing batch approach, where new transitions are added to the dataset each episode. Key modernizations include using the Adam optimizer, larger network architectures (256-256-100 hidden layers), ReLU activations, and action/state stacking to handle system latencies. The cost function includes shaped components to guide exploration and soft/hard stops at track ends.

## Key Results
- NFQ2.0 finds good policies on the real CartPole system within 120 episodes.
- Larger networks (256-256-100) are more robust and train faster than smaller ones.
- Action/state stacking with history length n=6 effectively handles ~100ms system latency.
- Shaped cost functions accelerate learning and reduce variance compared to time-optimal costs.
- Offline learning techniques can refine policies without further system interaction.

## Why This Works (Mechanism)

### Mechanism 1: Fitted Q-Iteration (Batch Q-Learning) Stability
Batch Q-learning, which updates Q-values synchronously over a fixed dataset of transitions, yields more stable and repeatable learning than online/asynchronous updates. NFQ2.0 alternates between computing new Q-targets via Bellman updates across all observed transitions and fitting a neural network to these targets using supervised learning. This decouples the non-stationary target problem: the network is trained on a stable set of targets per iteration, reducing divergence. The core assumption is that the environment is approximately Markovian given the state representation and the dataset covers relevant state-action regions.

### Mechanism 2: Handling Real-System Latency via Action/State Stacking
Concatenating recent states and actions as additional inputs makes a partially observed, delayed system effectively Markovian for Q-learning. The system has ~100ms latency (>2 control cycles). By including the last n=6 states and actions (stacking), the network receives temporal context, allowing it to infer hidden dynamics and delays. This transforms a high-order Markov process into a first-order Markov process in the augmented state space. The core assumption is that the latency and relevant dynamics are captured within the chosen history window.

### Mechanism 3: Shaped Cost Functions for Guided Exploration
Cost shaping (providing intermediate cost gradients toward the goal) accelerates learning and reduces variance across runs compared to sparse, time-optimal costs. The shaped cost function includes a term proportional to (1−cos(α))/2, creating a smooth gradient from the hanging position (high cost) to upright (low cost). This provides dense feedback, helping the agent learn to increase pole energy and reach the goal region faster. The core assumption is that the shaping is aligned with the true objective and does not create local optima that trap the policy.

## Foundational Learning

- **Concept: Q-Learning and the Bellman Equation**
  - Why needed here: NFQ2.0's core is the Q-learning update rule. Understanding how Q-values propagate expected future costs is essential to grasp how the algorithm iteratively improves policy estimates.
  - Quick check question: In the Q-learning update, what does the term min_a Q(s_{t+1}, a) represent, and why is it used instead of Q(s_{t+1}, a_t)?

- **Concept: Function Approximation with Neural Networks**
  - Why needed here: NFQ2.0 uses multi-layer neural networks to approximate the Q-function for continuous state spaces. This introduces challenges like overfitting, divergence, and the need for stable training.
  - Quick check question: Why might training a neural network on non-stationary Q-targets cause instability, and how does NFQ2.0's batch approach mitigate this?

- **Concept: Markov Property and Partial Observability**
  - Why needed here: The CartPole system has latency, making the raw state non-Markovian. The paper addresses this via action/state stacking to construct an approximately Markovian representation.
  - Quick check question: Why does violating the Markov assumption hinder Q-learning, and how does including historical information help?

## Architecture Onboarding

- **Component map:**
  Sensors (position, angle encoders) -> PLC (ProfiNet) -> IPC ("busy" component) -> ZMQ -> Python/psipy -> NFQ2.0 Loop

- **Critical path:**
  1. **System integration:** Ensure low-latency, jitter-free control loop (20 Hz). Verify sensor calibration and action command latency (~100ms).
  2. **Initial data collection:** Run random/exploratory episodes to build initial batch. Monitor for endstop hits.
  3. **Learning loop:** Alternate between data collection and batch training. Watch learning curves for consistent cost reduction.
  4. **Policy selection:** After ~100–200 episodes, select candidate policies based on evaluation metrics (N, e_∞) for extended testing.
  5. **Policy refinement:** Optionally extend action set, adjust cost function, or apply offline bootstrapping for smoother/more precise control.

- **Design tradeoffs:**
  - **Network size:** Larger networks (256³) train faster and are more robust but are computationally heavier. Smaller networks (20x20) may get stuck in suboptimal phases (e.g., windmill).
  - **Stacking depth:** Longer history (n=6) safely covers latency but increases input dimension. Too short (n=1) fails to capture dynamics.
  - **Cost shaping vs. time-optimal:** Shaped costs accelerate learning and reduce variance but require careful design. Time-optimal costs are simpler but need demonstrations or hints for efficient exploration.
  - **Online vs. offline learning:** Online exploration is needed initially to cover the state space. Offline training on collected data can refine policies without additional system interaction but requires careful stopping criteria to avoid overfitting.

- **Failure signatures:**
  - **Windmill behavior:** Policy swings pole continuously without balancing (common with small networks or poor hyperparameters). Solution: Use larger networks, ensure adequate exploration, or bootstrap from better data.
  - **Endstop crashes:** Frequent early crashes suggest cost function or exploration needs tuning (e.g., add soft-stop penalty).
  - **Policy degradation after ~200 episodes:** Overfitting to batch data. Solution: Stop early, reduce learning rate, or use offline replay to select best policy.
  - **No learning progress:** Check network training (MSE, Q-value spread), cost function scaling, and state normalization.

- **First 3 experiments:**
  1. **Baseline replication:** Run NFQ2.0 with default hyperparameters on the simulated CartPole swing-up task. Verify learning curve matches paper (stable policy by ~70–120 episodes).
  2. **Ablation on stacking:** On the real system, test stack sizes n=1,2,4,6,12. Measure learning speed and final policy stability.
  3. **Cost function comparison:** Compare shaped costs vs. time-optimal costs with and without a single demonstration episode. Quantify learning speed and variance.

## Open Questions the Paper Calls Out

- **Can the modernization techniques applied to discrete-action NFQ2.0 successfully improve NFQ-CA, the continuous action variant, for industrial control tasks?**
  - The discussion section states, "Future work could revisit NFQ-CA, NFQ's classic variant for continuous action spaces, to see if it could also benefit further from the modern techniques." This study focused exclusively on validating the modernized discrete NFQ2.0 on the CartPole benchmark; the continuous variant remains untested with these specific enhancements.

- **What is the precise mechanism causing the eventual degradation of policy performance ("unlearning") observed after approximately 200 episodes in growing batch learning?**
  - The authors observe performance degradation similar to overfitting but state, "we leave further investigation of this effect for future work." The paper identifies the phenomenon where policies worsen despite more data and Bellman updates but does not offer a theoretical or empirical explanation for the divergence.

- **Can time-optimal (sparse) cost functions be made reliably learnable without external demonstrations or extensive reward shaping?**
  - Ablation studies show that time-optimal costs result in high variance and frequent failure without demonstrations, whereas shaped costs succeed. The paper suggests shaping or demonstrations are practically necessary for robustness, leaving the challenge of solving sparse reward tasks "from scratch" using NFQ2.0 unaddressed.

## Limitations
- The primary validation relies on a proprietary CartPole setup with specific hardware latencies and control loop timing, limiting generalizability.
- The paper does not extensively test the robustness of cost shaping to poorly designed functions across different control problems.
- The effectiveness of offline bootstrapping for larger, more complex state spaces or sparse initial datasets remains uncertain.

## Confidence
- **High confidence:** NFQ2.0's stability advantages over online RL are well-supported by both theoretical framing and ablation studies.
- **Medium confidence:** The stacking mechanism for handling latency is convincingly demonstrated for this specific CartPole system, but its generalizability to other delayed systems is not fully established.
- **Medium confidence:** The benefits of cost shaping for exploration are demonstrated, but the paper does not rigorously test edge cases where shaping might mislead the agent.

## Next Checks
1. **Cross-task shaping validation:** Test NFQ2.0 with shaped costs on a different control task (e.g., balancing a double pendulum) to verify the exploration benefits are not task-specific.
2. **Offline learning scalability test:** Apply the offline bootstrapping method to a more complex environment (e.g., higher-dimensional state space) to assess its robustness and identify failure modes.
3. **Latency robustness analysis:** Systematically vary the simulated latency in the CartPole environment and test different stacking depths to determine the mechanism's breaking point and provide a more general guideline for its use.