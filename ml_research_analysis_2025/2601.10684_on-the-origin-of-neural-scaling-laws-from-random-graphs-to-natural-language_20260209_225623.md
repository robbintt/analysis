---
ver: rpa2
title: 'On the origin of neural scaling laws: from random graphs to natural language'
arxiv_id: '2601.10684'
source_url: https://arxiv.org/abs/2601.10684
tags:
- scaling
- laws
- language
- power
- fits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates the origin of neural scaling laws in transformers\
  \ by studying simplified models of sequence prediction. The authors train transformers\
  \ on random walks on graphs (Erd\u0151s-R\xE9nyi, Barab\xE1si-Albert, and bigram\
  \ graphs) and find that power-law scaling laws emerge even when the input data has\
  \ no inherent power-law structure."
---

# On the origin of neural scaling laws: from random graphs to natural language

## Quick Facts
- arXiv ID: 2601.10684
- Source URL: https://arxiv.org/abs/2601.10684
- Reference count: 16
- Primary result: Power-law scaling laws emerge in transformers trained on random graph sequences even without inherent power-law structure in the data

## Executive Summary
This paper investigates the fundamental origins of neural scaling laws by training transformers on synthetic data generated from random walks over graphs. The authors demonstrate that power-law scaling behaviors emerge even when the input data lacks any inherent power-law structure, challenging the conventional wisdom that data properties are the primary driver of scaling phenomena. Through systematic experiments using Erdős-Rényi, Barabási-Albert, and bigram graphs, they show that scaling exponents can be tuned by dataset complexity and that 2-layer transformers with short context can reproduce key findings from large language model scaling studies. The work provides both theoretical insights and practical recommendations for more accurate compute-optimal scaling predictions.

## Method Summary
The authors train decoder-only GPT-style transformers on next-token prediction tasks using synthetic random walk sequences from various graph structures (Erdős-Rényi, Barabási-Albert, and bigram graphs). Models are trained for one epoch with AdamW optimization using both maximal update parameterization (µP) and standard parameterization (SP). They systematically vary model sizes (embedding dimensions 128-4096, 2-4 layers) and dataset sizes (5K-2M edges) while monitoring cross-entropy loss. Scaling laws are analyzed by fitting 1D power laws L(N)_D = E + A·N^{-α} and L(D)_N = E + B·D^{-β}, where E represents irreducible loss. For compute-optimal predictions, they use neural network regression instead of traditional 2D parametric fits. Language experiments use Fineweb-edu with GPT-2 tokenization, progressively increasing dataset complexity from random walks to sequences generated by increasingly sophisticated language models.

## Key Results
- Power-law scaling laws emerge from transformers trained on Erdős-Rényi graphs despite the absence of power-law structure in the input data
- Scaling exponents evolve monotonically as dataset complexity increases from random walks to sequences generated by more sophisticated language models
- 2-layer transformers with context lengths of 50-100 tokens can reproduce key scaling law patterns observed in large language models
- Maximal update parameterization appears more parameter-efficient than standard parameterization for achieving target performance
- Neural network regression provides more accurate compute-optimal scaling predictions than traditional 2D parametric fits

## Why This Works (Mechanism)
The emergence of scaling laws in transformers trained on random graph sequences suggests that the architecture itself, rather than data structure, plays a fundamental role in creating power-law relationships. The transformer's attention mechanism and positional encoding likely impose a hierarchical structure on the learned representations, leading to self-similar patterns that manifest as power laws. This architectural bias toward scale-free organization appears robust across different input distributions, explaining why scaling laws are observed even in synthetic data without inherent power-law properties. The authors propose that this intrinsic architectural property, combined with the statistical structure imposed by the training process, drives the universal emergence of scaling phenomena.

## Foundational Learning
- **Power-law scaling laws**: Mathematical relationships of the form y = A·x^{-α} that describe how performance metrics scale with model size or data size. Needed because scaling laws are the primary object of study. Quick check: Verify that log-log plots of loss vs. model size yield approximately linear relationships.

- **Random graph models**: Erdős-Rényi (ER) graphs have edges placed randomly with fixed probability, while Barabási-Albert (BA) graphs exhibit preferential attachment creating power-law degree distributions. Needed because these provide controlled synthetic datasets for studying scaling behavior. Quick check: Generate small ER and BA graphs and verify their degree distributions match theoretical expectations.

- **Maximal update parameterization (µP)**: A parameter initialization scheme where the maximal update (the change in parameters that maximally affects the output) is independent of model size. Needed because the paper compares µP to standard parameterization for efficiency. Quick check: Train identical architectures with µP and SP to see if µP converges faster or achieves better final performance.

- **Compute-optimal scaling**: The principle that for fixed computational budget, there exists an optimal ratio between model size and training tokens that minimizes loss. Needed because the paper critiques traditional scaling law fits for compute-optimal predictions. Quick check: For a fixed FLOPs budget, sweep different model sizes and training steps to find the configuration achieving minimum loss.

- **Neural network regression for scaling**: Using multi-layer perceptrons to predict loss L(N,D) from model size N and data size D, rather than parametric 2D fits. Needed because the authors propose this as a more accurate alternative for compute-optimal predictions. Quick check: Compare MSE of neural network regression versus parametric fits on held-out data points.

## Architecture Onboarding

**Component Map**: Graph generation -> Data preprocessing -> Transformer training -> Loss monitoring -> Scaling law fitting -> Compute-optimal prediction

**Critical Path**: The critical path flows from synthetic data generation through transformer training to the extraction and analysis of scaling law parameters. The quality of the random walk generation directly impacts the training data distribution, which determines the learned representations and ultimately the scaling behavior observed. The fitting procedure (including the irreducible loss term E) critically determines the accuracy of scaling exponent estimates and subsequent compute-optimal predictions.

**Design Tradeoffs**: The paper trades model complexity (2-4 layers) for systematic exploration of scaling behavior across diverse graph structures. This simplification enables controlled experiments but may limit direct applicability to state-of-the-art LLMs. The choice of µP versus SP represents a fundamental tradeoff between parameter efficiency and training stability. Using neural network regression instead of parametric fits trades interpretability for predictive accuracy in compute-optimal scenarios.

**Failure Signatures**: Training instabilities manifest as sudden loss spikes during optimization, particularly with SP at large learning rates. Poor scaling law fits occur when the irreducible loss term E is omitted, leading to underestimation of exponents and unreliable extrapolation. Inaccurate compute-optimal predictions result from using inappropriate fitting windows or 2D parametric forms that don't capture the true loss surface geometry.

**First Experiments**:
1. Generate an Erdős-Rényi graph with 8K nodes and 50K edges, sample 10K random walks of length 50, and verify the resulting token distribution is uniform
2. Train a 2-layer transformer with n_embd=512 using µP, sweep learning rates from 1e-5 to 1e-3, and identify the optimal learning rate by monitoring training stability
3. Fit the scaling law L(N)_D = E + A·N^{-α} to the training results and verify that including the irreducible loss E significantly improves the fit quality compared to a 2-parameter model

## Open Questions the Paper Calls Out
- **Internal activation analysis**: Whether power laws emerge in the internal activations or representations of transformers even when the input data lacks power-law structure. This remains unresolved because the paper demonstrates scaling laws without data power laws but doesn't analyze the model's internal learned representations. Evidence would come from spectral or distribution analysis of neuron activations in models trained on ER graphs to detect emergent power-law correlations not present in the input.

- **Biased random walk scaling**: How the scaling exponents α_D and β_N evolve as the bias exponent κ of random walks is continuously tuned between 0 and 1. The study only analyzed the marginal cases of κ=0 (unbiased) and κ=1 (marginal power law), leaving the continuous relationship between data structure and scaling laws undefined. Evidence would come from experimental results training transformers on random walks with varying κ ∈ [0, 1] and plotting the resulting scaling exponents as a function of κ.

- **Architecture-limited scaling**: Whether datasets or architectures can be constructed where scaling is limited by architectural capacity (β_N ≫ α_D) rather than data size. The paper's results generally showed α_D ≳ β_N (data-limited regimes) across both random graphs and natural language, failing to identify a regime where model expressivity is the primary bottleneck. Evidence would come from identifying a synthetic or natural task where increasing model size yields significantly higher returns than increasing data size.

## Limitations
- Results are primarily based on 2-layer transformers with relatively short context lengths (50-100 tokens), which may limit generalizability to larger language models
- The language model experiments use simplified architectures (up to 4 layers) that may not fully capture the scaling behavior of state-of-the-art LLMs
- The paper uses a 3-parameter power law fit but does not extensively validate this assumption across all experimental conditions

## Confidence
- **High confidence**: The core finding that power-law scaling emerges from random graph sequences, the methodological approach to studying scaling laws, and the demonstration that 2-layer transformers can reproduce key scaling law patterns
- **Medium confidence**: The claims about maximal update parameterization being more parameter-efficient, and the observation that compute-optimal scaling exponents remain stable across different dataset complexities
- **Medium confidence**: The conclusion that conventional 2D parametric fits are inadequate for compute-optimal scaling predictions, though this requires further validation

## Next Checks
1. Verify the power law fitting methodology by explicitly testing whether including the irreducible loss term E significantly improves fit quality and extrapolation accuracy compared to standard 2-parameter fits
2. Test whether the observed scaling behavior persists when using longer context lengths (e.g., 512-2048 tokens) and deeper architectures (e.g., 6-12 layers) to assess scalability
3. Conduct ablation studies on the graph generation parameters (edge density, degree distribution) to determine which structural properties are essential for producing the observed scaling laws