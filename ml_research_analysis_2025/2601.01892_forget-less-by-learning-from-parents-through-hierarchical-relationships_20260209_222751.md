---
ver: rpa2
title: Forget Less by Learning from Parents Through Hierarchical Relationships
arxiv_id: '2601.01892'
source_url: https://arxiv.org/abs/2601.01892
tags:
- concepts
- concept
- fllp
- learning
- hyperbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in custom diffusion
  models (CDMs) when learning new concepts sequentially. The proposed Forget Less
  by Learning from Parents (FLLP) method leverages hyperbolic geometry to model hierarchical
  relationships between concepts, using previously learned concepts as guidance for
  new ones.
---

# Forget Less by Learning from Parents Through Hierarchical Relationships

## Quick Facts
- arXiv ID: 2601.01892
- Source URL: https://arxiv.org/abs/2601.01892
- Reference count: 40
- Addresses catastrophic forgetting in custom diffusion models through hierarchical hyperbolic embeddings

## Executive Summary
This paper introduces Forget Less by Learning from Parents (FLLP), a method for continual learning of custom concepts in diffusion models that leverages hyperbolic geometry to model hierarchical relationships. The approach embeds concept representations in Lorentzian manifolds and enforces parent-child entailment through geometric constraints, significantly reducing catastrophic forgetting compared to existing methods. Evaluated across three public datasets and one synthetic benchmark, FLLP achieves consistent performance improvements while maintaining knowledge retention as the number of concepts scales.

## Method Summary
FLLP extends custom diffusion models by projecting CLIP image features and stored attention maps into Lorentzian manifolds, then applying a Hierarchical Parent Entailment Loss that constrains new concepts to reside within the entailment cones of previously learned parent concepts. The method uses a Union-Find-based ParentSearch algorithm to identify hierarchical relationships, and applies constraints specifically to image attention maps rather than LoRA weights to preserve text alignment. Training employs dual-step optimization with a total loss combining diffusion loss and entailment constraints.

## Key Results
- Achieves average gains of +1.2 in image alignment and +1.1 in text alignment scores across all datasets
- Outperforms state-of-the-art methods consistently on CIFC, CelebA, and ImageNet datasets
- Scales effectively to 35 concepts while maintaining superior knowledge retention
- Reduces parameter drift by ~22% compared to CIDM baseline

## Why This Works (Mechanism)

### Mechanism 1
Embedding concept representations in a Lorentzian manifold enables more efficient modeling of hierarchical relationships compared to Euclidean space. The framework utilizes exponential volume growth property of hyperbolic space to accommodate tree-like hierarchies, creating a geometry where distances and angles naturally encode "parent-child" depth and similarity. This works because concept relationships in custom diffusion exhibit hierarchical properties that map effectively to negatively curved manifolds.

### Mechanism 2
Enforcing a "Hierarchical Parent Entailment Loss" constrains new concept learning to build upon existing knowledge, reducing parameter interference associated with catastrophic forgetting. The system identifies a "parent" using recursive search based on Lorentzian distance, then applies an entailment cone constraint requiring the new "child" concept to reside within the parent's cone. This geometric constraint translates to positive knowledge transfer and stability in weight space.

### Mechanism 3
Applying hyperbolic constraints specifically to image attention maps—rather than LoRA weights—preserves text alignment while improving image alignment. Direct optimization of LoRA weights in hyperbolic space distorts text-related weights, breaking semantic relationships. By isolating constraints to image attention maps, the model preserves visual hierarchy without corrupting the text encoder's semantic understanding.

## Foundational Learning

- **Concept: Lorentzian Geometry (Hyperbolic Manifolds)**
  - Why needed here: This is the mathematical substrate of FLLP. Unlike Euclidean space (flat), hyperbolic space expands exponentially, making it ideal for representing branching, tree-like structures the paper relies on.
  - Quick check question: Can you explain why a "geodesic" in a Lorentzian manifold differs from a straight line in Euclidean space, and what that implies for measuring concept similarity?

- **Concept: Entailment Cones**
  - Why needed here: This is the geometric tool used to enforce hierarchy. A cone at a point defines a "region of influence" or "descendants."
  - Quick check question: If a child concept lies outside the entailment cone of its parent, does the loss function increase or decrease, and geometrically, what does that signify about their relationship?

- **Concept: Catastrophic Forgetting (Stability-Plasticity Dilemma)**
  - Why needed here: This is the core problem FLLP solves. You need to grasp why sequential learning typically fails (weights overwrite old tasks) to understand why geometric anchoring is a proposed solution.
  - Quick check question: In the paper's 1D Gaussian experiment, what value did the model converge to when it collapsed, and why was that a failure mode?

## Architecture Onboarding

- **Component map:** Input (Reference images & Text Prompts) -> Feature Extraction (CLIP image features, Token Embeddings) -> Hyperbolic Projection (Exponential map to Lorentzian manifold) -> Parent Search (Union-Find algorithm) -> Constraint Application (Computes L_entailParent) -> Optimization (Dual-step optimization with L_total)

- **Critical path:** The ParentSearch algorithm (Alg 2) is the linchpin. If it fails to identify a semantically meaningful parent, the L_entailParent loss will enforce a meaningless geometric constraint, potentially degrading generation quality.

- **Design tradeoffs:** The architecture must apply the entailment loss to image attention maps, not LoRA weights directly. Applying it to LoRA weights improves Image Alignment but significantly hurts Text Alignment. The threshold β requires tuning per concept to balance "inheritance" vs. "novelty." The system is limited to ~35 concepts by the CLIP tokenizer's 77-token limit.

- **Failure signatures:** Semantic Drift (generated images converge toward "mean of means"), Zero-Sum Alignment (Text Alignment drops while Image Alignment rises), Loop Errors (ParentSearch gets stuck in cycles).

- **First 3 experiments:**
  1. Replicate the 1D Gaussian experiment. Train sequentially on 5 distinct means. Verify forgetting rate is lower with FLLP (~11.4) than baseline (18.6) or CIDM (13.2).
  2. Implement FLLP but switch L_entailment calculation to use LoRA weights instead of image attention maps. Confirm IA may rise but TA should drop.
  3. Run a sweep on threshold β (0.1 to 1.0) on a single concept from CIFC dataset. Plot CLIP scores to confirm different concepts have distinct optimal threshold values.

## Open Questions the Paper Calls Out

### Open Question 1
Can a new evaluation metric be developed to capture fine-grained or personalized attributes that CLIP scores miss? The paper acknowledges CLIP's reliance on global image-text similarity and the need for better metrics to handle nuanced attributes like "a dirty car."

### Open Question 2
How can the framework scale beyond the 35-concept limit imposed by the CLIP tokenizer? The paper notes an "architectural constraint imposed by the CLIP tokenizer," setting an upper bound of approximately 35 concepts.

### Open Question 3
Can LoRA weights be directly optimized in hyperbolic space without sacrificing text alignment? The paper poses whether direct hyperbolic optimization of LoRA weights could yield simultaneous improvements in both Image Alignment and Text Alignment scores.

## Limitations
- Effectiveness depends on meaningful parent-child relationships; may introduce noise for disjoint concept sets
- Fixed curvature parameter and learning rates for Lorentzian operations haven't been thoroughly explored for sensitivity
- Evaluation focuses on CLIP-based metrics without assessing downstream task performance or human perceptual quality

## Confidence

- **H1 (Hyperbolic Geometry Efficiency):** High Confidence - Well-supported by 1D Gaussian experiment and theoretical grounding
- **H2 (Entailment Cone Effectiveness):** Medium Confidence - Mechanism clearly defined but geometric containment-to-knowledge-transfer assumption lacks direct validation
- **H3 (Modality-Specific Constraints):** High Confidence - Ablation study in Table 3 provides clear evidence
- **H4 (Generalization Across Datasets):** Medium Confidence - Performance gains consistent but concept diversity limited

## Next Checks

1. **Disjoint Concept Performance Test:** Design experiment with minimal semantic overlap (e.g., "tropical fish," "Renaissance architecture," "quantum physics diagrams" sequentially). Measure whether FLLP still outperforms baselines when parent-child relationships are forced rather than natural.

2. **Geometric Parameter Sensitivity Analysis:** Systematically vary curvature parameter (0.1 to 2.0) and learning rates for Lorentzian operations. Identify dataset-specific optimal values and characterize performance tradeoffs.

3. **Human Perceptual Validation:** Conduct human study rating generated images for identity preservation, style consistency, and quality. Compare FLLP against CIDM and baseline across three main datasets to validate CLIP improvements correspond to perceptual improvements.