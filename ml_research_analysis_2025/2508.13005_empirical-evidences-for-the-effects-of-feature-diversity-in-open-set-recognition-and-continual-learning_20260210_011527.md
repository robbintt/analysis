---
ver: rpa2
title: Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition
  and Continual Learning
arxiv_id: '2508.13005'
source_url: https://arxiv.org/abs/2508.13005
tags:
- learning
- data
- recognition
- continual
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides empirical evidence that enhancing feature diversity
  improves both open set recognition (OSR) and continual learning. The authors conducted
  controlled experiments on a synthetic dataset where models learned combinations
  of colors and shapes.
---

# Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning

## Quick Facts
- arXiv ID: 2508.13005
- Source URL: https://arxiv.org/abs/2508.13005
- Reference count: 40
- One-line primary result: Models learning diverse features show improved open set recognition performance and better forward transfer in continual learning tasks.

## Executive Summary
This paper provides empirical evidence that enhancing feature diversity improves both open set recognition (OSR) and continual learning. The authors conducted controlled experiments on a synthetic dataset where models learned combinations of colors and shapes. They compared two supervised classifiers: one learning primarily color features (E1) and another learning both color and shape features (E2). Results showed that the more diverse feature model (E2) achieved better OSR performance, as measured by larger Mahalanobis distance histograms between close-set and open-set samples. For continual learning, the study found that models learning more diverse features better retained previously learned data and facilitated learning new tasks when feature overlap existed.

## Method Summary
The study used synthetic 64x64 images with controlled color-shape combinations. Two base models were trained: E1 (binary classification solvable by color alone) and E2 (3-class requiring shape discrimination). OSR evaluation used Mahalanobis distance histograms between close-set and open-set samples. Continual learning experiments involved sequential task updates without regularization, measuring feature forgetting via CKA similarity and forward transfer via linear probing accuracy. The CNN architecture consisted of Conv2D → AvgPool → Flatten → Linear layers with 20-dimensional features before the classifier.

## Key Results
- E2 models (learning both color and shape) achieved significantly better OSR performance with larger Mahalanobis distance separation between close-set and open-set samples
- Feature diversity improved forward transfer in continual learning when new tasks shared feature dimensions with previously learned tasks
- Diverse feature learning reduced forgetting of previously learned data when features could be reused across tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning diverse features improves open set recognition by increasing the separability between known and unknown class distributions in feature space.
- Mechanism: When a model learns multiple feature types (e.g., both color and shape), open-set samples that share some features with close-sets but differ in others occupy distinct regions in the learned representation space. This increases the Mahalanobis distance between open-set samples and their closest close-set class centers, reducing histogram overlap and enabling better threshold-based rejection.
- Core assumption: The open-set samples differ from close-sets along at least one learned feature dimension; the model has actually learned (not just exposed to) multiple feature types.
- Evidence anchors:
  - [abstract] "In OSR, models learning both color and shape features show better separation between close-set and open-set samples compared to models relying primarily on color."
  - [Section 3.2] E2 models (trained on 3 classes requiring shape discrimination) achieve larger Dhist values than E1 models (2 classes, color-sufficient), indicating better OSR separation. Figure 3 shows E2 curves at or above E1 curves across all open-set classes.
  - [corpus] "Know Yourself Better: Diverse Object-Related Features Improve Open Set Recognition" directly corroborates this mechanism, though other corpus papers focus on different OSR approaches without directly testing feature diversity.

### Mechanism 2
- Claim: Feature diversity improves forward transfer in continual learning when new tasks share feature dimensions with previously learned tasks.
- Mechanism: Diverse feature learning creates a richer representation basis. When new tasks arrive, features can be reused via linear probing without updating the feature extractor. If the base model has already encoded relevant features (e.g., shape features), new tasks requiring those features achieve higher accuracy with minimal adaptation.
- Core assumption: There exists feature overlap between old and new tasks; the model has encoded the relevant features in a sufficiently disentangled form that they can be selectively accessed.
- Evidence anchors:
  - [abstract] "feature diversity...improves forward transfer to new tasks when feature overlap exists between old and new data."
  - [Section 3.3.2] Linear probing accuracy: E5/E6 (tasks requiring shape only) shows 70.5% for E1 base vs. 87.33% for E2 base—a 16.8 percentage point gain because E2 learned shapes better. E9/E10 (pink ellipses) shows 53.17% vs. 95.5% respectively, because pink approximates red (color overlap) and E2 learned both features.
  - [corpus] No direct corpus support for this specific mechanism; corpus papers focus primarily on OSR rather than continual learning transfer.

### Mechanism 3
- Claim: Feature diversity reduces feature forgetting during task updates because shared features receive reinforcement from multiple task contexts.
- Mechanism: When features are shared across tasks (e.g., color), updating for new tasks provides implicit rehearsal of those feature representations. Diverse models encode more features overall, increasing the probability that any given feature will be reinforced across sequential tasks, reducing representational drift.
- Core assumption: Feature overlap provides implicit regularization; features not actively used in new tasks are the ones most vulnerable to drift.
- Evidence anchors:
  - [abstract] "feature diversity reduces forgetting of previously learned data"
  - [Section 3.3.1] CKA similarity scores: E3/E4 (color-reuse tasks) show 0.0213-0.0219 similarity; E5/E6 (shape-only tasks) show lower 0.0127-0.018 similarity, indicating more forgetting when learned features cannot be reused. The paper states: "the colors, which can show higher weights in the feature vectors, can therefore be forgotten more in E5 & E6."
  - [corpus] No direct corpus evidence for this mechanism; related work on catastrophic forgetting was not retrieved.

## Foundational Learning

- Concept: **Mahalanobis Distance for Distributional Outlier Detection**
  - Why needed here: The paper uses Mahalanobis distance (Mc) to quantify how far test samples lie from class-conditional distributions. Understanding that this metric accounts for covariance structure (not just Euclidean distance) is essential for interpreting why diverse features improve cluster compactness and separability.
  - Quick check question: Given a class with features that have high variance along dimension A but low variance along dimension B, would a sample deviating by 2 standard deviations on B be more or less anomalous than one deviating by 2 standard deviations on A under Mahalanobis distance?

- Concept: **Centered Kernel Alignment (CKA) for Representational Similarity**
  - Why needed here: CKA quantifies how similar two sets of neural representations are, enabling measurement of "feature forgetting" by comparing representations before and after learning new tasks. This metric is central to the continual learning analysis in Section 3.3.1.
  - Quick check question: If CKA between Task 0 and Task 1 representations is 1.0, what does that imply about feature drift? If it's 0.0?

- Concept: **CNN Texture/Color Bias**
  - Why needed here: The paper relies on CNNs' known bias toward color over shape [14, 38] to create controlled conditions where E1 models learn primarily color and E2 models are forced to learn shape. Understanding this bias explains why synthetic datasets are necessary for controlled feature manipulation.
  - Quick check question: Why might a CNN trained on colored shapes achieve high accuracy while still failing to learn shape features?

## Architecture Onboarding

- Component map:
  Input -> 5-Layer CNN (Conv2D → AvgPool → Flatten → Linear 10240→1000 → Linear 1000→20) -> Classifier head (Linear 20→num_classes) -> OSR scoring (Mahalanobis distance computation) -> Forgetting measurement (CKA similarity) -> Transfer measurement (Linear probing accuracy)

- Critical path:
  1. Design class configurations that force/diversify feature reliance (e.g., binary tasks solvable by color alone vs. multi-class tasks requiring shape)
  2. Train base model for 100 epochs, verify feature learning via confusion matrices
  3. Extract features and compute class statistics (μc, Σc) for OSR scoring
  4. For continual learning: add output neurons for new classes, train on new task without regularization
  5. Evaluate: (a) Dhist for OSR, (b) CKA for forgetting, (c) linear probing accuracy for forward transfer

- Design tradeoffs:
  - **Synthetic vs. real data**: Synthetic enables precise feature manipulation but limits generalizability. Paper acknowledges: "experiments are based on synthetic datasets, which are of small volume."
  - **Feature diversity vs. task difficulty**: Forcing diverse learning (E2) increases training complexity (longer convergence, higher confusion during early epochs as shown in Figure 2)
  - **Background manipulation**: Using white backgrounds for CKA evaluation introduces domain shift but is necessary to reduce ceiling effects in similarity scores
  - **No regularization vs. practical methods**: Naive sequential training produces ~50% accuracy on old data (catastrophic forgetting), but allows clean measurement of feature-level effects

- Failure signatures:
  - **Low Dhist despite diverse training**: Features may not be disentangled; color dominates representation. Check if Mahalanobis distances cluster by color regardless of shape.
  - **Identical transfer performance across base models**: No feature overlap between old/new tasks, or features learned but not in reusable form. Verify with linear probing on individual feature dimensions.
  - **High CKA but low accuracy**: Features preserved but classifier head catastrophically interfered. This is expected in the paper's setup (no regularization methods applied).

- First 3 experiments:
  1. **Replicate E1 vs. E2 OSR comparison**: Train binary color-only classifier (e.g., blue circles vs. red rectangles) and ternary color+shape classifier (add red circles). Compute Dhist for held-out open-set classes (e.g., green rectangles). Verify E2 achieves higher separation.
  2. **Ablate feature overlap in continual learning**: Using E2 as base, train sequential tasks with (a) color overlap only, (b) shape overlap only, (c) no overlap. Measure CKA and linear probing accuracy. Confirm transfer gains correlate with overlap type.
  3. **Test feature disentanglement hypothesis**: Train two models—one with standard cross-entropy, one with explicit feature disentanglement (e.g., adversarial feature separation). Compare OSR performance on challenging open-sets where features are correlated (e.g., pink ellipses vs. red circles). The paper hypothesizes disentanglement should help but leaves this for future work.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the observed benefits of feature diversity be replicated in complex, real-world datasets where features are not strictly controlled or disjoint?
- Basis in paper: [explicit] The authors state, "In future work, we hope new experiments can be designed for real and complex data" to validate findings derived from synthetic datasets.
- Why unresolved: The study relies entirely on a synthetic dataset (shapes and colors) which allows for precise manipulation but lacks the noise and high dimensionality of natural data.
- What evidence would resolve it: Empirical results demonstrating that models encouraged to learn diverse features on standard benchmarks (e.g., CIFAR, ImageNet) show statistically significant improvements in OSR and continual learning metrics.

### Open Question 2
- Question: Does explicitly disentangling features provide greater benefits for open set recognition and forward transfer than simply increasing feature diversity?
- Basis in paper: [explicit] The paper notes, "feature disentanglement remains an interesting topic to explore... We hypothesize that disentangling features can be positive for both OSR and continual learning."
- Why unresolved: The current experiments manipulate diversity by forcing the learning of distinct attributes (color vs. shape), but they do not implement or test mechanisms to disentangle these features within the latent space.
- What evidence would resolve it: Comparative experiments showing that disentangled representations allow for better separation of open-set samples or more efficient reuse of features in new tasks compared to entangled diverse features.

### Open Question 3
- Question: Is the advantage of feature diversity in continual learning maintained when standard regularization or replay techniques are applied?
- Basis in paper: [inferred] The methodology notes, "no extra continual learning approaches have been applied in all experiments here" in order to isolate feature effects.
- Why unresolved: It is unclear if feature diversity remains a dominant factor in reducing forgetting when mechanisms like knowledge distillation or memory replay are active, as these methods also constrain the feature space.
- What evidence would resolve it: Ablation studies combining diversity-encouraging methods with standard algorithms (e.g., EWC, iCaRL) to determine if diversity offers marginal gains or is rendered redundant by other stabilization techniques.

## Limitations
- Findings based entirely on synthetic datasets with controlled feature configurations, limiting generalizability to real-world scenarios
- No continual learning regularization methods applied, creating an artificial learning scenario with ~50% baseline forgetting
- Feature disentanglement not explicitly measured or tested, leaving the mechanism behind observed benefits unclear

## Confidence
- **High Confidence**: The empirical demonstration that diverse feature learning improves OSR performance through increased Mahalanobis distance separation is well-supported by controlled experiments.
- **Medium Confidence**: The continual learning findings regarding forward transfer are convincing but rely on synthetic task configurations that may not reflect real-world feature overlap patterns.
- **Low Confidence**: The claim about feature diversity reducing forgetting lacks strong empirical support since no regularization methods were tested to isolate the feature diversity effect from the baseline catastrophic forgetting.

## Next Checks
1. **Generalization Test**: Apply the E1/E2 methodology to a real-world dataset (e.g., CIFAR-100 with synthetic feature perturbations) to validate whether diverse feature learning transfers beyond synthetic conditions.
2. **Disentanglement Verification**: Implement explicit feature disentanglement (e.g., adversarial feature separation) and measure whether this amplifies the observed OSR and transfer benefits beyond what diversity alone provides.
3. **Cross-task Transfer Analysis**: Design continual learning experiments where new tasks require novel feature combinations (not just individual features) to test whether diverse base models better compose features for compositional generalization.