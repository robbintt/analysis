---
ver: rpa2
title: 'FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning'
arxiv_id: '2508.04064'
source_url: https://arxiv.org/abs/2508.04064
tags:
- flat
- attack
- attacks
- backdoor
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLAT addresses the limitation of existing FL backdoor attacks,
  which are constrained by fixed, single-target triggers that are easily detected.
  The core innovation is a latent-driven conditional autoencoder that generates diverse,
  target-specific perturbations.
---

# FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning

## Quick Facts
- arXiv ID: 2508.04064
- Source URL: https://arxiv.org/abs/2508.04064
- Authors: Tuan Nguyen; Khoa D Doan; Kok-Seng Wong
- Reference count: 24
- Primary result: Achieves 94.7% ASR on CIFAR-10 with visually adaptive triggers

## Executive Summary
FLAT introduces a novel backdoor attack framework for federated learning that overcomes the limitation of fixed, single-target triggers. The method employs a latent-driven conditional autoencoder to generate diverse, target-specific perturbations that can be dynamically sampled from a latent space. This enables attackers to select arbitrary target classes without retraining the attack model. Extensive experiments demonstrate FLAT achieves superior attack success rates while maintaining competitive clean accuracy and showing resilience against advanced federated learning defenses.

## Method Summary
FLAT proposes a latent-driven conditional autoencoder architecture that generates target-specific triggers for backdoor attacks in federated learning. The core innovation lies in using latent space sampling to create diverse, visually adaptive perturbations rather than relying on fixed triggers. During the attack, the conditional autoencoder samples from a learned latent distribution to generate unique triggers for any desired target class. This approach enables arbitrary target selection without retraining, addressing a key limitation of existing backdoor attacks. The method integrates seamlessly into the federated learning pipeline, with the backdoor trigger generation occurring locally on compromised clients.

## Key Results
- Achieves up to 94.7% attack success rate on CIFAR-10
- Maintains competitive clean accuracy of 82.1% on CIFAR-10
- Demonstrates superior resilience against advanced federated learning defenses compared to baseline attacks
- Outperforms existing methods across four benchmark datasets

## Why This Works (Mechanism)
FLAT works by leveraging the conditional autoencoder's ability to learn a rich latent representation of trigger patterns. By sampling from this latent space, the attack can generate diverse triggers that are visually adaptive to different target classes. This diversity makes detection more difficult compared to fixed triggers. The conditional aspect ensures that generated triggers are specifically optimized to misclassify inputs as the desired target class. The latent-driven approach allows for arbitrary target selection by simply changing the conditioning input, eliminating the need for retraining when switching targets.

## Foundational Learning

**Federated Learning**: Distributed machine learning where clients collaboratively train a global model without sharing raw data
- Why needed: FLAT operates within the federated learning paradigm where data privacy and distributed training are key
- Quick check: Understanding how federated averaging aggregates model updates across clients

**Backdoor Attacks**: Malicious modifications that cause models to behave incorrectly when specific triggers are present
- Why needed: FLAT is a specific type of backdoor attack targeting federated learning systems
- Quick check: Recognizing how triggers can be embedded in training data to manipulate model behavior

**Conditional Autoencoders**: Neural networks that learn to reconstruct inputs while being conditioned on additional information
- Why needed: FLAT uses this architecture to generate target-specific triggers based on desired output classes
- Quick check: Understanding how conditioning information influences the reconstruction process

**Latent Space Sampling**: Generating new data by sampling from the learned latent representation of a generative model
- Why needed: Enables FLAT to create diverse triggers by sampling different points in the latent space
- Quick check: Recognizing how latent space geometry affects generated output diversity

## Architecture Onboarding

**Component Map**: Input Data -> Conditional Autoencoder -> Latent Space -> Trigger Generator -> Backdoored Model

**Critical Path**: The attack pipeline follows: (1) conditional autoencoder training on poisoned data, (2) latent space sampling for trigger generation, (3) trigger application to benign inputs, (4) model training with backdoored data

**Design Tradeoffs**: FLAT trades increased model complexity (conditional autoencoder) for the benefit of arbitrary target selection and trigger diversity. The approach requires additional training overhead but eliminates the need for multiple attack models or retraining when changing targets.

**Failure Signatures**: Potential failures include: (1) insufficient trigger diversity leading to detection, (2) poor conditional generation causing ineffective triggers, (3) over-compression in latent space reducing trigger effectiveness, (4) detection through statistical analysis of trigger patterns

**First 3 Experiments**:
1. Validate basic trigger generation by testing whether generated triggers successfully cause misclassification to the target class
2. Measure trigger diversity by computing similarity metrics between multiple triggers generated for the same target
3. Test arbitrary target selection by generating triggers for multiple different target classes and measuring success rates for each

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Limited evaluation to standard benchmark datasets without testing on real-world, high-resolution, or domain-specific data
- Incomplete characterization of training requirements and convergence behavior under federated constraints
- Insufficient detail on specific defense mechanisms tested and their configurations
- Reliance on qualitative stealthiness assessment rather than rigorous quantitative detection metrics

## Confidence

**High Confidence**: Empirical results showing FLAT's superior ASR and ACC compared to baselines on tested datasets; the architectural feasibility of using latent-driven conditional autoencoders for trigger generation.

**Medium Confidence**: Claims about resilience against "advanced FL defenses" due to limited methodological detail; generalizability to non-standard datasets and real-world deployment scenarios.

**Low Confidence**: Stealthiness claims without quantitative detection-rate analysis; scalability assertions under heterogeneous and resource-constrained federated environments.

## Next Checks

1. **Real-World Dataset Validation**: Evaluate FLAT on high-resolution, domain-specific datasets (e.g., medical imaging or satellite imagery) to assess scalability and effectiveness beyond standard benchmarks.

2. **Defense Mechanism Benchmarking**: Systematically test FLAT against a broader range of state-of-the-art FL defenses (e.g., anomaly detection, robust aggregation, and Byzantine-robust methods) with detailed configuration reporting.

3. **Communication and Resource Overhead Analysis**: Quantify the communication and computational costs of training and deploying the conditional autoencoder in federated settings, particularly under client heterogeneity and bandwidth constraints.