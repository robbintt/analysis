---
ver: rpa2
title: 'Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in
  Narratives?'
arxiv_id: '2512.07777'
source_url: https://arxiv.org/abs/2512.07777
tags:
- story
- llms
- coherence
- incoherent
- coherent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  reliably detect incoherence in narratives by leveraging a dataset of paired coherent
  and incoherent stories. The authors use token-level perplexity, probing of hidden
  states, and explicit coherence/quality ratings to assess model performance.
---

# Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?

## Quick Facts
- **arXiv ID**: 2512.07777
- **Source URL**: https://arxiv.org/abs/2512.07777
- **Reference count**: 16
- **Primary result**: LLMs encode coherence information internally but fail to reliably report it behaviorally, showing greater sensitivity to event-setting than trait-behavior incoherence

## Executive Summary
This study investigates whether large language models can detect narrative incoherence by comparing their internal representations with explicit behavioral judgments. Using a dataset of paired coherent and incoherent stories, the authors find that models encode coherence information at the point of inconsistency (achieving ~80% probe accuracy) but this information degrades by story's end. Paradoxically, explicit coherence/quality ratings fail to distinguish coherent from incoherent narratives, even for reasoning models. Models also show asymmetric sensitivity to event-setting conflicts over trait-behavior conflicts, a pattern not observed in humans. These findings reveal a gap between LLMs' internal representations and their ability to report narrative coherence, suggesting incomplete narrative understanding.

## Method Summary
The study uses 25 paired coherent/incoherent stories (18 from Rizzella & O'Brien 2024, 7 augmented) with two incoherence types: trait-behavior (60%) and event-setting (40%). For each pair, models compute perplexity at the target sentence, extract hidden states at the last token of this sentence and story end, and train linear probes to classify coherence. Behavioral experiments collect 7-point Likert ratings and forced-choice True/False judgments using 30 prompt variations per story. The approach tests whether internal coherence representations can be linearly decoded and whether models can report coherence through explicit behavioral judgments.

## Key Results
- Linear probes achieve ~80% accuracy at classifying coherent vs. incoherent stories at the target token, but accuracy drops dramatically at story end
- Models show greater sensitivity to event-setting incoherence than trait-behavior incoherence, opposite to human patterns
- Explicit coherence/quality ratings fail to reliably distinguish coherent from incoherent narratives (best model shows only 1.66 point mean difference)
- Four of six tested models show <1 point mean difference between coherent and incoherent stories in rating experiments

## Why This Works (Mechanism)

### Mechanism 1: Local Coherence Encoding in Hidden States
When processing the target sentence containing an inconsistency, attention mechanisms integrate conflicting prior context, producing distinguishable activation patterns. Probes trained on these representations achieve ~80-95% accuracy classifying coherent vs. incoherent narratives at the target token. The most informative layers tend to be mid-to-upper layers, consistent with semantic integration findings. This suggests transformers can maintain state representations, though text-only pretraining may be insufficient.

### Mechanism 2: Perplexity Sensitivity to Incoherence via World Knowledge
LLMs assign higher perplexity to incoherent continuations, with stronger effects for event-setting conflicts (e.g., rain in the desert) than trait-behavior conflicts (e.g., vegetarian eating cheeseburger). This asymmetry reflects differential engagement of parametric knowledge vs. contextual reasoning, with event-setting conflicts conflicting with statistical patterns in pretraining data while trait-behavior conflicts require integrating story-specific character information.

### Mechanism 3: Coherence Information Decay Across Narrative Processing
Coherence representations are encoded locally at the target token but degrade by story's end, particularly in smaller models. Information at the target token position may not automatically propagate forward through the residual stream, leaving later tokens without access to coherence signals. This explains why probe accuracy drops from ~80% at the target to near-chance levels at story end.

## Foundational Learning

- **Probing classifiers**: The paper relies on linear probes to assess whether coherence information is encoded in hidden states. Without understanding probing, readers cannot evaluate whether the claimed "internal representation" findings are valid. Quick check: If a probe achieves 85% accuracy on layer 15 but 55% at the final layer, what does this suggest about where the model encodes the relevant information?

- **Perplexity as surprisal**: The paper uses perplexity differences between coherent and incoherent targets as evidence of sensitivity to narrative violations. Quick check: A model assigns PPL=12 to a coherent continuation and PPL=22 to an incoherent one. What does this 10-point difference represent in information-theoretic terms?

- **Situation models in discourse comprehension**: The paper frames narrative understanding through the lens of human reading comprehension research, where readers build and update mental representations. Quick check: What is the difference between local and global coherence in the situation model framework?

## Architecture Onboarding

- **Component map**: Paired narratives (coherent/incoherent) with identical introduction and conclusion → perplexity computation at critical sentence → hidden state extraction at target token and story end → linear probe classification → behavioral evaluation with ratings and forced-choice

- **Critical path**: 1) Construct paired dataset with controlled inconsistencies, 2) Extract perplexity at target sentence → confirm PPL(incoherent) > PPL(coherent), 3) Train linear probes on hidden states at target token → expect ~80%+ accuracy, 4) Probe at story-end token → expect accuracy drop, 5) Collect explicit ratings → expect weak separation

- **Design tradeoffs**: Dataset size vs. control (25 pairs enable precise manipulation but limit generalization), internal vs. behavioral measures (perplexity/probing reveal implicit sensitivity but require conflict location; ratings are task-relevant but show poor sensitivity), model scale (larger models show better performance but still fail behaviorally)

- **Failure signatures**: False positives in reasoning traces (identifying innocuous details as incoherent), asymmetric sensitivity (event-setting conflicts detected better than trait-behavior), rating collapse (four models show <1 point mean difference)

- **First 3 experiments**: 1) Replicate probing with attention head ablation to test whether coherence information requires specific attention patterns to persist, 2) Construct unexpected-but-coherent story pairs to disentangle perplexity from genuine incoherence detection, 3) Test event-setting conflicts in fantasy narratives with narrative-specific rules to isolate world knowledge contributions

## Open Questions the Paper Calls Out

- **Do LLMs conflate unexpectedness with incoherence, and can these be disentangled?**: The current dataset confounds these attributes; a proper study requires paired unexpected-incoherent stories. Evidence would come from a controlled dataset with unexpected-but-coherent stories testing whether LLMs differentiate them.

- **How do LLMs respond to event-setting conflicts in fantasy worlds with narrative-specific rules?**: Current event-setting conflicts engage pretrained world knowledge; fantasy settings would isolate within-story reasoning. Evidence would come from experiments using fantasy narratives where setting rules are established internally.

- **Why do LLMs show greater sensitivity to event-setting conflicts than trait-behavior conflicts when humans show no such asymmetry?**: The mechanism remains unclear—whether it reflects pretraining biases, insufficient character modeling, or something else. Evidence would come from ablation studies controlling for world knowledge access.

## Limitations

- **Dataset Scope**: Conclusions based on 25 carefully constructed story pairs with limited generalizability to broader narrative types and coherence phenomena.
- **Probing Interpretability**: High probe accuracy may reflect detection of unexpectedness rather than genuine understanding of narrative inconsistency.
- **Behavior vs. Internal Representations Gap**: Fundamental disconnect between models' ability to encode coherence information internally and their inability to reliably report this information behaviorally.

## Confidence

- **High Confidence**: Linear probes achieve high accuracy at target token but performance degrades at story end, consistently observed across multiple models.
- **Medium Confidence**: Models encode coherence information in hidden states, though interpretation that this reflects genuine understanding remains uncertain.
- **Low Confidence**: Behavioral judgment results showing models cannot reliably distinguish coherent from incoherent narratives, should be interpreted cautiously given small sample size and possible task design issues.

## Next Checks

1. **Attention Head Ablation at Story End**: Perform targeted ablation of attention heads showing high attention to conflict location when probing at story end. If coherence information requires specific attention patterns to persist, ablating these heads should cause probe accuracy to drop toward chance.

2. **Unexpected-but-Coherent Controls**: Construct paired stories where target sentence is genuinely surprising but logically consistent. If perplexity and probe accuracy are primarily detecting unexpectedness rather than incoherence, these pairs should show similar effects to incoherent conditions.

3. **Fantasy Setting Inconsistency Test**: Create event-setting incoherence pairs set in fantasy narratives with explicit world-building rules contradicting real-world expectations. If sensitivity to event-setting incoherence is driven by world knowledge, these pairs should show reduced perplexity differences and lower probe accuracy.