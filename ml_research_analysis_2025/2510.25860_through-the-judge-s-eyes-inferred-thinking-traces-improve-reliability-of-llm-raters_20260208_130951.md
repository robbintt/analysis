---
ver: rpa2
title: 'Through the Judge''s Eyes: Inferred Thinking Traces Improve Reliability of
  LLM Raters'
arxiv_id: '2510.25860'
source_url: https://arxiv.org/abs/2510.25860
tags:
- human
- traces
- story
- thinking
- raters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a human-LLM collaborative framework that uses
  rejection sampling to infer thinking traces from label-only annotations. These inferred
  traces are then used to fine-tune open LLM raters and refine annotation guidelines
  for proprietary models, significantly improving alignment with human judgments across
  multiple tasks.
---

# Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters

## Quick Facts
- arXiv ID: 2510.25860
- Source URL: https://arxiv.org/abs/2510.25860
- Authors: Xingjian Zhang; Tianhong Gao; Suliang Jin; Tianhao Wang; Teng Ye; Eytan Adar; Qiaozhu Mei
- Reference count: 40
- Key outcome: Rejection sampling of thinking traces from label-only annotations improves LLM rater alignment with human judgments across five benchmarks.

## Executive Summary
This paper introduces a human-LLM collaborative framework that infers reasoning traces from label-only annotations through rejection sampling. The framework generates multiple candidate traces using a Reasoning Language Model, accepts those matching human labels, and uses these traces to either fine-tune open LLM raters or refine annotation guidelines for proprietary models. Results show significant improvements in alignment with human judgments across five datasets, with Kendall's tau improving from 0.197 to 0.281 on average. The method addresses the challenge of aligning LLM raters with human judgment without requiring expensive full reasoning annotations.

## Method Summary
The framework works by first generating $k$ candidate thinking traces for each input using a Reasoning Language Model, then filtering through rejection sampling to keep only those where the predicted label matches the human gold standard. These accepted traces form a dataset that can be used in two ways: (1) supervised fine-tuning of open LLM raters to teach them how to reason toward correct judgments, or (2) codebook refinement that synthesizes explicit evaluation guidelines from common reasoning patterns found in the traces. The approach was validated across five benchmarks including HANNA, Kaggle Essay, SummEval, WMT-2020, and HelpSteer2, using datasets filtered for low inter-annotator variance.

## Key Results
- Reasoning-enhanced SFT significantly improved alignment, with Kendall's tau increasing from 0.197 to 0.281 on average across tasks
- Codebook refinement improved inter-rater reliability among different LLMs, reducing top-down bias
- SFT showed consistent improvements on objective tasks (complexity, consistency, fluency) but less reliable gains on subjective dimensions like engagement
- The refined codebooks successfully aligned different proprietary models (Claude, DeepSeek, GPT-5) on consistent ratings

## Why This Works (Mechanism)

### Mechanism 1
Conditioning acceptance of generated reasoning traces on their concordance with ground-truth human labels creates a scalable proxy for latent human reasoning. By filtering candidates where the model's predicted label matches the human gold standard, the framework assumes that reasoning paths resulting in correct labels are more likely to approximate human cognitive processes. Core assumption: reaching the correct conclusion implies a higher probability of following a valid reasoning trajectory. Break condition: if the generator model has low capability for the task, accepted traces may be rare hallucinations coincidentally aligned with the label.

### Mechanism 2
Supervised Fine-Tuning on reasoning traces improves rater alignment by teaching the model "how" to judge, rather than just "what" rating to assign. Training on $(Input, Accepted Trace, Label)$ sequences forces the model to internalize intermediate evaluation steps defined in the inferred traces, effectively regularizing toward a more robust decision boundary. Core assumption: inferred traces contain generalizable signal rather than just noise or dataset-specific artifacts. Break condition: if the seed dataset contains systematic biases, the model will learn to reproduce those biases with unjustified confidence.

### Mechanism 3
Synthesizing explicit codebooks from reasoning patterns reduces ambiguity in black-box models more effectively than manual prompt engineering. Inferred traces are aggregated to identify common step-by-step procedures and concrete critiques, which are distilled into a refined prompt. This converts implicit, distributed knowledge into explicit, structured constraints in the context window, focusing the model's attention on relevant features. Core assumption: proprietary models are sufficiently instruction-following to adhere to complex, synthesized procedures. Break condition: for inherently subjective tasks without objective criteria, refining instructions may reduce inter-rater reliability by eliminating shared systematic bias.

## Foundational Learning

- **Reasoning Language Models (RLMs) vs. Standard LLMs**: RLM models output "thinking tokens" (reasoning traces) before the final answer, unlike standard LLMs that output immediate answers. This distinction is critical because the entire pipeline depends on accessing intermediate reasoning tokens. Quick check: Can you identify the separation between the reasoning trace and the final label in a model output?

- **Rejection Sampling**: This is the core data processing step where multiple candidates are generated and filtered based on a verifier (human label) to approximate a distribution we cannot sample directly (human thoughts). Quick check: If the generator model only agrees with the human label 10% of the time, how does that impact the cost and diversity of the resulting dataset?

- **Kendall's Tau and ICC**: These are the primary metrics for success. Kendall's tau measures rank correlation (ordinal data) while ICC measures consistency across different raters. Quick check: Why is Kendall's Tau preferred over Accuracy for subjective Likert-scale ratings?

## Architecture Onboarding

- **Component map**: Seed Loader -> Trace Generator (RLM) -> Rejection Filter -> Trainer (SFT) or Refiner (Synthesizer)
- **Critical path**: The Rejection Filter efficiency. If the generator model's alignment with human labels is low, you will burn significant compute credits generating thousands of traces to find a few valid ones. Check the "acceptance rate" immediately upon starting.
- **Design tradeoffs**: Fine-tuning offers higher potential alignment but requires weights and training infrastructure, while codebook refinement is training-free and works on black-box APIs but may yield lower peak performance. The paper selects the first valid trace (Greedy), though a more complex system might score traces by reasoning quality or diversity.
- **Failure signatures**: Low acceptance rate indicates weak generator model or noisy labels; performance degradation on subjective dimensions like engagement suggests the inferred traces lack consensus signal; codebook bloat can occur when the refiner produces overly complex instructions.
- **First 3 experiments**: 1) Establish baseline alignment using original codebook on proprietary model against test set. 2) Run trace generator on small batch to verify acceptance rate and inspect accepted traces for hallucinations. 3) Compare refined vs original codebook on held-out set using proprietary model, looking for ICC improvements.

## Open Questions the Paper Calls Out

- **Trace fidelity**: Do inferred thinking traces faithfully represent human cognitive processes or merely constitute plausible post-hoc rationalizations? The paper notes this validation is beyond scope and suggests comparing against human think-aloud protocols.

- **Human applicability**: Can refined codebooks improve inter-rater reliability among human annotation teams, not just LLMs? The authors hypothesize this transferability but haven't tested it with human annotators.

- **Individual reasoning diversity**: Would modeling individual annotators' reasoning rather than aggregated labels enrich trace diversity and improve model robustness? Current methodology collapses diverse reasoning paths into consensus labels.

- **Human-AI collaboration efficiency**: Is a select-and-validate workflow where humans choose the best candidate trace more efficient than asking humans to write traces from scratch? The paper proposes this as future work without evaluating the trade-off.

## Limitations

- The rejection-sampling mechanism relies on a strong proxy assumption that models reaching correct labels via generated traces approximate human reasoning, which lacks direct human validation
- Efficacy of inferred traces versus human-authored traces remains untested, leaving open whether the improvements come from the reasoning content or just the SFT process
- The approach shows less reliable gains on highly subjective dimensions like engagement, suggesting limitations for tasks lacking objective consensus

## Confidence

- **High**: SFT on reasoning traces improves alignment metrics for objective tasks (complexity, consistency, fluency) - well-supported by quantitative results
- **Medium**: Codebook refinement reduces ambiguity and improves inter-rater reliability across LLMs - results show improvement but subjective tasks are less robust
- **Low**: The proxy assumption that accepted traces approximate human reasoning is plausible but unverified - no direct human validation of trace fidelity provided

## Next Checks

1. **Human validation of inferred traces**: Recruit 3 annotators to rate a sample of 50 inferred traces for "human-likeness" and "task relevance" on a 5-point scale. Compute inter-annotator reliability to assess trace quality beyond label-matching.

2. **Trace comparison experiment**: For one dataset, generate a parallel set of traces using human-authored step-by-step instructions. Fine-tune two separate models (one on inferred traces, one on human-authored) and compare performance on held-out data to isolate the impact of trace origin.

3. **Robustness stress test**: Systematically introduce label noise (10%, 20%, 30%) into the seed dataset and rerun the full pipeline. Measure degradation in SFT performance and acceptance rate to quantify sensitivity to seed data quality.