---
ver: rpa2
title: 'LTD-Bench: Evaluating Large Language Models by Letting Them Draw'
arxiv_id: '2511.02347'
source_url: https://arxiv.org/abs/2511.02347
tags:
- spatial
- ltd-bench
- evaluation
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LTD-Bench, a novel benchmark designed to evaluate
  large language models' spatial reasoning capabilities by requiring them to generate
  drawings through dot matrices or executable code. Traditional LLM evaluation paradigms
  rely on abstract numerical metrics that obscure fundamental limitations in spatial
  reasoning, creating a disconnect between reported performance and practical abilities.
---

# LTD-Bench: Evaluating Large Language Models by Letting Them Draw

## Quick Facts
- arXiv ID: 2511.02347
- Source URL: https://arxiv.org/abs/2511.02347
- Authors: Liuhao Lin; Ke Li; Zihan Xu; Yuchen Shi; Yulei Qin; Yan Zhang; Xing Sun; Rongrong Ji
- Reference count: 40
- Key outcome: Novel benchmark reveals LLMs struggle with spatial reasoning despite high scores on traditional benchmarks, with Deepseek-r1 achieving only 70% average accuracy versus 30% for other top models

## Executive Summary
LTD-Bench introduces a novel evaluation paradigm that transforms LLM spatial reasoning assessment from abstract numerical metrics to directly observable visual outputs. By requiring models to generate drawings through dot matrices or executable code, the benchmark exposes profound limitations in language-spatial mapping that traditional benchmarks obscure. Even state-of-the-art models demonstrate significant deficits in establishing bidirectional mappings between language and spatial concepts, with performance gaps ranging from 25% to 70% accuracy across different models and task types.

## Method Summary
LTD-Bench evaluates spatial perception and imagination through 183 samples across three difficulty levels: Easy (discrete dot matrices for character representation), Normal (continuous curve-based Python code), and Hard (complex real-world objects). The benchmark uses complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception). Evaluation employs both human assessment and GPT-4.1-based automated scoring (0.0-1.0 scale) for generation tasks, while recognition uses exact match accuracy. The methodology emphasizes temperature=0 settings and 5-run averaging for stability.

## Key Results
- Only Deepseek-r1 achieves average accuracy above 70%, while Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct score around 30%
- Models show asymmetric performance: enhanced reasoning improves recognition but not generation tasks
- Multimodal LLMs do not show clear advantages on text-based spatial tasks
- Visual outputs enable diagnostic analysis revealing stylistic similarities among model families
- Common failure modes include mirror/reversal errors, upside-down rendering, and chaotic line outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual outputs reveal spatial reasoning limitations that numerical metrics conceal
- **Mechanism:** By requiring models to produce renderable artifacts (dot matrices or executable code), the benchmark transforms abstract capabilities into inspectable visual evidence
- **Core assumption:** Human or automated visual inspection can detect spatial reasoning failures that accuracy scores obscure
- **Evidence anchors:** [abstract] "transforms abstract performance metrics into directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code"
- **Break condition:** If visual outputs can be gamed without genuine spatial understanding (e.g., memorized patterns), diagnostic value degrades

### Mechanism 2
- **Claim:** Bidirectional evaluation (generation + recognition) exposes asymmetric spatial capabilities
- **Mechanism:** Generation tasks test language→spatial mapping (imagination); recognition tasks test spatial→language mapping (perception). Deficits in either direction indicate incomplete spatial cognition
- **Core assumption:** Robust spatial reasoning requires competence in both directions
- **Evidence anchors:** [abstract] "complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception)"
- **Break condition:** If models achieve high scores through surface-level pattern matching rather than genuine spatial representation, the diagnostic power weakens

### Mechanism 3
- **Claim:** Progressive complexity identifies precise capability thresholds
- **Mechanism:** Three difficulty levels (discrete grid → continuous curves → real-world objects) create a diagnostic ladder; failure patterns at each level reveal specific deficits
- **Core assumption:** Spatial reasoning decomposes into hierarchical skills that can be isolated
- **Evidence anchors:** [section 3.1] Solution 3: "LTD-Bench implements a hierarchical structure with three difficulty levels"
- **Break condition:** If complexity jumps are too large, the benchmark may fail to pinpoint exact failure modes

## Foundational Learning

- **Concept: Language-Spatial Mapping (Bidirectional)**
  - Why needed here: The paper's central finding is that LLMs struggle to establish reliable mappings between linguistic symbols and spatial entities
  - Quick check question: Why might a model correctly identify a drawn letter yet fail to generate the same letter from a text description?

- **Concept: Spatial Imagination vs. Perception**
  - Why needed here: The benchmark explicitly separates these; models show asymmetric performance (Section 4.2)
  - Quick check question: Design a test that would fail a model with strong perception but weak imagination—what would the task structure look like?

- **Concept: Code-as-Spatial-Representation**
  - Why needed here: Normal and Hard levels require generating executable Python code; models must translate spatial concepts into programmatic curve compositions
  - Quick check question: What spatial reasoning step is required when converting "draw a W" into coordinate-based line commands?

## Architecture Onboarding

- **Component map:** Text prompts → LLM → dot matrix (Easy) OR Python code (Normal/Hard) → Image renderer → Evaluation (human/GPT-4.1)
- **Critical path:** 1. Prompt template design (enforce constraints: no TextPrint, curve-only) → 2. Model inference with temperature=0 → 3. Output parsing and rendering (code execution must succeed) → 4. Evaluation (5-run GPT-4.1 averaging for stability)
- **Design tradeoffs:** Dot matrix vs. code output: Discrete grids are easier to evaluate but limited in expressiveness; code allows complex shapes but introduces execution failures
- **Failure signatures:** Mirror/reversal errors (confusing '>' with '<', 'J' with 'L')—indicates weak orientation understanding; upside-down rendering—suggests absent global spatial frame; chaotic line outputs—signals disconnect between code generation and spatial visualization; simple math failures in spatial context (9:30 clock)—reveals context-dependent reasoning fragility
- **First 3 experiments:** 1. Run Easy-level generation across target models; visualize outputs to identify common failure modes (mirror errors, proportion issues) → 2. Compare recognition vs. generation accuracy within models to test bidirectional deficit hypothesis (replicate Table 4 analysis) → 3. Execute Hard-level tasks and conduct stylistic similarity analysis across model families (extend Table 6 approach)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do current multimodal LLMs fail to consistently outperform text-only models on text-based spatial reasoning tasks?
- **Basis in paper:** [explicit] Section 4.2 notes that results "challenge expectations based on human cognition" and explicitly calls for "further research on aligning visual and textual features in multimodal learning."
- **Why unresolved:** The paper identifies the performance gap but does not isolate whether the failure stems from misaligned training objectives or the specific inability to transfer visual priors to symbolic text outputs.
- **What evidence would resolve it:** Ablation studies correlating specific visual pre-training data compositions with performance gains on text-only spatial generation tasks.

### Open Question 2
- **Question:** Why does enhanced reasoning capability improve spatial recognition but fail to benefit spatial generation tasks?
- **Basis in paper:** [inferred] Section 4.2 hypothesizes that "generation tasks, which rely more on spatial imagination, are less amenable" to deep reasoning improvements, but offers no mechanistic proof.
- **Why unresolved:** The paper observes the asymmetry (Deepseek-r1 excelling at recognition but not generation) but leaves the underlying cognitive or computational bottleneck undefined.
- **What evidence would resolve it:** Analysis of intermediate attention states comparing how reasoning tokens influence output token probability in generation versus recognition contexts.

### Open Question 3
- **Question:** Can visual stylistic similarity serve as a rigorous, quantitative metric for assessing model similarity?
- **Basis in paper:** [explicit] Section 5 states the current model similarity analysis is "preliminary" and that "More systematic and quantitative approaches are needed to rigorously assess similarities."
- **Why unresolved:** The paper demonstrates a correlation only within a single model family (Qwen) using a small sample size, leaving the generalizability of this diagnostic method unproven.
- **What evidence would resolve it:** A large-scale study quantifying the correlation between model architecture/lineage and visual feature clusters in generated drawings.

## Limitations
- **Benchmark Coverage:** 183-sample dataset with heavy skew toward character representation may limit generalizability to practical spatial reasoning scenarios
- **Evaluation Reliability:** GPT-4.1 evaluation shows variance even at temperature=0, and human evaluation protocol details are unclear
- **Model API Dependence:** Results rely on specific model versions and configurations that may be difficult to reproduce exactly

## Confidence
- **High Confidence:** Core finding that LLMs struggle with bidirectional language-spatial mapping is well-supported by systematic performance gaps across all models
- **Medium Confidence:** Claim that multimodal LLMs show no clear advantage on text-based spatial tasks is supported but limited by benchmark's exclusive use of text prompts
- **Low Confidence:** Stylistic similarity analysis suggesting "family resemblances" among model outputs is intriguing but under-validated with unclear methodology

## Next Checks
1. **Expand Dataset Diversity:** Replicate experiments with a larger, more balanced dataset that includes more real-world objects and 3D spatial reasoning tasks to test the benchmark's generalizability
2. **Validate Evaluation Protocol:** Conduct ablation studies testing how evaluation variance changes with different GPT-4.1 aggregation methods (median vs. mean) and human evaluation protocols to establish reliability bounds
3. **Test Multimodal Extension:** Modify the benchmark to include visual input tasks (e.g., describe a provided image) to properly assess whether multimodal LLMs show advantages in spatial reasoning beyond text-based generation