---
ver: rpa2
title: 'RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams'
arxiv_id: '2507.19666'
source_url: https://arxiv.org/abs/2507.19666
tags:
- question
- o4-mini
- image
- correct
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoD-TAL, a novel multimodal dataset for Romanian
  driving license exams that includes text and image-based questions with annotated
  legal references. The authors evaluate large language models (LLMs) and vision-language
  models (VLMs) on information retrieval, question answering, visual information retrieval,
  and visual question answering tasks using retrieval-augmented generation pipelines.
---

# RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams

## Quick Facts
- arXiv ID: 2507.19666
- Source URL: https://arxiv.org/abs/2507.19666
- Reference count: 40
- Primary result: Domain-specific fine-tuning improves retrieval performance; chain-of-thought prompting and reasoning models boost QA accuracy beyond exam passing thresholds.

## Executive Summary
This paper introduces RoD-TAL, a novel multimodal dataset for Romanian driving license exams that includes text and image-based questions with annotated legal references. The authors evaluate large language models (LLMs) and vision-language models (VLMs) on information retrieval, question answering, visual information retrieval, and visual question answering tasks using retrieval-augmented generation pipelines. They find that domain-specific fine-tuning significantly improves retrieval performance, while chain-of-thought prompting and reasoning-optimized models enhance QA accuracy beyond exam passing thresholds. However, visual reasoning remains challenging, highlighting limitations in applying LLMs and VLMs to legal education in under-resourced languages.

## Method Summary
The authors construct a benchmark for Romanian driving license exams with 1,156 MCQA questions across four splits (text with/without legal refs, image with/without refs). They use a retrieval-augmented generation (RAG) pipeline with a fine-tuned mE5small dense retriever for information retrieval, feeding retrieved legal documents and traffic sign metadata into LLMs (o4-mini, GPT-4o-mini, Mistral, Gemma) for question answering. For visual tasks, they generate image captions via o4-mini and reformulate queries to combine image and text. The retriever is fine-tuned on question-document pairs using InfoNCE loss with positive-aware hard-negative mining, while LLMs use chain-of-thought prompting with explicit reasoning rules.

## Key Results
- Domain-specific fine-tuning of the retriever improves Recall@10 by 18% for question answering.
- RAG improves QA accuracy by 11% and chain-of-thought prompting adds another 9% improvement.
- Reasoning-optimized models (o4-mini) outperform standard models by 19% in QA.
- Visual reasoning tasks remain challenging, with VQA accuracy around 75-90% depending on approach.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Dense Retriever Fine-Tuning
By training a dense retriever (mE5small) on in-domain positive pairs (questions + their correct legal article references) and hard negatives, the model learns to prioritize legal relevance over surface semantic similarity. This domain adaptation is particularly impactful for low-resource languages like Romanian.

### Mechanism 2: Retrieval-Augmented Generation (RAG) for Factual Grounding
Providing retrieved legal documents to an LLM as context reduces hallucination and improves question-answering accuracy by forcing answers to be grounded in specific legal text.

### Mechanism 3: Chain-of-Thought (CoT) Prompting for Reasoning
Prompting the model to reason step-by-step before providing a final answer improves accuracy, particularly for nuanced legal questions.

## Foundational Learning

- **Dense Retrieval & Embeddings:**
  - Why needed here: A RAG system's first step is to find the right documents. Standard semantic search often fails on legal text.
  - Quick check question: What is the fundamental difference between keyword-based search (BM25) and dense retrieval, and why would fine-tuning an embedding model like mE5small be beneficial for a low-resource legal domain?

- **Prompt Engineering for Reasoning:**
  - Why needed here: An LLM's output is highly sensitive to prompt design. This paper demonstrates how specific prompts that enforce logical rules can mitigate systematic biases.
  - Quick check question: The paper identifies "safety bias" as a failure mode. How does the proposed "enhanced prompt" (Strategy 4) attempt to counteract this specific bias?

- **The RAG Architecture:**
  - Why needed here: This is the core system. A new engineer must understand how retrieval and generation components are coupled, how the context window is managed, and what the expected data flow is.
  - Quick check question: In the RoD-TAL architecture, what is the input to the dense retriever for the Question Answering (QA) task, and what does the retriever return to the LLM?

## Architecture Onboarding

- **Component Map:** Document Stores (Laws Vector Store, Signs Vector Store) -> Fine-tuned mE5small Retriever -> LLM (o4-mini) with RAG context -> Answer
- **Critical Path:** The most critical and highest-impact path is the **Information Retrieval (IR) component**. If retrieval fails, the entire downstream generation is compromised.
- **Design Tradeoffs:**
  - **Real vs. Synthetic Training Data:** The paper fine-tuned a retriever on both real question-law pairs and synthetic data. The synthetic model was much worse.
  - **Caption vs. Image Input:** For VQA, feeding the image directly to the model works better than relying on a pre-generated caption.
  - **General vs. Reasoning Model:** Reasoning models (o4-mini) perform best but likely have higher latency and cost.
- **Failure Signatures:**
  1. **Safety Bias:** The model selects the "safer" answer that contradicts the strict letter of the law.
  2. **Overthinking:** The model extrapolates beyond the provided facts or legal text.
  3. **Context Bloat:** Adding too many documents (e.g., k=40) degrades performance.
- **First 3 Experiments:**
  1. Baseline RAG vs. No-RAG Ablation: Measure accuracy drop when removing RAG component.
  2. Fine-Tuned Retriever Evaluation: Compare Recall@10 against baseline mE5small.
  3. Prompt Strategy Comparison: Compare zero-shot, simple CoT, and enhanced prompt on same test set.

## Open Questions the Paper Calls Out

### Open Question 1
Can preventing actual positives from appearing during hard-negative mining further improve retrieval performance for legal document embeddings in low-resource languages?

### Open Question 2
Can joint image–text embeddings outperform the current modular caption-based approach for Visual Information Retrieval in legal driving contexts?

### Open Question 3
Can targeted model fine-tuning reduce the "Safety Bias" and "Overthinking" failure modes observed in LLM legal reasoning?

### Open Question 4
Why does adding image captions to visual task queries not improve—and sometimes degrade—retrieval and QA performance?

## Limitations

- The RoD-TAL benchmark is built for Romanian, a low-resource language, and its generalizability to other languages or broader legal domains is unknown.
- Performance improvements are tied to specific models (o4-mini for reasoning, mE5small for retrieval) and may not transfer to other architectures.
- Synthetic data augmentation underperforms real data for retriever fine-tuning, limiting scalability in truly low-resource settings.
- Visual reasoning tasks remain challenging, with accuracy lagging behind text-only tasks.

## Confidence

**High Confidence**: The positive impact of domain-specific retriever fine-tuning on IR performance, and the value of retrieval-augmented generation for reducing hallucination and improving accuracy.

**Medium Confidence**: The effectiveness of chain-of-thought prompting and reasoning-optimized models (e.g., o4-mini) for legal reasoning tasks, as results are robust across multiple tasks but model-specific.

**Low Confidence**: Generalizability to other low-resource languages or legal systems, and the scalability of current approaches using synthetic data.

## Next Checks

1. **Cross-Lingual Transfer Test**: Evaluate the RoD-TAL pipeline on a similar driving exam dataset in another low-resource language (e.g., Bulgarian or Serbian) to assess generalization beyond Romanian.

2. **Synthetic Data Scaling Study**: Systematically compare real vs. synthetic data for retriever fine-tuning as the size of the synthetic corpus increases, to identify the point of diminishing returns.

3. **Visual Reasoning Ablation**: Isolate and test the contribution of image captions vs. direct image input (using VLMs) for VQA, and analyze failure modes for different types of visual questions (e.g., signs vs. situational images).