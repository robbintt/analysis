---
ver: rpa2
title: 'Since Faithfulness Fails: The Performance Limits of Neural Causal Discovery'
arxiv_id: '2502.16056'
source_url: https://arxiv.org/abs/2502.16056
tags:
- causal
- discovery
- neural
- performance
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fundamental limitations of neural causal
  discovery methods, revealing that they struggle to reliably distinguish causal relationships
  in finite-sample regimes due to approximation and estimation errors. The authors
  identify the faithfulness property as a critical bottleneck, showing that its violation
  directly undermines performance and that its proportion decreases exponentially
  with graph size and density.
---

# Since Faithfulness Fails: The Performance Limits of Neural Causal Discovery

## Quick Facts
- arXiv ID: 2502.16056
- Source URL: https://arxiv.org/abs/2502.16056
- Reference count: 40
- One-line primary result: Neural causal discovery methods struggle to reliably distinguish causal relationships in finite-sample regimes due to approximation and estimation errors, with faithfulness violations as the critical bottleneck.

## Executive Summary
This paper investigates the fundamental limitations of neural causal discovery methods, revealing that they struggle to reliably distinguish causal relationships in finite-sample regimes due to approximation and estimation errors. The authors identify the faithfulness property as a critical bottleneck, showing that its violation directly undermines performance and that its proportion decreases exponentially with graph size and density. Through controlled experiments, they demonstrate that neural networks lack the precision needed to recover ground-truth graphs even for small graphs and large sample sizes.

## Method Summary
The authors benchmark four neural causal discovery methods (DCDI, BayesDAG, DiBS, and SDCD) on synthetic Erdős-Rényi graphs with additive Gaussian noise and MLP functional relationships. They introduce the λ-strong faithfulness property as a measure of dataset difficulty and evaluate performance using ESHDCPDAG and F1-ScoreCPDAG metrics. The methods are trained with specified hyperparameters including ensemble sizes (N=3-29), regularization parameters, and learning rates. Performance is evaluated across different graph sizes (5, 10, 30 nodes) and sample sizes (20-8000).

## Key Results
- Neural networks cannot reliably distinguish between existing and non-existing causal relationships in finite sample regimes
- λ-strong faithfulness proportion decreases exponentially with graph size and density
- Performance plateaus despite increasing sample size, indicating fundamental scalability limits
- Neural methods show poor scalability and insufficient accuracy for real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: λ-strong faithfulness threshold predicts causal discovery difficulty
- Mechanism: The λ parameter captures the minimum detectable conditional correlation magnitude. Lower λ means weaker edges become indistinguishable from noise. The paper approximates λ as the threshold maximizing F1-score for classifying d-separated vs. d-connected pairs using partial Spearman correlation.
- Core assumption: Partial Spearman correlation serves as a sufficient statistic for detecting conditional dependencies in nonlinear additive noise models.
- Evidence anchors:
  - [abstract] "We identify the faithfulness property as a critical bottleneck: (i) it is likely to be violated across any reasonable dataset size range, and (ii) its violation directly undermines the performance of neural discovery methods."
  - [section 3.1] Defines ˆλ as threshold optimizing F1-score for d-separation classification
  - [corpus] dcFCI paper addresses discovery under unfaithfulness but uses different algorithmic approach; limited direct validation of λ as difficulty measure
- Break condition: When graph density creates overlapping weak edges that cannot be separated by any threshold (Figure 1b shows rapid decay in λ-faithful fraction)

### Mechanism 2
- Claim: Neural function approximators introduce systematic score errors in finite samples
- Mechanism: Neural networks trained on finite data produce likelihood estimates with irreducible variance. For causal discovery, this manifests as non-ground-truth graphs receiving statistically indistinguishable or superior scores compared to true structures. Bootstrap ensembling reveals confidence intervals that overlap across many DAGs.
- Core assumption: The ensemble approach (N=3-29 networks) adequately characterizes the distribution of approximation errors.
- Evidence anchors:
  - [abstract] "neural networks cannot reliably distinguish between existing and non-existing causal relationships in the finite sample regime"
  - [section 3.3] Case study shows 11-398 structures with overlapping or superior scores to target in 5-node graphs
  - [corpus] Corpus papers focus on algorithmic robustness rather than neural approximation error characterization
- Break condition: When sample size reaches threshold where bootstrap variance dominates signal (Table 2: even at 8000 samples, max 11 structures with significantly better scores)

### Mechanism 3
- Claim: Problem difficulty scales exponentially with graph size and density
- Mechanism: As graph size increases, the number of conditional independence tests grows combinatorially while the fraction of λ-strong faithful distributions decreases exponentially. This double pressure—more tests to pass with fewer faithful distributions—creates fundamental scalability limits.
- Core assumption: Erdős-Rényi graphs with MLP functional relationships adequately represent the distribution of real-world causal structures.
- Evidence anchors:
  - [abstract] "its proportion decreases exponentially with graph size and density"
  - [section 3.1, Figure 1] Shows exponential decay in λ-faithful fraction for ER graphs with nonlinear functions
  - [corpus] Max-Linear Bayesian Networks paper notes similar faithfulness violations for heavy-tailed distributions
- Break condition: Beyond ~10 nodes with expected degree 2, all tested methods show ESHD > 15 edges incorrectly predicted (Table 1)

## Foundational Learning

- Concept: **d-separation and faithfulness**
  - Why needed here: The entire analysis hinges on understanding when conditional independence in data corresponds to graphical separation. Without this, the λ metric makes no sense.
  - Quick check question: Given path A → C ← B, what happens to dependence between A and B when conditioning on C vs. not conditioning?

- Concept: **Markov Equivalence Classes (MECs) and CPDAGs**
  - Why needed here: Performance metrics (ESHDCPDAG, F1-ScoreCPDAG) evaluate recovery up to equivalence class, not exact DAG. Misunderstanding leads to incorrect interpretation of "good" performance.
  - Quick check question: Why can two different DAGs represent the same set of conditional independencies? What structure do they share?

- Concept: **Estimation error vs. approximation error in neural networks**
  - Why needed here: The paper's central claim is that estimation error (finite sample variance) dominates, not approximation error (network capacity). Experiments with matched architectures test this.
  - Quick check question: If you double the training data but halve the network size, which error type changes more?

## Architecture Onboarding

- Component map:
  - Score function: Neural likelihood estimator per node (MLP taking parents → child density)
  - Structure search: Continuous optimization with acyclicity constraint (varies by method: DCDI uses augmented Lagrangian, DiBS uses posterior sampling)
  - Regularization: Sparsity penalty (λ_sparse) + acyclicity penalty (γ)
  - Evaluation: ESHDCPDAG over posterior samples for Bayesian methods, single graph for non-Bayesian

- Critical path:
  1. Generate ER graph → sample SCM with MLP functions
  2. Compute ˆλ via partial Spearman correlation threshold optimization
  3. Train neural discovery method with hyperparameter grid search
  4. Extract graph(s) and compute ESHDCPDAG against ground truth CPDAG
  5. Correlate performance with ˆλ across dataset distribution

- Design tradeoffs:
  - Ensemble size (N) vs. computational cost: Paper uses N=3-29; higher N reduces variance but scales linearly
  - Architecture capacity: Experiments show [4,4] MLP often outperforms larger networks (overfitting concern in small data regime)
  - Regularization strength: λ_sparse=500 for BayesDAG, γ=0.3 for NN-opt; too high kills true edges, too low admits spurious

- Failure signatures:
  - ESHDCPDAG plateaus despite increasing sample size (Figure 4b: DiBS flattens after ~800 samples)
  - Bootstrap confidence intervals overlapping for many graphs (Figure 3: red target graph not distinguished from green/blue competitors)
  - Consistently high F1-ScoreCPDAG but poor ESHDCPDAG (correct edges but wrong orientations—within MEC)

- First 3 experiments:
  1. **Reproduce λ-faithfulness decay**: Generate 30 ER(6,k) graphs with k∈{1,1.5,2}, compute ˆλ for each, verify exponential decay pattern matches Figure 1b
  2. **Score separation test**: On 5-node graph with N=29 bootstrap networks, confirm that target structure has overlapping CI with non-MEC competitors (replicate Figure 3 case study)
  3. **Sample size ablation**: Run DCDI and DiBS on ER(10,2) with n∈{80,250,800,2500}, verify plateau behavior and compute correlation between method performance and dataset ˆλ

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the use of interventional data effectively circumvent the faithfulness bottleneck that limits neural causal discovery performance in observational settings?
- Basis in paper: [explicit] The authors note that interventional data may replace the need for the faithfulness assumption and state that evaluating benchmarked methods on interventional datasets is a "valuable extension of our research."
- Why unresolved: The current study restricted its evaluation to observational data, where violations of λ-strong faithfulness directly undermine accuracy.
- What evidence would resolve it: A benchmark study showing that the tested methods (DCDI, DiBS, etc.) achieve significantly higher accuracy on interventional datasets compared to observational ones, independent of λ-strong faithfulness levels.

### Open Question 2
- Question: Do alternative function approximators exist that offer superior precision to neural networks for causal discovery in finite-sample regimes?
- Basis in paper: [explicit] The paper concludes that progress is constrained within the current neural paradigm and suggests it "will be however interesting to investigate other classes of aproximators."
- Why unresolved: The authors demonstrate that neural networks suffer from unavoidable approximation and estimation errors in finite samples, causing them to fail at distinguishing causal relationships.
- What evidence would resolve it: Comparative experiments demonstrating that non-neural approximators (e.g., symbolic regression) can recover ground-truth graphs with lower sample complexity or higher accuracy than MLPs on the same synthetic benchmarks.

### Open Question 3
- Question: Do real-world causal distributions adhere to λ-strong faithfulness despite the theoretical exponential decay of this property with graph size?
- Basis in paper: [explicit] The authors acknowledge that while theory suggests faithful distributions vanish exponentially, "It may hold... that real-world distributions adhere to λ-strong faithfulness despite large sizes of the graph."
- Why unresolved: The results rely on synthetic Erdos-Renyi graphs, which may not reflect the structure of physical or biological systems where these methods are intended to be applied.
- What evidence would resolve it: An analysis of λ-strong faithfulness metrics on physical simulation testbeds (e.g., "Causal chambers") or real-world biological datasets to see if they exhibit higher λ values than random synthetic graphs.

## Limitations
- Limited validation of λ metric beyond dcFCI paper with different algorithmic approach
- Neural approximation error characterization depends on ensemble size uncertainty
- Results based on synthetic Erdős-Rényi graphs that may not reflect real-world causal structures

## Confidence

- **High**: Neural networks cannot reliably distinguish causal relationships in finite samples (supported by bootstrap analysis showing overlapping scores)
- **Medium**: λ-strong faithfulness violations exponentially decrease with graph size (empirical pattern observed but theoretical justification limited)
- **Medium**: Neural function approximation introduces systematic score errors (bootstrap methodology sound but ensemble size uncertainty remains)

## Next Checks

1. **External faithfulness validation**: Apply λ metric to real-world datasets (e.g., Sachs et al. flow cytometry data) and compare against algorithmic difficulty measures from dcFCI and other methods.

2. **Ensemble sensitivity analysis**: Systematically vary N from 1 to 50 for 5-node graphs and measure stability of ESHDCPDAG rankings and confidence intervals.

3. **Functional relationship generalization**: Replace MLP with polynomial, ReLU, and sigmoid functions across different graph sizes to test whether faithfulness violations are function-dependent rather than universal.