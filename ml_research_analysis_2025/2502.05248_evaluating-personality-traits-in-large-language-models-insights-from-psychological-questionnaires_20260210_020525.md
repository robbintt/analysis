---
ver: rpa2
title: 'Evaluating Personality Traits in Large Language Models: Insights from Psychological
  Questionnaires'
arxiv_id: '2502.05248'
source_url: https://arxiv.org/abs/2502.05248
tags:
- personality
- llms
- traits
- across
- questionnaires
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates personality traits in Large
  Language Models (LLMs) using psychological questionnaires to understand behavioral
  patterns. To address potential training data contamination, questionnaires were
  restructured and validated, with responses gathered across multiple iterations.
---

# Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires

## Quick Facts
- arXiv ID: 2502.05248
- Source URL: https://arxiv.org/abs/2502.05248
- Authors: Pranav Bhandari; Usman Naseem; Amitava Datta; Nicolas Fay; Mehwish Nasim
- Reference count: 18
- Primary result: LLMs exhibit distinct personality profiles with higher Agreeableness, Openness, and Conscientiousness scores

## Executive Summary
This study systematically evaluates personality traits in Large Language Models using psychological questionnaires while addressing training data contamination. By restructuring questionnaire items using GPT-4o and validating semantic similarity, the authors examine dimensional variability and dominance across five Big Five personality inventories. Results show that LLMs demonstrate distinct personality profiles, with GPT-4 models emphasizing Agreeableness and Llama models showing dominance in Conscientiousness or Openness. The study reveals that Neuroticism exhibits the highest variability across models, highlighting challenges in both definition and evaluation of this dimension.

## Method Summary
The study administers five Big Five personality questionnaires (BFI, HEXACO, TIPI, MINI-IPIP, NEO-PI-R) to multiple LLMs after restructuring items to reduce training data contamination. Original questions are reworded using GPT-4o, then validated with sentence-transformer models to ensure semantic similarity (≥0.7 threshold). Questionnaires are administered in batches of 10 items with randomized order, using minimum temperature settings (0 for OpenAI, 0.01 for Llama) across 100 iterations per model. Scores are computed per dimension, and Coefficient of Variation (CV) measures variability. Dimensional dominance is identified as the highest mean score per model.

## Key Results
- LLMs show higher scores in Agreeableness, Openness, and Conscientiousness compared to Extraversion and Neuroticism
- Neuroticism demonstrates the highest variability across models (CV up to 33.69%), while Extraversion and Agreeableness are more consistent
- Dimensional dominance varies by model: GPT-4 emphasizes Agreeableness, while Llama models show dominance in Conscientiousness (Llama-3) or Openness (Llama-3.1)
- Findings reflect variations in fine-tuning objectives and design goals across model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Restructured questionnaires reduce training data contamination effects while preserving construct validity.
- Mechanism: Questions are reworded using GPT-4o to alter surface structure while retaining semantic intent. Sentence-transformer models compute cosine similarity; only items below 0.7 threshold proceed. This breaks memorized associations between known test items and their expected responses, forcing models to process each question semantically rather than retrieving cached completions.
- Core assumption: LLMs encode surface-level patterns of well-known psychological instruments during pre-training, and altering phrasing disrupts pattern-matching without changing the underlying construct being measured.
- Evidence anchors: [abstract] "by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs"; [section 2.1] "each question in the questionnaire was reworded to contain a different structure while the purpose of the question was intact... A threshold of 0.7, indicating moderate to high similarity, was applied"; [corpus] Related work (arXiv:2509.10078) raises concerns about applying human-designed questionnaires directly to LLMs.

### Mechanism 2
- Claim: Low-temperature sampling produces stable, repeatable personality profiles across iterations.
- Mechanism: Setting temperature to near-zero (0 for OpenAI, 0.01 for Llama) reduces token selection stochasticity, making the model's probability distribution output deterministic. This allows 100-iteration reliability testing to reflect true model倾向 rather than sampling noise.
- Core assumption: Personality-like responses in LLMs are encoded as stable probability distributions over tokens, not artifacts of high-temperature exploration.
- Evidence anchors: [section 2.1] "temperature parameters were set to minimum values: 0 for OpenAI models and 0.01 for Llama models to ensure consistent and deterministic scores across multiple runs"; [section 2.2] "Across all models, 100 iterations were conducted using a minimum temperature setting"; [corpus] PTCBENCH (arXiv:2602.00016) examines contextual stability of personality traits.

### Mechanism 3
- Claim: Fine-tuning and alignment strategies shape dimensional dominance patterns differently across model families.
- Mechanism: RLHF and safety alignment embed cooperative, helpful behaviors that manifest as elevated Agreeableness. Open-source models with different fine-tuning objectives show alternative dominance (Llama-3: Conscientiousness; Llama-3.1: Openness). Training data composition and alignment reward functions act as implicit personality shaping mechanisms.
- Core assumption: Observed trait scores reflect training/alignment artifacts rather than emergent personality in a human psychological sense.
- Evidence anchors: [section 3.3] "GPT-4 models emphasise Agreeableness, while Llama models highlight Conscientiousness or Openness, reflecting variations in fine-tuning objectives and design goals"; [section 1] "we argue that even if they lack behaviour in a true psychological sense, their outputs may still reflect psychological traits"; [corpus] MindShift (arXiv:2512.09149) investigates LLMs absorbing personality traits via prompts.

## Foundational Learning

- Concept: **Big Five Personality Model (OCEAN)**
  - Why needed here: All five questionnaires in this study measure Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Understanding what each dimension captures is prerequisite to interpreting results.
  - Quick check question: Which dimension would elevated scores on "I enjoy trying new and different things" most strongly indicate?

- Concept: **Coefficient of Variation (CV)**
  - Why needed here: The paper uses CV = (SD/Mean) × 100% to compare variability across dimensions with different scales. High CV indicates inconsistent responses; low CV indicates stability.
  - Quick check question: If Neuroticism has mean=3.0 and SD=0.6, while Extraversion has mean=4.0 and SD=0.4, which shows higher relative variability?

- Concept: **Training Data Contamination**
  - Why needed here: LLMs trained on internet corpora may have memorized psychological test items. This creates validity threats when administering those same tests.
  - Quick check question: Why might a model score differently on "I see myself as disorganized" versus a restructured version "I would describe myself as lacking systematic organization"?

## Architecture Onboarding

- Component map: Questionnaire Bank -> Restructuring Pipeline -> Prompt Engineering -> Execution Layer -> Scoring Module -> Analysis Layer
- Critical path:
  1. Restructure questionnaire items (similarity < 0.7 threshold)
  2. Randomize item order within batches of 10
  3. Execute prompts at minimum temperature (100 iterations)
  4. Extract and validate numeric responses
  5. Apply scoring rules per instrument
  6. Compute CV and identify dominant dimension

- Design tradeoffs:
  - Brevity vs. depth: TIPI (10 items) is fast but lower reliability; HEXACO (100 items) offers more depth but higher cost and potential context-length issues
  - Determinism vs. exploration: Minimum temperature ensures repeatability but may not capture behavioral range
  - Standardized vs. restructured items: Restructuring reduces contamination but introduces semantic drift risk

- Failure signatures:
  - Malformed responses (non-numeric, missing indices) → prompt format failure
  - CV > 30% on stable traits → temperature misconfiguration or model instability
  - Identical scores across all models → questionnaire items not processed, default outputs returned
  - High similarity scores (> 0.9) on restructured items → insufficient rewording

- First 3 experiments:
  1. Baseline replication: Administer original (unrestructured) BFI to GPT-4 and Llama-3 at temperature=0 for 10 iterations. Record scores and CV per dimension.
  2. Contamination test: Compare original vs. restructured BFI scores. If difference < 5% across dimensions, contamination may be minimal or restructuring ineffective.
  3. Temperature sensitivity: Run restructured TIPI at temperature ∈ {0, 0.5, 1.0} on single model. Plot CV vs. temperature to validate low-temperature stability assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the dimension of Neuroticism exhibit significantly higher variability compared to other traits across different LLMs?
- Basis in paper: [explicit] The results section notes that "Neuroticism demonstrates the highest variability across the majority of the models," with a coefficient of variation reaching 33.69%, highlighting "challenges in both the definition and evaluation of Neuroticism."
- Why unresolved: The authors identify the instability of this dimension but do not isolate the root cause, leaving open whether the variance is due to training data noise, model architecture, or the inability of current inventories to measure emotional stability in non-sentient agents.
- What evidence would resolve it: A comparative analysis using alternative anxiety and stress-measurement tools specifically adapted for computational agents to see if the variability persists across different lexical frameworks.

### Open Question 2
- Question: How can psychological questionnaire design be refined to reduce uncertainty and enhance validity when assessing LLMs?
- Basis in paper: [explicit] The conclusion states that the uncertain results in Neuroticism "underscore the need for careful questionnaire design to enhance test validity."
- Why unresolved: While the study restructures questions to avoid contamination, it relies on human-validated tools (BFI, HEXACO) which may lack construct validity for artificial systems, leading to the observed inconsistencies.
- What evidence would resolve it: Development and validation of a new psychometric standard derived specifically from LLM behavioral outputs rather than adapted from human psychology.

### Open Question 3
- Question: To what extent do specific fine-tuning objectives (e.g., RLHF for safety vs. helpfulness) causally determine the dimensional dominance observed in different model families?
- Basis in paper: [inferred] The discussion mentions that differences in dominance (e.g., GPT-4's Agreeableness vs. Llama's Conscientiousness) "reflect variations in fine-tuning objectives and design goals," but the study compares pre-trained models without controlling for these variables.
- Why unresolved: The study observes the correlation between model type and personality but cannot disentangle the influence of base training data from post-alignment safety procedures.
- What evidence would resolve it: An ablation study administering the same personality inventories to identical base models before and after specific reinforcement learning alignment stages.

## Limitations
- The effectiveness of restructured questionnaires in eliminating training data contamination is assumed but not empirically validated
- Minimum temperature settings ensure stability but may not capture the full behavioral range of LLMs
- The interpretation of trait scores as "personality" is inherently metaphorical, as LLMs lack the psychological substrates that human personality measures assume

## Confidence
- High Confidence: The methodology for administering questionnaires (structured prompts, temperature settings, iteration counts) is clearly specified and reproducible
- Medium Confidence: The dimensional variability findings (Neuroticism showing highest CV) are well-supported by the data, though the interpretation of what this means for "personality" is uncertain
- Low Confidence: The claim that training/alignment strategies shape dimensional dominance is plausible but under-supported—the study shows correlation between model families and dominant traits but doesn't establish causal mechanisms

## Next Checks
1. **Contamination Validation Test**: Administer both original and restructured versions of the same questionnaire to the same models. If score differences exceed 10% on any dimension, the restructuring methodology needs refinement.
2. **Temperature Sensitivity Analysis**: Run the same questionnaire at multiple temperature settings (0.0, 0.5, 1.0) on a single model. Plot CV vs. temperature to empirically verify the stability assumption underlying the minimum-temperature protocol.
3. **Cross-Questionnaire Consistency Check**: Compare dimension scores across all five questionnaires for each model. High inter-instrument correlations would validate the construct measurement; inconsistent patterns would suggest questionnaire-specific artifacts rather than true dimensional differences.