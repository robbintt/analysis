---
ver: rpa2
title: 'Spectral Superposition: A Theory of Feature Geometry'
arxiv_id: '2602.02224'
source_url: https://arxiv.org/abs/2602.02224
tags:
- spectral
- features
- lemma
- feature
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a spectral theory of superposition in neural\
  \ networks, showing that feature geometry\u2014not just individual features\u2014\
  is fundamental to understanding model behavior. The authors develop a framework\
  \ using the frame operator $F = WW^\\top$ to analyze how features allocate representational\
  \ capacity across eigenspaces, revealing that capacity saturation forces spectral\
  \ localization: features collapse onto individual eigenspaces, forming tight frames\
  \ with discrete geometric classification via association schemes."
---

# Spectral Superposition: A Theory of Feature Geometry

## Quick Facts
- arXiv ID: 2602.02224
- Source URL: https://arxiv.org/abs/2602.02224
- Reference count: 40
- Primary result: Spectral theory shows feature geometry, not individual features, is fundamental to understanding neural network superposition

## Executive Summary
This paper introduces a spectral theory of superposition in neural networks, arguing that current interpretability methods miss crucial geometric structure by treating features as independent directions. The authors develop a framework using the frame operator F = WW^⊤ to analyze how features allocate representational capacity across eigenspaces. They prove that when capacity saturates, features must spectrally localize onto individual eigenspaces, forming tight frames that enable precise geometric classification through eigenvalue signatures. The work challenges the standard approach of sparse autoencoders by showing that the relationships between features—captured through their joint geometry—contain essential information about model behavior that is lost when features are analyzed in isolation.

## Method Summary
The authors train toy autoencoder models x' = ReLU(W^⊤ Wx + b) on synthetic sparse inputs with n=1024 features and varying sparsity S ∈ [0.0, 0.99]. They sweep across 3,200 experiments with M ∈ [16, 512] hidden dimensions and two random seeds per configuration. After training, they compute the frame operator F = WW^⊤ and its eigendecomposition, then measure fractional dimensionality D_i = ||W_i||^4 / Σ_j(W_i^⊤ W_j)^2 and spectral localization via weights p_{i,e} = ||P_e W_i||^2 / ||W_i||^2. The key verification involves confirming capacity saturation (Σ_i D_i ≈ m) and testing projective linearity by fitting D_i versus ||W_i||^2 slopes to verify k ≈ 1/λ_e.

## Key Results
- Capacity saturation forces spectral localization: when Σ_i D_i ≈ m, all features collapse onto individual eigenspaces
- Projective linearity verified: feature fractional dimensionality D_i scales linearly with squared norm ||W_i||^2, slope k ≈ 1/λ_e per eigenspace
- Geometric classification possible: eigenvalue signatures enable identification of learned structures (simplices, polygons, antiprisms) through association schemes

## Why This Works (Mechanism)
The spectral theory works because the frame operator F = WW^⊤ captures the global geometry of feature space, and its eigendecomposition reveals how representational capacity is distributed. When models reach capacity saturation, the optimization landscape forces features to localize onto individual eigenspaces to maximize utilization. This localization creates tight frames with specific geometric relationships that can be classified through their eigenvalue signatures. The key insight is that interference between features is not incidental but encodes meaningful structural information about the data relationships being modeled.

## Foundational Learning

1. **Frame Operator F = WW^⊤**
   - Why needed: Captures global geometry of feature space and how features interact through their dot products
   - Quick check: Verify F is symmetric positive semi-definite and compute its trace equals Σ_i ||W_i||^2

2. **Fractional Dimensionality D_i**
   - Why needed: Measures how much representational capacity a feature consumes relative to its orthogonal complement
   - Quick check: Confirm 0 ≤ D_i ≤ ||W_i||^2 and D_i = 0 when W_i = 0

3. **Spectral Localization**
   - Why needed: Determines whether features concentrate on single eigenspaces (meaningful geometry) or spread across multiple (incidental interference)
   - Quick check: Compute max_e p_{i,e} for each feature; values near 1 indicate localization

4. **Capacity Saturation**
   - Why needed: Theoretical condition where Σ_i D_i ≈ m that triggers spectral localization and enables geometric classification
   - Quick check: Verify Σ_i D_i / m approaches 1 as training progresses

## Architecture Onboarding

Component map: Synthetic sparse inputs -> Autoencoder (Wx + b -> ReLU -> W^⊤) -> Frame operator F -> Eigendecomposition -> Spectral measures -> Geometric classification

Critical path: The training process must reach capacity saturation (Σ_i D_i ≈ m) before spectral localization emerges and geometric classification becomes possible. The frame operator computation and eigendecomposition are essential intermediate steps that enable the spectral analysis.

Design tradeoffs: The toy model simplicity (single-layer autoencoder) enables clean mathematical analysis but may not capture the complexity of real neural networks where gradients flow through multiple non-linear layers. The synthetic sparse inputs provide controlled conditions but may not reflect natural data distributions.

Failure signatures: If features don't spectrally localize (Σ_i D_i ≠ m), the model hasn't reached capacity saturation—likely needs longer training or different sparsity regime. Poor linear fit in projective linearity test indicates features aren't properly localized or the model hasn't saturated capacity.

First experiments:
1. Train autoencoder with S=0.5 sparsity and M=128 hidden dimensions, verify convergence and compute initial frame operator
2. Measure spectral localization across different sparsity regimes (S ∈ [0.0, 0.99]) to find where features begin to localize
3. Test projective linearity by plotting D_i versus ||W_i||^2 for features in each eigenspace and fitting linear regression

## Open Questions the Paper Calls Out

1. Does spectral localization occur in real, large-scale neural networks or is it specific to toy autoencoders?
   - Basis: Paper suggests localization from HT-SR phenomenology but hasn't directly measured in real architectures
   - Resolution: Requires measuring per-feature spectral measures μ_i in trained language/vision models

2. Do practical neural networks operate at capacity saturation as required for full geometric recovery?
   - Basis: Paper only empirically verifies saturation in toy models with uniform sparsity
   - Resolution: Computing fractional dimensionalities across layers of trained models

3. Can the spectral framework automatically distinguish structural from incidental interference?
   - Basis: Framework identifies geometry but doesn't indicate semantic meaningfulness
   - Resolution: Demonstrating correlation between recovered geometries and known semantic relationships

4. Do spectral localization results generalize to multi-layer networks with complex gradient paths?
   - Basis: Toy model results rely on gradient mediation through Gram matrix
   - Resolution: Applying framework to transformers/CNNs and testing for localization emergence

## Limitations
- Verification limited to toy autoencoders; real network behavior remains untested
- Methodology for classifying specific geometric structures (simplices, polygons) from spectral signatures not fully specified
- Hyperparameters like learning rate, optimizer, and training duration unspecified, affecting reproducibility
- Cannot automatically distinguish semantically meaningful geometric relationships from compression artifacts

## Confidence

Core spectral localization and capacity saturation claims: **Medium**
- Verified through 3,200 toy model experiments with controlled conditions
- Mathematical proofs for spectral localization under capacity saturation are sound
- Geometric classification methodology lacks complete specification for practical implementation

Geometric structure identification (simplices, polygons, antiprisms): **Low**
- Paper claims eigenvalue signatures enable precise geometric classification
- Methodology for mapping spectral signatures to specific geometric configurations not fully detailed
- Requires additional validation steps beyond basic spectral measure computation

## Next Checks

1. **Capacity saturation verification**: After training, confirm that Σ_i D_i / m approaches 1 across different sparsity regimes to ensure models have reached the saturation regime where spectral localization emerges.

2. **Spectral measure concentration**: Compute max_e p_{i,e} for each feature and verify that features exhibit high concentration (e.g., >95%) on single eigenspaces, distinguishing them from delocalized features that shouldn't be included in geometric classification.

3. **Projective linearity test**: For features that are spectrally localized, plot D_i versus ||W_i||^2 and perform linear regression to verify that k ≈ 1/λ_e holds with high R^2, confirming the geometric relationship between feature dimensionality and spectral weights.