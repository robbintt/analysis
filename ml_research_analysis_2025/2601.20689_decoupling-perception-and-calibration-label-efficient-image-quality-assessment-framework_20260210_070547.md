---
ver: rpa2
title: 'Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment
  Framework'
arxiv_id: '2601.20689'
source_url: https://arxiv.org/abs/2601.20689
tags:
- quality
- image
- uni00000013
- assessment
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEAF proposes a label-efficient IQA framework that decouples perceptual
  quality learning from MOS calibration by distilling MLLM priors into a lightweight
  student. Instead of fine-tuning large MLLMs with extensive MOS labels, LEAF uses
  teacher-provided point-wise judgments and confidence-weighted pair-wise preferences
  for joint distillation, followed by calibration with a small MOS subset.
---

# Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework

## Quick Facts
- arXiv ID: 2601.20689
- Source URL: https://arxiv.org/abs/2601.20689
- Authors: Xinyue Li; Zhichao Zhang; Zhichao Zhang; Shubo Xu; Xiongkuo Min; Yitong Chen; Guangtao Zhai
- Reference count: 20
- Primary result: Achieves SRCC/PLCC of 0.777/0.801 on KonIQ-10k without MOS labels, matching or exceeding state-of-the-art with only 10-30% MOS supervision

## Executive Summary
LEAF introduces a label-efficient Image Quality Assessment (IQA) framework that decouples perceptual quality learning from MOS scale calibration by leveraging multi-modal large language model (MLLM) priors. Instead of expensive fine-tuning of large MLLMs with extensive human-annotated MOS labels, LEAF distills a frozen MLLM teacher's perception into a lightweight student model using a small set of MOS labels. The approach uses teacher-provided point-wise judgments and confidence-weighted pair-wise preferences for joint distillation, followed by calibration with a small MOS subset. Experiments on UGC and AIGC benchmarks demonstrate strong MOS-aligned performance, achieving high SRCC/PLCC scores without any MOS labels and matching or exceeding state-of-the-art results with only 10-30% MOS supervision.

## Method Summary
LEAF proposes a two-stage framework to learn image quality assessment in a label-efficient manner. First, a frozen MLLM teacher (InternVL-3.5-8B) generates point-wise soft scores and pair-wise preferences for a large unlabeled image dataset. These are distilled into a lightweight student model (ConvNeXt-Base) using a joint loss combining regression and ranking objectives, with pair-wise confidence weights derived from entropy. Second, the student is calibrated on a small subset of images with human-annotated MOS scores using a PLCC-based loss. This decoupling allows LEAF to learn perceptual quality from MLLM priors without extensive MOS annotations, then align the learned perception to the MOS scale with minimal supervision.

## Key Results
- Achieves SRCC/PLCC of 0.777/0.801 on KonIQ-10k without any MOS labels
- Reaches 0.861/0.867 on SPAQ and 0.749/0.811 on AGIQA-3K without MOS
- Matches or exceeds state-of-the-art with only 10-30% MOS supervision
- Demonstrates 10-30x reduction in labeling cost compared to fully supervised methods

## Why This Works (Mechanism)
The framework works by decoupling two distinct challenges in IQA: learning perceptual quality and calibrating to the MOS scale. MLLMs possess strong perceptual priors from large-scale vision-language training, making them effective teachers for quality perception. By freezing the teacher and using its outputs for distillation, LEAF avoids the computational expense of fine-tuning large models. The joint distillation of point-wise and pair-wise preferences captures both absolute quality and relative comparisons, while entropy-based confidence weighting filters unreliable supervision. The final calibration stage then adapts the learned perception to match human MOS scales using a small labeled set.

## Foundational Learning
- **MLLM Distillation**: Transferring knowledge from large frozen models to smaller, efficient models - needed to leverage MLLM perceptual priors without fine-tuning costs; quick check: verify teacher outputs are consistent across similar images
- **Point-wise vs Pair-wise Supervision**: Using both absolute scores and relative comparisons - needed to capture different aspects of quality perception; quick check: compare SRCC improvement when adding pair-wise loss
- **Entropy-based Confidence Weighting**: Using prediction uncertainty to weight supervision - needed to filter unreliable teacher judgments; quick check: plot confidence distribution and performance correlation
- **PLCC Calibration**: Aligning model outputs to human correlation metrics - needed to ensure MOS alignment beyond simple regression; quick check: verify PLCC improvement after calibration stage
- **MOS Scale Properties**: Understanding subjective quality rating distributions - needed to design appropriate calibration objectives; quick check: analyze MOS score distribution in calibration set

## Architecture Onboarding

**Component Map**: Teacher (InternVL-3.5-8B) -> Distillation Pipeline -> Student (ConvNeXt-Base) -> Calibration Pipeline -> Final IQA Model

**Critical Path**: Image -> Teacher Inference (Point-wise + Pair-wise) -> Distillation Loss Computation -> Student Update -> Calibration Loss Computation -> Final Prediction

**Design Tradeoffs**: The framework trades off teacher inference computational cost (during distillation data generation) for reduced labeling costs and smaller student model deployment costs. The confidence-weighted pair-wise supervision adds robustness but requires careful threshold tuning.

**Failure Signatures**: 
- High SRCC but low PLCC indicates successful ranking but poor MOS scale alignment
- Unstable training suggests overly noisy pair-wise supervision or incorrect confidence weighting
- Performance degradation on calibration set suggests domain shift between distillation and calibration data

**Three First Experiments**:
1. **Teacher Consistency Check**: Run teacher inference on identical images with different augmentations to measure output variance
2. **Distillation Ablation**: Train student with only point-wise loss, only pair-wise loss, and both to measure individual contributions
3. **Confidence Threshold Sweep**: Vary the pair-wise confidence threshold to find optimal balance between supervision quantity and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the computational overhead of generating dense pairwise and point-wise supervision from the MLLM teacher negate the efficiency benefits of the lightweight student model during the training phase?
- Basis in paper: [inferred] The paper highlights that MLLM inference is "computationally expensive" and LEAF moves this cost to the distillation phase to save deployment costs, but does not quantify the total energy/time cost of generating the supervision signals on large unlabeled datasets.
- Why unresolved: The framework reduces the student's inference cost and annotation cost, but shifts the burden to the teacher's inference during the distillation data generation.
- What evidence would resolve it: A comparative analysis of the total FLOPs/time required for the Teacher-Guided Distillation phase versus standard supervised training of a similar backbone.

### Open Question 2
- Question: Is the entropy-based confidence metric sufficient to distinguish between genuine perceptual ambiguity and hallucination errors in the teacher model?
- Basis in paper: [inferred] The method discards pairs with low confidence (high entropy), assuming low confidence implies comparable quality rather than teacher incapacity or hallucination.
- Why unresolved: High entropy in the teacher's output distribution could result from the images being genuinely similar in quality or the teacher failing to perceive the distortion; the paper does not analyze the "failure modes" of the teacher.
- What evidence would resolve it: A qualitative analysis of the discarded low-confidence pairs to determine the ratio of "ambiguous quality" vs. "teacher error."

### Open Question 3
- Question: How sensitive is the framework to domain shifts between the unlabeled distillation set and the small MOS calibration subset?
- Basis in paper: [inferred] The paper notes that "MOS annotations need to be recollected as scenarios and time changes," but evaluates LEAF where the distillation and calibration data are drawn from the same general benchmarks (UGC/AIGC).
- Why unresolved: It is unclear if the "perceptual priors" distilled from a general dataset (e.g., KonIQ) are robust enough to be calibrated effectively with a tiny MOS set from a drastically different domain (e.g., medical or satellite imagery).
- What evidence would resolve it: Cross-domain experiments where the student is distilled on natural images but calibrated on a specialized, out-of-domain dataset.

## Limitations
- Computational overhead of generating teacher supervision signals on large unlabeled datasets is not quantified
- Entropy-based confidence metric may not distinguish between genuine perceptual ambiguity and teacher model failures
- Sensitivity to domain shifts between distillation and calibration datasets is not thoroughly evaluated

## Confidence

**High Confidence**: The overall framework design (teacher-guided distillation followed by calibration fine-tuning) is logically coherent and addresses a valid research problem in label-efficient IQA.

**Medium Confidence**: The reported benchmark results (SRCC/PLCC on KonIQ-10k, SPAQ, AGIQA-3K) are plausible given the method's design, but exact reproduction depends on the missing hyperparameters.

**Low Confidence**: The absolute performance numbers are difficult to verify without the full experimental setup details.

## Next Checks

1. **Hyperparameter Sweep**: Run the distillation and calibration phases with a grid of learning rates, distillation weights (λ_dis, λ_cal), and confidence thresholds (τ) to identify the configuration that best reproduces the reported SRCC/PLCC trends.

2. **Ablation on Distillation Components**: Train ablations without the pair-wise ranking loss and without the PLCC calibration to isolate their individual contributions to the final performance.

3. **Teacher Model Comparison**: Substitute InternVL-3.5-8B with a different frozen MLLM (e.g., Qwen2-VL) as the teacher to test the robustness of the distillation framework to the choice of teacher model.