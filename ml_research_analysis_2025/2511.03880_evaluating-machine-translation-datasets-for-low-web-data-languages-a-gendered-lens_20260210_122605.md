---
ver: rpa2
title: 'Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered
  Lens'
arxiv_id: '2511.03880'
source_url: https://arxiv.org/abs/2511.03880
tags:
- gender
- language
- languages
- male
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates Machine Translation datasets for three low-resourced
  languages (Afan Oromo, Amharic, and Tigrinya) through a gendered lens. Using topic
  modeling, morphological analysis, named entity recognition, and masked language
  modeling, the authors identify significant gender bias: male names and verbs vastly
  outnumber female ones, and certain occupations and adjectives are strongly gendered.'
---

# Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens

## Quick Facts
- arXiv ID: 2511.03880
- Source URL: https://arxiv.org/abs/2511.03880
- Reference count: 40
- Primary result: Machine Translation datasets for Afan Oromo, Amharic, and Tigrinya contain significant gender bias, with male names and verbs vastly outnumbering female ones, and larger datasets containing more toxic content.

## Executive Summary
This paper evaluates Machine Translation datasets for three low-resourced languages (Afan Oromo, Amharic, and Tigrinya) through a gendered lens. Using topic modeling, morphological analysis, named entity recognition, and masked language modeling, the authors identify significant gender bias: male names and verbs vastly outnumber female ones, and certain occupations and adjectives are strongly gendered. Training datasets are dominated by religious and political content, while benchmarks focus on news, health, and sports. The Afan Oromo dataset also contains substantial code-mixing. The authors conclude that quantity does not guarantee quality, highlighting toxic content and biases in larger datasets, and recommend bias-aware data collection and evaluation practices.

## Method Summary
The study analyzes NLLB training data (16.14M Amharic, 3.23M Afan Oromo, 1.39M Tigrinya sentences) using four complementary methods: topic modeling with LDA to identify domain skews, morphological analysis with HornMorpho to quantify grammatical gender in verbs, NER to examine named entity distributions, and MLM probing to test gender stereotypes. Benchmarks include FLORES, HornMT, and MAFAND datasets. The analysis reveals that training data is dominated by religious and political content while benchmarks focus on news and health domains, creating a domain mismatch that may mask bias during evaluation.

## Key Results
- Male names outnumber female names by 72 percentage points in Tigrinya and 60 in Amharic
- Training data is dominated by religious (8-20%) and political topics while benchmarks focus on news, health, and sports
- The largest dataset (Amharic) contains the most harmful and toxic content
- Code-mixing affects 40% of Afan Oromo NLLB data, with sentences not in the target language

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-method triangulation exposes different manifestations of dataset bias that any single method would miss.
- **Mechanism:** Topic modeling surfaces domain skews (religious/political dominance), morphological analysis quantifies grammatical gender imbalance in verbs, NER reveals representational gaps in named entities, and MLM probing uncovers contextual stereotypical associations. Together, these provide converging evidence of systemic bias.
- **Core assumption:** Each method captures a distinct facet of bias that partially overlaps with but does not fully duplicate the others.
- **Evidence anchors:**
  - [abstract] "Using topic modeling, morphological analysis, named entity recognition, and masked language modeling, the authors identify significant gender bias"
  - [Section 4.2] Describes four distinct evaluation methods with different analytical targets
  - [corpus] Related work on gender bias in MT (GAMBIT+, EuroGEST) similarly employs multi-metric approaches, suggesting this is an established detection pattern
- **Break condition:** If methods produce contradictory findings without clear resolution, the triangulation assumption fails.

### Mechanism 2
- **Claim:** Linguistic-specific morphological analysis provides a systematic, quantifiable measure of gender representation in grammatically gendered languages.
- **Mechanism:** Amharic and Tigrinya mark grammatical gender through verb inflections (e.g., Amharic "ተማረ" vs. "ተማረች"). By using HornMorpho to extract and count gendered verb forms, the authors create an objective metric showing up to 72 percentage point gaps between male and female verb usage.
- **Core assumption:** HornMorpho's rule-based morphological analysis accurately identifies grammatical gender in context, despite acknowledged limitations with ambiguity.
- **Evidence anchors:**
  - [Section 2.1] Details on how gender is morphologically marked in each language
  - [Section 5.2] "Most verbs in the datasets have male grammatical gender... with the gaps being more pronounced for Tigrinya and Amharic"
  - [corpus] Limited direct evidence; corpus papers focus on corpus creation rather than morphological analysis for bias detection
- **Break condition:** If morphological analyzers misclassify gender at rates exceeding the observed gaps, the measured disparities could be artifacts.

### Mechanism 3
- **Claim:** Training-benchmark domain mismatch creates conditions for both poor model performance and undetected bias propagation.
- **Mechanism:** NLLB training data is dominated by religious and political text, while FLORES/HornMT/MAFAND benchmarks test news, health, and sports. Models trained on religious/political domains may not generalize, and any biases embedded in those domains won't be adequately assessed by mismatched benchmarks.
- **Core assumption:** Domain similarity between training and evaluation data affects both performance metrics and bias detection sensitivity.
- **Evidence anchors:**
  - [Section 5.1] "training data has a large representation of political and religious domain text, benchmark datasets are focused on news, health, and sports"
  - [Section 6] "our work reveals that the topics for training data and benchmark data do not always align"
  - [corpus] "Into the Void" paper discusses data quality issues in low-web languages, supporting broader concerns about data source alignment
- **Break condition:** If models demonstrate robust cross-domain transfer without performance degradation or bias amplification, the mismatch mechanism is less consequential.

## Foundational Learning

- **Concept: Grammatical vs. Notional Gender**
  - **Why needed here:** The three languages differ in how they encode gender—Amharic and Tigrinya have grammatical gender (every noun is gendered), while Afan Oromo has notional gender (optional marking). This affects which analytical methods apply and how bias manifests.
  - **Quick check question:** Given a sentence in Amharic containing the word "ሴት" (woman), what grammatical gender would accompanying verbs and adjectives carry, and how would this differ in Afan Oromo?

- **Concept: Latent Dirichlet Allocation (LDA) for Topic Modeling**
  - **Why needed here:** LDA is used to identify thematic clusters in large corpora, revealing that training data is dominated by religious/political content. Understanding how LDA assigns documents to topics based on word co-occurrence is essential for interpreting these findings.
  - **Quick check question:** If LDA identifies 50 topics in the NLLB dataset and 8-20% of topics are religious across languages, what does this suggest about the diversity of web-crawled data for these languages?

- **Concept: Masked Language Modeling for Bias Probing**
  - **Why needed here:** MLM probing tests whether models associate stereotypical gender with certain occupations or adjectives. Fine-tuning AfriBERTa on biased NLLB data increased male-gender predictions even for female-expected contexts, demonstrating how training data shapes model behavior.
  - **Quick check question:** After fine-tuning on NLLB data, why might an MLM predict male-gendered tokens for cloze sentences where the expected gender is female, and what does this reveal about the training data?

## Architecture Onboarding

- **Component map:**
  - NLLB training data (1.4M-16M sentences per language) -> HornMorpho morphological analyzer -> LDA topic modeling -> AfriBERTa/AfroXLMR NER and MLM models -> Gender distribution statistics and bias metrics

- **Critical path:**
  1. Dataset selection -> 2. Preprocessing (stopword removal, language filtering) -> 3. Parallel analysis via four methods -> 4. Human annotation for validation -> 5. Cross-method synthesis -> 6. Recommendations
  The most resource-intensive step is morphological analysis of NLLB vocabularies (28 hours for Amharic with 260K unique words).

- **Design tradeoffs:**
  - Sample vs. full analysis: Morphological analysis limited to vocabulary subsets due to computational constraints; NER results stratified rather than exhaustive
  - Model selection: AfriBERTa chosen for pre-training coverage of target languages; AfroXLMR chosen for Amharic NER despite excluding other target languages
  - Topic granularity: 50 topics for large datasets vs. 5 for benchmarks—affects precision of domain identification

- **Failure signatures:**
  - Language ID failure: 40% of "Afan Oromo" NLLB data is not Afan Oromo—indicates web-crawled data quality issues
  - Benchmark-training mismatch: Religious/political topics in training but not benchmarks suggests evaluation blind spots
  - Toxicity scaling: Largest dataset (Amharic) contains most harmful content—contradicts "more data is better" assumption

- **First 3 experiments:**
  1. **Language ID audit:** Random sample 300 sentences from each NLLB language split; have native speakers verify language correctness to establish baseline data quality before any bias analysis
  2. **Morphological pipeline test:** Run HornMorpho on 1000 verbs per language with known gender; manually verify accuracy rate to quantify measurement uncertainty
  3. **Domain overlap analysis:** Compare topic distributions between NLLB and benchmark datasets using consistent topic modeling parameters; quantify mismatch using KL divergence or similar metrics

## Open Questions the Paper Calls Out

- **Question:** To what extent do gender biases identified in training data propagate to downstream machine translation outputs for low-resourced languages?
  - **Basis in paper:** [inferred] The authors note they focused on the data stage, and suggest "Future work can extend our study with an evaluation of MT models that specifically addresses the pitfalls we identified."
  - **Why unresolved:** The study evaluates only the training data; it does not assess whether these biases manifest in actual translation outputs.
  - **What evidence would resolve it:** Train MT models on these datasets and evaluate their outputs using the same gender bias metrics (name distributions, verb gender, occupational stereotypes).

- **Question:** Does the quality of parallel sentence alignment correlate with gender representation disparities in the target language side of MT datasets?
  - **Basis in paper:** [inferred] The authors acknowledge they "did not evaluate the parallel English sentences" and propose future work "incorporate an evaluation of the parallel sentence and check if they are correctly aligned."
  - **Why unresolved:** Only the target language was examined; alignment quality between source and target sentences remains unassessed.
  - **What evidence would resolve it:** Measure alignment accuracy for gendered sentences and correlate with representation gaps found in the target language.

- **Question:** Can participatory and human-centric data collection methods effectively reduce toxic and harmful content in low-resource language datasets?
  - **Basis in paper:** [explicit] The authors suggest "More research into Language ID tools as well as human-centric and participatory approaches may offer better alternatives for future data collection schemes."
  - **Why unresolved:** The paper documents toxic content but does not test alternative collection methods.
  - **What evidence would resolve it:** Compare toxicity and bias metrics between web-crawled and participatory-collected datasets for the same languages.

- **Question:** How does the domain mismatch between religious/political training data and news-focused benchmarks affect the detection of gender bias in evaluation?
  - **Basis in paper:** [inferred] The authors found training data dominated by religious/political content while benchmarks focus on news, health, and sports, but did not explore how this mismatch affects bias measurement.
  - **Why unresolved:** The impact of domain discrepancy on bias evaluation validity was not assessed.
  - **What evidence would resolve it:** Create benchmarks matching training data domains and compare gender bias metrics across domain-matched and domain-mismatched evaluations.

## Limitations
- The reported gender imbalances are based on morphological analysis that may misclassify gender in ambiguous contexts, though the 72 percentage point gaps exceed typical ambiguity rates
- Training-benchmark domain mismatch findings rely on topic modeling with 50 topics for large datasets and 5 for benchmarks, creating potential granularity inconsistencies
- Code-mixing findings for Afan Oromo are based on corpus-level statistics without detailed linguistic analysis of what languages are mixed or their distribution patterns
- MLM probing results show increased bias after fine-tuning but don't establish causal pathways for how specific training data patterns translate to model behavior

## Confidence
- **High confidence** in the core finding that male names and verbs vastly outnumber female ones across all three languages, supported by multiple independent methods
- **Medium confidence** in the characterization of domain mismatch between training and benchmark datasets, as topic modeling provides strong but indirect evidence
- **Medium confidence** in the toxic content findings, as manual verification was limited and toxicity definitions may vary across annotators
- **Low confidence** in specific causal claims about how training data patterns directly cause MLM bias without additional controlled experiments

## Next Checks
1. Conduct manual validation of 500 randomly sampled morphological analyses per language to establish HornMorpho's accuracy rate and quantify measurement uncertainty
2. Re-run topic modeling with consistent parameters (50 topics) across all datasets to eliminate granularity differences in domain mismatch analysis
3. Perform controlled fine-tuning experiments where religious/political vs. news/health/sports domains are systematically varied to establish causal links between training data and benchmark performance/bias