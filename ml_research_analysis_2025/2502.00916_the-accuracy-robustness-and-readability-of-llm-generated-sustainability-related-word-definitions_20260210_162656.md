---
ver: rpa2
title: The Accuracy, Robustness, and Readability of LLM-Generated Sustainability-Related
  Word Definitions
arxiv_id: '2502.00916'
source_url: https://arxiv.org/abs/2502.00916
tags:
- definitions
- ipcc
- terms
- adherence
- readability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates how well three large language models (GPT-4o-mini,\
  \ Llama3.1 8B, and Mistral 7B) generate definitions for 300 climate-related terms\
  \ compared to official IPCC glossary definitions. Using SBERT embeddings, the models\
  \ showed average adherence scores of 0.57-0.59 \xB1 0.15 to the official definitions,\
  \ indicating reasonable semantic alignment but with notable variability, especially\
  \ for terms with multiple or ambiguous meanings."
---

# The Accuracy, Robustness, and Readability of LLM-Generated Sustainability-Related Word Definitions

## Quick Facts
- arXiv ID: 2502.00916
- Source URL: https://arxiv.org/abs/2502.00916
- Reference count: 3
- Primary result: LLM-generated definitions show 0.57-0.59 semantic adherence to IPCC glossary definitions but are consistently harder to read

## Executive Summary
This paper evaluates three large language models (GPT-4o-mini, Llama3.1 8B, and Mistral 7B) on their ability to generate definitions for 300 climate-related terms compared to official IPCC glossary definitions. Using SBERT embeddings, the models achieved average adherence scores of 0.57-0.59 ± 0.15 to the official definitions, indicating reasonable semantic alignment but with notable variability. The generated definitions were consistently more complex to read than the originals. Robustness analysis showed high consistency in most cases, but some terms like "Projection" and "Equity" varied significantly across prompts, highlighting potential ambiguities. The study underscores the potential of LLMs to support climate discourse but stresses the need to align outputs with standardized terminology for clarity and accessibility.

## Method Summary
The study compares LLM-generated definitions against official IPCC glossary definitions for 300 climate-related terms. Five prompt templates are used to generate 25 completions per term across three models. Semantic adherence and robustness are measured using SBERT cosine similarity between generated and official definitions, while readability is assessed using Flesch-Kincaid and Gunning-Fog scores with bootstrapping. The methodology includes ablation studies with IPCC-context and readability prompts to examine their effects on output quality.

## Key Results
- LLM-generated definitions show 0.57-0.59 semantic adherence to IPCC glossary definitions
- Generated definitions are consistently more complex to read than IPCC originals
- Some terms (Projection, Equity, Exposure) show significant robustness issues across prompts
- Readability prompting reduces complexity but increases verbosity, creating a partial tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SBERT cosine similarity captures semantic alignment between LLM-generated and official definitions without requiring exact lexical matches
- Mechanism: Sentence embeddings map both official IPCC definitions and LLM outputs into a shared vector space where cosine distance quantifies semantic proximity. The formula `adherence = (1/n) Σ sim(D, Mk)` averages similarity across multiple completions to smooth single-prompt variance
- Core assumption: SBERT embeddings accurately reflect human-judged semantic equivalence for domain-specific climate terminology
- Evidence anchors: [abstract] "analyzing adherence, robustness, and readability using SBERT sentence embeddings"; [section 3] "sim(A, B) the cosine distance between the SBERT sentence embeddings of the texts A and B"
- Break condition: If SBERT embeddings poorly capture domain-specific nuance (e.g., technical distinctions between "mitigation" and "adaptation"), adherence scores could be misleadingly high or low

### Mechanism 2
- Claim: Low robustness scores signal polysemous or ambiguously-defined terms that require contextual grounding
- Mechanism: When a term has multiple domain meanings (e.g., "Projection" in psychological vs. mathematical vs. climate contexts), unconstrained prompts allow the LLM to sample from different semantic clusters across completions. The robustness metric `sim(Mp, Mq)` across pairwise completions detects this variance
- Core assumption: Variability across completions reflects genuine polysemy rather than temperature-induced randomness (paper uses default temperatures)
- Evidence anchors: [abstract] "some terms like 'Projection' and 'Equity' varied significantly across prompts, highlighting potential ambiguities"; [section 4.2] "GPT-4o-mini's definition of 'Projection' spanned the psychological, mathematical, and environmental topics"
- Break condition: If model temperature is set high, robustness scores may conflate semantic ambiguity with sampling noise

### Mechanism 3
- Claim: Explicit readability prompting reduces complexity metrics but increases verbosity, creating a partial tradeoff
- Mechanism: When prompted to be "understandable by a 10-year old," LLMs substitute simpler vocabulary but expand sentence length through elaboration. Flesch-Kincaid scores dropped from 19.9 to 16.4, but word count increased from ~40 to ~43 words on average
- Core assumption: Readability formulas designed for paragraph-length texts remain valid for single-sentence definitions when aggregated via bootstrapping
- Evidence anchors: [section 4.4] "the readability prompt seems to have a greater effect, decreasing the Flesch-Kincaid score from 19.9 ± 0.2 to 16.4 ± 0.02"; [section 5] "prompting for readability made the model more verbose"
- Break condition: If readability metrics are poor proxies for actual comprehension (as corpus evidence suggests), optimization for these scores may not improve user understanding

## Foundational Learning

- Concept: **SBERT/Sentence-BERT embeddings**
  - Why needed here: The entire adherence and robustness framework depends on understanding how sentence embeddings encode semantic meaning and why cosine similarity is appropriate for comparing definitions
  - Quick check question: Given two definitions of "equity"—one financial, one social—would SBERT give them high or low similarity, and why might this matter for the paper's methodology?

- Concept: **Readability metrics (Flesch-Kincaid, Gunning-Fog)**
  - Why needed here: The paper's claim that LLM definitions are "harder to read" rests on these metrics; understanding their formulas explains why longer sentences increase scores even with simpler vocabulary
  - Quick check question: If an LLM replaces "anthropogenic forcing" with "human-caused warming" but doubles sentence length, would Flesch-Kincaid likely increase or decrease?

- Concept: **LLM temperature and output variance**
  - Why needed here: Robustness scores depend on consistency across completions; without understanding temperature's role, you cannot distinguish semantic ambiguity from sampling randomness
  - Quick check question: If the paper used temperature=0.9 instead of default settings, would you expect robustness scores to increase or decrease, and how would this change interpretation?

## Architecture Onboarding

- Component map: IPCC glossary terms -> Prompt templates -> LLM completions -> SBERT embeddings -> Cosine similarity -> Adherence/robustness metrics -> Readability analysis
- Critical path: 1) Term extraction from IPCC glossary (Selenium scraping -> filtering to 300-term subset); 2) Prompt template generation (ChatGPT-generated variants); 3) Completion generation at default temperature across all models; 4) SBERT embedding computation for all (definition, completion) pairs; 5) Metric aggregation per term and across corpus
- Design tradeoffs:
  - Single-sentence constraint: Ensures comparability but loses nuance in complex definitions
  - Bootstrapped readability: Enables metric application to short texts but introduces sampling variance
  - No context in prompts: Reveals polysemy but reduces domain alignment (explicit IPCC prompting showed no significant improvement)
- Failure signatures:
  - Adherence outliers: "Demand- and supply-side measures" (0.06) and "Poverty" consistently scored lowest across all models—suggests these terms have contested or underspecified definitions in training data
  - Robustness failures: "Projection," "Equity," "Exposure" show cross-domain activation—signals need for explicit context disambiguation
  - Readability inversion: IPCC+Readable prompt achieved only 17.3 Flesch-Kincaid (vs. target ~10 for 10-year-olds)—signals that current prompting strategies insufficiently control output complexity
- First 3 experiments:
  1. Temperature sweep on low-robustness terms: Run "Projection" and "Equity" at temperature 0.0, 0.3, 0.7, 1.0 to disentangle semantic polysemy from sampling variance. If robustness remains low at temp=0, polysemy is confirmed
  2. In-context definition injection: Prepend the exact IPCC definition to the prompt for the 10 lowest-adherence terms and measure improvement. This tests the paper's recommendation to "include the exact definitions in the prompts"
  3. Multi-sentence readability protocol: Allow 2-3 sentence definitions with explicit simplicity constraints, then compare readability metrics against human comprehension ratings (not just formulas) to validate whether the readability-accuracy tradeoff is real or artifact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be optimized to simplify climate definitions to a layperson reading level while maintaining high semantic adherence to official IPCC definitions?
- Basis in paper: [explicit] The conclusion states, "Future work could explore ways to improve accessibility by using LLMs to simplify language without compromising accuracy," and notes the challenge of "balancing simplicity with accuracy"
- Why unresolved: The study found that prompting for readability ("understandable by a 10-year old") reduced complexity only slightly (from college level to high school/college level) and often resulted in longer, more verbose sentences rather than truly accessible language
- What evidence would resolve it: An experiment using optimization techniques (e.g., RLHF or specific prompt engineering) targeted at lower Flesch-Kincaid scores that simultaneously maintains an adherence score > 0.80

### Open Question 2
- Question: Does incorporating official glossary definitions via in-context learning (retrieval-augmented generation) significantly increase adherence compared to zero-shot prompting?
- Basis in paper: [explicit] The author explicitly suggests, "Future work could explore... incorporating relevant official glossaries as part of an in-context learning approach," noting that explicit prompting alone did not significantly improve adherence
- Why unresolved: The "IPCC" ablation prompt (asking the model to adhere to the glossary without providing the text) failed to improve scores, showing that models cannot simply "recall" the standard definition on command; they likely require the text in the context window
- What evidence would resolve it: A comparative study measuring SBERT adherence between zero-shot generations and generations where the official IPCC definition is provided in the prompt as a reference example

### Open Question 3
- Question: How does the performance and robustness of sustainability definitions generalize across non-English languages and diverse model architectures?
- Basis in paper: [explicit] The conclusion states, "Studies across more models and languages would further inform how LLMs represent sustainability"
- Why unresolved: This study was limited to three specific models (GPT-4o-mini, Llama3.1 8B, Mistral 7B) and the English language IPCC glossary. It is unknown if the observed adherence (0.57–0.59) and robustness issues are universal or language/model-dependent
- What evidence would resolve it: Replicating the methodology using the official IPCC glossary translations (e.g., Spanish, French) on a distinct set of models (e.g., Claude, Gemini) to compare adherence distributions

## Limitations
- SBERT model selection is not specified, which could affect absolute adherence scores
- Temperature settings are not explicitly stated, critical for interpreting robustness scores
- Traditional readability formulas may not accurately predict comprehension for single-sentence definitions

## Confidence
- High confidence: The finding that LLM-generated definitions are more complex than IPCC originals (readability analysis) is robust across multiple metrics and models
- Medium confidence: The adherence scores (0.57-0.59) indicate reasonable semantic alignment, but absolute values depend on SBERT model choice
- Medium confidence: The identification of polysemous terms (Projection, Equity, Exposure) is well-supported by robustness analysis, but temperature effects could partially explain the variance

## Next Checks
1. Temperature sensitivity analysis: Run the three lowest-robustness terms ("Projection," "Equity," "Exposure") at multiple temperature settings (0.0, 0.3, 0.7, 1.0) to determine whether observed variance reflects true semantic ambiguity or sampling noise
2. Context injection experiment: For the 10 lowest-adherence terms, prepend the exact IPCC definition to the prompt and measure improvement. This directly tests the paper's recommendation to include official definitions in prompts
3. Multi-sentence readability validation: Allow 2-3 sentence definitions with explicit simplicity constraints, then compare readability metrics against human comprehension ratings to validate whether current readability-accuracy tradeoffs are real or artifacts of metric limitations