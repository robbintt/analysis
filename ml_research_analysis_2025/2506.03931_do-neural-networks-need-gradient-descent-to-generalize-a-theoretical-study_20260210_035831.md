---
ver: rpa2
title: Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study
arxiv_id: '2506.03931'
source_url: https://arxiv.org/abs/2506.03931
tags:
- train
- matrix
- generalization
- wiid
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper theoretically investigates the role of gradient descent\
  \ in neural network generalization by studying matrix factorization with varying\
  \ width and depth. The authors compare Guess & Check (G&C) optimization\u2014randomly\
  \ sampling weight settings that fit training data\u2014to gradient descent."
---

# Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study

## Quick Facts
- **arXiv ID**: 2506.03931
- **Source URL**: https://arxiv.org/abs/2506.03931
- **Reference count**: 40
- **Primary result**: Proves that in canonical matrix factorization settings, Guess & Check (G&C) optimization can be provably inferior to gradient descent, depending on width-depth trade-offs.

## Executive Summary
This paper investigates whether gradient descent is necessary for neural network generalization by comparing it to Guess & Check (G&C) optimization in matrix factorization. The authors prove that for wide networks with anti-symmetric activations, G&C generalization deteriorates to chance while gradient descent maintains strong performance. Conversely, for deep linear networks with normalized Gaussian priors, increasing depth improves G&C generalization, approaching gradient descent's performance. These results reveal that the necessity of gradient descent depends critically on subtle width-depth trade-offs, challenging the volume hypothesis and providing the first rigorous proof that G&C can be provably inferior to gradient descent in canonical cases.

## Method Summary
The authors study deep matrix factorization (W = W_d σ(…W_1)) as a tractable proxy for neural networks. They implement two optimization methods: Guess & Check (G&C) - randomly sampling weight settings that fit training data, and gradient descent with small initialization. For the width experiment, they fix depth d=2 and vary width k∈[100,600], using linear and tanh activations. For the depth experiment, they fix width k=5 and vary depth d∈[2,10] with normalized Gaussian priors. Generalization loss is computed over an orthonormal basis orthogonal to measurement span. The experiments systematically vary architectural dimensions while measuring the gap between G&C and gradient descent performance.

## Key Results
- G&C generalization deteriorates to no better than chance in wide networks with anti-symmetric activations due to statistical independence between training and generalization losses.
- For deep linear networks with normalized Gaussian priors, increasing depth improves G&C generalization by creating an implicit rank-one bias through spectral collapse.
- Gradient descent is provably necessary for generalization in wide networks because it actively selects solutions that G&C's random sampling misses.
- These theoretical findings are empirically validated across multiple activation functions and settings, including matrix completion.

## Why This Works (Mechanism)

### Mechanism 1: Statistical Independence in Infinite Width
For wide networks with anti-symmetric activations, G&C generalization deteriorates to chance because training and generalization losses become statistically independent. As width k tends to infinity, the weight matrix output converges to a centered Gaussian matrix with i.i.d. entries. Since measurement matrices are orthogonal to the generalization test basis, the events "low training loss" and "low generalization loss" become independent. This breaks down if activations are non-anti-symmetric or priors have non-zero mean.

### Mechanism 2: Implicit Rank-One Bias via Depth
In deep linear networks, random weight matrices from normalized Gaussian priors converge to approximate rank-one structure as depth increases. The product of many random Gaussian matrices exhibits concentration where the largest singular value dominates (Lyapunov exponent gap forms). A deep, randomly drawn factorized matrix is highly likely to be close to rank-one, aligning well with low-rank ground truth data. This proof relies on rank-one ground truth matrices.

### Mechanism 3: Gradient Descent as a Necessity for Width
Gradient descent is provably necessary for generalization in wide networks because it actively selects solutions that G&C misses. While G&C samples blindly from a prior uncorrelated with generalization in the wide limit, GD utilizes loss landscape geometry and converges to minimal nuclear norm solutions (implicit regularization), circumventing the independence trap. This necessity disappears if width is small or depth is large.

## Foundational Learning

- **Concept**: **Matrix Factorization (as a Neural Network Proxy)**
  - **Why needed**: Used as a tractable theoretical testbed to derive rigorous proofs that would be intractable for general non-linear neural networks.
  - **Quick check**: How does the product of matrices W_d … W_1 relate to the layers of a neural network in this model?

- **Concept**: **The Volume Hypothesis**
  - **Why needed**: Central counter-hypothesis the paper investigates - posits that "volume" of good solutions is large enough that random sampling finds them.
  - **Quick check**: In the wide-network limit, does the volume of solutions that fit training data but fail to generalize dominate the posterior? (Answer: Yes, per Theorem 1)

- **Concept**: **Lyapunov Exponents**
  - **Why needed**: Used to describe the spectrum of product of random matrices, explaining why deep random matrices look like rank-one matrices.
  - **Quick check**: What happens to the singular values of a product of random Gaussian matrices as depth increases? (Answer: They separate; top singular value dominates)

## Architecture Onboarding

- **Component map**: Ground Truth W* -> Measurement Matrices A_i -> Matrix Factorization W = W_d σ(…W_1) -> Training/Generalization Loss

- **Critical path**:
  1. Initialize prior (Regular distribution)
  2. If Width is variable: Observe decoupling of training/generalization losses
  3. If Depth is variable: Observe spectral collapse (Rank-1 approximation)

- **Design tradeoffs**:
  - Wide & Shallow: Must use Gradient Descent. Random sampling fails due to Gaussian/Random solution space
  - Deep & Narrow: Can rely more on architecture/prior. Random sampling is viable

- **Failure signatures**:
  - G&C in Wide Networks: Generalization loss remains constant even as training loss approaches zero (memorization/random guessing)
  - GD in Deep Networks: Vanishing gradients (limits empirical depth testing to d≈10)

- **First 3 experiments**:
  1. Width Scaling with G&C: Fix depth d=2, increase width k. Measure gap between GD and G&C performance. Confirm gap widens (Fig 1).
  2. Depth Scaling with G&C: Fix width k, increase depth d with normalized priors. Measure if G&C generalization approaches GD performance (Fig 2).
  3. Prior Normalization Ablation: Repeat depth scaling without normalizing Gaussian prior. Check if G&C performance collapses, validating the core assumption.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the relationships between width, depth, and gradient descent necessity established for matrix factorization hold for tensor factorization?
  - Basis: Section 7 identifies tensor factorization as a "natural next step"
  - Why unresolved: Current analysis restricted to matrix factorization; higher-order structure may alter width-depth trade-offs
  - Evidence needed: Theoretical extension of Theorems 1 and 2 to tensor setting, or empirical evaluations on tensor factorization

- **Open Question 2**: Does G&C generalization deteriorate with increasing width for non-anti-symmetric activations like ReLU?
  - Basis: Section 6 explicitly states Theorem 1 relies on anti-symmetric activation
  - Why unresolved: Proof relies on specific symmetry properties that ReLU violates
  - Evidence needed: Proof extending result to non-anti-symmetric activations, or empirical studies isolating width effect on ReLU networks

- **Open Question 3**: Does increasing depth improve G&C generalization for ground truth matrices with rank greater than one?
  - Basis: Section 6 and Appendix D highlight Theorem 2 restricted to rank-one ground truth
  - Why unresolved: Proof relies on product of random matrices converging to rank-one structure
  - Evidence needed: Rigorous extension of Theorem 2 providing bounds for rank r > 1 ground truth

## Limitations
- Theoretical results rely heavily on anti-symmetric activation functions and specific ground truth structures (rank-one matrices for depth analysis)
- Depth experiments limited by computational constraints, preventing exploration beyond d≈10 due to vanishing gradients
- Proofs do not generalize to arbitrary activation functions or rank structures without significant modifications

## Confidence
- **High Confidence**: Width-dependent deterioration of G&C generalization (Mechanism 1) - rigorously proven and empirically validated
- **Medium Confidence**: Depth-dependent improvement of G&C generalization (Mechanism 2) - theoretically proven but relies on strong assumptions about rank-one ground truth
- **Medium Confidence**: Gradient descent as necessity for wide networks (Mechanism 3) - follows logically but depends on volume hypothesis being invalid in wide limit

## Next Checks
1. Test G&C performance with ReLU and other non-anti-symmetric activations to empirically verify whether the independence phenomenon breaks down as predicted.

2. Systematically vary the ground truth rank in depth experiments (r=1,2,3) to quantify how depth improvement degrades as rank increases, validating the rank-one assumption.

3. Repeat depth experiments with non-normalized priors (e.g., standard Gaussian without scaling) to empirically confirm that spectral collapse mechanism requires the specific normalization condition.