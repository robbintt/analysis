---
ver: rpa2
title: Deep Multiple Quantization Network on Long Behavior Sequence for Click-Through
  Rate Prediction
arxiv_id: '2508.20865'
source_url: https://arxiv.org/abs/2508.20865
tags:
- behavior
- interest
- user
- long
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Deep Multiple Quantization Network (DMQN)
  for end-to-end modeling of long user behavior sequences in click-through rate prediction.
  The key idea is to compress the full long behavior sequence into approximately 100
  interest clusters using multiple learnable codebooks, then employ Hierarchical Sequential
  Transduction Unit (HSTU) to model interactions among these clusters.
---

# Deep Multiple Quantization Network on Long Behavior Sequence for Click-Through Rate Prediction

## Quick Facts
- **arXiv ID:** 2508.20865
- **Source URL:** https://arxiv.org/abs/2508.20865
- **Authors:** Zhuoxing Wei; Qi Liu; Qingchen Xie
- **Reference count:** 21
- **Primary result:** DMQN improves CTR by 3.5% and RPM by 2.0% in A/B testing over retrieval-based methods.

## Executive Summary
This paper introduces Deep Multiple Quantization Network (DMQN) for click-through rate prediction on long user behavior sequences. The key innovation is compressing the full sequence into approximately 100 learnable interest clusters using multiple codebooks, then modeling interactions among these clusters with a Hierarchical Sequential Transduction Unit (HSTU). Target attention is applied between the candidate item and the compressed representation to extract long-term user interests. The method addresses the "relevance distribution discrepancy" in two-stage retrieval methods by preserving the full distribution through end-to-end compression. Intermediate representations are cached for efficient online serving.

## Method Summary
DMQN operates in three stages: First, the Multi-Cluster Quantization Module (MCQM) projects the user's long behavior sequence into multiple latent spaces and assigns items to approximately 100 clusters using Gumbel-Softmax for differentiable quantization. Second, the Interest Cluster Interaction Module (ICIM) applies HSTU to model sequential interactions among the compressed cluster representations. Third, Cluster-aware Target Attention (CTAM) applies attention between the candidate item and the processed clusters. The model is trained with binary cross-entropy loss and employs caching of intermediate representations for online serving efficiency.

## Key Results
- DMQN achieves 0.7103 AUC offline, outperforming baselines including SIM (0.7089 AUC)
- Online A/B testing shows 3.5% CTR improvement and 2.0% RPM improvement
- Caching mechanism reduces latency by pre-computing user representations independent of candidate items

## Why This Works (Mechanism)

### Mechanism 1: Distribution Preservation via End-to-End Compression
The model compresses long behavior sequences into ~100 learnable interest clusters using multiple codebooks, preserving the full relevance distribution that retrieval methods lose. Gumbel-Softmax enables differentiable assignment while pooling aggregates item embeddings into cluster representations.

### Mechanism 2: Structural Interest Interaction via HSTU
HSTU models temporal interactions between interest clusters, capturing how different interest groups influence each other over time. This sequential modeling improves preference representation beyond static clustering.

### Mechanism 3: Candidate-Agnostic Caching for Online Efficiency
The system pre-computes and caches heavy sequence processing steps since they don't depend on the specific candidate item. This achieves low-latency inference by storing processed interest vectors in Redis and performing only lightweight target attention online.

## Foundational Learning

- **Vector Quantization (VQ) & Codebooks:** The MCQM relies on mapping continuous item embeddings to discrete codewords using codebooks as compression bottlenecks. *Quick check:* How does using multiple independent codebooks differ from a single codebook with N times the rows?

- **Gumbel-Softmax Trick:** Used to solve non-differentiability of discrete cluster assignment, allowing gradients to backpropagate through sampling. *Quick check:* What happens to cluster assignments if temperature τ is extremely high vs. low?

- **Target Attention (TA):** Final step where compressed history meets candidate item. *Quick check:* In DMQN, what serves as Query, Key, and Value? (Is sequence length L or W?)

## Architecture Onboarding

- **Component map:** Input Sequence → MCQM → ICIM (HSTU) → CTAM → Prediction
- **Critical path:** Training: Raw Sequence → MCQM → ICIM → CTAM → Loss. Inference: User ID → Redis Lookup → CTAM (with Candidate) → Prediction
- **Design tradeoffs:** Sequence length W affects detail vs. HSTU complexity; codebook count N affects multi-aspect capture vs. parameter count; cache update frequency balances staleness vs. freshness
- **Failure signatures:** Attention mode collapse (focuses on single cluster), stale cache for active users, temperature collapse freezing codebooks
- **First 3 experiments:** 1) Overfit test with/without Gumbel noise to verify codebook learning, 2) Measure ICIM latency vs Redis lookup + CTAM time, 3) Visualize t-SNE of codebook vectors to confirm semantic categories

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Does not compare against efficient Transformer variants for long sequences, only retrieval-based methods
- Lacks statistical significance testing details for A/B results
- Does not provide hyperparameter sensitivity analysis for cluster count and codebook configuration

## Confidence

- **High Confidence:** Technical soundness of quantization + HSTU architecture and caching mechanism for online serving
- **Medium Confidence:** Specific choice of ~100 clusters and relative performance gain over retrieval methods
- **Low Confidence:** Claim that "distribution discrepancy" is the primary failure mode without benchmarking against other long-sequence approaches

## Next Checks

1. **Distribution Analysis:** Visualize learned codebook vectors and cluster assignment distributions across user segments to verify semantic meaningfulness

2. **Retrieval Method Comparison:** Implement controlled experiment comparing DMQN against full-sequence attention with efficient Transformers to isolate quantization benefits

3. **Cache Staleness Impact:** Design experiment varying cache update frequency to measure CTR/RPM degradation across different user activity levels