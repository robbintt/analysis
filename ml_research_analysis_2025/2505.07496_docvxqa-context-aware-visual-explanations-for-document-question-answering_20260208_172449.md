---
ver: rpa2
title: 'DocVXQA: Context-Aware Visual Explanations for Document Question Answering'
arxiv_id: '2505.07496'
source_url: https://arxiv.org/abs/2505.07496
tags:
- answer
- question
- document
- explanations
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocVXQA, the first self-explainable document
  visual question answering (DocVQA) framework that learns to generate context-aware
  visual explanations alongside answer predictions. Unlike post-hoc explainability
  methods, DocVXQA integrates explainability directly into training via an information
  bottleneck objective balancing sufficiency (retaining predictive information) and
  minimality (focusing on relevant regions).
---

# DocVXQA: Context-Aware Visual Explanations for Document Question Answering

## Quick Facts
- **arXiv ID:** 2505.07496
- **Source URL:** https://arxiv.org/abs/2505.07496
- **Reference count:** 36
- **Primary result:** Introduces the first self-explainable DocVQA framework that learns context-aware visual explanations via an information bottleneck objective, achieving 38% accuracy at 70% mask threshold on standard benchmarks.

## Executive Summary
This paper introduces DocVXQA, the first self-explainable document visual question answering (DocVQA) framework that learns to generate context-aware visual explanations alongside answer predictions. Unlike post-hoc explainability methods, DocVXQA integrates explainability directly into training via an information bottleneck objective balancing sufficiency (retaining predictive information) and minimality (focusing on relevant regions). The approach learns a relevance mask over document images, guided by a pretrained vision-language model (ColPali) to enhance contextual awareness. Experiments on DocVQA and PFL-DocVQA datasets show that DocVXQA achieves superior accuracy (e.g., 38% vs. 6% for raw attention at 70% mask threshold) while maintaining compact, interpretable masks. Human evaluation confirms higher context-awareness (4.49/5) and clarity (3.56/5) compared to baselines. The method is model-agnostic and requires minimal architectural changes to existing DocVQA models.

## Method Summary
DocVXQA learns to generate pixel-wise relevance masks over document images by integrating explainability into the training objective. The framework uses an information bottleneck approach where the masked input (X ⊙ M) serves as a bottleneck representation, balancing sufficiency (preserving answer-predictive information) and minimality (compressing irrelevant regions). A mask head predicts relevance scores, which are regularized by an L1+TV loss for sparsity and smoothness. The mask is guided by a ColPali prior heatmap to ensure context-awareness beyond just answer tokens. The model then predicts answers from the masked image, creating a self-consistency constraint. The approach is model-agnostic, demonstrated with Pix2Struct and Donut backbones, and requires minimal architectural changes to existing DocVQA models.

## Key Results
- DocVXQA achieves 38% accuracy at 70% mask threshold versus 6% for raw attention baselines on DocVQA dataset
- Human evaluation shows context-awareness score of 4.49/5 versus 3.24/5 for raw attention and 3.56/5 for EaGERS
- Ablation study confirms token interaction loss is critical: S+M+TI achieves 0.38 accuracy vs 0.19 for S+M alone
- Model-agnostic validation: Donut backbone achieves 0.32 accuracy at 70% threshold, comparable to Pix2Struct performance

## Why This Works (Mechanism)

### Mechanism 1: Information Bottleneck for Explanation Learning
The mask learning problem is framed as an information bottleneck to enforce a principled trade-off between explanation completeness and compactness. The mask M defines a bottleneck representation T = X ⊙ M, with the objective L_IB = βI(X; X ⊙ M) − I(X ⊙ M; Y) simultaneously compressing the input (minimality) while preserving answer-predictive information (sufficiency). This creates compact yet predictive representations. The core assumption is that the masked input can retain sufficient information for answer prediction while discarding contextually irrelevant regions. Evidence shows the formulation is directly implemented in the loss function, though IB convergence with high-dimensional image data is noted as a practical challenge.

### Mechanism 2: ColPali Prior for Context-Aware Regularization
An external vision-language model's relevance signal is injected as a prior to prevent mask overfitting to answer tokens alone. ColPali generates multi-vector representations with late interaction matching between question and document tokens, producing a heatmap prior M_p. The L_MSE loss aligns the learned mask M with M_p, encouraging broader contextual coverage beyond the answer string. The core assumption is that ColPali's attention-based heatmaps, while noisy when used in isolation, contain useful prior signal about question-document relevance relationships. Ablation studies show S+M+TI achieves 0.38 accuracy versus 0.19 for S+M alone, validating the prior regularization effect.

### Mechanism 3: Sequential Mask-Then-Predict for Self-Consistency
The model is forced to predict answers from masked images, creating a self-consistency constraint that ensures explanations are genuinely predictive. The pipeline first predicts a mask from (question, full image), then applies the mask to create X ⊙ M, which is fed back through Pix2Struct for answer prediction. High accuracy under masking implies the mask captured necessary context. The core assumption is that performance on masked inputs correlates with explanation quality. The approach includes specific diagnostic examples showing that highlighting only answer tokens fails the sufficiency test, while context-aware masks maintain accuracy.

## Foundational Learning

- **Concept: Information Bottleneck Principle**
  - **Why needed here:** The paper's core formulation uses IB to formalize the sufficiency-minimality trade-off mathematically. Understanding mutual information I(X; T) and I(T; Y) is essential to interpret the loss terms.
  - **Quick check question:** Can you explain why maximizing I(T; Y) while minimizing I(X; T) creates a compact yet predictive representation?

- **Concept: Late Interaction Mechanisms (ColPali/ColBERT)**
  - **Why needed here:** ColPali's late interaction matching generates the mask prior. Understanding how token-level MaxSim operations produce relevance heatmaps is needed to debug prior quality.
  - **Quick check question:** How does late interaction differ from early fusion in terms of granularity of relevance signals?

- **Concept: Self-Explainable vs. Post-hoc Explainability**
  - **Why needed here:** DocVXQA embeds explanation generation into training, unlike Grad-CAM or attention rollout. This distinction affects how you evaluate success (training dynamics vs. inference-time attribution).
  - **Quick check question:** What is the key architectural difference between a self-explainable model and a post-hoc explainer applied to the same model?

## Architecture Onboarding

- **Component map:** (Question + Document Image) → Pix2Struct Encoder → Features → Mask Head → Mask M → Masked Image → Pix2Struct Decoder → Answer

- **Critical path:**
  1. Input (question + document image) → Pix2Struct encoder → features
  2. Features + decoder attention + position → Mask head → mask M
  3. ColPali generates prior M_p from (question + image)
  4. Apply mask: X_masked = X ⊙ M
  5. X_masked + question → Pix2Struct → predicted answer
  6. Compute L_VXQA = γ·L_MSE(M_p, M) + β·L_L1 + L_CE

- **Design tradeoffs:**
  - **β (minimality weight):** Higher β → sparser masks but risk of insufficient context (Table 2: S+M drops to 0.19 accuracy)
  - **γ (prior alignment weight):** Controls ColPali influence; too high may suppress novel mask discoveries
  - **Threshold k (postprocessing):** k=3 boxes balances conciseness vs. completeness (Table 5)

- **Failure signatures:**
  - Mask overfits to answer tokens only → likely missing token interaction loss (Section 3.4, Figure 7)
  - Noisy/fragmented masks → check continuity loss weight or mask head capacity
  - Accuracy collapse under masking → sufficiency term not dominating; check β/γ balance

- **First 3 experiments:**
  1. **Ablate token interaction loss (S+M vs. S+M+TI):** Reproduce Table 2 to confirm prior regularization effect on your data
  2. **Threshold sweep (Figure 4):** Map accuracy/ANLS/pixel-ratio trade-off to select operating point for your use case
  3. **Swap backbone (Pix2Struct → Donut):** Validate model-agnostic claim by adapting mask head to different encoder outputs (Section 5, Appendix B.4)

## Open Questions the Paper Calls Out

- **Question:** Can the self-explainable framework be effectively adapted to Large Vision-Language Models (LVLMs) with billions of parameters without incurring prohibitive computational costs or losing the specific "sufficiency" property achieved in smaller encoder-decoder models?
  - **Basis in paper:** [explicit] The conclusion states, "Future research will explore the generalization of our method across diverse DocVQA architectures," and the introduction contrasts the method with opaque LVLMs (e.g., GPT-4) that are currently inaccessible.
  - **Why unresolved:** The current implementation relies on Pix2Struct (282M params) and a frozen decoder; scaling the sequential mask-then-predict pipeline to generative LVLMs (7B+ params) presents optimization challenges not addressed in the experiments.
  - **What evidence would resolve it:** Successful application of the information bottleneck objective to a Large VLM (e.g., LLaVA) with maintained accuracy and similar pixel ratios on standard benchmarks.

- **Question:** Can the degradation of the base model's predictive performance on unmasked inputs be mitigated without sacrificing the quality of the explanations?
  - **Basis in paper:** [inferred] Table 1 shows that the "Our Model, Unmasked" accuracy (0.51) is lower than the baseline "Pix2Struct, Unmasked" (0.56), suggesting that the training process for explainability degrades the model's inherent reasoning capabilities.
  - **Why unresolved:** The paper attributes the general performance drop to the difficulty of the task, but the specific drop in the "unmasked" setting implies a trade-off where the model potentially "forgets" how to process full images optimally while learning to process masked ones.
  - **What evidence would resolve it:** A training strategy (e.g., alternate training loops or regularization) that results in an "Unmasked" accuracy statistically indistinguishable from the baseline while producing high-quality masks.

- **Question:** Does the reliance on the ColPali retrieval model as a "prior" constrain the explainability of the final model to the limitations or biases of ColPali's attention mechanism?
  - **Basis in paper:** [inferred] Section 3.4 mentions using ColPali to prevent the mask from overfitting to the answer string, but also notes that ColPali's visualizations "often fail to align with human intuition or provide comprehensive explanations."
  - **Why unresolved:** It is unclear if the DocVXQA training process can successfully "refine" a poor ColPali prior, or if the LMSE loss simply forces the final mask to replicate ColPali's potentially sub-optimal attention patterns.
  - **What evidence would resolve it:** Ablation studies showing the model's ability to generate correct, context-aware masks on samples where the ColPali prior is intentionally corrupted or objectively incorrect.

## Limitations
- Mutual information estimation instability with high-dimensional document images, particularly when masks are very sparse
- ColPali prior quality dependence - explanation quality entirely depends on the quality of ColPali's relevance heatmaps
- Self-consistency proxy assumption - the claim that high masked-image accuracy implies explanation quality is a proxy assumption without direct validation

## Confidence
- **High Confidence:** Model-agnostic architecture (tested with Pix2Struct and Donut), basic quantitative results (accuracy/ANLS/pixel-ratio on standard datasets), human evaluation showing context-awareness superiority over baselines
- **Medium Confidence:** Information bottleneck formulation effectiveness, ColPali prior regularization mechanism, sequential mask-then-predict self-consistency claim
- **Low Confidence:** IB convergence behavior on document images, ColPali prior quality robustness across question types, correlation between masked accuracy and true explanation quality

## Next Checks
1. **IB Loss Stability Test:** Train DocVXQA with varying β values (e.g., 1, 5, 10, 20) while monitoring mask sparsity and accuracy. Plot sufficiency-minimality trade-off curves to verify the IB objective creates stable, interpretable masks rather than oscillating or collapsing.

2. **Prior Corruption Ablation:** Generate synthetic ColPali priors by randomly permuting attention scores or shifting them spatially. Train DocVXQA with corrupted priors and measure degradation in accuracy, ANLS, and human-evaluated context-awareness. This isolates the prior's contribution to explanation quality.

3. **Masked Robustness Correlation:** For each test sample, compute the correlation between (a) accuracy when using the generated mask and (b) human-rated explanation quality scores. A strong positive correlation would validate the self-consistency proxy; weak/no correlation would indicate the need for additional explanation metrics.