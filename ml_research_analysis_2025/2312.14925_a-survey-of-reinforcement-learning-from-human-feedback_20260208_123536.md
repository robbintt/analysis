---
ver: rpa2
title: A Survey of Reinforcement Learning from Human Feedback
arxiv_id: '2312.14925'
source_url: https://arxiv.org/abs/2312.14925
tags:
- learning
- reward
- feedback
- human
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of Reinforcement Learning
  from Human Feedback (RLHF), a technique that learns agent objectives through interactive
  human feedback instead of engineered reward functions. The survey covers fundamental
  concepts including feedback types (comparisons, critiques, improvements, natural
  language), active learning for query selection, reward model training using human
  feedback models, and policy learning algorithms.
---

# A Survey of Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2312.14925
- Source URL: https://arxiv.org/abs/2312.14925
- Reference count: 40
- Primary result: Comprehensive survey of RLHF techniques learning agent objectives through interactive human feedback instead of engineered reward functions

## Executive Summary
This survey provides a comprehensive overview of Reinforcement Learning from Human Feedback (RLHF), a technique that learns agent objectives through interactive human feedback rather than engineered reward functions. The authors examine RLHF across multiple domains with particular focus on control applications and language models, discussing both theoretical advances and practical implementations. The survey synthesizes recent methodological advances such as meta-learning for adaptation, data augmentation techniques, and direct policy optimization approaches. Key challenges addressed include handling diverse feedback types, improving label collection efficiency, and evaluating learned reward functions.

## Method Summary
The survey synthesizes RLHF methodology across multiple domains, focusing on fundamental concepts including feedback types (comparisons, critiques, improvements, natural language), active learning for query selection, reward model training using human feedback models, and policy learning algorithms. The authors examine how different feedback mechanisms translate into training signals for reward models, how active learning can reduce human labeling burden, and how learned reward functions can guide policy optimization. The survey covers both theoretical foundations and practical implementations, with particular attention to the challenges of reward hacking, distribution shift, and the exploration-exploitation dilemma in the context of reward learning.

## Key Results
- RLHF learns agent objectives through interactive human feedback instead of engineered reward functions
- Active learning for query selection can reduce cognitive load on human labelers by maximizing information gain
- Direct Policy Optimization (DPO) can circumvent reward model training by analytically reparameterizing the reward function
- Reward hacking and distribution shift remain critical failure modes requiring mitigation strategies

## Why This Works (Mechanism)

### Mechanism 1: Reward Modeling via Maximum Likelihood
- **Claim:** A neural network can approximate the latent utility function if human feedback follows a probabilistic model like Bradley-Terry
- **Mechanism:** Human provides preference label (e.g., τ₁ ≻ τ₂), system models this as probabilistic choice based on returns, reward model R_ψ trained to maximize likelihood of observed choices
- **Core assumption:** Human choices are Boltzmann-rational; latent reward function is Markovian and representable by function approximator
- **Evidence anchors:** Section 2.3 details Bradley-Terry model for learning utility function R_ψ via maximum likelihood; Abstract states RLHF "learns agent objectives through interactive human feedback"

### Mechanism 2: Active Learning for Query Efficiency
- **Claim:** Reducing cognitive load on human labelers is possible if agent queries comparisons designed to maximize information gain about reward function
- **Mechanism:** System maintains measure of epistemic uncertainty (e.g., ensemble disagreement), selects queries maximizing acquisition function to reduce uncertainty efficiently
- **Core assumption:** Model's uncertainty correlates with informativeness of label; cost of labeling is primary bottleneck
- **Evidence anchors:** Section 4.1 details "Uncertainty" as primary factor in acquisition functions, citing "Ensemble variance" and "Volume removal"; Section 2.5 frames problem as minimizing labeled data via active learning

### Mechanism 3: Direct Policy Optimization (DPO)
- **Claim:** Policy alignment is achievable without explicit reward model training if reward function analytically reparameterized in terms of optimal policy
- **Mechanism:** Standard RLHF learns reward R then policy π; DPO substitutes optimal policy expression π*(y|x) ∝ exp(R(x,y)) into preference loss, allowing direct optimization using preference data
- **Core assumption:** Preference data satisfies Bradley-Terry model assumptions and optimal policy has closed form; system can operate offline without intermediate reward inference
- **Evidence anchors:** Section 6.3 describes DPO as "emerging branch" that "circumvents the reward-learning step" by reparameterization; Corpus:2503.05079 supports theoretical connection between IL and RLHF optimization

## Foundational Learning

### Concept: Markov Decision Processes (MDPs)
- **Why needed here:** RLHF is fundamentally an RL problem (Section 2.1); must understand states, actions, transitions, and returns to frame how "human feedback" replaces traditional "reward function"
- **Quick check question:** Can you explain the difference between the "return" of a trajectory and the "instantaneous reward" provided by an engineered function?

### Concept: Preference Learning (Bradley-Terry Model)
- **Why needed here:** This is mathematical bridge turning qualitative human comparisons into quantitative training signals (Section 2.3)
- **Quick check question:** If human prefers Trajectory A over Trajectory B, how does Bradley-Terry model mathematically translate that into probability regarding rewards of A and B?

### Concept: Actor-Critic Architecture
- **Why needed here:** Survey describes RLHF algorithms often using Actor-Critic schemes (Algorithm 1) where policy (actor) guided by value function (critic), both dependent on learned reward model
- **Quick check question:** In context of Algorithm 1, what is role of "critic" updates versus "reward model" updates?

## Architecture Onboarding

### Component map:
Agent (Policy π_θ) -> Environment -> Store Transitions in B -> Query Generator selects pairs from B -> Ask Oracle -> Oracle returns Label (Preference) -> Store in Dataset D -> Train/Update Reward Model R_ψ using D -> Update Policy π_θ using RL with rewards from R_ψ

### Critical path:
1. Agent explores Environment → Store Transitions in B
2. Query Generator selects pairs from B → Ask Oracle
3. Oracle returns Label (Preference) → Store in Dataset D
4. Train/Update Reward Model R_ψ using D
5. Update Policy π_θ using RL (e.g., PPO) with rewards from R_ψ

### Design tradeoffs:
- **Feedback Type:** Binary comparisons are easier for humans (low cognitive load) but information-sparse; scalar critiques are dense but prone to inconsistency (Section 3.2)
- **Online vs. Offline:** Online updates allow dynamic refinement of reward model but are logistically complex; Offline training risks over-optimization on static, imperfect reward model (Section 5.5.2)

### Failure signatures:
- **Reward Hacking:** Policy exploits spurious correlations in learned reward model (high R_ψ score, low actual utility) (Section 1.1)
- **Overoptimization:** Policy performance on true objective peaks then declines as agent overfits to imperfect reward model proxy (Section 5.2.5)
- **Distribution Shift:** Policy explores states not represented in reward model's training data, leading to unreliable rewards (Section 6.1)

### First 3 experiments:
1. **Ground Truth Baseline:** Train standard RL agent (e.g., PPO) with engineered environment reward to establish upper bound on performance
2. **Reward Model Accuracy:** Train R_ψ on fixed set of preference labels; evaluate accuracy on held-out test set to ensure model can learn human preferences before closing loop
3. **The "RLHF Loop":** Run full pipeline (Algorithm 1) using pairwise comparisons; monitor both reward model accuracy and true environment reward (if available) to detect reward hacking or overoptimization early

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can agents balance exploration-exploitation trade-off between visiting state-action pairs that maximize current rewards versus those that provide maximal information for reward learning?
- **Basis in paper:** [explicit] Section 5.5.3 states steering exploration towards regions beneficial for learning reward function "has not yet been explored" and Section 2.4 highlights this as "novel exploration-exploitation dilemma"
- **Why unresolved:** Most existing methods simply use trajectories generated during standard RL training rather than actively generating trajectories specifically to reduce epistemic uncertainty in reward model
- **Evidence needed:** Algorithm that actively generates high-uncertainty trajectories for labeling, demonstrating superior sample efficiency compared to methods relying solely on on-policy experience

### Open Question 2
- **Question:** Which acquisition function for active query selection is optimal for specific domains and feedback types?
- **Basis in paper:** [explicit] Section 4.1.1 explicitly states that "a better understanding of which acquisition function should be preferred in which situation or domain is still an open question"
- **Why unresolved:** Empirical comparisons show mixed results; for instance, random selection has been shown to perform competitively with uncertainty-based methods in some cases, making theoretical guidance necessary
- **Evidence needed:** Theoretical framework or comprehensive empirical study mapping domain characteristics (e.g., noise type, trajectory complexity) to optimal acquisition strategy

### Open Question 3
- **Question:** How can the human rationality coefficient (β) be estimated practically without access to ground-truth calibration reward function?
- **Basis in paper:** [inferred] Section 5.1.2 notes estimating β is beneficial but current methods "requires a calibration reward function," which is "not feasible for most tasks," leaving practical estimation of this factor a "challenge"
- **Why unresolved:** Paper notes that without calibration, coefficient is otherwise not identifiable, creating dependency on unknown data to model human behavior
- **Evidence needed:** Method for jointly estimating reward function and rationality coefficient solely from observed feedback that proves more robust than assuming fixed rationality level

## Limitations

- Limited empirical validation across domains for newer feedback types like open-ended critiques and dialog-based feedback
- Optimal acquisition function for active learning is domain-dependent with no universal solution identified
- Limited concrete solutions for critical failure modes like reward hacking beyond general mitigation strategies

## Confidence

- **High confidence:** Core RLHF pipeline mechanics, fundamental concepts (MDP formulation, preference learning), relationship between different feedback types and implementation challenges
- **Medium confidence:** Active learning methods for query selection and comparative effectiveness of different feedback types (domain-specific variations acknowledged)
- **Low confidence:** Direct policy optimization approaches and some emerging techniques (relatively recent developments with less established empirical track records)

## Next Checks

1. **Domain transfer validation:** Test same RLHF pipeline across multiple domains (control tasks, language tasks, recommendation systems) to empirically verify claims about domain-dependent effectiveness of different feedback types and acquisition functions

2. **Human factors study:** Conduct controlled experiments comparing cognitive load and labeling consistency across different feedback mechanisms (binary comparisons vs. scalar critiques vs. natural language feedback) to validate claimed tradeoffs

3. **Reward hacking stress test:** Design adversarial scenarios where reward model deliberately overfit to spurious correlations, then test effectiveness of proposed mitigation strategies (KL-regularization, replay buffer relabeling) in preventing reward hacking while maintaining learning efficiency