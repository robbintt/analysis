---
ver: rpa2
title: Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback
  from Large Vision-Language Models
arxiv_id: '2506.12822'
source_url: https://arxiv.org/abs/2506.12822
tags:
- learning
- reward
- task
- timestep
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing reward functions
  for reinforcement learning by proposing ERL-VLM, a method that learns reward functions
  from feedback provided by large vision-language models (VLMs). Unlike previous approaches
  that rely on pairwise comparisons, ERL-VLM queries VLMs for absolute ratings of
  individual trajectories, enabling more expressive feedback and improved sample efficiency.
---

# Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models

## Quick Facts
- arXiv ID: 2506.12822
- Source URL: https://arxiv.org/abs/2506.12822
- Reference count: 40
- Primary result: ERL-VLM achieves higher success rates than sparse environment rewards in manipulation and navigation tasks by using absolute VLM ratings instead of pairwise comparisons

## Executive Summary
This paper addresses the challenge of designing reward functions for reinforcement learning by proposing ERL-VLM, a method that learns reward functions from feedback provided by large vision-language models (VLMs). Unlike previous approaches that rely on pairwise comparisons, ERL-VLM queries VLMs for absolute ratings of individual trajectories, enabling more expressive feedback and improved sample efficiency. The authors also introduce enhancements to address instability issues caused by data imbalance and noisy labels. Experimental results across low-level manipulation tasks, high-level vision-language navigation, and real-world robotic manipulation demonstrate that ERL-VLM significantly outperforms existing VLM-based reward generation methods. Notably, ERL-VLM achieves higher success rates than even the sparse environment reward in certain tasks, highlighting the potential of VLMs as effective substitutes for human feedback in reward learning.

## Method Summary
ERL-VLM learns reward functions by querying large vision-language models for absolute ratings of trajectory segments. The method samples trajectory segments from the replay buffer, queries the VLM with a two-stage prompt (analysis followed by rating), and stores the ratings in a dataset. A reward model is trained using MAE loss with stratified batch sampling and class-weighted loss to handle imbalanced rating distributions. The learned reward function relabels the replay buffer, which is then used to update the policy via off-policy RL algorithms (SAC for continuous control, IQL for discrete actions). The method uses an ensemble of three reward models and supports both binary and multi-class rating schemes depending on task requirements.

## Key Results
- ERL-VLM outperforms pairwise comparison methods by leveraging absolute ratings that provide denser information per query
- The method achieves higher success rates than sparse environment rewards in MetaWorld manipulation and ALFRED navigation tasks
- MAE loss with stratified sampling significantly improves stability and performance compared to vanilla RbRL with cross-entropy loss

## Why This Works (Mechanism)

### Mechanism 1: Absolute ratings provide denser information per query than pairwise preferences
- **Claim:** Rating-based feedback extracts more task-relevant information per VLM query than preference-based comparisons, improving sample efficiency under fixed query budgets.
- **Mechanism:** A single rating (e.g., "bad", "average", "good") conveys absolute quality information, whereas a preference only encodes relative ordering between two segments. Under constrained query budgets (e.g., 75 samples per task in ALFRED), ratings provide global value estimates rather than binary comparisons that convey minimal information about value magnitudes.
- **Core assumption:** VLMs can assign consistent absolute ratings that correlate with task progress; the rating scale is calibrated to task structure.
- **Evidence anchors:**
  - [abstract]: "ERL-VLM queries large vision-language models (VLMs) for absolute ratings of individual trajectories, enabling more expressive feedback and improved sample efficiency"
  - [section 1]: "individual preferences convey little information, requiring a large number of queries to learn meaningful reward functions"
  - [section 5.2]: "ERL-VLM leverages absolute ratings, which offer greater global value than preferences within the same query budget"
  - [corpus]: Weak direct support; related work focuses on rating-based RL optimization but not the comparison mechanism.
- **Break condition:** If VLM ratings become highly inconsistent (stochastic even at temperature=0 with high variance across trials), the information advantage degrades. Paper shows rating accuracies of 65-90% with low variance (Section F.4), suggesting this condition is partially mitigated.

### Mechanism 2: Stratified sampling prevents reward collapse under imbalanced rating distributions
- **Claim:** Stratified batch sampling maintains gradient diversity across all rating classes, preventing the reward model from collapsing to predict only the dominant class during early training.
- **Mechanism:** Early in RL training, "bad" trajectories dominate the replay buffer (Figure 2b). Uniform sampling produces batches containing only one rating class, which prevents proper estimation of rating class boundaries (Eq. 1). Stratified sampling ensures each batch contains samples from all classes, allowing the reward model to learn discriminative boundaries. Weighted loss further compensates for class frequency imbalance.
- **Core assumption:** The reward model capacity is sufficient to distinguish rating classes when gradient diversity is maintained.
- **Evidence anchors:**
  - [section 4, Figure 2c/2d]: Shows that uniform sampling leads to unstable estimated returns and poor success rates, while stratified sampling stabilizes learning
  - [section 4]: "multiple training batches may contain samples from only a single rating class, which disrupts the effective selection of rating class boundaries"
  - [corpus]: "Performance Optimization of Ratings-Based Reinforcement Learning" (arXiv:2501.07755) explores optimization methods for RbRL but doesn't specifically address stratified sampling for imbalance.
- **Break condition:** If the replay buffer lacks sufficient samples from minority classes (e.g., no "good" trajectories early in training), stratified sampling cannot populate batches evenly. Paper doesn't explicitly address cold-start scenarios.

### Mechanism 3: MAE loss provides robustness to VLM label noise without requiring noise rate estimation
- **Claim:** Mean Absolute Error loss reduces sensitivity to noisy VLM ratings compared to Cross-Entropy, avoiding the need to estimate noise rates required by label smoothing approaches.
- **Mechanism:** VLMs can produce inconsistent ratings due to hallucinations or stochastic outputs. Cross-Entropy with label smoothing (Eq. 2) requires specifying a fixed noise rate `r`, which is difficult to estimate for multi-class ratings with unknown VLM error patterns. MAE loss (Eq. 3) is provably more robust to label noise as it penalizes absolute deviations rather than log-probabilities, reducing the influence of outliers without requiring noise rate specification.
- **Core assumption:** VLM noise is symmetric or bounded; MAE robustness properties from classification (Ghosh et al., 2017) transfer to the rating-based RL formulation.
- **Evidence anchors:**
  - [section 4]: "we adopt the Mean Absolute Error (MAE) as the training objective, as it is provably more robust to label noise compared to Cross Entropy (Ghosh et al., 2017)"
  - [section 5.4, Figure 5]: Ablation shows MAE provides the most significant improvement over vanilla RbRL
  - [section 5.4, Figure 6a]: Label smoothing was "ineffective in our experiment" for handling corrupted labels
  - [corpus]: No direct corpus evidence for MAE in rating-based RL specifically.
- **Break condition:** If VLM noise is systematic (e.g., consistently overrating certain trajectory types), MAE robustness may not apply. The paper assumes noise is approximately symmetric.

## Foundational Learning

- **Concept: Rating-based Reinforcement Learning (RbRL)**
  - **Why needed here:** ERL-VLM extends the RbRL framework (White et al., 2024) which models reward learning as matching predicted rating distributions to teacher-assigned ratings via cross-entropy loss over class probabilities (Eq. 1-2).
  - **Quick check question:** Can you explain why the probability model in Eq. 1 uses rating class boundaries rather than direct regression?

- **Concept: Off-policy RL with learned rewards (SAC, IQL)**
  - **Why needed here:** The learned reward function relabels the replay buffer (Algorithm 1, line 20), and standard off-policy algorithms (SAC for continuous control, IQL for discrete/structured actions) optimize the policy using these relabeled rewards.
  - **Quick check question:** Why does reward relabeling require replay buffer storage of trajectories rather than just (s, a, r, s') tuples?

- **Concept: VLM prompting for evaluative feedback**
  - **Why needed here:** The two-stage prompt design (analysis → rating) elicits reasoning before evaluation, improving rating quality for complex compositional tasks (Tables 6-8).
  - **Quick check question:** What information does the analysis stage provide that a single-step rating prompt might miss?

## Architecture Onboarding

- **Component map:**
  - VLM Teacher -> Rating Dataset D -> Reward Model -> Replay Buffer -> Policy Network -> Environment
  - (Policy Network -> Environment -> Replay Buffer forms the RL loop)

- **Critical path:**
  1. Agent collects trajectories → Replay Buffer
  2. Every K steps: sample N segments → query VLM → Rating Dataset
  3. Train reward model with stratified batches + MAE loss (Eq. 3)
  4. Relabel entire replay buffer with updated reward model
  5. Train policy with off-policy RL on relabeled data

- **Design tradeoffs:**
  - **Number of rating classes (n):** Paper finds n=2 optimal for binary-evaluable tasks (Sweep Into), n=3 for continuous-progress tasks (Drawer Open). n=4 degrades performance due to increased ambiguity (Figure 6b/c).
  - **Segment length H:** Paper uses H=1 for MetaWorld, H=8-15 for ALFRED. Longer segments provide context but increase VLM token costs and may dilute credit assignment.
  - **Query budget allocation:** Fixed budget (10K MetaWorld, 1.5K ALFRED) distributed across training; early queries suffer from "bad" trajectory dominance.

- **Failure signatures:**
  - **Reward collapse:** Predicted rewards cluster around single value → check batch composition for rating diversity
  - **Policy stagnation:** Success rate plateaus early → check if VLM ratings are all "bad" (insufficient positive examples)
  - **Noisy reward curves:** High variance in reward along expert trajectories → check VLM rating consistency (Section F.4)

- **First 3 experiments:**
  1. **Sanity check rating consistency:** Query VLM 10 times on identical expert trajectory segments at temperature=0. Verify rating distribution matches Figure 12/13 patterns. If variance is high, reconsider prompt design or rating class definitions.
  2. **Ablate single enhancement:** Train with vanilla RbRL (uniform sampling + CE loss) vs. only stratified sampling vs. only MAE. Reproduce Figure 5 pattern to verify implementation correctness before full experiments.
  3. **Monitor batch composition:** During early training, log the per-batch rating distribution. If any batch contains <2 samples from a minority class, stratified sampling may be failing (check sampling implementation).

## Open Questions the Paper Calls Out

- **Can integrating additional sensing modalities (e.g., tactile, audio) enhance the accuracy and reliability of feedback from foundation models beyond visual observations alone?**
  - **Basis in paper:** Impact Statement: "Future research can explore integrating additional sensing modalities to further enhance the accuracy and reliability of feedback from foundation models..."
  - **Why unresolved:** The current method relies exclusively on visual observations and descriptive actions to analyze behavior.
  - **What evidence would resolve it:** Experiments comparing policy performance and reward accuracy when VLM inputs are augmented with tactile or audio data versus visual-only baselines.

- **To what extent can refined prompt engineering mitigate the ambiguity and performance degradation observed when increasing the number of rating classes (n > 3)?**
  - **Basis in paper:** Section 5.4: "We anticipate that more refined prompt engineering, with carefully crafted rating descriptions, could further exploit the potential of VLMs for feedback."
  - **Why unresolved:** The study found that increasing rating classes to n=4 degraded performance due to ambiguity, but only standard prompts were tested.
  - **What evidence would resolve it:** Ablation studies using detailed, task-specific prompt descriptions for 4+ rating classes that achieve stable training and superior performance compared to n=3.

- **How do inherent biases in large foundation models propagate into the reward functions and policies of RL agents trained via ERL-VLM?**
  - **Basis in paper:** Impact Statement: "...it inherits their inherent biases, which could propagate into RL agents. This raises important considerations regarding robustness and safety..."
  - **Why unresolved:** The paper focuses on task success rates but does not quantify or analyze the transfer of specific societal or dataset biases from the VLM to the agent.
  - **What evidence would resolve it:** Analysis of agent behavior in scenarios designed to trigger VLM biases, demonstrating correlation between VLM output skew and agent policy deviations.

## Limitations

- Effectiveness of MAE loss for VLM-generated ratings hasn't been validated across different VLM providers or rating scales
- Stratified sampling's impact on cold-start scenarios where minority classes may be absent early in training remains unclear
- The assumption that VLM noise is symmetric may not hold across all task domains

## Confidence

- **High confidence:** Mechanism 1 (absolute ratings provide denser information) - supported by strong experimental evidence and theoretical consistency
- **Medium confidence:** Mechanism 2 (stratified sampling prevents collapse) - empirical support from ablation studies but implementation details remain unclear
- **Medium confidence:** Mechanism 3 (MAE robustness) - theoretical justification exists but VLM-specific noise patterns weren't thoroughly characterized

## Next Checks

1. Perform cross-VLM validation: test ERL-VLM with different VLM providers (e.g., GPT-4V, Claude 3) to verify MAE robustness isn't model-specific
2. Analyze cold-start behavior: deliberately initialize training with artificially imbalanced rating distributions to test stratified sampling edge cases
3. Quantify VLM noise characteristics: systematically measure VLM rating consistency across tasks to validate the symmetric noise assumption underlying MAE's effectiveness