---
ver: rpa2
title: Parameter Averaging in Link Prediction
arxiv_id: '2510.25361'
source_url: https://arxiv.org/abs/2510.25361
tags:
- ensemble
- averaging
- training
- prediction
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces parameter averaging, specifically weighted
  averaging, for knowledge graph embedding models to improve generalization in link
  prediction tasks. Instead of training multiple models as in traditional ensemble
  methods, the approach maintains a running average of model parameters during training,
  capturing diverse solutions in weight space.
---

# Parameter Averaging in Link Prediction

## Quick Facts
- arXiv ID: 2510.25361
- Source URL: https://arxiv.org/abs/2510.25361
- Reference count: 40
- Parameter averaging (SWA/ASWA) outperforms ensemble baselines like SnapE in link prediction tasks

## Executive Summary
This paper introduces parameter averaging, specifically weighted averaging, for knowledge graph embedding models to improve generalization in link prediction tasks. Instead of training multiple models as in traditional ensemble methods, the approach maintains a running average of model parameters during training, capturing diverse solutions in weight space. The authors propose Adaptive Stochastic Weight Averaging (ASWA), which selectively updates the parameter ensemble based on validation performance to avoid underfitting and overfitting. Extensive experiments on link prediction, literal-augmented models, and multi-hop query answering demonstrate that both SWA and ASWA consistently outperform conventional training and state-of-the-art ensemble approaches like SnapE.

## Method Summary
The method maintains a running average of model parameters during training rather than training multiple independent models. SWA implements this as an unconditional parameter averaging from a specified start epoch, while ASWA adds adaptive selection by comparing validation performance to decide when to update the ensemble. The approach works with standard KGE models (DistMult, ComplEx, QuatE, ConvE, TuckER) trained with Adam, requiring no architectural modifications and maintaining the same memory and inference costs as single models.

## Key Results
- SWA and ASWA consistently outperform conventional training and SnapE ensemble baselines across multiple datasets
- ASWA shows further improvements by mitigating overfitting through its adaptive update strategy
- The method achieves better generalization across diverse datasets while maintaining identical memory and inference costs to single models
- Notably, multi-hop query answering benefits from SWA/ASWA except for path queries (2p/3p) which degrade with SWA

## Why This Works (Mechanism)

### Mechanism 1: Gradient Noise Reduction via Parameter Averaging
- Claim: Maintaining a running average of model parameters smooths noisy gradient updates around minima, improving generalization.
- Mechanism: Mini-batch training introduces variance in gradient estimates, causing parameters to oscillate around optima. By averaging parameters across epochs, these oscillations are dampened, converging toward a more stable solution with broader optima characteristics.
- Core assumption: The optimization trajectory visits diverse but connected regions of the loss landscape that share similar training accuracy.
- Evidence anchors:
  - [abstract] "maintains a running average of model parameters during training, capturing diverse solutions in weight space"
  - [Section 3] Defines SWA update: ΘSWA ← (ΘSWA · n_models + Θ) / (n_models + 1)
  - [Section 6] "maintaining a running unweighted average of parameters becomes particularly useful around a minima by means of reducing the noise in the gradients of loss w.r.t. parameters that is caused by the mini-batch training"
  - [corpus] Souper-Model paper confirms weight averaging ("model souping") improves LLM performance across domains
- Break condition: If training epochs are insufficient or start epoch is too early, the average includes underfit parameters, degrading performance below baseline.

### Mechanism 2: Adaptive Selection Via Validation-Guided Ensemble Updates
- Claim: Selectively updating the parameter ensemble based on validation performance prevents both underfitting (from averaging too early) and overfitting (from averaging degraded parameters).
- Mechanism: ASWA implements three update types: (1) hard update—reset ensemble to current model when running model outperforms; (2) soft update—include current parameters in average when look-ahead improves validation; (3) rejection—skip update when validation degrades. This combines SWA's averaging with early stopping's rejection logic.
- Core assumption: Validation MRR reliably signals generalization quality, and optimal parameters may exist in non-adjacent epoch intervals rather than a contiguous sequence.
- Evidence anchors:
  - [abstract] "selectively updates the running average of the ensemble model parameters only when the generalization performance improves on a validation dataset"
  - [Algorithm 1, Lines 7-21] Implements hard/soft update logic with val_Θ comparisons
  - [Section 4] "ASWA can be seen as a combination of SWA with the early stopping technique"
  - [corpus] Weak direct evidence—no comparable adaptive averaging for KGE found; related checkpoint merging work (Parameter-Efficient Checkpoint Merging) uses metrics-weighted averaging but in different context
- Break condition: If validation set is small, noisy, or unrepresentative, adaptive selection overfits to validation artifacts rather than true generalization.

### Mechanism 3: Single-Trajectory Ensemble Diversity
- Claim: A single training run produces sufficient parameter diversity to approximate multi-model ensemble benefits without training multiple independent models.
- Mechanism: During optimization, parameters traverse a path through weight space that visits multiple near-optimal solutions. Temporal snapshots capture diversity comparable to training separate models, but at identical memory and inference cost to a single model.
- Core assumption: The loss landscape contains connected pathways between optima with near-constant training accuracy (mode connectivity).
- Evidence anchors:
  - [abstract] "Instead of training multiple models as in traditional ensemble methods, the approach maintains a running average of model parameters during training"
  - [Section 2] Cites Garipov et al. [12]: "optima of neural networks are connected by simple pathways having near constant training accuracy"
  - [Section 1] "at test time, the memory and running time requirements of using SWA parameter ensemble are identical to the requirements of using a single neural network"
  - [corpus] Neighbor paper on "Breaking Rank Bottlenecks" addresses complementary KGE architecture issues, not averaging
- Break condition: If learning rate schedule causes rapid convergence to narrow minima, trajectory diversity is insufficient for ensemble benefits.

## Foundational Learning

- Concept: **Knowledge Graph Embedding Scoring Functions**
  - Why needed here: Understanding φΘ(h,r,t) explains what parameters are being averaged (entity/relation embeddings, batch norm parameters).
  - Quick check question: For a triple (Paris, located_in, France), what does a high score from φΘ indicate versus a low score?

- Concept: **Stochastic Gradient Descent Dynamics**
  - Why needed here: SWA effectiveness depends on understanding why SGD creates oscillating trajectories around minima.
  - Quick check question: Why does decreasing learning rate typically improve final accuracy, and how does this relate to what SWA achieves through averaging?

- Concept: **Filtered Ranking Metrics (MRR, Hits@k)**
  - Why needed here: The paper evaluates using filtered MRR/Hits@1,3,10; understanding these metrics is essential for interpreting results.
  - Quick check question: If a model ranks the correct entity at position 5 for a query, what are the MRR contribution and Hits@10 status?

## Architecture Onboarding

- Component map:
  Running model (Θ_i) -> Validation evaluator -> Update controller -> Parameter ensemble (Θ_SWA/ASWA)

- Critical path:
  1. Initialize: Θ_0 → both running model and ensemble
  2. Per-epoch loop: Train running model → validate → update ensemble per SWA or ASWA rules
  3. SWA: Unconditionally average; ASWA: Compare val_Θ vs val_ASWA, execute hard/soft/reject
  4. Test time: Deploy final Θ_ASWA with standard single-model inference

- Design tradeoffs:
  - SWA simplicity vs. ASWA robustness: SWA requires manual start epoch tuning; ASWA auto-adapts but adds O(|D_val|) compute per epoch
  - Memory vs. diversity: Traditional ensembles scale linearly with K models; both SWA/ASWA use constant memory
  - Hard vs. soft update frequency: Frequent hard resets reduce averaging benefits; excessive soft updates may include degraded parameters

- Failure signatures:
  - **SWA underfitting**: Training MRR high, validation MRR low, ensemble worse than final epoch model (start epoch too early)
  - **SWA no improvement**: Ensemble ≈ final model performance (start epoch too late, insufficient averaging window)
  - **ASWA validation overfitting**: Validation MRR improves while test MRR degrades (validation set too small/noisy)
  - **Path query degradation**: Multi-hop 2p/3p queries worse with SWA than baseline (see Table 7: ComplEx+SWA 2p drops 0.007→0.002)

- First 3 experiments:
  1. **Reproduce FB15K-237 baseline comparison**: Train ComplEx (d=128, 300 epochs) with conventional training, SWA (start=epoch 150), ASWA. Report MRR, Hits@1, Hits@10. Expected: ASWA > SWA > baseline per Table 3.
  2. **Ablate start epoch sensitivity**: Run SWA on YAGO3-10 with start epochs at 25%, 50%, 75% of total training. Plot validation MRR curve to show sensitivity that motivates ASWA.
  3. **Validate multi-hop degradation pattern**: Test DistMult+SWA on FB15K-237 multi-hop queries (2p, 3p, 3i). Confirm whether path queries (2p/3p) degrade while intersection queries (3i) improve, as observed in Table 7. Investigate whether ASWA mitigates this pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can task-aware scheduling strategies for parameter averaging be designed to dynamically adjust based on query complexity or domain characteristics?
- Basis: [explicit] The conclusion explicitly identifies exploring task-aware scheduling strategies as future work.
- Why unresolved: The current ASWA method uses a generic adaptive schema governed solely by validation performance, without distinguishing between task types.
- What evidence would resolve it: A modified ASWA approach that dynamically weights parameters based on the complexity of multi-hop queries or specific domain features.

### Open Question 2
- Question: Does integrating uncertainty estimation into the averaging process improve model robustness and calibration in noisy or adversarial settings?
- Basis: [explicit] The conclusion proposes integrating uncertainty estimation to enhance robustness in noisy settings.
- Why unresolved: The current work evaluates performance using accuracy metrics (MRR, Hits@k) but does not assess model calibration or uncertainty.
- What evidence would resolve it: Experiments demonstrating that ASWA with uncertainty estimation outperforms standard ASWA on calibration metrics (e.g., Expected Calibration Error) or adversarial attacks.

### Open Question 3
- Question: Can the validation overhead of ASWA be reduced for massive knowledge graphs without compromising its generalization benefits?
- Basis: [inferred] Section 4 notes that the training time overhead of ASWA is linear in the size of the validation dataset.
- Why unresolved: While manageable for benchmark datasets, computing full validation performance at every epoch may be prohibitive for industrial-scale graphs.
- What evidence would resolve it: A variation of ASWA using sampled or approximate validation strategies that maintains predictive performance while reducing training time.

## Limitations

- The paper doesn't clearly specify the SWA start epoch across experiments, creating ambiguity about whether warmup periods are needed for specific datasets
- The multi-hop query degradation with SWA (particularly for 2p/3p queries) lacks thorough investigation into whether this is a general pattern or dataset-specific artifact
- The adaptive selection mechanism in ASWA relies heavily on validation MRR quality, which could overfit to small or noisy validation sets

## Confidence

- **High confidence**: The core mechanism of parameter averaging improving generalization through gradient noise reduction (supported by Souper-Model evidence in LLMs)
- **Medium confidence**: The superiority of ASWA over SWA on most datasets (consistent across link prediction, literal-augmented, and multi-hop tasks)
- **Medium confidence**: The claim that ASWA mitigates overfitting better than conventional early stopping (supported by validation performance patterns but lacking explicit overfitting quantification)

## Next Checks

1. **Ablation on validation set size**: Run ASWA with varying validation set fractions (5%, 10%, 20%) on FB15K-237 to quantify how validation quality affects the adaptive selection mechanism's effectiveness versus SWA.

2. **Multi-hop query architecture analysis**: For ComplEx and DistMult, analyze whether 2p/3p query degradation correlates with specific relation patterns or graph structures that might reveal the mechanism behind this counterintuitive result.

3. **SWA start epoch sensitivity sweep**: Systematically vary the SWA start epoch (0%, 25%, 50%, 75% of total epochs) on YAGO3-10 and plot validation MRR trajectories to quantify the sensitivity that motivates ASWA's adaptive approach.