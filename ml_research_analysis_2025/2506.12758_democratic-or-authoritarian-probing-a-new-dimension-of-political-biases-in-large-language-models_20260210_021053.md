---
ver: rpa2
title: Democratic or Authoritarian? Probing a New Dimension of Political Biases in
  Large Language Models
arxiv_id: '2506.12758'
source_url: https://arxiv.org/abs/2506.12758
tags:
- democracy
- leader
- autocracy
- electoral
- authoritarian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines political biases in LLMs along the democracy-authoritarianism
  spectrum using three probing methods: an adapted F-scale for authoritarian attitudes,
  a FavScore metric for leader favorability, and role-model generation tasks. Across
  eight diverse LLMs in English and Mandarin, models generally show pro-democratic
  leanings in English but exhibit reduced differentiation between regime types in
  Mandarin.'
---

# Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models

## Quick Facts
- arXiv ID: 2506.12758
- Source URL: https://arxiv.org/abs/2506.12758
- Reference count: 40
- Primary result: LLMs exhibit pro-democratic biases in English but show reduced differentiation and increased favorability toward authoritarian figures when prompted in Mandarin

## Executive Summary
This study systematically examines political biases in large language models along the democracy-authoritarianism spectrum using three distinct probing methods. The research reveals that while models generally favor democratic values in English, this differentiation significantly diminishes when prompted in Mandarin. The findings demonstrate systematic geopolitical biases that vary by language, with potential implications for how LLMs shape global political perspectives across different linguistic contexts.

## Method Summary
The study employs three probing methods to assess political biases: an adapted F-scale for authoritarian attitudes, a FavScore metric for leader favorability, and role-model generation tasks. Researchers tested eight diverse LLMs in both English and Mandarin using temperature=0 for reproducibility, with JSON-enforced outputs and Gemini 2.5 Flash as an LLM-judge for validation. The methodology includes 30 F-scale statements adapted from Adorno et al. (1950), 39 survey questions adapted from established public opinion instruments for 197 world leaders, and role-model prompts for 222 nationalities with V-Dem Regime Dataset classification.

## Key Results
- F-scale scores are systematically higher in Mandarin than in English across all models, indicating more authoritarian attitudes
- Leader evaluations via FavScore show greater separation between democratic and authoritarian leaders in English (Wasserstein Distance 0.14-0.24) than Mandarin (0.04-0.15)
- In non-political contexts, LLMs frequently cite authoritarian figures as role models, with 35.9% of English political role models classified as authoritarian (42.0% in Mandarin)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-of-interaction conditions political bias expression in LLMs
- Mechanism: Training corpora contain language-specific ideological distributions; Mandarin corpora may include more state-aligned content, while English corpora emphasize democratic discourse norms. When prompted in different languages, models retrieve from language-partitioned representational spaces that carry these distinct distributions.
- Core assumption: Language-specific training data reflects geopolitical discourse patterns (not proven; inferred from results).
- Evidence anchors:
  - [abstract] "models generally favor democratic values and leaders, but exhibit increased favorability toward authoritarian figures when prompted in Mandarin"
  - [Section 5.1] "F-scale scores are systematically higher in Mandarin than in English across all models"
  - [corpus] "The Language You Ask In: Language-Conditioned Ideological Divergence in LLM Analysis of Contested Political Documents" (FMR 0.57) directly supports language-conditioned bias
- Break condition: If models were trained on ideologically balanced multilingual data with explicit debiasing, language effects would diminish; if translation alone caused differences, we'd see symmetric patterns across model origins.

### Mechanism 2
- Claim: Implicit political preferences surface in ostensibly neutral contexts via associative retrieval
- Mechanism: Role model prompts activate semantic associations between nationalities and prominent figures. These associations encode training-data prominence rather than normative evaluation, causing models to list historically significant authoritarian figures without explicit endorsement.
- Core assumption: Models conflate historical prominence with normative worthiness as role models.
- Evidence anchors:
  - [abstract] "models are found to often cite authoritarian figures as role models, even outside explicit political contexts"
  - [Section 5.3] "LLMs often appear to adopt a looser interpretation, treating ['role model'] as a proxy for historical significance or leadership stature"
  - [corpus] Limited direct corpus support; "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes" addresses implicit bias but not role-model context specifically
- Break condition: If prompts included explicit normative definitions of "role model," authoritarian citation rates would decrease (partially tested in Section B.4; rates reduced but not eliminated).

### Mechanism 3
- Claim: FavScore captures regime-type favorability via aggregated opinion-survey adaptations
- Mechanism: Survey instruments (Pew, ANES, Eurobarometer) measure human leader approval; adapting these to LLM prompts forces models into evaluative stances. Averaging across 39 questions per leader reduces noise and reveals systematic favorability differentials between democratic and authoritarian leaders.
- Core assumption: Survey questions validly transfer from human political psychology to LLM evaluation contexts.
- Evidence anchors:
  - [Section 3.2] "FavScore, a metric that measures the response to a set of 39 questions that are relevant for leader perception, adapted from established public opinion instruments"
  - [Section 5.2] "In English, models consistently assign higher average FavScores to democratic leaders than to authoritarian ones"
  - [corpus] "Measuring Political Preferences in AI Systems: An Integrative Approach" (FMR 0.53) supports survey-based measurement transfer
- Break condition: If models refused to evaluate or hedged uniformly, FavScore distributions would converge; high hedging rates in Claude 3.7 Sonnet Mandarin (43.2%) partially confirm this.

## Foundational Learning

- Concept: F-scale psychometrics and authoritarian personality measurement
  - Why needed here: The paper adapts Adorno's F-scale (30 statements across 9 categories) to probe LLM authoritarian tendencies. Understanding construct validity and limitations (e.g., acquiescence bias, dated items) is essential for interpreting results.
  - Quick check question: Why does the paper use a 6-point Likert scale without a neutral midpoint, and what ablation tested this choice?

- Concept: Wasserstein distance (Earth Mover's Distance) for distribution comparison
  - Why needed here: FavScore analysis uses Wasserstein distance to quantify separation between favorability distributions for democratic vs. authoritarian leaders, capturing differences in central tendency, variance, and shape beyond simple mean comparisons.
  - Quick check question: Why is Wasserstein distance more informative than comparing only mean FavScores between regime types?

- Concept: V-Dem Regime of the World classification framework
  - Why needed here: Leader classification into Closed Autocracy, Electoral Autocracy, Electoral Democracy, and Liberal Democracy grounds the analysis in established political science. The binary grouping (authoritarian vs. democratic) follows comparative politics conventions.
  - Quick check question: How does the paper handle leaders from countries with ambiguous or transitional regime classifications?

## Architecture Onboarding

- Component map: F-scale value probing → 6-point Likert responses averaged across 30 items → FavScore leader evaluation → 39 questions per leader adapted from surveys, rescaled to [-1, +1] → Role model probing → nationality-prompted figure extraction → LLM-judge classification using V-Dem data → democratic/authoritarian categorization

- Critical path: FavScore execution (197 leaders × 39 questions × 8 models × 2 languages = ~123,000 API calls) dominates computational budget. LLM-judge validation (Gemini 2.5 Flash) scales with unique figures extracted (~12,000 classifications). Temperature=0 ensures reproducibility.

- Design tradeoffs: Forced-choice Likert scales prevent midpoint-hedging but may push models toward stances they'd otherwise avoid. Single-prompt execution per leader (budget-constrained) limits variance estimation; F-scale uses 3 runs for stability. LLM-as-judge introduces potential judge bias (mitigated via cross-validation with alternative judges, κ≈0.80).

- Failure signatures: High refusal rates invalidate comparisons—Claude 3.7 Sonnet (33% refusal) and Grok 3 Beta Mandarin (68.5% refusal) results require cautious interpretation. Semantic hedging (Claude Mandarin: 43.2%) indicates models avoiding evaluative commitment. Regime-type refusal imbalance would indicate systematic bias (tested; not observed).

- First 3 experiments:
  1. Replicate F-scale on a single model (e.g., Llama-4-Maverick) in English and Mandarin with 5 runs each to establish baseline variance and confirm the Mandarin-elevated pattern.
  2. Pilot FavScore on 10 diverse leaders (5 democratic, 5 authoritarian by V-Dem) with 3 prompt paraphrases each to validate question-wording robustness before full execution.
  3. Test role-model probing on 20 countries with explicit pro-social definition vs. baseline prompt to quantify reduction in authoritarian citations (per Section B.4 methodology).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do democratic-authoritarian biases manifest in languages beyond English and Mandarin, particularly in low-resource languages?
- Basis in paper: [explicit] The authors state future research should "explore this phenomenon across more languages" as the current scope "necessarily limits generalizability."
- Why unresolved: Budget constraints limited the study to two languages.
- What evidence would resolve it: Applying the F-scale, FavScore, and Role-Model probing framework to a diverse set of low-resource languages.

### Open Question 2
- Question: What are the concrete effects of these biases on downstream applications and real-world user interactions?
- Basis in paper: [explicit] The Conclusion calls for research to "examine its effects on downstream applications," and Limitations note prompts "may not fully reflect... real-world user interactions."
- Why unresolved: The study assesses raw model outputs via API rather than longitudinal user behavior or specific application performance.
- What evidence would resolve it: User studies measuring opinion formation or task performance in simulated real-world scenarios involving political content.

### Open Question 3
- Question: To what extent is the shift toward authoritarian favorability in Mandarin driven by training data composition versus translation artifacts?
- Basis in paper: [inferred] The paper notes Mandarin outputs "may reflect state-aligned content or translation effects" but does not isolate the cause.
- Why unresolved: The methodology measures the outcome (bias) but does not perform causal attribution for the language-specific shift.
- What evidence would resolve it: Ablation studies comparing models trained on controlled corpora and analysis of semantic shifts during the translation process of the prompts.

## Limitations
- High refusal rates for certain models (Claude 3.7 Sonnet at 33%, Grok 3 Beta Mandarin at 68.5%) create data gaps that may skew comparisons
- The F-scale's psychometrics were developed for human personality assessment, raising questions about construct validity when applied to LLMs
- Observed language-conditioned biases may reflect corpus composition differences rather than inherent model preferences

## Confidence
- **High Confidence**: Models show systematically higher F-scale scores (more authoritarian attitudes) when prompted in Mandarin versus English across all tested models.
- **Medium Confidence**: English-language prompts produce greater separation between democratic and authoritarian leader favorability distributions than Mandarin prompts, as measured by Wasserstein distance.
- **Medium Confidence**: LLMs frequently cite authoritarian figures as role models even in non-political contexts, though this may reflect historical prominence retrieval rather than normative endorsement.
- **Low Confidence**: The magnitude of language-conditioned bias differences is precisely quantified, given the uncontrolled variables in multilingual training data composition.

## Next Checks
1. **Ablation study on prompt definition**: Systematically test the role-model task with explicit normative definitions (e.g., "positive role models who promote human rights") versus baseline prompts across 50 countries to quantify reduction in authoritarian citations and assess whether the effect is semantic or definitional.
2. **Corpus analysis validation**: Conduct a controlled experiment where identical political content is translated into English and Mandarin, then measure LLM responses to determine if language alone (independent of training corpus) drives bias differences.
3. **Longitudinal tracking**: Replicate the F-scale and FavScore experiments quarterly over one year using the same model versions to assess whether observed biases persist, diminish, or shift as models are updated and training corpora evolve.