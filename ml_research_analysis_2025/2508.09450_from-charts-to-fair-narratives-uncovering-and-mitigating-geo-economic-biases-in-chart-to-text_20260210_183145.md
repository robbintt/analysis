---
ver: rpa2
title: 'From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases
  in Chart-to-Text'
arxiv_id: '2508.09450'
source_url: https://arxiv.org/abs/2508.09450
tags:
- chart
- bias
- opinion
- positive
- country
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates geo-economic bias in chart-to-text generation
  by large vision-language models (VLMs). Using 6,000 chart-country pairs across six
  models, it shows that VLMs tend to produce more positive summaries for high-income
  countries than for low-income ones, even when the underlying chart is identical.
---

# From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text

## Quick Facts
- **arXiv ID:** 2508.09450
- **Source URL:** https://arxiv.org/abs/2508.09450
- **Authors:** Ridwan Mahbub; Mohammed Saidul Islam; Mir Tafseer Nayeem; Md Tahmid Rahman Laskar; Mizanur Rahman; Shafiq Joty; Enamul Hoque
- **Reference count:** 40
- **Primary result:** VLMs produce more positive summaries for high-income countries than low-income ones, even with identical chart data.

## Executive Summary
This paper investigates geo-economic bias in chart-to-text generation by large vision-language models (VLMs). Using 6,000 chart-country pairs across six models, it shows that VLMs tend to produce more positive summaries for high-income countries than for low-income ones, even when the underlying chart is identical. For example, GPT-4o-mini showed statistically significant bias in 44.52% of country pairs, while Gemini-1.5-Flash showed it in 16.10%. Bias persisted across all chart trend types except negative trends. A prompt-based mitigation using positive distractors reduced bias in four models, with GPT-4o-mini showing a 20.34% reduction, but bias remained prevalent overall. Human evaluation confirmed the reliability of the VLM judges. The study highlights the need for stronger debiasing strategies in chart-based AI systems.

## Method Summary
The study used 100 anonymized charts from the VisText dataset, paired with 60 country names across three World Bank-defined income groups (20 per group). Six VLMs generated summaries for each chart-country pair (6,000 total), with sentiment ratings assigned by independent judge models (GPT-4o, Gemini-1.5-Pro). Wilcoxon Signed-Rank Tests identified statistically significant bias in 1,770 country pairs. A prompt-based mitigation using positive distractors was tested on four models, reducing bias by 20.34% in GPT-4o-mini but increasing it in two others.

## Key Results
- GPT-4o-mini showed statistically significant bias in 44.52% of country pairs
- Bias was prevalent across all chart trend types except negative trends
- Positive distractor mitigation reduced bias by 20.34% in GPT-4o-mini but increased it in Claude-3-Haiku and Qwen2-VL
- Human evaluation confirmed VLM judge reliability with 0.967 Pearson correlation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A prompt-based intervention using a positive distractor can reduce measured geo-economic bias in VLM chart-to-text generation for some models.
- **Mechanism:** The intervention adds a positively framed sentence to the prompt, steering the model's generation away from negative, stereotype-linked language for low-income countries. This is a form of adversarial triggering or soft prompting that alters the token probability distribution for the subsequent generation.
- **Core assumption:** The model's bias is partially manifest in its immediate autoregressive word choices, which can be influenced by preceding context. This assumes the bias is not solely fixed in the model's deeper parametric knowledge retrieval.
- **Evidence anchors:** [abstract] The paper states, "We further explore inference-time prompt-based debiasing techniques using positive distractors but find them only partially effective." [Section 3.2] Details the method: adding "The country is working very hard to improve the sector..." to the prompt. [Section 4.4, Table 3] Shows a 20.34% reduction in statistically significant biased pairs for GPT-4o-mini.
- **Break condition:** The mechanism fails when the model's bias is more fundamental than surface-level generation, as seen with Claude-3-Haiku and Qwen2-VL, where bias increased after the intervention.

### Mechanism 2
- **Claim:** VLMs exhibit geo-economic bias by generating more positive narratives for high-income countries compared to low-income ones, even when the underlying data is identical.
- **Mechanism:** The model associates country names with socio-economic priors from its training data. These priors act as a confounder, influencing the interpretation of chart data. When generating text, the model conditionally selects more favorable adjectives and framings for countries it "perceives" as high-income.
- **Core assumption:** This is not a form of logical reasoning but a probabilistic association. The assumption is that these associations are embedded in the model's weights as a result of training on vast, imperfect corpora.
- **Evidence anchors:** [abstract] "Our analysis reveals that existing VLMs tend to produce more positive descriptions for high-income countries compared to middle- or low-income countries, even when country attribution is the only variable changed." [Section 1, Figure 1] Provides a clear qualitative example of divergent opinions for Australia vs. South Sudan. [Section 4.2, Table 2] Shows statistically significant bias (p < 0.05) in GPT-4o-mini, Claude-3-Haiku, Phi-3.5, and LLaVA-NeXT-7B when comparing high vs. low-income groups.
- **Break condition:** This mechanism should not be taken as a universal rule for all VLMs or all data types. The study itself found Gemini-1.5-Flash and Qwen2-VL did not show statistically significant bias across income groups, though qualitative examples of bias still existed.

### Mechanism 3
- **Claim:** Geo-economic bias in chart-to-text generation is resistant to simple, inference-time mitigation.
- **Mechanism:** The bias is likely multi-factorial, stemming from deep-seated associations in the training data. A single-sentence distractor addresses only one part of the generation process (the initial framing) but cannot fully neutralize the myriad ways bias can manifest in vocabulary, focus, and narrative construction.
- **Core assumption:** The bias is a systemic issue requiring more than a surface-level patch. This is an assumption based on the observed complexity; the paper does not pinpoint the exact source of bias within the model's architecture or data.
- **Evidence anchors:** [abstract] "find them only partially effective, underscoring the complexity of the issue and the need for more robust debiasing strategies." [Section 4.4, Table 3] The mitigation strategy increased bias in two models, showing its instability and ineffectiveness as a general solution. [Section 5] The conclusion explicitly states that "simple prompt-based mitigation strategies fail to comprehensively address these biases."
- **Break condition:** This claim would be weakened if a more sophisticated prompting strategy or a different class of VLMs proved more amenable to this type of mitigation. The current conclusion is conditional on the specific intervention tested.

## Foundational Learning

- **Concept: Geo-economic Bias in AI Systems**
  - **Why needed here:** This is the core problem the paper addresses. Understanding that AI models can inherit and amplify real-world societal inequalities based on country-level economic data is essential before attempting any mitigation.
  - **Quick check question:** If a VLM is shown a chart of identical CO2 emissions for Canada and Chad, how might its generated summary differ, and what societal assumptions might drive that difference?

- **Concept: Large Vision-Language Models (VLMs) for Chart Reasoning**
  - **Why needed here:** The domain of study is chart-to-text generation. Understanding that VLMs (like GPT-4o or Gemini) are multimodal systems that process visual and textual information to perform reasoning is the technical basis for the experiments.
  - **Quick check question:** What are the two primary modalities a VLM must process to generate a summary of a chart, and what is a key challenge in aligning them?

- **Concept: Statistical Significance Testing for Bias Evaluation**
  - **Why needed here:** The paper's conclusions are not based on anecdote but on statistical tests (Wilcoxon Signed-Rank Test). Understanding how to quantify bias and determine if it's likely due to random chance or a systematic model flaw is critical for rigorous evaluation.
  - **Quick check question:** Why is a paired statistical test used here, and what is the null hypothesis being tested?

## Architecture Onboarding

- **Component map:** Chart anonymization -> Country pairing engine -> VLM generation pipeline -> Sentiment evaluation module -> Bias analysis engine

- **Critical path:**
  1. Chart anonymization and country pairing
  2. Prompting VLMs to generate summaries/opinions
  3. Using a different set of VLMs to assign sentiment scores to the outputs
  4. Running statistical tests on the scores to quantify bias
  5. Re-running the pipeline with the mitigation prompt to assess its effect

- **Design tradeoffs:**
  - **Judge Model Selection:** Using VLMs (GPT-4o/Gemini-1.5-Pro) as judges is scalable and aligns with human evaluation, but introduces the possibility of judge bias. The authors mitigated this by using two independent judges and validating with humans (0.967 Pearson correlation).
  - **Intervention Scope:** The choice of an inference-time prompt-based mitigation was practical and model-agnostic but proved limited. A more effective, but costly, approach would be fine-tuning or data augmentation.
  - **Dataset Construction:** The choice of VisText over Chart-to-Text prioritized visual diversity but may have limited topic breadth.

- **Failure signatures:**
  - **Increased Bias after Mitigation:** For Claude-3-Haiku and Qwen2-VL, the positive distractor prompt backfired, increasing measured bias. This signals that simple, one-size-fits-all prompts are unreliable.
  - **High Variance Between Models:** GPT-4o-mini (44.52% biased pairs) vs. Gemini-1.5-Flash (16.10%) shows that bias is highly model-dependent and not uniform across VLM architectures.

- **First 3 experiments:**
  1. **Reproduction on a Single Model:** Select one VLM (e.g., GPT-4o-mini), replicate the core bias experiment with 10 charts and 10 countries (5 high-income, 5 low-income) to validate the sentiment scoring pipeline and observe the effect firsthand.
  2. **Alternative Distractor Test:** Modify the mitigation prompt. Instead of a generic positive phrase, use a neutral, factual distractor (e.g., "Note: This chart presents data over a 10-year period.") and compare its effect on reducing bias against the original positive distractor.
  3. **Trend-Specific Analysis:** Isolate the experiment to only 'Volatile' charts. The paper notes bias is prevalent here. Run the generation and evaluation to confirm if this chart type is indeed a failure mode, then analyze the specific language differences in the generated summaries.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can robust mitigation strategies such as data augmentation or model weight refinement effectively eliminate geo-economic bias where inference-time prompting failed?
- **Basis:** [explicit] The authors explicitly state in the Conclusion that "more robust mitigation strategies tailored to the chart domain should be explored, including data augmentation, model weight refinement."
- **Why unresolved:** The inference-time prompt-based approach tested was only partially effective (working for 4/6 models) and actually increased bias for Claude-3-Haiku and Qwen2-VL.
- **What evidence would resolve it:** Comparative experiments fine-tuning VLMs on debiased chart-text pairs or applying model-weight regularization to minimize sentiment divergence between country groups.

### Open Question 2
- **Question:** How does the manifestation of bias in chart-to-text generation compare across non-economic dimensions such as gender, race, and disability?
- **Basis:** [explicit] The authors identify a need for future research where "biases should be examined across other dimensions such as gender, race, ethnicity, and disability."
- **Why unresolved:** The current study restricted its variable manipulation to geo-economic status (High vs. Middle vs. Low income) to prove the existence of bias.
- **What evidence would resolve it:** A multi-dimensional benchmark dataset containing charts linked to various demographic attributes, evaluated for sentiment disparity across these different groups.

### Open Question 3
- **Question:** Why do positive distractor prompts increase bias in some VLMs (e.g., Claude-3-Haiku, Qwen2-VL) while reducing it in others?
- **Basis:** [inferred] Table 3 shows the mitigation strategy increased statistically significant biased responses for two models, and the Limitations section notes the authors "do not offer a definitive explanation for why certain models exhibit particular biases."
- **Why unresolved:** The opacity of proprietary and complex open-source model architectures makes it difficult to pinpoint if the bias stems from training data associations or specific attention mechanisms.
- **What evidence would resolve it:** An ablation study analyzing the attention weights or hidden states of the failing models to determine how the "positive distractor" sentence alters the internal reasoning process compared to successful models.

## Limitations

- The mitigation strategy was only partially effective, working for 4 of 6 models while increasing bias in two others
- The study focuses exclusively on geo-economic bias, limiting generalizability to other bias dimensions
- VLM judges may introduce their own bias despite human validation showing 0.967 Pearson correlation

## Confidence

- **High confidence:** The existence of geo-economic bias in VLMs (supported by statistical significance testing across 6,000 chart-country pairs and validated by human evaluation)
- **Medium confidence:** The effectiveness of prompt-based mitigation strategies (only partially effective, worked for 4 of 6 models with variable success rates)
- **Low confidence:** The complete characterization of bias mechanisms (the paper identifies associations but cannot pinpoint exact sources within model architecture)

## Next Checks

1. **Model-agnostic validation:** Test the same chart-country pairs across additional VLMs not included in the original study to assess whether the observed bias patterns are consistent across different model architectures and training approaches.

2. **Cross-cultural human evaluation:** Expand human validation beyond the initial 150 samples to include diverse annotators from different cultural backgrounds to verify that judge VLMs' sentiment assessments align with broader human perspectives on geo-economic bias.

3. **Bias persistence testing:** Conduct ablation studies where the positive distractor prompt is modified to use neutral factual statements instead of explicitly positive framing, to determine whether the mitigation effect is specific to positive language or if any structured prompt modification provides similar benefits.