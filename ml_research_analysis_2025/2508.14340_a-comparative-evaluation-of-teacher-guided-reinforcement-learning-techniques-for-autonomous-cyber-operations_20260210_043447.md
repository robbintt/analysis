---
ver: rpa2
title: A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques
  for Autonomous Cyber Operations
arxiv_id: '2508.14340'
source_url: https://arxiv.org/abs/2508.14340
tags:
- teacher
- uni00000013
- uni00000003
- agent
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of slow convergence and poor
  early-stage performance in reinforcement learning for autonomous cyber operations
  by implementing teacher-guided techniques. Four distinct methods were integrated
  into the CybORG environment: feature space modification, reward shaping, action
  masking, and auxiliary loss, each leveraging a pretrained RL agent as a teacher.'
---

# A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations

## Quick Facts
- arXiv ID: 2508.14340
- Source URL: https://arxiv.org/abs/2508.14340
- Reference count: 17
- Primary result: Action masking achieves initial rewards of approximately 50, while auxiliary loss converges to teacher performance five times faster than baseline PPO

## Executive Summary
This study evaluates teacher-guided reinforcement learning techniques to accelerate training in autonomous cyber operations. Four methods were integrated into PPO within the CybORG environment: feature space modification, reward shaping, action masking, and auxiliary loss. The research demonstrates that action masking provides superior early performance while auxiliary loss achieves rapid convergence to teacher performance. Reward shaping and feature space modification showed no measurable improvements over baseline PPO. The findings suggest that direct policy modifications are more effective than indirect reward signal modifications in complex cyber defense scenarios.

## Method Summary
The study implemented four teacher-guided techniques using PPO as the base algorithm in CybORG Cage Challenge 2. A pretrained PPO agent (trained for 100 episodes) served as the teacher, providing guidance through four mechanisms: reward shaping (scalar bonuses for matching teacher actions), feature space modification (appending teacher recommendations to state), action masking (modifying policy distribution before sampling), and auxiliary loss (adding teacher recommendation to loss function). The research conducted 10 independent runs of 500 episodes per method, comparing initial performance, convergence speed, and final policy quality against baseline PPO.

## Key Results
- Action masking achieved initial rewards of approximately 50 during the guidance phase
- Auxiliary loss allowed agents to converge to teacher performance by episode 20, five times faster than baseline PPO
- Reward shaping and feature space modification provided no measurable improvements over baseline
- All methods maintained final policy quality around reward 60 by episode 500

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Action masking provides high initial performance by directly constraining the action distribution rather than shaping learning signals.
- **Mechanism:** The teacher's recommendation modifies the policy's probability distribution before action sampling: π_masked(a_t) = π_θ(a_t) × M_t(a), where M_t assigns weight 1 to recommended actions and c_3 < 1 to non-recommended actions. This immediately biases exploration toward teacher-approved actions without requiring the agent to learn this bias through gradient updates.
- **Core assumption:** The pretrained teacher's policy is sufficiently correlated with good actions that constraining exploration to its neighborhood yields better early returns than random exploration.
- **Evidence anchors:**
  - [abstract]: "action masking achieving initial rewards of approximately 50"
  - [section V.B]: "initial performance is superior during the masking phase" with "initial rewards of approximately 50 and the policy dropping to a lowest value of approximately 40 as the teacher's influence is decayed"
  - [corpus]: Neighbor paper "Guidelines for Applying RL and MARL in Cybersecurity Applications" discusses exploration challenges in ACD that masking directly addresses
- **Break condition:** If the teacher's policy is poorly aligned with the true optimal policy, masking will actively hinder learning by blocking discovery of better actions. Gradual decay of c_3 is required to avoid performance collapse during transition to independent RL.

### Mechanism 2
- **Claim:** Auxiliary loss accelerates convergence by providing direct gradient-level supervision rather than indirect reward signal modification.
- **Mechanism:** The teacher's recommendation is incorporated into the loss function: L_total(θ) = σ × L_A(θ) + (1-σ) × L_Teacher(θ) + c_4 × S(π_θ), where L_Teacher = -log π_θ(a_teacher|s_t). This directly shapes the policy gradient, creating a stronger learning signal than reward shaping, which must propagate through value estimation.
- **Core assumption:** The PPO optimization landscape can accommodate multiple loss objectives without destabilizing convergence, and the teacher's action distribution provides useful gradient direction.
- **Evidence anchors:**
  - [abstract]: "auxiliary loss allowed agents to converge to the teacher's performance five times faster than baseline PPO"
  - [section V.C]: "both implementations quickly converge to the teacher's performance by approximately episode 20, which is five time quicker than the baseline PPO agent, which does not reach the teacher's performance until around episode 110"
  - [corpus]: Limited direct corpus evidence on auxiliary loss specifically in ACO; this appears to be a novel application
- **Break condition:** If σ decay is too aggressive, the agent may not have sufficient time to stabilize its policy before teacher guidance is removed. Entropy regularization (c_4) is critical to prevent premature convergence to a suboptimal teacher policy.

### Mechanism 3
- **Claim:** Reward shaping and feature space modification fail because they do not create sufficiently strong or interpretable learning signals in this domain.
- **Mechanism:** Reward shaping adds scalar bonuses to the environment reward (r_env + c_1 if action matches teacher), but this indirect signal must be propagated through value function approximation, which dilutes its effect. Feature space modification appends teacher recommendations as input features, but this requires the agent to learn to interpret and use these features—a non-trivial mapping problem.
- **Core assumption:** The failure of reward shaping may stem from the already sparse reward structure in CybORG, where additional scalar bonuses are insufficient to overcome the credit assignment problem. Feature space modification failure suggests the neural network cannot easily learn the mapping from encoded recommendations to appropriate actions.
- **Evidence anchors:**
  - [section V.A]: "both implementations of our reward shaping technique exhibit no noticeable improvements from the PPO baseline"
  - [section V.D]: LIME analysis shows "the teacher's recommendation is not included in the top four actions of the agent's policy" even when feature weights appear high
  - [corpus]: "Towards Production-Worthy Simulation for Autonomous Cyber Operations" notes that simulated environments must "produce the necessary signals to support RL training"—suggesting reward signal design is critical
- **Break condition:** These approaches may work in domains with denser reward structures or simpler state-action mappings, but break in complex ACO environments where the teacher signal-to-noise ratio is insufficient.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** All teacher-guided techniques are implemented as modifications to the PPO algorithm. Understanding the actor-critic architecture, the clipped objective function, and how gradients flow through the loss is essential to see why auxiliary loss (modifying the loss directly) is more effective than reward shaping (modifying the reward signal).
  - **Quick check question:** Can you explain why modifying the loss function (auxiliary loss) creates a different gradient signal than modifying the reward (reward shaping), even if both encode the same teacher preference?

- **Concept: Exploration-Exploitation Tradeoff and Entropy Regularization**
  - **Why needed here:** Action masking and auxiliary loss both suppress exploration in favor of teacher guidance. The paper introduces entropy terms (c_4 × S(π_θ)) to counteract premature convergence. Understanding why entropy prevents policy collapse is critical for tuning the transition from teacher-guided to independent RL.
  - **Quick check question:** What happens to policy diversity if you apply hard action masking (c_3 = 0) without any entropy regularization, and why does this matter for eventually surpassing the teacher's performance?

- **Concept: Credit Assignment in Reinforcement Learning**
  - **Why needed here:** The failure of reward shaping in this study suggests that bonus rewards do not effectively propagate through the value function to guide policy updates. Understanding the temporal credit assignment problem explains why direct policy modifications (masking, auxiliary loss) outperform indirect reward modifications.
  - **Quick check question:** Why would adding +2.5 to the reward for selecting the teacher's action fail to change the agent's behavior, when the same information delivered through auxiliary loss succeeds?

## Architecture Onboarding

- **Component map:**
  CybORG Cage Challenge 2 environment -> PPO agent with teacher guidance -> CybORG environment

- **Critical path:**
  1. Train a teacher agent using baseline PPO until it achieves reasonable performance (~100 episodes per paper)
  2. Freeze teacher weights and integrate guidance mechanism (recommended: auxiliary loss or action masking)
  3. Train student agent with teacher guidance, implementing decay schedule for teacher influence
  4. Evaluate on: (a) initial performance, (b) timesteps to reach teacher performance, (c) final policy quality
  5. Compare against baseline PPO with identical hyperparameters

- **Design tradeoffs:**
  - **Action masking vs. Auxiliary loss:** Masking gives better initial performance but requires careful decay to avoid transition shock. Auxiliary loss converges faster but provides less early-stage protection. Consider your deployment constraint: if early performance is critical (e.g., online learning in production), masking may be preferred; if sample efficiency is paramount, auxiliary loss.
  - **Decay schedule:** Gradual decay (recommended) vs. hard stop. Hard stop creates performance drops during transition; gradual decay smooths but requires tuning the decay rate.
  - **Teacher quality:** A weak teacher (trained only 100 episodes) may provide noisy guidance. Paper suggests this is sufficient for early-stage benefit, but a stronger teacher may yield different tradeoffs.

- **Failure signatures:**
  - **Reward shaping shows no improvement:** Likely indicates reward bonus is too weak relative to environment rewards, or value function approximation is diluting the signal. Do not pursue in CybORG-like sparse reward domains.
  - **Feature space modification shows no improvement + LIME shows agents don't use teacher features:** Indicates the network cannot learn the mapping from encoded recommendation to action. Do not pursue without explicit attention mechanisms or different encoding strategies.
  - **Performance collapse after teacher removal:** Indicates decay schedule is too aggressive. Increase decay duration or add entropy regularization during transition.
  - **Student fails to surpass teacher:** Indicates insufficient exploration after guidance. Increase entropy coefficient c_4 during and after teacher-guided phase.

- **First 3 experiments:**
  1. **Reproduce auxiliary loss result:** Implement L_total with σ decay and entropy scheduling. Verify convergence to teacher performance by episode ~20. If slower, check learning rate and σ decay rate.
  2. **Reproduce action masking result:** Implement masked distribution with c_3 decay from 0 to 1. Verify initial rewards ~50 and measure transition shock magnitude. Compare gradual decay vs. hard stop.
  3. **Ablation on teacher quality:** Train teachers for 50, 100, and 200 episodes. Measure how teacher quality affects both guidance methods. Hypothesis: weaker teachers provide less early benefit but may allow faster surpassing; stronger teachers provide better early performance but may constrain exploration.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - **Question:** Can external knowledge sources replace pretrained RL agents as teachers to eliminate the need for preliminary training rounds?
  - **Basis in paper:** [explicit] The conclusion states, "Future work could explore integrating existing external knowledge sources, eliminating the need for additional training."
  - **Why unresolved:** The current study relies solely on pretrained RL agents as teachers, necessitating a two-stage training process.
  - **What evidence would resolve it:** Successful implementation using static expert systems or Large Language Models (LLMs) as the teacher without pre-training.

- **Open Question 2**
  - **Question:** Does combining multiple teacher-guided techniques (e.g., action masking and auxiliary loss) yield superior performance compared to individual methods?
  - **Basis in paper:** [explicit] The authors note, "Moreover, combining different teacher-guided techniques could further improve performance."
  - **Why unresolved:** The study evaluated four techniques in isolation and did not test hybrid approaches.
  - **What evidence would resolve it:** Experiments in the CybORG environment that apply auxiliary loss and action masking simultaneously to a single agent.

- **Open Question 3**
  - **Question:** Are the benefits of teacher-guided techniques consistent across different reinforcement learning algorithms beyond PPO?
  - **Basis in paper:** [inferred] The paper states, "In our work, we use Proximal Policy Optimization (PPO) as the RL algorithm for all implementations."
  - **Why unresolved:** The efficacy of specific guidance methods like auxiliary loss may be dependent on the optimization mechanics of PPO.
  - **What evidence would resolve it:** A comparative evaluation using alternative algorithms (e.g., DQN, SAC) with the same teacher-guided modifications.

## Limitations

- Results are limited to a single simulated cyber defense environment (CybORG Cage Challenge 2) and may not generalize to other ACO domains
- Only one configuration of each teacher-guided technique was tested, with no exploration of hyperparameter sensitivity
- The study does not investigate hybrid approaches combining multiple teacher guidance methods
- Teacher agents were trained for only 100 episodes, which may not represent optimal teacher quality

## Confidence

- **High:** Action masking and auxiliary loss outperform baseline PPO (measured with 10 independent runs and statistical error bars)
- **Medium:** Reward shaping and feature space modification provide no benefit (only one configuration tested, no hyperparameter sensitivity analysis)
- **Low:** Results generalize to other ACO environments or RL algorithms beyond PPO (only tested on single environment with PPO)

## Next Checks

1. Test teacher-guided techniques on a different ACO environment (e.g., MITRE ATT&CK simulator) to assess generalizability
2. Conduct ablation studies varying the decay schedules and hyperparameters for action masking and auxiliary loss to identify optimal configurations
3. Evaluate whether stronger teachers (trained for 200+ episodes) provide different performance tradeoffs than the 100-episode teachers used in this study