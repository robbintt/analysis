---
ver: rpa2
title: 'AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology
  Reasoning in LLMs'
arxiv_id: '2504.02404'
source_url: https://arxiv.org/abs/2504.02404
tags:
- reasoning
- naltrexone
- dataset
- questions
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AnesSuite is the first comprehensive dataset suite designed for
  anesthesiology reasoning in LLMs, addressing the gap in specialized medical domain
  evaluation. It includes AnesBench, a bilingual benchmark with 7,972 questions across
  three cognitive levels (System 1, 1.x, 2), and three training datasets: AnesCorpus
  (2.4M+ documents), AnesQA (20k+ QA pairs), and AnesR1 (10k+ verifiable MCQs with
  CoT).'
---

# AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology Reasoning in LLMs

## Quick Facts
- **arXiv ID:** 2504.02404
- **Source URL:** https://arxiv.org/abs/2504.02404
- **Reference count:** 40
- **Primary result:** First comprehensive dataset suite for anesthesiology reasoning with benchmark and baseline model collection

## Executive Summary
AnesSuite addresses the gap in specialized medical domain evaluation by providing the first comprehensive dataset suite for anesthesiology reasoning in large language models. The suite includes AnesBench, a bilingual benchmark with 7,972 questions across three cognitive levels (System 1, 1.x, 2), and three training datasets: AnesCorpus (2.4M+ documents), AnesQA (20k+ QA pairs), and AnesR1 (10k+ verifiable MCQs with CoT). Using this suite, Morpheus was developed as the first baseline model collection for anesthesiology reasoning by fine-tuning Qwen2.5 models with SFT and GRPO on AnesR1, achieving performance comparable to larger-scale models.

## Method Summary
The AnesSuite framework consists of four main components: AnesBench (evaluation benchmark with bilingual MCQs stratified by cognitive complexity), AnesCorpus (domain-specific continued pre-training corpus), AnesQA (general QA pairs for supervised fine-tuning), and AnesR1 (verifiable MCQs with CoT for SFT and GRPO training). The Morpheus baseline models were created by fine-tuning Qwen2.5-Base models through an optional CPT stage on AnesCorpus, followed by SFT on AnesR1, and GRPO refinement using outcome-based binary rewards. The framework evaluates performance across System 1 (factual recall), System 1.x (hybrid reasoning), and System 2 (complex decision-making) tasks.

## Key Results
- Morpheus achieved performance comparable to larger-scale models despite limited training data
- Performance degrades sharply on System 2 (complex reasoning) tasks compared to System 1 (factual recall)
- Continued pre-training with AnesCorpus improves English performance but degrades Chinese performance due to language imbalance
- GRPO training after SFT provides consistent improvements across domain-specific, general medical, and general-domain benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Level Stratification for Evaluation Granularity
The three-level taxonomy isolates different cognitive demands, revealing that model performance degrades most sharply on reasoning-intensive tasks rather than knowledge retrieval. This stratification exposes reasoning gaps that System 1 questions mask.

### Mechanism 2: SFT Cold-Start Enables Stable GRPO Convergence
Supervised Fine-Tuning on CoT-annotated data provides behavioral initialization that allows Group Relative Policy Optimization to refine reasoning without reward-hacking. The SFT stage establishes a prior for structured reasoning outputs that GRPO can then reinforce.

### Mechanism 3: Continued Pre-Training Corpus Language Balance Affects Cross-Lingual Transfer
Domain-specific CPT improves English performance but can degrade Chinese performance if the corpus language distribution is imbalanced, suggesting language-specific knowledge systems rather than shared representations. This indicates catastrophic forgetting in underrepresented languages.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: AnesR1's defining feature is explicit reasoning chains; understanding why longer CoT correlates with System 2 performance is essential for interpreting results.
  - Quick check question: Can you explain why CoT length matters more for System 2 than System 1 questions?

- **Concept: System 1 vs System 2 Thinking (Kahneman)**
  - Why needed here: The benchmark's cognitive taxonomy directly maps to dual-process theory; misinterpreting these categories will lead to wrong conclusions about model capabilities.
  - Quick check question: What type of clinical question would require System 2 processing that a medical knowledge base alone cannot solve?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: GRPO is the post-SFT training method; understanding reward signal design explains why rejection sampling on verifiable MCQs matters.
  - Quick check question: Why would a binary correctness reward be sufficient for reasoning refinement, and what failure modes might it miss?

## Architecture Onboarding

- **Component map:**
  - AnesCorpus -> CPT stage (2.4M documents)
  - AnesR1 -> SFT + GRPO stage (10,287 MCQs with CoT)
  - AnesQA -> SFT stage (20,713 QA pairs)
  - AnesBench -> Evaluation only (English: 4,418 MCQs; Chinese: 3,554 MCQs)

- **Critical path:**
  1. Start from Qwen2.5-7B/14B/32B-Base
  2. Optional CPT on AnesCorpus (monitor cross-lingual degradation)
  3. SFT on AnesR1 (100 steps, LR 1e-5, cosine schedule)
  4. GRPO on AnesR1 (group size 5, binary reward, max 4 epochs)

- **Design tradeoffs:**
  - CPT corpus language balance vs. domain purity: Adding general medical data complements anesthesiology-specific data but risks multilingual imbalance
  - CoT length vs. inference cost: Longer reasoning improves System 2 accuracy but increases latency
  - SFT-only vs. SFT+GRPO: GRPO adds ~5-10 points on Chinese subset but requires stable SFT initialization

- **Failure signatures:**
  - GRPO reward plateau <0.6: Check SFT cold-start quality; rejection sampling may have filtered too aggressively
  - Cross-lingual performance collapse after CPT: Corpus language ratio exceeded 3:1 English:Chinese without balancing
  - System 2 accuracy stuck near baseline: CoT annotations may lack intermediate reasoning steps

- **First 3 experiments:**
  1. Baseline probe: Evaluate Qwen2.5-7B-Instruct (no training) on AnesBench to establish System 1/1.x/2 gaps
  2. SFT-only ablation: Train on AnesR1 for 100 SFT steps only; compare against SFT+GRPO to isolate GRPO contribution
  3. Language balance test: Run CPT with balanced English/Chinese AnesCorpus subsets (1:1) vs. original (3:1); measure AnesBench-Chinese degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of additional clinical modalities (e.g., imaging, vital signs) impact LLM reasoning performance on AnesSuite compared to the current text-only format?
- Basis in paper: [explicit] The authors state in Section 6 that "future iterations of AnesSuite should consider incorporating additional modalities, to better reflect real-world clinical environments."
- Why unresolved: The current benchmark and datasets are strictly text-based, limiting the evaluation of holistic clinical reasoning.
- What evidence would resolve it: A new multi-modal version of AnesBench and comparative evaluation results showing performance deltas between text-only and multi-modal models.

### Open Question 2
- Question: To what extent does performance on abstract System 2 questions in AnesBench correlate with efficacy in real-world, longitudinal clinical cases?
- Basis in paper: [explicit] Section 6 notes that "the System 2 questions are constructed from abstract scenarios rather than real clinical cases."
- Why unresolved: Current validation relies on exam-style questions which may not fully capture the complexity of bedside decision-making.
- What evidence would resolve it: A comparative study evaluating models on both the current AnesBench and a newly curated dataset of real clinical case logs.

### Open Question 3
- Question: Why do System 2 (complex decision-making) tasks exhibit significantly smaller performance gains from increasing model scale compared to System 1 tasks?
- Basis in paper: [inferred] Section 5.1 observes that "the slope for System2 is significantly lower," indicating that performance gains from scaling are "markedly smaller for System2."
- Why unresolved: The paper identifies the diminishing returns for complex reasoning but does not explain the underlying mechanism or propose a solution.
- What evidence would resolve it: Ablation studies analyzing architectural constraints or specific reasoning training algorithms (e.g., process-supervised RL) to flatten the scaling curve for System 2 tasks.

### Open Question 4
- Question: What corpus management strategies during Continued Pre-Training (CPT) can prevent the degradation of cross-lingual performance while enhancing domain-specific reasoning?
- Basis in paper: [inferred] Section 5.2 reports that CPT "degrades performance on the AnesBench-Chinese," likely due to "catastrophic forgetting," and recommends "careful corpus management."
- Why unresolved: The authors identify the trade-off but do not test specific linguistic balancing strategies in the CPT corpus.
- What evidence would resolve it: Experiments varying the linguistic distribution ratios in the AnesCorpus CPT data and measuring the resulting trade-off between English and Chinese performance.

## Limitations

- Dataset construction methodology for AnesCorpus relies on keyword filtering without detailed validation of clinical accuracy
- Cognitive level classification in AnesBench is based on subjective annotation criteria that could vary between clinicians
- GRPO implementation uses only binary correctness rewards, which may not capture intermediate reasoning quality

## Confidence

- **High confidence:** The core claim that System 2 questions are significantly harder than System 1 questions (supported by clear performance gaps across multiple models)
- **Medium confidence:** The mechanism explaining why SFT cold-start enables GRPO convergence (reasonable but not extensively validated with ablation)
- **Low confidence:** The claim that CPT degrades Chinese performance due to independent knowledge systems (lacks direct evidence for this specific mechanism)

## Next Checks

1. Cross-lingual CPT balance experiment: Run CPT on balanced English/Chinese AnesCorpus subsets (1:1 ratio) versus the original 3:1 ratio to determine whether Chinese performance degradation is truly due to language imbalance or other factors.

2. GRPO reward granularity test: Implement intermediate reward signals that evaluate the quality of reasoning steps in CoT chains (not just final correctness) to determine whether binary rewards are masking systematic reasoning failures.

3. Cognitive level annotation validation: Have three independent anesthesiologists re-classify a random sample of 100 AnesBench questions to measure inter-rater reliability and determine whether performance gaps between cognitive levels reflect true reasoning complexity or annotation inconsistency.