---
ver: rpa2
title: 'Optimal Dataset Size for Recommender Systems: Evaluating Algorithms'' Performance
  via Downsampling'
arxiv_id: '2502.08845'
source_url: https://arxiv.org/abs/2502.08845
tags:
- performance
- downsampling
- algorithms
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates dataset downsampling as a strategy to
  optimize energy efficiency in recommender systems while maintaining competitive
  performance. The study evaluates two downsampling approaches across seven datasets
  and 12 algorithms, revealing significant reductions in runtime and carbon emissions.
---

# Optimal Dataset Size for Recommender Systems: Evaluating Algorithms' Performance via Downsampling

## Quick Facts
- arXiv ID: 2502.08845
- Source URL: https://arxiv.org/abs/2502.08845
- Reference count: 0
- Primary result: Dataset downsampling significantly reduces recommender system energy consumption while maintaining competitive performance

## Executive Summary
This thesis investigates dataset downsampling as a strategy to optimize energy efficiency in recommender systems while maintaining competitive performance. The study evaluates two downsampling approaches across seven datasets and 12 algorithms, revealing significant reductions in runtime and carbon emissions. For example, 30% downsampling reduces runtime by 52% and carbon emissions by up to 51.02 KgCO2e during the training of a single algorithm on an individual dataset. The analysis shows that simpler algorithms retain up to 81% of full-size performance using only 50% of the training set. These findings demonstrate the feasibility of balancing sustainability and effectiveness, offering practical insights for designing energy-efficient recommender systems and advancing sustainable AI practices.

## Method Summary
The research evaluates two dataset downsampling strategies—User-Based (sampling interactions per user) and User-Subset (sampling users with adjusted train/test ratios)—across seven datasets including MovieLens and Amazon product datasets, using 12 algorithms from LensKit, RecPack, and RecBole libraries. The study measures nDCG@10 performance and runtime to estimate carbon emissions, testing downsampling portions from 10% to 100% with 5 random seeds. K-core pruning (10-core and 30-core) is applied for preprocessing, and limited grid search hyperparameter tuning is performed.

## Key Results
- 30% downsampling reduces runtime by 52% and carbon emissions by up to 51.02 KgCO2e for a single algorithm on one dataset
- Simpler algorithms retain up to 81% of full-size performance using only 50% of the training set
- User-Subset downsampling can achieve higher nDCG@10 scores than full datasets by optimizing train/test ratios per user

## Why This Works (Mechanism)

### Mechanism 1
Runtime and carbon emissions scale near-linearly with training set size reduction. Downsampling reduces the number of training interactions the algorithm must process, directly lowering computational operations. The study estimates energy using a linear model: reduced runtime × energy per operation × carbon intensity factor.

### Mechanism 2
Algorithm complexity determines sensitivity to training data reduction. Simpler algorithms (e.g., Popular, Bias, BiasedMF) rely on global statistics or shallow patterns that converge quickly with limited data. Complex algorithms (e.g., UserKNN, NeuMF, matrix factorization variants) require more interactions to learn personalized patterns.

### Mechanism 3
Downsampling configuration (User-Subset) can produce higher nDCG@10 scores than full datasets by altering train/test ratios. In User-Subset downsampling, increasing the downsampling portion adds more users while keeping test set size fixed. This increases each user's train/test ratio, providing more training signal per user.

## Foundational Learning

- **Core pruning (k-core filtering)**: Used to preprocess datasets by retaining only users and items with ≥k interactions, affecting density and algorithm performance. Quick check: If a dataset has 1000 users but only 50 have ≥30 interactions, what percentage remains after 30-core pruning?

- **Train/validation/test split with fixed vs. dynamic user sets**: User-Based downsampling keeps the same users across all portions (fixed user set, varying interactions). User-Subset adds users progressively (dynamic user set, fixed total test interactions). Quick check: In User-Subset at 10% downsampling, do you have fewer users or fewer interactions per user compared to 100%?

- **Normalized Discounted Cumulative Gain at rank 10 (nDCG@10)**: The primary metric for evaluating recommendation quality, measuring ranking quality of top-10 items. Quick check: If an algorithm's full-dataset nDCG@10 is 0.45 and its 50% downsampled score is 0.36, what is its relative performance percentage?

## Architecture Onboarding

- **Component map**: Raw dataset → preprocessing (deduplication, k-core pruning) → split (80/10/10) → downsampling (User-Based or User-Subset) → 12 algorithms → evaluation (nDCG@10, runtime) → sustainability estimation

- **Critical path**: 1. Select dataset and apply k-core pruning (start with 10-core for broader dataset coverage) 2. Choose downsampling approach based on goal 3. Run algorithm with hyperparameter tuning (10 configurations typical) 4. Record nDCG@10 and runtime; compute relative performance and estimated carbon savings

- **Design tradeoffs**:
  - User-Based vs. User-Subset: User-Based provides predictable performance trends but ~16% less runtime reduction
  - 10-core vs. 30-core pruning: 30-core yields higher absolute nDCG@10 but may reduce relative efficiency under User-Based approach
  - Algorithm selection: Group 1 algorithms for maximum accuracy; Group 2 for efficiency-focused deployments

- **Failure signatures**:
  - nDCG@10 drops below 40% of baseline at >50% downsampling → dataset too sparse or algorithm too complex for reduced data
  - Runtime does not decrease proportionally with downsampling → batch size too small, hardware underutilized
  - User-Subset shows erratic performance across portions → test set composition changes too drastically

- **First 3 experiments**:
  1. Replicate the 50% User-Based downsampling result on MovieLens 100K with UserKNN to validate ~64% relative performance claim
  2. Compare 10-core vs. 30-core pruning on Gowalla to observe the reported ~200%+ absolute nDCG@10 improvement
  3. Run User-Subset downsampling on Amazon Toys and Games with a Group 2 algorithm (e.g., Popular) to confirm the downward trend as downsampling portion increases

## Open Questions the Paper Calls Out

### Open Question 1
How do state-of-the-art deep learning architectures, specifically graph neural networks and transformer-based models, respond to dataset downsampling compared to the traditional algorithms predominantly tested in this study? The study primarily evaluated traditional algorithms and included only one deep learning model (NeuMF), leaving the behavior of modern architectures under data reduction unknown.

### Open Question 2
What are the precise mechanisms driving the contrasting performance trends (upward in sparse data vs. downward in dense data) observed in advanced algorithms under the User-Subset downsampling approach? The authors observed that algorithms react differently to the User-Subset method depending on dataset density, but the exact causality between the train/test ratio adjustment and these specific trends was not fully isolated.

### Open Question 3
To what extent does dataset downsampling impact non-accuracy performance metrics such as diversity, novelty, and serendipity? The research relied exclusively on nDCG@10 to measure recommendation quality, which measures ranking relevance but fails to capture the variety or novelty of the recommendations.

## Limitations
- Energy efficiency claims rely on a linear runtime-to-carbon conversion model without empirical hardware utilization measurements
- nDCG@10 metric may not fully capture performance degradation in sparsity scenarios
- Hyperparameter optimization uses limited grid search without specifying exact ranges

## Confidence

- **High Confidence**: Runtime reduction scaling with downsampling (mechanistically straightforward, directly measured)
- **Medium Confidence**: Algorithm sensitivity patterns (based on observable trends but lacks independent validation)
- **Low Confidence**: Carbon emission estimates (linear model not empirically validated across hardware configurations)

## Next Checks

1. Measure actual GPU/CPU utilization across downsampling portions to validate the linear energy model assumption
2. Replicate experiments on an additional sparse dataset not included in the original study to test algorithm sensitivity generalization
3. Compare nDCG@10 trends with coverage and diversity metrics to assess whether ranking-only evaluation captures full performance impact