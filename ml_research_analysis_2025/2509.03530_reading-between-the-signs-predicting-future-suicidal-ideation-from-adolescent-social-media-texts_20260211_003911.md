---
ver: rpa2
title: 'Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent
  Social Media Texts'
arxiv_id: '2509.03530'
source_url: https://arxiv.org/abs/2509.03530
tags:
- suicide
- risk
- suicidal
- posts
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces EARLY-SIB, a transformer-based model for predicting
  future suicidal ideation and behavior (SIB) from adolescent social media posts.
  The model sequentially processes a user's written posts and replies to forecast
  whether they will express SIB, using no self-disclosed SIB as input.
---

# Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts

## Quick Facts
- arXiv ID: 2509.03530
- Source URL: https://arxiv.org/abs/2509.03530
- Reference count: 37
- Primary result: Transformer-based model achieves 0.73 balanced accuracy predicting future suicidal ideation from adolescent social media posts

## Executive Summary
This paper introduces EARLY-SIB, a transformer-based model for predicting future suicidal ideation and behavior (SIB) from adolescent social media posts. The model sequentially processes a user's written posts and replies to forecast whether they will express SIB, using no self-disclosed SIB as input. Experiments on a Dutch youth forum show EARLY-SIB achieves 0.73 balanced accuracy in early prediction, outperforming baselines. Ablation studies confirm each component's contribution, while SHAP-based explanations reveal that predictions rely on diverse interactions rather than single posts, underscoring the task's complexity.

## Method Summary
The method uses a two-stage pipeline: first detecting SIB posts using a fine-tuned LLaMA-3-8B model, then training EARLY-SIB to predict future SIB from historical interactions. The architecture processes up to 30 user interactions (prioritizing posts over replies) through a BERTje encoder, bidirectional LSTM with attention, and mean pooling, combined with a separate title+tag transformer. The model uses 50-50 class balancing during training and achieves 0.73 balanced accuracy on held-out test sets.

## Key Results
- EARLY-SIB achieves 0.73 balanced accuracy in predicting future SIB from historical posts
- Sequential processing outperforms single-post classification, with performance plateauing around 15 interactions
- SHAP explanations show predictions rely on distributed signals across multiple interactions (mean complexity 0.90±0.13)
- 23% of predictions occur less than one day before SIB disclosure, demonstrating early warning capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential processing of user interaction history captures temporal risk signals that single-post classification misses.
- Mechanism: The bidirectional LSTM processes ordered CLS embeddings from each user interaction, learning dependencies across time that precede explicit SIB disclosure.
- Core assumption: Risk indicators emerge gradually through posting patterns rather than in isolated posts.
- Evidence anchors:
  - [abstract] "EARLY-SIB sequentially processes the posts a user writes and engages with to predict whether they will write a SIB post"
  - [section 6.2.2] Performance increases with context window N, plateauing around N≈15 interactions
  - [corpus] Related work on CNN-BiGRU with attention (arXiv:2511.08636) similarly uses sequential modeling for suicidal ideation detection
- Break condition: If user history contains fewer than 3-5 interactions, sequential signal degrades; performance drops to 0.65±0.05 at N=1.

### Mechanism 2
- Claim: Title and tag metadata encode substantial predictive signal independent of post body content.
- Mechanism: A dedicated transformer processes concatenated titles+tags separately, and ablation shows this component alone achieves 0.70±0.02 balanced accuracy.
- Core assumption: Users select topic tags and write titles that implicitly reveal distress themes before explicit SIB disclosure.
- Evidence anchors:
  - [section 6.2.1] Title+Tag Transformer ablation (BTL→BT) drops performance from 0.73 to 0.71, confirming contribution
  - [section 6.2.1] Title+Tag Transformer alone (BTL) achieves 0.70±0.02
  - [corpus] Weak direct corpus support for title/tag specific mechanisms; related work focuses on post bodies
- Break condition: If tags are auto-generated or titles are formulaic, this signal pathway collapses.

### Mechanism 3
- Claim: Explainable predictions require analyzing distributed signals across multiple interactions, not single "smoking gun" posts.
- Mechanism: SHAP analysis reveals mean normalized complexity of 0.90±0.13 (Shannon entropy), indicating predictions rely on diverse interaction combinations.
- Core assumption: Suicidal ideation manifests through accumulated behavioral patterns visible to ML but not obvious to human review.
- Evidence anchors:
  - [abstract] "SHAP-based explanations reveal that predictions rely on diverse interactions rather than single posts"
  - [section 6.3] Mean complexity 0.90±0.13; predictions distributed across interaction history
  - [section 6.3] 23% of users' most predictive interaction occurs <1 day before SIB disclosure, highlighting prediction urgency
  - [corpus] Suicidal Comment Tree Dataset work (arXiv:2510.14395) also emphasizes contextual/sequential analysis over single-post detection
- Break condition: If model over-relies on a single interaction (entropy→0), the mechanism fails to capture genuine distributed risk patterns.

## Foundational Learning

- Concept: **BERTje (Dutch BERT) for subword tokenization and contextual embeddings**
  - Why needed here: The forum is Dutch-language; multilingual models like XLM-RoBERTa underperform compared to language-specific pretraining.
  - Quick check question: Can you explain why a Dutch-specific BERT model would capture adolescent mental health language better than a multilingual model trained predominantly on English Wikipedia?

- Concept: **Class imbalance handling via training-set downsampling**
  - Why needed here: Only 4% of users are SIB-positive; the paper found 50-50 downsampling outperformed weighted loss and upsampling strategies.
  - Quick check question: Why might downsampling the majority class preserve more discriminative signal than upsampling the minority class in this specific context?

- Concept: **SHAP (Shapley Additive Explanations) for feature attribution**
  - Why needed here: Clinical deployment requires transparency about which interactions drive predictions; SHAP provides interaction-level importance scores.
  - Quick check question: How would you interpret a SHAP explanation showing high positive contribution from a post about "school stress" but negative contribution from a reply offering peer support?

## Architecture Onboarding

- Component map:
  User History (N≤30 interactions) -> Body Transformer (BERTje) -> CLS embeddings -> BiLSTM -> Attention -> Mean-pool -> FC input A; Title+Tag Transformer (BERTje) -> CLS embedding -> FC input B; Concatenate [A, B] -> FC -> Sigmoid

- Critical path:
  1. Prioritize posts over replies when filling N=30 slots (posts are longer, more informative)
  2. Prefix title/tag string with interaction type ("User posted" / "User replied to")—removing this drops accuracy to 0.69
  3. DO NOT include reply context (the post being replied to) as input; this dilutes signal

- Design tradeoffs:
  - **Context window N=30 vs. longer**: Covers 90%+ of users; longer windows add computational cost with diminishing returns (plateau at N≈15)
  - **Separate title/tag transformer vs. unified**: Adds ~110M parameters but captures orthogonal signal
  - **Binary classification vs. ordinal risk levels**: Simpler but loses nuance (transient distress vs. chronic risk)

- Failure signatures:
  - Zero-shot LLaMA baselines fail (balanced accuracy 0.55-0.57) due to task refusal and role confusion (offering advice instead of classifying)
  - Including replies' parent posts as context degrades performance (0.72→0.72 but with lower recall)
  - Model may misclassify users if SIB-detection stage has false negatives (ground truth is inferred, not clinically verified)

- First 3 experiments:
  1. **Reproduce ablation**: Train with Body Transformer only, Title+Tag only, and full model; verify 0.67→0.70→0.73 progression
  2. **Context window sweep**: Test N∈{1,5,10,15,20,30} on held-out fold; confirm plateau pattern and identify optimal N for your compute budget
  3. **SHAP complexity audit**: For 50 random SIB-positive users, compute normalized entropy of SHAP distributions; verify mean ≈0.90 to confirm distributed prediction pattern holds on your deployment data

## Open Questions the Paper Calls Out

- **Cross-platform and multilingual generalizability**: The authors state future work should test whether the approach applies to other platforms and languages beyond the Dutch youth forum used in this study.

- **Clinical validity of predictions**: The ground truth relies on user text rather than clinician-confirmed behavior, and the authors note that absence of disclosure doesn't imply absence of risk.

- **Alternative explanation methods**: The authors suggest that abstractive techniques like keyphrase attribution or rationale generation could increase practical transparency compared to post-level SHAP explanations.

- **Ordinal risk modeling**: The binary classification framework ignores nuanced gradations of suicidal ideation such as frequency, intensity, or temporality, which may obscure important distinctions.

## Limitations

- Clinical validity: Ground truth SIB labels are inferred from post detection rather than clinician-confirmed behavior, introducing potential false positives/negatives.
- Language and cultural generalizability: Model trained exclusively on Dutch adolescent forum data; performance on other languages and cultures remains untested.
- Temporal generalization: Model performance evaluated within same dataset timeframe; ability to predict future SIB beyond training period unknown.

## Confidence

- **High confidence**: Sequential modeling architecture (BERTje + BiLSTM + attention) achieves measurable performance improvement over single-post baselines.
- **Medium confidence**: SHAP-based explanation methodology reliably shows distributed prediction patterns across interactions.
- **Low confidence**: Claims about earlier detection timing may overstate practical clinical utility without real-world intervention validation.

## Next Checks

1. **Temporal validation**: Train on data up to time T, test on held-out future period T+1; measure performance decay and calibration drift to assess temporal generalization.

2. **Clinical expert review**: Have mental health professionals independently evaluate 100 SHAP-explainable predictions, categorizing false positives/negatives and assessing clinical utility of explanations.

3. **Cross-cultural replication**: Apply pretrained model to analogous adolescent forum data from different language/cultural context; measure performance drop and identify culturally-specific failure modes.