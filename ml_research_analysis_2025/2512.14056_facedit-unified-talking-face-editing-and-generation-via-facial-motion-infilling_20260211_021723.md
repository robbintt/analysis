---
ver: rpa2
title: 'FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling'
arxiv_id: '2512.14056'
source_url: https://arxiv.org/abs/2512.14056
tags:
- motion
- editing
- speech
- face
- talking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FacEDiT, a unified framework for talking face
  editing and generation based on speech-conditional facial motion infilling. The
  key idea is to treat both tasks as subtasks of a single formulation, where the model
  learns to reconstruct masked facial motion sequences conditioned on paired speech
  and surrounding motion.
---

# FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling

## Quick Facts
- arXiv ID: 2512.14056
- Source URL: https://arxiv.org/abs/2512.14056
- Authors: Kim Sung-Bin; Joohyun Chang; David Harwath; Tae-Hyun Oh
- Reference count: 40
- One-line primary result: FacEDiT achieves state-of-the-art results in unified talking face editing and generation via speech-conditional facial motion infilling.

## Executive Summary
This paper proposes FacEDiT, a unified framework for talking face editing and generation based on speech-conditional facial motion infilling. The key idea is to treat both tasks as subtasks of a single formulation, where the model learns to reconstruct masked facial motion sequences conditioned on paired speech and surrounding motion. FacEDiT uses a Diffusion Transformer trained with flow matching and incorporates biased attention and temporal smoothness constraints to ensure accurate lip synchronization and seamless transitions. To address the lack of a standard benchmark for talking face editing, the authors introduce FacEDiTBench, the first high-quality dataset for this task, along with new evaluation metrics for identity preservation and boundary continuity. Extensive experiments demonstrate that FacEDiT achieves state-of-the-art results in both editing and generation, producing accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity.

## Method Summary
FacEDiT treats talking face editing and generation as subtasks of speech-conditional facial motion infilling. During training, random temporal spans of facial motion latents (derived from LivePortrait) are masked, and a Diffusion Transformer predicts the missing motion conditioned on surrounding context and paired speech features (from WavLM). The model is trained with flow matching and includes biased attention (local temporal window) and a temporal smoothness loss to ensure lip-sync accuracy and boundary continuity. At inference, editing is performed by masking spans corresponding to transcript changes, while generation masks all but initial frames. The predicted motion latents are then used to warp appearance features and render video frames.

## Key Results
- FacEDiT achieves state-of-the-art performance on both talking face editing and generation tasks.
- The unified motion infilling formulation outperforms separate models trained for each task.
- Introduction of FacEDiTBench provides the first high-quality benchmark for talking face editing with comprehensive evaluation metrics.

## Why This Works (Mechanism)

### Mechanism 1
Treating talking face editing and generation as subtasks of speech-conditional facial motion infilling enables a single model to handle both tasks. During training, random temporal spans of facial motion are masked, and the model learns to reconstruct them using surrounding motion context and paired speech as conditions. At inference, the mask is determined by comparing original and edited transcripts (for editing) or by masking all but initial frames (for generation). The model can learn to interpolate plausible facial motion from sparse context, and the same infilling capability generalizes across partial (editing) and near-complete (generation) masking ratios. Break condition: If edits require extending far beyond the training mask distribution (e.g., >80% masking when training used 30-60%), quality may degrade.

### Mechanism 2
Incorporating speech via cross-attention with biased temporal masking improves lip-sync accuracy and boundary continuity compared to early fusion. Speech features are projected into the DiT latent space and injected through multi-head cross-attention at each DiT block. A temporal bias B(i,j) restricts attention to a local window, encouraging alignment between speech and motion features at corresponding timesteps. Local temporal alignment between speech and motion is sufficient for accurate lip-sync; long-range dependencies can be captured through stacked attention layers. Break condition: If speech contains rapid phoneme transitions exceeding the attention window, temporal misalignment may occur.

### Mechanism 3
Temporal smoothness loss combined with biased attention reduces motion discontinuities at edit boundaries. An L1 penalty on frame-to-frame motion differences is added to the flow matching objective. This regularizes the predicted motion sequence to have low instantaneous velocity changes, complementing the local attention bias that already encourages smooth transitions. Smooth motion trajectories are perceptually preferable and correlate with natural facial dynamics. Break condition: Over-weighting λ_TS may cause over-smoothing, resulting in lethargic or mumbled animation.

## Foundational Learning

- Concept: Conditional Flow Matching (CFM)
  - Why needed here: FacEDiT uses CFM instead of standard diffusion training to learn the vector field transporting noise to facial motion latents. Understanding the linear interpolation path x_t = (1-t)x_0 + t*x_1 and the velocity objective is essential for debugging training.
  - Quick check question: Given x_0 (noise) and x_1 (data), what is the analytic velocity field at time t?

- Concept: Facial Motion Latents (LivePortrait representation)
  - Why needed here: The model operates on motion latents f ∈ R^75 derived from LivePortrait's motion extractor, not raw video. These encode expression deformation δ, head rotation R, and translation τ separately from appearance.
  - Quick check question: Why does decoupling motion from appearance enable localized editing without identity drift?

- Concept: Masked Autoencoder-style Pretraining
  - Why needed here: The infilling objective draws from MAE literature—randomly masking input spans and training the model to reconstruct them conditions the model on partial context, which is directly used at inference for editing.
  - Quick check question: How does masking ratio during training affect the edit-generation spectrum at inference?

## Architecture Onboarding

- Component map:
  Speech encoder (WavLM, frozen) -> Projection layer (trained) -> DiT backbone (22 layers, 16 heads, trained) -> Motion extractor (LivePortrait, frozen) -> Decoder (LivePortrait, frozen) -> Frame resampling module

- Critical path:
  1. Extract motion latents F_{1:T} from original video
  2. Generate mask M based on transcript alignment (editing) or full sequence masking (generation)
  3. Sample noise F_0, compute noisy state F_t = (1-t)F_0 + t*F_1
  4. DiT predicts velocity, iterate with Euler ODE solver to recover F_1
  5. Warp original appearance using predicted motion, decode to video frames
  6. Resample frames if edited segment duration differs from original

- Design tradeoffs:
  - Cross-attention vs. early fusion: Cross-attention improves lip-sync (+0.092 LSE-C) but adds compute per layer
  - Attention window size (w): Smaller w enforces locality (smoother boundaries) but may miss long-range phoneme dependencies
  - λ_TS weighting: Higher values smooth motion but risk over-damping expressions

- Failure signatures:
  - Discontinuities at edit boundaries: Check attention bias implementation; mask may not align with actual edit timestamps
  - Identity drift: Verify appearance features are extracted correctly and frozen; motion prediction should not alter identity
  - Mumbled lip-sync: Speech-motion alignment may be off—verify cross-attention temporal bias and speech encoder projections
  - Over-smoothed motion: Reduce λ_TS or increase attention window

- First 3 experiments:
  1. Reproduce ablation (Table 2): Train with early fusion vs. cross-attention, verify LSE-D/C improvement
  2. Boundary continuity stress test: Evaluate on FacEDiTBench edits at max span (7-10 words), plot P_continuity and M_continuity against mask ratio
  3. Generalization check: Zero-shot evaluation on unseen identities from CelebV-Dub test split; compare IDSIM and FVD against Hallo3 baseline

## Open Questions the Paper Calls Out
- Can the motion infilling framework be extended to synthesize coordinated full-body animation and upper-body gestures? The authors state in the Discussion: "Because our training focuses on synthesizing facial motion and head poses, upper-body gestures are not driven by speech... Extending the framework... to also edit or synthesize full-body animation... we plan to explore in future work."
- How can explicit emotional conditioning be integrated into the unified formulation without compromising lip-sync accuracy? The Discussion notes: "Emotional context is learned implicitly rather than explicitly modeled."
- Does the reliance on a pre-trained motion extractor (LivePortrait) limit the fidelity of subtle facial micro-expressions? The method depends on the LivePortrait motion representation [δ, R, τ]. While efficient, this intermediate representation may compress or discard subtle high-frequency details necessary for high-fidelity editing.

## Limitations
- The biased attention window size w is not explicitly specified in the main paper, which could affect boundary smoothness and lip-sync accuracy.
- Frame resampling module implementation details (facial/background separation, RAFT-based interpolation) are only briefly mentioned and may require significant engineering to replicate.
- The training mask span distribution and position sampling strategy are not fully detailed, making it difficult to precisely reproduce the self-supervised pretext task.

## Confidence
- High: The core claim that treating editing and generation as subtasks of motion infilling enables a unified model is well-supported by the experimental results and ablation studies.
- Medium: The assertion that cross-attention with biased temporal masking improves lip-sync over early fusion is supported by quantitative metrics (LSE-D/C improvements) but lacks direct comparison to other state-of-the-art methods in the corpus.
- Medium: The claim that temporal smoothness loss reduces motion discontinuities is supported by continuity metric improvements but may be sensitive to λ_TS hyperparameter tuning.

## Next Checks
1. Reproduce the cross-attention vs. early fusion ablation (Table 2) on FacEDiTBench to verify the reported LSE-D/C improvements.
2. Evaluate boundary continuity on extreme editing cases (7-10 word spans) to assess whether the biased attention and smoothness loss maintain quality under maximum masking ratios.
3. Conduct zero-shot evaluation on CelebV-Dub test split to validate generalization claims and compare IDSIM and FVD against Hallo3 baseline.