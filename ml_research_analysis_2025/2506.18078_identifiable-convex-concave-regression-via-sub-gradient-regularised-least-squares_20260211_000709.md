---
ver: rpa2
title: Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares
arxiv_id: '2506.18078'
source_url: https://arxiv.org/abs/2506.18078
tags:
- convex
- regularization
- constraints
- regression
- iccnls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICCNLS, a novel regression method that decomposes
  complex nonlinear functions into additive convex and concave components while ensuring
  identifiability through global statistical orthogonality constraints. The method
  addresses the ambiguity problem in convex-concave decomposition by requiring residuals
  to be orthogonal to both intercept and input variables, effectively eliminating
  degrees of freedom associated with affine shifts between components.
---

# Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares

## Quick Facts
- arXiv ID: 2506.18078
- Source URL: https://arxiv.org/abs/2506.18078
- Authors: William Chung
- Reference count: 0
- Primary result: ICCNLS achieves superior predictive accuracy and model simplicity compared to conventional CNLS and DC regression approaches while ensuring decomposition identifiability through orthogonality constraints.

## Executive Summary
This paper introduces ICCNLS, a novel regression method that decomposes complex nonlinear functions into additive convex and concave components while ensuring identifiability through global statistical orthogonality constraints. The method addresses the ambiguity problem in convex-concave decomposition by requiring residuals to be orthogonal to both intercept and input variables, effectively eliminating degrees of freedom associated with affine shifts between components. Empirical evaluation on synthetic and real-world insurance pricing data demonstrates that ICCNLS achieves superior predictive accuracy and model simplicity compared to conventional approaches.

## Method Summary
ICCNLS parameterizes the target function as f(x) = g_concave(x) + g_convex(x), where each component is represented via sub-gradient-constrained affine functions. The method minimizes squared residuals subject to shape constraints enforced through subgradient inequalities at each observation point. Identifiability is ensured by requiring residuals to be orthogonal to both intercept and input variables, eliminating affine degrees of freedom. The approach incorporates L1, L2, and elastic net regularization on subgradients to enhance generalization and promote sparsity. The optimization problem is formulated as a quadratic program with O(n²) linear inequality constraints for shape enforcement plus d+1 linear equality constraints for orthogonality.

## Key Results
- ICCNLS achieves lower RMSE and MAE than CNLS and DC regression on synthetic and insurance pricing data
- Regularization effectively reduces the number of active hyperplanes from 74 to 13 while improving predictive accuracy
- The method produces interpretable models suitable for forecasting and policy evaluation
- Residual orthogonality constraints ensure unique decomposition while maintaining statistical properties like residual orthogonality and mean preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual orthogonality constraints eliminate decomposition ambiguity by preventing arbitrary affine shifts between components.
- Mechanism: The convex-concave decomposition f = g_concave + g_convex is inherently non-unique because any affine function L(x) = a + b'x can be added to one component and subtracted from the other. By constraining residuals ε_i = y_i - f(x_i) to satisfy Σε_i = 0 and Σε_i x_ij = 0 for all j, the model forces residuals to be orthogonal to all affine functions, removing these degrees of freedom.
- Core assumption: The orthogonality constraints are compatible with the shape constraints defining valid convex/concave functions (feasibility).
- Evidence anchors: Abstract states orthogonality eliminates degrees of freedom; Proposition 1 provides formal proof of uniqueness under these constraints.

### Mechanism 2
- Claim: Global shape constraints can be enforced through local subgradient inequalities at each observation point.
- Mechanism: Each observation i is associated with local affine hyperplane parameters (α_i^c, β_i^c) for the concave component and (α_i^v, β_i^v) for the convex component. Concavity is enforced by requiring every hyperplane to lie above all points: α_h^c + (β_h^c)'x_i ≥ α_i^c + (β_i^c)'x_i. Convexity uses the opposite inequality. This creates an envelope of supporting hyperplanes.
- Core assumption: The underlying function admits a piecewise affine approximation that satisfies global shape properties through local constraints.
- Evidence anchors: Section 3.2 provides explicit constraint formulations; Fast and Exact Least Absolute Deviations uses similar piecewise affine lower-bounding approach.

### Mechanism 3
- Claim: Subgradient regularization reduces model complexity by shrinking or zeroing gradient coefficients, producing fewer effective hyperplanes.
- Mechanism: L1 penalty ‖β_i^c‖₁ + ‖β_i^v‖₁ promotes sparse gradients (variable selection); L2 penalty ‖β_i^c‖₂² + ‖β_i^v‖₂² promotes small, smooth gradients. Both reduce overfitting from the default behavior where H ≈ n hyperplanes fit local neighborhoods exactly.
- Core assumption: Smoother/sparser gradients correlate with better generalization on unseen data.
- Evidence anchors: Hyperplane count drops from 74 to 13 under strong regularization in Table 1; Lecture Notes on High Dimensional Linear Regression covers L1/L2 generalization properties.

## Foundational Learning

- Concept: **Subgradients and Convexity**
  - Why needed here: The entire parameterization uses subgradient inequalities to enforce global shape constraints through local affine approximations.
  - Quick check question: Given a convex function f, can you explain why α + β'x ≤ f(x) for all x defines a valid subgradient inequality at some point?

- Concept: **Identifiability in Additive Decompositions**
  - Why needed here: The paper's core contribution is resolving the non-uniqueness of convex-concave decompositions through statistical constraints.
  - Quick check question: If f(x) = g₁(x) + g₂(x) where g₁ is concave and g₂ is convex, why does f(x) = (g₁(x) + L(x)) + (g₂(x) - L(x)) also satisfy this for any affine L?

- Concept: **Quadratic Programming with Linear Constraints**
  - Why needed here: ICCNLS is solved as a QP with O(n²) linear inequality constraints (shape) plus d+1 linear equality constraints (orthogonality).
  - Quick check question: What property of the objective function ensures the QP has a unique global minimum?

## Architecture Onboarding

- Component map: Raw data {(x_i, y_i)}_{i=1}^n -> Parameter blocks {2n hyperplane parameter sets} -> Constraint matrix (shape + orthogonality) -> Regularization module -> QP solver -> Fitted values ŷ_i and component decompositions
- Critical path: 1) Data validation and preprocessing (centering recommended) 2) QP formulation with all constraints assembled 3) Solver call with regularization parameters 4) Post-processing: extract fitted values and component decompositions 5) Cross-validation for λ and α tuning
- Design tradeoffs: Higher n → more flexible fit but O(n²) constraint growth → computational bottleneck; Higher α (more L1) → sparser gradients, fewer hyperplanes, risk of underfitting; Stronger orthogonality enforcement → unique decomposition but potentially smaller feasible region; No regularization → perfect in-sample fit (H ≈ n), poor generalization
- Failure signatures: Solver infeasibility (orthogonality constraints incompatible with shape constraints); Overfitting detected (H ≈ n hyperplanes fitted; increase λ); Underfitting detected (RMSE rises sharply; decrease λ or reduce α); Numerical instability (large coefficient magnitudes; add L2 regularization)
- First 3 experiments: 1) Reproduction test: Run the synthetic experiment (n=80, d=3) from Section 6.1 and verify RMSE/MAE values match Table 1 for λ ∈ {0, 1, 10, 100} with α ∈ {0, 0.5, 1} 2) Identifiability ablation: Fit the model with orthogonality constraints disabled; verify that different solver initializations produce different component decompositions {g_concave, g_convex} but identical fitted values f̂(x) 3) Scalability benchmark: Measure solve time as n increases from 100 to 500 (fixed d=5); confirm approximately quadratic growth and identify practical limits for your hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical consistency and convergence rates of the ICCNLS estimator under subgradient regularization?
- Basis in paper: The author states that "full theoretical properties such as consistency and convergence rates are not derived in this paper," though they are expected to follow from established results.
- Why unresolved: The current work focuses on the methodological formulation and empirical validation rather than providing formal asymptotic proofs.
- What evidence would resolve it: A formal proof extending convex regression theory (e.g., Guntuboyina & Sen, 2015) to this double-envelope, orthogonally-constrained framework.

### Open Question 2
- Question: Can tailored decomposition algorithms (e.g., simplicial or dual decomposition) improve computational efficiency for large-scale applications?
- Basis in paper: The conclusion identifies computational efficiency as a challenge and suggests exploring "tailored decomposition algorithms" as a future direction.
- Why unresolved: The current implementation uses a general-purpose solver (GAMS/MINOS), which resulted in high latency (approx. 350 seconds for 176 records).
- What evidence would resolve it: Development and benchmarking of a specialized optimization algorithm that scales linearly or quadratically with sample size.

### Open Question 3
- Question: Can the ICCNLS framework be extended to incorporate monotonicity constraints or interaction structures without losing identifiability?
- Basis in paper: Section 7 lists "extending ICCNLS to accommodate monotonicity, sparsity, or interaction structures" as a primary avenue for future research.
- Why unresolved: The current identifiability relies on global orthogonality constraints which may require modification to coexist with hard monotonicity bounds or interaction terms.
- What evidence would resolve it: A modified model formulation that integrates monotonicity while maintaining a valid uniqueness proof for the decomposition.

## Limitations

- Quadratic formulation of orthogonality constraints may introduce numerical instability, particularly with high-dimensional data
- O(n²) growth in subgradient constraints makes the method computationally intensive for large datasets, limiting practical applicability
- Empirical evaluation relies heavily on synthetic data and a single insurance dataset, which may not generalize to other domains

## Confidence

- High confidence: The identifiability mechanism via residual orthogonality constraints (proven mathematically in Proposition 1)
- Medium confidence: Regularization effectiveness on generalization (supported by synthetic experiments but limited real-world validation)
- Low confidence: Computational scalability claims (complexity analysis provided but empirical validation limited to small datasets)

## Next Checks

1. Stress-test identifiability: Fit models with identical data but different solver initializations with and without orthogonality constraints; verify decomposition uniqueness only occurs with constraints enabled.
2. Benchmark computational scaling: Systematically measure solve times across datasets with n ∈ {100, 200, 400, 800} and d ∈ {2, 5, 10}; verify O(n²) constraint growth impact.
3. Cross-domain robustness: Apply ICCNLS to at least two additional real-world datasets from different domains (e.g., healthcare, finance) and compare performance against standard regression baselines.