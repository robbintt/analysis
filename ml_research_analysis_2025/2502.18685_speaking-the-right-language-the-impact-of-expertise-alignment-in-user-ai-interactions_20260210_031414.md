---
ver: rpa2
title: 'Speaking the Right Language: The Impact of Expertise Alignment in User-AI
  Interactions'
arxiv_id: '2502.18685'
source_url: https://arxiv.org/abs/2502.18685
tags:
- user
- expertise
- conversations
- expert
- proficient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the alignment of AI agents with users of varying
  expertise levels using 25,000 Bing Copilot conversations. The authors develop an
  ordinal 5-point scale expertise classifier to label user expertise, agent expertise,
  and gauged user expertise across conversations.
---

# Speaking the Right Language: The Impact of Expertise Alignment in User-AI Interactions

## Quick Facts
- arXiv ID: 2502.18685
- Source URL: https://arxiv.org/abs/2502.18685
- Reference count: 35
- Primary result: Agent expertise at proficient or expert levels correlates with positive user satisfaction, while underestimating user expertise reduces satisfaction, especially for complex tasks.

## Executive Summary
This study analyzes 25,000 Bing Copilot conversations to understand how expertise alignment between users and AI agents affects user satisfaction and engagement. The authors develop an ordinal 5-point scale to classify user expertise, agent expertise, and gauged user expertise across conversations. Results show that agents primarily respond at proficient or expert levels (77% of conversations), which correlates with positive user satisfaction. However, when agents underestimate user expertise—responding below the user's actual level—satisfaction decreases, particularly for high-complexity tasks. Users also engage more when agent expertise matches their own, suggesting a balance between high agent expertise and commensurate responses is optimal for user experience.

## Method Summary
The study analyzes 25,033 English Bing Copilot conversations from June 2024, filtered to include only those with at least two turns. Using GPT-4-Turbo with human-validated prompts, the researchers classify user expertise, agent expertise, and gauged user expertise on a 5-point ordinal scale (Novice to Expert). Satisfaction is inferred using the SPUR framework rather than explicit user ratings. Task complexity is categorized using Bloom's revised taxonomy (Low vs High). The researchers then correlate expertise gaps with satisfaction scores and engagement metrics (word count), using piecewise and linear regression analyses stratified by task complexity.

## Key Results
- Agents respond at proficient or expert levels in 77% of conversations, correlating with positive user satisfaction across all expertise levels
- Underestimating user expertise negatively impacts satisfaction, with stronger effects on high-complexity tasks (R² = 0.11, p = 1.36E−141)
- Users engage more (produce longer conversations) when agent expertise matches their own level

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agent expertise at proficient or expert levels correlates with positive user satisfaction across all user expertise levels.
- Mechanism: High agent expertise provides informational value that users recognize regardless of their own knowledge level—novices benefit from comprehensive answers while experts get sufficient depth.
- Core assumption: Users can recognize valuable information even when it exceeds their current knowledge level, and prefer over-inclusion to under-inclusion.
- Evidence anchors:
  - [abstract] "agent largely responds at proficient or expert levels of expertise (77% of conversations) which correlates with positive user experience regardless of the user's level of expertise"
  - [section] Figure 6 heatmap shows median SAT scores of 16-21 when agent responds at Proficient/Expert levels across all user expertise levels
  - [corpus] Limited direct support; related work on human-AI collaboration (SpeakRL, FMR=0.60) discusses bidirectional communication but doesn't validate expertise preferences
- Break condition: If response complexity creates cognitive overload that prevents information extraction

### Mechanism 2
- Claim: Underestimating user expertise (responding below user's actual level) reduces satisfaction, particularly for high-complexity tasks.
- Mechanism: Expert users experience friction when receiving explanations that don't acknowledge their existing knowledge—this signals the system doesn't understand them.
- Core assumption: Expert users have reasonably accurate self-assessment and can detect when responses are unnecessarily elementary.
- Evidence anchors:
  - [abstract] "Misalignment, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user experience"
  - [section] "when the user expertise exceeds the LLM expertise, there is a negative impact (R² = 0.11, p = 1.36E−141) on the SAT score, with the overall SAT score becoming negative in absolute terms once the user is one level more expert"
  - [corpus] No direct corpus validation for underestimation penalty specifically
- Break condition: If users inaccurately self-assess as more expert than they are (the study notes 57.4% of cases show overestimation by the agent)

### Mechanism 3
- Claim: Expertise alignment drives engagement—users produce longer conversations when matched with commensurate expertise levels.
- Mechanism: Matched expertise creates conversational flow; users expand discussions when they feel understood rather than talked-down-to or overwhelmed.
- Core assumption: Word count in user turns is a valid proxy for engagement (not frustration-based verbosity).
- Evidence anchors:
  - [abstract] "users engage more, as measured by the number of words in the conversation, when the agent responds at a level of expertise commensurate with that of the user"
  - [section] Figure 7: "Proficient" and "Expert" users engage more with Proficient/Expert LLMs (median 400+ words), while "Novice" users engage more with Novice/Beginner LLMs (median 60-80 words)
  - [corpus] No corpus papers validate the engagement-alignment relationship
- Break condition: If longer conversations reflect confusion loops rather than productive engagement

## Foundational Learning

- Concept: Ordinal expertise classification from dialogue signals
  - Why needed here: The entire methodology depends on classifying user expertise, agent expertise, and "gauged" user expertise from conversation text using a 5-point scale (Novice → Expert).
  - Quick check question: Can you explain why "gauged user expertise" (what the agent infers about the user) differs from "user expertise" (actual user level), and what this gap represents?

- Concept: Satisfaction estimation without explicit ratings
  - Why needed here: The study uses SPUR (Supervised Prompting for User Satisfaction Rubrics) to infer satisfaction from conversation patterns rather than direct user feedback.
  - Quick check question: What signals in a multi-turn conversation might indicate dissatisfaction that wouldn't appear in single-turn interactions?

- Concept: Task complexity taxonomies (Bloom's revised taxonomy)
  - Why needed here: The moderating effect of complexity on the alignment-satisfaction relationship requires classifying tasks into Low (Remember/Understand) vs High (Apply through Create) complexity.
  - Quick check question: Why might "explain quantum computing" (Understand) have different alignment sensitivity than "debug this quantum algorithm" (Analyze)?

## Architecture Onboarding

- Component map:
  - Conversation preprocessing (language detection, domain filtering) -> Expertise Classifier (GPT-4-Turbo) -> SAT Score Estimator (SPUR) -> Alignment Calculator -> Regression Analysis

- Critical path:
  1. Conversation preprocessing (≥2 turns, English detection, domain filtering)
  2. Parallel expertise labeling (3 separate GPT-4 calls per conversation)
  3. Alignment scoring (difference calculations)
  4. Outcome correlation (piecewise regression on SAT, stratified by complexity)

- Design tradeoffs:
  - Single model (GPT-4) for all classification ensures consistency but may propagate systematic biases
  - 5-point scale provides granularity but human validators struggled with fine distinctions (70% agreement on 3-class simplified version)
  - English-only limits generalizability but enables human validation

- Failure signatures:
  - **Overestimation cascade**: Agent assumes higher user expertise → user receives inadequate explanations → satisfaction drops but classifier may not detect the cause
  - **Domain mismatch**: Same expertise prompt applied across domains may miss domain-specific expertise markers
  - **Conversation length confound**: Longer conversations from frustrated users (clarification loops) could be misread as engagement

- First 3 experiments:
  1. **Baseline alignment audit**: Run expertise classifier on your conversation logs; compute distribution of user/agent/gauged expertise and identify overestimation vs underestimation rates
  2. **Complexity-stratified satisfaction analysis**: Split conversations by task complexity; correlate alignment gaps with satisfaction separately for low vs high complexity tasks to replicate the moderating effect
  3. **Engagement-alignment validation**: Test whether matched expertise actually increases productive engagement (follow-up questions, task completion) vs frustrated verbosity (repetition, rephrasing)

## Open Questions the Paper Calls Out

- **Open Question 1**: Does experimentally manipulating agent expertise to align with user expertise cause a direct increase in user satisfaction, or is the observed relationship merely correlational?
  - Basis: The authors state results are correlational and call for future experiments to determine causal impact.
  - Why unresolved: The study relies on observational data where expertise levels were not experimentally controlled.
  - Evidence needed: Randomized controlled trials (A/B tests) where the agent is systematically prompted to respond at specific expertise levels relative to the user.

- **Open Question 2**: What specific real-time intervention strategies can effectively balance the trade-off between high agent expertise (preferred for satisfaction) and commensurate expertise (preferred for engagement)?
  - Basis: The Conclusion notes the need to "strike a balance" between high expertise and matched expertise for engagement.
  - Why unresolved: The paper quantifies misalignment effects but doesn't propose or test a mechanism for the agent to dynamically adjust response complexity during a live conversation.
  - Evidence needed: Development and testing of an adaptive agent architecture that detects user expertise in real-time and adjusts response complexity.

- **Open Question 3**: Do the negative effects of expertise misalignment on user satisfaction persist in multi-lingual conversational settings?
  - Basis: The Limitations section notes the analysis was restricted to English and calls for extending findings to multi-lingual domains.
  - Why unresolved: Cultural and linguistic differences may alter how expertise is signaled or perceived.
  - Evidence needed: Replicating the classification and correlation analysis on non-English conversation datasets with human validation.

## Limitations

- The study relies on inferred expertise and satisfaction measures rather than explicit user feedback, creating potential measurement error
- The SPUR satisfaction estimation framework is referenced but not fully detailed, making exact reproduction challenging
- The analysis is limited to English conversations, potentially limiting generalizability to other languages and cultural contexts

## Confidence

- **High confidence**: The correlation between agent expertise and positive satisfaction (77% of conversations at Proficient/Expert levels), and the negative impact of underestimation on satisfaction (particularly for complex tasks)
- **Medium confidence**: The engagement-alignment relationship (word count as engagement proxy) due to potential confounding from frustrated verbosity
- **Medium confidence**: The 5-point expertise scale calibration, given human validators achieved only 70% agreement on the simplified 3-class version

## Next Checks

1. Conduct human annotation validation on a sample of conversations to verify the accuracy of inferred expertise levels against ground truth user self-assessment
2. Test the sensitivity of satisfaction correlations to different complexity thresholds by re-running analyses with alternative task complexity taxonomies
3. Validate the engagement metric by distinguishing between productive conversation expansion versus clarification-loop verbosity through manual coding of conversation endpoints