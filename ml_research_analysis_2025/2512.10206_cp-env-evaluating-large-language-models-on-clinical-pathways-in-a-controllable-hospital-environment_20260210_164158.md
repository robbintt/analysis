---
ver: rpa2
title: 'CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable
  Hospital Environment'
arxiv_id: '2512.10206'
source_url: https://arxiv.org/abs/2512.10206
tags:
- clinical
- medical
- patient
- diagnosis
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CP-Env introduces a controllable agentic hospital environment to
  evaluate large language models across end-to-end clinical pathways. It simulates
  realistic patient-physician interactions spanning registration, specialist consultation,
  diagnostic testing, and treatment planning, with adaptive branching and multi-agent
  collaboration.
---

# CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment

## Quick Facts
- arXiv ID: 2512.10206
- Source URL: https://arxiv.org/abs/2512.10206
- Reference count: 40
- Key outcome: CP-Env evaluates LLMs on end-to-end clinical pathways; GPT-5 achieves 47.43% DR@5 and outperforms open-source models.

## Executive Summary
CP-Env introduces a controllable agentic hospital environment to evaluate large language models across end-to-end clinical pathways. It simulates realistic patient-physician interactions spanning registration, specialist consultation, diagnostic testing, and treatment planning, with adaptive branching and multi-agent collaboration. A three-tiered evaluation framework assesses Clinical Efficacy, Process Competency, and Professional Ethics. Results show proprietary models (GPT-5, Gemini-2.5-Pro) significantly outperform open-source alternatives in navigating complex pathways, while hallucinations and excessive reasoning steps hinder other models. GPT-5 achieves the highest diagnosis recall (47.43% DR@5) and excels in process competency and empathy. The study highlights the importance of targeted reasoning over exhaustive analysis in clinical workflows.

## Method Summary
CP-Env evaluates LLMs as physician agents navigating end-to-end clinical pathways (registration → specialist consultation → diagnostic testing → treatment planning) in a multi-agent hospital simulation. Patient cases sourced from top-tier medical journals cover 24 departments. Physician agents query tools (get_info, search_pubmed, organize_mdt, Wiki) and document in electronic medical records. A three-tier framework—Clinical Efficacy (Work Completion, DR@k, Triage Precision), Process Competency (Inquiry Sufficiency, Logic Coherence, Record Compliance, Investigation Coverage, Result Utilization), Professional Ethics (Privacy Safeguard, Treatment Individualization, Empathic Dialogue, Follow-up Planning)—is scored using LLM judges. Simulation uses patient agent (GPT-OSS-120B on 2×H100) and physician agent (model under test); vLLM for open-source models (2–8×H100), SGLang for Qwen3-Next.

## Key Results
- GPT-5 achieves highest diagnosis recall (47.43% DR@5) and excels in process competency and empathy.
- Excessive reasoning steps degrade clinical pathway performance; targeted reasoning is more effective.
- Proprietary models (GPT-5, Gemini-2.5-Pro) significantly outperform open-source alternatives; mid-tier models show U-shaped tool usage pattern.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulating multi-agent clinical pathways exposes reasoning failures that static benchmarks miss.
- Mechanism: CP-Env stages interactions (registration, specialist consultation, diagnostic testing, treatment planning) across physician and patient agents, using a shared medical record and tool access. This requires sustained reasoning, memory, and decision-making over long horizons, causing hallucinations and loss of diagnostic detail to surface as failure modes.
- Core assumption: Errors in real clinical workflows arise from sequential decision-making under uncertainty, not just knowledge gaps.
- Evidence anchors:
  - [abstract] "Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios."
  - [section 3] "Throughout this healthcare pathway, patient behavior is dynamic and responsive to physician requests... information between pathway nodes is managed through electronic medical records."
  - [corpus] Related work on behavioral testing (DeVisE) and multi-agent inpatient pathways (MAP) similarly finds dynamic evaluation reveals different failure modes than static QA.
- Break condition: If agents cannot maintain coherent state across pathway stages or tools are unreliable, the evaluation environment itself introduces confounds unrelated to model reasoning capability.

### Mechanism 2
- Claim: Excessive reasoning steps degrade clinical pathway performance due to loss of broader workflow awareness.
- Mechanism: Models with verbose reasoning chains can become myopically focused on immediate analysis, accumulating errors or missing transitions. CP-Env rewards targeted reasoning calibrated to decision points rather than exhaustive analysis.
- Core assumption: Optimal clinical reasoning balances depth and breadth; excessive exploration increases hallucination risk.
- Evidence anchors:
  - [abstract] "Interestingly, excessive reasoning steps can sometimes prove counterproductive."
  - [section 5.2] "Qwen3-Next provided detailed responses with comprehensive reasoning chains, occasional hallucinations led to significant deviations... GPT-5 demonstrated appropriate reasoning restraint without excessive elaboration."
  - [corpus] Not directly addressed in corpus; limited external validation of this specific claim.
- Break condition: If constrained reasoning is enforced without regard to case complexity, simpler cases may be under-served.

### Mechanism 3
- Claim: Internalized knowledge in frontier models reduces dependency on external tools, but mid-tier models may compensate via strategic tool use.
- Mechanism: GPT-5 and Gemini-2.5-Pro show minimal tool invocations while achieving high diagnostic recall, suggesting sufficient internal knowledge or reasoning. Mid-tier models (GLM-4.5-Air, Llama-4-Scout) invoke tools more frequently, potentially compensating for knowledge gaps.
- Core assumption: Tool usage patterns reflect underlying knowledge and uncertainty calibration.
- Evidence anchors:
  - [abstract] "Top models tend to exhibit reduced tool dependency through internalized knowledge."
  - [section 5.3] "GPT-5... invoked tools negligibly... GLM-4.5-Air recorded an average invocation frequency of 0.88... U-shaped relationship between tool usage and diagnostic accuracy."
  - [corpus] MedCoAct explores confidence-aware tool use in multi-agent clinical settings, supporting the link between confidence and tool dependency.
- Break condition: If tools provide noisy or incomplete information, high invocation rates may not translate to better outcomes; if tools are essential for complex cases, avoiding them may miss critical evidence.

## Foundational Learning

- Concept: Clinical pathway logic and stage transitions (registration -> triage -> consultation -> testing -> treatment).
  - Why needed here: CP-Env evaluates end-to-end pathway navigation; understanding each stage's purpose and failure modes is essential for debugging agent behavior.
  - Quick check question: Can you map the four CP-Env stages and describe the primary task at each?

- Concept: Hallucination detection in long-horizon agentic tasks.
  - Why needed here: Primary failure mode identified; hallucinations compound across stages and may not be visible in single-turn evaluations.
  - Quick check question: What distinguishes a hallucination in a clinical context from a reasonable differential diagnosis?

- Concept: Tool-use calibration (when and how agents should invoke external resources).
  - Why needed here: Non-linear tool dependency (U-shaped pattern) is a core finding; naive tool integration may not improve outcomes.
  - Quick check question: Given the U-shaped tool usage finding, would you expect a medium-performing model to benefit more from more tools or better reasoning?

## Architecture Onboarding

- Component map: Patient agent (GPT-OSS-120B backbone), Physician agents (evaluated models), Environment controller (manages stage transitions, medical records, tool APIs), Tool suite (MDT, PubMed, Wiki, get_info), Evaluation framework (Clinical Efficacy, Process Competency, Professional Ethics with 12 metrics).
- Critical path: Stage A (Registration) -> Stage B (Specialist Consultation) -> Stage C (Diagnostic Testing) -> Stage D (Advanced Diagnosis/Treatment). Each transition requires state handoff via medical record and tool invocation.
- Design tradeoffs: Verbose reasoning vs. targeted reasoning; high tool usage vs. internalized knowledge; standardized evaluation metrics vs. clinical nuance. CP-Env optimizes for pathway-level assessment over single-turn accuracy.
- Failure signatures: (1) Hallucination loops in extended workflows (models "entrapped" in reasoning), (2) Tool-calling format errors, (3) Loss of diagnostic detail across stages, (4) Myopic focus on immediate analysis losing broader pathway context.
- First 3 experiments:
  1. Run a mid-tier model (e.g., GLM-4.5-Air) with and without tool access to isolate the impact of tool dependency on diagnostic recall.
  2. Compare diagnostic recall across departments (Hematology vs. Dermatology) to validate the finding that lab-dependent specialties underperform.
  3. Ablate reasoning verbosity (e.g., prompt with "be concise" vs. "show detailed reasoning") on Qwen3-Next to test if excessive reasoning degrades pathway completion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can "reasoning traps"—where excessive analytical steps degrade performance—be mitigated in clinical agents?
- Basis in paper: [explicit] The authors observe that "excessive reasoning steps can sometimes prove counterproductive," citing Qwen3-Next's failure due to "excessive elaboration" and "reasoning loops" compared to the more restrained GPT-5.
- Why unresolved: The paper identifies the phenomenon where models become "myopically focused" but does not propose architectural or training solutions to regulate reasoning depth dynamically.
- What evidence would resolve it: A study comparing standard LLMs against those with adaptive reasoning termination criteria or "thought-budget" constraints within the CP-Env benchmark.

### Open Question 2
- Question: Does the U-shaped relationship between tool usage and diagnostic accuracy necessitate distinct optimization strategies for models of different capability levels?
- Basis in paper: [explicit] The analysis reveals a "U-shaped relationship," where top models rely on internal knowledge (low tool use) and low-performing models lack tool competence, while mid-tier models use tools most heavily to compensate for uncertainty.
- Why unresolved: It remains unclear if mid-tier models should be optimized towards better tool utilization (as the text suggests) or if the field should solely focus on the internalized reasoning exhibited by top-tier models.
- What evidence would resolve it: Experiments fine-tuning mid-tier models specifically for tool selection versus internal knowledge retention to see which approach yields greater performance gains on the DR@k metric.

### Open Question 3
- Question: How can LLMs be improved to master proactive, goal-oriented clinical consultation strategies in departments reliant on non-specific symptoms?
- Basis in paper: [explicit] In Appendix A.1, the authors note that "current LLMs have not yet fully mastered proactive, goal-oriented clinical consultation strategies," specifically failing in Hematology and Nephrology where patients provide only non-specific symptoms.
- Why unresolved: The paper highlights that models fail to "guide patients to complete necessary examinations" when explicit features are absent, but does not test specific interventions for this proactive behavior.
- What evidence would resolve it: Evaluation of models fine-tuned on active information retrieval reward models to see if they improve Triage Precision and Diagnosis Recall in the identified low-performing departments.

## Limitations
- Case diversity and representativeness are unclear; only one example case is shown.
- Judge model consistency and bias may affect subjective metric scores (e.g., empathy, follow-up planning).
- Tool availability and realism may not reflect real-world clinical decision support systems.

## Confidence
- **High confidence**: The observation that excessive reasoning steps can degrade clinical pathway performance is well-supported by within-study comparisons (e.g., Qwen3-Next vs. GPT-5) and is mechanistically plausible.
- **Medium confidence**: The claim that proprietary models (GPT-5, Gemini-2.5-Pro) significantly outperform open-source alternatives is supported by the reported metrics, but limited by the unknown judge model and case diversity.
- **Low confidence**: The assertion that hallucinations and reasoning loops are the primary failure modes for mid-tier models is plausible but not independently validated.

## Next Checks
1. Obtain or generate a diverse, expert-annotated set of patient cases covering the 24 departments, and run a subset through CP-Env with both LLM judges and human expert scoring to compare agreement.
2. Repeat the evaluation using multiple judge models (e.g., GPT-4, Claude-3, open-source alternatives) and analyze variance in metric scores, especially for subjective dimensions like empathy and follow-up planning.
3. Systematically vary tool availability (full access, restricted access, no access) for mid-tier models (e.g., GLM-4.5-Air, Llama-4-Scout) and measure impact on diagnostic recall and process competency, isolating the effect of tool use from reasoning quality.