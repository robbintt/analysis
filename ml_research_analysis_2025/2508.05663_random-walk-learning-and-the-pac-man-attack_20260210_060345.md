---
ver: rpa2
title: Random Walk Learning and the Pac-Man Attack
arxiv_id: '2508.05663'
source_url: https://arxiv.org/abs/2508.05663
tags:
- pac-man
- node
- probability
- definition
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the vulnerability of random-walk (RW) learning
  algorithms to a stealthy adversarial threat, termed the "Pac-Man" attack, where
  a malicious node probabilistically terminates any RW that visits it. This causes
  gradual extinction of RWs, halting decentralized learning without triggering failure
  alarms.
---

# Random Walk Learning and the Pac-Man Attack

## Quick Facts
- arXiv ID: 2508.05663
- Source URL: https://arxiv.org/abs/2508.05663
- Reference count: 40
- One-line primary result: A fully decentralized mechanism duplicates random walks when benign nodes detect long intervals between visits, preventing extinction under adversarial termination while maintaining convergence to a boundedly perturbed optimum.

## Executive Summary
This work addresses the vulnerability of random-walk (RW) learning algorithms to a stealthy adversarial threat, termed the "Pac-Man" attack, where a malicious node probabilistically terminates any RW that visits it. This causes gradual extinction of RWs, halting decentralized learning without triggering failure alarms. To counter this, the authors propose the Average Crossing (AC) algorithm—a fully decentralized mechanism that duplicates RWs when a benign node detects long intervals between RW visits. Theoretical analysis establishes that (i) the RW population remains almost surely bounded under AC, (ii) a phase transition in extinction probability occurs based on the duplication threshold, and (iii) RW-based SGD converges under AC in the presence of the adversary, with bounded deviation from the true optimum. Extensive experiments on synthetic and real-world datasets validate the theoretical findings, showing effective convergence and bounded RW populations. The proposed AC mechanism successfully mitigates the Pac-Man attack while maintaining decentralized learning performance.

## Method Summary
The method introduces the Average Crossing (AC) algorithm to defend against the Pac-Man attack in random-walk-based decentralized learning. Each benign node tracks the time interval since its last visit by any RW. When this interval exceeds a threshold A_u, the node probabilistically duplicates the visiting RW with probability q. This compensates for walks terminated by the adversary, maintaining a bounded population of active RWs. The approach is fully decentralized, requiring no global coordination or knowledge of the adversary's location. Theoretical analysis proves almost sure boundedness of the RW population, identifies a phase transition in extinction probability as a function of the duplication threshold, and establishes convergence of RW-SGD to a perturbed optimum under AC. Experiments validate these properties on synthetic and real-world datasets including MNIST.

## Key Results
- AC algorithm maintains bounded RW populations under adversarial termination with probability 1
- Extinction probability exhibits phase transition behavior based on duplication threshold A
- RW-SGD converges under AC to a perturbed optimum with bounded deviation from true optimum
- AC successfully mitigates Pac-Man attack on complete, regular, ring, and Erdős–Rényi graphs
- MNIST experiments show convergence with bounded RW populations under AC defense

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Average Crossing (AC) algorithm maintains bounded random walk populations under adversarial termination.
- Mechanism: Each benign node tracks the time interval since its last visit by any RW. When this interval exceeds threshold A_u, the node probabilistically (probability q) duplicates the visiting RW as an identical copy, compensating for walks terminated by the adversary.
- Core assumption: The underlying graph G is "robustly connected" (Definition 5), meaning benign nodes remain mutually reachable even if the Pac-Man node is removed, and the Markov chain is aperiodic.
- Evidence anchors:
  - [abstract] "fully decentralized mechanism for duplicating RWs to prevent RW extinction in the presence of Pac-Man"
  - [Section III] Algorithm 1 lines 5-6: "if t − L_t^(u) > A_u then With probability q, node u forks a new RW"
  - [corpus] Self-Creating Random Walks paper (FMR=0.549) addresses similar threat model
- Break condition: If A_u is set too large relative to graph mixing time, extinction probability approaches 1 (Proposition 1a); if too small, uncontrolled population growth may occur (though Theorem 1 proves almost sure boundedness).

### Mechanism 2
- Claim: RW-SGD converges to a perturbed optimum under AC, with quantifiable deviation from the true global optimum.
- Mechanism: A "chain" of RWs (Definition 1) behaves asymptotically as a single effective RW following the quasi-stationary distribution ν^(ζ) of the sub-stochastic transition matrix Q^(ζ), rather than the target distribution π. This induces bounded bias in the final solution.
- Core assumption: Assumptions 1-4 hold: i.i.d. RW evolution, robust graph connectivity, µ-strong convexity and L-smoothness of local objectives, bounded gradients at optimum.
- Evidence anchors:
  - [Section IV-C] Theorem 2: "RW-SGD converges to the minimizer of... E_{u∼π^(ζ)_chain}[f_u(x)]"
  - [Section IV-C] Proposition 2: "∥x̃* − x*∥ ≤ (1/µ)∥∇f(x̃*)∥"
  - [corpus] No direct corpus validation of convergence bounds; claims remain paper-internal
- Break condition: If stepsize η_t does not decay to 0, convergence is to a random variable rather than deterministic point; adversarial bias grows with distance between ν^(ζ) and π.

### Mechanism 3
- Claim: Extinction probability exhibits a phase transition as function of duplication threshold A.
- Mechanism: For the simplified W-AC variant on almost-fully-connected graphs, there exist critical values ᾱ ≥ α such that extinction is almost sure for A ≥ ᾱ but has probability < 1 for A ≤ α. Empirically, this soft transition appears in full AC across multiple topologies.
- Core assumption: The analysis for Proposition 1 requires the weak version (W-AC) where each RW decides autonomously based on its own visit history, rather than node-level decisions that introduce inter-RW correlations.
- Evidence anchors:
  - [Section IV-B] Proposition 1: formal phase transition statement for W-AC
  - [Figure 2b] Empirical extinction probability drops sharply near critical A on complete graphs
  - [Figure 6] Phase transition replicated on regular, ring, and Erdős–Rényi graphs
  - [corpus] No external validation; phase transition mechanism is paper-specific
- Break condition: Phase transition thresholds are topology-dependent; values derived for complete graphs do not directly transfer to sparse topologies without re-analysis.

## Foundational Learning

- Concept: **Random walks with Metropolis-Hastings transition matrices**
  - Why needed here: The RW-SGD algorithm requires constructing a transition matrix P that has the target sampling distribution π as its stationary distribution. Understanding how P relates to graph structure and π is essential for analyzing convergence.
  - Quick check question: Given a connected graph G and target distribution π, can you construct P such that π is stationary? What properties ensure irreducibility and aperiodicity?

- Concept: **Quasi-stationary distributions (QSDs) and Yaglom limits**
  - Why needed here: When RWs can be absorbed (terminated), the limiting distribution conditioned on survival is a QSD. The paper proves that chains of RWs converge to ν^(ζ), which is precisely the QSD of Q^(ζ), explaining the perturbed optimum.
  - Quick check question: For a Markov chain with absorbing states, what is the QSD and how does it differ from the standard stationary distribution? Why must Q^(ζ) be irreducible for a unique QSD to exist?

- Concept: **Supermartingale convergence and Lyapunov functions**
  - Why needed here: Theorem 1's proof constructs a Lyapunov function V(z) and shows that the sampled RW population process M_k is a supermartingale, enabling the application of Doob's convergence theorem to prove almost sure boundedness.
  - Quick check question: What conditions must a stochastic process satisfy to be a supermartingale? How does the Lyapunov function in Definition 11 transform Z_t into a process with drift toward boundedness?

## Architecture Onboarding

- Component map:
  - RW State Tracker -> Visit Time Recorder -> Duplication Logic -> RW-SGD Engine
  - Pac-Man Node (adversarial) -> RW State Tracker (termination)

- Critical path:
  1. Initialize z_0 RWs at random nodes with model x_0
  2. Each time step: active RWs move via transition matrix P
  3. On arrival at node u: if u = Pac-Man, terminate with probability ζ; else compute local gradient update
  4. Duplication check: if (t − L_t^(u)) > A_u, fork current RW with probability q
  5. Repeat until convergence or extinction

- Design tradeoffs:
  - **Threshold A**: Small A → frequent duplication, higher overhead, lower extinction risk; Large A → slower response, potential extinction but lower steady-state population
  - **Fork probability q**: High q → faster population recovery but risk of overshoot; Low q → conservative but may fail to compensate for aggressive Pac-Man
  - **Graph topology**: Dense graphs (complete, regular) mix faster, enabling smaller A; Sparse graphs (ring) require larger A to avoid premature duplication triggers

- Failure signatures:
  - **Extinction**: Z_t → 0; learning halts entirely. Indicates A too large for graph topology.
  - **Population explosion**: Z_t grows unbounded (contradicts Theorem 1 in practice suggests implementation error). Check that duplication occurs only when visit interval condition is met.
  - **Convergence to wrong optimum**: Model converges but with high test error. Indicates significant gap between ν^(ζ) and π due to Pac-Man's influence or data heterogeneity.

- First 3 experiments:
  1. **Threshold sweep on synthetic data**: Run AC on complete graph with N=100, vary A ∈ {1, 5, 10, 20, 50}, measure extinction probability and final loss. Expected: phase transition near A ≈ 10-15 based on Figure 2b.
  2. **Topology comparison with fixed A**: Test A=10 on complete, regular (d=8), ring, and ER (p=0.1) graphs. Expected: extinction probability increases as graph becomes sparser (Figure 6).
  3. **Convergence deviation under varying ζ**: Set ζ ∈ {0.1, 0.5, 1.0}, measure ∥x̃* − x*∥ on synthetic convex problem. Expected: larger ζ produces larger deviation, bounded by Proposition 2's inequalities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the soft phase transition in extinction probability be rigorously established for the full Average Crossing (AC) algorithm, rather than only its simplified Weak Version (W-AC)?
- Basis in paper: [inferred] Section IV.B states that analyzing the phase transition of the standard AC algorithm is "challenging due to the implicit correlations in the duplication decisions," leading the authors to analyze a simplified variant (W-AC) instead.
- Why unresolved: The decision for a node to duplicate in the full AC algorithm depends on the time since the *last visit by any RW*, creating complex dependencies between the trajectories of active RWs that are difficult to model mathematically.
- What evidence would resolve it: A formal proof demonstrating that the extinction probability for the full AC algorithm drops sharply to near-zero when the duplication threshold $A$ falls below a specific critical value on general graph topologies.

### Open Question 2
- Question: How does the theoretical framework and convergence guarantee extend to settings with multiple malicious Pac-Man nodes?
- Basis in paper: [explicit] Remark 2 in Appendix A notes that while the framework extends to multiple Pac-Man nodes, the analysis becomes "more involved" and "tedious" for $\zeta < 1$, and is primarily sketched for $\zeta = 1$.
- Why unresolved: The current theoretical analysis relies heavily on modeling the single adversary as a unified "super node" or absorbing state; characterizing the interacting effects of multiple independent termination probabilities on the effective stationary distribution is not fully derived.
- What evidence would resolve it: A generalized expression for the perturbed optimum $\tilde{x}^*$ and the bound on the RW population that explicitly accounts for the number and location of multiple adversarial nodes.

### Open Question 3
- Question: How does the AC algorithm perform if the graph is not "robustly connected," specifically if the Pac-Man node lies on the critical path between partitions of benign nodes?
- Basis in paper: [inferred] Definition 5 defines a "robustly connected" graph as one where no pair of benign nodes is disconnected by the Pac-Man, and Assumption 2 requires this property to hold, leaving the non-robust case unaddressed.
- Why unresolved: If the Pac-Man partitions the graph, the assumption that the Markov chain restricted to benign nodes is irreducible fails, potentially invalidating the convergence guarantees to the global optimum derived in Theorem 2.
- What evidence would resolve it: An analysis of RW-SGD convergence and population persistence on non-robust topologies, such as a line graph or star graph where the Pac-Man is the central bridge.

## Limitations

- Phase transition thresholds are derived only for the simplified W-AC algorithm on complete graphs, with no rigorous extension to the full AC algorithm on general topologies
- Theoretical analysis assumes robust graph connectivity, leaving performance on non-robust topologies (where Pac-Man lies on critical paths) unexplored
- Convergence bounds depend on specific step-size schedule and distance between quasi-stationary and target distributions, which are not fully characterized in practice

## Confidence

**High Confidence**: The mechanism of RW duplication at benign nodes when visit intervals exceed threshold A is well-specified and theoretically grounded. The proof of almost sure boundedness of RW population (Theorem 1) is rigorous, relying on standard supermartingale convergence arguments.

**Medium Confidence**: The phase transition behavior in extinction probability is empirically validated across multiple graph topologies, but the theoretical derivation is limited to the simplified W-AC setting. The practical relevance of the phase transition depends on correct threshold selection, which requires knowledge of graph mixing times.

**Low Confidence**: The convergence analysis in Section IV-C establishes that RW-SGD converges to a perturbed optimum, but the bounds depend on assumptions about step-size decay and the distance between quasi-stationary and target distributions. Without explicit knowledge of η_t and without external validation of these bounds, the practical accuracy of the convergence guarantees remains uncertain.

## Next Checks

1. **Replicate the Phase Transition Curve**: Implement the full AC algorithm on a complete graph (N=100) and sweep the duplication threshold A across a dense grid (e.g., A ∈ {1, 2, 5, 10, 15, 20, 50}). Measure extinction probability over 50 independent runs for each A. Verify the sharp drop in extinction probability near the critical threshold, and compare the observed transition point to the theoretical values α and ᾱ from Proposition 1.

2. **Validate Convergence Deviation Bounds**: Run RW-SGD on a convex synthetic problem (e.g., quadratic with µ-strong convexity and L-smoothness) under AC with ζ=0.5. After convergence, measure ∥x̃* − x*∥ and compare it to the bound (1/µ)∥∇f(x̃*)∥ from Proposition 2. Repeat for varying ζ to verify the bound scales appropriately with the adversarial strength.

3. **Step-Size Sensitivity Analysis**: Repeat the MNIST experiment with three different step-size schedules: (i) constant η=0.01, (ii) polynomial decay η_t = η₀/(1 + γt), and (iii) step decay η_t = η₀ for t < T₁, η₀/10 for t ≥ T₁. Measure final test accuracy and convergence speed for each schedule. This will reveal whether the convergence claims in Theorem 2 are robust to the choice of η_t.