---
ver: rpa2
title: 'HALoGEN: Fantastic LLM Hallucinations and Where to Find Them'
arxiv_id: '2501.08292'
source_url: https://arxiv.org/abs/2501.08292
tags:
- atomic
- response
- text
- units
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HALoGEN is a benchmark for measuring hallucination in large language
  models (LLMs) across nine diverse domains including code, summarization, scientific
  attribution, and historical events. It provides over 10,000 prompts and automatic
  verifiers to decompose model generations into atomic facts and verify them against
  high-quality knowledge sources.
---

# HALoGEN: Fantastic LLM Hallucinations and Where to Find Them

## Quick Facts
- **arXiv ID:** 2501.08292
- **Source URL:** https://arxiv.org/abs/2501.08292
- **Reference count:** 37
- **Primary result:** HALoGEN benchmark reveals hallucination rates from 4% to 86% across 9 domains, with GPT-4/GPT-3.5 best but all models still producing significant factual errors.

## Executive Summary
HALoGEN is a comprehensive benchmark for measuring hallucinations in large language models across nine diverse domains including code generation, summarization, scientific attribution, and historical events. The benchmark provides over 10,000 prompts and implements automatic verifiers that decompose model generations into atomic facts and verify them against high-quality knowledge sources. Evaluation of 14 LLMs on 150,000 generations reveals significant variation in hallucination rates across domains, with even the best models frequently producing factual errors. The study introduces a novel error classification system to trace hallucinations back to pretraining data and finds that no single domain is predictive of hallucination behavior across others.

## Method Summary
HALoGEN uses a decomposition-verification pipeline where LLM outputs are first broken into atomic units (individual facts, code imports, citations) using either regex or LLM-based extraction. Each atomic unit is then verified against a trusted knowledge source using task-specific verifiers ranging from API lookups (PyPI for code, Semantic Scholar for citations) to LLM-based entailment classifiers for text tasks. The benchmark covers 9 domains with 10,923 prompts, evaluating 14 LLMs on 150,000 generations to produce comprehensive hallucination metrics across different model families and capabilities.

## Key Results
- Hallucination rates vary dramatically across domains: 4% (biography simplification) to 86% (code summarization) depending on task complexity
- GPT-4 and GPT-3.5 show the best overall performance but still produce significant errors in most domains
- Open-source models like Llama-2-70B show high hallucination rates, particularly in code and summarization tasks
- No single domain is predictive of hallucination behavior across others, highlighting the need for diverse evaluation
- Error analysis reveals many hallucinations stem from incorrect or contextually misused information in pretraining data

## Why This Works (Mechanism)

### Mechanism 1: Decomposition and Verification Pipeline
- **Claim:** Breaking model outputs into "atomic units" enables granular hallucination detection across diverse tasks.
- **Mechanism:** LLM outputs are processed by a decomposition engine to extract individual factual claims, code imports, or citations. Each unit is then passed to a task-specific verifier for validation against a trusted source.
- **Core assumption:** Verifiers are highly accurate; a faulty verifier can misclassify correct facts as hallucinations or vice versa.
- **Evidence anchors:** [abstract] "...automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source."

### Mechanism 2: Training Data Attribution via Pretraining Corpora Search
- **Claim:** Hallucination sources can be traced to a model's pretraining data by searching for hallucinated content within large corpora.
- **Mechanism:** Identified hallucinated atomic facts are used as search queries against public pretraining datasets (e.g., C4, Dolma, OpenWebText). Their presence or absence categorizes the error.
- **Core assumption:** Access to the actual pretraining data is available (for open models) or a proxy corpus is representative.
- **Evidence anchors:** [abstract] "...errors often stem from incorrect or contextually misused information."

### Mechanism 3: Task-Specific Verification Protocols
- **Claim:** Different tasks require fundamentally different verification logic; a single detector is insufficient.
- **Mechanism:** The framework uses a range of verifiers: programmatic checks (PyPI for code, Semantic Scholar API for citations), LLM-based entailment (for summarization/simplification), and structured logic checks (for rationalization tasks).
- **Core assumption:** Each task's "source of truth" is stable and queryable via the chosen verifier method.
- **Evidence anchors:** [section 3.1, Table 1] "We implement an assortment of verifiers... ranging from entailment-based approaches... to searches for Python packages and scientific references."

## Foundational Learning

### Concept: Atomic Factual Units
- **Why needed here:** The core of the HALoGEN benchmark is to break complex paragraphs into small, verifiable statements.
- **Quick check question:** Can you split "Barack Obama was the 44th U.S. President and won a Nobel Prize" into two atomic units?

### Concept: Entailment in NLP
- **Why needed here:** This logic is used to verify summarization and simplification tasks. You must understand if a summary statement *follows logically* from the source text.
- **Quick check question:** Does "The fire caused no damage" entail from "The fire was quickly extinguished"?

### Concept: Hallucination Typology (Extrinsic vs. Intrinsic)
- **Why needed here:** The paper distinguishes between hallucinations from the model's internal knowledge (extrinsic) vs. misinterpreting provided context (intrinsic).
- **Quick check question:** In a summarization task, is adding a fact not in the source article an intrinsic or extrinsic hallucination?

## Architecture Onboarding

**Component map:** Input Prompt → LLM → Decomposition Engine → Atomic Units → Task-Specific Verifier → Hallucination Score

**Critical path:** The reliability of the Decomposition Engine and the Verifier are the bottlenecks; if either produces errors, the final benchmark score is unreliable.

**Design tradeoffs:** Using LLMs for decomposition/entailment allows scaling but introduces the irony of using potentially hallucinating models to detect hallucinations. Programmatic verifiers (e.g., PyPI) are more reliable but limited to structured domains like code.

**Failure signatures:**
- **Over-decomposition:** Splitting a single fact into multiple units, diluting the hallucination score.
- **Verifier drift:** An LLM-based entailment verifier becoming too strict or lenient.
- **Data-attribution gap:** Being unable to trace a hallucination because the pretraining data for a specific model is closed.

**First 3 experiments:**
1. **Verify the Verifier:** Manually check a sample of atomic units flagged as hallucinations to measure the precision and recall of your decomposition and verification pipeline.
2. **Cross-Model Correlation:** Run the same set of prompts through two different LLMs and compare their hallucination scores on different tasks to see if hallucination patterns are model-specific or task-specific.
3. **Source Trace:** Pick a sample of hallucinated code packages and manually search for them in a public corpus like C4 or The Stack to classify them as Type A, B, or C errors.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can causal frameworks be constructed to determine the counterfactual impact of specific pretraining datapoints on the generation of specific hallucinations?
- **Basis in paper:** [explicit] The authors state in Section 6, "Future work would construct causal frameworks, to study counterfactual questions about the inclusion of specific datapoints and their effect on specific model hallucinations to shed more light on the root cause of hallucination."
- **Why unresolved:** The current study traces hallucinations to data sources (Types A, B, and C) to establish correlation, but it cannot isolate whether the presence of a specific incorrect fact in the pretraining data *causes* the model to hallucinate or if the model would have fabricated it regardless.
- **What evidence would resolve it:** Experiments involving the retraining of models on datasets where specific "Type B" errors (incorrect facts in training data) are removed or corrected, resulting in a measurable change in hallucination rates for those specific facts.

### Open Question 2
- **Question:** How can hallucination benchmarks accurately measure the "coverage" (completeness) of model responses alongside factual precision?
- **Basis in paper:** [explicit] Section 8 notes, "our metrics do not account for coverage— whether the model response contains all the information it should. Future work would introduce methodologies to measure coverage."
- **Why unresolved:** The current Utility Score optimizes for the *absence* of errors (precision) and appropriate refusal. Consequently, a model could theoretically achieve a high score by providing very short, safe responses that omit significant amounts of relevant requested information.
- **What evidence would resolve it:** A new evaluation metric that quantifies the ratio of relevant ground-truth atomic facts successfully included in a generation relative to the total available in the trusted knowledge source.

### Open Question 3
- **Question:** To what extent do LLMs attribute factually incorrect claims to *real* references (as opposed to fabricating non-existent references)?
- **Basis in paper:** [explicit] In Section 3.1 regarding the Scientific Attribution task, the authors state: "We wish to note that even if references themselves are not hallucinated, LLMs may still attribute incorrect claims to them. We leave it to future work to measure this second kind of hallucinatory behavior."
- **Why unresolved:** The current benchmark verifies hallucinations by checking if the generated reference title exists in the Semantic Scholar index. It does not verify whether the content of that real paper actually supports the claim made in the prompt.
- **What evidence would resolve it:** A verification pipeline that retrieves the abstracts of the generated real references and computes an entailment score against the user's prompt to detect semantic inconsistencies.

## Limitations
- The benchmark's reliability depends heavily on the accuracy of decomposition and verification pipelines, which are not fully validated
- The use of LLMs for both decomposition and entailment verification introduces potential circularity
- Pretraining data attribution method is limited to models with accessible training corpora, excluding most commercial LLMs
- Assumes atomic units are the appropriate granularity for hallucination detection, which may miss complex reasoning errors

## Confidence

**High Confidence:** The observation that hallucination rates vary significantly across domains (4% to 86%) and that no single domain predicts performance across others. The finding that GPT-4 and GPT-3.5 outperform open-source models is also well-supported.

**Medium Confidence:** The error classification system (Type A, B, C) and the conclusion that many hallucinations stem from incorrect or contextually misused information. The methodology for attribution is plausible but not exhaustively validated.

**Low Confidence:** The absolute hallucination scores for individual models, as they depend heavily on the specific decomposition and verification protocols used, which may not generalize across different evaluation settings.

## Next Checks

1. **Verifier Calibration Study:** Conduct a human evaluation of 200+ atomic units flagged as hallucinations to measure the precision and recall of the decomposition and verification pipeline, comparing results against the paper's reported ~91-92% agreement rates.

2. **Cross-Domain Consistency Test:** Select three models with varying hallucination rates (e.g., GPT-4, Llama-2-70B, and an open-source model) and evaluate them on a held-out set of prompts across all nine domains to confirm that performance in one domain does not predict performance in others.

3. **Pretraining Data Attribution Validation:** Manually search for a sample of hallucinated facts (e.g., 50 code packages or scientific references) in publicly available corpora like C4 or The Stack to validate the error classification (Type A, B, C) and assess the feasibility of tracing hallucinations to training data.