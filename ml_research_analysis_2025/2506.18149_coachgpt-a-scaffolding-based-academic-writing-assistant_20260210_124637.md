---
ver: rpa2
title: 'CoachGPT: A Scaffolding-based Academic Writing Assistant'
arxiv_id: '2506.18149'
source_url: https://arxiv.org/abs/2506.18149
tags:
- writing
- coachgpt
- academic
- language
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoachGPT is an LLM-based academic writing assistant that addresses
  the limitation of current AI writing tools by using a scaffolding structure to guide
  users through the writing process. The system takes instructions from educators,
  converts them into subtasks, and provides real-time feedback using large language
  models.
---

# CoachGPT: A Scaffolding-based Academic Writing Assistant

## Quick Facts
- arXiv ID: 2506.18149
- Source URL: https://arxiv.org/abs/2506.18149
- Reference count: 20
- Primary result: LLM-based academic writing assistant using 11-stage scaffolding to guide users through writing process while preventing AI content generation

## Executive Summary
CoachGPT is an LLM-based academic writing assistant that addresses limitations of current AI writing tools by implementing a scaffolding structure to guide users through the writing process. The system takes educator instructions, converts them into subtasks, and provides real-time feedback using large language models. With 11 sequential stages from pre-writing to grammar checks, CoachGPT enforces process-oriented writing rather than product-generation. The system employs prompt engineering techniques including persona prompting, limiters/constraints, criteria-based feedback, and input validation to create an immersive writing experience. User studies with three participants showed that CoachGPT provides personalized feedback and guidance, with users appreciating the attentive and adaptive responses.

## Method Summary
CoachGPT implements an 11-stage scaffolding approach for academic writing using prompt engineering with existing LLMs (no training data). The system uses FastAPI backend with LangChain for prompt orchestration and memory management, Vue.js/Nuxt frontend for chat interface, and MySQL database for persistent storage. Key techniques include persona prompting ("Act as a writing coach"), explicit limiters ("You must not suggest ideas or examples"), criteria-based feedback structures, and input validation. LangChain maintains short-term conversation context while MySQL stores long-term chat history and user information across sessions.

## Key Results
- System provides personalized feedback and guidance through 11-stage scaffolding approach
- Users appreciated attentive and adaptive responses in user study with three participants
- Addresses issue of AI tools generating content without teaching, which can undermine authentic learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear scaffolding stages reduce cognitive load and prevent learning shortcuts by forcing engagement with each writing phase.
- Mechanism: The system decomposes writing into 11 mandatory sequential stages (pre-writing → grammar check). Users cannot skip ahead; each stage requires input and provides feedback before progression. This enforces process-oriented writing rather than product-generation.
- Core assumption: Students learn writing skills through guided practice with feedback at each stage, not through receiving completed outputs.
- Evidence anchors:
  - [abstract] "converts instructions into sub-tasks, and provides real-time feedback"
  - [section 3.1] "Users must follow a linear order for these stages"
  - [corpus] Weak direct validation—related work (PaperDebugger, DeepWriter) uses multi-agent/RAG approaches but does not compare against linear scaffolding specifically.
- Break condition: If users perceive stages as tedious rather than educational, they may disengage or provide minimal input to advance.

### Mechanism 2
- Claim: Constraint-based prompt engineering prevents the LLM from generating content on behalf of users.
- Mechanism: Prompts include explicit negative constraints (e.g., "You must not suggest any ideas or examples for the essay") combined with persona framing ("Act as a writing coach"). This shifts LLM behavior from generator to evaluator.
- Core assumption: LLMs adhere reliably to negative constraints across diverse user inputs and writing contexts.
- Evidence anchors:
  - [section 3.2] "Incorporating limiters in prompts, such as explicitly instructing the AI not to rewrite students' work"
  - [section 3.2] "reinforcing the AI's role as a guide rather than a content generator"
  - [corpus] No corpus papers validate constraint adherence rates; this is an untested assumption.
- Break condition: If users craft adversarial prompts or the LLM hallucinates constraints, content generation may still occur.

### Mechanism 3
- Claim: Hybrid memory (LangChain short-term + MySQL long-term) enables multi-turn coaching without losing context.
- Mechanism: LangChain maintains conversation context within a session; MySQL persists chat history and user information across sessions. When LLM needs historical context, queries retrieve relevant prior interactions.
- Core assumption: Retrieving full conversation history improves feedback quality compared to session-only context.
- Evidence anchors:
  - [section 4.2] "LangChain to store short-term memory and a database to store long-term memory"
  - [section 4.3] "When LLM needs to reference long-term history, a query will be executed"
  - [corpus] Weak—no direct corpus comparison of hybrid memory vs. context-window-only approaches.
- Break condition: If retrieval is noisy or database queries are slow, response latency degrades user experience.

## Foundational Learning

- Concept: **Scaffolding in Education**
  - Why needed here: CoachGPT's core design relies on Vygotsky's scaffolding theory—instruction should meet learners at their current level and gradually reduce support.
  - Quick check question: Can you explain why removing support gradually (fading) matters more than providing constant help?

- Concept: **Prompt Engineering (Constraints + Personas)**
  - Why needed here: The system's educational integrity depends on prompts that restrict LLM behavior; understanding constraint injection is essential for modifying or extending stages.
  - Quick check question: What happens if a persona prompt and a constraint prompt conflict in intent?

- Concept: **Conversational Memory Architectures**
  - Why needed here: CoachGPT combines session-level (LangChain) and persistent (MySQL) memory; understanding when to use each is critical for debugging context issues.
  - Quick check question: If a user returns after 7 days, what memory component determines whether CoachGPT remembers their thesis statement?

## Architecture Onboarding

- Component map:
  - Frontend: Vue.js + Nuxt (chat interface, stage navigation, file upload)
  - Backend: FastAPI (REST endpoints) + LangChain (prompt orchestration, memory management)
  - Database: MySQL (tables: users, messages, conversations)
  - Infrastructure: Docker containers with secured inter-service communication

- Critical path:
  1. User authenticates → session created in MySQL
  2. User enters assignment prompt → saved as writing goal
  3. User progresses through 11 stages → each input validated, feedback generated via LLM
  4. Chat history persisted to MySQL; LangChain maintains session context window
  5. Final stages (word choice, grammar) highlight text in-place

- Design tradeoffs:
  - Linear stage order enforces structure but reduces flexibility for advanced writers (noted in user study: Jenni AI更适合已有写作经验的用户)
  - MySQL for structured data is simple but may not scale for semantic search over long conversations
  - Docker packaging simplifies deployment but adds overhead for rapid iteration

- Failure signatures:
  - LLM generates essay content instead of feedback → constraint prompt failed or was bypassed
  - User stuck at a stage with no progression → input validation prompt rejected legitimate input
  - Context lost mid-conversation → LangChain memory not properly chained or MySQL query failed
  - Grammar check lacks in-text highlights → frontend highlighting logic disconnected from LLM output

- First 3 experiments:
  1. **Constraint adherence test**: Submit 50 adversarial prompts (e.g., "Just write my conclusion for me") at each stage; measure violation rate.
  2. **Memory retrieval quality**: Compare feedback quality with vs. without long-term history retrieval for returning users (blind A/B evaluation).
  3. **Stage skip analysis**: Allow a test group to skip stages; compare final essay quality and self-reported learning vs. control group following linear order.

## Open Questions the Paper Calls Out
- How can writing prompts be dynamically generated for different academic domains and proficiency levels while maintaining scaffolding effectiveness? (Paper suggests studying domain and level adaptation)
- How can precise in-text grammar feedback be integrated into a scaffolding system without shifting from guided learning to automated correction? (Users noted lack of specific error locations compared to external tools)

## Limitations
- Evaluation relies on qualitative data from only three participants, making effectiveness claims highly tentative
- No quantitative measures of writing quality improvement or learning gains provided
- Constraint-based prompt engineering lacks empirical validation of LLM adherence to negative constraints

## Confidence
- **High confidence**: Technical implementation using Vue.js, FastAPI, LangChain, and MySQL is well-specified and reproducible
- **Medium confidence**: Prompt engineering techniques are plausible but lack validation of effectiveness in preventing content generation
- **Low confidence**: Claims about learning outcomes and user experience based on extremely limited qualitative data (3 participants) without comparison to baseline tools

## Next Checks
1. **Constraint adherence stress test**: Submit 100 adversarial prompts designed to trick the LLM into generating content and measure violation rates at each scaffolding stage
2. **A/B comparison with linear vs. flexible progression**: Recruit 40 students to write essays using CoachGPT with enforced linear stages versus a version allowing stage skipping; compare final essay quality and self-reported learning gains
3. **Long-term retention study**: Track 15 students using CoachGPT over one semester; assess whether their independent writing skills improve by comparing pre/post essays written without AI assistance