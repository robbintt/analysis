---
ver: rpa2
title: When Reasoning Meets Its Laws
arxiv_id: '2512.17901'
source_url: https://arxiv.org/abs/2512.17901
tags:
- reasoning
- compute
- arxiv
- preprint
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Laws of Reasoning (LoRe), a theoretical
  framework formalizing how large reasoning models should scale their computation
  and accuracy with question complexity. The authors propose two key principles: compute
  law (reasoning effort should scale linearly with complexity) and accuracy law (accuracy
  should decay exponentially with complexity), which they approximate via monotonicity
  and compositionality properties.'
---

# When Reasoning Meets Its Laws

## Quick Facts
- arXiv ID: 2512.17901
- Source URL: https://arxiv.org/abs/2512.17901
- Authors: Junyu Zhang; Yifan Sun; Tianang Leng; Jingyan Shen; Liu Ziyin; Paul Pu Liang; Huan Zhang
- Reference count: 40
- One-line primary result: Introduces Laws of Reasoning framework and SFT-Compo method that improves reasoning compositionality by 22.5% and boosts downstream performance by +5.0% Pass@1

## Executive Summary
This paper introduces the Laws of Reasoning (LoRe), a theoretical framework formalizing how large reasoning models should scale computation and accuracy with question complexity. The authors propose two key principles: compute law (reasoning effort should scale linearly with complexity) and accuracy law (accuracy should decay exponentially with complexity). To evaluate these principles, they create LORE-BENCH, a comprehensive benchmark measuring these properties across 10 reasoning models. Results show models exhibit reasonable monotonicity but fail at compositionality. To address this, the authors develop SFT-Compo, a simple fine-tuning approach enforcing compute compositionality. Extensive experiments demonstrate that SFT-Compo significantly improves compositionality and consistently enhances reasoning performance across six mathematical benchmarks.

## Method Summary
The paper proposes SFT-Compo, a fine-tuning method that enforces compositional compute allocation by selecting training triplets that minimize deviation from additive compute expectations. The method samples K outputs per question from a teacher model, then selects triplets (r₁*, r₂*, r₁₂*) that minimize |ℓ(r₁) + ℓ(r₂) - ℓ(r₁₂)| among all correct-answer combinations. This is applied to a dataset of ~3.9K question pairs constructed from the DeepScaler subset, annotated by GPT-4.1-mini into five categories. Models are fine-tuned for 5 epochs with batch size 16 (effective 8 with gradient accumulation) and learning rate grid search.

## Key Results
- SFT-Compo improves compositionality (nMAD) by 22.5% on 8B models
- SFT-Compo consistently improves reasoning performance across six mathematical benchmarks with average +5.0 Pass@1 accuracy gain
- Enforcing compositionality improves monotonicity (Spearman correlation increases from 0.875 → 0.977) and accuracy compositionality (nMAD drops 71.1% on 1.5B models)
- 8B models show best overall balance of compositionality improvement and downstream performance

## Why This Works (Mechanism)

### Mechanism 1: Compositional Supervision Selection
- **Claim**: Selecting training triplets that minimize compute deviation from additive expectation improves reasoning compositionality and downstream performance.
- **Mechanism**: During dataset construction, for each question pair (x₁, x₂) and their composition x₁₂, the method samples K outputs per question and selects the triplet (r₁*, r₂*, r₁₂*) that minimizes |ℓ(r₁) + ℓ(r₂) - ℓ(r₁₂)| among all correct-answer combinations.
- **Core assumption**: Correct reasoning paths that satisfy compositional compute represent learnable, transferable reasoning patterns.
- **Evidence anchors**: [abstract]: "SFT-Compo significantly improves compositionality (reducing nMAD by 22.5% on 8B models)"

### Mechanism 2: Compute Law Approximation via Proxy Properties
- **Claim**: Monotonicity and compositionality properties are sufficient practical proxies for the theoretical compute law (linear scaling with complexity).
- **Mechanism**: Direct complexity measurement is intractable, so the framework substitutes this with monotonicity (comparing compute across known complexity orderings) and compositionality (testing additivity over independent question pairs).
- **Core assumption**: Independence can be operationalized via disjoint concept sets, which approximates true complexity independence.
- **Evidence anchors**: [abstract]: "Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties... monotonicity and compositionality"

### Mechanism 3: Cross-Law Synergistic Transfer
- **Claim**: Enforcing compute compositionality implicitly improves monotonicity and accuracy compositionality without explicit training for those properties.
- **Mechanism**: SFT-Compo targets only compute compositionality, yet results show improvements in monotonicity and accuracy compositionality.
- **Core assumption**: Property improvements transfer because they share underlying mechanism representations.
- **Evidence anchors**: [Section 5.3]: "Enforcing compositionality in reasoning compute improves its monotonicity"

## Foundational Learning

- **Concept: Test-Time Compute Scaling**
  - **Why needed here**: The compute law assumes reasoning effort should adapt to problem complexity at inference time, not just via training.
  - **Quick check question**: If a model generates 1000 reasoning tokens for a simple addition problem but 500 tokens for a complex integration, which property does this violate?

- **Concept: Turing Machine Complexity (κ)**
  - **Why needed here**: The paper defines complexity as minimal unit-cost primitive steps of a deterministic Turing machine.
  - **Quick check question**: Why can't we directly compute κ(x) for arbitrary questions, necessitating proxy properties?

- **Concept: Normalized Mean Absolute Deviation (nMAD)**
  - **Why needed here**: The primary metric for measuring compositionality, normalizing compute deviation by total compute.
  - **Quick check question**: If C(x₁₂) = 1500, C(x₁) + C(x₂) = 2000, and Σ|C(x₁) + C(x₂)| over the dataset = 50,000, what is nMAD?

## Architecture Onboarding

- **Component map**: Seed question design → Variant generation → Concept annotation → Triplet construction → Output sampling → Compositional selection → SFT training → LORE-BENCH evaluation → Downstream benchmark validation

- **Critical path**: DeepScaler subset → GPT-4.1-mini annotation → MATH500 question pairing → Teacher output sampling (K=8) → Compositional triplet selection → Student fine-tuning → LORE-BENCH evaluation

- **Design tradeoffs**:
  - K=8 samples per question balances computational cost with selection quality
  - Coarse concept categories (Algebra, Geometry, etc.) ensure independence but may miss fine-grained dependencies
  - DeepSeek-R1-14B teacher choice affects triplet yield and quality

- **Failure signatures**:
  - "Fail to think" (1.5B models generate zero reasoning tokens)
  - Periodic shortcuts in variants with cyclic answer patterns
  - Underthinking on composites (composite compute < sub-question compute sum)

- **First 3 experiments**:
  1. Baseline monotonicity check: Run DeepSeek-R1-Distill-1.5B on LORE-MONO; expect domain-specific failures
  2. Compositionality violation quantification: Compute nMAD on LORE-COMPO before/after SFT-Compo; target 22-40% reduction
  3. Ablation on K: Test K ∈ {4, 8, 16} to characterize tradeoff between triplet availability and selection quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the accuracy law be directly enforced during training in a manner analogous to SFT-Compo for compute compositionality?
- **Basis in paper**: [explicit] The paper notes: "Accuracy compositionality is not easy to enforce directly, as it does not specify which reasoning path should be selected for supervision."
- **Why unresolved**: No equivalent criterion exists for accuracy multiplicativity since multiple reasoning paths can yield correct answers.
- **What evidence would resolve it**: Development of a training method that enforces accuracy compositionality and demonstrates improved nMAD for log-accuracy on LORE-COMPO.

### Open Question 2
- **Question**: What is the mechanistic explanation for why enforcing compute compositionality improves accuracy compositionality?
- **Basis in paper**: [explicit] Figure 7b shows nMAD for log-accuracy drops from 2.368 to 0.685 after SFT-Compo, described as "synergistic effects" without full explanation.
- **Why unresolved**: The paper empirically observes this cross-law benefit but does not provide theoretical justification.
- **What evidence would resolve it**: A theoretical analysis or ablation study identifying which aspects of compositional training cause accuracy improvements.

### Open Question 3
- **Question**: Can the independence criterion for compositionality be rigorously formalized beyond disjoint concept sets?
- **Basis in paper**: [explicit] Appendix C states: "We operationalize independence through disjoint sets of mathematical concepts. Although this proxy is not rigorous..."
- **Why unresolved**: Disjoint concept sets are a heuristic proxy; true independence may not hold even when concept sets are disjoint.
- **What evidence would resolve it**: A formal definition of question independence validated against measured complexity values.

## Limitations
- The independence assumption (disjoint concept sets implying independent complexity) is a heuristic that may not capture true computational dependencies
- The selection mechanism conflates mathematical correctness with appropriate computational allocation
- Synergistic transfer effects are observed but not mechanistically explained
- Teacher model quality significantly impacts triplet selection validity and downstream performance

## Confidence
**High confidence**:
- SFT-Compo improves compositionality (22.5% nMAD reduction on 8B models)
- SFT-Compo improves downstream reasoning performance (+5.0% Pass@1 average)

**Medium confidence**:
- LoRe framework provides useful theoretical foundation for reasoning model analysis
- Compute and accuracy laws are meaningful organizing principles
- Monotonicity and compositionality are sufficient proxies for theoretical laws

**Low confidence**:
- Synergy between reasoning properties has fundamental rather than coincidental nature
- Concept-disjoint independence assumption holds for LORE-COMPO
- SFT-Compo selection criterion captures genuine compositional reasoning rather than artifact

## Next Checks
1. **Ablation on Teacher Model Quality**: Run SFT-Compo using weaker and stronger teachers to quantify how teacher reasoning quality affects triplet selection validity and downstream performance.

2. **Hidden Dependency Detection**: For triplets where models show large compositionality violations, conduct human analysis to identify potential hidden computational dependencies that violate the independence assumption.

3. **Mechanism Isolation Experiment**: Create variants of SFT-Compo that target monotonicity and accuracy compositionality directly and compare performance to determine whether observed synergies are fundamental property interactions.