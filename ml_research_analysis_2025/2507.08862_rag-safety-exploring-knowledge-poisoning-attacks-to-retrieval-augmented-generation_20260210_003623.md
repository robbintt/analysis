---
ver: rpa2
title: 'RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation'
arxiv_id: '2507.08862'
source_url: https://arxiv.org/abs/2507.08862
tags:
- attack
- kg-rag
- adversarial
- knowledge
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of data poisoning
  attacks on knowledge graph-based retrieval-augmented generation (KG-RAG) systems.
  The authors propose an attack strategy that inserts perturbation triples into the
  knowledge graph to construct misleading inference chains, thereby degrading KG-RAG
  performance.
---

# RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2507.08862
- **Source URL:** https://arxiv.org/abs/2507.08862
- **Reference count:** 40
- **Primary result:** Attack can reduce QA F1 scores by up to 46% on WebQSP using minimal triple insertions

## Executive Summary
This paper presents the first systematic study of data poisoning attacks on knowledge graph-based retrieval-augmented generation (KG-RAG) systems. The authors propose an attack strategy that inserts perturbation triples into the knowledge graph to construct misleading inference chains, thereby degrading KG-RAG performance. The attack operates under a realistic black-box setting where the attacker can only insert a small number of triples using existing entities and relations. Experiments on two benchmarks and four representative KG-RAG methods show that the attack can significantly reduce QA performance, with F1 scores dropping by up to 46% on WebQSP. The attack is particularly effective in the retrieval stage, with over 90% of questions retrieving at least one adversarial triple.

## Method Summary
The attack follows a three-stage pipeline: (1) GPT-4 generates 5 plausible but incorrect answers per question, (2) LLMRoG generates relation paths to connect the question's topic entity to these adversarial answers, and (3) perturbation triples are inserted into the KG to complete these misleading inference chains. The attacker operates under a black-box threat model, only able to insert triples using existing KG entities and relations. The attack targets four KG-RAG methods (RoG, GCR, G-Retriever, SubgraphRAG) on WebQSP and CWQ benchmarks, measuring performance degradation through F1, Hits@1, and Exact Match metrics.

## Key Results
- Attack reduces RoG's F1 score by 46% on WebQSP with only 20 triples inserted per question
- Over 90% of poisoned triples are successfully retrieved during the attack
- Retrieval stage constitutes the critical vulnerability point across all tested KG-RAG methods
- Attack effectiveness is particularly pronounced on path-based retrievers compared to subgraph-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Inference Chain Completion
The attack succeeds by constructing coherent multi-hop reasoning paths rather than inserting isolated noise, leveraging the structural dependencies of KG-RAG retrievers. The attacker identifies a "topic entity" and an "adversarial answer entity" and inserts triples that bridge these entities via relation paths the retriever expects. Core assumption: retrievers prioritize connected subgraphs or valid relation paths over isolated triples. Evidence: abstract states "inserts perturbation triples to complete misleading inference chains in the KG."

### Mechanism 2: Retrieval Stage Brittleness
The retrieval stage acts as the primary attack surface because it lacks robust verification for the veracity of structural paths. Retrievers score triples based on semantic similarity and structural connectivity to the question entity. Since injected triples use existing entities and relations, they score high on structural relevance. Core assumption: retrievers prioritize structural validity/connectivity over provenance verification. Evidence: abstract states "retrieval stage being the critical weak point."

### Mechanism 3: Semantic Plausibility & Generator Over-reliance
The attack circumvents LLM fact-checking filters by selecting adversarial targets that are semantically plausible within the question's context. The attacker uses a general LLM to generate incorrect answers that match the type or context of the question. When the retriever presents the misleading chain, the generator accepts the plausible but incorrect structural evidence over its own parametric knowledge. Core assumption: LLMs rely heavily on provided context when it appears structurally coherent and semantically type-consistent. Evidence: section 3.1 states "guides an LLM to produce incorrect but semantically plausible answers."

## Foundational Learning

- **Concept: Knowledge Graph (KG) Triples & Reasoning Paths**
  - **Why needed here:** To understand the attack vector. Unlike standard RAG (text chunks), KG-RAG relies on structured data `(Subject, Relation, Object)`. The attack exploits the composition of these triples into multi-step paths.
  - **Quick check question:** If you insert the triple `(Manchester, cityOf, England)`, how does this create a "path" that might mislead a question about the movie "Manchester By The Sea"?

- **Concept: Black-Box Threat Model**
  - **Why needed here:** To scope the severity of the vulnerability. This paper demonstrates significant damage without requiring access to model weights or gradientsâ€”only the ability to edit a public database (the KG).
  - **Quick check question:** Does the attacker need to know the internal weights of the LLM used in the RAG system to execute this attack?

- **Concept: Type Consistency in Entity Linking**
  - **Why needed here:** To understand why the "Adversarial Answer Generation" step works. The attack fails if the wrong answer is obviously absurd (e.g., "Banana" as a country); it succeeds by picking wrong answers of the correct semantic type.
  - **Quick check question:** Why is "Russia" a better adversarial target than "Apple" for a question asking "Which country hosted the Olympics?"

## Architecture Onboarding

- **Component map:**
  - Generator (Attacker) -> Path Synthesizer -> Perturbation Injector -> (Poisoned KG) -> Retriever -> Generator (Target)

- **Critical path:**
  1. **Entity Linking:** Question is mapped to a topic entity in the KG
  2. **Retrieval:** The Retriever traverses the KG. If a poisoned path exists (e.g., `Topic -> PerturbedRelation -> AdversarialEntity`), it is retrieved with high priority due to structural connectivity
  3. **Generation:** The Target LLM consumes the retrieved path. If the path is coherent, the LLM outputs the adversarial entity

- **Design tradeoffs:**
  - Path-based vs. Subgraph Retrieval: Path-based methods (like RoG) might be more brittle to specific chain attacks than subgraph methods, though both suffer
  - Performance vs. Robustness: Specialized KG-LLMs (like RoG's fine-tuned model) achieve higher baseline performance but show greater relative degradation under attack compared to general models like GPT-4, likely because they trust structural paths more implicitly

- **Failure signatures:**
  - High A-RR (Attack Retrieved Ratio): >90% of poisoned triples are successfully retrieved
  - Type-Specific Hallucinations: The system answers with a factually wrong entity that is nonetheless semantically type-compatible
  - Performance Collapse: Sudden drops in Exact Match (EM) scores despite minimal injected data

- **First 3 experiments:**
  1. **Baseline Retrieval Validation:** Implement a standard KG-RAG retriever (e.g., RoG) on a clean dataset (WebQSP). Verify it retrieves the correct ground-truth path
  2. **Perturbation Injection:** Manually inject a single perturbation triple that creates a shorter or equally weighted path to a wrong entity. Re-run retrieval to confirm the poisoned path is fetched (High A-RR)
  3. **Generator Sensitivity Test:** Pass the poisoned retrieved context to different LLM backends (e.g., Llama-2 vs. GPT-4). Measure if the LLM corrects the error or hallucinates the wrong answer (High A-Precision)

## Open Questions the Paper Calls Out
None

## Limitations
- Knowledge Graph Representation Ambiguity: Attack effectiveness may vary across different KG schemas (RDF vs. property graphs) and levels of schema richness
- Path Grounding Complexity: The distribution and selection criteria for sampled intermediate entities when path grounding fails remain underspecified
- Evaluation Scope Constraints: Experiments focus on two benchmarks with constrained question types; effectiveness on diverse, open-domain queries remains uncertain

## Confidence

**High Confidence:** The core finding that retrieval-stage vulnerabilities enable significant performance degradation under poisoning attacks (particularly the 46% F1 drop on WebQSP for RoG)

**Medium Confidence:** The specific mechanism that path-based retrievers are more vulnerable than subgraph-based methods, as the comparison is limited to four methods and

## Next Checks
1. Verify the exact sampling strategy for "bridge entities" when path grounding fails by examining the implementation details
2. Confirm the fuzzy string matching threshold and library used to align GPT-4 outputs to KG entity IDs
3. Test the attack on additional KG-RAG methods beyond the four evaluated to assess generalizability of the vulnerability