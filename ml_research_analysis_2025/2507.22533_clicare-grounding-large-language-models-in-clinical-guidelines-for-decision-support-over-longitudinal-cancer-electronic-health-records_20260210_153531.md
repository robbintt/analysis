---
ver: rpa2
title: 'CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision
  Support over Longitudinal Cancer Electronic Health Records'
arxiv_id: '2507.22533'
source_url: https://arxiv.org/abs/2507.22533
tags:
- clinical
- patient
- clicare
- dataset
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CliCARE is a framework that addresses key challenges in clinical
  decision support over longitudinal cancer EHRs by transforming unstructured records
  into Temporal Knowledge Graphs and aligning them with clinical guidelines. The method
  overcomes long-context processing limitations and reduces clinical hallucination
  through structured temporal reasoning and guideline-grounded alignment.
---

# CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records

## Quick Facts
- **arXiv ID:** 2507.22533
- **Source URL:** https://arxiv.org/abs/2507.22533
- **Reference count:** 39
- **Key outcome:** CliCARE significantly outperforms leading long-context LLMs and KG-enhanced RAG methods on both Chinese and English cancer datasets, achieving superior performance on Clinical Summary and Clinical Recommendation tasks.

## Executive Summary
CliCARE addresses key challenges in clinical decision support over longitudinal cancer EHRs by transforming unstructured records into Temporal Knowledge Graphs (TKGs) and aligning them with clinical guidelines. The framework overcomes long-context processing limitations and reduces clinical hallucination through structured temporal reasoning and guideline-grounded alignment. A robust evaluation protocol using Expert-Validated LLM-as-a-Judge shows high correlation with expert oncologist assessments. Experimental results demonstrate CliCARE's superiority over strong baselines, including leading long-context LLMs and KG-enhanced RAG methods.

## Method Summary
CliCARE processes longitudinal cancer EHRs through a multi-stage pipeline: first, it uses a Longformer-based pipeline to compress historical records and BERT-based extraction to identify key clinical facts, then maps these to a structured Temporal Knowledge Graph with hierarchical timestamps. The framework aligns patient trajectories with clinical guidelines through semantic similarity matching, LLM-based reranking, and bootstrapping expansion. Finally, it generates Clinical Summaries and Recommendations by reasoning over the aligned patient-guideline paths, using either fine-tuned specialist models or prompted generalist LLMs.

## Key Results
- CliCARE achieves significantly higher scores than strong baselines on both Chinese CancerEHR and English MIMIC-Cancer datasets
- Expert-Validated LLM-as-a-Judge shows high correlation (ρ≈0.7) with oncologist assessments across multiple evaluation dimensions
- Ablation studies reveal critical tradeoffs in TKG compression and alignment expansion, with performance gains from both components
- Framework demonstrates robust performance across different model sizes and context lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring unstructured longitudinal EHRs into Temporal Knowledge Graphs (TKGs) mitigates long-context processing failures.
- Mechanism: The framework uses a Longformer-based pipeline to compress historical records and BERT-based extraction to identify key clinical facts (diagnoses, treatments). By mapping these to a structured graph with hierarchical timestamps, the system makes temporal dependencies explicit, reducing the "lost-in-the-middle" phenomenon where models overlook distant context.
- Core assumption: Key clinical information can be effectively captured by extractive summarization without losing the nuanced narrative required for decision-making.
- Evidence anchors: [abstract] Mentions transforming "unstructured records into Temporal Knowledge Graphs" to capture "long-range dependencies." [section 3.1] Describes the $f_{pipeline}$ using Longformer to process $D_{hist}$ and creating a structured event sequence $E_p$. [corpus] Related work "TIMER" highlights the difficulty of temporal reasoning in EHRs, supporting the need for explicit temporal structuring.
- Break condition: If a patient's history is sparse or highly fragmented (e.g., missing visits), the TKG may fail to capture the necessary causal links between past treatments and current status.

### Mechanism 2
- Claim: Aligning patient trajectories with normative guideline graphs reduces clinical hallucination by constraining the reasoning path.
- Mechanism: Instead of retrieving isolated text chunks (standard RAG), CliCARE matches the patient's TKG trajectory to the most similar "path" in a guideline Knowledge Graph (KG). It uses BERT for semantic similarity, an LLM for zero-shot reranking, and bootstrapping to expand the alignment. This forces the model to base recommendations on established clinical workflows rather than generic statistical correlations.
- Core assumption: The clinical guidelines represented in the KG are comprehensive enough to cover the specific variances found in real-world patient trajectories.
- Evidence anchors: [abstract] Claims the method "mitigates hallucinations by grounding... through a deep alignment of patient trajectories with clinical guidelines." [section 3.2] Details the Trajectory-Guideline Alignment, specifically the "Similarity Matching" (Eq. 3) and "LLM-based Reranking." [corpus] "CPGPrompt" and "CancerGUIDE" in the corpus suggest a broader trend of translating static guidelines into dynamic LLM inputs.
- Break condition: If the patient presents with a rare comorbidity or a contraindication not explicitly modeled in the guideline graph, the alignment may default to a generic path, potentially suggesting unsafe treatments.

### Mechanism 3
- Claim: An ensemble "LLM-as-a-Judge" protocol provides a scalable and reliable proxy for human expert evaluation.
- Mechanism: To overcome the limitations of metrics like BLEU/ROUGE (which fail to capture clinical validity), the authors use an ensemble of 3 powerful LLMs (GPT-4.1, Claude 4.0, Gemini 2.5). They mitigate systematic bias (positional, verbosity) by averaging scores and randomizing presentation order. A Spearman correlation of ~0.7 with expert oncologists validates this approach.
- Core assumption: The "judge" LLMs possess sufficient medical knowledge to assess "Clinical Soundness" and "Actionability" accurately, independent of human oversight.
- Evidence anchors: [abstract] States "robust evaluation protocol... shows high correlation with expert oncologist assessments." [section 4.5] Reports "Spearman’s rank correlation $\rho$ between our LLM judge’s scores and the physicians’ mean scores was approximately 0.7." [corpus] No strong corpus evidence specifically validates the *reliability* of LLM-as-a-Judge in high-stakes oncology against human experts; most corpus neighbors focus on prediction rather than evaluation methodology.
- Break condition: If the judge models share a common bias or hallucination regarding a specific medical procedure, they may consistently rate incorrect outputs higher than correct ones.

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs)
  - Why needed here: Standard RAG struggles with time. Understanding that a treatment happened *before* a recurrence is critical in oncology. TKGs allow the model to query "what happened next" or "what was the duration" explicitly.
  - Quick check question: If a patient has two CT scans 6 months apart showing growth, does a standard RAG system see them as two separate facts or a timeline of progression?

- Concept: Graph-Aware Retrieval vs. Standard RAG
  - Why needed here: The paper argues standard RAG retrieves "isolated text snippets." You must understand that Graph-Aware retrieval pulls "paths" or "subgraphs" to maintain context and logical flow.
  - Quick check question: Why might retrieving a single sentence about "surgery" be dangerous without retrieving the preceding "staging" information?

- Concept: Alignment & Bootstrapping
  - Why needed here: The paper bridges the gap between "what happened to the patient" (descriptive) and "what should happen" (prescriptive guidelines). This alignment is the core innovation.
  - Quick check question: How does the system handle a patient event that has no clear match in the clinical guidelines?

## Architecture Onboarding

- Component map: Ingestion (Longformer + BERT) -> TKG transformation -> Knowledge Base (Patient TKG vs Guideline KG) -> Alignment Engine (BERT Similarity -> LLM Reranking -> Bootstrapping Expansion) -> Evaluation (Ensemble LLM Judge)

- Critical path: The **Alignment Engine (Section 3.2)**. If the semantic matching threshold (0.7) or the LLM reranking prompt is flawed, the patient trajectory will map to the wrong clinical path, rendering the clinical recommendation invalid.

- Design tradeoffs: The ablation study (Table 3) reveals a critical tradeoff regarding **TKG-based Compression**. Removing compression *improved* performance for the smaller model (Qwen) on the simpler dataset (MIMIC-Cancer), suggesting aggressive summarization can strip necessary nuance in shorter records.

- Failure signatures:
  - **Hallucination despite KG:** Occurs if the "Alignment Expansion" bootstraps from a false positive seed pair, propagating errors through the path.
  - **Context Loss:** If the Longformer summary $S_{hist}$ fails to extract a subtle but critical symptom mentioned only once in a 20k-token history.

- First 3 experiments:
  1. **Validate the Judge:** Before trusting results, replicate the Spearman correlation experiment on a small held-out set to confirm the "LLM-as-a-Judge" aligns with your specific domain experts.
  2. **Stress Test Alignment:** Run the alignment module on a synthetic patient case with a deliberate contradiction (e.g., Stage I diagnosis but Stage IV treatment) to see if the "Clinical Soundness" metric catches the violation.
  3. **Context Window Ablation:** Test the "Long vs. Short" ablation (Table 4) using the smallest model you intend to deploy to define the maximum record length it can handle before TKG compression becomes counterproductive.

## Open Questions the Paper Calls Out

- **Question:** Can the CliCARE framework generalize effectively to clinical domains outside of oncology that may have less structured longitudinal data or different guideline formats?
- **Basis in paper:** [Explicit] The conclusion states, "Future work should focus on generalizing this framework to a wider range of clinical domains," noting the current validation is restricted to cancer datasets.
- **Why unresolved:** The framework relies heavily on process-oriented Clinical Practice Guidelines (CPGs) typical in oncology; other fields may lack such distinct normative pathways, potentially breaking the Trajectory-Guideline Alignment module.
- **What evidence would resolve it:** Successful application of the TKG and alignment pipeline on non-cancer datasets (e.g., cardiology or primary care) demonstrating statistically significant improvements over baselines similar to those seen in CancerEHR.

- **Question:** How can the EHR-to-TKG transformation be dynamically adapted to prevent information loss for shorter or less complex patient records?
- **Basis in paper:** [Inferred] The ablation study (Table 3) reveals that removing TKG-based compression "paradoxically boosts the scores" for smaller models on the MIMIC-Cancer dataset, suggesting the fixed compression strategy is too aggressive for simpler records.
- **Why unresolved:** The current pipeline applies a uniform structuration method that optimizes for long contexts (up to 20k tokens) but may degrade performance when aggressive compression is unnecessary for shorter histories.
- **What evidence would resolve it:** The development of an adaptive mechanism that modulates compression intensity based on input token length, resulting in consistent performance uplift across both short (MIMIC) and long (CancerEHR) context segments.

- **Question:** To what extent does the "Alignment Expansion" bootstrapping process introduce error propagation when mapping complex patient trajectories to guidelines?
- **Basis in paper:** [Inferred] The method uses high-confidence seeds to "iteratively expand" alignments, but the text acknowledges that purely algorithmic matching may fail to capture "complexities of clinical logic," raising the risk of forcing real-world data into normative paths incorrectly.
- **Why unresolved:** While the paper mitigates this with LLM reranking, the expansion step relies on the assumption that semantic neighborhoods remain consistent, which may not hold for off-label treatments or rare comorbidities not present in the guideline KG.
- **What evidence would resolve it:** An error analysis of the final alignment sets for patients with known non-standard treatments, quantifying the rate of misaligned entities compared to the initial seed set.

## Limitations

- The efficacy of the LLM-as-a-Judge protocol is validated only through correlation (ρ≈0.7) with oncologist assessments, not absolute accuracy; no human ground truth for the generated recommendations themselves is provided.
- The private CancerEHR dataset prevents full methodological replication; MIMIC-Cancer filtering criteria are unspecified, creating potential data leakage or inconsistency.
- The exact prompt templates for LLM reranking and final generation are omitted, making faithful reproduction of the alignment and reasoning pipeline difficult.

## Confidence

- **High Confidence:** The core architecture (TKG creation + guideline alignment) is clearly specified and demonstrably outperforms baseline methods in controlled experiments.
- **Medium Confidence:** The TKG compression step's benefit is conditional on record length; ablation shows it can harm performance for shorter histories, suggesting the design is not universally optimal.
- **Medium Confidence:** The expert-validated LLM-as-a-Judge is a novel and promising evaluation approach, but its reliability depends entirely on the medical knowledge of the judge models, which is not independently verified.

## Next Checks

1. **Validate Judge Reliability:** Replicate the Spearman correlation experiment on a small, held-out set with your domain experts to confirm the LLM-as-a-Judge correlates with human assessments for your specific oncology sub-specialty.

2. **Test Alignment Robustness:** Run the alignment module on synthetic patient cases designed with deliberate contradictions (e.g., mismatched staging and treatment) to verify the "Clinical Soundness" metric catches guideline violations.

3. **Probe Context Truncation:** Systematically vary the maximum context length for the Longformer summarizer and measure the impact on the "lost-in-the-middle" baseline to define the optimal record length for your deployment scenario.