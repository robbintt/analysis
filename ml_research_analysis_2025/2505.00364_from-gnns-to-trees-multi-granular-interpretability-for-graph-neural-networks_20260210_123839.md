---
ver: rpa2
title: 'From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks'
arxiv_id: '2505.00364'
source_url: https://arxiv.org/abs/2505.00364
tags:
- graph
- uni00000003
- uni00000044
- uni00000046
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a tree-like interpretable framework (TIF)
  for graph classification that addresses the limitation of existing interpretable
  GNNs in capturing multi-granular relationships. TIF transforms plain GNNs into hierarchical
  trees through iterative graph coarsening and perturbation modules, creating diverse
  branches that capture structural variations across multiple granularities.
---

# From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks

## Quick Facts
- arXiv ID: 2505.00364
- Source URL: https://arxiv.org/abs/2505.00364
- Reference count: 40
- Key outcome: Tree-like interpretable framework (TIF) transforms GNNs into hierarchical trees through iterative graph coarsening and perturbation modules, achieving competitive prediction while significantly enhancing interpretability across multiple granularities.

## Executive Summary
This paper introduces a tree-like interpretable framework (TIF) for graph classification that addresses the limitation of existing interpretable GNNs in capturing multi-granular relationships. TIF transforms plain GNNs into hierarchical trees through iterative graph coarsening and perturbation modules, creating diverse branches that capture structural variations across multiple granularities. The adaptive routing module then identifies informative root-to-leaf paths for both prediction and interpretability. Experiments on real-world and synthetic datasets demonstrate that TIF achieves competitive prediction performance while significantly enhancing interpretability, outperforming baseline methods in explanation accuracy and consistency metrics.

## Method Summary
TIF converts standard GNNs into interpretable tree structures through three core components: hierarchical graph coarsening via MLP-based cluster assignments, learnable perturbation matrices that create diverse structural branches, and MLP-based adaptive routing that selects informative paths. The framework iteratively compresses graphs using soft clustering while preserving connectivity through edge prediction loss. Learnable perturbations generate multiple semantically distinct branches per node, regularized for similarity and diversity. MLP routers then select optimal paths based on concatenated perturbed embeddings, with entropy regularization encouraging exploration during training.

## Key Results
- TIF achieves competitive classification accuracy on real-world and synthetic datasets while significantly improving interpretability metrics
- Explanation accuracy and consistency outperform baseline interpretable GNNs, particularly in capturing both local substructures and global patterns
- Ablation studies show optimal performance with 4 paths per node and compression ratio ~0.2-0.3
- Path consistency analysis demonstrates that TIF produces stable, repeatable interpretations compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Coarsening Preserves Multi-Scale Structure
- Claim: Iterative graph coarsening creates representations at multiple granularities that each contribute to classification.
- Mechanism: A clustering assignment matrix S^(l) = softmax(MLP(Z^(l))) maps nodes to clusters. Coarsened features and adjacency are computed via X^(l+1) = S^(l)ᵀZ^(l) and A^(l+1) = S^(l)ᵀA^(l)S^(l). This is repeated across L tree levels, with edge prediction loss preserving connectivity.
- Core assumption: Graph structure relevant to classification exists at multiple granularities simultaneously (e.g., functional groups → amino acids → protein domains).

### Mechanism 2: Learnable Perturbations Create Diverse Structural Variants
- Claim: Adding learnable perturbation matrices to clustering assignments creates semantically distinct branches that capture alternative structural decompositions.
- Mechanism: For each parent node, M perturbation matrices P are learned. Perturbed assignments S^(l),k(i) = S^(l),k + P^(l),k(i) generate M child nodes. Similarity regularization (L_similarity) keeps perturbations small; diversity regularization (L_diversity) ensures branches differ.
- Core assumption: Valid structural interpretations of a graph are not unique; multiple perspectives should be explored and the best selected.

### Mechanism 3: Adaptive Routing Selects Informative Paths Via Learned Preference
- Claim: MLP-based routers at each non-leaf node learn to select paths that maximize classification-relevant information.
- Mechanism: Routers concatenate perturbed embeddings, compute routing logits via 2-layer MLP, apply softmax to get path probabilities. The highest-probability path is selected: î_l,k = argmax_i p^(l),k,i. Entropy regularization (L_entropy) encourages exploration during training.
- Core assumption: The path with highest routing probability corresponds to the most informative multi-granular decomposition for the input graph.

## Foundational Learning

- Concept: **Graph Coarsening/Pooling**
  - Why needed here: TIF builds directly on hierarchical pooling methods like DiffPool. You must understand how soft cluster assignments S map nodes to supernodes and how gradients flow through the coarsening operation.
  - Quick check question: Given adjacency A and cluster assignment S, can you derive the coarsened adjacency SᵀAS?

- Concept: **Message Passing in GNNs**
  - Why needed here: The base encoder uses GCN layers (Equation 1). Understanding how Ẑ = D̂^(-1/2) Â D̂^(-1/2) Z W aggregates neighbor information is essential before reasoning about multi-granular extensions.
  - Quick check question: Why does adding self-loops (Â = A + I) matter for node self-representation?

- Concept: **Neural Decision Trees**
  - Why needed here: TIF combines neural networks with tree-structured routing. Prior work like NBDT and DNDF established soft routing with learned probabilities; TIF adapts this to graph-structured inputs.
  - Quick check question: How does soft routing (probabilistic path selection) differ from hard routing (argmax at each node), and why might soft be preferable during training?

## Architecture Onboarding

- Component map: Input graph G=(X,A) → GCN encoder → MLP cluster assignment S → M perturbations → M child nodes → MLP routers → selected path → final embedding → classifier MLP → class probabilities
- Critical path: 1) Coarsening quality (compression ratio q): Figure 6(a) shows accuracy degrades at both extremes. q ~ 0.2-0.3 works best. 2) Perturbation balance (λ/μ): Too similar = no diversity; too different = loses semantic meaning. 3) Router capacity: Linear routers (Bi-Tree) underperform MLP routers (Figure 7).
- Design tradeoffs: Paths per node (M): 4 paths optimal (Figure 6b). Fewer constrains diversity; more introduces noise. Tree depth (L): Deeper = more granularities, but risks over-compression. Perturbation learning: Shared across parents vs. per-node. Per-node (TIF) outperforms shared (SV variant in Figure 16).
- Failure signatures: Single-path dominance: If entropy regularization α₃ too low, model collapses to one path regardless of input. Granularity collapse: If compression ratio q too high, all structural detail lost; if too low, coarsening ineffective. Perturbation collapse: If λ too high, all branches identical; if μ too high, branches diverge semantically.
- First 3 experiments: 1) Ablation on compression ratio: Replicate Figure 6(a) on D&D or PROTEINS. Vary q ∈ {0.1, 0.2, 0.3, 0.5}. Plot classification and explanation accuracy. Expect U-shaped curve with peak at 0.2-0.3. 2) Path consistency test: Run same input 10 times, record selected paths. Compare TIF vs. Bi-Tree. TIF should show >80% consistency; Bi-Tree lower. This validates that routing is stable, not random. 3) Synthetic multi-granular test: Use GraphCycle or MultipleCycle datasets. Visualize selected path's intermediate coarsened graphs. Verify that finer levels capture local structures (cycles) and coarser levels capture global topology (cycle vs. non-cycle arrangement).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the Tree-like Interpretable Framework (TIF) be scaled to large graphs while maintaining moderate computational costs?
- **Basis in paper:** [explicit] The Conclusion explicitly states, "In the future, we will further explore scaling the interpretation framework from medium-scale graphs to large-scale graphs at moderate computational costs."
- **Why unresolved:** The current framework relies on iterative graph coarsening and MLP-based routing applied to the full graph, which may incur prohibitive memory and time overhead for large-scale graphs (e.g., millions of nodes).
- **What evidence would resolve it:** A variant of TIF utilizing neighbor sampling or stochastic coarsening techniques evaluated on large-scale benchmarks (e.g., OGB) with linear complexity analysis.

### Open Question 2
- **Question:** Can the optimal tree depth and branching factor be determined adaptively rather than through manual hyperparameter tuning?
- **Basis in paper:** [inferred] The ablation study (Section 4.4, Figure 6) demonstrates that prediction and explanation accuracy are highly sensitive to the compression ratio and number of paths, implying that fixed settings may not generalize across different graph complexities.
- **Why unresolved:** The framework currently requires manual specification of tree structural parameters (e.g., paths $M=4$), potentially failing to capture the intrinsic granularity of diverse datasets without exhaustive search.
- **What evidence would resolve it:** An adaptive routing mechanism that automatically prunes or expands tree branches based on information gain or entropy thresholds during training.

### Open Question 3
- **Question:** How can the coarsening module be modified to preserve semantic meaning in heterogeneous graphs with distinct node and edge types?
- **Basis in paper:** [inferred] While the introduction cites complex biological and social systems, the methodology employs standard GCN convolutions (Eq. 1) that do not distinguish between edge types during the hierarchical coarsening process.
- **Why unresolved:** Aggregating neighbors without considering edge heterogeneity may result in "mixed" coarsened nodes that conflate different semantic relationships, thereby degrading the quality of the multi-granular explanation.
- **What evidence would resolve it:** Integrating heterogeneous message passing (e.g., RGCN) into the coarsening module and testing on multi-relational datasets to verify that explanations respect edge types.

## Limitations
- Critical hyperparameters including GCN layer depth, hidden dimensions, and MLP architectures for routers are not specified
- Perturbation matrix initialization strategy and whether learned directly or via separate networks remains unclear
- Exact method for computing cluster count K^(l) beyond the optimal compression ratio q=0.2 is not detailed
- No experiments on large-scale graphs to validate scalability claims

## Confidence
- **High Confidence**: The core mechanism of hierarchical coarsening (Equations 1-5) is mathematically well-defined and reproducible. The perturbation regularization framework (L_similarity, L_diversity) is clearly specified.
- **Medium Confidence**: The adaptive routing mechanism (Equations 11-14) is specified, but router MLP architecture details remain unclear. Experimental results show consistent patterns across datasets.
- **Low Confidence**: Exact hyperparameter values, initialization strategies, and architectural choices are missing, preventing precise replication.

## Next Checks
1. **Compression Ratio Sensitivity**: Replicate Figure 6(a) across D&D, PROTEINS, and ENZYMES with q ∈ {0.1, 0.2, 0.3, 0.5}. Verify the U-shaped accuracy curve peaks at q=0.2-0.3.
2. **Path Consistency Verification**: Run 10 inference trials on identical inputs for both TIF and Bi-Tree. Measure path consistency percentages; TIF should exceed 80% while Bi-Tree remains lower.
3. **Multi-Granular Structure Validation**: For GraphCycle/MultipleCycle datasets, visualize intermediate coarsened graphs along selected paths. Confirm finer levels capture local cycle structures while coarser levels represent global topology.