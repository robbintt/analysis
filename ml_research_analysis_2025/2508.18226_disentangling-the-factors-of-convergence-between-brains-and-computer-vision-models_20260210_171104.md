---
ver: rpa2
title: Disentangling the Factors of Convergence between Brains and Computer Vision
  Models
arxiv_id: '2508.18226'
source_url: https://arxiv.org/abs/2508.18226
tags:
- brain
- images
- training
- dinov3
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the factors driving representational similarity\
  \ between deep neural networks and the human brain, specifically in visual processing.\
  \ To disentangle the effects of model architecture, training data type, and training\
  \ duration, the authors systematically trained multiple variants of DINOv3\u2014\
  a state-of-the-art self-supervised vision transformer\u2014while independently varying\
  \ these three factors."
---

# Disentangling the Factors of Convergence between Brains and Computer Vision Models

## Quick Facts
- **arXiv ID:** 2508.18226
- **Source URL:** https://arxiv.org/abs/2508.18226
- **Reference count:** 39
- **Key outcome:** Systematically disentangles architecture, training, and data effects on brain-model representational similarity using DINOv3 variants

## Executive Summary
This study systematically investigates how model architecture, training duration, and data type independently and interactively influence the representational similarity between deep neural networks and the human brain. By training multiple variants of DINOv3—a state-of-the-art self-supervised vision transformer—while varying these three factors, the authors demonstrate that all contribute to brain-model alignment. Critically, they reveal a developmental trajectory where early visual areas align with early model layers first, while higher-level and prefrontal representations require substantially more training, correlating with known cortical maturation properties.

## Method Summary
The authors systematically trained DINOv3 variants with different sizes (21M to 7B parameters), training durations (0 to 1e7 steps), and data types (human-centric, satellite, cellular images). They extracted activations from 22 model layers and compared them to human brain activity recorded via 7T fMRI (Natural Scenes Dataset) and MEG (THINGS-MEG) using ridge regression mapping. Three complementary metrics quantified brain-model similarity: encoding score (overall representational similarity), spatial score (correspondence with cortical hierarchy), and temporal score (correspondence with processing dynamics).

## Key Results
- All three factors—model size, training amount, and image type—independently and interactively influence brain-model similarity
- Larger models, more training data, and human-centric image datasets produced the highest similarity scores
- Brain-like representations emerge in a specific developmental sequence: early visual areas align first, while high-level and prefrontal representations require substantially more training
- This developmental trajectory correlates with known structural and functional properties of the human cortex

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Correspondence Through Layer-wise Processing Stages
- **Claim:** Vision transformer layers map onto both spatial (cortical) and temporal hierarchies in human visual processing when sufficient scale and training are applied.
- **Mechanism:** Deeper transformer layers accumulate increasingly abstract transformations of input features, paralleling the ventral stream's progression from edge detection to semantic object representations.
- **Core assumption:** Linear readability indicates functionally meaningful representational alignment rather than superficial statistical correlation.
- **Evidence anchors:** [abstract] models align with early representations first, then late and prefrontal representations with more training; [section 3.1] first and last layers align with earliest and latest MEG responses (temporal score R=0.96, p<1e-12).
- **Break condition:** If non-hierarchical architectures showed stronger brain alignment than hierarchical transformers, this mechanism would be challenged.

### Mechanism 2: Experience-Dependent Alignment With Ecologically Valid Statistics
- **Claim:** Training data matching human visual experience statistics produces higher brain-model similarity than non-ecological domains.
- **Mechanism:** Human-centric images contain statistical regularities that brains evolved to process, and self-supervised objectives force models to discover efficient codes for these regularities.
- **Core assumption:** Human visual cortex representations are optimized for the statistics of natural visual experience.
- **Evidence anchors:** [abstract] human-centric datasets produced highest similarity scores; [section 3.2] satellite and cellular image models showed lower encoding scores (p<1e-3) compared to human-centric training.
- **Break condition:** If models trained on synthetic images achieved equal or higher brain alignment than natural image training, ecological validity would not be the causal mechanism.

### Mechanism 3: Developmental Trajectory Mirroring Cortical Maturation Constraints
- **Claim:** Training sequence of brain-like representations correlates with structural and functional properties of cortical regions.
- **Mechanism:** Associative cortices with slower intrinsic timescales integrate information over longer windows, requiring more diverse training examples to develop stable representations.
- **Core assumption:** Training trajectory captures something functionally analogous to biological developmental timescales.
- **Evidence anchors:** [abstract] representations acquired last align with cortical areas with largest developmental expansion, thickness, least myelination, and slowest timescales; [section 3.3] half time correlates with cortical expansion R=0.88, myelin concentration R=-0.85, thickness R=0.77, intrinsic timescales R=0.71.
- **Break condition:** If curriculum-ordered training eliminated correlations between cortical properties and alignment timing, this mechanism would require revision.

## Foundational Learning

- **Concept: Encoding Models and Representational Similarity Analysis (RSA)**
  - **Why needed here:** The methodology depends on understanding that "representation" is operationalized as linearly decodable information, and similarity is quantified through cross-validated linear regression.
  - **Quick check question:** Can you explain why a high encoding score (R value) indicates representational similarity but not necessarily identical information processing?

- **Concept: Self-Supervised Learning Objectives (specifically DINO/DINOv3 approach)**
  - **Why needed here:** The paper uses self-supervised vision transformers rather than supervised classification models, which learn from image augmentations without labels.
  - **Quick check question:** How does a self-supervised objective differ from supervised classification, and why might self-supervision better capture generic visual representation learning?

- **Concept: Vision Transformer (ViT) Architecture and Layer Hierarchy**
  - **Why needed here:** Spatial and temporal score analyses depend on interpreting transformer layers as a processing hierarchy where layer depth corresponds to sequential processing stages.
  - **Quick check question:** Why is the layer index treated as a proxy for processing stage, and what assumptions does this make about information flow in transformers?

## Architecture Onboarding

- **Component map:** DINOv3 backbone (21M-7B params, 12-40 layers) -> Self-supervised training head (DINO contrastive/masking) -> Feature extraction (22 layers, normalized 0-1) -> Ridge regression mapping -> Encoding/spatial/temporal score computation -> Brain data interfaces (fMRI 7T, MEG bandpass 0.1-20Hz)
- **Critical path:** 1) Select model variant (size, checkpoint, data type) -> 2) Extract layer activations for image set -> 3) Fit ridge regression (5-fold CV, λ∈[10⁰, 10⁸]) -> 4) Compute encoding score (Pearson r) -> 5) Identify best-predicting layer per ROI for spatial score -> 6) Identify peak-prediction time per layer for temporal score -> 7) Repeat across checkpoints for training dynamics
- **Design tradeoffs:** Model size vs. interpretability (larger models achieve highest brain similarity but require more compute); image domain specialization (human-centric maximizes alignment but reduces domain generality); metric choice (encoding score captures overall alignment but not hierarchical organization)
- **Failure signatures:** Negative spatial/temporal scores at initialization (untrained models show inverted hierarchy); low encoding in prefrontal ROIs (expected early, persistent may indicate insufficient capacity); high noise in MEG temporal scores (check filtering and epoching)
- **First 3 experiments:** 1) Reproduce encoding score baseline with pretrained DINOv3-Large on NSD images, verify V1 (R≈0.28-0.45) and prefrontal alignment; 2) Ablate training data type by training DINO-Large from scratch on 10M human-centric vs. satellite vs. cellular images, expect human-centric to dominate in higher ROIs; 3) Measure training dynamics across DINOv3-7B checkpoints (0-1e7 steps), plot half-time curves, verify correlation with cortical properties (R>0.7 expected)

## Open Questions the Paper Calls Out

- **Open Question 1:** Why do encoding, spatial, and temporal scores emerge in a specific, non-simultaneous sequence during training?
- **Open Question 2:** Why do spatial and temporal scores initially start negative in untrained models, such that deep layers predict fast, low-level brain responses?
- **Open Question 3:** Do these convergence trajectories hold for non-hierarchical architectures or different training objectives?

## Limitations

- **Concurrent publication status:** DINOv3 architecture details may affect exact reproduction
- **Proprietary data constraints:** Human-centric dataset is proprietary and cannot be reconstructed for training
- **Statistical generalizability:** Findings may be specific to human visual statistics and may not generalize to other species or artificial environments

## Confidence

- **High confidence:** Encoding score differences across architectures and training durations
- **Medium confidence:** Spatial hierarchy emergence and temporal dynamics correspondence
- **Medium confidence:** Ecological validity of human-centric training effects
- **Lower confidence:** Developmental trajectory correlation with cortical properties (novel finding requiring replication)

## Next Checks

1. **Cross-species validation:** Test whether models trained on primate-specific visual statistics show higher alignment with macaque V1 responses compared to human-centric models
2. **Architecture generalization:** Apply the same disentanglement framework to non-transformer architectures (e.g., convolutional or recurrent networks) to test whether hierarchical correspondence is architecture-specific
3. **Curriculum training experiment:** Train models on curriculum-ordered data (easy-to-hard visual concepts) to test whether developmental timing is driven by data complexity rather than training duration alone