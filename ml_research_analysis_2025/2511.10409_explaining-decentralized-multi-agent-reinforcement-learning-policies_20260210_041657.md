---
ver: rpa2
title: Explaining Decentralized Multi-Agent Reinforcement Learning Policies
arxiv_id: '2511.10409'
source_url: https://arxiv.org/abs/2511.10409
tags:
- task
- agent
- each
- agents
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces methods to explain decentralized multi-agent
  reinforcement learning (MARL) policies. Decentralized MARL policies are challenging
  to interpret due to their inherent uncertainty, nondeterminism, and limited observability.
---

# Explaining Decentralized Multi-Agent Reinforcement Learning Policies

## Quick Facts
- arXiv ID: 2511.10409
- Source URL: https://arxiv.org/abs/2511.10409
- Authors: Kayla Boggess; Sarit Kraus; Lu Feng
- Reference count: 25
- Key outcome: Introduces methods to explain decentralized MARL policies using Hasse diagrams and query-based explanations

## Executive Summary
This paper addresses the challenge of interpreting decentralized multi-agent reinforcement learning (MARL) policies by introducing novel explanation methods. The authors propose using Hasse diagrams to summarize task orderings and agent cooperation, then develop query-based explanation techniques for three types of user questions: "When?", "Why Not?", and "What?". The approach combines abstract state extraction, partial comparability graphs, and the Quine-McCluskey algorithm to identify distinguishing conditions that explain agent behavior.

## Method Summary
The method employs Hasse diagrams to represent partial orders over task completions in decentralized MARL systems, where nodes represent tasks annotated with the agents that completed them, and edges encode precedence relationships. The explanation framework supports three query types: "When?" explanations identify conditions for task execution, "Why Not?" explanations determine why tasks were not executed, and "What?" explanations describe alternative actions. The algorithm extracts abstract states from the policy, constructs partial comparability graphs, and applies the Quine-McCluskey algorithm to minimize Boolean expressions that capture the distinguishing conditions for each query type.

## Key Results
- Proposed explanation methods significantly improve user question-answering performance across four MARL domains
- User studies show enhanced subjective ratings on understanding and satisfaction metrics
- The approach demonstrates generalizability across two different decentralized MARL algorithms
- Computational efficiency is maintained while providing interpretable explanations

## Why This Works (Mechanism)
The approach works by leveraging the structured nature of task-based MARL domains and applying formal methods from order theory and Boolean minimization. Hasse diagrams provide a natural way to represent the partial order of task completions while capturing agent cooperation patterns. The Quine-McCluskey algorithm efficiently identifies minimal conditions that distinguish between different execution paths, making explanations both precise and interpretable. By combining these elements with abstract state extraction, the method can generate explanations that are both theoretically sound and practically useful for understanding agent behavior.

## Foundational Learning

1. **Hasse Diagrams** - Partial order representation of task dependencies and agent cooperation
   - Why needed: Provides structured visualization of task completion sequences
   - Quick check: Verify diagram correctly captures precedence and concurrency

2. **Quine-McCluskey Algorithm** - Boolean function minimization for identifying distinguishing conditions
   - Why needed: Reduces explanation complexity while preserving logical accuracy
   - Quick check: Confirm minimal expression covers all relevant state transitions

3. **Abstract State Extraction** - Abstraction of high-dimensional state spaces for explanation purposes
   - Why needed: Simplifies complex state representations while maintaining behavioral fidelity
   - Quick check: Validate abstract states preserve critical decision boundaries

4. **Partial Comparability Graphs** - Graph-based representation of state relationships for explanation generation
   - Why needed: Enables systematic comparison of states to identify distinguishing features
   - Quick check: Ensure graph connectivity reflects actual state transitions

## Architecture Onboarding

**Component Map:** Abstract State Extractor -> Hasse Diagram Generator -> Partial Comparability Graph Builder -> Quine-McCluskey Minimizer -> Explanation Formatter

**Critical Path:** State observation → Abstract state extraction → Hasse diagram construction → Graph analysis → Boolean minimization → Explanation generation

**Design Tradeoffs:** The approach prioritizes interpretability over completeness, sacrificing some nuance in agent decision-making for human-understandable explanations. This tradeoff enables practical usability but may miss subtle behavioral patterns.

**Failure Signatures:** Explanation quality degrades when task structures are ambiguous, when state abstractions lose critical decision information, or when Boolean minimization produces overly complex conditions. Performance bottlenecks occur with large state spaces or numerous concurrent tasks.

**Three First Experiments:**
1. Validate Hasse diagram construction on simple sequential task domains
2. Test Quine-McCluskey minimization on synthetic Boolean functions representing policy decisions
3. Evaluate explanation quality on single-agent tasks before scaling to multi-agent scenarios

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies on simulated environments rather than real-world deployments
- Hasse diagram approach assumes well-defined task structures that may not fit real-world scenarios
- Scalability to large-scale MARL systems with many agents and tasks remains unclear

## Confidence
- Method correctness: High
- Theoretical framework: High
- Practical effectiveness: Medium
- Real-world applicability: Medium

## Next Checks
1. Test the method on a real-world MARL application with dynamic task structures to assess scalability and adaptability
2. Conduct a more extensive user study with domain experts to evaluate the practical utility of explanations in decision-making contexts
3. Compare the proposed method against alternative explanation techniques for MARL, such as attention-based or surrogate model approaches, to establish relative effectiveness