---
ver: rpa2
title: Assessing Robustness to Spurious Correlations in Post-Training Language Models
arxiv_id: '2505.05704'
source_url: https://arxiv.org/abs/2505.05704
tags:
- llama-3
- spurious
- docqa
- tasks
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Assessing Robustness to Spurious Correlations in Post-Training Language Models

## Quick Facts
- **arXiv ID:** 2505.05704
- **Source URL:** https://arxiv.org/abs/2505.05704
- **Reference count:** 13
- **Primary result:** Preference-based methods (DPO/KTO) demonstrate relative robustness to spurious correlations in mathematical reasoning, while SFT performs better in context-intensive tasks like document-grounded QA.

## Executive Summary
This paper systematically evaluates how different post-training methods (SFT, DPO, KTO) perform when fine-tuning LLMs on data with injected spurious correlations. The authors create synthetic datasets with two types of spurious artifacts—Feature Ambiguity and Distributional Narrowness—and test robustness by evaluating on clean, out-of-distribution data. They find that preference-based methods maintain better reasoning robustness in math tasks, while SFT preserves factual correctness better in context-heavy tasks. Interestingly, higher spuriousness ratios (90%) sometimes improve narrow-task accuracy, suggesting models can "specialize" in restricted domains.

## Method Summary
The authors fine-tune Llama-3.1 (8B, 70B) and Llama-3.2 (3B) models using three post-training methods: SFT (cross-entropy), DPO (Direct Preference Optimization), and KTO (Kahneman-Tversky Optimization). They inject spurious correlations into three task types—mathematical reasoning (GSM8K), document-grounded QA (QuAC), and instruction following—using synthetic datasets with Feature Ambiguity (FA) and Distributional Narrowness (DN) patterns. Training uses specific hyperparameters per method (SFT: LR 2e-5, 3 epochs; DPO: LR 3e-6, 1 epoch; KTO: LR 5e-6, 1 epoch) with global batch size 512. Evaluation uses rule-based matching for math and GPT-4o-mini judgment for QA/instruction tasks on unmodified test sets.

## Key Results
- Preference methods (DPO/KTO) maintain higher accuracy on clean math test sets when trained with high spuriousness (90%) compared to SFT
- SFT outperforms preference methods in document-grounded QA tasks, preserving factual correctness better
- High spuriousness (90%) can sometimes improve narrow-task accuracy through "specialization" effects
- Omission tasks with 90% spuriousness show catastrophic performance drops, demonstrating distributional narrowness failures

## Why This Works (Mechanism)

### Mechanism 1: Preference Contrast Enhances Reasoning Robustness
Preference-based methods optimize a pairwise ranking objective that contrasts "chosen" against "rejected" responses. This contrastive pressure forces models to differentiate structural correctness rather than relying on surface-level token matching, making them more robust to spurious cues in mathematical reasoning where correctness is often discrete.

### Mechanism 2: Full-Sequence Likelihood Preserves Context Attention
SFT minimizes cross-entropy loss over entire gold response sequences, requiring broader context attention. Preference methods might over-optimize for discriminative surface cues without full context comprehension, making SFT more effective for complex, context-intensive tasks.

### Mechanism 3: Distributional Narrowness Simplifies Optimization
High spuriousness artificially restricts valid output space, reducing optimization complexity. Models "specialize" in narrow ranges, increasing accuracy on specific narrow evaluation slices even while failing on full distributions.

## Foundational Learning

- **Concept: Spurious Correlations (Shortcuts)**
  - **Why needed here:** The paper injects shortcut features that correlate with correctness but are causally irrelevant, making the distinction between correlation and causation essential for interpreting degradation results.
  - **Quick check question:** If a model achieves 99% accuracy by detecting that all correct answers contain "the," has it learned the task? (Answer: No, it has learned a spurious correlation).

- **Concept: Preference Optimization (DPO/KTO) vs. Likelihood Maximization (SFT)**
  - **Why needed here:** The paper compares methods that maximize reference probability (SFT) against methods that maximize margins between preferred/dispreferred responses (DPO/KTO), explaining their different reactions to noise.
  - **Quick check question:** Does SFT require a "rejected" response to learn? (Answer: No, it only requires the positive "chosen" response).

- **Concept: Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** Robustness is defined by performance on data lacking training artifacts, testing whether models rely on core mechanisms rather than learned shortcuts.
  - **Quick check question:** Why test on data that lacks the spurious feature found in training? (Answer: To ensure the model relies on the core task mechanism rather than the artifact).

## Architecture Onboarding

- **Component map:** Base Models (Llama-3.1/3.2 Instruct variants) -> Post-Training Layer (SFT/DPO/KTO optimizers) -> Data Layer (Synthetic datasets with FA/DN spurious features)

- **Critical path:** Select model/task -> Inject spurious features (10% vs 90%) -> Apply post-training method -> Evaluate on clean/OOD test set using rule-based or LLM-based verification

- **Design tradeoffs:**
  - Math tasks: Preference methods preferred for reasoning robustness
  - Context tasks: SFT preferred for better context utilization
  - Spurious ratio: High spuriousness can induce specialization but harms generalization

- **Failure signatures:**
  - Gratuitous insertion of spurious tokens (dates, "the", "number") regardless of correctness
  - Severe accuracy drops in OOD settings (e.g., DocQA Omission task)
  - Model defaults to "no answer" when training distribution is artificially narrowed

- **First 3 experiments:**
  1. Reproduce Math Robustness: Train Llama-3.1-8B on Math Word Inclusion (90% spurious) using SFT and DPO, verify DPO maintains higher clean test accuracy
  2. Reproduce Context Fragility: Train on DocQA Date Inclusion (90% spurious) using SFT and DPO, verify SFT outperforms DPO in factual correctness
  3. Stress Test Omission: Run DocQA Omission experiment (90%) to confirm catastrophic performance drop

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data approach may not capture complex spurious patterns in naturally occurring datasets
- Reliance on LLM-based evaluation introduces potential circularity and subjectivity
- Negative sampling strategy for DPO/KTO is not explicitly detailed
- Does not address interactions between spurious correlations and other training factors

## Confidence

**High Confidence (80-100%)**
- Differential performance between SFT and preference methods on math reasoning tasks
- Observation that high spuriousness can improve narrow-task accuracy

**Medium Confidence (50-80%)**
- Mechanism explanations for preference methods' math robustness and SFT's context preservation
- Claims about generalization to naturally occurring spurious correlations

**Low Confidence (0-50%)**
- Broader implications for post-training methodology selection in production systems

## Next Checks
1. Apply SFT vs DPO/KTO comparison to naturally occurring dataset with known spurious correlations (e.g., NLI datasets) to validate synthetic findings transfer
2. Systematically vary negative sampling strategy for DPO/KTO to isolate impact of negative example quality on robustness
3. Evaluate fine-tuned models on entirely different benchmark datasets (MATH for math models, DROP for QA models) to assess broader capability transfer