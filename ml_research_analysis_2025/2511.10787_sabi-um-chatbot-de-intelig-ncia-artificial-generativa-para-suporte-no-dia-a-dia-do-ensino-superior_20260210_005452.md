---
ver: rpa2
title: "Sabi\xE1: Um Chatbot de Intelig\xEAncia Artificial Generativa para Suporte\
  \ no Dia a Dia do Ensino Superior"
arxiv_id: '2511.10787'
source_url: https://arxiv.org/abs/2511.10787
tags:
- para
- como
- informac
- respostas
- encia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of fragmented academic information\
  \ access in higher education. The proposed solution, Sabi\xE1, is a chatbot that\
  \ uses Generative AI and Retrieval-Augmented Generation (RAG) to provide quick,\
  \ accurate answers based on institutional documents."
---

# Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior

## Quick Facts
- arXiv ID: 2511.10787
- Source URL: https://arxiv.org/abs/2511.10787
- Reference count: 0
- Gemini 2.0 Flash excels in both quality and speed for RAG-based academic chatbot

## Executive Summary
This study addresses the challenge of fragmented academic information access in higher education. The proposed solution, Sabiá, is a chatbot that uses Generative AI and Retrieval-Augmented Generation (RAG) to provide quick, accurate answers based on institutional documents. Multiple AI models were evaluated using quality metrics and LLM-as-a-Judge, with Gemini 2.0 Flash excelling in both quality and speed, and Gemma 3n showing strong performance as an open-source option. The best models achieved LLM-as-a-Judge scores above 0.75 and processing times under 3 seconds. The results demonstrate that GenAI combined with RAG is a viable solution for improving access to institutional academic information.

## Method Summary
The Sabiá chatbot uses a RAG architecture with LangChain orchestration, ChromaDB vector store, and multiple LLMs accessed via OpenRouter API. Institutional PDFs are chunked and embedded into the vector database. When users ask questions, relevant documents are retrieved based on semantic similarity, combined with the query and prompt template, then passed to the selected LLM for generation. Evaluation used both traditional metrics (ROUGE, BLEU, METEOR, SBERT) and LLM-as-a-Judge with GPT-4.1-mini as the evaluator. The system was tested with a set of FAQ questions and reference answers from course coordination.

## Key Results
- Gemini 2.0 Flash achieved the best balance of quality (LLM-as-a-Judge > 0.75) and speed (< 3 seconds)
- Gemma 3n performed strongly as an open-source alternative with competitive quality scores
- The RAG architecture successfully reduced hallucination by grounding responses in institutional documents

## Why This Works (Mechanism)
The system works by combining semantic search with contextual generation. When a user asks a question, the RAG pipeline retrieves relevant document chunks from the vector database based on semantic similarity, then passes both the retrieved context and the original query to the LLM with a carefully crafted prompt. This approach grounds the LLM's response in actual institutional documentation rather than relying on its pre-trained knowledge, significantly reducing hallucinations and ensuring answers are based on official policies and procedures.

## Foundational Learning
- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: This is the core architecture of the Sabiá chatbot. RAG is not a model itself, but a framework that augments an LLM's response generation with an external knowledge retrieval step.
  - Quick check question: Can you explain how RAG differs from providing an LLM with documents directly in its context window versus fine-tuning the model on the documents?

- **Concept: Vector Embeddings and Semantic Search**
  - Why needed here: The system's ability to find relevant information relies on converting text (queries and documents) into numerical vectors. Understanding this is key to diagnosing retrieval failures.
  - Quick check question: If a user asks about "failing a class" but the official document uses the term "course reproval," how would a semantic search system handle this compared to a keyword search?

- **Concept: LLM-as-a-Judge Evaluation**
  - Why needed here: This is the primary method used to compare and select models in the paper. It represents a scalable alternative to human evaluation for open-ended text generation tasks.
  - Quick check question: What are the potential pitfalls of using one LLM to grade another's work? How might you mitigate these risks?

## Architecture Onboarding
- **Component map:** User Query -> LangChain Orchestrator -> Vector DB (Retrieval) -> Retrieved Context + Query + Prompt Template -> Selected LLM (Generation) -> Frontend (Response)
- **Critical path:** `User Query -> LangChain Orchestrator -> Vector DB (Retrieval) -> Retrieved Context + Query + Prompt Template -> Selected LLM (Generation) -> Frontend (Response)`. The evaluator is an offline component for development and model selection.
- **Design tradeoffs:**
  - **Latency vs. Quality:** More powerful or reasoning-focused models (like the paper notes for DeepSeek R1) may be slower. Gemini 2.0 Flash is highlighted for its balance of speed and quality.
  - **Cost vs. Control:** Proprietary models (GPT, Gemini) offer high performance but incur API costs and raise data privacy considerations. Open-source models (Gemma, Phi) can be self-hosted, offering greater control but requiring infrastructure management.
  - **Retrieval Granularity:** The size of document chunks affects retrieval. Smaller chunks improve precision but may lose context; larger chunks provide context but may introduce noise.
- **Failure signatures:**
  - **"Hallucinated" Policy:** The model invents rules or procedures not found in the source documents. This suggests a retrieval failure or a model not adhering to the RAG context.
  - **Vague/Generic Response:** The model fails to use the specific institutional terminology from the retrieved documents. This could indicate poor context injection or a template that is too restrictive.
  - **Slow Response Times:** End-to-end latency exceeds user expectations. This could be due to slow model inference, inefficient retrieval, or network latency from external APIs.
- **First 3 experiments:**
  1. **Chunk Size Tuning:** Systematically test different document chunk sizes (e.g., 256, 512, 1024 tokens) and overlap values. Evaluate the impact on retrieval relevance and the quality of the final generated answer using the LLM-as-a-Judge.
  2. **Prompt Template Ablation:** Create multiple prompt templates with varying levels of instruction (e.g., "Answer briefly," "Cite the source document," "Explain as if to a first-year student"). Run an A/B test using the same model and retrieval setup to measure the effect on the "Clarity" and "Conciseness" metrics defined in the paper.
  3. **Retrieval Failure Analysis:** Manually curate a set of user queries where the system previously provided a poor answer. For each, inspect the top-k retrieved document chunks. Determine if the failure was due to a missing document, poor chunking, or an embedding that failed to capture the query's semantic intent.

## Open Questions the Paper Calls Out
- **Open Question 1**
  - Question: Does utilizing multiple LLM judges improve the reliability of automated evaluation compared to a single judge?
  - Basis in paper: [explicit] The authors plan "a expansão da aplicação da técnica LLM-as-a-Judge, utilizando múltiplos juízes e mais execuções para aumentar a confiabilidade."
  - Why unresolved: The current study relied on a single model (GPT-4.1-mini) for judgment, which introduces potential bias or single-point-of-failure errors in scoring.
  - What evidence would resolve it: Comparative analysis of inter-rater reliability and variance reduction using a multi-judge ensemble on the same dataset.

- **Open Question 2**
  - Question: How do students and faculty evaluate the usability and accessibility of the chatbot in real-world interactions?
  - Basis in paper: [explicit] The text states: "está prevista a realização de testes empíricos com estudantes e docentes... para avaliar a usabilidade, a acessibilidade e a experiência de interação."
  - Why unresolved: The current evaluation relies solely on automated metrics (ROUGE, BLEU) and lacks human-centric validation of the user experience.
  - What evidence would resolve it: Results from empirical user studies, such as System Usability Scale (SUS) scores or qualitative feedback from the target demographic.

- **Open Question 3**
  - Question: How does the system's accuracy and latency perform when scaling the FAQ set and document corpus?
  - Basis in paper: [explicit] Future work includes "aumentar o escopo de testes, incluindo uma maior gama de perguntas ao FAQ."
  - Why unresolved: It is unclear if the current high scores (e.g., >0.75) and low latency (<3s) hold when the retrieval space is significantly larger and more complex.
  - What evidence would resolve it: Benchmarking quality metrics and response times against a substantially larger knowledge base.

## Limitations
- Key implementation details like embedding model, chunk size, and prompt templates are not specified, making exact reproduction difficult
- The study lacks empirical user testing with students and faculty to validate real-world usability and impact
- Performance on complex, multi-hop queries or cross-document reasoning scenarios was not evaluated

## Confidence
**High Confidence**: The core finding that Gemini 2.0 Flash achieves the best balance of quality (LLM-as-a-Judge > 0.75) and speed (< 3 seconds) is well-supported by the presented metrics.

**Medium Confidence**: The relative performance rankings of other models depend on the specific evaluation rubric and test set, which are not fully specified in the paper.

**Low Confidence**: The paper's discussion on practical impact lacks empirical data on user satisfaction, adoption rates, or actual reduction in information access time in real-world settings.

## Next Checks
1. **Replication of Core Results**: Reproduce the evaluation using the same LLM-as-a-Judge rubric (relevance, accuracy, completeness, clarity, conciseness) and a comparable set of FAQ-style questions. Confirm that Gemini 2.0 Flash achieves an average score above 0.75 and a processing time under 3 seconds, while tracking the specific metrics for all tested models.

2. **Sensitivity Analysis of RAG Components**: Systematically vary the document chunk size and the number of top-k retrieved chunks. Measure the impact on both the retrieval recall (using a manually curated set of relevant documents) and the final answer quality as judged by the LLM-as-a-Judge. This will help identify the optimal configuration for this specific document domain.

3. **Error Case Investigation**: Analyze a sample of questions where the system provided a low-quality answer. For each failure, inspect the retrieved context to determine if the error was due to a retrieval failure (wrong or missing documents), a generation failure (model ignoring context), or a prompt engineering issue. This will help identify the most critical failure modes for targeted improvement.