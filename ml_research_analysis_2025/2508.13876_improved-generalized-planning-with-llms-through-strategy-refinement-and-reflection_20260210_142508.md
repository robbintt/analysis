---
ver: rpa2
title: Improved Generalized Planning with LLMs through Strategy Refinement and Reflection
arxiv_id: '2508.13876'
source_url: https://arxiv.org/abs/2508.13876
tags:
- tasks
- program
- goal
- generated
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an improved LLM-based framework for generating
  generalized plans in PDDL domains by treating strategy generation as a distinct,
  debuggable sub-task. The approach generates strategies in pseudocode form, validates
  them using an LLM and symbolic plan validator, and incorporates reflection steps
  to identify and fix errors before code generation.
---

# Improved Generalized Planning with LLMs through Strategy Refinement and Reflection

## Quick Facts
- arXiv ID: 2508.13876
- Source URL: https://arxiv.org/abs/2508.13876
- Reference count: 19
- Primary result: LLMs can generate generalized plans for STRIPS domains by first producing pseudocode strategies, validating them with a symbolic validator, and refining through reflection steps

## Executive Summary
This paper introduces a framework for generating generalized plans in PDDL domains using LLMs by treating strategy generation as a distinct, debuggable sub-task. The approach generates strategies in pseudocode form, validates them using an LLM and symbolic plan validator, and incorporates reflection steps to identify and fix errors before code generation. It also produces multiple program variants and selects the best one based on debugging performance. Evaluated on 17 domains, the method substantially outperforms prior work, achieving perfect coverage on 12 domains where the generated programs solve all tasks generatable by the instance generators.

## Method Summary
The framework implements a three-stage pipeline: (1) NL generation from PDDL, (2) pseudocode strategy generation with symbolic validation-reflection loops, and (3) Python code generation with reflection-based debugging. The key innovation is introducing an intermediate pseudocode representation that allows for strategy validation before committing to code generation. The system uses GPT-4o with specific prompting strategies, generating multiple program variants by varying input examples and selecting the best based on debugging performance. The approach achieves significant improvements over direct code generation methods.

## Key Results
- Achieved perfect coverage on 12 out of 17 evaluated domains
- Outperformed prior work substantially on generalized planning benchmarks
- Demonstrated that pseudocode strategies with reflection steps improve LLM performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring strategies as pseudocode improves the generation of correct generalized plans.
- Mechanism: Pseudocode enforces a more structured and program-adjacent representation of the planning strategy compared to natural language. This intermediate representation reduces the cognitive load on the LLM during the subsequent code generation step by making the mapping from strategy to executable code more direct.
- Core assumption: The LLM is more proficient at translating structured pseudocode into Python than translating abstract natural language descriptions into Python.
- Evidence anchors: The corpus signals (e.g., 'ProgCo', 'iCLP') support the general idea of using intermediate structures or programs to improve LLM reasoning and planning.

### Mechanism 2
- Claim: A reflection step, before revision, improves the LLM's ability to debug both pseudocode strategies and generated Python code.
- Mechanism: Instead of asking the LLM to immediately correct an error based on feedback, a separate step prompts the LLM to first analyze the feedback, identify the faulty component (in pseudocode or Python), and explain why it failed. This separation of diagnosis and repair is posited to lead to more accurate fixes.
- Core assumption: LLMs are better at generating corrections when explicitly guided through a causal analysis of the failure first.
- Evidence anchors: The corpus indicates related work on self-correction (e.g., 'ProgCo') which this paper builds upon by separating the reflection phase.

### Mechanism 3
- Claim: Generating multiple program variants and selecting the best one increases the likelihood of finding a correct generalized plan.
- Mechanism: This approach introduces controlled stochasticity by varying the input prompt's example task (reordering objects/goals) to generate different initial programs. By running the full debugging pipeline on each variant and selecting the one with the best performance on debugging tasks, the system effectively performs a search over the space of possible programs, mitigating the risk of a single bad initialization.
- Core assumption: The LLM's code generation is sensitive to the format and ordering of examples in the prompt, and a better initial program exists within the space of generated variants.
- Evidence anchors: The corpus signals indicate this is a common technique in LLM code generation (e.g., 'Classical Planning with LLM-Generated Heuristics').

## Foundational Learning

- Concept: **PDDL (Planning Domain Definition Language)**
  - Why needed here: The paper's entire framework operates on PDDL domains and problems. Understanding the structure of PDDL—predicates, actions, parameters, preconditions, and effects—is essential to comprehend the inputs to the LLM and the nature of the "generalized plan" it must produce.
  - Quick check question: Given a PDDL action `(load-truck ?obj ?truck ?loc)` with a precondition `(at ?obj ?loc)`, what state must be true for the action to be applicable?

- Concept: **Symbolic Plan Validation (VAL)**
  - Why needed here: The core of the feedback loop is automated validation. The paper relies on a symbolic tool (VAL) to check if a plan (a sequence of PDDL actions) is a correct solution for a given problem. This external, deterministic check provides the ground truth signal that the LLM uses for reflection and correction.
  - Quick check question: If VAL reports "precondition not satisfied," what does this indicate about the plan sequence provided to it?

- Concept: **Generalized Planning**
  - Why needed here: This work is not about solving a single task but about creating a program that can solve any task from a given domain (e.g., Logistics with any number of packages and cities). Understanding this goal is key to appreciating why the final output is a Python program, not just a plan.
  - Quick check question: How does a generalized plan differ from a classical plan that solves a single, fixed PDDL problem instance?

## Architecture Onboarding

- Component map: NL Generation -> Strategy Generation & Debugging -> Code Generation & Debugging -> Selection
- Critical path: The path from PDDL to the final Python program is sequential. The correctness of the final program is fundamentally constrained by the quality of the pseudocode strategy produced in the earlier stage.
- Design tradeoffs:
  - **Pseudocode vs. Natural Language for Strategy**: Pseudocode is more precise but may require more tokens and could be harder for the LLM to generate if the domain logic is very complex.
  - **Number of Program Variants (N) vs. Debugging Steps (KC)**: More variants increase the chance of a good initial program but cost more. More debugging steps give a single program more chances to be fixed but may get stuck in a local minimum.
  - **Cost**: The system requires multiple LLM calls per domain (for strategy, plans, reflection, code). While cheaper than per-task planning, it's more expensive than a single-pass approach.
- Failure signatures:
  - **Timeouts**: The generalized plan (Python program) may enter an infinite loop on some tasks.
  - **Python Exceptions**: The generated code is syntactically invalid or has runtime errors (e.g., `KeyError`).
  - **Invalid Plan Output**: The code runs but produces a sequence of actions that VAL rejects (e.g., wrong action name, wrong parameters, unsatisfied preconditions).
  - **Goal Not Reached**: The plan executes all actions but the final state does not satisfy all goal conditions.
- First 3 experiments:
  1. End-to-End Validation: Implement the pipeline as described in the paper, starting with a simple PDDL domain (e.g., Logistics or Gripper). Use the same prompts and debugging setup. Measure the final coverage on a held-out test set.
  2. Ablation Study - Strategy Format: Run the pipeline with the strategy generation step producing natural language instead of pseudocode. Compare the final program success rate to the pseudocode version to quantify the specific benefit of the pseudocode representation.
  3. Ablation Study - Reflection: Run the pipeline but remove the reflection step, having the LLM attempt to correct pseudocode or code directly from the VAL error message. This isolates the contribution of the explicit reflection mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can providing boilerplate code derived from PDDL action definitions reduce errors in the implementation of action semantics within the generated Python programs?
- Basis in paper: [explicit] The Conclusion states that "One common source of mistakes... is an erroneous implementation of action semantics" and suggests this "may be fixable by providing the LLM with boilerplate code trivially derivable from the PDDL input."
- Why unresolved: The current framework requires the LLM to infer and implement action semantics (e.g., how a truck moves) from scratch, which is identified as a failure point.
- What evidence would resolve it: An experiment comparing the current error rates against a modified pipeline where the Python generation prompt includes helper functions for actions extracted directly from the PDDL domain file.

### Open Question 2
- Question: How can this framework be combined with symbolic search to overcome the limitation to polynomial-time programs and provide soundness guarantees?
- Basis in paper: [explicit] The Conclusion identifies "the combination with symbolic search methods" as an "important direction" to address the "lack of intrinsic guarantees and the fundamental limitation to polynomial-time programs."
- Why unresolved: The current method generates heuristic generalized plans without formal verification, and is restricted to strategies that can be encoded in polynomial-time code, potentially missing solutions for harder problem classes.
- What evidence would resolve it: A hybrid architecture where the LLM-generated program serves as a heuristic for a symbolic planner, or a verification step that proves the generalized plan's correctness across the domain.

## Limitations

- The approach is restricted to polynomial-time programs and lacks formal soundness guarantees
- Performance depends heavily on the quality and representativeness of debugging tasks
- The framework requires multiple LLM calls per domain, increasing computational cost compared to single-pass approaches
- Generated programs may overfit to the specific debugging task distribution

## Confidence

- **High Confidence**: The core contribution of introducing pseudocode strategies and reflection steps is well-supported by the experimental results showing substantial improvements over baseline methods.
- **Medium Confidence**: The claim that this approach achieves "perfect coverage" on 12 domains is accurate but should be interpreted cautiously given the limited scope of debugging tasks and potential overfitting.
- **Medium Confidence**: The effectiveness of multiple program variants depends on the assumption that LLM code generation is sensitive to prompt formatting, which is reasonable but not extensively validated.

## Next Checks

1. Conduct human evaluation of generated Python programs to assess their correctness, readability, and efficiency beyond what VAL can detect.
2. Perform cross-validation by using different sets of debugging tasks to ensure the selection mechanism doesn't overfit to specific instances.
3. Test the framework on domains with more complex action structures or conditional effects to evaluate its generalizability beyond STRIPS domains.