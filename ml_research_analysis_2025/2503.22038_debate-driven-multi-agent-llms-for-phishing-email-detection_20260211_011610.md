---
ver: rpa2
title: Debate-Driven Multi-Agent LLMs for Phishing Email Detection
arxiv_id: '2503.22038'
source_url: https://arxiv.org/abs/2503.22038
tags:
- phishing
- email
- detection
- debate
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a multi-agent LLM debate framework for phishing\
  \ email detection, where two LLM agents argue opposing positions on whether an email\
  \ is phishing, and a judge agent evaluates the arguments to make the final classification.\
  \ The approach is tested across five datasets using combinations of GPT-4 and LLaMA-2\
  \ models, with the GPT-4\u2013LLaMA-2\u2013GPT-4 configuration achieving the highest\
  \ accuracy, particularly 99.43% F1 on the Ling dataset."
---

# Debate-Driven Multi-Agent LLMs for Phishing Email Detection

## Quick Facts
- arXiv ID: 2503.22038
- Source URL: https://arxiv.org/abs/2503.22038
- Reference count: 24
- Best F1 score achieved: 99.43% on Ling dataset

## Executive Summary
This paper introduces a multi-agent LLM debate framework for phishing email detection, where two LLM agents argue opposing positions (phishing vs. legitimate) across two rounds, and a judge agent evaluates the arguments to make the final classification. The approach is tested across five datasets using combinations of GPT-4 and LLaMA-2 models, with the GPT-4–LLaMA-2–GPT-4 configuration achieving the highest accuracy. Results demonstrate that heterogeneous agent pairings consistently outperform homogeneous ones, and that the debate structure alone is sufficient for strong performance without additional prompting strategies.

## Method Summary
The method employs three LLM agents in a structured debate format: two debater agents (GPT-4 or LLaMA-2) argue opposing positions across two rounds, and a judge agent (GPT-4) evaluates the arguments to produce a final classification. The framework uses explicit prompt templates for each round, with emails filtered to the 75th percentile token length across five datasets totaling 12,798 emails. The debate structure includes predefined opposing stances and rebuttal rounds, which the authors found sufficient for eliciting strong reasoning without requiring additional prompting strategies like chain-of-thought or role prompting.

## Key Results
- GPT-4–LLaMA-2–GPT-4 configuration achieved highest accuracy on 4/5 datasets
- Heterogeneous agent pairings consistently outperformed homogeneous configurations
- Debate structure alone provided sufficient reasoning without extra prompting strategies
- Best performance: 99.43% F1 score on Ling dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured adversarial debate reduces confirmation bias in phishing classification.
- Mechanism: Two agents argue opposing positions across two rounds, forcing each to confront counterarguments rather than overfitting to initial reasoning paths. A judge evaluates argument quality rather than raw model confidence.
- Core assumption: Adversarial pressure exposes weak reasoning that single-agent analysis would miss.
- Evidence anchors:
  - [abstract] "two LLM agents to present arguments for or against the classification task, with a judge agent adjudicating the final verdict based on the quality of reasoning provided"
  - [section] Page 2: "a major limitation of single-agent LLM approaches is confirmation bias, where the model tends to overfit its initial reasoning path and fails to explore alternative interpretations"
  - [corpus] Du et al. [14] show multi-agent debate reduces hallucinations and improves reasoning, supporting—but not proving—this mechanism in phishing contexts.
- Break condition: If agents generate superficial arguments that don't meaningfully engage with opponent claims, debate provides no marginal benefit over single-agent classification.

### Mechanism 2
- Claim: Heterogeneous model pairings yield complementary reasoning patterns that improve classification accuracy.
- Mechanism: Different LLMs (e.g., GPT-4 and LLaMA-2) may attend to different linguistic cues or deceptive patterns. When forced to debate, their distinct reasoning styles expose blind spots the other misses.
- Core assumption: Models trained differently have non-overlapping failure modes that cancel out during debate.
- Evidence anchors:
  - [abstract] "mixed-agent configurations consistently outperform homogeneous configurations"
  - [section] Table II shows GPT-4–LLaMA-2–GPT-4 achieves highest accuracy on 4/5 datasets (up to 99.43% F1 on Ling), while homogeneous GPT-4 or LLaMA-2 alone underperform.
  - [corpus] Limited direct evidence; corpus shows related work on LLM phishing detection but minimal replication of heterogeneous agent effects.
- Break condition: If both models share similar reasoning biases despite different architectures, heterogeneity provides no advantage.

### Mechanism 3
- Claim: Debate structure alone elicits sufficient reasoning; additional prompting strategies provide no measurable gain.
- Mechanism: The two-round argument exchange naturally forces agents to justify positions with evidence. Predefined opposing stances and rebuttal rounds create built-in structure that chain-of-thought and role prompting attempt to add externally.
- Core assumption: The debate format is the primary driver of reasoning quality, not prompt engineering.
- Evidence anchors:
  - [abstract] "the debate structure itself is sufficient to yield accurate decisions without extra prompting strategies"
  - [section] Page 4: "neither CoT prompting, role prompting, nor their combination outperformed the baseline configuration without these enhancements"
  - [corpus] No corpus papers specifically test debate vs. prompt engineering tradeoffs.
- Break condition: If tasks require domain expertise not captured by general debate (e.g., technical header analysis), additional prompting may become necessary.

## Foundational Learning

- Concept: Confirmation bias in single-agent LLM reasoning
  - Why needed here: Understanding why debate helps requires recognizing that LLMs tend to rationalize initial conclusions rather than explore alternatives.
  - Quick check question: Can you explain why asking a single LLM to "think carefully" doesn't reliably overcome its tendency to defend its first answer?

- Concept: Multi-agent debate frameworks
  - Why needed here: The core architecture depends on adversarial interaction patterns; understanding general debate mechanisms transfers to implementation.
  - Quick check question: What is the role of the judge agent, and why isn't majority voting sufficient?

- Concept: Phishing linguistic signals (urgency, authority, obfuscation)
  - Why needed here: Interpreting agent arguments requires knowing what cues indicate phishing vs. legitimate communication.
  - Quick check question: Name three linguistic patterns the paper suggests agents analyze (hint: see the CoT guiding questions on page 4).

## Architecture Onboarding

- Component map:
  - Email input → Debater Agent 1 (phishing advocate) → Debater Agent 2 (legitimacy advocate) → Debater Agent 1 round 2 (rebuttal) → Debater Agent 2 round 2 (rebuttal) → Judge Agent → Classification output

- Critical path: Email input → Agent 1 round 1 → Agent 2 round 1 → Agent 1 round 2 (rebuttal) → Agent 2 round 2 (rebuttal) → Judge evaluation → Classification output

- Design tradeoffs:
  - Two rounds chosen for "computational efficiency" vs. deeper analysis; more rounds may improve accuracy but increase latency and cost.
  - Token limits required filtering emails to 75th percentile length; longer emails were excluded (page 3).
  - Heterogeneous pairing improves accuracy but adds deployment complexity (multiple model APIs).

- Failure signatures:
  - Agents generate generic arguments that don't engage with specific email content → judge has weak signal for decision.
  - Both models share similar blind spots → heterogeneity provides no benefit.
  - Judge defaults to one side consistently → indicates prompt bias or argument quality imbalance.

- First 3 experiments:
  1. Replicate homogeneous vs. heterogeneous comparison on one dataset (e.g., Ling) to validate the reported accuracy gap.
  2. Ablate the second debate round to measure marginal contribution of rebuttal vs. single-pass arguments.
  3. Test on edge cases (short emails, borderline phishing) to identify where debate structure fails to improve over baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement observed with heterogeneous agent pairings (GPT-4 and LLaMA-2) generalize to other combinations of open-source and proprietary models?
- Basis in paper: [explicit] The authors conclude that "mixed-model agent configurations consistently outperform homogeneous setups" and suggest this heterogeneity allows agents to "complement each other’s reasoning capabilities," but only test one specific mixed pairing.
- Why unresolved: The study is limited to GPT-4 and LLaMA-2; it is unverified if this synergy is unique to these specific architectures or a general property of diverse LLMs.
- What evidence would resolve it: Experiments repeating the debate framework using alternative mixed pairings (e.g., Claude, Gemini, Mistral) to see if the heterogeneity advantage holds.

### Open Question 2
- Question: Is the framework robust against adversarial, LLM-generated phishing emails designed specifically to bypass semantic detection?
- Basis in paper: [inferred] The introduction highlights that attackers "constantly refine their methods" and use "AI-generated content," yet the evaluation relies on existing datasets (e.g., Nazario, Ling) which may not represent sophisticated, AI-crafted phishing attempts.
- Why unresolved: The datasets used contain standard or older phishing tactics; the framework's ability to detect novel, AI-generated attacks remains untested.
- What evidence would resolve it: Evaluating the framework on a dataset of phishing emails generated by advanced LLMs prompted to evade detection.

### Open Question 3
- Question: What is the computational cost and latency overhead of the multi-round debate framework compared to single-agent or traditional classifiers in real-time filtering scenarios?
- Basis in paper: [inferred] The paper acknowledges limiting debate to two rounds to maintain "computational efficiency" but focuses solely on accuracy/F1 metrics without reporting inference time or API costs.
- Why unresolved: While effective, the method requires generating multiple arguments and a judgment per email, which may be prohibitively slow or expensive for high-volume email streams.
- What evidence would resolve it: A comparative analysis of inference latency and token costs between the debate framework and baseline single-pass detection methods.

## Limitations
- Model version ambiguity creates reproducibility gaps (specific GPT-4 variant and LLaMA-2 model size not specified)
- Debate effectiveness depends on agents generating substantive, opposing arguments
- Computational cost of running three LLM instances per email not addressed

## Confidence
- High confidence in heterogeneous advantage finding (99.43% F1 on Ling dataset)
- Medium confidence in debate reducing confirmation bias mechanism
- Low confidence in generalizability beyond tested datasets

## Next Checks
1. Replicate the heterogeneous vs. homogeneous comparison on the Ling dataset using exact prompt templates and model configurations to verify the reported accuracy gap.
2. Conduct an ablation study removing the second debate round to quantify the marginal contribution of rebuttal exchanges versus single-pass arguments.
3. Test the framework on emails containing technical headers or HTML formatting to assess performance beyond the text-based datasets used in the original study.