---
ver: rpa2
title: Constrained Preferential Bayesian Optimization and Its Application in Banner
  Ad Design
arxiv_id: '2505.10954'
source_url: https://arxiv.org/abs/2505.10954
tags:
- design
- function
- optimization
- banner
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces constrained preferential Bayesian optimization
  (CPBO), a novel method that extends preferential Bayesian optimization (PBO) to
  handle inequality constraints for the first time. CPBO incorporates constraints
  into PBO using a new acquisition function, expected utility of the best option with
  constraints (EUBOC), which extends the EUBO acquisition function for PBO.
---

# Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design

## Quick Facts
- **arXiv ID**: 2505.10954
- **Source URL**: https://arxiv.org/abs/2505.10954
- **Reference count**: 34
- **Key outcome**: CPBO extends preferential Bayesian optimization to handle inequality constraints for the first time, demonstrating success in both synthetic tests and banner ad design with human designers.

## Executive Summary
This paper introduces constrained preferential Bayesian optimization (CPBO), a novel method that extends preferential Bayesian optimization (PBO) to handle inequality constraints. CPBO incorporates constraints into PBO using a new acquisition function, expected utility of the best option with constraints (EUBOC), which extends the EUBO acquisition function for PBO. The authors validate CPBO through technical evaluations demonstrating its ability to identify optimal solutions by focusing on feasible regions while accounting for constraints. As a practical application, they propose a designer-in-the-loop framework for banner ad design using CPBO, where the objective is the designer's subjective preference and the constraint ensures a target predicted click-through rate. A user study with professional ad designers validates the framework's effectiveness in guiding creative design under real-world constraints.

## Method Summary
CPBO combines preferential Bayesian optimization with constraint handling through a dual-surrogate architecture. The method maintains separate Gaussian Process models for preferences (updated via pairwise comparisons using Thurstone-Mosteller likelihood) and constraints (updated from direct evaluations). The key innovation is the EUBOC acquisition function, which multiplies the EUBO utility by the joint probability that both candidates satisfy the constraint. The system optionally uses warm-starting by pre-training the constraint surrogate with random samples before human interaction begins. Optimization uses L-BFGS-B with multiple restarts to maximize EUBOC.

## Key Results
- CPBO successfully identifies optimal solutions by focusing exploration on feasible regions while accounting for constraints
- Technical evaluations show EUBOC achieves ~90-100% feasible sampling while reducing optimality gap faster than baselines
- User study with professional ad designers validates the framework's effectiveness, with designers positively receiving the concept and appreciating its potential to reduce design workload

## Why This Works (Mechanism)

### Mechanism 1: Constraint-Aware Acquisition Function Design
The EUBOC acquisition function multiplies EUBO (expected utility of the best of two candidates) by the joint probability that both candidates satisfy the constraint. This product is maximized only when preference utility is high AND constraint satisfaction is likely. The independence approximation for constraint probabilities (Equation 9) is sufficiently accurate for practical optimization, though it may fail when constraint functions are highly correlated across the search space.

### Mechanism 2: Dual Surrogate Model Architecture
The system builds two distinct Gaussian Process models—one updated from pairwise preference comparisons, another from direct constraint evaluations. This separation allows pre-training the constraint model before human interaction, enabling warm-starting strategies. The independence assumption between preference and constraint modeling enables this clean separation, though it may miss critical interactions when constraint satisfaction depends strongly on preference structure.

### Mechanism 3: Warm-Start Constraint Learning for Efficient Exploration
Pre-training the constraint surrogate with random samples before human interaction builds an initial feasible-region map, allowing EUBOC to focus on feasible candidates from iteration one. This reduces early exploration of infeasible regions and accelerates convergence. Random sampling adequately covers the feasible region in tested scenarios, though disconnected or complex feasible regions may be missed by random sampling.

## Foundational Learning

- **Concept: Bayesian Optimization with Acquisition Functions**
  - Why needed here: CPBO builds on standard BO machinery. Understanding EI (Expected Improvement) and the explore-exploit tradeoff is essential before grasping EUBO.
  - Quick check question: Given a GP surrogate with mean μ(x) and variance σ²(x), how does an acquisition function balance exploitation (high mean) vs. exploration (high variance)?

- **Concept: Preferential Learning and Pairwise Comparisons**
  - Why needed here: Unlike standard BO observing direct function values, PBO infers objectives from relative preferences. The Thurstone-Mosteller model (Equation 2) converts preferences to likelihood updates.
  - Quick check question: Why might pairwise comparisons be more reliable than absolute ratings with human evaluators?

- **Concept: Constraint Handling in Optimization**
  - Why needed here: The EIC approach (multiplying EI by constraint satisfaction probability) underpins EUBOC's constraint handling. Understanding feasible vs. infeasible regions is crucial.
  - Quick check question: What is the fundamental difference between an interior-point method and a penalty-based approach in constrained optimization?

## Architecture Onboarding

- **Component map**: Preference GP Surrogate (PairwiseGP) -> Constraint GP Surrogate (SingleTaskGP) -> EUBOC Acquisition Function -> Optimization Engine (L-BFGS-B) -> Evaluation Interface

- **Critical path**:
  1. Warm-start constraint GP with N random samples (optional, N=50-1000 in paper)
  2. Maximize EUBOC(x(i), x(j)) to select candidate pair (num_restarts=3, raw_samples=512)
  3. Obtain human preference; evaluate constraint at both points
  4. Update both GP surrogates; repeat for 50 iterations or convergence

- **Design tradeoffs**:
  - Warm-start size vs. pre-computation cost (diminishing returns beyond ~50 points)
  - Approximation accuracy vs. speed (independence assumption avoids bivariate normal CDFs)
  - Restart count vs. acquisition optimization quality

- **Failure signatures**:
  - High-dimensional spaces (>12D) requiring more iterations or dimensionality reduction
  - Disconnected feasible regions missed by random warm-start
  - Noisy constraint evaluations violating the noise-free assumption
  - Poor CTR prediction model causing incorrect constraint enforcement

- **First 3 experiments**:
  1. Reproduce synthetic 2D results (Gardner test function, Equations 10-11) to verify ~100% feasibility within 10-20 iterations
  2. Ablate warm-starting on 6D test function with sizes {0, 50, 200} and compare optimality gap to Figure 7
  3. Test on a simple 2D color adjustment task with ground-truth preference (distance to reference) and constraint (RGB bounds) before deploying to human evaluators

## Open Questions the Paper Calls Out

### Open Question 1
Can EUBOC be extended to support simultaneous comparison of more than two design candidates by incorporating qEUBO concepts? The authors state that "extending it by incorporating the concept of qEUBO [Astudillo et al., 2023], an EUBO extension capable of sampling multiple search points simultaneously, would be a valuable research direction." The current EUBOC formulation only evaluates two search points at a time, limiting efficiency when designers could reasonably compare more candidates.

### Open Question 2
How does the uncorrelated constraint approximation in EUBOC affect optimization accuracy when constraint function values at compared points are strongly correlated? Equation 9 approximates the bivariate normal CDF by assuming c(x^(i)) and c(x^(j)) are uncorrelated, but the paper acknowledges "a correlation generally exists" and that "deriving the cumulative distribution function values analytically for a bivariate normal distribution is challenging." The approximation error may accumulate in high-dimensional spaces or when comparing nearby points where correlation is significant.

### Open Question 3
What is the performance of CPBO in actual online banner ad delivery with measured click-through rates compared to predicted CTRs? The authors note "Validating the CTRs of banner ads designed with our framework in an actual online delivery environment would also be beneficial. However, such experiments are not easy due to monetary, timing, client cooperation, and ethical considerations." The system relies on predicted CTR from a surrogate model; discrepancies between predicted and actual CTR could undermine the constraint satisfaction guarantee.

## Limitations
- Independence approximation for constraint probabilities may fail when constraint functions are highly correlated across the search space
- Method assumes noise-free constraint evaluations, which may not hold in practice with human judgment or uncertain predictions
- Warm-starting effectiveness depends on random sampling adequately covering disconnected or complex feasible regions

## Confidence

- **High Confidence**: The core mechanism of combining preference utility with constraint feasibility in EUBOC acquisition function is well-supported by both theoretical formulation and empirical results showing rapid convergence to feasible regions
- **Medium Confidence**: The dual surrogate model architecture is novel but relies on the assumption that preferences and constraints can be modeled independently, which may not hold when constraint satisfaction strongly depends on preference structure
- **Medium Confidence**: The warm-start constraint learning approach shows clear benefits in experiments, but its effectiveness may diminish for high-dimensional problems or disconnected feasible regions

## Next Checks

1. **Ablation Study**: Test CPBO without warm-start on a synthetic problem with disconnected feasible regions to evaluate whether random sampling adequately covers the feasible space.

2. **Constraint Correlation Test**: Create a synthetic problem where constraint satisfaction is highly correlated across the search space, then evaluate whether the independence approximation in EUBOC leads to poor constraint modeling.

3. **Noisy Constraint Evaluation**: Introduce varying levels of noise into constraint evaluations (e.g., 5-20% noise) and measure how CPBO's performance degrades compared to noise-free conditions.