---
ver: rpa2
title: Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using
  Modality-wise Multiview Clustering and Large Language Models
arxiv_id: '2506.12156'
source_url: https://arxiv.org/abs/2506.12156
tags:
- data
- cluster
- clustering
- clusters
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for interpreting clustered sensor
  data from older adults recovering from lower-limb fractures. The approach clusters
  multimodal sensor data by modality (e.g., motion, heart rate, sleep) and uses a
  large language model with context-aware prompts to generate interpretable cluster
  labels.
---

# Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using Modality-wise Multiview Clustering and Large Language Models

## Quick Facts
- arXiv ID: 2506.12156
- Source URL: https://arxiv.org/abs/2506.12156
- Reference count: 32
- This paper introduces a method for interpreting clustered sensor data from older adults recovering from lower-limb fractures.

## Executive Summary
This paper presents a novel approach to interpret unsupervised sensor clusters in post-lower-limb fracture recovery by combining modality-wise clustering with LLM-generated labels. The method clusters multimodal sensor data separately by modality (motion, heart rate, sleep, GPS, step count, acceleration) and uses a large language model with context-aware prompts to generate interpretable cluster labels from cluster centroids. Tested on 560 days of data from 10 patients, the approach successfully created clinically meaningful labels for most modalities, with statistical validation showing significant associations between clusters and clinical scores for all but the acceleration view. The method enables clinicians to identify at-risk patients and improve health outcomes through unsupervised analysis.

## Method Summary
The approach involves clustering multimodal sensor data by modality using K-means, then employing a large language model to generate interpretable cluster labels from the resulting centroids. The MAISON-LLF dataset containing 560 days of data from 10 older adults recovering from lower-limb fractures was used, with features including GPS, motion, heart rate, sleep, and step count. After normalizing features and clustering each modality separately, cluster centroids were reverse-normalized and presented to GPT-4o with context-aware prompts including feature definitions and patient context. Clinical scores were backward-filled to daily records and used for statistical validation through t-tests, Mann-Whitney U tests, ANOVA, or Kruskal-Wallis tests depending on data distribution.

## Key Results
- Modality-wise clustering with LLM labeling successfully generated interpretable cluster labels for all sensor modalities except acceleration
- Statistical validation confirmed most modality-specific clusters were significantly associated with clinical scores (SIS, OHS, OKS, TUG)
- Acceleration-based clusters failed to show statistical significance, likely due to data quality issues with the sensor
- The approach demonstrated that unsupervised clustering can reveal clinically meaningful recovery patterns when combined with appropriate labeling methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partitioning multimodal sensor data by modality before clustering improves interpretability without sacrificing clinical relevance.
- Mechanism: K-means clustering operates independently on each sensor view, decomposing a high-dimensional joint clustering problem into modality-specific sub-problems, producing cluster centers that reflect coherent, domain-aligned patterns.
- Core assumption: Each modality encodes semi-independent information about recovery behavior; clinical signals are not exclusively emergent from cross-modal interactions.
- Evidence anchors:
  - [abstract] "Clustering was first carried out separately for each data modality to assess the impact of feature sets extracted from each modality on patients' recovery trajectories."
  - [section 3] "A single-view clustering approach could result in multiple clusters across many features that may be difficult to interpret. To address this, the proposed method clusters the data by each modality."
- Break condition: If clinical outcomes depend primarily on complex cross-modal interactions, modality-wise clustering may fragment signal and reduce discriminative power.

### Mechanism 2
- Claim: Context-aware prompting enables LLMs to generate clinically plausible cluster labels from raw cluster centroids alone.
- Mechanism: Cluster centers are reverse-normalized to original feature scales and presented to GPT-4o alongside explicit feature definitions and domain context. The LLM maps numeric patterns to semantically meaningful behavioral descriptions.
- Core assumption: LLM pretraining on broad text corpora includes sufficient implicit knowledge about geriatric recovery, mobility patterns, and sensor semantics to translate centroid statistics into meaningful labels.
- Evidence anchors:
  - [abstract] "Using context-aware prompting, a large language model was employed to infer meaningful cluster labels for the clusters derived from each modality."
  - [section 4.1] Prompt example includes feature definitions, centroid values, and patient context; LLM outputs interpretable labels with short rationales.
- Break condition: If cluster centroids lack discriminative statistics due to sensor noise or missingness, LLM will infer spurious or overly generic labels regardless of prompt quality.

### Mechanism 3
- Claim: Statistical divergence of clinical scores across clusters provides post-hoc validation that unsupervised sensor clusters capture meaningful health variation.
- Mechanism: Clinical assessments are propagated backward from biweekly collection to daily records. Clusters from each modality are tested for differences in clinical score distributions; significant p-values indicate clusters segregate patients along clinically relevant dimensions.
- Core assumption: Biweekly clinical scores reasonably approximate daily patient status when backward-filled; sensor-derived clusters should align with these ground-truth measures if they capture recovery-relevant behavior.
- Evidence anchors:
  - [abstract] "Statistical analysis confirmed that most modality-specific clusters were significantly associated with clinical scores, validating the interpretability of the labels."
  - [section 4.3] Table 3 shows significance results; acceleration view fails all tests, likely due to data quality issues.
- Break condition: If clinical scores are too sparse, noisy, or misaligned in time with sensor patterns, statistical tests may yield false negatives or false positives.

## Foundational Learning

- Concept: **K-means clustering and centroid interpretation**
  - Why needed here: The pipeline relies on K-means per modality; understanding how centroids summarize within-cluster variation is essential to diagnose label quality and detect when centroids misrepresent their cluster.
  - Quick check question: Given a 3-feature cluster centroid [2.1, 0.4, 85.0], what does each value represent relative to its cluster members?

- Concept: **Normalization and reverse normalization**
  - Why needed here: Features are normalized before clustering to prevent scale dominance; centroids are reverse-normalized before LLM prompting. Misalignment here corrupts label inference.
  - Quick check question: If a feature was min–max scaled to [0,1] and the centroid value is 0.73, what additional information is required to recover the original-scale value?

- Concept: **Non-parametric statistical tests (Mann–Whitney U, Kruskal–Wallis)**
  - Why needed here: Clustered clinical scores often violate normality; selecting and interpreting the correct test determines whether validation claims are sound.
  - Quick check question: When would you choose Kruskal–Wallis over ANOVA for comparing clinical scores across five clusters?

## Architecture Onboarding

- Component map:
  - Data layer (MAISON-LLF dataset) -> Preprocessing (feature filtering, normalization) -> Clustering module (K-means per modality) -> Labeling module (LLM prompting) -> Validation module (statistical testing)

- Critical path:
  1. Load and preprocess MAISON-LLF (filter to sensor features only)
  2. Normalize features; run K-means per view with fixed random seed
  3. Reverse-normalize centroids; construct context-aware prompt; call GPT-4o API; parse label + description
  4. Join cluster assignments with clinical scores; run statistical tests; generate significance table and plots

- Design tradeoffs:
  - **Modality-wise vs. joint clustering**: Modality-wise improves interpretability but may miss cross-modal patterns; joint clustering could capture interactions but yields harder-to-explain centroids
  - **Fixed vs. variable K per modality**: Optimal K per modality adapts to data structure but complicates cross-modality comparison; fixed K simplifies comparison but risks under/over-clustering
  - **LLM temperature and prompt specificity**: Higher temperature increases label diversity but reduces reproducibility; overly specific prompts may bias labels toward expected clinical narratives

- Failure signatures:
  - **Severe cluster imbalance**: Position view Cluster 1 (558 points) vs. Cluster 2 (2 points) causes NaN statistics and unreliable validation
  - **Sensor artifacts in centroids**: Heart rate centroids with zero minima (hardware noise) produce nonsensical labels unless explicitly described in prompts
  - **Non-significant validation across all modalities**: Suggests feature extraction or clustering hyperparameters fail to capture recovery-relevant structure; requires feature engineering revisit

- First 3 experiments:
  1. Reproduce clustering on a single modality (e.g., step view) with fixed seed; compare centroid values and LLM labels to paper outputs to verify pipeline integrity
  2. Ablate domain context from prompts (remove patient population and recovery context); assess whether LLM labels degrade in clinical plausibility
  3. Re-run clustering with alternative K selection (e.g., silhouette score vs. elbow method) on one modality; test whether statistical significance against clinical scores improves or degrades

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fusing the clustering outcomes from individual modalities into a final global clustering yield superior explainability compared to standard single-view clustering?
- Basis in paper: [explicit] The authors state in Section 5 that the most intuitive future direction is to "combine the outcomes of multiple clusterings from each modality to perform a final clustering on the overall dataset" to assess benefits over single-view methods.
- Why unresolved: The current study analyzed each modality in isolation and did not integrate these views into a unified model to evaluate synergistic effects on interpretability.
- What evidence would resolve it: A comparative study showing that ensemble multiview clusters align better with clinical scores or provide more coherent patient groupings than clusters derived from single-view aggregations.

### Open Question 2
- Question: Can spatio-temporal clustering algorithms effectively capture the dynamic relationship between time and sensor patterns during patient recovery?
- Basis in paper: [explicit] Section 5 notes that "The data have temporal and value-based dimensions; thus, spatio-temporal clustering algorithms... should be applied."
- Why unresolved: The current methodology used K-means, which does not inherently model temporal dependencies, potentially missing time-evolving recovery trajectories.
- What evidence would resolve it: Results from spatio-temporal models demonstrating the detection of time-dependent patterns (e.g., gradual improvement vs. sudden decline) that static clustering failed to identify.

### Open Question 3
- Question: Can domain-informed feature engineering or representation learning resolve the lack of statistical significance found in acceleration-based clusters?
- Basis in paper: [explicit] The authors note in Section 1 and Section 4.4 that acceleration clusters failed to align with clinical scores, likely due to data quality and sensitivity issues, and suggest "domain-informed feature engineering or representation learning techniques may further enhance the interpretability."
- Why unresolved: The current features extracted from acceleration data were insufficient to distinguish between different clinical states, rendering that modality ineffective for the pipeline.
- What evidence would resolve it: A re-analysis of the acceleration modality using advanced features (e.g., frequency domain features) that results in clusters with statistically significant p-values (< 0.05) against clinical scores.

### Open Question 4
- Question: Does incorporating clinical data directly as input features, rather than just for validation, improve the clustering performance and explainability?
- Basis in paper: [explicit] Section 5 asks whether "combining clinical data with sensor data... improves clustering performance and contributes to explainability."
- Why unresolved: The current pipeline strictly separated sensor data (input) from clinical scores (ground truth) to ensure the method worked in an unsupervised setting.
- What evidence would resolve it: Experiments showing that clusters formed by mixing clinical and sensor data provide more distinct or clinically relevant groupings than sensor data alone.

## Limitations
- The method relies on a single dataset with only 10 patients, raising questions about external validity and generalizability to broader populations.
- The acceleration modality's failure to show statistical significance suggests sensor quality issues that may affect other modalities in different contexts.
- LLM-generated cluster labels lack direct validation against human expert labels, making it difficult to assess labeling accuracy independently of the statistical validation method.

## Confidence
- **High Confidence**: The statistical validation framework using clinical scores to assess cluster quality is methodologically sound and well-implemented.
- **Medium Confidence**: The modality-wise clustering approach produces interpretable results and improves clinical relevance compared to joint clustering, though this advantage needs broader testing.
- **Low Confidence**: The LLM-generated cluster labels are clinically plausible but lack independent validation; their reliability across different patient populations or sensor configurations remains unproven.

## Next Checks
1. **Cross-dataset validation**: Test the complete pipeline on at least one additional post-fracture or post-discharge recovery dataset with different sensor configurations to assess external validity.

2. **Expert label comparison**: Recruit clinical experts to independently label a subset of clusters (without seeing LLM labels) and compare agreement rates to establish labeling reliability.

3. **Cross-modal interaction test**: Implement a hybrid approach that clusters jointly across modalities but separates cluster centers by modality for LLM labeling, then compare statistical validation results to the pure modality-wise approach.