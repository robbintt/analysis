---
ver: rpa2
title: 'RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings'
arxiv_id: '2509.14383'
source_url: https://arxiv.org/abs/2509.14383
tags:
- alignment
- cross-modal
- robustness
- adversarial
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving adversarial robustness
  in unified multi-modal encoders, particularly focusing on the LanguageBind model
  which shows vulnerability under adversarial attacks. The proposed solution, RLBind,
  introduces a two-stage adversarial-invariant cross-modal alignment framework.
---

# RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings

## Quick Facts
- arXiv ID: 2509.14383
- Source URL: https://arxiv.org/abs/2509.14383
- Reference count: 23
- LanguageBind model shows vulnerability to adversarial attacks, with RLBind achieving 56.76% robustness on images at ε=2/255 (up from 9.12%) while maintaining clean accuracy

## Executive Summary
This paper addresses the vulnerability of unified multi-modal encoders like LanguageBind to adversarial attacks by proposing RLBind, a two-stage adversarial-invariant cross-modal alignment framework. Stage 1 applies unsupervised fine-tuning (FARE) to align clean and adversarial embeddings, improving the visual encoder's robustness. Stage 2 introduces cross-modal correspondence alignment by minimizing the discrepancy between clean/adversarial features and text anchors, enforcing class-wise distributional alignment across modalities. Extensive experiments across image, audio, thermal, and video data demonstrate consistent improvements in both clean accuracy and norm-bounded adversarial robustness compared to LanguageBind and standard fine-tuning baselines.

## Method Summary
RLBind employs a two-stage adversarial-invariant cross-modal alignment framework. Stage 1 uses FARE unsupervised fine-tuning, minimizing L2 distance between clean and APGD-perturbed embeddings for non-text encoders to enhance robustness. Stage 2 initializes from Stage 1 and optimizes a combined loss function that includes clean and adversarial cross-entropy with a cross-modal alignment (CMA) component. The CMA aligns clean and adversarial correspondence scores to text anchors (class name embeddings) using dot-product and L2 alignment. The framework uses ViT-L/14 backbones, freezes the text encoder, and applies LoRA fine-tuning to other modality encoders, trained for 1 epoch per stage on 8 vGPUs.

## Key Results
- RLBind achieves significant robustness improvements: image robustness increases from 9.12% to 56.76% at ε=2/255
- Maintains or enhances clean accuracy across all tested modalities (ImageNet, ESC-50, LLVIP, MSR-VTT)
- Outperforms LanguageBind backbone and standard fine-tuning baselines in both clean and robust accuracy metrics
- Shows near-perfect thermal results (90% across all metrics), though this may indicate dataset-specific characteristics

## Why This Works (Mechanism)
RLBind improves adversarial robustness through a two-stage process that first makes the visual encoder robust to perturbations and then aligns cross-modal representations. Stage 1's FARE fine-tuning ensures that adversarial perturbations don't significantly change the embedding space by aligning clean and perturbed features. Stage 2's cross-modal alignment forces clean and adversarial features to maintain consistent correspondence to text anchors, creating an adversarial-invariant alignment that preserves semantic relationships even under attack. This approach leverages the text encoder as a stable reference point, making the overall system more resistant to norm-bounded adversarial attacks while preserving clean accuracy.

## Foundational Learning
- **Adversarial robustness**: The ability of models to maintain performance under adversarial attacks. Needed because multi-modal models like LanguageBind are vulnerable to small input perturbations. Quick check: Test clean vs. robust accuracy under APGD attacks at different ε values.
- **Cross-modal alignment**: Aligning representations from different modalities (text, image, audio, etc.) in a shared embedding space. Needed to enable unified processing across diverse sensor inputs. Quick check: Verify cosine similarity between aligned modality embeddings for same-class samples.
- **Norm-bounded attacks**: Adversarial attacks constrained by L∞ norm (ε), limiting perturbation magnitude. Needed to model realistic threat models. Quick check: Confirm APGD perturbations stay within ε constraint using L∞ norm.
- **Unsupervised fine-tuning**: Training without explicit labels, using self-supervised objectives like feature alignment. Needed for Stage 1 to improve robustness without clean labels. Quick check: Measure L2 distance between clean and adversarial embeddings before/after Stage 1.
- **Text anchors**: Class-specific text embeddings used as reference points for alignment. Needed to provide stable targets for cross-modal correspondence. Quick check: Verify text anchors are consistent class representatives by checking intra-class similarity.

## Architecture Onboarding
- **Component map**: ViT-L/14 Image/Audio/Thermal/Video encoders -> FARE alignment -> CMA alignment -> Text encoder (frozen) -> Shared embedding space
- **Critical path**: Clean/adversarial input -> Non-text encoder -> FARE alignment (Stage 1) -> CMA alignment with text anchors (Stage 2) -> Classification head
- **Design tradeoffs**: FARE provides robustness but may lose semantic information; CMA preserves semantics but adds complexity; text encoder freezing ensures stability but limits adaptation
- **Failure signatures**: Poor robustness despite Stage 1 indicates FARE insufficient; clean accuracy drop indicates over-alignment; modality-specific failures suggest anchor construction issues
- **First experiments**: 1) Run Stage 1 FARE only and measure robustness gain; 2) Test CMA with random vs. class-specific text anchors; 3) Vary λ CMA weight to find optimal robustness-clean accuracy balance

## Open Questions the Paper Calls Out
- **Open Question 1**: Can RLBind scale effectively to the full VIDAL-10M dataset (10M samples across 5 modalities) while preserving both robustness and generalization gains? Current experiments use smaller datasets with limited sampling.
- **Open Question 2**: Does RLBind transfer robustness to modalities excluded from training, such as depth, which LanguageBind supports but this work didn't evaluate?
- **Open Question 3**: Does RLBind's robustness against L∞ APGD attacks generalize to other threat models such as L2-norm attacks, spatial transformations, or natural corruptions?
- **Open Question 4**: What explains the near-perfect thermal results (90% clean and robust accuracy across all attack strengths) and does this indicate dataset-specific artifacts or overfitting?

## Limitations
- Missing hyperparameters (λ weight, APGD iterations, learning rate, batch size, LoRA rank) prevent exact reproduction
- Evaluation limited to norm-bounded APGD attacks at two ε values without testing other attack types
- Training duration very short (1 epoch per stage), raising questions about scalability of improvements
- Minimal improvement for Audio encoder at higher ε suggests modality-specific limitations
- Near-perfect thermal results may indicate dataset-specific characteristics rather than genuine robustness

## Confidence
- **High confidence**: Core methodological contribution (two-stage framework) is clearly described and coherent
- **Medium confidence**: Reported robustness gains are plausible but depend on unspecified hyperparameters and limited attack diversity
- **Low confidence**: Claims of "unified" robustness are not fully supported given narrow attack space and variable modality performance

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary λ in Stage 2 and LoRA rank in Stage 1 to determine robustness gains sensitivity and identify optimal configurations
2. **Attack diversity evaluation**: Evaluate RLBind against broader attack types including L2-bounded attacks (Carlini-Wagner), adaptive attacks, and larger ε values to test robustness limits
3. **Modality-specific analysis**: Conduct detailed per-modality ablation studies to understand performance variation and test whether modality-specific adjustments improve overall results