---
ver: rpa2
title: Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding
  of Knowledge Concepts in MOOCs
arxiv_id: '2505.10074'
source_url: https://arxiv.org/abs/2505.10074
tags:
- question
- learners
- knowledge
- concepts
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Graph RAG pipeline to support learners in
  understanding new knowledge concepts in MOOCs by leveraging Educational Knowledge
  Graphs (EduKGs) and Personal Knowledge Graphs (PKGs). The pipeline addresses the
  challenge of learners struggling to understand new concepts in MOOCs due to limited
  interaction with instructors.
---

# Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs

## Quick Facts
- **arXiv ID:** 2505.10074
- **Source URL:** https://arxiv.org/abs/2505.10074
- **Reference count:** 13
- **Primary result:** Graph RAG pipeline shows 45% QA accuracy and 2.862/3 linguistic quality for personalized questions in MOOCs

## Executive Summary
This paper introduces a Graph Retrieval-Augmented Generation (RAG) pipeline to address the challenge of learners struggling to understand new concepts in MOOCs due to limited instructor interaction. The system leverages Educational Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to provide personalized learning support. The pipeline implements two key methods: a PKG-based Question Generation approach that recommends questions based on learners' identified knowledge gaps, and an EduKG-based Question Answering method that retrieves relevant information from Wikipedia to answer learner questions. The system was evaluated by expert instructors across three MOOCs, demonstrating promising results for personalized question generation while highlighting limitations in answer specificity.

## Method Summary
The Graph RAG pipeline consists of two main components. The PKG-based Question Generation method identifies when learners mark concepts as "Did Not Understand" (DNU), then retrieves the DNU concept, slide text, and other concepts from the Neo4j graph database to generate personalized questions using GPT-3.5-turbo. These questions are re-ranked using sentence-transformer embeddings based on semantic similarity to the original slide text. The EduKG-based Question Answering method employs a two-stage retrieval process: first retrieving Wikipedia paragraphs via vector similarity, then traversing to Related Concepts (RCs) in the EduKG if initial retrieval fails. The system uses extractive prompting to reduce hallucinations by instructing the LLM to extract verbatim text from retrieved contexts, providing citations to source Wikipedia paragraphs.

## Key Results
- PKG-based Question Generation achieved linguistic quality scores averaging 2.862 out of 3 (1-3 scale)
- Question generation showed strong relevance to both slides and DNU concepts
- EduKG-based Question Answering achieved 45% accuracy across all three MOOCs
- Instructors noted answers were often too abstract due to extractive prompting approach

## Why This Works (Mechanism)

### Mechanism 1: PKG-based Personalized Question Generation via Graph-Guided Context Retrieval
Modeling learner knowledge gaps through Personal Knowledge Graphs enables contextually relevant question recommendations. When learners mark a concept as "Did Not Understand," the system retrieves the DNU concept, slide text, and other Main Concepts from Neo4j, then generates questions using zero-shot GPT-3.5-turbo with explicit rules preventing semantically similar questions. Questions are re-ranked using sentence-transformer embeddings based on similarity to slide text. The core assumption is that learners can accurately self-identify gaps and slide context provides sufficient grounding for pedagogically useful questions.

### Mechanism 2: Two-Stage Graph-Guided Retrieval for Question Answering
Hierarchical traversal from Main Concepts to Related Concepts in the EduKG enables broader answer coverage. Stage 1 chunks Wikipedia articles of MCs into paragraphs, indexes them as Neo4j nodes with vector embeddings, and retrieves top-k paragraphs via cosine similarity. Stage 2 triggers when Stage 1 yields no answer, using an LLM-based retriever to select relevant Related Concepts from the EduKG. The LLM reasons which RC's Wikipedia article likely contains the answer, enabling traversal from general to specialized topics.

### Mechanism 3: Extractive Prompting to Reduce Hallucinations
Constraining LLMs to extract verbatim text from retrieved contexts increases trustworthiness by grounding responses in citable sources. The QA prompt template instructs extraction "AS IS" from context, preventing paraphrasing or reasoning beyond retrieved text. Citations link answers to source Wikipedia paragraphs with answer highlighting. This reduces hallucinations but often yields abstract answers that don't directly address learner questions.

## Foundational Learning

- **Knowledge Graphs (Neo4j)**: EduKG and PKG are stored in Neo4j with nodes and typed edges. Understanding Cypher queries and graph traversal is essential for debugging retrieval. *Quick check: Can you write a query to fetch all Main Concepts marked as "Did Not Understand" by a specific learner for a given slide?*

- **Vector Embeddings & Semantic Similarity**: Question re-ranking and paragraph retrieval use sentence-transformer embeddings with cosine similarity. Understanding embedding spaces, chunk size effects, and similarity thresholds is critical for tuning retrieval quality. *Quick check: Why might cosine similarity between a question embedding and paragraph embeddings fail for domain-specific terminology not well-represented in the embedding model?*

- **Graph RAG vs. Standard RAG**: This system uses graph-based indexing rather than flat document retrieval. Understanding when graph structure helps (multi-hop reasoning, hierarchical concepts) vs. when it adds overhead is key to architectural decisions. *Quick check: What types of questions would benefit from graph traversal (MC→RC) versus single-hop vector retrieval?*

## Architecture Onboarding

- **Component map:** Neo4j Graph Database -> Graph Vector Store -> GPT-3.5-turbo -> Sentence-Transformer Model -> CourseMapper Platform

- **Critical path:** Learner marks DNU → Graph-guided retrieval fetches (DNU, slide_text, slide_concepts) → LLM generates questions → Re-ranked by slide similarity → Learner selects question → Vector retrieval of top-k Wikipedia paragraphs → If no answer, LLM retrieves RC → Extracts answer from RC Wikipedia → Answer displayed with citation

- **Design tradeoffs:** Extractive vs. Abstractive QA (hallucination control vs. pedagogical value), Wikipedia vs. Course Materials (breadth vs. relevance), RC Traversal Depth (coverage vs. latency)

- **Failure signatures:** Disambiguation failures (confusing UX "Emergency Exit" with building safety), Abstract answers (literal Wikipedia text not addressing questions), No output (questions requiring reasoning beyond contexts), Low relevance questions (insufficient slide context)

- **First 3 experiments:** 1) Compare hybrid graph-vector retrieval vs. pure vector retrieval on 100 held-out questions, 2) Test extractive vs. abstractive QA prompts on same question set, 3) Implement concept disambiguation before RC traversal and evaluate accuracy improvements

## Open Questions the Paper Calls Out
- **Open Question 1:** Does incorporating external learning resources (scientific literature, videos, other MOOCs) into the Educational Knowledge Graph improve the specificity and accuracy of generated answers compared to Wikipedia-only sources?
- **Open Question 2:** Can Chain-of-Thought prompting enable the LLM to perform advanced reasoning while maintaining factual grounding in provided evidence?
- **Open Question 3:** How can the graph retrieval mechanism be enhanced to disambiguate knowledge concepts that share labels but differ in meaning across domains?
- **Open Question 4:** Does the PKG-based Question Generation method lead to improved knowledge retention or learning gains for students compared to standard search tools?

## Limitations
- 45% accuracy rate indicates significant room for improvement in answer relevance and specificity
- Extractive prompting approach yields abstract answers that often don't directly address learner questions
- Limited evaluation scope (three MOOCs) restricts generalization claims
- Reliance on Wikipedia sources may not capture course-specific definitions and context

## Confidence
- **High confidence:** PKG-based question generation mechanism and its positive linguistic quality evaluation (2.862/3)
- **Medium confidence:** Graph-guided QA mechanism and its 45% accuracy rate (methodologically sound but limited by extractive prompting)
- **Low confidence:** Generalization to other domains (evaluation limited to three MOOCs) and the effectiveness of extractive prompting for educational contexts

## Next Checks
1. Compare hybrid graph-vector retrieval vs. pure vector retrieval on 100 held-out questions from the three MOOCs to quantify the graph advantage
2. Test extractive vs. abstractive QA prompts on the same question set to measure hallucination rates and instructor preference
3. Implement concept disambiguation before RC traversal and evaluate accuracy improvements on ambiguous terms (e.g., "Emergency Exit")