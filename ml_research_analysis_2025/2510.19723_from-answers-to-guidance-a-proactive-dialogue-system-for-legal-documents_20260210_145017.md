---
ver: rpa2
title: 'From Answers to Guidance: A Proactive Dialogue System for Legal Documents'
arxiv_id: '2510.19723'
source_url: https://arxiv.org/abs/2510.19723
tags:
- dialogue
- legal
- dataset
- topic
- follow-up
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EUDial, a multi-turn dialogue dataset constructed
  from European Parliamentary Research Service blogs, containing 880 dialogue turns
  across 204 dialogues that transform single-turn legal Q&A pairs into structured
  conversations with follow-up questions. The authors also propose LexGuide, a proactive
  dialogue framework that combines retrieval-augmented generation with hierarchical
  topic organization to guide users through legal information systematically.
---

# From Answers to Guidance: A Proactive Dialogue System for Legal Documents

## Quick Facts
- **arXiv ID**: 2510.19723
- **Source URL**: https://arxiv.org/abs/2510.19723
- **Reference count**: 40
- **Primary result**: Proposes EUDial dataset and LexGuide framework for proactive legal dialogue navigation with 98.7% groundedness and 73.7% topic coverage

## Executive Summary
This paper introduces EUDial, a multi-turn dialogue dataset constructed from European Parliamentary Research Service blogs, containing 880 dialogue turns across 204 dialogues that transform single-turn legal Q&A pairs into structured conversations with follow-up questions. The authors also propose LexGuide, a proactive dialogue framework that combines retrieval-augmented generation with hierarchical topic organization to guide users through legal information systematically. Experiments show LexGuide achieves 98.7% groundedness and 0.761 relevance scores while maintaining 73.7% topic coverage through structured navigation, demonstrating effective balance between comprehensive coverage and focused, accessible responses. The framework transitions reactive legal dialogue systems into proactive guides that help laypersons navigate complex legal information through systematic topic exploration.

## Method Summary
The authors constructed EUDial by converting 204 AskEP blogs into 880 multi-turn dialogues using GPT-4o-mini to generate Q&A pairs and map sections to dialogue turns. For the LexGuide framework, they implement a RAG architecture enhanced with hierarchical topic trees: retrieving legal fragments using FAISS with MMR ($\lambda=0.6$), clustering them via BERTopic into topic trees, and navigating using BFS strategy while maintaining state to track visited nodes. The system generates answers and follow-up questions based on representative words from the next topic node, using LLMs like GPT-4o-mini with temperature 0.3.

## Key Results
- LexGuide achieves 98.7% groundedness in legal information retrieval
- Maintains 73.7% topic coverage through systematic BFS navigation
- Achieves 0.761 relevance score while balancing completeness with focused responses
- Demonstrates effective transition from reactive Q&A to proactive legal guidance

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Topic Organization
- **Claim**: Structuring retrieved legal fragments into a topic tree may reduce information overload by allowing the system to navigate broad themes before descending into specifics.
- **Mechanism**: The system retrieves an initial set of fragments using Maximum Marginal Relevance (MMR) to ensure diversity. It then clusters these fragments using BERTopic and hierarchical clustering to form a topic tree $G=(V,E)$. This transforms a flat list of search results into a navigable hierarchy where nodes represent sub-topics.
- **Core assumption**: Assumes that clustering embedding vectors of legal fragments reliably corresponds to meaningful legal sub-topics that users would naturally want to explore sequentially.
- **Evidence anchors**:
  - [section] Section 4 details the "Hierarchical organization" process: "retrieved fragments R are clustered using BERTopic... This creates a topic tree G."
  - [abstract] Mentions leveraging "hierarchical topic organization to structure dialogue progression."
  - [corpus] Related work (e.g., LegalRAG, Fishing for Answers) supports the efficacy of RAG and iterative retrieval in legal domains, though specific evidence for *topic-tree navigation* specifically improving legal comprehension is limited in the provided neighbors.
- **Break condition**: Fails if the initial retrieval is irrelevant (garbage in, garbage out) or if the clustering algorithm produces incoherent groups (e.g., grouping unrelated regulations together).

### Mechanism 2: State-Driven Navigation over Graphs
- **Claim**: Explicitly tracking visited nodes and navigation strategy allows the system to maintain coherence across multi-turn dialogues, preventing the repetitive or lost context often seen in reactive chatbots.
- **Mechanism**: The framework maintains a navigation state $S^i_j$ containing visited nodes ($Y$), current node ($L$), and unexplored children ($Z$). Instead of treating each turn independently, the system uses this state to decide on the next move (descend, lateral, ascend) based on a strategy (BFS/DFS), ensuring systematic coverage.
- **Core assumption**: Assumes users prefer a guided, systematic tour of the topic space (BFS/DFS) rather than immediate specific answers, and that the "default" BFS strategy matches user mental models of legal exploration.
- **Evidence anchors**:
  - [section] Equation 2 in Section 4 defines the navigation state $S^i_j = (Y^i_j, L^i_j, Z^i_j, B^i_j, K^i_j, H^i_j)$.
  - [abstract] Highlights the transition to "proactive guides" helping users "navigate complex legal information through systematic topic exploration."
  - [corpus] ProactiveEval and PIVOT papers in the corpus suggest proactive acquisition is valuable, supporting the move away from purely reactive systems.
- **Break condition**: Fails if the user's intent shifts unpredictably and the system forces them back into the pre-computed tree hierarchy, causing friction.

### Mechanism 3: Constrained Follow-up Generation
- **Claim**: Generating follow-up questions based on the *next* topic node's representative words keeps the dialogue forward-moving and grounded in the pre-structured hierarchy.
- **Mechanism**: Rather than asking the LLM to hallucinate a follow-up based on general context, the system identifies the next topic node $g_{next}$ and extracts its top representative words ($W_{next}$). These words are fed into the LLM prompt to construct the follow-up question.
- **Core assumption**: Assumes that the keywords extracted via TF-IDF for a cluster are sufficient to generate a coherent and enticing follow-up question for a layperson.
- **Evidence anchors**:
  - [section] Section 4 describes: "follow-up question is generated using $W_{next}$ and dialogue history... where $W_{next}$ denotes the top-l topic representative words."
  - [section] Section 6 notes that this method promotes "substantial diversity (0.204-0.224) in follow-up questions."
  - [corpus] Weak evidence in corpus for *keyword-based* follow-up generation specifically; neighbors focus more on retrieval strategies than generation constraints.
- **Break condition**: Fails if the TF-IDF keywords are technical jargon, resulting in follow-up questions that confuse laypersons.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here**: LexGuide is fundamentally a RAG architecture enhanced with a navigation layer. Without understanding how retrieval context feeds the LLM, the grounding mechanism (98.7% groundedness) cannot be understood.
  - **Quick check question**: How does providing external document chunks to an LLM reduce hallucination compared to standard prompting?

- **Concept: Clustering and Topic Modeling (BERTopic)**
  - **Why needed here**: The core innovation is turning retrieval results into a "Topic Tree." You must understand how documents are embedded and clustered to visualize how the system organizes legal information.
  - **Quick check question**: If two legal fragments discuss different aspects of the same law, would standard clustering place them in the same cluster or separate ones based on vector proximity?

- **Concept: Graph Traversal (BFS vs. DFS)**
  - **Why needed here**: The "Navigation" component relies on graph theory. The paper defaults to Breadth-First Search (BFS) to ensure "diverse topical coverage" before going deep.
  - **Quick check question**: In a legal context, when might Depth-First Search (DFS) be preferred over BFS for a user query?

## Architecture Onboarding

- **Component map**: Corpus Processor -> Retriever (FAISS + MMR) -> Topic Builder (BERTopic + Clustering) -> Navigator (State Machine + BFS/DFS) -> Generator (LLM)
- **Critical path**: The **Topic Builder** is the most critical and fragile component. If the hierarchy is shallow or non-existent (e.g., $|V| < 2$), the system degrades to a basic RAG model with no navigation capabilities.
- **Design tradeoffs**:
  - **Completeness vs. Guidance**: The paper explicitly notes a 2.7% reduction in completeness (ROUGE-L) to prioritize "focused, relevant responses" and navigation.
  - **Pre-computation vs. Runtime**: The topic tree $G$ is built *at the start of the dialogue* (on query). This provides flexibility but adds latency to the first turn.
  - **Diversity vs. Relevance**: MMR with $\lambda=0.6$ forces diversity in the initial retrieval, potentially sacrificing some precision for broader topic coverage.
- **Failure signatures**:
  - **"Lost in Navigation"**: The system insists on visiting every node in the tree even if the user signals satisfaction early.
  - **Topic Bleed**: Clustering fails, resulting in a follow-up question about "Healthcare" when the user was asking about "Immigration" because fragments were incorrectly grouped.
  - **State Drift**: The system loses track of $Y$ (visited nodes) and repeats information from previous turns.
- **First 3 experiments**:
  1. **Sanity Check (Retrieval)**: Verify the MMR retriever actually returns diverse results. Run a query and check if the top 10 fragments cover distinct sub-topics or are just near-duplicates.
  2. **Static Validation (Tree Quality)**: Manually inspect the generated Topic Tree for 5-10 sample queries. Does the hierarchy make sense to a human? Are the node labels (TF-IDF words) descriptive?
  3. **Dynamic Test (Navigation Loop)**: Simulate a user who always says "Yes" to follow-ups. Does the system traverse the entire tree (BFS) and terminate correctly? Does the state $S$ update accurately at each turn?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LexGuide's proactive navigation improve laypersons' comprehension and task completion compared to reactive legal dialogue systems?
- Basis in paper: [explicit] "In our ongoing work, we are incorporating human studies with legal experts and lay users to validate factual faithfulness and navigational clarity beyond automatic metrics."
- Why unresolved: All reported evaluations use automated metrics (groundedness, BERTScore, ROUGE-L); no human-subject experiments assess whether proactive guidance actually helps users understand legal information or complete real tasks.
- What evidence would resolve it: Controlled user studies measuring comprehension accuracy, task success rates, and subjective satisfaction comparing LexGuide against reactive baselines with layperson participants.

### Open Question 2
- Question: How does LexGuide perform when users deviate from linear topic exploration, such as declining follow-ups or requesting topic changes mid-dialogue?
- Basis in paper: [explicit] "The dataset does not capture alternative conversational paths such as users declining information, requesting different topics mid-conversation, or terminating dialogue before entire topic coverage. These assumptions... represent areas for future extension."
- Why unresolved: EUDial assumes users always affirm follow-up questions, creating linear dialogues. Real-world deployment would encounter non-affirmative responses, yet the framework's handling of such cases remains unevaluated.
- What evidence would resolve it: Evaluation on dialogues containing user rejections, topic pivots, and early terminations; metrics assessing recovery success and user satisfaction in non-linear scenarios.

### Open Question 3
- Question: Can LexGuide's hierarchical navigation approach scale to legal domains with deeper topic structures beyond the observed two-level hierarchies?
- Basis in paper: [inferred] The paper notes "in the current EUDial dataset, dialogues yield shallow hierarchies of depth two, reflecting that citizen questions tend to anchor a dominant theme with a small number of focused subtopics. In other domains or dialogue styles, the same procedure may yield deeper trees without modification."
- Why unresolved: EUDial's shallow hierarchies may not stress-test the framework's navigation mechanisms. Whether BFS/DFS strategies remain effective with 4+ levels of topic depth is unknown.
- What evidence would resolve it: Testing LexGuide on legal corpora with verified multi-level topic structures; analysis of navigation efficiency, user disorientation rates, and topic coverage as hierarchy depth increases.

## Limitations

- The paper's core innovation depends critically on BERTopic clustering quality, which varies with legal document complexity and may not reliably produce meaningful topic hierarchies.
- The assumption that TF-IDF keywords can generate coherent follow-up questions for non-expert users remains untested without human evaluation.
- BFS navigation may force users through topics they've already understood, potentially reducing practical utility despite systematic coverage.

## Confidence

- **High Confidence**: The dataset construction methodology (EUDial from AskEP blogs) and the basic RAG retrieval with MMR (98.7% groundedness) are well-specified and reproducible.
- **Medium Confidence**: The hierarchical topic organization and navigation framework are sound in principle, but their effectiveness depends on clustering quality that varies with legal document complexity.
- **Low Confidence**: The claim that this proactive system is superior for "laypersons" navigating legal information lacks direct user study evidence - current validation is based on automated metrics rather than actual user comprehension or satisfaction.

## Next Checks

1. **User Comprehension Test**: Conduct a controlled study comparing LexGuide vs. standard RAG with 20-30 lay users on identical legal queries, measuring actual understanding (quiz scores) rather than just retrieval metrics.
2. **Clustering Stability Analysis**: Run the topic tree generation 10 times with different random seeds and measure variance in tree structure and node assignments to quantify reliability.
3. **Navigation Efficiency Evaluation**: Track how often users reach satisfactory answers through the minimum number of turns versus being forced through unnecessary topics, comparing BFS vs. adaptive navigation strategies.