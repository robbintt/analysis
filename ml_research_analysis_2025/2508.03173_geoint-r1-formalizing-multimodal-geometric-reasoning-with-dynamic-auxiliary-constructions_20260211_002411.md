---
ver: rpa2
title: 'Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary
  Constructions'
arxiv_id: '2508.03173'
source_url: https://arxiv.org/abs/2508.03173
tags:
- reasoning
- arxiv
- auxiliary
- geometric
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Geoint-R1, a multimodal geometric reasoning
  framework that integrates auxiliary line construction, formal reasoning via Lean4,
  and interactive visualization. It addresses the challenge of formal geometric reasoning,
  particularly the dynamic construction and verification of auxiliary geometric elements.
---

# Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions

## Quick Facts
- **arXiv ID:** 2508.03173
- **Source URL:** https://arxiv.org/abs/2508.03173
- **Reference count:** 8
- **Key outcome:** Geoint-R1, a multimodal geometric reasoning framework integrating auxiliary line construction, formal reasoning via Lean4, and interactive visualization, achieves 57.01% answer-based and 72.43% proof-based accuracy on the Geint benchmark, significantly outperforming existing models on problems requiring auxiliary constructions.

## Executive Summary
Geoint-R1 introduces a novel multimodal geometric reasoning framework that addresses the challenge of formal geometric reasoning, particularly the dynamic construction and verification of auxiliary geometric elements. The framework combines supervised fine-tuning with reinforcement learning using a verification reward model that evaluates correctness, auxiliary line construction, and format adherence. Trained on the Geint benchmark—a dataset of 1,885 rigorously annotated geometry problems—Geoint-R1 demonstrates significant improvements over existing models, particularly on problems requiring auxiliary constructions, with notably strong performance on both answer-based and proof-based questions.

## Method Summary
Geoint-R1 employs a two-stage training approach on the Qwen2.5-VL-7B base model. Stage 1 involves supervised fine-tuning (SFT) using negative log-likelihood loss over reference solutions including Lean4 code, establishing structural competence and formal language syntax. Stage 2 applies reinforcement learning via Group Relative Policy Optimization (GRPO) with a verification reward model, refining policy on rejection-sampled outputs. The verification reward combines three components: correctness, auxiliary construction accuracy, and format adherence. Curriculum learning via difficulty-sorted sampling stabilizes RL progression, presenting easier instances before harder ones. The framework generates formally verifiable solutions in Lean4, enabling interactive visualization and rigorous proof verification.

## Key Results
- Achieves 57.01% accuracy on answer-based questions and 72.43% on proof-based questions on the Geint benchmark
- Demonstrates notably strong performance on auxiliary line tasks: 68.63% (answer-based) and 68.40% (proof-based)
- Outperforms existing multimodal and math-specific models, particularly on problems requiring auxiliary constructions
- Ablation studies confirm effectiveness of verification reward model, reinforcement learning, and curriculum learning
- Case studies and error analysis identify remaining challenges in strict theorem application and symbolic tracking

## Why This Works (Mechanism)

### Mechanism 1: Verification Reward Model for Multi-Objective RL Fine-Tuning
The composite reward signal combining correctness, auxiliary construction accuracy, and format adherence enables targeted policy optimization beyond scalar outcome rewards. By computing R(x) = α·Fcorr(x) + β·Faux(x) + γ·Ffmt(x), the model receives explicit gradient pressure toward three distinct behavioral targets simultaneously, avoiding the collapse into a single pass/fail signal.

### Mechanism 2: Two-Stage Training (SFT → RL) Separates Syntax Acquisition from Policy Refinement
Supervised fine-tuning first establishes structural competence and formal language syntax through minimizing negative log-likelihood over reference solutions. Reinforcement learning then sharpens semantic correctness and auxiliary-line reasoning on rejection-sampled outputs, refining the policy without corrupting previously learned syntax.

### Mechanism 3: Curriculum Learning via Difficulty-Sorted Sampling Stabilizes RL Progression
Ordering training samples by ascending difficulty during RL improves sample efficiency and reduces training instability on complex auxiliary-construction tasks. Difficulty score d_i = 1 − (1/K)∑δ_{i,k} ranks problems by how often the model already solves them correctly, allowing the policy to consolidate simple patterns before tackling harder constructions.

## Foundational Learning

- **Lean4 Proof Assistant Basics:** Understanding typeclass constraints, definition syntax, and proof structure is required to debug model outputs. *Quick check:* Can you read a simple Lean4 geometry definition (e.g., `def A : Point := ⟨0, 0⟩`) and identify whether it declares a point, line, or angle?

- **Reinforcement Learning with GRPO:** Understanding policy gradients, reward normalization, and KL constraints helps diagnose training instability. *Quick check:* In GRPO, how does the group-relative advantage computation differ from vanilla REINFORCE, and why does it reduce variance?

- **Multimodal Vision-Language Model Architecture:** Understanding vision-language projectors and cross-modal alignment is necessary for architecture modifications. *Quick check:* What is the role of the multimodal projector, and what happens to visual representations if you freeze versus fine-tune it?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-7B (vision encoder frozen, LLM fine-tuned) → SFT (NLL loss, 4 epochs) → RL (GRPO with verification reward) → Curriculum learning by difficulty → DeepSeek-V3 evaluation

- **Critical path:**
  1. Verify Lean4 annotations in Geint dataset compile without errors (data quality gate)
  2. Run SFT to convergence on held-out validation set (target: stable proof structure, correct Lean4 syntax)
  3. Curate rejection-sampled RL dataset (K=8 candidates per problem, filter trivial/unsolvable)
  4. Tune α, β, γ on small held-out split before full RL run (reward balancing is sensitive)

- **Design tradeoffs:**
  - Freezing vision encoder preserves visual capacity but limits diagram-domain adaptation; if diagrams have systematic style differences from pre-training, consider partial unfreezing
  - Curriculum learning improves stability but adds compute overhead (scoring K candidates per problem); for large-scale runs, approximate difficulty with fewer samples
  - GPT-4o-based proof evaluation introduces dependency on closed model; for fully reproducible pipelines, replace with open-source judge or Lean4 compilation

- **Failure signatures:**
  - Reward hacking: Faux→1 but semantic reasoning incorrect (model generates syntactically valid but useless auxiliary lines). Mitigation: spot-check top-reward outputs weekly
  - Catastrophic forgetting: RL stage degrades SFT-learned syntax (malformed Lean4 code). Mitigation: add KL penalty or replay buffer of SFT samples
  - Curriculum collapse: Difficulty scores stuck near 0 or 1 (too easy/hard split). Mitigation: re-score difficulty periodically during RL

- **First 3 experiments:**
  1. SFT-only baseline: Train without RL stage, measure answer/proof accuracy and auxiliary-line success rate; establishes lower bound
  2. Reward ablation: Train three RL variants (no Faux, no Ffmt, all components) to quantify contribution of each reward term
  3. Curriculum vs. random ordering: Compare RL with curriculum learning vs. uniform sampling; measure sample efficiency and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be enhanced to prevent symbolic confusion and theorem misapplication during multi-step reasoning? The error analysis identifies "remaining challenges in strict theorem application and symbolic tracking," specifically citing cases where the model mislabels midpoints or arrives at mathematically impossible values (sin θ > 1) due to error propagation. The current verification reward model lacks a mechanism to verify the validity of intermediate symbolic states or theorem preconditions during training.

### Open Question 2
Why does correct auxiliary line construction fail to guarantee logical completeness in the subsequent proof steps? The authors highlight a case where the model "constructs the auxiliary line correctly but fails to establish the right proportional relationships," resulting in a correct visual formalization but an incomplete logical chain. This suggests a disconnect in the model's joint training, where the visual/construction module succeeds but fails to propagate necessary geometric constraints to the textual reasoning module.

### Open Question 3
To what extent does the generated Lean4 code successfully compile and verify against ground-truth theorems, independent of the LLM-based evaluation? While the paper proposes generating "formally verifiable" Lean4 solutions, the evaluation metrics rely on a DeepSeek-V3 LLM judge rather than actual Lean4 compiler execution. The paper does not report the exact compilation success rate of the generated code.

## Limitations

- **Reward Weight Sensitivity:** The effectiveness of the verification reward model depends on specific values of α, β, γ, and λ, which are not specified in the paper, making it difficult to reproduce exact performance gains.

- **Data Quality Dependency:** The framework's success hinges on the quality and completeness of the Geint dataset, particularly the accuracy of Lean4 annotations for auxiliary constructions. Systematic errors could propagate through both SFT and RL stages.

- **Scalability of Curriculum Learning:** The curriculum learning approach relies on accurate difficulty scoring from rejection sampling. With limited candidates per problem (K=8), the difficulty estimates may be noisy, potentially leading to mis-ranking and unstable learning.

## Confidence

- **High Confidence:** The two-stage training architecture (SFT → RL) is well-established and experimental results demonstrate significant improvements over baselines on the Geint benchmark.
- **Medium Confidence:** The specific formulation of the verification reward model and its hyperparameters are not fully specified, introducing uncertainty about the optimal configuration.
- **Low Confidence:** The claim that Geint-R1 achieves "human-level performance" on geometry problems is not directly supported by the experimental results.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Conduct systematic ablation study to determine optimal values of α, β, γ, and λ in verification reward model by varying each parameter independently and measuring impact on accuracy and auxiliary line success rate.

2. **Difficulty Score Robustness:** Evaluate robustness of curriculum learning by varying number of candidates per problem (K) used for difficulty scoring, comparing RL with curriculum learning to RL with uniform sampling across different K values.

3. **Human Evaluation on External Dataset:** Assess generalizability by evaluating performance on a separate, expert-curated dataset of geometry problems not included in the Geint benchmark, with human experts grading solutions on logical validity, completeness, correctness, auxiliary construction, and clarity.