---
ver: rpa2
title: Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in Multi-Armed
  Bandits using Stability-Penalty Matching
arxiv_id: '2502.08143'
source_url: https://arxiv.org/abs/2502.08143
tags:
- have
- lemma
- bounds
- inequality
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops new adaptive regret bounds for multi-armed
  bandits that simultaneously satisfy three key properties: being data-dependent (adaptive
  to problem structure like sparsity or total variation), best-of-both-worlds (BOBW
  - achieving optimal bounds for both stochastic and adversarial regimes), and having
  optimal $T$-dependency. The core method is real-time stability-penalty matching
  (SPM), an extension of the SPM technique for tuning learning rates in the follow-the-regularized-leader
  (FTRL) framework.'
---

# Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in Multi-Armed Bandits using Stability-Penalty Matching

## Quick Facts
- **arXiv ID:** 2502.08143
- **Source URL:** https://arxiv.org/abs/2502.08143
- **Reference count:** 40
- **Primary result:** First MAB algorithm achieving data-dependent adaptivity (sparsity $S$, variation $Q$), best-of-both-worlds (BOBW) guarantees, and optimal $T$-dependency simultaneously

## Executive Summary
This paper develops new adaptive regret bounds for multi-armed bandits that simultaneously satisfy three key properties: being data-dependent (adaptive to problem structure like sparsity or total variation), best-of-both-worlds (BOBW - achieving optimal bounds for both stochastic and adversarial regimes), and having optimal $T$-dependency. The core method is real-time stability-penalty matching (SPM), an extension of the SPM technique for tuning learning rates in the follow-the-regularized-leader (FTRL) framework. Unlike previous SPM methods that require taking expectations over all possible arms, real-time SPM uses observed losses directly, enabling tighter adaptivity. The paper establishes several main results: for bandits with sparse losses, it achieves a BOBW bound of $O(\sqrt{ST \ln K})$ in the adversarial regime and $O(S \ln T \ln K/\Delta_{\min})$ in the stochastic regime, improving upon the best known bounds; for bandits with small total variation $Q$, it obtains $O(\sqrt{Q \ln K})$ in the adversarial regime without requiring knowledge of $Q$; and through a coordinate-wise extension, it achieves bounds of $O(\min(\sqrt{K \ln T \min(Q_{\infty},L^*,T-L^*),K^{1/4}\sqrt{KT}}))$ in the adversarial regime and $O(\sum_{i \neq i^*} \ln T/\Delta_i)$ in the stochastic regime. These results are the first to simultaneously achieve BOBW guarantees, data-dependent adaptivity, and optimal $T$-dependency in multi-armed bandits.

## Method Summary
The paper presents a Follow-the-Regularized-Leader (FTRL) algorithm with real-time Stability-Penalty Matching (SPM) to achieve data-dependent BOBW guarantees. The method uses a hybrid regularizer combining $\alpha$-Tsallis Entropy and Log-Barrier terms, and dynamically tunes the learning rate through explicit stability-penalty matching formulas. The key innovation is using observed losses directly (rather than expectations) to compute the stability penalties, which enables tighter adaptivity to the actual data structure. The algorithm maintains cumulative loss estimates, solves a constrained convex optimization problem to find the next distribution over arms, and mixes with uniform exploration to ensure sufficient exploration.

## Key Results
- Achieves $O(\sqrt{ST \ln K})$ adversarial regret and $O(S \ln T \ln K/\Delta_{\min})$ stochastic regret for sparse loss sequences
- Obtains $O(\sqrt{Q \ln K})$ adversarial regret for small total variation $Q$ without requiring knowledge of $Q$
- Coordinate-wise extension achieves $O(\min(\sqrt{K \ln T \min(Q_{\infty},L^*,T-L^*),K^{1/4}\sqrt{KT}}))$ adversarial regret and $O(\sum_{i \neq i^*} \ln T/\Delta_i)$ stochastic regret
- First algorithm to simultaneously achieve BOBW guarantees, data-dependent adaptivity, and optimal $T$-dependency in MAB

## Why This Works (Mechanism)
The real-time SPM mechanism works by dynamically tuning the learning rate based on observed stability of the algorithm's predictions. Instead of computing expected stability over all possible arms (which would be intractable), it uses the actual observed losses to compute stability penalties. This allows the algorithm to adapt its learning rate in real-time to the observed data structure - using smaller learning rates when the sequence is stable and larger ones when it's changing rapidly. The hybrid regularizer combining Tsallis Entropy and Log-Barrier ensures both sparsity and numerical stability.

## Foundational Learning

**Follow-the-Regularized-Leader (FTRL)**: An online learning framework that minimizes cumulative loss plus a regularizer at each step. Why needed: Provides the basic algorithmic structure for the bandit algorithm. Quick check: Verify understanding of the FTRL update rule and its relationship to online mirror descent.

**Stability-Penalty Matching**: A technique for tuning learning rates based on the stability of the algorithm's predictions. Why needed: Enables adaptive learning rates without prior knowledge of problem parameters. Quick check: Understand how stability penalties relate to regret bounds.

**Tsallis Entropy Regularizer**: A non-logarithmic regularizer that encourages sparsity. Why needed: Promotes sparse distributions over arms, which is crucial for the sparsity-dependent bounds. Quick check: Compare Tsallis Entropy to standard entropy and understand the sparsity effect.

**Log-Barrier Regularizer**: A regularizer that prevents probabilities from approaching zero. Why needed: Ensures numerical stability when computing logarithms of probabilities. Quick check: Understand why the log-barrier term is necessary and how it prevents numerical issues.

**Total Variation**: A measure of how much a sequence changes over time. Why needed: Captures non-stationarity in the loss sequence, enabling adaptation to slowly changing environments. Quick check: Compute total variation for sample sequences and understand its relationship to regret.

## Architecture Onboarding

**Component Map**: FTRL Algorithm -> Stability-Penalty Matching -> Hybrid Regularizer (Tsallis + Log-Barrier) -> Constrained Convex Optimization -> Arm Selection

**Critical Path**: The most computationally intensive step is solving the constrained convex optimization problem for q_t at each round. This requires numerical optimization and could be a bottleneck for large K or T.

**Design Tradeoffs**: The hybrid regularizer balances sparsity (from Tsallis) with numerical stability (from Log-Barrier). The real-time SPM trades off between adaptivity and computational complexity compared to fixed learning rate approaches.

**Failure Signatures**: Numerical instability when probabilities approach zero, slow runtime due to convex optimization, and poor performance if the learning rate tuning doesn't match the actual problem structure.

**First Experiments**:
1. Implement the basic FTRL algorithm with a fixed learning rate (no SPM) and verify it achieves standard O(√T) bounds
2. Add the hybrid regularizer and test on sparse loss sequences to verify O(√ST) scaling
3. Implement the full real-time SPM and compare its performance against the fixed-rate version on both stochastic and adversarial problems

## Open Questions the Paper Calls Out

**Open Question 1**: Are the regret bounds provided in Theorem 3 tight under the hard sparsity constraint ‖ℓ_t‖_0 ≤ S, specifically regarding the dependencies on S and K? [explicit] Section 3.2 states, "It remains an open question whether the BOBW bounds in Theorem 3 are tight under the hard constraint ‖ℓ_t‖_0 ≤ S." The authors provide near-matching lower bounds for a "soft" sparsity constraint (where sparsity holds in expectation), but they do not provide a lower bound for the strict hard constraint setting where ‖ℓ_t‖_0 ≤ S always holds.

**Open Question 2**: Is the extra factor of √K^α in the adversarial regret bound of the coordinate-wise SPM (CoWSPM) algorithm a fundamental limitation of the method, or is it an artifact of the analysis? [explicit] Remark 10 in Section 5 notes: "It is unclear to us whether this extra factor is a fundamental limitation of CoWSPM or an artifact of our analysis." The CoWSPM adversarial bound is O(K^(α/2)√(KT)), which is strictly worse than the optimal O(√(KT)) dependency on K when α > 0.

**Open Question 3**: Can the real-time Stability-Penalty Matching (SPM) framework be extended to derive T-optimal, data-dependent, best-of-both-worlds guarantees for contextual linear bandits? [explicit] Section 6 (Conclusion) explicitly lists "applying real-time SPM on other bandits problems, such as contextual linear bandits" as a direction for future work. While the paper establishes the method for standard multi-armed bandits, it is untested whether the stability-penalty matching mechanism remains effective when combined with the linear structure and context-dependent losses of linear bandits.

**Open Question 4**: Can real-time SPM be adapted to achieve optimal bounds based on ℓ_1 and ℓ_2-norm path-length quantities? [explicit] Section 6 suggests future work includes "making real-time SPM adaptive towards other challenging data-dependent quantities like ℓ_1 and ℓ_2-norm path-length bounds." The current paper focuses on adaptivity to sparsity and total variation. Adapting to path-length (non-stationarity) requires different stability constraints that the current analysis does not cover.

## Limitations

- Lack of practical implementation details for the real-time SPM algorithm, particularly the numerical optimization solver
- No empirical validation showing performance on concrete problem instances or comparison with simpler baseline methods
- Numerical stability challenges when probabilities approach zero due to the Log-Barrier term

## Confidence

- **High confidence** in the theoretical validity of the bounds and the general algorithmic framework
- **Medium confidence** in the practical applicability of the algorithm
- **Low confidence** in the practical implementation details

## Next Checks

1. **Numerical Stability Test**: Implement a simplified version of the algorithm (e.g., with a fixed learning rate instead of SPM) and verify that the probability updates and loss estimates remain stable over many rounds without NaN or overflow issues.

2. **Solver Performance Benchmark**: Compare the runtime of the algorithm using different convex optimization approaches (generic solver vs. custom projected gradient descent) for the FTRL update to assess practical feasibility.

3. **Empirical Bound Verification**: Run simulations in both stochastic and adversarial regimes and plot the empirical regret against the theoretical bounds (e.g., √T for adversarial, ln T for stochastic) to confirm the algorithm achieves the promised scaling.