---
ver: rpa2
title: Training chord recognition models on artificially generated audio
arxiv_id: '2508.05878'
source_url: https://arxiv.org/abs/2508.05878
tags:
- chord
- winterreise
- dataset
- music
- billboard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the utility of artificially generated audio\
  \ data for training chord recognition models by comparing two Transformer-based\
  \ architectures\u2014Bidirectional Transformer for Chord Recognition (BTC) and Harmony\
  \ Transformer (HT)\u2014trained on various combinations of Artificial Audio Multitracks\
  \ (AAM), Schubert\u2019s Winterreise Dataset, and the McGill Billboard Dataset.\
  \ The models were evaluated using Root, MajMin, and Chord Content Metric (CCM)."
---

# Training chord recognition models on artificially generated audio

## Quick Facts
- arXiv ID: 2508.05878
- Source URL: https://arxiv.org/abs/2508.05878
- Reference count: 40
- Models trained on artificial audio achieve comparable performance to those trained on human-composed datasets for pop music chord recognition.

## Executive Summary
This study investigates the utility of artificially generated audio data for training chord recognition models. The researchers compared two Transformer-based architectures—Bidirectional Transformer for Chord Recognition (BTC) and Harmony Transformer (HT)—trained on combinations of Artificial Audio Multitracks (AAM), Schubert's Winterreise Dataset, and the McGill Billboard Dataset. The evaluation used Root, MajMin, and Chord Content Metric (CCM). Results demonstrated that models trained on AAM achieved the highest performance on the AAM dataset, while models trained on Billboard performed best on the Billboard dataset. The study found that AAM can enrich smaller training datasets of human-composed music and serve as a standalone training set for pop music chord recognition, despite structural differences between artificially generated and human-composed music.

## Method Summary
The researchers evaluated two Transformer architectures—BTC and HT—trained on various combinations of AAM, Winterreise, and Billboard datasets. BTC used raw audio converted to CQT features, while HT used pre-computed NNLS chroma features. Both models were trained using 6-fold cross-validation with pitch augmentation. BTC was trained from scratch and with fine-tuning of a pre-trained model, while HT was trained only from scratch. The study compared performance across different training set compositions and evaluated using Root, MajMin, and CCM metrics.

## Key Results
- Models trained on AAM achieved the highest performance on the AAM dataset, while models trained on Billboard performed best on the Billboard dataset
- Models trained on AAM achieved the highest average performance across all three datasets
- HT trained on AAM outperformed other HT models in all metrics except CCM on Billboard

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Artificially generated audio (AAM) can serve as a proxy for human-composed data in training chord recognition models, provided the synthetic data preserves the statistical distribution of chord progressions found in the target domain.
- **Mechanism:** **Distributional Similarity via Proxy.** The AAM dataset is generated algorithmically with "perfect alignment" to MIDI. Even though the audio is synthetic, the sequential patterns of chord progressions mimic the "musical grammar" of pop music. The Transformer learns these transition probabilities and harmonic structures from the clean synthetic data, which then generalize to the noisier, human-composed data in the Billboard dataset.
- **Core assumption:** The features learned from algorithmically composed progressions (syntax) transfer to human-composed progressions, despite differences in audio complexity/timbre.
- **Evidence anchors:**
  - [Abstract]: "AAM can enrich a smaller training dataset... or can even be used as a standalone training set for a model that predicts chord sequences in pop music."
  - [Section 5.2]: "Results of a model trained on AAM are comparable to those of the model trained on Winterreise... suggesting that AAM is a decent training set for a model that aims to predict chord sequences in pop music."
  - [Corpus]: Supported by "Exploring Procedural Data Generation..." (arXiv:2508.07987), which shows procedural audio generation successfully trains transcription models where real data is scarce.
- **Break condition:** Transfer fails when the target domain possesses significant structural complexity or vocabulary not present in the synthetic data.

### Mechanism 2
- **Claim:** Using Transformer architectures allows the model to leverage long-range temporal context, reducing errors that would occur if classifying frames in isolation.
- **Mechanism:** **Bidirectional Contextualization.** Both the Bidirectional Transformer (BTC) and Harmony Transformer (HT) utilize self-attention to weigh the importance of surrounding time frames. This allows the model to resolve ambiguity in a specific frame by observing the preceding and succeeding chord context.
- **Core assumption:** Chord recognition is fundamentally a sequence modeling problem, not just a frame-wise classification problem.
- **Evidence anchors:**
  - [Section 3.1]: "They both incorporate the information from the surrounding frames into the prediction process, so they predict a sequence of chords instead of a single chord."
  - [Section 2.1]: Mentions HT assigns segmentation to the encoder/recognition to decoder, and BTC uses attention maps to visualize context usage.
- **Break condition:** Performance degrades if the input sequence length is too short to capture the relevant harmonic context or if attention maps fail to converge due to noisy input features.

### Mechanism 3
- **Claim:** Pitch augmentation (transposition) effectively multiplies dataset size and forces the model to learn pitch-invariant harmonic features.
- **Mechanism:** **Transpositional Invariance.** By shifting the pitch of the audio and corresponding labels (e.g., -5 to +6 semitones), the model is exposed to the same relative harmonic relationships in different absolute keys. This prevents the model from overfitting to specific spectral frequencies of specific keys.
- **Core assumption:** The relationship between chord labels and audio features is relative; a I-IV-V progression should be recognized regardless of the root key.
- **Evidence anchors:**
  - [Section 3.4]: "Pitch augmentation ranging from -5 to +6 semitones was applied to all training data."
  - [Section 3.4]: "All training data are augmented by shifting the pitch... expanding the training set 12 times."
- **Break condition:** If the audio processing pipeline cannot handle extreme pitch shifts without introducing artifacts or losing frequency resolution, augmentation may introduce noise.

## Foundational Learning

- **Concept:** **Audio Feature Representations (Chroma vs. CQT)**
  - **Why needed here:** The two architectures use different inputs. BTC uses Constant-Q Transform (CQT) on raw audio, while HT uses pre-computed NNLS Chroma. Understanding that Chroma maps audio to the 12 pitch classes (folded frequency) while CQT preserves frequency resolution is critical for preprocessing.
  - **Quick check question:** How does the "bass chroma" in the HT input differ from the standard treble chroma, and why would this help distinguish a C major chord from a C/E chord?

- **Concept:** **Transformers in MIR (Sequence-to-Sequence)**
  - **Why needed here:** Unlike CNNs that look at local patterns, the Transformer's self-attention mechanism is used here to bridge time gaps. You must understand "positional encoding" and "attention heads" to debug why a model might miss a chord change.
  - **Quick check question:** In the context of this paper, does the BTC model process the entire song as one sequence or in fixed-length windows (e.g., 10 seconds)?

- **Concept:** **Chord Vocabularies (MajMin vs. Full)**
  - **Why needed here:** The paper collapses complex chords into a 25-class "MajMin" vocabulary. This simplification is a major design choice that trades granularity for classification accuracy.
  - **Quick check question:** If the ground truth is a "C:maj7" but the model predicts "C:maj" using the MajMin vocabulary, is this counted as a correct prediction or an error?

## Architecture Onboarding

- **Component map:** Raw Audio (.wav) -> Preprocessor (CQT for BTC / NNLS Chroma for HT) -> Augmentation (Pitch shifting) -> Encoder (Transformer Encoder layers) -> Decoder/Head (Linear projection to Vocabulary size -> Softmax)

- **Critical path:** The most fragile step is the **Preprocessing Alignment**. For HT, NNLS chroma extraction must be perfectly aligned with the audio. Different sampling rates (44.1kHz vs 22.05kHz) required different window sizes. If this alignment is off by even a few frames, the loss function penalizes the model for "wrong" predictions that are actually just time-shifted.

- **Design tradeoffs:**
  - **BTC vs. HT:** BTC takes raw audio (easier pipeline, learns features end-to-end) but requires more compute. HT uses hand-crafted Chroma (requires external tools like Sonic Visualizer/Vamp plugins) but converges faster and separates timbre from pitch effectively.
  - **Fine-tuning vs. Scratch:** The paper suggests fine-tuning a pre-trained BTC on Winterreise helped less than training from scratch, implying that the pre-trained weights (from pop) might bias the model away from classical structures.

- **Failure signatures:**
  - **High Variance across folds:** Indicates the dataset is too small or diverse.
  - **High Accuracy on AAM, Low on Billboard:** The model has overfitted to the "clean" structural simplicity of artificial audio and failed to learn robustness to the noise/complexity of human audio.
  - **Confusion Matrix Diagonal Offset:** If the model consistently predicts the "wrong" root (e.g., predicting F for C), it suggests a failure in pitch augmentation or global tuning estimation.

- **First 3 experiments:**
  1. **Sanity Check (BTC):** Train BTC from scratch on the AAM dataset (3,000 songs). Verify that the model overfits/trains successfully on the "easy" synthetic data to establish a performance upper bound (Expected: ~97% MajMin).
  2. **Cross-Domain Transfer (HT):** Train HT on Billboard (Real) and evaluate on AAM (Synthetic), then train on AAM and evaluate on Billboard. Measure the "transfer gap" to quantify how much structural information is shared.
  3. **Dataset Enrichment (HT):** Take the small Winterreise dataset (48 songs). Train two models: one on Winterreise only, one on Winterreise + AAM subset. Compare MajMin scores to verify if synthetic data acts as a regularizer/enrichment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does utilizing the complete Artificial Audio Multitracks (AAM) dataset for the Harmony Transformer (HT) significantly improve performance compared to the 192-song subset used in this study?
- Basis in paper: [explicit] Section 6 states that future work could involve automating the feature extraction process to "process the entire AAM dataset instead of a subset."
- Why unresolved: The current study was limited to a subset for HT due to the manual effort required for feature extraction.
- What evidence would resolve it: A comparison of evaluation metrics (Root, MajMin, CCM) for HT models trained on the full 3,000-track dataset versus the subset.

### Open Question 2
- Question: How does the utility of artificial audio change when models are trained and evaluated using a more complex chord vocabulary beyond the 25-class MajMin simplification?
- Basis in paper: [explicit] Section 6 suggests conducting "similar experiments with a larger vocabulary if an artificially generated dataset with richer annotations is released."
- Why unresolved: The current AAM dataset only contains MajMin annotations, forcing the simplification of richer ground truth data from the Billboard and Winterreise datasets.
- What evidence would resolve it: Performance benchmarks on a version of the AAM dataset that includes chord extensions (e.g., 7ths, diminished) compared against full-vocabulary human datasets.

### Open Question 3
- Question: Can the algorithmic composition of AAM be refined to better match the structural complexity and non-uniform chord distributions of human-composed music?
- Basis in paper: [inferred] The paper notes that AAM has a more uniform distribution and is "easiest to predict," suggesting a structural gap that may limit its effectiveness for transfer learning on complex human datasets.
- Why unresolved: While AAM helps when data is scarce, its "uniform" nature differs from the repetitive yet complex structures of pop and classical music.
- What evidence would resolve it: Experiments training models on AAM tracks generated with constraints to mimic Billboard chord progression frequencies, observing if this improves transfer performance.

## Limitations

- The study's evaluation is limited to pop music chord recognition, with classical music (Winterreise) showing different patterns that may not generalize to other genres.
- The dataset composition presents limitations: Billboard uses pre-extracted features rather than raw audio, and Winterreise is small (48 songs), making it difficult to draw definitive conclusions.
- The study does not address potential biases introduced by the algorithmic composition process used to generate AAM.

## Confidence

**High Confidence:** The claim that AAM can enrich smaller training datasets of human-composed music is well-supported by experimental results showing improved performance when AAM is added to Winterreise training data.

**Medium Confidence:** The assertion that AAM can serve as a standalone training set for pop music chord recognition is supported by results but may be overstated given the structural differences between synthetic and human audio.

**Medium Confidence:** The effectiveness of Transformer architectures for chord recognition is demonstrated, but the comparative advantage over other sequence models is not rigorously established.

**Low Confidence:** The generalizability of findings to non-pop music genres and more complex chord vocabularies remains untested.

## Next Checks

1. **Cross-Genre Validation:** Train and evaluate the same models on classical and jazz datasets to test whether artificially generated data provides similar benefits across musical styles with more complex harmonic structures.

2. **Ablation Study on Audio Quality:** Compare model performance when trained on high-quality human recordings versus artificially generated audio with varying levels of timbral complexity to quantify the impact of audio realism.

3. **Longitudinal Study on Transfer Learning:** Systematically vary the proportion of artificially generated data in training mixtures (0%, 25%, 50%, 75%, 100%) and track how this affects generalization to human-composed test sets across multiple genres.