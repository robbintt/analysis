---
ver: rpa2
title: Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed
  Neural Networks
arxiv_id: '2508.21571'
source_url: https://arxiv.org/abs/2508.21571
tags:
- have
- neural
- loss
- lemma
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes linear convergence of stochastic gradient
  descent and stochastic gradient flow for training two-layer physics-informed neural
  networks (PINNs) to solve partial differential equations. The main result shows
  that with high probability, these algorithms converge exponentially fast to a global
  minimum of the PINN loss function when the network is sufficiently wide and the
  step size is appropriately small.
---

# Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2508.21571
- Source URL: https://arxiv.org/abs/2508.21571
- Reference count: 40
- Primary result: Establishes linear convergence of SGD/SGF for training wide two-layer PINNs on linear PDEs

## Executive Summary
This paper provides a rigorous theoretical analysis of stochastic gradient descent and stochastic gradient flow convergence for training physics-informed neural networks (PINNs) with two hidden layers. The authors prove that when the network is sufficiently wide, these algorithms converge exponentially fast to global minima of the PINN loss function with high probability. The key innovation is extending deterministic convergence analysis to handle the dynamic randomness introduced by stochastic optimization, while carefully tracking the positive definiteness of Gram matrices throughout training.

## Method Summary
The paper analyzes two-layer PINNs with width m solving linear PDEs using stochastic gradient descent. The network architecture uses a semi-random parametrization where weights are initialized from normal distributions and coefficients from symmetric distributions. The loss function combines PDE residual and boundary condition errors, and convergence is established through a stopping time analysis that ensures the Gram matrices of gradients remain positive definite throughout training. The analysis requires locally Lipschitz three-times differentiable activation functions and proves linear convergence under specific step size constraints.

## Key Results
- SGD achieves linear convergence with rate E[L(t)] ≤ (1 - ηλθ/2)^t L(0) with high probability
- SGF achieves exponential convergence E[L(t)] ≤ exp(-λθt/2)L(0)
- Gram matrix positive definiteness is maintained throughout training in the lazy regime
- Width m must scale polynomially with the inverse of minimum Gram matrix eigenvalues

## Why This Works (Mechanism)
The convergence relies on maintaining positive definiteness of Gram matrices G_w and G_a throughout training. In the lazy training regime, parameters barely move from initialization, allowing the analysis to track the evolution of these matrices. The Polyak-Lojasiewicz property ensures convergence when gradient norms are bounded, while the semi-random parametrization guarantees the required eigenvalue lower bounds with high probability. The stopping time analysis carefully manages the interplay between parameter movement and Gram matrix properties.

## Foundational Learning
- **Positive Definite Gram Matrices**: Critical for establishing the PL condition that guarantees convergence. Without positive definiteness, the loss landscape may have spurious minima that trap optimization.
- **Lazy Training Regime**: Ensures parameters remain close to initialization, making Gram matrices approximately constant and enabling rigorous analysis. This regime requires very wide networks and small step sizes.
- **Semi-Random Parametrization**: Combines random weight initialization with data-dependent coefficient updates, providing the theoretical framework for analyzing wide network limits.

## Architecture Onboarding

- Component map:
  Input coordinates x ∈ ℝᵈ -> Two-layer network φ(x;w,a) = (1/√m)Σᵣ aᵣσ(wᵣᵀx̃) -> Loss function L(w,a) combining PDE residual and boundary errors -> Gram matrices G_w, G_a for convergence analysis

- Critical path:
  1. Define PDE and discretize with interior and boundary collocation points
  2. Initialize wide network with wᵣ(0) ~ N(0,I) and aᵣ(0) ~ Unif{-1,1}
  3. Compute gradients of loss components with respect to w and a
  4. Update parameters using mini-batch SGD with step size η
  5. Monitor exponential decay E[L(t)] ≤ (1-ηλ_θ/2)^t L(0)

- Design tradeoffs:
  - Width (m) vs. Speed: Larger m improves convergence probability and allows larger step sizes but increases computational cost
  - Step Size (η): Must be small enough for Gram matrix stability but large enough for meaningful progress
  - Activation Choice: Locally Lipschitz C³ requirement excludes ReLU but includes tanh/softplus

- Failure signatures:
  - Non-decreasing Loss: Gram matrices losing positive definiteness due to poor sampling or network under-parameterization
  - Instability: Step size too large, violating bounds needed for Gram matrix continuity
  - No Convergence on Nonlinear PDEs: Positive definiteness assumption fails for nonlinear operators

- First 3 experiments:
  1. Reproduce linear convergence on 1D Poisson problem with tanh activation, plotting log-loss vs iteration for varying widths
  2. Ablate network width below theoretical threshold, quantifying convergence rate degradation and success probability drop
  3. Compare activations (tanh, softplus vs ReLU) to validate C³ requirement and generality

## Open Questions the Paper Calls Out

- Can convergence analysis extend to nonlinear PDEs? The current analysis is restricted to linear cases because positive definiteness of Gram matrices may fail for nonlinear operators.

- What is the precise characterization of smallest Gram matrix eigenvalues? These eigenvalues are central to convergence but lack rigorous theoretical bounds in terms of network architecture and sampling geometry.

- Does linear convergence hold for deep PINNs? The two-layer analysis relies on specific structures that may not extend to multi-layer networks with complex inter-layer interactions.

## Limitations
- Restricted to linear PDEs, with no theoretical guarantees for nonlinear problems common in applications
- Requires extremely wide networks with polynomial scaling in 1/λ_min, potentially computationally prohibitive
- Step size constraints are extremely restrictive (η ≤ λ_θ/C_B^4 B^12), likely conservative for practice

## Confidence
- **High Confidence**: Linear convergence rate formula and mathematical derivation are rigorous
- **Medium Confidence**: Practical applicability given the extreme width and step size requirements
- **Low Confidence**: Extension to nonlinear PDEs and deep architectures beyond current theoretical scope

## Next Checks
1. Implement SGD on 1D/2D Poisson equation with varying widths m, verify exponential loss decay, and test practical necessity of theoretical width requirements
2. Systematically test ReLU/Leaky ReLU activations to quantify impact of C³ smoothness requirement versus theoretically allowed activations
3. Empirically test SGD on nonlinear PDE (Burgers' equation) to assess whether convergence breaks down as predicted by theory's limitations