---
ver: rpa2
title: Don't Think Twice! Over-Reasoning Impairs Confidence Calibration
arxiv_id: '2508.15050'
source_url: https://arxiv.org/abs/2508.15050
tags:
- confidence
- climate
- reasoning
- dataset
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates how reasoning capabilities
  and budgets affect confidence calibration in Large Language Models using the CLIMATE
  X dataset and a novel IARC carcinogenicity dataset. While recent reasoning models
  achieve 48.7% accuracy in assessing expert confidence, increasing reasoning budgets
  consistently impairs calibration, leading to systematic overconfidence that worsens
  with longer thinking budgets.
---

# Don't Think Twice! Over-Reasoning Impairs Confidence Calibration

## Quick Facts
- **arXiv ID:** 2508.15050
- **Source URL:** https://arxiv.org/abs/2508.15050
- **Reference count:** 17
- **Primary result:** Increasing reasoning budgets impairs confidence calibration in LLMs, while search-augmented generation dramatically improves it.

## Executive Summary
This study systematically evaluates how reasoning capabilities and budgets affect confidence calibration in Large Language Models using the CLIMATE X dataset and a novel IARC carcinogenicity dataset. While recent reasoning models achieve 48.7% accuracy in assessing expert confidence, increasing reasoning budgets consistently impairs calibration, leading to systematic overconfidence that worsens with longer thinking budgets. Search-augmented generation dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving relevant evidence. The results challenge the "test-time scaling" paradigm, suggesting that information access, rather than reasoning depth or inference budget, may be the critical bottleneck for improved confidence calibration in knowledge-intensive tasks.

## Method Summary
The study evaluates LLM confidence calibration across two datasets: CLIMATE X (climate science statements) and a novel IARC carcinogenicity dataset. Researchers systematically varied thinking budgets from 0 to 24,000 tokens and compared pure reasoning approaches against search-augmented generation. They measured accuracy and Cohen's kappa against expert ground truth, examining how different inference strategies affect the alignment between predicted confidence and actual correctness. The methodology isolates the impact of reasoning depth versus information retrieval on calibration quality.

## Key Results
- Reasoning models achieve only 48.7% accuracy in assessing expert confidence levels
- Increasing thinking budgets beyond 256 tokens causes accuracy to collapse to 35.7%
- Search-augmented generation achieves 89.3% accuracy by retrieving relevant evidence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extended reasoning budgets systematically degrade confidence calibration, causing models to become overconfident as they "think" longer.
- **Mechanism:** The paper suggests that longer chains of thought allow models to construct self-reinforcing justifications. Instead of converging on truth, the model generates spurious rationales or circular reasoning patterns that inflate internal confidence scores while reducing alignment with ground truth. This acts as a "justification trap" where the model persuades itself.
- **Core assumption:** The generated reasoning traces reflect a reinforcement of internal priors rather than an exploration of alternative hypotheses.
- **Evidence anchors:**
  - [abstract] "increasing reasoning budgets consistently impairs calibration, leading to systematic overconfidence that worsens with longer thinking budgets."
  - [section 5.2.1] "The model appears to over-reason, and longer chains of thought may introduce spurious rationales or circular reasoning patterns that hurt both accuracy and calibration."
  - [corpus] Neighbors like "Optimizing Chain-of-Thought Confidence" suggest active research in correcting CoT confidence, implying the raw signal is noisy.
- **Break condition:** If models are explicitly trained via RLVR (Reinforcement Learning from Verifiable Rewards) to penalize hallucination during reasoning, this degradation curve may shift, though the paper shows current reasoning models still suffer from it.

### Mechanism 2
- **Claim:** Information access via retrieval (RAG) resolves calibration failure more effectively than increasing inference compute.
- **Mechanism:** Confidence errors in knowledge-intensive tasks are primarily driven by the absence of specific evidence in the model's weights. Reasoning cannot synthesize missing information; it can only manipulate existing representations. Retrieval-Augmented Generation (RAG) bypasses this by injecting relevant evidence into the context, drastically reducing the "epistemic gap."
- **Core assumption:** The bottleneck is knowledge retrieval (finding the right fact), not logical synthesis (processing the fact).
- **Evidence anchors:**
  - [abstract] "search-augmented generation dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving relevant evidence."
  - [section 5.2.3] "LLM understanding of complex scientific claims is, to a surprising extent, bottlenecked by access to the right evidence rather than by the model's innate reasoning capacity."
  - [corpus] Weak direct corpus link for this specific RAG vs. Reasoning trade-off, but standard literature supports RAG for grounding.
- **Break condition:** This mechanism holds for knowledge-intensive domains (science, health) but may not hold for logic-heavy tasks (e.g., math coding) where knowledge is not the constraint.

### Mechanism 3
- **Claim:** Modest reasoning budgets improve accuracy, but returns diminish and invert rapidly beyond a "sweet spot."
- **Mechanism:** A minimal chain-of-thought (e.g., 64-192 tokens) activates the model's capacity to parse linguistic cues and structure. However, pushing beyond this threshold encourages the model to hallucinate constraints or over-analyze simple statements, leading to a collapse in alignment with human expert labels.
- **Core assumption:** The optimal compute budget is task-specific and generally lower than the "max" settings of current reasoning models.
- **Evidence anchors:**
  - [section 5.2.1] "A modest budget (64–192 tokens) lifts accuracy... Beyond 256 tokens, accuracy collapses, bottoming out near 35.7%."
  - [figure 2] Shows a non-monotonic curve where performance peaks early and then dips before partially recovering at extreme budgets.
- **Break condition:** If the task requires multi-step planning or synthesis of disparate concepts, the optimal budget window likely shifts to the right.

## Foundational Learning

- **Concept: Confidence Calibration**
  - **Why needed here:** This is the core metric of the paper. It measures whether a model's stated probability of being correct matches its actual accuracy. The paper demonstrates that reasoning models are poorly calibrated—they are "confidently wrong."
  - **Quick check question:** If a model claims 90% confidence on 100 answers but only gets 50 correct, is it well-calibrated?

- **Concept: Test-Time Scaling (Thinking Budget)**
  - **Why needed here:** This is the independent variable manipulated in the study. It refers to dynamically allocating more compute (tokens) at inference time to allow the model to "think" before answering. The paper challenges the paradigm that *more* scaling is always better.
  - **Quick check question:** What happens to the model's confidence if you force it to generate 1000 reasoning tokens instead of 10?

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - **Why needed here:** Implicit in the results. The paper deals with scientific statements where uncertainty is inherent (aleatoric) but the model fails to acknowledge its own lack of knowledge (epistemic). Over-reasoning inflates epistemic overconfidence.
  - **Quick check question:** Is the model unsure because the science is uncertain (aleatoric) or because it doesn't know the science (epistemic)?

## Architecture Onboarding

- **Component map:** Input Statement -> (Search Retrieval) -> [Reasoning Phase with Capped Budget] -> Confidence Prediction
- **Critical path:** Input Statement -> (Search Retrieval) -> [Reasoning Phase with Capped Budget] -> Confidence Prediction. *Note: The critical insight is that the Search Retrieval step is significantly more valuable than the Reasoning Phase for calibration.*
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Increasing the thinking budget improves accuracy slightly at first (diminishing returns) but increases latency and *decreases* calibration (safety).
  - **Compute vs. Retrieval:** Spending tokens on reasoning yields ~48% accuracy; spending compute on search yields ~89% accuracy. Investment in retrieval infrastructure outperforms investment in inference compute for these tasks.
- **Failure signatures:**
  - **The "Over-reasoning" Spiral:** As token generation increases, the model's confidence score decouples from accuracy (confidence goes up, accuracy goes down).
  - **Bulk Processing Drift:** Processing statements in bulk (long context) leads to lower calibration (45.3%) compared to itemized processing with retrieval.
- **First 3 experiments:**
  1. **Budget Sweep:** Run a specific reasoning model (e.g., Gemini 2.5 Flash) on a held-out test set with thinking budgets of [0, 64, 256, 1024, 4096]. Plot Accuracy vs. Budget to identify the local peak (hypothesis: peak is < 256 tokens).
  2. **Search vs. Reasoning Ablation:** Compare the best reasoning-only score against a search-augmented score (RAG) on the same dataset. Quantify the "Calibration Gap" (expected delta ~40% accuracy based on paper).
  3. **Domain Generalization:** Validate the "Over-reasoning" hypothesis on a new dataset (e.g., medical guidelines vs. climate) to see if the negative correlation between thinking budget and calibration holds across domains.

## Open Questions the Paper Calls Out
None

## Limitations
- Results focus on scientific/technical domains where factual accuracy is paramount; findings may not generalize to logic-heavy tasks like mathematical proofs
- Study evaluates only specific reasoning models (primarily Gemini 2.5 Flash), leaving open whether other architectures exhibit different calibration curves
- The mechanism of "justification traps" is inferred rather than directly observed, with alternative explanations possible

## Confidence

- **High Confidence:** Search-augmented generation (RAG) significantly outperforms pure reasoning for confidence calibration in knowledge-intensive tasks (89.3% vs 48.7% accuracy)
- **Medium Confidence:** Extended reasoning budgets systematically degrade calibration through self-reinforcing justifications, though the mechanism is inferred
- **Medium Confidence:** Optimal reasoning budget is task-specific and generally lower than current "max" settings, but specific thresholds may vary

## Next Checks

1. **Architecture Generalization Test:** Replicate the budget sweep experiment with multiple reasoning model architectures (e.g., OpenAI o-series, Claude 3.5 Sonnet, DeepSeek-R1) on the same datasets to determine whether the over-reasoning phenomenon is architecture-specific or universal across current reasoning models.

2. **Domain Transfer Validation:** Apply the same experimental protocol to non-scientific domains requiring multi-step reasoning (e.g., mathematical problem-solving, strategic game planning, or complex coding tasks) to test whether the negative correlation between thinking budget and calibration holds when the bottleneck shifts from knowledge access to logical synthesis.

3. **Training Paradigm Intervention:** Evaluate RLVR-trained reasoning models (where reasoning traces are optimized against verifiable rewards rather than human preference) to determine whether explicit reward shaping can mitigate the over-reasoning degradation observed in standard models, thereby testing the mechanism claim about "justification traps."