---
ver: rpa2
title: 'From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation
  in Optical Network Failure Analysis'
arxiv_id: '2509.00057'
source_url: https://arxiv.org/abs/2509.00057
tags:
- uni00000048
- class
- dataset
- uni0000004c
- uni00000051
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first systematic comparison of pre-, in-,
  and post-processing techniques for class imbalance mitigation in optical network
  failure management. It evaluates multiple methods across three datasets representing
  failure detection and identification scenarios, including Synthetic Minority Over-Sampling
  Technique (SMOTE), Random Under-Sampling (RUS), Generative AI methods like CTGAN
  and CVAE, ensemble learning, and post-processing approaches such as Threshold Adjustment.
---

# From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis

## Quick Facts
- arXiv ID: 2509.00057
- Source URL: https://arxiv.org/abs/2509.00057
- Reference count: 40
- Primary result: Post-processing (Threshold Adjustment) yields highest F1 gains for failure detection (up to 15.3%), while GenAI (CTGAN) provides best results for failure identification (up to 24.2%)

## Executive Summary
This study systematically compares pre-, in-, and post-processing techniques for addressing class imbalance in optical network failure analysis. Using three datasets representing failure detection and identification scenarios, the authors evaluate methods including SMOTE, RUS, CTGAN, CVAE, ensemble learning, and post-processing approaches like Threshold Adjustment. Results demonstrate that method effectiveness depends on dataset overlap (measured by Fisher Discriminant Ratio) and latency requirements, providing a practical framework for selecting appropriate mitigation strategies in optical networks.

## Method Summary
The study uses three experimental datasets: Dataset 1 (high overlap, FDR=0.769), Dataset 2 (low overlap, FDR=2.254), and Dataset 3 (multi-class, FDR=181.2) with features including BER and OSNR from Tx/Rx. Failure detection employs Random Forest classifiers while failure identification uses neural networks (2→20→10→6 neurons). Methods span pre-processing (SMOTE, RUS, CTGAN, CVAE), in-processing (Weighted Learning, Bagging/Boosting, Meta-Learning), and post-processing (Threshold Adjustment, Cost-sensitive Threshold). Performance is evaluated using F1-score across 100 iterations with inference time as secondary metric.

## Key Results
- Post-processing Threshold Adjustment achieves highest F1-score improvement for binary failure detection (up to 15.3%)
- Generative AI methods like CTGAN provide most significant gains for multi-class failure identification (up to 24.2%)
- Random Under-Sampling (RUS) provides fastest inference times while maintaining competitive F1-scores
- Method effectiveness depends critically on dataset overlap measured by Fisher Discriminant Ratio

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Threshold Adjustment achieves highest F1-score improvement in binary failure detection regardless of class overlap
- **Mechanism**: Iteratively scans probability thresholds (0 to 1) to find optimal $\tau^*$ that maximizes F1-score on validation data, shifting decision boundary to favor minority class without retraining
- **Core assumption**: Classifier outputs probabilities with sufficient rank-ordering for meaningful threshold separation
- **Evidence anchors**: Abstract reports 15.3% F1 improvement; Section 4.A confirms Threshold Adjustment as best-performing method
- **Break condition**: Fails if model outputs uniform probabilities for all instances

### Mechanism 2
- **Claim**: CTGAN provides significant performance gains in multi-class failure identification when class separability is high
- **Mechanism**: Learns probability distribution of minority classes to generate high-fidelity synthetic samples that augment training set effectively when classes are well-separated
- **Core assumption**: Feature space allows distinct clustering of failure types; heavy overlap degrades synthetic sample quality
- **Evidence anchors**: Abstract reports 24.2% gains; Section 4.B links performance to high FDR (181.2) in Dataset 3
- **Break condition**: Struggles with high overlap datasets where synthetic samples blur decision boundaries

### Mechanism 3
- **Claim**: Random Under-Sampling (RUS) provides fastest inference times while maintaining competitive F1-scores
- **Mechanism**: Randomly discards majority class samples, reducing training dataset size and computational complexity for predictions
- **Core assumption**: Majority class distribution is redundant or noisy, so discarding samples doesn't remove critical boundary information
- **Evidence anchors**: Abstract identifies RUS as fastest inference method; Section 4.A notes reduced dataset size drives performance
- **Break condition**: Complex or multi-modal majority class distribution leads to accuracy degradation

## Foundational Learning

- **Concept: Fisher Discriminant Ratio (FDR)**
  - **Why needed here**: Quantitative proxy for "class overlap" - high FDR implies classes are distinct (favoring GenAI), low FDR implies overlap (favoring Threshold Adjustment)
  - **Quick check question**: If your dataset has average FDR of 0.7, should you prioritize CTGAN or Threshold Adjustment based on this study?

- **Concept: F1-Score vs. Accuracy**
  - **Why needed here**: Optical network data is severely imbalanced (normal >> failure), making accuracy misleading - F1-score balances Precision and Recall for minority class
  - **Quick check question**: Why does the paper report F1-score improvements rather than accuracy improvements for Dataset 1?

- **Concept: Post-processing (Threshold Adjustment)**
  - **Why needed here**: "Prediction-centric" approach that decouples mitigation strategy from training pipeline, allowing retrofitting onto pre-trained models
  - **Quick check question**: Does Threshold Adjustment require retraining Random Forest if operational definition of failure changes?

## Architecture Onboarding

- **Component map**: Input (Optical network metrics) → Analysis (Calculate FDR, measure latency) → Selection Logic (Binary: RUS/Low overlap, Threshold Adjustment/High performance; Multi-class: SMOTE/High overlap, CTGAN/CVAE/Low overlap) → Model (Random Forest/Detection, Neural Network/Identification) → Output (Failure label/type)

- **Critical path**: Calculating Fisher Discriminant Ratio (FDR) - this metric drives mitigation strategy selection; skipping this may apply GenAI to overlapping datasets and fail to improve performance

- **Design tradeoffs**:
  - Inference Latency vs. Performance: RUS minimizes latency but may lose information; Threshold Adjustment maximizes F1-score but adds computational overhead
  - Data Quality vs. Method Complexity: GenAI requires high separability (clean data) to work; simple methods are robust to overlap but offer minimal gains

- **Failure signatures**:
  - GenAI Collapse: Generating synthetic samples that overlap with majority class due to low FDR (Dataset 1 behavior)
  - Bagging Failure: In multi-class NNs, Bagging failed due to under-sampling creating suboptimal learners with insufficient data
  - Calibration Failure: Probability Calibration showed minimal impact because SoftMax outputs were already reasonably calibrated

- **First 3 experiments**:
  1. Establish Baseline: Train Random Forest on Dataset 1 (High Overlap) without mitigation to measure baseline F1
  2. Validate Primary Mechanism: Apply Threshold Adjustment to Dataset 1 baseline model to verify 15.3% F1 improvement
  3. Verify Constraint: Train CTGAN on Dataset 3 (High FDR) and Dataset 1 (Low FDR) to confirm GenAI gains are conditional on class separability

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the proposed framework maintain performance rankings when applied to real-world operational optical network datasets?
  - **Basis**: Conclusion anticipates future work validating findings on real-world datasets
  - **Why unresolved**: Study relies on lab-generated datasets that may not capture live network complexity
  - **What evidence would resolve it**: Comparative study using operator data demonstrating Threshold Adjustment and CTGAN remain optimal

- **Open Question 2**: How does effectiveness of mitigation techniques change when applied to failure localization tasks?
  - **Basis**: Conclusion proposes extending analysis to localization tasks
  - **Why unresolved**: Localization is often regression or multi-label problem, potentially rendering specific techniques inapplicable
  - **What evidence would resolve it**: Adapting framework for localization metrics and evaluating GenAI/Threshold Adjustment effectiveness

- **Open Question 3**: Is superiority of post-processing (Threshold Adjustment) for failure detection dependent on Random Forest classifier?
  - **Basis**: Paper exclusively uses RF for detection without testing if optimal strategy shifts with different algorithms
  - **Why unresolved**: Different algorithms output probability estimates with varying calibration; Threshold Adjustment might be less effective on poorly calibrated models
  - **What evidence would resolve it**: Re-evaluating detection framework using alternative base learners (SVM, XGBoost)

## Limitations
- Does not specify exact train/test split ratios or cross-validation strategies, limiting reproducibility
- GenAI hyperparameters (CTGAN/CVAE) are not detailed, particularly for sample generation quality and quantity
- FDR threshold of 1.0 as cutoff between "low" and "high" overlap appears arbitrary without statistical justification
- Cost values for cost-sensitive methods in multi-class scenarios are unspecified

## Confidence
- **High**: Threshold Adjustment effectiveness in binary detection (supported by both abstract and corpus)
- **Medium**: CTGAN superiority in multi-class identification (relies heavily on single dataset with extreme FDR)
- **Medium**: RUS latency-performance tradeoff (inference time comparison is clear, but accuracy impact on complex datasets is untested)

## Next Checks
1. Replicate FDR calculation pipeline on new imbalanced dataset to verify 1.0 threshold as reliable decision boundary
2. Implement CTGAN with varying FDR levels (0.5, 1.0, 2.0, 10.0) to confirm synthetic sample quality degrades with overlap
3. Compare Threshold Adjustment against calibrated probability threshold (Platt scaling) to isolate contribution of threshold optimization vs. calibration