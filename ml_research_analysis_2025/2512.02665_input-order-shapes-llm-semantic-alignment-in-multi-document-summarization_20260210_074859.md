---
ver: rpa2
title: Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization
arxiv_id: '2512.02665'
source_url: https://arxiv.org/abs/2512.02665
tags:
- position
- input
- neutral
- summaries
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large language models exhibit structural
  biases when generating multi-document summaries, specifically whether the order
  of input articles affects their representational weight. Using 40 triplets of pro-neutral-con
  abortion-related news articles, the authors permuted each triplet into six input
  orders and prompted Gemini 2.5 Flash to generate neutral overviews.
---

# Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization

## Quick Facts
- **arXiv ID:** 2512.02665
- **Source URL:** https://arxiv.org/abs/2512.02665
- **Reference count:** 5
- **Primary result:** Position 1 consistently yielded higher BERTScore semantic similarity than Positions 2 and 3, indicating primacy effects in LLM multi-document summarization

## Executive Summary
This study investigates whether large language models exhibit structural biases when generating multi-document summaries, specifically whether input article order affects their representational weight. Using 40 triplets of pro-neutral-con abortion-related news articles, the author permuted each triplet into six input orders and prompted Gemini 2.5 Flash to generate neutral overviews. The findings reveal a significant primacy effect where summaries are more semantically aligned with the first-seen article, as detected by BERTScore but not by lexical overlap or factual consistency metrics. This suggests that early input disproportionately shapes the semantic framing of generated summaries, posing risks for applications relying on unbiased information synthesis.

## Method Summary
The study used 40 triplets of abortion-related news articles (pro/neutral/con stance), each article 300–1600 words, sourced from 6 outlets. Each triplet was permuted into 6 input orders and processed by Gemini 2.5 Flash with a neutral-summarization prompt. Summaries were evaluated using ROUGE-L (lexical overlap), BERTScore with RoBERTa-large (semantic similarity), and SummaC-Conv (factual consistency). One-way ANOVA with FDR-BH correction tested position effects, with Holm-corrected post-hoc pairwise t-tests where significant.

## Key Results
- Position 1 summaries showed significantly higher BERTScore semantic similarity than Positions 2 and 3 across all stances
- Positions 2 and 3 did not differ significantly from each other, suggesting a binary primacy effect
- ROUGE-L and SummaC metrics showed no significant positional differences
- Primacy effect was consistent across pro, neutral, and con stance conditions

## Why This Works (Mechanism)

### Mechanism 1: Semantic Framing Anchoring
The first document establishes a semantic and rhetorical template that constrains how subsequent documents are integrated into the summary. Early input tokens disproportionately shape the model's internal representation of the task, causing vocabulary and argumentative structures from Position 1 to be preferentially reproduced in the output.

### Mechanism 2: Position-Weighted Attention Distribution
LLMs allocate attention weight non-uniformly across input positions, with Position 1 receiving higher effective attention than subsequent positions. The transformer architecture's attention mechanism, combined with training data distributions, creates a systematic bias toward early-sequence tokens that propagates to generation.

### Mechanism 3: Binary First-Rest Salience
The primacy effect manifests as a categorical distinction between Position 1 and all others, not gradual positional decay. The model applies a saliency boost to the first document that is not applied to any subsequent document, regardless of relative ordering.

## Foundational Learning

- **Concept:** Primacy vs. Recency vs. Lost-in-the-Middle Effects
  - Why needed here: The paper's finding of Position 1 advantage differs from U-shaped curves reported in other long-context tasks; distinguishing these is essential for correct interpretation.
  - Quick check question: In a 7-document input, if accuracy peaks at positions 1 and 7 but drops at position 4, which phenomenon is this?

- **Concept:** Semantic vs. Lexical vs. Factual Evaluation Metrics
  - Why needed here: Only BERTScore detected the effect; understanding why ROUGE-L and SummaC did not is critical for designing future experiments.
  - Quick check question: Why would BERTScore detect semantic alignment with Position 1 while SummaC shows no positional difference?

- **Concept:** Reference-Free vs. Reference-Based Evaluation
  - Why needed here: The study compares summaries directly to source articles rather than human references, which affects interpretation of low absolute scores.
  - Quick check question: What are the tradeoffs of evaluating summaries against source documents versus human-written reference summaries?

## Architecture Onboarding

- **Component map:** Article triplets (PRO/NEUTRAL/CON) with length-controlled matching -> Permutation Engine (6 orders) -> Gemini 2.5 Flash with neutral-summarization prompt -> ROUGE-L/BERTScore/SummaC evaluation -> One-way ANOVA with FDR-BH correction + Holm-corrected post-hoc t-tests

- **Critical path:** Triplet construction with stance annotation and length matching -> Full factorial permutation (6 orders × 40 triplets = 240 summaries) -> Metric computation comparing each summary to all three source articles -> Position-based aggregation and ANOVA testing across stance conditions

- **Design tradeoffs:** Topic specificity (abortion) provides high control but limits generalizability claims; single model (Gemini 2.5 Flash) enables deep analysis but confounds findings with model-specific behavior; reference-free evaluation avoids human annotation cost but yields lower absolute scores and may miss nuances

- **Failure signatures:** Low absolute metric scores (ROUGE-L ~0.08, BERTScore ~0.02-0.05) are expected due to 220-word summaries vs. 300-1600-word sources; SummaC insensitivity to position is expected because entailment metrics penalize abstraction without capturing framing shifts; non-significant ROUGE-L results suggest the effect is semantic rather than surface-lexical

- **First 3 experiments:** 1) Replicate with additional controversial topics (gun control, immigration) to test domain generalization; 2) Compare across LLM families (GPT-4, Claude, Llama) to isolate architecture-specific vs. general effects; 3) Test mitigation strategies: (a) randomize input order and aggregate multiple summaries; (b) add explicit position-weighting instructions to the prompt

## Open Questions the Paper Calls Out

- **Open Question 1:** Do primacy effects in multi-document summarization generalize across other politically and socially controversial topics such as gun control, LGBTQ+ rights, and immigration? The paper calls for extending the dataset beyond abortion to assess whether observed order effects generalize across issue areas.

- **Open Question 2:** Do primacy effects persist across different LLM architectures (e.g., GPT, Claude, LLaMA) beyond Gemini 2.5 Flash? The paper suggests evaluating multiple LLM families to examine whether primacy effects persist across different model architectures.

- **Open Question 3:** How does document length interact with positional effects in multi-document summarization evaluation? The paper notes that length may influence metric values and suggests categorizing the dataset by length or conducting length-controlled analyses.

## Limitations
- Single-topic focus (abortion) provides strong experimental control but limits generalizability across domains
- Testing only one model family (Gemini 2.5 Flash) makes it unclear whether findings reflect architecture-specific behavior versus general LLM properties
- Reference-free evaluation approach yields low absolute metric scores and may not capture all relevant aspects of summary quality

## Confidence

**High Confidence:** The statistical finding of significant primacy effects in BERTScore across all stance conditions is robust, with clear ANOVA results and post-hoc significance patterns.

**Medium Confidence:** The claim that this effect represents a semantic rather than lexical bias is supported by the differential sensitivity of evaluation metrics.

**Low Confidence:** The binary vs. gradient distinction for position effects is speculative, with insufficient evidence to determine whether this pattern would hold for longer document sequences.

## Next Checks
1. **Cross-Domain Replication:** Replicate the study with at least two additional controversial topics (e.g., gun control, immigration) using the same methodology to test whether primacy effects persist across domains while controlling for topic-specific framing effects.

2. **Model Family Comparison:** Conduct parallel experiments across multiple LLM families (GPT-4, Claude, Llama) to distinguish between architecture-specific versus general LLM behavioral patterns, particularly focusing on whether BERTScore remains the most sensitive metric across models.

3. **Mitigation Strategy Testing:** Systematically test prompt-based interventions including (a) explicit position-weighting instructions, (b) random order generation with aggregation, and (c) structured input formatting to quantify the magnitude of primacy bias reduction possible through prompt engineering.