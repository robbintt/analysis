---
ver: rpa2
title: 'Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image
  Diffusion Models'
arxiv_id: '2503.09669'
source_url: https://arxiv.org/abs/2503.09669
tags:
- logo
- images
- image
- dataset
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Silent Branding Attack introduces a novel data poisoning method
  that manipulates text-to-image diffusion models to embed specific brand logos without
  text triggers. The attack exploits the model's tendency to memorize repeated visual
  patterns, using an automated pipeline that personalizes logos, generates natural
  insertion masks, and seamlessly blends logos into images.
---

# Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models

## Quick Facts
- arXiv ID: 2503.09669
- Source URL: https://arxiv.org/abs/2503.09669
- Authors: Sangwon Jang; June Suk Choi; Jaehyeong Jo; Kimin Lee; Sung Ju Hwang
- Reference count: 40
- Primary result: Novel data poisoning method embeds brand logos in diffusion model outputs without text triggers, achieving up to 45% logo inclusion rate with 100% poisoning ratio while maintaining image quality and stealth

## Executive Summary
Silent Branding Attack introduces a trigger-free data poisoning method that manipulates text-to-image diffusion models to embed specific brand logos in generated images without requiring text mentions. The attack exploits the model's tendency to memorize repeated visual patterns from training data, using an automated pipeline that personalizes logos, generates natural insertion masks, and seamlessly blends logos into images. Experiments across large-scale and style personalization datasets demonstrate high success rates in logo inclusion while maintaining image quality and text alignment. Human and automated evaluations confirm the stealthiness of the attack, as poisoned images remain difficult to detect, highlighting a new vulnerability in diffusion models that requires safeguards against unwanted branding in generated content.

## Method Summary
The attack uses a three-stage pipeline: (1) Logo personalization via DreamBooth fine-tuning with LoRA on U-Net, (2) Mask generation through iterative SDEdit with InstantStyle adapter and logo detection using OWLv2 + DINOv2 similarity, and (3) Inpainting via blended latent diffusion with zoom-in refinement. The method fine-tunes diffusion models on poisoned datasets containing subtly inserted logos, achieving high logo inclusion rates while maintaining task performance and image quality. The approach scales with poisoning ratio, showing 45% maximum success at 100% ratio while preserving CLIP scores and human naturalness evaluations.

## Key Results
- Logo Inclusion Rate (LIR) reaches 45% at 100% poisoning ratio on Midjourney dataset, demonstrating effective pattern memorization
- Human detection rate remains below 1% for poisoned images, confirming high stealthiness against human evaluators
- Secondary poisoning propagation shows LIR persistence across generations, with primary model LIR correlating with secondary model effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeated visual patterns in training data cause diffusion models to reproduce those patterns during inference, even without textual specification
- Mechanism: The diffusion training objective minimizes prediction error across noisy samples. When a specific visual element appears across multiple training images in varied contexts, the model's denoising network learns to predict and reconstruct that element as part of its generative prior
- Core assumption: The memorization effect scales with pattern frequency and consistency across training samples
- Evidence anchors: Section 5 demonstrates fine-tuning SDXL with toy images appearing in various locations/styles with no text mention; toy appears in all generated outputs with prompts like "a cozy campfire scene"

### Mechanism 2
- Claim: Style-aligned inpainting with iterative refinement enables stealthy logo insertion that preserves image-text alignment metrics
- Mechanism: The algorithm uses InstantStyle adapters to match logo appearance to target image aesthetics, blended latent diffusion for localized editing, and iterative SDEdit with small noise to gradually introduce logos while maintaining layout
- Core assumption: Human and automated evaluators prioritize global semantic consistency over subtle local anomalies
- Evidence anchors: Section 7.1 reports PSNR 24.81, LPIPS 0.095, CLIP-image 0.970 for poisoned vs. original images on Midjourney dataset—high similarity scores

### Mechanism 3
- Claim: Poisoned models maintain task performance while embedding logos, enabling stealthy deployment
- Mechanism: Fine-tuning on poisoned data minimizes L_LDM loss across both benign and manipulated samples. The logo appears in a subset of training contexts, allowing the model to learn the pattern as a generic visual element without disrupting learned associations between text prompts and image semantics
- Core assumption: Logo frequency in poisoned set is sufficient to establish memorization but not so high as to cause catastrophic forgetting or mode collapse
- Evidence anchors: Section 7.5 shows CLIP-score 0.313 (poisoned) vs. 0.314 (original) on Midjourney; style similarity 0.872 vs. 0.880 on Tarot—negligible degradation

## Foundational Learning

- **Latent Diffusion Models (LDMs)**: Why needed here—the attack targets LDMs specifically (SDXL, Stable Diffusion, FLUX). Understanding that these models operate in compressed latent space via VAE encoder E and denoising network ε_θ is essential for grasping why pattern memorization occurs and how fine-tuning modifies behavior. Quick check: Can you explain how Equation 1 (L_LDM loss) relates to the denoising process, and why this objective enables memorization of repeated visual patterns?

- **DreamBooth Personalization**: Why needed here—the attack uses DreamBooth for logo personalization, adapting models to generate custom logos. Understanding the balance between personalization loss (learning the logo) and prior preservation loss (maintaining class knowledge) clarifies why the attack preserves image quality. Quick check: How does removing the prior preservation loss term (as done in Section 6.1) affect the model's ability to generate logos versus maintaining general image quality?

- **Blended Latent Diffusion & SDEdit**: Why needed here—these enable mask-based local editing without full retraining. SDEdit adds noise then denoises to introduce changes while preserving global structure—critical for stealthy logo insertion. Quick check: How does noise strength in SDEdit control the trade-off between editing strength and preserving original image content?

## Architecture Onboarding

- **Component map**:
  Input: Original image + target logo reference
  ↓
  [Stage 1] Logo Personalization: DreamBooth fine-tuning on logo dataset (creates model ε_θ')
  ↓
  [Stage 2] Mask Generation: Style-aligned editing (InstantStyle adapter) → Iterative SDEdit (3 iterations, noise=0.3) → Logo detection (OWLv2 "logo" query + DINOv2 similarity to reference, τ=0.4-0.5) → Output: Binary mask for logo region
  ↓
  [Stage 3] Inpainting & Refinement: Paste detected logo onto original (optional initialization) → Iterative inpainting with mask (blended latent diffusion) → Zoom-in refinement (noise=0.25, 2 iterations) for logo detail
  ↓
  Output: Poisoned image x'_i with embedded logo L

- **Critical path**:
  1. Logo personalization quality: If DreamBooth doesn't sufficiently overfit to the logo, subsequent insertion stages fail
  2. Mask generation success: If SDEdit doesn't naturally place the logo, or detection fails (similarity < τ), the image is discarded
  3. Stealth preservation: If PSNR/LPIPS/CLIP scores degrade significantly, poisoned images become detectable during dataset filtering

- **Design tradeoffs**:
  - Poisoning ratio vs. attack effectiveness: Higher ratios increase LIR (45% at 100%) but risk detection
  - Noise strength vs. stealth: Lower noise in SDEdit preserves original content better but requires more iterations/retries
  - Mask size vs. detectability: Smaller masks increase stealth but may fail inpainting
  - Model choice: FLUX achieves higher naturalness but is computationally heavier

- **Failure signatures**:
  - Detection by set-based filtering: GPT-4o with multi-image context can identify repeated logo patterns
  - Low-complexity images: Snowfields, monotone backgrounds fail stealth requirements
  - Highly stylized logos: May evade DINOv2 detection, breaking refinement loop
  - Secondary poisoning propagation: Images generated by poisoned models can poison subsequent models with reduced LIR persistence

- **First 3 experiments**:
  1. Validate memorization mechanism: Fine-tune SDXL on 50 images containing a simple geometric shape in varied contexts with no text mention. Generate 100 images with unrelated prompts. Measure shape appearance rate
  2. Test stealth vs. effectiveness trade-off: Create poisoned datasets at 10%, 25%, 50%, 100% ratios. Measure LIR, FAE, and human detection rate
  3. Evaluate detection robustness: Apply CLIP-score filtering and GPT-4o set-based filtering to poisoned dataset. Measure filtering rate vs. false positive rate on clean images

## Open Questions the Paper Calls Out

- **Efficient set-based filtering methods**: The authors state that due to computational demands of using GPT-4o with long context inputs, efficient set-based filtering methods for detecting repeated visual patterns across poisoned datasets remain an open challenge for future work

- **Advanced model scalability**: The paper notes that using more advanced models like FLUX for both poisoning and as target models could enable more detailed and style-aligned logo insertions, but computational limitations prevented exploration of this direction

- **Secondary poisoning propagation dynamics**: While the paper demonstrates secondary poisoning occurs, the precise relationship between primary model LIR and secondary model effectiveness, and whether attack persistence can be predicted from primary model performance, requires further systematic study

## Limitations

- **Dataset composition dependency**: Attack effectiveness relies heavily on dataset composition, with high-entropy images prioritized while low-complexity images excluded, but exact thresholds and failure rates remain unclear
- **Scalability concerns**: The paper doesn't fully address scalability for real-world branding campaigns requiring millions of poisoned samples
- **Secondary poisoning characterization**: While secondary poisoning propagation is demonstrated, the paper lacks detailed analysis of long-term model behavior after multiple generations

## Confidence

- **High confidence**: Mechanism 1 (pattern memorization), Mechanism 3 (task preservation), stealth evaluation metrics
- **Medium confidence**: Mechanism 2 (stealth insertion pipeline), secondary poisoning propagation, GPT-4o detection limitations
- **Low confidence**: Real-world scalability, long-term model behavior after multiple generations, exact failure rates on excluded image types

## Next Checks

1. **Entropy filtering validation**: Create a test set with controlled image complexity (smooth backgrounds vs. high-entropy scenes) and measure LIR success rates to quantify the entropy threshold effect
2. **Large-scale poisoning simulation**: Generate a 10,000-image poisoned dataset (25% ratio) and measure LIR, detection rates, and computational cost to assess practical scalability limitations
3. **Multi-generation stability test**: Generate 1,000 images from a poisoned model, then fine-tune a new model on these outputs and measure LIR persistence and logo degradation across generations 1-3