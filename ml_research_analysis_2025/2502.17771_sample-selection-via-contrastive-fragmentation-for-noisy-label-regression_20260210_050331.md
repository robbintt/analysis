---
ver: rpa2
title: Sample Selection via Contrastive Fragmentation for Noisy Label Regression
arxiv_id: '2502.17771'
source_url: https://arxiv.org/abs/2502.17771
tags:
- latexit
- noise
- regression
- noisy
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConFrag addresses the problem of noisy label regression by leveraging
  the continuous and ordered correlation between labels and features in regression
  data. The core method involves partitioning the dataset into contrasting fragment
  pairs and training a mixture of expert feature extractors on these pairs.
---

# Sample Selection via Contrastive Fragmentation for Noisy Label Regression

## Quick Facts
- **arXiv ID:** 2502.17771
- **Source URL:** https://arxiv.org/abs/2502.17771
- **Reference count:** 40
- **Primary result:** ConFrag consistently outperforms 14 SOTA baselines on 6 benchmark datasets, achieving superior MRAE across symmetric and random Gaussian noise types.

## Executive Summary
ConFrag addresses noisy label regression by leveraging the continuous correlation between labels and features in regression data. The method partitions the dataset into contrasting fragment pairs and trains a mixture of expert feature extractors on these pairs. Sample selection is performed based on neighborhood agreement among the experts, enhanced by neighborhood jittering regularization. ConFrag introduces a new metric called Error Residual Ratio (ERR) to account for varying degrees of label noise severity.

## Method Summary
ConFrag transforms continuous regression labels into discrete fragments and pairs them contrastively (e.g., [1,4] and [2,3]). It trains binary classifiers (experts) on these pairs with neighborhood jittering regularization. Sample selection uses consensus from fragment-specific experts and their neighbors. The final regressor is trained only on selected clean samples. The approach achieves state-of-the-art performance across six benchmark datasets with various noise types.

## Key Results
- ConFrag achieves the best MRAE on all six benchmark datasets across different noise types
- Consistently outperforms 14 state-of-the-art baselines including CNLCU-H and BMM
- Introduces Error Residual Ratio (ERR) metric for more nuanced evaluation of noisy label methods
- Maintains high selection rates while minimizing error residuals

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Margin Maximization
Partitioning data into maximally distant fragment pairs creates larger decision margins than grouping all fragments together, improving feature extractor generalization. By training binary classifiers on distant fragments, the network encounters distinct feature clusters early in the forward pass, lowering prediction depth and producing more robust features against noise.

### Mechanism 2: Noise Severity Reduction (Closed-set to Open-set Conversion)
The fragment pairing strategy converts harmful "closed-set" label noise into less harmful "open-set" noise. When training a binary classifier on a contrastive pair, a sample from a different fragment that is mislabeled acts as open-set noise (an outlier), which networks learn to exclude more effectively than closed-set noise.

### Mechanism 3: Neighborhood Agreement Ensemble
Sample selection based on consensus of neighboring expert feature extractors is more robust than using a single network's prediction. A sample is deemed clean only if experts trained on the fragment and its immediate neighbors agree on its classification, enforcing consistency in both prediction and representation spaces.

## Foundational Learning

- **Concept: Prediction Depth**
  - Why needed here: Used as theoretical justification for why contrastive pairing works - lower prediction depth correlates with better generalization
  - Quick check question: Can you explain why a network that classifies a sample at Layer 3 is considered to have learned a "better" or "easier" feature representation than one that waits until Layer 10?

- **Concept: Mixture of Experts (MoE) with Gating**
  - Why needed here: Sample selection logic is modeled as an MoE where the "Fragment Prior" acts as soft gating
  - Quick check question: How does the Fragment Prior $\rho_f(y)$ (based on label distance) differ from standard learned gating networks in MoE?

- **Concept: Closed-set vs. Open-set Noise**
  - Why needed here: The method's efficacy relies on the empirical observation that open-set noise is less detrimental than closed-set noise
  - Quick check question: In a regression task with labels [0, 100], if we train a model on [0, 50], is a sample with label 80 mislabeled as 20 "closed-set" or "open-set" noise?

## Architecture Onboarding

- **Component map:** Fragmentation Engine -> Expert Feature Extractors -> Neighborhood Jittering -> MoE Selector -> Downstream Regressor
- **Critical path:** Discretize Labels → Pair Maximal Distance Fragments → Train Feature Extractors (Binary CE) with Jittering → Calculate Selection Probability → Train Regressor on selected clean subset
- **Design tradeoffs:** Fragment Count ($F$) - higher $F$ increases specialization but risks overfitting; Extractor Capacity - use smaller backbones than regressor to prevent memorizing noise
- **Failure signatures:** High Selection Rate + High MRAE indicates experts overfitting; Low Selection Rate suggests too strict agreement threshold
- **First 3 experiments:**
  1. Pairing Ablation: Compare "Contrastive Pairing" vs. "All-fragments" vs. "Adjacent Pairing" to verify margin benefit
  2. Jittering Analysis: Plot Feature Extractor Accuracy over epochs with/without Jittering to confirm regularization effect
  3. ERR vs. Selection Rate: Visualize trade-off to ensure ConFrag occupies "High Selection / Low Error Residual Ratio" quadrant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can parameter sharing or matrix decomposition techniques be integrated into ConFrag to mitigate the linear growth of parameters and expert redundancy?
- Basis in paper: [explicit] Authors identify "expert redundancy in MoEs' parameters" as prominent inefficiency and cite "parameter sharing" and "matrix product operators (MPO) decomposition" as promising future research
- Why unresolved: Current implementation uses disjoint feature extractors leading to linear parameter growth
- What evidence would resolve it: Implementation achieving comparable or superior MRAE with significantly fewer parameters than current architecture

### Open Question 2
- Question: How can the framework be modified to maintain generalization when the number of fragments ($F$) is increased significantly?
- Basis in paper: [explicit] Appendix G.2 notes "declining trend in performance as number of fragments increases" due to "finer division of training data... leading to overfitting"
- Why unresolved: Method struggles with larger $F$ values despite $F=4$ performing well
- What evidence would resolve it: Modification yielding stable or improved MRAE as $F$ increases beyond 4

### Open Question 3
- Question: To what extent can training efficiency strategies, such as distillation or sparse MoE techniques, be applied without compromising sample selection accuracy?
- Basis in paper: [explicit] Appendix B mentions ConFrag could benefit from "improving training efficiency" and cites distillation and pruning as relevant strategies
- Why unresolved: Current training requires simultaneous training of feature extractors and regression task
- What evidence would resolve it: Study comparing training time and GPU resources of distilled/sparse ConFrag against baseline while monitoring ERR

## Limitations

- Core claim of closed-to-open-set noise conversion relies on empirical assumption not rigorously proven within paper
- Effectiveness assumes strong label-feature correlation, which may not hold for non-linear or discontinuous relationships
- Dataset-specific balancing procedures and exact parameterization of random Gaussian noise injection remain underspecified

## Confidence

- **High confidence:** Core empirical performance claims and sample selection framework using mixture-of-experts consensus
- **Medium confidence:** Theoretical justification for contrastive fragmentation reducing prediction depth; closed-set to open-set noise conversion mechanism
- **Low confidence:** Exact impact of Neighborhood Jittering regularization and general applicability to datasets with weak label-feature correlation

## Next Checks

1. **Pairing Ablation Test:** Reproduce contrastive pairing vs. adjacent pairing vs. all-fragments comparison to verify maximum distance pairing provides statistically significant margin benefit
2. **Jittering Overfitting Test:** Monitor expert feature extractor accuracy on clean validation set with/without Neighborhood Jittering to confirm it prevents overfitting to noisy labels
3. **ERR-Selection Rate Trade-off Plot:** Generate scatter plot of Error Residual Ratio vs. Selection Rate for ConFrag and all baselines to verify claimed dominance in high selection / low ERR quadrant