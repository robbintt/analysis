---
ver: rpa2
title: 'Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End
  Autonomous Driving'
arxiv_id: '2511.19221'
source_url: https://arxiv.org/abs/2511.19221
tags:
- arxiv
- trajectory
- tokens
- perception
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Percept-WAM integrates 2D and 3D perception tasks into a unified
  World-Awareness-Action framework by encoding spatial coordinates and confidence
  into World-PV and World-BEV tokens. It employs grid-conditioned prediction, IoU-aware
  scoring, and parallel autoregressive decoding to enhance stability in long-tail
  and small-object scenarios.
---

# Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving

## Quick Facts
- arXiv ID: 2511.19221
- Source URL: https://arxiv.org/abs/2511.19221
- Reference count: 40
- Percept-WAM matches or surpasses classical detectors/segmenters on perception benchmarks and improves trajectory planning performance on nuScenes and NAVSIM.

## Executive Summary
Percept-WAM is a unified World-Awareness-Action model that integrates 2D and 3D perception tasks with trajectory planning for autonomous driving. It employs structured World-PV and World-BEV tokens that encode spatial coordinates and confidence, enabling persistent world states for both perception and action tasks. The model introduces grid-conditioned prediction with parallel autoregressive decoding and IoU-aware confidence scoring to improve stability in long-tail and small-object scenarios. Experimental results demonstrate state-of-the-art or competitive performance on perception benchmarks (COCO, nuScenes) and significant improvements in trajectory planning (90.2 PMDS on NAVSIM).

## Method Summary
Percept-WAM builds upon a VLM backbone (InternVL2-8B) and encodes multi-view images and optional LiDAR into structured spatial tokens. World-PV tokens capture perspective-view features in a grid structure, while World-BEV tokens lift 2D evidence into 3D BEV space via cross-attention. The model uses grid-conditioned prediction for dense object perception, where each grid token independently predicts bounding boxes or segmentation masks in parallel. IoU-aware confidence scoring is implemented through a confidence-tuning dataset generated from intermediate model predictions. For trajectory planning, the model employs a query-based decoder with vocabulary-based trajectory selection, trained through imitation learning and trajectory alignment stages.

## Key Results
- Achieves 51.7 mAP on COCO and 58.9 mAP on nuScenes detection benchmarks
- Improves trajectory planning PMDS to 90.2 on NAVSIM, a 2.1-point gain over DiffusionDrive
- Demonstrates robust open-vocabulary and long-tail generalization in qualitative results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified spatial token representations (World-PV and World-BEV) enable persistent world states that can be reused for both perception and action tasks.
- Mechanism: The model encodes spatial coordinates and confidence into structured tokens rather than relying on QA-style spatial reasoning. World-PV tokens capture perspective-view features in a grid structure, while World-BEV tokens are learnable queries that lift 2D evidence into 3D BEV space via cross-attention. These tokens serve as localized, reusable evidence for downstream reasoning and control.
- Core assumption: Explicit spatial encoding in token form is more effective than implicit spatial understanding through language-based QA for autonomous driving perception tasks.
- Evidence anchors:
  - [abstract] "Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence."
  - [section 3.1] "World-PV tokens are patchified into an H×W grid, and each grid location acts as a localized query for single-object perception."
  - [section 3.2] "World-BEV tokens are a set of learnable query tokens that serve as the foundation for BEV perception... used to query the features of the World-PV tokens via cross-attention."
  - [corpus] Related work (e.g., HERMES, CoReVLA) emphasizes reasoning in long-tail scenarios, but corpus evidence specifically for this token-based mechanism is weak or missing.
- Break condition: If perception tasks cannot be serialized into coordinate-based tokens or if the grid discretization loses critical spatial precision, the mechanism may fail.

### Mechanism 2
- Claim: Grid-conditioned prediction with parallel autoregressive decoding improves stability and throughput for dense object perception in long-tail and small-object scenarios.
- Mechanism: The model interpolates grid tokens from World-PV/World-BEV tokens, where each grid token independently predicts bounding boxes or segmentation masks. Parallel decoding allows all grid predictions simultaneously rather than sequentially. Attention masking ensures grid tokens and output tokens from different predictions are mutually masked.
- Core assumption: Dense spatial queries can be processed in parallel without significant inter-dependencies that require sequential reasoning.
- Evidence anchors:
  - [abstract] "We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding."
  - [section 3.1] "Percept-WAM decodes predictions in parallel across grids, significantly enhancing inference efficiency without sacrificing perception accuracy."
  - [section A.1] "To support grid-based parallel AR decoding, grid tokens and output tokens from different grid-based predictions are mutually masked."
  - [corpus] Weak direct evidence; related works (DiffE2E, CoReVLA) focus on diffusion or dual-stage frameworks, not parallel grid decoding.
- Break condition: If objects heavily overlap or require contextual reasoning across grid cells, the independent prediction assumption degrades performance.

### Mechanism 3
- Claim: IoU-aware confidence scoring with model-prediction-based training data reduces false positives and improves detection reliability compared to class-score-only baselines.
- Mechanism: The model predicts an explicit IoU-based confidence token for each bounding box. During training, a confidence-tuning dataset is generated using intermediate model predictions on training images, with IoU discretized into bins. This aligns confidence distributions with realistic model predictions rather than random perturbations.
- Core assumption: Confidence scores conditioned on predicted box attributes (class/coordinates/size) better reflect localization quality than softmax class probabilities.
- Evidence anchors:
  - [abstract] "incorporating IoU-aware scoring... improving stability in long-tail, far-range, and small-object scenarios."
  - [section 3.1.2] "We use the model from an intermediate training stage to inference on the training images, then the predicted boxes that match GT become samples paired with their IoU."
  - [Table 5] Ablation shows +1.5 AP improvement with real model-pred confidence data vs. baseline.
  - [corpus] No direct corpus evidence for this specific confidence mechanism.
- Break condition: If the confidence-tuning dataset generation process fails to capture the true distribution of model errors (e.g., due to insufficient intermediate checkpoints), confidence calibration degrades.

## Foundational Learning

- **Bird's Eye View (BEV) Representations**:
  - Why needed here: World-BEV tokens are core to Percept-WAM's 3D perception capability. Understanding how 2D image features are lifted to BEV space via cross-attention is critical.
  - Quick check question: Can you explain how LSS (Lift-Splat-Shoot) differs from query-based BEV lifting used in this paper?

- **Autoregressive vs. Parallel Decoding in LLMs**:
  - Why needed here: The paper leverages parallel AR decoding for efficiency while maintaining causal masking. Understanding this trade-off is essential.
  - Quick check question: What are the throughput implications of parallel decoding over N grid tokens versus sequential token generation?

- **Object Detection Metrics (mAP, IoU)**:
  - Why needed here: The paper reports mAP on COCO/nuScenes and uses IoU-aware confidence. Understanding these metrics is necessary to interpret results.
  - Quick check question: How does mAP account for both localization precision and classification accuracy?

## Architecture Onboarding

- Component map:
  1. VLM Backbone: InternVL2-8B pretrained model for general reasoning
  2. Image/LiDAR Encoders: InternViT for images, PointPillars (optional) for LiDAR
  3. World-PV Tokens: Grid-based features from perspective-view images (10×10 grid)
  4. World-BEV Tokens: Learnable BEV grid tokens (40×40 for detection, 10×10 for segmentation), optionally initialized from LiDAR features
  5. Grid Tokens: Interpolated from World-PV/World-BEV for dense prediction
  6. Action Head: Query-based trajectory decoder with Q_ego, Q_pv, Q_bev, Q_full
  7. Streaming KV Cache: For efficient inference with dual-recomputation

- Critical path:
  Training: Stage 1 (perception grounding) → Stage 2 (trajectory alignment)
  Inference: Multi-view images + optional LiDAR → World-PV/World-BEV tokens → Parallel grid prediction → Optional trajectory decoding

- Design tradeoffs:
  1. Grid resolution (10×10 vs. 40×40) balances granularity vs. computational cost
  2. Parallel decoding trades some inter-grid context for speed (16× speedup per Table 6)
  3. Confidence-tuning data generation adds training complexity but improves calibration

- Failure signatures:
  1. High false positives in crowded scenes: Indicates confidence calibration failure or insufficient IoU-tuning data
  2. BEV detection drift on long-range objects: May require higher grid resolution or LiDAR initialization
  3. Trajectory inconsistency under streaming: Check attention sink retention and dual-recomputation frequency

- First 3 experiments:
  1. Ablate confidence data source: Compare baseline (class score) vs. random-perturb vs. model-pred confidence tuning on nuImages 2D detection to replicate Table 5
  2. Grid resolution sweep: Test 10×10, 20×20, 40×40 grids for BEV detection to understand precision-speed tradeoff
  3. Streaming inference latency: Measure end-to-end latency with/without KV cache reuse and dual-recomputation on nuScenes val set

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a unified reinforcement learning (RL) framework with multi-objective reward design jointly optimize perception and trajectory prediction to enforce overall system consistency?
- **Basis in paper:** [explicit] The conclusion and limitations section explicitly state the intent to explore "offline/online RL with rollout-based rewards that couple perception accuracy with trajectory prediction."
- **Why unresolved:** The current model relies on imitation learning and query-based trajectory selection, which the authors note misaligns with real-world evaluation metrics, leading to potential distribution drift.
- **What evidence would resolve it:** Demonstration of a training scheme where RL rewards directly improve both perception mAP and planning PDMS simultaneously without requiring separate distillation from rule-based priors.

### Open Question 2
- **Question:** Can Mixture-of-Experts (MoE) architectures provide a more efficient and higher-accuracy alternative to the current dense backbone for multi-task joint training?
- **Basis in paper:** [explicit] The limitations section lists the exploration of "more efficient and higher-accuracy architectures... such as Mixture-of-Experts (MoE)" as future work.
- **Why unresolved:** The current unified VLM backbone may suffer from task interference or unnecessary computational overhead when processing distinct perception (World-PV/BEV) and reasoning tasks.
- **What evidence would resolve it:** A comparative study showing reduced inference latency or improved sub-task performance when specific "experts" are routed for perception versus reasoning tokens.

### Open Question 3
- **Question:** Does the reliance on a pre-clustered trajectory vocabulary limit the model's ability to generate novel safe trajectories in out-of-distribution scenarios compared to generative diffusion models?
- **Basis in paper:** [inferred] The method uses a vocabulary-based trajectory selector (Section A.3) to bridge the gap between IL and closed-loop metrics, contrasting with diffusion-based baselines that generate trajectories from noise.
- **Why unresolved:** A finite vocabulary inherently limits the solution space for planning, potentially preventing the model from finding optimal paths not represented in the training cluster distribution.
- **What evidence would resolve it:** Comparative analysis against diffusion-based planners on adversarial or highly constrained scenarios requiring trajectories outside the pre-defined vocabulary clusters.

## Limitations

- Long-tail generalization under distribution shift may fail if intermediate checkpoint distributions poorly represent deployment scenarios
- Parallel grid decoding scalability breaks down in highly cluttered scenes where objects interact spatially
- Streaming inference robustness is untested under high-frame-rate scenarios with rapid viewpoint changes

## Confidence

**High Confidence**:  
- Percept-WAM's state-of-the-art or competitive perception performance on COCO and nuScenes benchmarks (mAP scores reported)  
- Improved trajectory planning PMDS on nuScenes and NAVSIM (90.2 PMDS, 2.1-point gain over DiffusionDrive)  

**Medium Confidence**:  
- IoU-aware confidence scoring improves detection stability in long-tail scenarios (based on ablation Table 5, but limited direct comparison to alternatives)  
- Parallel autoregressive decoding achieves 16× speedup without accuracy loss (assumes independence; not validated under dense object conditions)  

**Low Confidence**:  
- Open-vocabulary and long-tail generalization capabilities (qualitative results only, no quantitative generalization tests on novel categories)  
- Streaming KV cache strategy's effectiveness in highly dynamic environments (no ablation on recomputation frequency or attention sink impact)  

## Next Checks

1. **Distribution shift robustness test**: Evaluate Percept-WAM on a curated long-tail dataset with out-of-distribution object categories (e.g., rare vehicles, unusual obstacles) to verify whether IoU-aware confidence calibration maintains calibration under unseen scenarios.

2. **Parallel decoding density sensitivity**: Systematically vary object density and occlusion levels in nuScenes validation scenes, measuring detection mAP and false positive rates for 10×10 vs. 40×40 grids to quantify the independence assumption's breakdown point.

3. **Streaming inference stress test**: Measure end-to-end latency and trajectory PMDS degradation under high-frame-rate streaming (e.g., 30 Hz) with varying attention sink retention intervals and dual-recomputation frequencies to identify stability thresholds.