---
ver: rpa2
title: 'ConvexBench: Can LLMs Recognize Convex Functions?'
arxiv_id: '2602.01075'
source_url: https://arxiv.org/abs/2602.01075
tags:
- uni000003ec
- reasoning
- uni0000017d
- uni00000176
- uni0000011e
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConvexBench, a benchmark for testing whether
  LLMs can recognize convex functions under deep functional composition. The benchmark
  generates compositional functions with controlled depth and mechanically verified
  labels, enabling systematic evaluation of compositional reasoning.
---

# ConvexBench: Can LLMs Recognize Convex Functions?

## Quick Facts
- arXiv ID: 2602.01075
- Source URL: https://arxiv.org/abs/2602.01075
- Authors: Yepeng Liu; Yu Huang; Yu-Xiang Wang; Yingbin Liang; Yuheng Bu
- Reference count: 35
- One-line result: Performance drops from F1-score of 1.0 at depth 2 to approximately 0.2 at depth 100

## Executive Summary
This paper introduces ConvexBench, a benchmark for testing whether LLMs can recognize convex functions under deep functional composition. The benchmark generates compositional functions with controlled depth and mechanically verified labels, enabling systematic evaluation of compositional reasoning. Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of 1.0 at depth 2 to approximately 0.2 at depth 100. Analysis of reasoning traces identifies two failure modes: parsing failures (models lose track of parentheses and operator scope) and lazy reasoning (models rely on shallow heuristics rather than step-by-step verification). To address these limitations, the authors propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score = 1.0 at depth 100).

## Method Summary
The ConvexBench benchmark generates compositional functions by recursively applying Disciplined Convex Programming (DCP) atoms (e.g., exp, log, max, norms) to create depth-controlled expressions. Each function is labeled as convex, concave, or neither using Jensen's inequality validation. Four evaluation paradigms are tested: (1) One-shot reasoning on raw expressions, (2) One-shot with AST decomposition via external tool, (3) Agentic reasoning with recursive per-subfunction checks and cumulative context, and (4) Agentic reasoning with dependency-focused context (only direct parent nodes). Models tested include GPT-5, Gemini-2.5-Pro, Qwen3-8B/30B with temperature 0.1 (except GPT-5), self-consistency N=64 for open-source, and max reasoning tokens 50,000.

## Key Results
- Performance degrades rapidly with increasing depth: F1-score drops from 1.0 at depth 2 to approximately 0.2 at depth 100
- Agentic divide-and-conquer framework achieves F1-Score of 1.0 at depth 100, substantially improving performance at large depths
- Two failure modes identified: parsing failures (losing track of parentheses and operator scope) and lazy reasoning (relying on shallow heuristics)
- Focused context improves accuracy by reducing attention distraction in long reasoning chains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offloading structural parsing to an external tool mitigates syntax-based failures.
- Mechanism: By deterministically converting raw symbolic expressions into an Abstract Syntax Tree (AST) or a sequence of sub-functions before the LLM processes them, the system removes the error-prone task of tracking parentheses and operator scope. This isolation prevents a structural misinterpretation from corrupting the entire reasoning chain.
- Core assumption: The external parsing tool is reliable and produces a decomposition the LLM can interpret correctly.
- Evidence anchors:
  - [abstract] "...offloads parsing to an external tool to construct an abstract syntax tree (AST)..."
  - [section 4.2] "To isolate reasoning errors from parsing failures, we provide the LLM with an explicit decomposition... We offload the parsing process to an external tool..."
  - [corpus] No direct corpus evidence found for this specific LLM-AST integration mechanism.
- Break condition: The external tool fails on malformed or ambiguous expressions, or the AST representation exceeds the model's ability to process structured inputs.

### Mechanism 2
- Claim: Enforcing recursive, step-by-step verification prevents "lazy reasoning" and ensures full chain traversal.
- Mechanism: An agentic framework decomposes the overall task into a sequence of local sub-tasks, forcing the model to explicitly verify intermediate states (curvature, range) before composing results upward. This prevents the model from abandoning the analysis or relying on shallow heuristics.
- Core assumption: The task is decomposable into verifiable steps, and the model is capable of performing each step correctly.
- Evidence anchors:
  - [abstract] "...enforces recursive reasoning over each intermediate sub-expression with focused context."
  - [section 4.2] "Rather than asking the model to reason over the entire decomposed sequence CG in a single pass, we split a depth-D composition into k local tasks and require an explicit check of each intermediate sub-expression gi before composing results upward."
  - [corpus] No direct corpus evidence for this specific recursive enforcement strategy.
- Break condition: The model consistently makes elementary errors in the sub-tasks, or the cost and latency of recursive calls become prohibitive.

### Mechanism 3
- Claim: Using dependency-focused context improves accuracy by reducing attention distraction in long reasoning chains.
- Mechanism: Instead of feeding the full history of intermediate states into each step, the system constructs a context containing only the direct dependencies (parent nodes) for the current sub-task. This lowers the signal-to-noise ratio, helping the model focus on relevant information and reducing compounding errors.
- Core assumption: The dependency graph accurately captures necessary information, and excluding non-dependent history does not harm the model's reasoning.
- Evidence anchors:
  - [section 4.2] "At each step, the sub-function gi depends only on a small set of previously defined sub-functions... We thus construct the dependency-focused context: C̄G(i) = {(gj, σj) | j ∈ Pa(i)}..."
  - [section 5.2.3] "...reasoning chains can still collapse as the context grows, and pruning irrelevant history provides a principled way to reduce such late-stage errors."
  - [corpus] No direct corpus evidence for this dependency-focused context mechanism.
- Break condition: The dependency graph is dense, providing minimal context reduction, or critical indirect dependencies are inadvertently excluded.

## Foundational Learning

### Concept: Abstract Syntax Trees (AST)
- Why needed here: To understand how complex symbolic expressions are represented hierarchically for both parsing and recursive reasoning.
- Quick check question: How would you represent the expression `a * (b + c)` as a simple AST?

### Concept: Disciplined Convex Programming (DCP) Rules
- Why needed here: To understand the domain-specific rules (e.g., "sum of convex is convex", "exp(convex) is convex") that the system must verify at each step.
- Quick check question: If `f(x)` is convex and increasing, and `g(x)` is convex, is `f(g(x))` convex?

### Concept: Long-Horizon vs. Long-Context Reasoning
- Why needed here: To distinguish between failures due to limited token windows and failures due to compounding errors in multi-step chains (as highlighted in the paper's token analysis).
- Quick check question: A model with a 128k context window fails on a 5k token task requiring 50 sequential steps. Is this primarily a context window issue or a long-horizon reasoning issue?

## Architecture Onboarding

### Component map
External parser -> AST generator -> Agentic reasoner with four modes: (1) One-shot with Decomp, (2) Agentic Reasoning (full context), (3) Agentic Reasoning with Focused Context

### Critical path
1. **Input**: Raw symbolic expression
2. **Decomposition**: External tool parses expression into an AST/sub-function list
3. **Context Construction**: Agent determines the relevant context (full or focused) for the current sub-task
4. **Stepwise Reasoning**: LLM analyzes a sub-expression, applies DCP rules, and outputs a state (curvature, range)
5. **Composition**: Intermediate states are aggregated to determine the final convexity of the target function

### Design tradeoffs
- **Granularity vs. Cost**: Finer decomposition improves performance but increases inference cost and latency (Section 5.2.3)
- **Context vs. Attention**: Full context preserves all history but risks distraction; focused context requires dependency tracking but improves accuracy
- **Model Size vs. Scaffold Need**: Stronger models may achieve high performance with one-shot decomposition, while smaller models require the full agentic framework (Section 6)

### Failure signatures
- **One-shot**: Sharp performance drop with depth, token count plateau indicating lazy reasoning (Figure 4a, 4e)
- **One-shot with Decomp**: Persistent errors on complex sub-expressions, non-monotonic performance
- **Agentic (Full Context)**: Errors concentrate in later steps due to growing, distracting context (Figure 6)

### First 3 experiments
1. **Reproduce the One-shot Baseline**: Evaluate the raw model on the benchmark to confirm the compositional reasoning gap and identify parsing/lazy reasoning modes
2. **Implement External Parser Integration**: Use a tool to generate the AST and evaluate "One-shot Reasoning with Decomp" to isolate reasoning errors from parsing failures
3. **Build the Focused Context Agent**: Implement the recursive reasoning loop with dependency-based context pruning and compare token usage and F1-score against baselines, targeting performance at depth 100

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can batching consecutive reasoning steps ($k > 1$) in the agentic framework maintain high accuracy while significantly reducing the 10x inference cost overhead?
- Basis in paper: [explicit] The conclusion suggests, "One way to improve this trade-off is to batch k consecutive steps into a single agentic call... reducing the number of agentic calls."
- Why unresolved: The paper only evaluates single-step recursive calls. Increasing the batch size $k$ might reintroduce the "lazy reasoning" failure mode or context distraction if the local context window becomes too large within a single batch.
- What evidence would resolve it: Ablation studies on ConvexBench varying the batch size $k$ (e.g., 5, 10, 20 steps) and plotting the F1-Score against the total token consumption to find the optimal efficiency-accuracy frontier.

### Open Question 2
- Question: Can a dynamic system effectively route between one-shot and agentic reasoning paradigms based on model capability and estimated problem depth?
- Basis in paper: [explicit] The conclusion states, "Reasoning frameworks should be model capability-aware: for stronger models, one-shot decomposition can capture most gains... whereas for smaller models, decomposition combined with recursive reasoning enables progress."
- Why unresolved: The study evaluates static frameworks separately. It does not test a unified system that autonomously predicts when the expensive "Agentic Reasoning with Focused Context" is necessary versus when "One-shot with Decomp" is sufficient.
- What evidence would resolve it: Implementing a router model that estimates composition depth or difficulty to select the reasoning mode, then comparing the aggregate accuracy and latency against fixed-paradigm baselines.

### Open Question 3
- Question: Do the "parsing failure" and "lazy reasoning" failure modes identified in symbolic ConvexBench tasks generalize to natural language mathematical reasoning?
- Basis in paper: [inferred] The paper attributes failure to "parsing failure" and "lazy reasoning" in symbolic expressions. However, the benchmark uses mechanically generated DCP-compliant syntax, leaving it unclear if these failure modes persist in messy, real-world research math or natural language.
- Why unresolved: The benchmark controls for structure strictly; real-world mathematical reasoning often involves implicit assumptions or non-standard notation which might trigger different failure modes.
- What evidence would resolve it: Applying the "focused context" and "tool-integrated decomposition" methods to datasets like MATH or Omni-MATH to see if they yield similar performance improvements, indicating shared underlying bottlenecks.

## Limitations
- The benchmark uses mechanically generated DCP-compliant syntax, which may not generalize to open-domain symbolic reasoning tasks
- The external parser's role is critical but underspecified - the paper doesn't detail the parsing tool or its failure modes
- The agentic framework assumes reliable step-by-step verification but doesn't address scenarios where the LLM makes systematic errors at the sub-task level
- The benchmark's "neither" class exclusion through Jensen's inequality validation may create selection bias toward easier compositional structures

## Confidence

- **High Confidence**: The compositional reasoning gap exists (performance degrades with depth) - directly measured and clearly demonstrated across multiple model sizes
- **Medium Confidence**: The agentic divide-and-conquer framework improves performance - well-supported by controlled experiments, though parser quality remains a confounder
- **Medium Confidence**: The two failure modes (parsing failures, lazy reasoning) are identified - analysis is compelling but based on qualitative inspection of reasoning traces rather than automated classification

## Next Checks

1. **Parser Independence Test**: Replace the external parser with a different symbolic decomposition tool and verify whether performance gains persist. This isolates whether improvements come from LLM reasoning or parser quality.

2. **Error Localization Analysis**: Instrument the agentic framework to automatically classify each failure as parsing error, sub-task error, or composition error. This validates the two failure mode hypothesis quantitatively.

3. **Generalization Benchmark**: Apply the same agentic framework to a non-convexity symbolic reasoning task (e.g., algebraic identity verification) to test whether the approach generalizes beyond DCP-specific rules.