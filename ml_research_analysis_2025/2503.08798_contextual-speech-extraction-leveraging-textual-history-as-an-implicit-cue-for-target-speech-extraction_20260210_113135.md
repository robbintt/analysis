---
ver: rpa2
title: 'Contextual Speech Extraction: Leveraging Textual History as an Implicit Cue
  for Target Speech Extraction'
arxiv_id: '2503.08798'
source_url: https://arxiv.org/abs/2503.08798
tags:
- speech
- target
- context
- speaker
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Contextual Speech Extraction (CSE), a novel
  speech extraction approach that leverages textual context as an implicit cue to
  extract the target speech from a mixture without requiring explicit cues like enrollment
  utterances or spatial information. The method uses dialogue or monologue history
  as a cue, which is naturally available in mobile messaging environments where voice
  recordings are interleaved with text messages.
---

# Contextual Speech Extraction: Leveraging Textual History as an Implicit Cue for Target Speech Extraction

## Quick Facts
- arXiv ID: 2503.08798
- Source URL: https://arxiv.org/abs/2503.08798
- Authors: Minsu Kim; Rodrigo Mira; Honglie Chen; Stavros Petridis; Maja Pantic
- Reference count: 40
- One-line primary result: Over 90% accuracy in identifying the correct target stream with only two previous dialogue turns, without requiring explicit cues like enrollment utterances or spatial information

## Executive Summary
This paper introduces Contextual Speech Extraction (CSE), a novel approach that leverages textual dialogue history as an implicit cue to extract target speech from multi-speaker mixtures. Unlike traditional speech extraction methods that require explicit cues like enrollment utterances or spatial information, CSE uses the natural textual context available in mobile messaging environments where voice recordings are interleaved with text messages. The method employs a cascaded approach initially and develops unified models (ContSep and ContExt) that directly incorporate context conditioning into the separator, avoiding error propagation from cascaded ASR systems. Experiments on three datasets demonstrate that CSE can achieve high accuracy in identifying the correct target stream using only a few turns of previous dialogue history.

## Method Summary
The CSE framework extracts target speech from a mixture using textual dialogue or monologue history as an implicit cue. The approach employs a cascaded system initially, combining speech separation, ASR, and LLM-based context selection. To improve efficiency, unified models are developed: ContSep separates all streams and classifies the target stream simultaneously, while ContExt directly predicts the target stream. A hybrid model H-ContExt combines textual context with enrollment utterances for enhanced flexibility. The method uses Llama 3-8B to encode dialogue history into context embeddings, which are concatenated with audio features at each transformer layer input. The models are trained with SI-SNR loss and evaluated on three datasets (DailyTalk, SpokenWOZ, TED-LIUM 3) with mixtures created at random SNR -5 to 5 dB.

## Key Results
- Over 90% accuracy in identifying the correct target stream with only two previous dialogue turns
- ContExt achieves 98.3% accuracy on SpokenWOZ versus 66.2% for cascaded approach
- SI-SNR improvement of 10.03 dB on DailyTalk dataset
- Performance plateaus around 20 context turns, with minimal improvement beyond this point

## Why This Works (Mechanism)

### Mechanism 1: Semantic Context Conditioning via Temporal Concatenation
Textual dialogue history provides an implicit cue that enables the model to identify which speaker's turn logically follows the conversation context. A pre-trained LLM encodes dialogue history into a context embedding $f_c$, which is concatenated with audio features at the input of every IntraTransformer and InterTransformer layer along the temporal dimension. The target speaker's utterance is semantically coherent with the preceding dialogue context, enabling the model to resolve speaker identity through topical continuity rather than acoustic fingerprints. Evidence shows over 90% accuracy with only two previous dialogue turns, and related work ELEGANCE demonstrates similar LLM-guided audio-visual extraction paradigms.

### Mechanism 2: Unified Architecture Avoides Error Propagation from Cascaded ASR
Direct end-to-end training of ContExt/ContSep bypasses the ASR transcription bottleneck that degrades the cascaded approach. In the cascaded system, errors compound: separation → ASR transcription → LLM selection. On SpokenWOZ, ASR WER is 38.59%, causing accuracy to drop to 66.2%. ContExt and ContSep integrate context conditioning directly into the separator, eliminating the transcription step entirely and learning a joint audio-text representation. The unified models learn to associate acoustic patterns with semantic context without explicit intermediate transcriptions.

### Mechanism 3: Hybrid Cue Training with Random Masking Enables Inference Flexibility
Joint training on both context embeddings and speaker embeddings with random cue dropout allows H-ContExt to operate with either cue, both, or neither at inference time. During training, one cue ($f_c$ or $f_s$) is randomly masked, forcing the model to learn robust representations from each cue independently. At inference, the model can use context-only (83.5% ACC on DailyTalk), audio-only (99.8% ACC), or both (99.8% ACC). Random masking prevents the model from over-relying on the stronger cue (likely speaker embedding), degrading context-only performance.

## Foundational Learning

- **Concept: Permutation Invariant Training (PIT)**
  - Why needed here: In multi-speaker separation, output streams have no fixed order; PIT loss resolves assignment ambiguity by computing losses over all permutations and selecting the minimum.
  - Quick check question: Given 3 predicted streams and 3 ground-truth sources, how many loss evaluations does PIT require per batch?

- **Concept: Dual-Path Transformer for Long Sequences**
  - Why needed here: Raw audio waveforms at 8 kHz produce long sequences; dual-path architecture reduces quadratic complexity.
  - Quick check question: In Equation 1, what does dimension $N$ represent, and why is it treated as the batch dimension?

- **Concept: Scale-Invariant Signal-to-Noise Ratio (SI-SNR)**
  - Why needed here: SI-SNR is the primary loss and evaluation metric; it normalizes for signal scale, making it suitable for waveform-level training.
  - Quick check question: In SI-SNR, why is the target signal projected onto the estimate before computing the error?

## Architecture Onboarding

- **Component map:** Raw waveform → Encoder (1D conv) → Masking Network (8 IntraTransformer + 8 InterTransformer layers × 2 blocks) → Mask Predictor → Decoder (1D transposed conv) → Extracted waveform

- **Critical path:** Audio mixture → Encoder → Masking Network (with context injection at every transformer layer) → Mask Predictor → Decoder → Extracted waveform. For H-ContExt, speaker embedding concatenation occurs alongside context.

- **Design tradeoffs:**
  - ContSep vs. ContExt: ContSep provides all separated streams but has lower SI-SNR improvement (5.42 dB vs. 10.03 dB on DailyTalk) due to multi-task bottleneck
  - Cascaded vs. Unified: Cascaded is interpretable but suffers ASR error propagation; Unified is end-to-end but harder to debug
  - LLM size: Larger LLMs (Llama 3-8B vs. OPT-350M) yield better accuracy (83.5% vs. 76.0%) but increase inference cost

- **Failure signatures:**
  - Low accuracy (~50%) with context-only input: Check if context embedding is being correctly passed; model may have overfitted to speaker embedding
  - High WER in cascaded system: ASR model failing on accented speech; switch to unified model
  - SI-SNR improvement near zero: Context length may be insufficient; verify context is not empty

- **First 3 experiments:**
  1. Reproduce DailyTalk 2-speaker baseline: Train ContExt from Sepformer checkpoint, verify ~10 dB SI-SNR improvement and ~83% accuracy with context-only input.
  2. Ablate context length: Evaluate ContExt on SpokenWOZ with context lengths 0, 2, 5, 10, 20 turns. Reproduce the curve in Figure 4.
  3. Hybrid cue dropout test: Train H-ContExt with masking probability 0.5 vs. 0.0. Evaluate context-only, audio-only, and both at inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependency on large language models (LLMs) for context embedding be reduced to enable efficient, real-time inference without significant performance degradation?
- Basis in paper: The authors explicitly note that "employing larger LLMs consistently yields better results," with Llama 3-8B significantly outperforming smaller models like OPT-350M. The Introduction also explicitly identifies the need to "improve efficiency and reduce error propagation" as the motivation for moving away from the cascaded model.
- Why unresolved: While the unified model improves efficiency over the cascaded approach, the best results rely on a computationally expensive 8B parameter model. The paper does not explore methods to compress this specific component or maintain high accuracy with lightweight encoders.
- What evidence would resolve it: Experiments demonstrating that a distilled, quantized, or adapter-based context encoder can achieve comparable SI-SNRi and ACC to the Llama 3-8B baseline while operating at a lower latency.

### Open Question 2
- Question: How robust is the method when the textual history is derived from imperfect (noisy) Automatic Speech Recognition (ASR) rather than ground-truth transcripts?
- Basis in paper: The paper identifies "mobile messaging environments" as the primary application, where text history is often available. However, the experiments utilize ground-truth transcripts from the datasets. In a real-world deployment, the context might be generated via voice typing, introducing typos or errors.
- Why unresolved: The study demonstrates that the cascaded model fails when the target stream's ASR is poor, but it does not evaluate the unified model's sensitivity to noise within the *input context* cue itself.
- What evidence would resolve it: An ablation study where the input context $c$ is synthetically corrupted or generated by a high-WER ASR system, measuring the resulting drop in extraction accuracy.

### Open Question 3
- Question: To what extent does extraction performance degrade in "cocktail party" scenarios with more than three overlapping speakers?
- Basis in paper: The authors explicitly evaluate 2-speaker and 3-speaker mixtures. The results show a performance gap, with accuracy dropping from ~98% (2 speakers) to ~64% (3 speakers) on SpokenWOZ/TED-LIUM.
- Why unresolved: The paper validates robustness up to 3 speakers but does not establish the upper limit of the method's utility. It is unclear if the context cue remains discriminative when the mixture density increases further.
- What evidence would resolve it: Benchmarks on mixtures of 4, 5, or more speakers to map the decay curve of accuracy and determine if the context cue is sufficient for highly crowded acoustic environments.

## Limitations
- The exact semantic features exploited by LLM embeddings are not fully validated through ablation studies
- Performance on non-native accented speech relies heavily on the unified approach, but context contribution versus architectural changes is unclear
- Limited evaluation of real-world ASR-generated context versus ground-truth transcripts

## Confidence
- **High Confidence:** The core claim that textual context can serve as an implicit cue for speech extraction is well-supported by experimental results across three datasets
- **Medium Confidence:** The mechanism by which LLM embeddings improve extraction performance is plausible but not fully validated
- **Medium Confidence:** The claim that unified models avoid error propagation from cascaded ASR is supported but comparison is somewhat indirect

## Next Checks
1. **Context Ablation Study:** Train a variant of ContExt where context embeddings are replaced with random vectors of the same dimension. If semantic conditioning is truly responsible for performance gains, this should significantly degrade accuracy while maintaining SI-SNR improvement.

2. **Cross-Topic Generalization:** Evaluate the model on mixtures where the target utterance is topically unrelated to the preceding context. This would test whether the model truly relies on semantic coherence versus simply identifying speaker turns.

3. **Speaker Overlap Test:** Create test cases where both speakers contribute to the dialogue context (mixed speaker turns in history) versus cases where only one speaker speaks in the context. This would reveal whether the model can handle more realistic dialogue scenarios where the target speaker isn't always the most recent contributor.