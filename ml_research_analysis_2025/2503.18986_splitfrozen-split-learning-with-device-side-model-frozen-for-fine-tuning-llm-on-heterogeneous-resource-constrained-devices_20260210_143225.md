---
ver: rpa2
title: 'SplitFrozen: Split Learning with Device-side Model Frozen for Fine-Tuning
  LLM on Heterogeneous Resource-Constrained Devices'
arxiv_id: '2503.18986'
source_url: https://arxiv.org/abs/2503.18986
tags:
- fine-tuning
- devices
- splitfrozen
- server
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SplitFrozen addresses efficient LLM fine-tuning on heterogeneous,
  resource-constrained devices by strategically freezing device-side layers and centralizing
  LoRA-based fine-tuning on a server. The framework partitions models into device-side
  frozen layers (performing only forward propagation) and server-side fine-tuning
  layers, using pipeline parallelism to overlap computations and reduce training time.
---

# SplitFrozen: Split Learning with Device-side Model Frozen for Fine-Tuning LLM on Heterogeneous Resource-Constrained Devices

## Quick Facts
- arXiv ID: 2503.18986
- Source URL: https://arxiv.org/abs/2503.18986
- Reference count: 16
- Primary result: Reduces device-side computations by 86.8% and training time by 50.2% while outperforming baselines by 69.4% accuracy under Non-IID data

## Executive Summary
SplitFrozen addresses the challenge of efficiently fine-tuning large language models across heterogeneous, resource-constrained devices with imbalanced data distributions. The framework freezes device-side transformer layers and centralizes LoRA-based fine-tuning on a server, using pipeline parallelism to overlap computations and minimize training time. By strategically partitioning models and shuffling device activations, SplitFrozen achieves significant accuracy improvements over federated and split learning baselines while drastically reducing device computational burden.

## Method Summary
SplitFrozen partitions LLMs into device-side frozen layers (performing only forward propagation) and server-side fine-tuning layers with LoRA adapters. Devices are allocated layers based on their computational capacity, with weaker devices receiving fewer layers. Activations are transmitted to the server, which aggregates, shuffles, and performs centralized LoRA updates using a three-stage pipeline that overlaps device forward, activation transmission, and server computation. The framework employs ZB-H1 temporal decomposition to further optimize server-side backward propagation.

## Key Results
- Outperforms FedLoRA and SplitLoRA by 69.4% model accuracy under extremely imbalanced (Non-IID) data
- Reduces device-side computations by 86.8% and total training time by 50.2% compared to baselines
- Successfully scales to Llama-3.2-1B on GSM8K for content generation tasks

## Why This Works (Mechanism)

### Mechanism 1
Freezing device-side layers and restricting devices to forward-only propagation drastically reduces on-device computation while preserving model quality. Pre-trained LLMs exhibit layer-wise feature specialization—lower transformer layers capture general linguistic features that remain stable across downstream tasks. By freezing these layers on devices, SplitFrozen eliminates gradient computation and backward propagation overhead (the most computationally intensive phase), while server-side LoRA adapts task-specific higher layers. The frozen layers contain sufficiently general features that do not require task-specific adaptation; performance degradation from freezing is offset by centralized server fine-tuning.

### Mechanism 2
Aggregating and shuffling device activations at the server before fine-tuning mitigates Non-IID data distribution degradation. In federated/split learning, Non-IID data causes client drift—local updates diverge from global optima. SplitFrozen centralizes all activations, then shuffles them to create a pseudo-IID batch distribution before server-side LoRA updates. This effectively converts a distributed Non-IID problem into a centralized near-IID optimization. Shuffled activations sufficiently approximate IID distribution for stable LoRA convergence; privacy is maintained via activation-based (not raw data) transmission.

### Mechanism 3
Heterogeneity-aware layer allocation synchronized with pipeline parallelism reduces total training time by overlapping computation and communication. Devices with higher computational capacity receive more frozen layers; weaker devices receive fewer. The server orchestrates a three-stage pipeline: (1) device forward, (2) activation transmission, (3) server forward/backward with ZB-H1 temporal decomposition (splitting gradient computation B and weight update W). This eliminates "time bubbles" where server idles waiting for slow devices. Layer allocation decisions can be computed at initialization and remain valid throughout training; transmission bandwidth is stable.

## Foundational Learning

- **Concept: Split Learning (SL)**
  - Why needed: SplitFrozen is a variant of SL; understanding the baseline client-server model partition (cut layer, smashed data/activations) is prerequisite
  - Quick check: Can you explain why SL reduces device memory compared to full-model local training?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed: Server-side fine-tuning uses LoRA; understanding low-rank matrix decomposition (W = W₀ + BA) is essential to grasp why server computation is efficient
  - Quick check: If LoRA rank R=4 and hidden dimension is 768, how many trainable parameters does one LoRA adapter add?

- **Concept: Pipeline Parallelism (GPipe/1F1B)**
  - Why needed: SplitFrozen's three-stage pipeline borrows from distributed training pipeline concepts; understanding micro-batch scheduling helps debug timing
  - Quick check: What is a "bubble" in pipeline parallelism, and why does it hurt throughput?

## Architecture Onboarding

- **Component map:**
  1. Device-side Frozen Encoder (first N transformer layers; forward pass only; outputs activations)
  2. Communication Channel (transmits activations + layer-count metadata; 600 Mbps in experimental setup)
  3. Server-side Adapter (remaining transformer layers + LoRA modules; executes forward + backward; aggregates/shuffles activations across devices)
  4. Pipeline Controller (orchestrates heterogeneity-aware layer allocation and ZB-H1 backward scheduling)

- **Critical path:**
  1. At initialization, devices report compute capacity → server assigns layer count (N)
  2. Per mini-batch: devices execute frozen forward → transmit activations → server completes forward, shuffles, executes LoRA backward (B then W)
  3. Updated LoRA weights remain server-side; frozen device weights never change

- **Design tradeoffs:**
  - More frozen layers on device: Lower device compute, but higher communication (larger activations if cut later) and potentially lower accuracy
  - Fewer frozen layers on device: Higher device compute, but server handles more layers (better accuracy potential)
  - Assumption: Paper does not quantify privacy guarantees; activations may leak information—consider differential privacy or encryption if required

- **Failure signatures:**
  - Device stragglers: Pipeline stalls if heterogeneity-aware allocation underestimates slowest device; check timing logs for "bubbles"
  - Non-IID collapse: If shuffle buffer too small, accuracy approaches FedLoRA/SplitLoRA baselines
  - Memory overflow on server: If too many devices transmit concurrently without batching, server VRAM saturates

- **First 3 experiments:**
  1. Baseline sanity check: Replicate Table II (GPT-2 on MRPC, IID) with SplitFrozen vs. FedLoRA vs. SplitLoRA; verify ~13.6% accuracy gain and ~86% device MFLOP reduction
  2. Non-IID stress test: Use Dirichlet α=0.1 on MRPC; confirm SplitFrozen approaches CenLoRA accuracy while baselines degrade
  3. Pipeline profiling: Instrument device forward time, transmission time, server forward/backward time; identify bottleneck stage and validate 50% total time reduction claim

## Open Questions the Paper Calls Out

- **Open Question 1:** How can activation compression techniques be integrated into SplitFrozen to reduce the communication overhead of transmitting intermediate features without degrading the LoRA fine-tuning accuracy? While the paper notes that standardized dimensions result in constant activation sizes (e.g., 6MB for GPT-2), it does not implement or evaluate compression methods, which remain a bottleneck for bandwidth-limited devices.

- **Open Question 2:** How can the SplitFrozen architecture be adapted for multi-server 6G environments to handle synchronization and load balancing? The current framework relies on a centralized server for aggregation and LoRA updates; it is unclear how the pipeline parallelism strategy copes with distributed server topologies or cross-server synchronization delays.

- **Open Question 3:** What is the vulnerability of the transmitted intermediate activations to reconstruction attacks, given that device-side layers are frozen and never updated? The paper claims privacy preservation as a benefit of Split Learning, but evaluates only accuracy and efficiency. Frozen weights create a deterministic mapping from data to activations, which may facilitate inversion attacks.

## Limitations

- Privacy risk from activation transmission: Transmitted activations from deeper layers can still leak significant information about raw inputs, particularly for short sequences
- Layer allocation stability assumption: Framework assumes fixed layer allocation based on initial device capacity reporting, which may not hold with fluctuating device availability
- Activation memory scaling: As sequence length increases, activation memory grows quadratically with hidden dimension, potentially overwhelming device memory or network bandwidth

## Confidence

- **High confidence:** Device-side computation reduction (86.8%) and total training time reduction (50.2%) claims are supported by quantitative experimental results across multiple datasets and model sizes
- **Medium confidence:** Non-IID accuracy improvement (69.4%) is well-demonstrated, though the exact contribution of shuffling versus centralized LoRA is not isolated experimentally
- **Low confidence:** Scalability claims for Llama-3.2-1B on GSM8K are based on a single generation task; broader validation across diverse LLM sizes and tasks would strengthen generalizability

## Next Checks

1. Privacy leakage quantification: Measure mutual information between transmitted activations and original inputs using DP-SGD or activation masking to assess actual privacy preservation
2. Dynamic layer reallocation test: Implement runtime device monitoring and adaptive layer reallocation; measure performance degradation if static allocation is used versus dynamic adjustment
3. Communication overhead scaling: Benchmark activation transmission time and memory usage across sequence lengths (128→1024 tokens) and model sizes (GPT-2→Llama-3B) to identify practical scaling limits