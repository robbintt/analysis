---
ver: rpa2
title: 'FedADP: Unified Model Aggregation for Federated Learning with Heterogeneous
  Model Architectures'
arxiv_id: '2505.06497'
source_url: https://arxiv.org/abs/2505.06497
tags:
- learning
- federated
- fedadp
- clients
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of heterogeneous model architectures
  in federated learning (FL), where clients with varying computational resources and
  hardware constraints employ different model structures. The proposed FedADP framework
  dynamically adjusts model architectures during aggregation to enable effective collaboration
  among heterogeneous clients, maximizing resource utilization while preserving data
  privacy.
---

# FedADP: Unified Model Aggregation for Federated Learning with Heterogeneous Model Architectures

## Quick Facts
- arXiv ID: 2505.06497
- Source URL: https://arxiv.org/abs/2505.06497
- Reference count: 32
- Primary result: Achieves 23.30% accuracy improvement over FlexiFed and 46.25% over Clustered-FL in heterogeneous FL settings

## Executive Summary
FedADP addresses the challenge of federated learning with heterogeneous model architectures by dynamically adjusting model structures during aggregation. The framework enables clients with varying computational resources to collaborate effectively by transforming their local models to align with a unified global structure. Through the NetChange component, FedADP preserves learned knowledge while enabling standard aggregation techniques, achieving significant accuracy improvements over existing approaches while maintaining superior convergence rates.

## Method Summary
FedADP uses NetChange transformations to convert heterogeneous client models to a unified global structure before FedAvg aggregation. The server constructs a global model as the union of all client architectures, then distributes adapted versions to each client. Clients train locally, transform their models to match the global structure, and upload them for aggregation. The process involves bidirectional transformations: widening/deepening for client-to-server upload and narrowing/shallowing for server-to-client distribution, with weight redistribution strategies preserving functional equivalence.

## Key Results
- Achieves 23.30% accuracy improvement over FlexiFed and 46.25% over Clustered-FL
- Maintains superior convergence rates compared to baseline methods
- Successfully balances training performance and efficiency across heterogeneous client environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting heterogeneous client models to a unified structure before aggregation preserves learned knowledge while enabling standard aggregation techniques.
- **Mechanism:** NetChange transforms model architectures bidirectionally—narrowing/shallowening for distribution to resource-constrained clients, and widening/deepening for aggregation. When widening, neurons are duplicated with weights adjusted to maintain output consistency. When deepening, identity-like layers are inserted (diagonal=1, others=0). This creates a common dimensional space for weighted averaging (FedAvg).
- **Core assumption:** Knowledge encoded in smaller models can be meaningfully expanded into larger architectures without significant distortion, and the union-of-all-structures global model effectively captures collective learning.
- **Evidence anchors:**
  - [abstract] "FedADP enables effective collaboration among clients with differing capabilities, maximizing resource utilization"
  - [section III.B] Algorithm 2 shows To-Wider duplicates neurons and redistributes weights: "vj ← vj/|Mi|" to preserve function
  - [corpus] Weak direct corpus support; related work FedSKD uses knowledge distillation rather than structural transformation
- **Break condition:** If the global model architecture becomes too large relative to the smallest client, repeated narrow→train→widen cycles may cause progressive knowledge degradation (no pruning strategy is validated for managing union-size growth).

### Mechanism 2
- **Claim:** Pruning and redistribution of weights during narrowing operations preserves functional equivalence, allowing low-capability clients to train meaningful models.
- **Mechanism:** When narrowing, excess neurons are removed and their weight contributions redistributed evenly to remaining neurons ("Add s/Ntar to each remaining neuron"). This ensures the transformed model approximates the original function, maintaining gradient quality during local training.
- **Core assumption:** Even weight redistribution approximates functional preservation sufficiently for downstream training.
- **Evidence anchors:**
  - [section III.B, Algorithm 3] "s ← the sum of neurons value after Ntar in r" followed by redistribution
  - [section III.B] "This redistribution...ensures that the model retains its original functionality and performance"
  - [corpus] No direct corpus validation of this redistribution strategy's effectiveness
- **Break condition:** If layer outputs are highly non-linear or sparse, uniform redistribution may distort representations, degrading client training quality.

### Mechanism 3
- **Claim:** Defining the global model as the union of all client architectures maximizes knowledge retention across heterogeneous participants.
- **Mechanism:** The server constructs the global model by taking the structural union (maximum depth and width) across all client models. All clients transform to this maximal structure for aggregation, ensuring no client's unique architecture is excluded.
- **Core assumption:** The union architecture is computationally tractable and doesn't create an intractably large global model; clients can effectively train sub-networks that project into this space.
- **Evidence anchors:**
  - [section III.B] "the system first constructs a global model by taking the union of the structures of all the client models"
  - [section III.B] Example with VGG variants: "The global model would be set to VGG-19-Wider"
  - [corpus] UnifiedFL (arXiv 2510.26350) similarly proposes dynamic unified learning but with different methodology
- **Break condition:** As heterogeneity increases, the union model may grow prohibitively large, creating memory/computation bottlenecks and widening dead neurons for clients with much smaller native architectures.

## Foundational Learning

- **Concept: Net2Net Function-Preserving Transformations**
  - **Why needed here:** NetChange extends Net2Net principles; understanding how network surgery preserves outputs is essential for debugging transformation issues.
  - **Quick check question:** Can you explain why duplicating a neuron and halving outgoing weights preserves layer output?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** FedADP uses FedAvg for aggregation after structural unification; the weighted averaging formula (Eq. 1-2) is core to the framework.
  - **Quick check question:** Given two models with identical architecture but different weights, how does FedAvg combine them using sample counts?

- **Concept: VGG Architecture Family**
  - **Why needed here:** All experiments use VGG variants (VGG-13, VGG-16, VGG-19, widened versions); understanding layer structure enables correct NetChange operations.
  - **Quick check question:** What distinguishes VGG-13 from VGG-19 in terms of layer count, and how would you identify which layers to widen?

## Architecture Onboarding

- **Component map:**
  - Client Side: Local model (variable VGG architecture) -> Local training (SGD, 10 epochs) -> NetChange(To-Deeper/To-Wider) -> Upload
  - Server Side: Receive transformed models -> FedAvg aggregation -> Global model storage -> NetChange(To-Shallower/To-Narrower) per client -> Distribute
  - NetChange Module: To-Wider (Algorithm 2), To-Narrower (Algorithm 3), To-Deeper (identity layer insertion), To-Shallower (layer removal)

- **Critical path:**
  1. Determine global model architecture as union of all client architectures (requires knowing all client configs upfront)
  2. For each round: server narrows/shallows global model to each client's architecture -> client trains -> client widens/deepens to global architecture -> server aggregates
  3. Verify dimensional alignment before FedAvg; mismatched dimensions will crash aggregation

- **Design tradeoffs:**
  - **Union vs. intersection approach:** Union maximizes participation but grows model size; intersection (FlexiFed) wastes non-overlapping knowledge
  - **Weight redistribution strategy:** Uniform (current) is simple but may be suboptimal; importance-weighted redistribution could improve but adds complexity
  - **Assumption:** The paper doesn't address dynamic client arrival/departure or architecture changes mid-training

- **Failure signatures:**
  - **Dimension mismatch during aggregation:** Check NetChange completeness; widening must exactly match target dimensions
  - **Accuracy collapse after many rounds:** Possible knowledge degradation from repeated transformations; add validation checkpoints
  - **Memory overflow at server:** Union model too large; implement architecture capping
  - **Slow convergence on heterogeneous data:** Non-IID data may interact poorly with structural transformations

- **First 3 experiments:**
  1. **Reproduce MNIST baseline with VGG-13/VGG-16/VGG-19 split:** 20 clients, 8 architecture types, 200 rounds. Verify ~0.983 accuracy. This validates the core pipeline.
  2. **Ablation on widening strategy:** Compare uniform weight duplication vs. random initialization for new neurons. Measure accuracy gap to quantify knowledge preservation effectiveness.
  3. **Stress test heterogeneity:** Increase architecture diversity (add significantly different VGG configurations). Observe where union-model size becomes problematic and accuracy degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the NetChange mechanism effectively support non-sequential architectures with skip connections, such as ResNets or Transformers?
- **Basis in paper:** [inferred] The experimental evaluation is restricted to VGG variants (sequential CNNs), and the methodology relies on Net2Net principles for layer-wise deepening and widening.
- **Why unresolved:** The "To-Deeper" operation inserts layers initialized as identity mappings. In architectures with skip connections (e.g., ResNet), inserting layers disrupts the residual path dimensions and mathematical identity, requiring more complex alignment strategies not discussed in the text.
- **What evidence would resolve it:** Experimental results applying FedADP to heterogeneous ResNet or Vision Transformer models to verify if structural alignment preserves convergence and accuracy.

### Open Question 2
- **Question:** How does FedADP scale in terms of server-side memory and communication efficiency as the heterogeneity and number of clients increase?
- **Basis in paper:** [inferred] The experimental setup uses a small fixed client pool (K=20) and a limited set of model variants.
- **Why unresolved:** The framework defines the global model as the "union" of all client structures. In large-scale systems with high structural diversity, this union could produce a global model of prohibitive size, causing communication bottlenecks when distributing or aggregating these expanded models.
- **What evidence would resolve it:** Performance benchmarks analyzing global model size growth and communication latency with larger client cohorts (K > 100) and higher architectural variance.

### Open Question 3
- **Question:** Does the computational overhead of the client-side NetChange operations negate the efficiency benefits for resource-constrained devices?
- **Basis in paper:** [inferred] The paper claims to maximize resource utilization for weak devices, but evaluates only accuracy and convergence rounds, omitting analysis of transformation costs.
- **Why unresolved:** Clients must execute "To-Deeper" and "To-Wider" transformations (Step 4) before uploading. If the target global structure is significantly larger than the local model, the memory and compute required for this structural expansion and weight initialization could burden weak clients.
- **What evidence would resolve it:** Quantitative measurement of the wall-clock time and memory footprint specifically required for the NetChange operations on edge-device hardware.

## Limitations
- **Union model scalability:** The union-of-all-structures approach may create intractably large global models as heterogeneity increases
- **Weight redistribution validation:** Uniform weight redistribution during narrowing lacks direct validation for preserving functional equivalence
- **Transformation overhead:** Client-side NetChange operations may burden resource-constrained devices, particularly when expanding to much larger global architectures

## Confidence

**Confidence Labels:**
- Mechanism 1 (Structural transformation preserving knowledge): Medium - Core transformation logic is specified but redistribution strategy lacks validation
- Mechanism 2 (Union architecture maximizing participation): Medium - Mathematically sound but computational scaling concerns unaddressed
- Empirical results (23.30% accuracy improvement): Low-Medium - Strong claims but reproduction requires resolving architectural ambiguities

## Next Checks
1. Implement ablation study comparing uniform vs. importance-weighted neuron redistribution during narrowing operations to quantify knowledge preservation effectiveness
2. Systematically vary client architecture diversity (from minimal to extreme heterogeneity) to identify the computational scalability boundary of the union approach
3. Test knowledge degradation through repeated narrow→train→widen cycles by measuring accuracy decay over multiple aggregation rounds