---
ver: rpa2
title: 'Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo Language'
arxiv_id: '2502.00421'
source_url: https://arxiv.org/abs/2502.00421
tags:
- oromo
- speech
- language
- dataset
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sagalee, the first open-source Automatic
  Speech Recognition (ASR) dataset for the Oromo language, which addresses the critical
  lack of speech resources for this widely spoken but underrepresented language. The
  dataset was collected through a crowdsourcing initiative using a mobile app, resulting
  in 100 hours of real-world audio recordings paired with transcriptions from 283
  speakers, capturing diverse phonetic variations and acoustic environments.
---

# Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo Language

## Quick Facts
- **arXiv ID:** 2502.00421
- **Source URL:** https://arxiv.org/abs/2502.00421
- **Reference count:** 29
- **Primary result:** Introduced the first open-source ASR dataset for Oromo language (100 hours, 283 speakers), achieving 15.32% WER with Conformer hybrid CTC-AED loss and 10.82% WER with Whisper fine-tuning

## Executive Summary
This paper introduces Sagalee, the first open-source Automatic Speech Recognition (ASR) dataset for the Oromo language, which addresses the critical lack of speech resources for this widely spoken but underrepresented language. The dataset was collected through a crowdsourcing initiative using a mobile app, resulting in 100 hours of real-world audio recordings paired with transcriptions from 283 speakers, capturing diverse phonetic variations and acoustic environments. The dataset's applicability for ASR tasks was demonstrated through experiments using Conformer models, achieving a Word Error Rate (WER) of 15.32% with hybrid CTC and AED loss and 18.74% with pure CTC loss. Additionally, fine-tuning the Whisper model resulted in a significantly improved WER of 10.82%. These results establish baselines for Oromo ASR and highlight the potential of Sagalee for advancing speech recognition research in low-resource languages.

## Method Summary
The Sagalee dataset was collected through a crowdsourcing initiative using a mobile app, gathering 100 hours of audio recordings paired with transcriptions from 283 speakers. The dataset was evaluated using two approaches: (1) training Conformer models from scratch with hybrid CTC-AED loss using the WeNet toolkit, and (2) fine-tuning the pre-trained Whisper Large-v3 model. The Conformer experiments used 12 encoder layers and 6 decoder layers with BPE tokenization (nbpe=500), achieving WERs of 15.32% with hybrid loss and 18.74% with pure CTC. Fine-tuning Whisper Large-v3 with batch_size=2, lr=1e-5 for 15 epochs achieved a significantly lower WER of 10.82%.

## Key Results
- Sagalee dataset: 100 hours of real-world Oromo speech from 283 speakers with 53,573 utterances
- Conformer baseline: 15.32% WER with hybrid CTC-AED loss, 18.74% WER with pure CTC loss
- Whisper fine-tuning: 10.82% WER, demonstrating the effectiveness of pre-trained multilingual models for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning large-scale multilingual pre-trained models on limited target-language data yields lower WER than training comparably-sized models from scratch. The Whisper Large-v3 model's 1.55B parameters encode transferable acoustic-phonetic representations from 680K hours of multilingual weak supervision. Fine-tuning re-aligns these representations to Oromo phonology via gradient updates across all layers, leveraging cross-linguistic feature sharing rather than learning acoustic patterns de novo. Core assumption: Oromo acoustic features share sufficient structure with languages in Whisper's pre-training corpus to enable effective transfer. Evidence: fine-tuning Whisper achieved 10.82% WER vs. 15.32-18.74% for from-scratch models. Break condition: If Oromo phonology diverges fundamentally from Afroasiatic relatives in Whisper's training distribution, transfer gains may degrade to near-scratch performance.

### Mechanism 2
Crowdsourced mobile recording produces acoustically diverse training data that improves real-world generalization. Recording via personal devices at 24kHz/16-bit across unrestricted environments captures natural variation in channel characteristics, background noise, and speaker demographics. This heterogeneity acts as implicit regularization, reducing overfitting to specific acoustic conditions. Core assumption: Crowdsourced "real-world" noise distributions approximate deployment conditions better than studio recordings. Evidence: dataset explicitly captures "diverse phonetic variations and acoustic environments" without recording restrictions. Break condition: If crowdsourcing introduces systematic biases (e.g., over-representation of specific age groups or device types), models may underperform on under-represented subpopulations.

### Mechanism 3
Hybrid CTC-AED loss outperforms pure CTC loss for Conformer models in low-resource settings. CTC provides frame-independent alignment supervision while AED learns attention-based sequence dependencies. The hybrid formulation combines CTC's regularization against degenerate solutions with AED's language modeling capacity, mitigating data scarcity constraints. Core assumption: The λ weighting appropriately balances alignment and attention objectives for Oromo's agglutinative morphology. Evidence: hybrid CTC-AED achieved 15.32% WER vs. 18.74% for pure CTC. Break condition: If AED attention becomes unstable with limited data, hybrid loss may converge slower or to worse local minima than pure CTC.

## Foundational Learning

- **Connectionist Temporal Classification (CTC)**: Enables training without frame-level alignment, critical when transcription resources are limited to utterance-level text. Quick check: Given an input sequence of 100 acoustic frames and an output of 15 characters, how does CTC handle the length mismatch during gradient computation?

- **Subword Tokenization (BPE)**: The paper experiments with BPE vocabulary sizes (50, 100, 500, 5000) and finds nbpe=500 optimal. Understanding subword units is essential for agglutinative languages like Oromo where morphological suffixes are common. Quick check: Why might a BPE vocabulary of 5000 produce excessive `<unk>` tokens in a 100-hour dataset, and how does reducing to 500 help?

- **Conformer Architecture**: Integrates convolution (local features) with self-attention (global context). The paper uses 12 encoder layers with 4 attention heads and a CNN kernel of 15. Understanding this hybrid design informs architectural decisions for similar low-resource deployments. Quick check: What inductive bias does the convolution module add that pure Transformers lack for speech processing?

## Architecture Onboarding

- **Component map:** 24kHz mono audio → 16-bit quantization → FBank/Mel features → Conformer encoder (12 layers, 256 dim, 4 heads, kernel=15) → CTC projection + Transformer decoder (6 layers, 4 heads) → BPE tokenizer (nbpe=500) → hybrid CTC-AED loss → WER evaluation

- **Critical path:**
  1. Validate data splits: 93.6h train / 4.2h dev / 2.4h test
  2. Tune BPE vocabulary (50→500→5000) on dev set before full training
  3. Monitor both CTC and AED losses separately to diagnose λ balance issues
  4. If training from scratch exceeds ~20% WER, pivot to Whisper fine-tuning

- **Design tradeoffs:**
  - From-scratch Conformer vs. Whisper fine-tuning: From-scratch requires ~100h for ~15-19% WER; Whisper fine-tuning achieves ~10.8% WER but requires 1.55B parameter inference cost
  - BPE size: nbpe=5000 causes `<unk>` proliferation; nbpe=50 under-represent vocabulary; 500 is empirically optimal
  - Loss function: Hybrid CTC-AED (15.32% WER) vs. pure CTC (18.74%) — hybrid adds decoder complexity and memory overhead

- **Failure signatures:**
  - Excessive `<unk>` tokens → BPE vocabulary too small or mismatched to Oromo orthography
  - Training loss plateaus early with high WER → potential data quality issues (misaligned transcriptions, corrupted audio)
  - Large gap between dev and test WER → overfitting to dev set or distribution shift

- **First 3 experiments:**
  1. Reproduce baseline: Train Conformer CTC (nbpe=500, 200 epochs) and verify WER ≈ 18.74% on test split
  2. Ablate BPE size: Compare nbpe ∈ {100, 500, 1000} on dev set; confirm 500 remains optimal
  3. Fine-tune Whisper: Replicate Whisper Large-v3 fine-tuning (lr=1e-5, batch=2, 15 epochs, 3-checkpoint average) and verify WER ≈ 10.82%

## Open Questions the Paper Calls Out

- **Can expanding Sagalee beyond 100 hours significantly close the performance gap between models trained from scratch and fine-tuned large multilingual models?** The current 15.32–18.74% WER from scratch vs. 10.82% from Whisper fine-tuning shows a substantial gap; it is unknown how much additional data would be required for from-scratch models to match fine-tuned performance. Train Conformer models on progressively larger versions of Sagalee (e.g., 200h, 500h) and report WER trajectories relative to the Whisper fine-tuning baseline.

- **How does ASR performance vary across the documented Oromo dialect groups (e.g., Wallaggaa-Maccaa, Harargee, Boorana-Gabra)?** The paper collects dialect metadata (7 categories) and shows uneven dialect distribution, but reports only aggregate WER without dialect-specific evaluation. Report per-dialect WER on the test set and/or conduct error analysis by dialect group.

- **Can Sagalee support robust text-to-speech synthesis and speaker recognition tasks given its current size and speaker diversity?** The dataset was designed for ASR with read speech and 283 speakers; adequacy for TTS (which often needs multiple hours per speaker) and speaker recognition (which benefits from many sessions per speaker) remains untested. Build TTS and speaker recognition benchmarks on Sagalee splits and report standard metrics (e.g., MOS for TTS; EER/accuracy for speaker recognition).

## Limitations
- Limited empirical validation of dataset quality - no systematic analysis of transcription accuracy or speaker demographic representation
- Missing architectural hyperparameters - hybrid CTC-AED loss lacks critical λ weight parameter specification
- Whisper fine-tuning scalability - does not evaluate whether smaller Whisper variants could achieve similar gains with reduced deployment costs

## Confidence
- **High confidence** - Dataset creation methodology, resulting dataset specifications, and basic evaluation framework are clearly documented and reproducible
- **Medium confidence** - Reported WER results are internally consistent but full replication requires addressing missing hyperparameters (λ weight, Conformer optimizer settings)
- **Low confidence** - Claims about dataset quality and real-world applicability lack supporting evidence and demographic analysis

## Next Checks
- **Check 1:** Replicate BPE sensitivity analysis - Train Conformer models with BPE vocabularies of 100, 500, and 1000 on the dev set. Verify that nbpe=500 consistently outperforms other values and produces acceptable <unk> token rates (<5%).
- **Check 2:** Ablate CTC-AED weight λ - Systematically vary λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} while keeping other hyperparameters constant. Identify the λ value that minimizes dev WER and verify whether the reported 15.32% WER corresponds to this optimal setting.
- **Check 3:** Evaluate demographic bias - Using available metadata (speaker age, gender, recording location if captured), measure WER variance across demographic subgroups. Compare performance across urban vs. rural speakers, different age cohorts, and gender groups to identify potential representation gaps in the crowdsourced dataset.