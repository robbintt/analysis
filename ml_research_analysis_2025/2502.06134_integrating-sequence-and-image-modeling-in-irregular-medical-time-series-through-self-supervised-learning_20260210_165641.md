---
ver: rpa2
title: Integrating Sequence and Image Modeling in Irregular Medical Time Series Through
  Self-Supervised Learning
arxiv_id: '2502.06134'
source_url: https://arxiv.org/abs/2502.06134
tags:
- time
- series
- image
- sequence
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses classification of irregular medical time series
  with substantial missing values by integrating sequence and image modeling approaches.
  The proposed framework combines a bidirectional RNN-based generator-discriminator
  for sequence imputation with a pre-trained Swin Transformer for image representations
  of time series data.
---

# Integrating Sequence and Image Modeling in Irregular Medical Time Series Through Self-Supervised Learning

## Quick Facts
- arXiv ID: 2502.06134
- Source URL: https://arxiv.org/abs/2502.06134
- Reference count: 17
- Primary result: Framework combining sequence and image modeling outperforms 7 baselines on clinical datasets with 60-94.9% missingness, achieving 3.1% accuracy improvement on PAM and 2.3-5.8% AUROC/AUPRC gains on P12/P19

## Executive Summary
This paper addresses classification of irregularly sampled medical time series with high missingness by integrating sequence and image modeling approaches. The proposed framework combines a bidirectional RNN-based generator-discriminator for sequence imputation with a pre-trained Swin Transformer for image representations of time series data. Three self-supervised learning strategies are employed: inter-sequence contrastive loss, sequence-image contrastive loss with margin, and clustering-based loss. The approach is evaluated on three real-world clinical datasets (PAM, P12, P19) with high missing rates (60-94.9%). Experimental results show that the method outperforms seven state-of-the-art baselines, achieving accuracy improvements of 3.1% on PAM and AUROC/AUPRC improvements of 0.9%/1.1% on P12 and 2.3%/5.8% on P19. Further testing with increased missingness through leave-sensors-out and leave-samples-out techniques demonstrates superior robustness compared to baselines.

## Method Summary
The method employs a two-branch architecture where the sequence branch uses a 4-layer BiGRU generator with decay-based imputation and a 5-layer RNN discriminator for adversarial learning, while the image branch transforms time series into six visual representations (line graphs, frequency spectrums, GAF/GADF, MTF, recurrence plots) processed by a pre-trained Swin Transformer. Joint representations are learned through contrastive learning: NT-Xent enforces consistency between forward/backward imputations and original/imputed sequences; sequence-image contrastive loss with margin pushes same-sample representations closer while separating inter-class pairs; clustering loss aligns semantically similar samples. The model is trained with three-stage updates per epoch: (1) update discriminator with L_dis, (2) update generator with L_adv + αL_rec, (3) update full model with L_clf + β1L_cont + β2L_cluster.

## Key Results
- Accuracy improvements of 3.1% on PAM dataset compared to best baseline
- AUROC improvements of 0.9% on P12 and 2.3% on P19 datasets
- AUPRC improvements of 1.1% on P12 and 5.8% on P19 datasets
- Robustness testing shows model maintains accuracy within 5% drop under 30% additional missingness, versus >10% drop for baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining sequence and image representations improves robustness for irregular medical time series with high missingness.
- Mechanism: Sequence modeling captures temporal dynamics via bidirectional RNN with decay-based imputation; image modeling (via six transformations + Swin Transformer) captures inter-sensor relationships and frequency-domain patterns that remain informative despite sparsity. Joint representation fuses complementary information through contrastive learning.
- Core assumption: Neither representation alone sufficiently handles extreme missingness; their fusion captures non-overlapping signal structure.
- Evidence anchors:
  - [abstract] "We propose a joint learning framework that incorporates both sequence and image representations."
  - [section: Approach, Joint Representations] "To ensure both the quality and consistency of the joint representation, we implement contrastive learning within each batch."
  - [corpus] MedFuse (arxiv 2511.09247) similarly argues for embedding fusion strategies but focuses on multiplicative embedding fusion, not image modalities—suggesting the fusion concept is convergent but the specific sequence-image pairing is novel here.

### Mechanism 2
- Claim: Self-supervised contrastive losses stabilize imputation and improve cross-modal alignment.
- Mechanism: NT-Xent loss enforces consistency between forward/backward imputations and original/imputed sequences; sequence-image contrastive loss with margin pushes same-sample representations closer while separating inter-class pairs more aggressively; clustering loss aligns semantically similar samples across batches.
- Core assumption: Positive pairs (same sample, different modality) share latent structure worth aligning; inter-class margin improves separability.
- Evidence anchors:
  - [section: Sequence Branch with Imputation] "We use NT-Xent to enforce consistency between the forward and backward predictions, as well as between the original and imputed sequences."
  - [section: Joint Representations] "we introduce an additional margin m for these special negative pairs, enforce the model to exert greater effort to distinguish them"
  - [corpus] VITAL (arxiv 2509.22121) uses variable-aware representation learning with LLMs for missingness, but does not employ contrastive sequence-image alignment—suggesting the specific cross-modal contrastive approach is underexplored.

### Mechanism 3
- Claim: Multiple image transformation types provide complementary invariances to missingness.
- Mechanism: Line graphs capture raw temporal trends; frequency spectrums (via FFT) are robust to extreme missingness; GAF/MTF/Recurrence plots encode temporal correlations and periodicity insensitive to time-step distribution. Swin Transformer's hierarchical attention extracts multi-scale features.
- Core assumption: At least some transformations remain informative under high missingness; pre-trained visual features transfer to medical time series images.
- Evidence anchors:
  - [section: Imaging Time Series] "Frequency Spectrums are generated based on the Fourier transform, considering that frequency domain signals tend to be more robust in cases of extreme data missingness."
  - [section: Imaging Time Series] "Markov Transition Fields... are insensitive to the distribution of the time series and temporal step information, allowing them to effectively capture correlations between observations with substantial missing data."
  - [corpus] Time-IMM (arxiv 2506.10412) notes that real-world multimodal time series are irregular and messy, but focuses on benchmarking—no direct evidence for this specific multi-transformation strategy.

## Foundational Learning

- Concept: **Contrastive learning (NT-Xent)**
  - Why needed here: Core to both intra-sequence imputation stability and cross-modal (sequence-image) alignment; requires understanding of positive/negative pair construction and temperature scaling.
  - Quick check question: Can you explain why same-sample sequence-image pairs are treated as positives and how margin affects negative pairs?

- Concept: **Adversarial imputation with generator-discriminator**
  - Why needed here: The sequence branch uses a GAN-style setup where the generator imputes missing values and the discriminator distinguishes real from imputed observations.
  - Quick check question: How does the decay mechanism (γt) in the BiRNN generator modulate hidden state retention over missing intervals?

- Concept: **Swin Transformer hierarchical attention**
  - Why needed here: Image encoder uses window-based and shifted-window attention; understanding patch merging and multi-scale representation extraction is essential for debugging image branch failures.
  - Quick check question: What is the difference between W-MSA and SW-MSA, and why does the model use both?

## Architecture Onboarding

- Component map:
  Sequence encoder: 4-layer BiGRU generator (128 units) + 5-layer RNN discriminator ({128,64,16,64,128})
  Image encoder: Pre-trained Swin Transformer (ImageNet-21K), patch size 4, window size 7
  Joint projection: 3-layer MLP ({1024,512,1024}) concatenating sequence and image representations
  SSL losses: L_rec (NT-Xent for imputation), L_cont (sequence-image contrastive with margin), L_cluster (K-means on fused features)
  Classification head: Cross-entropy (PAM) or Focal Loss (P12/P19)

- Critical path:
  1. Input X, mask M, lag matrix δ → Generator G → imputed X′ → sequence representation s
  2. X → six image transformations → I → Swin Transformer → image representation v
  3. Concatenate s, v → joint projection → u → classification + SSL losses

- Design tradeoffs:
  - Using six image transformations increases compute but provides robustness; ablation shows image-only and sequence-only underperform joint by 2.6-4.5% F1.
  - NT-Xent vs MSE for reconstruction: NT-Xent yields +0.8% accuracy over MSE in ablation.
  - Clustering loss requires epoch-level updates; may slow training but improves cross-batch semantic alignment.

- Failure signatures:
  - If imputed values are unrealistic, discriminator may dominate (check L_dis vs L_adv balance).
  - If image branch learns nothing, check transformation outputs for degenerate patterns (all-black or uniform images indicate insufficient signal).
  - If contrastive loss collapses, verify positive/negative pair construction and margin settings.

- First 3 experiments:
  1. **Baseline sanity check**: Run sequence-only and image-only variants on PAM; expect ~93-95% accuracy vs 98.3% for full model.
  2. **Ablation on SSL losses**: Disable L_cont and L_cluster separately; expect ~1% accuracy drop each per ablation table.
  3. **Robustness stress test**: Apply leave-sensors-out at 30% additional missingness on PAM; expect accuracy drop <5% for full model vs >10% for baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the six implemented image transformation types provide unique, complementary information, or is there significant redundancy among them?
- Basis in paper: [inferred] The ablation study evaluates the "image" branch as a holistic component against the "sequence" branch but does not isolate the contribution of individual transformations (e.g., Recurrence Plots vs. Frequency Spectrums).
- Why unresolved: It is unclear if the performance gain relies on the diversity of all six views or if a subset dominates the learning process, potentially allowing for model simplification.
- What evidence would resolve it: A fine-grained ablation study reporting classification performance when incrementally adding or removing specific image transformation types.

### Open Question 2
- Question: How robust is the framework under "Missing Not At Random" (MNAR) mechanisms where the missingness is correlated with the outcome label?
- Basis in paper: [inferred] The robustness tests utilize "leave-sensors-out" and "leave-samples-out," which simulate random or structural missingness but may not capture scenarios where data is missing specifically because of the patient's critical condition (MNAR).
- Why unresolved: Medical data often exhibits MNAR patterns (e.g., a test not performed because the patient is too unstable), and the model's reliance on imputation may fail if the missingness pattern itself is a predictive feature not captured by the masking strategy.
- What evidence would resolve it: Evaluation on synthetic or semi-synthetic datasets where the probability of missingness is explicitly tied to the target class or hidden state.

### Open Question 3
- Question: Can the learned joint representations be interpreted to explain specific clinical predictions?
- Basis in paper: [inferred] The authors conduct a "Clinical Turing test" to validate the visual realism of imputed signals but do not provide analysis on the interpretability of the final joint representation used for classification.
- Why unresolved: While the model successfully fuses sequence and image data, the opacity of the resulting embedding poses a barrier to clinical adoption where reasoning is required.
- What evidence would resolve it: Implementation of attention visualization or gradient-based attribution methods to map classification decisions back to specific time steps or image regions.

## Limitations

- The method's performance gains are demonstrated on three specific clinical datasets with relatively controlled conditions, limiting generalizability.
- Exact hyperparameter settings for loss weights (α, β1, β2), margin m, temperature τ, and clustering parameters (k, τc) are not fully specified in the main text.
- The computational overhead of six image transformations plus Swin Transformer may limit scalability to larger datasets or real-time applications.

## Confidence

- **High Confidence**: The framework architecture combining sequence and image representations is clearly described and implemented; the ablation study results showing sequence-only and image-only variants underperforming the joint model are reproducible.
- **Medium Confidence**: The quantitative performance improvements over baselines are well-documented, though the exact implementation details of hyperparameters and image transformation parameters require external verification.
- **Medium Confidence**: The robustness claims under increased missingness are supported by the experimental results, but the leave-sensors-out and leave-samples-out methodologies would benefit from more detailed description.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary α, β1, β2, margin m, and temperature τ across a grid search to determine optimal settings and quantify performance sensitivity to these parameters.
2. **Extreme Missingness Test**: Apply the method to a held-out test set with artificially increased missingness beyond 95% to stress-test the claimed robustness and identify the breaking point.
3. **Computational Efficiency Benchmarking**: Measure training time, inference latency, and memory usage for the full model versus baselines, and evaluate whether the performance gains justify the additional computational overhead.