---
ver: rpa2
title: Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade
  Cameras
arxiv_id: '2510.09230'
source_url: https://arxiv.org/abs/2510.09230
tags:
- video
- diagnosis
- shoulder
- judgment
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies multimodal large language models to preliminary
  diagnosis of shoulder disorders using videos from consumer-grade cameras. It proposes
  a Hybrid Motion Video Diagnosis framework (HMVDx) that separates action understanding
  and disease diagnosis into two MLLMs, and introduces a Usability Index to evaluate
  model performance across the full diagnostic process.
---

# Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras

## Quick Facts
- arXiv ID: 2510.09230
- Source URL: https://arxiv.org/abs/2510.09230
- Reference count: 5
- Primary result: HMVDx framework achieves 79.6% higher accuracy than direct video diagnosis for shoulder disorder screening

## Executive Summary
This study applies multimodal large language models to preliminary diagnosis of shoulder disorders using videos from consumer-grade cameras. It proposes a Hybrid Motion Video Diagnosis framework (HMVDx) that separates action understanding and disease diagnosis into two MLLMs, and introduces a Usability Index to evaluate model performance across the full diagnostic process. Results show HMVDx accuracy increased by 79.6% compared to direct video diagnosis, and the Usability Index rose to 0.81, significantly outperforming both the baseline GPT-4o model (0.48) and direct video diagnosis (0.53). The work highlights potential for low-cost, scalable AI-assisted medical diagnostics in resource-limited settings.

## Method Summary
The study collected 761 shoulder movement videos (504 with disorders, 257 healthy) using consumer-grade cameras. The Hybrid Motion Video Diagnosis (HMVDx) framework employs two MLLMs working in parallel: Gemini-1.5-Pro for action understanding and DeepSeek-R1 for diagnostic reasoning. Videos are processed through Prompt-B, which generates detailed action descriptions using relative anatomical landmark references rather than numerical angles. These descriptions feed into Prompt-C, which applies diagnostic rules to produce final diagnoses. The framework was compared against GPT-4o (frame-by-frame processing) and direct video diagnosis (Gemini-1.5-Pro only) baselines.

## Key Results
- HMVDx achieved 0.88 accuracy in final judgment scenario (Scenario 1), up from 0.49 for direct video diagnosis
- Usability Index reached 0.81, significantly higher than GPT-4o (0.48) and direct video diagnosis (0.53)
- Task decomposition improved accuracy by 79.6% compared to single-model direct video diagnosis
- In strict whole-process evaluation (Scenario 3), HMVDx maintained F1 score of 0.19, showing limitations in action recognition

## Why This Works (Mechanism)

### Mechanism 1
Task decomposition improves diagnostic accuracy by separating visual understanding from clinical reasoning. HMVDx assigns Gemini-1.5-Pro to generate detailed action descriptions from video, then feeds these descriptions to DeepSeek-R1 for rule-based diagnostic reasoning. This division prevents information loss that occurs when a single model must both interpret visual motion and perform clinical reasoning simultaneously.

### Mechanism 2
Relative position descriptors outperform numerical quantification for movement assessment in LLMs. The Motion Trajectories Prompt Framework replaces absolute measurements (e.g., "flex 30 degrees") with relative references to anatomical landmarks (e.g., "higher than the top of the head"), which better aligns with how MLLMs process spatial information through next-token prediction.

### Mechanism 3
Multi-stage evaluation reveals performance gaps that single-metric assessments miss. The three-level constraint scenarios (final judgment only → judgment + behavior rationality → full process including action recognition) progressively filter outputs, showing that HMVDx's 0.88 accuracy in Scenario 1 drops to 0.27 in Scenario 3, exposing weaknesses in action recognition that aggregate metrics hide.

## Foundational Learning

- **Multimodal video understanding vs. frame-by-frame processing**: Why needed here: The paper explicitly contrasts GPT-4o's frame-based approach with Gemini-1.5-Pro's native video processing, showing temporal modeling is critical for movement analysis. Quick check: Can you explain why processing video as sequential frames loses information relevant to diagnosing limited range of motion?

- **Chain-of-Thought (CoT) prompting for medical reasoning**: Why needed here: The diagnostic prompts explicitly guide models through action recognition → movement diagnosis → final diagnosis, mimicking physician reasoning. Quick check: How does CoT prompting differ from direct classification prompts when diagnosing whether a shoulder movement indicates pathology?

- **Precision-recall tradeoffs in screening contexts**: Why needed here: The paper reports HMVDx precision of 0.988 with recall of 0.833 in Scenario 1, raising questions about missed diagnoses in preliminary screening. Quick check: In a resource-limited screening context, would you prioritize high precision or high recall for shoulder disorder detection?

## Architecture Onboarding

- **Component map**: Video Input Pipeline -> Action Understanding Module -> Diagnostic Reasoning Module -> Evaluation Layer
- **Critical path**: Video quality → landmark visibility → action description completeness → rule matching accuracy → final diagnosis. Failure at action description stage propagates irrecoverably.
- **Design tradeoffs**:
  - Two-model vs. single-model: Two-model approach adds latency but improves Scenario 2 F1 from 0.18 to 0.68
  - Relative vs. absolute descriptions: Reduces numerical hallucination but requires visible anatomical landmarks
  - Strict vs. lenient evaluation: Scenario 3 reveals true limitations but makes all current methods appear inadequate
- **Failure signatures**:
  - Compound action descriptions that reasoning model cannot decompose
  - Front-facing camera angle preventing assessment of limb verticality for external rotation
  - Back-view videos with high waist misrecognition rates
  - Left/right directional confusion in model outputs
- **First 3 experiments**:
  1. Baseline comparison: Run your video dataset through GPT-4o, Gemini-1.5-Pro, and HMVDx to replicate accuracy differences
  2. Prompt ablation: Test HMVDx with numerical quantification prompts instead of relative position prompts
  3. Error analysis on Scenario 3 failures: Manually annotate where action recognition fails to determine whether improvements should target vision model or description format

## Open Questions the Paper Calls Out

### Open Question 1
Can the HMVDx framework be effectively generalized to diagnose other visual musculoskeletal conditions, such as lumbar muscle strain or tenosynovitis? The authors state that "a series of diseases that can be interpreted based on visual representations... will have the opportunity to be transformed into standardized intelligent screening tools." This remains unresolved as the current study validated exclusively on shoulder joint disorders.

### Open Question 2
To what extent would Supervised Fine-Tuning (SFT) or Retrieval Augmented Generation (RAG) improve performance in the strict "whole-process constraint" scenario (Scenario 3)? The authors note that relying on inherent capabilities resulted in a low F1 score (0.19) in Scenario 3, suggesting that "if more advanced optimization technologies are introduced... HMVDx is expected to further improve."

### Open Question 3
How can the model's "awareness of action intention" be improved to prevent misjudging preparatory movements as formal clinical actions? Qualitative analysis identified that the model struggles to distinguish between process actions (e.g., raising arms) and the formal clinical action (e.g., holding the head) due to a lack of action intention awareness.

## Limitations

- Dataset accessibility: The hospital-collaborated dataset of 761 videos is not publicly available, limiting external validation and raising concerns about selection bias
- Prompt design reproducibility: Exact text of Prompt-B and Prompt-C is not provided, making precise replication impossible despite described design principles
- Clinical validation gap: Study evaluates against reference standards but lacks physician validation or assessment of clinical utility and patient outcomes

## Confidence

- **High Confidence** (Task decomposition mechanism): The reported 79.6% accuracy improvements and process-based evaluation provide strong empirical support for separating action understanding from diagnostic reasoning
- **Medium Confidence** (Relative position descriptors): While performance superiority is demonstrated, lack of direct comparison data and corpus validation makes the claim less certain
- **Low Confidence** (Usability Index as comprehensive metric): The novel index introduces subjective scoring for intermediate reasoning steps that may not reliably capture true clinical diagnostic capability across different evaluators

## Next Checks

1. **Prompt Template Release and Replication**: Request exact text of Prompt-B and Prompt-C from authors, then conduct controlled experiments comparing relative position descriptions against numerical quantification and alternative prompt formulations

2. **Cross-Dataset Generalization Test**: Apply HMVDx to independent, publicly available shoulder movement video datasets to assess whether 79.6% accuracy improvement generalizes beyond original hospital dataset

3. **Physician-in-the-Loop Validation**: Conduct blind study where physicians evaluate HMVDx outputs alongside their own assessments of same videos, measuring inter-rater agreement and identifying clinical scenarios where model reasoning diverges from expert judgment