---
ver: rpa2
title: Enhancing Safe and Controllable Protein Generation via Knowledge Preference
  Optimization
arxiv_id: '2507.10923'
source_url: https://arxiv.org/abs/2507.10923
tags:
- protein
- proteins
- harmful
- safety
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KPO, a framework that improves the safety
  of protein language models by incorporating domain-specific knowledge from a Protein
  Safety Knowledge Graph. KPO prunes this graph to retain key safety-related nodes,
  then fine-tunes PLMs using preference optimization to reduce the generation of harmful
  proteins while preserving functionality.
---

# Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization

## Quick Facts
- **arXiv ID:** 2507.10923
- **Source URL:** https://arxiv.org/abs/2507.10923
- **Reference count:** 22
- **Key outcome:** Introduces KPO framework that significantly reduces harmful protein generation in PLMs while maintaining functionality.

## Executive Summary
This paper presents KPO (Knowledge Preference Optimization), a novel framework for enhancing the safety of protein language models (PLMs). KPO integrates a Protein Safety Knowledge Graph (PSKG) with preference optimization to reduce harmful protein generation while preserving functional performance. The approach combines graph-based knowledge representation with direct preference optimization, demonstrating substantial improvements in safety metrics across multiple PLM architectures including ProtGPT2, ProGen2, and InstructProtein.

## Method Summary
KPO fine-tunes PLMs using a Protein Safety Knowledge Graph that encodes relationships between harmful and benign proteins via Gene Ontology terms. The method employs a weighted pruning algorithm to make the large-scale graph computationally tractable, retaining the most informative safety-relevant nodes. Preference pairs are constructed by finding benign proteins structurally similar to harmful ones, and these pairs are used in Direct Preference Optimization (DPO) to shift the model's generation distribution away from harmful sequence space while maintaining functional capabilities.

## Key Results
- KPO significantly reduces sequence similarity to harmful proteins (BLAST/MMseqs2 scores decrease substantially)
- Toxicity predictions (ToxinPred3 scores) show marked improvement compared to baseline PLMs
- Functional performance on benchmark datasets (GB1, PhoQ, UBC9, GFP) is maintained or improved
- The approach generalizes across different PLM architectures (ProtGPT2, ProGen2, InstructProtein)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating a structured, domain-specific knowledge graph allows the model to learn fine-grained distinctions between harmful and benign proteins that are not separable by sequence similarity alone.
- **Mechanism:** The Protein Safety Knowledge Graph (PSKG) encodes biochemical properties and relationships (e.g., via Gene Ontology terms) connecting harmful proteins (e.g., toxins) with benign counterparts. This graph-based structure provides indirect relational information (a harmful protein and a benign protein sharing a GO term) which is richer than direct sequence alignment. The model uses this to construct preference pairs for optimization, learning to prefer sequences that are functionally similar to harmful ones but lack their toxic properties.
- **Core assumption:** The assumption is that harmfulness is not solely a function of primary sequence similarity but also of higher-order functional relationships which can be captured in a knowledge graph. The model assumes the PSKG accurately represents these relationships.
- **Evidence anchors:**
  - [abstract] The abstract states KPO "integrates prior knowledge via a Protein Safety Knowledge Graph" and uses a "graph pruning strategy to identify preferred sequences."
  - [section 4.1] The paper details the construction of the PSKG from UniProt and Swiss-Prot, defining indirect relationships between harmful ($P_H$) and benign ($P_B$) proteins mediated by shared GO terms ($g_z$), forming triples $(p_i^H, g_z, p_j^B)$.
  - [corpus] Weak. The related papers focus on preference optimization for attributes or energy, not specifically on using a knowledge graph for safety alignment. There is no direct corpus evidence for this specific knowledge graph mechanism.
- **Break condition:** The mechanism breaks if the relationships in the PSKG are sparse, noisy, or incorrect, preventing the formation of meaningful preference pairs. It would also fail if the model cannot learn from these graph-derived preferences or if functional similarity does not correlate with safety differences.

### Mechanism 2
- **Claim:** A weighted metric-based pruning algorithm is necessary and effective for making the large-scale PSKG computationally tractable while retaining the most critical safety-relevant information.
- **Mechanism:** The full PSKG contains hundreds of thousands of nodes, which is computationally prohibitive. The proposed pruning algorithm calculates a weighted importance score $S(p_j^B)$ for each benign protein node. This score is a combination of its connection to high-scoring Gene Ontology (GO) nodes ($C_{GO}$) and its degree centrality ($C_{Deg}$) in the graph. High-scoring GO nodes are identified by their ability to "bridge" harmful and benign proteins. By pruning nodes with low scores, the graph is reduced in size, and the fine-tuning process focuses only on the most informative examples for creating preference pairs.
- **Core assumption:** This mechanism assumes that nodes with higher bridging centrality and degree centrality are more critical for teaching the model to differentiate between harmful and benign sequences. It assumes that a pruned subgraph retains the essential "safety boundary" information of the full graph.
- **Evidence anchors:**
  - [abstract] The abstract mentions an "efficient graph pruning strategy to identify preferred sequences."
  - [section 4.2] This section details the pruning process, including the formulas for node importance score (Eq. 1), harmful-benign bridging degree (Eq. 2), and neighbor breadth (Eq. 3). It explicitly states the method "retains the most informative benign protein nodes."
  - [corpus] Weak. None of the related papers appear to address knowledge graph pruning for protein language models.
- **Break condition:** This mechanism fails if the pruning criteria (weighted scores) incorrectly discard critical nodes, thereby creating a "blind spot" in the model's safety knowledge. It also fails if the computational savings from pruning do not outweigh the loss of information.

### Mechanism 3
- **Claim:** Direct Preference Optimization (DPO) fine-tunes the PLM using carefully constructed preference pairs to shift its generation distribution away from harmful sequence space without compromising functional performance.
- **Mechanism:** The KPO framework uses the pruned PSKG to find benign proteins ($p_j^B$) that are structurally and functionally similar to harmful ones ($p_i^H$). This similarity is measured using both graph-based distance (shortest path) and embedding-based similarity (cosine similarity of node embeddings from TransE). These pairs form the basis for DPO, where the model is optimized to increase the likelihood of generating the benign sequence and decrease the likelihood of generating the harmful one. This directly aligns the model's generative preferences with safety constraints.
- **Core assumption:** The mechanism assumes that pushing the model's probability mass away from a specific harmful sequence and toward a similar benign sequence will generalize, causing the model to avoid the broader "harmful region" of sequence space. It assumes this can be done without catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] The abstract states KPO "employs reinforcement learning [DPO] to minimize the risk of generating harmful proteins."
  - [section 4.3] This section details how preference pairs are constructed using combined similarity scores (Eq. 8) and then used in the KPO loss function (Eq. 9) for fine-tuning.
  - [corpus] Moderate. The corpus contains multiple papers applying DPO to protein models for goals like stability (38575, 33766) and functional annotation (37098), supporting the general efficacy of DPO in this domain.
- **Break condition:** The mechanism breaks if the preference pairs are not of high quality (e.g., the benign protein is not functionally similar, or the harmful protein is mislabeled). It would also fail if the DPO process is too aggressive, causing the model to lose generative diversity.

## Foundational Learning

- **Concept: Knowledge Graphs (KGs) and Node Embeddings**
  - **Why needed here:** This paper is built around a Protein Safety Knowledge Graph (PSKG). Understanding how a KG is a structured representation of entities (proteins, GO terms) and their relationships is essential. The method uses node embeddings (from TransE) to quantify similarity, so one must grasp how a graph structure can be converted into a vector space where proximity implies semantic relatedness.
  - **Quick check question:** Given a knowledge graph with proteins and GO terms as nodes, how would the TransE algorithm represent a "harmful protein" node that is connected to a "membrane lysis" GO term node in the embedding space?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** DPO is the core learning algorithm for fine-tuning the PLM. Unlike traditional RLHF, which trains a separate reward model, DPO optimizes the policy (the PLM) directly using a classification loss on preference pairs. A reader must understand how this loss function works to increase the probability of a "preferred" output relative to a "dispreferred" one.
  - **Quick check question:** If a DPO model is presented with a prompt, a preferred response $y_w$, and a dispreferred response $y_l$, what happens to the log-probability of $y_w$ and $y_l$ under the fine-tuned model, relative to the initial model?

- **Concept: Generative Protein Language Models (PLMs)**
  - **Why needed here:** The object being optimized is a generative PLM (e.g., ProtGPT2). One must understand that these models are autoregressive, predicting the next amino acid. The goal of KPO is to modify the conditional probability distribution $P(sequence | prompt)$ learned by these models to align with safety constraints.
  - **Quick check question:** How does the training objective of a standard autoregressive PLM (maximizing log-likelihood of sequences) differ from the objective during the KPO fine-tuning phase?

## Architecture Onboarding

- **Component map:**
  Data Sources & PSKG Construction -> Graph Pruning Module -> Preference Pair Generator -> PLM Fine-Tuning with DPO

- **Critical path:** The most critical path is the **Quality of the Preference Pairs**. The DPO process hinges on the premise that for each harmful protein, the system can find a highly similar benign protein. Flawed PSKG construction or pruning will result in noisy pairs (e.g., pairing a toxin with another unlabeled toxin), leading to a failure in safety alignment. The Graph Pruning and Pair Generation steps are the linchpins.

- **Design tradeoffs:**
  - **Graph Size vs. Computational Cost:** The pruning thresholds (Q and K) represent a tradeoff. More aggressive pruning (lower Q, K) reduces training time but risks losing critical safety information.
  - **Pair Selection vs. Generalization:** The choice of similarity threshold for creating preference pairs involves a tradeoff. A very strict threshold ensures high-quality pairs but yields fewer training examples.

- **Failure signatures:**
  - **Safety Alignment Failure:** The model continues to generate sequences with high similarity to harmful proteins. Diagnosed by plotting embeddings of generated sequences and seeing overlap with the harmful protein cluster.
  - **Catastrophic Forgetting:** The model loses its ability to generate functional proteins. Detected by a sharp drop in performance on functional evaluation datasets (GB1, PhoQ, etc.) after KPO fine-tuning.

- **First 3 experiments:**
  1. **Ablation on Graph Pruning:** Replace the proposed weighted pruning with random pruning or no pruning. Compare the resulting model's safety metrics to validate the pruning method.
  2. **Baseline Comparison (KPO vs. Standard DPO):** Fine-tune a PLM using standard DPO with randomly sampled benign proteins (without the PSKG). Compare against KPO to quantify the added value of the knowledge graph.
  3. **Embedding Space Visualization:** Generate sequences with both the original and KPO-fine-tuned models. Compute embeddings and visualize using t-SNE. A successful experiment will show a clear separation between the KPO-generated sequences and the harmful proteins.

## Open Questions the Paper Calls Out

- **Question:** How can structural-level safety constraints, such as avoiding toxic 3D conformations, be integrated directly into the KPO fine-tuning process rather than relying on downstream evaluation tools?
- **Question:** Can the KPO framework maintain its effectiveness and efficiency when scaled to Protein Language Models (PLMs) significantly larger than those tested (e.g., models with billions of parameters)?
- **Question:** How can the Protein Safety Knowledge Graph (PSKG) be dynamically updated to adapt to newly discovered safety insights or evolving biological knowledge without requiring a complete reconstruction of the graph?
- **Question:** Is the KPO framework robust against adversarial attacks where users intentionally design prompts to bypass safety alignment and generate harmful sequences?

## Limitations
- The safety knowledge graph (PSKG) completeness and accuracy could affect the quality of preference pairs and overall effectiveness
- Computational efficiency gains from pruning are not rigorously quantified relative to potential information loss
- Exact performance trade-off between safety gains and functional capability retention is not fully quantified across all datasets

## Confidence
- **High confidence:** The general framework of using preference optimization with knowledge graphs to improve protein generation safety is sound and supported by the experimental results showing reduced harmful sequence similarity and maintained functionality.
- **Medium confidence:** The specific pruning algorithm effectively retains critical safety information while improving computational efficiency. The paper shows this improves runtime but doesn't fully validate that no important safety information is lost.
- **Medium confidence:** The method generalizes across different PLM architectures (ProtGPT2, ProGen2, InstructProtein). Results show consistent safety improvements across models, but the magnitude of improvement varies.

## Next Checks
1. **Ablation study on pruning aggressiveness:** Systematically vary the pruning threshold (Q and K values) to quantify the exact trade-off between computational efficiency and safety alignment performance.
2. **Out-of-distribution safety evaluation:** Test the KPO-tuned models on generating proteins from novel functional families not represented in the training data to assess generalization of safety alignment.
3. **Preference pair quality analysis:** Manually examine a sample of the generated preference pairs to verify that the "similar benign" proteins are truly functionally analogous to their harmful counterparts beyond just GO term overlap.