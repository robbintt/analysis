---
ver: rpa2
title: Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments
arxiv_id: '2512.18670'
source_url: https://arxiv.org/abs/2512.18670
tags:
- learning
- policy
- tasks
- demonstration
- dgcrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DGCRL, a demonstration-guided continual reinforcement
  learning framework that addresses the stability-plasticity dilemma in dynamic environments.
  The key innovation is storing prior knowledge as external demonstrations in a self-evolving
  repository, which directly guides RL exploration behavior through a dynamic curriculum-based
  strategy.
---

# Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments

## Quick Facts
- arXiv ID: 2512.18670
- Source URL: https://arxiv.org/abs/2512.18670
- Reference count: 40
- Key outcome: DGCRL framework achieves significantly higher average performance (e.g., 93.85 vs 71.16 in Hopper) than baseline methods on continual RL tasks while maintaining high forward transfer and low forgetting.

## Executive Summary
This paper proposes DGCRL, a demonstration-guided continual reinforcement learning framework that addresses the stability-plasticity dilemma in dynamic environments. The key innovation is storing prior knowledge as external demonstrations in a self-evolving repository, which directly guides RL exploration behavior through a dynamic curriculum-based strategy. For each new task, the agent selects the most relevant demonstration and gradually transitions from demonstration-guided to self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks show DGCRL significantly outperforms baseline methods, achieving the highest average performance, highest forward transfer, lowest forgetting, and best training efficiency.

## Method Summary
DGCRL extends TD3 actor-critic with a demonstration-guided approach. The method maintains a repository of trajectory-level action sequences from prior tasks/policies. For each new task, the agent evaluates all demonstrations and selects the best-performing one as a guide policy. During rollout, the agent follows the guide policy for the first h timesteps, then switches to self-exploration for the remaining steps. After each rollout, if the performance exceeds a threshold, the trajectory is added to the repository and the guide horizon h is decreased by Δh. The exploration policy is trained from the mixed data. Both actor and critic networks are reset between tasks.

## Key Results
- DGCRL achieves significantly higher average performance than baselines across all tested environments (e.g., 93.85 vs 71.16 in Hopper)
- The method demonstrates highest forward transfer and lowest forgetting among compared approaches
- DGCRL shows best training efficiency with faster convergence to optimal policies
- Ablation studies confirm the importance of resetting both actor and critic components
- Sensitivity analysis validates the effectiveness of demonstration-guided exploration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct demonstration-guided exploration accelerates adaptation and sample efficiency in dynamic environments.
- **Mechanism:** For each new task, DGCRL selects the most relevant demonstration from a repository to serve as a "guide policy." This policy directly determines the agent's actions during the initial phase of each rollout, placing it in informative regions of the state space.
- **Core assumption:** The repository contains at least one demonstration relevant enough to the new task to provide a better starting point for exploration than random or naive exploration.
- **Evidence anchors:** [abstract] "...dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning..." [section 4, p. 10] "...the agent initially adheres to πg,i by imitating the actions from the demonstration for the first h timesteps..."

### Mechanism 2
- **Claim:** A self-evolving demonstration repository provides a mechanism for long-term knowledge retention and plasticity, mitigating catastrophic forgetting.
- **Mechanism:** The repository is not static. If a newly learned policy achieves a return higher than a set threshold for a task, its trajectory is added as a new demonstration.
- **Core assumption:** A policy's high return on a task is a reliable proxy for its utility as a demonstrator for that or similar future tasks.
- **Evidence anchors:** [abstract] "...self-evolving repository, which directly guides RL exploration..." [section 4, p. 10] "...if a newly learned policy outperforms existing demonstrations, we store it as a new demonstration..."

### Mechanism 3
- **Claim:** A curriculum-based strategy that gradually transitions from demonstration-guided to self-exploration stabilizes learning.
- **Mechanism:** The agent's rollout policy is a mix of the guide and exploration policies. The guide's influence is controlled by a horizon `h`, which is decremented over time.
- **Core assumption:** The exploration policy will improve sufficiently during the guided phase to take over effectively as the guide's influence fades.
- **Evidence anchors:** [abstract] "...gradually shifting from demonstration-guided exploration to fully self-exploration." [section 4, p. 10-11] "After the rollout... the guide horizon h is reduced by Δh..."

## Foundational Learning

**Concept:** Markov Decision Process (MDP)
- **Why needed here:** The paper's problem formulation is built on a sequence of stationary MDPs (tasks) that the agent must solve. Understanding states, actions, rewards, and transitions is essential.
- **Quick check question:** Can you describe the components of an MDP and how they change across tasks in DGCRL's formulation?

**Concept:** Stability-Plasticity Dilemma
- **Why needed here:** This is the core problem DGCRL aims to solve. You must understand the tension between preserving old knowledge (stability) and acquiring new knowledge (plasticity).
- **Quick check question:** Why is it difficult for a single neural network to learn new tasks without forgetting old ones?

**Concept:** Actor-Critic Architecture (specifically TD3)
- **Why needed here:** DGCRL is implemented using the TD3 algorithm. The method involves resetting and training both actor and critic networks.
- **Quick check question:** What are the roles of the actor and critic networks in an actor-critic RL algorithm? What is the significance of the ablation study on resetting them?

## Architecture Onboarding

**Component map:**
- Demonstration Repository -> Selection Mechanism -> Guide Policy (πg) -> Mixed Policy (π) -> Actor-Critic Network
- Exploration Policy (πe) -> Actor-Critic Network
- Self-Evolving Mechanism -> Demonstration Repository
- Curriculum Schedule -> Guide Horizon (h)

**Critical path:**
1. Agent faces a new task Mi
2. Select best demonstration πg,i from the repository based on initial evaluation
3. Set guide horizon h=H
4. **Loop:** Roll out mixed policy πi, train exploration policy πe,i from the data
5. Evaluate πi. If performance > threshold, add trajectory to repository and decrease h
6. Repeat until h=0 or task budget is exhausted

**Design tradeoffs:**
- Direct behavioral guidance vs. optimization-based transfer: DGCRL chooses the former, trading off the flexibility of implicit knowledge in neural network weights for the efficiency of explicit demonstrations
- Resetting strategies: The ablation study shows resetting both actor and critic yields the best performance
- Repository size vs. retrieval efficiency: A larger repository (via self-evolution) provides more options but could slow down the selection process

**Failure signatures:**
- Slow learning or no improvement: Could be due to poor initial demonstrations or a transition schedule (Δh) that is too fast
- Performance collapse: Could result from a suboptimal demonstration being selected or the exploration policy failing to generalize
- Negative forgetting: Though the paper frames this as a feature of their method, it highlights a failure of the conventional metric in this context

**First 3 experiments:**
1. **2D Navigation Ablation:** Implement DGCRL on the simplest navigation task (e.g., Type I, goal changes) and ablate the curriculum schedule (fixed h vs. decreasing h) to confirm its contribution
2. **Reset Strategy Test:** On a MuJoCo environment like Hopper, compare performance when resetting only the actor, only the critic, and both, to validate the ablation study's findings in your own implementation
3. **Sensitivity Analysis:** Run DGCRL on a chosen task with varying initial demonstration quantities (e.g., 20%, 60%, 100% of a collected set) to observe its robustness to the quality and coverage of the initial repository

## Open Questions the Paper Calls Out

**Open Question 1:** Can DGCRL maintain its performance advantages in real-world physical environments where exploration is expensive or safety-critical?
- **Basis in paper:** [explicit] The authors state that "extending DGCRL to real-world operational scenarios remains an important future direction" (Page 29)
- **Why unresolved:** All reported experiments were conducted in simulation (2D navigation and MuJoCo), and the authors acknowledge these are distinct from real-world operational scenarios
- **What evidence would resolve it:** Empirical results from deploying DGCRL on physical robotic hardware or safety-critical benchmarks

**Open Question 2:** How can demonstration retrieval be optimized to prevent efficiency bottlenecks when scaling to large demonstration repositories?
- **Basis in paper:** [explicit] The conclusion notes that "retrieval efficiency may become a bottleneck" and suggests exploring strategies like "indexing, labeling, [or] pruning" (Page 29)
- **Why unresolved:** The experiments utilized relatively small repositories (50-60 demonstrations) where retrieval overhead was negligible, leaving the scaling problem unaddressed
- **What evidence would resolve it:** Comparative analysis of retrieval algorithms on datasets with thousands of demonstrations, measuring both retrieval time and policy performance

**Open Question 3:** What evaluation metrics can replace conventional forgetting metrics to accurately assess CRL methods that allow for negative forgetting (improvement on past tasks)?
- **Basis in paper:** [explicit] The paper highlights that "conventional forgetting metrics have inherent limitations in demonstration-guided settings as negative scores may be produced" (Page 29)
- **Why unresolved:** The standard definition of forgetting assumes performance peaks at the end of training, which does not hold for DGCRL's self-evolving demonstration strategy
- **What evidence would resolve it:** A proposed metric that robustly handles performance improvements on prior tasks via external memory retrieval

## Limitations

- The paper lacks precise specification of the demonstration selection mechanism, particularly the evaluation protocol for identifying the "best-performing" demo
- The method's effectiveness depends heavily on having at least one sufficiently relevant demonstration for each new task, which may not hold in cases of truly novel task structures
- The computational overhead of searching through an increasingly large demonstration repository is not addressed, potentially becoming prohibitive in long-horizon continual learning scenarios

## Confidence

- **High Confidence:** The core mechanism of demonstration-guided exploration and the curriculum-based transition strategy (Mechanism 1 and 3) are well-supported by the described methodology and experimental results
- **Medium Confidence:** The self-evolving repository's effectiveness in mitigating catastrophic forgetting (Mechanism 2) is supported, but the assumption that high-return trajectories make good demonstrators warrants further investigation
- **Low Confidence:** The paper's treatment of negative forgetting as a positive feature requires careful interpretation, as it highlights a limitation in conventional evaluation metrics rather than definitively proving the method's superiority in knowledge retention

## Next Checks

1. **Evaluation Protocol Clarification:** Implement and test multiple demonstration selection mechanisms (e.g., single rollout vs. averaged performance over multiple rollouts) to determine the optimal evaluation protocol for the "best-performing" demo criterion

2. **Robustness to Demonstration Quality:** Systematically vary the initial demonstration repository quality (from highly relevant to minimally relevant) across tasks to quantify the method's sensitivity to demonstration relevance and establish failure boundaries

3. **Repository Scaling Analysis:** Measure and analyze the computational cost of demonstration selection as the repository grows, and implement a pruning or indexing strategy to maintain retrieval efficiency in long-horizon continual learning scenarios