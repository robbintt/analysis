---
ver: rpa2
title: Physics Augmented Machine Learning Discovery of Composition-Dependent Constitutive
  Laws for 3D Printed Digital Materials
arxiv_id: '2507.02991'
source_url: https://arxiv.org/abs/2507.02991
tags:
- materials
- tension
- torsion
- material
- composition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of modeling composition-dependent
  mechanical behavior in multi-material 3D printed digital materials. Traditional
  constitutive modeling approaches become impractical due to the vast space of material
  compositions created by polymer jetting.
---

# Physics Augmented Machine Learning Discovery of Composition-Dependent Constitutive Laws for 3D Printed Digital Materials

## Quick Facts
- **arXiv ID**: 2507.02991
- **Source URL**: https://arxiv.org/abs/2507.02991
- **Reference count**: 40
- **Key outcome**: Physics-augmented neural network (PANN) combining partially input convex neural network (pICNN) for hyperelastic strain energy and quasi-linear viscoelastic (QLV) model with composition-dependent relaxation captures composition-dependent mechanical behavior of multi-material 3D printed digital materials with R² > 0.98 for trained compositions.

## Executive Summary
This work addresses the challenge of modeling composition-dependent mechanical behavior in multi-material 3D printed digital materials, where traditional constitutive modeling becomes impractical due to the vast space of material compositions. The authors develop a physics-augmented neural network that learns thermodynamically valid, composition-dependent constitutive models by combining a partially input convex neural network for strain energy with a quasi-linear viscoelastic formulation for time-dependent response. The approach accurately captures nonlinear, rate-dependent behavior of five different digital material compositions in both uniaxial tension and torsion, achieving excellent interpolation accuracy while highlighting the limitations of extrapolation beyond trained composition ranges.

## Method Summary
The method employs a physics-augmented neural network (PANN) architecture that combines a partially input convex neural network (pICNN) for learning thermodynamically valid hyperelastic strain energy functions with a quasi-linear viscoelastic (QLV) model for time-dependent response. The pICNN ensures convexity with respect to strain invariants while allowing non-convex dependence on composition, and L0 sparsification improves interpretability. The model is trained through a staged procedure: first learning the instantaneous elastic response from fastest-rate tension data, then alternating updates between hyperelastic and viscoelastic components, adding torsion data, and finally applying L0 regularization. A separate MLP predicts composition-dependent viscoelastic relaxation parameters.

## Key Results
- The PANN accurately captures nonlinear, rate-dependent behavior of five digital material compositions with R² > 0.98 in 21 of 24 datasets for training compositions
- L0 sparsification produces compact, interpretable constitutive models by setting irrelevant network weights exactly to zero, enabling extraction of closed-form strain energy functions
- The model successfully interpolates to intermediate compositions (DM-40) with R² > 0.96 but fails to extrapolate to compositions outside the training range (DM-70), highlighting the limits of learned composition dependence
- Stress-stretch curves show systematic underprediction of initial bump at low stretch and sharp upturn at high stretch, likely due to smoothing from L0 sparsification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The partially input convex neural network (pICNN) architecture enables learning thermodynamically valid, composition-dependent strain energy functions by enforcing convexity only with respect to strain inputs while permitting arbitrary non-convex dependence on composition.
- **Mechanism**: The network bifurcates processing into two branches: (1) a convex branch that processes strain invariants (Ī₁, Ī₂) through layers with non-negative weights and monotonically non-decreasing activations (softplus), ensuring polyconvexity; (2) a non-convex branch that processes the composition parameter c through standard dense layers. These branches fuse at each hidden layer via learned connections, allowing composition to modulate the strain energy landscape without violating convexity requirements. Passthrough connections reintroduce raw invariants at each layer without trainable weights, preserving convexity while reducing parameter redundancy.
- **Core assumption**: Strain energy convexity with respect to deformation measures is necessary and sufficient for material stability, while composition dependence need not be convex—materials can exhibit non-monotonic property changes across composition space.
- **Evidence anchors**:
  - [abstract]: "The pICNN ensures convexity with respect to strain invariants while allowing non-convex dependence on composition."
  - [Section 3.1.2, p.10-11]: "To ensure convexity of the network output with respect to I, all subsequent input weights for h ≥ 2 are non-negative... This architecture guarantees the convexity of the strain energy potential ΨNN with respect to the invariants Ī while enabling expressive and nonlinear modeling of material behavior through its dependence on the mechanical parameters c."
  - [corpus]: Related polyconvex neural network approaches exist (e.g., "Polyconvex Physics-Augmented Neural Network Constitutive Models in Principal Stretches"), but the partial convexity formulation for composition-aware modeling represents an extension not explicitly documented in corpus neighbors.
- **Break condition**: If convex branch weights become negative during training (violating polyconvexity), or if the non-convex composition branch fails to capture composition-property relationships (evidenced by poor interpolation), the mechanism degrades. The paper shows break condition manifestation: DM-70 extrapolation fails (R² = 0.251) because composition c=2.0895 lies outside training range [0, 1].

### Mechanism 2
- **Claim**: A quasi-linear viscoelastic (QLV) formulation with a composition-dependent relaxation coefficient γ captures rate-dependent mechanical response by separating the instantaneous hyperelastic response from time-dependent relaxation, where only γ varies with composition.
- **Mechanism**: The total Cauchy stress σ(t) is computed as the convolution of the instantaneous hyperelastic stress σₑ(t) with a relaxation kernel D'(t-s) = -γ/τ · e^{-(t-s)/τ}. A separate MLP maps composition c to γ ∈ [0,1] via softplus hidden activations and a sigmoid output constraint. The relaxation time τ is fixed at 10 seconds based on the observation timescale. This factorization allows the pICNN to learn composition-independent hyperelastic skeleton behavior while the MLP captures composition-specific viscoelastic sensitivity.
- **Core assumption**: (1) Viscoelastic response can be represented as a superposition of instantaneous elastic response modulated by a relaxation kernel; (2) relaxation time τ is approximately composition-independent or can be fixed without significant accuracy loss given limited data; (3) γ ∈ [0,1] is sufficient to capture the range of viscoelastic behavior across compositions.
- **Evidence anchors**:
  - [abstract]: "...quasi-linear viscoelastic (QLV) formulation for time-dependent response... introduce a multilayer perceptron (MLP) to predict viscoelastic relaxation parameters from composition."
  - [Section 3.2, p.13-14]: "Since γ and τ appear together as the ratio γ/τ in equation 15, the parameters are coupled. Given the limited amount of training data fixing τ helps reduce the complexity."
  - [corpus]: Weak—no direct corpus evidence for this specific QLV-MLP composition coupling; standard QLV formulations are established but composition-aware γ prediction via MLP is a novel integration in this context.
- **Break condition**: If materials exhibit fundamentally different relaxation spectra across compositions (requiring τ to vary), or if viscoelastic behavior cannot be factorized into instantaneous elastic + relaxation components (non-QLV behavior), predictions at intermediate rates will diverge. Evidence: The model captures rate dependence well for training compositions (R² > 0.98 across rates) but extrapolation failure at DM-70 (R² = 0.251 at slowest rate) suggests γ extrapolation is unreliable.

### Mechanism 3
- **Claim**: Smoothed L0 sparsification produces compact, interpretable constitutive models by learning binary gates that set irrelevant network weights exactly to zero, enabling symbolic extraction of the strain energy function.
- **Mechanism**: Each weight θ is multiplied by a continuous gate z ∈ [0,1] parameterized via a smooth approximation using sampled uniform noise u and learned parameters α. During training, Monte Carlo sampling approximates the expected L0 penalty. The loss includes separate L0 terms for three parameter groups (fully-convex, non-convex, and connecting layers) with distinct regularization weights. At inference, gates are deterministic: θ* = θ* ⊙ ź where ź uses the learned α without sampling. This produces exact zeros (not small magnitudes), enabling extraction of closed-form expressions.
- **Core assumption**: The true constitutive relationship can be represented by a sparse subset of network parameters; exact zeros (vs. L2-induced small weights) yield more interpretable and generalizable models without sacrificing accuracy.
- **Evidence anchors**:
  - [abstract]: "L0 sparsification improves interpretability."
  - [Section 3.1.3, p.11-13]: "L0 regularization permits parameters to be set exactly to zero. This strategy preserves model flexibility while promoting compact and physically interpretable representations."
  - [Section 4.2.5, p.27]: After sparsification, the extracted strain energy function (Equation 26) contains only 17 convex weights and 16 non-convex weights from an initial 1,357 + 75.
  - [corpus]: L0 regularization for neural networks (Louizos et al., 2017) is established, but its application to physics-augmented constitutive models for interpretability is an emerging direction also explored in "Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics" (cited in-paper).
- **Break condition**: If regularization strength λ is too high, physically meaningful terms are pruned (underfitting); if too low, the model remains overparameterized and uninterpretable. The multi-stage training (sparsification only in Step 7) mitigates this by first establishing fit, then pruning.

## Foundational Learning

- **Concept**: Strain invariants and isochoric decomposition
  - **Why needed here**: The pICNN operates on isochoric invariants Ī₁ and Ī₂ rather than strain tensor components. Understanding that Ī₁ = tr(C̄) and Ī₂ = ½[(tr(C̄))² - tr(C̄²)] where C̄ = J^(-2/3)C removes volumetric deformation is essential for interpreting Equation 3-5 and understanding why invariants (not strains) are the convex branch inputs.
  - **Quick check question**: For uniaxial tension with stretch λ and incompressibility (J=1), can you derive why Ī₁ = λ² + 2λ^(-1)?

- **Concept**: Polyconvexity and thermodynamic admissibility
  - **Why needed here**: The entire pICNN architecture is structured to guarantee polyconvexity of the strain energy function. Without understanding that polyconvexity ensures material stability (real wave speeds, ellipticity of equilibrium equations), the non-negative weight constraints appear arbitrary rather than physically motivated.
  - **Quick check question**: Why does convexity of Ψ with respect to C (rather than F or invariants) not fully guarantee material stability, motivating the invariant-based formulation?

- **Concept**: Quasi-linear viscoelasticity (QLV) vs. fully nonlinear viscoelasticity
  - **Why needed here**: The model assumes time-dependence can be separated from nonlinearity via the QLV formulation. Understanding that QLV assumes σ(t) = ∫G(t-s) · (∂σₑ/∂s)ds where σₑ is the instantaneous elastic response explains why the pICNN (elastic) and MLP-γ (viscous) can be trained somewhat independently.
  - **Quick check question**: What physical behavior would violate the QLV assumption, requiring fully nonlinear viscoelasticity instead?

## Architecture Onboarding

- **Component map**:
  ```
  Inputs: Ī₁, Ī₂ (strain invariants from deformation) + c (composition: Agilus/Digital-ABS ratio)

  [pICNN]
  ├── Non-convex branch: c → Dense(5) → Softplus → Dense(5) → Softplus → features C_h
  ├── Convex branch: Ī₁,Ī₂ → Dense(30) → Softplus + fusion(C_h) → ... → Ψ_NN
  └── Output: Strain energy Ψ → ∂Ψ/∂C via autodiff → instantaneous stress σ'_e

  [MLP-γ]
  └── c → Dense(8) → Softplus → Dense(1) → Sigmoid → γ ∈ [0,1]

  [QLV Integration]
  └── σ'_e + γ → Convolution with kernel D'(t-s) = -γ/τ · e^{-(t-s)/τ} → σ(t)

  Output: σ₁₁ (tension) or T = 2π∫σ_zθ(r)r²dr (torsion)
  ```

- **Critical path**:
  1. **Stage 1 (Steps 1-5)**: Train pICNN on fastest-rate tension data for σ'_e, then train MLP-γ across all rates for viscoelastic σ(t), alternating until convergence.
  2. **Stage 2 (Step 6)**: Introduce torsion data by ramping α_torsion from 0→0.1 while maintaining α_tension=1.
  3. **Stage 3 (Step 7)**: Apply L0 regularization (α_fc→5×10⁻⁴, α_nc,α_ncfc→10⁻⁶) over 1000 epochs to sparsify.

- **Design tradeoffs**:
  - **Fixed τ (10s) vs. learned τ**: Fixed to reduce parameter coupling with limited data; risk is incorrect relaxation timescale for untested compositions.
  - **Sparsification strength**: α_fc=5×10⁻⁴ vs. α_nc=10⁻⁶ reflects higher tolerance for complexity in non-convex (composition) branch; aggressive sparsification of convex branch yields interpretable expressions but may underfit localized nonlinearities (observed: initial bump and final upturn underpredicted).
  - **Training composition coverage**: A, DM-50, DM-60 trained; DM-40 held out (interpolation succeeds, R²>0.96); DM-70 held out (extrapolation fails, R²=0.251). Assumption: compositions are approximately linearly spaced in property space—violated at DM-70 boundary.

- **Failure signatures**:
  - **Extrapolation breakdown**: DM-70 tension at slow rate shows R²=0.251, sMAPE=34.5%; torsion R²=-0.118 (worse than mean baseline). Root cause: non-convex composition dependence cannot extrapolate beyond c∈[0,2.09].
  - **Local feature underfitting**: Stress-stretch curves show systematic underprediction of (a) initial bump at low stretch and (b) sharp upturn at high stretch. Likely causes: smoothing from L0 sparsification, simultaneous tension-torsion fitting constraint.
  - **Loss divergence on held-out compositions**: Test loss for DM-70 remains flat across training steps (Fig. 9c), indicating architecture cannot generalize beyond trained composition range.

- **First 3 experiments**:
  1. **Ablation: No L0 regularization**: Train identical architecture with α_fc=α_nc=α_ncfc=0. Compare final parameter count and test accuracy on DM-40/DM-70. Hypothesis: accuracy improves slightly but model becomes uninterpretable (cannot extract Equation 26-like form).
  2. **Sensitivity: τ variation**: Retrain with τ ∈ {1s, 5s, 50s} fixed, or with τ as a learnable parameter (second MLP output). Compare rate-dependent accuracy across compositions. Hypothesis: learned τ may improve DM-70 extrapolation if relaxation timescale varies with composition.
  3. **Composition coverage**: Train on A, DM-40, DM-50, DM-60, DM-70 (leave out DM-50 for testing). Assess whether inclusion of boundary composition (DM-70) enables better interpolation across the full range. Hypothesis: DM-50 test accuracy improves, confirming extrapolation limitation is composition-coverage rather than architecture fundamental.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can this framework be extended to discover full viscoelastic constitutive relations directly from data, rather than relying on a predefined Quasi-Linear Viscoelastic (QLV) structure?
- **Basis in paper**: [explicit] The authors state in the Conclusion that "Future work, enabled by richer datasets with less noise, could extend this approach to discover full viscoelastic constitutive relations directly from data without relying on predefined forms."
- **Why unresolved**: The current study fixes the QLV structure, learning only the relaxation coefficient (γ) from data rather than the entire viscoelastic kernel or internal variable structure.
- **What evidence would resolve it**: A PANN implementation that successfully identifies the mathematical form of the relaxation kernel and internal variables from noisy experimental data without prior assumption.

### Open Question 2
- **Question**: Can the physics-augmented neural network (PANN) framework be adapted to model polymer blends with three or more base components and user-defined micro-structures?
- **Basis in paper**: [explicit] The Conclusion suggests "this method could be expanded to polymer blends and composites with three or more components and user-defined micro-structures."
- **Why unresolved**: The current work focuses on "digital materials" composed of only two base photopolymers (Agilus and Digital ABS) mixed at the droplet level.
- **What evidence would resolve it**: Demonstration of the model successfully predicting mechanical behavior for ternary or quaternary mixtures where composition inputs include multiple volume fractions and microstructural descriptors.

### Open Question 3
- **Question**: How can the model's reliability be improved for extrapolating to material compositions outside the training distribution (e.g., DM-70)?
- **Basis in paper**: [inferred] The results show that while the model interpolated well for DM-40, it failed to extrapolate for DM-70 (highest Digital ABS content), with the test loss remaining "substantially above the training loss."
- **Why unresolved**: The non-convex branch of the pICNN allows flexible dependence on composition but lacks physical constraints to ensure physically valid behavior outside the convex hull of the training data.
- **What evidence would resolve it**: Incorporation of physics-based monotonicity constraints or asymptotic consistency conditions on the composition input that prevent prediction collapse in extrapolation regimes.

## Limitations
- The model fails to extrapolate to compositions outside the training range (DM-70), suggesting the non-convex composition dependence learned by the pICNN cannot generalize to unseen material blends
- L0 sparsification produces interpretable models but may sacrifice accuracy on local features, with systematic underprediction of initial bump at low stretch and sharp upturn at high stretch
- The QLV assumption with fixed relaxation time τ may not capture composition-dependent viscoelastic spectra accurately across all compositions
- Limited training data (three compositions) raises questions about the generalizability of the learned composition-property mappings

## Confidence

- **High confidence**: Convexity enforcement via pICNN architecture produces thermodynamically valid strain energy functions for trained compositions; accurate interpolation on DM-40 (R² > 0.96); sparsity enables interpretable model extraction
- **Medium confidence**: QLV formulation captures rate-dependent behavior for trained compositions (R² > 0.98); staged training procedure converges reliably; extrapolation failure at DM-70 is expected given composition coverage limits
- **Low confidence**: Composition-γ relationship learned by MLP generalizes beyond training range; fixed τ=10s is appropriate for all compositions; the multi-stage training with L0 regularization consistently produces optimal parameter configurations

## Next Checks
1. **Ablation study**: Remove L0 regularization (α_fc=α_nc=α_ncfc=0) and compare parameter count, test accuracy, and interpretability on DM-40/DM-70 to quantify sparsity-performance tradeoff
2. **τ sensitivity analysis**: Vary τ ∈ {1s, 5s, 50s} or make τ learnable via second MLP; assess impact on rate-dependent accuracy across compositions and extrapolation to DM-70
3. **Composition coverage experiment**: Train on all five compositions (A, DM-40, DM-50, DM-60, DM-70) with DM-50 held out; evaluate whether boundary composition inclusion enables better interpolation across the full range