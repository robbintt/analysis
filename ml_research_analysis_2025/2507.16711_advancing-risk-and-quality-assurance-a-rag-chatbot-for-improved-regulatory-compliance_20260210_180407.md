---
ver: rpa2
title: 'Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory
  Compliance'
arxiv_id: '2507.16711'
source_url: https://arxiv.org/abs/2507.16711
tags:
- chatbot
- retrieval
- search
- context
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a Retrieval-Augmented Generation (RAG) chatbot
  for regulatory compliance in highly regulated industries, addressing the operational
  bottlenecks of relying on specialized experts for Risk and Quality (R&Q) queries.
  The system combines Large Language Models (LLMs) with hybrid search (vector and
  text-based) and relevance boosting to enhance retrieval accuracy and response quality.
---

# Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance

## Quick Facts
- arXiv ID: 2507.16711
- Source URL: https://arxiv.org/abs/2507.16711
- Reference count: 10
- Best performance: 3.79/5 answer correctness score using GPT-4o

## Executive Summary
This paper presents a Retrieval-Augmented Generation (RAG) chatbot designed to address operational bottlenecks in highly regulated industries where employees rely on specialized experts for Risk and Quality (R&Q) regulatory compliance queries. The system combines Large Language Models with hybrid search (vector and text-based) and relevance boosting to enhance retrieval accuracy and response quality. Evaluated on 124 expert-annotated real-world queries, the chatbot achieved a correctness score of 3.79 (±0.037) on a 0-5 scale using GPT-4o, outperforming other LLM backbones and baseline RAG approaches.

## Method Summary
The system implements a hybrid search strategy combining vector similarity and BM25 full-text search with reciprocal rank fusion, along with relevance boosting (2×) for internal documents. It uses GPT-4o as the LLM backbone with temperature=0, chunk size 512/overlap 64, and the text-embedding-3-large model. The evaluation framework employs G-Eval scoring with a 0.70 Pearson correlation to expert assessments. The system was tested on 124 expert-annotated R&Q question-answer pairs across internal and external regulatory sources.

## Key Results
- Best configuration achieved 3.79 (±0.037) answer correctness score on 0-5 scale using GPT-4o
- Hybrid search outperformed single-modality retrieval (3.76 vs 3.60-3.72 for vector/text-only)
- Automated evaluation framework achieved 0.70 correlation with expert assessments
- Relevance boosting improved performance from 3.76 to 3.79

## Why This Works (Mechanism)

### Mechanism 1
Hybrid search outperforms single-modality retrieval (vector-only or text-only) for complex regulatory queries. The system executes vector similarity (semantic) and BM25 (lexical) searches in parallel, then merges results using Reciprocal Rank Fusion (RRF). This captures exact legal terminology via BM25 while preserving conceptual matching via embeddings. Evidence shows Hybrid scoring 3.76 vs Vector (3.72) and Text (3.60) for Answer Correctness.

### Mechanism 2
Explicitly boosting the rank of internal documents improves answer correctness in regulated environments. During ingestion, internal document chunks are tagged for higher priority. During retrieval, a 2× boosting factor is applied to the relevance scores of these trusted internal sources, biasing the context window toward proprietary policy over external/general data. Table II shows "+Relevance Boosting" lifting Answer Correctness from 3.76 to 3.79.

### Mechanism 3
GPT-4o using the G-Eval framework can serve as a proxy for human expert evaluation in this specific domain. An LLM (GPT-4o) is prompted with a chain-of-thought "Evaluation Steps" sequence to score answers on a 0-5 scale. The study achieved a Pearson correlation coefficient of r = 0.70 between LLM-based scores and manual evaluations.

## Foundational Learning

### Concept: Reciprocal Rank Fusion (RRF)
Why needed: You are merging two distinct ordered lists (vector results vs. keyword results) into a single ranking. Simple averaging doesn't work well because scores are on different scales; RRF handles this based on rank position rather than raw score.
Quick check: If document A is #1 in vector search and #10 in keyword search, and document B is #2 in both, which one does RRF likely rank higher?

### Concept: Chunk Overlap
Why needed: Regulatory text is dense. If a key clause sits on the boundary of two chunks, splitting it might destroy its meaning. Overlap ensures context continuity across boundaries.
Quick check: Why does increasing overlap increase the size of your vector index, and what is the tradeoff regarding "noise"?

### Concept: RAG vs. Fine-Tuning
Why needed: The paper solves the problem of knowledge access, not behavioral style. Fine-tuning teaches the model how to speak; RAG teaches it what it knows right now.
Quick check: If regulations change weekly, why is RAG preferred over fine-tuning for compliance?

## Architecture Onboarding

### Component map
Ingestion: Unstructured -> Chunking (512/64) -> Embedding (text-embedding-3-large)
Storage: Azure AI Search (vectors + raw text)
Retrieval: Hybrid Query -> Vector Index + BM25 Index -> Reranker (RRF + Relevance Boosting)
Generation: LLM (GPT-4o) + Prompt Template (Language detection + Context + Citations)

### Critical path
The Retrieval stage. If the hybrid search retrieves the wrong chunk, the LLM cannot generate a correct answer (hallucination or "I don't know"). The boosting logic is the single point of failure for trust.

### Design tradeoffs
- Chunk Size 512 vs 1024: 512 provided slightly better correctness in ablation. Smaller chunks reduce noise but risk fragmenting context.
- Embedding Model: Switching from ada-002 to 3-large yielded higher performance but likely increases latency/cost per query.
- Evaluation: Automated evaluation saves human time but introduces a 0.30 uncorrelated error margin compared to experts.

### Failure signatures
- "Stale Boost": The model answers confidently with an outdated policy because the boosting mechanism forces an old internal document to the top.
- "Citation Drift": The model cites [doc/1] but the fact is actually in [doc/2]. This usually happens when chunks overlap too much or retrieval is imprecise.

### First 3 experiments
1. A/B Test Search Type: Run the 124 queries against a Vector-only index vs. Hybrid to reproduce the performance delta.
2. Boosting Sensitivity: Vary the boosting factor (e.g., 1.5× vs 2.0× vs 3.0×) to see if the 3.79 score plateaus or declines.
3. Embedding Migration: Compare ada-002 vs 3-large on a sample of 20 queries specifically checking retrieval latency vs. semantic nuance.

## Open Questions the Paper Calls Out

### Open Question 1
How does a dynamic multi-agent architecture compare to the current single-agent RAG pipeline in handling complex regulatory queries requiring multi-hop reasoning? The Conclusion states future research will focus on extending the chatbot to a dynamic multi-agent system capable of intelligent query dissection, clarifying questions, and multi-hop reasoning. This remains unresolved as the current system utilizes a sequential pipeline without the ability to dissect queries or ask clarifying questions.

### Open Question 2
What are the primary drivers of the performance gap between context correctness (2.90) and answer correctness (3.79), and can retrieval strategies be further optimized to improve context quality? Tables II and III consistently show a significant gap where the generated answer scores significantly higher than the retrieved context used to create it. The paper optimizes for the final "Answer" score but does not extensively investigate why the "Context" retrieval scores remain relatively low.

### Open Question 3
Does the automated evaluation framework using GPT-4o exhibit a self-preference bias when evaluating responses generated by GPT-4o compared to other models or human evaluation? While the paper establishes a 0.70 correlation with human experts, it does not isolate whether GPT-4o rates its own generated outputs more favorably than humans do, potentially inflating the "best model" results.

## Limitations
- The evaluation dataset (124 expert-annotated pairs) is not publicly available, limiting independent verification.
- The study uses proprietary internal documents without disclosing content specifics.
- Correlation of 0.70 between automated and expert evaluation indicates 30% uncorrelated variance.
- The "2× boosting factor" for internal documents was empirically chosen without showing sensitivity analysis.

## Confidence
- **High Confidence:** Hybrid search mechanism superiority, GPT-4o evaluation framework correlation, and chunk size optimization.
- **Medium Confidence:** Relevance boosting contribution (relies on unpublished internal document characteristics).
- **Low Confidence:** Generalizability to other regulatory domains - study focuses on pharmaceutical/compliance without testing in financial, aerospace, or other regulated industries.

## Next Checks
1. Test the exact system architecture on regulatory queries from a different industry (e.g., financial services) to assess domain transferability.
2. Systematically vary the boosting factor (1.0× to 3.0×) on the same 124 queries to determine optimal weighting and identify over-boosting thresholds.
3. Compare G-Eval scores against 20 additional hand-annotated examples from domain experts to quantify systematic bias in the automated framework.