---
ver: rpa2
title: Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling
  Policy Optimization
arxiv_id: '2512.07478'
source_url: https://arxiv.org/abs/2512.07478
tags:
- reward
- answer
- vspo
- grpo
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of training large language
  models (LLMs) for tool-integrated reasoning (TIR) tasks, where agents must iteratively
  plan, call external tools, and integrate returned information to solve complex problems.
  Two key issues hinder effectiveness: sparse, non-instructive rewards that provide
  limited guidance for intermediate steps and gradient degradation in Group Relative
  Policy Optimization (GRPO) when identical rewards within a rollout group yield zero
  advantage.'
---

# Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization

## Quick Facts
- arXiv ID: 2512.07478
- Source URL: https://arxiv.org/abs/2512.07478
- Reference count: 5
- The paper proposes Progressive Reward Shaping (PRS) and Value-based SamplingPolicy Optimization (VSPO) to improve LLM-based tool-integrated reasoning agents

## Executive Summary
This paper addresses the challenge of training large language models (LLMs) for tool-integrated reasoning (TIR) tasks, where agents must iteratively plan, call external tools, and integrate returned information to solve complex problems. Two key issues hinder effectiveness: sparse, non-instructive rewards that provide limited guidance for intermediate steps and gradient degradation in Group Relative Policy Optimization (GRPO) when identical rewards within a rollout group yield zero advantage. To overcome these challenges, the authors propose Progressive Reward Shaping (PRS), a curriculum-inspired reward design that introduces dense, stage-wise feedback to encourage models to first master parseable tool calls and then optimize for factual correctness and answer quality. They also introduce Value-based Sampling Policy Optimization (VSPO), an enhanced GRPO variant that replaces zero-advantage samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to SFT, PPO, and GRPO baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

## Method Summary
The authors propose two complementary approaches to enhance agentic reinforcement learning for tool-integrated reasoning tasks. Progressive Reward Shaping (PRS) introduces a curriculum-inspired reward design that provides dense, stage-wise feedback, helping models first learn to make parseable tool calls before optimizing for answer quality. This addresses the problem of sparse, non-instructive rewards that provide limited guidance for intermediate steps. Value-based Sampling Policy Optimization (VSPO) is an enhanced GRPO variant that addresses gradient degradation by replacing zero-advantage samples with prompts selected using a task-value metric that balances difficulty and uncertainty, while applying value-smoothing clipping to stabilize gradient updates. The combination of these approaches enables more effective training of LLM-based TIR agents that demonstrate better generalization across domains.

## Key Results
- PRS consistently outperforms traditional binary rewards on multiple short-form and long-form QA benchmarks
- VSPO achieves superior stability, faster convergence, and higher final performance compared to SFT, PPO, and GRPO baselines
- The combination of PRS and VSPO yields LLM-based TIR agents with better cross-domain generalization

## Why This Works (Mechanism)
None provided in the source material.

## Foundational Learning
1. **Tool-integrated reasoning (TIR)**: The ability of agents to iteratively plan, call external tools, and integrate returned information to solve complex problems. This is the target capability being enhanced.
   - Why needed: Modern complex reasoning tasks often require accessing external knowledge or computation that cannot be performed by LLMs alone
   - Quick check: Can the agent successfully parse tool outputs and incorporate them into subsequent reasoning steps?

2. **Group Relative Policy Optimization (GRPO)**: A policy optimization method that computes advantages relative to a group of rollouts, but suffers from gradient degradation when rewards are identical within a rollout group
   - Why needed: Standard RL methods need adaptation for the unique challenges of LLM training and tool integration
   - Quick check: Does the advantage computation produce non-zero gradients for meaningful learning updates?

3. **Task-value metric**: A metric that balances difficulty and uncertainty to select samples for training, replacing zero-advantage samples in VSPO
   - Why needed: Traditional sampling strategies can miss important learning opportunities when dealing with sparse rewards
   - Quick check: Does the metric successfully identify challenging yet learnable samples that improve model performance?

## Architecture Onboarding

Component map: LLM -> Tool Call Parser -> External Tools -> Answer Quality Evaluator -> PRS/VSPO Trainer

Critical path: Input query → LLM reasoning → Tool call generation → Tool execution → Response integration → Reward computation → Policy update

Design tradeoffs:
- Dense vs. sparse rewards: PRS trades computational overhead for more informative feedback during training
- Sampling strategy: VSPO's value-based sampling improves stability but may introduce bias toward certain difficulty levels
- Curriculum design: PRS's staged approach requires careful reward shaping but enables better learning progression

Failure signatures:
- Vanishing gradients when rewards are too sparse or identical across rollout groups
- Poor generalization when training focuses too heavily on either tool calling or answer quality
- Instability during training if value-smoothing clipping is not properly calibrated

First experiments:
1. Compare PRS with binary rewards on a simple tool-calling task to verify improved learning efficiency
2. Test VSPO's stability by measuring gradient variance across training iterations compared to standard GRPO
3. Evaluate cross-domain generalization by testing trained models on unseen task types within the TIR framework

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The paper does not adequately address potential computational overhead introduced by PRS's curriculum design and VSPO's value-based sampling
- While demonstrating effectiveness on QA benchmarks, the generalization claims to "multiple domains" are primarily supported by variations within the QA task family rather than truly diverse application domains
- The VSPO approach relies on a task-value metric with limited discussion of how it is computed and whether it introduces additional hyperparameters that require tuning

## Confidence
High confidence in the core technical contributions and their mathematical formulation. The PRS and VSPO algorithms are clearly defined with appropriate theoretical grounding.
Medium confidence in the empirical evaluation results. While the reported improvements are consistent, the ablation studies could be more comprehensive to isolate the contributions of each component.
Medium confidence in the generalization claims. The experiments cover multiple benchmarks but remain within the tool-integrated reasoning domain, limiting broader applicability conclusions.

## Next Checks
1. Conduct a detailed ablation study comparing PRS alone, VSPO alone, and their combination to quantify individual contributions and potential synergies, including analysis of training time and computational overhead.
2. Test the methods on non-QA domains such as code generation, mathematical reasoning, or creative writing tasks to evaluate true cross-domain generalization beyond tool-integrated reasoning.
3. Perform sensitivity analysis on the task-value metric parameters and value-smoothing clipping thresholds to understand their impact on convergence stability and final performance.