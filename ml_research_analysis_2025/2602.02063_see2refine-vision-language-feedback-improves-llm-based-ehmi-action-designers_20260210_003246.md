---
ver: rpa2
title: 'See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers'
arxiv_id: '2602.02063'
source_url: https://arxiv.org/abs/2602.02063
tags:
- ehmi
- action
- human
- arxiv
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents See2Refine, a human-free framework that uses
  vision-language model (VLM) perceptual evaluation as automated visual feedback to
  improve LLM-based eHMI action designers. Given a driving context and a candidate
  eHMI action, the VLM evaluates the perceived appropriateness of the action, and
  this feedback is used to iteratively revise the designer's outputs, enabling systematic
  refinement without human supervision.
---

# See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers

## Quick Facts
- arXiv ID: 2602.02063
- Source URL: https://arxiv.org/abs/2602.02063
- Reference count: 40
- Primary result: VLM-based feedback improves LLM eHMI action designers across three modalities with human-aligned performance gains

## Executive Summary
This paper introduces See2Refine, a human-free framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve LLM-based external human-machine interface (eHMI) action designers. The framework iteratively refines action generation through VLM scoring across six metrics, constructing preference pairs for direct preference optimization (DPO). Evaluated across lightbar, eyes, and arm modalities, See2Refine consistently outperforms prompt-only LLM designers and manual baselines, with improvements that generalize across settings and align with human preferences.

## Method Summary
See2Refine operates through an iterative loop: scenarios are generated from factor combinations, LLMs produce eHMI actions, Blender renders these to videos, and a VLM evaluates them across two phases (blind then informed). Kernel scores aggregate six metrics into a quality indicator used to construct preference pairs. DPO fine-tunes the LLM with importance sampling selecting 20% of scenarios per round based on low scores and small gaps. The process repeats for three rounds, improving action quality without human supervision while maintaining format constraints across three distinct eHMI modalities.

## Key Results
- DesignerLLM achieved 7.8% average improvement over base model across metrics in human studies
- Largest gains in targeting (15.4%) and acceptance (5.0-12.5%) metrics
- Performance improvements generalize across lightbar, eyes, and arm modalities
- VLM evaluations show strong alignment with human preferences when score differences are sufficiently large

## Why This Works (Mechanism)

### Mechanism 1
VLM-based perceptual evaluation provides a proxy reward signal that correlates with human judgments for eHMI action quality. Rendered eHMI action videos are evaluated by a VLM across six metrics (targeting, trust, user acceptance, consistency, certainty, similarity), aggregated into a kernel score serving as quality indicator for preference pairs. Core assumption: VLM perceptual judgments are sufficiently aligned with human preferences for this domain.

### Mechanism 2
Iterative DPO with importance sampling improves action generation more efficiently than random sampling or single-pass training. Each round selects 20% of scenarios prioritized by low best-scores, small score gaps, and fewer prior attempts. Core assumption: importance score formula correctly identifies scenarios most amenable to improvement.

### Mechanism 3
Two-phase VLM evaluation (blind then informed) captures distinct aspects of communication quality that improve at different rates. Phase 1 evaluates intention recognition, targeting, and trust without revealing intended message. Phase 2 evaluates user acceptance and consistency after revealing message. Core assumption: both phases are necessary for effective training.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: See2Refine uses DPO instead of full RLHF because it avoids training a separate reward model
  - Quick check: Given two action outputs y+ (preferred) and y- (dispreferred), can you explain why DPO optimizes the log-ratio of probabilities rather than maximizing absolute scores?

- Concept: Vision-Language Model (VLM) as Judge
  - Why needed here: The entire framework depends on VLM-generated scores being reliable proxies for human judgment
  - Quick check: If the VLM judge gives systematically higher trust scores to one modality (e.g., eyes) than another (e.g., lightbar), how would this affect DPO training across modalities?

- Concept: eHMI Design Space
  - Why needed here: The three modalities have different control dimensions and constraints
  - Quick check: Why might the 16-bit lightbar format (e.g., "0000000011111111") cause higher formatting errors in LLMs compared to JSON-style outputs for arm joints?

## Architecture Onboarding

- Component map: Scenario Generator -> Action Renderer (Blender) -> VLM Evaluator -> Shared Action Database -> DesignerLLM -> Importance Sampler
- Critical path: Scenario → LLM generates action → JSON validation → Blender renders → VLM evaluates → Kernel score computed → Preference pairs constructed → DPO fine-tunes LLM → Repeat
- Design tradeoffs: Plain backgrounds reduce rendering time 10× but may reduce robustness to real-world visual clutter; two-phase evaluation adds VLM API cost but captures distinct quality dimensions
- Failure signatures: High formatting errors (>10%) indicate the model struggles with structured outputs—especially for lightbar (16-bit strings); flat trust scores across rounds suggest VLM-human misalignment on this metric
- First 3 experiments: 1) Baseline validation: Run format-aware fine-tuning only (no DPO) on held-out scenarios; 2) Single-metric ablation: Train with only Phase 2 metrics; 3) Cross-modality transfer: Train DesignerLLM on eyes modality, evaluate zero-shot on arm modality

## Open Questions the Paper Calls Out

### Open Question 1
How can real human annotators be effectively integrated into the See2Refine refinement loop to capture nuanced preferences across different cultural backgrounds and age groups? Current work uses VLM as substitute but cannot capture fine-grained individual or cultural differences in eHMI perception.

### Open Question 2
How does the simplified rendering pipeline (plain backgrounds, excluded environmental factors) affect transferability of learned eHMI actions to real-world scenarios with varying lighting, occlusions, and weather conditions? Paper deliberately excludes these factors to reduce rendering time and computational cost.

### Open Question 3
Why does the "trust" metric show minimal improvement during training and weak correlation between VLM and human raters, and can alternative formulations better capture this critical dimension? Trust metrics inherited from prior eHMI work without validating their appropriateness for VLM-based evaluation.

### Open Question 4
At what minimum score difference threshold does VLM-human preference alignment degrade, and how does this threshold vary across eHMI modalities? Paper does not empirically determine this threshold or investigate modality-specific differences in alignment reliability.

## Limitations

- VLM-human alignment is strongest only when rating score differences between actions are sufficiently large, with unclear minimum threshold
- Simplified rendering pipeline excludes environmental factors like lighting conditions, obstructions, and weather that could influence visual perception
- Small human study sample size (N=12) with high inter-rater variance (ICC 0.087-0.131) indicates individual preferences vary substantially

## Confidence

- VLM-as-feedback mechanism: Medium - demonstrates correlation but lacks systematic validation on edge cases and cross-cultural contexts
- Importance sampling mechanism: Medium-High - ablation shows performance drops when removed but lacks direct random sampling comparison
- Human-subject results: Low-Medium - small sample size and high variance, but acknowledges these limitations

## Next Checks

1. Cross-cultural VLM validation: Test whether VLM judgments maintain human alignment when evaluated by participants from different cultural backgrounds, particularly for gesture-based arm modality

2. VLM judge ablation: Compare performance using different VLM judges (GPT-4o, Claude-3.5) to assess whether results depend on specific model capabilities versus general VLM architecture

3. Real-world deployment stress test: Evaluate DesignerLLM actions in scenarios with visual clutter, occlusions, and adverse weather conditions that differ from the plain-background training environment to assess robustness transfer