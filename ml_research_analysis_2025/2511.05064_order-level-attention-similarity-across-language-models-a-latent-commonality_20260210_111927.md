---
ver: rpa2
title: 'Order-Level Attention Similarity Across Language Models: A Latent Commonality'
arxiv_id: '2511.05064'
source_url: https://arxiv.org/abs/2511.05064
tags:
- attention
- should
- across
- language
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether context aggregation patterns across
  language models share commonalities. The authors propose Order-Level Attention (OLA)
  as a unified representation of attention mechanisms across different models, derived
  from order-wise decomposition of Attention Rollout.
---

# Order-Level Attention Similarity Across Language Models: A Latent Commonality

## Quick Facts
- arXiv ID: 2511.05064
- Source URL: https://arxiv.org/abs/2511.05064
- Reference count: 40
- Key outcome: Transferable OLA Adapter (TOA) successfully transfers adapters across 12 language models, improving performance on relation extraction, named entity recognition, dependency parsing, and part-of-speech tagging tasks.

## Executive Summary
This paper investigates whether context aggregation patterns across language models share commonalities by proposing Order-Level Attention (OLA) as a unified representation derived from order-wise decomposition of Attention Rollout. The authors demonstrate that OLA at the same order exhibits significant similarity across models and implicitly encodes syntactic knowledge. Based on these findings, they introduce Transferable OLA Adapter (TOA), a training-free cross-model adapter transfer method that successfully transfers adapters across 12 language models, achieving substantial performance improvements on downstream tasks.

## Method Summary
The paper introduces Order-Level Attention (OLA) through order-wise decomposition of Attention Rollout matrices, isolating specific depths of context mixing by expanding the product into polynomial terms. The Transferable OLA Adapter (TOA) method trains an adapter on source LM OLA matrices and transfers it to target LM OLA without retraining. The process involves extracting raw attention weights, computing 1st and 2nd-order OLA, preprocessing with outlier removal and normalization, training an Axial Transformer adapter, and freezing it for transfer to target models.

## Key Results
- OLA matrices at the same order show significant similarity across different language models
- TOA successfully transfers adapters across 12 language models without retraining
- Transferring TOA from LLaMA3-3B to Qwen2-1.5B increased relation prediction accuracy from 7.69% to 34.90%
- TOA performs better on syntax-dependent tasks (dependency parsing, POS tagging) than on semantics-driven tasks (relation extraction, NER)

## Why This Works (Mechanism)

### Mechanism 1: Order-Wise Decomposition as a Unifying Representation
The paper decomposes Attention Rollout by aggregation "order" to reveal latent structural similarities between diverse language models. By isolating specific depths of context mixing through polynomial expansion, different LMs converge to analogous "optimal attention patterns" for syntax that manifest primarily in lower-order aggregation paths.

### Mechanism 2: OLA as a Syntactic Knowledge Encoder
OLA functions as a model-agnostic representation of syntactic structure, implicitly encoding dependency trees and POS information. Since OLA captures how tokens relate across layers, the resulting matrices reflect the "backbone" of sentence structure, allowing lightweight adapters to map these matrices to syntactic labels without accessing LM hidden states.

### Mechanism 3: Feature-Space Alignment for Training-Free Transfer
Replacing native hidden-state feature space with unified OLA space enables zero-shot cross-model adapter transfer. Adapters trained on source LM OLA can be applied to target LM OLA because the input distribution remains consistent across models, allowing adapters to learn mappings from "syntactic fingerprints" to labels that generalize across models.

## Foundational Learning

- **Attention Rollout (Abnar & Zuidema, 2020):** Base signal tracking information flow across layers through multiplying attention matrices with identity matrices. *Why needed:* Understanding how multiplying attention matrices $(A+I)$ across layers differs from averaging final layer attention heads is crucial for grasping why decomposition is necessary.

- **Polynomial Expansion / Order-wise Decomposition:** Viewing information flow as a sum of paths with $k$ aggregations (order $k). *Why needed:* The core novelty is isolating structural dependencies through mathematical expansion of $(A+B)^N$ to separate 0-order (residual only) vs N-order (full mixing) paths.

- **Adapter Transfer / PEFT (Parameter-Efficient Fine-Tuning):** Moving small adapter layers between models without full fine-tuning. *Why needed:* Understanding standard adapter architecture (tiny bottleneck layers) is essential to recognize why input feature alignment is the critical bottleneck for transfer.

## Architecture Onboarding

- **Component map:** LM Backbone -> OLA Extractor -> TOA Adapter -> Task-specific heads
- **Critical path:** The OLA Extractor is vital, particularly the normalization step that masks outliers ($> \mu + 3\sigma$) and normalizes by row sum to ensure visual patterns match between models.
- **Design tradeoffs:** Order selection uses 1st and 2nd order OLA (higher orders become noisy; lower orders lack context); tokenizer alignment requires resizing OLA maps to fixed dimensions (e.g., 50x50) with triangular masking for causal LMs.
- **Failure signatures:** Attention Sink Dominance (OLA visualizations look identical due to improper outlier removal); Tokenizer Mismatch Artifacts (fine-grained syntactic relations lost in downsampling).
- **First 3 experiments:**
  1. Visual Sanity Check: Extract 1st-order OLA from BERT and RoBERTa, plot heatmaps side-by-side to verify structural similarity.
  2. Syntactic Probe: Train MLP on BERT-base 1st-order OLA for POS tagging on CoNLL2000; verify >80% accuracy.
  3. Low-Resource Transfer: Train TOA adapter on small model (BERT-base) for NER, freeze, and apply to different model (RoBERTa-base) to confirm transfer gain.

## Open Questions the Paper Calls Out

### Open Question 1
Can OLA be extended to capture semantic knowledge in addition to syntactic structures? The current decomposition effectively isolates structural dependencies but fails to retain complex semantic features required for tasks like Relation Extraction. A modified OLA incorporating FFN interactions or different aggregation strategies could demonstrate high transferability on semantic benchmarks comparable to syntactic tasks.

### Open Question 2
Does TOA utility diminish as target model capacity increases? The paper notes TOA may not improve performance for large, high-capacity models but delivers significant gains for smaller models. A scaling law analysis plotting TOA performance gain against target model parameter count would determine if transfer efficiency drops to zero at higher scales.

### Open Question 3
What adjustments are required to close the transfer performance gap between MLMs and CLMs? TOA achieves stronger performance with MLMs than CLMs due to bidirectional attention capturing richer contextual information. A successful adaptation of OLA specifically for CLMs (e.g., modified masking strategy) achieving parity with MLMs would resolve this gap.

## Limitations
- Core claims rest on a relatively small sample of 12 language models trained on similar web-scale corpora, with generalizability to models trained on fundamentally different data distributions untested.
- While successful for syntactic tasks, transfer performance on semantic tasks remains modest, suggesting OLA's effectiveness may be domain-limited.
- Implementation details of the Axial Transformer adapter are incompletely specified, particularly regarding head count and FFN dimensions, which could affect reproducibility.

## Confidence
- OLA similarity across models: High (strong visual evidence and quantitative metrics support cross-model similarity)
- OLA as syntactic knowledge encoder: Medium (reasonable performance on syntactic tasks but lacks direct linguistic validation)
- Training-free adapter transfer: Medium (promising results with significant variance across task-model combinations)

## Next Checks
1. Domain generalization test: Evaluate OLA similarity and TOA transfer when source and target models are trained on fundamentally different data distributions (e.g., code-specialized vs. general web text).
2. Higher-order OLA analysis: Systematically evaluate performance contribution of higher-order OLA components (3rd order and beyond) across different task types to clarify whether focus on 1st and 2nd order is optimal.
3. Adapter architecture ablation: Conduct controlled experiment varying Axial Transformer's architectural parameters (layer count, head count, FFN dimensions) while holding OLA extraction constant to isolate primary driver of transfer success.