---
ver: rpa2
title: 'PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured Data
  with Text and Relational Information'
arxiv_id: '2502.21087'
source_url: https://arxiv.org/abs/2502.21087
tags:
- data
- semi-structured
- text
- information
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PASemiQA addresses question answering on semi-structured data
  containing both text and relational information, which existing RAG and KGQA methods
  struggle with individually. The method uses a two-step approach: first, it generates
  a plan identifying relevant text and relational paths in the semi-structured data
  through a path generation model; second, it employs an LLM agent to traverse the
  data according to this plan to extract necessary information for answering the question.'
---

# PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured Data with Text and Relational Information

## Quick Facts
- arXiv ID: 2502.21087
- Source URL: https://arxiv.org/abs/2502.21087
- Authors: Hansi Yang; Qi Zhang; Wei Jiang; Jianguo Li
- Reference count: 34
- Key outcome: PASemiQA achieves Hit@1 scores of 0.2968, 0.4316, and 0.4586 on PrimeKG, MAG, and Amazon datasets respectively, outperforming existing RAG and KGQA baselines.

## Executive Summary
PASemiQA addresses question answering on semi-structured data containing both text and relational information, which existing RAG and KGQA methods struggle with individually. The method uses a two-step approach: first, it generates a plan identifying relevant text and relational paths in the semi-structured data through a path generation model; second, it employs an LLM agent to traverse the data according to this plan to extract necessary information for answering the question. Experiments on three semi-structured datasets (PrimeKG, MAG, Amazon) show PASemiQA outperforms existing RAG and KGQA baselines, achieving Hit@1 scores of 0.2968, 0.4316, and 0.4586 respectively. The method demonstrates the effectiveness of jointly utilizing text and relational information for question answering on semi-structured data.

## Method Summary
PASemiQA is a two-stage framework for question answering on semi-structured data represented as text-attributed heterogeneous graphs. In the planning stage, it uses hybrid node matching (keyword + embedding) to identify relevant nodes, then fine-tunes a path generation model (LLaMa2-7B) to produce relation paths from these nodes to answer nodes. In the execution stage, a GPT-4 agent iteratively executes Search, Query, and Finish actions to traverse the graph and extract information. The Search action retrieves neighbors via graph structure, Query retrieves nodes via text similarity, and Finish terminates the process. The agent operates for T steps with K neighbors per step, balancing accuracy and latency.

## Key Results
- Achieves Hit@1 scores of 0.2968 on PrimeKG, 0.4316 on MAG, and 0.4586 on Amazon datasets
- Outperforms VSS baseline by margins of 0.0654, 0.0254, and 0.0484 respectively on the three datasets
- Ablation studies show removing plan generation reduces performance to 0.3754 on Amazon, confirming the importance of the planning module

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid node matching improves retrieval of relevant starting nodes when entity names are ambiguous or absent.
- Mechanism: The system first attempts keyword matching to find nodes by exact name. If no matches are found, it falls back to embedding-based similarity between the question and node text descriptions. This hybrid approach handles both explicit entity references (e.g., drug names) and descriptive queries (e.g., "fashionable reversible bucket hats").
- Core assumption: Questions either contain explicit entity names OR have sufficient semantic content in descriptions to enable embedding-based matching.
- Evidence anchors:
  - [section 4.1.1]: "We first go through the question to extract all mentioned nodes based on their names as keyword... If we do not obtain any nodes in keyword matching, we will select top-k nodes based on the cosine similarity"
  - [Table 5]: Shows PASemiQA hybrid method outperforms exact name matching (0.4316 vs 0.4062 on MAG) and embedding similarity alone (0.4316 vs 0.4158)
  - [corpus]: Weak direct evidence; corpus focuses on semi-structured content broadly rather than hybrid retrieval specifically
- Break condition: Fails when questions lack both explicit names AND have ambiguous or generic descriptions that embed poorly.

### Mechanism 2
- Claim: Fine-tuned path generation model produces more faithful relation paths than in-context learning alone.
- Mechanism: A pre-trained LLM (LlaMa2-7B) is fine-tuned using ground-truth relation paths between topic nodes and answers. The training objective maximizes the probability of generating correct relation paths given the question. This supervised learning grounds path generation in the actual graph structure rather than relying solely on the LLM's pre-trained knowledge.
- Core assumption: Ground-truth paths in training data generalize to unseen questions with similar reasoning patterns.
- Evidence anchors:
  - [section 4.1.2]: "we maximize the probability of path generation model to generate faithful relation paths"
  - [Table 6]: Fine-tuned PASemiQA (0.2968 PrimeKG, 0.4586 Amazon) outperforms GPT-4 in-context (0.2674 PrimeKG, 0.4162 Amazon) and None baseline (0.2868 PrimeKG, 0.3754 Amazon)
  - [corpus]: No direct evidence; corpus papers do not compare fine-tuned vs in-context path generation
- Break condition: Fails when test questions require relation paths not covered in training distribution, or when graph schema differs significantly.

### Mechanism 3
- Claim: Agent-based traversal with structured actions enables joint reasoning over text and graph structure.
- Mechanism: The LLM agent iteratively executes three action types: Search[node] retrieves top-K neighbors using the graph structure; Query[query] retrieves nodes via text embedding similarity; Finish[answer] terminates. This allows the agent to alternate between exploiting relational paths (Search) and textual similarity (Query) within a unified reasoning chain.
- Core assumption: LLMs can reliably select appropriate actions and synthesize information across multiple retrieval steps.
- Evidence anchors:
  - [section 4.2]: "The agent can take three different actions in total: Search, Query and Finish, which focus on different aspects of semi-structured data"
  - [Table 7]: Case study shows agent correctly using Search[Th. Friedrich] followed by Finish with correct paper title
  - [corpus]: "LLM/Agent-as-Data-Analyst: A Survey" discusses agent-based approaches for data analysis but does not validate this specific action schema
- Break condition: Fails when the agent hallucinates actions, selects wrong retrieval type, or exceeds step limit (T) before reaching answer.

## Foundational Learning

- Concept: Semi-structured data as text-attributed heterogeneous graphs
  - Why needed here: Understanding Definition 1 is prerequisite for grasping why PASemiQA needs both text and relational reasoning—unlike pure RAG (text only) or KGQA (relations only).
  - Quick check question: Given a biomedical graph with drug nodes having text descriptions and edges for drug-drug interactions, which existing method would fail to answer "What side effects are mentioned for drugs that interact with aspirin?"

- Concept: KL divergence for distribution matching
  - Why needed here: The path generation model is trained using KL divergence (Equation 4) between the model distribution and target distribution over ground-truth paths.
  - Quick check question: Why does minimizing KL divergence(Q || P) encourage the model to assign higher probability to ground-truth relation paths?

- Concept: Agent frameworks with thought-action-observation cycles
  - Why needed here: Section 4.2 implements a ReAct-style agent that generates thoughts, executes actions, and processes observations iteratively.
  - Quick check question: What is the difference between Search[node] and Query[query] actions in terms of which aspect of semi-structured data they access?

## Architecture Onboarding

- Component map:
  - **Planning Module** (Section 4.1): Node matching (keyword + embedding) → Path generation model (fine-tuned LlaMa2-7B)
  - **Agent Framework** (Section 4.2): LLM agent (GPT-4) with Search/Query/Finish actions, T-step iteration loop
  - **Data Interface**: Text-attributed heterogeneous graph G = {V, E, R, f_T, f_R}

- Critical path:
  1. Question q → Node matching → Initial node set V_q
  2. Question q → Path generation model → Relation paths {z_i}
  3. Agent loop (T iterations): Thought → Action → Observation → Update triplet set O_t
  4. Finish[answer] → Output final answer node

- Design tradeoffs:
  - **Accuracy vs. Latency**: Table 3 shows PASemiQA (28.19s PrimeKG) is slower than VSS (0.54s) but comparable to VSS+GPT-4 reranker (26.97s). Trade latency for higher Hit@1.
  - **K (neighbors per step)**: Figure 3 shows K=3-5 is optimal; too small loses recall, too large adds noise and latency.
  - **T (reasoning steps)**: Figure 4 shows T≈5 balances completeness vs. efficiency.
  - **LLM choice**: Table 4 shows GPT-4 outperforms LLaMa3-70B and Qwen2-72B on some datasets; model selection depends on domain (e.g., Qwen2 competitive on Amazon).

- Failure signatures:
  - **Amazon with no plan generation**: Table 6 shows performance drops to 0.3754 (vs 0.4586) because text-only questions need the "text" path indicator.
  - **ToG on PrimeKG**: Hit@1 = 0.1321 (Table 2) likely due to biomedical terminology unfamiliarity.
  - **Keyword-only matching on Amazon**: Table 5 shows 0.2102 vs 0.4586 because product names are unstable.

- First 3 experiments:
  1. **Baseline comparison on STaRK datasets**: Replicate Table 2 by running PASemiQA vs VSS, ToG, RoG, GoG on PrimeKG/MAG/Amazon test splits; expect Hit@1 within ±0.02 of reported values.
  2. **Ablation of plan generation**: Remove path generation module (set {z_i} = empty) and measure performance drop; expect largest drop on Amazon (0.3754 vs 0.4586 per Table 6).
  3. **Hyperparameter sweep for K and T**: Run grid search K ∈ {1,3,5,7} and T ∈ {1,3,5,7}; expect optimal at K≈5, T≈5 per Figures 3-4.

## Open Questions the Paper Calls Out
- Can a specialized LLM agent, trained specifically on the action trajectories of a target semi-structured dataset, outperform general-purpose models like GPT-4 within the PASemiQA framework?
- How robust is the fine-tuned path generation model when applied in zero-shot settings to semi-structured data with completely different edge types or graph schemas?
- To what extent does the "Plan-Assisted" constraint limit the agent's ability to reason compared to unrestricted traversal, particularly if the generated relation path is sub-optimal?

## Limitations
- Generalizability beyond STaRK benchmark domains is uncertain due to reliance on ground-truth relation paths for training
- Computational cost of running GPT-4 for multiple reasoning steps limits practical deployment
- No analysis of scalability for graphs with millions of nodes or handling dynamic graph updates

## Confidence
- **High Confidence**: Empirical performance improvements over baselines on STaRK datasets are well-supported by experimental results (Tables 2-7)
- **Medium Confidence**: Mechanism explanations for hybrid node matching and fine-tuned path generation are plausible but rely on indirect evidence
- **Low Confidence**: Claims about applicability to arbitrary semi-structured data types are not substantiated

## Next Checks
1. **Cross-domain generalization**: Evaluate PASemiQA on semi-structured datasets from different domains (e.g., financial data, legal documents) not seen during training to assess whether the path generation model can transfer learned reasoning patterns.

2. **Error analysis on edge cases**: Systematically analyze failure cases where PASemiQA underperforms, particularly focusing on questions requiring: (a) reasoning over multiple disconnected subgraphs, (b) handling of ambiguous entity mentions, and (c) questions with implicit rather than explicit relational constraints.

3. **Latency vs. accuracy tradeoff validation**: Conduct a comprehensive study varying the number of reasoning steps T and neighbors K across all three datasets to validate that the claimed optimal values (T=5, K=5) generalize and to quantify the accuracy-cost tradeoff curve.