---
ver: rpa2
title: Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level
  Physics Problem Solving
arxiv_id: '2510.00919'
source_url: https://arxiv.org/abs/2510.00919
tags:
- physics
- answer
- question
- arxiv
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhoPile, the first multimodal RAG benchmark
  for Olympiad-level physics, featuring 390 evaluation questions and 2,662 retrieval
  corpus items across diverse physics subfields. It evaluates both text-only and vision-language
  models with multiple retrievers, demonstrating that retrieval-augmented generation
  can improve model performance, with the best configurations achieving up to 30.51%
  pass rate and 6.20 average score on multimodal tasks.
---

# Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving

## Quick Facts
- **arXiv ID:** 2510.00919
- **Source URL:** https://arxiv.org/abs/2510.00919
- **Reference count:** 40
- **Primary result:** Introduces PhoPile, the first multimodal RAG benchmark for Olympiad-level physics, demonstrating retrieval-augmented generation can improve model performance with up to 30.51% pass rate.

## Executive Summary
This paper introduces PhoPile, the first multimodal RAG benchmark for Olympiad-level physics, featuring 390 evaluation questions and 2,662 retrieval corpus items across diverse physics subfields. The study evaluates both text-only and vision-language models with multiple retrievers, demonstrating that retrieval-augmented generation can significantly improve model performance. The best configurations achieve up to 30.51% pass rate and 6.20 average score on multimodal tasks. The research highlights the importance of domain-specific retrievers and introduces a new LLM-as-judge evaluation framework tailored for physics reasoning.

## Method Summary
The study constructs PhoPile, a benchmark dataset of 390 Olympic-level physics problems with 2,662 retrieval corpus items. It employs a RAG framework where retrievers (BM25, Contriever, CLIP, etc.) find relevant solved problems, which are then injected into prompts for generators (GPT-4, Gemini-Pro, Llama-3). An LLM-as-judge (GPT-4) evaluates solutions on a 0-10 scale, measuring Average Score and Pass Rate. The framework also includes a reflection mechanism where GPT-4 compares RAG and non-RAG answers to mitigate retrieval noise.

## Key Results
- Retrieval-augmented generation improves performance, with Gemini-Pro + Contriever achieving 30.51% pass rate on multimodal tasks
- Dense retrievers (Contriever) outperform sparse ones (BM25) for some models, though BM25 is faster (7s vs 255s per query)
- 2-shot retrieval is optimal; 3-shot retrieval introduces significant noise penalty
- LLM-as-judge achieves 87% accuracy with tolerance threshold of k=3 points
- Reflection mechanism reduces negative impact of noisy retrievals, improving scores from 5.4 to 19.38

## Why This Works (Mechanism)

### Mechanism 1: Problem Retrieval Primes Solution Structure
Retrieving solved physics problems improves reasoning by priming the model with relevant laws and solution structures, analogous to a student reviewing past exams. The retriever matches target problems against the corpus to find top-k similar question-answer pairs, which are injected into the prompt to activate relevant physical principles and formulaic patterns. This works because the model possesses sufficient underlying logical capacity to adapt retrieved solution strategies to new problem parameters. Performance degrades when retrievers select semantically similar but physically distinct examples, introducing noise.

### Mechanism 2: LLM-as-Judge Enables Partial Credit Evaluation
An LLM-as-judge can approximate human grading for complex physics solutions by scoring reasoning steps rather than just final answers. GPT-4 is prompted with a standardized rubric and compares the model's candidate solution against a reference solution, assigning partial credit for intermediate steps. This mitigates the binary failure mode of exact string matching. The mechanism fails when outputs differ significantly in format or notation from the judge's training distribution, or when reference solutions contain errors.

### Mechanism 3: Reflection Mitigates Retrieval Noise
A "Reflection" mechanism mitigates retrieval noise by explicitly comparing RAG-generated answers against non-RAG baselines. The system generates two candidate answers and uses GPT-4 as a referee to analyze which is more accurate based on problem constraints. This works because the reflection model can identify when retrieved context is misleading better than it can solve problems from scratch. The mechanism fails if both candidates are incorrect but one appears superficially more detailed through hallucination.

## Foundational Learning

- **In-Context Learning (ICL) with Noise**: The system relies on models learning from retrieved examples (ICL), but unlike standard ICL, examples are dynamically retrieved and may contain irrelevant data. Quick check: Can the model distinguish between the structure of a retrieved solution and the specific numeric values in it?

- **Multimodal Alignment (Text-Image)**: 33% of problems contain images, requiring the generator to map visual data (diagrams, graphs) to textual physical constraints. Quick check: Does the retriever index image embeddings (CLIP/ALIGN) or just the text surrounding the image?

- **Chain-of-Thought (CoT) Evaluation**: Physics problems require partial credit for derivations, necessitating an evaluation framework that parses thinking rather than just final answers. Quick check: How does the grading prompt handle valid alternative solution paths that differ from the reference answer?

## Architecture Onboarding

- **Component map:** Query Input -> Retrieval (BM25/Contriever/CLIP) -> Prompt Augmentation -> Generation (GPT-4/Llama-3) -> Reflection (Optional) -> Evaluation (GPT-4 Judge)
- **Critical path:** 1) Query Input (Text + Image placeholders) 2) Retrieval (Top-k search on PhoPile) 3) Prompt Augmentation (Injecting retrieved Q&A) 4) Generation (Model outputs solution) 5) Reflection (Optional: Compare RAG vs. No-RAG answers) 6) Evaluation (Score 0â€“10 via Judge)
- **Design tradeoffs:** BM25 vs. Dense Retrieval: BM25 is faster (7s vs 255s) but dense retrievers (Contriever) yield higher pass rates for some models. Shot Count: 2-shot retrieval is standard; increasing to 3-shot can introduce noise.
- **Failure signatures:** Condition Leakage (using variables from retrieved examples not in target question), Guidance instead of Solution (outputting hints rather than answers), Visual Hallucination (misinterpreting diagrams).
- **First 3 experiments:** 1) Baseline Retrieval Test: Run GPT-3.5/4 on PhoPile-Test using BM25 with k=2 to establish text-only baseline. 2) Ablation on Noise: Test 1-shot vs. 3-shot retrieval on "Hard" problems to quantify noise penalty. 3) Judge Calibration: Compare GPT-4 evaluator scores against human annotators on 20 samples to verify k=2 tolerance threshold.

## Open Questions the Paper Calls Out

1. **Domain-Specific Retrievers:** Can a retriever designed to match physics problems based on shared physical theorems rather than semantic similarity outperform general-purpose retrievers? The paper notes general retrievers fail to find relevant questions using the same theorem, instead prioritizing semantic similarity. This remains unresolved as the study only benchmarks general-purpose retrievers.

2. **Context Conflation Prevention:** How can foundation models be refined to strictly distinguish between retrieved context and target problem constraints to prevent context conflation errors? The paper identifies this as a distinct failure mode where models use conditions from retrieved questions as if they were known conditions in the current question, but the reflection mechanism doesn't specifically solve this issue.

3. **Multimodal Cross-Referencing:** Does incorporating multimodal cross-referencing between text, formulas, and images significantly improve reasoning accuracy over current joint embedding methods? The paper lists lack of "multimodal cross-referencing" as a limitation, suggesting it has potential to further improve accuracy and robustness, but current approaches use joint text-image embeddings that may not capture precise structural relationships.

## Limitations

- **Grader Bias Risk:** Reliance on LLM-as-judge (GPT-4) may introduce grader bias, particularly for novel solution approaches, despite achieving 87% accuracy with tolerance
- **Domain-Specificity:** Retrieval corpus is physics Olympiad-specific, limiting generalizability to other STEM fields
- **Incomplete Ablation:** Reflection mechanism effectiveness demonstrated through single case study rather than systematic ablation
- **No Direct Retrieval Quality:** Benchmark doesn't evaluate retrieval quality directly (e.g., recall@k), focusing instead on end-task performance

## Confidence

- **High Confidence:** Core claim that RAG improves physics problem-solving is well-supported by quantitative results (30.51% pass rate improvement) and clearly specified methodology
- **Medium Confidence:** Dense retrievers (Contriever) outperforming sparse ones is context-dependent and requires further validation; LLM-as-judge framework is novel but needs more validation against human grading
- **Low Confidence:** Reflection mechanism effectiveness shown through limited evidence rather than systematic study, making general applicability uncertain

## Next Checks

1. **Grader Consistency Validation:** Conduct direct comparison between GPT-4 evaluator scores and human expert grading on stratified sample of 50 PhoPile problems (spanning difficulty levels and subfields) to quantify systematic biases and establish confidence intervals for tolerance threshold

2. **Retrieval Quality Analysis:** For top-2 retrieved examples per question, compute recall@k and mean reciprocal rank against ground truth solutions in corpus to separate retrieval quality from generation quality in overall performance

3. **Cross-Domain Transfer Test:** Adapt PhoPile framework to different STEM domain (e.g., chemistry Olympiad problems) using same retriever-generator-judge architecture to test whether observed RAG benefits generalize beyond physics