---
ver: rpa2
title: 'FFN Fusion: Rethinking Sequential Computation in Large Language Models'
arxiv_id: '2503.18908'
source_url: https://arxiv.org/abs/2503.18908
tags:
- layers
- fusion
- attention
- block
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FFN Fusion reduces sequential computation in LLMs by identifying\
  \ and parallelizing sequences of Feed-Forward Network layers that can operate independently.\
  \ The method fuses consecutive FFN layers into wider parallel layers after attention\
  \ pruning, achieving a 1.71\xD7 speedup in inference latency and 35\xD7 lower per-token\
  \ cost while maintaining model accuracy."
---

# FFN Fusion: Rethinking Sequential Computation in Large Language Models

## Quick Facts
- arXiv ID: 2503.18908
- Source URL: https://arxiv.org/abs/2503.18908
- Reference count: 24
- Reduces sequential computation in LLMs by fusing consecutive FFN layers into parallel execution, achieving 1.71× inference speedup with minimal accuracy loss

## Executive Summary
FFN Fusion is a technique that identifies and parallelizes sequences of Feed-Forward Network layers that can operate independently, significantly reducing sequential computation in large language models. The method works by pruning attention layers using the Puzzle framework, computing inter-layer dependencies, and fusing consecutive FFN blocks into wider parallel layers. Applied to Llama-3.1-405B, this approach created Ultra-253B-Base with strong benchmark performance while achieving substantial inference speedups and cost reductions.

## Method Summary
The method proceeds in three main stages: First, apply Puzzle NAS to prune attention layers from Llama-3.1-405B, identifying 50 consecutive FFN-only blocks. Second, compute a block-wise dependency matrix using cosine distance between model outputs with/without each block dropped, identifying low-dependency FFN sequences suitable for fusion. Third, fuse these sequences by concatenating their SwiGLU weights along the hidden dimension (excluding the last FFN in each sequence which is uniquely sensitive), then fine-tune using multi-stage knowledge distillation on the Distillation Mix dataset. The approach is particularly effective at larger scales and complements existing optimization techniques.

## Key Results
- 1.71× inference latency speedup on Ultra-253B-Base
- 35× lower per-token cost compared to Llama-3.1-405B
- Strong benchmark performance across MMLU, MMLU-Pro, Arena Hard, HumanEval, and MT-Bench
- Fusion effectiveness increases with model scale

## Why This Works (Mechanism)
The technique exploits the observation that many consecutive FFN layers in LLMs can operate independently without significantly affecting model outputs. By identifying these low-dependency sequences through attention pruning and dependency analysis, FFN Fusion parallelizes what would otherwise be sequential computation. The fusion works by concatenating weights of multiple FFNs, allowing them to process inputs simultaneously rather than in sequence, while maintaining the same overall model capacity through wider hidden dimensions.

## Foundational Learning

**Attention Pruning**: Removes attention layers to create consecutive FFN-only blocks. Needed because FFN layers are naturally parallelizable while attention layers create dependencies. Quick check: Verify that pruning maintains model accuracy while creating sufficient FFN sequences.

**Dependency Matrix Computation**: Measures inter-layer dependencies using cosine distance between outputs with/without each block. Needed to identify which FFN sequences can be safely fused without accuracy loss. Quick check: Visualize heatmaps to confirm low-dependency regions match fusion boundaries.

**SwiGLU Weight Concatenation**: Combines multiple FFN weight matrices by concatenating along hidden dimension. Needed to implement parallel execution while preserving model capacity. Quick check: Verify fused weights maintain dimensional compatibility with input/output.

## Architecture Onboarding

**Component Map**: Llama-3.1-405B -> Puzzle NAS (pruning) -> Dependency Analysis -> FFN Fusion -> KD Training -> Ultra-253B-Base

**Critical Path**: Attention pruning → Dependency computation → FFN fusion → Knowledge distillation. Each stage must complete successfully before the next can proceed.

**Design Tradeoffs**: 
- Fusion improves latency but increases per-layer memory usage due to wider hidden dimensions
- Excluding final FFNs preserves accuracy but reduces potential speedup
- Multi-stage KD balances convergence with computational cost

**Failure Signatures**:
- Sharp accuracy drop indicates fusing high-dependency blocks or including final FFNs
- OOM errors suggest fusion groups are too large for available memory
- Poor convergence suggests inadequate KD or improper weight initialization

**First Experiments**:
1. Run Puzzle NAS to identify FFN-only regions and verify 50 consecutive blocks exist
2. Generate dependency matrix and confirm low values in proposed fusion regions [66,73], [74,85], etc.
3. Test fusion of small FFN sequences (2-3 layers) to validate methodology before scaling up

## Open Questions the Paper Calls Out

**Open Question 1**: Can FFN Fusion be effectively extended to Mixture-of-Experts (MoE) architectures while maintaining sparse activation patterns and efficient expert routing? The paper notes this as future work but highlights that MoE's routing mechanisms make direct weight concatenation infeasible.

**Open Question 2**: What mechanisms cause the final FFN layer in attention-removed sequences to be uniquely sensitive to fusion compared to earlier layers? The paper documents this empirical phenomenon but offers no theoretical explanation for why these specific layers resist parallelization.

**Open Question 3**: What actual speedups can full transformer block parallelization achieve in specialized inference frameworks like TensorRT-LLM or vLLM? Current experiments used unoptimized HuggingFace implementations, and the paper acknowledges that production frameworks lack native support for this parallelization pattern.

**Open Question 4**: Why does FFN Fusion effectiveness increase with model scale, and what does this imply about the relationship between model size and natural parallelizability? The paper demonstrates the trend empirically but hasn't established whether this scales further or identified the underlying cause.

## Limitations
- Heavy dependence on Puzzle NAS framework implementation details not specified in paper
- Multi-stage KD hyperparameters (learning rate, optimizer, batch size, temperature) not provided
- No numeric thresholds specified for dependency-based fusion decisions
- Requires substantial computational resources (1.2TB GPU memory for base model)

## Confidence

**High Confidence**: The core fusion methodology and theoretical justification are sound; 1.71× speedup and 35× cost reduction are plausible given parallelization of sequential operations.

**Medium Confidence**: Effectiveness at 253B scale is demonstrated but not independently verified; benchmark results appear reasonable but lack comparison to specific competitors.

**Low Confidence**: Exact reproducibility of Ultra-253B-Base due to unspecified Puzzle NAS details and KD hyperparameters; "strong performance" claims lack quantitative competitor benchmarks.

## Next Checks

1. Generate block-wise dependency matrix M_ij on calibration dataset and verify low cosine distance values in fused sequences [66,73], [74,85], [86,100], [101,114] match paper claims.

2. Systematically test fusion of different FFN sequence lengths (varying n) and verify that excluding the last FFN in each sequence is necessary to prevent accuracy degradation.

3. Measure actual GPU memory consumption during inference of fused model versus base model, focusing on memory spike from concatenated FFN weights to verify 640GB constraint is met while maintaining 1.71× speedup.