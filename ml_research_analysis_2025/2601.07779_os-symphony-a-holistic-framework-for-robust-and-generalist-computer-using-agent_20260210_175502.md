---
ver: rpa2
title: 'OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using
  Agent'
arxiv_id: '2601.07779'
source_url: https://arxiv.org/abs/2601.07779
tags:
- agent
- arxiv
- agents
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OS-SYMPHONY introduces a holistic framework for robust and generalist
  computer-using agents (CUAs) by addressing two critical challenges: long-horizon
  task robustness and generalization to novel domains. The framework consists of an
  Orchestrator that coordinates two key innovations: (1) a Reflection-Memory Agent
  that uses milestone-driven long-term memory and structured auditing to generate
  trajectory-level reflections, enabling self-correction in extended workflows, and
  (2) Versatile Tool Agents featuring a Multimodal Searcher that employs a SeeAct
  paradigm to navigate browser-based sandboxes and synthesize visually aligned tutorials,
  overcoming fidelity issues in unseen scenarios.'
---

# OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent

## Quick Facts
- **arXiv ID:** 2601.07779
- **Source URL:** https://arxiv.org/abs/2601.07779
- **Reference count:** 40
- **Key outcome:** OS-SYMPHONY achieves 65.84% on OSWorld, 63.5% on WindowsAgentArena, and 46.0% on MacOSArena by addressing long-horizon robustness and generalization via milestone-driven memory and visual-centric tutorial synthesis.

## Executive Summary
OS-SYMPHONY introduces a holistic framework for robust and generalist computer-using agents (CUAs) by addressing two critical challenges: long-horizon task robustness and generalization to novel domains. The framework consists of an Orchestrator that coordinates two key innovations: (1) a Reflection-Memory Agent that uses milestone-driven long-term memory and structured auditing to generate trajectory-level reflections, enabling self-correction in extended workflows, and (2) Versatile Tool Agents featuring a Multimodal Searcher that employs a SeeAct paradigm to navigate browser-based sandboxes and synthesize visually aligned tutorials, overcoming fidelity issues in unseen scenarios. Experimental results demonstrate that OS-SYMPHONY achieves state-of-the-art performance across three major benchmarks, significantly improving upon existing methods, particularly for smaller models, by leveraging visual-aware tutorial retrieval and granular historical context management.

## Method Summary
OS-SYMPHONY is an inference-only multi-agent framework consisting of three core components: (1) Orchestrator for action prediction using short-term memory and RMA reflections, (2) Reflection-Memory Agent (RMA) with milestone-driven long-term memory, step-level summary, trajectory-level reflection, and loop detection, and (3) Tool Agents including Multimodal Searcher (SeeAct in browser sandbox), General Grounder (UI-TARS-1.5-7B), OCR Grounder (EasyOCR), and Coder for file operations. The framework uses VLMs like GPT-5/GPT-5-Mini or Qwen3-VL-32B with temperature=0.1 and context capped at 8 × 1920×1080 images. Key hyperparameters include pHash Hamming ≤1, SSIM threshold=0.99, and loop detection window N=3.

## Key Results
- Achieves 65.84% task success on OSWorld-Verified benchmark
- Demonstrates 63.5% success on WindowsAgentArena and 46.0% on MacOSArena
- Shows +45% relative improvement for open-source VLMs (Qwen3-VL-32B) compared to baselines
- Significantly outperforms existing methods in long-horizon and unseen task scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Milestone-driven long-term memory enables trajectory-level self-correction in extended workflows
- **Mechanism:** The Reflection-Memory Agent (RMA) selectively retains only "milestone" screenshots alongside abstract trajectory summaries, rather than all intermediate frames. This compression preserves decision-critical visual context while avoiding context window saturation. The RMA then audits historical states against a structured message protocol (On-track/Off-track/Completed/Infeasible) to detect intent drift, loops, or GUI errors before they cascade.
- **Core assumption:** Screenshots exhibit high temporal redundancy; most intermediate observations provide diminishing informational value for future decisions.
- **Evidence anchors:** Abstract mentions trajectory-level self-correction via milestone-driven memory; section 3.2 details RMA's compression strategy with milestone marker mj as gatekeeper.
- **Break condition:** If RMA's visual perception fails on subtle cues (e.g., highlighting, overlapping windows), it may issue false positive errors that mislead the Orchestrator.

### Mechanism 2
- **Claim:** Visual-centric search-as-a-tool paradigm bridges knowledge gaps for out-of-distribution (OOD) tasks
- **Mechanism:** The Multimodal Searcher operates in an isolated browser sandbox using a SeeAct paradigm—interpreting rendered pages visually rather than parsing HTML. It synthesizes step-by-step tutorials only when the RMA signals "Lack of Tutorial" errors, ensuring on-demand retrieval aligned with the agent's immediate execution state. Retrieved tutorials are permanently appended to the Orchestrator's context.
- **Core assumption:** Text-based RAG struggles with GUI scenarios because it cannot interpret screenshot-heavy tutorials; visual browsing preserves critical spatial-semantic cues.
- **Evidence anchors:** Abstract describes Multimodal Searcher adopting SeeAct paradigm to synthesize visually aligned tutorials; section 3.3 details the isolated browser sandbox with restricted action space Asearch = {click, type, scroll}.
- **Break condition:** If the Searcher returns low-relevance tutorials, contamination of the Orchestrator's context could disrupt downstream decision-making.

### Mechanism 3
- **Claim:** Hierarchical decomposition with context folding prevents information bottlenecks between planner and workers
- **Mechanism:** The Orchestrator delegates subtasks to specialized Tool Agents (Coder, Grounders, Searcher), each operating in isolated execution contexts. Detailed trajectories are "folded" into concise summaries before returning to the Orchestrator, maintaining a seamless logical flow without polluting the central context window. The Orchestrator synthesizes a sliding window of recent interactions (H_short) with RMA reflections (R_i).
- **Core assumption:** Precise textual articulation of visual affordances is inherently lossy; delegating fine-grained tasks to specialized agents mitigates this bottleneck.
- **Evidence anchors:** Section 3.1 describes ti, ai = F_O(I, R_i, o_i, T, H_short) with H_short as short-term memory; section A.2 explains context folding principle for both Searcher and Coder.
- **Break condition:** Error propagation remains inherent in multi-agent systems—a false alarm from RMA or hallucinated tutorial from Searcher can cascade through subsequent steps.

## Foundational Learning

- **Concept:** Partially Observable Markov Decision Processes (POMDPs)
  - **Why needed here:** GUI environments have infinite state spaces; agents must infer true states from screenshot observations. The paper formalizes CUA interaction as a POMDP (S, A, O, T, O) in Appendix A.1.
  - **Quick check question:** Can you explain why aggregating historical observations helps construct a belief state that approximates Markov properties?

- **Concept:** Context Compression for Long-Horizon Tasks
  - **Why needed here:** Direct incorporation of full multimodal histories (≥128K tokens) induces hallucinations and computational overhead. The paper seeks an optimal compression function C that filters redundancy while preserving decision-critical information.
  - **Quick check question:** What trade-offs exist between retaining more visual context versus maintaining reasoning quality?

- **Concept:** Multimodal RAG vs. Unimodal RAG
  - **Why needed here:** Existing RAG methods either rely on unimodal text (losing visual semantics) or depend on costly local knowledge bases. Visual-centric retrieval preserves spatial layouts critical for GUI tasks.
  - **Quick check question:** Why might text-based retrieval fail when interpreting screenshot-heavy tutorials?

## Architecture Onboarding

- **Component map:** Orchestrator <- Reflection-Memory Agent (RMA) <- Tool Agents (Multimodal Searcher, General Grounder, OCR Grounder, Coder)
- **Critical path:**
  1. Orchestrator receives task instruction I and current screenshot o_i
  2. RMA generates reflection R_i based on long-term memory H_long and loop detection
  3. If R_i signals "Lack of Tutorial," Orchestrator invokes Searcher → returns tutorial T
  4. Orchestrator predicts thought t_i and action a_i; executes via appropriate Grounder or Coder
  5. Step-level summary S_i verifies GUI action success; RMA updates H_long and determines milestone status
  6. Loop continues until "Completed" or "Infeasible"

- **Design tradeoffs:**
  - **Robustness vs. Latency:** Multi-agent coordination introduces overhead; execution is "tens of times slower than human performance"
  - **Recall vs. Precision in Reflection:** RMA prioritizes high precision to avoid false alarms, but may miss subtle visual errors (e.g., partial alignment)
  - **Context Length vs. Reasoning Quality:** Empirically, capping visual context at 8 images optimizes performance; fewer images lose history, more images induce information overload

- **Failure signatures:**
  - **Perceptual Blindness:** RMA fails on subtle visual cues (highlighting, overlapping windows) → false positive errors
  - **Error Propagation:** False alarms from RMA or hallucinated tutorials mislead Orchestrator
  - **Ambiguous Instructions:** Evaluation functions may reject valid solutions (e.g., selecting ARN airport instead of "STO" dropdown)
  - **Premature Termination:** Agents often output "done" with high confidence before verifying completion

- **First 3 experiments:**
  1. **Ablate RMA on Workflow domain:** Run OS-SYMPHONY with and without RMA on cross-application tasks (Tab. 3 shows +20% relative improvement with RMA). Measure step efficiency and error recovery rate.
  2. **Compare Multimodal vs. Unimodal Search on Daily domain:** Use SearXNG + Crawl4AI for text-based retrieval vs. SeeAct visual browsing (Tab. 3 shows +10.3% relative gain for multimodal). Analyze tutorial relevance and task success correlation.
  3. **Scale Analysis with Open-Source VLMs:** Test Qwen3-VL-32B-Instruct with and without OS-SYMPHONY framework (Tab. 1 shows +45% relative improvement). Track Searcher invocation frequency to validate that weaker models compensate via external knowledge retrieval.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the "granularity gap" in visual perception be bridged to prevent Reflection-Memory Agents (RMA) from issuing false-positive errors in complex visual environments?
- **Basis in paper:** [explicit] The Discussion section notes that while robust, the RMA falters against subtle visual nuances (e.g., highlighting, overlapping windows). This "perceptual blindness" leads to false positives, paradoxically allowing baselines without RMA to outperform the full framework in visually complex domains.
- **Why unresolved:** Current VLMs struggle to resolve fine-grained visual cues, causing the RMA to misidentify successful execution states as errors.
- **What evidence would resolve it:** The development of targeted prompt engineering or enhanced image post-processing techniques that enable the RMA to distinguish subtle UI states without triggering false error reflections.

### Open Question 2
- **Question:** How can framework design harness the "volatility" of VLMs to improve single-pass consistency rather than relying on aggregation?
- **Basis in paper:** [explicit] The Discussion section highlights a disconnect between deployment stability and latent competence. While the model is volatile (detrimental to single-run stability), Pass@5 results reach 79.40%, surpassing human baselines.
- **Why unresolved:** The paper observes significant task-level volatility despite strict control over prompts and temperature, suggesting the model has the capability but lacks the consistency to deploy reliably.
- **What evidence would resolve it:** A framework mechanism that stabilizes the high latent potential of VLMs into consistent single-shot performance without requiring multiple inference runs.

### Open Question 3
- **Question:** Can hybrid paradigms integrating native CUA capabilities resolve the information bottleneck inherent in the Planner-Worker architecture?
- **Basis in paper:** [explicit] The Discussion identifies a "Bottleneck in the Planner-Worker Paradigm," stating that precise textual articulation of visual affordances (e.g., element boundaries) is challenging for the Orchestrator.
- **Why unresolved:** Textual abstraction of visual coordinates creates an information loss between the Orchestrator and the Grounder, capping the performance of modular frameworks.
- **What evidence would resolve it:** Demonstrating a hybrid architecture that successfully integrates end-to-end native CUA capabilities to eliminate the need for textual abstraction of visual grounding tasks.

### Open Question 4
- **Question:** Can dynamic "fast and slow" reasoning mechanisms reduce the latency of multi-agent systems to a level suitable for real-time deployment?
- **Basis in paper:** [explicit] The Limitations section notes that the structural complexity of the multi-agent system introduces high token consumption and latency, making execution "tens of times slower than human performance."
- **Why unresolved:** The extensive inter-agent interactions required for robustness currently preclude the framework from being used in real-time applications.
- **What evidence would resolve it:** Implementation of simplified architectures or dynamic reasoning switching that maintains the 65.84% OSWorld benchmark performance while significantly reducing step latency.

## Limitations
- **Evaluation ambiguity:** OS-SYMPHONY sometimes fails to meet strict evaluation criteria (e.g., selecting "ARN" airport instead of "STO" dropdown) despite functionally correct execution
- **Multi-agent coordination risks:** Error propagation from false alarms or hallucinated tutorials can cascade through subsequent steps
- **Resource constraints:** Framework execution is "tens of times slower than human performance" with no efficiency metrics provided

## Confidence
- **High Confidence:** The milestone-driven memory compression mechanism effectively prevents context window saturation while preserving decision-critical visual information
- **Medium Confidence:** The hierarchical decomposition with context folding provides meaningful robustness improvements over single-agent baselines
- **Low Confidence:** The framework's generalization to novel domains through on-demand tutorial synthesis is consistently reliable

## Next Checks
1. **Ablation Study on Milestone Thresholds:** Systematically vary the milestone detection sensitivity (current pHash Hamming ≤1, SSIM ≥0.99) to quantify the precision-recall tradeoff in RMA's visual context retention. Measure how different thresholds affect long-horizon task success rates.
2. **Cross-Domain Generalization Test:** Evaluate OS-SYMPHONY on GUI tasks from domains not represented in training benchmarks (e.g., enterprise software, specialized scientific applications). Compare performance against single-agent baselines to validate true generalization capabilities.
3. **Error Recovery Benchmark:** Design experiments where the agent encounters deliberate GUI errors (malformed screenshots, missing elements) to test RMA's ability to detect and recover from visual perception failures. Measure the frequency and effectiveness of self-correction attempts.