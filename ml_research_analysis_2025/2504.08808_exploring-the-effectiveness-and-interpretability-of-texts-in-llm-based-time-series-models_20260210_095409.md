---
ver: rpa2
title: Exploring the Effectiveness and Interpretability of Texts in LLM-based Time
  Series Models
arxiv_id: '2504.08808'
source_url: https://arxiv.org/abs/2504.08808
tags:
- time
- series
- text
- prototypes
- timellm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether textual data enhances the interpretability
  and forecasting performance of Large Language Model (LLM)-based time series models.
  Through extensive experiments on two state-of-the-art models (TimeLLM and CALF),
  the authors find that incorporating textual information does not consistently improve
  forecasting accuracy across different language models and tasks.
---

# Exploring the Effectiveness and Interpretability of Texts in LLM-based Time Series Models

## Quick Facts
- **arXiv ID:** 2504.08808
- **Source URL:** https://arxiv.org/abs/2504.08808
- **Reference count:** 40
- **Primary result:** Textual data does not consistently improve forecasting performance or interpretability in LLM-based time series models.

## Executive Summary
This study investigates whether incorporating textual data enhances the forecasting performance and interpretability of Large Language Model (LLM)-based time series models. Through extensive experiments on two state-of-the-art models (TimeLLM and CALF) across seven datasets and three language models (GPT-2, BERT, LLaMA), the authors find that text-augmented approaches do not consistently outperform baseline models without textual information. Visualization analyses reveal that learned text prototypes and attention weights lack meaningful alignment with time series patterns, suggesting limited interpretability. The authors propose a novel Semantic Matching Index (SMI) metric to quantify the alignment between time series patches and their textual representations, finding that different language models exhibit varying abilities to match time series with text.

## Method Summary
The study evaluates two LLM-based time series models: TimeLLM (using GPT-2 by default) and CALF. Both models incorporate textual information through two mechanisms: text prototypes (learnable anchors mapping time series patches to language semantic space via cross-attention) and prompt-as-prefix (textual prompts providing task context). The TimeLLM model uses 16-step patches, 100-1000 text prototypes, and frozen LLMs with cross-attention for modality fusion. Experiments span seven datasets (ETTh1, ETTh2, ETTm1, ETTm2, Electricity, Traffic, Weather) with prediction horizons of {96, 192, 336, 720}. Performance is measured using MSE and MAE, while interpretability is assessed through visualization analysis and the proposed SMI metric.

## Key Results
- Text-augmented models do not consistently outperform baselines across different language models and tasks
- Visualization reveals learned text prototypes and attention weights lack meaningful alignment with time series patterns
- SMI analysis shows different language models exhibit varying abilities to match time series with text
- Random replacement of prompt embeddings sometimes improves performance, suggesting limited semantic value
- Models without text (Base, Base_Prompt) often match or exceed full TimeLLM performance on 4/7 datasets

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Alignment via Text Prototypes
- **Claim:** Text prototypes serve as learnable anchors that map time series patches to language semantic space through cross-attention
- **Mechanism:** A learned linear mapping layer or PCA extracts text prototypes from vocabulary embeddings. Time series patches attend to these prototypes via multi-head attention, producing aligned embeddings that the frozen LLM can process
- **Core assumption:** Time series patterns have meaningful correspondences with natural language semantics that can be discovered through gradient-based learning
- **Evidence anchors:** Visualization shows attention weights concentrate on tokens like "˙Gthe" and "ÿ" with no semantic relevance; different extraction methods yield similar performance, indicating prototype semantics may not be the driving factor
- **Break condition:** If learned prototypes cluster semantically and attend uniformly across patches without differentiation, the alignment mechanism has collapsed

### Mechanism 2: Prompt-as-Prefix for Task Instruction
- **Claim:** Textual prompts provide task context and domain knowledge to guide the frozen LLM's processing of time series embeddings
- **Mechanism:** A formatted prompt is prepended to aligned time series tokens before input to the LLM
- **Core assumption:** Pre-trained language understanding transfers to numerical time series reasoning when prompted appropriately
- **Evidence anchors:** Random replacement of prompt embeddings at 100% ratio does not consistently degrade performance; attention visualization shows LLM focuses primarily on the first token "<"
- **Break condition:** If performance remains stable when prompt embeddings are randomized or removed entirely, the prompt mechanism provides no semantic value

### Mechanism 3: Semantic Matching via Text-to-Patch Association
- **Claim:** Aligned time series embeddings should correspond to interpretable textual tokens, enabling post-hoc understanding of what patterns the model detects
- **Mechanism:** Aligned embeddings are projected to vocabulary space via cosine similarity, retrieving top-k tokens as textual representations
- **Core assumption:** Clustering in embedding space reflects clustering in time series feature space
- **Evidence anchors:** Patches belonging to the same token set show high variance with no clear pattern; similarity analysis shows no higher affinity for time series-related words than unrelated words
- **Break condition:** If intra-class feature distance exceeds inter-class distance, the matching mechanism is performing worse than random assignment

## Foundational Learning

- **Concept: Cross-Attention for Modality Bridging**
  - **Why needed here:** The core architectural choice is using attention rather than concatenation to merge time series and text
  - **Quick check question:** Given a query from time series patches and keys/values from text prototypes, what does a uniform attention distribution across all prototypes indicate?

- **Concept: Semantic Space Alignment**
  - **Why needed here:** The paper's central question is whether numerical and textual modalities occupy a shared representational space
  - **Quick check question:** If two embeddings have cosine similarity 0.95 but correspond to unrelated concepts, what property of the embedding space has failed?

- **Concept: Interpretability via Probing**
  - **Why needed here:** The SMI metric and similarity visualizations are probing methods
  - **Quick check question:** If a time series embedding projects closest to the token "forecast," does this mean the model is forecasting or that the embedding happens to lie near that token in a potentially misaligned space?

## Architecture Onboarding

- **Component map:** Time Series Input → Patching (16-step) → Linear Projection → LLM Vocabulary → Linear Mapping Layer → Text Prototypes → Cross-Attention → Aligned Embeddings → Text Prompts → Concatenation → Frozen LLM → Linear Projection → Forecast Output

- **Critical path:** The reprogramming layer (cross-attention) is the sole mechanism for modality fusion. If this attention collapses to uniform weights or concentrates on a single prototype, the entire textual branch provides no discriminative information

- **Design tradeoffs:**
  - **Linear mapping vs PCA prototypes:** Learned adaptation vs potential semantic incoherence
  - **100 vs 1000 prototypes:** Diversity of anchors vs noise and attention dilution
  - **Frozen vs fine-tuned LLM:** Preserves pre-trained knowledge vs cannot adapt to numerical patterns
  - **GPT-2 vs BERT:** Decoder vs encoder inductive bias with mixed forecasting results

- **Failure signatures:**
  - Only 2-3 prototypes receive >90% of attention weights across all patches
  - Learned prototypes cluster near each other in embedding space (high cosine similarity)
  - Shuffling or zeroing text inputs causes <5% performance change
  - Patches mapped to same token show variance exceeding dataset-wide variance

- **First 3 experiments:**
  1. Run Base (no text), Base_Prompt, Base_Prototype, and full TimeLLM on a single dataset. If Base outperforms variants, the text branch is actively harmful
  2. Compute pairwise cosine similarity among learned prototypes. If mean similarity >0.7, the mapping layer has collapsed
  3. Calculate SMI for random patch-to-token assignment on your dataset. Compare model SMI against this baseline—if within 10%, the matching is no better than random

## Open Questions the Paper Calls Out

- **How can time series data be better aligned to the semantic space of text in TS-LLM models to improve both forecasting performance and interpretability?**
  - Basis: "On the basis of standard time series tasks, it still remains a challenging issue to better align time series to the semantic space of text, or to better integrate these two modalities"
  - Why unresolved: Current alignment methods show inconsistent effectiveness across language models, and visualization reveals misalignment between learned text prototypes and time series patterns
  - What evidence would resolve it: Development of new alignment techniques that demonstrate consistent performance improvements across multiple language models and datasets

- **What language-related auxiliary tasks can enhance TS-LLM models' ability to understand time series through textual modality?**
  - Basis: "Potential future works can focus on 1) adding language-related tasks besides the original tasks"
  - Why unresolved: Current models use text primarily as prompts or prototypes without explicit language-related supervision
  - What evidence would resolve it: Frameworks incorporating language-related tasks that show improved alignment metrics and forecasting performance

- **How can cross-modality models be designed to genuinely understand time series according to textual instructions while preserving interpretability?**
  - Basis: "designing cross-modalities models which can understand time series according to instructions" as future work
  - Why unresolved: Current models show attention patterns where prompts and time series are barely connected except through special tokens
  - What evidence would resolve it: Models where attention mechanisms show meaningful interaction between textual instructions and specific time series segments

## Limitations

- **Architecture generalization:** The study validates findings across seven datasets and three LLMs, but the text prototype mechanism remains underexplored with only 1000 prototypes by default
- **Interpretability claims:** Conclusions about lack of meaningful alignment rest heavily on visual inspection of attention weights and token sets without quantitative benchmarks for what constitutes "meaningful alignment"
- **Task scope:** Results are confined to univariate forecasting with fixed horizons, and mechanisms may behave differently for multivariate forecasting or anomaly detection

## Confidence

**High confidence:** The core empirical finding that text-augmented models don't consistently outperform baselines is well-supported by ablation studies showing Base and Base_Prompt models matching or exceeding full TimeLLM performance on 4/7 datasets

**Medium confidence:** The conclusion that learned prototypes and attention weights lack semantic relevance is supported by visualizations but could benefit from additional quantitative validation

**Low confidence:** Claims about specific LLMs being better suited for text-based TS modeling (BERT showing higher SMI but mixed forecasting results) are based on limited samples and may not generalize

## Next Checks

1. **Prototype diversity audit:** Compute pairwise cosine similarity among learned text prototypes for TimeLLM. If mean similarity exceeds 0.7, the mapping layer has collapsed to a narrow region of vocabulary space

2. **Random alignment baseline:** Generate random patch-to-token assignments matching the distribution of the model's assignments. Compare the resulting SMI against the model's SMI - if within 10%, the matching mechanism provides no value beyond random assignment

3. **Cross-task transferability:** Apply the best-performing model (likely Base or Base_Prompt) to a multivariate forecasting task or anomaly detection task. If performance degrades substantially, the findings may be specific to univariate forecasting