---
ver: rpa2
title: Large Language Models for EDA Cloud Job Resource and Lifetime Prediction
arxiv_id: '2512.19701'
source_url: https://arxiv.org/abs/2512.19701
tags:
- resource
- prediction
- dataset
- cloud
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting resource requirements
  and execution times for Electronic Design Automation (EDA) cloud jobs, which is
  critical for efficient scheduling and cost management in semiconductor design workflows.
  Traditional machine learning approaches struggle with the complex, semi-structured
  nature of EDA job configurations, requiring extensive feature engineering.
---

# Large Language Models for EDA Cloud Job Resource and Lifetime Prediction

## Quick Facts
- **arXiv ID**: 2512.19701
- **Source URL**: https://arxiv.org/abs/2512.19701
- **Reference count**: 5
- **Primary result**: LLM-based text-to-text regression achieves Pearson correlations above 0.8 for EDA cloud job resource prediction, outperforming traditional baselines by 30-50% on real-world datasets.

## Executive Summary
This paper addresses the challenge of predicting resource requirements and execution times for Electronic Design Automation (EDA) cloud jobs, which is critical for efficient scheduling and cost management in semiconductor design workflows. Traditional machine learning approaches struggle with the complex, semi-structured nature of EDA job configurations, requiring extensive feature engineering. The authors propose a novel framework that fine-tunes Large Language Models (LLMs) to perform text-to-text regression, directly predicting resource metrics from serialized job configurations. Key innovations include representing numerical outputs in scientific notation to handle wide dynamic ranges and implementing constrained decoding to ensure output format reliability. The framework demonstrates significant improvements over traditional baselines and user-requested resources across two real-world EDA datasets, achieving Pearson correlations above 0.8 for most resource metrics.

## Method Summary
The approach serializes semi-structured EDA job configurations into JSON strings, orders fields by importance, and transforms numerical targets into scientific notation token sequences. A decoder-only LLM (Gemma-3 or Qwen-3) is fine-tuned using LoRA (rank=8, alpha=16, dropout=0.05) with cross-entropy loss applied only to metric tokens. Full attention is used during fine-tuning regardless of pre-training attention mechanism. Inference employs constrained decoding with deterministic prefix bypass for JSON keys and numeric vocabulary masking for values. The model predicts CPU, RAM, disk, and lifetime metrics, with extreme values clipped to training range bounds.

## Key Results
- Achieved Pearson correlations above 0.8 for most resource metrics on two real-world EDA datasets
- Outperformed traditional baselines and user-requested resources by 30-50% on key metrics
- Full-attention fine-tuning improved sliding-window LLM accuracy by up to 0.078 Pearson correlation
- Constrained decoding reduced inference time by over 30% while ensuring parseable outputs

## Why This Works (Mechanism)

### Mechanism 1: Text-to-Text Regression via Scientific Notation Tokenization
Converting continuous regression targets into scientific notation token sequences stabilizes training and handles wide dynamic ranges better than traditional regression heads with MSE loss. Instead of appending a linear layer to predict continuous values directly, numerical targets are tokenized into fixed-format scientific notation (e.g., `<1><.><2><3><e><+><0><4>` for 12300). This transforms regression into discrete token-level classification optimized with cross-entropy loss, leveraging the Transformer's sequence modeling capabilities. The approach obviates the need for per-task normalization of target values spanning 10⁰–10⁵.

### Mechanism 2: Full Attention Fine-Tuning for Sliding-Window Pretrained LLMs
Fine-tuning sliding-window-attention LLMs with full attention improves prediction accuracy by enabling global context visibility across the entire job configuration. Even when pre-trained with sliding windows (512–1024 tokens), switching to full attention during fine-tuning allows the model to capture long-range dependencies in semi-structured EDA configurations that would otherwise be fragmented across disconnected windows. The benefit is most pronounced in smaller models, where we observe a performance gap of +0.078 Pearson on Dataset 1.

### Mechanism 3: Constrained Decoding with Deterministic Prefix Bypass
Enforcing structural constraints during decoding ensures parseable outputs and reduces inference latency by bypassing generation of deterministic tokens. Two-phase decoding: (1) Deterministic Prefix Bypass auto-fills JSON keys and delimiters directly into the context window; (2) Constrained Numeric Sampling restricts vocabulary at each generation step to valid digit/symbol tokens only. This eliminates malformed outputs and reduces forward passes, achieving >30% latency reduction while maintaining zero parsing failures.

## Foundational Learning

- **Concept: Semi-Structured Data Serialization**
  - Why needed here: EDA configurations are heterogeneous (tool settings, design parameters, dependencies, scripts). The paper serializes these into JSON strings preserving hierarchy, avoiding brittle feature engineering that flattens rich structure into fixed vectors.
  - Quick check question: Explain why flattening hierarchical EDA configurations into fixed-length feature vectors causes loss of contextual relationships.

- **Concept: Autoregressive Generation with Cross-Entropy Loss**
  - Why needed here: The paper frames regression as sequence generation (P(y_t | x_job, y_{<t})). Understanding token-by-token generation with CE loss is essential to grasp how numeric values emerge from the model.
  - Quick check question: Given input tokens from a serialized job config, how does the model sequentially generate the tokens `1.23e+04`?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The paper uses LoRA (rank=8, alpha=16, dropout=0.05) for efficient fine-tuning. Implementing this cost-effectively requires understanding parameter-efficient training.
  - Quick check question: What are the memory and compute tradeoffs of LoRA vs. full fine-tuning for a 12B parameter model?

## Architecture Onboarding

- **Component map:**
  Configuration serialization → JSON serialization (descending importance order) → tokenization → left-truncate to 2048 tokens → LLM backbone (Gemma-3/Qwen-3) with full attention → LoRA-based fine-tuning → Constrained decoding (prefix bypass + numeric vocabulary mask) → parse JSON output → Clip extreme values

- **Critical path:**
  Configuration serialization → Tokenization → Full-attention forward pass → Constrained decoding → Output parsing → Clip extreme values to training range

- **Design tradeoffs:**
  - Sequence length vs. information: 2048 tokens balances retention and compute; longer sequences (4096, 8192) yield marginal gains
  - Model scale vs. latency: 12B models achieve r > 0.85 but have higher inference cost than 1B/4B variants
  - Full vs. sliding attention: Full attention improves accuracy (~+0.078 Pearson for 1B) but increases memory for long sequences

- **Failure signatures:**
  - Malformed JSON / invalid syntax → constrained decoding not correctly applied
  - Extreme out-of-range predictions → model hallucinating beyond training distribution (apply output clipping)
  - Rapid temporal degradation → overfitting to training-period distribution; monitor on future time windows

- **First 3 experiments:**
  1. Reproduce baseline: Fine-tune Gemma-3-1B on Dataset 1 with scientific notation targets; report MAE and Pearson correlation vs. user-requested baseline.
  2. Attention ablation: Compare sliding-window vs. full-attention fine-tuning on same model; expect ~+0.078 Pearson improvement for 1B on Dataset 1.
  3. Constrained decoding validation: Measure inference latency and parsing error rate with vs. without constraints; expect >30% latency reduction and zero parsing failures with constraints enabled.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the model generalize across fundamentally different chip designs, or is it overfitting to the characteristics of a specific project?
- **Basis in paper**: [inferred] Section 4.2 describes the datasets as coming from a "specific chip project" for both Dataset 1 and Dataset 2.
- **Why unresolved**: The results demonstrate temporal generalization (future time windows) but do not validate performance on unseen chip architectures or different EDA workflow types.
- **What evidence would resolve it**: Evaluation of the fine-tuned model on design verification jobs from a distinct semiconductor project with different design characteristics.

### Open Question 2
- **Question**: Does the truncation of job configurations to 2048 tokens significantly degrade prediction accuracy for the most complex jobs?
- **Basis in paper**: [inferred] Section 3.1 states that while 50% of examples exceed 4k tokens and 10% exceed 18k, the sequence length is truncated to 2048.
- **Why unresolved**: While Section 4.5.3 shows general improvements with increased length up to 8192, it uses normalized MAE; it is unclear if the "long-tail" complex jobs specifically require the full 18k+ context to avoid critical errors.
- **What evidence would resolve it**: An ablation study isolating performance on the subset of jobs whose raw token length exceeds 8192, comparing truncated inputs versus full-context inputs.

### Open Question 3
- **Question**: Is the text-to-text generation approach strictly superior to a modern neural regression model equipped with log-scaled targets?
- **Basis in paper**: [inferred] The paper argues in Section 3 that standard regression heads struggle with dynamic ranges, but the baselines in Table 1 are limited to user requests and heuristics, excluding a direct comparison with a strong, non-generative neural regressor.
- **Why unresolved**: It remains unverified if the LLM's performance gain comes from the generative nature/scientific notation or simply from the capacity of the Transformer backbone to process semi-structured data.
- **What evidence would resolve it**: A comparison against a Transformer-encoder model of similar size trained with a regression head (MSE loss) on log-normalized target values.

## Limitations

- **Data Generalization and Distribution Shift**: The paper relies on proprietary Google-internal EDA datasets without public release, making independent validation impossible. The model could be memorizing patterns specific to the training period or Google's internal job configurations rather than learning robust transferrable features.

- **Scientific Notation Representation Dependency**: The claimed advantage of scientific notation tokenization depends critically on the exact tokenization scheme and model vocabulary. The paper does not provide details about whether mantissa/exponent tokens are special tokens or standard characters, nor how the tokenizer handles edge cases like zero values or sub-normal numbers.

- **Constrained Decoding Scalability**: While constrained decoding shows 30% latency reduction, the approach assumes fixed JSON schemas and predetermined output structures. The paper does not address scenarios where output formats vary across job types or when schema evolution occurs.

## Confidence

**High Confidence (Experimental Evidence Strong)**:
- Text-to-text regression framework with scientific notation representation
- Overall performance improvements over traditional baselines (Pearson > 0.8)
- LoRA fine-tuning implementation and hyperparameters
- Constrained decoding latency reduction (>30%)

**Medium Confidence (Evidence Present but Limited)**:
- Full-attention fine-tuning benefits for sliding-window models (+0.078 Pearson improvement)
- Sequence length optimization findings (2048 tokens optimal)
- Model scale performance trade-offs (12B vs 1B/4B)

**Low Confidence (Limited or No Evidence)**:
- Temporal robustness and distribution shift resistance
- Scalability to different EDA domains or job types
- Impact of tokenization choices on scientific notation handling
- Computational overhead of full attention vs. memory constraints

## Next Checks

1. **Temporal Validation**: Split the existing datasets by time rather than random sampling to create train/validation/test sets that reflect temporal distribution shifts. Retrain and evaluate to assess whether the model maintains performance on future job configurations or degrades due to temporal drift.

2. **Attention Overhead Quantification**: Measure and report the memory consumption and computational overhead of full-attention fine-tuning versus sliding-window fine-tuning across different sequence lengths (2048, 4096, 8192 tokens). Include wall-clock training time comparisons and GPU memory usage profiles.

3. **Cross-Domain Generalization**: Apply the trained models to a different EDA dataset or semiconductor design workflow (e.g., FPGA compilation, verification tasks) to test whether the approach generalizes beyond the specific Google-internal DV job configurations. Report performance degradation and identify which resource predictions transfer versus fail.