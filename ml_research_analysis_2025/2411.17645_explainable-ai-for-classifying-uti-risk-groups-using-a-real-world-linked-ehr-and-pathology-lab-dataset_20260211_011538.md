---
ver: rpa2
title: Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR
  and Pathology Lab Dataset
arxiv_id: '2411.17645'
source_url: https://arxiv.org/abs/2411.17645
tags:
- risk
- data
- patient
- clinical
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an explainable AI framework for classifying
  urinary tract infection (UTI) risk groups using a large linked electronic health
  record (EHR) dataset from the UK. The framework addressed challenges of limited
  labeled UTI outcomes by implementing a clinician-informed risk estimation system
  that assigns likelihood scores (0-1) based on antibiotic dispensations, pathology
  results, and hospital diagnosis codes.
---

# Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset

## Quick Facts
- arXiv ID: 2411.17645
- Source URL: https://arxiv.org/abs/2411.17645
- Reference count: 31
- Primary result: Pairwise XGBoost models achieved 94% accuracy and 0.98 AUC-ROC for classifying highest UTI risk groups using linked EHR data with SHAP interpretability

## Executive Summary
This study developed an explainable AI framework for classifying urinary tract infection (UTI) risk groups using a large linked electronic health record (EHR) dataset from the UK. The framework addressed challenges of limited labeled UTI outcomes by implementing a clinician-informed risk estimation system that assigns likelihood scores (0-1) based on antibiotic dispensations, pathology results, and hospital diagnosis codes. Pairwise XGBoost models were trained to differentiate adjacent UTI risk categories, with SHAP analysis identifying key predictors across risk groups. The models showed strong performance, particularly for higher-risk categories (0.8 vs 1.0) with 94% accuracy and 0.98 AUC-ROC.

## Method Summary
The approach combined a clinician-informed risk estimation framework with pairwise XGBoost classification and SHAP interpretability. UTI risk likelihood (0-1) was assigned using a lookup table based on antibiotic dispensation categories, urine pathology results, and hospital diagnosis codes. Six binary XGBoost models were trained to differentiate adjacent risk pairs, with class imbalance handled via scale_pos_weight hyperparameter. SHAP values quantified feature contributions to identify risk-stratified predictors from a 12-month observation window before each patient's highest risk event.

## Key Results
- Pairwise XGBoost models achieved 94% accuracy and 0.98 AUC-ROC for the highest-risk pair (0.8 vs 1.0)
- Performance varied across risk pairs, with 0.2 vs 0.4 achieving only 0.62 AUC-ROC
- Feature importance shifted across risk groups, with prior UTI-specific antibiotic use, age, and comorbidities like dementia becoming increasingly important at higher risk levels
- The 0.4 risk group was notably small (N=1,969), affecting model robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A clinician-informed risk estimation framework enables UTI likelihood assignment without explicit diagnosis labels.
- **Mechanism:** The framework combines three evidence sources—antibiotic dispensations (categorized by UTI-specificity), urine pathology results (categorized by bacteria-UTI association), and hospital ICD-10 codes (N39.0)—into a composite likelihood score (0, 0.2, 0.4, 0.6, 0.8, or 1.0). Each evidence combination maps to a discrete risk level via a lookup table informed by clinical expertise.
- **Core assumption:** The three proxy indicators collectively approximate ground-truth UTI status when definitive labels are unavailable; the clinician-defined weighting reflects actual diagnostic reasoning.
- **Evidence anchors:**
  - [abstract]: "Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines."
  - [Section 4, Figure 2]: The framework table explicitly maps evidence combinations to likelihood scores, with hospital diagnosis codes (N39.0) yielding likelihood=1.0.
  - [corpus]: Weak direct evidence; neighboring papers address explainability and EHR prediction but not this specific proxy-labeling approach.
- **Break condition:** If proxy labels systematically misclassify patients (e.g., antibiotics prescribed for non-UTI conditions inflate risk scores), model predictions will reflect labeling bias rather than true UTI risk.

### Mechanism 2
- **Claim:** Pairwise XGBoost models between adjacent risk categories identify risk-stratified predictors while improving separability.
- **Mechanism:** Rather than multi-class classification across all six risk levels, the approach trains binary XGBoost models for adjacent pairs (e.g., 0 vs 0.2, 0.8 vs 1.0). SHAP values quantify feature contributions per model, revealing how predictor importance shifts as risk increases. Class imbalance is addressed via `scale_pos_weight` hyperparameter.
- **Core assumption:** Adjacent risk categories share sufficient similarity that pairwise boundaries capture meaningful transitions; features differentiating non-adjacent categories would be less clinically interpretable.
- **Evidence anchors:**
  - [Section 5.2]: "Each pairwise models were designed to differentiate neighboring risk groups (e.g., 0 vs. 0.2, 0.2 vs. 0.4), with the higher-risk group within each pair considered the positive class."
  - [Section 6.2, Table 2]: Performance varies by pair; 0.8 vs 1.0 achieves 94% accuracy and 0.98 AUC-ROC, while 0.2 vs 0.4 achieves only 0.62 AUC-ROC.
  - [corpus]: Neighbor paper "Integrating Knowledge Graphs and Bayesian Networks" similarly uses hybrid approaches for explainable risk prediction, supporting the value of interpretable stratification.
- **Break condition:** If risk categories are not ordinal or transitions are non-monotonic, pairwise models will produce inconsistent feature importance rankings across adjacent pairs.

### Mechanism 3
- **Claim:** Temporal data extensions align misaligned EHR events to capture the clinical episode window.
- **Mechanism:** Two extension strategies: (1) Antibiotic dispensations are backward-extended 3 days (pre-symptomatic period) and forward-extended by estimated treatment duration from dosage guidelines; (2) Pathology specimen collections are extended ±7 days to account for sample processing and treatment lag. These extensions create overlapping evidence windows that feed into the risk framework.
- **Core assumption:** The extension windows (3-day backward, duration-based forward for antibiotics; ±7 days for pathology) correctly approximate the true clinical episode; patients were symptomatic or affected throughout these windows.
- **Evidence anchors:**
  - [Section 4, Data Extension 1 & 2]: Explicit window definitions based on clinical guidance.
  - [Appendix A3]: Examples show how extended windows create overlaps that determine likelihood scores (e.g., Nitrofurantoin + E. coli culture yields L=0.8).
  - [corpus]: No direct neighbor evidence on temporal extension strategies for EHR alignment.
- **Break condition:** If extension windows are too narrow (miss relevant events) or too wide (conflate distinct episodes), risk scores will be systematically under- or over-estimated.

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations) values**
  - **Why needed here:** SHAP quantifies each feature's contribution to individual predictions, enabling interpretation of why a patient is classified into a higher risk category. The paper relies on SHAP summary plots to show evolving predictor importance across risk groups.
  - **Quick check question:** If a patient's model output is 0.7 and the SHAP value for "prior UTI-specific antibiotics" is +0.3, what does this mean?

- **Concept: Class imbalance handling (scale_pos_weight vs. SMOTE)**
  - **Why needed here:** Higher risk categories (0.4, 1.0) have small sample sizes relative to lower categories. The paper compares no intervention, `scale_pos_weight`, and SMOTE, ultimately choosing the weighted approach for stability.
  - **Quick check question:** Why might SMOTE alter feature distributions in ways that harm interpretability compared to loss-function weighting?

- **Concept: EHR data challenges (heterogeneity, sparsity, temporal misalignment, missingness)**
  - **Why needed here:** The paper explicitly frames its contributions as solutions to these four challenges. Understanding them is prerequisite to evaluating why the preprocessing pipeline and risk framework are necessary.
  - **Quick check question:** If clinical tests are only conducted when disease is suspected, what type of missingness is this, and why does simple imputation risk bias?

## Architecture Onboarding

- **Component map:**
  Raw EHR sources → Data cleaning module → Temporal discretization → Data extension module → Risk estimation framework → Feature extraction → Pairwise XGBoost training → SHAP explanation layer

- **Critical path:** Raw EHR → Cleaning → Temporal discretization → Data extension → Risk framework labeling → Feature extraction → Pairwise model training → SHAP analysis. Errors in data extension or risk labeling propagate irrecoverably.

- **Design tradeoffs:**
  - Discrete likelihood scores (0, 0.2, ..., 1.0) vs. continuous probabilities: Chosen for interpretability and alignment with clinical reasoning, but may lose granularity.
  - Pairwise models vs. multi-class: Chosen for interpretability of transitions, but increases training complexity (6 models for 6 categories).
  - `scale_pos_weight` vs. SMOTE: Chosen for feature distribution stability; SMOTE introduced synthetic samples that altered SHAP explanations.
  - 12-month observation window: Standardizes feature extraction but may miss longer-term risk factors.

- **Failure signatures:**
  - Very low precision/recall for small classes (e.g., 0.2 vs 0.4: F1=0.10) → Class imbalance or category definition problem; consider merging adjacent groups.
  - SHAP feature importance dominated by data artifacts (e.g., record counts rather than clinical variables) → Insufficient feature engineering or label noise.
  - Inconsistent feature importance rankings across adjacent pairwise models → Risk categories may not be ordinal or framework scoring is inconsistent.
  - High AUC but low precision → Threshold selection issue; calibrate for clinical use case.

- **First 3 experiments:**
  1. **Validate temporal extension windows:** Ablation study comparing current windows (3-day backward, ±7-day pathology) against alternatives (e.g., 5-day, 14-day) by measuring model performance and SHAP stability. Hypothesis: Current windows are near-optimal; deviations degrade performance.
  2. **Test alternative risk groupings:** Merge 0.4 with 0.2 or 0.6 and retrain pairwise models. Evaluate whether F1-scores improve for affected pairs without degrading others. Hypothesis: The small 0.4 group is poorly defined and benefits from merging.
  3. **External validation on held-out time period:** Train on Oct 2019–Dec 2020, test on Jan 2021–July 2022. Assess temporal drift in feature importance and performance. Hypothesis: Post-pandemic healthcare utilization shifts may affect risk group distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can data-driven strategies (e.g., clustering or merging adjacent categories) improve the analytical robustness of the UTI risk estimation framework compared to the current 0.4 risk category?
- Basis in paper: [explicit] The Discussion states the "relatively small size of the 0.4 risk group may affect the robustness of analyses," and suggests "exploring data-driven strategies for reclassifying smaller or less robust groups."
- Why unresolved: The current 0.4 group is defined by heuristics but results in a class size too small for effective modeling, creating a potential weak point in the pairwise classification chain.
- Evidence would resolve it: Comparative performance metrics (F1-score, AUC-ROC) and stability analyses showing improved model discrimination when the 0.4 category is redefined or merged using unsupervised learning techniques.

### Open Question 2
- Question: Do the identified clinical and demographic predictors generalize to external populations and diverse healthcare settings outside the BNSSG region?
- Basis in paper: [explicit] The authors state that "Enhanced validation through external datasets and collaborations with diverse healthcare settings will further ensure the robustness and generalizability of the findings."
- Why unresolved: The study utilizes a single linked dataset from Bristol, North Somerset, and South Gloucestershire, which may contain regional biases in prescribing or coding practices not present elsewhere.
- Evidence would resolve it: Successful replication of the preprocessing pipeline and similar SHAP feature importance rankings when applied to independent EHR datasets from different geographic regions.

### Open Question 3
- Question: How do UTI risk predictors and model performance vary across specific patient sub-strata, particularly for underrepresented populations?
- Basis in paper: [explicit] The Discussion notes the need for "investigating subgroup-specific risk factors to ensure fairness and inclusivity, particularly for underrepresented patient populations."
- Why unresolved: While the study identifies global predictors (e.g., age, prior antibiotics), it does not extensively validate whether these factors hold equal weight or perform equitably across different demographic or clinical subgroups.
- Evidence would resolve it: Stratified analysis revealing consistent feature importance and prediction accuracy across subgroups defined by variables such as socioeconomic status, specific comorbidities, or residency status (e.g., homeless vs. housed).

## Limitations

- The framework's reliance on proxy indicators (antibiotics, pathology, hospital codes) rather than confirmed UTI diagnoses introduces potential systematic bias, particularly for antibiotic dispensations prescribed for non-UTI indications.
- The small sample size in the 0.4 risk group (N=1,969) raises concerns about model stability and category definition validity.
- The study's performance metrics are based on a single health system's data, limiting generalizability across different EHR structures and clinical practices.

## Confidence

- **High Confidence:** The pairwise XGBoost modeling approach with SHAP interpretability is technically sound and well-executed, with clear methodology and strong performance metrics (94% accuracy, 0.98 AUC-ROC for highest-risk pair).
- **Medium Confidence:** The UTI risk estimation framework's clinical validity depends on the accuracy of proxy indicators and clinician-defined mappings, which were not externally validated.
- **Medium Confidence:** Performance differences across risk pairs (0.62 AUC for 0.2 vs 0.4 vs 0.98 AUC for 0.8 vs 1.0) may reflect both true risk stratification and category definition issues.

## Next Checks

1. **Temporal Validation Study:** Train on pre-pandemic data (Oct 2019–Dec 2020) and test on post-pandemic period (Jan 2021–July 2022) to assess model stability across healthcare utilization changes.

2. **Risk Group Definition Sensitivity:** Merge the 0.4 risk group with adjacent categories and retrain models to determine if performance improves, suggesting the current six-category structure may be over-specified.

3. **External Dataset Validation:** Apply the framework to an independent EHR system with different clinical coding practices to assess generalizability and identify potential source-specific biases.