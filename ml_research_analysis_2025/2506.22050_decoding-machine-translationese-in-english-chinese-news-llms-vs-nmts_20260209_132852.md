---
ver: rpa2
title: 'Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs'
arxiv_id: '2506.22050'
source_url: https://arxiv.org/abs/2506.22050
tags:
- llms
- features
- nmts
- translation
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates Machine Translationese (MTese) in English-to-Chinese
  news translation by comparing Neural Machine Translation (NMT) and Large Language
  Model (LLM) outputs. Using a dataset of 4 sub-corpora and a comprehensive five-layer
  feature set, the research employs chi-square feature selection and classification/clustering
  algorithms.
---

# Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs

## Quick Facts
- arXiv ID: 2506.22050
- Source URL: https://arxiv.org/abs/2506.22050
- Reference count: 17
- Primary result: MTese is present in both NMT and LLM outputs, with original Chinese texts nearly perfectly distinguishable from machine translations

## Executive Summary
This study investigates Machine Translationese (MTese) in English-to-Chinese news translation by comparing Neural Machine Translation (NMT) and Large Language Model (LLM) outputs. Using a dataset of 4 sub-corpora and a comprehensive five-layer feature set, the research employs chi-square feature selection and classification/clustering algorithms. Results show that MTese is present in both NMT and LLM outputs, with original Chinese texts nearly perfectly distinguishable from machine translations. NMTs and LLMs show approximately 70% classification accuracy, with LLMs exhibiting greater lexical diversity and NMTs using more brackets.

## Method Summary
The study constructs a corpus of 4 sub-corpora: Original Chinese News (OCN), Original English News (OEN), NMT outputs from 5 engines, and LLM outputs from 6 models. Texts are preprocessed using the LTP toolkit for word segmentation, PoS tagging, and dependency parsing. A comprehensive 236-feature set is extracted across five levels (Lexical, Syntactical, Readability, Translatability, N-POS-gram), normalized as ratios. Chi-square feature selection identifies the most discriminative features, which are then used in classification (Naïve Bayes, Logistic Regression, SVM, Decision Tree, Random Forest) and K-means clustering with ARI evaluation.

## Key Results
- Original Chinese texts are nearly perfectly distinguishable from both LLM and NMT outputs
- NMTs and LLMs can be distinguished with approximately 70% classification accuracy
- LLMs exhibit greater lexical diversity and NMTs use more brackets (average ratio ~0.04 vs ~0.02)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine Translationese (MTese) is detectable in both NMT and LLM outputs with near-perfect classification from original human-written Chinese text.
- Mechanism: Stylometric features—particularly sentence length and conjunction usage patterns—serve as discriminative signals. MT systems produce shorter sentences (median <40 characters vs. 50 in original Chinese) and use adversative conjunctions ("但是", "然而") more than twice as frequently as original texts, likely due to source language interference and literal translation strategies.
- Core assumption: The linguistic patterns observed reflect systematic translation behavior rather than content-specific artifacts.
- Evidence anchors:
  - [abstract] "Original Chinese texts are nearly perfectly distinguishable from both LLM and NMT outputs."
  - [section 5.1] "OCN texts contain more characters per sentence, with a median of 50, compared to less than 40 in MT texts... MTs employ adversative conjunctions more than twice as often as OCN."
  - [corpus] No direct corpus evidence on mechanism causality; inference based on distributional patterns.
- Break condition: If source texts were pre-edited to reduce adversative conjunctions or sentence complexity, the discriminative power of these features would likely diminish.

### Mechanism 2
- Claim: LLMs and NMTs can be distinguished (~70% accuracy) primarily through lexical diversity and punctuation handling differences.
- Mechanism: LLMs leverage broader pre-training data, yielding higher MTLD (Measure of Textual Lexical Diversity) scores. NMTs apply explicit post-processing rules for proper nouns—adding original English names in brackets after Chinese translations—which increases bracket ratios (average ~0.04 vs. ~0.02 for LLMs). DeepL shows the strongest bracket-insertion behavior.
- Core assumption: Bracket usage reflects engineered post-processing rules in NMT systems rather than model-intrinsic behavior.
- Evidence anchors:
  - [abstract] "LLMs exhibiting greater lexical diversity and NMTs using more brackets."
  - [section 5.2] "LLMs have higher MTLD scores compared to NMTs... NMT systems use brackets more frequently (average ratio around 0.04) compared to LLMs (around 0.02)."
  - [corpus] Weak corpus evidence on training data composition; mechanism inferred from output patterns.
- Break condition: If LLMs adopted similar post-processing rules for proper nouns, or if NMTs reduced bracket insertion, classification accuracy would drop toward chance.

### Mechanism 3
- Claim: Translation-specific LLMs (e.g., TowerInstruct) exhibit lower lexical diversity but higher causal conjunction usage compared to generic LLMs.
- Mechanism: Translation-specialized fine-tuning on parallel corpora may prioritize fidelity and explicit logical connectivity over lexical variety. Generic LLMs, trained on broader corpora, produce more diverse vocabulary choices.
- Core assumption: Fine-tuning objectives for translation-specific models explicitly or implicitly optimize for different stylistic targets than generic models.
- Evidence anchors:
  - [abstract] "Translation-specific LLMs demonstrate lower lexical diversity but higher causal conjunction usage compared to generic LLMs."
  - [section 5.3] "LTO has a lower MTLD value than LCG, LCL, LGL, and LKM... In terms of the proportion of causal conjunctions... LTO has higher frequency of causal conjunctions than other LLM engines."
  - [corpus] No corpus evidence on fine-tuning data; assumption based on model design documentation.
- Break condition: If translation-specific models were fine-tuned with diversity-promoting objectives (e.g., diverse beam search), MTLD differences may converge.

## Foundational Learning

- Concept: **Translationese / Machine Translationese (MTese)**
  - Why needed here: Core phenomenon under investigation; understanding systematic linguistic patterns in translated vs. original texts is prerequisite to interpreting classification results.
  - Quick check question: Can you explain why adversative conjunction frequency differs between original Chinese and translated texts?

- Concept: **Lexical Diversity Metrics (MTLD, TTR)**
  - Why needed here: MTLD serves as a primary discriminative feature between LLMs and NMTs; understanding how it measures vocabulary variation is essential for feature interpretation.
  - Quick check question: Why might MTLD be more robust than Type-Token Ratio for comparing texts of different lengths?

- Concept: **Chinese NLP Preprocessing (Word Segmentation, PoS Tagging, Dependency Parsing)**
  - Why needed here: Chinese lacks explicit word boundaries; the LTP toolkit's segmentation accuracy (99.18%) directly affects feature extraction quality.
  - Quick check question: What preprocessing step is uniquely required for Chinese text that is unnecessary for English?

## Architecture Onboarding

- Component map:
  Corpus collection (post-2022 news) -> LTP preprocessing (segmentation, PoS tagging, dependency parsing) -> Feature extraction (236 features) -> Chi-square feature selection (top-30) -> Binary/multi-class classification (5 classifiers) -> K-means clustering (ARI evaluation)

- Critical path:
  1. Corpus collection (post-2022 news to avoid training data contamination)
  2. LTP preprocessing (ensures consistent segmentation/tagging)
  3. Feature extraction (236 features normalized as ratios)
  4. Chi-square feature selection (reduces dimensionality)
  5. Binary/multi-class classification (OCN vs. MTs; LLMs vs. NMTs)
  6. K-means clustering validation (ARI scoring)

- Design tradeoffs:
  - Larger OCN corpus (2,000 vs. 200) improves coverage but introduces class imbalance; mitigated by ratio-based features
  - Excluding Human Translations (HTs) simplifies design but limits comparison to "human parity" claims
  - Using N-POS-grams instead of lexical n-grams ensures content-independence but may miss topic-specific patterns

- Failure signatures:
  - Classification accuracy drops below 55% for intra-group NMT/LLM comparisons (below random baseline) -> features insufficient for fine-grained distinctions
  - Cluster ARI < 0.3 -> significant overlap between categories; reconsider feature set
  - High variance across classifiers -> unstable feature importance; investigate individual classifier behavior

- First 3 experiments:
  1. Replicate OCN vs. MTs binary classification using only the top-10 chi-square features to validate discriminative power
  2. Ablation study: Remove bracket-ratio feature from LLMs vs. NMTs classification to quantify its contribution
  3. Extend to include Human Translations (HTs) subset to triangulate MTese vs. Translationese patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the linguistic profile of LLM and NMT outputs change when compared directly to Human Translation (HT) rather than just Original Chinese Text (OCN)?
- Basis in paper: [explicit] The authors state in the "Limitations and future work" section that they "have not included human translations (HTs) in this study" and suggest future research should "incorporate HT to further explore the linguistic differences."
- Why unresolved: The current study establishes distinctiveness only against original native text, leaving it unclear which specific "machine" artifacts are distinct from general "translation" artifacts common to both human and machine processes.
- What evidence would resolve it: A comparative analysis using the same feature set (lexical, syntactic, etc.) across four corpora: OCN, HT, NMT, and LLM outputs.

### Open Question 2
- Question: Can incorporating complex semantic and discourse-level features improve the classification accuracy between LLM and NMT outputs, which currently overlap significantly?
- Basis in paper: [explicit] The authors note that their experimental features are "confined to the general and pre-tagged level... without fully addressing more complex aspects of semantics and discourse."
- Why unresolved: The current feature set, focusing on lexical and syntactic tags, yields only ~70% accuracy in distinguishing LLMs from NMTs, suggesting the current linguistic markers are insufficient to fully separate the two technologies.
- What evidence would resolve it: Experiments adding semantic role labeling, discourse connective analysis, or coreference resolution features to the classification model to test if the distinctiveness increases.

### Open Question 3
- Question: To what extent does the "machine translationese" identified in mainstream news manifest in user-generated news discourse?
- Basis in paper: [explicit] The authors list "dataset selection" as a limitation, noting the study "does not encompass user-generated news discourse" and suggest "expanding the database... could help capture a broader spectrum of language features."
- Why unresolved: User-generated content often exhibits different stylistic norms and grammatical strictness compared to the "mainstream news reports" analyzed here, potentially altering the detectability or nature of MTese.
- What evidence would resolve it: A replication of the study using a corpus of user-generated news articles or blog posts as the source and reference texts.

## Limitations
- Corpus Construction Bias: The study excludes Human Translations (HTs) entirely, preventing direct comparison of MTese versus Translationese
- Feature Generalizability: The focus on news corpora may limit broader applicability to other domains
- Causality Inference: Mechanisms attributing bracket usage to NMT post-processing rules are inferred rather than empirically validated

## Confidence
- **High Confidence**: OCN vs. MT classification results (near-perfect accuracy) and the fundamental observation that MTese exists in both NMT and LLM outputs
- **Medium Confidence**: LLMs vs. NMT classification (~70% accuracy) and the specific mechanisms of lexical diversity and bracket usage differences
- **Low Confidence**: Claims about translation-specific vs. generic LLM differences and the fine-tuning mechanisms underlying these differences

## Next Checks
1. **Human Translation Integration**: Replicate the classification experiments including Human Translations (HTs) to directly compare MTese versus Translationese patterns and assess whether MT systems produce distinct signatures from human translators
2. **Cross-Domain Validation**: Apply the same feature set and classification approach to non-news corpora (e.g., literature, technical documentation) to test the generalizability of the identified MTese patterns and feature importance rankings
3. **Controlled Fine-tuning Experiment**: Conduct a controlled experiment fine-tuning a base LLM with different objectives (translation-specific vs. generic) on identical parallel corpora to empirically validate whether fine-tuning objectives cause the observed lexical diversity and conjunction usage differences