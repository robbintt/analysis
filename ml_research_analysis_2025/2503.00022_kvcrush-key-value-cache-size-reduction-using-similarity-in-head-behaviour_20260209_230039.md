---
ver: rpa2
title: 'KVCrush: Key value cache size-reduction using similarity in head-behaviour'
arxiv_id: '2503.00022'
source_url: https://arxiv.org/abs/2503.00022
tags:
- cache
- kvcrush
- tokens
- accuracy
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KVCrush, a novel approach to optimize KV\
  \ caching in large language models by providing an alternative binary representation\
  \ of tokens based on attention score patterns across heads. This binary representation\
  \ enables efficient token grouping and pruning, allowing KVCrush to reduce KV cache\
  \ size by up to 4\xD7 with less than 1% accuracy drop on LongBench benchmarks."
---

# KVCrush: Key value cache size-reduction using similarity in head-behaviour

## Quick Facts
- arXiv ID: 2503.00022
- Source URL: https://arxiv.org/abs/2503.00022
- Authors: Gopi Krishna Jha; Sameh Gobriel; Liubov Talamanova; Nilesh Jain
- Reference count: 7
- Key outcome: Reduces KV cache size by up to 4× with <1% accuracy drop on LongBench benchmarks

## Executive Summary
KVCrush introduces a novel binary token representation method that exploits similarity in attention score patterns across heads to enable efficient KV cache compression. The approach generates binary vectors by thresholding per-head attention scores, groups similar tokens using Hamming distance clustering, and prunes redundant tokens while maintaining semantic integrity. By combining with existing eviction strategies (25% KVCrush, 75% baseline), KVCrush achieves state-of-the-art average accuracy with less than 0.5% inference latency overhead, outperforming importance-based token retention schemes across 16 LongBench tasks.

## Method Summary
KVCrush operates in three phases: (1) Binary token generation where each token is represented as an H-dimensional binary vector based on per-head attention score thresholding; (2) Token grouping where tokens are clustered into representative buckets using Hamming distance to a selected anchor point; (3) Representative selection where the centroid-nearest token from each bucket is retained as the representative. The method integrates with existing KV compression techniques by allocating a fraction of the cache budget (25% in evaluations) to KVCrush-retained tokens while using baseline eviction methods for the remainder. The approach requires no model retraining or architectural changes and maintains O(S) complexity where S is sequence length.

## Key Results
- Achieves up to 4× KV cache compression with less than 1% accuracy drop on LongBench benchmarks
- Maintains state-of-the-art average accuracy across 16 tasks while adding <0.5% inference latency overhead
- Outperforms importance-based token retention schemes when combined with H2O, SnapKV, and PyramidKV baselines

## Why This Works (Mechanism)
KVCrush leverages the observation that tokens with similar attention patterns across attention heads tend to have similar importance signatures in the KV cache. By binarizing these attention patterns and grouping tokens based on Hamming distance similarity, the method identifies and prunes redundant tokens that contribute similar information to the model's predictions. The binary representation serves as a compressed signature of token importance across heads, enabling efficient clustering without requiring expensive distance computations in the original attention space. This semantic-preserving compression allows significant cache reduction while maintaining model accuracy.

## Foundational Learning
- **Attention score normalization**: Required to compare attention patterns across different heads and layers; quick check: verify scores sum to 1 per head per token
- **Hamming distance computation**: Enables efficient similarity measurement between binary token vectors; quick check: confirm using bitwise XOR and popcount operations
- **Centroid selection in binary space**: Determines representative tokens within each bucket; quick check: verify arithmetic mean rounding or weighted median approaches
- **Cache budget allocation**: Balances between pivotal and representative tokens for optimal accuracy; quick check: test different 25-75% splits across workloads
- **Anchor point selection**: Influences bucket distribution and clustering quality; quick check: compare mean vector vs alternating pattern vs random anchors
- **Integration with baseline eviction**: Coordinates KVCrush pruning with existing methods; quick check: verify timing of pruning relative to baseline eviction

## Architecture Onboarding

**Component map:** Attention scores -> Binary vectors -> Hamming distance clusters -> Representative tokens -> Combined with baseline eviction -> Compressed KV cache

**Critical path:** Token generation (Alg 1) → Clustering (Alg 2) → Representative selection → Cache integration

**Design tradeoffs:** Single anchor point maintains O(S) complexity but may create uneven bucket distributions; multi-anchoring could improve clustering at cost of latency; static budget allocation is simple but suboptimal for varying contexts

**Failure signatures:** Accuracy drops >2% indicate poor anchor selection or threshold values; latency overhead >1% suggests inefficient Hamming distance computation; compression ratio <2× may indicate ineffective clustering

**First experiments:** (1) Implement Algorithm 1 with multiple threshold strategies to isolate impact on binary representation quality; (2) Benchmark Hamming distance computation using optimized bitwise operations versus naive implementations; (3) Test different anchor point selection methods to measure effects on bucket distribution and accuracy

## Open Questions the Paper Calls Out
- **Open Question 1**: How can dynamic cache budget allocation between pivotal and representative tokens be implemented to maximize accuracy across varying input contexts? The paper plans to investigate adaptive partitioning algorithms that adjust the $B_{important}$ vs. $B_{representative}$ split based on context, compared against static baselines on LongBench.
- **Open Question 2**: To what extent can a refined multi-anchoring approach improve clustering accuracy over the single-anchor method without inducing significant latency? The authors intend to develop efficient multi-anchor schemes (e.g., hierarchical or grid-based) that balance the trade-off between clustering quality and overhead.
- **Open Question 3**: How does the choice of per-head thresholds ($\theta_h$) for binary token generation impact the semantic preservation of the KV cache under extreme compression? The sensitivity of binarization to different thresholding strategies and their layer-wise variation remains unexplored.

## Limitations
- Threshold values $\theta_h$ for attention score binarization are unspecified, affecting binary representation quality
- Centroid computation method for representative selection is not defined, potentially impacting accuracy
- Integration timing between KVCrush pruning and baseline eviction methods is unclear
- Static 25% budget allocation for representative tokens may be suboptimal for certain workloads
- Single-anchor approach may create uneven bucket distributions for some input patterns

## Confidence
- 4× compression with <1% accuracy drop: Medium - plausible given approach but lacks specified parameter values
- <0.5% inference latency overhead: Medium - depends on efficient Hamming distance implementation
- State-of-the-art average accuracy: Medium - reasonable extrapolation from described technique
- Compatibility with existing methods: High - algorithmic integration appears straightforward
- No model retraining required: High - approach works on attention scores without model changes

## Next Checks
1. Implement KVCrush with multiple threshold strategies (fixed percentage per head, adaptive per-head) and compare accuracy outcomes to isolate the impact of threshold selection
2. Benchmark the latency overhead of Hamming distance computation using optimized bitwise operations versus naive implementations to verify the claimed <0.5% overhead
3. Test KVCrush with different anchor point selection methods (mean vector, alternating pattern, random) to measure their effect on bucket distribution and final accuracy