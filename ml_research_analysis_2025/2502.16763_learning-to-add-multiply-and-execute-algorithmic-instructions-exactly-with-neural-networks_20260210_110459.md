---
ver: rpa2
title: Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with
  Neural Networks
arxiv_id: '2502.16763'
source_url: https://arxiv.org/abs/2502.16763
tags:
- input
- instruction
- output
- instructions
- copy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses whether neural networks can learn to execute
  algorithmic instructions exactly, focusing on binary arithmetic and control flow
  tasks. The authors propose a framework using the Neural Tangent Kernel (NTK) to
  study two-layer fully connected networks in the infinite-width limit.
---

# Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks

## Quick Facts
- arXiv ID: 2502.16763
- Source URL: https://arxiv.org/abs/2502.16763
- Authors: Artur Back de Luca; George Giapitzakis; Kimon Fountoulakis
- Reference count: 40
- One-line primary result: Proves neural networks can learn to execute binary arithmetic and control flow algorithms exactly using template matching in NTK regime

## Executive Summary
This paper establishes that neural networks can learn to execute algorithmic instructions exactly by decomposing algorithms into bit-level templates and training on carefully structured data. The authors leverage the Neural Tangent Kernel (NTK) framework to prove that two-layer fully connected networks can learn binary permutations, addition, multiplication, and Turing-complete control flow operations with high probability. The key insight is that algorithmic instructions can be represented as sets of local templates, and by controlling correlations in the NTK regime, models can align their predictions with target algorithmic executions.

The work demonstrates that ensemble complexity scales as O(ℓ²) for bit length ℓ, meaning only logarithmically many training examples are needed relative to the number of possible inputs. Since the framework encompasses Subtract and Branch if Negative (SBN) instructions, which are Turing-complete, this approach extends to learning any computable function. Empirical validation on permutation tasks confirms the theoretical predictions about ensemble size growth and perfect post-rounding accuracy.

## Method Summary
The authors propose a template matching framework where algorithmic instructions are decomposed into local bit-level templates. They train neural networks in the infinite-width NTK regime on orthonormalized versions of these templates, which allows them to prove exact execution with high probability. The approach involves structuring training data to isolate bit-level rules and controlling correlations in the NTK regime to align model predictions with target algorithmic executions. By using an ensemble of neural networks, they achieve perfect post-rounding accuracy on binary permutations, addition, multiplication, and SBN instructions. The template matching principle allows iterative composition of simple rules to execute complete algorithms.

## Key Results
- Proves neural networks can learn to execute binary permutations, addition, multiplication, and SBN instructions exactly with high probability
- Ensemble complexity scales as O(ℓ²) for bit length ℓ, requiring only logarithmically many training examples relative to input space size
- Since SBN is Turing-complete, the framework extends to learning any computable function
- Empirical validation shows polynomial growth in required ensemble size with bit length while achieving perfect post-rounding accuracy

## Why This Works (Mechanism)
The paper's success stems from the template matching principle combined with NTK regime properties. By decomposing algorithmic instructions into local bit-level templates and training on orthonormalized versions, the network learns to recognize and compose these patterns. The infinite-width limit in NTK regime ensures that the model's behavior becomes deterministic and aligns with the template structure. The ensemble approach provides redundancy that guarantees exact execution with high probability. The careful structuring of training data to isolate bit-level rules prevents interference between different template components, allowing the network to learn the complete algorithm through iterative composition of simple patterns.

## Foundational Learning
- Neural Tangent Kernel (NTK): Why needed - provides theoretical framework for analyzing infinite-width neural networks; Quick check - verify NTK convergence in your training setup
- Template Matching: Why needed - decomposes algorithms into learnable bit-level patterns; Quick check - confirm template isolation in training data
- Ensemble Learning: Why needed - guarantees exact execution with high probability; Quick check - measure variance reduction across ensemble members
- Orthonormalization: Why needed - prevents correlation interference between template components; Quick check - verify orthogonality of training templates
- Turing-Completeness: Why needed - establishes framework's ability to learn any computable function; Quick check - confirm SBN instruction set coverage

## Architecture Onboarding

Component Map:
Input -> Two-Layer FC Network -> Template Matching -> Ensemble Aggregation -> Exact Output

Critical Path:
Data preparation (orthonormalized templates) -> NTK training -> Ensemble inference -> Post-rounding

Design Tradeoffs:
- Width vs. depth: infinite width preferred for NTK analysis but may be impractical
- Ensemble size vs. accuracy: larger ensembles guarantee higher probability of exact execution
- Template granularity vs. composability: finer templates are easier to learn but require more composition steps

Failure Signatures:
- Model predicts correct template but wrong composition order
- Ensemble members agree but all are wrong (systematic error)
- Post-rounding fails due to insufficient precision in template matching

First Experiments:
1. Train single network on binary permutation with varying widths to observe NTK convergence
2. Test template isolation by training on individual bit-level patterns separately
3. Build small ensemble and measure exact execution probability vs. ensemble size

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Requires infinite-width networks for theoretical guarantees, which may be impractical
- Ensemble approach increases computational cost and memory requirements
- Assumes clean template decomposition, which may not exist for all algorithms
- Limited to bit-level operations and may not scale to continuous or high-dimensional data

## Confidence

| Claim | Confidence |
|-------|------------|
| Exact execution is achievable in NTK regime | High |
| Ensemble complexity scales as O(ℓ²) | High |
| Framework extends to Turing-complete functions | Medium |
| Empirical validation matches theoretical predictions | Medium |

## Next Checks
1. Verify NTK convergence empirically for finite-width networks on template matching tasks
2. Test ensemble size scaling empirically across different bit lengths and algorithmic complexities
3. Attempt to apply framework to non-bit-level algorithms to assess template decomposition limitations