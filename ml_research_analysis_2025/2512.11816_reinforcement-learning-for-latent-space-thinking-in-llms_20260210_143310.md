---
ver: rpa2
title: Reinforcement Learning for Latent-Space Thinking in LLMs
arxiv_id: '2512.11816'
source_url: https://arxiv.org/abs/2512.11816
tags:
- latent
- training
- steps
- thinking
- qwen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates latent-space thinking as an alternative
  to discrete token-based reasoning in large language models. It demonstrates that
  existing supervised fine-tuning methods for latent reasoning are sensitive to design
  choices and underperform on mathematical reasoning tasks.
---

# Reinforcement Learning for Latent-Space Thinking in LLMs

## Quick Facts
- **arXiv ID:** 2512.11816
- **Source URL:** https://arxiv.org/abs/2512.11816
- **Reference count:** 18
- **Primary result:** Latent-space CoT models achieve 22.6% pass@1 on GSM8K vs 72.6% for language-space CoT

## Executive Summary
This study investigates latent-space thinking as an alternative to discrete token-based reasoning in large language models. It demonstrates that existing supervised fine-tuning methods for latent reasoning are sensitive to design choices and underperform on mathematical reasoning tasks. Reinforcement learning approaches, including a novel latent RL method with direct latent-step supervision, fail to improve performance and exhibit training instability. Results show that while language-space CoT models achieve 72.6% pass@1 on GSM8K, latent-space models remain at 22.6%, highlighting that current latent-space training methods cannot match traditional reasoning approaches.

## Method Summary
The study uses Qwen2.5 1.5B Base as the foundation, adding three special tokens for latent-space reasoning. The Coconut SFT approach employs a 3-stage curriculum where language steps are progressively replaced with latent steps (2 latent per removed step, max 6 latent). Cross-entropy loss is masked on question and latent tokens. GRPO is modified to exclude latent embeddings from loss calculations. A novel Latent RL method trains a value head on latent-step embeddings with BCE loss, then optimizes the policy to maximize value predictions. The study evaluates on GSM8K-Aug-NL (300K pairs for SFT), OpenR1-Math-220k (10K for RL), and test sets GSM8K (1.3K) and MATH500 (500).

## Key Results
- Language-space CoT achieves 72.6% pass@1 on GSM8K; latent-space CoT achieves 22.6%
- Coconut SFT step-wise removal (17.9% pass@1) vastly outperforms token-wise removal (7.5%)
- Latent RL shows training instability with oscillating losses and reward hacking issues
- GRPO fails to improve latent-space models (21.8% vs 22.6% baseline)

## Why This Works (Mechanism)
None

## Foundational Learning
**Latent-space reasoning:** Alternative to token-based CoT using compressed latent embeddings instead of explicit language steps. Needed because it could reduce computational costs while maintaining reasoning quality. Quick check: Compare token counts between language and latent reasoning outputs.

**Curriculum learning for latent steps:** Progressive replacement of language reasoning steps with latent steps. Needed to smoothly transition models from explicit to compressed reasoning. Quick check: Verify stage-wise performance improvements in Coconut SFT.

**Reinforcement learning with latent embeddings:** Policy optimization using direct supervision from latent-step embeddings rather than language tokens. Needed because traditional RL methods cannot optimize intermediate latent steps. Quick check: Monitor ROC-AUC vs F1 gap for value model.

## Architecture Onboarding

**Component map:** Qwen2.5 Base -> Add latent tokens -> Coconut SFT (3 stages) -> GRPO/Latent RL -> Evaluation

**Critical path:** The most critical sequence is the Coconut SFT curriculum training, as performance is highly sensitive to step-wise vs token-wise removal and epoch per stage choices. The latent RL training path is equally critical but unstable, requiring careful monitoring of value head convergence before policy optimization.

**Design tradeoffs:** Fixed vs adaptive latent step count (fixed performs poorly, adaptive unexplored), excluding vs including latent tokens in RL loss (exclusion prevents optimization of intermediate steps), stage duration in curriculum learning (1 epoch optimal, 2 epochs causes overfitting).

**Failure signatures:** Coconut SFT performance drops from 17.9% to 7.5% when switching from step-wise to token-wise removal; GRPO fails completely on latent models with no performance gain; Latent RL exhibits oscillating losses and reward hacking even with frozen value head.

**3 first experiments:** 1) Train Coconut SFT with step-wise removal for 1 epoch per stage and verify 17.9% pass@1 on GSM8K. 2) Apply GRPO to language-space model and confirm 72.6% pass@1, then apply to latent-space and verify failure (21.8%). 3) Train Latent RL value head on latent embeddings and monitor for training instability.

## Open Questions the Paper Calls Out
1. **Latent RL stabilization:** What training paradigms can stabilize direct latent-step supervision and prevent oscillating losses and reward hacking? The paper concludes future work should address this, showing fluctuating losses and reward hacking even with frozen value heads.

2. **Adaptive latent steps:** Can mechanisms for adaptive/dynamic latent-step count improve performance over fixed-step configurations? The paper calls for developing such mechanisms after showing varying fixed steps (2-64) does not help.

3. **Cross-domain generalization:** Does latent-space thinking generalize differently across domains (logical reasoning, programming, general knowledge) compared to mathematical reasoning? The paper notes its mathematical focus limits generalizability to other tasks.

4. **Modified credit assignment:** What explains the lack of GRPO gains for latent-space models, and can modified credit assignment or dense rewards enable effective RL? The paper identifies the issue but does not validate alternative approaches.

## Limitations
- Mathematical reasoning focus limits generalizability to other tasks
- Training instability in novel Latent RL approach prevents reliable optimization
- GRPO fails to improve latent-space models due to inability to optimize intermediate steps
- Fixed latent-step configurations may not be optimal; adaptive mechanisms unexplored

## Confidence
- **High confidence:** Core finding that latent-space CoT underperforms language-space CoT (22.6% vs 72.6% pass@1 on GSM8K)
- **Medium confidence:** Specific performance numbers for individual ablations (e.g., token-wise vs step-wise removal)
- **Medium confidence:** Conclusion that latent-space reasoning cannot currently match language-space reasoning

## Next Checks
1. Implement and validate Coconut SFT curriculum with both step-wise and token-wise removal approaches, verifying the performance gap (17.9% vs 7.5% pass@1).
2. Replicate GRPO evaluation comparing language-space vs latent-space models, ensuring latent tokens are correctly excluded from loss calculations.
3. Test latent RL approach with both value head training and policy optimization phases, monitoring training stability and performance sustainability.