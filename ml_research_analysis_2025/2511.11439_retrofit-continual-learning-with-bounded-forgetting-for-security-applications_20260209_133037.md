---
ver: rpa2
title: 'Retrofit: Continual Learning with Bounded Forgetting for Security Applications'
arxiv_id: '2511.11439'
source_url: https://arxiv.org/abs/2511.11439
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000015
- uni0000001c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RETROFIT addresses continual learning under data-sensitive security
  constraints, where models must adapt to evolving threats and representations without
  access to historical data. The core method merges previously trained and newly fine-tuned
  models through parameter-level consolidation, using low-rank and sparse updates
  to confine parameter changes to independent subspaces and confidence-guided knowledge
  arbitration to balance contributions.
---

# Retrofit: Continual Learning with Bounded Forgetting for Security Applications

## Quick Facts
- arXiv ID: 2511.11439
- Source URL: https://arxiv.org/abs/2511.11439
- Reference count: 40
- Primary result: 38.6% retention improvement over continual learning baselines in malware detection

## Executive Summary
RETROFIT addresses continual learning in security applications where models must adapt to evolving threats without accessing historical data. The method combines previously trained and newly fine-tuned models through parameter-level consolidation, using low-rank and sparse updates to confine parameter changes to independent subspaces and confidence-guided knowledge arbitration to balance contributions. In malware detection under temporal drift, RETROFIT improves retention by 38.6% over continual learning baselines while exceeding oracle performance on new data. In binary summarization across decompilation levels, it achieves more than twice the BLEU score of prior transfer learning methods on stripped binaries and surpasses all baselines in cross-representation generalization.

## Method Summary
RETROFIT implements continual learning through a sequential model consolidation process. For each new task, a LoRA-style low-rank module is trained on new data while keeping the base model frozen. A sparse mask is learned via confidence-guided arbitration that determines whether to prioritize predictions from the old or new model based on calibrated confidence scores. The low-rank update is then merged into the running model using the learned mask. This process repeats for each new task, accumulating knowledge without accessing historical data. The method uses hard arbitration for classification tasks (malware detection) and soft arbitration for generation tasks (binary summarization), with Sparse Group-Lasso regularization to enforce mask sparsity.

## Key Results
- 38.6% improvement in retention (PTR) over continual learning baselines in temporal malware detection
- 2x higher BLEU score on stripped binary summarization compared to transfer learning methods
- Exceeds oracle performance on new data in malware detection while maintaining superior retention

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Subspace Confinement for Interference Reduction
RETROFIT restricts model updates to low-rank subspaces using masked LoRA, confining task-specific knowledge to a subspace of dimension r≪d_in. This reduces interference between old and new knowledge by limiting the capacity for updates to overwrite parameters critical to old tasks. The frozen random projection matrix A acts as a near-isometry, preserving geometric relationships and ensuring interactions in the low-rank coefficient space are a high-fidelity proxy for interactions in the full parameter space. The interference scales with r/D, making the constraint effective when r is sufficiently small.

### Mechanism 2: Confidence-Guided Knowledge Arbitration for Functional Preservation
The method uses model confidence to arbitrate between old and new teacher models, preserving reliable prior knowledge while allowing adaptation. A student model reconciles outputs from the old model and a newly fine-tuned model, using the old model's prediction as supervision when it has high calibrated confidence on the true label (Hard Arbitration) or weighting it more heavily (Soft Arbitration). This creates a "keep" vs. "gain" dynamic where confident correct predictions from the old model are protected from being overwritten by potentially noisy new updates.

### Mechanism 3: Model Consolidation via Parameter Merging
Sequentially merging masked low-rank updates into a running model allows knowledge accumulation without historical data access. Instead of training on old data, the previous model's parameters serve as a proxy for past knowledge. The update process trains only B_t on new data, learns a sparse mask M_t via arbitration to control merging, and merges A(M_t ⊙ B_t) into the running model θ_prev. This iteratively builds a consolidated model that integrates knowledge from non-stationary distributions without the error accumulation seen in naive sequential fine-tuning.

## Foundational Learning

### Concept: Catastrophic Forgetting
- Why needed here: This is the core problem RETROFIT solves. Sequential fine-tuning causes new updates to overwrite representations of old data, leading to a severe drop in performance on earlier tasks.
- Quick check question: If I fine-tune my malware detector on 2024 data, what happens to its accuracy on 2020 data?

### Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA
- Why needed here: RETROFIT builds on LoRA for its structural mechanism. Understanding that ΔW = AB and that A can be frozen while B is trained is essential.
- Quick check question: How does freezing the down-projection matrix A help with model merging and stability?

### Concept: Model Calibration
- Why needed here: The arbitration mechanism relies on "confidence." A miscalibrated model would break the arbitration logic.
- Quick check question: My model outputs 0.99 confidence on all samples, but is only 60% accurate. How would RETROFIT's arbitration react?

## Architecture Onboarding

### Component map
Base Model (frozen) -> LoRA Module (A frozen, B trainable, M trainable) -> Predictions -> Confidence Calculation -> Arbitration Loss -> Train M & B -> Merge into Base Model

### Critical path
Data → Base Model (frozen) + LoRA Module → Predictions → Calculate Old/New Confidence → Arbitration Loss → Train M & B → Merge into Base Model

### Design tradeoffs
- **Rank r**: Lower rank reduces interference but limits model capacity for new tasks
- **Arbitration Mode**: Hard arbitration gives sharper retention but may be brittle to noise; soft arbitration is smoother but may fail to protect critical knowledge
- **Sparsity Regularization**: Higher sparsity in M isolates tasks better but may hinder knowledge transfer across tasks

### Failure signatures
- **Model collapses on new data**: Likely tau is too high, causing the old model to always dominate arbitration
- **Forgetting on old data**: Likely rank r is too high, or the sparse mask M is not learning to be sparse enough, allowing interference
- **Training instability**: The confidence thresholds (tau) may be poorly initialized or not adapting to the data distribution

### First 3 experiments
1. **Rank Ablation**: Run a sweep on LoRA rank r (e.g., 4, 8, 16, 32) on a 2-task split to measure retention vs. adaptation tradeoff. Find the inflection point.
2. **Arbitration Threshold Scan**: Fix model and task, vary confidence threshold tau. Plot old task accuracy vs. new task accuracy to visualize arbitration tradeoff curve.
3. **Mask Sparsity Check**: Visualize learned mask M after training on a task. If not sparse (many non-zero values), increase regularization strength lambda.

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks specific LoRA rank values, leaving low-rank constraint effectiveness unverifiable across different r values
- Does not explore hyperparameter sensitivity or robustness to confidence miscalibration
- Evaluation focuses on two security domains with limited diversity in task types and model architectures

## Confidence
- **High Confidence**: Claims about outperforming baselines in presented tasks are supported by provided metrics, assuming reproducible experimental setup
- **Medium Confidence**: Theoretical justifications for low-rank and arbitration mechanisms are sound but practical impact only demonstrated in narrow setting
- **Low Confidence**: Does not address long-term stability under many sequential tasks; "bounded forgetting" is relative to baselines, not absolute guarantee

## Next Checks
1. **Rank Sensitivity Analysis**: Reproduce key results across range of LoRA ranks (r=4, 8, 16, 32). Plot retention vs. rank to confirm interference reduction claim.
2. **Calibration Stress Test**: Intentionally miscalibrate old model and measure impact on RETROFIT's performance to test arbitration robustness.
3. **Long Sequence Evaluation**: Extend evaluation to longer task sequence (5+ years malware data or 3+ decompilation levels). Measure degradation in old-task performance to assess "bounded forgetting" claim.