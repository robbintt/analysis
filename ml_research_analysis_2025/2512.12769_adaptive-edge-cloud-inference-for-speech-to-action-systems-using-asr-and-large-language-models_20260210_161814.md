---
ver: rpa2
title: Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large
  Language Models
arxiv_id: '2512.12769'
source_url: https://arxiv.org/abs/2512.12769
tags:
- inference
- edge
- systems
- latency
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing performance and
  privacy in voice-controlled IoT systems by proposing ASTA, an adaptive edge-cloud
  inference solution that dynamically routes voice commands based on real-time system
  metrics. The core method integrates on-device ASR and lightweight offline LLM inference
  with cloud-based processing, using a rule-based routing mechanism guided by CPU
  workload, device temperature, and network latency.
---

# Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models

## Quick Facts
- arXiv ID: 2512.12769
- Source URL: https://arxiv.org/abs/2512.12769
- Authors: Mohammad Jalili Torkamani; Israt Zarin
- Reference count: 34
- Primary result: 62.5% ASR accuracy, 47.5% commands generated without repair, balanced online/offline routing

## Executive Summary
This paper presents ASTA, an adaptive edge-cloud inference solution for voice-controlled IoT systems that dynamically routes voice commands between local and remote processing based on real-time system metrics. The system integrates on-device ASR with lightweight offline LLM inference and cloud-based processing, using a rule-based routing mechanism guided by CPU workload, device temperature, and network latency. A command validation and repair component ensures reliable execution. Evaluated on an NVIDIA Jetson platform with 80 spoken commands, ASTA demonstrates the viability of adaptive edge-cloud orchestration for resource-aware voice-controlled IoT systems.

## Method Summary
ASTA employs a hybrid architecture combining on-device ASR using an 8-bit quantized faster-whisper model with both local (TinyLlama-1.1B-chat-v1.0) and cloud (GPT-3.5-turbo) LLM inference. A rule-based routing mechanism evaluates CPU workload, device temperature, and network latency to determine the optimal inference path. The system includes a three-layer command validation process checking action validity, device existence, and index specificity, with history-based repair for missing components. A 0.5 probability metric balancer was used to artificially distribute load between online and offline paths during evaluation.

## Key Results
- Achieved 62.5% ASR accuracy on 80 spoken commands
- Generated executable commands without repair for 47.5% of inputs
- Successfully balanced online (53.8%) and offline (46.2%) inference decisions
- Demonstrated average CPU workload of 38.5%, latency of 87.3ms, and temperature of 46.0°C

## Why This Works (Mechanism)

### Mechanism 1: Metric-Aware Adaptive Routing
Real-time system metrics guide inference path selection to balance resource utilization and responsiveness. A rule-based decision unit evaluates CPU workload, device temperature, and network latency against thresholds. Offline inference is selected when CPU > 80% AND temperature > 50°C, or when network latency > 150ms; otherwise online inference is preferred. The core assumption is that threshold values generalize across deployment contexts and workloads.

### Mechanism 2: Lightweight On-Device ASR with Quantization
8-bit quantized ASR models enable on-device transcription with acceptable accuracy for resource-constrained edge devices. The faster-whisper implementation with 8-bit quantization processes audio locally, reducing memory footprint and latency while maintaining transcription capability. The core assumption is that quantization-induced accuracy loss remains acceptable for command recognition tasks.

### Mechanism 3: Rule-Based Command Validation and Repair
Multi-layer validation with history-aware repair improves end-to-end command executability beyond raw ASR/LLM output. Three-layer validation checks action validity, device existence, and index specificity. Missing components trigger automatic repair using frequency-based heuristics from command history. The core assumption is that historical command patterns predict user intent for ambiguous inputs.

## Foundational Learning

- Concept: Edge-Cloud Tradeoffs (latency vs. capability vs. privacy)
  - Why needed here: Understanding when to prefer local vs. remote inference is core to ASTA's routing logic.
  - Quick check question: Given CPU at 45%, temperature at 48°C, and network latency at 200ms, which inference path should be selected and why?

- Concept: ASR Error Modes (phonetic confusion, numeric transcription)
  - Why needed here: Identifying ASR failure patterns informs validation/repair design and expectation-setting.
  - Quick check question: Why might "turn off light two" be transcribed as "turn of light too"?

- Concept: LLM Capability Gap (small offline vs. large online models)
  - Why needed here: The 72% vs. 18.9% correct generation gap between online/offline paths directly impacts repair burden.
  - Quick check question: What compensating mechanisms are needed when offline inference produces incomplete commands?

## Architecture Onboarding

- Component map: Audio capture → Faster-Whisper ASR (8-bit quantized) → Metrics collector (CPU/temp/latency) → Rule-based router → LLM interface (TinyLlama-1.1B offline OR GPT-3.5-turbo online) → Three-layer validator (action/device/index) → Repair engine (history-based) → Execution layer → History logger

- Critical path: Latency-sensitive path: ASR → Metrics check → Router decision → LLM inference → Validation → Execution. Repair path branches when validation fails at any layer.

- Design tradeoffs: Fixed thresholds (80% CPU, 50°C, 150ms) vs. adaptive learning-based routing. Rule-based repair vs. LLM-based repair (current: rule-based for determinism). 50% metric perturbation for balanced routing evaluation vs. real-world workload patterns.

- Failure signatures: ASR errors on numeric words ("five" → "5") and phonetic confusions ("off" → "of"). Offline LLM (TinyLlama) generates correct commands at only 18.9% rate. Missing device index triggers repair; empty history table causes repair failure. Thermal throttling under sustained load not explicitly modeled.

- First 3 experiments: 1) Baseline routing: Run 80-command dataset with metrics unperturbed to establish natural online/offline distribution and identify threshold appropriateness. 2) Stress testing: Deliberately vary CPU load and temperature to validate routing threshold boundaries and measure command success rates under each path. 3) Ablation on repair: Disable repair mechanism and measure end-to-end success rate to quantify repair contribution (current data suggests ~19% absolute improvement).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NLP techniques be integrated to identify and correct missing words during transcription, and what impact would this have on overall ASR accuracy beyond the current 62.5%?
- Basis in paper: The authors state: "As future work, we suggest investigating the use of natural language processing techniques to identify and correct missing words during transcription and further improving the ASR accuracy."
- Why unresolved: The current pipeline lacks a dedicated post-ASR correction layer; errors in numeric representations and phonetically similar words persist.
- What evidence would resolve it: A comparative study showing improved end-to-end command accuracy after integrating NLP-based transcription correction.

### Open Question 2
- Question: Would an adaptive or learning-based routing mechanism outperform the current rule-based threshold approach (CPU > 80%, temperature > 50°C, latency > 150ms) across diverse edge hardware and workload scenarios?
- Basis in paper: The routing mechanism uses fixed thresholds and a probabilistic balancer (p = 0.5) to artificially distribute load, which the authors note should be "interpreted with caution."
- Why unresolved: Static thresholds may not generalize across different devices, environments, or usage patterns; no exploration of dynamic threshold adaptation.
- What evidence would resolve it: Experiments comparing rule-based vs. learning-based routing on varied hardware under realistic, unbalanced workloads.

### Open Question 3
- Question: What architectural improvements or alternative edge-suitable models could narrow the substantial performance gap between offline inference (18.9% correct command generation) and online inference (72%)?
- Basis in paper: The TinyLlama-1.1B model's low success rate compared to GPT-3.5-turbo highlights a significant capability disparity, yet only one offline model was evaluated.
- Why unresolved: No investigation of other compact LLMs, quantization strategies, or model distillation techniques tailored for command generation.
- What evidence would resolve it: Benchmarking multiple edge-optimized LLMs on the same command set, measuring accuracy, latency, and resource consumption.

### Open Question 4
- Question: How does ASTA scale to larger IoT ecosystems with more devices, command complexity, and higher concurrent request throughput?
- Basis in paper: Evaluation was limited to 80 commands controlling only two USB-powered lights, leaving multi-device, multi-user, and high-throughput scenarios unexplored.
- Why unresolved: The single-device, controlled dataset does not demonstrate generalizability to complex smart home or industrial IoT deployments.
- What evidence would resolve it: Deployment and evaluation on a multi-device testbed with diverse command types, concurrent users, and sustained workloads.

## Limitations
- Rule-based routing thresholds may not generalize across different edge devices with varying thermal characteristics
- 62.5% ASR accuracy represents a significant bottleneck where 52.5% of commands require repair
- Command repair mechanism relies on frequency-based heuristics that may not generalize to new users or multi-user scenarios
- Evaluation on only 80 commands limits statistical confidence in reported metrics
- No ablation study isolates the individual contribution of each component to overall system performance

## Confidence

- **High Confidence**: The basic architecture design (edge-cloud routing based on system metrics) is sound and well-supported by related work on adaptive inference systems.
- **Medium Confidence**: The 62.5% ASR accuracy and 47.5% command generation without repair metrics are specific and measurable, though their generalizability across different environments remains uncertain.
- **Low Confidence**: The effectiveness of the frequency-based repair mechanism in real-world deployment scenarios, particularly with diverse users and contexts.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the CPU (70-90%), temperature (40-60°C), and latency (100-200ms) thresholds across multiple edge devices to determine optimal values and identify threshold ranges where routing decisions significantly impact command success rates.

2. **User Diversity Study**: Evaluate the system with 10+ different users across varied environments (quiet office, noisy home, outdoor) to assess how ASR accuracy and command repair effectiveness vary with user voice characteristics, accents, and acoustic conditions.

3. **Ablation Study**: Disable the repair mechanism and measure end-to-end command execution success rates to quantify the absolute contribution of repair to system performance, then compare with alternative repair strategies (context-aware vs frequency-based).