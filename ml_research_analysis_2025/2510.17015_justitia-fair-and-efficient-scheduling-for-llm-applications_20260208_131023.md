---
ver: rpa2
title: 'Justitia: Fair and Efficient Scheduling for LLM Applications'
arxiv_id: '2510.17015'
source_url: https://arxiv.org/abs/2510.17015
tags:
- uni00000013
- application
- applications
- fair
- justitia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Justitia, a fair and efficient scheduler for
  Large Language Model (LLM) applications in shared GPU clusters. The key idea is
  to serve applications in a "saturated manner" following their "fair completion order,"
  which trades short-term fairness for higher efficiency.
---

# Justitia: Fair and Efficient Scheduling for LLM Applications

## Quick Facts
- **arXiv ID:** 2510.17015
- **Source URL:** https://arxiv.org/abs/2510.17015
- **Reference count:** 40
- **Key result:** Reduces average application completion time by 57.5% compared to state-of-the-art VTC scheduler while ensuring 92% of applications finish no later than under VTC

## Executive Summary
This paper proposes Justitia, a fair and efficient scheduler for Large Language Model (LLM) applications in shared GPU clusters. The key innovation is serving applications in a "saturated manner" following their "fair completion order," which trades short-term fairness for higher efficiency. Justitia addresses three core challenges: modeling service cost with a memory-centric approach that captures cumulative KV cache occupation over time, predicting demand using application-specific MLP models for lightweight and accurate forecasting, and employing a virtual-time based fair queuing algorithm to efficiently determine application priorities with theoretical worst-case delay guarantees. Implemented on vLLM, Justitia achieves significant improvements in job completion time while maintaining fairness guarantees.

## Method Summary
Justitia is a virtual-time fair queuing scheduler implemented on vLLM that prioritizes applications based on their expected completion order under an idealized fair-sharing scheme (GPS). The scheduler uses a memory-centric cost model ($c = pd + d^2/2$) that captures cumulative KV cache occupation, and employs application-specific MLP predictors trained on 100 historical samples to forecast service demand. Applications are served sequentially in "fair completion order" rather than with instantaneous fair sharing, allowing the prioritized application to use all available resources until completion. The system integrates with vLLM's scheduler to intercept request arrival and completion events while maintaining non-preemptive semantics to avoid KV cache swap overhead.

## Key Results
- Reduces average application completion time by 57.5% compared to VTC scheduler
- Ensures 92% of applications finish no later than under VTC, with only 8% experiencing slight delays
- Memory-centric cost modeling provides 42.3% improvement over compute-centric alternatives
- MLP predictor achieves 53.0% average relative error vs 452% for DistilBERT baseline
- Theoretical worst-case delay bound of 2c_max + C_max/M for avoiding starvation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Serving LLM applications sequentially in "fair completion order" rather than with instantaneous fair sharing can reduce average completion time without violating long-term fairness guarantees.
- Mechanism: Instead of allocating each application its fair share of resources at every time step, Justitia prioritizes applications one-by-one based on their expected completion order under an idealized fair-sharing scheme (GPS), allowing the prioritized application to use all available resources until completion.
- Core assumption: Users care primarily about application completion time (long-term fairness) rather than instantaneous resource allocation (short-term fairness); application completion order can be accurately predicted at arrival time.
- Evidence anchors: [abstract] "serves applications in a 'saturated manner' following their 'fair completion order,' which trades short-term fairness for higher efficiency"; [section III-A] "serving them—not concurrently with the fair share—but sequentially in the fair completion order with all the resources—can reduce the average completion time, with no application actually delayed."

### Mechanism 2
- Claim: Memory-centric cost modeling (KV token-time) more accurately captures LLM inference service cost than compute-centric models.
- Mechanism: The cost metric $c = pd + d^2/2$ sums cumulative KV cache occupation over all iterations, capturing both spatial (memory size) and temporal (duration) dimensions. This quadratic relationship with decode length reflects that longer outputs both occupy more memory per iteration and require more iterations.
- Core assumption: GPU memory (KV cache space) is the primary bottleneck in LLM inference serving under frameworks like vLLM; prompt sizes are typically small relative to total KV cache capacity.
- Evidence anchors: [abstract] "modeling service cost with a memory-centric approach (capturing cumulative KV cache occupation over time)"; [section IV-A] "the relationship between the cost volume and the generation length (d) is indeed quadratic"; [section V-D] "compute-centric cost modeling would incur a JCT performance degradation of up to 42.3%".

### Mechanism 3
- Claim: Application-specific MLP models can provide lightweight and sufficiently accurate demand prediction for scheduling decisions.
- Mechanism: For each application type, a 4-layer MLP is trained on ~100 historical samples using TF-IDF vectorized input prompts. The model predicts total KV token-time cost, enabling virtual finish time calculation at application arrival.
- Core assumption: Applications of the same type exhibit similar input-output patterns across runs; 100 samples provide sufficient training data; TF-IDF captures the relevant features for cost prediction.
- Evidence anchors: [abstract] "uses a simple neural network model to conduct light-weight and also accurate demand prediction"; [section IV-B] "such an application-specific prediction method can attain better accuracy performance"; [section V-D] MLP shows 53.0% average relative error vs 452% for DistilBERT, with 2.16ms vs 55.7ms inference overhead.

## Foundational Learning

- **Concept: KV Cache in Auto-regressive LLM Inference**
  - Why needed here: Justitia's cost model depends on understanding how KV cache grows during token generation and why memory is the bottleneck.
  - Quick check question: Can you explain why the KV cache occupation increases with each generated token and how this affects the maximum batch size in vLLM?

- **Concept: Virtual-time Fair Queuing and GPS (Generalized Processor Sharing)**
  - Why needed here: Justitia borrows networking fair queuing concepts to determine application ordering. Understanding virtual time $V(t)$ and how it differs from real time is essential for following the priority calculation.
  - Quick check question: If three applications arrive simultaneously with costs $C_1=100$, $C_2=200$, $C_3=300$, and $M=100$ KV-blocks, what is their virtual finish time order under GPS?

- **Concept: LLM Application Patterns (MapReduce, Self-Consistency, Fact Verification)**
  - Why needed here: Justitia schedules at the *application* level, not individual inference level. Understanding that applications consist of multiple correlated inferences explains why inference-level schedulers like vLLM's FCFS fail.
  - Quick check question: For a Self-Consistency application that spawns 10 parallel reasoning paths and then aggregates results, what happens to end-to-end latency if each path is interleaved with requests from other applications?

## Architecture Onboarding

- **Component map:** vLLM Integration Layer -> MLP Demand Predictor -> Virtual Time Manager -> Fair Queuing Scheduler -> Cost Aggregator

- **Critical path:**
  1. Application arrives → TF-IDF vectorization of input prompt (~2ms)
  2. MLP predicts total KV token-time cost $C_j$
  3. Virtual Time Manager calculates $F_j = V(a_j) + C_j$
  4. Application inserted into priority queue by $F_j$
  5. On inference completion, scheduler selects next application with minimum $F_j$
  6. All inferences from selected application served until application completes or preemption point
  7. Update virtual time based on resources consumed

- **Design tradeoffs:**
  - Memory vs Compute-centric Cost: Paper empirically validates memory-centric is better (42.3% improvement), but this may not hold for all model/hardware configurations
  - MLP vs Transformer Predictor: MLP is 25x faster with 8.5x lower error on this workload, but MLP may not generalize to highly diverse application types
  - Saturated vs Fair-share Serving: Improves average JCT by 57.5%, but 8% of applications experience slight delays (max 26% worst-case)
  - Non-preemptive Scheduling: Avoids KV cache swap overhead, but limits responsiveness to newly-arrived high-priority applications

- **Failure signatures:**
  - **Starvation:** Unlike SRJF (which can starve large applications indefinitely), Justitia bounds delay to $2c_{max} + C_{max}/M$. If you observe unbounded delays, check virtual time calculation.
  - **Prediction Drift:** If MLP error increases over time, application input distribution may have shifted; retrain on recent data.
  - **Queue Order Inversion:** If applications with earlier virtual finish times complete later, verify that cost prediction is not systematically biased for certain application types.
  - **Memory Fragmentation:** If available KV blocks exist but requests remain queued, prompt sizes may exceed assumptions; the paper assumes prompts are small relative to total KV cache.

- **First 3 experiments:**
  1. **Baseline Comparison:** Run the provided workload suite (300 apps, mixed types) with vLLM, Parrot, VTC, SRJF, and Justitia; measure average JCT, P90 JCT, and finish-time fair ratio. Expected: Justitia within 5% of SRJF efficiency with bounded worst-case delay.
  2. **Ablation on Cost Model:** Replace Justitia's memory-centric cost model with VTC's compute-centric model ($p + 2d$); measure JCT degradation. Expected: 30-45% worse average JCT per Section V-D.
  3. **Starvation Stress Test:** Submit one large MRS application followed by continuous stream of small applications at 1/second; measure delay of the large application under SRJF vs Justitia as mice count increases. Expected: Justitia delay bounded; SRJF delay grows unbounded.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Justitia's theoretical delay bound degrade in scenarios where memory fragmentation is non-negligible due to large prompt sizes or heterogeneous KV cache lifetimes?
- Basis: [explicit] Section IV-D states the analysis assumes "prompt size of typical inferences are much smaller in scale than the total KV cache space" and explicitly neglects fragmentation to simplify the proof.
- Why unresolved: The theoretical guarantee for $f_j - \bar{f}_j$ relies on the assumption that available KV-blocks are always fully utilized without fragmentation loss, which may not hold for workloads with highly variable sequence lengths.
- What evidence would resolve it: An empirical study analyzing the gap between the theoretical bound and observed delays under synthetic workloads designed to maximize memory fragmentation.

### Open Question 2
- Question: Can the scheduling efficiency be maintained for "cold-start" applications where insufficient historical data exists to train the application-specific MLP models?
- Basis: [inferred] Section IV-B notes the MLP is trained on "100 samples per application," assuming data is "readily available," but offers no fallback for newly deployed applications lacking this history.
- Why unresolved: The system relies on accurate cost prediction for the fair queuing priority; the paper does not address performance robustness when predictions are missing or significantly erroneous due to lack of training data.
- What evidence would resolve it: Evaluation of Justitia's sensitivity to training set size (e.g., < 10 samples) or testing with a fallback heuristic predictor for unseen applications.

### Open Question 3
- Question: Does the memory-centric cost model remain the optimal proxy for service cost on hardware where compute logic (rather than KV cache capacity) is the primary bottleneck?
- Basis: [inferred] Section IV-A justifies the model by stating "memory is prevalently a bottleneck," but acknowledges that inference consumes both resources without testing compute-bound scenarios.
- Why unresolved: Optimizing for memory-time (KV token-time) might not correlate with wall-clock time if the serving framework is constrained by GPU FLOPs rather than VRAM.
- What evidence would resolve it: Comparative experiments on compute-constrained setups (e.g., smaller batch sizes or heavily quantized models) contrasting Justitia against a compute-centric scheduling baseline.

## Limitations

- Application diversity: Results demonstrated across 9 specific application types may not generalize to all LLM workloads
- Non-preemptive assumption: Theoretical delay bound analysis assumes no KV cache swapping, which may not hold under severe memory pressure
- Static predictor models: MLP models are trained once and not retrained during operation, potentially leading to prediction drift over time

## Confidence

**High Confidence** (Empirical evidence with controlled experiments):
- Memory-centric cost modeling provides 42.3% improvement over compute-centric alternatives
- MLP predictor achieves 53.0% average relative error vs 452% for DistilBERT baseline
- Overall JCT reduction of 57.5% vs state-of-the-art VTC scheduler
- Virtual time update formula and fair queuing algorithm implementation

**Medium Confidence** (Theoretical guarantees with limited stress testing):
- Worst-case delay bound of 2c_max + C_max/M for avoiding starvation
- The "fair completion order" approach actually preserves long-term fairness
- 8% of applications experiencing slight delays represents acceptable trade-off

**Low Confidence** (Limited exploration or assumptions):
- Performance with highly heterogeneous application types beyond the 9 tested classes
- Impact of predictor drift over extended operation periods
- Behavior under extreme memory pressure requiring frequent preemption

## Next Checks

1. **Predictor Robustness Test:** Implement a time-varying workload where application input patterns shift gradually over the scheduling period. Measure MLP prediction error drift and resulting JCT degradation to quantify the impact of static predictor models.

2. **Memory Pressure Stress Test:** Design scenarios where available KV cache is consistently below the sum of active applications' requirements. Measure the frequency and overhead of KV cache swapping, and verify whether the theoretical delay bound still holds under these conditions.

3. **Generalizability Benchmark:** Apply Justitia to a broader set of LLM workloads including chat completion patterns, code generation tasks, and multimodal applications. Compare performance against specialized schedulers designed for these specific patterns to identify scenarios where Justitia's general approach may underperform.