---
ver: rpa2
title: Pushing the Limits of Beam Search Decoding for Transducer-based ASR models
arxiv_id: '2506.00185'
source_url: https://arxiv.org/abs/2506.00185
tags:
- beam
- decoding
- search
- alsd
- blank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of slow beam search decoding
  in transducer-based ASR models, which limits their usability in low-resource languages
  and context-biasing tasks. The authors propose a universal acceleration method that
  enables two optimized algorithms, ALSD++ and AES++, by leveraging batched operations,
  a tree-based hypothesis structure, novel blank scoring for shallow fusion, and CUDA
  graph execution.
---

# Pushing the Limits of Beam Search Decoding for Transducer-based ASR models

## Quick Facts
- arXiv ID: 2506.00185
- Source URL: https://arxiv.org/abs/2506.00185
- Authors: Lilit Grigoryan; Vladimir Bataev; Andrei Andrusenko; Hainan Xu; Vitaly Lavrukhin; Boris Ginsburg
- Reference count: 0
- Primary result: Reduces beam search decoding speed gap to 10-20% while achieving 14-30% WER improvement over greedy decoding

## Executive Summary
This paper addresses the challenge of slow beam search decoding in transducer-based ASR models, which limits their usability in low-resource languages and context-biasing tasks. The authors propose a universal acceleration method that enables two optimized algorithms, ALSD++ and AES++, by leveraging batched operations, a tree-based hypothesis structure, novel blank scoring for shallow fusion, and CUDA graph execution. This reduces the speed gap between beam and greedy decoding to 10-20% for the whole system while achieving 14-30% relative WER improvement compared to greedy decoding and up to 11% improvement in shallow fusion for low-resource scenarios. All algorithms are open-sourced.

## Method Summary
The authors develop a universal acceleration method for transducer-based ASR beam search decoding that combines several optimization techniques. The core approach involves implementing batched operations across multiple hypotheses, using a tree-based hypothesis structure to improve memory efficiency and cache locality, introducing a novel blank scoring mechanism for shallow fusion integration, and leveraging CUDA graph execution to minimize kernel launch overhead. These optimizations enable two specific algorithms: ALSD++ (Accelerated Look-Ahead Search) and AES++ (Accelerated Efficient Search), both designed to maintain accuracy while significantly improving decoding speed. The method is particularly effective for context-biasing tasks and low-resource language scenarios where beam search's superior accuracy is most valuable.

## Key Results
- Reduces beam search decoding speed gap to 10-20% compared to greedy decoding
- Achieves 14-30% relative WER improvement compared to greedy decoding
- Up to 11% WER improvement in shallow fusion for low-resource scenarios

## Why This Works (Mechanism)
The acceleration works by addressing the computational bottlenecks in transducer-based beam search. By batching operations across multiple hypotheses, the method reduces redundant computations and improves GPU utilization. The tree-based hypothesis structure optimizes memory access patterns and reduces pointer chasing overhead. The novel blank scoring mechanism enables more efficient integration with shallow fusion techniques. CUDA graph execution eliminates the overhead of repeated kernel launches by pre-compiling the execution graph. These optimizations work synergistically to dramatically reduce the computational cost while maintaining or improving accuracy.

## Foundational Learning
- **Transducer-based ASR models**: Models that predict output sequences conditioned on both input and previous outputs, needed to understand the specific challenges in beam search decoding
- **Beam search decoding**: A heuristic search algorithm that explores multiple hypotheses in parallel, required to grasp why this method is more computationally intensive than greedy decoding
- **Shallow fusion**: A technique for integrating external language models during decoding, important for understanding the blank scoring innovation
- **CUDA graph execution**: A mechanism to pre-compile GPU kernels for repeated execution, crucial for understanding the performance optimization
- **Batched operations**: Processing multiple data elements simultaneously, fundamental to the computational efficiency gains
- **Tree-based hypothesis structure**: An organizational approach for hypotheses that improves memory access patterns, key to the architectural innovation

## Architecture Onboarding

**Component map**: Input features -> Encoder -> Joint network -> Beam search (ALSD++/AES++) -> Output hypotheses -> Optional shallow fusion integration

**Critical path**: The beam search algorithm is the critical path, where the main optimizations (batching, tree structure, blank scoring, CUDA graphs) are applied to reduce computational overhead while maintaining search quality.

**Design tradeoffs**: The main tradeoff is between computational efficiency and search completeness. The optimizations reduce the number of operations but must preserve enough search breadth to maintain accuracy improvements over greedy decoding. The tree structure improves memory efficiency but adds complexity to hypothesis management.

**Failure signatures**: Performance degradation on very long sequences where memory pressure becomes significant, accuracy drops if the batched operations oversimplify hypothesis scoring, CUDA graph compilation failures on heterogeneous hardware configurations.

**First experiments**: 1) Benchmark baseline vs optimized decoding speed on a standard dataset, 2) Measure WER improvement over greedy decoding with and without shallow fusion, 3) Profile memory usage and identify bottlenecks in the tree-based hypothesis management.

## Open Questions the Paper Calls Out
None

## Limitations
- CUDA graph optimization performance may not generalize across different hardware configurations
- Limited evaluation details for low-resource language scenarios
- Performance portability concerns for non-NVIDIA hardware

## Confidence
- Algorithmic improvements (ALSD++, AES++): High confidence
- CUDA graph optimization benefits: Medium confidence
- 14-30% WER improvement claims: Medium confidence
- 11% shallow fusion improvement in low-resource scenarios: Low confidence

## Next Checks
1. Benchmark the implementation across different GPU architectures (e.g., A100, H100, consumer GPUs) to verify CUDA graph performance portability
2. Conduct ablation studies isolating the contribution of each optimization (tree structure, batched operations, blank scoring, CUDA graphs) to quantify their individual impact
3. Evaluate the approach on truly low-resource languages with limited training data (â‰¤100 hours) to validate the 11% shallow fusion improvement claim in the intended target scenario