---
ver: rpa2
title: Writing as a testbed for open ended agents
arxiv_id: '2503.19711'
source_url: https://arxiv.org/abs/2503.19711
tags:
- writing
- actions
- document
- human
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates large language models (LLMs) as open-ended\
  \ agents in writing assistance, focusing on action diversity, human alignment, and\
  \ iterative refinement. Three models\u2014Gemini 1.5 Pro, Claude 3.5 Sonnet, and\
  \ GPT-4o\u2014were evaluated on 22 documents."
---

# Writing as a testbed for open ended agents

## Quick Facts
- arXiv ID: 2503.19711
- Source URL: https://arxiv.org/abs/2503.19711
- Authors: Sian Gooding; Lucia Lopez-Rivilla; Edward Grefenstette
- Reference count: 7
- Primary result: LLMs evaluated as open-ended writing agents show diverse action generation but struggle with reliable self-evaluation and maintaining human alignment during iterative refinement.

## Executive Summary
This paper investigates large language models as open-ended agents for writing assistance, focusing on three dimensions: action diversity, human alignment, and iterative refinement. Three models—Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o—were evaluated on 22 diverse documents across five document types. The study reveals that while models can generate diverse actions through embedding-based filtering, they struggle to reliably select high-quality actions and often fail to align with human preferences during iterative refinement. Human editors demonstrated greater action variability and a preference for subtractive edits compared to models' additive bias, highlighting challenges in building agents capable of adaptive, evaluative, and goal-grounded problem-solving.

## Method Summary
The study evaluated three LLMs on writing assistance tasks using 22 publicly available Google Documents spanning academic articles, business memos, creative writing, instructional guides, and opinion essays. For each document, models generated actions with diversity-maximizing sampling, followed by embedding-based similarity filtering to remove redundant suggestions. Human annotators rated 300 actions (100 per model across 5 documents) for quality and benefit. The evaluation included action-document cosine similarity, diversity metrics, human preference percentages, F1-scores for self-filtering at varying agreement thresholds, and iterative refinement across 10 steps with 5 actions per batch. Linear mixed-effects models analyzed the relationship between model performance and document characteristics.

## Key Results
- Gemini 1.5 Pro achieved the highest quality document-improving actions and best self-filtering performance
- GPT-4o produced the most document-specific suggestions and stable iterative improvements
- Human editors demonstrated greater action variability and preference for subtractive edits (removing/restructuring content) compared to models' additive bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diversity-maximizing sampling expands the explorable action space for open-ended writing tasks.
- Mechanism: Embedding-based similarity filtering removes redundant suggestions, forcing models to generate semantically distinct actions. This counters the tendency toward homogeneous, surface-level edits.
- Core assumption: Diverse actions contain higher-value suggestions at sufficient rate to justify filtering cost.
- Evidence anchors:
  - [abstract] "demanding both expansive exploration and adaptable strategies"
  - [Section 4.1] "Without it, models tend to generate homogeneous and often superficial suggestions"
  - [corpus] Weak direct corpus support; neighboring work on open-ended agents (MAGELLAN, MLR-Bench) emphasizes exploration but not specifically embedding-based filtering.
- Break condition: If diversity sampling yields actions that are distinct but uniformly low-quality, filtering improves coverage without improving outcomes.

### Mechanism 2
- Claim: Document-context-aware prompting improves action quality and self-evaluation for some models.
- Mechanism: Providing the full document as context allows models to ground suggestions in specific content rather than generating generic writing advice. Gemini 1.5 Pro showed consistent gains from document context; GPT-4o and Claude 3.5 Sonnet relied more on few-shot examples.
- Core assumption: Models can integrate long-context document information to tailor actions, rather than defaulting to generic suggestions.
- Evidence anchors:
  - [Section 7.1] "Gemini relies more on document context than examples... slight improvement when using zero-shot document-aware prompting"
  - [Section 4.2] "GPT-4o produces the most document-specific suggestions"
  - [corpus] No direct corpus evidence on document-context vs. few-shot tradeoffs for self-evaluation.
- Break condition: If document context increases inference cost without measurable quality gains for a given model, revert to example-based prompting.

### Mechanism 3
- Claim: Subtractive edits correlate with higher user preference; additive edits correlate with semantic drift and lower ratings.
- Mechanism: Human editors frequently remove, restructure, or simplify content. Models exhibit an "additive bias" (linked to sycophantic tendencies), proposing elaborations that expand text without improving it. Over iterations, this causes semantic drift—divergence from original intent.
- Core assumption: User preference for conciseness generalizes across document types; subtractive edits are underexplored in model training.
- Evidence anchors:
  - [abstract] "Human editors demonstrated greater action variability and a preference for subtractive edits... compared to models' additive bias"
  - [Section 8.1] "larger insertions correlated with lower user ratings (p < 0.001), while larger deletions correlated with higher ratings"
  - [corpus] Weak corpus support; neighboring papers don't address additive/subtractive edit asymmetry.
- Break condition: If subtractive edits are preferred in editing tasks but not in generative tasks (e.g., brainstorming), the mechanism is task-specific.

## Foundational Learning

- Concept: **Open-ended task design**
  - Why needed here: Writing lacks objective success criteria; understanding how to define and evaluate open-endedness is prerequisite to building agents for such domains.
  - Quick check question: Can you distinguish between tasks with verifiable rewards (math, code) vs. subjective rewards (writing, creative tasks)?

- Concept: **Semantic drift in iterative refinement**
  - Why needed here: Multi-step self-refining processes accumulate misaligned interventions, causing progressive divergence from original goals.
  - Quick check question: In a 10-step revision loop, what metric would you track to detect drift from the original document intent?

- Concept: **Action space exploration vs. exploitation**
  - Why needed here: Models must balance generating diverse candidate actions (exploration) with selecting high-quality ones (exploitation). The paper shows models can explore but struggle to evaluate.
  - Quick check question: If a model generates 100 diverse edit suggestions but cannot reliably rank them, what component is missing?

## Architecture Onboarding

- Component map:
  Document → Action Generator → Diversity Filter → Self-Evaluator → Action Applier → Revised Document → (loop)

- Critical path: The Self-Evaluator is the bottleneck; paper shows prompting alone is insufficient for reliable action selection.

- Design tradeoffs:
  - Diversity vs. quality: Aggressive diversity filtering yields broader action space but includes more low-quality suggestions
  - Correctness vs. preference: Claude had highest execution correctness but lowest user preference due to heavy insertions
  - Document context vs. few-shot examples: Gemini benefits from context; GPT-4o/Claude benefit from examples

- Failure signatures:
  - Semantic drift: Cosine similarity to original document drops below threshold over iterations
  - Additive cascade: Document length grows without quality improvement (Claude pattern)
  - Self-evaluation collapse: Model accepts its own low-quality actions at high rate

- First 3 experiments:
  1. **Baseline diversity audit**: Generate 100 actions per document without diversity filtering; measure redundancy rate and quality distribution.
  2. **Self-evaluation calibration**: Compare model self-filtering against human ratings; compute F1 at 0.5 agreement threshold to establish baseline.
  3. **Iterative drift detection**: Run 10-step revision on 5 documents; track insertion length, deletion length, and cosine similarity to original at each step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be trained to explore under-represented actions, such as subtraction and constructive criticism, rather than defaulting to additive elaboration?
- Basis in paper: [explicit] The authors note models favour "additive elaboration" and "sycophantic" affirmations, whereas humans frequently engage in "deletions, simplifications, and structural adjustments" (Finding 1.3; Section 9.1).
- Why unresolved: Current training incentives likely reinforce existing content rather than encouraging the critical evaluation necessary to reduce or restructure text.
- What evidence would resolve it: Demonstration of a fine-tuned model that exhibits a balanced distribution of additive and subtractive actions matching human editor profiles.

### Open Question 2
- Question: Can fine-tuning overcome the limitations of prompting strategies to enable reliable self-evaluation in autonomous agents?
- Basis in paper: [inferred] Finding 3.0 concludes that since prompting yields low filtering performance, "fine-tuning may be necessary to improve self-selection capabilities."
- Why unresolved: The study evaluated zero-shot and few-shot prompting but did not test fine-tuned models, leaving the efficacy of training-based self-evaluation unconfirmed.
- What evidence would resolve it: Benchmarks showing fine-tuned models achieving significantly higher F1-scores in filtering low-quality suggestions compared to prompting baselines.

### Open Question 3
- Question: What mechanisms can effectively preserve alignment with document-level goals and prevent semantic drift over successive revisions?
- Basis in paper: [explicit] The paper highlights that "small errors and misaligned interventions accumulate," leading to a "progressive divergence from the author's intent" (Section 2; Finding 4.0).
- Why unresolved: Models struggle to maintain "goal-grounded problem-solving" over multiple steps without external human oversight.
- What evidence would resolve it: An iterative refinement experiment where models maintain stable semantic similarity and user preference scores over 20+ revision steps.

## Limitations

- **Self-evaluation reliability**: All models perform poorly at self-filtering, with Gemini 1.5 Pro showing slight improvement but still falling short of human-level selection accuracy.
- **Task transferability**: Findings are based on writing tasks across five document types; mechanisms may not generalize to non-textual or non-iterative domains.
- **Human preference correlation**: Preference measure based on single rating scale may reflect annotator fatigue or subjective biases rather than objective quality improvements.

## Confidence

- **High confidence**: Gemini 1.5 Pro achieves highest quality document-improving actions and best self-filtering performance; models exhibit additive bias contrasting with human subtractive preferences; semantic drift occurs during iterative refinement.
- **Medium confidence**: Document-context-aware prompting improves action quality for Gemini; GPT-4o produces most document-specific suggestions; diversity filtering effectively expands action space.
- **Low confidence**: Specific embedding similarity thresholds for diversity filtering; exact generalizability of writing-specific findings to other open-ended domains.

## Next Checks

1. **Self-evaluation calibration experiment**: Compare model self-filtering against human ratings across varying agreement thresholds (0.3 to 0.7) to establish precise performance boundaries and identify if specific prompt engineering could improve selection accuracy.

2. **Domain transfer validation**: Apply the writing testbed methodology to a non-textual open-ended domain (e.g., code refactoring or visual design) to test whether embedding-based diversity filtering and subtractive bias patterns hold across modalities.

3. **Long-term drift measurement**: Extend iterative refinement from 10 to 50 steps on diverse document types, tracking not just cosine similarity but also topic coherence and task-specific metrics to quantify semantic drift accumulation rates.