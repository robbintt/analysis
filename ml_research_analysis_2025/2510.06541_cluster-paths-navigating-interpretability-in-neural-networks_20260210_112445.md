---
ver: rpa2
title: 'Cluster Paths: Navigating Interpretability in Neural Networks'
arxiv_id: '2510.06541'
source_url: https://arxiv.org/abs/2510.06541
tags:
- cluster
- hair
- paths
- actual
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces cluster paths, a post-hoc interpretability
  method that clusters neural network activations at selected layers to create compact,
  example-based explanations of model decisions. Cluster paths are evaluated using
  four novel metrics: path complexity, weighted-path purity, decision-alignment faithfulness,
  and path agreement.'
---

# Cluster Paths: Navigating Interpretability in Neural Networks

## Quick Facts
- arXiv ID: 2510.06541
- Source URL: https://arxiv.org/abs/2510.06541
- Authors: Nicholas M. Kroeger; Vincent Bindschaedler
- Reference count: 40
- One-line primary result: Cluster paths achieve 90% faithfulness on a five-class CelebA hair-color task while maintaining 96% agreement under Gaussian noise.

## Executive Summary
This paper introduces cluster paths, a post-hoc interpretability method that clusters neural network activations at selected layers to create compact, example-based explanations of model decisions. Cluster paths are evaluated using four novel metrics: path complexity, weighted-path purity, decision-alignment faithfulness, and path agreement. Experiments demonstrate that cluster paths identify spurious cues in a color-biased CIFAR-10 task, achieve 90% faithfulness on a five-class CelebA hair-color task, maintain 96% agreement under Gaussian noise, scale to large Vision Transformers, and detect out-of-distribution samples effectively. The method provides concise, human-readable explanations and outperforms baseline OOD detectors, achieving AUROC values near 0.97 and FPR@95%TPR below 0.07. Cluster paths complement existing explainers like DkNN and saliency maps, offering a practical tool for model debugging and reliability assessment.

## Method Summary
Cluster paths convert continuous neural network activations into discrete cluster IDs using k-means clustering at selected layers. During training, the method extracts activations from chosen layers, fits k-means++ clustering with 10 random restarts per layer, and stores centroids. For inference, each sample's activations are assigned to nearest centroids, creating a path tuple representing the sequence of cluster IDs. A Random Forest proxy model trained on these paths evaluates faithfulness by predicting the original model's outputs. Out-of-distribution detection uses path rarity - samples following low-probability paths are flagged as anomalous. The method scales to large models by storing only centroids (O(LKd)) rather than all neighbors, offering computational efficiency over alternatives like DkNN.

## Key Results
- Cluster paths achieve 90% decision-alignment faithfulness on five-class CelebA hair-color classification
- OOD detection performance reaches AUROC values near 0.97 with FPR@95%TPR below 0.07
- Path agreement remains at 96% under Gaussian noise, demonstrating robustness
- The method scales to Vision Transformers while maintaining interpretability
- Cluster paths successfully identify spurious color cues in a biased CIFAR-10 task

## Why This Works (Mechanism)

### Mechanism 1: Activation Clustering Induces Semantic Concepts
Grouping neural network activations into discrete clusters likely creates semantic abstractions that are more robust to noise than raw activations. K-means partitions the activation space at selected layers into K centroids. If the activation geometry maps to semantic features, the centroid ID acts as a discrete concept label (e.g., "blue patch" or "gray hair"). This quantization filters out high-frequency noise while retaining the "region" of the feature space the input occupies. The core assumption is that the manifold of learned features is locally convex or clustered such that Euclidean distance (k-means) correlates with semantic similarity.

### Mechanism 2: Path Enumeration Preserves Decision Logic
The sequence of cluster IDs (the "path") acts as a sufficient statistic for the model's final classification, allowing a proxy model to mimic the original network. By encoding layer-wise cluster traversal as a tuple (e.g., 2→5→1), the method maps high-dimensional continuous transformation into a discrete state sequence. A Random Forest trained on these sequences can reconstruct the decision boundary, implying the path captures the causal chain of the network's logic. The core assumption is that intermediate layers chosen for clustering contain necessary information for final output, and their interactions can be linearized/discretized without total loss of fidelity.

### Mechanism 3: Path Rarity Signals Distributional Drift
Out-of-distribution (OOD) samples traverse low-probability or unseen sequences of clusters, enabling detection without access to weights or gradients. The frequency of path tuples is empirically estimated from training data. At inference, if a sample falls into a centroid configuration that rarely or never occurs in training (path probability < ε), it is flagged as anomalous. This leverages the structural fragility of OOD samples - they rarely follow the "beaten path" of in-distribution data. The core assumption is that training data distribution creates a "heavy-tailed" set of dominant paths, and anomalies do not coincidentally align with these dominant routes.

## Foundational Learning

- **Concept:** K-Means Clustering & Vector Quantization
  - **Why needed here:** This is the fundamental operation that converts continuous, high-dimensional neural activations into discrete "concept" IDs. Understanding how centroids represent the mean of a distribution is crucial for debugging why a sample landed in a specific cluster.
  - **Quick check question:** If a cluster has high variance, what does that imply about the "concept" it represents?

- **Concept:** Hidden Layer Activations (Feature Hierarchies)
  - **Why needed here:** The method relies on extracting activations from specific layers (e.g., Block 4 vs. Block 8 in a ViT). You must understand that early layers capture low-level features (edges/colors) while later layers capture semantic concepts (objects/scenes).
  - **Quick check question:** Why would clustering the input layer fail to capture the model's learned "reasoning"?

- **Concept:** Surrogate (Proxy) Models
  - **Why needed here:** To measure "Faithfulness" (DAF), the authors train a Random Forest on the cluster paths to mimic the neural network. You need to understand that this tests if the discrete path retains enough information to replicate the original model's output.
  - **Quick check question:** If the Random Forest achieves 100% accuracy on the cluster paths, does that mean the path is "correct," or just that it perfectly memorizes the mapping?

## Architecture Onboarding

- **Component map:** Dataset → Forward Pass → Activation Extraction → K-Means Fit (get Centroids) → Nearest Centroid Lookup → Path Tuple Construction → OOD Lookup or Proxy Model

- **Critical path:** The selection of layers and the number of clusters K. If layers are too early (noise) or K is too small (under-segmentation), the paths become meaningless. The paper suggests starting where "activations correlate with class separation."

- **Design tradeoffs:**
  - **Complexity (Ω) vs. Faithfulness:** Increasing K increases faithfulness (granularity) but explodes the path complexity (Ω = ∏ K_l), making human interpretation difficult.
  - **Storage vs. Resolution:** Unlike DkNN which stores all neighbors (O(NLd)), Cluster Paths store only centroids (O(LKd)). This is efficient but loses local neighborhood nuance.

- **Failure signatures:**
  - **Low Faithfulness (< 70%):** Indicates the clustering is too coarse or the wrong layers are selected; the path is not a sufficient proxy.
  - **Low Path Agreement:** High sensitivity to Gaussian noise suggests the clusters are too small or packed tightly, capturing noise rather than robust concepts.
  - **One Dominant Path:** If 99% of samples take the same path, K is likely too low or the model is not learning discriminatory features.

- **First 3 experiments:**
  1. **Hyperparameter Sweep (K):** Run the clustering on a validation set with varying K (e.g., 5, 10, 20) and plot "Decision-Alignment Faithfulness" to find the "elbow" where faithfulness plateaus.
  2. **Spurious Correlation Check:** Train a model on the SpuriousCIFAR-10 dataset provided in the paper to verify if the "path" actually collapses when the color cue is removed (validating RQ1).
  3. **Visualization Consistency:** For a specific class (e.g., "Blond Hair"), visualize the images belonging to the top 3 most frequent paths to qualitatively check if the clustering has captured distinct visual sub-concepts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can cluster paths be adapted for recurrent neural networks (RNNs) or attention-only architectures where sequential or temporal dependencies are central?
- Basis in paper: [explicit] The authors state: "Extending them to recurrent or attention-only architectures will require thoughtful choices about which time steps, heads, or token positions to cluster. We leave the design of such adaptations to future research."
- Why unresolved: The current methodology is optimized for CNNs and Vision Transformers with distinct spatial layers; it has not yet defined how to aggregate or cluster temporal sequences or attention heads in a way that preserves the "path" abstraction.
- What evidence would resolve it: A demonstration of cluster paths successfully tracing decision routes in an RNN or Large Language Model (LLM), accompanied by the specific definitions for "layer" equivalents in those architectures.

### Open Question 2
- Question: Can alternative clustering algorithms (e.g., hierarchical or density-based) capture more complex activation manifolds without sacrificing the computational tractability currently offered by k-means?
- Basis in paper: [explicit] The authors note: "While k-means offers scalability... alternative clustering methods might capture more complex manifolds at the cost of tractability; exploring these remains future work."
- Why unresolved: K-means was chosen for its linear scalability and simplicity, but it assumes spherical clusters which may not fit all internal representations; the trade-off between manifold accuracy and computational cost for other algorithms is unknown.
- What evidence would resolve it: A comparative analysis of cluster path quality (using metrics like faithfulness and purity) versus computational overhead when using density-based methods versus k-means on high-dimensional activations.

### Open Question 3
- Question: What strategies can effectively prune cluster paths to manage the combinatorial explosion of path enumeration as network depth and granularity increase?
- Basis in paper: [explicit] The authors acknowledge: "Path enumeration can grow quickly with depth and granularity, so exploring more efficient clustering or path-pruning strategies is a promising avenue for future work."
- Why unresolved: While the current method limits clustering to specific layers, the theoretical maximum number of paths (Ω) still grows multiplicatively, potentially overwhelming human interpretation for very deep networks.
- What evidence would resolve it: The development of an algorithm that merges or prunes low-impact paths, resulting in a reduced path complexity Ω while maintaining high decision-alignment faithfulness (DAF).

## Limitations

- **Layer Selection Sensitivity:** The paper suggests excluding early layers but provides limited justification for which intermediate layers to select. The "layers correlated with class separation" are not rigorously defined, and the optimal layer selection may vary significantly across architectures and tasks.

- **Concept Granularity vs. Faithfulness Tradeoff:** While the paper mentions DAF saturates around 3C clusters per layer, the exact relationship between K and faithfulness across different architectures (CNN vs. ViT) is not systematically explored. The choice of K remains largely empirical.

- **OOD Detection Generalization:** The OOD method relies on path rarity, which assumes OOD samples will follow low-probability paths. However, this assumption may fail when OOD data shares semantic features with ID data (e.g., new classes that blend with existing ones), leading to false negatives.

## Confidence

- **High Confidence:** The core clustering mechanism (converting activations to discrete IDs) is well-established. The faithfulness evaluation using Random Forest proxies is a standard approach, and the reported metrics (AUROC ~0.97, DAF ~90%) are internally consistent with the methodology described.

- **Medium Confidence:** The OOD detection claims are supported by experimental results but rely on distributional assumptions that may not hold universally. The path agreement results under Gaussian noise (96%) suggest robustness, but the method's performance under structured perturbations is unknown.

- **Low Confidence:** The interpretability claims (e.g., "identifying spurious cues in color-biased tasks") are demonstrated on synthetic datasets but lack validation on real-world biased models. The method's ability to reveal why a model makes a decision versus what decision it will make remains unclear.

## Next Checks

1. **Layer Sensitivity Analysis:** Systematically vary the selected layers (e.g., test only early, only late, and combinations of intermediate layers) on the CelebA dataset and measure the impact on DAF and path agreement. This would quantify how critical layer selection is to the method's performance.

2. **Semantic OOD Test:** Create an OOD test set where samples are semantically similar to ID data but from a different distribution (e.g., a new hair color class not in CelebA's 5-class setup). Evaluate whether the path rarity method still flags these as OOD, or if it produces false negatives.

3. **Faithfulness vs. Complexity Tradeoff:** For the ViT-Base/16 model on ImageNet, sweep K from 2 to 20 clusters per layer and plot both DAF and Path Complexity (Ω) to identify the point of diminishing returns. This would provide practical guidance on hyperparameter selection.