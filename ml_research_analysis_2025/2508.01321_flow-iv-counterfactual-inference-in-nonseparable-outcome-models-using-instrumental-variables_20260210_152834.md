---
ver: rpa2
title: 'Flow IV: Counterfactual Inference In Nonseparable Outcome Models Using Instrumental
  Variables'
arxiv_id: '2508.01321'
source_url: https://arxiv.org/abs/2508.01321
tags:
- flow
- outcome
- counterfactual
- causal
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of performing counterfactual
  inference in nonseparable structural models using instrumental variables (IVs).
  Existing methods typically assume additive noise in the outcome, which limits their
  applicability to real-world scenarios where the outcome function may be nonseparable.
---

# Flow IV: Counterfactual Inference In Nonseparable Outcome Models Using Instrumental Variables

## Quick Facts
- **arXiv ID**: 2508.01321
- **Source URL**: https://arxiv.org/abs/2508.01321
- **Reference count**: 28
- **Primary result**: Flow IV achieves lower counterfactual MSE than Deep IV and Usual IV in nonlinear nonseparable data generating processes by leveraging normalizing flows to model nonseparable structural outcome functions.

## Executive Summary
This paper addresses the challenge of performing counterfactual inference in nonseparable structural models using instrumental variables (IVs). Traditional IV methods assume additive noise in the outcome, limiting their applicability when the outcome function is nonseparable. Flow IV proposes a novel approach that leverages normalizing flows to model the structural functions and maximize the likelihood of observed data under the assumption that latent noises in treatment and outcome are strictly monotonic and jointly Gaussian. This allows for counterfactual inference even when the outcome function cannot be separated into additive components. The method is evaluated on synthetic data and a real-world application, demonstrating its ability to accurately recover the underlying outcome function and outperform existing methods in certain scenarios.

## Method Summary
Flow IV models the structural causal model (SCM) using causal graphical normalizing flows (cGNF). The approach represents the joint distribution of instrument Z, treatment A, and outcome Y as a sequence of invertible mappings from latent Gaussian noise. Specifically, it uses Neural Spline Flows to learn conditional distributions P(A|Z) and P(Y|A) while incorporating a correlation parameter ρ between the latent noises. The method optimizes the log-likelihood of observed data through a three-stage training procedure: first optimizing the instrument flow, then the treatment flow, and finally the outcome flow jointly with the correlation parameter. Counterfactual inference is performed by inverting the observed outcome to recover the specific noise realization, then forward-passing with the new treatment value.

## Key Results
- Flow IV achieves lower counterfactual MSE compared to Deep IV and Usual IV in a nonlinear nonseparable data generating process (DGP 2)
- The method successfully recovers the underlying outcome function in synthetic experiments with separable and nonseparable DGPs
- Flow IV fails when the noise assumption is violated (DGP 3 with non-Gaussian noise), validating the importance of the theoretical assumptions
- Real-world application on Chinese foreign aid data demonstrates practical utility

## Why This Works (Mechanism)

### Mechanism 1: Identifiability via Gaussian Noise Constraints
If latent treatment and outcome noises are strictly monotonic and jointly Gaussian, the structural outcome function gY(A, εY) is uniquely identifiable from observational data, even if the function is nonseparable. The method leverages a theoretical proof showing that under a bivariate Gaussian noise assumption, no alternative function can reproduce the observed distribution without violating the Instrument's exclusion restriction.

### Mechanism 2: Causal Graphical Normalizing Flows (cGNF)
Normalizing flows can approximate the structural causal model (SCM) by learning invertible mappings from a latent Gaussian space to the observed variables (Z, A, Y), enabling likelihood maximization. The architecture uses Neural Spline Flows to model the conditional distributions P(A|Z) and P(Y|A).

### Mechanism 3: Counterfactual Abduction via Inversion
Once the flow is trained, counterfactual inference for an individual i is achieved by inverting the observed factuals (ai, yi) to recover the specific latent noise εY,i, and then forwarding the noise with a new treatment a'. This utilizes Pearl's 3-step counterfactual logic: Abduction, Action, and Prediction.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) vs. Regression**
  - Why needed here: Standard ML regression models associations P(Y|A). Flow IV models the data generating process Y = gY(A, εY).
  - Quick check question: If I observe that people carrying umbrellas have a higher chance of rain, does banning umbrellas stop the rain?

- **Concept: Instrumental Variables (IV) Assumptions**
  - Why needed here: The entire validity of Flow IV rests on finding a valid instrument Z.
  - Quick check question: Does the instrument Z (e.g., distance to a clinic) affect the outcome Y (health) only through the treatment A (drug uptake)?

- **Concept: Normalizing Flows & Change of Variables**
  - Why needed here: The "Flow" in Flow IV refers to Normalizing Flows.
  - Quick check question: Why must a Normalizing Flow be bijective (invertible) to allow for density estimation?

## Architecture Onboarding

- **Component map**: Z -> A -> Y (instrument flows through treatment to outcome)
- **Stage 1 (Z-Flow)**: hZ(εZ; θZ) → Ẑ (Models instrument density)
- **Stage 2 (A-Flow)**: hA(Z,εA; θA) → Â (Models treatment conditioned on IV)
- **Stage 3 (Y-Flow)**: hY(A,εY; θY) → Ŷ (Models outcome conditioned on treatment)
- **Correlation Module**: A parameter ρ linking the sampling of εA and εY

- **Critical path**:
  1. Sampling: Draw (εA, εY) ~ N2(ρ)
  2. Forward Pass: Compute Ẑ, Â, Ŷ using flow functions
  3. Loss Calculation: Compute Log-Likelihood comparing generated Ẑ, Â, Ŷ to ground truth
  4. Counterfactual Inference: Invert ground truth (a,y) to get ε, then re-run forward pass with new a'

- **Design tradeoffs**:
  - Decomposed vs. Joint Training: Decomposing the likelihood (training Z, then A, then Y sequentially) helps optimization
  - Flow Flexibility vs. Convergence: Neural Spline Flows are highly flexible but require more data to converge than linear "Usual IV"

- **Failure signatures**:
  - Non-Gaussian Noise: If the true noise is non-Gaussian, the model fails to identify the function (MSE spikes to >0.8 in DGP 3)
  - Non-monotonicity: If the true gY is not monotonic w.r.t noise, the abduction step (inversion) fails
  - Weak Instrument: If Z does not strongly affect A, the gradients for the outcome stage become unstable

- **First 3 experiments**:
  1. Synthetic Sanity Check (DGP 1): Implement Flow IV on a separable, nonlinear dataset. Verify that it matches Deep IV performance (MSE < 0.02) to ensure the flow is learning correctly.
  2. Stress Test (DGP 2 - Nonseparable): Test on Y = (sin(A+1.5)+1) · εY. Verify that Flow IV significantly outperforms Deep IV (Deep IV MSE ≈ 0.6, Flow IV MSE ≈ 0.07).
  3. Boundary Test (DGP 3 - Non-Gaussian): Test on a linear DGP with non-Gaussian noise. Verify that Flow IV fails (MSE > 0.8) to understand the strictness of the Gaussian assumption.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the Flow IV framework be adapted to handle discrete instruments, treatments, or outcomes?
  - Basis in paper: The authors state, "In practice, variables are often discrete and adapting our approach to handle such variables remains an open task."
  - Why unresolved: The current implementation relies on Neural Spline Flows, which require continuous variables to model the bijective mapping between latent Gaussian noise and observed variables.
  - What evidence would resolve it: A modified Flow IV implementation that successfully recovers structural functions and counterfactuals on synthetic datasets with discrete variables.

- **Open Question 2**: Can the identifiability results and the Flow IV methodology be generalized to arbitrary Directed Acyclic Graphs (DAGs) beyond the specific instrument-treatment-outcome structure?
  - Basis in paper: The authors note, "Furthermore, we plan to generalize the identifiability result and Flow IV approach of this paper to arbitrary DAGs."
  - Why unresolved: The current theoretical proof (Theorem 1) and the specific flow architecture are tailored to the three-variable SCM (Z -> A -> Y) defined in the paper.
  - What evidence would resolve it: A theoretical proof of identifiability for complex DAGs and a demonstration of the method's accuracy on datasets generated from non-linear graph structures.

- **Open Question 3**: How can the Flow IV assumptions regarding latent noise distributions be translated into verifiable assumptions about hidden confounders to aid domain experts?
  - Basis in paper: The authors write, "In future research, we aim at translating the Flow IV assumptions about the noise terms to assumptions about hidden confounders and independent noise."
  - Why unresolved: It is currently difficult for practitioners to validate if latent noises are "strictly monotonic" and "jointly Gaussian," making it hard to assess the method's suitability for real-world data.
  - What evidence would resolve it: A set of formal criteria or a mapping that links observable data properties or theoretical confounder structures to the required noise distribution parameters.

## Limitations

- The method relies critically on the assumption that latent noises are strictly monotonic and jointly Gaussian, which may not hold in many real-world scenarios
- Computational complexity increases with the number of variables and complexity of the SCM, potentially limiting scalability
- Performance depends heavily on having a strong, valid instrument, which can be difficult to find in practice

## Confidence

**High Confidence Claims**:
- The Flow IV architecture can model nonseparable outcome functions where Deep IV fails (demonstrated on DGP 2)
- The method fails when noise assumptions are violated (DGP 3 results)
- The three-stage optimization strategy improves training stability compared to joint optimization

**Medium Confidence Claims**:
- Flow IV outperforms existing methods in the nonlinear nonseparable DGP
- The real-world application demonstrates practical utility
- The correlation parameter ρ is successfully estimated and improves counterfactual accuracy

**Low Confidence Claims**:
- Generalization to arbitrary nonseparable functions beyond the tested DGPs
- Robustness to approximate violations of Gaussian noise assumption
- Performance on high-dimensional treatments or outcomes

## Next Checks

1. **Robustness to Noise Distribution Violations**: Systematically test Flow IV on a spectrum of non-Gaussian noise distributions (varying degrees of non-normality, different tail behaviors) to characterize the breakdown point and understand when the Gaussian assumption becomes problematic in practice.

2. **Instrument Strength Sensitivity Analysis**: Create synthetic datasets with varying instrument strength (weak to strong instruments) and measure how counterfactual MSE scales with instrument relevance. This would reveal the method's sensitivity to a fundamental IV assumption that affects all IV-based approaches.

3. **Scalability Benchmark**: Evaluate Flow IV on progressively larger datasets (10×, 100× the original size) and more complex SCMs (additional confounders, multiple instruments) to characterize computational scaling and identify practical limits on problem complexity.