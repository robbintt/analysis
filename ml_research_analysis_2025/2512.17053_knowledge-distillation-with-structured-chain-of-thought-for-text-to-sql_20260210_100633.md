---
ver: rpa2
title: Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL
arxiv_id: '2512.17053'
source_url: https://arxiv.org/abs/2512.17053
tags:
- reasoning
- structured
- student
- query
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Struct-SQL, a knowledge distillation framework
  that transfers structured reasoning from large language models to small language
  models for the Text-to-SQL task. The method distills a formal query execution plan
  as a structured teaching signal, contrasting with standard unstructured Chain-of-Thought
  distillation.
---

# Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL

## Quick Facts
- arXiv ID: 2512.17053
- Source URL: https://arxiv.org/abs/2512.17053
- Authors: Khushboo Thaker; Yony Bresler
- Reference count: 36
- Key outcome: Struct-SQL achieves 8.1% absolute improvement in execution accuracy over unstructured baseline on BIRD mini-dev benchmark

## Executive Summary
This paper introduces Struct-SQL, a knowledge distillation framework that transfers structured reasoning from large language models to small language models for the Text-to-SQL task. The method distills a formal query execution plan as a structured teaching signal, contrasting with standard unstructured Chain-of-Thought distillation. Evaluated on the BIRD mini-dev benchmark, the distilled small model achieves significant performance gains while addressing enterprise constraints of cost and security.

## Method Summary
Struct-SQL uses knowledge distillation to transfer reasoning capabilities from a large teacher model (GPT-4o) to a small student model (Qwen3-4B-Instruct) for Text-to-SQL tasks. The approach generates a formal query execution plan as structured teaching signal, then fine-tunes the student using QLoRA on the concatenated sequence of structured plan and SQL. The framework uses success-based sampling to construct distillation datasets and evaluates on BIRD mini-dev benchmark with execution accuracy as primary metric.

## Key Results
- 8.1% absolute improvement in execution accuracy over unstructured baseline on BIRD mini-dev
- Significant reduction in syntactic errors, particularly "No Such Column" hallucinations
- Structured distillation approach generalizes across different model architectures
- Trade-off identified: higher accuracy but 3.6x more inference tokens due to query plan generation

## Why This Works (Mechanism)

### Mechanism 1
Structured reasoning signals reduce schema hallucination by enforcing explicit schema linking prior to query generation. The Query Plan Chain-of-Thought (QP-CoT) forces the model to generate a formal execution blueprint before SQL generation, grounding the subsequent SQL in valid schema elements rather than latent associations.

### Mechanism 2
Distilling formal logic provides a lower-entropy, less ambiguous supervisory signal compared to free-form Chain-of-Thought. Standard CoT allows high variance in reasoning traces, while Struct-SQL uses a rigid, formalized template that simplifies the optimization landscape for the small model.

### Mechanism 3
Knowledge distillation via structured plans prioritizes correction of severe syntactic errors over subtle semantic mismatches. Learning the scaffold of a valid query first establishes a "syntactic floor," making syntactically valid but semantically imperfect queries preferable to hallucinated, unexecutable queries.

## Foundational Learning

- **Knowledge Distillation (KD)**: Needed to transfer reasoning capabilities from large to small models for solving cost/security/performance trilemma. Quick check: How does minimizing NLL of teacher's output differ from supervised fine-tuning on gold labels?

- **Parameter-Efficient Fine-Tuning (PEFT/QLoRA)**: Needed to enable training on limited hardware while mitigating catastrophic forgetting. Quick check: How does freezing base model weights and updating only adapter weights affect retention of general language knowledge?

- **Schema Linking & Hallucination**: Needed to understand the core failure mode of "schema hallucination" that Struct-SQL addresses. Quick check: Does the model generate schema link from input question alone or retrieve from provided context?

## Architecture Onboarding

- **Component map**: Teacher (GPT-4o) -> Prompting Strategy (QP-CoT) -> Student (Qwen3-4B-Instruct) -> Adapter (QLoRA) -> Data Curation (Active generation with success-based sampling)

- **Critical path**: 1) Dataset Generation: Prompt Teacher with QP-CoT -> Execute SQL -> Keep only successful triples; 2) Training: Fine-tune Student using QLoRA on concatenated sequence; 3) Inference: Prompt Student with QP-CoT -> Generate Plan -> Generate SQL

- **Design tradeoffs**: Accuracy vs. Latency (3.6x more inference tokens); Rigidity vs. Generalization (fixed plan structure ensures syntactic validity but may struggle with complex joins)

- **Failure signatures**: High Syntactic Error Rate (student failed to learn QP-CoT structure); Low Performance on JOINs (expected behavior); Overfitting to Plan (model generates perfect plan but SQL is invalid)

- **First 3 experiments**: 1) Baseline Verification: Replicate FN-Gold vs. ReasonSQL vs. Struct-SQL comparison on 100 samples; 2) Ablation on Plan Length: Truncate "Preparation Steps" to reduce token overhead; 3) Error Taxonomy Check: Categorize errors into SYN vs. SEM for failure analysis

## Open Questions the Paper Calls Out

1. Can structured distillation approach improve performance in complex reasoning domains other than Text-to-SQL? (Unresolved: current study only evaluates on Text-to-SQL task)

2. Does integrating structured query plans with unstructured reasoning traces yield superior distillation signal? (Unresolved: current experiments treat methods as separate baselines)

3. How can Struct-SQL framework be adapted to improve performance on multi-table JOIN operations? (Unresolved: structured plans may not capture logical nuance for complex joins as effectively as free-form reasoning)

## Limitations

- Prompt template fidelity unknown - exact QP-CoT formulation not disclosed, limiting reproducibility
- Dataset construction and stratification unclear - success-based sampling procedure and stratification details unspecified
- Generalization to other domains limited - evaluation only on BIRD benchmark, SQLite-specific

## Confidence

- **High Confidence**: Structured reasoning reduces syntactic errors (well-supported by error analysis and ablation studies)
- **Medium Confidence**: 8.1% improvement in execution accuracy (statistically significant but benchmark-specific)
- **Low Confidence**: Approach solves "Adoption Trilemma" (aspirational, performance claims benchmark-specific)

## Next Checks

1. Conduct prompt ablation study to isolate contribution of structural constraint from teacher-specific prompt engineering

2. Evaluate Struct-SQL on non-SQLite benchmark (e.g., Spider or WikiSQL) to assess robustness to different schema structures

3. Quantify impact of 3.6x token increase on end-to-end latency and user experience for latency-sensitive applications