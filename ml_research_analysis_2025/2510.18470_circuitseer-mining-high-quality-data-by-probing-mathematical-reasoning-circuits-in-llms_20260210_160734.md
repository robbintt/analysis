---
ver: rpa2
title: 'CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits
  in LLMs'
arxiv_id: '2510.18470'
source_url: https://arxiv.org/abs/2510.18470
tags:
- reasoning
- data
- heads
- selection
- circuitseer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CircuitSeer addresses the challenge of efficiently curating high-quality\
  \ reasoning datasets for large language models by leveraging the model\u2019s internal\
  \ attention mechanisms. The method identifies a sparse set of specialized attention\
  \ heads critical for reasoning through ablation studies and uses their activation\
  \ patterns to score data samples, with higher scores indicating greater reasoning\
  \ complexity."
---

# CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs

## Quick Facts
- arXiv ID: 2510.18470
- Source URL: https://arxiv.org/abs/2510.18470
- Reference count: 40
- One-line primary result: CircuitSeer achieves superior mathematical reasoning performance by fine-tuning on just 10% of data selected via attention-based scoring, outperforming full-dataset training by 1.4 points in Pass@1.

## Executive Summary
CircuitSeer addresses the challenge of efficiently curating high-quality reasoning datasets for large language models by leveraging the model's internal attention mechanisms. The method identifies a sparse set of specialized attention heads critical for reasoning through ablation studies and uses their activation patterns to score data samples, with higher scores indicating greater reasoning complexity. Experiments across four models and nine datasets show that fine-tuning on just 10% of data selected by CircuitSeer achieves superior performance compared to full-dataset training, with a 1.4-point gain in average Pass@1 on Qwen2.5-Math-7B. This approach provides a principled, model-internal alternative to costly external heuristics for data selection.

## Method Summary
CircuitSeer operates in two stages: head detection and data selection. First, it identifies reasoning-critical attention heads by ablating each head in a reference model using undifferentiated attention (uniform attention distribution) and measuring loss increases on a probe dataset. The top 5% of heads causing the largest loss increases are selected as the reasoning head set. Second, for each data sample, the method computes the variance of mean attention distributions across tokens, using only the identified reasoning heads. This variance score serves as a proxy for reasoning complexity. Samples are then soft-sampled based on normalized scores to create a high-quality subset (10% of original data) for fine-tuning, achieving better performance than training on the full dataset.

## Key Results
- Fine-tuning on 10% of data selected by CircuitSeer achieves 1.4-point higher average Pass@1 on Qwen2.5-Math-7B compared to full-dataset training.
- Soft sampling of selected data provides better generalization and source diversity than deterministic top-k selection, with 64.5 vs 62.8 average Pass@1.
- High-score samples consistently outperform low-score samples across all nine benchmarks, with AIME24 showing 37.5 vs 29.8 Pass@1.
- The method demonstrates 4.6× data efficiency while maintaining superior performance across multiple model families and reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A sparse subset of attention heads is selectively critical for mathematical reasoning, and ablating them causes disproportionate performance degradation.
- Mechanism: Head importance is quantified by ablating each attention head via scaled undifferentiated attention and measuring the resulting loss increase on a probe dataset. Heads causing the largest loss deltas are retained as "reasoning heads."
- Core assumption: Loss increase under ablation indicates causal contribution to reasoning; such heads form a reusable circuit across samples.
- Evidence anchors: [abstract] identifies sparse specialized heads; [section 3.2] defines ablation and head selection; [corpus] references circuit specialization theory.

### Mechanism 2
- Claim: Variance of mean attention distributions across tokens—computed only from identified reasoning heads—reflects reasoning complexity of a sample.
- Mechanism: For each input, extract attention matrices from H_math, compute per-token mean attention across heads, then score as the variance of this distribution. Higher variance implies focal attention at key reasoning junctures.
- Core assumption: Complex multi-step reasoning concentrates attention on logical keypoints; simpler problems yield more uniform attention.
- Evidence anchors: [section 3.3] links score to reasoning quality; [table 3] shows high-score samples outperform low-score samples; [corpus] lacks direct validation of variance-complexity mapping.

### Mechanism 3
- Claim: Soft (probability-proportional) sampling over normalized scores yields better generalization than deterministic top-k selection by balancing quality and diversity.
- Mechanism: Normalize sample scores into a selection distribution; sample stochastically until target subset size is reached. This occasionally includes lower-score samples that broaden coverage.
- Core assumption: Diversity of reasoning patterns matters for generalization; pure score maximization over-concentrates on a narrow distribution.
- Evidence anchors: [table 4] shows soft sampling outperforms top-k; [figure 4] demonstrates better source diversity; [corpus] lacks direct comparison studies.

## Foundational Learning

- Concept: Transformer Circuits
  - Why needed here: The method operationalizes circuit theory to identify functionally specialized heads rather than treating all attention uniformly.
  - Quick check question: Given an attention head whose ablation causes a large loss increase on reasoning but not on non-reasoning tasks, what does this suggest about its functional role?

- Concept: Ablation Studies
  - Why needed here: Head importance is established via intervention (ablating heads) and measuring downstream effect; this differs from purely correlational analysis.
  - Quick check question: If ablation loss change is small across all heads, what might this indicate about the probe dataset or model architecture?

- Concept: Supervised Fine-Tuning Data Selection
  - Why needed here: CircuitSeer aims to maximize downstream reasoning performance by curating a compact subset; understanding SFT dynamics contextualizes the efficiency claim.
  - Quick check question: Why might training on 10% of carefully selected data outperform full-dataset training in some settings, and when would you expect this to fail?

## Architecture Onboarding

- Component map:
  - Stage 1: Reference LLM -> Probe Dataset -> Per-head Ablation -> Importance Scoring -> Top-k Head Selection -> H_math
  - Stage 2: For each sample -> Forward pass through M_ref -> Extract attention from H_math -> Compute mean attention -> Compute variance score -> Normalize scores -> Soft-sample subset D'

- Critical path:
  1. Probe dataset curation (representative, high-loss samples) determines head relevance.
  2. Ablation rigor (proper scaling, no gradient leakage) ensures clean importance estimates.
  3. Score normalization and soft-sampling hyperparameters directly affect diversity–quality tradeoff.

- Design tradeoffs:
  - k (number of reasoning heads): Smaller k increases specificity but risks missing distributed reasoning; larger k introduces noise from non-reasoning heads.
  - Input vs output scoring: Input-based scoring better captures reasoning demands; output-based scoring may conflate style with complexity.
  - Soft vs top-k: Soft sampling improves diversity; top-k may yield marginal gains on narrow benchmarks but weaker generalization.

- Failure signatures:
  - Scores near-uniform -> likely probe mismatch or insufficient head specialization.
  - Selected subset heavily skewed to one problem type -> check soft-sampling entropy and source distribution.
  - Performance degrades vs random -> head detection may have identified non-reasoning heads; re-validate on probe set.

- First 3 experiments:
  1. Head Validation Ablation: Compare performance when ablating H_math vs random heads vs all heads on held-out reasoning benchmark.
  2. Score Sanity Check: Manually inspect high- vs low-scoring samples to verify correspondence with multi-step vs trivial reasoning.
  3. Sampling Ablation: Compare soft sampling, top-k, and random selection at fixed budget across at least two model families.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CircuitSeer effectively identify reasoning circuits and select high-quality data for domains beyond mathematics, such as code generation or logical deduction?
- Basis in paper: [inferred] The paper focuses exclusively on mathematical reasoning datasets and benchmarks. While the introduction suggests mechanistic interpretability is a general principle, the experiments do not validate if the "reasoning heads" identified are math-specific or general-purpose.
- Why unresolved: It remains unclear if the observed sparse activation of attention heads is a universal property of complex reasoning or an artifact of the structured nature of mathematical problem-solving.
- What evidence would resolve it: Applying CircuitSeer to non-mathematical benchmarks and comparing the overlap of identified "reasoning heads."

### Open Question 2
- Question: How sensitive is the identification of "reasoning heads" to the composition and size of the probe dataset?
- Basis in paper: [inferred] The methodology relies on a specific probe set to ablate heads. The paper does not analyze if this small, loss-centric sample introduces selection bias or if the identified circuits remain stable across different probe sets.
- Why unresolved: If the probe set is not representative of the full data distribution, the identified "reasoning heads" might only capture features of high-loss samples rather than general reasoning capabilities.
- What evidence would resolve it: An ablation study measuring the stability of head importance rankings when varying probe set size and sampling strategy.

### Open Question 3
- Question: Does the "weak-to-strong" generalization hold when the reference model is significantly smaller or less capable than the target model?
- Basis in paper: [explicit] The paper states the use of Qwen2.5-Math-1.5B-Instruct as the reference model but evaluates targets of similar scale.
- Why unresolved: A very small reference model may lack the internal circuits necessary to "comprehend" and score the complexity of data required to train a much larger, more capable model.
- What evidence would resolve it: Experiments where a small reference model is used to select data for fine-tuning a significantly larger model, compared against using a larger reference model.

## Limitations
- Probe Set Representativeness: The head detection stage relies on 300 high-loss samples that may over-represent certain problem types and bias the identified reasoning heads toward specific features rather than general mathematical reasoning.
- Mechanism Validation Gap: The paper demonstrates that data selected by CircuitSeer outperforms full-dataset training but does not directly validate that the identified heads form a coherent "reasoning circuit" through ablation studies on the reference model.
- Scoring Method Sensitivity: The variance-based scoring mechanism may be driven by non-reasoning factors such as sequence length or formatting artifacts, and lacks systematic analysis of what features correlate with high scores.

## Confidence
**High Confidence**:
- The circuit detection methodology is sound and produces meaningful data scores that correlate with downstream performance.
- The soft sampling approach consistently outperforms top-k selection in terms of source diversity and generalization.

**Medium Confidence**:
- The claim that high-score samples contain more complex reasoning problems is supported by qualitative examples but lacks systematic quantitative validation.
- The efficiency claim (10% subset outperforming full dataset) is demonstrated but may be sensitive to dataset composition and training hyperparameters.

**Low Confidence**:
- The specific claim that identified heads form "core reasoning circuits" analogous to established circuit theory is not directly validated through ablation studies.
- The assumption that variance of mean attention distributions is a reliable proxy for reasoning complexity is plausible but not rigorously tested against alternative measures.

## Next Checks
1. **Head Ablation Validation**: Perform systematic ablation of the identified reasoning heads on the reference model and measure degradation on a held-out reasoning benchmark, comparing against ablation of random heads and all heads.

2. **Score-Complexity Correlation**: Develop a quantitative complexity metric for math problems and measure the correlation between CircuitSeer scores and this metric across the full dataset to validate whether the variance-based scoring captures reasoning complexity.

3. **Probe Set Sensitivity Analysis**: Repeat the head detection process with probe sets constructed using different criteria and measure how the resulting head sets and downstream performance vary to assess robustness to probe set composition.