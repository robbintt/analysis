---
ver: rpa2
title: 'Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption
  through Empirical and Theoretical Analysis'
arxiv_id: '2511.04481'
source_url: https://arxiv.org/abs/2511.04481
tags:
- energy
- agents
- agent
- mindact
- consumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the environmental sustainability concerns
  of web agents powered by large language models (LLMs). The authors empirically benchmark
  five open-source web agents on eight different GPUs using the Mind2Web benchmark,
  measuring energy consumption alongside task performance.
---

# Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis

## Quick Facts
- arXiv ID: 2511.04481
- Source URL: https://arxiv.org/abs/2511.04481
- Reference count: 9
- Primary result: Empirical benchmarking shows AutoWebGLM is most energy-efficient web agent (0.33 kWh on H100-NVL) while achieving highest success rate (53.53%)

## Executive Summary
This paper addresses the environmental sustainability concerns of web agents powered by large language models (LLMs). The authors empirically benchmark five open-source web agents on eight different GPUs using the Mind2Web benchmark, measuring energy consumption alongside task performance. They also propose a theoretical estimation method for agents using proprietary LLMs. Key findings include: AutoWebGLM was both the most energy-efficient agent (0.33 kWh on H100-NVL) and achieved the highest average step success rate (53.53%). The least efficient open-source agent, Synatra, consumed 3.31 kWh—ten times more than AutoWebGLM—without better performance. The authors estimate that LASER, a proprietary LLM-based agent, would consume approximately 99.21 kWh, roughly 10× more than MindAct's estimated 9.01 kWh. They highlight that energy benchmarking is feasible and should become standard practice, while theoretical estimation can be unreliable (overestimating MindAct's consumption by a factor of 7). The study advocates for incorporating energy consumption metrics into web agent evaluation benchmarks.

## Method Summary
The authors empirically measured energy consumption of five open-source web agents (AutoWebGLM, MindAct, MultiUI, Synapse, Synatra) on the Mind2Web benchmark using carbontracker library with start/end flags around execution. They ran 5 repetitions per agent per GPU across 8 NVIDIA GPUs (A100-SXM4, A100-PCIe, RTX A6000, RTX 3090, H100-SXM5, H100-NVL, H200-SXM5, L40S), reporting mean±std. Energy per token was calculated using each LLM's tokenizer. For proprietary-LLM agents, they developed a theoretical estimation approach based on GPT-4's specifications (1.8T total parameters, 222B active per forward pass) and worst-case token assumptions, validated against MindAct measurements.

## Key Results
- AutoWebGLM achieved both highest efficiency (0.33 kWh on H100-NVL) and highest success rate (53.53%) among tested agents
- Energy consumption varied by up to 10× between agents (Synatra at 3.31 kWh vs AutoWebGLM at 0.33 kWh) without corresponding performance differences
- GPU architecture significantly impacts absolute energy consumption (H100-NVL most efficient, RTX A6000 least efficient) but agent efficiency rankings remain stable across hardware
- Theoretical estimation for proprietary models can be highly inaccurate (7× overestimation for MindAct) but may indicate order-of-magnitude differences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggressive preprocessing that reduces token count can lower total energy consumption even when per-token energy is higher.
- **Mechanism:** AutoWebGLM consumes more energy per input token (890–890×10⁻⁹ kWh/token) than Synatra (86.7×10⁻⁹ kWh/token), yet uses ~10× less total energy because preprocessing reduces total processed tokens from ~24.34M to ~0.25M on the cross-domain split.
- **Core assumption:** Preprocessing overhead is negligible compared to LLM inference costs; token reduction translates directly to compute reduction.
- **Evidence anchors:**
  - [Table 3, Page 4]: AutoWebGLM processes 0.25×10⁶ tokens vs Synatra's 24.34×10⁶ tokens while having higher per-token energy.
  - [Page 4, Results]: "While the energy per token was consistently highest for AutoWebGLM, the overall energy consumed was lowest due to AutoWebGLM's preprocessing."
  - [corpus]: Related work on quantized LLMs (arxiv:2504.03360) confirms energy per token as a useful metric, but does not address preprocessing effects—gap exists.
- **Break condition:** Preprocessing becomes compute-intensive (e.g., complex DOM parsing exceeds inference savings), or context compression degrades task performance below acceptable thresholds.

### Mechanism 2
- **Claim:** GPU architecture significantly influences absolute energy consumption, but relative agent efficiency rankings remain stable across hardware.
- **Mechanism:** The H100-NVL achieved lowest energy consumption across all agents (0.33–3.31 kWh), while RTX A6000 showed highest (0.76–8.72 kWh). However, agent ordering (AutoWebGLM < MultiUI < MindAct < Synapse < Synatra) by efficiency persists across all 8 GPUs tested.
- **Core assumption:** GPU power draw and utilization during inference are the primary determinants; cooling/overhead scale linearly.
- **Evidence anchors:**
  - [Table 6, Pages 10-11]: Complete GPU comparison showing consistent ranking patterns across architectures (Ampere, Hopper, Ada Lovelace).
  - [Page 3]: "On average, the most energy-efficient GPU in our benchmarking tests was the Nvidia H100-NVL."
  - [corpus]: LC-Opt benchmark (arxiv:2511.00116) addresses data center cooling optimization but not web agent GPU selection—insufficient direct evidence.
- **Break condition:** New GPU architectures fundamentally change memory bandwidth/compute ratios, or quantization techniques alter the inference efficiency landscape.

### Mechanism 3
- **Claim:** Theoretical energy estimation for proprietary LLM-based agents is unreliable for absolute values but may indicate orders-of-magnitude differences.
- **Mechanism:** Estimation for MindAct yielded 8.5 kWh vs measured 1.22 kWh (7× overestimate), driven by conservative upper-bound token assumptions in candidate generation. For LASER with GPT-4, estimation relies on leaked architecture specs and heuristics, compounding uncertainty.
- **Core assumption:** Token counts and energy-per-token values from literature are accurate; GPU utilization follows documented patterns (~70% during inference).
- **Evidence anchors:**
  - [Page 6]: "Using MindAct as an example — estimated at 8.5 kWh and benchmarked at 1.22 kWh — we showcase that a theoretical estimation approach works as a very coarse estimation in terms of orders of magnitude."
  - [Page 5]: Gap stems from "early termination, token truncation, and token reuse" not captured in worst-case assumptions.
  - [corpus]: No corpus papers validate theoretical estimation methods for proprietary models—this remains an open gap.
- **Break condition:** Proprietary model providers release actual token-energy metrics; inference patterns deviate significantly from assumed worst-case scenarios.

## Foundational Learning

- **Concept:** LLM Inference Energy per Token
  - **Why needed here:** Core metric for comparing agent efficiency; differs from training energy and varies by model size, quantization, and hardware.
  - **Quick check question:** If Agent A uses 10⁻⁶ kWh/token and processes 1M tokens, while Agent B uses 10⁻⁷ kWh/token but processes 100M tokens, which consumes more total energy?

- **Concept:** DOM (Document Object Model) as Web Agent Input
  - **Why needed here:** Web agents parse HTML DOM trees; token counts scale with DOM complexity (avg 1135 elements/site in Mind2Web), directly affecting energy.
  - **Quick check question:** Why might a preprocessing step that filters DOM elements reduce energy consumption more than switching to a smaller LLM?

- **Concept:** Mixture-of-Experts (MoE) Active Parameters
  - **Why needed here:** GPT-4 uses MoE with 1.8T total parameters but only ~222B active per forward pass; this affects FLOP calculations for energy estimation.
  - **Quick check question:** How does the active-parameter count in MoE architectures change the relationship between model size and inference energy compared to dense models?

## Architecture Onboarding

- **Component map:**
  Web Agent Pipeline: HTML/DOM Input -> Preprocessing (optional) -> LLM(s) (1 or more) -> Action Selection & Execution
  Energy Measurement: carbontracker library (start/end flags) -> GPU energy + benchmark completion -> total kWh

- **Critical path:** Preprocessing efficiency -> token count reduction -> LLM calls -> total energy. AutoWebGLM optimizes all three; Synatra processes raw HTML with a 7B model without filtering, maximizing token exposure.

- **Design tradeoffs:**
  - Small specialized models (DeBERTa-86M + flan-T5XL) vs. large general model (GPT-4, CodeLlama-7B)
  - Preprocessing overhead vs. inference savings
  - Step success rate vs. energy consumption (these are NOT necessarily correlated—AutoWebGLM achieves both)

- **Failure signatures:**
  - Energy per token decreasing but total energy increasing -> insufficient preprocessing, token explosion
  - Theoretical estimate >10× measured value -> over-conservative token assumptions, not accounting for early termination
  - High variance across GPU runs (±0.29 kWh for MindAct on H100-NVL) -> agent non-determinism or GPU thermal throttling

- **First 3 experiments:**
  1. **Baseline measurement:** Run carbontracker on your web agent with Mind2Web's cross-domain split (largest, 2350 tasks) on a single GPU. Record: total kWh, time, tokens processed, step success rate.
  2. **Ablation on preprocessing:** Disable or simplify any DOM filtering; measure energy increase. Quantify: tokens-before vs. tokens-after filtering.
  3. **Hardware sensitivity:** Run same agent on 2–3 different GPUs (e.g., consumer RTX vs. data center H100). Calculate: energy-per-token for each, confirm ranking stability across hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can theoretical estimation models be refined to overcome the high uncertainty (e.g., the observed 7x overestimation) inherent in current calculation methods?
- **Basis in paper:** [explicit] The authors note that even for the fully open-source MindAct agent, they "overestimate its energy consumption by a factor of 7," concluding that estimation is unreliable.
- **Why unresolved:** The paper identifies that estimation relies on conservative "worst-case" assumptions (like token counts and lengths) because internal optimization strategies (e.g., early termination) are opaque.
- **What evidence would resolve it:** A refined estimation methodology that consistently aligns with empirical benchmarks within a small margin of error (e.g., <20%).

### Open Question 2
- **Question:** How can standardized energy metrics be effectively integrated into existing web agent benchmarks to incentivize sustainable agent design?
- **Basis in paper:** [explicit] The paper advocates for "a change in thinking of how we evaluate web agents," noting that "incentives for developers are missing, as benchmarks do not penalize energy consumption—yet."
- **Why unresolved:** While the authors propose "energy per benchmark" as a metric, the community has not established a standardized protocol for comparing energy efficiency alongside task success rates.
- **What evidence would resolve it:** The adoption of a unified benchmark suite where energy consumption is a weighted component of the final evaluation score.

### Open Question 3
- **Question:** Does providing real-time energy consumption or CO2 emission feedback to end-users significantly alter their usage patterns or agent preferences?
- **Basis in paper:** [inferred] The authors argue that energy costs are currently "invisible" to users and suggest that "displaying estimated CO2 emissions per task could help users become aware."
- **Why unresolved:** The paper establishes the feasibility of measuring these costs but does not conduct user studies to validate the behavioral impact of making this data visible.
- **What evidence would resolve it:** User study results demonstrating a statistical shift in agent selection or usage frequency when energy metrics are displayed in the UI.

## Limitations

- Energy estimation for proprietary LLM-based agents relies on unverified assumptions about token counts and energy-per-token values, leading to potential large errors (7× overestimation observed)
- carbontracker measurements may miss CPU or memory energy contributions, particularly for preprocessing steps
- Five tested agents represent only a small fraction of the web agent landscape, limiting generalizability

## Confidence

**High Confidence:** The empirical benchmarking methodology using carbontracker is reproducible and the observed GPU architecture effects (H100-NVL most efficient, RTX A6000 least efficient) align with known hardware characteristics. The relative agent efficiency rankings (AutoWebGLM < MultiUI < MindAct < Synapse < Synatra) appear robust across different GPUs.

**Medium Confidence:** The core finding that preprocessing dramatically reduces energy consumption by filtering DOM elements before LLM processing is well-supported by the data, though the specific implementation details and their generalizability to other web agents require further validation.

**Low Confidence:** The theoretical energy estimation for LASER and other proprietary-LLM agents is highly speculative, relying on leaked specifications and conservative upper-bound assumptions that may not reflect real-world usage patterns.

## Next Checks

1. **Carbontracker Validation:** Implement energy measurement using an alternative method (e.g., nvidia-smi or RAPL) on the same hardware and agents to verify carbontracker readings and identify potential missing energy contributions.

2. **Preprocessing Ablation Study:** Systematically vary preprocessing intensity (DOM filtering, tokenization strategies) across multiple agents to quantify the relationship between preprocessing overhead and token reduction, determining the optimal balance point.

3. **Proprietary Model Energy Calibration:** Use any available proprietary model API access to measure actual token counts and energy consumption for a small set of representative tasks, then compare against theoretical estimates to refine the estimation methodology.