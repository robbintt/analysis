---
ver: rpa2
title: 'ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in
  Autonomous Driving'
arxiv_id: '2505.24317'
source_url: https://arxiv.org/abs/2505.24317
tags:
- reward
- driving
- traffic
- responsibility
- autonomous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ROAD, a responsibility-oriented reward design
  for reinforcement learning in autonomous driving. The method incorporates traffic
  regulations into the reward function using a Traffic Regulation Knowledge Graph
  and Vision-Language Models with Retrieval-Augmented Generation to automate responsibility
  assignment.
---

# ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving

## Quick Facts
- arXiv ID: 2505.24317
- Source URL: https://arxiv.org/abs/2505.24317
- Reference count: 32
- Primary result: Responsibility-weighted reward design improves traffic law compliance in RL driving agents

## Executive Summary
This paper introduces ROAD, a responsibility-oriented reward design framework for autonomous driving reinforcement learning. The method integrates traffic regulations into the reward function using a Traffic Regulation Knowledge Graph (TRKG) and Vision-Language Models with Retrieval-Augmented Generation (RAG) to automate responsibility assignment. Experiments in the MetaDrive simulator show that ROAD-trained models achieve higher success rates and lower primary responsibility in traffic incidents compared to baseline methods. The responsibility classifier trained offline using TRKG and VLM achieves 79% accuracy in liability assignment.

## Method Summary
ROAD combines a Traffic Regulation Knowledge Graph with a Vision-Language Model pipeline to assign accident responsibility weights for reward shaping. The method extracts driving scenes and legal standards from traffic laws into a Neo4j graph database. During training, collision images are processed through a four-stage pipeline (scene identification → TRKG retrieval → image info extraction → LLM inference) to classify accidents as primary, shared, or secondary liability. The responsibility classifier is trained offline on labeled images using a Vision Transformer architecture. The RL agent (PPO from Stable-Baselines3) receives responsibility-weighted crash penalties: rcrash = λ × Pbaseline × Accident_liability_ratio, where primary liability maps to ratio 1.0, shared to 0.5, and secondary to 0.

## Key Results
- ROAD-trained agents achieve 73.2% success rate at intersections vs 65.0% baseline
- Primary responsibility rate reduced to 43.5% vs 57.0% baseline at intersections
- Responsibility classifier accuracy reaches 79.21% using RAG-VLM approach
- Shared responsibility rate increases to 13.0% (intersection) and 12.5% (roundabout)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding VLM responsibility judgments in a structured knowledge graph reduces hallucination and improves classification accuracy.
- Mechanism: The Traffic Regulation Knowledge Graph (TRKG) stores legal rules as three node types (Driving Scene, Applicable Standard, Responsibility Assignment Information). When a collision image is processed, RAG retrieves relevant standards before the VLM assigns liability, constraining outputs to legally-grounded reasoning.
- Core assumption: The TRKG ontology sufficiently covers real-world accident scenarios; gaps in coverage would propagate as classification errors.
- Evidence anchors:
  - "we introduced a Traffic Regulation Knowledge Graph and leveraged Vision-Language Models alongside Retrieval-Augmented Generation techniques to automate reward assignment"
  - RAG-based method achieves 79.21% accuracy vs. GPT-4o baseline at 63.37% on intersection scenarios
  - Neighbor papers discuss RL reward design challenges but do not evaluate RAG-based hallucination mitigation

### Mechanism 2
- Claim: Scaling crash penalties by assigned responsibility ratio shifts learned policy toward lower-liability behaviors.
- Mechanism: The reward function replaces a fixed crash penalty (Pbaseline = -20) with a dynamic penalty: rcrash = λ × Pbaseline × Accident_liability_ratio. Primary liability maps to ratio 1.0, shared to 0.5, secondary to 0. The agent thus receives stronger gradient signals to avoid primary-liability collisions.
- Core assumption: The offline classifier's liability predictions during training correlate sufficiently with true legal responsibility; systematic misclassification would misalign penalties.
- Evidence anchors:
  - Formal definition of responsibility-weighted crash penalty
  - ROAD-trained agents show reduced primary responsibility (43.5% vs. 57.0% baseline at intersections, 50.8% vs. 56.5% at roundabouts) and higher success rates (73.2% vs. 65.0%)
  - "Balancing Progress and Safety" paper similarly addresses risk-aware RL objectives

### Mechanism 3
- Claim: Decoupling visual extraction from legal reasoning via chain-of-thought prompting improves assignment reliability.
- Mechanism: The four-stage pipeline (scene identification → TRKG retrieval → image info extraction → LLM inference) separates perception (VLM) from reasoning (LLM with legal context). This modularization reduces compounding errors from hallucinated scene elements influencing legal judgment.
- Core assumption: Each stage's errors are largely independent; systematic correlated failures across stages would still produce unreliable outputs.
- Evidence anchors:
  - Pipeline architecture and Table 2 prompting structure
  - "Decoupling visual information extraction from responsibility inference reduces potential inaccuracies from hallucinated scene elements"
  - No direct neighbor comparison for chain-of-thought in driving contexts

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG anchors VLM outputs to structured legal knowledge, reducing hallucination in responsibility assignment. Without understanding RAG, the TRKG-VLM integration will be opaque.
  - Quick check question: Given a collision at an uncontrolled intersection, can you explain how RAG would retrieve the right-of-way rule before the VLM assigns liability?

- Concept: Reward Shaping in Reinforcement Learning
  - Why needed here: ROAD's core contribution is a modified reward function. Understanding how penalty magnitude affects policy gradient signals is essential to interpret results.
  - Quick check question: If crash penalty were fixed rather than responsibility-weighted, what behavior would the agent likely learn when collisions are unavoidable?

- Concept: Vision Transformers (ViT) for Classification
  - Why needed here: The offline responsibility classifier uses ViT architecture. Training dynamics (overfitting to dominant class, validation instability) directly impact reward signal quality.
  - Quick check question: Why might validation loss increase while training accuracy improves, as observed in the intersection scenario?

## Architecture Onboarding

- Component map:
  - TRKG (Neo4j) → stores 93 driving scenes, 92 standards, 93 responsibility info nodes
  - VLM + RAG Pipeline → generates labeled accident images (primary/shared/secondary)
  - ViT Classifier → trained offline on labeled images; outputs Accident_liability_ratio
  - PPO Agent (Stable-Baselines3) → learns policy using responsibility-weighted reward
  - MetaDrive Simulator → provides intersection and roundabout environments

- Critical path:
  1. TRKG construction quality determines retrieval relevance
  2. Classifier accuracy determines reward signal fidelity
  3. Reward scaling (λ) must balance crash penalty against other rewards (distance, centering, speed)
  4. PPO hyperparameters (learning rate, n_steps) must suit environment complexity

- Design tradeoffs:
  - Offline classifier vs. real-time VLM API: Offline reduces cost and latency but requires retraining for new jurisdictions; real-time adapts but is expensive and slower
  - Three-class liability (primary/shared/secondary) vs. continuous scale: Discrete classes simplify classification but lose granularity; continuous would require regression and more training data
  - Validation set size vs. reliability: Small validation sets (9 shared-liability samples for intersections) cause unstable metrics; larger sets improve monitoring but require more labeling effort

- Failure signatures:
  - Validation loss increasing while training loss decreases → likely overfitting to dominant class; address with class balancing or more data
  - Success rate plateaus below 60% → check if crash penalty scaling (λ) is too weak relative to distance/speed rewards
  - High shared-liability rate → classifier may lack discriminative features; consider augmentation or architecture changes
  - Agent hesitates excessively at intersections → crash penalty may be over-weighted; reduce λ or increase arrival reward

- First 3 experiments:
  1. Replicate TRKG construction using a different jurisdiction's traffic code (e.g., EU regulations) to validate ontology transferability. Measure node coverage and retrieval relevance.
  2. Train the ViT classifier with balanced class sampling and doubled validation set size. Compare validation stability and final accuracy against baseline.
  3. Run ablation on λ values (0.5, 1.0, 2.0) in roundabout scenario. Measure success rate, primary responsibility rate, and episode length to identify optimal penalty scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ROAD framework maintain reliable performance when generalized to heterogeneous legal systems and region-specific traffic regulations?
- Basis in paper: The conclusion states: "Future research may focus on incorporating more granular and region-specific traffic laws to improve the generalizability of the model in heterogeneous legal systems."
- Why unresolved: The current TRKG was constructed exclusively from the "Road Traffic Safety Law of the People's Republic of China," limiting the agent's operational design domain to a single legal jurisdiction.
- What evidence would resolve it: Successful training and evaluation of agents using a TRKG constructed from the traffic laws of multiple distinct countries or regions.

### Open Question 2
- Question: How does the performance of the responsibility classifier degrade when transferring from simulated collision images to real-world visual data?
- Basis in paper: The entire experimental setup (data collection, training, and testing) is confined to the MetaDrive simulator, creating a potential domain gap.
- Why unresolved: The Vision Transformer (ViT) classifier is trained on synthetic images, which may lack the visual complexity, lighting variation, and noise present in real-world accident footage.
- What evidence would resolve it: Benchmarking the trained responsibility classifier's accuracy on real-world dashcam accident datasets or demonstrating the policy in a high-fidelity physical simulation.

### Open Question 3
- Question: Does the observed instability in validation loss and the ~79% accuracy ceiling for the responsibility classifier limit the system's safety guarantees?
- Basis in paper: The paper notes that "validation loss exhibited substantial fluctuations and even an increasing trend in later epochs" and the classifier achieved 79.21% accuracy.
- Why unresolved: A 21% error rate in liability assignment (and unstable training dynamics) implies the reward signal may be frequently incorrect, potentially rewarding hazardous behaviors or penalizing safe ones during RL training.
- What evidence would resolve it: A sensitivity analysis correlating classifier error types with resulting agent failures, or improvements in classifier stability/accuracy that demonstrate a direct link to lower accident rates.

## Limitations

- The TRKG coverage may be incomplete for novel scenarios, causing RAG retrieval to return irrelevant standards
- The responsibility classifier trained on synthetic images may not generalize well to real-world visual data
- The discrete three-class liability system may oversimplify complex real-world responsibility scenarios

## Confidence

- **High Confidence:** Responsibility-weighted reward shaping mechanism is well-supported by experimental results showing reduced primary responsibility and improved success rates
- **Medium Confidence:** RAG-VLM integration is theoretically sound but lacks comparative validation against other hallucination-mitigation approaches
- **Low Confidence:** Chain-of-thought pipeline reliability lacks direct comparative evidence in driving contexts

## Next Checks

1. Replicate TRKG construction using a different jurisdiction's traffic code (e.g., EU regulations) to validate ontology transferability. Measure node coverage and retrieval relevance.
2. Train the ViT classifier with balanced class sampling and doubled validation set size. Compare validation stability and final accuracy against baseline.
3. Run ablation on λ values (0.5, 1.0, 2.0) in roundabout scenario. Measure success rate, primary responsibility rate, and episode length to identify optimal penalty scaling.