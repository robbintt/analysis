---
ver: rpa2
title: Evaluating Role-Consistency in LLMs for Counselor Training
arxiv_id: '2601.08892'
source_url: https://arxiv.org/abs/2601.08892
tags:
- evaluation
- llms
- raters
- conversation
- counselor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VirCo, a virtual client designed to train
  online counselors by simulating realistic client interactions. The study evaluates
  the role-consistency of Large Language Models (LLMs) in maintaining assigned personas
  during counseling sessions, particularly under adversarial conditions.
---

# Evaluating Role-Consistency in LLMs for Counselor Training

## Quick Facts
- arXiv ID: 2601.08892
- Source URL: https://arxiv.org/abs/2601.08892
- Reference count: 38
- Primary result: Vicuna-13B-1.5 demonstrates high role-consistency in counseling scenarios but automated GPT-4 evaluation diverges from human judgment under adversarial conditions

## Executive Summary
This paper introduces VirCo, a virtual client system designed to train online counselors by simulating realistic client interactions using Large Language Models. The study evaluates how well LLMs maintain assigned personas during counseling sessions, particularly under adversarial conditions including out-of-context questions and toxic responses. An adversarial dataset was created to test model robustness, and various open-source LLMs were compared, including quantized versions. The findings show promising performance for counselor training applications while highlighting the need for robust evaluation methods and further development to handle challenging scenarios effectively.

## Method Summary
The study evaluates role-consistency of LLMs by testing their ability to maintain assigned client personas during simulated counseling dialogues. Seven expert-crafted German persona descriptions were created from real counseling cases, and a test set of 164 cases was developed covering persona consistency, conversation coherence, and six types of adversarial attacks. Models including Vicuna-13B-1.5/16k, Llama3-German-8B, and SauerkrautLM variants were evaluated using G-Eval-inspired prompts and human raters. Both automated GPT-4 evaluation and three human raters assessed responses on a three-class scale for coherence and persona consistency, with inter-rater reliability measured via Krippendorff's alpha.

## Key Results
- Vicuna-13B-1.5 maintains high role-consistency and coherence in standard counseling scenarios
- Automated GPT-4 evaluation diverges significantly from human judgment on adversarial examples
- 8-bit and 4-bit quantized models show no significant performance loss compared to unquantized versions
- SauerkrautLM-1.5B performed worst, while 8B models maintained adequate role-consistency
- Low inter-rater reliability (0.28-0.46) indicates ambiguity in evaluation criteria

## Why This Works (Mechanism)

### Mechanism 1: Persona-Embedded Prompt Engineering for Role Maintenance
- Claim: LLMs can maintain assigned client personas across multi-turn counseling dialogues when provided with detailed, expert-crafted character descriptions in the prompt context.
- Mechanism: Domain experts create persona descriptions from real counseling cases (email sessions, forum posts); these are embedded as system prompts, providing the LLM with character attributes, background context, and emotional state to guide response generation. The LLM's pre-training on diverse conversational data enables extrapolation of character-consistent behavior even when specific details aren't explicitly stated.
- Core assumption: The model's learned representations of human behavior and dialogue patterns can be constrained by natural language persona specifications without fine-tuning.
- Evidence anchors:
  - [abstract]: "evaluates the role-consistency of Large Language Models (LLMs) in maintaining assigned personas during counseling sessions"
  - [section]: "These persona descriptions were crafted by domain experts, using real-world sources such as documented email counseling sessions and public forum posts" (Page 6); "human raters... continued to classify approximately 85% of responses as coherent and consistent" (Page 20)
  - [corpus]: "Consistent Client Simulation for Motivational Interviewing-based Counseling" (arXiv:2502.02802) addresses similar persona-maintenance challenges in counseling contexts
- Break condition: Adversarial inputs (out-of-context questions, toxicity, repeated nonsense patterns) cause degraded role-consistency; multi-turn adversarial scenarios may compound this effect beyond the single-turn evaluation reported.

### Mechanism 2: Dual-Track Evaluation with Divergent Failure Modes
- Claim: Automated GPT-4 evaluation approximates human judgment for standard conversational scenarios but systematically diverges when counselor inputs contain adversarial patterns.
- Mechanism: Evaluation uses G-Eval-inspired prompts assessing coherence, content accuracy, and persona consistency. However, GPT-4 conflates counselor behavior quality with client response quality—when the counselor provides unethical advice or toxic input, GPT-4 rates the *interaction* negatively rather than evaluating whether the client's response was appropriate to their character.
- Core assumption: Human raters can separate "did the counselor behave appropriately?" from "did the client respond in character?" while the automated evaluator cannot make this distinction consistently.
- Evidence anchors:
  - [abstract]: "Automated evaluations using GPT-4 diverged from human assessments in these challenging situations"
  - [section]: "In most cases, the underlying reason for the label 2 rating of GPT-4 can be attributed to the counselor's behavior... GPT-4 is not suitable for assessing VirCo's performance in exceptional situations" (Page 12-13); IRR Krippendorff's alpha = 0.46 (task 1), 0.28 (task 2) indicating substantial rater disagreement
  - [corpus]: Ψ-Arena (arXiv:2505.03293) similarly identifies limitations in static LLM evaluation for counseling contexts
- Break condition: When counselor messages contain unethical advice, toxicity, or adversarial patterns, GPT-4's evaluation loses validity for assessing client response quality.

### Mechanism 3: Quantization Efficiency with Retained Role Fidelity
- Claim: 8-bit and 4-bit quantization of 7B+ parameter models preserves role-consistency performance sufficiently for counselor training applications.
- Mechanism: Quantization reduces weight precision (16-bit → 8-bit/4-bit), decreasing memory footprint and inference cost. The task of persona-consistent generation appears to rely on learned semantic patterns that remain intact at lower precision, as opposed to tasks requiring fine-grained numerical reasoning.
- Core assumption: Role-consistency depends more on high-level semantic coherence than on precise weight activations, making it robust to quantization-induced noise.
- Evidence anchors:
  - [abstract]: "8-bit and 4-bit quantized models maintained performance comparable to their unquantized counterparts"
  - [section]: "the llama3-german-8b and sauerkrautm8b models with 8-bit quantization again demonstrate no significant performance loss compared to their unquantized versions" (Page 18); Figure 7 shows overlapping violin plot distributions for quantized vs. unquantized models
  - [corpus]: Limited direct corpus evidence on quantization effects specifically for role-play scenarios; this is a relatively underexplored area
- Break condition: Very small models (SauerkrautLM-1.5B showed worst performance) fail to maintain adequate role-consistency; complex emotional nuances may degrade at 4-bit precision in edge cases not captured by the current dataset.

## Foundational Learning

- Concept: **Role-Consistency vs. Dialogue Coherence**
  - Why needed here: These are evaluated as separate dimensions—a response can be coherent with conversation history but inconsistent with the persona, or vice versa. The paper evaluates both independently.
  - Quick check question: A client persona "Lina" is in withdrawal and scared of separation. If asked "What's your favorite movie?", should she (a) refuse to answer, (b) answer enthusiastically, or (c) answer briefly while returning to her concerns? Which evaluation dimension does each choice affect?

- Concept: **Adversarial Robustness Categories**
  - Why needed here: The paper introduces 6 types of "prompt attacks" (OOC questions, unethical advice, toxicity, long input, OOD styles, adversarial noise). Understanding these helps design more robust virtual clients.
  - Quick check question: If a counselor-in-training types "and true is true and true is true..." (repeated pattern), what's the appropriate client response? How does this differ from handling an out-of-context question like "What's the longest river?"

- Concept: **Inter-Rater Reliability Limitations**
  - Why needed here: Low IRR values (0.28-0.46) indicate that even human experts disagree on what constitutes "moderately coherent" or "persona-inconsistent" responses. This sets expectations for evaluation system design.
  - Quick check question: Three raters evaluate the same response: one says "fits well," one says "fits moderately," one says "fits poorly." What does this tell you about the response, the evaluation criteria, or both?

## Architecture Onboarding

- **Component map:**
  Persona Dataset -> Prompt Constructor -> LLM Backend -> Response
  ↓ ↓ ↓
  Adversarial Test Set Learning Platform UI
  ↓ ↓
  GPT-4 Evaluator <- Feedback Aggregation
  ↓
  Human Rater Interface

  - Persona Dataset: 7 expert-crafted personas covering addiction, educational counseling scenarios
  - Adversarial Test Set: 164 cases (65 persona consistency, 45 coherence, 54 prompt attacks)
  - LLM Backend: Vicuna-13B-16k (primary), LLama3-German-8B, SauerkrautLM variants with quantization options
  - Learning Platform: Course enrollment, chat interface, note-taking, trainer/peer/AI feedback mechanisms

- **Critical path:**
  1. Domain expert creates persona description from real case materials
  2. Prompt constructor embeds persona + conversation history
  3. LLM generates client response
  4. Response evaluated via GPT-4 (screening) + human raters (calibration)
  5. Feedback integrated into learning platform for trainee review

- **Design tradeoffs:**
  - Vicuna-13B vs. quantized 8B models: ~2% performance gain vs. ~50% memory reduction
  - GPT-4 evaluation vs. human-only: Scale vs. accuracy in adversarial scenarios
  - Detailed personas vs. concise prompts: Consistency vs. response naturalness/emotional expressiveness

- **Failure signatures:**
  - GPT-4 labeling responses "not coherent" when counselor input was problematic (not a client-side failure)
  - Low emotional expressiveness in client responses (identified as deficiency across evaluators)
  - 1.5B parameter model receiving highest "bad" ratings—insufficient capacity for nuanced role-play
  - Raters disagreeing most on "moderately coherent" (label 1)—suggests ambiguous edge cases

- **First 3 experiments:**
  1. **Baseline divergence analysis:** Run Vicuna-13B on the full adversarial dataset, compute GPT-4 vs. human agreement rates separately for each test category (persona consistency, coherence, prompt attacks) to quantify where automated evaluation becomes unreliable.
  2. **Quantization validation:** Deploy LLama3-German-8B at 16-bit, 8-bit, and 4-bit precision; use the ranking/bucketing methodology with 3 human raters to confirm no statistically significant degradation (target: <5% mean rank change).
  3. **Adversarial recovery testing:** Create a held-out adversarial set focused on toxicity/unethical advice; measure whether the virtual client (a) maintains persona, (b) responds realistically to provocation, and (c) whether these correlate with human expectations for "authentic client behavior under stress."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does role-consistency degrade in multi-turn adversarial scenarios compared to single-turn adversarial attacks?
- Basis in paper: [explicit] "This ability of the model to maintain the role despite adversarial content might decrease more in a multi-turn evaluation with adversarial attacks."
- Why unresolved: The current study evaluated adversarial attacks in single-turn settings only; sustained adversarial pressure across multiple conversation turns was not tested.
- What evidence would resolve it: Longitudinal evaluation measuring role-consistency scores across 5+ consecutive adversarial turns compared to isolated adversarial inputs.

### Open Question 2
- Question: Can LoRA or DPO fine-tuning improve role-consistency under adversarial conditions?
- Basis in paper: [explicit] Future work section proposes "fine-tuning Large Language Models (LLMs) using advanced methods such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization (DPO)."
- Why unresolved: The study evaluated only pre-trained models without task-specific fine-tuning for counseling role-play scenarios.
- What evidence would resolve it: Comparative evaluation of base versus fine-tuned models on the adversarial dataset, with statistical analysis of role-consistency improvements.

### Open Question 3
- Question: How can automated evaluation methods be calibrated to match human judgment on adversarial examples?
- Basis in paper: [explicit] "GPT-4's evaluations diverged notably from those of human raters" with "discrepancies of at least 20%" on adversarial content.
- Why unresolved: The paper demonstrates GPT-4 focuses on counselor behavior rather than client response quality in adversarial settings, but offers no corrective approach.
- What evidence would resolve it: New evaluation prompts or multi-stage evaluation pipelines achieving Krippendorff's alpha >0.6 with human raters on adversarial examples.

### Open Question 4
- Question: What methods effectively enhance emotional expressiveness in virtual client responses?
- Basis in paper: [explicit] Future work states "Enhancing the emotional expressiveness of LLMs is essential" after identifying "low emotionality on the part of the client" as a deficiency.
- Why unresolved: The paper documents flat emotional responses (e.g., "Okay, thanks for the help" to dismissive counselor input) but tests no remediation strategies.
- What evidence would resolve it: Integration of emotion-annotated training data or sentiment-aware prompting, validated through human ratings of emotional authenticity.

## Limitations

- GPT-4 evaluation systematically diverges from human judgment on adversarial examples, conflating counselor behavior quality with client response quality
- Low inter-rater reliability (0.28-0.46) indicates significant ambiguity in evaluation criteria for coherence and persona consistency
- Emotional expressiveness in client responses was identified as deficient but not systematically measured or addressed
- Small adversarial dataset (164 cases) may not capture full spectrum of challenging counseling scenarios

## Confidence

- **High Confidence**: Role-consistency maintenance in standard counseling scenarios, quantization efficiency findings (8-bit vs. 4-bit performance retention)
- **Medium Confidence**: GPT-4 evaluation limitations in adversarial contexts, general applicability to broader counseling domains
- **Low Confidence**: Specific performance thresholds under extreme adversarial conditions, long-term persona consistency across extended multi-turn dialogues

## Next Checks

1. **Adversarial Robustness Validation**: Test the Vicuna-13B model on a larger, independently generated adversarial dataset focused on toxic and unethical counselor inputs to quantify the exact failure rate and characterize response patterns.

2. **Quantization Edge Case Analysis**: Deploy the SauerkrautLM-8B model at 4-bit precision across the full evaluation suite to identify specific response characteristics (emotional nuance, contextual awareness) that degrade at lower precision.

3. **Human Evaluation Protocol Refinement**: Conduct a calibration study with 5-7 raters using the ambiguous "moderately coherent" category to develop clearer guidelines and measure whether inter-rater reliability improves with structured training.