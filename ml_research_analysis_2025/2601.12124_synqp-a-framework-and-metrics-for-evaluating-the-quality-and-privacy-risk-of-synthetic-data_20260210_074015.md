---
ver: rpa2
title: 'SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk
  of Synthetic Data'
arxiv_id: '2601.12124'
source_url: https://arxiv.org/abs/2601.12124
tags:
- data
- synthetic
- privacy
- risk
- synqp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SynQP is an open framework for benchmarking privacy risks in synthetic
  data generation (SDG) using simulated sensitive data. The core innovation is generating
  pseudo-identifiable datasets from non-identifiable real data, enabling fair privacy
  evaluations that account for SDG's probabilistic nature.
---

# SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data

## Quick Facts
- arXiv ID: 2601.12124
- Source URL: https://arxiv.org/abs/2601.12124
- Authors: Bing Hu; Yixin Li; Asma Bahamyirou; Helen Chen
- Reference count: 33
- Primary result: Open framework for benchmarking privacy risks in synthetic data generation using simulated identifiable data and new SD-IDR metric that captures near-miss re-identification scenarios

## Executive Summary
SynQP introduces an open benchmarking framework for evaluating privacy risks in synthetic data generation (SDG) using simulated pseudo-identifiable datasets. The core innovation is generating quasi-identifiers from census distributions and linking them to non-identifiable real data, enabling reproducible privacy evaluations without exposing sensitive information. The framework introduces SD-IDR, a variational identity disclosure risk metric that addresses limitations of exact-match approaches by considering small numerical variations in synthetic records. When applied to CTGAN, TVAE, and GaussianCopula models with and without differential privacy, the framework demonstrates consistent privacy-utility trade-offs and provides standardized tools for improving privacy protections in synthetic health data applications.

## Method Summary
The SynQP framework generates simulated quasi-identifiable populations using census distributions for demographic attributes, then links these to non-identifiable health records (diabetes, BMI) through nearest-neighbor matching. The linked data is split into training (7,000) and holdout (3,000) sets, then used to train CTGAN, TVAE, and GaussianCopula models from the Synthetic Data Vault library. Local differential privacy is applied via Laplacian noise injection before training. Quality is evaluated using Hellinger distance for fidelity and maximum likelihood estimation for utility, while privacy is assessed using the new SD-IDR metric (with numerical tolerance thresholds) and SD-MIA for membership inference risk detection.

## Key Results
- Non-private models achieved near-perfect utility (≥0.97) while privacy risk remained below the 0.09 regulatory threshold when DP was applied
- SD-IDR showed that exact-match IDR metrics systematically underestimate privacy risk by missing near-identical synthetic records
- Differential privacy consistently reduced both SD-IDR and membership-inference attack risk across all three tested models
- CTGAN and GaussianCopula showed negative SD-MIA values indicating underfitting, while TVAE showed positive values indicating overfitting

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Identifiable Data Simulation
Generating simulated quasi-identifiers from census distributions and linking them to non-identifiable real data creates open benchmark datasets for privacy evaluation without exposing sensitive information. The framework uses inverse transform sampling for demographic attributes (age, gender) from census distributions, then links non-identifiable health records through nearest-neighbor matching on shared attributes. This creates synthetic "identifiable" populations that can be freely shared and used to benchmark SDG privacy risks. Core assumption: simulated quasi-identifier distributions are sufficiently representative of real-world patterns to produce meaningful privacy risk estimates.

### Mechanism 2: Variational Identity Disclosure Risk (SD-IDR)
Traditional IDR metrics relying on exact matches systematically underestimate privacy risk in synthetic data because probabilistic SDG models produce small numerical variations that mask near-identical records. SD-IDR modifies the binary match indicator to return 1 when categorical columns match exactly AND cumulative numerical differences fall below a threshold ε. This captures "near-miss" re-identification scenarios that exact-match IDR misses. Core assumption: small numerical variations are artifacts of probabilistic generation rather than meaningful privacy protection—attackers can plausibly match records within tolerance windows.

### Mechanism 3: Differential Privacy Trade-off Quantification
Local differential privacy noise injection creates a quantifiable trade-off between utility/fidelity and privacy risk, enabling comparison against regulatory thresholds (0.09 in the paper's health context). Laplacian noise is added to input data before SDG training, and the framework measures degradation across Hellinger distance, MLE scores, and SD-IDR/SD-MIA at different privacy budgets. Core assumption: the local input-level DP approach produces privacy-utility trade-offs comparable to or representative of other DP mechanisms.

## Foundational Learning

### Concept: Quasi-Identifiers and Re-identification
Why needed: Understanding that combinations of non-unique attributes (age + gender + postal code + occupation) can uniquely identify individuals is essential for interpreting why SD-IDR measures privacy risk even without explicit identifiers in the data. Quick check: A dataset contains only age (25-30), gender (M/F), and 3-digit postal code prefix. Could an attacker potentially re-identify a specific individual? (Answer: Yes—if the combination of values is unique within the population, that individual can be distinguished from all others)

### Concept: Membership Inference Attacks
Why needed: SD-MIA measures whether synthetic data leaks information about which specific records were in the training set. Understanding MIA is required to interpret negative SD-MIA values (indicating underfitting) versus positive values (indicating overfitting and potential privacy leakage). Quick check: If synthetic data contains a record almost identical to a real individual's data, what does this suggest about whether that individual was in the training set? (Answer: It suggests the model overfit to that record during training, providing evidence of membership)

### Concept: Differential Privacy Noise Mechanisms
Why needed: The paper applies Laplacian noise at the input level. Understanding how noise scale relates to the privacy budget (ε) is needed to interpret why ε=0.8 is considered "large" and how to reason about the utility-privacy trade-off curve. Quick check: If the privacy budget ε decreases from 0.8 to 0.1, would you expect more or less noise added per data value? What happens to data utility? (Answer: More noise is added because smaller ε means stronger privacy guarantees; utility typically decreases as noise increases)

## Architecture Onboarding

### Component Map:
SynQP Framework -> Population Simulator -> Census Distribution Sampler -> Random Attribute Generator -> Address Generator
                           -> Data Linker -> K-NN Matcher
                           -> SDG Model Trainer -> Base Models: CTGAN / TVAE / GaussianCopula -> DP Noise Injector
                           -> Quality Evaluators -> Hellinger Distance Calculator -> ML Efficiency Tester
                           -> Privacy Evaluators -> SD-IDR Calculator -> SD-MIA Calculator

### Critical Path:
1. Generate pseudo-identifiable population from census distributions (10,000 rows demonstrated)
2. Link non-identifiable health data via conditional sampling and k-NN matching
3. Split into training (7,000) and holdout (3,000) sets
4. Train SDG model (with/without DP at ε∈{0, 0.8}) on training split
5. Generate synthetic records (10,000 demonstrated)
6. Compute metrics: Hellinger distance (fidelity), MLE (utility), SD-IDR and SD-MIA (privacy at multiple variational budgets)

### Design Tradeoffs:
- Simulation realism vs. open sharability: Simulated quasi-identifiers enable reproducible benchmarks but may not capture complex real-world attribute correlations
- Variational budget selection for SD-IDR: Higher tolerance (ε=2,3) catches more near-misses but risks false positives; paper recommends evaluating multiple thresholds
- Local DP vs. gradient-level DP: Input-level noise is simpler to implement but typically requires larger ε for equivalent protection

### Failure Signatures:
- Negative SD-MIA values: Model is underfit—synthetic data is less similar to training data than random samples (observed in CTGAN/GaussianCopula at variational budgets ≥1)
- Zero IDR at ε=0 with elevated SD-IDR at ε>0: Confirms exact-match metrics miss near-identifications; validates need for variational thresholds
- High HD (>0.3) combined with positive SD-MIA: Model has memorized outlier records while failing to capture overall distribution (pattern seen in TVAE with average HD=0.34 and positive SD-MIA)

### First 3 Experiments:
1. Baseline validation on provided diabetes dataset: Run SynQP with default CTGAN (no DP). Confirm SD-IDR at variational budget 0 matches traditional IDR output, then verify SD-IDR increases at budgets 1, 2, 3 consistent with Table 2 patterns (0.049 → 0.247)
2. DP sensitivity sweep: Train GaussianCopula at ε∈{0, 0.2, 0.4, 0.6, 0.8} and plot the utility-privacy frontier (MLE vs. SD-IDR at budget 2). Compare slope to CTGAN to determine which model achieves better privacy-utility efficiency
3. Cross-dataset generalization test: Apply the full SynQP pipeline to a different non-identifiable health dataset (e.g., UCI heart disease or breast cancer datasets). Validate that SD-MIA sign patterns remain interpretable (negative=underfit, positive=overfit) and that DP reduces both SD-IDR and SD-MIA

## Open Questions the Paper Calls Out

### Open Question 1
How does incorporating realistic correlations between quasi-identifiers (e.g., age/gender correlations with occupation, marital status, address, and ethnicity) affect the accuracy of privacy risk estimates in SynQP? The current framework uses independent random sampling for most quasi-identifiers, potentially creating simulated populations that deviate from real-world population structure. What evidence would resolve it: Comparative experiments showing SD-IDR and SD-MIA results when using correlated vs. independent quasi-identifier sampling, validated against real population statistics.

### Open Question 2
What is the optimal variational budget (IDRΔ) threshold for SD-IDR that balances accurate privacy risk detection against false positive matches in numerical columns? The paper demonstrates that different variational budgets yield different risk scores but provides no guidance on which budget(s) should be used for regulatory compliance. What evidence would resolve it: Empirical analysis across diverse datasets establishing threshold ranges where SD-IDR reliably detects re-identification risk without flagging acceptable numerical noise as privacy violations.

### Open Question 3
Can the negative SD-MIA values observed for CTGAN and GaussianCopula reliably distinguish model underfitting from other failure modes, and at what point should training continue? While the authors propose SD-MIA as an overfitting/underfitting diagnostic, the interpretation of negative values as underfitting remains speculative. What evidence would resolve it: Experiments tracking SD-MIA across training epochs with ground-truth convergence metrics to establish whether negative SD-MIA consistently indicates recoverable underfitting.

## Limitations

- Framework's reliance on simulated quasi-identifiers raises questions about external validity when real-world attribute correlations differ from census-based simulations
- Variational IDR threshold selection lacks clear domain-specific justification, creating potential for either over-counting coincidental similarities or under-counting meaningful risks
- Local input-level DP rather than more standard DP-SGD approaches means reported utility-privacy trade-offs may not generalize to deployed systems

## Confidence

- **High Confidence**: The mathematical formulation of SD-IDR (section 2.4.1) and its implementation as an extension of traditional IDR metrics is well-defined and reproducible
- **Medium Confidence**: The claim that DP consistently reduces both SD-IDR and SD-MIA across models is supported by results but requires validation on additional datasets to establish generality
- **Low Confidence**: The assumption that census-based simulated quasi-identifiers adequately represent real-world re-identification risks without capturing complex attribute correlations

## Next Checks

1. Apply SynQP to a completely different non-health dataset (e.g., UCI census or credit datasets) to test generalizability of the utility-privacy trade-off patterns
2. Conduct a sensitivity analysis on the variational IDR threshold (ε) using domain expert consultation to establish appropriate bounds for different application contexts
3. Compare local DP results against DP-SGD implementations to quantify how much the choice of DP mechanism affects the measured trade-offs