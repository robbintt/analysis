---
ver: rpa2
title: 'Multi-task Code LLMs: Data Mix or Model Merge?'
arxiv_id: '2601.21115'
source_url: https://arxiv.org/abs/2601.21115
tags:
- code
- merging
- arxiv
- data
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two approaches for creating multi-task code
  language models: data mixing (fine-tuning once on combined datasets) and model merging
  (fine-tuning separate specialists then merging weights). Experiments across Qwen
  Coder and DeepSeek Coder families at 2B and 7B scales reveal a scale-dependent pattern:
  smaller models benefit from data mixing, while larger models perform better with
  merging.'
---

# Multi-task Code LLMs: Data Mix or Model Merge?

## Quick Facts
- arXiv ID: 2601.21115
- Source URL: https://arxiv.org/abs/2601.21115
- Reference count: 40
- 7B merged model achieved 92.7% pass@1 on HumanEval, outperforming task-specific fine-tuned equivalent at 90.9%

## Executive Summary
This paper investigates whether multi-task code language models should be trained via data mixing (fine-tuning once on combined datasets) or model merging (fine-tuning separate specialists then merging weights). Experiments across Qwen Coder and DeepSeek Coder families at 2B and 7B scales reveal a scale-dependent pattern: smaller models benefit from data mixing, while larger models perform better with merging. The merged 7B model achieved 92.7% pass@1 on HumanEval versus 90.9% for its task-specific fine-tuned equivalent, with only 2.1% average performance degradation. Weight analysis shows smaller models have high correlation between task-specific updates, causing interference during merging, while larger models show lower correlation enabling successful combination.

## Method Summary
The authors fine-tune base models (Qwen2.5-Coder and DeepSeekCoder at 1.3B/1.5B/7B scales) separately on code generation (KodCode) and summarization (CodeXGLUE) tasks, then compare three approaches: data mixture (joint training), and model merging using Linear, TIES, DARE, and DELLA methods. All models use identical hyperparameters for fair comparison. They evaluate code generation on HumanEval/MBPP and summarization on CodeXGLUE metrics. Weight analysis includes layer-wise L2 norms and Pearson correlation between task-specific weight updates to understand merging viability.

## Key Results
- Scale-dependent strategy: 2B models show ~8-9% average degradation with merging (data mixing superior), while 7B models show only ~2% degradation with merging (merging superior)
- Merging weight sensitivity: Optimal weight ratios vary significantly—DELLA at 0.5/0.5 for Qwen-7B, DARE at 0.6/0.4 for DeepSeek-7B
- Correlation patterns: Smaller models exhibit high Pearson correlation (r>0.7) between task updates, causing interference; larger models show lower correlation enabling successful merging
- L2 norm differences: Data mixture shows higher weight shifts (0.7-1.3) than merging (0.3-0.8), indicating more optimization pressure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model merging effectiveness correlates with inter-task weight update orthogonality, which emerges at larger model scales.
- Mechanism: Larger models (7B) develop task-specific weight updates with low Pearson correlation across layers, enabling parameters to be combined without destructive interference. Smaller models (~2B) show high correlation, meaning both tasks modify similar weight subsets, causing conflicts during merging.
- Core assumption: Lower correlation between task-specific weight deltas implies more separable task representations that can coexist in merged weights.
- Evidence anchors:
  - [section] "smaller models (Qwen 1.5B, DeepSeek 1.3B) exhibit consistently high correlation across layers, while larger models (7B variants) show substantially lower correlation" (Figure 3 analysis, Section 4.2)
  - [section] "high correlation models benefit from data mixing to avoid parameter conflicts, while low correlation models excel at model merging due to orthogonal task representations"
  - [corpus] Neighbor paper "LEWIS" discusses layer-wise sparsity for guided merging, suggesting layer-specific analysis is relevant; limited direct corpus evidence on correlation-as-predictor.
- Break condition: If fine-tuning produces highly correlated weight updates even at large scale (e.g., very similar tasks), merging advantage may not emerge.

### Mechanism 2
- Claim: Data mixture SFT induces larger and more heterogeneous parameter shifts than merging because it must jointly satisfy conflicting optimization gradients.
- Mechanism: Mixed-task training creates competing gradient signals from different tasks, forcing the model to find compromise representations. This results in higher L2 distance from base weights (~0.7–1.3) compared to merged models (~0.3–0.8). Merging preserves specialist adaptations by interpolating post-hoc.
- Core assumption: Larger weight shifts indicate more optimization pressure and potential interference; smaller, targeted shifts preserve task-specific knowledge better.
- Evidence anchors:
  - [section] "Data Mixture approach with green triangular line consistently exhibits the highest L2 norms throughout the network layers, ranging from approximately 0.7–1.3" (Section 4.1, Figure 2)
  - [section] "DARE and DELLA yield the most conservative changes... By dropping many minor weight updates and rescaling the remaining ones, these methods keep the majority of layers very close to the original weights"
  - [corpus] "Merge to Mix" paper explores reverse direction (merging to inform mixing); confirms interaction between the two paradigms but doesn't directly validate L2 hypothesis.
- Break condition: If tasks are highly complementary (positive transfer), data mixture may outperform merging even at large scale by enabling cross-task learning.

### Mechanism 3
- Claim: Task asymmetry in parameter sensitivity affects merging—code summarization requires minimal weight changes, while code generation requires more substantial adaptation.
- Mechanism: Code summarization SFT shows lowest L2 norms (minimal deviation from base), suggesting it leverages existing representations. Code generation modifies more layers significantly. This asymmetry means summarization-heavy weight ratios often improve merged models.
- Core assumption: Tasks requiring smaller weight shifts are more robust to parameter perturbation during merging.
- Evidence anchors:
  - [section] "individual SFT model for Code Summarization shows the lowest L2 norms (especially at larger scales), suggesting minimal parameter deviation from the base model"
  - [section] "consistent success of summarization-heavy weight ratios suggests semantic understanding from summarization provides transferable benefits to generation tasks"
  - [corpus] "Dynamic Fisher-weighted Model Merging" suggests task-specific importance weighting; supports asymmetry but uses different methodology.
- Break condition: If a new task requires large structural changes (e.g., learning new syntax patterns not in pretraining), it may degrade disproportionately under merging.

## Foundational Learning

- **Concept: Task Vectors and Model Arithmetic**
  - Why needed here: Understanding that merged models combine weight differences (task vectors) from base models, not raw weights.
  - Quick check question: Can you explain why merging (θ_base + α·Δθ_task1 + β·Δθ_task2) differs from averaging final weights?

- **Concept: Interference in Multi-Task Learning**
  - Why needed here: The paper's core finding hinges on how gradient conflicts during joint training differ from parameter conflicts during merging.
  - Quick check question: Why might two tasks trained together perform worse than two separate models combined?

- **Concept: Pearson Correlation for Weight Update Analysis**
  - Why needed here: The paper uses layer-wise correlation between task-specific weight deltas as a diagnostic for merging viability.
  - Quick check question: If r_Δw ≈ 0.9 across layers, which strategy should you prefer and why?

## Architecture Onboarding

- **Component map:**
  Base model -> Separate task specialists -> Merger (Linear/TIES/DARE/DELLA) -> Evaluation on HumanEval/MBPP/CodeXGLUE

- **Critical path:**
  1. Fine-tune base model on each task separately with identical hyperparameters
  2. Compute layer-wise L2 norms and inter-task Pearson correlation
  3. Apply merging with weight sweep (0.1–0.9); compare against data mixture SFT baseline

- **Design tradeoffs:**
  - Data mixture: Single training run, but requires retraining for task changes; may suffer gradient interference
  - Model merging: Modular task addition, no retraining needed; requires storing multiple checkpoints; effectiveness scale-dependent
  - Merging algorithm choice: DELLA works best for Qwen-7B; DARE works best for DeepSeek-7B—architecture matters

- **Failure signatures:**
  - Merged 2B model shows >7% avg degradation → correlation likely too high; switch to data mixture
  - Merged model improves one task but catastrophically fails another → weight ratio too asymmetric; rebalance
  - DELLA produces near-zero BLEU scores on summarization → density/selection parameters misconfigured

- **First 3 experiments:**
  1. **Weight correlation diagnostic:** Fine-tune separate specialists at your target scale; compute r_Δw per layer. If avg > 0.5, prefer data mixture; if < 0.3, proceed with merging.
  2. **Merging weight sweep:** Using 0.2/0.8 to 0.8/0.2 ratios, identify optimal balance point. Expect summarization-heavy ratios (0.3/0.7 or 0.2/0.8) to perform well for Qwen.
  3. **Algorithm ablation:** Compare DARE vs DELLA vs TIES on your architecture. Default to DARE for DeepSeek-family; DELLA for Qwen-family based on paper findings.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Does the scale-dependent preference for merging over data mixing persist when expanding to a broader suite of tasks and multiple programming languages?
  - Basis in paper: [explicit] The Conclusion states the authors plan to "extend our study beyond two tasks to a broader multi-task suite and multiple programming languages" to characterize strategy crossovers.
  - Why unresolved: The current study is restricted to only two tasks (generation and summarization) and a single language (Python).
  - What evidence would resolve it: Experimental results comparing data mixing and merging on 3+ diverse tasks (e.g., repair, translation) across languages like Java and C++.

- **Open Question 2**
  - Question: Can new merging strategies be developed that dynamically adapt to the specific ways downstream tasks modify model weights?
  - Basis in paper: [explicit] The Conclusion lists a goal to go "beyond evaluating existing merging methods by exploring new merging strategies that adapt to how different downstream tasks modify the model."
  - Why unresolved: The current evaluation relies on existing static methods (Linear, TIES, DARE, DELLA) rather than adaptive, task-aware algorithms.
  - What evidence would resolve it: A novel merging algorithm that analyzes task-specific weight updates to optimize coefficients, outperforming static baselines.

- **Open Question 3**
  - Question: What are the theoretical links connecting weight update correlation to the success or failure of model merging?
  - Basis in paper: [explicit] The Conclusion explicitly notes the intent to "investigate theoretical links between correlation and merging success."
  - Why unresolved: The paper empirically observes that high correlation hurts merging in small models but lacks a formal theoretical explanation for this mechanism.
  - What evidence would resolve it: A formal framework or mathematical proof explaining the interference dynamics between task vectors based on their angular/magnitude relationships.

## Limitations
- Correlation-based explanation lacks direct causal validation—observed correlation doesn't prove it's the mechanism enabling merging
- Findings restricted to code generation and summarization tasks; generalization to other domains untested
- Merging weight ratio sensitivity suggests results may not generalize across different training runs or architectures

## Confidence
- High confidence: Core empirical findings (scale-dependent performance trends, specific algorithm-task-architecture pairings)
- Medium confidence: The L2-norm differences between merging and mixing approaches, and their interpretation as evidence of gradient interference
- Low confidence: The Pearson correlation mechanism as the primary driver of merging success—while measured, it's not experimentally validated as causal

## Next Checks
1. Replicate the correlation analysis across multiple random seeds for the same fine-tuning procedure to test whether high/low correlation patterns are consistent or due to initialization variance
2. Conduct an ablation study on task similarity by merging models trained on highly similar vs. dissimilar code tasks to test if correlation predicts merging success across the full range of possible task relationships
3. Test the proposed scale-correlation-merging relationship on non-code domains (e.g., multi-task summarization of news vs. scientific text) to evaluate domain generalizability