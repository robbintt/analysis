---
ver: rpa2
title: 'IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision
  Support Systems'
arxiv_id: '2506.21310'
source_url: https://arxiv.org/abs/2506.21310
tags:
- explanations
- ixaii
- users
- explainable
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IXAII, an interactive explainable AI interface
  that integrates four post-hoc XAI methods (LIME, SHAP, Anchors, and DiCE) to enhance
  transparency and user trust in AI systems. The prototype allows users to choose
  from five predefined perspectives and customize explanation content and format,
  promoting user agency and understanding.
---

# IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems

## Quick Facts
- arXiv ID: 2506.21310
- Source URL: https://arxiv.org/abs/2506.21310
- Reference count: 36
- Key outcome: IXAII integrates four XAI methods (LIME, SHAP, Anchors, DiCE) with user-controlled perspectives and formats, evaluated with 7 participants showing improved transparency and intuitive interface design.

## Executive Summary
IXAII addresses the challenge of making AI decision-making transparent and trustworthy by providing an interactive interface that integrates multiple post-hoc explanation methods. The system allows users to choose from five predefined stakeholder perspectives and customize explanation content and format, promoting active engagement rather than passive consumption. Through semi-structured interviews with three experts and four lay users, the prototype demonstrated intuitive design and increased transparency, though experts noted limitations in advanced data exploration capabilities.

## Method Summary
The study developed IXAII as a Python-based interface using Scikit-learn and Plotly Dash, integrating LIME, SHAP, Anchors, and DiCE explanation methods on the IRIS dataset. The system features nine design elements including reference methods organized by question type (Why, Why Not, What If, When), multiple visualization formats (text, table, chart, formal expressions), and five audience perspectives. Evaluation involved semi-structured interviews lasting 60-120 minutes with seven participants who tested the interface and provided qualitative feedback on usability, transparency, and user agency.

## Key Results
- Users found IXAII intuitive and helpful for increasing transparency in AI decision-making
- Experts requested more advanced data exploration options while lay users appreciated accessibility and guidance
- The multi-method approach and user-controlled formats were perceived as enhancing understanding and trust
- Five predefined perspectives successfully addressed different stakeholder needs, though role definitions could be clearer

## Why This Works (Mechanism)

### Mechanism 1: Multi-Method XAI Integration
Providing multiple post-hoc explanation methods in a unified interface enhances user comprehension by offering complementary perspectives on model behavior. IXAII integrates LIME, SHAP, Anchors, and DiCE, each answering different user questions through distinct formats. Users benefit from seeing multiple explanation types rather than relying on a single method that may obscure certain aspects of model behavior.

### Mechanism 2: User Agency Through Format Control
Granting users control over explanation content and presentation format promotes trust and subjective understanding. Users actively select reference methods and switch between format methods including text, tables, charts, and formal expressions. This transforms users from passive recipients into active participants in the explanation process.

### Mechanism 3: Audience-Specific View Tailoring
Pre-configured views for distinct stakeholder groups reduce information overload by filtering explanations to role-relevant content. Five perspectives (developer, user, business entity, regulatory entity, affected party) receive tailored information displays. Different stakeholders have fundamentally different explanatory goals and cognitive requirements, making role-based customization essential.

## Foundational Learning

- **Concept: Post-hoc XAI Methods**
  - Why needed here: IXAII's core value proposition relies on understanding what LIME, SHAP, Anchors, and DiCE each contribute differently
  - Quick check question: Can you explain why SHAP values have additive properties that LIME explanations lack?

- **Concept: Local vs. Global Explanations**
  - Why needed here: The system routes lay users to local explanations while enabling experts to switch fluidly between both types
  - Quick check question: What is the difference between explaining one prediction instance versus explaining overall model behavior?

- **Concept: Reference Methods Taxonomy**
  - Why needed here: IXAII organizes explanations by question type (Why, Why Not, What If, When) rather than by algorithm
  - Quick check question: How does a "Why Not" explanation differ structurally from a counterfactual "What If" explanation?

## Architecture Onboarding

- **Component map:**
  - Perspective selector → System applies role-based defaults
  - Data Information tab → Inputs, output ranges, prototypical examples per class
  - Explanation tabs → LIME, SHAP, Anchors, DiCE visualizations with switchable reference methods
  - Format selector → Toggle between text, tables, charts, formal expressions
  - Guide overlay → XAI method descriptions for lay users

- **Critical path:**
  1. User selects perspective → system applies role-based defaults
  2. User explores Data Information tab → builds mental model of input space
  3. User selects XAI method + reference method → generates explanation
  4. User adjusts format → customizes visualization
  5. User consults guide if needed → interprets output

- **Design tradeoffs:**
  - Minimal IRIS dataset reduces complexity for interface testing but limits domain validity
  - Expert feedback requested more advanced data exploration; lay users valued simplicity
  - Descriptive naming of reference methods improves findability but may oversimplify technical distinctions

- **Failure signatures:**
  - Users overwhelmed by option multiplicity without guide consultation
  - Experts abandoning tool due to limited histogram/exploration depth
  - Contradictory signals between XAI methods causing confusion
  - Lay users misinterpreting formal expressions (Anchors rules)

- **First 3 experiments:**
  1. Replace IRIS with a higher-dimensional real-world dataset (e.g., credit scoring) to validate scalability and observe where UI breaks down
  2. Run A/B test: IXAII full interactivity vs. single-method static baseline, measuring task completion time and subjective trust scores
  3. Audit explanation consistency: Run all four XAI methods on edge cases and document where explanations diverge or contradict

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (n=7) with only three experts limits generalizability of qualitative findings
- IRIS dataset provides overly simplified context that may not capture real-world complexity
- Several design claims are asserted without empirical validation of their relative effectiveness
- Evaluation focused on subjective perceptions rather than objective measures of decision-making quality

## Confidence
- High confidence: Providing multiple complementary XAI methods offers users different explanatory perspectives
- Medium confidence: User agency over explanation format improves trust and engagement
- Low confidence: Predefined audience perspectives effectively reduce information overload

## Next Checks
1. Replace the IRIS dataset with a real-world, higher-dimensional dataset (e.g., credit scoring or medical diagnosis) to test interface scalability and identify breaking points
2. Conduct an A/B experiment comparing IXAII's full interactivity against a single-method static baseline, measuring both objective task completion metrics and subjective trust ratings
3. Perform an explanation consistency audit by running all four XAI methods on edge cases and documenting where they diverge or contradict, then testing whether this affects user trust and understanding