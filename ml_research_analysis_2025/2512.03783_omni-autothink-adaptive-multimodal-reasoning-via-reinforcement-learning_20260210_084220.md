---
ver: rpa2
title: 'Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning'
arxiv_id: '2512.03783'
source_url: https://arxiv.org/abs/2512.03783
tags:
- reasoning
- adaptive
- arxiv
- rate
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Omni-AutoThink addresses the challenge of rigid reasoning behaviors
  in multimodal models, where models either overthink simple problems or fail to reason
  when necessary. The core method introduces a two-stage training framework: Adaptive
  Supervised Fine-Tuning (Adaptive SFT) to endow the model with fundamental reasoning
  capability using large-scale reasoning-augmented data, and Adaptive Reinforcement
  Learning (Adaptive GRPO) to optimize reasoning behaviors based on task complexity
  and reward feedback.'
---

# Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.03783
- Source URL: https://arxiv.org/abs/2512.03783
- Authors: Dongchao Yang; Songxiang Liu; Disong Wang; Yuanyuan Wang; Guanglu Wan; Helen Meng
- Reference count: 10
- Primary result: Dynamic reasoning depth selection achieves higher accuracy on complex problems while reducing unnecessary reasoning on simple ones

## Executive Summary
Omni-AutoThink addresses the challenge of rigid reasoning behaviors in multimodal models by introducing a two-stage training framework that enables dynamic adjustment of reasoning depth based on problem difficulty. The approach combines Adaptive Supervised Fine-Tuning to establish fundamental reasoning capability with Adaptive Reinforcement Learning to optimize reasoning behaviors. The framework learns to choose between thinking and no-thinking modes, achieving significant improvements in adaptive reasoning performance across text-only, text-audio, text-visual, and text-audio-visual modalities.

## Method Summary
Omni-AutoThink employs a two-stage training framework: Adaptive SFT for fundamental reasoning capability using large-scale reasoning-augmented data, followed by Adaptive GRPO for optimizing reasoning behaviors based on task complexity and reward feedback. The method enables dynamic adjustment of reasoning depth according to problem difficulty, with models learning to choose between thinking and no-thinking modes through asymmetric accuracy-based rewards and forced balanced sampling during RL to prevent mode collapse.

## Key Results
- Achieves higher accuracy on complex problems while reducing unnecessary reasoning on simple ones
- Successfully prevents mode collapse to single reasoning strategy through adaptive sampling
- Demonstrates significant improvements in adaptive reasoning performance across four multimodal benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-tier SFT data construction establishes reasoning capability while preventing mode collapse.
- Mechanism: Coarse-level data (large-scale, 2:1 reasoning-to-non-reasoning ratio) builds general reasoning ability; precise-level data with explicit difficulty annotations refines discrimination between thinking and no-thinking modes.
- Core assumption: Models require sufficient exposure to reasoning trajectories before they can learn when to deploy them.
- Evidence anchors:
  - [abstract] "Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data"
  - [section 3.3] "We first train the base model for one epoch on the large-scale coarse-level dataset to acquire general reasoning ability; and (2) we then continue fine-tuning it for one epoch on the precise-level adaptive SFT dataset"
  - [corpus] Related work (Omni-R1, R1-Omni) similarly uses RL for reasoning but without the explicit two-tier SFT foundation.
- Break condition: If base model already has strong reasoning capability, coarse-level SFT may be unnecessary; conversely, if base model lacks sufficient reasoning exposure, precise-level data alone fails to induce adaptive behavior (Table 9 shows combining both yields best results).

### Mechanism 2
- Claim: Forcing balanced sampling of both reasoning modes during RL prevents premature collapse to a single strategy.
- Mechanism: During training, prompts are manually modified to sample G thinking outputs (appending "  \n") and G no-thinking outputs (appending " \n\n") per query, ensuring the policy explores both trajectories before learning which to prefer.
- Core assumption: Without forced exploration, the model will exploit whichever mode initially yields higher rewards (typically no-thinking due to lower error rate early in training).
- Evidence anchors:
  - [section 3.4] "we explicitly force the model to sample both thinking and no-thinking trajectories by manually adjusting the input prompts"
  - [section 3.2, Table 1] Directly applying GRPO without adaptive sampling causes collapse to no-thinking mode (0.00 thinking rate across all difficulty levels)
  - [corpus] Corpus lacks direct comparison for this specific adaptive sampling mechanism.
- Break condition: If the SFT model has already developed near-perfect mode discrimination, forced balanced sampling becomes computational overhead; rejection strategy (masking easy samples) partly addresses this.

### Mechanism 3
- Claim: Asymmetric accuracy-based rewards create an implicit preference for efficiency without sacrificing correctness.
- Mechanism: The reward function gives +2 for correct no-think (efficient), +1 for correct think (accurate but costly), 0 for incorrect think, and -1 for incorrect no-think (failed efficiency gamble). This differential incentivizes the model to attempt no-thinking first, only engaging reasoning when necessary.
- Core assumption: Problem difficulty correlates with whether reasoning improves accuracy; easy problems can be solved without reasoning overhead.
- Evidence anchors:
  - [section 3.4, Equation 9] Full reward specification showing asymmetric scoring
  - [section 3.4] "Its primary motivation is to encourage the model to solve easy problems without invoking the reasoning process"
  - [corpus] Related work (Audio-Thinker, AdaptCoT) uses both format and accuracy rewards; this paper simplifies to accuracy-only.
- Break condition: If reasoning provides no accuracy benefit on hard problems (model lacks reasoning capability), the model will always prefer no-thinking; this is why SFT warm-up is prerequisite.

## Foundational Learning

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: The RL stage builds on GRPO rather than PPO/DPO; understanding group-based advantage normalization is essential for debugging reward signals.
  - Quick check question: Can you explain how GRPO computes advantages differently from PPO's learned value function?

- Concept: Mode Collapse in RL Fine-tuning
  - Why needed here: The paper's central problem is that naive RL causes collapse to no-thinking; understanding why this happens (reasoning costs tokens, higher error surface) motivates the adaptive sampling solution.
  - Quick check question: Why would a model prefer no-thinking mode even when it reduces accuracy?

- Concept: Difficulty Calibration via Model Tiers
  - Why needed here: The benchmark uses M1/M2/M3 tier models to assign difficulty labels (L1-L5); this is not ground truth but a proxy that may not transfer across model families.
  - Quick check question: What happens if your evaluation model is stronger/weaker than the M3 tier used for calibration?

## Architecture Onboarding

- Component map:
  - Input: Multimodal query (text/audio/visual) → tokenizer + modality encoders
  - Policy Model πθ: Qwen2.5-Omni-7B backbone (initialized from SFT checkpoint)
  - Sampling Layer: Prompt modification logic (append thinking/no-thinking control tokens)
  - Reward Computation: Accuracy-based scalar (no separate reward model)
  - Optimization: GRPO with rejection masking (filter easy samples when r_avg > τ)

- Critical path:
  1. Base model → Coarse SFT (1 epoch, lr=1e-5) → Acquires reasoning capability
  2. Coarse SFT → Precise SFT (1 epoch, lr=1e-5) → Learns mode discrimination
  3. Precise SFT → Adaptive GRPO (1 epoch, lr=1e-6) → Refines adaptive policy

- Design tradeoffs:
  - **Reward simplicity vs. control**: Using only accuracy rewards (not format rewards) simplifies training but removes explicit control over reasoning style.
  - **Rejection threshold τ**: Higher τ retains more samples (faster training) but may include trivial gradients; lower τ focuses on hard problems but reduces throughput.
  - **Group size G**: Larger G improves advantage estimation but increases sampling cost; paper doesn't specify optimal G.

- Failure signatures:
  - **Collapse to no-thinking**: Thinking rate = 0% across all difficulty levels → SFT stage insufficient or rejection threshold too aggressive.
  - **Collapse to all-thinking**: Thinking rate = 100% even on L1-L2 difficulty samples → Reward function may be incorrectly weighted.
  - **No accuracy improvement**: RL degrades SFT performance → Learning rate too high or reward signal noisy.

- First 3 experiments:
  1. **Sanity check**: Run SFT model on benchmark with forced thinking vs. forced no-thinking prompts; verify that thinking improves accuracy on L4-L5 but not L1-L2.
  2. **Ablation on rejection threshold**: Train with τ ∈ {0.5, 0.7, 0.9} and plot thinking rate vs. difficulty; confirm adaptive behavior emerges at appropriate threshold.
  3. **Cross-modality transfer**: Train on text-only RL data, evaluate on audio/visual benchmarks; assess whether adaptive reasoning generalizes or is modality-specific.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset dependency and calibration uncertainty: Difficulty calibration using tiered models may not generalize across different model architectures or out-of-distribution tasks
- Reward function simplicity: Asymmetric accuracy-based rewards may not capture cases where partial reasoning helps but full reasoning is unnecessary
- Single epoch training: Both SFT stages and GRPO stage use only one epoch, which may limit convergence and make results sensitive to initialization

## Confidence
- **High Confidence**: Core mechanism of preventing mode collapse through forced balanced sampling during RL is well-supported by empirical evidence
- **Medium Confidence**: Asymmetric reward function's ability to induce efficient reasoning is theoretically sound but lacks sensitivity analysis
- **Low Confidence**: Generalizability of difficulty calibration method across different model families and sufficiency of single-epoch training

## Next Checks
1. **Cross-Model Calibration Validation**: Evaluate whether difficulty labels assigned by M3-tier models for one model family transfer accurately to another family
2. **Reward Function Sensitivity Analysis**: Systematically vary the reward weights and measure the impact on thinking rate distribution and overall accuracy
3. **Multi-Epoch Training Study**: Train Omni-AutoThink for 2-3 epochs in each stage and compare adaptive reasoning performance against the single-epoch baseline