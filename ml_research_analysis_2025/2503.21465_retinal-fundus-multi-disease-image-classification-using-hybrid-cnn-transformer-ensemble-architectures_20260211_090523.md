---
ver: rpa2
title: Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble
  Architectures
arxiv_id: '2503.21465'
source_url: https://arxiv.org/abs/2503.21465
tags:
- retinal
- image
- transformer
- these
- diseases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurate multi-disease classification
  of retinal fundus images, motivated by the need for accessible healthcare solutions
  in underserved regions. The authors developed hybrid models combining deeper CNN
  backbones, Transformer encoders, and ensemble architectures to classify retinal
  images into 20 disease labels.
---

# Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures

## Quick Facts
- **arXiv ID:** 2503.21465
- **Source URL:** https://arxiv.org/abs/2503.21465
- **Reference count:** 20
- **Primary result:** Hybrid CNN-Transformer-ensemble models achieved 0.9166 model score on 20-disease retinal classification, surpassing 0.9 baseline

## Executive Summary
This study addresses the challenge of accurate multi-disease classification of retinal fundus images, motivated by the need for accessible healthcare solutions in underserved regions. The authors developed hybrid models combining deeper CNN backbones, Transformer encoders, and ensemble architectures to classify retinal images into 20 disease labels. Their approach utilized dynamic patch extraction and domain knowledge integration. The C-Tran ensemble model achieved a model score of 0.9166, surpassing the baseline of 0.9, with improved performance across multiple metrics including ML mAP (0.712), ML F1 (0.683), and ML AUC (0.954). IEViT variants also demonstrated promising results with improved computational efficiency. The research contributes to advancing retinal disease diagnosis through enhanced accuracy and interpretability.

## Method Summary
The method employs hybrid architectures combining CNN backbones (DenseNet201, ResNet152d, EfficientNet) with Transformer encoders and ensemble strategies. The approach extracts spatial features from fundus images using pretrained CNNs, applies 2D positional encoding, and processes features through Transformer layers that model feature-label interactions via self-attention. Ensemble variants combine predictions from multiple backbones using weighted averaging, while iterative expansion (IE) variants append full-image embeddings to each Transformer layer output to preserve global context. The models are trained on the MuReD dataset using BCEWithLogitsLoss with weighted random sampling to address class imbalance, employing extensive data augmentation and hyperparameter optimization.

## Key Results
- C-Tran ensemble model achieved 0.9166 model score, surpassing 0.9 baseline
- EV1 (DenseNet201 + ResNet152d ensemble) achieved highest Model Score of 0.9166 with ML F1 of 0.6831
- IEViT variants showed promising outcomes with improved computational efficiency
- ML AUC reached 0.954 and ML mAP achieved 0.712 on the 20-disease classification task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining CNN feature extraction with Transformer self-attention improves multi-label disease classification by capturing both local visual features and label dependencies.
- Mechanism: CNN backbones (DenseNet201, ResNet152d) extract spatial feature embeddings from fundus images. These embeddings are augmented with 2D positional encodings and processed through Transformer encoder layers that model feature-label interactions via self-attention. This allows the model to learn which visual features correlate with specific disease labels, including co-occurring conditions.
- Core assumption: Retinal diseases share visual features that can be captured by CNNs, and label correlations exist that Transformers can exploit.
- Evidence anchors:
  - [abstract] "utilizing hybrid models combining deeper Convolutional Neural Networks (CNNs), Transformer encoders, and ensemble architectures sequentially and in parallel to classify retinal fundus images into 20 disease labels"
  - [section 4.2] "The Transformer encoder captures feature-label interactions using a self-attention mechanism, fostering the learning of diverse dependencies between features and labels"
  - [corpus] Related work on hybrid approaches (e.g., "Adaptive Multiscale Retinal Diagnosis") similarly combines transfer learning with multi-model strategies for fundus disease detection, suggesting broader validation of hybrid architectures.
- Break condition: If diseases are visually distinct with no shared features, or if label correlations are weak, Transformer attention provides limited benefit over standalone CNNs.

### Mechanism 2
- Claim: Weighted ensemble of multiple CNN backbones improves robustness and accuracy by leveraging complementary feature representations.
- Mechanism: Two backbone models (DenseNet201 and ResNet152d) process the same input image independently. Their predictions are combined using weighted averaging (Pc = a · Pdn + b · Prn), where weights reflect each backbone's relative strength. DenseNet's dense connectivity captures fine-grained details while ResNet's residual connections capture hierarchical features—the ensemble benefits from both.
- Core assumption: Different CNN architectures capture complementary disease-relevant features; their prediction errors are not perfectly correlated.
- Evidence anchors:
  - [abstract] "C-Tran ensemble model achieved a model score of 0.9166, surpassing the baseline of 0.9"
  - [section 4.3] "The ensemble model combines the predictions from two distinct backbone models, DenseNet201 and ResNet152D, utilizing a weighted ensemble strategy"
  - [section 6, Table 6] "EV1 (S: DN, W: RN)" achieved highest Model Score of 0.9166 with ML F1 of 0.6831
  - [corpus] Ensemble approaches for retinal disease classification appear in related literature, though direct comparison is limited.
- Break condition: If backbones make similar errors or if one backbone consistently dominates, ensemble provides marginal gain over single model.

### Mechanism 3
- Claim: Iterative expansion (IE) preserves global image context throughout Transformer encoding, improving feature representation.
- Mechanism: IEViT appends a CNN-derived image embedding (ximg) to each Transformer layer output (ẑl = [zl, ximg]). This iterative concatenation ensures the full-image representation persists across all encoding layers, preventing information loss during attention operations. For C-Tran variants (IECTe, IeECT), backbone embeddings are similarly appended after each Transformer layer.
- Core assumption: Standard Transformer attention may lose global context; reintroducing full-image embeddings at each layer mitigates this.
- Evidence anchors:
  - [abstract] "experiments with the IEViT model showcased equally promising outcomes with improved computational efficiency"
  - [section 4.4] "the original input image is iteratively added to the output of each Transformer encoder layer... enhancing its capability to capture spatial information and maintain global context"
  - [section 4.5] "the IECTe model continuously 'remembers' the full image information, facilitating effective information fusion"
  - [corpus] Limited direct corpus validation for IE mechanism in fundus imaging specifically.
- Break condition: If the appended embeddings introduce noise or redundancy that degrades attention quality, performance may not improve.

## Foundational Learning

- Concept: **Vision Transformer (ViT) patch embedding**
  - Why needed here: IEViT variants build on ViT's patch-based processing; understanding patch extraction and positional encoding is essential.
  - Quick check question: Can you explain how an image is divided into patches and embedded for Transformer input?

- Concept: **Multi-label classification with BCE loss**
  - Why needed here: The task involves 20 disease labels that can co-occur; binary cross-entropy per label is the training objective.
  - Quick check question: How does multi-label classification differ from multi-class classification in terms of loss function and output activation?

- Concept: **Transfer learning with ImageNet-pretrained CNNs**
  - Why needed here: All backbone models (DenseNet, ResNet, EfficientNet) are pretrained on ImageNet and used as feature extractors.
  - Quick check question: Why might ImageNet features transfer well to medical imaging, and what are the limitations?

## Architecture Onboarding

- Component map:
  Input Image (fundus) -> CNN Backbone (DenseNet201 | ResNet152d) -> Feature Embedding [n×b×d] -> Positional Encoding (PE2D) -> Transformer Encoder (6-12 layers) -> Self-attention over features + label embeddings -> MLP Classification Head -> Sigmoid -> Multi-label predictions (20 classes)

- Critical path:
  1. Image preprocessing (resize, augment, normalize with ImageNet stats)
  2. CNN backbone feature extraction (this determines representation quality)
  3. Positional encoding (spatial awareness for Transformer)
  4. Transformer self-attention (label-feature interaction learning)
  5. Loss computation (BCEWithLogitsLoss) -> backprop

- Design tradeoffs:
  - **Accuracy vs. compute:** Ensemble variants (EV1, EV2) achieve best scores but require two backbone forward passes. IEViT variants offer improved efficiency with competitive performance.
  - **Model depth vs. overfitting:** Deeper backbones (DenseNet201 vs. DenseNet121) improve features but increase overfitting risk on small datasets (MuReD has only 1,766 training samples).
  - **Weighted vs. single ensemble:** EV1 allows fine-grained backbone weighting; EV2 uses single Transformer for fused embeddings (simpler but less flexible).

- Failure signatures:
  - **Low ML F1 with high ML AUC:** Model is confident but poorly calibrated; may need threshold tuning or focal loss.
  - **Training loss decreasing but validation metrics plateau:** Overfitting—reduce model capacity, increase dropout, or use stronger augmentation.
  - **Large performance gap between DenseNet and ResNet paths in ensemble:** Weight imbalance; adjust ensemble weights (a, b) or check backbone compatibility.
  - **IE variants underperforming baseline:** Image embedding may introduce noise; try reducing ximg dimension or adding projection layer.

- First 3 experiments:
  1. **Reproduce C-Tran baseline with DenseNet201 backbone:** Verify setup by matching reported ML AUC (~0.92) and Model Score (~0.90). Use Weighted Random Sampler for class imbalance. Debug data pipeline and augmentation if scores are significantly lower.
  2. **Ablate ensemble weights:** Train EV1 with varying weight ratios (e.g., 0.7/0.3, 0.6/0.4, 0.5/0.5 for DenseNet/ResNet) to find optimal balance. Log per-label performance to identify which diseases benefit from which backbone.
  3. **Compare IEViT v2 DPD vs. uniform patches:** Test whether dynamic patch decomposition improves over fixed 32×32 patches. Monitor training time and memory to quantify efficiency gains. If DPD underperforms, check importance weight initialization and NPD formula implementation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of Shapley values specifically improve the interpretability and causal explanation of the C-Tran ensemble's diagnostic predictions?
- Basis in paper: [explicit] The Conclusion states, "In our future work, we aim to explore the integration of Shapley values to unravel the causal relationships behind our model’s predictions, making them more transparent and understandable."
- Why unresolved: While the models achieve high accuracy (0.9166 Model Score), they currently function as "black boxes." The authors explicitly identify the need to unravel causal relationships to enhance practical utility and trust in medical diagnostics.
- What evidence would resolve it: A study implementing Shapley Additive exPlanations (SHAP) on the trained models, visualizing feature importance maps that align with known clinical markers (e.g., drusen, hemorrhages) for the 20 disease labels.

### Open Question 2
- Question: Do the proposed dynamic patch extraction methods (DPD) and hybrid architectures maintain their performance advantages when scaled to the larger RFMiD 2.0 and JSIEC datasets?
- Basis in paper: [explicit] The Dataset section notes, "In the future, we intend to incorporate the RFMiD 2.0 and JSIEC datasets in our analysis as [RFMiD 2.0] is the latest dataset... and encompasses all the essential labels of MuReD."
- Why unresolved: The current results are derived exclusively from the relatively small MuReD dataset (2,208 images). It is undetermined if the improvements seen in the IEViT variants are robust enough to generalize to the larger label space (49 labels) and sample size of RFMiD 2.0.
- What evidence would resolve it: Benchmark results showing that the C-Tran and IEViT models achieve comparable or superior Model Scores on the RFMiD 2.0 and JSIEC datasets compared to the current MuReD baselines.

### Open Question 3
- Question: To what extent does the small size of the MuReD dataset introduce randomness or overfitting into the reported evaluation metrics?
- Basis in paper: [explicit] The Conclusion acknowledges, "We acknowledge the constraints imposed by data size and class imbalances, which could lead to randomness and potential overfitting."
- Why unresolved: The authors admit that the high performance metrics (e.g., 0.954 ML AUC) might be partially influenced by the limited validation data (only 442 images) rather than purely by model capability.
- What evidence would resolve it: Results from rigorous cross-validation (e.g., 5-fold or 10-fold) or performance analysis on a completely independent, external validation cohort not derived from the original RFMiD split.

## Limitations
- Evaluation based on single dataset (MuReD) with 2,208 total images, limiting generalizability
- Ensemble weights (a=0.6, b=0.4) are assumed rather than reported from best configuration
- Iterative expansion mechanism contribution is not isolated through ablation studies
- Small validation set (442 images) may introduce randomness into performance metrics

## Confidence
- **High confidence** in the general hybrid architecture approach combining CNN feature extraction with Transformer attention for multi-label classification
- **Medium confidence** in the specific ensemble weight configurations and their contribution to the reported 0.9166 score
- **Low confidence** in the clinical significance of the improvement and the generalizability beyond the MuReD dataset

## Next Checks
1. **Reproduce baseline C-Tran model** with DenseNet201 backbone to verify the reported ML AUC (~0.92) and Model Score (~0.90) on the MuReD validation set using the specified augmentation pipeline and hyperparameters.

2. **Ablation study of iterative expansion** by training IEViT variants with and without the image embedding concatenation at each Transformer layer to quantify the specific contribution of the IE mechanism to overall performance.

3. **Cross-dataset validation** by evaluating the best-performing model (C-Tran EV1) on an independent retinal fundus dataset (e.g., EyePACS or Messidor) to assess generalization beyond the MuReD dataset used in training.