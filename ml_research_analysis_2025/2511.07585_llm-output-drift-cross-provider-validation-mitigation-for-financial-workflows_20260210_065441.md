---
ver: rpa2
title: 'LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows'
arxiv_id: '2511.07585'
source_url: https://arxiv.org/abs/2511.07585
tags:
- financial
- drift
- consistency
- https
- regulatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that nondeterministic outputs (output drift)
  in Large Language Models undermine auditability and trust in financial workflows.
  The authors quantify drift across five model architectures (7B-120B parameters)
  on regulated financial tasks, revealing that smaller models (Granite-3-8B, Qwen2.5-7B)
  achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5%
  consistency regardless of configuration.
---

# LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows

## Quick Facts
- **arXiv ID**: 2511.07585
- **Source URL**: https://arxiv.org/abs/2511.07585
- **Reference count**: 40
- **Primary result**: Smaller models (7-8B) achieve 100% output consistency at T=0.0; larger models (70-120B) exhibit significant drift regardless of configuration.

## Executive Summary
This paper addresses output drift in Large Language Models (LLMs) when deployed for regulated financial workflows, where nondeterministic behavior undermines auditability and trust. Through systematic testing across five model architectures (7B-120B parameters) on financial tasks including SEC 10-K RAG Q&A, JSON summarization, and SQL generation, the authors demonstrate that model size inversely correlates with output consistency. The study establishes a finance-calibrated deterministic test harness and three-tier classification system enabling risk-appropriate model selection, with practical implications for financial compliance and audit trails.

## Method Summary
The authors evaluated output drift across five models (Qwen2.5-7B, Granite-3-8B, Llama-3.3-70B, Mistral-Medium-2505, GPT-OSS-120B) on three financial tasks using SEC 2024 10-K filings and synthetic finance databases. They employed a deterministic test harness combining greedy decoding at T=0.0, fixed seeds, and SEC structure-aware retrieval ordering. The evaluation measured identity rates (normalized edit distance ≤ ε), factual drift through citation mismatches and ±5% materiality thresholds, and latency. Testing involved 480 runs (n=16 per condition) across temperature and concurrency variations, with cross-provider validation between local (Ollama) and cloud (watsonx.ai) deployments.

## Key Results
- Smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency regardless of configuration (p<0.0001).
- Structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift ranging from 25-75%.
- Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments for Tier 1 models.
- A three-tier model classification system enables risk-appropriate deployment decisions for financial workflows.

## Why This Works (Mechanism)

### Mechanism 1: Inverse Scale-Consistency Relationship
- **Claim**: Smaller models (7-8B parameters) achieve higher output consistency than larger models (70-120B) at identical configurations.
- **Core assumption**: Nondeterminism in larger models is an emergent property of architecture and inference implementation, not fixable via configuration alone.
- **Evidence anchors**: [abstract] Smaller models achieve 100% consistency while GPT-OSS-120B exhibits only 12.5% consistency; [section 2.1] Batch-size variation identified as primary cause of drift.

### Mechanism 2: Task-Specific Drift Sensitivity
- **Claim**: Structured generation tasks (SQL, JSON) maintain determinism at T>0, while synthesis tasks (RAG) drift significantly even with modest temperature increases.
- **Core assumption**: Schema constraints actively constrain the sampling space during decoding, not just post-hoc filtering.
- **Evidence anchors**: [abstract] SQL remains stable at T=0.2 while RAG shows 25-75% drift; [section 4.3, Table 11] RAG: 56.25% consistency at T=0.2 vs SQL: 100%.

### Mechanism 3: Layered Application-Layer Determinism Controls
- **Claim**: Achieving reproducibility requires multiple stacked controls: greedy decoding + fixed seeds + deterministic retrieval ordering + invariant checking.
- **Core assumption**: Application-layer controls are sufficient when the underlying model architecture permits determinism (i.e., Tier 1 models).
- **Evidence anchors**: [abstract] Finance-calibrated deterministic test harness combining greedy decoding, fixed seeds, and SEC 10-K structure-aware retrieval ordering; [section 3] Prior runs showed T=0.0 alone was insufficient due to retrieval order nondeterminism.

## Foundational Learning

- **Concept: Batch-invariant operations**
  - **Why needed here**: Understanding why "deterministic" settings (T=0, fixed seed) don't guarantee consistency—batch composition affects numerical outcomes in non-invariant kernels.
  - **Quick check question**: If you run the same prompt with batch size 1 vs. batch size 16, will outputs match at T=0? (Answer: Only if the model uses batch-invariant kernels.)

- **Concept: Materiality thresholds in financial compliance**
  - **Why needed here**: The ±5% tolerance isn't arbitrary—it reflects GAAP auditing standards for what constitutes a material discrepancy requiring re-work.
  - **Quick check question**: A model outputs $1.05M vs. expected $1.00M. Is this drift or acceptable variance? (Answer: 5% exactly—edge case requiring policy decision.)

- **Concept: SEC 10-K structure-aware retrieval ordering**
  - **Why needed here**: RAG drift often stems from non-obvious sources—retrieval order affects context, which affects generation. Normalizing this order is a compliance requirement, not just optimization.
  - **Quick check question**: Why sort by section_priority before snippet similarity score? (Answer: Regulatory disclosure precedence—MD&A sections have different weight than risk factors.)

## Architecture Onboarding

- **Component map**: Model Selection → Tier Classification → Deterministic Config Layer (T=0.0, seeds, greedy) → DeterministicRetriever (multi-key ordering) → Generation → Invariant Checking (±5%, citations, schema) → Audit Trail (traces/*.jsonl, bi-temporal)

- **Critical path**:
  1. **Model selection**: Must be Tier 1 (7-8B) for regulated tasks—verify with 16-run consistency test before deployment.
  2. **Configuration manifest**: Pin temperature=0.0, seed=42, decoding_method="greedy", top_p=1.0 in versioned config.
  3. **Retrieval ordering**: Implement DeterministicRetriever with SEC structure-aware multi-key sort (not just similarity).
  4. **Invariant gates**: ±5% numeric tolerance, exact citation matching, schema validation before accepting output.
  5. **Audit logging**: Every run generates JSONL trace with full manifest (model digest, corpus version, timestamps).

- **Design tradeoffs**:
  - **Tier 1 vs. Tier 2**: 7-8B models give 100% consistency but may lack capability for complex reasoning; 40-70B models may work for SQL-only workflows but not RAG.
  - **T=0.0 vs. T>0**: Strict zero temperature is required for RAG compliance; SQL can tolerate T=0.2 for slight output variety without breaking invariants.
  - **Local vs. Cloud**: Cross-provider validation shows both work, but local gives full control over batching; cloud requires verifying provider's kernel implementation.

- **Failure signatures**:
  - **Consistency <100% at T=0.0**: Architecture issue—model is Tier 2/3; switch to Tier 1.
  - **Citation drift (different [doc_id] across runs)**: Retrieval ordering nondeterministic—check DeterministicRetriever implementation.
  - **Numeric drift >5%**: Invariant check failing—may indicate factual hallucination, not just output drift.
  - **Cross-provider mismatch**: Deployment environment difference—verify identical config manifests, check for hidden API-side temperature defaults.

- **First 3 experiments**:
  1. **Baseline consistency test**: Run identical prompt 16 times at T=0.0 with fixed seed on candidate model. Target: 100% identical outputs. If <100%, reject model for regulated tasks.
  2. **Task-sensitivity probe**: Repeat with T=0.2 across RAG, SQL, and JSON tasks. Identify which tasks tolerate modest temperature for your use case.
  3. **Cross-provider validation**: Run same task on local (Ollama) and cloud (watsonx.ai) with identical manifest. Verify outputs match—confirms deployment portability for hybrid architectures.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does fine-tuning on financial domain data affect output consistency in the 7B-8B parameter range? [explicit] Section 8.4 states future work should "investigate fine-tuning effects on output consistency in the 7B-8B parameter range."

- **Open Question 2**: Can architectural modifications (e.g., batch-invariant kernels, constrained decoding) enable deterministic behavior in 70B+ parameter models? [inferred] Section 8.5 notes it is "theoretically possible to constrain very large LLMs (70B-120B) with engineered kernels or consensus mechanisms" but concludes architectural limitations persist.

- **Open Question 3**: How does the determinism-accuracy tradeoff manifest across financial tasks—do models achieving 100% consistency sacrifice correctness? [explicit] Section 7 states "Determinism controls randomness, not truthfulness—we measure repeatability, not correctness."

- **Open Question 4**: Do multimodal financial AI systems (e.g., document+chart analysis) exhibit similar or exacerbated output drift? [explicit] Section 8.4 calls for future work to "extend to emerging architectures including multimodal models."

## Limitations

- The primary uncertainty centers on the Tier 3 model (GPT-OSS-120B) results, as the paper doesn't specify model access details, raising questions about whether the 12.5% consistency rate represents fundamental architectural limitations or deployment-specific issues.
- The paper's cross-provider validation only tested on Tier 1 models (Granite-3-8B and Qwen2.5-7B), leaving uncertainty about whether deterministic behavior transfers to larger architectures.
- The three-tier classification system's boundary conditions remain empirically validated only for the tested models, leaving uncertainty about where exactly other architectures might fall on this spectrum.

## Confidence

- **High Confidence**: The inverse scale-consistency relationship and task-specific drift sensitivity are well-supported by direct experimental evidence across multiple model sizes and task types.
- **Medium Confidence**: The claim that application-layer controls are sufficient for Tier 1 models assumes the underlying architecture permits determinism, but edge cases are not explored.
- **Low Confidence**: The three-tier classification system as a predictive framework for untested models, since boundary conditions between tiers are not empirically established beyond the tested architectures.

## Next Checks

1. **Tier 2/3 Boundary Validation**: Test intermediate-sized models (e.g., 30-40B parameters) to empirically determine where the Tier 2/Tier 3 boundary lies, validating whether the 70-120B range consistently shows <50% consistency at T=0.0.

2. **Extended Cross-Provider Testing**: Validate the deterministic behavior transfer hypothesis by testing Tier 2 models (70-120B) across multiple cloud providers and local deployments to confirm whether the architecture-level nondeterminism persists regardless of deployment environment.

3. **Extreme Configuration Stress Test**: Push Tier 1 models to extreme conditions (varying batch sizes, concurrent requests beyond typical workloads) to identify whether application-layer controls break down under stress, potentially revealing hidden nondeterminism in architectures presumed deterministic.