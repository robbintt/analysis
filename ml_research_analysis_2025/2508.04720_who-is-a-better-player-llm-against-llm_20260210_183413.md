---
ver: rpa2
title: 'Who is a Better Player: LLM against LLM'
arxiv_id: '2508.04720'
source_url: https://arxiv.org/abs/2508.04720
tags:
- uni00000010
- uni00000048
- uni00000013
- uni00000014
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Qi Town, an adversarial benchmarking framework
  that evaluates LLM capabilities through board game competitions. The platform supports
  20 LLMs across five game types (Gomoku, Chess, Reversi, Tic-Tac-Toe, and Free-Style)
  using Elo ratings, Performance Loop Graphs (PLGs), and Positive Sentiment Scores
  (PSS) as metrics.
---

# Who is a Better Player: LLM against LLM

## Quick Facts
- arXiv ID: 2508.04720
- Source URL: https://arxiv.org/abs/2508.04720
- Reference count: 40
- Primary result: Gemini-2.0-Flash achieved highest average performance (Elo variance >160 points) in board game competitions against 19 other LLMs

## Executive Summary
This study introduces Qi Town, an adversarial benchmarking framework that evaluates LLM capabilities through board game competitions. The platform supports 20 LLMs across five game types using Elo ratings, Performance Loop Graphs (PLGs), and Positive Sentiment Scores (PSS) as metrics. Results show that Gemini-2.0-Flash achieved the highest average performance, while most LLMs maintained optimistic emotional states despite technical limitations. PLGs revealed cyclic win-loss patterns exposing skill instability, with non-transferable capabilities across games. The framework demonstrates that board games provide richer, dynamic evaluation than static Q&A benchmarks, highlighting both technical and psychological dimensions of LLM performance in adversarial environments.

## Method Summary
Qi Town implements a round-robin tournament where 20 LLMs compete across five board games (Gomoku, Chess, Reversi, Tic-Tac-Toe, and Free-Style). Each LLM plays 19 opponents per cycle with three repetitions per game (2,850 total matches). Games proceed via text-based prompts where LLMs receive board states and output moves, emotions (A-E), and analysis. Elo ratings track technical performance using the standard formula with K-factor updates. Performance Loop Graphs model win-loss relationships as directed graphs to identify cyclic patterns. Positive Sentiment Scores aggregate self-reported emotions after each move to measure psychological resilience.

## Key Results
- Gemini-2.0-Flash achieved highest average Elo performance with variance >160 points
- Most LLMs maintained optimistic emotional states regardless of win/loss outcomes
- Performance Loop Graphs revealed cyclic win-loss patterns exposing skill instability
- Capabilities showed non-transferable patterns across different game types
- Board games revealed richer, dynamic evaluation than static Q&A benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial interactions generate dynamic evaluation states that expose reasoning limitations hidden by static datasets
- **Mechanism:** Board games produce combinatorial explosion of unique board states, forcing LLMs to generalize reasoning rather than retrieve memorized answers
- **Core assumption:** Winning board games requires integrated cognitive capabilities (logic, planning) that correlate with general reasoning proficiency
- **Evidence anchors:** Framework compensates for "limitation of data dependency of mainstream Q&A based benchmark"; adversarial benchmarks enable "continuous, self-generated assessment content"; related work supports measuring "general reasoning capabilities"
- **Break condition:** If LLM memorizes optimal openings or end-game databases without reasoning, evaluation measures retrieval, not generalization

### Mechanism 2
- **Claim:** LLMs exhibit affective stability (optimism) independent of technical competence, quantifiable as "mental fitness" metric
- **Mechanism:** Framework prompts LLMs to report emotional state after every move; Positive Sentiment Score aggregates these self-reports
- **Core assumption:** Self-reported "emotions" reflect stable behavioral trait or alignment target rather than random token noise
- **Evidence anchors:** "Most LLMs remain optimistic about winning and losing, demonstrating greater adaptability to high-stress adversarial environments"; no significant correlation between technical performance and emotional stability; related work explores LLM deception and social reasoning
- **Break condition:** If model's emotion output driven by immediate win/loss probability logic, it becomes status report rather than personality trait metric

### Mechanism 3
- **Claim:** Performance Loop Graphs reveal non-transitive skill instabilities that scalar Elo ratings average out
- **Mechanism:** By modeling wins as directed edges, system identifies cyclic graphs exposing "scissor-paper-rock" dynamics where specific models have tactical weaknesses against specific opponents
- **Core assumption:** Skill in LLMs is multi-faceted and context-dependent, not single linear variable
- **Evidence anchors:** PLGs revealed "cyclic win-loss patterns exposing skill instability, with non-transferable capabilities across games"; complex relationship between cyclic wins and losses exposes "instability of LLMs' skill play"
- **Break condition:** If cycles caused by random noise rather than strategic mismatches, PLG indicates instability but not meaningful strategic diversity

## Foundational Learning

- **Concept:** **Elo Rating System**
  - **Why needed here:** Primary technical benchmark used to rank 20 LLMs; must understand expected score calculation and K-factor updates to interpret results
  - **Quick check question:** If Player A (Elo 1600) beats Player B (Elo 1500), does Player A's Elo increase by large or small amount compared to beating player with Elo 1400?

- **Concept:** **Graph Theory (Cycles/DAGs)**
  - **Why needed here:** Performance Loop Graph novel contribution for analyzing stability; understanding directed graphs required to distinguish transitive hierarchy from cyclic instability
  - **Quick check question:** In perfect transitive ranking of 20 players, how many loops should exist in win/loss graph?

- **Concept:** **Prompt Engineering (State Representation)**
  - **Why needed here:** LLMs play blind; efficacy depends entirely on how game state (coordinates, board history) is serialized into text context
  - **Quick check question:** How does system represent board state to LLMâ€”is it image or text matrix?

## Architecture Onboarding

- **Component map:** Scheduler -> Game Controller -> LLM Interface -> Rater -> Sentiment Analyzer
- **Critical path:** 1) Prompt Construction: Serializing board state into specific text format; 2) Move Validation: Rejecting illegal moves and re-prompting or forfeiting; 3) State Update: Updating textual board representation for next turn
- **Design tradeoffs:** Text-only vs. Multimodal (chose "purely textual" interface using algebraic notation, isolates reasoning but removes visual-spatial processing); Fixed Rules vs. Free-Style (includes standard games for benchmarking and "Free-Style" for testing negotiation/rule-generation)
- **Failure signatures:** Strategic Hallucination (attempting illegal moves or nonexistent coordinates); Infinite Loops (games failing to terminate); Sentiment Drift (models defaulting to single emotion regardless of game state)
- **First 3 experiments:** 1) Tic-Tac-Toe Validation: Run full round-robin to verify Elo and PLG pipelines; 2) LLM "Sanity Check" (Free-Style): Test capable models negotiating playable rule set; 3) Stress Test (Chess): Execute longest average game to check for context window overflow or memory degradation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What underlying mechanisms drive cyclic win-loss patterns observed in Performance Loop Graphs?
- **Basis in paper:** Abstract states complex relationship of cyclic wins and losses exposes skill instability, "warranting further explanation and exploration"
- **Why unresolved:** Paper visualizes loops but doesn't identify if they stem from specific strategic blind spots, inconsistent state-tracking, or stochastic output variations
- **What evidence would resolve it:** Causal analysis of specific game states where upsets occur, coupled with ablation studies on reasoning chains used by losing models in those matches

### Open Question 2
- **Question:** Does Positive Sentiment Score reflect functional internal state or linguistic persona simulation?
- **Basis in paper:** Interprets self-reported emotions as "mental fitness," yet results show no consistent correlation between high sentiment and high Elo, questioning metric's validity
- **Why unresolved:** LLMs trained on human text where "optimism" associated with winning; stating "I am excited" may be predictive completion rather than evidence of actual adaptability
- **What evidence would resolve it:** Experiments testing if forcing specific emotional prompts degrades reasoning performance, proving functional link between sentiment and capability

### Open Question 3
- **Question:** Why are strategic capabilities non-transferable across different board games for same LLM?
- **Basis in paper:** Highlights that top-performing LLMs vary by game category and capabilities are "non-transferable," but root cause remains unidentified
- **Why unresolved:** Unclear if performance variance due to training data over-representation or fundamental inability to generalize abstract strategic reasoning to new rule sets
- **What evidence would resolve it:** Evaluating LLMs on novel, synthetic games with controlled complexity to separate memorization from general reasoning ability

## Limitations

- **Major Uncertainty 1:** Positive Sentiment Score mechanism assumes LLM-reported emotions reflect genuine behavioral patterns rather than random token generation or deterministic win/loss reporting
- **Major Uncertainty 2:** Text-based state representation may disadvantage LLMs with stronger visual-spatial processing capabilities, skewing results toward language-optimized models
- **Major Uncertainty 3:** Free-Style game's negotiation protocol remains underspecified, making it difficult to assess whether rule generation represents creative problem-solving or pattern matching

## Confidence

- **High Confidence:** Technical performance metrics (Elo ratings) and Performance Loop Graph construction are methodologically sound with clear mathematical foundations
- **Medium Confidence:** Interpretation of cyclic PLGs as evidence of strategic instability is reasonable but could alternatively reflect random noise or temperature-induced hallucinations
- **Low Confidence:** Positive Sentiment Score as measure of "mental fitness" lacks validation, as relationship between self-reported emotions and actual psychological states in LLMs remains speculative

## Next Checks

1. **Sentiment Analysis Validation:** Run sentiment detection on LLM outputs using independent emotion classifiers to verify whether PSS reflects actual affective patterns or random noise
2. **Cycle Stability Test:** Re-run PLG analysis with multiple temperature settings (0.0, 0.7, 1.0) to determine whether cyclic patterns persist or disappear with reduced randomness
3. **Memorization Control:** Test whether LLMs achieve high Elo scores through optimal opening databases versus genuine in-game reasoning by comparing performance against known game solutions versus random opponents