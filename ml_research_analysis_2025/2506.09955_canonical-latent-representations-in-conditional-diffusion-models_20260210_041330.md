---
ver: rpa2
title: Canonical Latent Representations in Conditional Diffusion Models
arxiv_id: '2506.09955'
source_url: https://arxiv.org/abs/2506.09955
tags:
- class
- diffusion
- clarid
- student
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Canonical Latent Representation Identifier
  (CLARID), a method to extract interpretable and robust latent representations from
  conditional diffusion models (CDMs) by removing class-irrelevant directions from
  the latent space. The resulting Canonical LAtent Representations (CLAReps) preserve
  essential categorical information while discarding background or context cues, producing
  representative samples and features for each class.
---

# Canonical Latent Representations in Conditional Diffusion Models

## Quick Facts
- arXiv ID: 2506.09955
- Source URL: https://arxiv.org/abs/2506.09955
- Reference count: 40
- Primary result: CLARID extracts interpretable latent representations from conditional diffusion models by removing class-irrelevant directions, enabling robust feature distillation that improves both adversarial robustness and generalization

## Executive Summary
This paper introduces CLARID (Canonical Latent Representation Identifier), a method to extract interpretable and robust latent representations from conditional diffusion models (CDMs) by removing class-irrelevant directions from the latent space. The resulting Canonical LAtent Representations (CLAReps) preserve essential categorical information while discarding background or context cues, producing representative samples and features for each class. Using CLAReps, the authors develop CaDistill, a feature distillation framework where a student model is trained on the full dataset while receiving core class knowledge from a CDM teacher via only 10% of the data in the form of CLAReps. Experiments on CIFAR10 and ImageNet show that CaDistill improves both adversarial robustness (e.g., 21.9% clean accuracy, 22.5% APGD-DLR robustness on ImageNet) and generalization (e.g., 6.7% ImageNet-A accuracy) compared to baselines.

## Method Summary
CLARID extracts Canonical LAtent Representations by first inverting training images to a diffusion timestep t_e using DDIM, then computing the Jacobian of CDM features at layer l. Right singular vectors of the Jacobian with large singular values identify "extraneous directions" that cause large feature changes while preserving class identity. These directions are projected out adaptively (k determined per sample via elbow point detection in explained variance ratio) to produce CLAReps, which are then decoded to Canonical Samples. CaDistill uses these Canonical Features in a distillation framework with CKA-based loss functions, training a student model on the full dataset while aligning its representations to those from the CDM teacher.

## Key Results
- CaDistill achieves 21.9% clean accuracy and 22.5% APGD-DLR robustness on ImageNet with 10% CLAReps supervision
- Improves ImageNet-A generalization by 6.7% compared to baselines
- Reduces dependence on spurious background cues while preserving class-discriminative features
- CLARID generalizes across model architectures (DiT, UNet, Stable Diffusion 2.1) and text-conditioned models

## Why This Works (Mechanism)

### Mechanism 1: Extraneous Direction Identification via Jacobian Analysis
- Claim: Directions in CDM latent space that cause large feature changes while preserving class identity correspond to class-irrelevant variations (e.g., background, context).
- Mechanism: CLARID computes the Jacobian J_t^l of CDM intermediate features at layer l and time step t_e. Right singular vectors of J with large singular values identify directions causing large output changes. Under class conditioning, moving along these "extraneous directions" alters appearance but not class identity. Projecting these out (Eq. 2) yields CLAReps.
- Core assumption: Class-defining semantics lie on a low-dimensional manifold; tangent directions to this manifold carry class-irrelevant information.

### Mechanism 2: Adaptive k Selection via Explained Variance Ratio
- Claim: The optimal number k of extraneous directions to remove varies per sample and can be identified by finding the elbow point in the EVR sequence.
- Mechanism: For each sample, compute S₁, S₂, ..., S_n where S_k = Σᵢ₌₁ᵏ σ²ᵢ / Σⱼ₌₁ⁿ σ²ⱼ. The elbow point (max perpendicular distance to line from S₁ to S_n) identifies where additional directions contribute diminishing variation. This adapts k to each sample's specific entanglement.
- Core assumption: Extrinsic directions with larger singular values carry less core class semantics; the transition point in EVR separates relevant from irrelevant.

### Mechanism 3: CKA-Based Feature Distillation
- Claim: Aligning student representations to Canonical Features via Centered Kernel Alignment transfers class-discriminative structure without over-constraining the student's feature basis.
- Mechanism: L_dist (Eq. 5) maximizes CKA between student features (of both training images and Canonical Samples) and Canonical Features. CKA is invariant to isotropic scaling and orthonormal transformations, allowing structural transfer without forcing identical basis. This outperforms FitNet, AT, and RKD losses.
- Core assumption: Canonical Features encode class-discriminative structure; aligning linear subspaces preserves this structure while allowing student flexibility.

## Foundational Learning

- **Concept: Diffusion Model Inversion (F_inv, F_dec)**
  - Why needed here: CLARID starts by inverting training images to latent space (t_e), modifying the latent, then decoding back. Understanding DDIM inversion is essential for implementing the pipeline.
  - Quick check question: Can you explain how DDIM enables deterministic inversion while preserving semantic information at intermediate time steps?

- **Concept: Jacobian Matrix and SVD for Sensitivity Analysis**
  - Why needed here: Identifying extraneous directions requires computing J = ∇_x f_θ(x_te) and its right singular vectors. Engineers must understand what SVD components represent.
  - Quick check question: Given a 768×64×64 feature map, what does a right singular vector of the Jacobian represent in terms of input perturbation direction?

- **Concept: Centered Kernel Alignment (CKA)**
  - Why needed here: L_dist uses CKA to measure representational similarity between teacher and student. Understanding CKA's invariance properties explains why it outperforms L₂ losses.
  - Quick check question: Why would CKA be preferred over L₂ distance when aligning representations from architectures with different feature orderings?

## Architecture Onboarding

- **Component map:**
  Training Image → DDIM Inversion (to t_e) → Compute Jacobian at layer l → SVD → Top-k extraneous directions → Project out directions → CLARep (x̃_te) → DDIM Decode → Canonical Sample (x̃_0) → Extract Canonical Feature at t_r, layer l' → Student Training (CaDistill with 4 losses)

- **Critical path:**
  1. Find t_e: Use two-stage sampling (unconditional then conditional), measure classifier accuracy on generated samples, find saturation point (Section H.1, Figure 20: t_e=0.8T for DiT).
  2. Find layer l for Jacobian: Use last layer (l=27 for DiT) to ensure adequate input changes (Section E, Figure 11).
  3. Find t_r for feature extraction: Sweep t_r on ImageNet20, use NMI clustering quality; optimal at t_r=0.1T (Section G.1, Figure 18).
  4. Find n (total extraneous directions): Sweep n, evaluate NMI; n=10 for DiT (Figure 23).
  5. Compute k per sample: Use elbow detection on EVR sequence (Algorithm 1).

- **Design tradeoffs:**
  - **t_e selection**: Too small → minimal image change; too large → class conditioning ineffective (Section H.1, Figure 21).
  - **n selection**: Too small → insufficient background removal; too large → discards class-relevant cues (Figure 22, fish example).
  - **Layer l for Jacobian**: Earlier layers may preserve more background; last layer ensures stronger effect (Figure 11).
  - **Data efficiency**: 10% CLAReps sufficient (Figure 34), but assumes class manifolds are low-dimensional.

- **Failure signatures:**
  - Artefacts in Canonical Samples: Suboptimal t_e or k (Figure 29, 30); green boxes show optimal, red boxes show CLARID's selection.
  - Student overfitting to CLAReps: If CFG magnitude too high, student learns converging prior not semantics (Section G.2, Figure 36).
  - No improvement from distillation: Check that Canonical Features have non-trivial NMI (>0.4 on ImageNet20); if already 1.0 (CIFAR10), use brute-force hyperparameter search (Section G.2).

- **First 3 experiments:**
  1. **Validate CLARID on a single ImageNet class**: Invert 10 images, compute Jacobian, project k=1-7 directions, visualize Canonical Samples vs. original and CFG. Check that background is simplified while foreground is preserved (compare to Figure 6).
  2. **Hyperparameter sweep on ImageNet20**: Fix n=10, sweep t_e∈{0.6T,0.7T,0.8T,0.9T} and adaptive k vs. fixed k, measure NMI. Confirm t_e=0.8T and adaptive k achieve highest NMI (replicate Figure 3).
  3. **Minimal CaDistill on ImageNet100**: Train ResNet50 with L_cls + L_dist only (λ_cs=0, λ_dist=1.0), using 10% CLAReps. Compare to vanilla ResNet50 on Clean and AutoAttack. Then add L_align and L_cano (λ_cs=0.4, λ_cf=0.5) and verify improvement (Table 12 baseline).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can performing extraneous direction projection at multiple diffusion timesteps improve CLARep quality and downstream distillation performance compared to single-timestep projection?
- Basis in paper: [explicit] Appendix H.2.1 states: "We only perform CLARID in a single time step instead of selecting multiple te. A series of projecting away extraneous directions has the potential of discarding more class-irrelevant information."
- Why unresolved: The current method fixes a single projection timestep (te) adaptively per sample, but does not explore iterative or multi-timestep projections that could progressively remove more class-irrelevant information across the denoising trajectory.
- What evidence would resolve it: Ablation experiments comparing single-timestep CLARID against multi-timestep variants (e.g., projecting at te, te-Δt, te-2Δt, ...) with quantitative evaluation of NMI on Canonical Features and downstream CaDistill robustness/generalization metrics.

### Open Question 2
- Question: Does CLARID generalize effectively to much larger-scale datasets (e.g., ImageNet-22K) and more complex conditional generation tasks?
- Basis in paper: [explicit] Limitation section states: "Whether the application of CLAReps can be effective on larger-scale problems, e.g., ImageNet22K, remains a question."
- Why unresolved: All experiments are conducted on CIFAR-10 and ImageNet-1K. Scaling to orders of magnitude more classes increases computational costs (Jacobian SVD) and raises questions about whether low-dimensional class manifolds remain identifiable in such high-cardinality label spaces.
- What evidence would resolve it: Application of CLARID to ImageNet-22K pretrained CDMs with analysis of (1) computational feasibility, (2) NMI cluster quality across thousands of classes, and (3) CaDistill student performance on robustness benchmarks.

### Open Question 3
- Question: Can selective (non-cumulative) extraneous direction projection improve upon the current top-k removal strategy?
- Basis in paper: [explicit] Appendix H.2.1 notes: "Regarding the number of extraneous directions, we only perform experiments on cumulative projection, i.e. projecting away the top-k extraneous directions. A careful selection of the extraneous directions can contribute to better results."
- Why unresolved: The current method removes the top-k singular vectors based on explained variance ratio, but orthogonal directions may carry unrelated semantics; selective removal could preserve more class-relevant information while discarding irrelevant cues.
- What evidence would resolve it: Development and evaluation of alternative direction selection criteria (e.g., based on gradient-based relevance scores, semantic clustering of directions, or iterative greedy removal) compared against cumulative top-k projection on Canonical Sample visual quality and downstream task performance.

### Open Question 4
- Question: Do pixel-space diffusion models yield better Canonical Features for distillation than latent-space models on high-resolution datasets like ImageNet?
- Basis in paper: [explicit] Discussion section L states: "We assume that the missing discriminative information can negatively affect the performance of all feature distillation methods, because the diffusion features lie in the low-resolution latent space. Due to the limits on computational resources, we leave the investigation on pixel-space DM on ImageNet as a future direction."
- Why unresolved: ImageNet experiments use latent diffusion (Stable Diffusion, DiT in VAE latent space), which may lose fine-grained discriminative details; pixel-space CDMs could provide richer Canonical Features but are computationally expensive.
- What evidence would resolve it: Training or utilizing pixel-space CDMs on ImageNet, extracting CLAReps, and comparing CaDistill student performance against latent-space teacher baselines on clean accuracy, adversarial robustness, and out-of-distribution generalization.

## Limitations

- Computational cost: CLARID requires SVD on high-dimensional Jacobian matrices (768×64×64 feature maps), which may be prohibitive for larger models.
- Parameter sensitivity: The method relies on multiple hyperparameters (t_e, n, layer l, λ weights) that require careful tuning and may vary across datasets and architectures.
- Generalizability beyond natural images: The method is validated primarily on ImageNet and CIFAR-10; performance on other domains remains untested.

## Confidence

- High confidence: The core mechanism of extracting extraneous directions via Jacobian analysis and the basic distillation framework are well-supported by theoretical grounding and experimental results.
- Medium confidence: The adaptive k selection via EVR elbow detection shows promise but relies on subjective elbow identification. The choice of hyperparameters for different datasets appears somewhat empirical.
- Medium confidence: Claims about reducing background dependency and improving generalization are supported by experiments but could benefit from additional ablation studies isolating each component's contribution.

## Next Checks

1. **Runtime Analysis**: Measure and report the computational overhead of CLARID preprocessing (Jacobian computation, SVD, projection) for different batch sizes and model scales.

2. **Ablation Study**: Systematically disable individual components (adaptive k, CKA loss, CLAReps) to quantify their marginal contribution to robustness gains.

3. **Cross-Domain Validation**: Apply CLARID to a non-natural-image dataset (e.g., medical imaging or tabular data) to test generalizability beyond the presented experiments.