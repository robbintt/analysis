---
ver: rpa2
title: Graph-Convolutional-Beta-VAE for Synthetic Abdominal Aorta Aneurysm Generation
arxiv_id: '2506.13628'
source_url: https://arxiv.org/abs/2506.13628
tags:
- latent
- data
- reconstruction
- space
- mesh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a \u03B2-Variational Autoencoder Graph Convolutional\
  \ Neural Network (GCN-\u03B2-VAE) framework for generating synthetic Abdominal Aortic\
  \ Aneurysm (AAA) datasets. The model leverages graph-based representations and a\
  \ \u03B2-VAE architecture to capture complex anatomical variations while promoting\
  \ latent disentanglement."
---

# Graph-Convolutional-Beta-VAE for Synthetic Abdominal Aorta Aneurysm Generation

## Quick Facts
- arXiv ID: 2506.13628
- Source URL: https://arxiv.org/abs/2506.13628
- Reference count: 40
- Primary result: GCN-β-VAE generates anatomically plausible AAAs with 88-94% reconstruction accuracy, outperforming PCA on Chamfer distance

## Executive Summary
This paper introduces a β-Variational Autoencoder Graph Convolutional Neural Network (GCN-β-VAE) framework for generating synthetic Abdominal Aortic Aneurysm (AAA) datasets. The model leverages graph-based representations and a β-VAE architecture to capture complex anatomical variations while promoting latent disentanglement. A Procrustes-based RandAugment (ProcAug) strategy is employed to enrich training data with minimal anatomical distortion. The approach is evaluated against PCA baselines and demonstrates superior performance on Chamfer distance, indicating better shape preservation. Reconstruction accuracy ranges from 88.19% to 94.61% across latent dimensions 4–24. Increasing β enhances latent compactness and reduces correlation, though with slight reconstruction trade-offs. The model successfully generates anatomically plausible synthetic AAAs via interpolation and extrapolation in latent space, supporting its potential for clinical research, device testing, and in silico trials. Limitations include partial disentanglement and the need for watertight meshes preprocessing.

## Method Summary
The method employs a GCN-β-VAE architecture that operates directly on 3D mesh graphs using spectral graph convolutions with Chebyshev polynomial filters applied to the graph Laplacian. The model incorporates a Procrustes-based RandAugment strategy that aligns meshes to a reference and samples augmentation transforms from the observed dataset statistics, preserving anatomical integrity. The β-VAE framework is modified with weighted reconstruction losses including vertex L1, Chamfer distance, edge length, and normal consistency. The architecture uses spatial pooling/unpooling matrices based on quadric error metrics for multi-resolution processing. Training employs 10-fold cross-validation on 60 patient-specific AAA meshes, with latent dimension optimization suggesting d=8 as optimal for the generation task.

## Key Results
- GCN-β-VAE achieves reconstruction accuracy of 88.19%–94.61% across latent dimensions 4–24
- Outperforms PCA baseline on Chamfer distance, indicating superior shape preservation
- Increasing β from 1e-3 to 8.5e-3 improves latent compactness (det(R) approaches identity matrix)
- ProcAug augmentation reduces Chamfer distance by up to 18% compared to no augmentation
- Latent space enables meaningful interpolation and extrapolation of AAA morphologies

## Why This Works (Mechanism)

### Mechanism 1: Non-linear Graph-Based Shape Encoding
The model captures complex anatomical variations more effectively than linear methods (PCA) by operating directly on the mesh graph structure using spectral convolutions. Instead of flattening 3D data into vectors, the architecture utilizes Chebyshev polynomial filters applied to the graph Laplacian, allowing the network to learn local geometric features (curvature, surface normals) and global topology simultaneously, preserving the connectivity information inherent in patient-specific meshes. The assumption is that the eigen-decomposition of the graph Laplacian provides a robust basis for filtering shape signals, and that non-linear deep feature extraction is necessary to capture AAA morphological nuances that linear subspaces miss. Evidence shows the model performs more robustly on unseen data compared to PCA-based approaches.

### Mechanism 2: Latent Disentanglement via Beta Regularization
Increasing the weight (β) of the KL-divergence term in the VAE loss forces the latent space to become more disentangled, allowing specific anatomical features to be controlled by independent latent variables. By weighting the KL term higher than the reconstruction term (e.g., β > 1), the model is penalized for creating correlated latent variables. This constraint pushes the approximate posterior qφ(z|x) toward an isotropic Gaussian prior N(0,I), effectively decoupling shape factors (e.g., aneurysm sac size vs. neck angle). The assumption is that a disentangled representation corresponds to meaningful, independent morphological modes rather than just statistical noise. Evidence shows increasing β results in higher determinant of the correlation matrix, pushing it closer to the identity matrix while reconstruction accuracy marginally declines.

### Mechanism 3: Procrustes-Constrained Data Augmentation (ProcAug)
Synthetic augmentation using transformations derived from Procrustes alignment improves generalization on unseen data by expanding the effective dataset size without violating anatomical validity. Standard augmentations (random rotation/scaling) might create physically impossible orientations. ProcAug first aligns the dataset to a reference mesh to calculate the precise range of observed rotations and scales, then samples strictly from this observed distribution (Procrustes RandAugment), effectively interpolating valid geometric poses to regularize the encoder. The assumption is that the primary modes of geometric variance in the small dataset (N=60) are representative of the broader population, justifying the re-combination of existing scaling and rotation parameters. Evidence shows ProcAug consistently achieves lower L2 and RCD errors compared to "No ProcAug," reducing relative error by up to ~18%.

## Foundational Learning

- **Concept: Spectral Graph Convolutions**
  - Why needed here: Standard CNNs require regular grids (pixels/voxels). AAA meshes are irregular graphs. You must understand how the Chebyshev polynomial filters approximate spectral filtering on the graph Laplacian to debug reconstruction artifacts.
  - Quick check question: How does the Chebyshev order K affect the receptive field of the convolution on the mesh?

- **Concept: The Evidence Lower Bound (ELBO) in VAEs**
  - Why needed here: The paper modifies the ELBO with β. You need to distinguish the reconstruction term (accuracy) from the regularization term (KL divergence) to tune the trade-off between a "sharp" exact replica and a "fuzzy" generative model.
  - Quick check question: Does increasing β tighten the lower bound or relax it, and what does that imply for reconstruction loss?

- **Concept: Chamfer Distance vs. L2 Norm**
  - Why needed here: The paper highlights that L2 norm alone is insufficient for shape quality. You must grasp why Chamfer distance (point-to-nearest-point) is used as the primary metric for "shape preservation" despite its computational cost.
  - Quick check question: Why might two meshes have a low L2 error but a high Chamfer distance?

## Architecture Onboarding

- **Component map:**
  Input: 3D Mesh (Vertices V, Faces F) -> ProcAug (Online Augmentation) -> Encoder: 4 blocks of [Spectral Graph Conv (Cheb) + Spatial Pooling] -> Linear Layers -> Latent μ, σ -> Reparameterization (z = μ + σ ⊙ ε) -> Decoder: Linear Layers -> 4 blocks of [Spectral Graph Conv + Spatial Unpooling] -> Output: Reconstructed Mesh

- **Critical path:**
  The Spatial Pooling/Unpooling matrices (Qd, Qu). These are precomputed based on quadric error metrics and are fixed during training. If these mappings are not saved or loaded correctly, the model cannot decode the vertex locations back to the original resolution. The β hyperparameter. This is the single "knob" controlling the transition from an auto-encoder (β ≈ 0) to a generative model (β > 1).

- **Design tradeoffs:**
  Latent Dimension (d=4 to 24): Lower d forces compactness (high det(R)) but limits reconstruction accuracy (88% vs 94%). The paper suggests d=8 as a sweet spot for generation. Loss Weights (α): The paper weights Chamfer and Vertex loss at 1.0, while Normal and Edge loss are 0.1. Deviating from this may prioritize smoothness over global shape accuracy.

- **Failure signatures:**
  Flying Vertices: Isolated vertices far from the mesh surface. Indicates Edge Loss (Ledge) is too low or learning rate is too high. Mode Collapse: Generated AAAs look identical despite different latent inputs. Indicates β is too low (posterior collapse) or latent dimension is too small. Anatomical Hallucinations: Invalid protrusions. Indicates the model is overfitting noise due to insufficient data or excessive augmentation.

- **First 3 experiments:**
  1. Baseline Reconstruction: Train with β=0 and No ProcAug. Establish the upper bound of reconstruction accuracy (should be ~95%+) to quantify the cost of generative features.
  2. Latent Sweep: Fix β=1e-3. Train 5 models with latent dimensions d ∈ {4, 8, 12, 16, 24}. Plot Reconstruction % vs. Latent Size to find the "knee" of the curve (information saturation point).
  3. Disentanglement Visualisation: Train with high β (e.g., 8.5e-3). Traverse individual latent dimensions one at a time while keeping others at 0. Visualize the mesh deformation to verify if single variables control specific anatomical features (e.g., length vs. width).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to support variable-topology meshes or raw point clouds to eliminate the dependency on watertight mesh preprocessing?
- Basis in paper: [explicit] The conclusion states the method is "designed for watertight meshes" requiring preprocessing for open surfaces, and explicitly lists "extension to variable-topology meshes or point-cloud representations" as a future direction.
- Why unresolved: The current architecture relies on fixed graph connectivity for spectral convolution and pooling operations, which breaks down if the mesh topology changes or requires closing (e.g., creating geometric lids).
- What evidence would resolve it: Successful generation of anatomies with varying branching structures or open surfaces without the need for artificial geometric closure, validated by standard metrics on non-watertight inputs.

### Open Question 2
- Question: To what extent does incorporating supervised clinical labels or centerline features improve the disentanglement of correlated anatomical factors in the latent space?
- Basis in paper: [explicit] Section 4 notes that "latent disentanglement achieved is partial" and suggests "incorporating supervised clinical labels could guide the latent space toward clinically actionable features."
- Why unresolved: The current unsupervised approach results in latent factors that may still entangle correlated anatomical changes, limiting the precise control over specific morphological attributes.
- What evidence would resolve it: A quantitative analysis showing that conditioning the model on labels (e.g., sac diameter, neck angulation) yields a latent space where specific dimensions map 1-to-1 to clinical features without interfering with other attributes.

### Open Question 3
- Question: Does the model maintain its reconstruction fidelity and generative diversity when validated on large-scale, multi-center external datasets?
- Basis in paper: [explicit] The authors identify "large-scale validation on external datasets" as a necessary future direction to confirm the model's utility beyond the 60-patient cohort used in the study.
- Why unresolved: The current results are derived from a small dataset (60 samples), and while ProcAug aids generalization, it is unclear if the model captures the full spectrum of population-level anatomical variability or overfits to the limited training distribution.
- What evidence would resolve it: Reporting of Chamfer distance and reconstruction accuracy on a hold-out set from a different institution or a significantly larger public database.

## Limitations
- Requires watertight meshes, necessitating preprocessing that may introduce geometric artifacts
- Trained on only 60 patient samples, raising questions about generalizability to rare morphologies
- Partial latent disentanglement achieved, with correlated anatomical features still entangled in some latent dimensions

## Confidence
- **High Confidence**: The GCN-β-VAE architecture and its components (spectral convolutions, β-VAE loss) are well-established and correctly implemented based on the literature; ProcAug augmentation strategy is novel and the reported improvements in Chamfer distance are credible given the small dataset size
- **Medium Confidence**: The claim that the model outperforms PCA on unseen data is supported by the metrics, but the comparison is limited to a single baseline without broader validation; the assertion that the model generates "anatomically plausible" AAAs is primarily based on visual inspection rather than quantitative anatomical validation
- **Low Confidence**: The paper's assertion of "superior performance" is based on Chamfer distance alone; other metrics (e.g., Hausdorff distance, anatomical landmark accuracy) could provide a more complete picture; clinical utility claims (device testing, in silico trials) are aspirational and not directly validated in this work

## Next Checks
1. **Clinical Anatomical Validation**: Engage a vascular surgeon to qualitatively assess the anatomical plausibility of 20 randomly generated AAAs. Score them on aneurysm neck angle, sac shape, and vessel continuity to ensure they fall within realistic clinical parameters.

2. **Latent Disentanglement Quantification**: Implement a quantitative disentanglement metric (e.g., FactorVAE metric or mutual information gap) to measure the semantic independence of latent dimensions. Compare the β=8.5e-3 model against a non-β-VAE baseline to provide objective evidence of improved disentanglement.

3. **Generalization to Out-of-Distribution Samples**: Test the model on a small, independent set of AAA meshes from a different hospital or scanner protocol. Measure the drop in reconstruction accuracy and Chamfer distance to assess the model's robustness to variations in imaging and anatomical presentation.