---
ver: rpa2
title: 'From Words to Wisdom: Discourse Annotation and Baseline Models for Student
  Dialogue Understanding'
arxiv_id: '2511.20547'
source_url: https://arxiv.org/abs/2511.20547
tags:
- dialogue
- task
- discourse
- context
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of automatically identifying
  knowledge construction (KC) and task production (TP) discourse in student conversations,
  which is crucial for understanding how students engage in learning. To bridge the
  gap in educational dialogue datasets, the authors create a novel annotated dataset
  of undergraduate engineering student discussions, featuring KC and TP discourse.
---

# From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding

## Quick Facts
- arXiv ID: 2511.20547
- Source URL: https://arxiv.org/abs/2511.20547
- Reference count: 40
- Best reported F1-score: 0.57 on KC/TP classification task

## Executive Summary
This study tackles the challenge of automatically identifying knowledge construction (KC) and task production (TP) discourse in student conversations, which is crucial for understanding how students engage in learning. The authors created a novel annotated dataset of undergraduate engineering student discussions and established baseline models using GPT-3.5 and Llama-3.1 with prompting and fine-tuning techniques. Experimental results reveal that these models perform suboptimally, with the highest F1-score reaching only 0.57, particularly struggling with KC classification. The findings highlight the need for further research into models and methods better suited for educational discourse analysis.

## Method Summary
The study created a novel dataset of 32 undergraduate engineering student conversations (19 topics, avg. 6404 tokens/conversation) annotated with KC and TP discourse labels. Baseline models included GPT-3.5-turbo and Llama-3.1-8B-Instruct tested under zero-shot, few-shot, and fine-tuning conditions. Prompt engineering explored variations with context, topic descriptions, and label definitions. Llama-3.1 was fine-tuned using Unsloth with LoRA/quantization (5 epochs, LR=1e-4). Evaluation used weighted F1-score with 5-fold cross-validation for fine-tuning.

## Key Results
- GPT-3.5 achieved highest zero-shot F1 of 0.57 with optimized prompt including topic description
- Llama-3.1 in few-shot setting performed best overall but misclassified ~50% of KC instances as TP
- Topic diversity in training improved model generalization compared to single-topic training
- Fine-tuning on small domain-specific data often degraded performance due to overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing explicit task description and label definitions improves zero-shot discourse classification by anchoring model reasoning to the communicative goal of the conversation.
- Mechanism: The model uses task description as a schema to interpret ambiguous utterances, reducing the search space for label assignment.
- Core assumption: The model's pre-trained knowledge includes sufficient understanding of educational discourse patterns to apply the schema when explicitly prompted.
- Evidence anchors:
  - [section V.A]: "In the zero-shot setting, the best result is obtained by GPT-3.5 with optimized prompt 3, which includes a topic description in addition to the previous dialogue contexts."
  - [abstract]: "Experimental results reveal that these models perform suboptimally, with the highest F1-score reaching only 0.57."
- Break condition: If task descriptions are themselves ambiguous or if student discourse drifts far from the stated task.

### Mechanism 2
- Claim: Knowledge construction discourse is systematically misclassified as task production because KC utterances often contain surface-level features that overlap with TP markers.
- Mechanism: The model relies on lexical and syntactic patterns learned from general corpora, where statements about "doing next steps" or asking clarifying questions typically correlate with task-oriented discourse.
- Core assumption: Models trained on general dialogue corpora lack priors for the specific epistemological framing patterns that distinguish KC from TP in educational contexts.
- Evidence anchors:
  - [section V.C, Figure 5]: "It shows that the model is good at predicting the task production labels. However, it struggles with predicting the knowledge construction label and misclassifies about half of these instances as task production."
  - [section III.C.3]: "The disagreement mostly happens under two conditions: (i) when students discuss the details of their problem-solving steps, and (ii) when students ask each other questions."
- Break condition: If training data includes explicit reasoning chains or expert annotations that surface the "why" behind KC labels.

### Mechanism 3
- Claim: Training on diverse topics improves cross-topic generalization because it exposes the model to a broader distribution of linguistic and conceptual patterns, reducing overfitting to topic-specific discourse structures.
- Mechanism: Heterogeneous training forces the model to learn more abstract discourse-level features that generalize across content domains.
- Core assumption: The underlying KC vs. TP distinction is topic-invariant at some level of abstraction.
- Evidence anchors:
  - [section V.C, Table II]: "The results, summarized in Table II, reveal that, the model performs better when trained on data from different topics, except for prompt 5... This finding suggests that exposure to a broader range of linguistic and conceptual patterns enhances the model's generalization ability."
  - [section V.B]: "We hypothesize that since our data is very domain-specific, it may have reduced the model's generalization ability by overfitting to narrow linguistic or conceptual patterns."
- Break condition: If the KC vs. TP distinction is fundamentally content-dependent, topic diversity alone may be insufficient.

## Foundational Learning

- Concept: **Epistemological framing in dialogue**
  - Why needed here: The entire classification scheme depends on understanding that students can frame the same activity either as "getting the task done" (TP) or "building understanding" (KC).
  - Quick check question: Given the utterance "So then graph V-I versus time and take the area under the curve" — is this TP or KC? What additional context would you need to decide?

- Concept: **Inter-annotator agreement and Cohen's κ**
  - Why needed here: The paper reports κ = 0.45 (moderate agreement), which signals that even expert humans struggle with this task.
  - Quick check question: If two expert annotators disagree on 30% of labels, what is the theoretical ceiling for model accuracy on this task?

- Concept: **Zero-shot vs. few-shot vs. fine-tuning paradigms**
  - Why needed here: The paper evaluates all three and finds they perform differently across models.
  - Quick check question: Why might few-shot prompting outperform fine-tuning on a domain-specific task with limited data?

## Architecture Onboarding

- Component map: Data layer (32 convos, 19 topics, 321 turns) -> Prompt engineering layer (5 templates) -> Model layer (GPT-3.5, Llama-3.1) -> Evaluation layer (F1, confusion analysis)

- Critical path:
  1. Annotate data with KC/TP labels
  2. Design prompt templates incorporating context, task description, and label definitions
  3. Run zero-shot and few-shot experiments with both models
  4. Fine-tune Llama-3.1 on heterogeneous topic splits
  5. Analyze confusion patterns and topic generalization

- Design tradeoffs:
  - **Closed vs. open models**: GPT-3.5 offers convenience but no weight access; Llama-3.1 enables fine-tuning but requires GPU infrastructure
  - **Prompt complexity vs. model capacity**: Longer prompts with more context increase token costs and may overwhelm smaller models
  - **Topic diversity vs. data scarcity**: More topics improve generalization but each topic has fewer examples

- Failure signatures:
  - **KC→TP confusion**: Model defaults to TP when utterance contains procedural language or questions
  - **Fine-tuning degradation**: Performance drops after fine-tuning on narrow data
  - **Few-shot overload**: Adding examples sometimes hurt GPT-3.5 performance

- First 3 experiments:
  1. **Prompt ablation study**: Systematically test each prompt component (previous context, topic description, label definitions, succeeding context) in isolation
  2. **Error analysis on human disagreement cases**: Extract the 726 utterances where annotators disagreed and analyze model predictions specifically on this subset
  3. **Chain-of-thought prompting for KC detection**: Implement reasoning chains in prompts to test whether explicit intermediate reasoning steps help distinguish KC from TP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating reasoning chains (Chain-of-Thought prompting) improve model performance in distinguishing Knowledge Construction (KC) from Task Production (TP) discourse?
- Basis in paper: The authors state, "For future work, we plan to create reasoning chains that will help the model better understand the definitions of the labels."
- Why unresolved: Current prompting methods yield suboptimal results (max F1 0.57), with models misclassifying half of the KC instances as TP.
- What evidence would resolve it: Experimental results comparing CoT prompting against standard zero-shot prompts showing significant F1 improvement on the KCTP task.

### Open Question 2
- Question: Can annotating low-level discourse structures (dialogue moves) facilitate the automatic detection of high-level KC and TP concepts?
- Basis in paper: The authors intend to "annotate low-level discourse structure... so that looking at the lower levels might help to see how the higher-order concepts emerge."
- Why unresolved: The study currently models discourse at a high level without utilizing potential predictive signals from underlying conversational structures.
- What evidence would resolve it: A hierarchical model trained on low-level dialogue acts that outperforms the flat baselines established in this paper.

### Open Question 3
- Question: Does increasing topical diversity by including a broader range of undergraduate subjects mitigate overfitting and improve model generalization?
- Basis in paper: The authors aim to "expand the dataset by including a broader range of undergraduate subjects... to facilitate the development of models capable of domain-general discourse understanding."
- Why unresolved: The current dataset is restricted to a thermal fluids course, and fine-tuning showed limited gains, suggesting potential overfitting to narrow linguistic patterns.
- What evidence would resolve it: Cross-domain testing showing stable performance on unseen topics from diverse disciplines compared to the single-course baseline.

## Limitations
- Dataset restricted to undergraduate engineering homework discussions, limiting generalizability
- Moderate inter-annotator agreement (κ = 0.45) indicates inherent task ambiguity
- Small dataset size (32 conversations) may explain poor model performance and overfitting issues

## Confidence
- **High Confidence**: Models perform suboptimally on KC/TP classification (F1=0.57); KC misclassified as TP in ~50% of cases; topic diversity improves generalization
- **Medium Confidence**: Prompt engineering with task descriptions helps zero-shot performance; fine-tuning degrades performance due to overfitting; chain-of-thought prompting may improve results
- **Low Confidence**: The specific reasons for KC→TP confusion are fully understood; whether performance would improve with larger datasets; whether alternative model architectures would perform better

## Next Checks
1. **Error analysis on human disagreement cases**: Extract the 726 utterances where annotators disagreed and analyze whether model predictions align with one annotator or the other
2. **Cross-domain generalization test**: Evaluate the best-performing model on a different educational dialogue dataset to assess whether the KC/TP distinction generalizes beyond homework discussions
3. **Prompt engineering ablation with expert input**: Systematically test each prompt component while collecting qualitative feedback from educational experts on model outputs to identify which signals most effectively capture the epistemological framing distinction