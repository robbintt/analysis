---
ver: rpa2
title: Enhancing patent retrieval using automated patent summarization
arxiv_id: '2507.16371'
source_url: https://arxiv.org/abs/2507.16371
tags:
- patent
- retrieval
- summaries
- description
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using automated patent summarization to improve
  prior-art retrieval effectiveness. The authors extract key patent segments (abstract,
  claims, description, and semantically coherent sections) and apply both extractive
  (BERT, SBERT) and abstractive (BigBird) summarization methods to generate concise
  queries.
---

# Enhancing patent retrieval using automated patent summarization

## Quick Facts
- arXiv ID: 2507.16371
- Source URL: https://arxiv.org/abs/2507.16371
- Reference count: 38
- This paper demonstrates that automated patent summaries significantly outperform traditional patent sections as retrieval queries, with adjusted BigBird summaries achieving up to 35.40% MAP@100 versus 27.72% for raw claims.

## Executive Summary
This paper addresses the challenge of prior-art retrieval in patent search by proposing automated patent summarization as a query generation technique. The authors extract key patent segments (abstract, claims, description, and semantically coherent sections) and apply both extractive (BERT, SBERT) and abstractive (BigBird) summarization methods to generate concise queries. These generated summaries are then evaluated as retrieval queries across three benchmark patent datasets using embedding-based retrieval. Results show that automated summaries consistently outperform traditional patent sections when used as queries, with the adjusted BigBird model producing the best performance at 250-300 words summary length.

## Method Summary
The method involves three main stages: patent document parsing and section extraction using HUPD-derived heading dictionaries, summarization using extractive (BERT/SBERT) and abstractive (BigBird-Pegasus) approaches, and retrieval using GTE-large-en-v1.5 embeddings indexed in FAISS. The authors test multiple configurations including different input sections (claims, description, abstract) and summary lengths (50-100 words default, 250-300 words adjusted). Evaluation is performed on three datasets: CLEF-IP 2013 (citation-based relevance), USPTO Kaggle (similarity-based relevance), and BIGPATENT (intrinsic evaluation). The best-performing approach uses the adjusted BigBird model with 250-300 word summaries generated from claims or description sections.

## Key Results
- Automated summaries consistently outperform raw patent sections as retrieval queries across all three datasets
- Adjusted BigBird summaries (250-300 words) achieve 35.40% MAP@100 versus 27.72% for raw claims on CLEF-IP
- Claims and description sections provide better input for summary generation than abstracts alone
- SBERT-based summaries from description averaged 807 words while BigBird summaries were ~250 words
- The approach is particularly effective for long patent documents where traditional queries become unwieldy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Purpose-specific summaries concentrate semantic signal while reducing noise, improving embedding-based retrieval precision.
- Mechanism: Summarization models select or synthesize the most informative sentences from lengthy patent text, producing queries that are semantically denser than raw sections. This reduces dilution from boilerplate language and tangential technical details.
- Core assumption: The embedding model captures semantic similarity better when input text is information-dense rather than exhaustive.
- Evidence anchors:
  - "summarization-based queries significantly improve prior-art retrieval effectiveness"
  - Table 4 shows adjusted BigBird summaries from claims achieved MAP@100 of 35.40% vs. 27.72% for raw claims text
- Break condition: If embedding model cannot faithfully represent technical/legal terminology in condensed form, semantic drift may occur

### Mechanism 2
- Claim: Longer summaries (250-300 words) outperform shorter summaries (50-100 words) for retrieval because they retain more discriminative technical detail.
- Mechanism: Adjusted BigBird generation parameters produce extended summaries that preserve claim boundaries and technical specifications without truncating key concepts.
- Core assumption: Retrieval benefits from comprehensive coverage up to the embedding model's effective token capacity (3,000 tokens in this study).
- Evidence anchors:
  - "the adjusted BigBird model with longer summaries (250-300 words) achieved the best retrieval performance"
  - Table 4: BigBird** (224 words) from claims achieved 35.40% MAP vs. BigBird* (62 words) at 31.60%
- Break condition: If summary length exceeds embedding model's attention capacity or introduces redundancy, marginal returns diminish

### Mechanism 3
- Claim: Claims and description sections provide higher-value input for summary generation than abstracts alone.
- Mechanism: Claims define legal scope with precise technical language; descriptions provide contextual elaboration. Summarization models leverage these dense sources to produce retrieval-effective queries.
- Core assumption: Patent sections differ in information density and semantic coherence; targeted extraction captures more relevant signals.
- Evidence anchors:
  - "the description is consistently identified as the most informative and valuable source for query generation"
  - Table 5: "Brief desc and first claim" achieved 22.64% MAP@50 vs. 16.60% for abstract
- Break condition: If patent structure varies significantly across jurisdictions, automated extraction fails

## Foundational Learning

- Concept: Extractive vs. Abstractive Summarization
  - Why needed here: The paper uses both approaches; extractive (SBERT) preserves original phrasing while abstractive (BigBird) rephrases for coherence
  - Quick check question: Can you explain why SBERT summaries from description averaged 807 words while BigBird summaries were ~250 words?

- Concept: Embedding-Based Retrieval with Vector Databases
  - Why needed here: The pipeline uses FAISS and GTE-large-en-v1.5 embeddings; understanding semantic similarity is essential
  - Quick check question: Why might full patent sections (4,000+ words) underperform compared to 250-word summaries in embedding space?

- Concept: Patent Document Structure (claims, description, abstract, summary segments)
  - Why needed here: Query effectiveness depends on selecting the right input section; structure varies by jurisdiction
  - Quick check question: What is the "brief description" segment and how does it differ from the abstract?

## Architecture Onboarding

- Component map: Patent document parser -> Section extractor (HUPD-derived heading dictionary) -> Summarization layer (BERT/SBERT or BigBird) -> Embedding layer (GTE-large-en-v1.5) -> Retrieval layer (FAISS vector index)

- Critical path:
  1. Extract patent sections (claims, description, brief description when detectable)
  2. Generate summary using configured model (BigBird** with 250-300 word target recommended)
  3. Embed summary as query using GTE-large-en-v1.5
  4. Retrieve top-k from FAISS index

- Design tradeoffs:
  - Extractive (SBERT) preserves technical precision but yields longer outputs (~800 words)
  - Abstractive (BigBird) produces concise outputs (~250 words) but may introduce paraphrasing errors
  - Claims input -> higher precision; description input -> higher recall

- Failure signatures:
  - MAP drops below baseline (raw claims): Check embedding model alignment with corpus
  - SBERT outputs exceed 1,000 words: Input section may not be properly segmented
  - BigBird generates incoherent text: Fine-tuning may not have converged; verify training data quality
  - Segment extraction fails on European patents: Heading dictionary is USPTO-specific

- First 3 experiments:
  1. Replicate CLEF-IP baseline: Compare raw claims vs. BigBird** summary (claims input) on MAP@100
  2. Ablate summary length: Test BigBird at 100, 200, 300 words to validate optimal range
  3. Cross-jurisdiction test: Apply extraction heuristics to non-USPTO patents; measure segment detection rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can using automated summaries to represent the document corpus (rather than just claims) improve retrieval effectiveness?
- Basis in paper: Section 4.4 states that using different sections as corpus representations offers promising avenues but was left for future work; the Conclusion reiterates the need to explore alternative corpus representations.
- Why unresolved: The study restricted the vector index to embeddings derived solely from patent claims to isolate the impact of the generated summaries used as queries.
- What evidence would resolve it: Retrieval experiments where the indexed corpus is encoded using generated summaries or full descriptions instead of claims, compared against the current baseline.

### Open Question 2
- Question: How do summary-based queries perform when integrated with keyword-based (lexical) or hybrid retrieval methods?
- Basis in paper: Section 4.4 notes the authors focused exclusively on embedding-based retrieval and explicitly left the integration of additional retrieval methods for future work.
- Why unresolved: The experimental design aimed to isolate the contribution of the generated summaries from the retrieval technique itself by avoiding keyword-based approaches.
- What evidence would resolve it: A comparative evaluation of the generated summaries as queries using BM25 or hybrid search (lexical + semantic) on the CLEF-IP and USPTO datasets.

### Open Question 3
- Question: How can summarization models be better adapted to capture the full contextual understanding of patent documents without being limited by token constraints?
- Basis in paper: Section 3 identifies that models are typically not trained on full patent text due to token limits (e.g., 512-4096), which hinders full contextual capture.
- Why unresolved: The authors mitigated this by truncating inputs (e.g., to 700-800 words for fine-tuning) or using segments, meaning the "full text" issue remains unaddressed.
- What evidence would resolve it: Experiments utilizing architectures capable of ingesting inputs exceeding 8,000 tokens to generate summaries, compared against segment-based approaches.

## Limitations

- Segment extraction reliability may not generalize to non-USPTO patent documents where section headings and structures differ from USPTO format
- Corpus composition ambiguity exists as the exact composition and sampling methodology of the 200,000 patent corpus are not specified
- Results are dependent on a specific embedding model (GTE-large-en-v1.5) with particular configuration that may not generalize to other embedding architectures

## Confidence

- High Confidence: The comparative effectiveness of automated summaries versus raw patent sections as retrieval queries is well-supported by consistent improvements across multiple datasets and metrics
- Medium Confidence: The optimal summary length finding (250-300 words) is based on controlled experiments but lacks direct comparison with alternative embedding models or retrieval architectures
- Medium Confidence: The superiority of claims and description as input sources is demonstrated through relative performance metrics, though the underlying assumption that these sections are universally more information-dense requires further validation

## Next Checks

1. Cross-jurisdiction performance validation: Test the segment extraction heuristics on a sample of European and Japanese patents to quantify detection rate drop and measure whether summary-based retrieval still outperforms raw sections when extraction is imperfect

2. Embedding model sensitivity analysis: Replicate the retrieval experiments using alternative embedding models (e.g., SBERT-base, Contriever) to determine whether the observed performance improvements are specific to GTE-large-en-v1.5 or generalize across embedding architectures

3. Summary length boundary testing: Conduct a finer-grained analysis of summary length effects by testing 150, 200, 250, 300, and 350-word summaries with the adjusted BigBird model to precisely identify where marginal returns diminish and potential performance degradation begins