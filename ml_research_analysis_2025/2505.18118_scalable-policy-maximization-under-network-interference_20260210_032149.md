---
ver: rpa2
title: Scalable Policy Maximization Under Network Interference
arxiv_id: '2505.18118'
source_url: https://arxiv.org/abs/2505.18118
tags:
- network
- interference
- algorithm
- reward
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of learning optimal policies in
  networked systems where individuals' outcomes depend on their neighbors' treatments
  - a problem known as network interference. The authors show that under common assumptions
  about interference patterns, the node-level rewards can be expressed as a linear
  function of unknown parameters.
---

# Scalable Policy Maximization Under Network Interference

## Quick Facts
- arXiv ID: 2505.18118
- Source URL: https://arxiv.org/abs/2505.18118
- Authors: Aidan Gleich; Eric Laber; Alexander Volfovsky
- Reference count: 25
- Primary result: Thompson sampling algorithm for network policy optimization with sublinear Bayesian regret

## Executive Summary
This paper addresses the challenge of learning optimal policies in networked systems where individuals' outcomes depend on their neighbors' treatments - a problem known as network interference. The authors develop a scalable Thompson sampling algorithm that can handle networks orders of magnitude larger than existing methods. By leveraging a linear reward assumption under common interference patterns, they establish theoretical regret bounds that are sublinear in both network size and time horizon. Empirical results demonstrate that the algorithm learns quickly and outperforms existing approaches, even when assumptions are partially violated.

## Method Summary
The authors propose a Thompson sampling algorithm for policy optimization under network interference. The key insight is that under common assumptions about interference patterns, node-level rewards can be expressed as a linear function of unknown parameters. The algorithm maintains a posterior distribution over these parameters and selects treatment vectors to maximize expected rewards. The approach scales to large networks by exploiting the linear structure, enabling efficient posterior updates and policy optimization. The method is evaluated against existing approaches on both synthetic and real-world network datasets.

## Key Results
- Thompson sampling algorithm scales to networks orders of magnitude larger than existing methods
- Theoretical regret bounds are sublinear in both network size and time horizon
- Algorithm outperforms existing approaches in empirical evaluations, even when linear assumptions are violated

## Why This Works (Mechanism)
The approach works by leveraging the linearity of rewards under network interference assumptions. When treatment effects propagate through networks in predictable ways, the reward at each node can be expressed as a linear combination of unknown parameters. This allows the use of Bayesian linear regression techniques within a Thompson sampling framework. The algorithm maintains uncertainty about these parameters and explores treatments that could reveal information about them, while exploiting current knowledge to maximize rewards.

## Foundational Learning
1. **Network Interference** - Why needed: Essential for understanding how treatments affect interconnected individuals; Quick check: Can you explain the difference between SUTVA and interference settings?
2. **Thompson Sampling** - Why needed: Core algorithm for balancing exploration and exploitation; Quick check: Can you derive the posterior update for linear regression?
3. **Bayesian Linear Regression** - Why needed: Enables tractable uncertainty quantification in the linear model; Quick check: Can you write the posterior distribution for the regression coefficients?
4. **Bandit Regret** - Why needed: Primary performance metric for sequential decision making; Quick check: Can you explain the difference between frequentist and Bayesian regret?
5. **Graph Theory Basics** - Why needed: Required to model network structures and interference patterns; Quick check: Can you define neighborhood and degree in graph terminology?
6. **Causal Inference under Interference** - Why needed: Provides theoretical foundation for valid treatment effect estimation; Quick check: Can you state the assumptions needed for valid causal inference in networks?

## Architecture Onboarding

**Component Map:** Data Generator -> Network Structure -> Linear Reward Model -> Thompson Sampler -> Treatment Selection -> Reward Observation -> Posterior Update

**Critical Path:** The algorithm iterates through: (1) maintaining posterior over linear parameters, (2) sampling parameters from posterior, (3) solving optimization to select treatments, (4) observing rewards, (5) updating posterior with new data.

**Design Tradeoffs:** The linear reward assumption enables scalability but may limit applicability. Thompson sampling provides good exploration but can be computationally intensive for very large networks. The approach trades off some modeling flexibility for computational efficiency and theoretical guarantees.

**Failure Signatures:** Performance degradation when reward structure is highly nonlinear, slow learning when interference patterns are weak, computational bottlenecks for extremely large networks, poor performance when network structure is sparse or irregular.

**First Experiments:** 1) Validate linear reward assumption on synthetic data with known interference patterns, 2) Compare regret performance against contextual bandit baselines on small networks, 3) Test scalability by increasing network size and measuring computational time.

## Open Questions the Paper Calls Out
None

## Limitations
- Linear reward assumption may not hold in many real-world networks with complex interference patterns
- Theoretical regret bounds rely heavily on validity of linear model assumptions
- Empirical evaluation focuses primarily on synthetic networks with limited real-world testing

## Confidence
- Theoretical claims: Medium-High (sound under stated assumptions)
- Empirical results: Medium (reliance on synthetic data, limited real-world scenarios)
- Scalability claims: Medium-High (supported by both theory and experiments)
- Robustness to assumption violations: Medium (some empirical evidence but limited exploration)

## Next Checks
1. Test the algorithm on real-world network datasets (e.g., social networks, communication networks) to evaluate performance under more complex interference patterns.
2. Conduct sensitivity analysis by systematically relaxing the linear reward assumption and measuring the impact on both theoretical guarantees and empirical performance.
3. Compare the Thompson sampling approach with alternative policy optimization methods (e.g., contextual bandits, reinforcement learning) in settings where the linear assumption is partially violated.