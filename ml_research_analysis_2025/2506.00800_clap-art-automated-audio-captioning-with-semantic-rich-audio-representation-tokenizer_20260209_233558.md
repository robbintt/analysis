---
ver: rpa2
title: 'CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation
  Tokenizer'
arxiv_id: '2506.00800'
source_url: https://arxiv.org/abs/2506.00800
tags:
- audio
- discrete
- tokens
- clap-art
- beats-rvq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses automated audio captioning (AAC), where the
  goal is to generate natural language descriptions of general sounds, including acoustic
  events and scenes. Existing methods like EnCLAP use discrete tokens from EnCodec
  as input to language models, but EnCodec is designed for waveform reconstruction
  rather than capturing semantic information of sounds, limiting AAC performance.
---

# CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer

## Quick Facts
- arXiv ID: 2506.00800
- Source URL: https://arxiv.org/abs/2506.00800
- Reference count: 0
- SPIDEr improves from 46.3 to 49.8 on AudioCaps, and from 29.1 to 30.4 on Clotho

## Executive Summary
CLAP-ART introduces a novel automated audio captioning method that replaces waveform-focused EnCodec tokens with semantically rich discrete tokens derived from BEATs audio features via Residual Vector Quantization (RVQ). The approach integrates these tokens with CLAP embeddings and a BART language model to generate natural language descriptions of general sounds. By capturing richer semantic information through multi-layer tokenization, CLAP-ART significantly outperforms the baseline EnCLAP method on both AudioCaps and Clotho benchmarks. The method demonstrates that semantically grounded discrete tokens reduce reliance on global auxiliary embeddings while improving captioning quality.

## Method Summary
CLAP-ART processes audio through parallel branches: one extracting global CLAP embeddings, and another using BEATs to generate continuous features that are then quantized into multi-layer discrete tokens via Residual Vector Quantization. These tokens, combined with CLAP embeddings, are input to BART for caption generation. The RVQ tokenizer is pre-trained using k-means clustering on BEATs features from the training dataset, creating 16 residual codebooks of size 1024 each. The model is trained on AudioCaps for 15 epochs, then fine-tuned on Clotho, using a combined captioning and masked codec modeling loss with inverse square root learning rate scheduling.

## Key Results
- SPIDEr improves from 46.3 to 49.8 on AudioCaps benchmark
- SPIDEr improves from 29.1 to 30.4 on Clotho benchmark
- Ablation shows CLAP-ART maintains performance when CLAP embedding is removed, while EnCLAP drops significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discrete tokens derived from semantic audio representations (AR) outperform reconstruction-based audio codecs (e.g., EnCodec) as input for language models in automated audio captioning (AAC).
- **Mechanism:** Standard language models (LMs) like BART are optimized for discrete symbolic input (text). When audio is tokenized into discrete units that align with high-level semantic concepts (events, scenes) rather than low-level acoustic waveforms, the LM can more efficiently map these "audio words" to natural language captions.
- **Core assumption:** The performance bottleneck in prior AAC methods (like EnCLAP) was the semantic sparsity of EnCodec tokens, not the architecture of the LM adapter.
- **Evidence anchors:**
  - [abstract] Mentions EnCodec is designed for waveform reconstruction rather than semantic contexts, which CLAP-ART addresses.
  - [section 1] States that "pre-trained language models can leverage discrete inputs better compared to their continuous inputs" and notes EnCodec tokens yield inferior performance in downstream tasks.
  - [corpus] *DSA-Tokenizer* supports the need to disentangle semantic content from acoustic style for effective LM processing; *DUO-TOK* highlights the tension between reconstruction quality and LM learnability.

### Mechanism 2
- **Claim:** Multi-layer residual vector quantization (RVQ) captures richer semantic information than single-layer tokenization.
- **Mechanism:** Single-layer quantization (e.g., standard VQ) forces a choice on a single "best fit" token, potentially losing nuance. RVQ stacks multiple codebooks; the first captures the primary semantic concept, while subsequent layers encode residuals (finer details), creating a composite representation that packs more information into the token sequence.
- **Core assumption:** The residual errors in the semantic feature space (BEATs features) contain meaningful detail relevant to captioning, rather than just noise.
- **Evidence anchors:**
  - [abstract] Notes "further gains from multi-layer tokenization."
  - [section 4.1] Explains BEATs-RVQ generates multi-layer tokens to pack more semantic information than BEATs-AT (single-layer).
  - [section 5.4] Table 3 shows performance increasing with the number of RVQ layers (N=1 to N=16), validating that residuals carry useful signal.
  - [corpus] *DUO-TOK* utilizes dual codebooks to separate vocals and accompaniment, corroborating the utility of hierarchical or multi-track tokenization for complex audio.

### Mechanism 3
- **Claim:** Semantic richness in discrete tokens reduces dependency on global auxiliary embeddings (like CLAP).
- **Mechanism:** Previous methods relied heavily on a global CLAP embedding to provide the "semantic gist" because their discrete tokens (EnCodec) were acoustically rich but semantically poor. By upgrading the discrete tokens to be semantically rich (BEATs-RVQ), the model retains caption quality even if the global embedding is ablated, indicating the tokens themselves now carry the semantic load.
- **Core assumption:** The CLAP embedding in prior works was compensating for the semantic poverty of EnCodec tokens.
- **Evidence anchors:**
  - [section 5.3] Table 2 shows that removing the CLAP embedding drops EnCLAP SPIDEr by -15.6, but only drops CLAP-ART (BEATs-RVQ) by -2.2.
  - [abstract] Highlights that CLAP-ART utilizes "semantic-rich and discrete" tokens derived from AR.
  - [corpus] *MAGIC-Enhanced Keyword Prompting* and *Auditory Intelligence* emphasize the importance of semantic grounding in audio tasks, supporting the move toward features that directly encode meaning.

## Foundational Learning

- **Concept: Vector Quantization (VQ) & Residual VQ (RVQ)**
  - **Why needed here:** This is the mathematical engine that converts continuous audio features into the discrete tokens required by the language model.
  - **Quick check question:** Can you explain why calculating the *residual* between the input vector and the closest codebook entry allows the model to represent finer details with a second codebook?

- **Concept: Audio Representations (AR) vs. Codecs**
  - **Why needed here:** The paper explicitly contrasts "semantic" features (BEATs, CLAP) with "acoustic/reconstruction" features (EnCodec). Understanding this distinction is critical for feature selection.
  - **Quick check question:** Would a model trained to reconstruct the exact waveform of a dog bark prioritize the *concept* of "dog" or the *pitch* and *texture* of the sound?

- **Concept: Encoder-Decoder Language Models (BART)**
  - **Why needed here:** BART is the generator. Understanding how it accepts inputs (embeddings vs. token IDs) and processes them via attention masks is vital for the "Architecture Onboarding."
  - **Quick check question:** In BART, does the decoder attend to the encoder's output, the input embeddings directly, or both?

## Architecture Onboarding

- **Component map:** Audio Waveform -> CLAP Encoder + BEATs Encoder -> ART (RVQ) -> Concatenation with CLAP emb -> BART -> Text Caption
- **Critical path:** The **RVQ Lookup** is the critical novel path. You must pre-compute codebooks via k-means on the target dataset's BEATs features *before* training the captioning model. If these codebooks do not cover the feature space well, the discrete tokens will be "noisy," degrading BART's performance.
- **Design tradeoffs:**
  - **BEATs-AT vs. BEATs-RVQ:** BEATs-AT is off-the-shelf (single layer), easier to implement. BEATs-RVQ (multi-layer) requires preprocessing (k-means) and more hyperparameter tuning (N-layers, codebook size) but yields higher performance (+1.6 SPIDEr on AudioCaps).
  - **Codebook Size (1024) vs. Layers (16):** The paper fixes codebook size at 1024. Increasing layers adds capacity; decreasing them risks semantic loss.
- **Failure signatures:**
  - **Semantic Mismatch:** Using tokenizers trained on speech (e.g., HuBERT, SpeechTokenizer) results in no performance gain (Table 1), as speech phonemes do not map to general sound events (e.g., "car engine").
  - **Reconstruction-semantic Gap:** Using EnCodec tokens results in a heavy reliance on the CLAP embedding; if the CLAP branch fails or is ablated, performance collapses (Table 2).
- **First 3 experiments:**
  1. **Sanity Check (Token Type):** Run the baseline (EnCLAP) vs. CLAP-ART (BEATs-RVQ) on a validation split to confirm the "semantic token" hypothesis holds for your specific audio domain.
  2. **Ablation (Global vs. Local):** Disable the CLAP embedding ($e_{clap}$) to verify that your ART tokens are indeed capturing sufficient semantic information (mimicking Table 2).
  3. **Scaling (RVQ Depth):** Vary the number of RVQ layers (e.g., N=1, 4, 8, 16) to find the saturation point where more layers no longer improve SPIDEr scores, ensuring you aren't wasting compute.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does integrating CLAP-ART with Large Language Models (LLMs) and post-processing strategies impact overall captioning performance?
- Basis in paper: [explicit] The conclusion states, "Future work includes investigating the integration of LLM-based methodologies and post-processing strategies, such as reranking [31] and CLAP-based text decoding [32]."
- Why unresolved: The current experiments utilize BART (base and large), but the compatibility of ART tokens with larger decoder-only LLMs or specialized decoding techniques remains untested.
- What evidence would resolve it: Experimental results replacing BART with modern LLM architectures (e.g., LLaMA, OPT) and evaluating the impact of reranking or CLAP-score optimization on the generated captions.

### Open Question 2
- Question: Does preserving the time-frequency structure of audio features during vector quantization yield better semantic representation than the flattened sequence approach used in BEATs-RVQ?
- Basis in paper: [inferred] Section 4.1 notes that BEATs-RVQ feeds features into RVQ "without reshaping them to reflect the original time-frequency structure," but does not justify why discarding this structure is superior or neutral for semantic tasks.
- Why unresolved: The paper does not ablate the input format of the features; it is unclear if the flattened vector sequence loses critical spatial relationships that could aid the language model.
- What evidence would resolve it: An ablation study comparing the performance of flattened vector inputs against inputs structured to preserve time and frequency dimensions during the tokenization process.

### Open Question 3
- Question: Can a universal codebook trained on large-scale external data perform comparably to the dataset-specific k-means codebooks used in the proposed BEATs-RVQ?
- Basis in paper: [inferred] Section 4.1 describes obtaining codebooks via k-means clustering on the "training dataset," which implies a potential limitation regarding generalizability or the need to retrain the tokenizer for new domains.
- Why unresolved: The methodology relies on target-dataset-specific codebooks; it is unknown if the tokens generalize well without re-clustering or if a fixed, universal codebook would sacrifice performance.
- What evidence would resolve it: A comparison of AAC performance using codebooks derived specifically from AudioCaps/Clotho versus codebooks derived from a massive, diverse dataset like AudioSet without retraining.

## Limitations

- **Data Dependency:** CLAP-ART's performance hinges on the quality of the RVQ codebooks trained on the specific AudioCaps dataset. Codebooks trained on different domains may not capture the semantic richness needed for general sound captioning.
- **Discretization Loss:** The vector quantization process inherently introduces approximation error. While RVQ mitigates this by encoding residuals, extremely complex or novel audio scenes not well-represented in the codebook may still lose critical detail.
- **Model Scale:** The reported gains (+1.5 SPIDEr on AudioCaps, +1.3 on Clotho) are modest. Whether this improvement is worth the added complexity compared to simpler methods is an open question.

## Confidence

- **High Confidence:** The superiority of semantically rich discrete tokens (BEATs-RVQ) over reconstruction-based codecs (EnCodec) is strongly supported. The ablation study (Table 2) shows CLAP-ART retains performance when the global CLAP embedding is removed, while EnCLAP fails catastrophically.
- **Medium Confidence:** The benefit of multi-layer RVQ over single-layer quantization is evidenced (Table 3), but the exact point of diminishing returns and the optimal codebook size are not thoroughly explored.
- **Low Confidence:** The specific choice of BEATs_iter3+ features over other semantic audio encoders is not rigorously justified. While BEATs is shown to work, the paper does not prove it is the best option.

## Next Checks

1. **Generalization Test:** Evaluate CLAP-ART on a held-out domain (e.g., FSD50K or a custom environmental sound set) to verify that the RVQ codebooks trained on AudioCaps generalize to novel audio scenes.
2. **RVQ Capacity Sweep:** Systematically vary the number of RVQ layers (N=1, 2, 4, 8, 16) and codebook size (256, 512, 1024) on a validation split to find the configuration that maximizes SPIDEr without unnecessary computational cost.
3. **Alternative Feature Encoder:** Replace BEATs with another semantic audio encoder (e.g., PANNs or OpenL3) and re-train the ART tokenizer and captioning model. Compare the resulting SPIDEr scores to assess whether BEATs is the best choice for this task.