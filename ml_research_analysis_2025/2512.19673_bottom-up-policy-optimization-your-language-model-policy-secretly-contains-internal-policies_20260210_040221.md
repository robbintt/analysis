---
ver: rpa2
title: 'Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains
  Internal Policies'
arxiv_id: '2512.19673'
source_url: https://arxiv.org/abs/2512.19673
tags:
- policy
- layer
- internal
- entropy
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the internal policy structures of large
  language models (LLMs) during reinforcement learning by decomposing the overall
  policy into Internal Layer Policies and Internal Modular Policies. Entropy analysis
  reveals universal patterns: early layers exhibit high-entropy exploration while
  top layers converge to deterministic refinement, with Qwen models showing progressive,
  human-like reasoning compared to Llama''s abrupt final-layer convergence.'
---

# Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies

## Quick Facts
- **arXiv ID**: 2512.19673
- **Source URL**: https://arxiv.org/abs/2512.19673
- **Reference count**: 40
- **Primary result**: BuPO improves AIME24 by 4.69 points and AIME25 by 2.30 points over GRPO with Qwen3-4B

## Executive Summary
This paper investigates the internal policy structures of large language models during reinforcement learning by decomposing the overall policy into Internal Layer Policies and Internal Modular Policies. Entropy analysis reveals universal patterns where early layers exhibit high-entropy exploration while top layers converge to deterministic refinement, with Qwen models showing progressive reasoning compared to Llama's abrupt final-layer convergence. The authors propose Bottom-up Policy Optimization (BuPO), which optimizes internal layer policies in early training stages to establish stronger reasoning foundations. Extensive experiments on MATH, AMC23, AIME24, and AIME25 benchmarks demonstrate BuPO's effectiveness, achieving significant improvements over standard approaches.

## Method Summary
The paper proposes Bottom-up Policy Optimization (BuPO), which optimizes internal layer policies during early training stages rather than only the final output policy. The method decomposes the overall policy into Internal Layer Policies and Internal Modular Policies, enabling optimization of reasoning foundations before fine-tuning the final policy. BuPO leverages entropy analysis to identify universal patterns in layer behavior, where early layers explore while top layers refine, and uses this understanding to structure the optimization process. The approach forces lower layers to capture high-level reasoning representations early, providing a more robust foundation for subsequent learning.

## Key Results
- BuPO achieves 4.69 point improvement on AIME24 benchmark over GRPO with Qwen3-4B
- BuPO achieves 2.30 point improvement on AIME25 benchmark over GRPO with Qwen3-4B
- Entropy analysis reveals universal patterns: early layers show high-entropy exploration while top layers converge to deterministic refinement
- Qwen models demonstrate progressive, human-like reasoning compared to Llama's abrupt final-layer convergence

## Why This Works (Mechanism)
BuPO works by recognizing that LLMs contain internal policies that can be optimized independently, rather than treating the model as a monolithic policy. By optimizing internal layer policies early in training, the method establishes strong reasoning foundations before final policy refinement. The entropy analysis shows that different layers serve different functions - early layers explore while top layers refine - and BuPO leverages this by optimizing the exploration phase first. This creates a more robust feature representation in lower layers that captures high-level reasoning patterns, which then supports better final policy performance. The approach effectively restructures the learning trajectory to prioritize reasoning quality over immediate reward optimization.

## Foundational Learning
- **Reinforcement Learning with Language Models**: Understanding how RLHF/GRPO works with LLMs is essential for grasping why internal policy optimization matters - quick check: compare standard RLHF vs BuPO optimization targets
- **Entropy-based Policy Analysis**: The method relies on analyzing entropy across layers to identify functional roles - quick check: verify entropy patterns across different model families
- **Layer-wise Feature Decomposition**: Understanding how different layers contribute to overall reasoning is crucial - quick check: examine layer ablation studies
- **Policy Decomposition Theory**: The concept that overall policy can be split into internal layer policies is fundamental - quick check: validate decomposition through controlled experiments
- **Reasoning Foundation Building**: The idea that early optimization establishes better reasoning foundations is central - quick check: compare reasoning quality at different training stages

## Architecture Onboarding

**Component Map**: Input -> Early Layer Optimization (High Entropy) -> Middle Layer Refinement -> Top Layer Convergence (Low Entropy) -> Final Policy Output

**Critical Path**: The critical path is establishing strong lower-layer reasoning representations before top-layer refinement, as this determines the quality of features available for final policy optimization.

**Design Tradeoffs**: The main tradeoff is between exploration (high entropy early layers) and exploitation (low entropy top layers). BuPO prioritizes early exploration to build better foundations, potentially at the cost of slower initial convergence but with better final performance.

**Failure Signatures**: Models may fail if internal optimization is applied too late (missing the foundation-building phase), if entropy patterns don't match expected universal patterns, or if the decomposition into internal policies is incorrect.

**First Experiments**:
1. Verify entropy patterns across different model architectures to confirm universal behavior
2. Test layer-wise ablation to identify critical layers for reasoning performance
3. Compare BuPO optimization timing (early vs late) to establish optimal training schedule

## Open Questions the Paper Calls Out
None

## Limitations
- Results primarily validated on math and competition-style reasoning tasks, limiting generalization claims
- The causal mechanism linking internal optimization to feature refinement remains partially theoretical
- Heavy reliance on synthetic reasoning tasks without comprehensive real-world validation

## Confidence
- **Universal entropy patterns across architectures**: Medium - primarily tested on Qwen and Llama models
- **BuPO effectiveness on benchmarks**: Medium - substantial improvements but limited domain coverage
- **Mechanism of internal optimization**: Medium - theoretical grounding with limited empirical causal evidence
- **Cross-architecture generalization**: Low - insufficient validation across diverse model families

## Next Checks
1. Cross-architecture validation testing BuPO across diverse model families beyond Qwen and Llama, including transformer variants with different layer configurations
2. Domain generalization experiments applying the same methodology to non-mathematical reasoning tasks such as code generation, creative writing, or scientific reasoning to verify universality of internal policy structures
3. Extended ablation studies isolating the contribution of each internal optimization component to establish which aspects of BuPO are essential versus supplementary for observed performance gains