---
ver: rpa2
title: 'Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond
  in Speed'
arxiv_id: '2512.14067'
source_url: https://arxiv.org/abs/2512.14067
tags:
- block
- training
- attention
- token
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically explores how to convert pretrained autoregressive
  (AR) language models into diffusion language models (dLMs) that achieve faster generation
  while retaining strong accuracy. The key insight is that with an appropriate training
  scheme in terms of attention patterns and objectives, pretrained AR models can be
  converted into faster dLMs that support parallel decoding with KV cache at low training
  cost.
---

# Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed

## Quick Facts
- arXiv ID: 2512.14067
- Source URL: https://arxiv.org/abs/2512.14067
- Authors: Yonggan Fu, Lexington Whalen, Zhifan Ye, Xin Dong, Shizhe Diao, Jingyu Liu, Chengyue Wu, Hao Zhang, Enze Xie, Song Han, Maksim Khadkevich, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov
- Reference count: 40
- Primary result: Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively

## Executive Summary
This work systematically explores how to convert pretrained autoregressive (AR) language models into diffusion language models (dLMs) that achieve faster generation while retaining strong accuracy. The key insight is that with an appropriate training scheme in terms of attention patterns and objectives, pretrained AR models can be converted into faster dLMs that support parallel decoding with KV cache at low training cost. The authors identify limitations in the attention patterns and objectives of existing AR-to-dLM methods and propose a continuous pretraining scheme featuring a block-wise attention pattern and position-dependent token masking.

## Method Summary
The Efficient-DLM conversion method involves three key components: (1) a block-wise causal attention pattern where blocks attend bidirectionally within themselves but only to clean context from previous blocks, (2) position-dependent token masking that assigns higher masking probabilities to later tokens during training to mimic test-time behavior, and (3) an extended continuous pretraining procedure (50B-500B tokens) that refines likelihood estimation for better parallel decoding. The method builds on pretrained AR models (Qwen2.5 1.5B, Qwen3 4B/8B) and trains with a masked denoising objective, achieving faster generation through parallel decoding while maintaining or improving accuracy compared to both the original AR models and existing dLMs.

## Key Results
- Efficient-DLM 8B achieves +5.4% higher accuracy with 4.5x higher throughput compared to Dream 7B
- Efficient-DLM 8B achieves +2.7% higher accuracy with 2.7x higher throughput compared to Qwen3 4B
- The method enables parallel decoding with KV caching, a capability that standard dLMs struggle to achieve

## Why This Works (Mechanism)

### Mechanism 1
Block-wise causal attention minimizes weight drift during AR-to-dLM conversion compared to fully bidirectional attention. Fully bidirectional attention forces pretrained weights to adapt to a global visibility pattern, causing large divergence from the original AR initialization (which is strictly causal). Block-wise attention retains causality *between* blocks while allowing bidirectional visibility *within* blocks. This intermediate structure preserves the "block-wise causality" of the pretrained model, resulting in smaller weight changes in Attention and FFN layers.

### Mechanism 2
Position-dependent token masking aligns training noise distributions with test-time inference behavior. Standard uniform masking assumes all token positions are equally likely to be "denoised" at any step. However, the authors observe that dLMs exhibit a "left-to-right" tendency during confidence-based sampling (tokens at the start of a block are resolved first). By assigning higher masking probabilities to later tokens during training (mimicking the harder cases seen at inference), the model closes the train-test distribution gap.

### Mechanism 3
Extended continuous pretraining improves the reliability of parallel decoding via better likelihood estimation. Parallel decoding relies on model confidence (likelihood) to decide which tokens to generate simultaneously. The authors posit that while short training (approx. 10B tokens) recovers task accuracy, longer training (100B+ tokens) acts as a refinement process for the model's confidence estimates, enabling more aggressive parallelism (higher Tokens Per Forward) without accuracy collapse.

## Foundational Learning

- **KV Caching in Autoregressive vs. Diffusion Models**
  - Why needed here: Standard dLMs often fail to use KV caching because bidirectional attention requires recomputing keys/values for the entire sequence whenever a single token changes. This mechanism explains why the "block-wise" design is critical for speed.
  - Quick check question: Why does fully bidirectional attention prevent the reuse of cached Key-Value states?

- **Masked Denoising Objective (Discrete Diffusion)**
  - Why needed here: The paper optimizes a specific loss $\mathcal{L}(\theta)$ based on predicting masked tokens. Understanding this is necessary to grasp why "clean context" and "position-dependent masking" change the optimization landscape.
  - Quick check question: In the proposed objective, what specifically does the model predict for a masked token position?

- **Train-Test Distribution Gap**
  - Why needed here: The paper identifies a specific mismatch: training uses uniform masking, while testing uses confidence-based (left-to-right) decoding. This concept is central to the "Position-dependent Token Masking" contribution.
  - Quick check question: How does the distribution of mask tokens during training differ from the distribution of decoded tokens during inference?

## Architecture Onboarding

- **Component map:**
  - Input: Pretrained Autoregressive (AR) Transformer (e.g., Qwen)
  - Attention Masking: Block-wise causal mask (Global causal between blocks, local bidirectional within blocks)
  - Context Injection: Concatenation of Noisy Block + Clean Context History
  - Masking Strategy: Position-dependent probability function ($w_i(t)$) replacing uniform masking
  - Objective: Masked token prediction (no token shift)

- **Critical path:**
  1. **Initialization:** Load AR weights
  2. **Attention Override:** Implement the custom block-wise attention mask (Crucial: ensure blocks attend only to clean history)
  3. **Masking:** Apply exponential positional weights to determine which tokens to mask in the current block
  4. **Training:** Continuous pretraining on 10B-500B tokens (depending on aggressiveness of parallel decoding required)

- **Design tradeoffs:**
  - **Block Size:**
    - *Small blocks:* Insufficient context for accurate denoising
    - *Large blocks:* Excessive corruption/noise leads to larger weight drift and potential instability
    - *Sweet spot:* Paper finds 16 for 1.5B model, 64 for larger models
  - **Token Shift:**
    - *Preserve:* Previous works suggested keeping the AR "next token" shift
    - *Remove:* Paper finds removing it (predicting the mask directly) is easier and more accurate

- **Failure signatures:**
  - **Catastrophic Accuracy Drop:** Likely caused by using "noisy context" (corrupted history) instead of "clean context" during block-wise training
  - **No Speedup:** Forgetting to implement the KV-cache logic for the block-wise structure, effectively treating it as a standard bidirectional dLM
  - **Poor Parallel Generation:** Training for insufficient tokens (e.g., <10B), resulting in uncalibrated confidence scores that lead to errors when TPF > 1

- **First 3 experiments:**
  1. **Attention Pattern Validation:** Train a small adapter (or LoRA) on a toy dataset comparing Bidirectional vs. Block-wise vs. Block-wise with Clean Context. Verify that weight drift is lowest in the final configuration.
  2. **Block Size Ablation:** Run a sweep of training block sizes (e.g., 4, 8, 16, 32) on a validation task (e.g., GSM8K) to identify the saturation point where context corruption outweighs context richness.
  3. **Masking Strategy Calibration:** Compare uniform masking vs. position-dependent masking ($\lambda=0.1$) specifically measuring the "Tokens Per Forward" (TPF) achievable while maintaining 95% of baseline accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptive block sizes dynamically adjust during generation to maintain dLM efficiency advantages at larger batch sizes? The authors state that efficiency benefits of dLMs diminish at larger batch sizes (e.g., Efficient-DLM 8B falls behind Qwen3 1.7B at batch size 32) and list "adaptive block sizes" as a potential workaround they "leave for future work."

- **Open Question 2:** Can the position-dependent token masking strategy be automated or learned rather than manually tuned via the λ hyperparameter? The authors state: "The key contribution of our work is to highlight this design factor, and we hope it can inspire more advanced and automated schemes in the future."

- **Open Question 3:** Why does removing the token shift—contrary to prior work—improve AR-to-dLM conversion, and does this finding generalize across different base architectures? The paper finds token shift "unnecessary and potentially harmful," contradicting prior work [10, 4], but only experiments on Qwen models.

## Limitations
- The method requires substantial continuous pretraining (50B-500B tokens), raising practical deployment concerns in resource-constrained settings
- The optimal hyperparameters (block size, λ value) may be task-specific, but the paper only reports results for specific configurations
- The fundamental tension between bidirectional attention and KV caching in dLMs remains unresolved for larger architectures

## Confidence

**High Confidence:** The empirical results demonstrating speed-accuracy trade-offs are well-supported by the experimental data. The block-wise attention pattern's superiority over fully bidirectional attention is convincingly demonstrated through weight drift visualizations and accuracy metrics.

**Medium Confidence:** The mechanism explanations for why position-dependent masking works are plausible but rely on behavioral observations rather than causal analysis. The claim that extended pretraining (100B+ tokens) significantly improves likelihood estimation is supported by results but lacks ablation studies showing the marginal benefit.

**Low Confidence:** The generalization of these findings to other model families and tasks remains untested. The interaction between these architectural choices and different decoding strategies is not thoroughly explored.

## Next Checks
1. **Cross-Architecture Generalization Test:** Apply the Efficient-DLM conversion to a diverse set of pretrained AR models (e.g., Llama, Mistral) across different scales (3B, 7B, 13B) and evaluate whether the same hyperparameters (block size 64, λ=0.1) maintain performance.

2. **Longer Sequence Length Evaluation:** Test the block-wise attention pattern on sequences exceeding 4K tokens to identify whether the block size (64) becomes a bottleneck for long-context reasoning tasks.

3. **Decoupling Clean Context vs. Masking Ablation:** Conduct an ablation study that independently varies clean context availability and masking strategy (uniform vs. position-dependent) to isolate their individual contributions to the final performance.