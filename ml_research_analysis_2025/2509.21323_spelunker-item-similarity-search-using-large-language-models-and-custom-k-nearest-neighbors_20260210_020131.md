---
ver: rpa2
title: 'SPELUNKER: Item Similarity Search Using Large Language Models and Custom K-Nearest
  Neighbors'
arxiv_id: '2509.21323'
source_url: https://arxiv.org/abs/2509.21323
tags:
- search
- language
- similarity
- system
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid system for intuitive item similarity
  search that combines a Large Language Model (LLM) with a custom K-Nearest Neighbors
  (KNN) algorithm. Unlike black-box dense vector systems, this architecture provides
  superior interpretability by first using an LLM to convert natural language queries
  into structured, attribute-based searches.
---

# SPELUNKER: Item Similarity Search Using Large Language Models and Custom K-Nearest Neighbors

## Quick Facts
- arXiv ID: 2509.21323
- Source URL: https://arxiv.org/abs/2509.21323
- Reference count: 17
- This paper presents a hybrid system for intuitive item similarity search that combines a Large Language Model (LLM) with a custom K-Nearest Neighbors (KNN) algorithm.

## Executive Summary
This paper introduces SPELUNKER, a novel hybrid system that bridges natural language queries with structured item similarity search by combining LLM interpretation with custom KNN algorithms. Unlike traditional black-box dense vector approaches, SPELUNKER first uses an LLM to extract structured attributes from user queries, then employs a custom KNN with heterogeneous distance metrics for interpretable search results. The system was evaluated on a dataset of 500 wine reviews, demonstrating superior performance with an F1-score of 0.9779 for information extraction and statistically significant recall improvements when LLM re-ranking was applied.

## Method Summary
The SPELUNKER architecture employs a two-stage approach: first, a Large Language Model converts natural language queries into structured, attribute-based search specifications; second, a custom K-Nearest Neighbors algorithm with BallTree search strategy computes similarities using heterogeneous distance metrics that preserve distinct data types. This design provides superior interpretability compared to dense vector embeddings by maintaining clear mappings between user intent and search results. The LLM's role is specifically limited to query interpretation and re-ranking, while the KNN handles the core similarity computation, creating a transparent pipeline where each component's contribution can be understood and potentially optimized independently.

## Key Results
- LLM achieved F1-score of 0.9779 in information extraction from wine reviews
- Jaro string similarity of 0.9321 demonstrated high fidelity in attribute extraction
- LLM-based re-ranking showed statistically significant recall improvement (p=0.013) in KNN results

## Why This Works (Mechanism)
The system works by addressing the fundamental disconnect between human language and machine-understandable item representations. Natural language queries contain nuanced preferences that are difficult to capture with dense vector embeddings alone. By using an LLM to first parse these queries into structured attributes, the system creates a bridge that allows traditional similarity algorithms to work effectively on semantically rich queries. The heterogeneous distance metric in the KNN preserves the distinct characteristics of different attribute types (e.g., categorical vs. numerical), preventing the loss of interpretability that occurs when all features are compressed into a single vector space. The LLM re-ranking then adds a semantic layer that can identify subtle relevance patterns beyond simple attribute matching.

## Foundational Learning
- **LLM Information Extraction**: Why needed - To convert unstructured natural language queries into structured attribute-based search specifications; Quick check - F1-score > 0.95 indicates reliable attribute extraction
- **Heterogeneous Distance Metrics**: Why needed - To preserve distinct data types (categorical, numerical, textual) during similarity computation; Quick check - BallTree search performance metrics remain stable across attribute type distributions
- **KNN with BallTree Search**: Why needed - To enable interpretable similarity search with efficient nearest-neighbor computation; Quick check - Query response time < 1 second for datasets of 10,000+ items
- **LLM Re-ranking**: Why needed - To add semantic refinement beyond attribute-based similarity; Quick check - Recall@10 improvement of >5% when re-ranking is applied
- **Interpretability in Search Systems**: Why needed - To provide transparency and explainability in recommendation results; Quick check - Users can trace reasoning from query to results through intermediate representations

## Architecture Onboarding

**Component Map**: User Query -> LLM Parser -> Structured Query -> Custom KNN -> KNN Results -> LLM Re-ranker -> Final Results

**Critical Path**: The critical path for query processing flows through LLM parsing, KNN computation, and LLM re-ranking. Each stage must complete before the next begins, with the KNN computation typically being the bottleneck due to its computational complexity.

**Design Tradeoffs**: The system trades computational efficiency for interpretability and transparency. While dense vector approaches can be faster, SPELUNKER's multi-stage pipeline allows users to understand exactly why items are recommended. The heterogeneous distance metric adds complexity but preserves meaningful distinctions between attribute types that would be lost in homogeneous vector spaces.

**Failure Signatures**: System failures manifest as either poor attribute extraction (low F1-score from LLM) or inadequate similarity ranking (poor recall metrics). LLM failures typically show up as irrelevant attributes being extracted or relevant ones being missed, while KNN failures appear as items with similar attributes being ranked lower than less relevant alternatives.

**First 3 Experiments**:
1. Test LLM attribute extraction on 50 diverse wine review queries, measuring F1-score and Jaro similarity against ground truth
2. Evaluate KNN performance with heterogeneous distance metrics on a 1,000-item subset, measuring recall@10 and query response time
3. Implement LLM re-ranking on KNN results and measure recall improvement across 20 query types with statistical significance testing

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 500 wine reviews severely limits generalizability to larger, more diverse product domains
- Reliance on proxy metrics (F1-score, Jaro similarity) rather than direct user satisfaction measures
- Limited statistical validation with only one augmentation test claiming p=0.013 significance

## Confidence

**High Confidence**: The technical architecture combining LLM with KNN is well-described and methodologically sound. The heterogeneous distance metric implementation appears robust.

**Medium Confidence**: The evaluation results are internally consistent but limited in scope. The F1-score of 0.9779 for information extraction appears high but lacks comparison to baseline systems.

**Low Confidence**: Claims about interpretability and transparency are qualitative and lack systematic validation. The "statistically significant improvement" claim is based on limited testing.

## Next Checks
1. **Cross-Domain Validation**: Test the system on at least three distinct product categories (e.g., electronics, books, clothing) with minimum 5,000 items each to assess generalizability beyond wine reviews.
2. **User Study Implementation**: Conduct a randomized controlled trial with 50+ participants comparing user satisfaction and task completion rates between SPELUNKER and standard dense vector similarity search systems.
3. **Scalability Benchmarking**: Evaluate system performance and accuracy degradation as dataset size increases from 500 to 50,000+ items, measuring both query response time and recall@10 metrics.