---
ver: rpa2
title: 'SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular
  Synthetic Data Generation via Prompt Engineering'
arxiv_id: '2508.08529'
source_url: https://arxiv.org/abs/2508.08529
tags:
- data
- privacy
- prompt
- synthetic
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SynLLM introduces a structured, prompt-based framework for generating\
  \ high-quality synthetic medical tabular data using 20 open-source large language\
  \ models, without requiring model fine-tuning. By encoding schema metadata, statistical\
  \ summaries, and clinical rules into four distinct prompt types\u2014from example-driven\
  \ to rule-based\u2014the framework achieves strong balance between data fidelity,\
  \ clinical plausibility, and privacy preservation."
---

# SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular Synthetic Data Generation via Prompt Engineering

## Quick Facts
- **arXiv ID:** 2508.08529
- **Source URL:** https://arxiv.org/abs/2508.08529
- **Reference count:** 40
- **Primary result:** SynLLM achieves strong balance between data fidelity, clinical plausibility, and privacy preservation using prompt engineering without model fine-tuning.

## Executive Summary
SynLLM introduces a structured, prompt-based framework for generating high-quality synthetic medical tabular data using 20 open-source large language models, without requiring model fine-tuning. By encoding schema metadata, statistical summaries, and clinical rules into four distinct prompt types—from example-driven to rule-based—the framework achieves strong balance between data fidelity, clinical plausibility, and privacy preservation. Evaluation across Diabetes, Stroke, and Cirrhosis datasets shows that rule-based prompts yield the best privacy-quality harmonic scores, while maintaining high statistical fidelity (e.g., Wasserstein distance near real data) and robust machine learning utility in classification tasks (accuracy >0.9, AUC-ROC >0.85).

## Method Summary
The framework uses zero-shot LLM inference with four prompt types (SEED_EX → FEATDESC → STATGUIDE → CLIN_RULE) to generate synthetic medical tabular data. Schema metadata, statistical summaries, and clinical rules are encoded into prompts, which are fed to 20 open-source LLMs (e.g., Mistral, Zephyr, GPT-2 variants) at temperature=0.7 and top-p=0.9. Generated JSON records are validated against clinical rules and parsed for data typing. The method avoids fine-tuning by leveraging the LLMs' instruction-following capabilities and is evaluated on three public datasets (Diabetes, Stroke, Cirrhosis) using metrics including Wasserstein distance, nearest-neighbor privacy scores, and TSTR/TRTS ML utility.

## Key Results
- Rule-based prompts (CLIN_RULE) achieve the best privacy-quality harmonic scores across all datasets.
- Statistical fidelity is maintained with Wasserstein distances close to real data, especially for numerical features.
- Machine learning utility remains high: TSTR accuracy >0.9, AUC-ROC >0.85 for classification tasks.
- Prompt structure is the dominant factor influencing both synthetic data quality and privacy risk.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured prompt encoding acts as a surrogate for model fine-tuning, constraining the output space to align with clinical schemas.
- **Mechanism:** By explicitly defining feature types, ranges, and dependencies in the prompt (e.g., FEATDESC or STATGUIDE), the LLM conditions its token generation probabilities on these constraints. This creates a "soft" enforcement of data types and logical bounds during inference, reducing syntax errors and out-of-distribution values.
- **Core assumption:** The LLM has sufficient instruction-following capability and context window utilization to adhere to the provided schema definitions without gradient updates.
- **Evidence anchors:**
  - [abstract] "By encoding schema metadata, statistical summaries, and clinical rules into four distinct prompt types... the framework achieves strong balance between data fidelity [and] clinical plausibility."
  - [section III-C] Describes how prompts encode schema and domain knowledge to "control generation without model fine-tuning."
  - [corpus] Neighbor "Generative adversarial networks vs large language models" supports the viability of zero-shot generation via prompting over training-based methods.
- **Break condition:** If the schema complexity exceeds the model's context window or the instruction-tuning is weak (e.g., base models like GPT-2), the model ignores constraints, leading to malformed JSON or invalid feature types.

### Mechanism 2
- **Claim:** Removing real data examples from prompts (Rule-Based) reduces memorization risk while maintaining utility through explicit statistical guidance.
- **Mechanism:** The CLIN RULE prompt eliminates seed records, forcing the model to rely on generalized medical knowledge and provided statistical parameters (mean, std) rather than copying patterns from specific examples. This increases the "distance" between synthetic and real records (privacy) while preserving marginal distributions via explicit instruction.
- **Core assumption:** The LLM possesses robust internal representations of medical concepts (e.g., "HbA1c implies diabetes") to generate plausible records without few-shot examples.
- **Evidence anchors:**
  - [abstract] "Rule-based prompts yield the best privacy-quality harmonic scores."
  - [section IV-C2] "CLIN RULE consistently yields high harmonic scores... includes no real data examples—only domain rules and metadata."
  - [corpus] Weak direct evidence for the specific "rule-vs-example" tradeoff in neighbors; this appears to be a specific contribution of this paper.
- **Break condition:** If the statistical instructions are ambiguous, the model hallucinates correlations or fails to capture rare but critical edge cases (under-coverage).

### Mechanism 3
- **Claim:** Instruction-tuning quality determines the efficiency of constraint satisfaction.
- **Mechanism:** Models fine-tuned for dialogue/instruction (e.g., Zephyr, OpenChat) outperform base models (GPT-2) because they are optimized to follow complex, multi-part instructions (schema + stats + rules) accurately. This reduces the post-processing burden and invalid generation rate.
- **Core assumption:** The relationship between model size and tabular data quality is secondary to the alignment method (Instruct/Chat vs. Base).
- **Evidence anchors:**
  - [section IV-C] "Instruction-tuned models such as Zephyr 7B and OpenChat 7B demonstrate balanced utility... while GPT-2 variants perform surprisingly well in privacy [but] show greater variability."
  - [table III] Highlights the distinction between Base (Ba), Instruct (In), and Chat (Ch) fine-tuning types.
  - [corpus] "Private Text Generation by Seeding Large Language Model Prompts" aligns with the need for well-aligned models to handle sensitive seeding effectively.
- **Break condition:** If the model is over-aligned for conversational fluency at the expense of structural rigidity, it may add conversational fillers that break the required JSON format.

## Foundational Learning

- **Concept:** Train-on-Synthetic, Test-on-Real (TSTR)
  - **Why needed here:** This is the primary validation method to prove synthetic data utility. If a model trained on SynLLM data performs well on real held-out data, the synthetic data has captured the signal distribution.
  - **Quick check question:** If TSTR accuracy is high but TRTS (Train-on-Real, Test-on-Synthetic) accuracy is low, what does this imply about the synthetic data's distribution? (Answer: It likely covers the decision boundary well but may not cover the full diversity of the real data distribution).

- **Concept:** Nearest Neighbor Distance Ratio (Privacy)
  - **Why needed here:** Used to measure the "closeness" of synthetic records to real records. Unlike Differential Privacy, this is an empirical metric.
  - **Quick check question:** Why is a high Nearest Neighbor Distance considered better for privacy in this context? (Answer: It implies synthetic records are not exact copies or trivial transformations of real individuals).

- **Concept:** Prompt Taxonomy (Seed vs. Rule)
  - **Why needed here:** Understanding the trade-off spectrum is critical. Seed prompts offer high fidelity but risk privacy; Rule prompts offer high privacy but rely heavily on model knowledge.
  - **Quick check question:** Which prompt type would you select for a use case requiring strict HIPAA compliance where no real data can leave the secure enclave, even for prompting? (Answer: CLIN RULE, as it requires only metadata/logic, not raw records).

## Architecture Onboarding

- **Component map:** Schema Extractor -> Prompt Builder -> LLM Inference Engine -> Validator/Parser -> Evaluator
- **Critical path:** The **Prompt Builder** to **Validator** loop. If the prompt is poorly constructed (e.g., ambiguous stats), the LLM generates garbage, and the Validator discards high volumes of records, killing efficiency.
- **Design tradeoffs:**
  - **Seed vs. No-Seed:** Using SEED_EX (examples) boosts statistical fidelity (lower Wasserstein distance) but increases identifiability risk. CLIN RULE sacrifices some tail-distribution accuracy for maximum privacy.
  - **Model Size vs. Speed:** Larger models (Nous Hermes 34B) offer higher quality but significantly lower throughput (Global Fidelity Index vs. Speed trade-off).
- **Failure signatures:**
  - **Schema Drift:** The LLM invents columns not in the definition (hallucination).
  - **Logic Collapse:** The model ignores conditional rules (e.g., "If A then B") in the CLIN RULE prompt, often seen in weaker base models like GPT-2.
  - **Format Rupture:** Generating conversational text instead of JSON, requiring robust parsing/cleaning logic.
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run `SEED_EX` vs. `CLIN_RULE` on a small dataset (e.g., Cirrhosis) using a mid-range model (Mistral 7B). Compare the valid JSON parsing rate.
  2. **Privacy Boundary Test:** Generate 1000 records using `SEED_EX` and `CLIN_RULE`. Calculate the Identifiability Score (exact matches) to verify the paper's claim that CLIN RULE reduces leakage.
  3. **Statistical Fidelity Check:** Compare the Wasserstein distance of numerical features between Real and Synthetic data for `STATGUIDE`. Verify if providing explicit mean/std actually aligns the synthetic distribution closer to real data than `FEATDESC`.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SynLLM-generated datasets satisfy formal differential privacy guarantees, or do they remain vulnerable to white-box adversarial membership inference attacks?
- **Basis in paper:** [explicit] The Discussion section explicitly notes that current privacy metrics are "heuristic and empirically grounded," suggesting that future work should incorporate "formal differential privacy analysis or white-box adversarial testing to strengthen guarantees."
- **Why unresolved:** The current evaluation relies on empirical distance metrics (Nearest Neighbor Distance Ratio) and identifiability scores rather than mathematical privacy bounds or robustness against optimized attacks on the model's memory.
- **What evidence would resolve it:** A theoretical analysis proving differential privacy bounds for the prompt-based generation process, or empirical results showing resistance to state-of-the-art white-box membership inference attacks.

### Open Question 2
- **Question:** Can metric-guided or reinforcement learning-based prompt tuning automate the optimization of the privacy-utility trade-off better than static manual templates?
- **Basis in paper:** [explicit] The Conclusion identifies "adaptive prompt optimization strategies, including metric-guided or reinforcement learning-based prompt tuning" as a key direction for improving the framework.
- **Why unresolved:** The study evaluates a fixed taxonomy of four manually designed prompt types. It does not explore dynamic prompt refinement loops that could automatically adjust constraints to maximize the harmonic privacy-utility score for specific datasets.
- **What evidence would resolve it:** Experiments comparing the static CLIN_RULE prompt against an adaptive system that iteratively rewrites prompt constraints based on a reward function of statistical fidelity and privacy risk.

### Open Question 3
- **Question:** Does the zero-shot, prompt-based framework scale effectively to multimodal Electronic Health Records (EHRs) containing unstructured clinical text or imaging data?
- **Basis in paper:** [explicit] The Conclusion lists "expanding support for multimodal EHRs (e.g., clinical text, imaging)" as a future direction to enhance utility and privacy.
- **Why unresolved:** The current validation is restricted to structured tabular data (Diabetes, Stroke, Cirrhosis). It is unclear if the token limitations and schema encoding strategies can maintain high clinical consistency and fidelity when handling high-dimensional unstructured modalities.
- **What evidence would resolve it:** Application of the SynLLM framework to a multimodal dataset (e.g., MIMIC-IV) to evaluate the statistical fidelity of generated unstructured clinical notes alongside structured tabular fields.

## Limitations
- Exact synthetic record count (k) per dataset and prompt remains unspecified, impacting privacy and utility metric stability.
- Complete clinical validation rule sets for all three datasets are not provided; only partial examples are shown.
- Evaluation relies heavily on synthetic datasets from Kaggle, which may not reflect real-world clinical data complexity and imbalance.

## Confidence
- **High Confidence**: The superiority of rule-based prompts for privacy-quality trade-off is well-supported by harmonic scores and NN distance results.
- **Medium Confidence**: The claim that instruction-tuned models outperform base models is consistent but requires careful parsing of fine-tuning distinctions.
- **Low Confidence**: Generalizability to rare disease or highly imbalanced datasets, as the current experiments use moderately sized, public datasets.

## Next Checks
1. **Sensitivity Analysis**: Vary k (synthetic records) from 500 to 5000 per prompt type and measure stability of Wasserstein distance and NN privacy scores.
2. **Rule Coverage Audit**: Implement a complete clinical rule engine for each dataset and measure the proportion of synthetic records failing validation—identify if specific prompt types systematically violate certain rules.
3. **Cross-Domain Transfer Test**: Apply SynLLM prompts to a held-out, non-public medical dataset (e.g., MIMIC-III subset) to test robustness beyond the training corpus.