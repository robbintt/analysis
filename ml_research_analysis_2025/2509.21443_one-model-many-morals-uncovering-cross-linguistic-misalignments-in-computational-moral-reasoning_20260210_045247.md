---
ver: rpa2
title: 'One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational
  Moral Reasoning'
arxiv_id: '2509.21443'
source_url: https://arxiv.org/abs/2509.21443
tags:
- moral
- reasoning
- languages
- across
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines how language and culture influence
  moral reasoning in LLMs by translating two moral reasoning benchmarks into five
  diverse languages and conducting zero-shot evaluations. The analysis reveals significant
  cross-linguistic inconsistencies in moral judgments, with English consistently outperforming
  other languages.
---

# One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning

## Quick Facts
- arXiv ID: 2509.21443
- Source URL: https://arxiv.org/abs/2509.21443
- Reference count: 40
- One-line primary result: Language and culture significantly influence moral reasoning in LLMs, with English consistently outperforming other languages

## Executive Summary
This study systematically examines how language and culture influence moral reasoning in LLMs by translating two moral reasoning benchmarks into five diverse languages and conducting zero-shot evaluations. The analysis reveals significant cross-linguistic inconsistencies in moral judgments, with English consistently outperforming other languages. The research uncovers that models' reasoning patterns vary across cultures, with South Asian languages emphasizing duty in early stages while Western languages focus on outcomes later. A key finding shows that pretraining data sources, particularly psychology and education domains, significantly influence models' moral reasoning patterns.

## Method Summary
The authors translated two moral reasoning benchmarks (MoralExceptQA and ETHICS) into five languages (Hindi, Spanish, Urdu, Arabic, German) plus English using SeamlessM4T. They evaluated multiple LLMs (OLMo-2, Llama-3.1, Qwen-2.5) using zero-shot prompting with structured JSON output format. The analysis employed LLM-as-judge (Llama-3.3-70B) for framework and phase annotation, eMFD-based moral value extraction, and regression analysis to map moral foundations to decisions. For pretraining analysis, they used OLMoTrace to link model reasoning to source documents.

## Key Results
- Models consistently underperform on non-English benchmarks, with Urdu and Hindi showing highest disagreement rates
- South Asian languages cluster together showing deontological emphasis in early reasoning stages
- Pretraining data from psychology, education, and policy domains disproportionately influences moral reasoning patterns
- Models primarily abstract rather than replicate training content when making moral judgments
- Low-resource languages exhibit attenuated moral value signals and higher prediction variance

## Why This Works (Mechanism)

### Mechanism 1: Pretraining Data Source Shapes Moral Reasoning Patterns
- Claim: Models' moral reasoning draws disproportionately from specific pretraining domains rather than uniformly from the corpus.
- Mechanism: When generating moral justifications, models retrieve and abstract patterns from high-similarity training sources—particularly psychology, education, and policy texts—which provide explicit ethical frameworks that are structurally reusable.
- Core assumption: The OLMo case study findings generalize to other models with similar pretraining distributions.
- Evidence anchors:
  - [section 7] "content from psychology, education, and policy/legal domains consistently align more closely with model reasoning"
  - [section 7] Semantic similarity between model reasoning and cited sources "lies mostly between 0.1 to 0.3, indicating abstraction rather than direct memorization"
  - [corpus] Weak direct corpus support; related work on moral reasoning acquisition (arXiv:2509.24102) discusses generalization challenges but does not directly test pretraining source attribution.
- Break condition: If a model's pretraining corpus contains minimal structured ethical content (e.g., no psychology/policy domains), moral reasoning patterns may emerge from less coherent sources or show greater instability.

### Mechanism 2: Language Acts as a Contextual Cue Activating Culturally-Associated Ethical Frameworks
- Claim: Prompting in different languages shifts which ethical frameworks the model invokes, independent of scenario content.
- Mechanism: The target language prompt primes the model to activate reasoning patterns associated with that language's cultural-linguistic distribution in pretraining data. South Asian languages trigger deontological framing; Western languages trigger consequentialist framing.
- Core assumption: Framework activation reflects statistical associations in training data rather than intrinsic language properties.
- Evidence anchors:
  - [section 5] "Hindi and Urdu show strong alignment with Deontology and Divine Command Theory, whereas English and Spanish exhibit higher influence from Utilitarianism and Social Contract Theory"
  - [section 5] "Urdu and Hindi demonstrate pronounced engagement with early-stage reasoning activities such as Stakeholder Analysis and Context Evaluation, whereas Spanish and English display stronger emphasis on downstream stages like Decision Articulation"
  - [corpus] arXiv:2503.04792 identifies cross-linguistic disagreement from semantic differences but does not directly test framework activation.
- Break condition: If models are trained on culturally-balanced multilingual corpora where each language draws from diverse ethical traditions, language-specific framework clustering should weaken.

### Mechanism 3: Low-Resource Languages Exhibit Attenuated Moral Value Signals in Reasoning
- Claim: Models produce reasoning with weaker and less consistent moral value salience in low-resource languages compared to high-resource languages.
- Mechanism: Sparse representation of moral vocabulary and reasoning patterns in low-resource language training data leads to shallower activation of moral foundations during generation, resulting in lower value salience scores and higher prediction variance.
- Core assumption: The effect is primarily due to training data scarcity rather than tokenizer inefficiency or translation artifacts.
- Evidence anchors:
  - [section 4] "low-resource or culturally distinct languages" like Urdu and Hindi "consistently stand out as outliers" with higher disagreement
  - [section 5, Figure 4a] "Languages like Urdu and German display generally smaller radii overall, indicating lower average value salience"
  - [section A.3.1] Semantic shift analysis shows translations cluster closely, "eliminating translation artifacts as the primary source of variation"
  - [corpus] arXiv:2504.19759 confirms moral reasoning performance degrades in low-resource languages but does not isolate mechanism.
- Break condition: If a low-resource language has disproportionately high-quality moral reasoning content in pretraining (e.g., curated ethical corpora), value signal strength could match high-resource languages despite overall data scarcity.

## Foundational Learning

- Concept: Moral Foundations Theory (MFT) — five dimensions (Care, Fairness, Loyalty, Authority, Sanctity) that vary in salience across cultures.
  - Why needed here: The paper uses MFT to quantify how models weight different moral values across languages; understanding these foundations is prerequisite to interpreting the regression analyses.
  - Quick check question: Which foundation emphasizes respect for hierarchy and duty, and which language in the study showed the strongest positive association with it for permissibility judgments?

- Concept: Normative Ethical Frameworks — Utilitarianism, Deontology, Virtue Ethics, etc., as distinct reasoning paradigms.
  - Why needed here: The paper classifies model reasoning into ten frameworks; distinguishing outcome-based (utilitarian) from duty-based (deontological) reasoning is essential for understanding cross-linguistic framework clustering.
  - Quick check question: If a model reasons "this action is permissible because it maximizes overall wellbeing," which framework is it applying?

- Concept: Zero-Shot Evaluation — assessing model performance on tasks without task-specific training examples.
  - Why needed here: All experiments use zero-shot prompting to isolate inherent model behavior without fine-tuning confounds.
  - Quick check question: Why might zero-shot evaluation be preferred over few-shot when probing inherent moral reasoning tendencies?

## Architecture Onboarding

- Component map:
  - Input layer: Language-specific prompts (6 languages) → model receives scenario + reasoning instruction
  - Generation layer: Model produces structured output (thought_process, decision, reasoning)
  - Analysis layer: (a) eMFD-based moral value extraction per reasoning step; (b) LLM-as-judge (Llama-3.3-70B) for framework/phase annotation; (c) regression analysis mapping moral foundations → permissibility decisions
  - Tracing layer: OLMoTrace (Infinigram index) links output spans to pretraining documents

- Critical path:
  1. Translate benchmarks (MoralExceptQA, ETHICS) → target languages using SeamlessM4T
  2. Prompt models in target language with structured JSON output format
  3. Extract compliant outputs → compute F1 scores and compliance rates
  4. Annotate reasoning → moral values (eMFD), frameworks, phases (LLM-judge)
  5. Cluster languages by disagreement patterns → identify cultural/regional groupings

- Design tradeoffs:
  - Translation vs. native benchmark: Translation enables controlled comparison but may introduce artifacts (mitigated by semantic shift analysis)
  - LLM-as-judge vs. human annotation: Scalable but inherits judge model biases; paper uses Llama-3.3-70B without validation against human labels
  - Zero-shot vs. few-shot: Zero-shot reveals inherent tendencies but yields lower absolute performance; few-shot could mask cross-linguistic differences

- Failure signatures:
  - Low compliance rate: Model fails to follow JSON format (common in Urdu/Hindi for some models)
  - Language leakage: Model reasons in English when prompted in Hindi (Table 5)
  - Asymmetric judgment: Same scenario receives opposite verdicts in different languages (Figure 3)
  - Framework mismatch: Model applies utilitarian reasoning to a context where cultural norms expect deontological framing

- First 3 experiments:
  1. Reproduce semantic shift analysis: Translate 50 English scenarios to target languages, compute LaBSE embeddings, verify clustering by scenario rather than language
  2. Pilot framework annotation: Run Llama-3.3-70B judge on 20 model outputs across 2 languages, manually validate framework labels for accuracy
  3. Single-model cross-linguistic probe: Evaluate one model (e.g., Llama-3.1-8B) on MoralExceptQA across all 6 languages, plot disagreement heatmap to verify South Asian clustering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can culturally balanced moral reasoning corpora be constructed to capture authentic local norms rather than relying on translations of English-centric benchmarks?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion and the FAULT typology discussion that overcoming moral reasoning errors requires "culturally balanced moral reasoning corpora" rather than just translations (Section 8).
- Why unresolved: The current study relies on translating English datasets (MoralExceptQA, ETHICS) which risks imposing Western moral frameworks on non-Western languages, but the paper does not propose or test a methodology for creating native, culturally grounded datasets.
- What evidence would resolve it: The creation and validation of a multilingual dataset derived from native cultural sources (e.g., local literature, laws, or surveys) that shows higher alignment with human moral judgments in those languages than translated datasets.

### Open Question 2
- Question: Does the strong influence of psychology, education, and policy pretraining domains on moral reasoning generalize to other open-weight models besides OLMo-2?
- Basis in paper: [inferred] The pretraining analysis is presented as a "case study" limited to OLMo-2-32B due to the availability of tracing tools (Section 7), leaving the generalizability of this finding to other model architectures unconfirmed.
- Why unresolved: Different LLMs (e.g., Llama, Qwen) utilize vastly different pretraining corpora; it is unclear if they also rely heavily on these specific domains or if other data sources shape their moral reasoning.
- What evidence would resolve it: Applying data attribution or tracing methods (like OLMoTrace) to other models to see if similar domains (psychology/policy) correlate with moral reasoning outputs.

### Open Question 3
- Question: Which specific fine-tuning interventions successfully reduce "Asymmetric Judgments" (opposite verdicts for the same scenario) across languages?
- Basis in paper: [explicit] The paper identifies "Asymmetric Judgments" as a critical error type in the FAULT typology and calls for "targeted fine-tuning strategies" to address it, but does not experiment with solutions (Section 8).
- Why unresolved: The study is diagnostic, identifying that models exhibit conflicting moral decisions across languages, but does not test remediation methods to align these judgments.
- What evidence would resolve it: Experiments demonstrating that specific fine-tuning techniques (e.g., consistency regularization or multilingual value alignment) significantly lower the disagreement rate for translated equivalent scenarios.

## Limitations
- Heavy reliance on LLM-as-judge annotations without human validation creates potential cascading biases
- Cross-linguistic framework clustering may reflect statistical artifacts from pretraining data distribution
- Semantic shift analysis cannot completely rule out subtle translation effects on moral framing
- Low-resource language findings could be influenced by tokenizer coverage differences

## Confidence
- High confidence: Cross-linguistic performance disparities exist (empirically observed across multiple models and benchmarks)
- Medium confidence: Pretraining data sources shape moral reasoning patterns (strong evidence from OLMoTrace but limited generalizability)
- Medium confidence: Language acts as a cultural framework cue (statistically significant clustering but mechanism not fully validated)
- Low confidence: Low-resource languages inherently produce weaker moral reasoning (confounded by potential technical artifacts)

## Next Checks
1. Conduct human annotation validation on 100 randomly selected model outputs across 3 languages to verify LLM-judge accuracy
2. Test framework activation hypothesis by creating controlled prompts that explicitly mention cultural contexts in different languages
3. Evaluate models on native-language benchmarks (not translations) for Hindi and Urdu to isolate translation effects