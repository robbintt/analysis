---
ver: rpa2
title: 'Confident or Seek Stronger: Exploring Uncertainty-Based On-device LLM Routing
  From Benchmarking to Generalization'
arxiv_id: '2502.04428'
source_url: https://arxiv.org/abs/2502.04428
tags:
- routing
- data
- arxiv
- uncertainty
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive benchmark of 8 uncertainty
  quantification (UQ) methods across 14 datasets and 8 small language models (SLMs)
  to evaluate uncertainty-based routing to large language models (LLMs). The authors
  find that the alignment between uncertainty and correctness varies significantly
  across UQ methods, with trained probes, out-of-distribution probes, and perplexity
  showing the strongest alignment.
---

# Confident or Seek Stronger: Exploring Uncertainty-Based On-device LLM Routing From Benchmarking to Generalization

## Quick Facts
- **arXiv ID:** 2502.04428
- **Source URL:** https://arxiv.org/abs/2502.04428
- **Reference count:** 40
- **Primary result:** Calibration data enables effective routing initialization on new datasets without requiring additional downstream data, achieving performance close to using the full dataset.

## Executive Summary
This paper presents a comprehensive benchmark of 8 uncertainty quantification (UQ) methods across 14 datasets and 8 small language models (SLMs) to evaluate uncertainty-based routing to large language models (LLMs). The authors find that the alignment between uncertainty and correctness varies significantly across UQ methods, with trained probes, out-of-distribution probes, and perplexity showing the strongest alignment. They observe that confidence distributions depend more on the SLM and UQ method than on the dataset, leading to the development of a calibration data construction pipeline. Experiments demonstrate that calibration data enables effective routing initialization on new datasets without requiring additional downstream data, achieving performance close to using the full dataset.

## Method Summary
The paper benchmarks 8 UQ methods including Perplexity, Verbalization, and Probe-based approaches across 14 datasets using 8 SLMs and 2 LLMs. The core innovation is a calibration data construction pipeline that pools diverse datasets, bins uncertainty distributions into 30 bins, and samples 10% from each bin to create a proxy dataset for threshold calibration. Trained probes use 4-layer MLPs with LeakyReLU activation trained on hidden states from the 8th-to-last transformer layer, while OOD probes are trained on all datasets except the target one. The routing decision is made by comparing the confidence score against a calibrated threshold, with low-confidence queries offloaded to an LLM.

## Key Results
- Perplexity and trained/OOD probes achieve significantly higher ROC AUC alignment with correctness than verbalization methods
- Confidence distributions are predominantly determined by the chosen SLM and UQ method, with minimal dependence on the downstream dataset
- Calibration data enables effective routing initialization on new datasets, achieving performance close to using the full dataset
- The routing performance using calibration data is within 1-2% accuracy of using the full target dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Routing efficacy depends on the alignment between the chosen Uncertainty Quantification (UQ) method and the model's actual correctness, rather than just the presence of uncertainty scores.
- **Mechanism:** The system extracts a confidence score (e.g., via perplexity or a trained probe) from the SLM. If the score exceeds a threshold, the response is kept; otherwise, it is routed to an LLM. The paper argues that methods like **Perplexity** and **OOD Probes** correlate more strongly with correctness (measured via ROC AUC) than **Verbalization**, leading to better routing decisions.
- **Core assumption:** The internal states or token probabilities of the SLM contain a signal that correlates with the likelihood of the generated answer being correct.
- **Evidence anchors:**
  - [Section 3.2, Observation ❸]: "A good routing standard highly depends on UQ methods with good uncertainty-correctness alignment."
  - [Figure 1]: Shows ROC AUC variance across methods, with Trained/OOD Probes and Perplexity outperforming Verbalization.
  - [Corpus]: The survey on "Collaborative Mechanisms Between Large and Small Language Models" supports the general paradigm of SLM-LLM collaboration to address SLM robustness limitations.
- **Break condition:** If the SLM is consistently overconfident on incorrect answers (misalignment), the router offloads too many correct answers (wasting cost) or keeps too many wrong ones (wasting accuracy).

### Mechanism 2
- **Claim:** Confidence distributions are predominantly determined by the SLM architecture and the UQ method, rather than the specific downstream dataset.
- **Mechanism:** The authors observe that the histogram of confidence scores remains consistent in shape across diverse datasets (e.g., Math vs. Commonsense) for a fixed SLM+UQ pair. This implies that a threshold calculated on one data distribution is likely valid for another, provided the model and extraction method remain constant.
- **Core assumption:** The semantic difficulty of a dataset does not drastically shift the statistical distribution of confidence scores compared to the intrinsic properties of the model.
- **Evidence anchors:**
  - [Section 4.3, Insights ❶]: "The extracted confidence distribution is predominantly determined by the chosen SLM and uncertainty quantification (UQ) method, with minimal dependence on the downstream dataset."
  - [Figure 4]: Visualizes overlapping confidence distributions across 14 datasets.
  - [Corpus]: Evidence is weak in the provided corpus for this specific statistical property; it is a specific finding of the target paper.
- **Break condition:** If a new dataset is significantly out-of-distribution (e.g., a different language or modality not seen in calibration), the confidence distribution may shift, breaking the calibration transfer.

### Mechanism 3
- **Claim:** A "Calibration Data" hold-out set can bootstrap routing thresholds for new domains without accessing downstream data.
- **Mechanism:** Instead of tuning the routing threshold on the target dataset (which requires labels), the system constructs a proxy dataset by sampling from uncertainty bins across various domains. This data establishes a "generalizable" threshold.
- **Core assumption:** The relationship between confidence scores and the optimal routing cutoff is stable enough to be approximated by a weighted sample of diverse data.
- **Evidence anchors:**
  - [Abstract]: "Calibration data enables effective routing initialization on new datasets... achieving performance close to using the full dataset."
  - [Algorithm 1]: Describes the weighted sampling from uncertainty bins.
  - [Corpus]: While "Privacy-Preserving Cloud-Device Collaboration" papers (like LSRP) value on-device processing, this specific *calibration* mechanism is unique to the target paper.
- **Break condition:** The calibration data fails if the target task requires a significantly different cost-benefit trade-off (e.g., medical advice requiring 99% accuracy vs. casual chat requiring 60%) than the one implicitly defined during calibration construction.

## Foundational Learning

- **Concept:** Uncertainty Quantification (UQ) Methods (White-box vs. Black-box)
  - **Why needed here:** The paper evaluates 8 distinct methods (e.g., Perplexity vs. Verbalization vs. Probes). Understanding that "White-box" methods (accessing logits/hidden states) generally outperform "Black-box" methods (verbalized confidence) is central to the paper's findings.
  - **Quick check question:** Can you explain why asking an SLM "How confident are you?" (Verbalization) might yield a lower ROC AUC score than calculating the entropy of its output tokens?

- **Concept:** Routing Thresholds and ROC AUC
  - **Why needed here:** The paper uses ROC AUC to measure how well a confidence score separates correct from incorrect answers. A high AUC means the router can easily find a threshold that filters out errors.
  - **Quick check question:** If a UQ method has a ROC AUC of 0.5 (random guess), what would happen to the cost-accuracy curve if you tried to use it for routing?

- **Concept:** Distribution Shift and Generalization
  - **Why needed here:** The core problem is applying a routing strategy trained on source data to a target domain. The paper posits that confidence distributions are invariant to data shifts, a non-intuitive claim requiring understanding of how models process "difficulty" vs. "uncertainty."
  - **Quick check question:** Why does the paper argue that we don't need the downstream dataset to tune the router, contrary to traditional transfer learning approaches?

## Architecture Onboarding

- **Component map:**
  - SLM Inference Engine -> UQ Extractor -> Router Logic -> LLM Fallback
  - Calibration Store (provides threshold)

- **Critical path:**
  1. Select a UQ method (Paper suggests **Perplexity** for simplicity or **OOD Probe** for performance)
  2. Run the **Calibration Data Construction Pipeline** (Algorithm 1) to generate the proxy dataset
  3. Determine the routing threshold on this proxy data
  4. Deploy router to the new downstream task without further tuning

- **Design tradeoffs:**
  - **Verbalization vs. Probes:** Verbalization is easier to implement (black-box) but the paper shows it has poor alignment (Observation ❷). Probes offer better routing but require training data and access to internal states
  - **Generalization vs. Specificity:** Using the calibration set allows instant deployment but may leave ~1-2% accuracy on the table compared to tuning on the full target dataset (Figure 5)

- **Failure signatures:**
  - **"Over-confident Offload":** High routing ratio but low accuracy improvement. This suggests the UQ method is misaligned (flagging correct answers as uncertain)
  - **"Stagnant Accuracy":** Increasing the routing ratio (sending more to LLM) doesn't improve overall accuracy. This implies the router is selecting the *wrong* queries to offload (e.g., sending easy queries to LLM and keeping hard ones)

- **First 3 experiments:**
  1. **UQ Alignment Check:** Pick your SLM (e.g., Llama-3.2-1B) and run the 8 UQ methods on a small subset of data. Calculate ROC AUC. Discard methods with AUC < 0.6 (likely Verbalization)
  2. **Calibration Validity Test:** Construct calibration data from 13 datasets. Set a threshold to achieve 80% SLM utilization. Apply this threshold to the held-out 14th dataset. Does the actual utilization match the target?
  3. **Routing Curve Profiling:** Plot Accuracy vs. Routing Ratio (Figure 2) for your chosen method. Verify the "sweet spot" where accuracy rises steeply with minimal routing ratio (cost efficiency)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model compression (e.g., pruning, quantization) impact the uncertainty distributions and routing effectiveness of on-device models?
- Basis in paper: [explicit] Section 5, Point 3 explicitly asks about the performance of compressed models compared to pre-trained small models
- Why unresolved: While compressed models may retain the output distributions of larger models, it is unknown if their uncertainty estimates remain calibrated enough for reliable routing
- What evidence would resolve it: A comparative benchmark of routing performance between standard SLMs and their compressed counterparts (using methods like SparseGPT or AWQ) on the proposed calibration datasets

### Open Question 2
- Question: How does the integration of visual modalities affect uncertainty-aware routing in multimodal language models (MLLMs)?
- Basis in paper: [explicit] Section 5, Point 4 identifies uncertainty-aware routing in MLLMs as a valuable research direction
- Why unresolved: The properties of vision tokens significantly influence output generation, meaning text-based uncertainty quantification methods may not generalize to Vision-Language Models (VLMs)
- What evidence would resolve it: Extending the UQ benchmark to include MLLMs (e.g., LLaVA, InternVL) to determine if existing calibration pipelines hold for visual inputs

### Open Question 3
- Question: How can private on-device data be leveraged to continuously enhance personalized routing strategies without compromising user privacy?
- Basis in paper: [explicit] Section 5, Point 1 asks how to utilize private on-device data to strengthen calibration data quality
- Why unresolved: Calibration data provides a general initialization, but real-world deployment on edge devices offers local data that could improve efficiency, though striking a balance with privacy remains an open challenge
- What evidence would resolve it: A proposed method for updating routing thresholds using local data (e.g., via federated learning or differential privacy) showing improved accuracy over static calibration without leaking sensitive information

## Limitations
- The calibration pipeline's effectiveness on truly out-of-distribution data (multilingual, code, medical/legal domains) remains untested
- Probe architecture details lack critical implementation specifications including hidden dimension sizes and state aggregation methods
- The calibration pipeline's weighted sampling strategy is ambiguously described, potentially affecting proxy dataset robustness

## Confidence
- **High Confidence:** The empirical finding that perplexity and trained probes show stronger alignment with correctness than verbalization methods (ROC AUC variance shown in Figure 1)
- **Medium Confidence:** The claim that confidence distributions depend primarily on the SLM and UQ method rather than the dataset
- **Medium Confidence:** The calibration data pipeline's ability to generalize routing thresholds without downstream data

## Next Checks
1. **OOD Generalization Test:** Evaluate the calibration data pipeline on a dataset from a completely different domain (e.g., medical text or code) to verify whether confidence distributions remain stable across domain shifts

2. **Probe Architecture Validation:** Implement the trained and OOD probes with specific hidden dimensions (e.g., 1024) and state aggregation (e.g., mean pooling of last token states) to test whether exact architectural details affect routing performance

3. **Cost-Benefit Analysis:** Quantify the trade-off between using calibration data versus full downstream tuning by measuring accuracy loss and computational savings across diverse routing scenarios with different cost constraints