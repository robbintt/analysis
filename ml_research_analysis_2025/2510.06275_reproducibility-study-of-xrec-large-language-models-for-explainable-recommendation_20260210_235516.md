---
ver: rpa2
title: 'Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"'
arxiv_id: '2510.06275'
source_url: https://arxiv.org/abs/2510.06275
tags:
- xrec
- embeddings
- explanations
- generated
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study reproduces and extends "XRec: Large Language Models
  for Explainable Recommendation" by Ma et al. (2024).'
---

# Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"

## Quick Facts
- **arXiv ID**: 2510.06275
- **Source URL**: https://arxiv.org/abs/2510.06275
- **Reference count**: 20
- **Primary result**: XRec effectively generates personalized explanations and benefits from collaborative information injection for stability, but does not consistently outperform all baseline models

## Executive Summary
This reproducibility study examines "XRec: Large Language Models for Explainable Recommendation" by Ma et al. (2024), a collaborative instruction-tuning framework that enables large language models to provide personalized explanations for recommendations. The framework integrates graph-based collaborative filtering with language models using a LightGCN for user-item embeddings, a Mixture of Experts (MoE) adapter for alignment with LLM token representations, and injection into reserved token positions. The study successfully replicates the original results using Llama-3 instead of GPT-3.5-turbo and conducts two extension experiments on the Amazon-books dataset. Results confirm XRec's ability to generate personalized explanations and demonstrate the importance of collaborative information injection for stability, while also revealing that MoE embeddings significantly influence explanation structure and performance.

## Method Summary
The reproduction study replicated XRec's core methodology using a similar experimental setup but with Llama-3 as the underlying LLM instead of GPT-3.5-turbo. The study evaluated the framework on the Amazon-books dataset, comparing it against baseline models including a plain LLM (LLM), a recommender model (LightGCN), and two ensemble approaches (LLM+LightGCN and LLM+user-item tags). The researchers conducted two extension experiments: (1) removing the MoE adapter to test the necessity of the collaborative filtering embeddings, and (2) replacing the MoE-generated embeddings with fixed embeddings to assess their importance. Performance was measured using BERTScore, BLEURT, and BERTScore-FL, which evaluate the similarity between generated explanations and ground-truth reviews.

## Key Results
- XRec successfully generates personalized explanations for recommendations, with collaborative information injection improving generation stability
- XRec does not consistently outperform all baseline models across all evaluation metrics
- Extension experiments show that MoE embeddings are crucial for maintaining explanation quality and structure, with their removal leading to altered sentence structure and lower performance
- The framework demonstrates the potential of using reserved token positions for injecting collaborative filtering information into LLMs

## Why This Works (Mechanism)
XRec works by bridging the gap between collaborative filtering recommendations and natural language explanations through a carefully designed architecture. The LightGCN generates user and item embeddings that capture collaborative filtering information from the user-item interaction graph. The MoE adapter then transforms these embeddings into a format compatible with the LLM's token representation space, allowing the information to be injected at the embedding level. By placing these adapted embeddings into reserved token positions, XRec ensures that the collaborative information is accessible to the LLM during generation while maintaining the model's pre-trained capabilities. This approach allows the LLM to leverage both its language understanding and the personalized recommendation context to generate coherent, personalized explanations.

## Foundational Learning

**Collaborative Filtering**: A recommendation technique that predicts user preferences based on patterns in user-item interactions. Needed to understand how XRec generates user-specific recommendations. Quick check: Can identify how user-item matrices are used to predict preferences.

**Mixture of Experts (MoE)**: A neural network architecture where multiple specialized models (experts) are combined, with a gating network determining which experts to use for each input. Needed to understand how XRec adapts embeddings for LLM compatibility. Quick check: Can explain how gating mechanisms route inputs to different experts.

**Reserved Token Injection**: A technique where specific token positions in an LLM are designated for injecting external information during inference. Needed to understand how XRec incorporates collaborative filtering data. Quick check: Can describe how token position embeddings can be manipulated during generation.

**BERTScore/BLEURT**: Evaluation metrics that use pre-trained language models to assess text similarity by comparing contextual embeddings. Needed to understand how explanation quality is measured. Quick check: Can differentiate between these metrics and traditional n-gram based evaluation.

## Architecture Onboarding

**Component Map**: User-Item Graph -> LightGCN -> User/Item Embeddings -> MoE Adapter -> Adapted Embeddings -> LLM (Reserved Token Positions) -> Generated Explanation

**Critical Path**: The essential sequence is LightGCN generating embeddings → MoE adapter transforming them → LLM receiving adapted embeddings at reserved positions → explanation generation. Each component must function correctly for coherent explanations.

**Design Tradeoffs**: The framework trades computational efficiency (MoE adds complexity) for personalization quality and explanation faithfulness. Using an LLM provides natural language generation but may introduce hallucination risks.

**Failure Signatures**: Poor explanations may result from: LightGCN failing to capture meaningful patterns (bad embeddings), MoE adapter misalignment (incompatible token representations), or incorrect reserved token positioning (information not properly injected).

**First Experiments**: 1) Test LightGCN embedding quality independently on a simple recommendation task. 2) Verify MoE adapter correctly transforms embeddings by examining output distributions. 3) Confirm reserved token injection works by generating explanations with known injected content.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the faithfulness of XRec's explanations be evaluated given that the ground truths are LLM-generated and may not reflect the actual reasoning of the recommender system?
- **Basis in paper**: The authors state, "the model does not explicitly guarantee the faithfulness of its explanations—whether they accurately reflect the internal decision-making of the recommendation system—a point worth exploring in future research."
- **Why unresolved**: The current evaluation relies on similarity metrics (BERTScore, etc.) against LLM-generated reviews, which measure linguistic alignment rather than logical fidelity to the recommendation decision.
- **What evidence would resolve it**: A new evaluation methodology or metric that correlates explanation features with the internal states or gradient importance of the recommender model, or a human study assessing the accuracy of the reasoning provided.

### Open Question 2
- **Question**: Beyond transferring semantic information, to what extent do the MoE-adapted embeddings function as structural "soft prompts" to enforce specific sentence prefixes and formats?
- **Basis in paper**: The authors suggest that "the further influence of [MoE] embeddings on the generated explanations can be the topic of an interesting follow-up study" after observing that their removal alters sentence structure and conversational style.
- **Why unresolved**: The extension experiments showed that removing embeddings changes the output style (e.g., losing the "The user would enjoy" prefix), but the paper does not isolate if this is an emergent property of the semantic alignment or a learned structural bias.
- **What evidence would resolve it**: A mechanistic interpretability analysis of the MoE adapter to determine if specific dimensions of the output embedding correlate strongly with structural tokens independent of user-item content.

### Open Question 3
- **Question**: Does XRec's independence from the specific recommender system architecture degrade its performance when explaining outlier predictions or edge cases?
- **Basis in paper**: The authors note that XRec is model-agnostic but "hypothesize that this could become particularly problematic when the recommender system makes outlier predictions, as XRec may not know how the recommender system came to this unlikely prediction."
- **Why unresolved**: The reproduction study evaluated general performance on standard datasets but did not filter for or test specifically on edge cases or outlier predictions where the rationale is less obvious.
- **What evidence would resolve it**: An experiment comparing explanation quality (e.g., via human evaluation) on standard high-confidence recommendations versus low-confidence or anomalous recommendations.

## Limitations
- The study substituted Llama-3 for GPT-3.5-turbo, which may affect direct comparability with the original results
- Only two extension experiments were conducted on a single dataset (Amazon-books), limiting generalizability
- No statistical significance testing was performed to quantify the reliability of performance differences between models
- The study did not analyze computational efficiency or resource requirements for the MoE adaptation process

## Confidence
- XRec's core recommendation and explanation generation capability: Medium
- Benefits of collaborative information injection for stability: Medium
- MoE embeddings' impact on explanation structure: Medium

## Next Checks
1. Conduct statistical significance testing across multiple runs to quantify the reliability of performance differences between XRec and baseline models
2. Replicate the experiments using additional datasets from different domains (e.g., movie, music, or product recommendations) to assess generalizability
3. Perform ablation studies with varying numbers of MoE experts and different embedding dimensions to optimize the trade-off between performance and computational efficiency