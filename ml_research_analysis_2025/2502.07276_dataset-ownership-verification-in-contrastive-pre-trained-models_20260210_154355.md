---
ver: rpa2
title: Dataset Ownership Verification in Contrastive Pre-trained Models
arxiv_id: '2502.07276'
source_url: https://arxiv.org/abs/2502.07276
tags:
- uni00000013
- dataset
- uni00000011
- uni00000048
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the first dataset ownership verification
  method for contrastive pre-trained models, addressing the gap in existing approaches
  that are limited to supervised models. The method leverages two key observations
  about contrastive learning: (1) encoders trained on a target dataset produce more
  similar representations for augmentations of seen samples compared to unseen samples
  (unary relationship), and (2) pairwise similarity between seen samples remains stable
  after augmentations, unlike unseen samples (binary relationship).'
---

# Dataset Ownership Verification in Contrastive Pre-trained Models

## Quick Facts
- **arXiv ID:** 2502.07276
- **Source URL:** https://arxiv.org/abs/2502.07276
- **Reference count:** 32
- **Primary result:** Introduces the first dataset ownership verification method for contrastive pre-trained models, achieving p < 0.05 in 98.5% of cases on ImageNet.

## Executive Summary
This paper addresses the critical gap in dataset ownership verification for contrastive self-supervised learning models. While existing methods work for supervised models, they fail for contrastive pre-trained encoders due to their unique training objectives. The authors propose a novel verification method based on the "contrastive relationship gap" - the observation that encoders trained on specific datasets produce more similar representations for augmentations of seen samples compared to unseen samples. The method employs multi-scale augmentations and statistical hypothesis testing to verify ownership with high accuracy across multiple contrastive architectures.

## Method Summary
The method trains a shadow encoder on an unrelated dataset to establish a baseline "innocent" behavior. It then calculates the Contrastive Relationship Gap by applying multi-scale augmentations to both the public and private datasets, computing unary similarity sets (self-similarity of augmented views) and binary similarity sets (stability of pairwise similarities across augmentations). A one-tailed T-test compares the gap statistic between the suspicious and shadow encoders, rejecting the null hypothesis (p < 0.05) when ownership is verified.

## Key Results
- Achieves p < 0.05 rejection rate of 98.5% on ImageNet, outperforming DI4SSL (48.7%) and EncoderMI (70.3%)
- Robust to variations in sampling size, shadow dataset settings, and training hyperparameters
- Effective across multiple contrastive learning models (SimCLR, BYOL, SimSiam, MOCO v3, DINO)
- Maintains high performance even with moderate differential privacy (DP-SGD) applied

## Why This Works (Mechanism)

### Mechanism 1: Unary Relationship Gap (Augmentation Invariance)
Contrastive learning objectives maximize similarity between positive pairs (different views of the same image). The encoder "memorizes" this alignment for seen samples more effectively than unseen ones, resulting in higher cosine similarity for augmentation pairs of training data. This mechanism may fail if the encoder uses heavy regularization or if inference augmentations don't match training distribution.

### Mechanism 2: Binary Relationship Gap (Structural Stability)
The pairwise geometric relationships between different samples in the training set remain more stable under augmentation than those of unseen samples. For seen data, relative positions of different samples are robust to augmentation noise, whereas unseen samples may shift unpredictably. This fails if the encoder architecture produces representations where inter-sample distances fluctuate significantly regardless of training history.

### Mechanism 3: Statistical Discrimination via Shadow Models
Ownership is verified by comparing the Contrastive Relationship Gap in the suspicious model against a shadow model known not to have used the data. A one-tailed T-test determines if the suspect's gap is statistically larger (p < 0.05). This may fail if the shadow dataset is too similar to the public dataset or if the suspect model uses a drastically different architecture.

## Foundational Learning

- **Concept: Contrastive Learning (Self-Supervised)**
  - Why needed: The detection mechanism relies on properties of encoders trained to maximize similarity between positive pairs. Understanding this is essential for grasping the "relationship gap."
  - Quick check: Given an image of a cat, does a contrastive encoder try to make the embedding of a cropped cat similar to the original, or dissimilar?

- **Concept: Hypothesis Testing (T-Test)**
  - Why needed: The method outputs a statistical probability (p-value) comparing distributions rather than a direct binary answer. Interpreting p < 0.05 is central to the system's logic.
  - Quick check: If the null hypothesis is "the suspect model did not use my data," does a p-value of 0.001 support or reject the claim of theft?

- **Concept: Multi-scale Augmentation**
  - Why needed: The method relies on global and local augmentations to probe the encoder's memory. The similarity set calculations explicitly require these distinct views.
  - Quick check: Why would a local crop (e.g., 5% of image area) provide different verification signals than a global crop?

## Architecture Onboarding

- **Component map:** Inputs (D_pub, D_pvt, M_sus) -> Shadow Generator (M_sdw on D_sdw) -> Probes (T^g, T^l augmentations) -> Metrics (S_U, S_B similarity sets) -> Decision Engine (Contrastive Relationship Gap d -> One-tailed T-test)

- **Critical path:** 1) Train M_sdw on irrelevant data to establish baseline 2) Apply multi-scale augmentations to D_pub and D_pvt, feed into both M_sus and M_sdw 3) Compute S_U (self-similarity) and S_B (pairwise stability) 4) Calculate gap statistic d for both models 5) Perform T-test to check if d_sus > d_sdw significantly

- **Design tradeoffs:** Sampling size (smaller k speeds up verification but may reduce sensitivity), hyperparameter a (adjusts weight of positive gaps to tune sensitivity vs specificity), shadow model fidelity (claims robustness to architecture mismatch)

- **Failure signatures:** False positives (unlikely per paper), false negatives (occurs with MAE models or heavy DP-SGD), degradation with full fine-tuning on downstream tasks

- **First 3 experiments:** 1) Sanity Check: Train SimCLR on CIFAR10-1, verify p < 0.05 2) Negative Control: Train suspicious model on CIFAR10-2 (disjoint set), verify p > 0.05 3) Ablation: Run verification using only Unary vs only Binary relationships on ImageNet pre-trained model

## Open Questions the Paper Calls Out

1. **Adapting to MIM models:** How can ownership verification be adapted for pre-training methods like Masked Image Modeling (MIM) where the contrastive relationship gap is not naturally present? The paper states the method doesn't apply effectively to MAE because representations are harder to distinguish.

2. **Non-visual data:** Can this verification methodology be effectively transferred to non-visual data modalities, such as text or audio? The paper lists adapting to other data types as promising future work.

3. **Adversarial attacks:** Is the method robust against adversarial attacks explicitly designed to minimize the contrastive relationship gap while preserving model utility? The paper doesn't evaluate active, gradient-based attacks intended to obscure the verification "fingerprint."

## Limitations
- Method does not apply effectively to Masked Image Modeling (MIM) approaches like MAE where relationship gaps are not naturally present
- Effectiveness degrades when models are fully fine-tuned on downstream tasks, though this can be partially mitigated by increasing hyperparameter a
- Relies on statistical assumptions that may be violated if shadow dataset shares too many characteristics with the public dataset

## Confidence
- **High Confidence:** Statistical framework and ablation results showing consistent verification across multiple contrastive architectures
- **Medium Confidence:** Claims of robustness to hyperparameter variation and scaling to ImageNet based on reported results
- **Medium Confidence:** Effectiveness against fine-tuned models when hyperparameter a is increased

## Next Checks
1. **Architecture Mismatch Test:** Verify method performance when shadow encoder uses different architecture (e.g., shadow: ResNet18, suspect: Vision Transformer)

2. **Masked Model Comparison:** Apply verification to MAE-based encoders versus contrastive models to validate the claim that the method fails for masked prediction objectives

3. **Shadow Dataset Domain Test:** Systematically vary domain similarity between D_sdw and D_pub to quantify impact on false positive rates