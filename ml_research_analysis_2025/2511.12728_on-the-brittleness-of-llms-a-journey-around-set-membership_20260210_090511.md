---
ver: rpa2
title: 'On the Brittleness of LLMs: A Journey around Set Membership'
arxiv_id: '2511.12728'
source_url: https://arxiv.org/abs/2511.12728
tags:
- related
- unrelated
- above
- prompt
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the brittleness of large language models
  (LLMs) on the fundamental task of set membership queries using extremely simple,
  explicitly listed sets of four elements. The authors conduct large-scale experiments
  across diverse prompt phrasings, semantic structures, element orderings, and multiple
  LLMs (7 models, over 60 million queries).
---

# On the Brittleness of LLMs: A Journey around Set Membership

## Quick Facts
- **arXiv ID**: 2511.12728
- **Source URL**: https://arxiv.org/abs/2511.12728
- **Reference count**: 40
- **Primary result**: LLMs exhibit high brittleness to minor prompt variations and element orderings in set membership queries, despite high overall accuracy.

## Executive Summary
This study investigates the brittleness of large language models on the fundamental task of set membership queries using explicitly listed sets of four elements. The authors conduct large-scale experiments across diverse prompt phrasings, semantic structures, element orderings, and multiple LLMs (7 models, over 60 million queries). Despite high overall accuracy, they find consistent brittleness: models are highly sensitive to minor prompt variations, element ordering, and semantic relationships among set elements—properties that should not affect set membership decisions. Different LLMs exhibit substantially different error patterns, with no consistent "understanding" of the set concept. The work demonstrates that the simplicity of the task enables comprehensive mapping of failure modes, providing a valuable methodology for LLM evaluation.

## Method Summary
The study tests set membership queries on 22 explicitly listed 4-element sets using 7 instruction-tuned LLMs. Researchers generate query instances across 24 permutations of each set, using 4 prompt template classes (NL1, NL2, CS, CA) with 480-960 templates per class. Queries include positive (element in set), negative-member (removed element), and negative-intruder (foreign element) types. The methodology uses greedy decoding via HuggingFace transformers with fixed answer parsing logic, testing all permutations to analyze error patterns across prompt/semantics/ordering/model dimensions.

## Key Results
- Minor prompt variations cause significant accuracy differences despite semantic equivalence
- Element ordering in explicitly listed sets affects correctness, violating mathematical set invariance
- Semantic associations create interference: "leakage" (false positives for related items) and "boosting" (improved accuracy for coherent sets)
- No prompt template achieves perfect accuracy across all 7 models; different models show distinct error patterns

## Why This Works (Mechanism)

### Mechanism 1: Prompt Feature Sensitivity
- Claim: Minor variations in prompt formulation cause significant accuracy variations despite semantically equivalent queries.
- Mechanism: LLMs process prompts as token sequences where positional embeddings and attention patterns create different activation trajectories for superficially different prompts. The model's learned associations bind to specific linguistic patterns rather than abstract logical structures.
- Core assumption: LLM representations function as pattern-matching systems rather than abstract reasoning engines.
- Evidence anchors:
  - [abstract]: "Performance varied significantly with minor prompt changes...dimensions that should not affect correctness."
  - [Section 4.1, Figure 2]: Shapley analysis shows different features (arrangement, listing type, quotation) have radically different importance across models—e.g., `listing: -` strongly positive for Llama3.1-70B but negative for Llama3.3-70B.
  - [corpus]: "Systematic Diagnosis of Brittle Reasoning in Large Language Models" (arxiv 2510.08595) supports structured failure-point diagnosis.

### Mechanism 2: Permutation Sensitivity (Violated Set Invariance)
- Claim: Element ordering in explicitly listed sets affects correctness, contradicting the mathematical definition of set membership.
- Mechanism: Transformers process sequential token streams where position affects attention weights. Without explicit set-theoretic data structures, the model represents "sets" as position-dependent sequences. Different orderings produce different attention patterns, leading to inconsistent outputs for logically identical queries.
- Core assumption: Standard transformer architectures lack inherent permutation-invariant representations for set-like structures.
- Evidence anchors:
  - [Section 4.2]: "Inconsistency is much higher than the overall error rate...indicating that many sets have 'adversarial' orderings."
  - [Figure 3]: Permutation inconsistency reaches 6-8% for some model/prompt combinations (e.g., Mistral-24B with NL1 prompts on unrelated numbers).
  - [corpus]: Permutation invariance discussed as a desired property in prior work (Zaheer et al. 2017, "Deep Sets").

### Mechanism 3: Semantic Interference (Leakage and Boosting)
- Claim: Pre-trained semantic associations interfere with logical reasoning, causing both errors (leakage) and improvements (boosting) depending on context.
- Mechanism: Semantic co-occurrence patterns from pre-training create strong prior associations. When evaluating membership, these priors either (a) cause false positives for semantically related items (leakage) or (b) provide scaffolding that improves accuracy for coherent sets (boosting). The model cannot fully suppress semantic priors during abstract logical operations.
- Core assumption: Semantic associations persist through instruction fine-tuning and cannot be selectively disabled.
- Evidence anchors:
  - [Section 4.3, Table 1]: Negative-member FPR (1.157% avg) exceeds negative-intruder FPR (0.752% avg), showing semantically related items trigger more false positives.
  - [Figure 5]: For NL1 prompts, 70-85% of templates perform better on related word sets (points below diagonal); for CS prompts with numbers, the pattern reverses for some models.
  - [corpus]: Weak direct corpus support; "Are Humans as Brittle as Large Language Models?" (arxiv 2512.12218) explores human vs. LLM brittleness comparison.

## Foundational Learning

- **Concept: Set Membership Invariance**
  - Why needed here: The paper tests whether LLMs understand that set membership is independent of element order and presentation format.
  - Quick check question: Given S = {apple, banana, cherry}, should the answer to "Is apple ∈ S?" differ if S = {cherry, apple, banana}?

- **Concept: Prompt Brittleness**
  - Why needed here: Understanding that semantically equivalent prompts can produce different outputs is central to interpreting the experimental design.
  - Quick check question: Should "Does {a,b} contain x?" and "Is x an element of the set {a,b}?" ever produce different answers?

- **Concept: Semantic Leakage vs. Boosting**
  - Why needed here: The paper documents two opposing semantic effects; distinguishing them is essential for understanding failure modes.
  - Quick check question: If an LLM incorrectly says "plum ∈ {apple, orange, banana}" more often than "car ∈ {apple, orange, banana}," which effect is this?

## Architecture Onboarding

- **Component map**:
  - 22 test sets (10 complete word sets, 3 related word sets, 3 related number sets, 3 unrelated word sets, 3 unrelated number sets)
  - 4 prompt template classes (NL1, NL2, CS, CA) with 480-960 templates per class
  - 3 query types (Positive, Negative-member, Negative-intruder)
  - All 24 permutations per 4-element set
  - 7 instruction-tuned LLMs (Llama-3.2-3B, Llama-3.1-8B, Llama-3.1-70B, Llama-3.3-70B, Mistral-24B, Qwen2.5-32B, Phi-3.5-MoE)

- **Critical path**:
  Define sets → Generate prompt templates from feature combinations → Instantiate queries across all permutations → Execute with greedy decoding → Analyze error patterns across prompt/semantics/ordering/model dimensions

- **Design tradeoffs**:
  - Set size = 4: Tractable permutation space (24) while still revealing patterns; larger sets increase error rates
  - Greedy decoding only: Isolates prompt/semantic effects from sampling noise
  - No prompt engineering: Goal was failure mode mapping, not accuracy maximization

- **Failure signatures**:
  - **Permutation inconsistency**: Same set/query, different orderings → different answers
  - **Semantic leakage**: Higher FPR for negative-member (semantically related) vs. negative-intruder queries
  - **Model-specific template success**: No prompt template achieves perfect accuracy across all 7 models; NL1 has zero universal templates

- **First 3 experiments**:
  1. Baseline accuracy sweep: Run all 22 sets with one template per class to establish per-model accuracy baselines.
  2. Permutation sensitivity probe: For a single complete set (e.g., cardinal directions), test all 24 orderings with fixed template; compute consistency rate.
  3. Semantic leakage quantification: Compare FPR between negative-member and negative-intruder queries across related word sets using NL1 templates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mechanistic interpretability techniques identify specific internal circuits or attention heads responsible for "semantic leakage" and "permutation inconsistency" in set membership tasks?
- Basis in paper: [explicit] The Limitations section explicitly states the authors have not looked "under the hood" using mechanistic interpretability techniques to understand the observed brittleness.
- Why unresolved: The current study maps failure modes empirically through input-output analysis but does not examine the internal model states that cause semantic features to override logical set operators.
- What evidence would resolve it: Identification of specific causal mechanisms within the model architecture that activate differently for semantically related versus unrelated sets, potentially allowing for targeted intervention.

### Open Question 2
- Question: Are the observed brittleness properties, such as semantic boosting and ordering sensitivity, specific to LLMs, or do they mirror cognitive biases found in human reasoning?
- Basis in paper: [explicit] The Limitations section notes the need for human experiments to determine if "certain brittleness properties (e.g., semantic leakage or boosting) are specific to LLMs."
- Why unresolved: The paper establishes that LLMs fail inconsistently but does not provide a baseline of human performance on the exact same controlled set membership tasks to determine if this is a machine-specific flaw.
- What evidence would resolve it: A comparative study using the same prompt variations and set permutations on human subjects to see if they exhibit similar drops in accuracy or consistency.

### Open Question 3
- Question: Does robust performance on this "simple task" set membership benchmark serve as a reliable proxy for predicting reliability in complex, high-stakes reasoning tasks?
- Basis in paper: [inferred] The Conclusion suggests the experimental design could serve as a "valuable methodology for LLM evaluation in general," implying an open question regarding the correlation between mastery of this fundamental concept and broader reasoning capabilities.
- Why unresolved: While the authors demonstrate that high-performing models fail simple tasks, they do not verify if fixing these basic failure modes correlates with improved performance on standard complex benchmarks.
- What evidence would resolve it: Correlation analysis showing that models fine-tuned to be robust against set membership brittleness show measurable improvements in general reasoning benchmarks (e.g., MMLU, GSM8K).

### Open Question 4
- Question: How does the magnitude of permutation inconsistency scale with set cardinality beyond the four-element sets tested?
- Basis in paper: [inferred] The authors note in Section 3.1 that they fixed the set size at four because "increasing the size of the set increases the error rates," leaving the specific interaction between set size and permutation sensitivity unexplored.
- Why unresolved: It is unclear if the ordering sensitivity is a constant noise factor or if it grows non-linearly as the cognitive load (number of items) increases.
- What evidence would resolve it: Empirical results tracking permutation inconsistency rates for sets of 5, 6, 10, and 20 elements across the same prompt templates.

## Limitations
- The study uses greedy decoding only, which may underestimate the impact of sampling strategies on brittleness patterns
- Error parsing mechanism introduces uncertainty with ~0.14% of responses classified as "undecided"
- Semantic effects rely on qualitative categorization of semantic relationships that may not fully capture nuanced associations
- Restricted set size (4 elements) and specific prompt designs may not capture brittleness patterns in larger or more complex sets

## Confidence
- **High confidence**: The core finding that LLMs exhibit brittleness to prompt variations and element ordering is well-supported by the comprehensive experimental design and statistical analysis
- **Medium confidence**: The semantic interference mechanisms (leakage vs. boosting) are supported by the data but rely on subjective categorization of semantic relationships
- **Low confidence**: The generalizability of these brittleness patterns to real-world applications remains uncertain

## Next Checks
1. **Sampling strategy comparison**: Repeat the experiments using temperature-based sampling (e.g., temperature=0.7, top-p=0.9) to determine whether the brittleness patterns persist or diminish with stochastic decoding strategies

2. **Cross-task brittleness mapping**: Test whether the identified brittleness patterns in set membership transfer to other fundamental reasoning tasks (e.g., logical implication, transitive reasoning) using the same methodology of exhaustive permutation and prompt variation

3. **Intervention effectiveness**: Systematically test architectural interventions designed to address identified failure modes: (a) permutation-invariant attention mechanisms, (b) semantic prior suppression layers, and (c) canonical prompt normalization preprocessing to evaluate whether the brittleness can be mitigated