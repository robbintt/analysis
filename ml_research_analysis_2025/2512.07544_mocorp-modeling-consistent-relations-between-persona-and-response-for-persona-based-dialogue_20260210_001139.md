---
ver: rpa2
title: 'MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based
  Dialogue'
arxiv_id: '2512.07544'
source_url: https://arxiv.org/abs/2512.07544
tags:
- dialogue
- persona
- dataset
- relation
- mocorp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating engaging and context-specific
  dialogue while maintaining a coherent personality in persona-based dialogue systems.
  The key issue is that existing datasets lack explicit relations between persona
  sentences and responses, making it difficult for models to effectively capture and
  utilize persona information.
---

# MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue

## Quick Facts
- arXiv ID: 2512.07544
- Source URL: https://arxiv.org/abs/2512.07544
- Authors: Kyungro Lee; Dongha Choi; Hyunju Lee
- Reference count: 40
- Primary result: MoCoRP achieves superior persona consistency and engaging dialogue generation on ConvAI2 and MPChat datasets.

## Executive Summary
This paper addresses the challenge of generating engaging and context-specific dialogue while maintaining a coherent personality in persona-based dialogue systems. The key issue is that existing datasets lack explicit relations between persona sentences and responses, making it difficult for models to effectively capture and utilize persona information. The authors propose MoCoRP, a framework that incorporates explicit relations into language models by leveraging an NLI expert to extract relations between persona sentences and responses. This allows the model to selectively incorporate appropriate persona information. MoCoRP is applied to pre-trained models like BART and extended to large language models through alignment tuning. Experiments on ConvAI2 and MPChat datasets demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging dialogue generation.

## Method Summary
MoCoRP uses an NLI expert (RoBERTa-based classifier) to extract entailment/neutral/contradiction relations between persona sentences and responses. These relation scores are used as supervision during training. The BART encoder processes dialogue history and persona sentences, with a relation prediction head (RP Head) mapping the mask token to relation scores. These scores are transformed into a relation vector and added to the decoder start token embedding, conditioning generation on persona-response relations. The model is trained with three losses: language modeling, response selection, and relation prediction (KL divergence). For LLMs, MoCoRP uses a prior-posterior framework where the prior model generates responses, the NLI expert computes relation labels, and the posterior model conditions on these labels during fine-tuning.

## Key Results
- MoCoRP achieves higher Hits@1 and C score (persona consistency) than baselines on ConvAI2 and MPChat
- The model shows significant improvements in both quantitative metrics (F1, BLEU, ROUGE) and qualitative aspects
- Optimal relation learning data proportion is 10% for ConvAI2 and 5% for MPChat, with larger proportions hurting smaller datasets
- LLM extension shows improvements but is more sensitive to prior model quality and NLI label noise

## Why This Works (Mechanism)

### Mechanism 1: NLI-Guided Relation Extraction
- Claim: Extracting explicit NLI relations between persona sentences and responses enables the model to selectively incorporate persona information that is actually relevant to each response.
- Mechanism: An NLI expert (RoBERTa-based classifier) is trained on the Dialogue NLI dataset to classify persona-response pairs as entailment, neutral, or contradiction. These scores serve as supervision signals for the dialogue model.
- Core assumption: NLI relations capture meaningful connections between persona attributes and appropriate responses; neutral relations indicate persona sentences that should not influence a given response.
- Evidence anchors:
  - [abstract]: "MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information."
  - [section 3.2.1]: "We utilize the NLI expert in two ways. First, it is employed to analyze persona-based dialogue datasets. Second, it is used to estimate the 'NLI score,' which serves as a guide for the relation prediction task."
  - [corpus]: Related work (Dialogue NLI dataset, DECODE dataset) confirms NLI is used for consistency in dialogue, though corpus evidence for this specific extraction approach is limited.
- Break condition: If the NLI expert has low accuracy, or if NLI relations do not correlate with persona-response relevance in your target domain, this mechanism fails.

### Mechanism 2: Knowledge Distillation via Relation Alignment
- Claim: Aligning the dialogue model's internal relation representations with NLI expert scores enables persona-consistent generation at inference without requiring explicit relation labels.
- Mechanism: KL divergence loss minimizes the difference between the model's predicted relation scores (from RP Head) and the NLI expert's scores, distilling the expert's capability into the dialogue model.
- Core assumption: The relation prediction capability can be internalized through alignment, allowing the model to implicitly recognize persona-response relations during inference.
- Evidence anchors:
  - [section 3.2.4]: "Aligning the relation score vector ẑ closely with the NLI score vector z enables BART to acquire the relation prediction capability of the NLI expert. This alignment allows the model to leverage necessary persona information from context during inference without explicit relations."
  - [section 3.2]: "Once the training of MoCoRP is complete, the model is expected to generate persona-consistent responses even in the absence of explicit relation information."
  - [corpus]: No direct corpus evidence for this specific distillation approach; assumption-based.
- Break condition: If the dialogue model cannot learn accurate relation prediction, or if the alignment loss conflicts with generation quality, this mechanism fails.

### Mechanism 3: Relation-Conditioned Generation via Start Token Embedding
- Claim: Injecting relation information into the decoder start token embedding biases the entire generation process toward persona-consistent responses.
- Mechanism: Relation scores are transformed into a relation vector (via dense layer + mean pooling) and added to the decoder start token embedding, conditioning subsequent generation on persona-response relations.
- Core assumption: Modifying the start token embedding effectively propagates relation information through all decoder layers and generated tokens.
- Evidence anchors:
  - [section 3.2.3]: "z_rel = 1/k Σ Dense(ẑ_i) ∈ R^d_model; Ē[r] = E[r] + z_rel. Here, the decoder starts from the embedding vector Ē[r] to produce the final representation."
  - [section 3.2.2]: Chen et al. [5] proposed a similar embedding modification approach for entailment relations.
  - [corpus]: No direct corpus evidence; corpus neighbors focus on persona alignment but not this specific architectural choice.
- Break condition: If the relation vector magnitude or dimension is poorly calibrated, it may fail to influence generation or destabilize training.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: MoCoRP's core innovation uses NLI (entailment/neutral/contradiction) to classify persona-response relationships. Without understanding NLI, the relation extraction mechanism is opaque.
  - Quick check question: Given persona "I love hiking" and response "I prefer staying indoors," what NLI label applies and why does this matter for persona consistency?

- Concept: Knowledge Distillation / Teacher-Student Training
  - Why needed here: MoCoRP distills the NLI expert's relation prediction capability into BART through KL divergence alignment.
  - Quick check question: Why might distillation be preferred over directly using the NLI expert at inference time?

- Concept: Conditional Text Generation with Control Codes
  - Why needed here: The relation vector modifies the decoder start token to condition generation, a form of soft control code.
  - Quick check question: How does modifying the start token embedding differ from prepending a control token to the input?

## Architecture Onboarding

- Component map:
  NLI Expert (RoBERTa classifier) -> BART Encoder -> RP Head (relation prediction) -> Relation Vector Module -> BART Decoder (start token modified) -> Generated Response

- Critical path:
  1. Train NLI expert on Dialogue NLI dataset (achieves 92.43% accuracy per paper)
  2. Relation learning: Pre-train BART on sampled NLI data to learn balanced relations
  3. Dialogue learning: Fine-tune on persona-dialogue data with NLI expert providing relation supervision
  4. For LLMs: Prior model generates response -> NLI expert computes labels -> Posterior model conditions on labels

- Design tradeoffs:
  - **Relation learning data proportion**: Paper found 10% for ConvAI2, 5% for MPChat optimal; larger proportions hurt smaller datasets
  - **LM loss during relation learning**: "RL LM [none]" best for data-rich ConvAI2; applying LM loss helped smaller MPChat
  - **LLM extension**: MoCoRP LLM avoids architectural changes, using NLI labels as text input instead—trades some expressiveness for compatibility

- Failure signatures:
  - High PPL with good C-score: Over-fitting to persona at expense of fluency
  - Posterior ≤ Prior in LLM variant: NLI labels not being effectively utilized; check prompt formatting
  - C-score not improving: Relation prediction loss may be too weak (increase α) or NLI expert quality is insufficient

- First 3 experiments:
  1. Validate NLI expert accuracy on your target domain; if <85%, retrain or fine-tune expert before proceeding
  2. Ablate relation prediction loss (α=0) on a held-out set; confirm C-score drops to isolate the mechanism's contribution
  3. Sweep relation learning data proportion (0%, 5%, 10%, 20%) on your dataset; optimal proportion depends on dialogue data size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal proportion of relation learning data be determined automatically for new datasets given the divergent optimal ratios observed between ConvAI2 and MPChat?
- Basis in paper: [inferred] from Section 5.3 (Ablation Study), noting that MPChat is highly sensitive to the amount of relation learning data (optimal at 5%) while ConvAI2 is relatively insensitive (optimal at 10-20%).
- Why unresolved: The authors attribute these differences to general dataset characteristics (e.g., utterance length, data volume) but do not propose a method to predict the optimal ratio without manual tuning.
- What evidence would resolve it: A heuristic or metric derived from dataset statistics (e.g., persona-to-utterance ratio) that consistently predicts the optimal relation learning data proportion across multiple distinct dialogue datasets.

### Open Question 2
- Question: Does the reliance on text-based NLI labels in MoCoRP LLM (to avoid architectural modification) result in a lower performance ceiling compared to direct parameter integration?
- Basis in paper: [inferred] from Section 3.3, where the authors explicitly avoid architectural changes in LLMs to prevent "unexpected performance drops" due to data scale imbalance, opting instead for a prior/posterior text-injection method.
- Why unresolved: The paper demonstrates a workaround for LLMs but does not investigate if the architectural approach used in BART would yield superior results if applied to LLMs with sufficient training data.
- What evidence would resolve it: A comparative study scaling the BART-style architectural integration (adding an RP Head) to larger parameter counts, trained on a correspondingly larger corpus to match the model's capacity.

### Open Question 3
- Question: Does explicitly modeling "Neutral" relations during the relation learning phase provide a functional benefit for response generation, or does it merely serve to identify irrelevant context?
- Basis in paper: [inferred] from Section 3.2.2 and Appendix B, which highlight that neutral relations constitute the vast majority (84%) of persona-response pairs in dialogue datasets.
- Why unresolved: While the model is trained to predict these relations, the paper does not isolate the specific utility of predicting "Neutral" versus simply ignoring non-entailed information during the generation process.
- What evidence would resolve it: An ablation study comparing the proposed 3-class relation modeling against a binary (Entailment/Contradiction) model to measure the specific impact of neutral classification on dialogue quality.

## Limitations
- NLI expert accuracy dependence: The approach assumes the NLI expert can reliably extract meaningful relations between persona sentences and responses
- Lack of mechanism ablation: The paper doesn't clearly demonstrate that each mechanism (relation prediction, start token modification) contributes independently to improvements
- LLM extension vulnerability: The approach relies heavily on prior model quality, making it sensitive to noisy NLI labels

## Confidence

**High Confidence**: The quantitative improvements (Hits@1, F1, C score) on ConvAI2 and MPChat are well-supported by the experimental results. The methodology for measuring persona consistency via NLI expert is sound and reproducible.

**Medium Confidence**: The mechanism claims (NLI-guided relation extraction, knowledge distillation, relation-conditioned generation) are logically coherent but lack direct corpus evidence. The paper provides architectural details but doesn't sufficiently validate that each mechanism contributes independently to the observed improvements.

**Low Confidence**: The LLM extension's effectiveness is questionable. The paper shows some improvements but doesn't demonstrate that MoCoRP LLM is consistently better than simpler approaches. The reliance on prior model quality as a prerequisite is a significant vulnerability.

## Next Checks

1. **NLI Expert Domain Adaptation**: Train the NLI expert on ConvAI2/MPChat data (or fine-tune on these datasets) and measure accuracy drop. If accuracy falls below 85%, retrain the expert before proceeding with MoCoRP training.

2. **Mechanism Ablation Study**: Train three variants: (a) MoCoRP without relation prediction loss (α=0), (b) MoCoRP without relation vector injection (start token unmodified), (c) MoCoRP with relation tokens prepended instead of start token modification. Compare C scores to isolate each mechanism's contribution.

3. **LLM Extension Robustness**: Generate responses from the prior model on a held-out test set, compute NLI labels, then train the posterior model. Measure posterior C score improvement. If improvement <5%, investigate whether prior model quality or NLI label noise is the bottleneck.