---
ver: rpa2
title: 'DRAssist: Dispute Resolution Assistance using Large Language Models'
arxiv_id: '2509.01962'
source_url: https://arxiv.org/abs/2509.01962
tags:
- party
- disputes
- dispute
- insurance
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DRAssist is an LLM-based prototype system for dispute resolution\
  \ assistance. It processes disputes from two domains\u2014automobile insurance and\
  \ domain name disputes\u2014by extracting structured summaries and generating outputs\
  \ at three levels: overall stronger party prediction, demand-wise decisions, and\
  \ argument-wise evaluation."
---

# DRAssist: Dispute Resolution Assistance using Large Language Models

## Quick Facts
- **arXiv ID:** 2509.01962
- **Source URL:** https://arxiv.org/abs/2509.01962
- **Reference count:** 19
- **One-line primary result:** DRAssist achieves macro-F1 up to 0.78 for stronger party prediction in dispute resolution using zero-shot Chain-of-Thought prompting.

## Executive Summary
DRAssist is a zero-shot prototype system for dispute resolution assistance using large language models. It processes unstructured dispute descriptions from automobile insurance and domain name disputes, generating structured summaries and producing outputs at three levels: overall stronger party prediction, demand-wise decisions, and argument-wise evaluation. The system employs three prompting strategies, with Chain-of-Thought prompting outperforming direct and demand-wise approaches. DRAssist achieves strong performance metrics (up to 0.78 macro-F1) while providing justifications for transparency, though it exhibits domain-specific biases requiring further investigation.

## Method Summary
DRAssist uses a zero-shot approach with three prompting strategies (S1 Direct, S2 Demand-wise, S3 Chain-of-Thought) applied to structured summaries of disputes. The system extracts key elements from raw text, including facts, arguments, demands, and statutes, then processes them through LLMs (Mistral-7B-Instruct, Llama-3-8B-Instruct, GPT-4o-mini). The Chain-of-Thought strategy, which evaluates individual arguments before predicting the stronger party, shows the best performance. An ensemble approach combining multiple LLMs via majority vote further improves results. The system generates justifications for decisions and evaluates them using ROUGE and BERTScore metrics.

## Key Results
- Chain-of-Thought prompting (S3) outperforms direct (S1) and demand-wise (S2) strategies, achieving macro-F1 of 0.78 for stronger party prediction
- Argument-wise evaluation reaches macro-F1 of 0.73 with CoT prompting
- Ensemble of three LLMs improves robustness and accuracy compared to single models
- Domain-specific biases observed, particularly Mistral's tendency to predict "complainant" wins in domain name disputes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chain-of-Thought (CoT) prompting improves dispute resolution performance by forcing explicit reasoning.
- **Mechanism:** By requiring the LLM to evaluate individual arguments (strong/weak) before predicting the overall stronger party, CoT prompts create a reasoning trace that guides the model to a more accurate conclusion.
- **Core assumption:** LLMs have sufficient internal representation of legal/logical reasoning to benefit from explicit structuring of the thought process.
- **Evidence anchors:** "Chain-of-Thought approach outperforming others" (abstract); "S3 is better than S2 and S2 is better than S1... this establishes the effectiveness of chain-of-thought like reasoning..." (Section 5.5).

### Mechanism 2
- **Claim:** Structured summarization creates a standardized, useful input for the LLM, improving its focus.
- **Mechanism:** The structured summary extracts key dispute elements (facts, arguments, demands, statutes) into a consistent format. This reduces noise and directs the LLM's attention to the most relevant information for its prediction.
- **Core assumption:** The summarization step itself (performed by an LLM) is accurate and does not discard critical details or introduce its own errors/hallucinations.
- **Evidence anchors:** "identifies certain key structural elements... and summarizes the unstructured dispute descriptions to produce a structured summary" (abstract); structured summary elements and generation described in Section 4.1.

### Mechanism 3
- **Claim:** An ensemble of multiple LLMs can reduce individual model biases and improve overall prediction robustness.
- **Mechanism:** Aggregating predictions (e.g., by majority vote) from models with different architectures/training data can smooth out individual model idiosyncrasies and errors.
- **Core assumption:** The errors made by individual models are not perfectly correlated, allowing the ensemble to cancel them out.
- **Evidence anchors:** "S3 (Ensemble) achieves macro-F1 of 0.78..." (Section 5.5); error analysis shows cases where one LLM reasoned correctly where another did not (Section 5.6).

## Foundational Learning

- **Concept:** Prompt Engineering (Zero-Shot Chain-of-Thought)
  - **Why needed here:** DRAssist relies entirely on prompting strategies to guide LLMs without fine-tuning. Understanding CoT is critical to reproducing and improving the S3 results.
  - **Quick check question:** Can you explain the difference between the S1, S2, and S3 prompting strategies described in the paper?

- **Concept:** Structured Summarization / Information Extraction
  - **Why needed here:** The system's first core step is transforming raw, unstructured dispute text into a predefined schema. Understanding how this is done is key to the data pipeline.
  - **Quick check question:** What are the key structural elements extracted from a dispute document?

- **Concept:** Ensemble Methods (Voting)
  - **Why needed here:** The best reported results use an ensemble of three different LLMs. Understanding this simple aggregation technique is required to implement the top-performing version of the system.
  - **Quick check question:** How is the ensemble prediction for the "stronger party" formed from the outputs of the individual LLMs?

## Architecture Onboarding

- **Component map:** Input (raw dispute) -> Structured Summarization -> Prompting Strategy (S1/S2/S3) -> LLM(s) -> (Optional Ensemble) -> Output (stronger party, demand-wise decisions, argument-wise evaluation)

- **Critical path:** The chain is `Input -> Structured Summarization -> Prompting Strategy -> LLM(s) -> (Optional Ensemble) -> Output`. The S3 (Chain-of-Thought) prompt is the most critical component for achieving best results.

- **Design tradeoffs:**
  - **CoT (S3) vs. Direct (S1):** S3 gives better prediction accuracy but generates less elaborate justifications.
  - **Single LLM vs. Ensemble:** Ensemble improves accuracy and robustness but increases latency, cost, and system complexity.
  - **Zero-Shot vs. Fine-Tuning:** The system uses a zero-shot approach, requiring no training data but potentially capping performance. Fine-tuning could improve results but requires a labeled dataset.

- **Failure signatures:**
  - **Domain Bias:** LLMs may show a consistent bias towards one party (e.g., predicting "complainant" wins too often in domain name disputes).
  - **Summarization Errors:** Errors in the initial structured summary (hallucinations, omissions) will propagate and corrupt the final resolution.
  - **Justification Degradation:** CoT prompts (S3) may produce repetitive, template-like justifications compared to direct prompts (S1).

- **First 3 experiments:**
  1. **Reproduce the Baseline:** Run a single LLM (e.g., GPT-4o-mini) with the S1 (Direct) and S3 (CoT) prompts on a small set of disputes to verify the performance gap reported in the paper.
  2. **Test Summarization Quality:** Pass a sample dispute through the summarization module and manually verify the accuracy of the extracted "Arguments" and "Demands" against the original text.
  3. **Ensemble vs. Single Model:** Run the same set of disputes through two different LLMs and a simple majority-vote ensemble to observe if the ensemble reduces specific errors, as claimed.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the systematic bias towards the "complainant" class in domain name disputes be effectively mitigated? The authors note this as future work, observing that Mistral predicts "complainant" in nearly all DDN cases with low recall for "respondent" even in the best model.

- **Open Question 2:** Can multi-agent LLM frameworks reduce reasoning errors compared to the current single-agent or ensemble approaches? The paper plans to explore this after observing incorrect reasoning as a source of error and contradictory logical interpretations between models.

- **Open Question 3:** What is the qualitative impact of DRAssist on the efficiency and decision-making confidence of human experts? The authors plan user studies to assess the system's actual utility as an assistant to judges or arbitrators, as current evaluation relies solely on automated metrics.

## Limitations
- Dataset accessibility issues prevent independent verification of results
- Domain-specific biases persist, particularly Mistral's strong "complainant" bias in domain name disputes
- Zero-shot constraint may inherently limit performance compared to fine-tuned models for nuanced legal reasoning

## Confidence

- **High Confidence:** The core mechanism of using Chain-of-Thought prompting (S3) to improve prediction accuracy over direct prompting (S1) is well-supported by experimental results (macro-F1 0.78 vs. 0.71 for stronger party prediction).
- **Medium Confidence:** The effectiveness of structured summarization and ensemble approaches is supported, but lack of full prompt details and dataset access introduces uncertainty in precise replication.
- **Low Confidence:** The extent and impact of domain-specific biases, and how robustly they can be mitigated across diverse legal domains, are not fully characterized.

## Next Checks

1. **Dataset Acquisition and Replication:** Obtain the DAI and DDN datasets or replicate the scraping/filtering process. Run the full DRAssist pipeline to verify reported macro-F1 scores and S3 performance advantage.

2. **Prompt Fidelity Test:** Implement S1 and S2 prompts based on S3 template and descriptions. Conduct ablation study comparing all three strategies on held-out validation set to confirm performance ranking.

3. **Bias Characterization and Mitigation:** Analyze per-model and per-domain prediction distributions to quantify domain-specific biases. Test alternative mitigation strategies (weighted voting, bias-aware prompting) to assess effectiveness in reducing biases without harming accuracy.