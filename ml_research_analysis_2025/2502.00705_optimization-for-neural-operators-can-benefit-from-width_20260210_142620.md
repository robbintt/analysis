---
ver: rpa2
title: Optimization for Neural Operators can Benefit from Width
arxiv_id: '2502.00705'
source_url: https://arxiv.org/abs/2502.00705
tags:
- neural
- where
- condition
- operator
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the open problem of establishing optimization
  convergence guarantees for training neural operators using gradient descent. The
  authors propose a unified framework based on restricted strong convexity (RSC) and
  smoothness conditions, which are satisfied by both Deep Operator Networks (DONs)
  and Fourier Neural Operators (FNOs) when the networks are sufficiently wide.
---

# Optimization for Neural Operators can Benefit from Width

## Quick Facts
- arXiv ID: 2502.00705
- Source URL: https://arxiv.org/abs/2502.00705
- Reference count: 40
- Primary result: Wider neural operators lead to better optimization convergence guarantees

## Executive Summary
This paper establishes optimization convergence guarantees for training neural operators using gradient descent by proving that their empirical loss functions satisfy restricted strong convexity (RSC) and smoothness conditions. The key insight is that increasing network width makes these conditions easier to satisfy, leading to faster convergence. The authors demonstrate this for both Deep Operator Networks (DONs) and Fourier Neural Operators (FNOs) across three canonical operator learning problems, showing that wider networks consistently achieve lower training losses and faster convergence.

## Method Summary
The authors propose a unified framework based on RSC and smoothness conditions to guarantee optimization convergence for neural operators. They prove these conditions hold for both DONs and FNOs when networks are sufficiently wide and properly initialized. The theoretical analysis derives bounds on the Hessian spectral norm for both architectures, accounting for the complex structure of DONs (inner product of two neural networks) and the cross-derivatives in FNOs (between data and Fourier domains). Empirical validation uses Adam optimization on three problems (antiderivative, diffusion-reaction, and Burger's equation) while varying width from 10 to 500 neurons.

## Key Results
- Wider networks lead to better optimization convergence for both DONs and FNOs
- RSC condition becomes less restrictive as width increases, making loss "strongly convex-like"
- Larger widths enlarge the neighborhood around initialization where theoretical guarantees hold
- Increasing width consistently leads to lower training losses and faster convergence across all three tested problems

## Why This Works (Mechanism)

### Mechanism 1: Restricted Strong Convexity (RSC) as a Convergence Proxy
The paper uses RSC instead of global convexity to prove convergence for non-convex neural operators. Locally, the loss landscape curves upwards enough relative to the gradient when RSC and smoothness conditions are met. If the loss decreases faster than the curvature bound, gradient descent provably reduces the loss. The loss function must be β-smooth and iterates must stay within a neighborhood B(θ₀) where the RSC parameter α_t > 0.

### Mechanism 2: Width Reduces RSC Restrictiveness
Increasing network width reduces the magnitude of negative terms in the RSC parameter α_t. The RSC parameter is derived as 2κ²||∇Ḡ_t||² - cK²(1/√m_f + 1/√m_g). As width (m) increases, the negative penalty term (dependent on 1/√m) vanishes, making the loss maintain strongly convex-like properties more easily even with smaller gradients.

### Mechanism 3: Width Expands the Stability Neighborhood
Over-parameterization enlarges the radius of the neighborhood around initialization where theoretical guarantees hold. The analysis depends on parameters staying within a set B^Euc_ρ,ρ₁(θ₀). Constraints on initialization variance imply the radius ρ scales with √m. Therefore, wider networks allow parameters to travel further from initialization without breaking RSC/Smoothness assumptions.

## Foundational Learning

### Concept: Restricted Strong Convexity (RSC)
**Why needed:** This is the theoretical alternative to global convexity used to prove convergence. The loss isn't truly convex, but behaves like it "restricted" to a specific set of directions near current weights.
**Quick check:** Does RSC imply the Hessian of the loss is positive semi-definite everywhere? (Answer: No, only in specific directions/sets defined by the theory)

### Concept: Neural Operator Architectures (DON vs. FNO)
**Why needed:** The paper analyzes the Hessian of two distinct architectures. Understanding that DONs use an inner product of Branch/Trunk nets while FNOs use spectral convolutions is vital for understanding why their Hessian bounds are derived differently.
**Quick check:** Why is the Hessian analysis for a DON more complex than a standard Feedforward Network? (Answer: It involves cross-derivatives between two separate sub-networks)

### Concept: Hessian Spectral Norm
**Why needed:** The proofs rely on bounding the spectral norm of the Hessian to establish "smoothness." If you cannot bound the second derivatives, you cannot guarantee the loss won't explode or oscillate.
**Quick check:** What does a large Hessian spectral norm imply about the loss landscape? (Answer: High curvature, potentially causing gradient descent to diverge or overshoot)

## Architecture Onboarding

### Component map:
DON: Input -> Branch Net (f) + Trunk Net (g) -> Inner Product (∑f_k g_k)
FNO: Input -> Encoder (P) -> [Spectral Conv (F⁻¹R F) + Bypass (W)] × L -> Decoder (Q)
Loss: Empirical L₂ distance between G_θ(u) and G†(u)

### Critical path:
1. **Initialization:** Must strictly follow Assumptions 3 (DON) and 6 (FNO) regarding variance scaling with width (σ₀ ≈ 1/√m)
2. **Width Selection:** Choose width m such that ∇Ḡ_t ≈ Ω(1/√m) is satisfied
3. **Optimization:** Run Gradient Descent (GD). Theory guarantees loss decrease only if iterates remain in B^Euc_ρ,ρ₁(θ₀)

### Design tradeoffs:
- **Width vs. Computation:** Increasing width m improves convergence guarantees but increases cost of Hessian/gradient computations
- **DON vs. FNO Complexity:** DONs require handling cross-interaction terms in the Hessian; FNOs require handling cross-derivatives between spatial and spectral domains

### Failure signatures:
- **Stagnation:** If loss plateaus, check if width is insufficient to keep α_t > 0
- **Divergence:** If iterates leave neighborhood B(θ₀), local RSC conditions no longer apply

### First 3 experiments:
1. **Width Ablation:** Train DON/FNO on Antiderivative problem sweeping widths m ∈ {10, 50, 100, 200, 500}. Verify if training loss decreases monotonically with width
2. **Neighborhood Radius Check:** Initialize wide FNO (m=500). Perturb weights by noise of scale ρ. Find threshold where loss stops decreasing, verifying theoretical ρ ∝ √m scaling
3. **Hessian Norm Tracking:** During DON training, compute spectral norm of Hessian blocks (H_ff, H_fg). Check if it remains bounded by constant c₂K²

## Open Questions the Paper Calls Out
1. Can theoretical convergence guarantees be extended to neural operators with vector-valued input functions?
2. Do optimization guarantees hold for FNOs with trainable encoders?
3. Can the convergence framework be adapted for non-smooth activation functions like ReLU?

## Limitations
- Theoretical guarantees only hold within bounded neighborhoods of initialization, not globally
- Width scaling relationships are asymptotic and empirical validation limited to three specific problems
- Specific network architectures (depth L, output dimension K) for DONs not explicitly specified in experiments

## Confidence
- **High Confidence:** Mechanism by which width affects RSC parameter (Mechanism 2) is mathematically rigorous
- **Medium Confidence:** Empirical demonstration that wider networks converge faster (Mechanism 3) is convincing for tested problems
- **Low Confidence:** Claims extend to general operator learning problems beyond three canonical examples

## Next Checks
1. For trained DON with width m=500, systematically increase perturbation radius ρ and measure where loss stops decreasing to verify ρ ∝ √m scaling
2. Track gradient norm ||∇Ḡ_t||² during training across different widths to verify it scales as 1/√m for wide networks
3. During FNO training, compute spectral norm of cross-derivative blocks between spatial and Fourier domains to verify it remains bounded by c₂K²