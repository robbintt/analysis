---
ver: rpa2
title: 'A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems:
  The Lifecycle of Knowledge in Visual Reasoning Task'
arxiv_id: '2504.17547'
source_url: https://arxiv.org/abs/2504.17547
tags:
- knowledge
- reasoning
- visual
- question
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically reviews knowledge-based vision question
  answering (KB-VQA) systems, categorizing them into three key stages: knowledge representation,
  knowledge retrieval, and knowledge reasoning. The paper highlights the unique challenges
  of KB-VQA, such as aligning heterogeneous multimodal information and retrieving
  relevant knowledge from large-scale sources.'
---

# A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task

## Quick Facts
- arXiv ID: 2504.17547
- Source URL: https://arxiv.org/abs/2504.17547
- Authors: Jiaqi Deng; Zonghan Wu; Huan Huo; Guandong Xu
- Reference count: 40
- Key outcome: Systematic review of KB-VQA systems categorizing them into knowledge representation, retrieval, and reasoning stages, exploring challenges of aligning multimodal information and retrieving relevant knowledge from large-scale sources.

## Executive Summary
This survey systematically reviews knowledge-based vision question answering (KB-VQA) systems, categorizing them into three key stages: knowledge representation, knowledge retrieval, and knowledge reasoning. The paper highlights the unique challenges of KB-VQA, such as aligning heterogeneous multimodal information and retrieving relevant knowledge from large-scale sources. It explores both explicit (structured/unstructured) and implicit (LLM-based) knowledge sources, as well as hybrid approaches. Key methods include graph-based reasoning, Chain-of-Thought prompting, and dynamic retrieval strategies. The survey identifies persistent challenges, such as noise in knowledge bases and interpretability, and outlines future research directions like adaptive systems and unified benchmarks. This work provides a comprehensive foundation for advancing KB-VQA models and their real-world applications.

## Method Summary
The paper conducts a qualitative literature review of approximately 40 academic papers from top-tier CV/NLP venues, analyzing KB-VQA systems through a knowledge lifecycle pipeline. The methodology involves systematic classification of architectures into three stages: Query Construction (Representation), Knowledge Retrieval, and Knowledge Reasoning. The survey synthesizes performance benchmarks using Exact Match (EM), VQAScore, and Retrieval Recall@K metrics across standard KB-VQA datasets including OK-VQA, A-OKVQA, KVQA, and InfoSeek. The analysis focuses on both explicit knowledge sources (structured/unstructured) and implicit sources (LLMs), examining hybrid approaches that combine these paradigms.

## Key Results
- Established a comprehensive taxonomy categorizing KB-VQA systems into three lifecycle stages: knowledge representation, knowledge retrieval, and knowledge reasoning
- Identified the critical challenge of aligning heterogeneous multimodal information across visual inputs and knowledge sources
- Highlighted the trade-offs between explicit knowledge bases (structured/unstructured) and implicit LLM-based knowledge sources
- Synthesized performance benchmarks showing the effectiveness of hybrid approaches combining multiple knowledge sources
- Outlined persistent challenges including knowledge noise, interpretability issues, and the need for unified evaluation benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Query Construction via Image-to-Text/Graph Translation
- Translating visual inputs into textual descriptions or structured graphs enables the system to bridge the modality gap, allowing text-only reasoners (like LLMs) or structured knowledge bases to process visual context. The system converts the input image into a modality compatible with the knowledge source through Image-to-text (captions, dense labels), Image-to-continuous-embedding (ViT features), or Image-to-graph (scene graphs). This alignment allows the retrieval module to search for relevant knowledge using visual concepts extracted from the image. The conversion process fails if generated captions or graphs omit critical visual details or introduce noise that misleads the retriever.

### Mechanism 2: Retrieval-Augmented Reasoning with Late Interaction
- Using late-interaction mechanisms (like FLMR) and iterative retrieval improves the granularity and relevance of knowledge fetched from large-scale corpora compared to single-vector encoding. Instead of encoding the whole image-question pair into one vector, late interaction computes fine-grained similarity scores between query tokens and document tokens. This allows the system to weight specific visual or textual cues more heavily during retrieval. Additionally, dynamic retrieval strategies allow agents to re-query based on intermediate results. The mechanism breaks if the asymmetric nature of the query (multimodal) and the document (unimodal text) is not handled correctly, leading to a semantic gap where visual concepts cannot find textual matches.

### Mechanism 3: Structural Reasoning over Heterogeneous Graphs
- Explicitly structuring reasoning using Graph Neural Networks (GNNs) or Chain-of-Thought (CoT) allows the system to infer answers requiring multi-step logic or high-order semantics that simple attention mechanisms cannot capture. The system constructs a graph containing visual objects and knowledge entities, then applies GNNs (e.g., GCN, RGCN) or hypergraphs to propagate information across nodes, effectively "reasoning" over the connections. Alternatively, CoT prompts LLMs to generate intermediate reasoning steps. Structural reasoning fails if the graph construction is noisy (wrong edges) or if the reasoning chain exceeds the model's capacity to track long-range dependencies.

## Foundational Learning

- **Concept: Dense Passage Retrieval (DPR)**
  - Why needed here: DPR is the dominant baseline for "Corpus Retrieval." You must understand how bi-encoders map queries and documents to a shared embedding space using contrastive loss before understanding advanced "late interaction" methods.
  - Quick check question: How does DPR handle the encoding of a query versus a document, and what is the primary loss function used to train the retriever?

- **Concept: Scene Graph Generation**
  - Why needed here: This is critical for "Image-to-graph" representation and "Graph-based Reasoning." The model must extract objects, attributes, and relationships from an image to form a structured graph for the reasoner.
  - Quick check question: What are the standard components (nodes/edges) of a scene graph, and how might a "visual graph" differ from a "semantic graph"?

- **Concept: Fusion-in-Decoder (FiD) vs. Fusion-in-Encoder (FiE)**
  - Why needed here: The survey explicitly distinguishes these fusion strategies. FiD is preferred for handling multiple retrieved documents without overwhelming the context window.
  - Quick check question: In FiD, how does the encoder process multiple knowledge documents differently than the decoder, and why is this beneficial for large evidence sets?

## Architecture Onboarding

- **Component map:** Image ($I$) + Question ($Q$) -> Query Constructor ($M_1$) -> Retriever ($M_2$) -> Post-Processor -> Reasoner ($M_3$) -> Answer
- **Critical path:** The alignment between the Query Constructor and the Retriever is the system's bottleneck. If the image-to-text conversion fails to capture the question-relevant visual entity, the Retriever will fetch irrelevant documents, and the Reasoner will hallucinate or fail.
- **Design tradeoffs:**
  - **Explicit vs. Implicit Knowledge:** Explicit (KGs) offers interpretability and grounded facts but lacks flexibility. Implicit (LLMs) offers zero-shot flexibility but risks hallucination.
  - **Image-to-Text vs. Image-to-Embed:** Text is human-readable and LLM-ready but lossy. Embeddings preserve visual semantics but require complex alignment networks.
  - **FiE vs. FiD:** FiE allows early interaction but is compute-heavy with many documents. FiD scales better but may miss early cross-document interactions.
- **Failure signatures:**
  - **Hallucination:** Answer contains information not present in image or retrieved knowledge (common in Implicit-only systems).
  - **Noise Sensitivity:** Answer is derived from an irrelevant retrieved document (failure in Retrieval Post-processing).
  - **Information Gap:** Model answers a generic question about the image type rather than the specific visual instance (failure in Query Construction).
- **First 3 experiments:**
  1. **Baseline Construction:** Implement a pipeline using a standard captioning model (Image-to-Text) -> DPR retrieval (Corpus) -> LLM generation. Evaluate on OK-VQA to establish a baseline for "knowledge retrieval."
  2. **Ablation on Representation:** Replace the Image-to-Text component with an Image-to-Embed (e.g., CLIP-based alignment) method. Compare retrieval recall (Recall@K) to see if visual semantics are better preserved.
  3. **Reasoning Stress Test:** Introduce a "multi-hop" reasoning dataset (like A-OKVQA or Encyclopedic-VQA). Compare a simple Attention-based reasoner against a Graph-based or CoT reasoner to measure performance on complex logical chains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can systems effectively perform multi-source knowledge fusion and conflict resolution when reasoning over heterogeneous information sources like structured graphs, unstructured text, and multimodal data?
- Basis in paper: Section VIII states that existing methods often rely on a single source, limiting their ability to resolve knowledge conflicts, and suggests future research should focus on multi-source fusion and noise mitigation.
- Why unresolved: Aligning semantics across different modalities and formats remains difficult, and noise from one source often propagates through multi-stage pipelines, degrading answer reliability.
- What evidence would resolve it: A model architecture that demonstrates superior performance on a benchmark specifically designed to test cross-source contradiction handling and noise filtering.

### Open Question 2
- Question: How can Reinforcement Learning from Human Feedback (RLHF) and continual learning be integrated to enhance the adaptability and interpretability of LLM-based KB-VQA models?
- Basis in paper: Section VIII identifies this as a promising direction to correct reasoning chains and validate outputs without catastrophic forgetting.
- Why unresolved: LLMs generally suffer from catastrophic forgetting when updated with new knowledge, and applying standard RLHF to the complex, multimodal retrieval-reasoning loop is non-trivial.
- What evidence would resolve it: A KB-VQA model that successfully updates its knowledge base in real-time and aligns its reasoning paths with human logic using a novel RLHF framework.

### Open Question 3
- Question: What is the best approach to developing a unified, large-scale benchmark that standardizes evaluation criteria for diverse KB-VQA tasks, specifically moving beyond closed knowledge bases?
- Basis in paper: Section VIII notes that systems currently choose varying datasets for evaluation, leading to unfair comparisons, and explicitly calls for a generalized benchmark to unify criteria.
- Why unresolved: Current datasets focus on specific niches (e.g., named entities vs. commonsense), making it difficult to fairly compare models that prioritize retrieval against those that prioritize reasoning.
- What evidence would resolve it: The release of a standardized dataset with metrics that evaluate both fine-grained retrieval accuracy and complex multi-hop reasoning simultaneously.

### Open Question 4
- Question: How can knowledge editing techniques be effectively adapted for KB-VQA to correct factual errors in implicit knowledge sources without requiring full model retraining?
- Basis in paper: Section IV-B3 notes that while solutions like knowledge editing exist for general LLMs, how to effectively apply them to the multimodal context of KB-VQA remains an open challenge.
- Why unresolved: Implicit knowledge is embedded in model parameters and cannot be easily updated, leading to hallucination risks that current post-processing filters cannot fully eliminate.
- What evidence would resolve it: A method that can precisely locate and modify specific factual triples within a multimodal model's weights to reflect updated world knowledge.

## Limitations

- The taxonomy of KB-VQA systems into "Explicit" and "Implicit" knowledge sources may not fully capture the nuances of emerging hybrid approaches that combine both paradigms.
- Performance benchmarks rely heavily on reported results from individual papers, which may use different evaluation protocols, knowledge corpus versions, and hyperparameter settings.
- The paper identifies persistent challenges such as knowledge noise and interpretability but does not provide systematic empirical validation of proposed solutions across diverse KB-VQA scenarios.

## Confidence

**High Confidence:** The three-stage lifecycle framework (Representation → Retrieval → Reasoning) accurately captures the essential components of KB-VQA systems. This is well-supported by the systematic analysis of ~40 papers and the clear delineation of architectural components in each stage.

**Medium Confidence:** The taxonomy distinguishing Explicit vs. Implicit knowledge sources is useful but incomplete for emerging hybrid approaches. The classification works well for traditional systems but requires refinement for modern LLM-integrated methods.

**Medium Confidence:** The identification of key challenges (knowledge noise, interpretability, multi-hop reasoning) reflects the current state of the field but lacks quantitative analysis of their relative impact on system performance.

## Next Checks

1. **Taxonomy Validation Experiment:** Implement a controlled classification experiment where 10 diverse KB-VQA models (including hybrid approaches) are independently mapped to the survey's lifecycle stages by multiple annotators. Measure inter-rater reliability and identify edge cases where current taxonomy breaks down.

2. **Benchmark Standardization Test:** Select 3-4 representative models from different taxonomy groups and re-implement them using standardized knowledge corpora (same Wikipedia dump, same KG version) and evaluation protocols. Compare performance consistency with originally reported results to assess benchmark reliability.

3. **Noise Impact Analysis:** Design an ablation study that systematically introduces controlled noise into knowledge retrieval results and measures degradation across different reasoning strategies (attention-based, graph-based, CoT). This would quantify the actual impact of knowledge noise on various KB-VQA architectures.