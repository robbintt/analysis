---
ver: rpa2
title: Attention Maps in 3D Shape Classification for Dental Stage Estimation with
  Class Node Graph Attention Networks
arxiv_id: '2509.07581'
source_url: https://arxiv.org/abs/2509.07581
tags:
- attention
- node
- cgat
- graph
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for explainable deep learning in
  high-stakes applications, specifically 3D shape analysis for dental stage assessment.
  The authors introduce the Class Node Graph Attention Network (CGAT), a graph neural
  network architecture that leverages graph attention convolutions and a virtual CLS
  node to generate interpretable attention maps explaining model decisions.
---

# Attention Maps in 3D Shape Classification for Dental Stage Estimation with Class Node Graph Attention Networks

## Quick Facts
- arXiv ID: 2509.07581
- Source URL: https://arxiv.org/abs/2509.07581
- Reference count: 40
- One-line primary result: CGAT achieves 0.76 weighted F1 score for third molar Demirjian stage classification with interpretable attention maps aligned with clinical expertise

## Executive Summary
This paper addresses the need for explainable deep learning in high-stakes applications, specifically 3D shape analysis for dental stage assessment. The authors introduce the Class Node Graph Attention Network (CGAT), a graph neural network architecture that leverages graph attention convolutions and a virtual CLS node to generate interpretable attention maps explaining model decisions. Applied to third molar meshes from CBCT images for Demirjian stage classification, CGAT demonstrates superior performance with a weighted F1 score of 0.76 and MAE of 0.25, outperforming baseline models including GAT (0.67 F1). The architecture shows that combining local mean curvature and distance-to-centroid features yields more comprehensive attention visualizations aligned with clinical expertise. Attention maps produced by CGAT highlight anatomically relevant regions (roots, crown, furcation) in a manner consistent with human expert assessment. The study concludes that CGAT offers both competitive predictive performance and transparent, human-understandable explanations, making it suitable for adoption in critical domains like forensic odontology.

## Method Summary
CGAT architecture adds a learnable CLS node to each input graph, connected to all mesh nodes via directed or undirected edges. The model uses L stacked GATv2 convolution blocks (8 attention heads, max-pooled), followed by LayerNorm and linear layers with residual connections. Classification is performed using only the CLS node embedding passed through an MLP. Attention rollout recursively aggregates attention weights across layers to create interpretable node-level attribution maps. The model is trained on 528 CBCT-derived third molar meshes (751±1 nodes) with mean curvature and distance-to-centroid features, using weighted batch sampling, cross-entropy loss, and Adam optimizer with learning rate scheduling.

## Key Results
- CGAT achieves weighted F1 score of 0.76 and MAE of 0.25 on third molar Demirjian stage classification
- CGAT outperforms baseline GAT model (0.67 F1) and consistently produces more interpretable attention maps
- Combining mean curvature and distance-to-centroid features yields more comprehensive attention visualizations aligned with clinical expertise
- Directed CLS edges produce more distributed attention across anatomical regions compared to undirected edges
- Model depth of 6-12 blocks recommended for optimal performance and attention quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The virtual CLS node creates a bottleneck that forces all classification-relevant information through a single embedding, enabling node-level attribution via attention weights.
- Mechanism: A learnable CLS node embedding is appended to each input graph and connected to all mesh nodes via directed edges (node→CLS). After L graph convolution blocks, only the CLS embedding passes to the MLP classifier. Since all decision information must transit through CLS, the learned attention weights α^CLS→i directly indicate each node's contribution to the final prediction.
- Core assumption: The model learns meaningful attention distributions rather than uniform/noisy attention; classification requires genuine shape discrimination rather than dataset artifacts.
- Evidence anchors:
  - [abstract] "models incorporating directed edges to a global CLS node produced more intuitive attention maps"
  - [section 2.2] "the classification section...takes the learned CLS node embedding...as the only input, therefore ensuring whatever information the final decision is based on flows through the CLS node"
  - [corpus] Weak direct corpus support; neighboring papers discuss interpretability but not CLS-node mechanisms specifically.
- Break condition: If attention weights become near-uniform (entropy → log(n)), the mechanism fails to provide discriminative explanations.

### Mechanism 2
- Claim: Attention rollout aggregates multi-layer attention scores into a single node-level attribution map that reflects cumulative information flow.
- Mechanism: At each layer l, attention matrix A^(l) ∈ ℝ^(n+1)×(n+1) captures learned edge weights. Rollout recursively computes Ã^(l) = (A^(l) + I)Ã^(l-1), where I accounts for residual connections. The final Ã^(L) values a_CLS→i are visualized on mesh nodes to show holistic contribution across all layers.
- Core assumption: Attention across layers is approximately compositional; residual connections preserve identity pathways.
- Evidence anchors:
  - [abstract] "visualized via attention rollout, to explain its decision-making process"
  - [section 2.2] "With this recursive operation, the 'flow' of the attention from the CLS node to each input node can be represented holistically"
  - [corpus] No corpus papers reference attention rollout specifically for 3D meshes.
- Break condition: If attention matrices are near-identity or layer-wise attention becomes uncorrelated, rollout may amplify noise rather than signal.

### Mechanism 3
- Claim: Pose-invariant geometric features (mean curvature, distance-to-centroid) constrain attention to anatomically meaningful regions, yielding explanations aligned with clinical expertise.
- Mechanism: Mean curvature κ̄ captures local convexity/concavity—high at root apices and furcation. Distance-to-centroid d captures global shape elongation during root development. Both are rotation/translation invariant, avoiding spurious pose-dependent attention. Combined features produce attention maps that integrate curvature-sensitive (local) and extent-sensitive (global) regions.
- Core assumption: Clinical staging criteria are primarily shape-based and align with these geometric descriptors; pose does not carry diagnostic signal.
- Evidence anchors:
  - [abstract] "combining local mean curvature and distance-to-centroid features yields more comprehensive attention visualizations aligned with clinical expertise"
  - [section 2.3] "mean curvature...is indicative of the concavity of the neighborhood...a high mean curvature value in a node can be indicative of the development of the roots"
  - [corpus] Neighboring papers (DentalGPT, interpretability analysis of molar staging) confirm XAI importance in dental AI but don't evaluate these specific features.
- Break condition: If features lack discriminative power across classes (overlapping distributions in Fig. 5), attention may not localize meaningfully.

## Foundational Learning

- **Graph Attention Networks (GAT/GATv2):**
  - Why needed here: CGAT builds on GATv2 convolutions; understanding how attention weights α_ij are computed (LeakyReLU on concatenated transformed features) is prerequisite to interpreting rollout.
  - Quick check question: Can you explain why GATv2's dynamic attention (Eq. 3) is more expressive than GAT's static attention (Eq. 2)?

- **Multi-head Attention and Head Aggregation:**
  - Why needed here: CGAT uses K=8 attention heads merged via max-pooling; this design choice affects both representation power and attention map characteristics.
  - Quick check question: What is the trade-off between concatenating vs. max-pooling vs. averaging attention heads for explainability?

- **3D Mesh Representation and Geodesic Topology:**
  - Why needed here: Meshes are converted to undirected graphs G(V,E); understanding that edges encode geodesic connectivity (not just spatial proximity) clarifies why GNNs outperform point cloud methods here.
  - Quick check question: Why does the paper note that point clouds suffer from "topologically unassociated" neighbors while meshes do not?

## Architecture Onboarding

- **Component map:** Input node features → Linear projection → CGAT blocks (GATv2Conv→LayerNorm→Linear→LayerNorm) → CLS embedding → Dropout → MLP → Softmax

- **Critical path:** Node features X → projection → CGAT blocks → CLS embedding → MLP → logits. All explainability flows through attention weights α^CLS→i computed across layers.

- **Design tradeoffs:**
  - Directed (→) vs. undirected (↔) CLS edges: Directed yields more distributed attention across multiple anatomical regions; undirected enables 2-hop global connectivity but can over-concentrate attention in deeper models
  - Model depth L: 1-block models show noisy/patchy attention; 6-12 blocks recommended; >14 blocks risk over-smoothing
  - Feature selection: Combined features slightly outperform single features (0.76 vs. 0.71 F1) and produce more comprehensive attention maps

- **Failure signatures:**
  - Noisy/mosaic attention maps → model too shallow (L=1), increase depth
  - Attention concentrated on single irrelevant region → undirected edges with deep model, try directed edges
  - Near-uniform attention across all nodes → features may lack discriminative power or model undertrained
  - Performance degradation with depth → over-smoothing; reduce L or add residual connections

- **First 3 experiments:**
  1. Feature ablation: Train separate models with mean curvature only, distance-to-centroid only, and both combined; compare F1 scores and attention map characteristics to validate feature-geometry coupling.
  2. Depth sweep with attention visualization: Train models with L ∈ {1, 3, 7, 11, 14} blocks; plot F1 vs. depth and visualize attention rollout at each depth to identify optimal operating range and observe attention smoothing.
  3. Edge direction comparison: For a fixed L (e.g., 11 blocks), train identical models with directed vs. undirected CLS edges; quantify attention entropy and qualitatively assess whether attention localizes to clinically relevant regions (roots, crown, furcation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of attention head merging function (max-pooling vs. mean vs. concatenation) affect both classification performance and the quality of attention-based explanations in CGAT?
- Basis in paper: [explicit] "In order to further analyze the explanation capabilities of the CGAT architecture, we argue that the merging function of the attention heads is an important avenue to explore. Many permutation-invariant merging functions exist, such as the mean and concatenation, aside from the max-pooling that we employed."
- Why unresolved: The paper only evaluated max-pooling for merging multi-head attention outputs; alternative merging strategies were not compared.
- What evidence would resolve it: Systematic comparison of classification metrics (F1, MAE) and attention map quality across different merging functions on the same dental staging task.

### Open Question 2
- Question: Does CGAT generalize to other graph-level classification/regression tasks beyond dental meshes, such as molecular property prediction or biological network analysis?
- Basis in paper: [explicit] "Further applications of the CGAT models on different mesh datasets where shape information is critical, and on different graph-definable data domains such as molecular interactions or image classification, can reveal more insight into the attention-based explanations generated by CGAT."
- Why unresolved: CGAT was demonstrated only on third molar meshes for Demirjian staging; no experiments on other domains were conducted.
- What evidence would resolve it: Evaluation of CGAT on benchmark graph classification datasets (e.g., molecular graphs, brain connectivity networks) with attention map analysis.

### Open Question 3
- Question: Can attention map quality be quantitatively validated against expert annotations of clinically relevant anatomical regions?
- Basis in paper: [inferred] The paper evaluates attention maps qualitatively through visual inspection and alignment with Demirjian criteria, but no quantitative metric or expert annotation-based validation is provided.
- Why unresolved: Attention map evaluation relied on subjective alignment with anatomical expectations rather than ground-truth region importance.
- What evidence would resolve it: Correlation analysis between attention rollout scores and expert-labeled regions of interest (roots, crown, furcation) across multiple test samples.

### Open Question 4
- Question: How robust are CGAT attention explanations to variations in mesh resolution and decimation levels?
- Basis in paper: [inferred] Meshes were uniformly decimated to ~751 nodes using quadric mesh decimation, but the effect of this preprocessing choice on attention stability was not analyzed.
- Why unresolved: The degree of mesh simplification may affect which anatomical features are preserved and consequently alter attention patterns.
- What evidence would resolve it: Comparison of attention maps across multiple decimation levels (e.g., 500, 750, 1000, 2000 nodes) on the same tooth samples to assess attention consistency.

## Limitations
- Lack of quantitative validation for clinical relevance of attention maps beyond expert intuition
- 0.76 F1 score still leaves substantial classification error unexplained given inherent ambiguity in Demirjian staging
- Feature distribution overlaps suggest inherent limitations in discriminative power that attention maps cannot fully resolve
- Absence of ablation studies on edge directionality and model depth in main results limits understanding of architectural sensitivity

## Confidence
- High confidence in architectural design and implementation details
- Medium confidence in attention mechanism's ability to generate clinically interpretable explanations (validation relies on expert assessment rather than quantitative metrics)
- Medium confidence in feature selection (modest performance gains from combining features, overlapping feature distributions)

## Next Checks
1. **Attention-map quantification:** Develop metrics to measure attention-map alignment with clinically annotated regions of interest across all classes, beyond qualitative expert assessment.
2. **Robustness testing:** Evaluate model performance and attention stability under mesh perturbation (noise, decimation, rotation) to verify pose-invariance claims.
3. **Clinical deployment simulation:** Test model and attention explanations on an external dataset of CBCT scans from different populations to assess generalizability and practical utility for forensic odontologists.