---
ver: rpa2
title: 'Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual
  and Supervised Fine-Tuning Study'
arxiv_id: '2512.03976'
source_url: https://arxiv.org/abs/2512.03976
tags:
- tibetan
- translation
- language
- fine-tuning
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of adapting large language models
  to low-resource languages by developing a two-stage framework for Tibetan adaptation.
  The approach combines continual pretraining (CPT) to establish Tibetan linguistic
  grounding using monolingual text, followed by supervised fine-tuning (SFT) for task
  and translation specialization.
---

# Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study

## Quick Facts
- arXiv ID: 2512.03976
- Source URL: https://arxiv.org/abs/2512.03976
- Reference count: 7
- Major improvement in Tibetan language model adaptation via two-stage CPT+SFT framework

## Executive Summary
This study tackles the challenge of adapting large language models to low-resource languages by developing a two-stage framework for Tibetan adaptation. The approach combines continual pretraining (CPT) to establish Tibetan linguistic grounding using monolingual text, followed by supervised fine-tuning (SFT) for task and translation specialization. The empirical results demonstrate substantial improvements across all evaluation metrics, with perplexity decreasing by nearly half and translation quality improving dramatically for both Chinese→Tibetan and English→Tibetan directions.

## Method Summary
The researchers employed a two-stage fine-tuning approach to adapt large language models to Tibetan. The first stage involved continual pretraining on monolingual Tibetan text to establish basic linguistic competence. The second stage applied supervised fine-tuning on task-specific data, including translation pairs and other supervised objectives. This sequential approach leverages monolingual data for foundational language understanding before applying task-specific supervision for specialized capabilities.

## Key Results
- Perplexity reduced from 2.98 to 1.54 after adaptation
- Chinese→Tibetan translation BLEU improved from 0.046 to 0.261 (5.7× improvement)
- English→Tibetan translation chrF increased from 2.2 to 6.6 (3× improvement)

## Why This Works (Mechanism)
The two-stage adaptation framework works because it first establishes Tibetan linguistic grounding through monolingual data exposure, creating a foundation that the model can build upon during task-specific fine-tuning. The CPT stage enables the model to learn Tibetan-specific syntax, morphology, and semantic patterns, while the subsequent SFT stage specializes this foundation for specific tasks like translation. This sequential approach allows the model to develop task-relevant representations that build upon rather than overwrite the foundational Tibetan language understanding.

## Foundational Learning
- **Continual Pretraining (CPT)**: Adapting pretrained models to new domains/languages using additional training on target-domain data; needed because base models lack Tibetan linguistic patterns, verified by comparing perplexity before/after CPT
- **Supervised Fine-Tuning (SFT)**: Adapting models to specific tasks using labeled data; essential for translation specialization, confirmed by performance improvements on translation tasks
- **Cross-lingual Transfer**: Knowledge transfer between languages sharing structural similarities; explains English→Tibetan gains, validated by improved performance on related language pairs
- **Layer-wise Adaptation**: Different model layers learn different aspects of the adaptation; critical for understanding where knowledge is encoded, supported by analysis showing embedding/output heads dominate early adaptation
- **Low-resource Language Adaptation**: Specialized techniques for languages with limited training data; motivates the entire study, evidenced by dramatic improvements in translation quality

## Architecture Onboarding

Component Map:
Embedding Layer -> Encoder Blocks -> Decoder Blocks -> Output Heads

Critical Path:
Monolingual Tibetan text → CPT stage → Translation pairs → SFT stage → Task-specific adaptation

Design Tradeoffs:
The two-stage approach trades computational efficiency for better adaptation quality compared to single-stage methods. While more expensive computationally, it allows for better separation of concerns between language grounding and task specialization. The sequential nature may limit flexibility compared to parallel adaptation approaches.

Failure Signatures:
- Minimal perplexity reduction indicates insufficient CPT data or poor data quality
- Translation performance improvements without perplexity reduction suggests overfitting to translation data without general language understanding
- Uneven layer-wise adaptation may indicate optimization issues or architectural limitations

First Experiments:
1. Compare single-stage versus two-stage adaptation to quantify the contribution of each stage
2. Test different data compositions in CPT (monolingual Tibetan vs. bilingual data)
3. Evaluate model performance on non-translation tasks to assess general language capability

## Open Questions the Paper Calls Out
The paper identifies several key open questions regarding the generalizability of the two-stage adaptation approach to other low-resource languages, the optimal balance between CPT and SFT stages, and the potential for extending the framework to additional NLP tasks beyond translation. The authors also highlight the need for more comprehensive evaluation across diverse Tibetan language tasks and larger evaluation datasets to establish statistical significance.

## Limitations
- Evaluation limited primarily to translation tasks, not assessing broader Tibetan language capabilities
- Small evaluation datasets raise concerns about statistical significance of results
- Two-stage framework may not be optimal compared to alternative adaptation architectures
- Layer-wise analysis based on single model configuration without ablation studies

## Confidence

High confidence:
- The general effectiveness of the two-stage adaptation approach for Tibetan language modeling

Medium confidence:
- The specific layer-wise adaptation patterns identified in the analysis
- The cross-lingual transfer capabilities for English→Tibetan translation

## Next Checks

1. Conduct ablation studies to isolate the contribution of CPT versus SFT stages, testing variations such as direct SFT without CPT and different CPT data compositions

2. Expand evaluation beyond translation to include diverse Tibetan NLP tasks such as summarization, question answering, and named entity recognition to assess general capability

3. Perform statistical significance testing with confidence intervals on all reported metrics, particularly for the small evaluation datasets, to establish robust performance claims