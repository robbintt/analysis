---
ver: rpa2
title: Multi-Task Learning for Extracting Menstrual Characteristics from Clinical
  Notes
arxiv_id: '2503.24116'
source_url: https://arxiv.org/abs/2503.24116
tags:
- menstrual
- clinical
- retrieval
- dysmenorrhea
- notes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the challenge of extracting key menstrual\
  \ health attributes\u2014dysmenorrhea, regularity, flow volume, and intermenstrual\
  \ bleeding\u2014from unstructured clinical notes, which are rarely documented in\
  \ structured EHRs. A pipeline combining GatorTron with Multi-Task Prompt-Based Learning\
  \ and hybrid retrieval preprocessing was developed."
---

# Multi-Task Learning for Extracting Menstrual Characteristics from Clinical Notes

## Quick Facts
- arXiv ID: 2503.24116
- Source URL: https://arxiv.org/abs/2503.24116
- Reference count: 28
- Multi-task prompt-based learning achieves 90% average F1-score for extracting five menstrual health attributes from clinical notes

## Executive Summary
This study addresses the challenge of extracting key menstrual health attributes—dysmenorrhea, regularity, flow volume, and intermenstrual bleeding—from unstructured clinical notes, which are rarely documented in structured EHRs. A pipeline combining GatorTron with Multi-Task Prompt-Based Learning and hybrid retrieval preprocessing was developed. This approach retrieves the most relevant text segments using keyword and semantic search, then performs classification across five menstrual attributes. The method achieved an average F1-score of 90% across all tasks, outperforming baselines including Supervised Fine-Tuning and In-Context Learning. Hybrid retrieval consistently improved performance, and the multi-task framework generalized well to unseen data.

## Method Summary
The pipeline uses hybrid retrieval (BM25 + MedEmbed) to extract top 10 text segments from clinical notes, followed by Multi-Task Prompt-Based Learning with GatorTron-Base. Five task-specific prompt templates and verbalizers are used to classify five menstrual attributes: dysmenorrhea (yes/no/unknown), dysmenorrhea severity (mild/moderate/severe/unknown), regularity (regular/irregular/unknown), flow (scanty/normal/abundant/unknown), and intermenstrual bleeding (yes/no/unknown). The model is trained on 91 notes with 3-fold cross-validation and evaluated on 49 held-out notes.

## Key Results
- Average F1-score of 90% across all five menstrual attribute classification tasks
- Hybrid retrieval preprocessing consistently improved performance across all approaches
- Multi-task framework generalized well to unseen data, outperforming single-task baselines
- Outperformed Supervised Fine-Tuning and In-Context Learning baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid retrieval preprocessing significantly boosts classification performance by filtering noise and prioritizing relevant clinical context.
- **Mechanism:** A hybrid search combining BM25 (lexical matching) and MedEmbed (semantic similarity) extracts the top-k segments from lengthy notes. This reduces the input length to fit model constraints (e.g., 512 tokens) and focuses the model's attention on segments containing menstrual health keywords, rather than irrelevant clinical history.
- **Core assumption:** Menstrual characteristics are localized in specific text spans that match the predefined retrieval query (e.g., keywords like "dysmenorrhea", "flow volume").
- **Evidence anchors:**
  - [abstract]: "The retrieval step consistently improves performance across all approaches, allowing the model to focus on the most relevant segments of lengthy clinical notes."
  - [section]: "retrieval increases the F1-score from 0.231 to 0.927... for ClinicalLongformer" and "retrieval is not only beneficial for overcoming token length constraints but also crucial for structuring and filtering relevant text segments."
  - [corpus]: Neighbor paper "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs" supports the general efficacy of retrieval in managing long EHR contexts.
- **Break condition:** If relevant information is scattered across more than the top-10 retrieved segments, or if the retrieval query fails to capture the specific phrasing used in the note.

### Mechanism 2
- **Claim:** Multi-Task Prompt-Based Learning (MTPBL) improves generalization on limited data by learning shared representations across related attributes.
- **Mechanism:** The model processes multiple related classification tasks (e.g., flow, regularity, pain) simultaneously using a shared backbone. This acts as a regularizer, allowing the model to leverage common semantic features of "menstrual health" context to improve performance on tasks where data is sparse (e.g., intermenstrual bleeding).
- **Core assumption:** The extraction tasks for different menstrual attributes share underlying semantic cues and dependencies within the clinical text.
- **Evidence anchors:**
  - [abstract]: "The multi-task framework generalized well to unseen data."
  - [section]: "MTPBL + retrieval achieves the best overall F1-score (0.903)... generalizes more effectively to unseen data compared to single-task methods."
  - [corpus]: General MTL benefits are implied by standard NLP literature, but specific corpus neighbors for this paper focus more on extraction agents than MTL specifically.
- **Break condition:** If tasks are conflicting or "negative transfer" occurs, where learning one task degrades performance on another.

### Mechanism 3
- **Claim:** Prompt-based learning (PBL) is more data-efficient than supervised fine-tuning (SFT) by aligning the task with the model's pre-training objective.
- **Mechanism:** Instead of randomly initializing a classification head, PBL uses manual templates and verbalizers to map inputs to label words (e.g., "heavy" -> "abundant"). This allows the model to leverage the pre-trained Masked Language Modeling (MLM) knowledge directly, requiring fewer gradient updates.
- **Core assumption:** The pre-trained language model (GatorTron) already possesses sufficient semantic understanding of clinical terminology to map masked tokens to the correct verbalizer labels.
- **Evidence anchors:**
  - [abstract]: "It outperforms baseline methods... despite being trained on fewer than 100 annotated clinical notes."
  - [section]: "PBL uses the masked language modeling (MLM) objective... enhancing performance with minimally labeled data."
  - [corpus]: Weak specific evidence in neighbor papers regarding PBL vs. SFT; neighbors focus largely on general LLM extraction capabilities.
- **Break condition:** If the label space is complex or the "verbalizer" words (e.g., "scanty", "abundant") are ambiguous or rare in the model's pre-training corpus.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - **Why needed here:** The paper uses GatorTron, an MLM-based model. Understanding that the model predicts hidden tokens based on context explains why "fill-in-the-blank" prompts are effective for classification.
  - **Quick check question:** If you mask the word "severe" in "The patient reported [MASK] dysmenorrhea," what probability distribution should the model produce over your verbalizer labels?

- **Concept: Hybrid Retrieval (BM25 + Semantic Search)**
  - **Why needed here:** The pipeline relies on retrieving relevant segments before classification. You must understand how BM25 (exact keyword matching) complements semantic search (conceptual matching) to handle medical synonyms and misspellings.
  - **Quick check question:** Why would a semantic search model retrieve a segment containing "painful periods" when your query only specified "dysmenorrhea"?

- **Concept: Verbalizers**
  - **Why needed here:** The prompt-based approach maps model outputs to classes via verbalizers. You need to define these mappings (e.g., "regular" -> `regular`) to decode the model's prediction.
  - **Quick check question:** If a clinical note uses the phrase "cycles are like clockwork," would your "regularity" verbalizer capture this, or does the prompt template need adjustment?

## Architecture Onboarding

- **Component map:** Raw Clinical Note -> Hybrid Retrieval (BM25 + MedEmbed) -> Top 10 Segments -> Prompt Template Engine + Verbalizer Map -> Shared GatorTron-Base backbone -> Softmax over verbalizer logits -> Class Label

- **Critical path:** The **Hybrid Retrieval** step is the highest leverage component. The paper notes that naive segmentation (splitting by double spaces) caused errors (Example 2 in text). If retrieval fails to capture the segment, the model cannot classify correctly regardless of the prompt design.

- **Design tradeoffs:**
  - **k=10 segments:** The authors chose 10 assuming 4 attributes wouldn't span more. Increasing `k` increases context but adds noise and compute cost.
  - **Manual Prompts:** High performance but requires domain expertise to craft templates; difficult to scale to new attributes automatically compared to In-Context Learning (ICL).

- **Failure signatures:**
  - **Segmentation Errors:** Splitting text at double spaces breaks semantic units (e.g., separating "Dysmenorrhea:" from "Moderate"), causing retrieval misses.
  - **Narrative vs. Structured:** The model struggles with narrative descriptions (e.g., "not painful") compared to structured entries (e.g., "Dysmenorrhea: None").
  - **Underdocumentation:** The model defaults to "unknown" if the attribute is not explicitly mentioned (e.g., intermenstrual bleeding only appears in 13% of notes).

- **First 3 experiments:**
  1. **Retrieval Ablation:** Run the pipeline with `k=0` (no retrieval) vs. `k=10` to establish the baseline performance gain on the specific dataset.
  2. **Segmentation Audit:** manually review 20 cases where the model predicted "unknown" for a known positive case to check if the retrieval step actually fetched the relevant sentence.
  3. **Verbalizer Sensitivity:** Test if expanding the verbalizer list (e.g., adding "clockwork" for regularity) improves recall on narrative notes.

## Open Questions the Paper Calls Out

- **Question:** Can the MTPBL pipeline maintain high performance when validated on larger, multi-institutional datasets?
  - **Basis in paper:** [explicit] The conclusion states that "validating these methods on larger, multi-institutional datasets... will be critical to ensuring their robustness and broader applicability."
  - **Why unresolved:** The current study used a small dataset (N=140) from a single institution (Mount Sinai), which may not capture the variance in clinical note structures across different hospital systems.
  - **What evidence would resolve it:** Evaluation of the model's F1-scores on a diverse, multi-site dataset to demonstrate generalizability beyond the specific source data.

- **Question:** Can automated prompt generation and adaptive retrieval replace manual design without sacrificing accuracy?
  - **Basis in paper:** [explicit] The conclusion identifies the need to focus on "reducing manual effort through automated prompt generation and adaptive retrieval" to address the limitation of manually creating templates and queries.
  - **Why unresolved:** The current success relies on manually engineered prompts and retrieval queries, which is labor-intensive and may require specific domain expertise for new attributes.
  - **What evidence would resolve it:** A comparative study showing that automatically generated prompts achieve statistical parity (non-inferiority) with the manually designed templates used in this study.

- **Question:** What is the impact of advanced segmentation strategies on reducing retrieval errors in clinical notes?
  - **Basis in paper:** [inferred] The Discussion notes that the "rule-based approach, using double spaces as delimiters... did not always work reliably," and explicitly calls for "more robust segmentation strategies."
  - **Why unresolved:** The error analysis revealed that simple segmentation led to missing context (e.g., splitting "Dysmenorrhea: None Moderate"), directly causing misclassification.
  - **What evidence would resolve it:** Ablation studies comparing the current rule-based splitter against learned sentence boundary detection models to quantify the reduction in extraction errors.

## Limitations

- The dataset contains only 140 total clinical notes (91 train, 49 test), with class imbalances (e.g., intermenstrual bleeding in only 13% of notes), raising concerns about model generalizability.
- Key hyperparameters (learning rate, batch size, epochs, optimizer configuration, warmup steps) are unspecified, making exact replication difficult.
- The hybrid retrieval combination method (how BM25 and semantic scores are weighted) is not detailed, preventing faithful reproduction of the retrieval pipeline.

## Confidence

- **High Confidence:** The core mechanism of hybrid retrieval improving performance by filtering noise and focusing on relevant segments is well-supported by the evidence provided (F1 increase from 0.231 to 0.927 when retrieval is applied).
- **Medium Confidence:** The superiority of multi-task learning for this specific application is demonstrated but could benefit from more extensive ablation studies comparing MTL vs. single-task performance across all attributes.
- **Medium Confidence:** The data efficiency claim for prompt-based learning is supported by the small training set (fewer than 100 annotated notes) but lacks direct comparison metrics with SFT on identical conditions.

## Next Checks

1. **Retrieval Ablation Study:** Run the complete pipeline with k=0 (no retrieval) vs. k=10 on the test set to quantify the exact performance gain attributable to hybrid retrieval in this specific context.

2. **Segmentation Robustness Audit:** Manually examine 20 cases where the model predicted "unknown" for attributes known to be present in the ground truth to determine if retrieval failures or segmentation errors are the primary cause.

3. **Cross-Dataset Generalization Test:** Evaluate the trained model on an independent dataset of clinical notes from a different institution or clinical setting to assess whether the 90% F1-score generalizes beyond the Mount Sinai well-woman visit notes.