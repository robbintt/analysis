---
ver: rpa2
title: 'Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance
  Transfer in LLMs'
arxiv_id: '2511.17572'
source_url: https://arxiv.org/abs/2511.17572
tags:
- epistemic
- community
- russian
- frame
- bucha
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) aligned
  to specific online communities exhibit generalizable epistemic behaviors under uncertainty
  or merely recall training patterns. The authors introduce a framework that removes
  factual knowledge about target events (validated through multiple probes) and tests
  whether aligned models maintain community-specific response patterns when faced
  with novel scenarios.
---

# Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs

## Quick Facts
- arXiv ID: 2511.17572
- Source URL: https://arxiv.org/abs/2511.17572
- Authors: Patrick Gerard; Aiden Chang; Svitlana Volkova
- Reference count: 40
- Key outcome: Aligned LLMs maintain community-specific epistemic patterns (JS<0.16) even after aggressive knowledge deletion, demonstrating generalizable stance transfer beyond surface mimicry.

## Executive Summary
This study investigates whether large language models aligned to specific online communities exhibit generalizable epistemic behaviors under uncertainty or merely recall training patterns. The authors introduce a framework that removes factual knowledge about target events (validated through multiple probes) and tests whether aligned models maintain community-specific response patterns when faced with novel scenarios. Using Russian-Ukrainian military discourse and U.S. partisan Twitter data, they find that even after aggressive knowledge deletion, aligned LLMs maintain stable, community-specific behavioral patterns. These results demonstrate that alignment encodes structured, generalizable epistemic stances beyond surface mimicry, providing evidence that community-aligned models can generalize behaviors under ignorance.

## Method Summary
The study uses six alignment conditions (lightweight prompting and heavyweight fine-tuning) on community corpora around target events, applies SAMI knowledge deletion to remove factual recall, and evaluates stance transfer via JS divergence against organic community baselines. Models are classified using Qwen3-8B on anonymized event prompts, with validation through LAMA-style probes, adversarial paraphrases, and entropy measurements to ensure stance stability under knowledge deletion.

## Key Results
- Aligned models maintain JS divergence <0.16 vs. 0.29-0.51 for misaligned models after knowledge deletion
- Entropy remains stable (H<0.7 bits) for aligned models vs. H>1.2 bits for controls (p<0.001)
- Both lightweight (prompting) and heavyweight (fine-tuning) alignment methods produce qualitatively similar epistemic stance transfer

## Why This Works (Mechanism)

### Mechanism 1: Separation of Factual Knowledge from Epistemic Stance
Community alignment encodes reasoning patterns that operate independently of specific factual knowledge. Alignment procedures embed community-specific response policies into model parameters or context, stored in different attention heads and distributed representations than event-specific facts.

### Mechanism 2: Stance Generalization via Response Operation Distributions
Aligned models learn stochastic mappings over response operations (dispute, seek_alternative_cause, suspend_judgment, demand_evidence, defer_to_authority) that transfer to novel scenarios. Rather than memorizing specific responses, models learn distributions πc(r|u) over response operations conditioned on uncertainty context.

### Mechanism 3: Cross-Method Alignment Consistency
Both lightweight (prompting) and heavyweight (fine-tuning) alignment methods produce qualitatively similar epistemic stance transfer. System prompts establish persona context that persists across inference; fine-tuning embeds similar patterns into weights.

## Foundational Learning

- **Epistemic Stance**: A structured response policy for handling uncertainty (how to evaluate evidence, attribute causality, defer judgment).
  - Why needed here: The paper's core construct; without understanding this, you cannot interpret what the models are transferring.
  - Quick check question: Can you distinguish "disputing evidence" from "seeking alternative cause" as distinct epistemic operations?

- **Jensen-Shannon Divergence**: A symmetric, bounded [0,1] measure of distributional similarity between two probability distributions.
  - Why needed here: Primary metric for quantifying stance transfer; JS<0.16 indicates strong alignment, JS>0.29 indicates misalignment.
  - Quick check question: If two distributions have JS=0.05, are they more or less similar than distributions with JS=0.40?

- **SAMI (Scalar Attention Module Intervention)**: A method that identifies attention heads encoding specific knowledge and suppresses them during inference.
  - Why needed here: The technique enabling controlled experiments by deleting factual knowledge while preserving reasoning.
  - Quick check question: Why is attention-based deletion preferable to simply removing training examples containing target events?

## Architecture Onboarding

- **Component map**: Community corpus → Frame annotation pipeline → Alignment (prompt/finetune) → SAMI knowledge deletion → Stance classification → JS divergence computation vs. organic baseline
- **Critical path**: SAMI deletion validation is the highest-risk component—if deletion is incomplete, observed stance patterns may reflect residual memorization rather than transfer.
- **Design tradeoffs**: Lightweight alignment (fast, reproducible) vs. fine-tuning (slightly better fidelity, requires data collection). Larger models show marginally lower entropy but smallest tested models (1B) still demonstrate transfer.
- **Failure signatures**:
  - High post-deletion factual probe accuracy (>20%): incomplete knowledge removal
  - High entropy (>1.0 bits) for aligned models post-deletion: stance not truly internalized
  - Cross-community condition matching baseline: alignment failed specificity check
- **First 3 experiments**:
  1. Validate SAMI deletion on your target event: confirm <5% accuracy on factual probes while maintaining >90% on unrelated control tasks
  2. Replicate single-community alignment with system prompt vs. fine-tuning: verify JS divergence patterns match paper (0.10-0.16 range)
  3. Test cross-community specificity: confirm Russian-aligned models evaluated on Ukrainian organic discourse show high JS divergence (>0.28), establishing alignment is community-specific not generic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does epistemic stance transfer generalize to non-temporal, non-political domains such as scientific forums, medical discourse, or academic disciplines?
- Basis in paper: [explicit] The authors state: "Testing on non-temporal epistemic communities (e.g., scientific forums, academic disciplines) would strengthen claims about epistemic stance transfer across diverse knowledge domains, and we plan to extend our methodology to such contexts in future work."
- Why unresolved: The study only tests temporally bounded events (military conflicts, political debates) with clean pre/post-event data separation, limiting claims to these contexts.
- What evidence would resolve it: Replicating the SAMI deletion and alignment protocol on scientific or medical community corpora, demonstrating similar JS divergence patterns between aligned models and organic baselines.

### Open Question 2
- Question: At what point during fine-tuning does robust epistemic stance transfer emerge, and can this be identified through mechanistic interpretability techniques?
- Basis in paper: [explicit] The authors state: "We plan to incorporate mechanistic interpretability techniques (e.g., Su et al. [13]) to pinpoint when robust stance transfer emerges during fine-tuning."
- Why unresolved: The study demonstrates that stance transfer occurs but does not investigate the training dynamics or specific parameter changes that enable it.
- What evidence would resolve it: Layer-wise or epoch-wise analysis of model checkpoints during fine-tuning, correlating attention head patterns with emerging stance fidelity to organic community distributions.

### Open Question 3
- Question: What internal model pathways specifically drive epistemic stance transfer, and are they distinct from those encoding factual knowledge?
- Basis in paper: [explicit] The authors state: "Longer term, our goal is to...investigate the internal model pathways that drive framework transfer."
- Why unresolved: SAMI deletion shows that factual and stance knowledge can be dissociated, but the specific circuits or attention patterns encoding generalized epistemic behaviors remain unidentified.
- What evidence would resolve it: Causal intervention studies (e.g., activation patching) isolating attention heads or MLP layers responsible for stance-consistent responses under knowledge deletion.

## Limitations
- SAMI's selective deletion effectiveness may not completely eliminate all factual knowledge, with potential semantic leakage through indirect associations
- The study only tests temporally bounded events with clean pre/post-event separation, limiting generalizability to other contexts
- Cross-method alignment consistency shown on small sample (3 communities) limits broader claims about method equivalence

## Confidence
- **High Confidence**: Community-specific response pattern stability (JS divergence <0.16 for aligned vs. 0.29-0.51 for misaligned models)
- **Medium Confidence**: Stance generalization claim—interpretation as "generalizable epistemic behaviors" requires stronger validation
- **Medium Confidence**: Cross-method alignment consistency—small sample limits generalizability

## Next Checks
1. **Semantic Probing**: Test whether aligned models' responses contain implicit factual references that weren't explicitly prompted but may influence epistemic stance
2. **Cross-Event Transfer**: Apply SAMI deletion to one community/event, then test stance transfer to completely unrelated events to determine if patterns are truly event-specific or represent broader epistemic frameworks
3. **Adversarial Community Boundaries**: Construct synthetic community alignments that blend multiple perspectives and test whether models maintain coherent stances or produce hybrid responses, validating the discrete community assumption