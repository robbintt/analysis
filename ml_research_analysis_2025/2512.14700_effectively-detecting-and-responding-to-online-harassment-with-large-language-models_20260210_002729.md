---
ver: rpa2
title: Effectively Detecting and Responding to Online Harassment with Large Language
  Models
arxiv_id: '2512.14700'
source_url: https://arxiv.org/abs/2512.14700
tags:
- online
- harassment
- messages
- responses
- simulated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an LLM-based pipeline for detecting online
  harassment in private Instagram messages and generating simulated responses to such
  harassment. The method employs two LLM agents in a cascading fashion, each evaluating
  messages with contextual conversation history, achieving an overall accuracy of
  0.9772 and a recall of 0.6500 for online harassment cases.
---

# Effectively Detecting and Responding to Online Harassment with Large Language Models

## Quick Facts
- arXiv ID: 2512.14700
- Source URL: https://arxiv.org/abs/2512.14700
- Reference count: 40
- Primary result: LLM-based cascading pipeline achieves 0.9772 accuracy and 0.6500 recall for Instagram DM harassment detection, with generated responses rated more helpful than human responses (p=0.01857)

## Executive Summary
This paper introduces an LLM-based pipeline for detecting online harassment in private Instagram messages and generating simulated responses to such harassment. The method employs two LLM agents in a cascading fashion, each evaluating messages with contextual conversation history, achieving an overall accuracy of 0.9772 and a recall of 0.6500 for online harassment cases. Simulated responses generated by a second pipeline were compared to original human responses, showing higher helpfulness (p=0.01857) and naturalness in context. The approach outperforms baseline models (e.g., BERT) and a model ensemble, offering a scalable, privacy-preserving alternative for online harassment detection and intervention in private messaging contexts.

## Method Summary
The paper presents a two-stage LLM pipeline for online harassment detection and response generation in Instagram direct messages. First, a cascading classification system uses two agents: Agent 1 classifies messages with 50-message conversation history, and if it outputs 1, Agent 2 reviews with Agent 1's reasoning as additional input. This structure reduces false positives while maintaining recall. Second, for detected harassment, a response generation pipeline selects from 9 evidence-based intervention strategies and generates 1-3 consecutive responses (1-13 words each) in adolescent-appropriate language. Human labelers created a ground truth dataset of 14,607 Instagram messages with 89 harassment cases (41 after filtering), enabling evaluation of both detection performance and response effectiveness through human comparison.

## Key Results
- Classification accuracy of 0.9772 with 0.6500 recall for harassment detection in private Instagram messages
- False positive reduction from 215 (Agent 1 alone) to 158 after cascading, with no change in recall
- Simulated responses rated more helpful than original human responses (p=0.01857, 95% CI: 0.507–0.567)
- Human responses preferred for naturalness over simulated responses (p=2.287e-12)

## Why This Works (Mechanism)

### Mechanism 1: Cascading Two-Agent Classification with Contextual History
A cascading two-agent LLM pipeline improves online harassment detection in private messages by leveraging conversation context and staged decision-making. Agent 1 classifies messages with 50-message conversation history as context. If Agent 1 outputs 0, the final label is 0. If Agent 1 outputs 1, Agent 2 reviews with Agent 1's reasoning as additional input, providing the final classification. This structure reduces false positives while maintaining recall. The core assumption is that harassment detection requires conversational context that single-message classification cannot capture; staged review can filter over-sensitive first-stage classifications without losing true positives.

### Mechanism 2: Strategy-Guided Response Generation with Literature-Grounded Prompts
LLM-generated responses incorporating empirically-studied intervention strategies are rated as more helpful than original human responses for handling online harassment. Agent 1 selects from 9 evidence-based strategies (warning, denouncing, empathy, moral suasion, etc.) derived from literature. Agent 2 receives the selected strategies with reasoning and generates 1-3 consecutive responses constrained to 1-13 words with adolescent-appropriate language style. The core assumption is that structured strategy selection produces more systematically helpful responses than unguided generation; adolescents respond better to age-appropriate linguistic patterns.

### Mechanism 3: Human-Grounded Evaluation with Ground Truth Construction
Creating a human-labeled dataset through multi-round annotation with tie-breaking enables reliable evaluation of harassment detection in a domain lacking prior datasets. Seven labelers annotated messages; a second-round labeler re-annotated blind to first labels; a third labeler resolved disagreements. This yielded 14,607 labeled messages (89 harassment cases; 41 after filtering to non-user-originated messages). Classifier performance is measured against this ground truth. The core assumption is that multi-annotator consensus with adjudication produces sufficiently reliable ground truth for low-prevalence phenomena in private messaging.

## Foundational Learning

- Concept: Cascading classification
  - Why needed here: The paper's core detection method uses sequential agents where the second agent only processes positive classifications from the first. Understanding this pattern is essential for reproducing or modifying the pipeline.
  - Quick check question: If Agent 1 produces 100 positive classifications and Agent 2 overturns 30 of them, what is the effective false positive reduction?

- Concept: Context window utilization for classification
  - Why needed here: The pipeline provides 50 previous messages as context for each classification decision. This differs from single-message toxic content classifiers and is critical for understanding harassment that unfolds conversationally.
  - Quick check question: Why would a message saying "you're so dumb" be classified differently with vs. without 50 messages of friendly banter context?

- Concept: Low-prevalence classification evaluation
  - Why needed here: Only 41 of 7,531 messages (0.54%) were harassment. Standard accuracy metrics are misleading here; understanding recall, precision, and class-imbalanced evaluation is essential.
  - Quick check question: A classifier achieves 99% accuracy by always predicting "not harassment." What metric reveals this failure?

## Architecture Onboarding

- Component map:
  - **Data Layer**: Instagram DM dataset (80,056 messages from 26 participants) → filtered to 14,607 labeled → 7,531 non-user-originated for evaluation
  - **Classification Pipeline**: Agent 1 (Llama-4-Scout-17B-16E-Instruct + 50-message context + system/user prompts) → cascading gate → Agent 2 (receives Agent 1 output) → final label
  - **Response Generation Pipeline**: Agent 1 (strategy selection from 9 options) → Agent 2 (generates 1-3 responses, 1-13 words each, adolescent style)
  - **Evaluation Layer**: Human labelers (3 for response comparison) → 6-question survey comparing simulated vs. human responses

- Critical path:
  1. Human labeling establishes ground truth (multi-round with tie-breaking)
  2. Classification pipeline processes messages with context
  3. Detected harassment feeds response generation pipeline
  4. Human evaluators compare simulated vs. original responses on helpfulness and naturalness

- Design tradeoffs:
  - **Recall vs. Precision**: Cascading reduced false positives (215→158) but retained 65% recall. Lower threshold would catch more harassment but increase false positives requiring human review.
  - **Helpfulness vs. Naturalness**: Simulated responses rated more helpful but less natural than human responses (p=2.287e-12 for naturalness favoring human responses). This suggests AI assistance may need style adaptation.
  - **Privacy vs. Context**: Using 50 messages of context improves detection but requires access to conversation history. The paper notes this as a privacy consideration.

- Failure signatures:
  - **Low recall on subtle harassment**: Recall of 0.6500 suggests 35% of harassment is missed. Query: Are these specific harassment types (e.g., passive-aggressive, emoji-based)?
  - **False positives on benign messages**: 158 false positives remain after cascading. Query: Do these cluster around sarcasm, inside jokes, or specific linguistic patterns?
  - **Unnatural response style**: Human raters significantly preferred human responses for naturalness. Query: Would fine-tuning on adolescent Instagram language improve this?

- First 3 experiments:
  1. **Ablate context window**: Run classification with 0, 10, 25, 50, 100 messages of context. Measure impact on recall and precision to validate context contribution.
  2. **Single-agent vs. cascading**: Run Agent 1 alone, Agent 2 alone, and cascaded. Compare metrics to isolate cascading benefit vs. prompt differences.
  3. **Strategy effectiveness analysis**: For each of the 9 strategies, generate responses using only that strategy. Have evaluators rate helpfulness by strategy to identify which strategies drive the helpfulness improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-generated simulated responses effectively stop harassment behavior (impact on the harasser) and mitigate user distress (impact on the victim) when deployed in real-world private messaging environments?
- Basis in paper: [explicit] The authors state in Future Work: "It will be important to verify that the tool can buffer negative feelings and stop online harassment. This verification can be broken down into two separate questions: the software's impact on the harasser and the software's impact on the user."
- Why unresolved: The current study evaluates helpfulness and naturalness via human labelers in a simulated setting, but does not deploy the tool to measure actual behavioral changes in harassers or psychological impacts on victims.
- What evidence would resolve it: Results from a live pilot study or field experiment measuring cessation of harassment messages and user-reported mental health outcomes after using the tool.

### Open Question 2
- Question: Which specific response strategies (e.g., warning, empathy, denouncing) are most effective at de-escalating private online harassment incidents?
- Basis in paper: [explicit] The authors note: "The LLM agents seem to strongly favor some of them [strategies]." They propose future work to "look into the effectiveness of each strategy for responding... allow us to compare the effectiveness of each strategy."
- Why unresolved: The current pipeline generates responses based on a mix of 9 strategies, but the paper does not isolate or compare the performance of individual strategies in successfully resolving conflicts.
- What evidence would resolve it: An ablation study where generated responses are constrained to single strategies and evaluated on their success rate in de-escalation by human annotators or in simulation.

### Open Question 3
- Question: Can fine-tuning LLMs for specific internet writing styles (e.g., textese, abbreviations) bridge the gap in "naturalness" between simulated responses and actual human responses?
- Basis in paper: [explicit] The results show human responses were preferred for naturalness ($p=2.287e-12$). The authors suggest: "methods that can improve the naturalness of the simulated responses, such as fine-tuning for internet writing styles, should be explored."
- Why unresolved: The current model uses prompt engineering to simulate teenage styles, but labelers still found human responses significantly more natural.
- What evidence would resolve it: A comparative evaluation of prompt-based vs. fine-tuned models on the "naturalness" metric using the same dataset of Instagram conversations.

### Open Question 4
- Question: How do inherent biases in Large Language Models impact the classification accuracy and false positive rates for online harassment across different demographic groups?
- Basis in paper: [explicit] The authors state: "Further investigations are needed to examine how biases in LLMs may impact the results of this labeling method and to identify available mitigation techniques."
- Why unresolved: The dataset consists of messages from 26 users aged 12–18, and the authors acknowledge the model may possess unique traits and biases that were not analyzed for fairness.
- What evidence would resolve it: A fairness audit of the classifier pipeline across stratified subgroups (e.g., based on dialect, gender, or cultural context) to identify disparities in detection rates.

## Limitations

- Model availability uncertainty: The Llama-4-Scout-17B-16E-Instruct model referenced throughout the paper may not be publicly accessible, limiting faithful reproduction.
- Small ground truth dataset: Only 41 harassment cases from 7,531 messages limit statistical power and generalizability, particularly for minority harassment types.
- Privacy vs. context tradeoff: The 50-message context window requirement for detection raises questions about the approach's privacy-preserving nature in practice.

## Confidence

**High Confidence**: The cascading two-agent architecture design and its core mechanism for reducing false positives while maintaining recall are well-specified and theoretically sound. The human evaluation methodology for comparing responses is clearly described and appropriately controlled.

**Medium Confidence**: The classification performance metrics (accuracy 0.9772, recall 0.6500) are reported with clear methodology, but depend on the availability and exact configuration of the referenced LLM. The response generation pipeline's effectiveness is supported by statistical significance (p=0.01857 for helpfulness) but the effect size and practical significance require further validation.

**Low Confidence**: The generalizability of results beyond the specific Instagram DM dataset and the privacy-preserving nature of the approach given context window requirements. The effectiveness of the 9 intervention strategies across different harassment types is asserted but not empirically validated for each strategy.

## Next Checks

1. **Context Window Sensitivity Analysis**: Systematically test classification performance with varying context windows (0, 10, 25, 50, 100 messages) to empirically validate the claimed importance of conversational context and identify the optimal window size for balancing performance with privacy considerations.

2. **Strategy-by-Strategy Effectiveness Testing**: Generate responses using only one intervention strategy at a time (rather than the full 9-strategy pipeline) and have evaluators rate helpfulness by strategy. This isolates which specific strategies drive the reported helpfulness improvement versus other pipeline components.

3. **Cross-Dataset Validation**: Apply the classification pipeline to an independent harassment dataset (such as Twitter or Reddit harassment corpora) to test generalizability beyond Instagram DMs and validate that the cascading architecture performs similarly across different platform contexts and conversation styles.