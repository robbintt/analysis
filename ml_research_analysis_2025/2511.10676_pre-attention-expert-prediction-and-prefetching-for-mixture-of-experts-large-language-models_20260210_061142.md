---
ver: rpa2
title: Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large
  Language Models
arxiv_id: '2511.10676'
source_url: https://arxiv.org/abs/2511.10676
tags:
- expert
- prediction
- experts
- accuracy
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient expert prefetching
  in Mixture-of-Experts (MoE) Large Language Models (LLMs), specifically the limitations
  of cross-layer prediction methods that use previous layer activations for expert
  selection. The authors propose a novel pre-attention expert prediction approach
  that leverages same-layer information by using pre-attention normalized weights
  within the current layer to predict which experts will be selected.
---

# Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models

## Quick Facts
- arXiv ID: 2511.10676
- Source URL: https://arxiv.org/abs/2511.10676
- Reference count: 10
- Primary result: 93.03% exact-match accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE

## Executive Summary
This paper introduces a novel pre-attention expert prediction approach for Mixture-of-Experts (MoE) Large Language Models that addresses limitations in cross-layer prediction methods. The key insight is that softmax and layer normalization functions are ranking-preserving, allowing expert selection ranking to be approximated using simple linear functions. By leveraging pre-attention normalized weights within the current layer, the method eliminates architectural complexity and cross-layer communication overhead present in existing approaches. Experimental results demonstrate substantial improvements over state-of-the-art methods, achieving 15-19 percentage point absolute improvements in exact-match accuracy.

## Method Summary
The paper proposes pre-attention expert prediction for MoE LLMs, using pre-attention normalized weights within the current layer to predict which experts will be selected. The method exploits the mathematical property that softmax and layer normalization functions are ranking-preserving, enabling simple linear functions to approximate expert selection rankings. Two lightweight predictor architectures are evaluated: one with batch normalization and GELU activation, and another with SiLU activation. Both use ranking-aware loss functions and are trained on 10M samples from the MMLU benchmark. The approach enables parallel expert prefetching during inference by predicting expert selections before the attention block completes.

## Key Results
- Achieves 93.03% exact-match accuracy on DeepSeek V2 Lite (15.85 pp improvement over prior work)
- Achieves 94.69% exact-match accuracy on Qwen3-30B (19.27 pp improvement)
- Achieves 97.62% exact-match accuracy on Phi-mini-MoE (15.51 pp improvement)

## Why This Works (Mechanism)
The method works by exploiting the ranking-preserving properties of softmax and layer normalization functions. When pre-attention activations are normalized, the relative ordering of expert scores remains consistent through subsequent transformations. This allows a simple linear predictor to accurately estimate which experts will be selected without needing to wait for attention computations or use complex cross-layer architectures. By operating on pre-attention activations within the same layer, the approach eliminates communication overhead between layers and architectural complexity.

## Foundational Learning

1. **Mixture-of-Experts (MoE) Architecture**
   - Why needed: Understanding MoE is fundamental to grasping why expert prediction matters for efficiency
   - Quick check: Can you explain how MoE routes tokens to different experts based on router scores?

2. **Ranking-Preserving Functions**
   - Why needed: The core mathematical insight relies on softmax and layer norm preserving expert score rankings
   - Quick check: Verify that if a > b, then softmax(a) > softmax(b) for any inputs a and b

3. **Pre-Attention vs Post-Attention Activations**
   - Why needed: The method specifically targets pre-attention normalized weights as prediction input
   - Quick check: Identify the exact layer in a transformer where pre-attention activations are available

4. **Expert Router Prediction**
   - Why needed: Understanding existing cross-layer prediction methods helps appreciate the innovation
   - Quick check: Compare the communication patterns between cross-layer and pre-attention prediction

5. **Loss Functions for Ranking Tasks**
   - Why needed: The ranking-aware BCE loss is critical for training effective predictors
   - Quick check: Can you implement weighted BCE loss with different weights for top vs non-top experts?

6. **Over-Provisioning Accuracy**
   - Why needed: The metric captures practical utility beyond exact match requirements
   - Quick check: Calculate over-provisioning accuracy given ground truth top-6 experts and predicted top-8

## Architecture Onboarding

**Component Map:** Input tensor → Linear(d,2048) → Activation → Linear(2048,E) → Loss → Expert predictions

**Critical Path:** Pre-attention layer-norm output → Expert predictor → Expert selection → Prefetching

**Design Tradeoffs:** Pre-attention approach eliminates cross-layer communication but requires accurate timing of activation extraction; simpler architecture reduces computational overhead but may sacrifice some accuracy compared to more complex models.

**Failure Signatures:** Low accuracy (~55-78%) indicates wrong activation extraction point; training instability suggests improper loss weighting; poor performance on high-expert models points to insufficient class balancing.

**3 First Experiments:**
1. Verify pre-attention activation extraction by comparing predictor outputs with Fate's cross-layer predictions on same samples
2. Implement and compare both ranking-aware loss variants (weighted BCE vs ranking-aware BCE) on small validation set
3. Test expert prediction accuracy across different transformer layers to identify performance degradation patterns

## Open Questions the Paper Calls Out

None

## Limitations

- Missing implementation details: Optimizer configuration, batch size, and data preprocessing pipeline are unspecified
- Ambiguous extraction point: Paper doesn't clarify whether pre-attention activations are extracted before or after layer normalization
- Incomplete loss specification: Ranking-aware loss function is mentioned but lacks full mathematical specification

## Confidence

- **High Confidence:** Core algorithmic insight about ranking preservation is mathematically sound and well-explained
- **Medium Confidence:** Accuracy improvements are plausible but depend on proper implementation of training pipeline
- **Low Confidence:** Reproducing exact numbers requires precise matching of unspecified training details

## Next Checks

1. Verify pre-attention activation extraction timing by comparing with Fate's cross-layer predictions on the same samples—predictions should be highly correlated if extraction points align.

2. Implement and compare both ranking-aware loss variants (weighted BCE vs ranking-aware BCE) on a small validation set to determine which achieves better accuracy.

3. Test expert prediction accuracy across different layers (early vs. late transformer layers) to identify whether performance degrades in lower layers as suggested by the paper's analysis.