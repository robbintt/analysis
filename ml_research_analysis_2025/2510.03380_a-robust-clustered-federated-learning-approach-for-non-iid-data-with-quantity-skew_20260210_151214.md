---
ver: rpa2
title: A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity
  Skew
arxiv_id: '2510.03380'
source_url: https://arxiv.org/abs/2510.03380
tags:
- clients
- data
- clustering
- algorithm
- cornflqs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Quantity Skew (QS) in Clustered
  Federated Learning (CFL), where clients hold highly heterogeneous data volumes.
  The authors propose CORNFLQS, a novel iterative CFL algorithm that combines both
  weight-based and loss-based clustering to achieve robust performance in both QS
  and non-QS scenarios.
---

# A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew
## Quick Facts
- arXiv ID: 2510.03380
- Source URL: https://arxiv.org/abs/2510.03380
- Reference count: 14
- Authors: Michael Ben Ali; Imen Megdiche; André Peninou; Olivier Teste
- One-line primary result: CORNFLQS achieves highest average ranking in accuracy and clustering quality across six datasets and 270 Non-IID configurations.

## Executive Summary
This paper introduces CORNFLQS, a novel iterative clustered federated learning (CFL) algorithm designed to address Quantity Skew (QS) in federated learning. QS occurs when clients have highly heterogeneous data volumes, which can bias clustering algorithms and degrade model performance. CORNFLQS combines weight-based and loss-based clustering in an alternating optimization framework to achieve a stable clustering agreement. Extensive experiments on six image classification datasets show that CORNFLQS consistently outperforms state-of-the-art CFL methods in both accuracy and clustering quality, particularly in QS scenarios.

## Method Summary
CORNFLQS addresses Quantity Skew in clustered federated learning by combining weight-based and loss-based clustering in an iterative framework. The method starts with a warm-up phase of two rounds of standard FedAvg to ensure a fair initialization. It then enters the CORN phase, alternating between server-side weight clustering (using hierarchical clustering) and client-side loss-based cluster selection until stability. After convergence, it switches to a pure loss-based refinement phase, followed by standard FedAvg within fixed clusters. The approach seeks a common clustering agreement rather than relying on a single strategy, improving robustness in both QS and non-QS scenarios.

## Key Results
- CORNFLQS achieves the highest average ranking in both accuracy and clustering quality across six datasets and 270 Non-IID configurations.
- The method demonstrates superior robustness to QS perturbations compared to existing state-of-the-art CFL algorithms.
- CORNFLQS consistently ranks first or second in average ranking across datasets, validating its effectiveness in handling heterogeneous data volumes.

## Why This Works (Mechanism)
### Mechanism 1: Alternating Optimization for Clustering Consensus
Alternating between weight-based and loss-based clustering allows the system to find a stable "agreement" on client groupings that single-strategy methods miss. The algorithm iterates between server-side clustering of model weights (structural similarity) and client-side cluster selection based on local loss (functional performance). By requiring cluster assignment to stabilize across these two views, the system filters out spurious correlations caused by data volume disparities. The loop terminates if clustering configuration remains unchanged between rounds or if the maximum round limit is reached.

### Mechanism 2: Volume-Agnostic Initialization
A standardized warm-up phase reduces initial bias towards clients with large datasets. Before clustering begins, the system performs two rounds of standard Federated Averaging (FedAvg) involving all clients. This ensures the initial global model is a functional starting point for all clients, rather than a model immediately dominated by gradients of high-volume clients. Failure here would manifest as high variance in early clustering, requiring more iterations in the subsequent CORN phase to correct.

### Mechanism 3: Adaptive Protocol Switching
Transitioning to a loss-based only strategy after consensus improves cluster-specific model convergence. Once the hybrid "CORN" phase establishes a stable cluster structure, the algorithm switches to pure loss-based cluster assignment. This allows clients to fine-tune their association based on performance (loss) without the computational overhead of weight clustering or the instability of weight drift in later training stages. The mechanism fails if initial clustering from the CORN phase is poor, as loss-based refinement cannot easily recover from a fundamentally wrong partition.

## Foundational Learning
- **Concept: Quantity Skew (QS)**
  - Why needed here: This is the specific failure mode the paper addresses. Unlike label distribution skew, QS refers to clients holding vastly different amounts of data.
  - Quick check question: Can you distinguish between "Label Skew" (imbalanced classes) and "Quantity Skew" (imbalanced dataset sizes)?

- **Concept: Clustered Federated Learning (CFL) Paradigms**
  - Why needed here: CORNFLQS is a hybrid of the two main CFL paradigms. You must understand Weight-based (Server clusters clients by model parameter similarity) vs. Loss-based (Clients pick the cluster model yielding lowest error).
  - Quick check question: In a weight-based approach, does the server or the client decide the cluster assignment?

- **Concept: Non-IID Data Taxonomy**
  - Why needed here: The evaluation benchmarks rely on specific types of data shifts: Concept Shift (features/labels) vs. Feature Distribution Skew. Understanding these is required to interpret the experimental results.
  - Quick check question: If two clients have the same labels but the images are rotated differently (e.g., 0° vs 90°), is this Concept Shift or Label Skew?

## Architecture Onboarding
- **Component map:** Client holds local dataset and performs local training; Server maintains cluster models and performs weight clustering; Coordinator manages state transitions through Initialization → CORN (Hybrid) → LossBasedCFL (Refinement) → FedAvgforCFL (Final Training).
- **Critical path:** 1. Initialization: 2 rounds of global FedAvg. 2. CORN Phase: Loop { Cluster Weights → Broadcast → Client Loss Eval → Update Assignments } until stability. 3. Refinement: Loop { Client Loss Eval → Update Assignments } until stability. 4. Final Training: Fixed clusters, standard FedAvg per cluster.
- **Design tradeoffs:** Cluster Count (K) - over-estimating K is safer than under-estimating. Underestimation forces distinct data distributions into one cluster, degrading accuracy. Overhead - CORN phase requires broadcasting K models to all clients every round, increasing communication bandwidth compared to single-model FL.
- **Failure signatures:** Oscillation - clustering assignment fluctuates constantly without converging in CORN phase. Dominance - one cluster absorbs all clients. Check if initialization phase was skipped or if learning rate is too high, causing weight divergence to vanish.
- **First 3 experiments:** 1. Baseline Sanity Check: Run standard FedAvg on dataset with high Quantity Skew. Observe accuracy drop compared to CORNFLQS to quantify "CFL gain." 2. Ablation on Initialization: Run CORNFLQS with 0 warm-up rounds vs. 2 warm-up rounds. Measure standard deviation of client accuracy to verify if warm-up reduces early bias against low-volume clients. 3. Sensitivity Analysis: Test CORNFLQS with K=2 (underestimated), K=4 (true), and K=8 (overestimated). Verify paper's claim that overestimation degrades performance less than underestimation.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the number of clusters (K) in CORNFLQS be determined adaptively without manual grid search or prior domain knowledge?
- Basis in paper: [explicit] The conclusion states that future work should "refine methods for selecting the number of clusters," and the sensitivity analysis notes the method is "sensitive to underestimation" when K is fixed.
- Why unresolved: The current implementation requires K to be set as a hyperparameter (fixed at 4), which is often unknown in real-world scenarios.
- What evidence would resolve it: A modified version of CORNFLQS that dynamically adjusts K during training, achieving accuracy comparable to the current grid-search approach on the same Non-IID benchmarks.

### Open Question 2
- Question: How does CORNFLQS perform when integrated with privacy-preserving techniques like Differential Privacy (DP)?
- Basis in paper: [explicit] The conclusion suggests future work should look into "leveraging its efficacy in combination with FL techniques such as differential privacy."
- Why unresolved: The current study focuses on accuracy and clustering robustness but does not evaluate the impact of noise addition (required for DP) on the weight-based and loss-based clustering agreements.
- What evidence would resolve it: Experimental results showing the trade-off between privacy budgets (epsilon), clustering quality (ARI), and model accuracy under Quantity Skew settings.

### Open Question 3
- Question: Does CORNFLQS maintain its robustness when applied to larger, more complex model architectures?
- Basis in paper: [explicit] The authors state that future work includes "extending CORNFLQS to larger and more complex models," as the current evaluation is limited to MLPs and small CNNs.
- Why unresolved: The weight-based clustering component relies on Euclidean distance on model parameters; it is unclear if this metric remains effective in high-dimensional parameter spaces typical of large models.
- What evidence would resolve it: Benchmarking CORNFLQS on tasks requiring large architectures (e.g., Transformers on text or ResNets on high-res images) demonstrating consistent performance over baselines.

### Open Question 4
- Question: What are the computational and communication overheads of the iterative CORN clustering process in practical deployments?
- Basis in paper: [explicit] The conclusion identifies the need to "investigate computational and communication costs in practical deployments."
- Why unresolved: The paper evaluates effectiveness (accuracy/ARI) but does not provide a complexity analysis or runtime comparison against baselines like IFCA and FL+HC.
- What evidence would resolve it: Profiling data comparing communication rounds, server-side clustering time, and client-side computation load against baselines.

## Limitations
- The paper does not specify key optimization hyperparameters (learning rate, batch size, optimizer type) that could significantly affect reproducibility.
- The CNN architecture details are underspecified beyond layer counts, making exact reproduction challenging.
- The claim about CORNFLQS being "highly robust" to QS perturbations would benefit from more diverse QS scenarios beyond the two types tested.

## Confidence
- **High confidence:** The core algorithmic framework (alternating weight-based and loss-based clustering) is clearly described and addresses a well-defined problem.
- **Medium confidence:** Experimental results appear comprehensive, but exact implementation details needed for perfect reproduction are missing.
- **Low confidence:** The effectiveness of the hybrid approach depends critically on proper weight clustering implementation and the assumption that alternating views will converge to a stable partition.

## Next Checks
1. Implement CORNFLQS with the exact clustering parameters (Ward linkage, Euclidean distance) and verify stability on QS-Type-1 data.
2. Run ablation studies comparing CORNFLQS to pure weight-based and pure loss-based CFL variants under QS conditions.
3. Test CORNFLQS sensitivity to the cluster count K parameter across different underestimation/overestimation scenarios.