---
ver: rpa2
title: Citations and Trust in LLM Generated Responses
arxiv_id: '2501.01303'
source_url: https://arxiv.org/abs/2501.01303
tags:
- trust
- citations
- citation
- questions
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the impact of citations on user trust in
  AI-generated responses. The authors conducted a live experiment with a commercial
  chatbot, presenting participants with
---

# Citations and Trust in LLM Generated Responses

## Quick Facts
- arXiv ID: 2501.01303
- Source URL: https://arxiv.org/abs/2501.01303
- Authors: Yifan Ding; Matthew Facciani; Amrit Poudel; Ellen Joyce; Salvador Aguinaga; Balaji Veeramani; Sanmitra Bhattacharya; Tim Weninger
- Reference count: 27
- Key outcome: Citations increase user trust in LLM-generated responses even when citations are random or irrelevant.

## Executive Summary
This study investigated how citation presence, count, and relevance affect user trust in AI-generated responses through a live experiment with a commercial chatbot. The researchers found that simply including citations—even random or irrelevant ones—significantly boosted user trust compared to responses without citations. They also discovered that users who checked citations (via mouse hover) tended to trust the response less, suggesting that citation-checking behavior is a sign of skepticism rather than a trust-building action. Additionally, they found that a single citation was sufficient for trust gains, with no additional benefit from including multiple citations.

## Method Summary
The study conducted a live Randomized Controlled Trial (RCT) with 303 participants recruited via Prolific. Participants asked open-ended questions that were answered by ChatGPT-4 (truncated to 3 sentences). The experiment manipulated three factors: citation presence (0, 1, or 5 citations), citation type (valid vs. random URLs), and tracked whether users hovered over citations to check them. Trust was measured using a 1-10 slider scale, and the analysis controlled for demographic variables including age, gender, race, education, political orientation, and urban/rural status.

## Key Results
- Citations significantly increase perceived trustworthiness regardless of relevance (even random citations boost trust).
- Users who checked citations (via hover) reported lower trust, supporting the "trust as anti-monitoring" theory.
- One citation provides the same trust benefit as five citations; additional citations offer no incremental trust gain.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Citations increase perceived trustworthiness of LLM-generated responses, regardless of citation relevance.
- Mechanism: Citations function as social proof signals—users perceive the response as externally endorsed even without verification. The mere presence of a citation triggers a heuristic trust boost.
- Core assumption: Users interpret citation markers [1], [2] as evidence of source backing without automatically verifying.
- Evidence anchors:
  - [abstract] "We found a significant increase in trust when citations were present, a result that held true even when the citations were random"
  - [section] "citations within the outputs serve as strong indicators of social proof because they signal to users that the output is endorsed by some source"
  - [corpus] Related work on AI transparency ("Highlight All the Phrases") explores visual factuality indicators; limited direct corpus evidence on the "random citation still works" effect—this is a novel contribution.
- Break condition: When users actually check random citations, trust drops to baseline (no-citation level).

### Mechanism 2
- Claim: Citation-checking behavior inversely correlates with trust; checking is a symptom of skepticism, not a trust-building action.
- Mechanism: Trust as anti-monitoring—users who trust relinquish control and don't verify; skeptical users monitor (check citations), revealing lower baseline trust.
- Core assumption: Hover behavior accurately proxies verification intent and underlying trust levels.
- Evidence anchors:
  - [abstract] "we also found a significant decrease in trust when participants checked the citations"
  - [section] "This finding aligns with the theory of trust as anti-monitoring, as skeptical participants sought to verify (i.e., monitor) the source"
  - [corpus] Corpus evidence on anti-monitoring in AI contexts is limited; related behavioral trust work exists but doesn't directly replicate this finding.
- Break condition: If checking becomes habitual (e.g., professional fact-checkers) rather than trust-driven, the anti-monitoring signal decouples.

### Mechanism 3
- Claim: A single citation is sufficient for trust gains; additional citations provide no incremental benefit.
- Mechanism: Users apply a binary heuristic—"is there a citation?"—rather than a quantitative count. Diminishing returns set in immediately after one.
- Core assumption: Users don't systematically evaluate citation count as a quality signal.
- Evidence anchors:
  - [section] "there was no significant difference between the one citation and five citation conditions (p > .05)"
  - [section] "This negative result is contrary to our initial hypothesis. One plausible reason... might be due to the principle of diminishing returns"
  - [corpus] No direct corpus evidence on citation count effects; this is a novel finding with no replication in the retrieved neighbors.
- Break condition: In high-stakes domains (medical, legal), users may demand multiple independent sources; one citation may prove insufficient.

## Foundational Learning

- Concept: Trust as Anti-Monitoring (Baier 1986)
  - Why needed here: This framework explains why verification behavior indicates low trust rather than building trust. Essential for interpreting user analytics correctly.
  - Quick check question: If users in your system check sources frequently, should you celebrate engagement or diagnose a trust problem?

- Concept: Principle of Social Proof (Cialdini 2009)
  - Why needed here: Explains why external endorsements (citations) boost trust heuristically, even without user verification.
  - Quick check question: Why would a broken or random link still increase trust compared to no link at all?

- Concept: RAG (Retrieval-Augmented Generation) Systems
  - Why needed here: Most production citation systems use RAG; understanding the retrieval-citation pipeline is prerequisite for system design.
  - Quick check question: In a RAG system, does the LLM generate citations, or does the retriever provide them?

## Architecture Onboarding

- Component map: User Query -> LLM Backend (ChatGPT-4) -> Citation Injector (ScaleSERP) -> Frontend Display -> User Interaction -> Telemetry
- Critical path: 1) User query → LLM generates response → truncation; 2) If citation condition → search API retrieves 5 results → attach citations; 3) If random condition → swap valid URLs with random URLs; 4) User hovers (or doesn't) → User rates trust → Log all signals
- Design tradeoffs:
  - Citation presence vs. quality: Random citations still boost trust vs. no citations, but checking reveals deception → short-term gain, potential long-term trust erosion
  - One vs. five citations: No trust difference, but 5 citations increase checking behavior (χ²=21.19, p<0.001) → more exposure to citation quality issues
  - Hover vs. click tracking: Hover is low-effort proxy; click indicates stronger verification intent (study used hover only)
- Failure signatures:
  - Random citations + user checks → trust drops to no-citation baseline (M=7.55 vs. M=7.73, non-significant)
  - High perplexity/complex prompts → slightly lower trust (r=-0.06, p=0.002)
  - Demographics matter: males, liberals, urban users check more often → may expose citation issues faster
- First 3 experiments:
  1. High-stakes domain test: Replicate in medical/legal/financial contexts to test if one citation remains sufficient or if users demand multiple sources.
  2. Trust recovery dynamics: Track trust over time after users encounter random citations—does trust recover, stabilize lower, or continue declining?
  3. Click vs. hover calibration: Compare hover (low effort) vs. actual URL clicks (high effort) as anti-monitoring signals to refine the trust proxy.

## Open Questions the Paper Calls Out

- Does the complexity of a user's prompt significantly influence the perceived trustworthiness of the LLM's response? The authors note insufficient complex queries to determine if they elicit higher trust due to demonstrated capability or lower trust due to inferential hallucinations.

- What specific cultural or societal factors drive the observed higher trust levels in non-white demographic groups? The study lacked statistical power to deeply analyze racial subgroups, leaving the cause undetermined.

- Do behavioral measures of trust (e.g., reliance or compliance) correlate with the self-reported trust ratings observed in this study? The authors suggest self-reported trust doesn't necessarily predict real-world adoption or reliance.

## Limitations

- External validity limited by use of ChatGPT-4 and specific citation-injection mechanism; findings may not generalize to other LLM systems.
- Random citation behavior implementation unclear; validity and relevance of the URL pool is uncertain.
- Citation checking tracked via mouse hover, which may not accurately capture actual verification behavior compared to explicit clicks.

## Confidence

- **High Confidence:** Citations increase perceived trust regardless of relevance is well-supported by data (significant main effect).
- **Medium Confidence:** Anti-monitoring interpretation of citation-checking behavior is plausible but relies on behavioral proxy assumptions.
- **Medium Confidence:** One-citation sufficiency finding is supported, but mechanism remains speculative without user interviews.

## Next Checks

1. Click-Through Validation: Replicate the study tracking actual URL clicks rather than hover events to verify if behavioral anti-monitoring signals hold with higher verification intent.

2. Domain-Specific Replication: Test the one-citation sufficiency claim in high-stakes domains (medical, legal, financial) where users may demand multiple independent sources.

3. Longitudinal Trust Recovery: Track trust ratings over time in the random-citation condition to measure if users recover trust, stabilize at lower levels, or continue declining after discovering citation irrelevance.