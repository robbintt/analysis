---
ver: rpa2
title: Automated Modeling Method for Pathloss Model Discovery
arxiv_id: '2505.23383'
source_url: https://arxiv.org/abs/2505.23383
tags:
- methods
- data
- expressions
- interpretability
- kans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AutoPL, an automated method for discovering
  pathloss (PL) models that balances accuracy and interpretability. Traditional PL
  modeling relies on manual statistical methods, which are time-consuming and lack
  data-driven refinement.
---

# Automated Modeling Method for Pathloss Model Discovery

## Quick Facts
- **arXiv ID:** 2505.23383
- **Source URL:** https://arxiv.org/abs/2505.23383
- **Reference count:** 40
- **Primary result:** Automated pathloss model discovery using KANs (accuracy) and DSR (interpretability), reducing prediction errors by up to 75% compared to traditional methods

## Executive Summary
This paper introduces AutoPL, an automated framework for discovering pathloss models that balances accuracy and interpretability. Traditional pathloss modeling relies on manual statistical methods, which are time-consuming and lack data-driven refinement. AutoPL automates model formulation, evaluation, and refinement using two complementary approaches: Kolmogorov-Arnold Networks (KANs) for high accuracy with two-level interpretability, and Deep Symbolic Regression (DSR) for full interpretability through human-readable mathematical expressions. The method is evaluated on both synthetic and real-world datasets, demonstrating significant improvements over traditional modeling approaches.

## Method Summary
AutoPL is an automated framework that discovers pathloss models using two parallel approaches. KANs employ learnable B-spline activation functions on network edges, leveraging the Kolmogorov-Arnold representation theorem to achieve near-perfect predictive accuracy (R² ≈ 1). DSR uses reinforcement learning to search a constrained space of symbolic formulas, generating compact human-interpretable mathematical expressions. The framework processes pre-processed pathloss datasets through either path, with post-processing to extract interpretable expressions from KANs and validation checks for physical plausibility of all generated models.

## Key Results
- KANs achieve near-perfect R² values (≈1) with minimal prediction error on synthetic ABG and CI models
- DSR generates compact mathematical expressions with moderate accuracy but full interpretability
- Compared to traditional methods, AutoPL reduces prediction errors by up to 75%
- The framework successfully handles both synthetic analytical models and real-world indoor/outdoor empirical data

## Why This Works (Mechanism)

### Mechanism 1: KANs for Superior Accuracy
KANs achieve superior predictive accuracy by learning univariate spline functions on network edges, naturally aligning with the mathematical form of physical propagation models. Unlike MLPs that use fixed activation functions on nodes, KANs place learnable activation functions (B-splines) on edges, decomposing complex multivariate functions into sums of univariate functions per the Kolmogorov-Arnold theorem.

### Mechanism 2: DSR for Interpretability
Deep Symbolic Regression produces compact, human-interpretable mathematical expressions by using a reinforcement learning policy to search a constrained space of symbolic formulas. An RNN generates sequences of tokens forming candidate expressions, which are evaluated against data using a reward function based on prediction error.

### Mechanism 3: Automated Discovery Loop
Automating the model formulation, evaluation, and refinement loop accelerates discovery and reduces prediction error by systematically exploring a broader hypothesis space than traditional manual methods. The data-driven optimization process reduces human bias and uncovers models that better fit measurements.

## Foundational Learning

- **Pathloss (PL) Modeling**: Core problem domain quantifying signal power attenuation with distance, frequency, and obstacles. Understanding standard forms like ABG and CI models is essential to interpret AutoPL outputs.
  - Quick check: What are the primary physical factors that influence pathloss in the standard ABG and CI models?

- **Symbolic Regression**: Technique powering AutoPL's fully interpretable branch, searching for both equation structure and parameters rather than fitting coefficients to fixed equations.
  - Quick check: How does symbolic regression differ from standard linear or polynomial regression?

- **Reinforcement Learning (RL) Policy Gradient**: Training engine for Deep Symbolic Regression, using a risk-seeking policy gradient to guide the search for expressions through agent, policy, action, and reward dynamics.
  - Quick check: In DSR, what acts as the "agent," what "action" does it take, and what serves as the "reward"?

## Architecture Onboarding

- **Component map:** Input Data -> Model Discovery Engine (KANs or DSR) -> Post-Processing/Interpretation -> Evaluation & Validation
- **Critical path:** For maximum accuracy: Input Data -> KAN Training -> Spline Learning -> (Optional) Symbolic Mapping. For maximum interpretability: Input Data -> DSR Training -> Symbolic Expression Generation -> Physical Validity Check
- **Design tradeoffs:** 
  1. Accuracy vs. Interpretability: KANs provide highest accuracy (R² ≈ 1) but medium interpretability; DSR provides high interpretability but moderate accuracy
  2. Automation vs. Physical Validity: Fully automating discovery reduces bias but can produce expressions violating physical principles
- **Failure signatures:**
  1. Low R² / High Error: Model fails to capture data trend (convergence failure for DSR, insufficient capacity for KANs)
  2. Physically Invalid Expression: Good numerical fit but nonsensical functional form (e.g., periodic functions for monotonic relationships)
- **First 3 experiments:**
  1. Replicate synthetic data test: Generate ABG/CI data with Table I parameters, train KAN to verify near-perfect R² recovery
  2. DSR on synthetic data: Use same dataset to train DSR, compare generated expression to ground-truth, test physical validity
  3. Real-world generalization: Train both KAN and DSR on 80% of empirical indoor data, evaluate on held-out 20%, compare to traditional baselines

## Open Questions the Paper Calls Out

### Open Question 1
How can the symbolic mapping process in KANs be refined to ensure generation of physically valid pathloss expressions? The current `auto_symbolic()` function fits known functions to learned splines without enforcing physical constraints, leading to expressions with invalid periodic terms (e.g., `cos(f)`). Resolution requires a modified framework constraining symbolic search space to monotonic functions for frequency/distance.

### Open Question 2
Does utilizing a non-logarithmic form of pathloss improve precision and stability of automated model discovery? While logarithmic functions are standard, they present challenges for some symbolic regression techniques. Resolution requires comparative study training AutoPL on linear-scale versus dB-scale target variables.

### Open Question 3
What specific methodological tools are required to assess validity of composition graphs learned by KANs? Currently, validity is assessed by manually inspecting extracted formulas without automated mechanism to verify physical plausibility of internal spline structure. Resolution requires quantitative metric or visual diagnostic tool flagging non-physical dependencies.

## Limitations
- Reproducibility concerns due to unspecified random seeds and precise sampling distributions for synthetic data generation
- Reliance on proprietary libraries (pykan, deep-symbolic-optimization) without detailed algorithmic pseudocode introduces implementation ambiguity
- Symbolic expression validity check is mentioned but not formally defined, raising concerns about physically nonsensical results achieving high numerical accuracy

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| KANs achieve near-perfect R² values (≈1) on pathloss modeling | High |
| 75% error reduction compared to traditional methods | Medium |
| DSR generates compact interpretable expressions with moderate accuracy | Medium |

## Next Checks

1. **Replicate KAN Training Stability:** Train KAN models on normalized ABG and CI synthetic datasets across multiple random seeds to verify consistency of near-perfect R² achievement and identify sensitivity to initialization.

2. **Validate Physical Plausibility Filter:** Implement and test the expression validity constraint by generating DSR models on synthetic data and verifying that all outputs include both frequency (f) and distance (d) terms while avoiding physically invalid constructs like periodic functions for monotonic relationships.

3. **Cross-Dataset Generalization Test:** Train both KAN and DSR models on the indoor dataset, then evaluate performance on the outdoor dataset to assess whether claimed accuracy improvements generalize across different propagation environments or are dataset-specific.