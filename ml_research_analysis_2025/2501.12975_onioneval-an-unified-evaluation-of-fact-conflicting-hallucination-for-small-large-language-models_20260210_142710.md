---
ver: rpa2
title: 'OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large
  Language Models'
arxiv_id: '2501.12975'
source_url: https://arxiv.org/abs/2501.12975
tags:
- hallucination
- context
- sllms
- atomic
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces OnionEval, a new evaluation framework designed\
  \ to assess fact-conflicting hallucinations in small large language models (SLLMs).\
  \ The framework evaluates model performance across multiple context layers using\
  \ a metric called the Context Influence (CI) Score, which quantifies how context\
  \ affects a model\u2019s hallucination detection ability."
---

# OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models

## Quick Facts
- **arXiv ID**: 2501.12975
- **Source URL**: https://arxiv.org/abs/2501.12975
- **Reference count**: 9
- **Primary result**: SLLMs show significant performance drops on context-dependent hallucinations despite strong atomic fact detection

## Executive Summary
This paper introduces OnionEval, a novel evaluation framework designed to assess fact-conflicting hallucinations in Small Large Language Models (SLLMs) with fewer than 10B parameters. The framework evaluates model performance across multiple context layers using a metric called the Context Influence (CI) Score, which quantifies how context affects a model's hallucination detection ability. Through systematic experiments, the authors demonstrate that while SLLMs perform comparably to larger models on isolated atomic facts, their performance drops significantly when facts are embedded in complex contexts. The study also shows that simple Chain-of-Thought prompting can substantially improve SLLMs' ability to handle context-dependent hallucinations, providing a standardized approach for evaluating SLLMs' factual accuracy and contextual reasoning capabilities.

## Method Summary
OnionEval employs a three-layer evaluation framework that progressively increases contextual complexity around factual statements. The benchmark consists of 3,356 QA pairs derived from 515 entities across 19 categories, combining atomic facts from Google Knowledge Graph with hallucinations generated by GPT-4. Layer 1 presents atomic facts only, Layer 2 embeds facts in book-reading stories, and Layer 3 further embeds these stories in coffee shop scenarios. Models are evaluated using binary classification (Yes/No) on whether given sentences contain hallucinated information. The framework introduces the Context Influence (CI) Score to quantify performance degradation as context complexity increases. Experiments use standard inference settings (temperature=0, top-p=1, max_tokens=32) across multiple SLLMs including Llama 3.1/3.2, Gemma, and Qwen 2.5, testing various mitigation strategies including Chain-of-Thought, RAG, and few-shot prompting.

## Key Results
- SLLMs maintain high accuracy (>90%) on atomic fact detection but performance drops to near-zero on context-embedded layers
- Context Influence (CI) Score effectively quantifies the performance degradation as context complexity increases
- Chain-of-Thought prompting significantly improves SLLMs' ability to detect hallucinations in complex contexts
- Simple instruction-following failures (unmatched answers) can account for up to 50.5% of poor performance

## Why This Works (Mechanism)
The OnionEval framework works by systematically isolating the effect of context on hallucination detection. By structuring evaluation across three distinct layers—from atomic facts to multi-level contextual embedding—it creates a controlled environment where the impact of contextual reasoning can be measured independently of factual knowledge. The binary classification task with strict "yes/no" output requirements exposes both the model's hallucination detection capability and its instruction-following reliability. The Context Influence Score mathematically captures the relationship between context complexity and performance degradation, providing a quantitative measure of contextual reasoning ability that traditional benchmarks cannot capture.

## Foundational Learning
- **Context Influence (CI) Score**: A metric quantifying how context complexity affects model performance, calculated as CI = ρ_h + (∑Δ_i)/n where ρ_h is atomic hallucination rate and Δ_i represents performance differences across layers. Why needed: Traditional accuracy metrics cannot capture the nuanced relationship between context and hallucination detection.
- **Three-layer contextual framework**: Progressive embedding of facts (Layer 1: atomic, Layer 2: story context, Layer 3: scenario context). Why needed: Isolates the impact of contextual reasoning from factual knowledge.
- **Binary classification with strict output constraints**: Models must answer only "yes" or "no" to whether sentences contain hallucinations. Why needed: Exposes instruction-following reliability as a separate failure mode from hallucination detection.
- **Hallucination generation rules**: Systematic transformation of ground truth facts into plausible but incorrect statements using entity attribute modification. Why needed: Creates controlled, verifiable test cases for hallucination detection.
- **Chain-of-Thought prompting**: Explicit reasoning steps before final answer generation. Why needed: Demonstrates that contextual reasoning can be improved through simple prompting strategies.

## Architecture Onboarding

**Component Map**
OnionEval Benchmark -> Three Context Layers -> Binary Classification Task -> CI Score Calculation -> Model Evaluation

**Critical Path**
Data Construction (entities + facts + hallucinations) -> Layer Assembly (atomic → story → scenario) -> Prompt Template Application -> Model Inference -> Output Parsing -> Accuracy/CI Score Calculation

**Design Tradeoffs**
- Binary classification simplicity vs. loss of nuanced reasoning information
- Strict "yes/no" constraints vs. instruction-following reliability
- Progressive context complexity vs. evaluation tractability
- GPT-4 hallucination generation vs. potential bias toward larger model capabilities

**Failure Signatures**
- High "Unmatched Answer" rates (>20%) indicating instruction-following failures
- Near-zero accuracy on Layer 1/2 suggesting context processing inability
- Consistent CI Score values across models indicating context insensitivity

**First Experiments**
1. Run Llama-3.2-3B on Layer 1 atomic facts to establish baseline accuracy
2. Apply Chain-of-Thought prompting to Layer 2 and measure accuracy improvement
3. Calculate CI Score for a single model across all three layers to verify context sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt templates for hallucination generation and evaluation are not fully specified
- High "Unmatched Answer" rates create confounding factors between instruction-following and hallucination detection
- RAG mitigation implementation lacks detail on retrieval mechanisms and context injection
- Reliance on commercial Fireworks API creates practical barriers to reproduction

## Confidence

**High Confidence**: SLLMs experience substantial performance degradation moving from atomic to context-embedded scenarios; Chain-of-Thought significantly improves context-dependent hallucination detection.

**Medium Confidence**: SLLMs perform comparably to larger models on atomic facts but struggle with contextual reasoning; quantitative accuracy values may be sensitive to prompt configurations.

**Low Confidence**: Specific performance percentages across all models and layers due to sensitivity to prompt templates and inference settings.

## Next Checks

1. Replicate the "Unmatched Answer" phenomenon by running OnionEval prompts on Llama-3.2-3B using different chat templates, measuring non-"yes/no" response rates.

2. Independently compute the Context Influence Score using the provided formula on atomic vs. context-embedded layers to verify performance drop patterns.

3. Implement Chain-of-Thought mitigation on Layer 1 and Layer 2 data, measuring accuracy improvements and comparing against reported gains.