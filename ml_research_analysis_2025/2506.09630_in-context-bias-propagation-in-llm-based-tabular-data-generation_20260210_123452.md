---
ver: rpa2
title: In-Context Bias Propagation in LLM-Based Tabular Data Generation
arxiv_id: '2506.09630'
source_url: https://arxiv.org/abs/2506.09630
tags:
- bias
- data
- in-context
- examples
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large Language Models can generate synthetic tabular data for\
  \ data-scarce domains, but their in-context learning is vulnerable to bias propagation\
  \ from in-context examples. We systematically show that statistical biases in in-context\
  \ examples\u2014including marginal, conditional, and intersectional biases\u2014\
  linearly propagate to synthetic data distributions."
---

# In-Context Bias Propagation in LLM-Based Tabular Data Generation

## Quick Facts
- arXiv ID: 2506.09630
- Source URL: https://arxiv.org/abs/2506.09630
- Reference count: 40
- Key outcome: Statistical biases in in-context examples linearly propagate to synthetic data distributions, enabling adversarial bias injection that skews downstream classifier fairness while maintaining utility.

## Executive Summary
This paper exposes a critical vulnerability in LLM-based tabular data generation: biases in the in-context examples used for prompting linearly propagate to synthetic data distributions. Through systematic experimentation across multiple datasets and model families, the authors demonstrate that an attacker can inject a small number of feature-aligned biased examples into the prompt to skew the synthetic distribution and compromise downstream classifier fairness for targeted subgroups, while maintaining high utility. The work evaluates in-context preprocessing defenses, finding that while frequency balancing and fairness-constrained selection reduce disparity, the model's inherent sensitivity to prompt statistics remains a persistent challenge.

## Method Summary
The study uses multiple large language models (Granite-8b, Mistral-7b, Mixtral-8x7b, Qwen-70b) to generate synthetic tabular data via in-context learning. The experimental pipeline involves crafting prompts with curated in-context examples, generating 5000 synthetic rows, and training downstream Random Forest classifiers on this synthetic data. The research evaluates both utility (Macro F1 score) and fairness metrics (Statistical Parity Difference, Equalized Odds, Equal Opportunity) across multiple datasets (Adult, Compas, Diabetes, Thyroid). The methodology systematically injects biases at varying intensities (π) and context sizes (k) to measure propagation effects, while also testing preprocessing defenses like frequency balancing and Fair-SPD.

## Key Results
- Statistical biases in in-context examples—including marginal, conditional, and intersectional biases—linearly propagate to synthetic data distributions
- An adversarial scenario exists where a small number of feature-aligned biased examples can skew the synthetic distribution and compromise downstream classifier fairness for targeted subgroups while maintaining high utility
- In-context preprocessing defenses like frequency balancing and fairness-constrained selection reduce disparity but cannot fully eliminate the model's sensitivity to prompt statistics

## Why This Works (Mechanism)
The vulnerability works because LLMs are fundamentally sensitive to the statistical patterns present in their in-context examples. When generating synthetic tabular data, the model treats the prompt as a representative sample of the target distribution, faithfully reproducing correlations between features—including biased ones—in the output. This becomes problematic because the downstream classifier learns these correlations as ground truth, amplifying the bias. The stealthiness of the attack is maintained because the feature-aligned examples are constructed to preserve overall utility metrics while degrading fairness metrics for specific subgroups.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** This paper's entire vulnerability model rests on understanding ICL as a mechanism where an LLM's output distribution is fundamentally shaped by the examples in its prompt. Without this, the idea of "bias propagation" from a prompt makes little sense.
  - **Quick check question:** Can you explain the difference between an LLM learning a pattern from its pre-training data versus learning a pattern from examples provided in its current prompt?

- **Concept: Statistical Parity Difference (SPD)**
  - **Why needed here:** SPD is the primary metric used to quantify "bias" and "fairness" throughout the paper. Understanding how it measures the disparity in favorable outcomes between a protected group and a privileged group is essential to interpreting the experimental results.
  - **Quick check question:** If a synthetic dataset has a very low SPD score, what does that tell you about the relationship between the protected attribute (e.g., race) and the positive class label?

- **Concept: Generative vs. Discriminative Models in this Context**
  - **Why needed here:** This work studies a two-stage pipeline: an LLM acts as a *generative* model (producing synthetic tabular data), which is then used to train a *discriminative* model (a classifier). The vulnerability exists because biases introduced in the generative stage are faithfully learned and potentially amplified by the discriminative stage.
  - **Quick check question:** Why is a flaw in a generative model's output (e.g., synthetic data) particularly problematic when that output is used as the training set for a downstream discriminative model?

## Architecture Onboarding

- **Component map:** Attacker -> In-Context Examples -> LLM Generator -> Synthetic Data -> Downstream Classifier -> Utility & Fairness Metrics
- **Critical path:** The attack's success flows directly from the prompt construction. The attacker must create feature-aligned examples that correlate a protected attribute with a target label using plausible feature values. The LLM then faithfully propagates this correlation into the synthetic dataset, where it is learned by the downstream classifier.
- **Design tradeoffs:**
  - **Context Size (k):** A larger k improves the fidelity of the synthetic data to the prompt's distribution, which can be beneficial for accuracy but also makes the system more vulnerable to adversarial bias injection (as shown in Figure 3).
  - **Mitigation Strategy:** Preprocessing defenses like Fair-SPD (selecting examples to minimize SPD) can reduce downstream disparity but may not eliminate it entirely because they often fail to capture higher-order correlations and intersectional biases. This involves a tradeoff between the computational cost of the defense and the level of fairness achieved.
- **Failure signatures:**
  - **Stealthiness:** A key indicator of the attack is a stable or even improved Macro F1 score on the downstream classifier, even as its fairness metrics (SPD, EOD, EO) significantly degrade. Standard utility monitoring will not detect this failure mode.
  - **Mitigation Ineffectiveness:** If an in-context defense reduces the SPD of the synthetic data but the SPD of the downstream model remains high, this indicates that bias is leaking through via correlated features that marginal metrics like SPD do not capture.
- **First 3 experiments:**
  1. **Reproduce Marginal Bias Propagation:** Using a dataset like Adult, construct prompts with increasing proportions (π) of a target demographic in the in-context examples. Generate synthetic data and measure the change in that demographic's proportion in the output, confirming the linear relationship observed in Figure 2.
  2. **Launch a Feature-Aligned Attack:** On a dataset like Compas, craft a small set of adversarial examples where a protected group (e.g., African-American) has a high positive label rate but uses plausible feature values (e.g., 3-8 priors). Inject these into a prompt (k=80, π=0.3) and train a downstream Random Forest to measure the change in downstream SPD and race feature importance.
  3. **Evaluate a Preprocessing Defense:** Implement the Fair-SPD mitigation on a biased prompt. Generate synthetic data and train a downstream model. Compare the resulting SPD of the synthetic data and the downstream model to a baseline with no mitigation to quantify the defense's effectiveness and its impact on utility (Macro F1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can in-processing or post-processing techniques effectively mitigate adversarial in-context bias injection while preserving synthetic data utility?
- Basis in paper: [explicit] The authors state: "We leave as future work the study of in-processing and post-processing techniques to further mitigate adversarial in-context bias without compromising the utility of the generated data."
- Why unresolved: Only preprocessing defenses (frequency balancing, Fair-SPD, correlation filtering) were evaluated, and these fail to fully neutralize higher-order correlation-based bias propagation.
- What evidence would resolve it: Systematic evaluation of in-processing interventions (e.g., attention head modifications) and post-processing synthetic data adjustments showing reduced SPDD with F1 preservation.

### Open Question 2
- Question: How can the utility-fairness trade-off in context window sizing be formally characterized and optimized?
- Basis in paper: [inferred] The paper demonstrates that larger k improves fidelity to prompt statistics but simultaneously amplifies bias propagation, yet provides no principled framework for selecting k.
- Why unresolved: The trade-off is characterized empirically (βk increases monotonically with k) but no optimal k selection criterion or adaptive sizing mechanism is proposed.
- What evidence would resolve it: A theoretical or empirical framework linking k selection to desired fairness-utility operating points, validated across datasets and model families.

### Open Question 3
- Question: Do in-processing fairness constraints applied during LLM generation more effectively capture higher-order correlations than marginal preprocessing approaches?
- Basis in paper: [inferred] The authors note biases "often persist due to the model's intrinsic priors and the leakage of higher-order correlations that marginal constraints fail to capture."
- Why unresolved: Fair-SPD and group balancing only enforce marginal parity, leaving conditional and intersectional patterns undetected; no mechanism targets multivariate correlations directly during generation.
- What evidence would resolve it: Comparison of marginal vs. joint distribution constraints in preprocessing, or attention-level interventions, showing reduced conditional/intersectional bias propagation measured via multivariate SPD decomposition.

## Limitations

- The experiments primarily use synthetic prompts with artificially injected bias rather than real-world scenarios where an attacker might influence data curation.
- Defense evaluations focus on preprocessing mitigations but don't explore post-processing or architectural solutions that might be more robust.
- While the paper demonstrates linear bias propagation, the absolute magnitude of downstream impact varies significantly across datasets and model families, suggesting attack effectiveness depends heavily on domain characteristics.

## Confidence

**High Confidence**: The core mechanism of in-context bias propagation is well-established through multiple experimental validations. The linear relationship between prompt bias and synthetic data bias is consistently demonstrated across different datasets and model families.

**Medium Confidence**: The stealthiness claim (maintained utility while degrading fairness) holds in the tested scenarios, but real-world datasets may have different statistical properties that could make the attack more or less detectable through utility metrics.

**Low Confidence**: The effectiveness of proposed defenses is evaluated only in limited scenarios. While frequency balancing and Fair-SPD show promise, the paper acknowledges they don't capture intersectional biases, suggesting current defenses may provide false security in complex real-world applications.

## Next Checks

1. **Real-world Attack Simulation**: Implement an end-to-end attack where an attacker controls data collection or prompt curation in a realistic pipeline (e.g., synthetic data generation for a healthcare analytics platform), measuring both the attack's success and its detectability through standard monitoring systems.

2. **Defense Robustness Testing**: Evaluate the proposed preprocessing defenses against prompts with intersectional biases and higher-order correlations, using metrics beyond SPD (such as subgroup fairness measures) to assess whether defenses that appear effective on marginal metrics actually prevent bias propagation.

3. **Cross-Domain Generalizability**: Test the attack and defense mechanisms across diverse tabular domains (financial, medical, criminal justice) with varying feature distributions and correlations to determine whether the observed vulnerability patterns generalize or are domain-specific.