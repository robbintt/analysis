---
ver: rpa2
title: Retrieval augmented generation based dynamic prompting for few-shot biomedical
  named entity recognition using large language models
arxiv_id: '2508.06504'
source_url: https://arxiv.org/abs/2508.06504
tags:
- shot
- biomedical
- prompt
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of few-shot biomedical named
  entity recognition (NER) by combining static and dynamic prompt engineering with
  retrieval-augmented generation (RAG). The proposed method uses structured static
  prompts and dynamically selects in-context examples via retrieval engines (TF-IDF,
  SBERT, ColBERT, DPR) to improve LLM performance in biomedical NER.
---

# Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models

## Quick Facts
- **arXiv ID:** 2508.06504
- **Source URL:** https://arxiv.org/abs/2508.06504
- **Authors:** Yao Ge; Sudeshna Das; Yuting Guo; Abeed Sarker
- **Reference count:** 40
- **Primary result:** Dynamic retrieval of in-context examples improves few-shot biomedical NER F1-scores by 7.3% (5-shot) and 5.6% (10-shot) over static prompting.

## Executive Summary
This study addresses few-shot biomedical named entity recognition by combining structured static prompts with dynamic retrieval of similar training examples. The method uses retrieval engines (TF-IDF, SBERT, ColBERT, DPR) to select contextually relevant in-context learning examples at inference time. Evaluation across five biomedical datasets with GPT-4, GPT-3.5, and LLaMA 3 shows that structured static prompts increased average F1-scores by 12% for GPT-4 and 11% for other models, while dynamic prompting further improved performance by 7.3% and 5.6% in 5-shot and 10-shot settings respectively.

## Method Summary
The method combines static and dynamic prompt engineering with retrieval-augmented generation for few-shot biomedical NER. Static prompts are enhanced with structured components including dataset descriptions, high-frequency examples, entity definitions, and error analysis guidelines. During inference, a retrieval engine selects the top-k most similar training examples based on input text similarity, dynamically updating the prompt for each instance. The approach uses token-label formatting to ensure precise output alignment and evaluation stability. The system was evaluated on five biomedical datasets using GPT-4, GPT-3.5, and LLaMA 3 across 1-shot to 20-shot settings.

## Key Results
- Structured static prompts improved average F1-scores by 12% for GPT-4 and 11% for GPT-3.5 and LLaMA 3
- Dynamic prompting with TF-IDF and SBERT achieved the highest gains, improving F1-scores by 7.3% (5-shot) and 5.6% (10-shot)
- Performance degradation observed for LLaMA 3 when increasing shot size from 10 to 20, with F1-score decreasing by 1.46%
- Complex retrieval methods (ColBERT, DPR) generally underperformed simpler TF-IDF approach

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Retrieval of Contextually Similar In-Context Examples
Dynamically retrieving training examples based on similarity to input text improves few-shot NER performance by providing adaptive context for each specific input. At inference, a retrieval engine selects top-k most similar examples from training set and injects them into LLM's prompt, reducing variance inherent in static prompting. This bridges gap between model's general pretraining and specific extraction task. Effectiveness depends on training set diversity and retrieval metric correlating with task-relevant similarity.

### Mechanism 2: Structured Static Prompting for Task Framing
Decomposing prompt into structured, task-specific components significantly improves performance by grounding LLM's internal knowledge in specific constraints of target dataset. Rich context including dataset descriptions, entity definitions, and high-frequency examples acts as strong signal that frames the task. LLMs possess sufficient underlying biomedical knowledge, and performance is limited by inability to correctly frame and apply that knowledge to narrowly defined extraction task.

### Mechanism 3: Token-Label Formatting for Output Alignment
Presenting input as pre-tokenized lists and instructing LLM to output token-label pairs improves alignment and evaluation stability. Generative LLMs are not inherently sequence-labeling models, so this method enforces one-to-one correspondence between input and output tokens. This eliminates common errors where LLM's internal tokenization mismatches dataset's tokenization, preventing failures due to format errors.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** Core technique allowing model to perform task without weight updates using only examples in prompt. This paper's dynamic prompting optimizes selection of these examples.
  - **Quick check question:** If you provide a model with 5 NER examples in prompt, what key assumption are you making? (Answer: That model has learned generalizable representation of NER and medical domain from pre-training and needs examples only to understand specific output format and label schema).

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Paper applies RAG to training set for example lookup rather than knowledge base for factual lookup. Retrieval step is "Which labeled sentence is most like this one?" not "What is the capital?"
  - **Quick check question:** In this paper's use of RAG, what acts as "query" and what is "corpus"? (Answer: Input sentence is query; pool of labeled training examples is corpus).

- **Concept: Static vs. Dynamic Prompting**
  - **Why needed here:** Paper compares these two approaches. Static prompting uses fixed prompt for all inputs. Dynamic prompting builds new prompt for each input by selecting relevant examples, reducing variance and improving average performance.
  - **Quick check question:** Why might dynamic prompting show less improvement as shot size increases (e.g., from 5-shot to 20-shot)? (Answer: As number of examples in static prompt increases, set becomes more diverse by chance, reducing marginal benefit of carefully selecting for similarity).

## Architecture Onboarding

- **Component map:** Input Preprocessor -> Retrieval Engine -> Prompt Constructor -> LLM -> Output Parser
- **Critical path:** Retrieval step is critical for each inference call, involving vectorizing input, querying training index, fetching top-k matches, and injecting them into prompt. Latency here adds directly to inference time.
- **Design tradeoffs:**
  * Retrieval Engine: TF-IDF is fastest but lacks semantic understanding; SBERT adds semantic matching but higher overhead. Paper found complex engines provided minimal benefit.
  * Shot Size (k): Increasing k provides more context but consumes context window. Paper observed diminishing returns from 10-shot to 20-shot.
  * Static vs. Dynamic: Dynamic prompting improves performance but adds infrastructure complexity; static prompting is stateless and simpler but performs worse.
- **Failure signatures:**
  * Token Mismatch: Output parser fails because LLM generated different number of tokens than provided in input
  * Performance Drop vs. Static: Indicates retrieval engine selecting distracting or incorrect examples
  * No Improvement with More Examples: Suggests model reached capability ceiling or added examples are noisy/redundant
- **First 3 experiments:**
  1. Baseline Reproduction (Static): Implement static prompt with "all components" on single dataset to verify prompting and evaluation pipeline aligns with reported F1 scores
  2. Ablation Study (Static): Run static prompt while adding one component at a time to identify which elements provide most value
  3. Dynamic Retrieval Validation: Implement TF-IDF retrieval engine over training set, integrate to build dynamic prompts, compare F1 score to static prompt baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do dense retrieval methods (ColBERT, DPR) underperform simpler sparse methods (TF-IDF) in few-shot biomedical NER contexts?
- Basis in paper: Authors note ColBERT and DPR "generally underperformed" and hypothesize they may "overfit to irrelevant semantic similarities" or fail to capture "domain-specific distinctions" compared to TF-IDF
- Why unresolved: Paper presents this as hypothesis to explain empirical failure without providing ablation study verifying if retrieved documents were actually less relevant or if LLM simply struggled to utilize dense representations
- What evidence would resolve it: Comparative error analysis of top-k documents retrieved by TF-IDF versus ColBERT/DPR, specifically measuring lexical overlap and semantic relevance to ground truth entities

### Open Question 2
- Question: What specific mechanism causes performance degradation in open-source models (e.g., LLaMA 3) as number of in-context examples increases from 10-shot to 20-shot?
- Basis in paper: Results report that for LLaMA 3, "mean F1-score decreases by -1.46%" from 10-shot to 20-shot. Discussion speculates this is due to "diminishing returns," "noise," or "input token limits"
- Why unresolved: Paper observes inverse relationship but does not isolate whether drop is caused by context window truncation, "lost-in-the-middle" phenomenon, or specific quality of additional retrieved examples
- What evidence would resolve it: Experiments varying context window size independently of shot count, or analyzing attention weights to determine if later examples are effectively ignored

### Open Question 3
- Question: Why does incorporating external UMLS knowledge improve performance on clinical datasets (BC5CDR) while reducing performance on social media datasets (REDDIT-IMPACTS)?
- Basis in paper: Results indicate UMLS knowledge improved recall for BC5CDR but decreased F1-score for REDDIT-IMPACTS. Discussion suggests background information may have "diluted model's ability to capture task-specific cues" in noisy datasets
- Why unresolved: Paper does not identify specific nature of "noise" introduced or why formal ontological definitions conflict with informal social media language
- What evidence would resolve it: Qualitative analysis contrasting UMLS definitions provided for entities found in BC5CDR versus those in REDDIT-IMPACTS to identify domain mismatches

## Limitations
- Context window constraints not thoroughly explored, with diminishing returns at 20-shot noted but not deeply analyzed
- Retrieval quality assumptions rely on similarity correlating with task-relevant similarity without explicit validation
- Generalizability limited to tested biomedical datasets without addressing transfer to non-biomedical domains or datasets with different characteristics

## Confidence
- **High Confidence:** Core claim that dynamic retrieval improves few-shot NER performance over static prompting is well-supported by experimental results
- **Medium Confidence:** Assertion that structured static prompts improve performance by 12% for GPT-4 and 11% for other models is supported but specific component contributions not fully isolated
- **Low Confidence:** Comparison of different retrieval engines limited by observation that complex engines provided minimal benefit, could indicate genuine finding or suboptimal implementation

## Next Checks
1. Component Ablation Study: Systematically remove each static prompt component one at a time to quantify individual contributions to 12% performance improvement
2. Retrieval Quality Analysis: Conduct qualitative analysis of top-5 retrieved examples for sample of test inputs to assess whether retrieval engine selects truly relevant examples or just superficially similar ones
3. Context Window Stress Test: Systematically increase shot size beyond 20 to identify point of diminishing returns more precisely and understand how context window constraints affect performance