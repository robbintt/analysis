---
ver: rpa2
title: Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants
arxiv_id: '2508.08266'
source_url: https://arxiv.org/abs/2508.08266
tags:
- error
- mean
- tool
- accuracy
- virginia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were benchmarked for geolocating colonial
  Virginia land grants using 43 curated test cases. Direct-to-coordinate prompting
  achieved a mean error of 23 km, outperforming a single-analyst GIS baseline by 67%
  and a Stanford NER geoparser by 70%.
---

# Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants

## Quick Facts
- **arXiv ID:** 2508.08266
- **Source URL:** https://arxiv.org/abs/2508.08266
- **Reference count:** 33
- **Primary result:** Direct-to-coordinate prompting achieved 23 km mean error, outperforming a single-analyst GIS baseline by 67%.

## Executive Summary
This study benchmarks large language models for geolocating 17th-18th century Virginia land grants described in archaic metes-and-bounds text. Using 43 curated test cases with verified ground truth, direct LLM prompting achieved 23 km mean error, significantly outperforming both a single-analyst GIS baseline and Stanford's neural geoparser. An ensemble of five o3-2025-04-16 calls reduced error to 19 km, while tool-augmented chain-of-thought reasoning actually degraded performance by 30%. The research demonstrates LLMs can deliver accurate, cost-effective historical georeferencing at scale, though epistemological limitations around training data contamination remain.

## Method Summary
The study evaluated 12 different LLM approaches on 43 colonial Virginia land patent abstracts from Cavaliers and Pioneers, Vol. 3. Methods ranged from direct one-shot prompting to tool-augmented chain-of-thought reasoning with external geocoding APIs. Ground truth coordinates were derived from One Shared Story GIS polygons. Performance was measured using Haversine distance, with cost and latency tracked. The best approach used a five-call ensemble of o3-2025-04-16 with DBSCAN clustering, achieving 19 km mean error at minimal additional cost.

## Key Results
- Direct-to-coordinate prompting achieved 23 km mean error, outperforming single-analyst GIS baseline by 67%
- Five-call ensemble of o3-2025-04-16 reduced error to 19 km, a 48.6% improvement over median LLM
- Tool-augmented chain-of-thought reasoning degraded accuracy by 30% due to cascading search bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large Language Models (LLMs) can directly predict latitude/longitude coordinates from archaic text by leveraging internalized spatial associations without external tools.
- **Mechanism:** The model ingests the "metes-and-bounds" narrative (e.g., "on S. side the Black Water"), identifies toponyms and spatial relationships (adjacency, watershed), and maps these linguistic tokens to a latent geographic coordinate space derived from pre-training.
- **Core assumption:** The model has encountered sufficient modern or historical geographic data during training to resolve archaic toponyms (e.g., "Nottoway Path") to approximate physical locations.
- **Evidence anchors:**
  - [abstract]: "Direct-to-coordinate prompting achieved a mean error of 23 km, outperforming a single-analyst GIS baseline by 67%."
  - [section]: Section 7.2 states, "Sophisticated language models like o3 already encode substantial geographic knowledge... placing grants within their correct watershed without external reference data."
  - [corpus]: Related work (e.g., *ISS-Geo142*) confirms LLMs can geolocate from visual/textual features, but specific colonial-text-to-coordinate mechanisms are unique to this paper's findings.
- **Break condition:** Descriptions relying exclusively on private landmarks (e.g., "William Rickett's house") or obsolete names with no surviving geographic trace cause the association chain to fail.

### Mechanism 2
- **Claim:** Stochastic ensembling reduces positional error by averaging out uncorrelated prediction noise.
- **Mechanism:** Five independent inference calls are made with different random seeds. The algorithm clusters these points (using DBSCAN) and computes a spherical centroid. This cancels out extreme outliers ("hallucinations") that occur in single-shot generation.
- **Core assumption:** Geolocation errors are largely random variance (noise) rather than systematic bias, allowing averaging to converge on the true location.
- **Evidence anchors:**
  - [abstract]: "A five-call ensemble of o3-2025-04-16 reduced error to 19 km... outperforming the median LLM by 48.6%."
  - [section]: Section 4.7 describes the "majority-vote strategy reduces random scatter and mitigates occasional large-error outliers."
  - [corpus]: Weak link. Neighbor papers on Agent RAG systems discuss cache/retrieval, but ensemble mechanisms for spatial regression specifically are less documented in the provided corpus.
- **Break condition:** If the model exhibits systematic bias (e.g., consistently mapping a historical parish to a modern city of the same name), the ensemble will reinforce the error rather than correct it.

### Mechanism 3
- **Claim:** Tool-augmented chain-of-thought (CoT) fails to improve accuracy because modern geocoding APIs introduce "cascading search bias" when resolving historical terms.
- **Mechanism:** The LLM queries a modern API (Google Geocoding) with an archaic term. The API returns a modern match (e.g., a town name instead of a defunct swamp). The LLM accepts this incorrect anchor, and subsequent reasoning steps propagate the error, resulting in higher mean error than pure intuition.
- **Core assumption:** Assumption: External tools generally improve precision by providing "ground truth." This assumption breaks for historical data where modern databases lack context.
- **Evidence anchors:**
  - [abstract]: "Tool-augmented chain-of-thought did not improve accuracy."
  - [section]: Section 6.1 notes that "supplementing gpt-4.1... with explicit Google-Maps queries did not improve accuracyâ€”in fact, the tool-chain variant T-4 performed 30% worse."
  - [corpus]: The neighbor paper *GeoAgent* claims tools *empower* LLMs for address standardization, highlighting that this failure is specific to the historical/archaic domain.
- **Break condition:** This mechanism applies specifically when external knowledge bases are temporally mismatched to the input text. It would likely *not* break if the tool accessed a historical gazetteer.

## Foundational Learning

**Concept: Metes-and-Bounds Descriptions**
- **Why needed here:** The input data is not structured addresses but narrative legal text defining boundaries relative to landmarks ("beg. on S. side the Black Water"). Understanding this structure is required to design prompts or interpret failure modes.
- **Quick check question:** Can you distinguish between a "corner" (a point) and a "course" (a direction/distance) in a surveyor's description?

**Concept: Haversine (Great-Circle) Distance**
- **Why needed here:** The primary metric (Mean Error in km) relies on calculating the distance between two spherical coordinates. Standard Euclidean distance fails over long distances due to Earth's curvature.
- **Quick check question:** Why is the distance calculation different for two points at the equator versus two points at the same latitude difference near the pole?

**Concept: DBSCAN (Density-Based Clustering)**
- **Why needed here:** Used in the ensemble method (E-1) to determine which of the 5 predictions cluster together. It separates "consensus" from "noise" better than simple averaging.
- **Quick check question:** How does DBSCAN treat a single prediction that is 50km away from a tight cluster of 4 other predictions?

## Architecture Onboarding

**Component map:** OCR pipeline -> Text strings -> OpenAI API client -> Inference Core (Direct/Tool paths) -> Post-Processor -> (Optional) Ensembler

**Critical path:** The **Prompt Engineering** (Section A.2) is the highest-leverage component. The study found that a well-designed direct prompt (M-2) outperformed complex toolchains.

**Design tradeoffs:**
- **Cost vs. Accuracy:** `gpt-4o` costs ~$1/1k grants but has ~28km error. `o3` (ensemble) costs ~$200/1k grants for ~19km error.
- **Speed vs. Robustness:** Tool-chains add latency (3-10 seconds overhead) and cost without improving accuracy for this specific domain.

**Failure signatures:**
- **"Modern Bias":** Model predicts a modern city center (e.g., Richmond) when the text refers to a broad historical region.
- **"Cascading Error":** Tool-augmented runs lock onto a wrong API result and refuse to backtrack (see Section 6.4).

**First 3 experiments:**
1. **Reproduce M-2 vs. T-4:** Run the provided `run_experiment.py` on the 43-case test set to confirm the "tool penalty" (37 km vs 23 km error).
2. **Ablation on "Reasoning Effort":** Test `o3` with `reasoning_effort={low, medium, high}` to verify the paper's claim that increased reasoning tokens yield diminishing returns (<1 km error difference).
3. **Name-Redaction Test:** Re-run the ensemble with patentee names masked (e.g., replace "JOHN POYTHRESS" with "[NAME]") to test if the model relies on memorizing famous landholders vs. geographic reasoning.

## Open Questions the Paper Calls Out

**Open Question 1:** Can open-source LLMs fine-tuned on historical prose replicate the geolocation performance of proprietary models like GPT-4 or o3?
- **Basis in paper:** [explicit] The "Future Work" section explicitly calls for "testing open-source LLMs fine-tuned on historical prose" to verify scalability outside the OpenAI ecosystem.
- **Why unresolved:** The study restricted models to the OpenAI GPT/o-series to control for cross-vendor confounders such as tokenization and tool-call semantics.
- **What evidence would resolve it:** A replication study applying the same benchmark to fine-tuned Llama or Mistral variants.

**Open Question 2:** Does the integration of historical-specific gazetteers (rather than modern APIs) improve the accuracy of tool-augmented chain-of-thought reasoning?
- **Basis in paper:** [inferred] Tool-augmented pipelines underperformed one-shot prompting, partly due to "cascading search bias" from modern geocoder errors on historical toponyms (e.g., matching "St. Paul" to a modern town 400 km away).
- **Why unresolved:** The study only tested Google's Geocoding API, which is optimized for modern place names and may introduce noise for 18th-century locations.
- **What evidence would resolve it:** Ablation studies using custom tools wrapping historical gazetteers instead of standard Google Maps API calls.

**Open Question 3:** Are the observed results influenced by training-data contamination (memorization) rather than spatial reasoning?
- **Basis in paper:** [explicit] The "Limitations" section identifies training data contamination as a significant uncertainty, noting models may have been exposed to the ground-truth GIS datasets used for verification.
- **Why unresolved:** Standard contamination detection methods are unreliable for spatial datasets, and heuristic checks (Google search, Min-hash) could not definitively exclude indirect exposure.
- **What evidence would resolve it:** Evaluation on guaranteed unseen spatial datasets or systematic contamination analysis.

**Open Question 4:** How does the top-performing LLM approach compare to end-to-end pipelines built with Google's Geospatial Reasoning framework?
- **Basis in paper:** [explicit] The "Future Work" section suggests a follow-on study to "benchmark end-to-end pipelines built with Google's nascent Geospatial Reasoning framework."
- **Why unresolved:** The study only compared LLMs against standard neural geoparsers and a human baseline, not agentic geospatial frameworks from other major vendors.
- **What evidence would resolve it:** A head-to-head benchmark comparing the o3-ensemble's accuracy and cost against a Gemini-based geospatial agent.

## Limitations

**Model Availability:** The study relies on specific model snapshots (e.g., `o3-2025-04-16`) that may not be available for reproduction. Exact numerical results depend on these dated versions.

**Human Baseline Opacity:** The single-analyst GIS baseline (H-1) uses a "bespoke" analysis script developed by a contractor. Without access to this specific implementation, the claimed 67% improvement over human performance cannot be independently verified.

**Training Data Contamination:** The authors acknowledge they cannot determine if models memorized ground truth coordinates during pre-training. This creates an epistemic uncertainty about whether high accuracy generalizes to new historical corpora or merely reflects pattern matching on the evaluation set.

## Confidence

**High Confidence:** The claim that direct-to-coordinate prompting (M-2) outperforms tool-augmented chain-of-thought (T-4) is well-supported by controlled experiments showing a consistent 30% error penalty for the tool approach.

**Medium Confidence:** The ensemble method (E-1) reducing error from 23 km to 19 km is credible given the described clustering mechanism, but depends on the availability of specific model snapshots and the assumption that errors are random rather than systematic.

**Low Confidence:** The claim about cost-effectiveness at scale assumes stable API pricing and that the 43-case test set is representative of the full 5,471-abstract corpus, which may not account for increasing difficulty with more obscure patents.

## Next Checks

1. **Model Snapshot Verification:** Attempt to access and run the exact model versions specified (`o3-2025-04-16`, `gpt-4o-2024-08-06`) through the OpenAI API. Document any version drift or unavailability that would prevent faithful reproduction.

2. **Ground Truth Independence Test:** Run the ensemble method on a held-out subset of abstracts (5-10 cases) not included in the original 43-case benchmark to assess whether accuracy holds for truly unseen data.

3. **Tool Chain Error Attribution:** When reproducing the tool-augmented method (T-4), log and categorize each failure: distinguish between "API returned wrong place" versus "LLM misinterpreted API result." This will validate whether the failure mode is truly cascading search bias as claimed.