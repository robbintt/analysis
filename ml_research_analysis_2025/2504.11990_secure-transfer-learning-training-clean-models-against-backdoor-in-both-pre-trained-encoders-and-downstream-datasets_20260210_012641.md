---
ver: rpa2
title: 'Secure Transfer Learning: Training Clean Models Against Backdoor in (Both)
  Pre-trained Encoders and Downstream Datasets'
arxiv_id: '2504.11990'
source_url: https://arxiv.org/abs/2504.11990
tags:
- clean
- backdoor
- dataset
- samples
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses backdoor attacks in transfer learning where
  pre-trained encoders and/or downstream datasets may be poisoned. The key challenge
  is defending against unknown backdoor threats with limited computational resources,
  as existing defenses fail due to their reactive nature and assumptions that don't
  scale across different training paradigms.
---

# Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets

## Quick Facts
- arXiv ID: 2504.11990
- Source URL: https://arxiv.org/abs/2504.11990
- Reference count: 40
- Primary result: Achieves attack success rates below 10% while maintaining high accuracy against multiple backdoor attacks in transfer learning

## Executive Summary
This paper addresses backdoor attacks in transfer learning where pre-trained encoders and/or downstream datasets may be poisoned. The key challenge is defending against unknown backdoor threats with limited computational resources, as existing defenses fail due to their reactive nature and assumptions that don't scale across different training paradigms. The authors propose a proactive Trusted Core (T-Core) Bootstrapping framework that identifies clean elements rather than searching for poisoned ones. Their method involves sifting high-credibility seed data using topological invariance across network layers, bootstrapping a clean subset through loss-guided expansion, filtering trusted encoder channels, and progressively training with trusted data. Evaluated across 5 encoder poisoning attacks, 7 dataset poisoning attacks, and 14 baseline defenses on 5 benchmark datasets, T-Core achieves attack success rates below 10% while maintaining high accuracy.

## Method Summary
The Trusted Core (T-Core) framework takes an untrusted pre-trained encoder and downstream dataset as input and outputs a secure fine-tuned model. The method operates in four phases: (1) Topological Invariance Sifting (TIS) identifies clean seed data by filtering samples that maintain consistent nearest neighbors and belong to the majority cluster across multiple layers; (2) Seed Expansion uses confusion training to select samples with maximal loss (likely clean hard examples) to build a trusted subset; (3) Encoder Filtering purifies channels through selective unlearning and recovery, isolating backdoor functionality into specific channels; and (4) Bootstrapping progressively expands the training set, leveraging different learning dynamics of complex clean samples versus backdoor samples. The framework is proactive rather than reactive, identifying clean elements instead of detecting poisoned ones.

## Key Results
- Achieves attack success rates below 10% against 5 encoder poisoning attacks, 7 dataset poisoning attacks, and 14 baseline defenses
- Maintains high accuracy across 5 benchmark datasets including CIFAR-10, CIFAR-100, GTSRB, SVHN, and STL-10
- Demonstrates scalability and efficiency compared to existing defenses while being effective against adaptive attacks and transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Identifying clean seed data is more reliable than detecting poisoned samples in resource-constrained transfer learning (TL) because poisoned samples lose their distinct clustering properties when only the classification head is fine-tuned.
- **Mechanism:** Topological Invariance Sifting (TIS) filters samples that maintain consistent nearest neighbors and belong to the majority cluster across multiple layers of the fine-tuned model. Subsequently, a "confusion training" expansion selects samples with maximal loss (likely clean hard examples) to build a trusted subset.
- **Core assumption:** Poisoned samples are a minority and exhibit inconsistent topological behavior or lower loss compared to clean samples in the specific "confused" training state.
- **Evidence anchors:**
  - [Abstract]: "identifying and expanding trustworthy data... iteratively filters clean samples using topological invariance."
  - [Section 4.1.1]: Describes the "Majority Rule" and "Consistency Rule."
  - [Corpus]: Weak direct support; neighbor papers focus on detection or distillation, not topological sifting in TL.
- **Break condition:** High poisoning ratios (>50%) breaking the majority rule, or adaptive attacks specifically designed to mimic topological consistency.

### Mechanism 2
- **Claim:** Backdoor functionality in pre-trained encoders can be isolated into specific channels by contrasting the model's behavior on "untrusted" general data versus "trusted" clean data.
- **Mechanism:** The framework first "unlearns" on the full dataset by maximizing loss via normalization layers, degrading the backdoor. It then "recovers" performance exclusively on the trusted subset by learning channel masks; channels essential for recovering clean accuracy are marked as trusted, while those ignored or detrimental are re-initialized.
- **Core assumption:** Backdoor features rely on activation pathways that are distinct from, or at least less critical for, the semantic features required by the clean subset during the recovery phase.
- **Evidence anchors:**
  - [Section 4.2]: "selectively imbalanced unlearn-recover process to filter clean channels."
  - [Abstract]: "purifies encoder channels through selective unlearning and recovery."
  - [Corpus]: *Pruning and Malicious Injection* (Corpus ID 51359) discusses pruning-related attacks, indirectly supporting that channel importance varies, but this specific "unlearn-recover" defense is novel.
- **Break condition:** Entangled features where backdoor channels are identical to those required for high clean accuracy.

### Mechanism 3
- **Claim:** A model can be "bootstrapped" to robustness by progressively expanding the training set, leveraging the different learning dynamics of complex clean samples versus backdoor samples.
- **Mechanism:** The process reinitializes "untrusted" channels and trains on the growing clean pool. It initially prioritizes low-loss samples (easy clean data) and then uses meta-guidance (comparing loss reduction between a temporary model trained on the full set vs. the clean set) to distinguish "clean hard" samples from "poisoned" ones.
- **Core assumption:** Poisoned samples, while potentially high-confidence (low loss), do not exhibit the same gradient/loss-reduction pattern as complex clean samples when contrasted against a partially clean training trajectory.
- **Evidence anchors:**
  - [Section 4.3]: "Clean Pool Expansion with Meta Guidance... select samples with the smallest loss reduction."
  - [Abstract]: "bootstraps learning by progressively expanding the trusted dataset."
  - [Corpus]: *Can Distillation Mitigate Backdoor...* (Corpus ID 86913) suggests standard distillation fails; T-Core offers a specific training-dynamic alternative.
- **Break condition:** Adaptive attacks that explicitly optimize for the meta-learning objective used for selection.

## Foundational Learning

- **Concept: Transfer Learning & Linear Probing**
  - **Why needed here:** The defense is constrained by the TL setting where the encoder is frozen or only partially tuned. Understanding the difference in feature spaces between "fine-tuning all" vs. "fine-tuning head" explains why standard defenses fail.
  - **Quick check question:** If I freeze the encoder, does the latent representation of a poisoned sample cluster differently than if I train the whole network? (The paper argues yes, they blend in more).

- **Concept: Backdoor Trigger Dynamics**
  - **Why needed here:** Distinguishing Threat-1 (Encoder) from Threat-2 (Dataset) from Threat-3 (Both) is crucial. The mechanism relies on the backdoor relying on specific channel subsets which can be isolated.
  - **Quick check question:** Does the backdoor require specific neurons in the *encoder*, or can it exist solely in the *head*? (Mechanism 2 assumes encoder channels are the culprit for Threat-1).

- **Concept: Confusion Training**
  - **Why needed here:** Used in the "Seed Expansion" phase. It inverts standard training logic: instead of finding low-loss samples to keep, it uses mislabeled clean data to confuse the model, making the backdoor (which remains consistent) stand out or clean hard data appear high-loss.
  - **Quick check question:** If I train a model to minimize loss on mislabeled clean data, what happens to the loss of correctly labeled clean data vs. poisoned data?

## Architecture Onboarding

- **Component map:** Input: Untrusted Encoder ($g$) + Untrusted Dataset ($D$) -> TIS Module -> Seed Data ($S$) -> Seed Expansion -> Clean Subset ($D_{sub}$) -> Encoder Filtering -> Trusted Channels ($\chi$) & Untrusted Channels ($\psi$) -> Bootstrapping -> Final Model

- **Critical path:** The **TIS Module** (Mechanism 1) is the foundation. If the initial seed data contains poisoned samples, the subsequent "Recovery" in Mechanism 2 will reinforce the backdoor.

- **Design tradeoffs:**
  - **Seed Ratio ($\alpha$):** Too small = insufficient data for recovery; too large = risk of including poison.
  - **Expansion Rate ($\rho$):** Stopping early (e.g., 70% of data) maintains high security (low ASR) but may drop accuracy. The paper suggests 90% as a balance.

- **Failure signatures:**
  - **High ASR (>10%) with High ACC:** Likely failure in Encoder Filtering (Mechanism 2); the backdoor channels were marked as "trusted."
  - **Low ACC & High ASR:** Failure in Seed Expansion (Mechanism 1); "clean" set was actually poisoned, leading to unlearning of clean features.
  - **SVHN Performance Drop:** Specific failure signature noted in the paper due to the noisy nature of the dataset (extraneous digits).

- **First 3 experiments:**
  1. **Verify Seed Purity:** Run TIS on a known poisoned dataset (e.g., CIFAR-10 with BadNets) and calculate the False Positive Rate (number of poisoned samples in the top 1% selection). Target: 0.
  2. **Encoder Isolation Test:** Apply the "Unlearn-Recover" module to a BadEncoder checkpoint. Check if the mask values for known backdoor-related channels (if identifiable) drop below the threshold $\sigma$.
  3. **End-to-End ASR Evaluation:** Run the full T-Core pipeline against Threat-3 (Encoder + Dataset poisoned) and compare ASR against a "No Defense" baseline and a standard fine-tuning defense.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the T-Core framework be adapted to maintain effectiveness in low-data regimes where identifying a sufficient clean seed set is difficult?
- **Basis in paper:** [explicit] The authors state that "A key limitation arises when the training data is scarce," noting performance drops on smaller datasets like STL-10.
- **Why unresolved:** The framework relies on bootstrapping from a "trusted core" of sifted clean samples; insufficient data volume prevents the robust initialization required for this expansion.
- **What evidence would resolve it:** Demonstrating a modified T-Core method that maintains high accuracy (ACC) and low attack success rates (ASR) on few-shot downstream tasks.

### Open Question 2
- **Question:** Can the proactive defense approach be generalized to detect backdoors in language models or multimodal zero-shot architectures like CLIP?
- **Basis in paper:** [explicit] The authors list extending T-Core to "language-domain backdoors or multimodal zero-shot settings" as an open challenge for future work.
- **Why unresolved:** Current validation is limited to image classification, and the concept of topological invariance across layers may manifest differently in textual embeddings or joint multimodal spaces.
- **What evidence would resolve it:** Successful application of clean-element identification techniques to NLP backdoors (e.g., BERT) or vision-language models.

### Open Question 3
- **Question:** How can the topological invariance sifting (TIS) be strengthened to defend against high-budget adaptive attacks designed to bypass the majority and consistency rules?
- **Basis in paper:** [inferred] While effective against moderate threats, Table 12 shows the Attack Success Rate (ASR) surges to 92.52% under strong adaptive attacks with a perturbation budget ($l_\infty$) of 16.
- **Why unresolved:** The current sifting relies on geometric heuristics (majority/consistency) that can be explicitly optimized against by an adversary with sufficient perturbation freedom to mimic clean topological properties.
- **What evidence would resolve it:** A mechanism that maintains robustness even when adversarial perturbations are large enough to superficially satisfy the topological invariance criteria.

## Limitations
- The framework's effectiveness hinges on the majority assumption (poisoned samples <50%), making it potentially vulnerable to adaptive attacks designed to mimic clean topological patterns or pass the unlearning/recovery channel selection.
- Performance degradation on noisy datasets (e.g., SVHN) suggests sensitivity to data quality and potential overfitting to specific data characteristics during the topological sifting phase.
- The method requires multiple passes over the dataset and model checkpoints, which, while more efficient than many baselines, still presents computational overhead compared to single-pass fine-tuning.

## Confidence
- **High Confidence:** The general framework design (TIS for seed selection, unlearn-recover for channel filtering) is logically sound and addresses a critical gap in existing TL defenses.
- **Medium Confidence:** Empirical results demonstrating low ASR across diverse attacks are strong, but the paper could benefit from more analysis of failure modes under extreme poisoning ratios or adaptive attacks.
- **Medium Confidence:** The claim that poisoned samples lose distinct clustering in TL head-only fine-tuning is supported by the mechanism but lacks extensive ablation studies varying the degree of encoder tuning.

## Next Checks
1. **Adaptive Attack Resilience:** Evaluate T-Core against a specifically crafted adaptive attack where the backdoor is designed to mimic the topological consistency or loss patterns of clean samples during the seed expansion phase.
2. **Scalability and Efficiency Benchmark:** Conduct a head-to-head runtime comparison of T-Core against the top-3 most efficient baseline defenses (e.g., activation clustering, spectral signatures) on a large-scale dataset (e.g., ImageNet) to quantify the claimed computational advantage.
3. **Poison Ratio Stress Test:** Systematically vary the poisoning ratio (e.g., 10%, 30%, 50%, 70%) in the dataset and encoder to identify the precise breaking point of the majority assumption and the framework's overall robustness.