---
ver: rpa2
title: 'Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with
  Large Language Models'
arxiv_id: '2512.00590'
source_url: https://arxiv.org/abs/2512.00590
tags:
- entity
- object
- relation
- subject
- wikontic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Wikontic, a pipeline for constructing knowledge
  graphs (KGs) from unstructured text using large language models (LLMs) and Wikidata
  ontology constraints. The method extracts candidate triplets with qualifiers, enforces
  schema constraints through entity typing and relation validation, and normalizes
  entity names to reduce duplication.
---

# Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models

## Quick Facts
- arXiv ID: 2512.00590
- Source URL: https://arxiv.org/abs/2512.00590
- Reference count: 23
- Outperforms retrieval-augmented baselines on multi-hop QA while using only constructed KGs

## Executive Summary
Wikontic is a pipeline for constructing knowledge graphs from unstructured text using large language models (LLMs) and Wikidata ontology constraints. The method extracts candidate triplets with qualifiers, enforces schema constraints through entity typing and relation validation, and normalizes entity names to reduce duplication. When used as the sole knowledge source for multi-hop question answering, Wikontic achieves 76.0 F1 on HotpotQA and 59.8 F1 on MuSiQue, matching or surpassing retrieval-augmented baselines that still access raw text. The method attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%) and constructs KGs using less than 1,000 output tokens—about 3× fewer than AriGraph and <1/20 of GraphRAG.

## Method Summary
Wikontic constructs knowledge graphs through a three-stage pipeline. First, an LLM extracts candidate triplets with subject, relation, object, qualifiers, and entity types from input text. Second, the pipeline refines these candidates by retrieving top-10 entity types from Wikidata via dense retrieval, validating relations against domain-range constraints, and selecting the ontology-compliant configuration using an LLM. Third, entities are normalized by matching surface forms to existing KG entities using alias matching and embedding similarity, with LLM determination of matches versus new entities. The resulting KG supports iterative multi-hop question answering through sequential subquestion decomposition grounded in KG neighborhoods, enabling reasoning without raw text access.

## Key Results
- Achieves 76.0 F1 on HotpotQA and 59.8 F1 on MuSiQue as sole knowledge source
- Constructs KGs using ~881 output tokens (3× fewer than AriGraph, <1/20 of GraphRAG)
- State-of-the-art 86% information retention on MINE-1 benchmark
- Correct answer entity appears in 96% of generated triplets on MuSiQue
- Only 3.5% of triplets flagged as ontology-misaligned

## Why This Works (Mechanism)

### Mechanism 1: Ontology-Guided Triplet Refinement
- Claim: Wikidata schema constraints prune invalid triplets before they enter the KG, improving downstream reasoning accuracy.
- Mechanism: After LLM extracts candidate triplets, the pipeline (1) retrieves top-10 entity types via dense retrieval, (2) filters relations by domain-range constraints from Wikidata, (3) re-ranks by semantic similarity, and (4) LLM selects the ontology-valid configuration.
- Core assumption: Wikidata's type hierarchy sufficiently covers open-domain entities and relations for validation to be meaningful.
- Evidence anchors:
  - [abstract] "enforcing Wikidata-based type and relation constraints" produces "ontology-consistent" KGs
  - [Section 2.2, Stage 2] "This stage enforces structural validity, semantic alignment, and consistency with Wikidata's ontology"
  - [Section 3.2, Table 2] Full Wikontic achieves 96.5% ontology entailment; only 3.5% of triplets flagged as misaligned

### Mechanism 2: Alias-Aware Entity Normalization
- Claim: Deduplicating surface forms via alias matching produces denser, more connected KGs with higher answer coverage.
- Mechanism: For each refined triplet, retrieve top-10 existing entities with compatible types from KG using embedding similarity. LLM determines match vs. new entity. Surface forms stored as aliases; canonical names used for graph edges.
- Core assumption: Entities with same type and semantically similar names refer to the same real-world entity.
- Evidence anchors:
  - [abstract] "normalizing entities to reduce duplication"
  - [Section 2.2, Stage 3] "aligns entity names to a unified vocabulary... to reduce duplication and ensure consistency"
  - [Section 3.2, Table 2] Wikontic with normalization achieves highest avg. entity degree (4.3) and unique entities per relation (2.5)

### Mechanism 3: Iterative Multi-hop Retrieval over KG
- Claim: Decomposing multi-hop questions into sequential subquestions grounded in KG neighborhoods enables reasoning without raw text access.
- Mechanism: LLM (1) extracts entities from question, (2) links to KG nodes, (3) retrieves subgraph neighborhood, (4) answers subquestion, (5) conditions next subquestion on prior answer. Repeats up to 5 hops.
- Core assumption: The constructed KG contains paths connecting question entities to answer entities within 5-10 hops.
- Evidence anchors:
  - [abstract] "On MuSiQue, the correct answer entity appears in 96% of generated triplets"
  - [Section 2.3] "iterative retrieval that decomposes the question into subquestions, grounding each step in the retrieved KG context"
  - [Section 3.6, Table 6] Single-step QA variant drops to 31.3 EM vs. 42.6 EM (−11.3), confirming multi-hop decomposition importance

## Foundational Learning

- Concept: Wikidata ontology structure (properties, types, domain-range constraints)
  - Why needed here: The entire pipeline depends on understanding how Wikidata defines valid subject-object pairs for each relation.
  - Quick check question: Given Wikidata relation "director" (P57), what entity types are valid for subject and object?

- Concept: Dense retrieval with embedding similarity
  - Why needed here: Both ontology lookup (entity type matching) and entity normalization use Contriever embeddings + cosine similarity.
  - Quick check question: Why would semantic similarity alone be insufficient for entity disambiguation?

- Concept: Multi-hop question decomposition
  - Why needed here: The QA component requires breaking compositional questions into sequential 1-hop subquestions.
  - Quick check question: For "Who directed the film starring the actor who played Iron Man?", what are the decomposition steps?

## Architecture Onboarding

- Component map:
  1. **Ontology Database**: MongoDB storing 2,464 Wikidata properties with type constraints; dense index over type/relation names
  2. **KG Database**: MongoDB storing triplets, canonical entities, aliases; dense index over aliases
  3. **Stage 1 - Triplet Extraction**: LLM prompt extracts (subject, relation, object, qualifiers, types)
  4. **Stage 2 - Ontology Refinement**: Entity typing via retrieval + LLM selection; relation filtering by constraints
  5. **Stage 3 - Entity Normalization**: Alias matching + deduplication via retrieval + LLM matching
  6. **QA Retrieval**: Iterative decomposition, entity linking, subgraph retrieval

- Critical path: Text → Stage 1 (triplet extraction) → Stage 2 (type assignment + relation validation) → Stage 3 (entity merging) → KG storage. For QA: Question → Entity extraction → KG linking → Subgraph retrieval → Subquestion decomposition → Answer.

- Design tradeoffs:
  - Strict ontology enforcement (100% entailment) vs. coverage (97.5% without ontology vs. 93.8% with strict filtering per Table 3)
  - Token efficiency vs. quality: Full pipeline uses ~881 output tokens; removing stages increases coverage but degrades F1 (−19.0 EM without qualifiers, −15.9 EM without ontology+normalization per Table 6)
  - Retaining misaligned triplets (3.5%) vs. discarding: Paper retains for interpretability/debugging

- Failure signatures:
  - Low answer coverage (<80%): Likely missing entity normalization or sparse KG construction
  - High ontology misalignment (>10%): Type retrieval failing or Wikidata schema gaps
  - QA stall (no final answer): KG missing intermediate entities for multi-hop path
  - Entity fragmentation (low degree, high |E|): Normalization stage not matching aliases

- First 3 experiments:
  1. **Ontology ablation**: Run Wikontic on a small corpus with Stage 2 disabled. Measure ontology entailment rate and compare answer coverage vs. full pipeline.
  2. **Entity normalization stress test**: Feed documents with known synonym variations (e.g., "NYC", "New York City", "The Big Apple"). Verify alias consolidation and measure KG size reduction.
  3. **QA path tracing**: For 10 multi-hop questions, log the KG path traversed during iterative retrieval. Confirm answer entity is reachable and measure hop depth.

## Open Questions the Paper Calls Out
- Can smaller, task-specific models fine-tuned on Wikontic-generated data replace the LLM-based pipeline stages while maintaining comparable KG quality and QA performance?
- How does Wikontic perform when adapted to specialized domain ontologies beyond Wikidata, such as biomedical or legal knowledge bases?
- What is the actual end-to-end latency and throughput of Wikontic compared to baselines, beyond token-count efficiency metrics?
- How should the trade-off between ontology strictness and factual coverage be managed when ontology-misaligned triplets contain valid information absent from Wikidata's schema?

## Limitations
- Dependency on Wikidata's type taxonomy may not cover specialized domains or emerging entities
- LLM-based refinement stages introduce stochastic elements affecting reproducibility
- Method requires pre-constructed KGs, unlike retrieval-augmented approaches that can process arbitrary queries
- Only 3.5% of triplets flagged as misaligned, suggesting some valid facts may be schema-incompatible

## Confidence
- **High Confidence**: Token efficiency claims (881 output tokens vs. 3× AriGraph, <1/20 GraphRAG) are directly measurable and well-supported by Table 6. QA performance improvements (76.0 F1 HotpotQA, 59.8 F1 MuSiQue) are robust across ablation studies.
- **Medium Confidence**: Ontology alignment metrics (96.5% entailment, 3.5% misalignment) depend on Wikidata's type hierarchy quality and LLM reasoning ability, which may vary across domains and model versions.
- **Medium Confidence**: MINE-1 information retention benchmark (86%) uses a small sample (100 articles) and compares against methods with different objectives.

## Next Checks
1. **Ontology Coverage Stress Test**: Evaluate Wikontic on a domain-specific corpus (e.g., biomedical literature) to measure ontology alignment rates and answer coverage when Wikidata's type hierarchy is sparse or missing domain-specific relations.
2. **Ablation of LLM Refinement**: Run the pipeline with deterministic entity type selection (e.g., majority vote from top-10 retrievals) instead of LLM-based selection to quantify the contribution of LLM reasoning vs. retrieval quality to ontology alignment.
3. **Cross-Model Consistency**: Implement the pipeline using different LLM families (e.g., Claude, Gemini) to assess whether the 3.5% ontology misalignment rate is model-dependent or inherent to the constraint validation approach.