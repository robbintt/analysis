---
ver: rpa2
title: Spatio-Temporal Pruning for Compressed Spiking Large Language Models
arxiv_id: '2508.20122'
source_url: https://arxiv.org/abs/2508.20122
tags:
- pruning
- spiking
- temporal
- layer
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of Large Language
  Models (LLMs) by introducing Spiking Neural Networks (SNNs) and proposing a novel
  spatio-temporal pruning framework for compressed Spiking LLMs. The method revisits
  spatial and temporal pruning strategies, adapting them specifically for SNNs to
  optimize computational efficiency while preserving performance.
---

# Spatio-Temporal Pruning for Compressed Spiking Large Language Models

## Quick Facts
- arXiv ID: 2508.20122
- Source URL: https://arxiv.org/abs/2508.20122
- Reference count: 40
- Primary result: 58.51%-84.19% ACs of baseline, 28.86%-50.91% latency of baseline, maintained GLUE accuracy

## Executive Summary
This paper introduces a novel spatio-temporal pruning framework for compressed Spiking Large Language Models (LLMs) to address the computational inefficiency of standard transformer-based LLMs. The method combines spatial pruning (reducing active neurons and attention heads using Fisher Information Matrix × Average Spiking Rate) with temporal pruning (dynamically allocating timesteps per layer via PCA analysis of spiking activity) to optimize computational efficiency while preserving model performance. Evaluated on the GLUE benchmark using SpikingBERT, the framework achieves significant reductions in computational operations and inference latency while maintaining competitive accuracy.

## Method Summary
The framework operates in two stages: post-training analysis followed by retraining. During post-training, spatial pruning uses a composite importance score (FIM × ASR) to rank and prune neurons and attention heads while maintaining a 60% ACs constraint, then temporal pruning applies PCA to layer-wise spiking activity to determine optimal timesteps per layer with a scaling factor b=1.02. The retraining stage involves staged optimization with computational penalty and activity loss, with periodic PCA re-analysis. The method combines these pruning strategies with extreme quantization and knowledge distillation, where the pre-trained BERT serves as teacher for SpikingBERT student using implicit differentiation at equilibrium.

## Key Results
- Achieves 58.51%-84.19% of baseline accumulation operations (ACs) across GLUE tasks
- Reduces inference latency to 28.86%-50.91% of baseline timesteps per layer
- Maintains competitive accuracy on SST-2 (87.27% vs baseline) with spatio-temporal pruning
- Sequential spatial-then-temporal pruning outperforms joint pruning (87.27% vs 85.55% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining Fisher Information Matrix with Average Spiking Rate provides more accurate importance ranking for spatial pruning in SNNs than FIM alone.
- Mechanism: FIM approximates parameter sensitivity to loss (second-order derivative), while ASR captures spiking-specific activity relevance. Multiplying FIM × ASR creates a composite importance score that ranks units by both sensitivity and computational contribution.
- Core assumption: Neurons with low sensitivity (small FIM diagonal) AND low activity (small ASR) are redundant and removable without significant performance loss.
- Evidence anchors: [section] "The final importance of neuron masks and head masks can be summarized as follows: I(hi) = Iii · ASRi" (Page 4, Eq. 7-8); [section] "Post-Training (FIM + ASR) combines these three scores, which achieves the highest accuracy" (Page 7, Table II)

### Mechanism 2
- Claim: PCA on temporal ASR sequences reveals layer-wise temporal complexity, enabling differential timestep allocation.
- Mechanism: Record ASR per layer across timesteps during convergence, apply PCA to this temporal sequence, use the number of principal components needed to explain variance as a proxy for "temporal complexity." Map this to timesteps via power function: tl = ⌊b^(cl/max(cj)) × Tconv⌋ where b > 1.
- Core assumption: Layers with higher temporal variance require more timesteps to represent their dynamics; low-variance layers converge quickly and can operate with fewer timesteps.
- Evidence anchors: [section] "we record the ASR of each layer at every timestep, then apply PCA to this temporal sequence to determine the number of principal components required to explain the majority of the temporal variance" (Page 5); [section] Table II shows Post-Training (PCA) + Retraining reduces ACs to 47.14-68.66% while maintaining accuracy (Page 7)

### Mechanism 3
- Claim: Sequential spatial→temporal pruning with intermediate retraining outperforms joint simultaneous pruning.
- Mechanism: Apply spatial pruning first, retrain to recover accuracy, then apply temporal pruning with additional retraining. This staged approach allows the model to adapt to reduced capacity before facing reduced temporal resolution.
- Core assumption: Simultaneous spatial and temporal reduction creates competing optimization pressures that lead to suboptimal local minima; staged approach provides recovery buffers.
- Evidence anchors: [section] "performing a sequential strategy that apply spatial pruning followed by retraining, and then applying temporal pruning with an additional retraining stage, leads to better accuracy compared to jointly retraining" (Page 8, Table IV); [section] Sequential achieves 87.27% accuracy vs 85.55% for joint retraining on SST-2 (Table IV)

## Foundational Learning

- Concept: **Leaky Integrate-and-Fire (LIF) Neuron Dynamics**
  - Why needed here: Understanding Eq. 1 (ut = γut-1 + Σ(wij × sj) + b - si-1 × Vth) is essential for grasping why timestep reduction affects information flow and why ASR matters for pruning.
  - Quick check question: Given a LIF neuron with leak factor γ=0.9 and threshold Vth=1.0, if membrane potential is 0.95 at t-1 and no input spike arrives, what happens to the potential at time t?

- Concept: **Fisher Information Matrix as Sensitivity Approximator**
  - Why needed here: The paper uses diagonal FIM to approximate Hessian for importance scoring; understanding this approximation determines whether you can adapt it to other architectures.
  - Quick check question: Why does a small diagonal FIM value Iii suggest a parameter can be pruned with minimal loss increase?

- Concept: **Knowledge Distillation in Spiking Networks**
  - Why needed here: The framework uses pre-trained BERT as teacher for SpikingBERT student; implicit differentiation at equilibrium enables gradient flow without BPTT.
  - Quick check question: Why does computing gradients at steady-state ASR convergence (Eq. 4) reduce memory compared to BPTT?

## Architecture Onboarding

- Component map: Post-training stage → FIM computation → ASR collection → Attention output analysis → Mask generation (neurons + heads) → PCA temporal analysis → Timestep allocation → Retraining stage → Initialize with post-training masks → Add ACs penalty to loss → Periodic PCA re-analysis → Stochastic spike generation for sequential mapping

- Critical path: Post-training spatial pruning (60% ACs constraint) → Retrain → Post-training temporal pruning (PCA with b=1.02) → Retrain → Optional adaptive threshold scaling → Final model. Total: 2 retraining phases minimum.

- Design tradeoffs:
  - ACs constraint (0.4-1.0): Lower = more compression but accuracy drop. Paper selects 0.6 as balance point.
  - PCA scaling factor b (1.0-1.2): Higher = more aggressive temporal pruning. Paper uses 1.02.
  - Activity loss weight η: Controls sparsity vs accuracy tradeoff. Dataset-specific (0.0005-0.002).
  - Sequential vs joint pruning: Sequential is more accurate but requires 2 retraining cycles.

- Failure signatures:
  - Accuracy drops >2% after spatial pruning: ACs constraint too aggressive or FIM+ASR scores not generalizing. Reduce constraint or increase retraining epochs.
  - Temporal pruning causes NaN losses: Timestep allocation too low for critical layers. Increase PCA scaling factor b.
  - Retraining diverges: Learning rate too high for quantized model. Paper uses 5e-10 to 1e-7 (very low).
  - Stochastic spike generation introduces too much variance: Activity loss weight η may be too high.

- First 3 experiments:
  1. **Spatial-only ablation**: Implement post-training FIM+ASR pruning with 60% ACs constraint on SST-2. Compare FIM-only vs FIM+ASR vs FIM+ASR+Attention. Target: reproduce Table II accuracy progression (85.32% → 85.89%).
  2. **Temporal-only ablation**: Apply PCA-based timestep allocation (b=1.02, 0.99999 variance threshold) without spatial pruning. Measure ACs reduction and latency. Target: 47-68% ACs of baseline per Table II.
  3. **Two-stage integration**: Sequential spatial→retrain→temporal→retrain on single dataset. Compare against joint simultaneous pruning. Target: reproduce Table IV gap (87.27% sequential vs 85.55% joint).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed spatio-temporal pruning framework be effectively generalized to generative decoder-based spiking LLMs (e.g., SpikeGPT) or other non-encoder architectures?
- Basis in paper: [explicit] The conclusion explicitly states future work includes "extending the application of our pruning framework to other architectures."
- Why unresolved: The current experimental validation is restricted to the SpikingBERT model (encoder-only) on classification tasks, leaving the framework's efficacy on generative or decoder-based architectures unproven.
- What evidence would resolve it: Successful application of the pruning method to a decoder-based spiking model, demonstrating similar computational savings and latency reductions on generative benchmarks without performance collapse.

### Open Question 2
- Question: How does the "stochastic spiking LLM" mapping strategy perform when deployed on actual resource-constrained hardware (e.g., FPGAs or neuromorphic chips) compared to the theoretical simulation?
- Basis in paper: [inferred] The paper proposes a "resource-constrained sequential mapping" strategy using Bernoulli sampling to solve memory overhead (Section III-B), but experiments are conducted via simulation using "Normalized #C" rather than physical deployment.
- Why unresolved: The stochastic regeneration of spikes based on Average Spiking Rate (ASR) may introduce noise or statistical variances in physical hardware that are not fully captured by the software simulation.
- What evidence would resolve it: empirical data from a hardware implementation showing that the stochastic sampling method maintains accuracy and achieves the projected energy and latency reductions.

### Open Question 3
- Question: Is the PCA scaling factor (set to 1.02) robust across diverse model scales and datasets, or does it require dataset-specific tuning?
- Basis in paper: [inferred] In Section IV-D (Ablation Studies), the authors determine the optimal PCA scaling factor $b$ by testing values between 1.0 and 1.2, selecting 1.02 as the best trade-off.
- Why unresolved: The optimal value appears derived from empirical search on specific datasets; it is unclear if this heuristic holds for larger models or different languages without requiring an expensive hyperparameter sweep.
- What evidence would resolve it: An analysis showing the sensitivity of the scaling factor across different model sizes (e.g., BERT-Base vs. BERT-Large) or a proposed automated mechanism to set this value.

### Open Question 4
- Question: Can additional compression techniques specifically tailored for spiking dynamics, such as low-rank factorization, be integrated to further reduce the parameter count?
- Basis in paper: [explicit] The conclusion identifies "exploring additional compression methods specifically tailored for spiking LLMs" as a direction for future research.
- Why unresolved: The current work focuses on the interplay of spatial/temporal pruning, quantization, and distillation, but does not investigate other standard compression techniques like low-rank factorization mentioned in the introduction.
- What evidence would resolve it: A study integrating low-rank factorization into the spatio-temporal framework, measuring the additive effect on model size reduction versus accuracy retention.

## Limitations

- The work lacks direct corpus comparisons to baseline FIM-only or ASR-only methods in spiking LLM contexts, making claims about superiority less robust.
- Several implementation details are referenced to external papers without explicit specification in this paper, creating reproducibility gaps.
- The 2% accuracy drop threshold for acceptability may be dataset-specific and could vary across GLUE tasks or other domains.

## Confidence

- **High Confidence**: The staged sequential pruning approach (spatial→retrain→temporal→retrain) showing better accuracy than joint retraining is well-supported by Table IV results (87.27% vs 85.55%).
- **Medium Confidence**: The FIM×ASR importance scoring mechanism appears effective (Post-Training (FIM+ASR) achieves highest accuracy), but lacks direct comparison to FIM-only or ASR-only baselines within the spiking LLM context.
- **Medium Confidence**: PCA-based temporal pruning with b=1.02 scaling achieves significant ACs reduction (47.14-68.66%), but the choice of b and its sensitivity analysis is not explored.

## Next Checks

1. **Ablation Study**: Implement and compare FIM-only, ASR-only, and FIM+ASR spatial pruning on SST-2 to quantify the contribution of each component to the final accuracy.
2. **Dataset Generalization**: Test the full spatio-temporal pruning pipeline on tasks outside GLUE (e.g., SQuAD, text classification) to assess whether the 2% accuracy tolerance holds across domains.
3. **Compression-Accuracy Tradeoff Analysis**: Systematically vary the ACs constraint (0.4, 0.6, 0.8, 1.0) and record accuracy/latency curves to identify optimal operating points and verify the claimed 58.51%-84.19% ACs reduction is achievable without catastrophic accuracy loss.