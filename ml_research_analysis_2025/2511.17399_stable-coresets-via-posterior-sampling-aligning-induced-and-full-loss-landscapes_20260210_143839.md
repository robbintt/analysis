---
ver: rpa2
title: 'Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes'
arxiv_id: '2511.17399'
source_url: https://arxiv.org/abs/2511.17399
tags:
- loss
- training
- selection
- coreset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of loss landscape misalignment
  in gradient-based coreset selection methods, which can degrade performance especially
  under label noise and small data budgets. The authors propose a novel framework
  that uses posterior sampling of model weights to smooth the loss landscape and improve
  coreset stability and alignment.
---

# Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes

## Quick Facts
- arXiv ID: 2511.17399
- Source URL: https://arxiv.org/abs/2511.17399
- Reference count: 40
- Primary result: Novel coreset selection method using posterior sampling over model weights achieves 7% higher accuracy than next-best approach under 50% label corruption

## Executive Summary
This paper addresses loss landscape misalignment in gradient-based coreset selection methods, which degrades performance especially under label noise and small data budgets. The authors propose a framework that uses posterior sampling of model weights to smooth the loss landscape and improve coreset stability and alignment. Their method samples Gaussian perturbations over batch normalization layer parameters, computes expected gradient differences during coreset selection, and achieves theoretical guarantees on gradient and Hessian alignment. Extensive experiments demonstrate superior accuracy, faster convergence, and lower memory usage compared to state-of-the-art approaches across vision and NLP datasets.

## Method Summary
The method introduces a smoothed loss function based on posterior sampling over model weights, specifically targeting batch normalization layers for efficiency. It generates M weight perturbations δ ~ N(0, σI) around current parameters and computes expected gradient differences during coreset selection. This Monte Carlo smoothing marginalizes over a local neighborhood in weight space, dampening spurious oscillations from noise or small sample sizes while preserving underlying landscape structure. The algorithm uses greedy selection based on submodular optimization to build shadow datasets, which are then used for mini-batch SGD training. The approach maintains computational efficiency while providing theoretical guarantees on gradient and Hessian alignment between coreset-induced and full-data loss landscapes.

## Key Results
- Under 50% label corruption, method surpasses next best approach by 7% accuracy
- Achieves O(1/√(MRT)) convergence rate, improving on prior O(1/√(RT)) by factor √M
- Provides theoretical bounds showing Hessian matrices aligned with full-data Hessians within bounded error
- Consistently outperforms state-of-the-art approaches across various noise levels and architectures

## Why This Works (Mechanism)

### Mechanism 1: Posterior Sampling for Loss Landscape Alignment
Gaussian posterior sampling over model weights produces coreset-induced loss landscapes that better match the full-data loss landscape. The algorithm samples M weight perturbations δ ~ N(0, σI) around current parameters and computes expected gradient differences during coreset selection. This Monte Carlo smoothing marginalizes over a local neighborhood in weight space, dampening spurious oscillations from noise or small sample sizes while preserving underlying landscape structure.

### Mechanism 2: Hessian Alignment via Stability Constraints
Coresets satisfying (σ, ε, w̄)-stability produce Hessian matrices aligned with full-data Hessians within bounded error. Stability constraint bounds expected gradient deviation under weight perturbation. Theorem proves this implies Hessian spectral norm differences ≤ O(ε^(1/2)) and trace differences ≤ O(ε/σ), plus bounded Newton step differences—meaning coreset optimization follows similar trajectories to full-data optimization.

### Mechanism 3: Convergence Improvement with Multiplicative Noise
Under multiplicative coreset noise, the method achieves O(1/√(MRT)) convergence rate, improving on prior O(1/√(RT)) by factor √M where M is the number of posterior samples. Convergence analysis accounts for dual randomness—batch sampling noise and coreset selection noise. For multiplicative noise E[‖ξ₂‖] ≤ ε‖∇l(w)‖, setting σ²₂d = 1/(M√T) and η = min{1/√T, 1/β} yields the improved rate.

## Foundational Learning

- **Submodular Optimization for Coreset Selection**: Why needed here: The greedy selection algorithm transforms gradient-matching into a submodular cover problem, enabling efficient O(nk) selection with approximation guarantees. Quick check: Can you explain why greedy selection provides provable approximation guarantees for submodular objectives?
- **Gaussian Smoothing / Randomized Smoothing**: Why needed here: Understanding how adding Gaussian noise to weights creates a smoothed objective function—directly connects to Sharpness-Aware Minimization literature. Quick check: How does smoothing a loss function affect its curvature and generalization properties?
- **Newton Step and Hessian-based Optimization**: Why needed here: Theorem 3.2 connects gradient alignment to Newton step alignment; understanding why Hessian information matters for optimization trajectories. Quick check: Why does Newton step alignment imply similar optimization behavior between coreset and full-data training?

## Architecture Onboarding

- **Component map**: Posterior Sampler -> Gradient Matching Module -> Greedy Selector -> Mini-batch SGD
- **Critical path**: Initialize model w₀ → Each epoch: subsample candidates → sample weight perturbations → compute smoothed gradient distances → greedy select samples → union into shadow dataset → run B batches of SGD → repeat until convergence
- **Design tradeoffs**: σ (posterior variance) vs. convergence speed; M (number of samples) vs. computational overhead; sampling location (BN layers vs. all parameters); P (selection repetitions) vs. shadow dataset diversity
- **Failure signatures**: Divergence under high corruption when σ too small; memory blowout with Hessian-based posteriors; slow convergence with large σ
- **First 3 experiments**: 1) Ablation on sampling location comparing BN-only vs. all-parameters vs. FC-layer-only on CIFAR-10; 2) Corruption robustness sweep comparing against Random, Crest, Craig, Glister on CIFAR-10/100; 3) Convergence rate validation tracking gradient norm over training with different M values

## Open Questions the Paper Calls Out

- **Can a rigorous theoretical explanation be established for why restricting weight perturbations to batch normalization layers provides the optimal trade-off between perturbation and stability?** The paper notes this is empirically motivated but lacks mathematical derivation of why BN layers specifically outperform other sampling locations.

- **Does the proposed stability framework generalize to complex data modalities such as audio or video where temporal dependencies might alter the loss landscape geometry?** The method was validated primarily on static images and text, leaving unknown how temporal coherence in video/audio interacts with Gaussian smoothing of weights.

- **Can more sophisticated posterior distributions be designed to fine-tune loss landscape alignment without incurring the computational costs associated with Hessian-based methods?** The paper compares basic distributions but leaves unexplored the space of intermediate or adaptive posteriors that might achieve better alignment with reasonable efficiency.

## Limitations
- The exact implementation details of the gradient target in greedy selection remain ambiguous—whether full dataset gradients or subsampled approximations are used
- Stability of convergence guarantees under practical conditions where smoothness assumptions may not hold in deep networks
- Memory usage comparisons may not account for different implementation details across methods

## Confidence
- **High confidence** in theoretical framework and alignment guarantees under stated assumptions
- **Medium confidence** in empirical performance claims, as some hyperparameter choices show sensitivity and may not generalize across all architectures
- **Medium confidence** in convergence rate improvement claim, as it depends critically on coreset selection noise being multiplicative rather than additive

## Next Checks
1. **Gradient Target Implementation Verification**: Implement both full-dataset gradient matching and subsampled gradient approximation variants; measure selection time and accuracy differences across corruption levels to identify the practical regime used in experiments.
2. **Assumption Stress Testing**: Systematically vary Hessian smoothness constants c₁, c₂ and stability parameter ε in synthetic experiments; measure degradation in alignment guarantees and identify breaking points where theory no longer matches practice.
3. **Memory Usage Profiling**: Implement memory tracking for all compared methods (Craig, Crest, Glister, proposed) under identical conditions; verify claimed 40% memory reduction and identify which components contribute most to savings.