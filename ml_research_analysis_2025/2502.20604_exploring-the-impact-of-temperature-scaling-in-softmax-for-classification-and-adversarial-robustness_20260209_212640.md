---
ver: rpa2
title: Exploring the Impact of Temperature Scaling in Softmax for Classification and
  Adversarial Robustness
arxiv_id: '2502.20604'
source_url: https://arxiv.org/abs/2502.20604
tags:
- temperature
- learning
- adversarial
- class
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of temperature scaling in the
  softmax function for image classification and adversarial robustness. Through extensive
  experiments on multiple benchmark datasets using CNNs and transformers, we find
  that moderate temperatures generally improve overall model performance.
---

# Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness

## Quick Facts
- arXiv ID: 2502.20604
- Source URL: https://arxiv.org/abs/2502.20604
- Authors: Hao Xuan; Bokai Yang; Xingyu Li
- Reference count: 28
- Key outcome: Moderate temperatures improve image classification and adversarial robustness by smoothing gradients and promoting balanced learning across classes.

## Executive Summary
This study investigates temperature scaling in softmax for image classification and adversarial robustness. Through extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using CNNs and transformers, the authors find that moderate temperatures generally improve overall model performance. The theoretical analysis reveals that temperature scaling influences both learning step size and optimization direction, with higher temperatures promoting more balanced learning across all classes. Surprisingly, elevated temperatures enhance model robustness against common corruptions, natural perturbations, and non-targeted adversarial attacks like PGD.

## Method Summary
The study modifies the standard softmax cross-entropy loss by introducing a temperature parameter τ that scales the logits before applying softmax. During training, temperature values τ ∈ {0.1, 0.5, 1, 10, 30, 50, 70, 100} are tested. ResNet50 models are trained from scratch with SGD (lr=0.1, Cosine Annealing scheduler), while ViT-small-patch16-224 models are pretrained on ImageNet-21K and fine-tuned with Adam. Temperature is applied only during training; evaluation and adversarial attacks use τ=1 (standard softmax). Robustness is evaluated against CIFAR-C, Tiny-ImageNet-C corruption benchmarks (severity 3) and adversarial attacks (PGD20, C&W with L∞ ε=8/255).

## Key Results
- Moderate temperatures (τ ∈ [30, 70]) generally improve clean accuracy and corruption robustness across CNNs and transformers
- High training temperatures (τ=50-100) provide significant robustness against untargeted PGD attacks while maintaining good clean accuracy
- Temperature scaling creates more uniform distances between features and negative class prototypes, reducing vulnerability to adversarial attacks
- CNNs benefit more from high temperatures than transformers, likely due to pre-training stability differences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing temperature τ reduces the effective learning step size and stabilizes training.
- **Mechanism:** In the gradient calculation $\frac{\partial L_{ce}}{\partial w_j} = \frac{1}{\tau} P_j(x) f(x)$, the gradient magnitude is inversely proportional to τ. Higher temperatures dampen gradient updates, preventing overshooting and ensuring smoother convergence.
- **Core assumption:** The optimizer benefits from dampened updates in high-error regions.
- **Evidence anchors:** Equations 4, 5, and 6 show the 1/τ scaling factor; abstract mentions temperature influences learning step size.

### Mechanism 2
- **Claim:** Elevated temperatures de-bias optimization direction by forcing balanced learning across all classes rather than focusing solely on "hard" pairs.
- **Mechanism:** Low τ creates a sharp distribution where the gradient is dominated by the class with the highest probability. High τ flattens the distribution, making the gradient update a weighted sum of all class prototypes, forcing global inter-class relationships.
- **Core assumption:** Excessive focus on error-prone classes early in training harms generalizable feature formation.
- **Evidence anchors:** Analysis of Eqn. 9 and Fig. 1 visualize how τ biases direction; abstract states higher temperatures promote balanced learning.

### Mechanism 3
- **Claim:** High training temperatures confer robustness against untargeted adversarial attacks by equalizing distances to negative class prototypes.
- **Mechanism:** Attacks like PGD rely on pushing features toward the "easiest" negative class. High τ training forces the feature f(x) to be roughly equidistant from all negative prototypes, removing the "weak direction" for attacks to exploit.
- **Core assumption:** Attack efficacy depends on a distinct gradient direction toward a single error-prone class.
- **Evidence anchors:** Fig. 6 and 7 show reduced variance in Euclidean distance to negative prototypes for high τ; Eqn. 10 discusses gradient analysis for adversarial generation.

## Foundational Learning

- **Concept:** Softmax Derivatives & Cross-Entropy
  - **Why needed here:** The theoretical argument rests on how the 1/τ term modifies the derivative of the loss with respect to weights and features.
  - **Quick check question:** In the gradient $\frac{\partial L}{\partial w}$, does τ appear in the numerator or denominator, and what does that imply for step size?

- **Concept:** Latent Space Geometry
  - **Why needed here:** Understanding robustness requires visualizing how class prototypes (w) and sample features (f(x)) relate geometrically.
  - **Quick check question:** If a sample is equidistant from all negative class prototypes, which direction does the gradient point?

- **Concept:** Adversarial Attack Mechanics (White-box)
  - **Why needed here:** To distinguish why a defense works on PGD (gradient-based) but might differ against other attacks.
  - **Quick check question:** How does an untargeted attack decide which direction to perturb the input?

## Architecture Onboarding

- **Component map:** Backbone (f) → Linear Layer (W) → Softmax (τ=T) → Loss
- **Critical path:** The temperature parameter τ is a hyperparameter injected into the loss calculation during training only. Do not apply τ during validation/testing or inside adversarial attack generation (which assumes τ=1).
- **Design tradeoffs:**
  - **Robustness vs. Accuracy Tradeoff:** High τ (50-100) significantly boosts robustness and often clean accuracy on CNNs, but may slightly lower clean accuracy in Adversarial Training scenarios compared to standard training.
  - **Architecture Sensitivity:** Transformers (ViT) showed less performance gain than CNNs (ResNet), likely due to pre-training stability.
- **Failure signatures:**
  - **Non-convergence:** Setting τ < 0.5 with a standard/high learning rate causes gradient explosion.
  - **False Security:** Assuming robustness against targeted attacks; the paper explicitly notes this mechanism fails there.
- **First 3 experiments:**
  1. **Sanity Check (CNN):** Train ResNet18 on CIFAR-10 with τ ∈ {1.0, 10.0, 50.0}. Plot clean accuracy and training loss curves to verify "smoother convergence."
  2. **Robustness Probe:** Take checkpoints from Exp 1. Run standard PGD-20 attack. Verify τ=50 model maintains higher accuracy than τ=1.
  3. **Geometry Validation:** For a fixed batch of data, calculate the variance of distances from f(x) to all negative class prototypes w. Compare τ=1 vs. τ=50 to confirm "uniformity" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can an optimal temperature value be systematically determined for a given model and dataset, rather than relying on manual tuning?
- **Basis in paper:** The authors explicitly state as a limitation: "One limitation of this study was that we didn't report an explicit algorithm to set the best temperature values."
- **Why unresolved:** The paper demonstrates that temperatures between 30-70 work well across tested scenarios, but provides no principled method to select the exact value for new architectures or datasets.
- **What evidence would resolve it:** A theoretically-grounded or empirically-validated algorithm that, given model architecture, dataset characteristics, or training dynamics, outputs a near-optimal temperature.

### Open Question 2
- **Question:** Can temperature scaling be effectively integrated with advanced adversarial training methods beyond the vanilla AT baseline tested?
- **Basis in paper:** The authors explicitly state: "While further extension to other adversarial training methods is possible, it remains a complex problem... further exploration of fitting this into other adversarial training methods falls beyond the scope of this paper."
- **Why unresolved:** Modern adversarial training methods (e.g., TRADES, MART) use complex multi-term loss functions where integrating temperature scaling is non-trivial due to potential conflicts between loss components.
- **What evidence would resolve it:** Experiments showing temperature scaling's effect (and optimal placement) within multi-term adversarial training losses, with analysis of inter-term interactions.

### Open Question 3
- **Question:** What mechanisms can extend the robustness benefits of high-temperature training to defend against targeted adversarial attacks?
- **Basis in paper:** The authors explicitly note: "Despite the model trained with high temperatures showing superb robustness against untargeted PGD attack... it does not hold robustness against targeted attacks."
- **Why unresolved:** High-temperature training creates uniform distances to negative class prototypes, defeating untargeted attacks. Targeted attacks bypass this by specifying the target class directly.
- **What evidence would resolve it:** Novel training techniques combining temperature scaling with mechanisms that obscure class-specific directions in the feature space, demonstrated through improved targeted attack robustness.

## Limitations
- The geometric robustness mechanism (Mechanism 3) lacks strong direct evidence in the corpus; validation is primarily empirical rather than theoretical.
- Training hyperparameters for ResNet50 (epochs, batch size, weight decay, momentum) and ViT (fine-tuning epochs, Adam lr, data augmentation) are unspecified, potentially affecting reproducibility.
- The exact PGD step size (α) is not provided, which could influence attack efficacy results.

## Confidence
- **High Confidence:** The step-size dampening effect (Mechanism 1) is mathematically clear from the gradient equations and empirically verified.
- **Medium Confidence:** The balanced-learning claim (Mechanism 2) is supported by gradient analysis but relies on implicit assumptions about training dynamics.
- **Medium Confidence:** The robustness mechanism (Mechanism 3) shows empirical validation but lacks theoretical grounding and direct corpus support.

## Next Checks
1. **Geometry Validation:** Calculate and compare the variance of distances from features to all negative class prototypes between τ=1 and τ=50 models to empirically verify the "equidistant prototype" hypothesis central to Mechanism 3.
2. **Learning Rate Sensitivity:** Systematically vary the learning rate for low-temperature training (τ=0.1, 0.5) to identify the precise threshold where non-convergence occurs, validating the step-size claim.
3. **Architecture Ablation:** Test the temperature scaling effect on a simpler architecture (e.g., MLP on synthetic data) to isolate the mechanism from complex interactions in CNNs/transformers, strengthening confidence in the theoretical claims.