---
ver: rpa2
title: Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management
  of Wild Salmon Fisheries in Indigenous Rivers
arxiv_id: '2505.06637'
source_url: https://arxiv.org/abs/2505.06637
tags:
- salmon
- fisheries
- data
- management
- monitoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the challenge of monitoring wild salmon
  populations in remote Indigenous rivers, where climate change, habitat loss, and
  infrastructure limitations hinder effective fisheries management. The project integrates
  multimodal foundation AI and expert-in-the-loop frameworks to automate salmon species
  identification, counting, and length measurement from video and sonar data.
---

# Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers

## Quick Facts
- arXiv ID: 2505.06637
- Source URL: https://arxiv.org/abs/2505.06637
- Reference count: 14
- One-line result: Multimodal AI system improves salmon species classification and counting accuracy while reducing manual effort in remote Indigenous river monitoring

## Executive Summary
This research addresses the critical challenge of monitoring wild salmon populations in remote Indigenous rivers where traditional methods face limitations due to climate change, habitat loss, and infrastructure constraints. The project develops an integrated system combining multimodal foundation AI with expert-in-the-loop frameworks to automate salmon species identification, counting, and length measurement from video and sonar data. By leveraging vision language models enhanced with expert validation and cross-modal data fusion, the system achieves improved accuracy in challenging environments with occlusion and noise while reducing manual labor requirements.

The approach demonstrates how AI can support sustainable fisheries management by enabling real-time, adaptive in-season decision-making that respects Indigenous data sovereignty and cultural values. The framework's open-source nature and focus on ethical co-development with Indigenous communities positions it as a scalable model for conservation efforts across different river systems and species. Results show measurable improvements in classification metrics (mAP@50, F1 score) and counting precision (MAPE, MAE), establishing a foundation for broader application in fisheries science and AI-driven environmental monitoring.

## Method Summary
The system integrates multimodal foundation AI with expert-in-the-loop frameworks to automate salmon monitoring in Indigenous rivers. It combines video and sonar data processing through cross-modal fusion, using vision language models for initial species classification that are then refined through expert feedback. The approach employs transformer-based models adapted for temporal tracking and spatial detection, with early fusion of sonar frames and echograms to improve noise suppression. The framework emphasizes co-development with Indigenous communities to ensure cultural appropriateness and data sovereignty while maintaining technical accuracy through iterative validation loops.

## Key Results
- Enhanced species classification accuracy measured by mAP@50 and F1 score compared to baseline models
- Improved counting precision with reduced MAPE and MAE metrics in challenging river environments
- Significant reduction in manual annotation effort while maintaining or improving detection accuracy

## Why This Works (Mechanism)
The system leverages the complementary strengths of multiple data modalities to overcome individual limitations. Vision data provides detailed species characteristics while sonar captures movement patterns and density information that may be obscured visually. Expert-in-the-loop refinement addresses the domain-specific knowledge gaps in general-purpose vision language models, particularly for subtle biological features like spotting patterns and gum coloration. The cross-modal fusion approach enables noise suppression by correlating detections across modalities, while the transformer architecture handles the temporal dynamics of fish movement effectively.

## Foundational Learning
- Vision Language Models: General-purpose VLMs lack domain-specific knowledge for accurate salmon species identification, requiring expert refinement to distinguish subtle biological features
- Cross-modal Data Fusion: Combining video and sonar data compensates for individual modality limitations (visual occlusion vs. background noise) to improve overall detection robustness
- Expert-in-the-Loop Systems: Iterative human feedback loops are essential for correcting AI hallucinations and fine-tuning models for specialized domains like fisheries science
- Transformer-based Tracking: Temporal modeling through transformers enables better handling of fish movement patterns and improves tracking continuity across frames

## Architecture Onboarding

Component Map:
Raw Sensor Data -> Multimodal Preprocessor -> Cross-modal Fusion -> Species Classification -> Expert Validation -> Final Output

Critical Path:
Data acquisition → preprocessing → cross-modal fusion → species classification → expert refinement → counting/length measurement

Design Tradeoffs:
- Modality fusion complexity vs. single-modality simplicity
- Expert feedback latency vs. model autonomy
- Model size and computational requirements vs. real-time deployment feasibility

Failure Signatures:
- High false positive rates when environmental conditions differ significantly from training data
- Classification errors for species with similar visual characteristics
- Synchronization failures between video and sonar data streams

First Experiments:
1. Ablation study comparing single-modality vs. cross-modal performance across different river conditions
2. Expert refinement impact assessment measuring classification accuracy improvements after feedback loops
3. Real-time deployment test measuring system latency and accuracy in field conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal foundation models maintain high accuracy across distinct river environments without extensive site-specific retraining?
- Basis in paper: [explicit] Section 7 states that "the performance of such AI models for automated detections, tracking, counting, and length measurement across different sites remains uncertain."
- Why unresolved: Domain generalization is difficult due to environmental variability (turbidity, lighting) and differences in river geometry, requiring multiple iterations of testing.
- What evidence would resolve it: Standardized evaluation metrics (mAP, MAE) collected from new deployments in diverse Indigenous river systems beyond the preliminary Yakoun River site.

### Open Question 2
- Question: To what extent does the synchronization and early fusion of sonar frames with echograms improve tracking robustness compared to single-modality sonar processing?
- Basis in paper: [inferred] Section 4.3 proposes fusing these modalities to suppress noise, but frames the implementation as a proposal ("We propose adapting SAM2") rather than a completed validation.
- Why unresolved: Sonar data contains substantial background noise, and it is not yet confirmed if the proposed transformer-based fusion of spatial (frames) and temporal (echograms) inputs successfully mitigates false positives.
- What evidence would resolve it: Ablation studies comparing the HOTA (Higher Order Tracking Accuracy) and IDF1 scores of the multimodal model against single-modality baselines.

### Open Question 3
- Question: How effectively can expert-in-the-loop feedback correct the specific visual hallucinations (e.g., misidentifying spotting or gum color) made by Vision Language Models?
- Basis in paper: [explicit] Figure 3 highlights that off-the-shelf models like OpenAI o1 confuse species (e.g., Sockeye vs. Chinook) by misinterpreting features, necessitating a "refinement" workflow.
- Why unresolved: General-purpose VLMs lack domain-specific knowledge, and the efficiency of using expert comments to fine-tune these models for subtle biological distinctions is still under exploration.
- What evidence would resolve it: Measurements of the reduction in manual annotation burden and the increase in species classification F1 scores following iterative expert refinement loops.

## Limitations
- Validation primarily conducted on Pacific Northwest Indigenous river datasets, limiting generalizability to other regions
- Detailed participatory processes with Indigenous communities not fully documented
- Practical deployment challenges in remote areas with limited connectivity and power infrastructure not thoroughly addressed

## Confidence
- Technical Performance Claims: Medium confidence - promising metrics but limited geographic validation
- Ethical AI and Indigenous Partnership Claims: Low-Medium confidence - framework emphasizes co-development but lacks detailed documentation
- Scalability and Real-world Deployment: Medium confidence - system claims real-time capability but practical deployment challenges remain unclear

## Next Checks
1. Conduct field validation across multiple Indigenous river systems with varying environmental conditions, salmon species compositions, and infrastructure limitations to assess real-world performance and robustness of the multimodal AI system.

2. Implement and document a formal participatory evaluation process with multiple Indigenous communities, including structured feedback mechanisms, to validate the ethical AI co-development claims and assess community acceptance of the technology.

3. Perform a comprehensive cost-benefit analysis comparing the AI-assisted monitoring system against traditional manual methods, including total cost of ownership, required technical expertise, and maintenance needs for remote deployment scenarios.