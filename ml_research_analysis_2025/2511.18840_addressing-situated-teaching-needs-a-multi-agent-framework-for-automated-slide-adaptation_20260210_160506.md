---
ver: rpa2
title: 'Addressing Situated Teaching Needs: A Multi-Agent Framework for Automated
  Slide Adaptation'
arxiv_id: '2511.18840'
source_url: https://arxiv.org/abs/2511.18840
tags:
- slide
- adaptation
- teaching
- slides
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of adapting teaching slides\
  \ to instructors\u2019 pedagogical styles and students\u2019 contexts. The authors\
  \ propose a multi-agent framework that translates high-level instructor specifications\
  \ into concrete slide modifications through a hierarchical architecture that separates\
  \ strategic planning from execution."
---

# Addressing Situated Teaching Needs: A Multi-Agent Framework for Automated Slide Adaptation

## Quick Facts
- arXiv ID: 2511.18840
- Source URL: https://arxiv.org/abs/2511.18840
- Reference count: 32
- Primary result: Multi-agent framework achieves F1=0.89 for operational agreement with human experts while significantly outperforming baselines in intent alignment and factual accuracy

## Executive Summary
This paper addresses the challenge of adapting teaching slides to instructors' pedagogical styles and students' contexts through a hierarchical multi-agent framework. The system translates high-level instructor specifications into concrete slide modifications while preserving pedagogical control. The framework achieves high intent alignment (F1=0.89) and content coherence, performing on par with baselines in visual clarity but significantly outperforming them in fulfilling instructor intent and factual accuracy. While enabling efficient slide adaptation, the system still requires human validation for factual correctness.

## Method Summary
The approach uses a hierarchical multi-agent architecture separating strategic planning from execution. The system takes as input situated teaching needs, original slides, and instructor materials, then processes them through two phases: Strategic Planning (Lesson Planner categorizes requests and generates instructional design; Adaptation Organizer generates atomic operation sequences) and Execution/QA (Executor performs modifications via MCP tools; Validator checks outputs iteratively). The framework uses 4 agents (LP, AO, EX, VA) with Tavily Search API for resource augmentation and python-pptx-based MCP tools for slide manipulation. The evaluation used 8 slide decks (10-25 slides each) from OER platforms across CS, business, and humanities, with 16 modification requests.

## Key Results
- Achieves F1=0.89 for operational agreement with human experts
- Outperforms baselines in intent alignment and factual accuracy while matching visual clarity
- Demonstrates high precision (0.90) and recall (0.88) through validation loop
- Requires 159s per adaptation vs. 17s for baseline, trading speed for quality

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Plan-Act Separation
- Decoupling strategic planning from execution improves intent alignment by creating an interpretable intermediate operation sequence
- Core assumption: Natural language requests can be decomposed into discrete operations without losing pedagogical intent
- Break condition: Ambiguous requests requiring iterative clarification may fail without multi-turn dialogue support

### Mechanism 2: In-Situ Modification Preserves Pedagogical Context
- Directly modifying existing slides maintains stylistic consistency and embedded pedagogical design choices
- Core assumption: Original slide deck contains pedagogically meaningful design patterns worth preserving
- Break condition: Poor original design propagates through in-situ modification rather than improving

### Mechanism 3: Validator-Driven Feedback Loop Reduces Execution Errors
- Post-execution validation with iterative correction improves factual grounding and reduces operation discrepancies
- Core assumption: Validation criteria can adequately catch pedagogically relevant errors without human input
- Break condition: Incomplete validation criteria allow errors to pass undetected

## Foundational Learning

- **Plan-Act Agent Architecture**
  - Why needed: Understanding that separating planning from execution creates an interpretable intermediate artifact that improves reliability
  - Quick check: Can you explain why generating an explicit operation sequence before execution would reduce errors compared to direct action?

- **Model Context Protocol (MCP) Tools**
  - Why needed: The system uses MCP tools as the interface between LLM agents and the PowerPoint file format
  - Quick check: What are the atomic operations needed to modify a slide, and why must they be granular rather than monolithic?

- **Factual Accuracy vs. Intent Alignment Trade-off**
  - Why needed: The system optimizes for intent alignment but explicitly does not guarantee factual correctness—this is a critical limitation
  - Quick check: Why would RAG-augmented search still produce hallucinations in educational content?

## Architecture Onboarding

- Component map: Input STN + OS + IM → Strategic Planning (LP → AO) → Execution/QA (EX → VA) → Output S'
- Critical path: The AO's operation sequence generation is the bottleneck—incorrect operations cannot be recovered downstream
- Design tradeoffs: In-situ modification preserves style but limits visual improvements; multi-agent separation adds latency but improves quality; RAG helps but doesn't eliminate hallucination risk
- Failure signatures: Low Intent Alignment → LP misclassification or AO decomposition failure; Low Factual Accuracy → Validator lacks domain-specific fact-checking; Operations timeout → MCP tool failures on complex layouts
- First 3 experiments:
  1. Run on 10-slide deck with "Refine" request; inspect AO's JSON operation sequence before execution
  2. Compare output with/without Validator enabled; measure precision/recall difference
  3. Test "Recompose" request requiring external knowledge; manually verify factual claims

## Open Questions the Paper Calls Out

- How can the system be evolved from single-turn execution to support multi-turn, iterative refinement in a collaborative design paradigm?
- Can the framework's context management scale to maintain robust performance across semester-long lecture series rather than single presentations?
- How can the framework's factual accuracy be improved to bridge the "Critical Gap" caused by LLM hallucinations?
- How can multi-modal foundation models be integrated to enable understanding and manipulation of complex visual components like charts and videos?

## Limitations

- System achieves high intent alignment but requires human validation for factual accuracy (2.38/5 score)
- In-situ modification trades visual clarity for pedagogical intent fulfillment
- Multi-agent separation adds significant latency (159s vs. 17s for baseline)

## Confidence

- **High Confidence**: Plan-act separation demonstrably improves intent alignment over direct execution baselines with well-defined operational precision/recall metrics
- **Medium Confidence**: In-situ modification preserves pedagogical context, though evidence is indirect (visual clarity parity)
- **Low Confidence**: Factual accuracy limitations are acknowledged but lack quantitative characterization of hallucination rates

## Next Checks

1. Manually review 50 factual claims in Recompose outputs to quantify hallucination rate and characterize error types
2. Test system on slide decks from domains not represented in corpus (medical, legal education) to assess generalization
3. Evaluate system performance on sequential modification requests to measure context maintenance across multiple adaptation cycles