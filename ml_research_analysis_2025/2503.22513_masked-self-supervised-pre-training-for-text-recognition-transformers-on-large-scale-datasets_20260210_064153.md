---
ver: rpa2
title: Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale
  Datasets
arxiv_id: '2503.22513'
source_url: https://arxiv.org/abs/2503.22513
tags:
- text
- pre-training
- dataset
- self-supervised
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates masked self-supervised pre-training for
  text recognition transformers using large-scale unlabeled data. The authors propose
  two key modifications: progressively increasing masking probability during training
  and incorporating non-masked patches into the loss function.'
---

# Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets

## Quick Facts
- arXiv ID: 2503.22513
- Source URL: https://arxiv.org/abs/2503.22513
- Authors: Martin Kišš; Michal Hradiš
- Reference count: 40
- Primary result: Progressive masking with non-masked patches achieves up to 30% relative CER improvement across 9k to 1M annotated lines

## Executive Summary
This paper investigates masked self-supervised pre-training for text recognition transformers using large-scale unlabeled data. The authors propose two key modifications: progressively increasing masking probability during training and incorporating non-masked patches into the loss function. Extensive experiments on 50M unlabeled text lines and four differently sized annotated datasets (9k to 1M lines) demonstrate that pre-training consistently improves character error rate, with up to 30% relative improvement. The approach is competitive with transfer learning while not requiring extra annotated text lines. Results show that progressive masking performs best, and training smaller models on limited datasets yields better performance than larger models.

## Method Summary
The authors investigate masked self-supervised pre-training for text recognition transformers on large-scale unlabeled datasets. Their approach involves training the model to predict masked patches in text images using a transformer architecture. The key innovation is two-fold: progressively increasing the masking probability during training to improve representation learning, and including non-masked patches in the loss function to provide additional context. The method is evaluated across four annotated datasets ranging from 9k to 1M text lines, using a 50M unlabeled text line corpus for pre-training.

## Key Results
- Progressive masking with non-masked patches achieves up to 30% relative character error rate improvement
- Pre-training is competitive with transfer learning while avoiding need for annotated text lines
- Smaller models trained on limited datasets outperform larger models in same conditions

## Why This Works (Mechanism)
The progressive masking strategy allows the model to first learn basic patch representations with lower masking rates, then gradually focus on harder prediction tasks as masking increases. Including non-masked patches in the loss provides contextual information that helps the model better understand relationships between visible and masked regions. This dual approach enables more robust representation learning compared to static masking strategies or loss functions that only consider masked regions.

## Foundational Learning

**Self-supervised learning**: Training models using only unlabeled data by creating pretext tasks - needed to leverage massive unlabeled text datasets without manual annotation - quick check: verify input data contains no labels

**Masked prediction**: Predicting missing or masked portions of input data - needed to force model to learn contextual relationships and semantic understanding - quick check: confirm masking ratio increases during training

**Transformer architectures**: Self-attention based models that process sequences in parallel - needed for efficient handling of text line patches and capturing long-range dependencies - quick check: verify multi-head attention implementation

## Architecture Onboarding

**Component map**: Text line images -> Patch embedding -> Transformer encoder -> Masked patch prediction head -> Loss (masked + non-masked patches)

**Critical path**: Input text line → Patch partitioning → Linear embedding → Positional encoding → Transformer layers → Prediction head → Loss computation

**Design tradeoffs**: Progressive masking vs. static masking (adapts difficulty over training), masked-only vs. full-patch loss (contextual vs. focused learning)

**Failure signatures**: Overfitting on small datasets, plateauing performance with excessive masking, unstable training with improper masking schedules

**First experiments**: 1) Validate baseline model performance without pre-training, 2) Test static masking baseline, 3) Implement progressive masking alone before adding non-masked patches to loss

## Open Questions the Paper Calls Out
None

## Limitations
- Findings primarily validated on specific Transformer architectures without exploring alternative model designs
- Limited evaluation to certain types of text images, not testing on handwritten or low-resolution documents
- Lack of explicit ablation studies to quantify individual contribution of progressive masking vs. non-masked patch inclusion

## Confidence
- High confidence in general effectiveness of masked self-supervised pre-training
- Medium confidence in specific contributions of proposed modifications
- Low confidence in broader generalization claims across all text recognition scenarios

## Next Checks
1. Conduct ablation studies to quantify individual impact of progressive masking and non-masked patch inclusion
2. Test the approach on diverse text recognition datasets including handwritten and low-quality text images
3. Compare with explicit transfer learning baselines using the same model architectures and datasets