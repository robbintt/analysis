---
ver: rpa2
title: Causal Ordering for Structure Learning from Time Series
arxiv_id: '2510.24639'
source_url: https://arxiv.org/abs/2510.24639
tags:
- causal
- orderings
- ordering
- temporal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of causal structure discovery
  in time series data, where traditional methods struggle with the combinatorial complexity
  of identifying true causal relationships, especially as the number of variables
  and time points increase. The authors propose DOTS (Diffusion Ordered Temporal Structure),
  a novel approach that leverages multiple valid causal orderings generated via diffusion
  models, instead of relying on a single ordering as is common practice.
---

# Causal Ordering for Structure Learning from Time Series

## Quick Facts
- **arXiv ID**: 2510.24639
- **Source URL**: https://arxiv.org/abs/2510.24639
- **Reference count**: 40
- **Primary result**: DOTS improves mean window-graph F1 from 0.63 (best baseline) to 0.81 on synthetic benchmarks

## Executive Summary
This paper introduces DOTS (Diffusion Ordered Temporal Structure), a novel approach for causal structure discovery in time series data that leverages multiple valid causal orderings generated via diffusion models. Traditional methods struggle with the combinatorial complexity of identifying true causal relationships, especially as variables and time points increase. DOTS addresses this by aggregating multiple orderings to recover the transitive closure of the underlying DAG, effectively filtering out spurious edges that plague single-ordering approaches.

The method integrates temporal constraints and uses score matching with diffusion processes for efficient Hessian estimation. Empirical evaluations demonstrate significant performance improvements over state-of-the-art baselines on both synthetic and real-world datasets. On the CausalTime benchmark, DOTS achieves the highest average summary-graph F1 while halving runtime relative to graph-optimization methods.

## Method Summary
DOTS tackles causal structure discovery by first converting time series data into a lag-embedded representation, then training a diffusion model to denoise this embedded data. Multiple causal orderings are extracted by finding leaf nodes at different diffusion timesteps using Hessian diagonal variance. These orderings are aggregated via soft voting and thresholded to produce a preliminary graph structure. CAM pruning removes indirect edges, and temporal constraints eliminate backward-in-time edges. The approach reduces the NP-hard DAG search to a more scalable ordering search while maintaining acyclicity.

## Key Results
- DOTS achieves F1_W of 0.81 on synthetic benchmarks compared to 0.63 for best baseline
- On CausalTime real-world benchmark, DOTS attains highest average summary-graph F1 while halving runtime relative to graph-optimization methods
- Multi-ordering aggregation effectively recovers transitive closure, filtering spurious edges
- Diffusion timesteps provide diverse orderings by emphasizing different frequency components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating multiple causal orderings converges to the transitive closure of the underlying temporal DAG, filtering spurious edges inherent in single-ordering approaches.
- Mechanism: A DAG admits multiple valid topological orderings. Each ordering is a linear extension of the reachability relation (partial order). By Proposition 1, the intersection of edge sets across orderings asymptotically recovers the transitive closure (G+), which contains all true ancestral relationships (direct and indirect). Single orderings over-specify by treating every later node as a potential descendant, introducing spurious edges. Aggregation removes edges that do not appear consistently across diverse orderings, reducing false positives while preserving true reachability.
- Core assumption: The true causal structure is a temporal DAG (Assumption 1), causal sufficiency (Assumption 4), and the sampled orderings are diverse enough to approximate the set of all valid topological sorts.
- Evidence anchors:
  - [abstract] "By integrating multiple orderings, DOTS effectively recovers the transitive closure of the underlying directed acyclic graph (DAG), mitigating spurious artifacts inherent in single-ordering approaches."
  - [Section 3] Proposition 1 and Example 1 formally show that the intersection of all topological orderings yields G+.
  - [corpus] Related work on summary causal graphs (arXiv:2508.21742) aligns with the need for robust temporal abstractions but does not directly validate multi-ordering aggregation.
- Break condition: If sampled orderings are near-identical (low diversity), aggregation yields little new information and may not filter spurious edges.

### Mechanism 2
- Claim: Diffusion timesteps provide diverse orderings by emphasizing different frequency components, enabling efficient multi-scale causal discovery without retraining.
- Mechanism: A forward diffusion process adds Gaussian noise, acting as a low-pass filter with bandwidth decreasing as k increases (Section 4.1, Figure 5). Large k emphasizes low-frequency (coarse) structure; small k emphasizes high-frequency (fine) details. The score network ε_θ(x,k) approximates the score at each k, and the Hessian diagonal variance identifies leaf nodes. Repeating leaf-finding at multiple k yields diverse orderings because different noise scales reveal different adjacency constraints, all from a single trained network.
- Core assumption: Additive noise model (Assumption 3: TiMINo) with noise distribution regularity (Assumption 5) for leaf detection via Hessian variance; stationarity (Assumption 2).
- Evidence anchors:
  - [abstract] "...leverages multiple valid causal orderings generated via diffusion models... using diffusion-based causal discovery for temporal data."
  - [Section 4.1] Frequency analysis of diffusion steps and Figure 5 illustrate the multi-scale decomposition.
  - [corpus] Corpus lacks direct validation of the frequency-to-ordering link; confirm via controlled ablations.
- Break condition: If the diffusion model is undertrained or noise scales are not diverse enough, orderings will be similar, reducing aggregation benefit.

### Mechanism 3
- Claim: Temporal constraints (causes precede effects) reduce the search space and filter backward-in-time edges, improving reliability.
- Mechanism: Lag-embedding represents time series as D ∈ R^(T−τ_max)×d(τ_max+1). The leaf-finding procedure operates on this embedded space. After aggregation, only edges satisfying t−τ < t (temporal priority principle) are retained, enforcing a natural filtering mechanism. This complements aggregation by removing non-temporal spurious edges that might otherwise survive voting.
- Core assumption: Temporal priority principle (Assumption 1) forbids backward causation; stationarity (Assumption 2).
- Evidence anchors:
  - [abstract] "...uses score matching with diffusion processes for efficient Hessian estimation. Empirical evaluations... demonstrate that DOTS outperforms state-of-the-art baselines."
  - [Section 3.3] Formal definition of temporally constrained aggregated edge set and its role.
  - [corpus] Related work (DCD, arXiv:2602.01433) emphasizes challenges of non-stationarity/autocorrelation but does not contradict the benefit of temporal constraints.
- Break condition: If contemporaneous (instantaneous) edges are present but not modeled (experiments assume τ>0), performance may degrade; or if stationarity is violated.

## Foundational Learning

- **Concept: Topological ordering / Causal ordering**
  - Why needed here: DOTS reduces NP-hard DAG search to ordering search, which is more scalable and enforces acyclicity.
  - Quick check question: Given a DAG with nodes {A,B,C} and edges A→B, B→C, list all valid topological orderings and their intersection's implied edges.

- **Concept: Transitive closure of a DAG**
  - Why needed here: Multi-ordering aggregation converges to G+, the reachability structure, which is then pruned to recover G.
  - Quick check question: Compute the transitive closure of a DAG with edges X→Y, Y→Z; what extra edges does G+ have?

- **Concept: Score matching and Hessian estimation**
  - Why needed here: DOTS uses diffusion models to estimate ∇_x log p(x) and its Hessian; Hessian diagonal variance identifies leaf nodes for ordering.
  - Quick check question: Why does a leaf node have low variance in the Hessian diagonal of the log-likelihood score?

## Architecture Onboarding

- **Component map**: Data preprocessor (lag-embedding) -> Diffusion model trainer (score network ε_θ(x,k)) -> Multi-scale ordering extractor (Hessian-based leaf detection at multiple k) -> Ordering aggregator (soft voting + thresholding) -> Pruning module (CAM) -> Temporal constraint filter

- **Critical path**: Lag-embed → train diffusion model once → for each k in selected noise scales: estimate Hessian, find leaf, mask, repeat until all nodes ordered → aggregate orderings via vote matrix → threshold → prune with CAM → enforce temporal constraints → output adjacency

- **Design tradeoffs**: (a) More orderings (higher S) improve robustness but increase compute; empirically 4–10 orderings balance cost and performance (Figure 9). (b) Lower threshold θ increases recall but may add indirect edges; ablations suggest θ=0 with CAM pruning works well. (c) Larger τ_max covers more lags but may slow training; ablations show setting τ_max higher than true lag is acceptable.

- **Failure signatures**: (1) Low diversity in orderings (check Kendall tau distances, Figure 15) → aggregation ineffective. (2) Undertrained diffusion model → Hessian estimates unreliable. (3) Violation of stationarity or sufficiency → degraded performance (not fully tested). (4) Insufficient sample size (T<1000) → lower F1.

- **First 3 experiments**:
  1. Replicate synthetic experiment (d=3, T=2000, τ=1) with 1 vs. 5 vs. 10 orderings to verify performance scaling (target: F1_W from ~0.6 to ~0.8).
  2. Ablate temporal constraint on a non-temporal baseline (e.g., CAM-C) to quantify precision/recall impact (target: modest precision gain).
  3. Test on CausalTime AQI subset (d=36, T combined) with default DOTS settings; compare average F1_S to baseline VARLiNGAM (target: DOTS avg. F1_S ~0.45 vs. VARLiNGAM ~0.44).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DOTS framework perform in non-stationary environments where causal mechanisms vary over time?
- Basis in paper: [explicit] Section 5.7 states that current robustness claims are limited to the tested data characteristics and that "future work should investigate our method’s robustness to non-stationary... settings."
- Why unresolved: The method relies on Assumption 2 (Stationarity), meaning the causal structure $G$ and mechanisms are assumed invariant across time.
- What evidence would resolve it: Empirical evaluations on synthetic or real-world benchmarks containing regime shifts or time-varying causal mechanisms.

### Open Question 2
- Question: Can DOTS reliably estimate causal structure in the presence of unobserved latent confounders?
- Basis in paper: [explicit] Section 5.7 explicitly notes that future work is needed to investigate the method’s robustness to "confounded settings," as the current work assumes Causal Sufficiency (Assumption 4).
- Why unresolved: The theoretical identifiability results and pruning steps depend on the assumption that all common causes are observed.
- What evidence would resolve it: Theoretical proofs or experiments showing the algorithm's behavior when Assumption 4 is violated (e.g., using datasets with hidden common causes).

### Open Question 3
- Question: What alternative aggregation strategies could more effectively recover the true DAG $G$ from the transitive closure $G^+$?
- Basis in paper: [explicit] Section 7 lists "exploring additional aggregation strategies for causal orderings" as a primary direction for future research.
- Why unresolved: The current approach uses soft voting and likelihood-based CAM pruning, which may not be optimal for distinguishing direct edges from indirect ones in the closure.
- What evidence would resolve it: Comparative studies showing higher F1 scores using different aggregation or pruning techniques within the DOTS pipeline.

### Open Question 4
- Question: How can the framework be modified to increase edge detection (recall) without compromising the high precision observed in experiments?
- Basis in paper: [explicit] Appendix B.1 notes that top-performing methods often miss edges (recall lower than 1) and states, "Future work could focus on improving edge detection while maintaining high precision levels."
- Why unresolved: The current diffusion and pruning steps appear conservative, leading to high precision but leaving some true causal links undetected.
- What evidence would resolve it: Modifications to the algorithm that result in higher recall rates on standard benchmarks while maintaining a precision rate near 1.0.

## Limitations

- The multi-ordering aggregation approach critically depends on obtaining diverse orderings; if diffusion timesteps fail to produce sufficiently different orderings, the intersection-based aggregation may not effectively filter spurious edges.
- The method assumes stationary, noise-free, and causally sufficient data, but real-world datasets often violate these assumptions—particularly non-stationarity and contemporaneous confounding—which could degrade performance significantly.
- Exact implementation details for CAM pruning and the selection of diffusion timesteps remain underspecified, making exact replication challenging.

## Confidence

- **High confidence**: The core mechanism of multi-ordering aggregation converging to transitive closure (Mechanism 1) is well-founded mathematically and supported by synthetic results showing clear F1 improvements (0.63→0.81).
- **Medium confidence**: The diffusion-based ordering extraction (Mechanism 2) is theoretically plausible but lacks direct empirical validation of the frequency-to-ordering diversity claim; performance gains depend heavily on the diffusion model quality and hyperparameter tuning.
- **Medium confidence**: The temporal constraint filtering (Mechanism 3) is a valid addition but its marginal benefit is not rigorously isolated from other improvements in the experiments.

## Next Checks

1. **Ordering Diversity Validation**: Generate multiple orderings from DOTS on a simple DAG (e.g., d=4, chain structure) and compute pairwise Kendall tau distances to confirm diversity. Vary the number and range of diffusion timesteps to identify conditions where diversity plateaus.

2. **Stationarity Violation Test**: Modify a synthetic dataset to include non-stationary regime shifts (e.g., changing causal coefficients mid-series) and compare DOTS performance against a stationary baseline to quantify robustness degradation.

3. **CAM Pruning Ablation**: Run DOTS with and without CAM pruning on the same synthetic dataset, measuring F1_W and F1_S separately to isolate the contribution of indirect edge removal versus ordering aggregation.