---
ver: rpa2
title: ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional
  Framing Induces Bias in LLM Outputs
arxiv_id: '2507.21083'
source_url: https://arxiv.org/abs/2507.21083
tags:
- tone
- emotional
- negative
- neutral
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GPT-4's responses are significantly biased by the emotional tone
  of prompts, exhibiting "emotional rebound" and a "tone floor." When prompted negatively,
  GPT-4 is three times less likely to respond negatively (14%) than neutrally phrased
  questions, often shifting to neutral (58%) or positive (28%) tones. Conversely,
  neutral or positive prompts rarely trigger negative responses, suggesting a built-in
  resistance to negativity.
---

# ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs

## Quick Facts
- arXiv ID: 2507.21083
- Source URL: https://arxiv.org/abs/2507.21083
- Authors: Franck Bardol
- Reference count: 3
- GPT-4's responses are significantly biased by the emotional tone of prompts, exhibiting "emotional rebound" and a "tone floor."

## Executive Summary
GPT-4 exhibits strong bias in responses based on the emotional tone of user prompts, with negative framing rarely producing negative outputs—instead shifting to neutral or positive tones. This "emotional rebound" and "tone floor" suggest learned alignment policies discourage negativity and overcompensate for user frustration. However, on sensitive topics like politics or justice, tone-based variation is suppressed, indicating alignment overrides emotional framing. Embedding analysis confirms semantic drift based on tone, with near-zero Frobenius distances on sensitive topics indicating strong consistency.

## Method Summary
The study used 52 base questions across everyday and sensitive domains, generating three tone variants each (neutral, positive, negative). GPT-4 responses were collected with temperature=0.7 and no system prompt. The model self-rated its responses for valence (negative/neutral/positive) with confidence scores. Transition matrices mapped input tone to output valence, and Frobenius distances quantified tone sensitivity across topic categories. Embedding analysis visualized semantic drift via PCA/UMAP.

## Key Results
- Negative prompts rarely produce negative responses (14%)—instead shifting to neutral (58%) or positive (28%).
- Neutral and positive prompts almost never trigger negative responses, suggesting a built-in "tone floor."
- On sensitive topics, tone-based variation is suppressed, with near-zero Frobenius distances indicating alignment overrides emotional framing.

## Why This Works (Mechanism)

### Mechanism 1: Emotional Rebound via RLHF-Derived Positivity Bias
- **Claim:** Negative prompts trigger compensatory "comfort mode," shifting responses toward neutral or positive tones.
- **Mechanism:** Model detects user frustration and conditions generation toward reassuring outputs, likely due to RLHF reward models penalizing harshness and rewarding supportive responses.
- **Core assumption:** Rebound stems from RLHF training biases rather than architectural properties.
- **Evidence anchors:** [abstract]: Negatively framed prompts rarely led to negative answers (~14%)—instead rebounding to neutral (~58%) or positive (~28%). [section 6.1]: Emotional rebound and tone floor suggest learned policy discouraging negativity. [corpus]: Related work (Vinay et al. 2025, "Mind Your Tone") confirms politeness affects accuracy, but no direct rebound replication.
- **Break condition:** If base pretrained model exhibits similar rebound, mechanism is not RLHF-specific.

### Mechanism 2: Tone Floor as a Negativity Lower Bound
- **Claim:** GPT-4 has effective lower bound on negativity—neutral/positive prompts almost never produce negative responses.
- **Mechanism:** Generative distribution constrained so negative valence outputs require explicit negative framing and absence of alignment triggers. Asymmetry emerges because RLHF penalizes negativity more than rewarding positivity.
- **Core assumption:** Tone floor is property of alignment training rather than base model's natural distribution.
- **Evidence anchors:** [section 3.2]: Even neutral questions elicit disproportionately positive answers (49%), suggesting tone floor. [section 5]: Negative responses appear more dispersed or pushed toward other clusters. [corpus]: Weak direct evidence—other papers discuss bias but don't quantify valence floors.
- **Break condition:** If carefully constructed neutral prompts on non-sensitive topics can reliably elicit negative responses (>30%), tone floor claim is overstated.

### Mechanism 3: Hierarchical Alignment Override on Sensitive Topics
- **Claim:** On politically/morally sensitive topics, alignment constraints suppress emotional modulation entirely.
- **Mechanism:** Two-level control hierarchy: (1) everyday topics—free tone adaptation; (2) sensitive topics—hard-coded policies lock responses to alignment-stable outputs regardless of affective framing.
- **Core assumption:** Suppression due to explicit alignment mechanisms rather than model's inherent uncertainty about sensitive topics.
- **Evidence anchors:** [abstract]: On sensitive topics, tone-based variation is suppressed, suggesting alignment override. [section 3.4]: Smallest distances (0.53–0.55) occur between tone variants on sensitive topics, suggesting consistency regardless of tone. [corpus]: No direct replication—finding is novel.
- **Break condition:** If extreme negative framing on sensitive topics still produces tone-invariant responses, hierarchical control is robust.

## Foundational Learning

- **Concept: Valence Transition Matrices**
  - **Why needed here:** Core quantitative evidence comes from 3×3 matrices mapping input tone → output valence frequencies. Essential for replicating analysis.
  - **Quick check question:** If transition matrix shows [0.10, 0.70, 0.20] for negative→[neg, neu, pos], what does 0.70 represent?

- **Concept: Frobenius Distance Between Matrices**
  - **Why needed here:** Paper uses Frobenius distance to quantify how much tone effects differ between topic categories. Lower values mean more consistent behavior across tones.
  - **Quick check question:** If Frobenius distance between two transition matrices is 0.53, does this indicate high or low consistency in response patterns?

- **Concept: RLHF and Reward Model Biases**
  - **Why needed here:** Hypothesized mechanism for emotional rebound and tone floor relies on RLHF creating implicit positivity preferences.
  - **Quick check question:** If RLHF reward model penalizes "harsh" outputs more than "neutral" ones, what behavioral asymmetry would you expect?

## Architecture Onboarding

- **Component map:** Prompt Triplet Generator -> LLM Query Layer -> Self-Evaluation Wrapper -> Transition Matrix Builder -> Frobenius Distance Calculator -> Embedding Analyzer
- **Critical path:** 1. Design 52 base questions across everyday + sensitive domains; 2. Generate 3 tone variants each (156 prompts); 3. Query GPT-4 once per prompt (temp=0.7, no system prompt); 4. Extract self-rated valence from structured output; 5. Build separate transition matrices for sensitive vs. non-sensitive subsets; 6. Compute Frobenius distances to quantify tone-sensitivity by domain
- **Design tradeoffs:** Self-evaluation vs. external classifier (potential bias vs. independent validation); Temperature=0.7 vs. 0 (natural variation vs. deterministic isolation); Manual vs. automated sensitive-topic labeling (subjective vs. scalable categorization)
- **Failure signatures:** Inconsistent triplet framing (confounds framing with intent); Self-rating drift (lower confidence for negative responses suggests unreliable self-assessment); Topic boundary ambiguity (manual labeling may obscure true alignment activation boundaries)
- **First 3 experiments:** 1. Replicate with external sentiment classifier to validate rebound and tone floor under independent measurement; 2. Test base model (no RLHF) to determine if rebound is RLHF-derived or inherent to architecture; 3. Stress-test alignment override with increasingly extreme negative framings on sensitive topics to identify threshold where alignment breaks down

## Open Questions the Paper Calls Out
None

## Limitations
- Self-evaluation bias: GPT-4's own valence classification may be systematically skewed, with negative responses showing lower confidence than positive/neutral
- Manual sensitive-topic labeling: Division between "everyday" and "sensitive" topics is subjective, potentially obscuring true boundaries of alignment overrides
- RLHF mechanism assumption: Causal attribution to RLHF reward model biases not experimentally separated from base model behavior

## Confidence
- **High confidence:** Empirical observation that negative prompts rarely produce negative responses (14% vs. 58% neutral, 28% positive) is robust across datasets and visualization methods
- **Medium confidence:** Hierarchical alignment override on sensitive topics is well-supported by distance metrics but relies on manual topic classification
- **Low confidence:** Causal attribution of emotional rebound and tone floor to RLHF reward model biases not directly tested

## Next Checks
1. External sentiment validation: Replicate analysis using independent sentiment classifier instead of self-evaluation to verify effects persist under objective measurement
2. Base model comparison: Run same triplet protocol on pretrained-only model to determine whether emotional rebound is RLHF artifact or inherent to transformer architecture
3. Alignment threshold stress test: Systematically escalate negative framing intensity on sensitive topics to identify whether alignment produces refusal responses or tone-invariant answers persist under extreme emotional pressure