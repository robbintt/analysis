---
ver: rpa2
title: How Wide and How Deep? Mitigating Over-Squashing of GNNs via Channel Capacity
  Constrained Estimation
arxiv_id: '2511.06443'
source_url: https://arxiv.org/abs/2511.06443
tags:
- information
- representation
- entropy
- graph
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses over-squashing in graph neural networks (GNNs),
  where information loss occurs during propagation. The authors propose Channel Capacity
  Constrained Estimation (C3E), an information-theoretic framework that models GNNs
  as communication channels and estimates optimal hidden dimensions and depth by maximizing
  channel capacity under constraints.
---

# How Wide and How Deep? Mitigating Over-Squashing of GNNs via Channel Capacity Constrained Estimation

## Quick Facts
- arXiv ID: 2511.06443
- Source URL: https://arxiv.org/abs/2511.06443
- Authors: Zinuo You; Jin Zheng; John Cartlidge
- Reference count: 40
- Primary result: Proposes C3E, an information-theoretic framework that estimates optimal GNN architectures by maximizing channel capacity under constraints, mitigating over-squashing without modifying propagation methods.

## Executive Summary
This paper addresses over-squashing in graph neural networks (GNNs), where information loss occurs during propagation. The authors propose Channel Capacity Constrained Estimation (C3E), an information-theoretic framework that models GNNs as communication channels and estimates optimal hidden dimensions and depth by maximizing channel capacity under constraints. Extensive experiments on nine datasets demonstrate that C3E-estimated architectures mitigate over-squashing and consistently improve performance compared to heuristic configurations. Key findings include: over-squashing results from cumulative information compression in representation matrices, increasing hidden dimensions effectively reduces this compression, and the role of depth is nuanced—beneficial for low compression ratios but harmful when excessive. C3E achieves these improvements without modifying propagation methods or graph structures, and solutions are generated in seconds versus hours for trial-and-error approaches.

## Method Summary
C3E treats GNN propagation as a communication channel and derives upper bounds on channel capacity based on spectral graph theory and maximum entropy principles. The framework estimates optimal hidden dimensions and depth by solving a constrained nonlinear optimization problem that maximizes effective channel capacity while avoiding over-compression. The method requires pre-computing propagation variance for each layer, then uses SLSQP to solve for width and depth configurations satisfying information-theoretic constraints. The approach is validated across nine datasets using various spectral GNN architectures, comparing C3E-derived architectures against heuristic configurations.

## Key Results
- C3E solutions generated in seconds versus hours for grid search approaches
- C3E-estimated architectures consistently outperform heuristic configurations across all nine datasets
- Over-squashing correlates with cumulative information compression measured by representation compression ratio θ
- Depth is beneficial only when compression ratio is low; harmful when excessive
- Solutions maintain stable weight distributions post-training, avoiding spike/sparse patterns

## Why This Works (Mechanism)

### Mechanism 1: Channel Capacity Upper Bound Estimation
The framework treats layer-wise representation learning as a Gaussian communication channel under maximum entropy assumptions. Since pre-training network states are unknown, the principle of maximum entropy justifies treating weight and representation matrices as independent Gaussians, yielding a tractable upper bound on channel capacity. The core assumption is that entries of weight and representation matrices are independent draws from distributions with known mean and variance, with activation functions not increasing maximum entropy.

### Mechanism 2: Representation Compression Ratio (θ) as a Diagnostic
θ quantifies how much channel capacity is compressed per equivalent representation dimension. When θ → ∞, severe compression occurs; when θ → 0, dimensions are over-provisioned. The ratio exhibits dual dependency: increasing with depth when hidden width is below threshold w*, and decreasing otherwise. This provides a diagnostic tool for identifying over-squashing conditions.

### Mechanism 3: Effective Channel Capacity with Architectural Constraints
Real GNNs with finite layer widths have effective capacity φ' < φ, constrained by inter-layer bottlenecks. The effective channel capacity accounts for adjacent-layer width mismatches, modeling structural bottlenecks when disparities are large. This enforces smooth width transitions across layers to prevent information bottlenecks.

## Foundational Learning

- **Concept:** Shannon Channel Capacity
  - Why needed here: The entire C3E framework rests on modeling GNNs as communication channels; understanding capacity limits is essential.
  - Quick check question: Can you explain why channel capacity depends on both signal power and noise, and how this maps to GNN hidden dimensions and propagation variance?

- **Concept:** Spectral Graph Neural Networks
  - Why needed here: C3E derivations assume spectral GNNs where propagation can be collapsed into a single matrix operator S_l for analytical tractability.
  - Quick check question: What is the key difference between spectral and spatial GNN propagation that makes spectral methods more amenable to closed-form analysis?

- **Concept:** Maximum Entropy Principle (Jaynes)
  - Why needed here: Justifies the Gaussian and independence assumptions used to derive channel capacity bounds in the absence of prior network knowledge.
  - Quick check question: Why does maximum entropy favor independent Gaussian distributions when only mean and variance constraints are known?

## Architecture Onboarding

- **Component map:** Propagation variance calculator -> Channel capacity estimator -> Compression ratio monitor -> Nonlinear optimizer
- **Critical path:**
  1. Compute σ²_Sl for each layer based on graph structure and propagation method
  2. Formulate optimization problem with constraints: w > w*, ln(w̄) > -K, ln(n) ≤ φ' ≤ (1/η)ln(n)
  3. Solve for w(L) = {w_1, ..., w_L} and L; round hidden dimensions post-hoc
  4. Validate by checking θ and φ' fall within prescribed bounds

- **Design tradeoffs:**
  - η selection (regularizer): Lower η (0.10–0.25) yields more candidate solutions but longer solve times; higher η (>0.45) risks missing optimal solutions
  - Depth vs width: Depth helps when compression ratio θ is low; harms when θ is high
  - Computational cost: C3E solves in seconds to minutes vs hours for grid search, but requires pre-computing σ²_Sl

- **Failure signatures:**
  - No feasible solution: Constraints may be too tight; relax η or check graph entropy bound ln(n)
  - θ too high at solution: Width under-provisioned; increase lower bound on w
  - Post-training weight/activation explosion: Check Appendix I visualizations; C3E solutions should show stable Gaussian-like distributions

- **First 3 experiments:**
  1. Baseline comparison on small graph (Cora): Run C3E with η=0.10–0.30; compare accuracy and θ against heuristic widths (16, 64, 128, 256). Verify C3E solutions land in high-performance regions.
  2. Depth sensitivity analysis: Fix a C3E-derived width profile; vary L from 1–7. Plot θ and accuracy to confirm the dual-regime behavior: accuracy should peak then decline as θ rises.
  3. Large-scale validation (ogbn-arxiv): Test scalability by timing C3E solution generation (target: <240s per Table 1) and comparing against baseline GNNs with heuristic dimensions. Monitor post-training weight distributions for stability.

## Open Questions the Paper Calls Out

### Open Question 1
Can the C3E framework be generalized to spatial GNNs and Graph Transformers that rely on learned, feature-dependent propagation mechanisms?
The authors state in the Conclusion that "the primary focus for future research is to generalize C3E to spatial GNNs that depend on learned features for propagation, such as Graph Transformers." The current theoretical derivation relies on analytically tractable spectral propagation matrices (S_l) that can be collapsed into a single operator, a property not inherent to spatial message-passing schemes.

### Open Question 2
How can the theoretical entropy upper bounds be refined to more accurately reflect the information capacity of practical, trained networks?
The Conclusion acknowledges the current framework is grounded in maximum entropy, which "may overestimate what practical networks can achieve," and proposes tightening bounds with empirical results. The assumption of Gaussian-distributed weights and representations serves as a theoretical ceiling; however, trained networks often exhibit structured correlations and sparsity that reduce actual information flow compared to this maximum.

### Open Question 3
To what extent does the C3E estimation depend on strict adherence to variance-preserving weight initializations?
The derivation of the channel capacity in Theorem 1 and Appendix B.3 relies on the assumption that σ²_w + μ²_w = 1 to simplify the formula. While common (e.g., Xavier/Kaiming), not all initializations strictly preserve variance. If this assumption is violated, the calculated optimal dimensions may not effectively mitigate over-squashing.

## Limitations

- Theoretical gap in activation modeling: The channel capacity bounds rely on maximum entropy Gaussian assumptions that may not hold for nonlinear activations, potentially causing loose upper bounds.
- Dataset and model scope constraints: C3E is validated on 9 node/graph property datasets with spectral GNNs; performance on spatial GNNs, heterogeneous graphs, or molecular/property prediction tasks remains untested.
- Propagation variance estimation sensitivity: C3E requires accurate pre-computation of σ²_Sl; if graph signal is non-stationary or numerical instability occurs, estimated architectures may be suboptimal.

## Confidence

- Channel capacity as an effective over-squashing diagnostic: High
- Hidden dimension scaling improves information retention: High
- Depth is beneficial only under low compression ratios: Medium
- C3E estimates architectures in seconds vs. hours for NAS: High
- C3E solutions generalize across GNN variants: Medium

## Next Checks

1. **Activation correlation analysis:** For C3E-derived architectures, compute empirical layer-wise mutual information and compare against the Gaussian entropy upper bounds to assess bound tightness.

2. **Robustness to propagation variance estimation:** Perturb σ²_Sl by ±10% and re-solve the C3E optimization to measure sensitivity of estimated architecture and final accuracy.

3. **Spatial GNN extension:** Implement C3E for a spatial GNN (e.g., GAT or GraphSAGE) by adapting the propagation variance calculation to message-passing aggregation and validate on Cora/Citeseer.