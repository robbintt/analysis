---
ver: rpa2
title: Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation
arxiv_id: '2307.02146'
source_url: https://arxiv.org/abs/2307.02146
tags:
- lyric
- melody
- lyrics
- length
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating singable lyrics
  from melody input, aiming to improve the alignment between lyrical content and musical
  structure. The core method combines prompt-based length control with a two-stage
  training approach: self-supervised learning on text-only lyrics for length awareness,
  followed by supervised fine-tuning on paired melody-lyric data with auxiliary classification
  objectives to capture prosodic patterns.'
---

# Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation

## Quick Facts
- **arXiv ID**: 2307.02146
- **Source URL**: https://arxiv.org/abs/2307.02146
- **Reference count**: 24
- **Primary result**: Prompt-based length control + two-stage training improves singable lyric generation alignment by 3.8% (line count) and 21.4% (syllable count) absolute without degrading text quality.

## Executive Summary
This paper addresses the challenge of generating singable lyrics from melody input by combining prompt-based length control with a two-stage training approach. The method first uses self-supervised learning on text-only lyrics to teach the model length awareness through special `<len n>` tokens, then fine-tunes on paired melody-lyric data with auxiliary classification objectives to capture prosodic patterns. The approach achieves significant improvements in adherence to line-count and syllable-count requirements compared to task-specific baselines, with human evaluation showing 42.2-74.2% relative gains in overall quality.

## Method Summary
The method employs a two-stage training approach using a BART-base backbone. Stage 1 involves self-supervised length-aware training on text-only lyrics, where special length tokens (`<len n>`) are prepended to teach the model to associate specific syllable counts with output formatting. Stage 2 performs supervised fine-tuning on paired melody-lyric data, incorporating auxiliary classifiers that predict syllable stress, word importance, and vowel duration from the melody encoder's hidden states. The melody is quantized by normalizing onset/duration values by the shortest note duration and discretizing to tokens, with separate embeddings for onset, duration, pitch, and rest attributes.

## Key Results
- Improves adherence to line-count requirements by 3.8% absolute
- Improves adherence to syllable-count requirements by 21.4% absolute
- Achieves 42.2% and 74.2% relative gains in human-rated overall quality over two task-specific baselines
- Shows 2.63%, 1.45%, and 2.21% improvements in fine-grained compatibility metrics (Dur-str, Peak-str, Dur-vow)

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Length Control via Self-Supervised Pretraining
Length tokens (`<len n>`) are embedded as learnable vectors and prepended to masked lyric input during text-only reconstruction. The model learns to condition its reconstruction on these tokens through the denoising objective, and this association transfers to the supervised stage where the same tokens guide syllable-level generation. Evidence shows removing length prompts during self-supervised training drops line-count accuracy from 99.35% to 14.28% and syllable accuracy from 97.15% to 13.15%.

### Mechanism 2: Auxiliary Classification Tasks Encourage Prosodic Encoding
Position-wise classifiers attached to the melody encoder force it to learn representations that predict stress, word importance, and vowel duration. The joint loss backpropagates through the encoder, enriching hidden states with prosodic information. Full model improves Dur-str by +2.63%, Peak-str by +1.45%, and Dur-vow by +2.21% over single-task baseline.

### Mechanism 3: Quantized Melody Representation Reduces Temporal Sparsity
Raw onset/offset times are converted to quantized integers via relative normalization by the shortest note duration within a paragraph. Each attribute (onset, duration, pitch, rest) has separate embeddings summed per note. This reduces sparsity issues that hinder learning temporal patterns from absolute timing in seconds.

## Foundational Learning

- **Concept: Prompt-Based Fine-Tuning**
  - **Why needed here**: The core length-control mechanism relies on extending the tokenizer with special tokens and conditioning generation on them.
  - **Quick check question**: Can you explain how adding `<len 7>` to the input changes what the model generates, and why this requires modifying the embedding matrix?

- **Concept: Multi-Task Learning with Auxiliary Objectives**
  - **Why needed here**: The supervised stage jointly optimizes sequence generation and three classification tasks.
  - **Quick check question**: If the stress classifier has high accuracy but generation still misplaces stressed syllables, what does this suggest about the decoder's use of encoder representations?

- **Concept: Denoising Autoencoder Pretraining (BART)**
  - **Why needed here**: The foundation model uses span-masking objectives.
  - **Quick check question**: Why does masking spans rather than individual tokens help the model learn paragraph-level structure for lyrics?

## Architecture Onboarding

- **Component map**: Melody Input → Quantization → [Emb_on, Emb_dur, Emb_pitch, Emb_rest] → Sum → Concat → Transformer Encoder → [h] → Transformer Decoder → Lyrics; Length Prompts → [Emb_len] → ↓ [Linear_str, Linear_imp, Linear_vow] → Classification Losses

- **Critical path**: The encoder's hidden states `h` must capture both rhythmic structure from melody embeddings and prosodic cues from auxiliary classifier gradients. If either signal is weak, the decoder generates fluent but unsingable lyrics.

- **Design tradeoffs**: BART-base (139M params) chosen for interpretability over larger models; limits absolute quality but clearer attribution of gains. Strict 1:1 note-to-syllable alignment sacrifices flexibility for reliability. Vowel duration categories are coarse approximations of phonetics.

- **Failure signatures**: Line count matches but syllable counts vary → length prompts not embedded during self-supervised stage. Fluent lyrics with awkward stress placement → auxiliary classifiers undertrained or encoder-decoder coupling weak. Nonsensical output across all conditions → foundation model corrupted or learning rate too high.

- **First 3 experiments**:
  1. Verify length control in isolation: Train only self-supervised stage on text-only lyrics, then generate with various `<len n>` prompts. Confirm exact syllable counts before adding melody conditioning.
  2. Ablate auxiliary tasks one at a time: Remove stress, importance, and vowel classifiers individually to measure their contribution to fine-grained compatibility metrics.
  3. Test quantization robustness: Inject artificial "outlier" short notes into melodies and measure whether generation quality degrades due to distorted `dur_u` scaling.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can melody-to-lyric generation models maintain stable alignment while allowing for controlled flexibility in syllable counts (e.g., melisma) rather than enforcing strict one-to-one note–syllable mapping?
**Basis in paper**: Appendix D notes that human lyricists often deviate from strict syllable counts to preserve semantic clarity, stating that "achieving stable melody–lyric alignment under flexible length settings remains non-trivial and is left for future exploration."
**Why unresolved**: The current self-supervised training method relies on strict length prompts to ensure alignment, making it difficult for the model to learn when and how to apply intentional deviations without breaking the rhythm.
**What evidence would resolve it**: A model that successfully generates lyrics where syllable counts deviate from note counts while maintaining high scores in human evaluations for "singability" and "naturalness."

### Open Question 2
**Question**: Does incorporating explicit metric information (e.g., BPM, measure structure) improve generation robustness compared to normalizing timing by the shortest note duration?
**Basis in paper**: Appendix D highlights that the current normalization method (scaling by the shortest note duration within a paragraph) may be unstable and suggests future work could "benefit from assuming access to musically meaningful metric information such as quantized MIDI with bpm and measure-level structure."
**Why unresolved**: The current relative normalization loses global temporal context and can be sensitive to local variations in the shortest note, potentially hindering the model's ability to capture structural rhythms.
**What evidence would resolve it**: A comparative study showing that conditioning the encoder on absolute metric positions (beats/bars) results in higher accuracy in fine-grained compatibility metrics compared to the relative scaling method.

### Open Question 3
**Question**: To what extent do expressive singing features (beyond text and melody) contribute to the perceived singability of generated lyrics?
**Basis in paper**: Section 6 lists "direct incorporation of expressive singing features" as a specific direction for future work to narrow the remaining singability gap.
**Why unresolved**: The current system models compatibility based on text and melody alone, which may miss nuanced vocal performance attributes that influence how lyrics are experienced by a listener.
**What evidence would resolve it**: Experiments demonstrating that integrating acoustic features from vocal performances into the training objective yields statistically significant improvements in human "Overall Quality" ratings over the text-and-melody-only baseline.

## Limitations

- Results reported on single dataset (DALI v2) without external validation on diverse musical styles or languages
- Quantized melody representation may lose expressive timing information critical for certain genres (e.g., jazz)
- Strict 1:1 note-to-syllable alignment assumption may not hold for professional lyrics that use melismas or multiple notes per syllable
- Auxiliary classifiers rely on CMU Pronouncing Dictionary which may not cover all pronunciations or non-English lyrics

## Confidence

- **High confidence**: Length control mechanism (demonstrated by ablation showing 85.07% drop in accuracy when removed) and basic generation quality (human ratings show 42.2-74.2% relative gains over baselines)
- **Medium confidence**: Fine-grained compatibility improvements (Dur-str, Peak-str, Dur-vow gains of 1.45-2.63%) due to potential confounding from controlled experimental conditions and limited human evaluation scope
- **Medium confidence**: Generalization of self-supervised length awareness to melody-conditioned generation, as the transfer mechanism is theoretically sound but not independently validated

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the trained model on an independent melody-lyric dataset (e.g., WikiLyrics or custom pop music collection) to verify that length control and prosodic encoding transfer beyond DALI v2.

2. **Ablation of quantization sensitivity**: Systematically vary the quantization resolution (instead of fixed 10× scaling) and measure impact on both fine-grained compatibility metrics and generation quality to determine if the current choice is optimal or fragile.

3. **Human evaluation of strict alignment**: Conduct targeted human studies comparing the model's strict 1:1 alignment against more flexible approaches on metrics like naturalness and singability, particularly for genres where rhythmic deviation is stylistically important.