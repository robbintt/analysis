---
ver: rpa2
title: 'TaleFrame: An Interactive Story Generation System with Fine-Grained Control
  and Large Language Models'
arxiv_id: '2512.02402'
source_url: https://arxiv.org/abs/2512.02402
tags:
- story
- entity
- entities
- events
- taleframe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TaleFrame is an interactive story generation system that addresses\
  \ the challenge of fine-grained control in creative story generation by integrating\
  \ large language models (LLMs) with human-computer interaction (HCI). The system\
  \ decomposes stories into four foundational units\u2014entities, events, relationships,\
  \ and story outlines\u2014and uses a preference dataset of 9,851 JSON-formatted\
  \ entries to fine-tune a Llama-3-8B model."
---

# TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models

## Quick Facts
- arXiv ID: 2512.02402
- Source URL: https://arxiv.org/abs/2512.02402
- Reference count: 40
- TaleFrame integrates LLMs with HCI for precise, interactive story generation

## Executive Summary
TaleFrame is an interactive story generation system that addresses the challenge of fine-grained control in creative story generation by integrating large language models (LLMs) with human-computer interaction (HCI). The system decomposes stories into four foundational units—entities, events, relationships, and story outlines—and uses a preference dataset of 9,851 JSON-formatted entries to fine-tune a Llama-3-8B model. This approach enables precise control over story generation through structured data and an intuitive interface supporting drag-and-drop, attach, and connect interactions. An ablation study demonstrated that the full model significantly outperforms ablated versions in seven evaluation dimensions (p < 0.05 for most metrics), particularly in readability and emotional clarity. A user study with 19 participants confirmed the system's effectiveness in facilitating controlled, iterative story creation while maintaining creative flexibility.

## Method Summary
TaleFrame leverages a preference dataset of 9,851 JSON-formatted entries to fine-tune a Llama-3-8B model, enabling fine-grained control over story generation. The system decomposes stories into four foundational units—entities, events, relationships, and story outlines—and uses these structured components to guide the LLM. An ablation study compared the full model against ablated versions, demonstrating superior performance in seven evaluation dimensions (p < 0.05 for most metrics). A user study with 19 participants validated the system's usability and creative flexibility.

## Key Results
- Ablation study showed the full model significantly outperforms ablated versions in seven evaluation dimensions (p < 0.05 for most metrics).
- User study with 19 participants confirmed the system's effectiveness in facilitating controlled, iterative story creation.
- The system achieved high scores in readability and emotional clarity, key metrics for creative storytelling.

## Why This Works (Mechanism)
The system works by decomposing stories into four foundational units—entities, events, relationships, and story outlines—and using a preference dataset to fine-tune the LLM. This structured approach allows for precise control over story generation while maintaining creative flexibility. The drag-and-drop, attach, and connect interactions enable users to iteratively shape the narrative, ensuring alignment with their creative vision.

## Foundational Learning
- **Story decomposition into entities, events, relationships, and outlines**: Needed to provide structured control over narrative elements. Quick check: Verify the system can generate coherent stories when only one unit is modified.
- **Preference dataset for fine-tuning**: Required to align the LLM's outputs with user preferences. Quick check: Assess whether the fine-tuned model outperforms the base model in user-defined metrics.
- **Ablation study methodology**: Essential to validate the contribution of each component. Quick check: Confirm statistical significance (p < 0.05) across all evaluation dimensions.

## Architecture Onboarding

### Component Map
User Interface -> Preference Dataset -> Fine-tuned Llama-3-8B -> Story Generation -> Output

### Critical Path
User input (drag-and-drop, attach, connect) -> Preference dataset alignment -> Fine-tuned LLM processing -> Story generation -> Output delivery

### Design Tradeoffs
- **Structured vs. unstructured generation**: Structured units provide control but may limit creativity. Tradeoff: balance precision with flexibility.
- **Fine-tuning vs. prompt engineering**: Fine-tuning aligns outputs with preferences but requires more data. Tradeoff: data collection vs. adaptability.
- **Ablation study scope**: Comprehensive ablation validates components but increases complexity. Tradeoff: thoroughness vs. simplicity.

### Failure Signatures
- **Poor readability**: Indicates misalignment between structured units and LLM outputs.
- **Low emotional clarity**: Suggests insufficient preference dataset diversity.
- **User dissatisfaction**: Points to usability issues in the interface or interaction design.

### 3 First Experiments
1. Test the system's ability to generate coherent stories with only one unit (e.g., entities) modified.
2. Evaluate the impact of increasing the preference dataset size on story quality.
3. Assess the system's performance with alternative LLM architectures (e.g., GPT-4).

## Open Questions the Paper Calls Out
None

## Limitations
- The user study sample size (n = 19) is relatively small for drawing robust conclusions about real-world usability.
- The preference dataset, while substantial, may lack diversity due to limited participant pool.
- The evaluation framework focuses on predefined dimensions, potentially overlooking emergent creative qualities.

## Confidence

### High Confidence
- Technical implementation of the preference dataset and ablation study methodology
- Statistical significance of most evaluation metrics (p < 0.05)

### Medium Confidence
- User study findings regarding usability and creative control
- Generalizability of results across diverse storytelling contexts

### Low Confidence
- Long-term creative outcomes and adaptability to different cultural or narrative traditions

## Next Checks
1. Conduct a longitudinal study with a larger, more diverse participant pool (n > 50) to assess the system's usability and creative impact across varied storytelling contexts and cultural backgrounds.
2. Test the system's performance with alternative LLM architectures (e.g., GPT-4, Claude) to evaluate scalability and robustness of the preference-based fine-tuning approach.
3. Expand the evaluation framework to include qualitative measures of emergent creativity, such as narrative originality and emotional resonance, through expert literary analysis.