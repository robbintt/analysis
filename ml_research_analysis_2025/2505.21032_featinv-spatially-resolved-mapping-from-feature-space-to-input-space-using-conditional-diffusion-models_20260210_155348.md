---
ver: rpa2
title: 'FeatInv: Spatially resolved mapping from feature space to input space using
  conditional diffusion models'
arxiv_id: '2505.21032'
source_url: https://arxiv.org/abs/2505.21032
tags:
- feature
- space
- input
- maps
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FeatInv proposes a conditional diffusion model to learn high-fidelity\
  \ probabilistic mappings from spatially resolved feature maps to input space, conditioned\
  \ on the penultimate layer of CNNs/ViTs. This enables interpretable visualization\
  \ of internal representations across diverse architectures (ResNet50, ConvNeXt,\
  \ SwinV2), achieving cosine similarities 0.53 and top-5 matching predictions 94%\
  \ for unpooled features, with FIDs of 8\u201313 indicating realistic samples."
---

# FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models

## Quick Facts
- arXiv ID: 2505.21032
- Source URL: https://arxiv.org/abs/2505.21032
- Reference count: 40
- Primary result: Achieves cosine similarities >0.53 and top-5 matching predictions >94% for unpooled features, with FIDs of 8–13 indicating realistic samples

## Executive Summary
FeatInv introduces a conditional diffusion model to learn high-fidelity probabilistic mappings from spatially resolved feature maps to input space, conditioned on the penultimate layer of CNNs/ViTs. This enables interpretable visualization of internal representations across diverse architectures (ResNet50, ConvNeXt, SwinV2), achieving strong reconstruction quality and robustness to OOD, adversarial, and corrupted data. The method demonstrates significant improvements over prior work using pooled features and supports applications like concept steering visualization and investigations into composite feature space structures.

## Method Summary
FeatInv proposes a conditional diffusion model to learn high-fidelity probabilistic mappings from spatially resolved feature maps to input space, conditioned on the penultimate layer of CNNs/ViTs. This enables interpretable visualization of internal representations across diverse architectures (ResNet50, ConvNeXt, SwinV2), achieving cosine similarities >0.53 and top-5 matching predictions >94% for unpooled features, with FIDs of 8–13 indicating realistic samples. The method outperforms prior work using pooled features and demonstrates robustness to OOD, adversarial, and corrupted data. Applications include concept steering visualization in input space (FeatInv-Viz) and investigations into composite feature space structures via linear interpolations. Limitations include domain specificity to natural images and computational overhead for each model/layer.

## Key Results
- Reconstruction quality: cosine similarities >0.53 and top-5 matching predictions >94% for unpooled features
- Sample realism: FID scores of 8–13 indicating high-quality generated images
- Robustness: strong performance across OOD, adversarial, and corrupted data

## Why This Works (Mechanism)
The method leverages a conditional diffusion model (ControlNet) to map spatially resolved feature maps back to input space, enabling high-fidelity reconstructions. By conditioning on the penultimate layer of CNNs/ViTs, it captures rich spatial information that pooled features lack. The diffusion process, guided by a text prompt ("a high-quality, detailed, and professional image"), generates realistic samples while preserving classification predictions. The conditional encoder (bilinear upsampling + shallow CNN) bridges the gap between feature map dimensions and diffusion model requirements.

## Foundational Learning
- Conditional diffusion models: Needed to learn probabilistic mappings from feature space to input space; Quick check: Verify the diffusion model can generate samples conditioned on feature maps.
- ControlNet architecture: Enables conditioning on spatial feature maps; Quick check: Ensure the ControlNet integrates correctly with the frozen MiniSD backbone.
- Bilinear upsampling: Required to match feature map resolution to diffusion model input; Quick check: Confirm upsampling preserves spatial structure.
- Shallow CNN encoder: Projects feature maps to diffusion model internal space; Quick check: Validate channel dimensions match diffusion model expectations.
- Cosine similarity metric: Measures reconstruction fidelity; Quick check: Compare cosine similarity between original and reconstructed feature maps.
- FID score: Evaluates sample quality; Quick check: Compute FID between generated and real images.

## Architecture Onboarding

Component Map:
MiniSD (frozen) -> ControlNet (trainable) -> Conditional Encoder (bilinear upsampling + shallow CNN) -> Feature Maps (spatially resolved)

Critical Path:
1. Extract penultimate layer feature maps from pretrained backbone
2. Condition ControlNet on feature maps via conditional encoder
3. Generate input space samples using diffusion process

Design Tradeoffs:
- Spatially resolved vs. pooled features: Spatially resolved features retain more information but increase computational complexity.
- Text prompt choice: "a high-quality, detailed, and professional image" balances generality but may not be optimal for all tasks.
- Computational overhead: Training separate models for each architecture/layer limits scalability.

Failure Signatures:
- Using pooled features instead of spatially resolved features results in poor reconstruction (cosine similarity <0.2, FID >30).
- Mismatched feature map dimensions cause runtime errors or degraded performance.
- Suboptimal text prompts may lead to unrealistic samples.

First Experiments:
1. Reproduce unpooled feature mapping results on ImageNet for ResNet50 and verify cosine similarity and FID scores match reported values (>0.53, 8-13).
2. Evaluate robustness by testing reconstruction quality and classification match on ImageNet-A and ImageNet-C.
3. Implement and visualize concept steering using FeatInv-Viz for a target concept (e.g., "add stripes") and assess generated samples.

## Open Questions the Paper Calls Out
None

## Limitations
- Domain specificity: Currently limited to natural images, restricting generalizability to other data modalities.
- Computational overhead: Requires training separate models for each architecture/layer, making large-scale deployment resource-intensive.
- Unspecified architectural details: Exact architecture of the conditional encoder's shallow CNN and model-specific guidance scale values are not fully disclosed.

## Confidence
- Reconstruction quality (cosine similarities >0.53, top-5 matching predictions >94%, FIDs 8-13): High confidence, supported by clear metrics and multiple evaluation criteria.
- Robustness across OOD, adversarial, and corrupted data: Medium confidence, validated with specific datasets and metrics but limited by some missing training/inference details.
- Interpretability via concept steering and feature space structure: Low confidence, demonstrated through case studies but more illustrative than quantitatively rigorous.

## Next Checks
1. Reproduce the unpooled feature mapping results on ImageNet for ResNet50 and verify cosine similarity and FID scores match reported values (>0.53, 8-13).
2. Evaluate the model's robustness by testing reconstruction quality and classification match on ImageNet-A and ImageNet-C and compare with reported metrics.
3. Implement and visualize concept steering using FeatInv-Viz for a target concept (e.g., "add stripes") and assess whether the generated samples consistently incorporate the desired feature while preserving classification.