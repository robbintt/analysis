---
ver: rpa2
title: Develop AI Agents for System Engineering in Factorio
arxiv_id: '2502.01492'
source_url: https://arxiv.org/abs/2502.01492
tags:
- system
- agents
- systems
- engineering
- factorio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that AI agents need dynamic, system-level evaluation
  environments to develop true system engineering capabilities. Static benchmarks
  fail to capture essential skills like managing trade-offs between efficiency and
  adaptability, long-horizon planning, and handling uncertainty.
---

# Develop AI Agents for System Engineering in Factorio

## Quick Facts
- arXiv ID: 2502.01492
- Source URL: https://arxiv.org/abs/2502.01492
- Reference count: 26
- Primary result: AI agents need dynamic, system-level evaluation environments to develop true system engineering capabilities, and Factorio is proposed as an ideal sandbox for this purpose.

## Executive Summary
This paper argues that static benchmarks fail to capture essential system engineering skills like managing trade-offs between efficiency and adaptability, long-horizon planning, and handling uncertainty. Factorio is proposed as an ideal sandbox environment because its core mechanics center on designing, automating, and scaling complex production systems. The game supports automation, provides rich metrics (e.g., science per minute), allows multi-agent interactions, and has extensive modding capabilities. Using Factorio enables researchers to test AI agents' abilities to design robust systems, maintain dynamic equilibrium, and adapt to changing conditions—skills critical for real-world system engineering tasks.

## Method Summary
The paper proposes using Factorio as a dynamic evaluation environment for AI agents, suggesting an Agent-Evaluator framework where evaluators inject perturbations while agents maintain factory operations. The method involves building programmatic API interfaces for agent control, implementing perception pipelines for GUI and visual state processing, and designing memory architectures for long-horizon episodes spanning tens to hundreds of hours. Evaluation focuses on Science Per Minute (SPM) metrics while testing agents' ability to balance System 3 (efficiency) and System 4 (adaptability) capabilities through hierarchical planning and feedback incorporation.

## Key Results
- Static benchmarks fail to capture crucial system engineering skills like managing uncertain trade-offs and ensuring proactive adaptability
- Factorio operationalizes the Law of Requisite Variety by requiring agents to possess internal complexity matching environmental complexity
- Multi-agent and Agent-Evaluator frameworks extend evaluation to test coordination, competition, and adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic evaluation environments expose capabilities that static benchmarks cannot measure
- Mechanism: Static benchmarks evaluate single-pass problem-solving without requiring agents to maintain system stability over time. Dynamic environments introduce feedback loops where early decisions constrain later options, forcing agents to balance present efficiency against future adaptability—a core tension described in the Viable System Model (Systems 3 vs. 4)
- Core assumption: Real-world system engineering requires maintaining dynamic equilibrium, which can only emerge under conditions where the environment changes over time and early decisions have compounding effects
- Evidence anchors: [abstract] "static benchmarks dominating agent evaluations today fail to capture the crucial skills required for implementing dynamic systems, such as managing uncertain trade-offs and ensuring proactive adaptability" [section 3.1] "the longevity and effectiveness of a system fundamentally depend on its capacity to assimilate feedback and adapt to inevitable changes"

### Mechanism 2
- Claim: Automation-oriented sandbox games operationalize the Law of Requisite Variety for agent evaluation
- Mechanism: Ashby's Law states that systems must possess internal variety (complexity) matching environmental variety to remain viable. Factorio enforces this through escalating tech trees, resource constraints, and external disruptions (e.g., Biters attacking in response to pollution). Agents must continuously expand their internal repertoire to handle new environmental demands
- Core assumption: The game's abstraction level captures the essential reasoning patterns of real-world system engineering without requiring full physical fidelity
- Evidence anchors: [section 3.2] "systems must possess enough complexity (known as variety) internally to handle the complexity of potential external disruptions" [section 4.1] "SPM makes for a great summary benchmark metric, with human novice bases at ~0-30 SPM, intermediate at ~30-200 SPM and advanced bases at ~200-1000+ SPM"

### Mechanism 3
- Claim: Multi-agent and Agent-Evaluator frameworks extend single-agent evaluation to test coordination, competition, and adversarial robustness
- Mechanism: Factorio's multiplayer support and modding API enable configurations where one agent (evaluator) introduces perturbations—supply shortages, power failures, market fluctuations—while another agent must maintain factory operations. This creates co-adaptive dynamics that test both System 3 (present optimization) and System 4 (future planning) capabilities simultaneously
- Core assumption: Multi-agent dynamics in Factorio sufficiently approximate real-world coordination challenges (e.g., supply chain negotiation, hierarchical task delegation)
- Evidence anchors: [section 4.3] "introducing dynamic markets would allow agents to buy and sell resources, negotiate prices, and even form alliances or contracts—key elements of real-world logistics and supply chains"

## Foundational Learning

- Concept: **Viable System Model (VSM)**
  - Why needed here: The paper maps Factorio gameplay directly to VSM levels—System 1 (assemblers), System 2 (belts/splitters), System 3 (resource monitoring), System 4 (research planning), System 5 (mission definition). Understanding this hierarchy is essential for designing evaluation protocols that test specific system engineering capabilities
  - Quick check question: Can you explain why System 3 and System 4 are in tension, and how Factorio's tech tree creates this tension?

- Concept: **Law of Requisite Variety**
  - Why needed here: This is the theoretical justification for why dynamic environments are necessary. It predicts that agents will fail when environmental complexity exceeds their internal response repertoire—a failure mode static benchmarks cannot detect
  - Quick check question: Given a Factorio base producing 100 SPM, what environmental changes could push it outside its viable state space?

- Concept: **Long-Horizon Planning with Feedback Delays**
  - Why needed here: Factorio bases develop over tens to hundreds of hours. Agents must make decisions whose consequences materialize much later, while continuously incorporating feedback. This differs fundamentally from single-turn benchmark tasks
  - Quick check question: How would you design a memory architecture that retains relevant information from hour 5 when making decisions at hour 50?

## Architecture Onboarding

- Component map: Interface layer -> Perception module -> Memory system -> Planning module -> Execution layer -> Evaluation harness
- Critical path: 1. Build API layer for agent control (analogous to Mineflayer for Minecraft) 2. Implement perception pipeline for GUI + visual state 3. Design memory architecture for 50-100+ hour episodes 4. Create evaluation scenarios with defined SPM targets and perturbation schedules
- Design tradeoffs:
  - Reasoning time vs. real-time action: More deliberation improves planning but delays responses in a real-time environment
  - Abstraction level: Using game's native GUI vs. custom observation/action spaces—native interfaces test real-world capability but increase complexity
  - Episode length: Shorter episodes enable faster iteration but may not test long-horizon planning; longer episodes are more realistic but computationally expensive
- Failure signatures:
  - Agent produces "spaghetti" factories that cannot scale (over-optimized for immediate efficiency, System 3-dominant)
  - Agent continually rewrites plans without executing (System 4-dominant, analysis paralysis)
  - Agent fails to respond to Biters or resource shortages (insufficient requisite variety)
  - Memory collapse: agent "forgets" earlier factory sections and creates incompatible expansions
- First 3 experiments:
  1. Establish baseline: Run existing frontier agents on Factorio with native keyboard/mouse interface; measure maximum SPM achieved and time-to-first-bottleneck
  2. Isolate VSM levels: Create constrained scenarios testing single levels (e.g., pure resource routing for System 2, tech tree path selection for System 4)
  3. Perturbation response: Introduce controlled disruptions (power outage, Biter wave, resource depletion) and measure recovery time and strategy adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can frontier AI agents autonomously construct a functional API layer to interact with the game, and how does this compare to using native mouse-and-keyboard interfaces?
- Basis in paper: [explicit] Section 4.4 suggests that building a control API "could even be a task for an AI agent to perform," while Section 4.2 highlights the difficulty of processing the "traditional keyboard-and-mouse interface" with "numerous GUI windows"
- Why unresolved: Current agents struggle with high-bandwidth visual inputs and complex GUIs; it is unknown if agents can generate the code required to bridge their own reasoning processes with the game's internal state
- Evidence: A comparison of success rates and throughput (SPM) between agents forced to use the visual GUI versus those that successfully generate and utilize a custom API

### Open Question 2
- Question: How can we effectively evaluate an agent's ability to maintain "dynamic equilibrium" between efficiency (System 3) and adaptability (System 4) when facing open-ended disruptions?
- Basis in paper: [explicit] Section 3.2 and Section 3.4 argue that maintaining dynamic equilibrium is a "persistent challenge" and that static benchmarks fail to measure the trade-off between "System 3" (present optimization) and "System 4" (future planning)
- Why unresolved: Current benchmarks like SWE-bench are static; there is no standard metric for quantifying how well an agent balances immediate production goals against the need to refactor systems for future resilience
- Evidence: Developing a scenario where agents must optimize for Science per Minute (SPM) while subjected to random "evaluator" perturbations (e.g., resource shortages, attacks) that punish rigid, non-adaptive designs

### Open Question 3
- Question: What emergent cooperative or competitive behaviors arise when agents interact within a Factorio-based economy involving dynamic pricing and trading?
- Basis in paper: [explicit] Section 4.3 proposes that "introducing dynamic markets would allow agents to buy and sell resources, negotiate prices, and even form alliances," offering a testbed for "cooperative and competitive strategies"
- Why unresolved: While multi-agent research exists, the combination of complex supply chain logistics with economic negotiation in a continuous, long-horizon environment has not been thoroughly explored
- Evidence: Analysis of negotiation logs and resource distribution efficiency in a multiplayer server populated by agents using mods like "Megablackmarket" or "Diplomacy"

## Limitations
- The paper presents Factorio as a promising evaluation environment but does not validate its effectiveness empirically
- No baseline results or quantitative validation are provided to support claims about Factorio's superiority over other sandbox environments
- Key uncertainties include whether Factorio's abstraction level genuinely captures real-world system engineering complexity and whether agents can transfer skills to practical applications

## Confidence

- **High confidence**: Factorio's suitability as a rich, dynamic environment for testing system engineering skills. The game's mechanics (automation, scalability, feedback loops) align well with VSM principles, and its modding capabilities enable customizable evaluation scenarios
- **Medium confidence**: The theoretical argument that static benchmarks fail to capture essential system engineering capabilities like long-horizon planning and adaptability
- **Low confidence**: Claims about Factorio's effectiveness as a research platform relative to other sandbox environments (e.g., Minecraft, WebArena). No comparative analysis or baseline results are provided

## Next Checks

1. **Empirical Transfer Test**: Train frontier agents (e.g., Claude, GPT-4) on Factorio and evaluate whether skills transfer to real-world system tasks (e.g., optimizing supply chains, debugging software architectures). Measure performance gap between in-game and real-world outcomes.

2. **Perturbation Robustness Analysis**: Implement the Agent-Evaluator framework and test whether it distinguishes between agents with genuine adaptability versus those exploiting game mechanics. Compare evaluation signal quality against static benchmarks.

3. **Abstraction Fidelity Study**: Analyze whether Factorio's complexity requirements (via LRV) map to real-world system variety. Conduct ablation studies varying game parameters (tech tree depth, disruption frequency) to identify the minimal complexity needed for meaningful evaluation.