---
ver: rpa2
title: 'Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces'
arxiv_id: '2506.07903'
source_url: https://arxiv.org/abs/2506.07903
tags:
- diffusion
- multimodal
- data
- generation
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a general framework for building multimodal
  diffusion models on arbitrary state spaces without relying on external preprocessing
  protocols like tokenizers or variational autoencoders. The core method introduces
  a decoupled noise schedule for each modality, enabling both unconditional and modality-conditioned
  generation within a single model simultaneously.
---

# Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces

## Quick Facts
- arXiv ID: 2506.07903
- Source URL: https://arxiv.org/abs/2506.07903
- Reference count: 40
- Primary result: Achieves competitive text-image generation without pre-trained models or tokenizers using native diffusion on arbitrary state spaces

## Executive Summary
This paper proposes a general framework for building multimodal diffusion models on arbitrary state spaces without relying on external preprocessing protocols like tokenizers or variational autoencoders. The core method introduces a decoupled noise schedule for each modality, enabling both unconditional and modality-conditioned generation within a single model simultaneously. By combining native diffusion models designed for each data modality, the authors derive a unified learning objective and theoretically justify score learning under multiple time variables. The framework is validated on text-image generation and mixed-type tabular data synthesis, achieving competitive performance with more parameter-efficient models and without using pre-trained models or powerful extra encoders.

## Method Summary
The method introduces decoupled noise schedules for each modality, allowing independent noising levels across different data types. The framework combines continuous stochastic differential equations (SDEs) for continuous modalities and continuous-time Markov chains (CTMCs) for discrete modalities within a unified score matching framework. Training follows a three-stage process: joint embedding alignment, unimodal prediction, and full multimodal training. The approach enables versatile generation modes including joint generation, text-to-image, and image-to-text within a single model architecture without requiring pre-trained components.

## Key Results
- Achieves competitive FID-30K scores on text-to-image generation using SAM-LLaVA dataset without pre-trained models
- Demonstrates successful multimodal generation on mixed-type tabular data (categorical + continuous)
- Shows improved generation quality using proposed noisy guidance mechanism over standard classifier-free guidance

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Noise Schedules
Assigning independent time variables to each modality allows a single model to switch between joint generation and conditional generation. By manipulating $t_i$, users can set specific modalities to $t=0$ (clean data, acting as condition) and others to $t=T$ (pure noise, to be generated). This allows each modality to be noised asynchronously.

### Mechanism 2: Factorization of Multimodal Scores
The multimodal learning objective decomposes into a sum of standard unimodal loss terms, avoiding complex joint score estimations. Theoretically, the gradient of the joint log-density simplifies to the sum of individual unimodal conditional scores under conditional independence assumptions.

### Mechanism 3: Noisy Guidance
Replacing standard unconditional score in classifier-free guidance with a partially noised conditional score improves sample quality and diversity trade-offs. This interpolates between scores conditioned on clean data and partially noised data, acting as a smoother corrector than raw unconditional distribution.

## Foundational Learning

- **Stochastic Differential Equations (SDEs) & Backward SDEs**
  - Why needed: Continuous modalities (images, numerical data) are modeled as SDEs. Understanding drift $f$ and diffusion $g$ terms is required to implement forward noising and backward denoising samplers.
  - Quick check: Can you derive the reverse-time SDE drift term given a forward Ornstein-Uhlenbeck process?

- **Continuous-Time Markov Chains (CTMCs) & Rate Matrices**
  - Why needed: Discrete modalities (text, categorical data) are modeled as CTMCs defined by rate matrices $Q_t$. The backward process requires constructing the reverse rate matrix $\bar{Q}$.
  - Quick check: How does the stationary distribution of a CTMC influence the initialization of the discrete diffusion process?

- **Score Matching & Denoising Score Matching**
  - Why needed: The core training objective relies on matching the score (gradient of log-density) of perturbed data distributions. You must distinguish between explicit score matching and denoising score matching.
  - Quick check: Why does denoising score matching avoid the partition function calculation required in explicit score matching?

## Architecture Onboarding

- **Component map:** Input Layer (modality-specific encoders) -> Backbone (multimodal Transformer) -> Output Layer (unimodal heads)
- **Critical path:** Data Prep (sample time pair) -> Noise Injection (apply SDE/CTMC noise) -> Forward Pass (concatenate embeddings + time encodings) -> Loss (compute weighted sum of continuous and discrete losses)
- **Design tradeoffs:** Decoupled time enables versatile generation but complicates training pipeline; operating on native data avoids VAE artifacts but increases computational cost
- **Failure signatures:** Text incoherence without pre-trained text encoder; modality dominance when continuous loss overpowers discrete loss; guidance collapse with incorrect noise level $\sigma$
- **First 3 experiments:**
  1. Sanity Check (Unimodal): Train on single modality using proposed framework to match standard baselines
  2. Tabular Synthesis: Validate "Arbitrary State Space" claim on mixed-type tabular data
  3. Noisy Guidance Ablation: Implement text-to-image generation with and without proposed guidance

## Open Questions the Paper Calls Out

- Can pretrained unimodal diffusion models be effectively utilized as initialization to boost training efficiency? The paper trained from scratch without exploring transfer learning potential.
- How can sampling efficiency be improved by aligning the order of discrete and continuous solvers? Current implementation uses mismatched first-order and second-order solvers.
- Does noisy guidance provide consistent quality improvements across non-Euclidean modalities like Riemannian manifolds? Current validation is restricted to text-image generation.

## Limitations

- The decoupled noise schedule assumption of conditional independence across modalities may not hold in practice, potentially degrading unified conditional generation capability
- The discrete score entropy loss implementation details are not fully specified, making exact reproduction challenging
- Noisy guidance effectiveness is highly sensitive to hyperparameter choices, particularly the noise level $\sigma$, with limited systematic exploration

## Confidence

- **High Confidence:** Unimodal diffusion implementations (continuous SDE and discrete CTMC) are standard and well-established; three-stage training strategy is clearly specified
- **Medium Confidence:** Theoretical framework for decoupled noise schedules and score factorization is mathematically rigorous but practical implementation challenges exist
- **Low Confidence:** Noisy guidance mechanism's performance is highly sensitive to hyperparameter choices with limited systematic exploration

## Next Checks

1. **Conditional Independence Stress Test:** Systematically vary semantic coupling between modalities to test robustness of decoupled noise schedule mechanism and measure degradation when conditional independence assumption is violated

2. **Discrete Score Entropy Implementation:** Replicate discrete score matching objective with varying entropy term implementations and compare against baseline using only score matching loss to isolate component contribution

3. **Noisy Guidance Parameter Sensitivity:** Conduct systematic grid search over guidance noise level $\sigma$ and interval $[s_{min}, s_{max}]$ for multiple datasets beyond text-image generation to quantify performance variance and establish practical guidelines