---
ver: rpa2
title: Can You Trick the Grader? Adversarial Persuasion of LLM Judges
arxiv_id: '2508.07805'
source_url: https://arxiv.org/abs/2508.07805
tags:
- solution
- arxiv
- persuasive
- persuasion
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that large language models (LLMs) can be systematically
  biased by persuasive language embedded in mathematical solutions, despite correctness
  being independent of rhetorical style. Grounded in Aristotle's rhetorical framework,
  we formalized seven persuasion techniques (Majority, Consistency, Flattery, Reciprocity,
  Pity, Authority, Identity) and embedded them into otherwise identical incorrect
  solutions.
---

# Can You Trick the Grader? Adversarial Persuasion of LLM Judges

## Quick Facts
- **arXiv ID:** 2508.07805
- **Source URL:** https://arxiv.org/abs/2508.07805
- **Reference count:** 30
- **Primary result:** Large language models can be systematically biased by persuasive language embedded in mathematical solutions, despite correctness being independent of rhetorical style.

## Executive Summary
This study reveals that large language models (LLMs) can be systematically biased by persuasive language embedded in mathematical solutions, despite correctness being independent of rhetorical style. Grounded in Aristotle's rhetorical framework, the authors formalized seven persuasion techniques and embedded them into otherwise identical incorrect solutions. Across six math benchmarks, all 14 tested LLM judges assigned inflated scores to flawed solutions when persuasion cues were present, with Consistency proving most effective (+3.55% average score increase). Increasing model size did not reduce vulnerability, and combining multiple techniques amplified the bias. Even pairwise evaluation settings and counter-prompting strategies failed to fully mitigate the effect. These findings expose a critical vulnerability in LLM-as-a-Judge pipelines, demonstrating that automated evaluators remain susceptible to adversarial persuasion despite task objectivity.

## Method Summary
The study uses six math benchmarks (MATH, GSM8k, MathQA, MMLU, AMC, SVAMP) with faulty solutions generated by GPT-4o containing computational, logical, or symbolic errors. Seven persuasion techniques (Majority, Consistency, Flattery, Reciprocity, Pity, Authority, Identity) with five templates each are prepended to incorrect solutions. The solutions are then evaluated by 14 LLM judges (GPT-3.5/4o variants, Qwen2/2.5, LLaMA 3.x) using a 0-5 scoring prompt at temperature=0, with 3-run averaging for closed-source models. The primary metric is score change percentage between baseline and persuasion-enhanced solutions.

## Key Results
- All 14 tested LLM judges assigned inflated scores to flawed solutions when persuasion cues were present
- Consistency technique proved most effective (+3.55% average score increase across benchmarks)
- Increasing model size did not reduce vulnerability; larger models were more susceptible
- Combining multiple persuasion techniques amplified the bias effect
- Counter-prompting strategies failed to fully mitigate the effect

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Appeals to logical coherence (Consistency) bias LLM judges more effectively than emotional appeals in objective tasks.
- **Mechanism:** The Consistency technique simulates alignment with the model's perceived prior validations, exploiting a potential preference for internal semantic coherence over independent error detection.
- **Core assumption:** LLMs assign higher probability to outputs that maintain narrative or logical self-consistency with the prompt's framing, even if the factual content is wrong.
- **Evidence anchors:** Abstract states "Consistency causing the most severe distortion"; Table 2 shows Consistency yielding the highest average score increase (+3.55%).

### Mechanism 2
- **Claim:** Embedding persuasive cues directly into the evaluated content bypasses standard safety filters.
- **Mechanism:** Unlike instruction attacks, this modifies the user content, causing the judge to process the rhetorical wrapper as part of "answer quality" rather than separating stylistic persuasion from logical correctness.
- **Core assumption:** The attention mechanism attends to the persuasive preamble with sufficient weight to skew the final classification token.
- **Evidence anchors:** Abstract notes "Strategically embedded persuasive language... into otherwise identical responses"; Section 2.1 contrasts with prior work on instruction-level manipulation.

### Mechanism 3
- **Claim:** Increasing model scale correlates with increased susceptibility to persuasion (the Capability Trap).
- **Mechanism:** Larger models possess superior language comprehension, allowing them to better understand and thus be influenced by the nuance of persuasive rhetoric.
- **Core assumption:** Better reasoning capabilities imply better ability to process—and be deceived by—sophisticated arguments.
- **Evidence anchors:** Abstract states "Increasing model size did not reduce vulnerability"; Section 6 notes LLaMA 70B is more vulnerable than 8B, and GPT-4o more than GPT-4o mini.

## Foundational Learning

- **Concept:** **LLM-as-a-Judge Paradigm**
  - **Why needed here:** This is the target domain. You must understand that LLMs are now acting as automated graders (0-5 score) for open-ended text to grasp the stakes of the vulnerability.
  - **Quick check question:** Does the model grade based on a reference answer or by evaluating the candidate solution's internal logic?

- **Concept:** **Aristotle's Rhetorical Framework (Logos/Pathos/Ethos)**
  - **Why needed here:** The paper operationalizes persuasion using these three modes (e.g., Consistency = Logos, Flattery = Pathos). Mapping techniques to these categories helps predict which bias affects which model.
  - **Quick check question:** Which category does the "Authority" technique (citing an expert) fall under?

- **Concept:** **Prompt Injection vs. Context Contamination**
  - **Why needed here:** Distinguishing between hacking the instructions and hacking the data is crucial. This paper focuses on contaminating the input data (the solution) to skew the output.
  - **Quick check question:** If you strip the prompt instructions away, does the bias still exist if it lives in the solution text?

## Architecture Onboarding

- **Component map:** Benchmarks (MATH, GSM8k) -> Solver (GPT-4o generating incorrect solutions) -> Injector (heuristic templates adding rhetorical cues) -> Judge (target LLM scoring 0-5)

- **Critical path:** The interaction between the Injector's specific technique (Consistency) and the Judge's inability to decouple rhetorical style from mathematical correctness.

- **Design tradeoffs:**
  - **Math vs. Open-Ended:** The paper uses Math (objective) to prove the bias exists; in subjective tasks, this bias would be invisible/impossible to measure.
  - **Single vs. Pairwise:** Pairwise comparison is often more stable than single scoring, but the paper shows persuasion still biases pairwise comparisons.

- **Failure signatures:**
  - **Score Inflation:** Incorrect solutions receiving scores >3.0 consistently
  - **CoT Amplification:** Chain-of-Thought prompting increases the bias, suggesting the model uses persuasion to "justify" a higher score

- **First 3 experiments:**
  1. Baseline Consistency Check: Run the "Consistency" template on 50 incorrect math solutions with your target judge. Measure the delta between "Original" and "Consistency" scores.
  2. Scale Test: Compare the attack success rate of your smallest model vs. largest model. If the larger model is more susceptible, you have confirmed the "Capability Trap."
  3. Defense Ablation: Test "Direct Prompting" (telling the model to ignore persuasion) vs. "Blind Grading" (stripping the first/last sentence of the solution) to see if the bias is robust to simple filters.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do persuasion vulnerabilities persist in subjective evaluation domains where "correctness" is not strictly defined?
- **Basis in paper:** The authors state in the Limitations section that "future research could examine whether similar persuasive effects arise in other practical domains, such as AI-assisted hiring," noting the current study is restricted to mathematical solutions.
- **Why unresolved:** The study relies on math benchmarks where correctness is objective; it is unclear if rhetorical cues have the same or greater impact on tasks like creative writing or summarization.
- **What evidence would resolve it:** Replicating the seven persuasion techniques on subjective benchmarks (e.g., MT-Bench or hiring datasets) and measuring score inflation.

### Open Question 2
- **Question:** Can fine-tuning explicitly immunize LLM judges against rhetorical manipulation?
- **Basis in paper:** The authors note that they "do not explore whether LLM judges can be explicitly trained or fine-tuned to detect and discount these strategies," despite testing prompting defenses.
- **Why unresolved:** The paper demonstrates that counter-prompting (Direct and CoT) fails to mitigate bias, but does not test if model weights can be updated to recognize and resist these specific persuasion cues.
- **What evidence would resolve it:** Fine-tuning a judge model on a dataset containing adversarial persuasion examples labeled as "manipulative," then re-evaluating robustness.

### Open Question 3
- **Question:** Why does increasing model size correlate with higher susceptibility to persuasion?
- **Basis in paper:** Section 6 (Takeaway 3) observes that larger models (e.g., LLaMA 70B) are more vulnerable than smaller ones, which contrasts with robustness trends for other biases. The authors hypothesize that advanced "linguistic and cognitive capacities" increase influence but do not verify the mechanism.
- **Why unresolved:** It is unknown if the increased susceptibility is due to better comprehension of the rhetorical nuances or an emergent "sycophancy" effect in high-capacity models.
- **What evidence would resolve it:** Mechanistic interpretability analysis (e.g., probing specific attention heads) to determine if larger models assign higher weights to rhetorical style tokens over logical reasoning tokens.

## Limitations

- The study's findings are limited to mathematical domains where correctness is objectively defined, and may not generalize to subjective evaluation tasks.
- The effectiveness of persuasion techniques may depend on specific mathematical contexts tested, and the ranking of techniques warrants additional validation.
- The claim that CoT prompting increases vulnerability shows substantial variance across benchmarks and the mechanism remains unclear.

## Confidence

- **High Confidence:** The core finding that persuasive language systematically inflates scores for incorrect mathematical solutions is well-supported by consistent effect sizes across multiple benchmarks and models.
- **Medium Confidence:** The ranking of persuasion techniques shows statistical significance but may depend on specific mathematical contexts tested.
- **Low Confidence:** The claim that CoT prompting increases vulnerability requires more systematic exploration due to substantial variance across benchmarks.

## Next Checks

1. **Cross-Domain Transfer Test:** Apply the seven persuasion techniques to incorrect solutions in physics or chemistry problems to verify whether Consistency maintains its dominance outside pure mathematics.

2. **Counter-Prompting Efficacy Test:** Systematically evaluate the effectiveness of explicit instructions ("ignore preamble," "focus only on mathematical correctness") across different model families and temperature settings.

3. **Attention Analysis:** Use integrated gradients or attention visualization to confirm whether persuasive preambles receive disproportionate weight in the scoring decision, particularly for the Consistency technique.