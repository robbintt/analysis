---
ver: rpa2
title: 'Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust
  Balanced Accuracy in Imbalanced Learning'
arxiv_id: '2509.02592'
source_url: https://arxiv.org/abs/2509.02592
tags:
- data
- threshold
- accuracy
- balanced
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Group-aware threshold calibration outperforms synthetic data augmentation
  methods for handling class imbalance while improving fairness across demographic
  groups. The approach sets different decision thresholds for different protected
  groups rather than applying a single threshold across all data.
---

# Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning

## Quick Facts
- arXiv ID: 2509.02592
- Source URL: https://arxiv.org/abs/2509.02592
- Authors: Hunter Gittlin
- Reference count: 23
- Primary result: Group-aware thresholds achieve 1.5-4% higher balanced accuracy than SMOTE/CT-GAN augmentation while improving worst-group performance

## Executive Summary
Group-aware threshold calibration outperforms synthetic data augmentation methods for handling class imbalance while improving fairness across demographic groups. The approach sets different decision thresholds for different protected groups rather than applying a single threshold across all data. Experiments on two financial benchmark datasets show that group-specific thresholds achieve 1.5-4% higher balanced accuracy than SMOTE and CT-GAN augmented models while also improving worst-group balanced accuracy. Critically, applying group thresholds to synthetically augmented data yields minimal additional benefit, suggesting these approaches are fundamentally redundant. This demonstrates that group-aware threshold calibration offers a simpler, more interpretable, and more effective solution to class imbalance that directly optimizes the Pareto frontier between balanced accuracy and worst-group performance.

## Method Summary
The method trains standard models on original imbalanced data, then learns separate decision thresholds for each demographic group using a validation set. After model training, a grid search finds thresholds τ_g per group g that maximize either overall balanced accuracy (Fair-BalAcc) or worst-group balanced accuracy (Fair-MinBalAcc). During inference, each instance uses its group's specific threshold to classify probabilities. The approach spans seven model families including linear, tree-based, instance-based, and boosting methods, confirming broad applicability. Crucially, the method requires only probability outputs and group membership information at inference time.

## Key Results
- Group-aware thresholds achieve 1.5-4% higher balanced accuracy than SMOTE and CT-GAN augmented models on financial benchmark datasets
- Applying group thresholds to synthetically augmented data yields minimal additional benefit (0.009 BA gain vs. 0.044 on original data), indicating redundancy
- The method improves worst-group balanced accuracy while maintaining or improving overall performance
- Threshold optimization directly maximizes the target metric, avoiding the proxy optimization inherent in synthetic augmentation approaches

## Why This Works (Mechanism)

### Mechanism 1
Group-aware thresholds outperform synthetic augmentation because they directly optimize the target metric rather than relying on proxy improvements. Threshold calibration searches over group-specific cutoffs to maximize balanced accuracy explicitly. Synthetic methods instead modify training distributions hoping this indirectly improves classification—introducing an extra optimization layer that can degrade rather than help. The relationship between training data distribution and final classification performance is not monotonic; balancing classes does not guarantee better decision boundaries.

### Mechanism 2
Synthetic augmentation and threshold optimization address the same underlying problem through different mechanisms, making them largely redundant when combined. Both approaches shift effective decision boundaries—sampling alters class priors in training, while threshold-moving adjusts boundaries at inference. Theoretical work shows these are equivalent optimizations; empirically, combining them yields diminishing returns. The classifier learns sufficient signal from the original imbalanced data to enable meaningful probability ranking across groups.

### Mechanism 3
Different demographic groups often have different optimal decision thresholds due to varying base rates or feature distributions. A single global threshold minimizes a global loss but can systematically disadvantage groups whose score distributions differ. Group-specific thresholds τ_g enable independent control of each group's true positive and false positive rates, moving along the Pareto frontier between overall and worst-group performance. This requires protected attribute information to be available and legally/ethically permissible to use at inference time.

## Foundational Learning

- **Concept: Balanced Accuracy vs. Accuracy**
  - Why needed here: The paper uses balanced accuracy (average of TPR and TNR) as the primary metric because standard accuracy is misleading under imbalance—a 95% negative class can achieve 95% accuracy while missing all positives.
  - Quick check question: If your classifier predicts "negative" for every instance in a 95/5 imbalanced dataset, what is its accuracy? What is its balanced accuracy?

- **Concept: Decision Threshold Calibration**
  - Why needed here: The core intervention is moving from default 0.5 threshold to learned group-specific thresholds τ_g. Understanding that classifier outputs are probabilities requiring thresholding is essential.
  - Quick check question: A logistic regression outputs p=0.3 for a positive-class instance. With threshold τ=0.5, how is it classified? With τ=0.2?

- **Concept: Pareto Frontier in Fairness-Accuracy Tradeoffs**
  - Why needed here: The paper claims group-aware thresholds "optimize the Pareto frontier" between balanced accuracy and worst-group balanced accuracy—you cannot always improve both simultaneously.
  - Quick check question: If improving worst-group accuracy from 0.65 to 0.70 reduces overall accuracy from 0.80 to 0.78, is this a Pareto improvement?

## Architecture Onboarding

- **Component map:** Model training on original data → probability extraction → validation set split → grid search for τ_g per group → inference with group-specific thresholds
- **Critical path:** 1) Train model on original data (no augmentation) 2) Extract predicted probabilities on validation set 3) Run grid search over group-specific thresholds 4) Evaluate on held-out test set using both BA and WG-BA 5) Compare against SMOTE/CT-GAN baselines
- **Design tradeoffs:** Fair-BalAcc vs. Fair-MinBalAcc: Optimizing overall BA may leave some groups behind; optimizing worst-group BA may sacrifice aggregate performance. Threshold granularity: Binary protected attribute → two thresholds; multi-group or intersectional attributes → combinatorial growth in thresholds and data requirements per group. Legal/ethical: Using protected attributes at inference requires regulatory justification.
- **Failure signatures:** Thresholds collapse to extreme values (0.0 or 1.0): indicates poor probability calibration or insufficient validation data per group. Large gap between BA and WG-BA after thresholding: suggests fundamental model bias not addressable by thresholds alone. High variance across cross-validation folds: threshold selection is unstable; increase validation set size or reduce threshold granularity.
- **First 3 experiments:** 1) Baseline replication: Train logistic regression, compute BA/WG-BA with default 0.5 threshold, then implement group-aware threshold search and compare against SMOTE-augmented baseline. 2) Redundancy test: Apply group-aware thresholds to SMOTE-augmented data and verify minimal gains. 3) Objective comparison: Run both Fair-BalAcc and Fair-MinBalAcc threshold optimization on same model and plot tradeoff between overall BA and worst-group BA.

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed redundancy of synthetic augmentation persist in datasets with extreme class imbalance (e.g., fraud detection with <1% positive rate), or do synthetic methods become necessary when minority class data is severely limited? The authors note their datasets had moderate imbalance ratios (approximately 4:1) and explicitly state that "extreme imbalance might benefit more from synthetic approaches." This remains untested in the current study.

### Open Question 2
How does group-aware threshold calibration perform in multi-class classification settings or when multiple protected attributes intersect (e.g., race and sex simultaneously)? The authors list "extension to multi-class and multi-label settings" and "handling multiple intersecting protected attributes" as necessary future work to broaden applicability. The study restricted experiments to binary classification with single binary protected attributes.

### Open Question 3
Under what specific theoretical data distribution conditions does synthetic data generation outperform threshold optimization? The paper calls for "theoretical analysis of when synthetic methods might outperform threshold optimization, perhaps in extreme imbalance scenarios or with specific data characteristics." The empirical results show threshold calibration dominates, but lack a theoretical framework explaining why synthetic methods fail or if there exists a data manifold where they are superior.

## Limitations
- The method assumes group membership is available at inference time, which may not be legally or practically feasible in all scenarios
- Performance under extreme imbalance (beyond the 22-25% positive rates tested) remains unclear, as synthetic augmentation might provide complementary benefits when minority class signal is extremely sparse
- The threshold optimization relies on having sufficient validation data per group to enable stable grid search, which could fail with very small group sizes or intersectional attributes

## Confidence

- **High confidence:** The fundamental claim that group-aware thresholds directly optimize the target metric while synthetic augmentation provides only indirect improvements
- **Medium confidence:** The generalization claim across seven model families, though hyperparameter settings were not fully specified
- **Medium confidence:** The Pareto frontier optimization claim, as comprehensive multi-objective optimization analysis was not provided

## Next Checks

1. **Legal/ethical boundary test:** Design a scenario where group attributes are unavailable at inference and evaluate whether proxy methods (e.g., using predicted group membership from other features) maintain performance or introduce bias.

2. **Extreme imbalance stress test:** Apply the method to datasets with <5% positive rates or heavily skewed group distributions to identify the breaking point where synthetic augmentation becomes necessary rather than redundant.

3. **Threshold stability analysis:** Perform repeated cross-validation with different random seeds to quantify the variance in learned thresholds and identify conditions under which threshold selection becomes unstable or overfits the validation set.