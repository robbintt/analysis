---
ver: rpa2
title: 'SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments'
arxiv_id: '2601.09750'
source_url: https://arxiv.org/abs/2601.09750
tags:
- tool
- tools
- calls
- opaca
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces SAGE, a system for dynamically integrating\
  \ LLM tools via the OPACA framework, enabling real-time access to evolving services.\
  \ SAGE supports multiple prompting strategies\u2014Simple, Simple-Tools, Tool-Chain,\
  \ and Orchestration\u2014each varying in complexity and model independence."
---

# SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments

## Quick Facts
- arXiv ID: 2601.09750
- Source URL: https://arxiv.org/abs/2601.09750
- Reference count: 20
- Multi-agent LLM system with dynamic tool integration achieves 4.16-4.72/5.0 response scores

## Executive Summary
SAGE introduces a system for dynamically integrating LLM tools via the OPACA framework, enabling real-time access to evolving services without model retraining. The system supports four prompting strategies—Simple, Simple-Tools, Tool-Chain, and Orchestration—each varying in complexity and model independence. Evaluated against a custom benchmark of 102 actions across 15 agents, the methods achieved strong response scores (4.16-4.72/5.0) with open-source models performing comparably to GPT-4o-mini in multi-tool tasks.

## Method Summary
SAGE dynamically discovers available agent actions from the OPACA Runtime Platform via REST API and converts them to LLM-compatible tool schemas. Four prompting strategies are implemented: Simple (single LLM, JSON-parsing), Simple-Tools (native tool calling with parallel support), Tool-Chain (specialized modules for tool selection and evaluation), and Orchestration (hierarchical tool abstraction via agent grouping). The system routes user queries through selected methods, validates tool parameters, invokes OPACA actions, and iterates until evaluation passes, then streams final responses to a Vue.js frontend.

## Key Results
- Response scores: 4.16-4.72/5.0 across all methods, with Tool-Chain achieving highest (4.72)
- Correct tool usage: 85-95% for Simple-Tools and Tool-Chain methods
- Token efficiency: Orchestration uses lowest tokens (2.557M) but has highest latency (14.26s)
- Open-source models (Mistral-Small-24B + Qwen2.5-32B) perform comparably to GPT-4o-mini in multi-tool tasks

## Why This Works (Mechanism)

### Mechanism 1
Dynamic tool discovery enables real-time integration of evolving services without model retraining. SAGE queries the OPACA Runtime Platform's REST API to acquire available agent actions at inference time, converts them to LLM-compatible tool schemas, and injects them into the prompt. This zero-shot approach avoids fine-tuning while maintaining current tool state. Core assumption: Tools are self-descriptive with adequate natural-language descriptions and typed parameters.

### Mechanism 2
Role-specialized module decomposition improves tool-calling accuracy and response quality over monolithic approaches. Tool-Chain separates concerns—Generator selects/formulates tool calls, Evaluator judges sufficiency and summarizes results. This allows each module to use optimized prompts; the Evaluator notably uses user-message prompting (no system prompt) for better summarization performance. Core assumption: Specialized prompts outperform single-prompt approaches for multi-step reasoning.

### Mechanism 3
Hierarchical tool abstraction via agent grouping enables scaling beyond LLM tool-context limits. Orchestration's Orchestrator module sees only agent names/descriptions, routes subtasks to Agent-Trios, which receive full tool specs only for their assigned agent. This divide-and-conquer approach caps per-module token load. Core assumption: Agent names provide sufficient semantic signal for task routing.

## Foundational Learning

- **OpenAPI/JSON Schema specification**: Why needed here: OPACA agent actions are defined using OpenAPI + JSON Schema; understanding parameter types, required fields, and description fields is essential for debugging tool-call failures. Quick check: Can you read an OpenAPI spec and identify which parameters are required vs optional?
- **LLM native function-calling (tool_call field)**: Why needed here: Simple-Tools, Tool-Chain, and Orchestration rely on models' built-in tool features; Simple uses manual JSON parsing instead. Quick check: What's the difference between an LLM outputting JSON in chat content vs using a structured tool_call response field?
- **ReAct-style reasoning loops**: Why needed here: All SAGE methods iterate: plan → execute → evaluate → (repeat if needed). Understanding when loops terminate is critical. Quick check: In Tool-Chain, what triggers another iteration vs returning to the user?

## Architecture Onboarding

- **Component map**: Frontend (Vue.js) -> Backend (Python FastAPI) -> OPACA Runtime Platform -> LLM Proxies (vLLM/LiteLLM/OpenAI)
- **Critical path**: 1. User query → Backend REST endpoint 2. Backend fetches available actions from OPACA RP 3. Selected method's LLM module(s) generate tool calls 4. Backend validates parameters, invokes OPACA actions 5. Results fed back to LLM; loop until evaluation passes 6. Final response streamed to frontend
- **Design tradeoffs**: Simple: Fastest (3.81s single-tool), but no parallel calls, JSON-parsing fragility; Simple-Tools: Balanced speed (5.24s) with parallel tool support; Tool-Chain: Best quality (4.72 score), 2× module overhead; Orchestration: Best token efficiency, worst latency (14.26s), struggles with cross-agent tasks
- **Failure signatures**: Premature termination (Simple): Model outputs chatter alongside JSON; parser exits loop early; Tool limit overflow (Tool-Chain): >128 tools truncates available actions (OpenAI hard limit); Routing confusion (Orchestration): Similar agent capabilities cause Orchestrator misassignment; Quantization errors: Worker module fails with 4-bit models—use 16-bit for tool formulation
- **First 3 experiments**: 1. Method baseline: Run Simple-Tools and Tool-Chain on your own tool set with 10 diverse queries; compare response scores using GPT-4.1 as judge. 2. Tool capacity stress test: Add tools incrementally (10, 50, 100, 150) and measure where Tool-Chain Generator quality degrades or truncation occurs. 3. Open-source viability: Substitute the paper's open-source-composition (Mistral-Small-24B + Qwen2.5-32B) for GPT-4o-mini in Tool-Chain; measure score gap and latency difference.

## Open Questions the Paper Calls Out

### Open Question 1
How can proactive and introspective functionalities be integrated into SAGE to enable autonomous self-configuration based on inferred user preferences and future task scheduling? Basis in paper: Section 8.1 explicitly states the authors are "currently investigating ways to include more ‘proactive’ and ‘introspective’ functionalities" to allow the system to adapt and manage tasks over time. Why unresolved: The current system architecture focuses on reactive, zero-shot inference in response to immediate user requests, lacking the mechanisms for persistent state tracking or autonomous scheduling.

### Open Question 2
How can the Orchestration method be improved to effectively handle complex coordination scenarios where Agent-Trios possess similar capabilities or require high inter-trio interaction? Basis in paper: Section 6.4 notes that the Orchestration method "can have problems if different Agent-Trios have similar capabilities, or if tasks require much interaction between different Agent-Trios." Why unresolved: The current "divide & conquer" approach distributes subtasks to isolated trios, creating a bottleneck when subtasks are not easily separable or when agents provide redundant functionalities.

### Open Question 3
How does SAGE's reliability and latency compare when using simulated benchmark agents versus real-world, non-deterministic services? Basis in paper: Section 7.1 notes that the evaluation used "specifically designed ACs" that "simulate real-world scenarios" due to the limited availability of public agent containers. Why unresolved: Simulated containers may not exhibit the network latencies, authentication failures, or data inconsistencies common in live microservice architectures, potentially inflating performance metrics.

## Limitations
- System prompt specification details are missing, creating uncertainty in reproducing method-specific behaviors
- Benchmark representativeness may not generalize to real-world tool diversity across broader domains
- Open-source model composition implementation details are insufficient for direct comparison

## Confidence
- High confidence: Simple-Tools and Tool-Chain accuracy claims (85-95% correct tool usage), token efficiency differences across methods
- Medium confidence: Orchestration's hierarchical routing mechanism effectiveness, open-source model performance parity claims
- Low confidence: Judge-LLM evaluation reliability, cross-domain generalizability of response scores

## Next Checks
1. **Prompt template validation**: Implement and test each method with the exact prompt specifications inferred from the paper. Measure response score variance across 10-fold cross-validation on the benchmark.
2. **Tool limit stress test**: Systematically increase tool count (10, 50, 100, 150) and document where Tool-Chain and Orchestration performance degrades or fails, particularly around the 128-tool OpenAI limit.
3. **Open-source parity test**: Reproduce the open-source model composition setup and run Tool-Chain on the full benchmark. Compare against reported GPT-4o-mini results for response score, time, and token usage.