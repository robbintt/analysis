---
ver: rpa2
title: 'TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge'
arxiv_id: '2506.21618'
source_url: https://arxiv.org/abs/2506.21618
tags:
- trajectory
- trajtok
- trajectories
- dataset
- tokenizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrajTok introduces a trajectory tokenizer that combines data-driven
  and rule-based methods to generate more representative and robust discrete tokens
  for behavior generation models. Unlike purely data-driven approaches like k-disks,
  which have limited and asymmetric coverage, TrajTok uses gridding, filtering, and
  expansion to create symmetric, noise-resistant tokens that cover trajectories beyond
  those seen in the dataset.
---

# TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge

## Quick Facts
- **arXiv ID:** 2506.21618
- **Source URL:** https://arxiv.org/abs/2506.21618
- **Reference count:** 7
- **Primary result:** Second place overall in 2025 Waymo Open Sim Agents Challenge with realism score of 0.7852

## Executive Summary
TrajTok introduces a trajectory tokenizer that combines data-driven and rule-based methods to generate more representative and robust discrete tokens for behavior generation models. Unlike purely data-driven approaches like k-disks, which have limited and asymmetric coverage, TrajTok uses gridding, filtering, and expansion to create symmetric, noise-resistant tokens that cover trajectories beyond those seen in the dataset. It also proposes spatial-aware label smoothing, which assigns label probabilities inversely proportional to the squared error from the ground-truth, improving generalization. Evaluated with the SMART model on the 2025 Waymo Open Sim Agents Challenge, TrajTok achieves a realism score of 0.7852, ranking second overall, and shows state-of-the-art map-based performance of 0.9207. The tokenizer improves coverage, symmetry, and robustness compared to existing methods, leading to more realistic and diverse agent behavior generation.

## Method Summary
TrajTok is a hybrid trajectory tokenizer designed for discrete next-token-prediction behavior generation models. It extracts L=5 trajectories from the Waymo Open Motion Dataset, normalizes them to agent-centric coordinates, and flips them along the x-axis for symmetry. A spatial grid is defined with agent-specific ranges (vehicles: -5 to 20m x, -1.5 to 4.5m y; bicycles: -1 to 8m x, -1 to 1m y; pedestrians: -1.5 to 4.5m y, -2 to 2m y), then applies filtering to remove isolated cells (noise) and expansion to include plausible unobserved trajectories. Tokens are generated by averaging trajectories in populated cells or interpolating for expanded cells. The method uses spatial-aware label smoothing with ε=0.1, where probability is inversely proportional to squared error from ground-truth. It trains a modified SMART-tiny model with separate prediction heads per agent type, achieving 8040 vehicle tokens, 3001 bicycle tokens, and 2798 pedestrian tokens.

## Key Results
- Achieves second place overall in 2025 Waymo Open Sim Agents Challenge with realism score of 0.7852
- Achieves state-of-the-art map-based performance of 0.9207
- Demonstrates superior coverage, symmetry, and robustness compared to k-disks tokenizer

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Grid-Based Tokenization with Filtering and Expansion
Combining rule-based gridding with data-driven filtering and expansion produces trajectory vocabularies with better coverage, symmetry, and robustness than purely data-driven methods like k-disks. A spatial grid is defined over endpoint space; trajectories from the dataset are associated with cells. A binary validity map filters isolated cells (likely noise) and expands into dense-adjacent empty cells (plausible but unobserved trajectories). Tokens are generated by averaging trajectories within valid cells or interpolating for expanded cells. Core assumption: trajectory endpoint noise manifests as spatially isolated points, while valid trajectory regions form contiguous spatial clusters. Evidence: abstract states tokens "cover trajectories beyond those seen in the dataset," Section 2.2 explains filtering enhances robustness and expansion improves coverage, Section 3.3 shows TrajTok covers unobserved but plausible trajectories. Break condition: if noise appears in contiguous regions rather than isolation, filtering will fail to remove it. If valid rare trajectories are spatially isolated, filtering will incorrectly discard them.

### Mechanism 2: Spatial-Aware Label Smoothing
Replacing uniform label smoothing with distance-weighted probabilities (inversely proportional to squared error from ground-truth) improves generalization for trajectory token classification. Instead of distributing ε probability equally among all non-ground-truth tokens, weight each by k_i = 1/(||c_i - c_j||² + ε₁), so spatially proximate tokens receive more probability mass than distant ones. Core assumption: Euclidean distance between trajectory tokens meaningfully correlates with behavioral similarity—tokens close in space represent similar driving behaviors. Evidence: abstract mentions it "assigns label probabilities inversely proportional to the squared error from the ground-truth, improving generalization," Section 2.3 explains spatial proximity should make models more tolerant of nearby tokens, no direct corpus evidence found. Break condition: if Euclidean distance doesn't capture behavioral similarity (e.g., similar endpoints but divergent intermediate paths), probability weighting becomes misaligned with actual token utility.

### Mechanism 3: Symmetry Enforcement via Axis Flipping
Forcing symmetric token coverage along the x-axis through trajectory flipping improves generalization across diverse traffic scenarios and driving directions. All extracted trajectories are flipped along the x-axis and concatenated with originals before tokenization, ensuring the resulting vocabulary is symmetric. Core assumption: Real-world driving exhibits inherent symmetry—left/right lane changes, mirrored turn radii in left-hand vs. right-hand traffic. Evidence: Section 2.2 states flipping "ensure the symmetry of trajectories, which results in symmetric tokens along the x-axis," Section 3.3 notes vehicle kinematic models and traffic rules suggest symmetric candidate trajectories, Section 3.3 shows TrajTok ensures symmetry while k-disks tokens are significantly asymmetric. Break condition: if target deployment has inherent asymmetries that should be preserved (e.g., right-hand traffic where right turns have consistently smaller radii), forced symmetry may reduce accuracy.

## Foundational Learning

- **Concept: Next-Token Prediction (NTP) for Behavior Generation**
  - Why needed here: TrajTok is designed for discrete NTP-based models (SMART, Trajeglish) that predict trajectory tokens autoregressively, analogous to language modeling.
  - Quick check question: Why does treating trajectory prediction as language modeling require discrete tokens rather than continuous outputs?

- **Concept: Trajectory Tokenization Trade-offs**
  - Why needed here: The paper argues k-disks has "limited and asymmetric coverage"—understanding what makes a tokenizer good (coverage, symmetry, robustness) is essential.
  - Quick check question: Why would a purely data-driven tokenizer fail to cover plausible trajectories not seen in the training distribution?

- **Concept: Label Smoothing for Classification**
  - Why needed here: The spatial-aware variant modifies standard label smoothing; you must understand what problem label smoothing solves first.
  - Quick check question: Standard label smoothing distributes probability uniformly—why might this be suboptimal when tokens have spatial semantics?

## Architecture Onboarding

- **Component map:** Data Preparation -> Gridding Module -> Binary Map Generator -> Token Generator -> Loss Modifier
- **Critical path:** 1) Extract trajectories from WOMD, normalize, flip 2) Apply agent-specific grid parameters (vehicles: -5 to 20m x, -1.5 to 4.5m y) 3) Generate binary validity map via k×k neighborhood check 4) Create vocabulary (vehicles: 8040 tokens, bicycles: 3001, pedestrians: 2798) 5) Train SMART-tiny with separate prediction heads per agent type + spatial-aware loss
- **Design tradeoffs:** Vocabulary size: larger improves resolution but risks underfitting; optimal ~8000 (vehicle), ~2000-3000 (pedestrian/bicycle) per Table 3. Filtering vs. expansion aggressiveness: too aggressive filtering discards rare valid trajectories; too aggressive expansion includes implausible ones. Separate vs. shared prediction heads: paper uses separate heads per agent type (modification from original SMART), trading parameter efficiency for specialization.
- **Failure signatures:** Asymmetric vocabulary: skipping flipping produces biased tokens, overfitting to dataset handedness. Noise tokens: insufficient filtering creates tokens from erroneous trajectory endpoints. Underfitting with large vocabulary: exceeding optimal sizes (Table 3) degrades Realism Meta. Misweighted loss: if ||c_i - c_j||² doesn't reflect behavioral similarity, spatial smoothing backfires.
- **First 3 experiments:** 1) Tokenizer ablation: compare k-disks (2048) vs. TrajTok (8040/3001/2798) on held-out validation split, reporting Realism Meta 2) Component ablation: test TrajTok with filtering-only, expansion-only, and both to isolate each contribution 3) Loss comparison: train identical models with uniform label smoothing vs. spatial-aware smoothing to quantify generalization gain

## Open Questions the Paper Calls Out
- **Does TrajTok transfer effectively to other next-token-prediction architectures like MotionLM?** The evaluation is limited to the SMART-tiny model despite mentioning Trajeglish and MotionLM. Performance gains may be partially driven by the implementation of separate prediction heads rather than the tokenizer itself. Cross-model ablation studies integrating TrajTok into Trajeglish or MotionLM backbones would resolve this.
- **What is the computational and memory cost trade-off given the 4x increase in vocabulary size?** TrajTok uses 8,040 vehicle tokens versus k-disks' 2,048 but discusses only accuracy, not efficiency. Larger vocabularies increase softmax computation and memory, potentially hindering real-time simulation. Inference latency (ms/frame) and GPU memory benchmarks comparing TrajTok to k-disks would resolve this.
- **Do the tokens generated via curve interpolation for empty grid cells satisfy kinematic constraints?** The method generates tokens for empty cells using curve interpolation (Eq. 7) without dynamic validation. Geometric interpolation focuses on path continuity but may produce physically impossible acceleration or yaw rates. A kinematic feasibility analysis (e.g., max acceleration, jerk) of the generated "expanded" tokens would resolve this.

## Limitations
- Evaluation is limited to WOMD dataset and WOSAC metrics, with no evidence of generalization to other driving datasets or urban layouts.
- Specific hyperparameters for filtering and expansion steps are unspecified, making exact reproduction difficult.
- Curve interpolation method for generating tokens in empty valid grid cells is not detailed.

## Confidence
- **High Confidence:** The hybrid grid-based tokenization approach (combining rule-based gridding with data-driven filtering and expansion) is well-specified and its benefits (coverage, symmetry, robustness) are supported by the ablation study and comparison to k-disks.
- **Medium Confidence:** The spatial-aware label smoothing mechanism improves generalization, but its reliance on Euclidean distance as a proxy for behavioral similarity is a weak assumption that could limit its effectiveness in edge cases.
- **Low Confidence:** The forced symmetry via axis flipping improves generalization, but its benefit is context-dependent and may not translate to real-world deployment in regions with inherent asymmetries (e.g., right-hand traffic).

## Next Checks
1. **Token Representation Quality:** Visualize the generated trajectory tokens to verify symmetry and coverage. Check for any systematic biases or gaps in the vocabulary that could impact the model's ability to generate diverse and realistic behaviors.

2. **Label Smoothing Sensitivity:** Conduct an ablation study comparing spatial-aware label smoothing with uniform label smoothing and no label smoothing. Measure the impact on generalization and robustness to out-of-distribution trajectories.

3. **Domain Generalization:** Evaluate TrajTok on a different driving dataset (e.g., nuScenes, Argoverse) or in a simulated environment with different traffic rules (e.g., left-hand traffic). Assess whether the tokenizer's design choices (grid parameters, vocabulary sizes) require adaptation for new domains.