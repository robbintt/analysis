---
ver: rpa2
title: Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs
arxiv_id: '2510.04633'
source_url: https://arxiv.org/abs/2510.04633
tags:
- relevance
- judge
- judgments
- adapters
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Topic-specific relevance classifiers address the unjudged document\
  \ problem in information retrieval by training LoRA adapters on a single assessor's\
  \ judgments for each topic, creating topic-specific judge models that replicate\
  \ that assessor's notion of relevance. These classifiers achieve Spearman's \u03C1\
  \ 0.95 correlation with ground truth system rankings and Krippendorff's \u03B1 =\
  \ 0.81-0.88 agreement with human judgments."
---

# Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs

## Quick Facts
- arXiv ID: 2510.04633
- Source URL: https://arxiv.org/abs/2510.04633
- Authors: Lukas Gienapp; Martin Potthast; Harrisen Scells; Eugene Yang
- Reference count: 40
- Key outcome: Topic-specific LoRA adapters achieve Spearman's ρ > 0.95 and Krippendorff's α = 0.81-0.88 using as few as 128 human judgments per topic, outperforming LLM-as-a-judge approaches while avoiding circularity.

## Executive Summary
This paper addresses the unjudged document problem in information retrieval evaluation by training topic-specific relevance classifiers using LoRA adapters. Rather than treating unjudged documents as non-relevant (which penalizes new retrieval systems), the approach trains specialized classifiers on a single assessor's judgments for each topic. These topic-specific judge adapters achieve high agreement with human judgments while requiring minimal training data, making test collections more reusable. The method outperforms larger LLMs in agreement with human relevance assessments while avoiding circularity issues inherent in using the same models for both retrieval and evaluation.

## Method Summary
The approach trains independent LoRA adapters for each topic using a base ranking model (monoT5-base-msmarco-10k) fine-tuned on 128-256 human-judged query-document pairs from a single assessor. Each adapter deliberately overfits to capture that specific assessor's notion of relevance for the topic. The adapters use weighted MSE loss to handle class imbalance (typically 6% relevant documents) and are evaluated against held-out ground truth judgments. The method is tested across three TREC collections (Robust04, DL19, DL20) and compared against various LLM-as-a-judge approaches, demonstrating superior agreement with human assessments while requiring significantly less data and computational resources than fine-tuning full models.

## Key Results
- Spearman's ρ > 0.95 correlation between system rankings from judge adapters and ground truth judgments at k=50
- Krippendorff's α = 0.81-0.88 agreement with human relevance assessments
- F1 scores of 0.88-0.93 across test collections
- Outperforms LLM-as-a-judge approaches (α = 0.32-0.58) while using 220M vs 229B parameters
- Achieves reliable results with as few as 128 initial human judgments per topic

## Why This Works (Mechanism)

### Mechanism 1
Topic-specific LoRA adapters can replicate a specific human assessor's relevance criteria with minimal training data, effectively converting a generic ranking model into a specialized "judge." The paper utilizes LoRA to fine-tune monoT5 on the judgment pool of a single topic, training independent adapters for each topic (r=64, α=128) that "deliberately overfit" to the specific decision boundary of the assessor. This inverts the standard bias-variance tradeoff, creating topic-specific models that ignore cross-topic generalization in favor of capturing the assessor's unique relevance patterns. The approach assumes initial human judgments are consistent and representative of the assessor's criteria for the entire collection.

### Mechanism 2
Relevance judgment is primarily a calibration task derived from semantic matching rather than complex reasoning, making specialized rankers more efficient judges than larger generative LLMs. Smaller ranking models (monoT5, 220M params) outperform much larger LLMs (up to 229B params) in agreement with human judgments (α=0.81 vs 0.32-0.58). Rankers, pre-trained on relevance tasks, require less capacity to distinguish relevant text than generative models which may apply inconsistent, lenient thresholds. The approach assumes the base ranking model has sufficiently strong semantic representations for the target domain, though domain shift remains a known friction point.

### Mechanism 3
Validating systems with LLM-generated judgments risks "circularity" (self-preference bias), whereas topic-specific adapters mitigate this by being structurally incapable of general-purpose retrieval. An LLM can function as both a ranker and a judge, potentially confirming its own biases. A topic-specific adapter is a rigid function f_t(q_t, d) → [0,1] that lacks the generative capacity or general parameterization to act as a retriever for arbitrary queries, breaking the circularity loop. The approach assumes adapters are not misused to generate synthetic training data for the system being evaluated, which would re-introduce overfitting concerns.

## Foundational Learning

**Concept: LoRA (Low-Rank Adaptation)**
Why needed here: This is the efficiency backbone allowing training of 50+ specialized models without prohibitive cost of full fine-tuning.
Quick check question: How does setting r=64 affect the trade-off between capturing assessor's nuance and maintaining base model's semantic priors?

**Concept: The Unjudged Document Problem**
Why needed here: This is the core motivation. Standard evaluation ("unjudged = non-relevant") penalizes new systems that find new, potentially relevant documents.
Quick check question: Why does treating unjudged documents as non-relevant break comparability of new retrieval systems against old baselines?

**Concept: Inter-Annotator Agreement (Krippendorff's α)**
Why needed here: The paper argues system-level correlation (Spearman's ρ) is insufficient; true alignment requires high label-level agreement (α).
Quick check question: Why might an LLM achieve high system ranking correlation (ρ > 0.9) while having poor agreement (α < 0.6) with human labels?

## Architecture Onboarding

**Component map:** Query + Document → monoT5-base-msmarco-10k → LoRA adapter (r=64, α=128) → Binary/Scalar Relevance Score

**Critical path:**
1. Select a topic and sample 128-256 human-judged query-document pairs (stratified sampling)
2. Train LoRA adapter on this specific topic pool for 10 epochs
3. Apply the adapter to score unjudged documents retrieved by new systems
4. Compute evaluation metrics (nDCG, MAP) using these inferred labels

**Design tradeoffs:**
- Mono-decoder vs. Bi-encoder: The paper favors mono-decoders (cross-attention) for higher accuracy (F1 > 0.88) despite higher inference cost than bi-encoders
- In-domain vs. Out-of-domain: Performance drops on Robust04 (out-of-domain) compared to DL19/20 (MS MARCO); selection of base model must consider corpus domain

**Failure signatures:**
- High False Positives: If loss weighting is not tuned for class imbalance (approx. 6% relevant in Robust04), classifier defaults to "relevant"
- Circularity/Overfitting: If adapter is used to train ranker being evaluated, validity is voided

**First 3 experiments:**
1. **Sanity Check:** Train adapter on topic with 256 judgments. Verify label agreement (α) against held-out ground truth
2. **Data Efficiency Curve:** Train adapters with k ∈ {64, 128, 192, 256} samples. Plot degradation of Spearman's ρ to determine minimum viable sample size for your domain
3. **Circularity Test:** Compare ranking of monoT5-based retrieval system using (A) LLM judge and (B) your topic adapter. Check if LLM judge artificially inflates monoT5 system's score relative to adapter

## Open Questions the Paper Calls Out

**Open Question 1:** Can judge adapters be effectively trained to model non-topical relevance dimensions, such as trustworthiness and understandability? The authors identify this as future work, noting that in many search scenarios dimensions like trustworthiness and understandability are key, and plan to investigate if the approach can be used to model dimensions beyond topical relevance.

**Open Question 2:** Can topic-specific judge adapters be adapted to evaluate the quality of Retrieval-Augmented Generation (RAG) responses across different utility dimensions? The authors identify "the reliable offline evaluation of RAG responses" as a major open problem and state they plan to investigate the same technique to evaluate RAG response quality.

**Open Question 3:** Can topic-specific judge adapters effectively predict graded relevance levels rather than just binary relevance? The paper methodology explicitly notes that "Annotated graded relevance labels are cast to binary relevance" for all experiments, discarding nuance that is often critical for evaluating modern retrieval systems.

## Limitations

**Cross-domain generalization limits:** While strong performance on Robust04 (out-of-domain for MS MARCO-trained monoT5), the 5-10% performance drop on Robust04 versus in-domain collections suggests these classifiers may not generalize well to domains with different topical distributions or relevance criteria.

**Class imbalance sensitivity:** With relevance rates as low as 6% in Robust04, classifiers are trained with heavily weighted loss functions, creating risk of overfitting to minority class during training, potentially inflating precision metrics on test set while degrading real-world performance.

**Ground truth dependency:** The approach requires initial set of human judgments (minimum 128 per topic) to train each adapter. While far fewer than full manual judgment of all retrieved documents, it still requires significant human annotation effort, and quality of these initial judgments directly determines classifier reliability.

## Confidence

**High confidence** in claims about LoRA efficiency and fundamental mechanism: The technical approach of using LoRA adapters to create topic-specific relevance classifiers is well-specified and theoretically sound. The core insight that specialized rankers outperform LLMs for this task is strongly supported by empirical results.

**Medium confidence** in claims about avoiding circularity: While the paper argues topic-specific adapters cannot function as general-purpose rankers, preventing self-preference bias, this depends on how practitioners use the adapters. If adapters are used to re-rank or filter documents in retrieval pipeline being evaluated, circularity could still occur.

**Low confidence** in claims about reasoning limitations: The assertion that relevance judgment is "primarily a calibration task rather than a reasoning task" is based on observed LLM underperformance. However, this conclusion may be premature given rapid evolution of LLM capabilities and possibility that current prompting strategies are suboptimal for this task.

## Next Checks

1. **Out-of-domain stress test:** Train topic-specific adapters using monoT5-base-msmarco-10k on Robust04, then evaluate on completely different domain (e.g., scientific literature or legal documents) to measure degradation in F1 and Spearman's ρ, quantifying practical limits of cross-domain transfer.

2. **Initial judgment quality audit:** Systematically vary quality of initial human judgments (using expert vs. crowd-sourced annotations) and measure resulting impact on adapter performance, establishing minimum human judgment quality required for reliable classifier training.

3. **Multi-assessor alignment study:** Train adapters on judgments from different assessors for same topics and measure inter-adapter agreement, revealing whether topic-specific classifiers amplify individual assessor biases or converge toward consensus notion of relevance.