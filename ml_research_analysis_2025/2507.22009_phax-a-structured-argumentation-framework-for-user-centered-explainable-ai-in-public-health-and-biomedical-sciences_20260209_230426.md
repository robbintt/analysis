---
ver: rpa2
title: 'PHAX: A Structured Argumentation Framework for User-Centered Explainable AI
  in Public Health and Biomedical Sciences'
arxiv_id: '2507.22009'
source_url: https://arxiv.org/abs/2507.22009
tags:
- argumentation
- phax
- health
- reasoning
- public
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PHAX is a structured argumentation framework that enhances explainability\
  \ in AI-driven public health and biomedical systems by generating user-adaptive,\
  \ context-aware justifications. It combines formal reasoning (Dung\u2019s AF, ASPIC+),\
  \ adaptive NLP, and user modeling to deliver explanations tailored to clinicians,\
  \ policymakers, and patients."
---

# PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences

## Quick Facts
- arXiv ID: 2507.22009
- Source URL: https://arxiv.org/abs/2507.22009
- Authors: Bahar İlgen; Akshat Dubey; Georges Hattab
- Reference count: 33
- Primary result: Novel framework combining formal argumentation with adaptive NLP for explainable AI in healthcare

## Executive Summary
PHAX introduces a structured argumentation framework designed to make AI decisions in public health and biomedical sciences more interpretable and trustworthy. The framework integrates formal argumentation theory (Dung's AF, ASPIC+), adaptive natural language processing, and user modeling to generate explanations tailored to different stakeholders including clinicians, policymakers, and patients. By modeling AI outputs as defeasible reasoning chains, PHAX enables traceable and interactive explanations that align with human communication needs.

## Method Summary
PHAX employs a layered architecture combining Dung's abstract argumentation frameworks with ASPIC+ for structured argumentation, augmented by adaptive NLP techniques and user modeling components. The framework generates defeasible reasoning chains that can be contextualized based on user expertise and domain requirements. Argumentation schemes provide the logical structure for explaining AI decisions, while user modeling ensures explanations are appropriately adapted to the audience. The system has been demonstrated in use cases including medical text simplification, evidence synthesis, and policy communication, though empirical validation remains limited.

## Key Results
- Framework successfully integrates formal argumentation with adaptive NLP for healthcare explainability
- Demonstrated applicability across multiple domains: medical text simplification, evidence synthesis, and policy communication
- Provides traceable reasoning chains that can be adapted to different user types (clinicians, policymakers, patients)

## Why This Works (Mechanism)
PHAX works by structuring AI decision-making as formal argumentation processes that mirror human reasoning patterns. By combining Dung's argumentation frameworks with ASPIC+'s structured approach, the system creates defeasible reasoning chains that can be questioned and revised—just like human arguments. The integration with adaptive NLP allows these formal structures to be translated into natural language explanations appropriate to each user's expertise level. User modeling components ensure that the same AI decision can be explained differently to a clinician versus a patient, maintaining both accuracy and accessibility.

## Foundational Learning
- **Dung's Abstract Argumentation (AF)**: Provides the theoretical foundation for representing arguments and their relationships (attacks/defense); needed for modeling non-monotonic reasoning where conclusions can be defeated by new evidence
- **ASPIC+ Framework**: Extends abstract argumentation with structured rules and preferences; needed to formalize how arguments are constructed from premises and how they defeat others
- **Defeasible Reasoning**: Allows conclusions to be withdrawn when new contrary evidence appears; needed for medical contexts where new evidence can overturn previous recommendations
- **User Modeling**: Creates profiles of different stakeholder expertise levels; needed to tailor explanation complexity appropriately
- **Argumentation Schemes**: Provides templates for constructing valid arguments in specific domains; needed to ensure explanations follow accepted reasoning patterns in healthcare
- **Adaptive NLP**: Enables dynamic adjustment of language complexity and terminology; needed to make formal arguments comprehensible to non-expert users

## Architecture Onboarding
- **Component Map**: User Input -> User Modeling -> Adaptive NLP -> Formal Argumentation (AF + ASPIC+) -> Explanation Generation -> Output
- **Critical Path**: User query → User modeling → Argument construction → Explanation adaptation → Presentation
- **Design Tradeoffs**: Formal rigor vs. accessibility (maintains logical correctness while adapting language complexity); computational efficiency vs. explanation depth (detailed arguments require more processing time)
- **Failure Signatures**: Circular reasoning chains, explanations that exceed user comprehension thresholds, formal arguments that cannot be adequately translated to natural language
- **First Experiments**: 1) Implement medical text simplification use case with clinical text corpus; 2) Create policy communication explanation for a public health intervention; 3) Test explanation comprehension across three user groups (clinicians, policymakers, patients)

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Lacks empirical validation demonstrating improved user trust, comprehension, or decision-making outcomes
- Framework complexity may present practical implementation challenges in resource-constrained healthcare settings
- Scalability and computational resource requirements for real-world deployment not addressed

## Confidence
- **Medium Confidence**: Theoretical framework combining Dung's argumentation, ASPIC+, and user modeling is well-founded and coherent
- **Low Confidence**: Claims about improved trust and interpretability require empirical validation
- **Medium Confidence**: Use cases (medical text simplification, evidence synthesis) are relevant but unproven

## Next Checks
1. Conduct controlled user studies comparing PHAX explanations against standard AI explanations across different user groups (clinicians, policymakers, patients) to measure comprehension and trust metrics
2. Implement a prototype system to evaluate computational efficiency and identify practical implementation bottlenecks in real healthcare settings
3. Develop and validate a methodology for quantifying the quality of defeasible reasoning chains generated by PHAX to ensure they meet clinical standards for evidence-based decision support