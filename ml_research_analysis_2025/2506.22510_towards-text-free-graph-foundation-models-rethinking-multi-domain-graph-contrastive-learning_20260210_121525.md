---
ver: rpa2
title: 'Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive
  Learning'
arxiv_id: '2506.22510'
source_url: https://arxiv.org/abs/2506.22510
tags:
- graph
- pre-training
- domain
- domains
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of building foundation models
  for text-free graphs by proposing a novel multi-domain pre-training framework called
  MDGCL. Unlike existing methods that ignore domain differences, MDGCL explicitly
  recognizes and captures domain-specific discrepancies through a contrastive learning
  strategy that treats subgraphs from different domains as negative pairs.
---

# Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning

## Quick Facts
- arXiv ID: 2506.22510
- Source URL: https://arxiv.org/abs/2506.22510
- Reference count: 40
- Primary result: MDGCL achieves up to 19.33% accuracy and 19.13% Macro-F1 improvement over state-of-the-art methods on multi-domain graph transfer tasks.

## Executive Summary
This paper addresses the challenge of building foundation models for text-free graphs by proposing MDGCL, a multi-domain pre-training framework that explicitly captures domain-specific differences. Unlike existing methods that ignore domain distinctions, MDGCL treats subgraphs from different domains as negative pairs in contrastive learning, forcing the model to encode domain-specific information rather than collapsing all domains into similar representations. The approach introduces domain tokens to encode global domain information and employs domain-level attention during fine-tuning to enable fine-grained knowledge transfer. Extensive experiments on five benchmark datasets demonstrate significant performance improvements, with MDGCL achieving up to 19.33% accuracy and 19.13% Macro-F1 gains over state-of-the-art methods.

## Method Summary
MDGCL is a multi-domain pre-training framework for text-free graphs that explicitly recognizes and captures domain-specific discrepancies through contrastive learning. The method samples subgraphs from multiple source domains, computes domain tokens via sum-pooled node features, and trains a GNN to classify whether subgraph pairs are from the same domain or different domains. During downstream fine-tuning, each target node computes attention scores against all source domain tokens, with the weighted sum of value-projected tokens added to node features. The approach uses SVD to unify feature dimensions across domains (50 dimensions) and employs random walk sampling to generate subgraphs. The pre-training objective is binary cross-entropy classification of same-domain vs. different-domain subgraph pairs.

## Key Results
- MDGCL achieves up to 19.33% accuracy improvement and 19.13% Macro-F1 gain over state-of-the-art methods on five benchmark datasets
- Performance scales with the number of source domains, showing consistent improvements as more domains are added
- The method demonstrates robustness across both homophilic graphs (Cora, PubMed) and heterophilic graphs (Squirrel, Chameleon), with performance gains maintained in both settings
- Ablation studies show that both domain tokens and domain-level attention contribute significantly to performance, with MDGCL-V2 (without tokens) and MDGCL-V3 (without attention) underperforming the full model

## Why This Works (Mechanism)

### Mechanism 1
Traditional graph contrastive learning treats all non-identical subgraphs as equivalent negatives, causing models like SAMGPT and GCOPE to map multi-domain nodes to similar distributions. MDGCL instead constructs positive pairs only from same-domain subgraphs and negative pairs from different-domain subgraphs, training via binary cross-entropy to predict domain membership. This forces the model to encode domain-specific differences rather than collapsing all domains into similar representations.

### Mechanism 2
Domain tokens computed via sum-pooled node features provide global domain context that improves both pre-training discrimination and downstream transfer. Each source domain receives a token ti = sum(X̃i) connected to all nodes in its subgraphs during pre-training. This creates a unified graph structure where the token acts as a domain-level "anchor" node, propagating global statistics through message passing.

### Mechanism 3
Domain-level attention enables fine-grained transfer by weighting source domain contributions based on target node similarity. During fine-tuning, each downstream node vi computes attention scores αim against all source domain tokens via learned query/key projections. The weighted sum of value-projected tokens is added to node features, allowing selective mixing of source domain knowledge rather than uniform aggregation.

## Foundational Learning

- **Concept: Contrastive Learning for Graphs**
  - Why needed here: MDGCL builds on graph contrastive learning but fundamentally changes the negative sampling strategy. Understanding standard GraphCL/SimGRACE objectives clarifies why domain-aware negatives matter.
  - Quick check question: Can you explain why treating all non-identical subgraphs as negatives would cause domain collapse in multi-domain settings?

- **Concept: Homophily vs. Heterophily in GNNs**
  - Why needed here: The paper explicitly tests on both homophilic (Cora, PubMed) and heterophilic (Squirrel, Chameleon) graphs, showing MDGCL works across both. Understanding this distinction helps interpret Table 3 results.
  - Quick check question: Why might a GNN trained only on homophilic graphs fail to transfer to heterophilic target domains?

- **Concept: Feature Dimension Alignment via SVD**
  - Why needed here: Multi-domain graphs have different feature dimensions (d_i). The paper uses SVD to project all to unified dimension d̃ = 50.
  - Quick check question: What information loss occurs when reducing citation network features (1433 dimensions in Cora) to 50 dimensions via SVD?

## Architecture Onboarding

- Component map: Source graphs G_s → SVD projection (d̃=50) → Random walk subgraph sampling → Domain token computation (sum pooling) → Merge(subgraph_pair, domain_tokens) → GNN encoder → ReadOut → Projection head → Binary classification: same_domain or different_domain?

- Critical path:
  1. Verify SVD projection preserves discriminative features across domains (check reconstruction error)
  2. Ensure random walk sampling (K=50 subgraphs, length=50) captures sufficient domain statistics
  3. Monitor pre-training accuracy on domain discrimination task (should not reach 100% too quickly—may indicate trivial separation)

- Design tradeoffs:
  - Sum pooling for domain tokens is computationally cheap but may lose nuanced statistics; alternatives (mean, attention-weighted) not tested
  - Binary cross-entropy loss treats all cross-domain pairs equivalently; could weight by domain similarity if known
  - Adding attention output to features (vs. concatenation) assumes compatible feature scales

- Failure signatures:
  - Pre-training domain accuracy stuck at ~50%: subgraphs may not carry domain signal; increase walk length or subgraph count
  - Downstream attention weights uniform across domains: target domain may be too different; consider domain similarity filtering
  - Performance degrades with more source domains (opposite of Figure 3): negative transfer from conflicting domains; check domain-level t-SNE plots

- First 3 experiments:
  1. Reproduce single baseline comparison (GCN, GCOPE, SAMGPT) on Cora/CiteSeer with unified SVD dim=50 to verify implementation
  2. Ablate domain tokens only (MDGCL-V2) vs. attention only (MDGCL-V3) to isolate contribution of each component on your target domain
  3. Sweep number of source domains (2, 3, 4) while holding target fixed to confirm scaling behavior matches Figure 3

## Open Questions the Paper Calls Out
None

## Limitations

- **Dimension reduction risk:** The paper relies on SVD to unify feature dimensions across domains (d̃=50), but this aggressive compression from Cora's original 1433 dimensions could lose critical discriminative information.
- **Single token per domain assumption:** Using a single sum-pooled domain token assumes each source domain has consistent semantic structure, which could fail for domains with high internal heterogeneity.
- **Computational cost not characterized:** While the paper shows performance gains, it doesn't report pre-training time, memory requirements, or scaling behavior with more domains.

## Confidence

- **High confidence:** The core hypothesis that domain-aware contrastive learning prevents domain collapse appears well-supported by ablation studies and consistent performance improvements across benchmarks.
- **Medium confidence:** The domain attention mechanism's contribution is demonstrated through ablations, but the paper doesn't explore alternative attention designs or compare against simpler aggregation methods.
- **Low confidence:** The effectiveness of 50-dimensional SVD compression for preserving domain-specific features is assumed rather than validated.

## Next Checks

1. **SVD dimension sensitivity test:** Systematically vary the unified feature dimension (d̃=25, 50, 100, 200) and measure reconstruction error plus downstream transfer performance to identify the optimal compression level that preserves domain discriminability.

2. **Domain similarity weighting experiment:** Implement a soft-negative sampling strategy where cross-domain pairs are weighted by learned or heuristic domain similarity, then compare against the current binary treatment of all cross-domain pairs.

3. **Attention mechanism ablation with alternative aggregations:** Replace the multi-head attention with simple weighted averaging of source domain tokens (weights based on target-domain similarity) to test whether the complexity of attention is necessary or if simpler methods achieve comparable transfer.