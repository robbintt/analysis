---
ver: rpa2
title: Continual Self-supervised Learning Considering Medical Domain Knowledge in
  Chest CT Images
arxiv_id: '2501.04217'
source_url: https://arxiv.org/abs/2501.04217
tags:
- learning
- data
- stage
- images
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a continual self-supervised learning (CSSL)
  method for chest CT images that addresses data interference between domains. The
  method uses an enhanced dark experience replay (DER) with k-means sampling to maintain
  diversity and representativeness in the rehearsal buffer across sequential learning
  stages.
---

# Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images

## Quick Facts
- **arXiv ID:** 2501.04217
- **Source URL:** https://arxiv.org/abs/2501.04217
- **Reference count:** 0
- **Primary result:** Proposed CSSL method achieves ACC=0.863, AUC=0.940, F1=0.863 on COVID-19 classification

## Executive Summary
This paper introduces a continual self-supervised learning (CSSL) framework for chest CT images that addresses domain interference between different imaging conditions. The method employs enhanced Dark Experience Replay with k-means sampling to maintain diverse and representative rehearsal buffers across sequential learning stages. Additionally, it incorporates mixup augmentation and feature distillation to strengthen feature representations. The approach is validated on two CT imaging domains (mediastinal and lung windows) and evaluated on COVID-19 classification using the SARS-CoV-2 CT-Scan Dataset.

## Method Summary
The proposed CSSL method consists of three sequential stages: (1) initial MAE pretraining on the first domain (D1) with a ViT-B encoder, (2) k-means clustering to construct a representative rehearsal buffer by sampling from both D1 and D2, and (3) continual pretraining on the second domain (D2) with the buffer using mixup augmentation and feature distillation. The rehearsal buffer is constructed by clustering combined embeddings from D1 and D2, then sampling samples based on their proximity to target domain cluster centers using predefined ratios. During continual pretraining, the method applies mixup to buffer samples and employs feature distillation loss between the current model and the frozen initial model to preserve learned representations.

## Key Results
- CSSL method achieves accuracy of 0.863, AUC of 0.940, and F1 score of 0.863
- Outperforms state-of-the-art methods like MedCoSS in COVID-19 classification
- Demonstrates superior performance when pretraining in order D1 → D2 compared to reverse order

## Why This Works (Mechanism)
The method addresses catastrophic forgetting by maintaining a diverse rehearsal buffer that captures representative samples from both domains. The k-means sampling strategy ensures the buffer contains samples that are both diverse and representative of the target domain distribution. Feature distillation preserves the knowledge learned during initial pretraining by constraining the feature representations during continual learning. The mixup augmentation further enhances feature learning by creating interpolated samples that improve generalization across domains.

## Foundational Learning
- **MAE pretraining**: Masked autoencoder framework for self-supervised learning; needed to learn robust visual representations from unlabeled data; quick check: verify reconstruction quality on validation set
- **Catastrophic forgetting**: Phenomenon where neural networks forget previously learned information when trained on new tasks; needed context for understanding the problem being solved; quick check: measure performance degradation when training sequentially without mitigation
- **Experience replay**: Technique for mitigating forgetting by storing and revisiting past experiences; needed to understand the rehearsal buffer mechanism; quick check: verify buffer samples are representative of both domains
- **Feature distillation**: Method for transferring knowledge between models by matching feature representations; needed to understand how the method preserves learned representations; quick check: monitor distillation loss during training
- **K-means clustering**: Unsupervised algorithm for partitioning data into clusters; needed to understand the buffer construction process; quick check: visualize cluster assignments in embedding space

## Architecture Onboarding
- **Component map**: ViT-B encoder -> MAE decoder -> Feature extractor -> K-means clustering -> Rehearsal buffer -> Continual pretraining module
- **Critical path**: MAE pretraining → Buffer construction → Continual pretraining → Fine-tuning → Evaluation
- **Design tradeoffs**: Buffer size vs. computational efficiency; distillation strength vs. adaptation flexibility; sampling ratios vs. representativeness
- **Failure signatures**: Degraded performance when buffer is not representative; feature collapse when distillation is too strong; overfitting when buffer is too small
- **First experiments**: 1) Implement k-means sampling and visualize buffer representativeness; 2) Test feature distillation ablation by freezing initial model; 3) Compare mixup strategies (uniform vs. beta distribution)

## Open Questions the Paper Calls Out
- **Cross-modality generalization**: Does the strategy of prioritizing representativeness (selecting samples closer to the target domain distribution) degrade performance when applied to highly dissimilar medical domains or distinct modalities? The study only validates the method on two specific chest CT window settings which share high structural similarity.
- **Long-sequence performance**: How does the enhanced Dark Experience Replay (DER) strategy perform in a long-sequence continual learning scenario involving more than two domains? The experimental setup is limited to a two-stage sequential process (D1 → D2).
- **Dense prediction tasks**: Can the feature representations learned via this CSSL method generalize effectively to complex downstream tasks beyond binary classification, such as semantic segmentation? The downstream evaluation is restricted to a binary COVID-19 classification task.

## Limitations
- MAE decoder architecture details are not specified, which could affect pretraining convergence
- SARS-CoV-2 dataset split indices are not explicitly provided, introducing potential variability
- Impact of warm-up learning rate schedule on feature distillation stability is not discussed
- Method is only validated on two sequential domains, limiting understanding of long-sequence performance

## Confidence
- Method description: Medium
- Experimental setup: Medium
- Reported results: High (assuming correct implementation)
- Reproducibility: Low

## Next Checks
1. Re-implement k-means sampling with provided γ ratios and verify domain representativeness by visualizing buffer embeddings
2. Compare mixup strategies (uniform vs. beta distribution) to assess impact on buffer diversity
3. Test feature distillation ablation by freezing M1 weights and monitoring LFD loss dynamics during continual pretraining