---
ver: rpa2
title: 'MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering
  Multiple Languages, Models,Prompts, and Scenarios'
arxiv_id: '2507.21693'
source_url: https://arxiv.org/abs/2507.21693
tags:
- code
- python
- problem
- llms
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiAIGCD, a comprehensive dataset for AI-generated
  code detection covering three programming languages (Python, Java, Go), six LLMs
  (including reasoning models like OpenAI o3-mini), three prompting approaches, and
  three usage scenarios (code generation, runtime error fixing, and output correction).
  The dataset contains 32,148 human-written and 121,271 AI-generated code snippets
  across 800 programming problems from CodeNet.
---

# MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios

## Quick Facts
- arXiv ID: 2507.21693
- Source URL: https://arxiv.org/abs/2507.21693
- Reference count: 7
- Introduces MultiAIGCD dataset with 32,148 human-written and 121,271 AI-generated code snippets across three languages, six LLMs, and three scenarios

## Executive Summary
This paper introduces MultiAIGCD, a comprehensive dataset for AI-generated code detection covering three programming languages (Python, Java, Go), six LLMs (including reasoning models like OpenAI o3-mini), three prompting approaches, and three usage scenarios (code generation, runtime error fixing, and output correction). The dataset contains 32,148 human-written and 121,271 AI-generated code snippets across 800 programming problems from CodeNet. Through qualitative analysis, the study reveals distinct coding style differences between human-written and AI-generated code, with human code being longer and containing more comments and functions. Benchmark experiments on three state-of-the-art detection models show that OpenAI's Ada embeddings consistently yield the highest prediction accuracy, with strong performance in detecting code generated from problem definitions but significantly lower accuracy for AI-fixed code samples. Detection performance varies across programming languages and scenarios, with cross-language setups showing substantial performance decline.

## Method Summary
The paper constructs a dataset using IBM CodeNet's 800 programming problems, extracting human submissions across three languages and three problem statuses. AI code is generated using six LLMs (Llama-3.3-70B, Qwen2.5-Coder-32B, GPT-4o, DeepSeek-V3, o3-mini, Claude 3.5 Sonnet v2) with three prompting strategies across three scenarios. Three detection models are benchmarked: SVM with OpenAI Ada embeddings, SVM with CodeT5+ embeddings, and fine-tuned CodeBERTa. The dataset is split 80/10/10 for training, validation, and testing with score-based stratification. Models are evaluated using accuracy, precision, recall, and F1-score metrics.

## Key Results
- SVM with OpenAI's Ada embeddings achieves highest prediction accuracy across all scenarios and languages
- Detection performance drops significantly for AI-fixed code samples (F1 ~0.70-0.80) compared to generated-from-scratch samples (F1 ~0.95+)
- Cross-language detection shows catastrophic failure, with F1 scores as low as 0.07-0.30 when training on one language and testing on another
- Human code is longer and contains more comments and functions compared to AI-generated code

## Why This Works (Mechanism)

### Mechanism 1: Stylistic Artifact Recognition
Detection models likely succeed by identifying statistical differences in verbosity and structural formatting between human and AI authors. The paper demonstrates that human code tends to be longer and contains more comments and distinct function definitions, whereas AI code is more compact and often lacks comments (especially when explicitly prompted). Classifiers like SVM or CodeBERTa exploit these distributional differences in token patterns and structural length.

### Mechanism 2: Scenario-Based Signal Dilution
Detection efficacy is contingent on the "generative freedom" allowed by the usage scenario. In "ScenarioScratch" (generating from problem definitions), the LLM imposes its full native stylistic and semantic fingerprint on the code. In "ScenarioRuntime" or "ScenarioOutput" (fixing code), the model is conditioned on existing human code, forcing it to adopt the structure and variable names of the input, thereby masking its native artifacts and reducing classifier confidence.

### Mechanism 3: Embedding Geometry and Language Specificity
Detection relies on the embedding space's ability to linearly separate human/AI samples, which varies by programming language and embedding model. OpenAI's Ada embeddings and CodeBERTa create vector representations of code where AI-generated code clusters separately from human code. However, the geometry is language-dependent; boundaries learned for Python/Java do not transfer effectively to Go, likely due to syntax differences or training data prevalence in the embedding models.

## Foundational Learning

- **Concept: Code Embeddings (Transformer-based)**
  - Why needed here: The paper benchmarks SVMAda and CodeBERTa, which rely on converting source code into high-dimensional vectors. Understanding that code is treated as a sequence of tokens (like natural language) is essential to grasp why an SVM can classify it.
  - Quick check question: How does the tokenization of a Python function differ from a natural language sentence when fed into CodeBERTa?

- **Concept: Dataset Contamination / Temporal Leakage**
  - Why needed here: The authors explicitly use CodeNet data from pre-2021 to ensure human samples are "pure" and not AI-assisted.
  - Quick check question: Why would using StackOverflow snippets from 2024 as "human" ground truth risk invalidating a detection benchmark?

- **Concept: Few-Shot / Zero-Shot Classification**
  - Why needed here: The "Cross LLM" experiment (Table 7) is a leave-one-out test. This mimics a real-world scenario where a detector encounters code from a new model not seen during training.
  - Quick check question: In the cross-model setup, why does performance degrade for specific models like Llama (F1 0.66) while remaining high for others like GPT-4o (F1 0.92)?

## Architecture Onboarding

- **Component map:** Source Data (CodeNet) -> Generation Engine (6 LLMs, 3 prompts, 3 scenarios) -> Post-Processor (AST Parsers, Heuristics) -> Detection Head (Ada/CodeT5+ embeddings -> SVM or CodeBERTa fine-tuned)
- **Critical path:** The Post-Processing stage is the critical bottleneck. Raw LLM outputs often contain triple backticks or explanations. If the AST parser is too strict, you lose data; if too lenient, you introduce noise (markdown) into the classifier.
- **Design tradeoffs:**
  - Reasoning Models (o3-mini) vs. Speed: o3-mini has high accuracy but the highest "missing" rate (timeouts/failures to respond). An architecture prioritizing data yield must handle latency/retries or discard reasoning models for high-volume generation.
  - Monolithic vs. Scenario-Specific Models: Results suggest a "one-model-fits-all" approach is weak. You likely need specialized detectors for "Fixed Code" vs. "Generated Code."
- **Failure signatures:**
  - The "Fixer" Blindspot: Standard detectors drop ~20% accuracy on code that has been "fixed" or edited by an AI rather than generated from scratch.
  - The Go Language Barrier: Cross-language training fails specifically for Go (likely due to lower representation in training sets compared to Python/Java).
- **First 3 experiments:**
  1. Baseline Validation (Scenario Scratch): Train SVMAda on the "Scratch" split for Python. Verify you achieve ~0.91+ Accuracy to validate the pipeline matches the paper.
  2. Stress Test (Scenario Output): Evaluate the same model on the "Output Correction" split. Confirm the performance drop (expect ~0.70 accuracy) to understand the "signal dilution" phenomenon.
  3. Syntax Removal Ablation: Remove all comments and normalize variable names in the dataset, then re-train. This tests if the detector relies on surface-level style (comments) or deeper semantic logic.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Generalization: Only uses 800 problems from CodeNet, which may not represent the full diversity of real-world coding tasks
- Detection Mechanism Transparency: Does not fully explain how embeddings capture stylistic differences at a feature level
- Scenario-Specific Performance Gaps: Lacks detailed analysis of which specific code characteristics cause signal dilution in AI-fixed code

## Confidence
- **High Confidence**: Dataset construction methodology, scenario-based experimental design, and overall benchmark results are clearly documented and reproducible
- **Medium Confidence**: Explanation of detection mechanisms (stylistic artifacts, scenario signal dilution, embedding geometry) is plausible but not fully validated with ablation studies or adversarial testing
- **Low Confidence**: Assertion that cross-language detection fails due to "lower representation" in embedding models is speculative without testing this hypothesis

## Next Checks
1. Ablation on Code Features: Remove comments and normalize variable names from the dataset, then retrain detectors to confirm whether detection relies on surface-level style or deeper semantic logic
2. Adversarial Prompting Test: Generate code using prompts that explicitly instruct LLMs to mimic human verbosity and commenting style, then evaluate whether detection accuracy degrades as predicted
3. Embedding Analysis: Visualize and compare the embedding spaces (e.g., Ada, CodeBERTa) for human vs. AI code across languages to verify whether the claimed "separation" is consistent and language-dependent