---
ver: rpa2
title: 'Cross-Lingual Generalization and Compression: From Language-Specific to Shared
  Neurons'
arxiv_id: '2506.01629'
source_url: https://arxiv.org/abs/2506.01629
tags:
- uni00000045
- uni00000052
- uni00000043
- uni00000058
- uni00000030
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how multilingual language models (MLLMs)
  develop cross-lingual generalization during pre-training. The authors analyze three
  MLLM families (BLOOM-560M, BLOOM-7B, and a custom 257M parameter model) by examining
  internal representations across training checkpoints.
---

# Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons

## Quick Facts
- arXiv ID: 2506.01629
- Source URL: https://arxiv.org/abs/2506.01629
- Reference count: 27
- Key outcome: MLLMs develop shared cross-lingual representations through compression, with middle layers showing substantial neuron overlap across languages

## Executive Summary
This paper investigates how multilingual language models develop cross-lingual generalization during pre-training by analyzing internal representations across three MLLM families (BLOOM-560M, BLOOM-7B, and a custom 257M parameter model). The authors find that language-specific information decreases in middle layers while final layers maintain strong identification capabilities, indicating functional specialization. Neuron analysis reveals that semantic concept representations gradually align across languages during training, with substantial overlap emerging in middle layers (10-17). The behavioral manipulation experiments demonstrate that later checkpoints can generate English text when guided by Spanish or Chinese-derived neurons, supporting the hypothesis that capacity constraints force the development of shared cross-lingual representations rather than maintaining separate language-specific encodings.

## Method Summary
The authors analyze multilingual language models through a comprehensive probing approach across multiple training checkpoints. They use language identity probes to measure how much language-specific information is encoded at different layers, tracking changes from initial to later training stages. Neuron analysis examines semantic concept representations by identifying neurons selective to specific concepts across different languages, measuring overlap and alignment patterns. The behavioral manipulation experiments involve deriving concept-specific neurons from non-English data and using them to steer English text generation, demonstrating cross-lingual generalization. The study tracks language identity probing accuracy decreasing from ~70% to ~57% in early layers and measures neuron overlap, finding approximately 1/6 of top 500 concept-selective neurons shared between language pairs.

## Key Results
- Language-specific information decreases in middle layers during pre-training while final layers maintain strong identification capabilities
- Substantial neuron overlap emerges in middle layers (10-17) across languages, with approximately 1/6 of top 500 concept-selective neurons shared between language pairs
- Later training checkpoints can generate English text when guided by concept-specific neurons derived from Spanish or Chinese data
- Cross-lingual generalization develops as models are forced to compress representations under capacity constraints

## Why This Works (Mechanism)
The cross-lingual generalization emerges through a compression mechanism where model capacity constraints force the development of shared representations rather than maintaining separate language-specific encodings. During pre-training, the model initially encodes language-specific information throughout all layers, but as training progresses, middle layers gradually shift toward language-agnostic representations while final layers preserve language-specific capabilities for generation. This functional specialization allows the model to efficiently represent semantic concepts across languages using shared neural resources. The neuron overlap analysis shows that semantic concepts become increasingly aligned across languages, with the same neurons encoding similar concepts regardless of the input language. This compression-driven sharing enables the behavioral manipulation experiments where neurons derived from one language can effectively guide generation in another language.

## Foundational Learning
- **Multilingual language models**: Neural networks trained on multiple languages simultaneously, requiring mechanisms to handle linguistic diversity while maintaining efficiency
  - Why needed: Understanding how models process and represent multiple languages is crucial for developing efficient multilingual systems
  - Quick check: Verify model architecture supports multiple languages and has been trained on diverse linguistic data

- **Language identity probing**: Using classifiers to measure how much information about the input language is encoded in model representations
  - Why needed: Quantifies the degree of language-specific versus language-agnostic representations at different layers
  - Quick check: Ensure probe accuracy reflects meaningful language discrimination rather than random guessing

- **Neuron selectivity analysis**: Identifying neurons that respond strongly to specific semantic concepts across different languages
  - Why needed: Reveals how semantic representations align or diverge across languages in the model's internal representations
  - Quick check: Validate that identified neurons consistently activate for their target concepts across multiple examples

## Architecture Onboarding

Component map: Input -> Embedding layer -> Transformer blocks (layers 0-23) -> Output layer

Critical path: The middle layers (10-17) are identified as the critical region where cross-lingual generalization occurs, showing the highest degree of neuron overlap and semantic alignment across languages.

Design tradeoffs: The model must balance between maintaining language-specific representations for accurate generation and developing shared semantic representations for efficiency. The compression hypothesis suggests that capacity constraints force this tradeoff toward shared representations in middle layers.

Failure signatures: If cross-lingual generalization fails to emerge, the model would maintain high language identity accuracy throughout all layers, indicating persistent language-specific encoding without compression-driven sharing.

Three first experiments:
1. Apply language identity probes at each layer to track how language-specific information evolves during training
2. Identify top concept-selective neurons for specific semantic categories in each language and measure their overlap
3. Manipulate identified neurons from non-English languages to steer English text generation and observe cross-lingual effects

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on a limited set of languages (primarily English, Spanish, and Chinese), potentially missing patterns in other language families
- Probe-based methods may not capture all aspects of language-specific representations, particularly for morphologically complex languages
- Behavioral manipulation experiments use synthetic steering methods that may not reflect natural language generation patterns

## Confidence
- High confidence: Findings about middle layer specialization and final layer maintenance of language-specific information, supported by consistent patterns across multiple model sizes and probing methods
- Medium confidence: Claims about neuron overlap and semantic alignment across languages, supported by systematic analysis but requiring broader language coverage
- Medium confidence: Behavioral evidence for cross-lingual generalization through neuron manipulation, limited by artificial steering approach
- Medium confidence: Compression hypothesis linking capacity constraints to shared representations, showing correlation but not establishing causation

## Next Checks
1. Extend analysis to additional language families (Semitic, Dravidian, or Uralic languages) to test generality of cross-lingual generalization patterns
2. Compare representations using alternative probing methods beyond linear classifiers to validate robustness of language-specific information measurements
3. Conduct controlled experiments varying model capacity parameters to directly test whether compression effects drive shared representations versus other training dynamics