---
ver: rpa2
title: Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement
  Learning of Large Language Models
arxiv_id: '2509.26114'
source_url: https://arxiv.org/abs/2509.26114
tags:
- entropy
- training
- policy
- rlvr
- clipping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates entropy collapse in RLVR training of LLMs,\
  \ where models converge to deterministic behavior, limiting exploration. The authors\
  \ show that the clipping mechanisms in PPO/GRPO\u2014specifically clip-low and clip-high\u2014\
  introduce biases that affect entropy: clip-low increases entropy while clip-high\
  \ decreases it."
---

# Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models

## Quick Facts
- arXiv ID: 2509.26114
- Source URL: https://arxiv.org/abs/2509.26114
- Reference count: 40
- Primary result: Shows clip-low increases entropy while clip-high decreases it in RLVR, enabling entropy control through clipping hyperparameter tuning without KL penalties.

## Executive Summary
This paper investigates entropy collapse in RLVR training of LLMs, where models converge to deterministic behavior, limiting exploration. The authors show that the clipping mechanisms in PPO/GRPO—specifically clip-low and clip-high—introduce biases that affect entropy: clip-low increases entropy while clip-high decreases it. They prove this theoretically under random rewards and validate empirically across multiple model families (Qwen, Llama, Olmo). By tuning clipping hyperparameters, entropy can be controlled—using a more aggressive clip-low prevents collapse and improves exploration (measured by pass@k) without sacrificing reasoning performance (mean@k). This provides a practical, clipping-based alternative to KL penalties for stabilizing RLVR training.

## Method Summary
The authors analyze PPO/GRPO's clipped surrogate objective by decomposing it into clip-low and clip-high components. They prove theoretically that clip-low increases entropy while clip-high decreases it under random rewards, then validate empirically across multiple LLM families (Qwen2.5-1.5B/3B/7B, Llama3-8B/3.2-1B, OLMo-2-0425-1B) on GSM8K and DAPO-Math-17k datasets. The approach uses Dr. GRPO variant (no std normalization on advantages) with AdamW optimizer (lr=5e-7 or 1e-6), batch size 512, group size 8, and temperature 1.0. Key innovation is treating ε_low and ε_high as independent hyperparameters for entropy control, with experiments showing ε_low=0.15 and ε_high=∞ achieves balanced entropy maintenance.

## Key Results
- Entropy decreases under standard symmetric clipping (ε_low=ε_high=0.2) even with random rewards
- Disabling clip-high (ε_high=∞) and using aggressive clip-low (ε_low=0.15) maintains higher entropy
- High entropy training setups show improved pass@8 (exploration) while maintaining comparable mean@8 (performance)
- Theoretical analysis proves clip-low increases entropy and clip-high decreases entropy under random rewards

## Why This Works (Mechanism)

### Mechanism 1: Clip-low increases entropy
Clip-low (lower clipping threshold on negative advantages) increases policy entropy by blocking gradient contributions that would further decrease logits of low-probability actions. Under random rewards, the entropy change includes a positive clip-low contribution term proportional to P(X_k) × (E[Q] - E[Q|X_k]), where the condition E[Q] - E[Q|X_k] ≥ 0 typically holds. This prevents the policy from过度-suppressing low-probability actions, maintaining diversity in the action distribution.

### Mechanism 2: Clip-high decreases entropy
Clip-high (upper clipping threshold on positive advantages) decreases policy entropy by preventing "exploration tokens" from being reinforced beyond the threshold. When probability ratio exceeds 1+ε_high, further gradient updates that would increase these action probabilities are clipped. This asymmetrically suppresses the growth of diverse reasoning paths. Under standard symmetric clipping (ε_low = ε_high = 0.2), clip-high events dominate, causing net entropy reduction even with random rewards containing no learning signal.

### Mechanism 3: Deliberate clipping tuning for entropy control
By disabling clip-high (ε_high = ∞) and setting aggressive clip-low (ε_low = 0.15), the net clipping bias shifts toward entropy maintenance or increase. This counteracts RLVR's natural entropy reduction from suppressing incorrect reasoning paths. The controlled entropy preserves the model's ability to explore diverse solution trajectories (measured by pass@k) while maintaining single-sample accuracy (mean@k).

## Foundational Learning

- **Concept: PPO/GRPO clipping mechanism**
  - Why needed: The entire paper's contribution rests on understanding how the clipped surrogate objective works, specifically how clip-low and clip-high operate on negative and positive advantages respectively.
  - Quick check: Given a probability ratio r_t(θ) = 1.3 and advantage A_t = 0.5 with ε_high = 0.2, what is the clipped objective value?

- **Concept: Policy entropy in language models**
  - Why needed: Entropy is the core metric being analyzed; understanding what high vs. low entropy means for token distributions is essential.
  - Quick check: If a model has entropy approaching 0 at a token position, what does that imply about its output distribution?

- **Concept: pass@k vs. mean@k metrics**
  - Why needed: The paper uses these metrics to measure exploration vs. exploitation; pass@k captures whether any of k samples succeeds (exploration), while mean@k captures average performance.
  - Quick check: If pass@8 increases but mean@8 stays constant during training, what does this indicate about the model's behavior?

## Architecture Onboarding

- **Component map:** GRPO loss function with asymmetric clipping (ε_low, ε_high) -> Advantage estimation (Dr. GRPO: mean-centering only) -> Entropy monitoring (token-level Shannon entropy) -> Performance metrics (pass@k, mean@k)

- **Critical path:** 1) Start with symmetric clipping (ε_low=ε_high=0.2) and monitor entropy trajectory 2) If entropy collapses, disable clip-high (ε_high=∞) and tune ε_low downward 3) Target entropy level: maintain near base model entropy without explosion

- **Design tradeoffs:** Lower ε_low → more entropy, more exploration, risk of instability; Higher ε_low → less entropy control, risk of collapse; Disabling clip-high removes trust-region protection, may require smaller learning rates

- **Failure signatures:** Entropy collapse (>50% drop from base within 100 steps → clip-high too aggressive); Entropy explosion (>2× base → ε_low too low); Performance degradation with high entropy (mean@k drops significantly → entropy too high for task)

- **First 3 experiments:** 1) Random reward baseline with symmetric clipping (ε_low=ε_high=0.2) to confirm clip-high dominance 2) Clip ablation: disable clip-low (ε_low=1.0) and clip-high (ε_high=∞) separately to isolate effects 3) Entropy control tuning: grid search ε_low ∈ {0.1, 0.15, 0.2, 0.3} with ε_high=∞, evaluate pass@8 and mean@8

## Open Questions the Paper Calls Out

### Open Question 1
How can clipping hyperparameters be systematically tuned to maximize downstream performance, given that performance optimization correlates with but is not equivalent to maintaining appropriate entropy levels? The paper demonstrates entropy control through clipping but does not establish a principled method for finding performance-optimal clipping configurations.

### Open Question 2
Under what conditions do the key inequalities (E[Q]−E[Q|X_k]≥0 and E[Q]−E[Q|Y_k]≥0) fail, and what entropy dynamics emerge when they do? The paper empirically validates the inequalities hold in their settings but does not characterize failure modes or their consequences.

### Open Question 3
Can the theoretical analysis be extended to relax the assumptions of random rewards and simplified optimization (policy gradient/natural policy gradient) to cover realistic GRPO with AdamW optimization? The theoretical results assume random rewards and simplified update rules for tractability, while empirical validation uses standard GRPO with AdamW.

### Open Question 4
How should optimal clipping configurations adapt across different model families, scales, and task domains beyond the mathematical reasoning tasks tested? Experiments cover Qwen, Llama, and Olmo models on math tasks only, but generalizability to other domains remains untested.

## Limitations

- Theoretical claims rely on conditions (E[Q]−E[Q|X_k]≥0, E[Q]−E[Q|Y_k]≥0) that may not hold universally, with counterexamples constructible
- Limited domain generalization—effectiveness only demonstrated on mathematical reasoning tasks (GSM8K, DAPO-Math-17k)
- Does not benchmark against KL regularization, making it unclear whether clipping offers advantages beyond entropy control alone

## Confidence

**High Confidence**: The empirical observation that standard symmetric clipping leads to entropy reduction even with random rewards is well-supported by controlled experiments across multiple model families (Qwen, Llama, Olmo).

**Medium Confidence**: The theoretical derivation showing clip-low increases entropy under random rewards is mathematically sound, but relies on assumptions about the reward distribution that may not hold universally.

**Low Confidence**: The claim that this clipping-based approach provides a practical alternative to KL penalties for stabilizing RLVR training lacks direct comparison with KL-regularized training.

## Next Checks

1. **Theoretical boundary analysis**: Systematically identify and characterize conditions where E[Q] - E[Q|X_k] < 0 occurs, and test whether clip-low still increases entropy under these adversarial conditions.

2. **Cross-domain generalization study**: Apply the asymmetric clipping approach to non-mathematical reasoning tasks (e.g., code generation, creative writing, instruction following) and measure whether entropy control via clipping generalizes or requires domain-specific hyperparameter tuning.

3. **Comparison with KL regularization**: Implement a parallel RLVR training pipeline using KL divergence penalties with tuned β hyperparameters, and directly compare entropy trajectories, pass@k vs mean@k tradeoff curves, and final task performance.