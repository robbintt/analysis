---
ver: rpa2
title: Is Relevance Propagated from Retriever to Generator in RAG?
arxiv_id: '2502.15025'
source_url: https://arxiv.org/abs/2502.15025
tags:
- context
- relevance
- documents
- https
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether topical relevance in retrieved documents
  translates to improved answer quality in retrieval-augmented generation (RAG) systems.
  It defines utility as the relative improvement in answer quality when using retrieved
  context versus zero-shot generation, and correlates this with retrieval effectiveness
  (nDCG).
---

# Is Relevance Propagated from Retriever to Generator in RAG?

## Quick Facts
- **arXiv ID:** 2502.15025
- **Source URL:** https://arxiv.org/abs/2502.15025
- **Authors:** Fangzheng Tian; Debasis Ganguly; Craig Macdonald
- **Reference count:** 40
- **Primary result:** Higher topical relevance in retrieval does not guarantee proportional improvements in downstream generation quality; correlation between context relevance and utility decreases as context size increases.

## Executive Summary
This study investigates whether topical relevance in retrieved documents translates to improved answer quality in retrieval-augmented generation (RAG) systems. The authors define utility as the relative improvement in answer quality when using retrieved context versus zero-shot generation, and correlate this with retrieval effectiveness (nDCG). Using IR test collections and LLMs, they find that while more effective retrieval models generally yield higher utility, the correlation between context relevance and utility is small and decreases as context size increases. This suggests that relevance is not fully transmitted from retrieval to generation, and that simply increasing context size or improving retrieval effectiveness has diminishing returns on answer quality.

## Method Summary
The study uses TREC DL'19 (43 queries) and DL'20 (54 queries) test collections with the MS MARCO passage collection. Retrieval is performed using BM25 and MonoT5 (re-ranking top-100 BM25 results). Context sizes of k ∈ {2, 5, 10, 15} are tested. Answers are generated using a quantized Llama-3-8b model with a provided prompt template. Utility is calculated as the relative improvement in BERTScore F1 (semantic similarity to ground truth relevant documents) compared to zero-shot generation. The correlation between nDCG@k and utility is measured using Pearson's r.

## Key Results
- Higher topical relevance (nDCG) in retrieval does not guarantee proportional improvements in downstream generation quality (utility).
- The correlation between context relevance and utility is small and decreases as context size increases.
- While effective retrieval models (MonoT5) generally yield higher utility, the gains become statistically indistinguishable from perfect retrieval beyond a saturation point.

## Why This Works (Mechanism)

### Mechanism 1: Relevance-Utility Decoupling
The generator (LLM) acts as a filter that does not uniformly process all retrieved information. Topical overlap (relevance) does not equate to answer-bearing utility. The LLM generates answers based on a synthesis of context, but this synthesis is imperfect and prone to ignoring relevant but semantically distant or poorly integrated context. This mechanism breaks if the retriever optimizes for utility rather than topical relevance.

### Mechanism 2: Context Length Dilution
As context size grows, the probability of including non-relevant or distracting documents increases, degrading the signal-to-noise ratio. The LLM has a finite attention capacity or is susceptible to "noise" that disrupts the effective use of highly relevant documents when they are surrounded by others. This mechanism breaks if the context is effectively filtered for noise before generation.

### Mechanism 3: Retrieval Effectiveness Saturation
Beyond a certain point, significant gains in retrieval effectiveness result in statistically indistinguishable utility gains. The generator's performance is bounded by its intrinsic reasoning capabilities or the quality of ground truth relevant documents, not just the precision of the retriever. This mechanism breaks if the downstream task requires extremely precise evidence where small retrieval improvements matter more.

## Foundational Learning

- **Concept: Utility vs. Relevance**
  - **Why needed here:** The paper's central thesis relies on distinguishing these two. Relevance is the standard IR metric (topical match), whereas Utility is defined as the relative improvement in generation quality compared to zero-shot.
  - **Quick check question:** If an LLM answers a question perfectly without context, and the retrieved documents are relevant but add no new info, is the utility high or low?

- **Concept: BERTScore (FBERT)**
  - **Why needed here:** This metric evaluates "answer quality" by computing semantic similarity between the generated answer and the ground truth relevant documents (qrels).
  - **Quick check question:** Why might BERTScore be preferred over lexical overlap (BLEU) for evaluating semantic answer quality in this context?

- **Concept: nDCG (Normalized Discounted Cumulative Gain)**
  - **Why needed here:** This is the independent variable representing "retrieval effectiveness." It measures the quality of the ranked list of context documents based on graded relevance judgments.
  - **Quick check question:** Does a high nDCG score imply that the documents are "useful" for generation, or just that they are topically relevant?

## Architecture Onboarding

- **Component map:** Retriever -> Context Builder -> Generator -> Evaluator
- **Critical path:**
  1. Retrieve top-$k$ docs for a query
  2. Construct prompt with query and docs
  3. Generate answer $a_q$
  4. Compute similarity $P(a_q)$ against qrels
  5. Calculate Utility $U$ as relative gain over 0-shot $P(a_q)$

- **Design tradeoffs:**
  - Lexical (BM25) vs. Neural (MonoT5): Neural provides higher relevance (nDCG) and generally higher utility, but BM25 remains competitive for larger context sizes
  - Context Size ($k$): Higher $k$ increases absolute utility but decreases the correlation with relevance (more noise)
  - Ordering: Reversing document order (least relevant first) hurts performance for large $k$, but matters less for small $k$

- **Failure signatures:**
  - High nDCG, Low Utility: "Relevance lost in transmission" — the retriever found good docs, but the LLM failed to use them
  - Negative Utility: Retrieved context actively degrades performance compared to zero-shot (often caused by adversarial/non-relevant context)

- **First 3 experiments:**
  1. Zero-shot Baseline: Generate answers using only the query to establish inherent LLM capability ($P(a_q)$ with empty context)
  2. Correlation Analysis: For a fixed $k$ (e.g., 5), plot nDCG vs. Utility across multiple queries to measure Pearson's $r$
  3. Scaling Analysis: Vary $k$ (e.g., 1 to 15) and observe the trend line of correlation coefficients to confirm the "dilution" effect

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can novel retrieval models be designed to optimize for downstream task utility directly, rather than just topical relevance, to mitigate the "lost in transmission" effect?
- **Basis in paper:** [explicit] The conclusion states that the small positive correlation "leaves the scope of devising novel retrieval models in the future, which will be aware of the downstream task objective in addition to topical relevance."
- **Why unresolved:** Current models optimize for topical relevance (nDCG), which the paper shows does not fully propagate to generation quality.
- **What evidence would resolve it:** Developing and testing a retrieval model trained to maximize generated answer similarity (utility) rather than standard relevance metrics.

### Open Question 2
- **Question:** How can Query Performance Prediction (QPP) methods be adapted to estimate the relevance of a RAG context for per-query workflow control without relying on pre-existing relevance assessments?
- **Basis in paper:** [explicit] The authors state they "would like to apply the findings... to a more practical RAG workflow setup without the knowledge of any relevance assessments" using QPP.
- **Why unresolved:** Current utility measurement relies on ground-truth qrels, which are unavailable in practical, real-time RAG applications.
- **What evidence would resolve it:** A QPP model that correlates highly with context utility on unseen queries, allowing dynamic adjustment of context size or retrieval strategy.

### Open Question 3
- **Question:** Does the "Lost in the Middle" phenomenon, where relevant documents at the center of the context are ignored, occur when relevance is defined by topical overlap rather than answer containment?
- **Basis in paper:** [explicit] The authors note their study "cannot comment on the effect of including relevant documents in the middle of a RAG context" because their relevance definition (topical) differs from the answer containment definition used in prior work.
- **Why unresolved:** The paper only tested reversing the order (highest to lowest relevance) but did not construct specific "middle-heavy" contexts to test this specific positional bias for topical relevance.
- **What evidence would resolve it:** Experiments reconstructing contexts where relevant documents are placed in the middle indices and measuring the resulting utility drop compared to top/bottom placement.

## Limitations

- The study uses a single LLM (Llama-3-8b) and two retrieval models, which may not generalize to other architectures or domains.
- The semantic threshold at which the "relevance is not fully transmitted" claim becomes practically significant is not established.
- The study is limited to TREC DL factoid questions and may not generalize to non-factoid QA tasks or open-domain generation.

## Confidence

- **High confidence:** The empirical observation that average utility increases with context size while correlation with relevance decreases.
- **Medium confidence:** The claim that retrieval effectiveness beyond a certain point yields diminishing utility returns.
- **Low confidence:** The generalizability of the "relevance-utility decoupling" mechanism to non-factoid QA tasks or open-domain generation.

## Next Checks

1. **Cross-domain validation:** Replicate the correlation analysis on non-factoid QA datasets (e.g., Natural Questions, TriviaQA) to test whether the relevance-utility decoupling persists across question types.
2. **Alternative utility definitions:** Measure utility using different answer quality metrics (e.g., ROUGE, human judgment) to verify that the observed phenomena are not artifacts of the BERTScore choice.
3. **Zero-shot capability control:** For queries where zero-shot performance is already high, verify whether retrieved context consistently degrades utility, confirming the "noise" hypothesis in mechanism 2.