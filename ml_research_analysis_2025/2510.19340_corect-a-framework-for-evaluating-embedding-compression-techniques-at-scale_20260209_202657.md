---
ver: rpa2
title: 'CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale'
arxiv_id: '2510.19340'
source_url: https://arxiv.org/abs/2510.19340
tags:
- compression
- retrieval
- corpus
- performance
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CoRECT, a framework for large-scale evaluation\
  \ of embedding compression techniques in dense retrieval. The authors address the\
  \ challenge of memory constraints in retrieval systems by systematically comparing\
  \ compression methods\u2014such as scalar quantization, vector truncation, and learning-to-hash\u2014\
  across varying corpus sizes (10K to 100M) and retrieval granularities."
---

# CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale

## Quick Facts
- arXiv ID: 2510.19340
- Source URL: https://arxiv.org/abs/2510.19340
- Authors: L. Caspari; M. Dinzinger; K. Ghosh Dastidar; C. Fellicious; J. Mitrović; M. Granitzer
- Reference count: 40
- Primary result: Framework evaluates embedding compression across 10K-100M corpora, showing non-learned methods (SBQ) achieve near-lossless compression with minimal performance degradation.

## Executive Summary
This paper introduces CoRECT, a framework for large-scale evaluation of embedding compression techniques in dense retrieval. The authors address the challenge of memory constraints in retrieval systems by systematically comparing compression methods—such as scalar quantization, vector truncation, and learning-to-hash—across varying corpus sizes (10K to 100M) and retrieval granularities. A key contribution is the CoRE dataset collection, which uses intelligent subsampling to maintain retrieval difficulty at smaller corpus sizes. Experiments on four embedding models show that non-learned compression achieves substantial size reduction with minimal performance loss, but no single method is universally optimal. The work highlights the importance of corpus complexity and model-specific evaluation in selecting compression techniques. All code and data are publicly available.

## Method Summary
CoRECT systematically evaluates embedding compression methods across five corpus sizes (10K-100M) and multiple retrieval granularities. The framework uses CoRE datasets with intelligent subsampling to preserve retrieval difficulty, evaluates four transformer-based embedding models, and compares eight compression techniques including quantization, truncation, and hashing. Compression parameters are learned per batch, and retrieval performance is measured using NDCG@10, Recall@100, and MRR. The framework includes statistical significance testing and provides both CoRE and BeIR datasets for comprehensive evaluation.

## Key Results
- Non-learned compression methods (scalar binary quantization) outperform learned methods (PCA, LSH) at scale
- Compression-induced performance degradation is amplified by corpus complexity, with larger corpora showing greater sensitivity to compression
- No single compression method is universally optimal across all models and corpus sizes
- Retrieval granularity matters: document-level retrieval degrades faster than passage-level under compression
- Intelligent subsampling preserves retrieval difficulty better than naive random sampling

## Why This Works (Mechanism)

### Mechanism 1
Intelligent subsampling from TREC pooled rankings preserves retrieval difficulty that naive random sampling destroys. The framework mines distractor documents from TREC submission runs, discards bottom 20% by effectiveness, applies Reciprocal Rank Fusion to merge top-ranked documents, and selects the top 100 irrelevant/unjudged documents per query as "hard negatives" before filling remaining corpus slots with random documents. Core assumption: TREC submission runs contain meaningful hard negatives that represent realistic retrieval challenges.

### Mechanism 2
Scalar and binary quantization preserves retrieval performance better than dimensionality truncation at equivalent compression ratios. Quantization reduces bits-per-dimension while retaining full vector dimensionality, preserving pairwise distance relationships across the full semantic space; truncation removes dimensions entirely, which discards learned semantic signals unless the model was explicitly trained (MRL) to concentrate information in early dimensions. Core assumption: Semantic information is distributed across embedding dimensions rather than concentrated in a subset.

### Mechanism 3
Compression-induced performance degradation is amplified by corpus complexity (size and granularity), but the effect is metric-dependent. Larger corpora populate the semantic space more densely, causing compressed embeddings to confuse semantically similar documents; document-level retrieval degrades faster than passage-level due to longer texts creating more overlap opportunities; however, top-10 rankings remain stable longer than top-100 because random documents rarely achieve top-10 similarity scores. Core assumption: The semantic neighborhood becomes more crowded as corpus complexity increases.

## Foundational Learning

- **Concept**: Scalar and Binary Quantization
  - **Why needed here**: The paper's strongest results show SBQ methods achieve near-lossless compression; understanding how binning strategies (equidistant vs. percentile) and thresholding (zero vs. median) affect performance is essential for method selection.
  - **Quick check question**: Can you explain why percentile binning might outperform equidistant binning when embeddings contain outliers?

- **Concept**: Matryoshka Representation Learning (MRL)
  - **Why needed here**: MRL-trained models (Jina V3, Snowflake V2) tolerate aggressive truncation that catastrophically degrades non-MRL models (E5, Snowflake V1); knowing whether your embedding model supports MRL determines viable compression strategies.
  - **Quick check question**: If an MRL model was trained on cutoffs [32, 64, 128, 256, 512], would truncating to 200 dimensions likely outperform truncating to 128?

- **Concept**: Corpus Complexity in Dense Retrieval
  - **Why needed here**: The paper demonstrates that compression performance cannot be evaluated on small corpora and extrapolated to production scales; understanding how corpus size and retrieval granularity affect baseline and compressed performance prevents deployment surprises.
  - **Quick check question**: Why might a compression method show <1% degradation on a 100K corpus but 15% degradation on a 100M corpus?

## Architecture Onboarding

- **Component map**: Data Layer (CoRE datasets) -> Embedding Layer (Transformer models) -> Compression Layer (Modular compression implementations) -> Retrieval Layer (Cosine similarity search) -> Evaluation Layer (Multi-metric computation)
- **Critical path**: Load model and verify native precision (TensorType from HuggingFace) -> Generate embeddings in batches with consistent random seed -> Apply compression per-batch, learning thresholds/parameters on current batch only -> Apply same learned parameters to query embeddings -> Compute retrieval metrics against ground-truth relevance labels -> Statistical significance testing (Wilcoxon, p=0.05) against uncompressed baseline
- **Design tradeoffs**: Per-batch vs. global parameter learning (per-batch is simpler but may introduce inconsistency; global requires pre-scanning entire corpus); CoRE-only vs. CoRE+BeIR evaluation (CoRE enables controlled complexity analysis; BeIR validates generalization but lacks corpus size variation); Recall@100 vs. NDCG@10 as primary metric (Recall measures first-stage retriever quality; NDCG measures end-user ranking quality)
- **Failure signatures**: Truncation ratio exceeds MRL cutoffs on non-MRL model -> rapid Recall degradation starting at 1M corpus; Median thresholding on E5 embeddings -> Recall@100 drops from 84% to 19% at 100M corpus; Binary quantization at 100M+ corpus without rescoring -> relevant documents pushed below rank 100
- **First 3 experiments**: Baseline characterization (Run full-precision retrieval on all CoRE corpus sizes for your target embedding model to establish degradation profile; compare against Table 2 values to verify model behavior); Compression method sweep at fixed ratio (At 8× compression, compare equidistant binning, percentile binning, MRL truncation (if applicable), and PQ across 1M and 10M corpora to identify model-specific winners); Combined method exploration (For your best single method at 8×, test combining moderate truncation (2×) with quantization (4×) to achieve 8× total; compare against direct 8× quantization following Figure 3 patterns)

## Open Questions the Paper Calls Out

### Open Question 1
Can modern Learning-to-Hash (LTH) methods outperform the non-learned compression baselines established in this study when evaluated on the CoRE dataset? Basis in paper: Section 5 identifies the exclusion of recent LTH approaches (like JPQ or Distill-VQ) as a clear limitation, stating that integrating them is a "valuable extension" for future work. Why unresolved: The authors deliberately excluded train-intensive LTH methods to focus on simpler, widely supported baselines like scalar quantization and PCA. What evidence would resolve it: Benchmarking state-of-the-art LTH methods within the CoRECT framework, specifically on the 100M passage corpus, to compare against the SBQ and truncation results.

### Open Question 2
Why does binary quantization via zero thresholding cause significant performance degradation for the E5 model while maintaining performance for others like Snowflake V2? Basis in paper: Section 4.2 notes that zero thresholding fails specifically for E5, stating, "a deeper interpretation of the underlying causes is left for future work." Why unresolved: The paper identifies the model-specific failure mode empirically but does not analyze the distributional properties (e.g., mean centering) of the E5 embeddings that cause zero thresholding to be less effective than median thresholding. What evidence would resolve it: A statistical analysis of the embedding value distributions for all models to determine if E5 embeddings are biased away from zero, causing aggressive information loss during binarization.

### Open Question 3
How do the evaluated compression techniques compare in terms of computational overhead (indexing time and query latency) relative to their storage savings? Basis in paper: Section 5 explicitly lists a limitation: "we only evaluate compression techniques based on their performance, ignoring other factors like their storage efficiency, computation time and retrieval speed." Why unresolved: While the paper proves that high compression ratios preserve ranking quality, it does not verify if these methods introduce latency penalties that would make them unsuitable for real-time production systems. What evidence would resolve it: Extending the CoRECT framework to log query latency and indexing throughput for each method, establishing a Pareto frontier of speed vs. accuracy.

## Limitations
- Framework excludes advanced Learning-to-Hash methods like JPQ and BPR due to implementation complexity
- Only evaluates compression techniques based on retrieval performance, ignoring computational overhead and storage efficiency
- Uses per-batch compression parameter learning which may introduce inconsistencies across batches

## Confidence
- High: Core findings that corpus complexity dramatically affects compression performance and that non-learned methods outperform learned methods at scale
- Medium: Specific ranking of compression methods, as performance is highly model-dependent and optimal choice varies with corpus size and granularity
- Low: Generalizability beyond evaluated models and datasets, as only considers dense retrieval with transformers-based embeddings

## Next Checks
1. **Global vs. Batch Parameter Learning**: Compare per-batch compression parameter learning against global parameter learning on a subset of CoRE corpora to quantify consistency effects and identify optimal learning strategies.
2. **Advanced Method Implementation**: Implement JPQ and BPR compression methods to evaluate whether their exclusion significantly impacted the method ranking conclusions, particularly for high-compression scenarios.
3. **Cross-Dataset Generalization**: Evaluate the top-performing compression methods from CoRECT on additional BeIR datasets not included in the original study to validate the robustness of the findings across different retrieval domains and document types.