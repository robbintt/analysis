---
ver: rpa2
title: Improving Phishing Email Detection Performance of Small Large Language Models
arxiv_id: '2505.00034'
source_url: https://arxiv.org/abs/2505.00034
tags:
- llms
- phishing
- email
- detection
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores using small LLMs for phishing email detection
  to reduce computational costs compared to large models. Researchers employed LLaMA-3.2-3B-Instruct,
  Phi-4-mini-Instruct, and Qwen-2.5-1.5B-Instruct models (around 3 billion parameters)
  and enhanced their performance through Prompt Engineering, Explanation Augmented
  Fine-tuning, and Model Ensemble techniques.
---

# Improving Phishing Email Detection Performance of Small Large Language Models

## Quick Facts
- **arXiv ID:** 2505.00034
- **Source URL:** https://arxiv.org/abs/2505.00034
- **Authors:** Zijie Lin; Zikang Liu; Hanbo Fan
- **Reference count:** 29
- **Key result:** Fine-tuned small LLMs (3B parameters) achieved 0.860-0.968 accuracy and 0.673-0.944 F1 scores on phishing detection, outperforming larger models and traditional ML approaches.

## Executive Summary
This study demonstrates that small large language models (around 3 billion parameters) can achieve state-of-the-art phishing email detection performance when enhanced with explanation-augmented fine-tuning and structured prompting. The researchers show that standard fine-tuning on label-only targets causes significant performance degradation due to misalignment with LLM pretraining objectives, but adding natural language explanations to training targets dramatically improves accuracy. Their approach enables efficient phishing detection using consumer-grade hardware while maintaining interpretability through generated reasoning.

## Method Summary
The researchers employed three small LLMs (LLaMA-3.2-3B-Instruct, Phi-4-mini-Instruct, and Qwen-2.5-1.5B-Instruct) and enhanced their performance through three key techniques: structured prompting requiring reasoning-before-answer, explanation-augmented fine-tuning using GPT-4o-mini-generated explanations, and model ensemble methods. The explanation-augmented approach transforms the task from closed-form classification to open-ended generation, aligning with LLM pretraining objectives. Models were fine-tuned using LoRA on 1,000 training samples from SpamAssassin, then evaluated on test sets and transferred to unseen datasets (Enron, Ling) to assess generalization.

## Key Results
- Without fine-tuning, small LLMs showed poor performance (accuracy 0.387-0.647, F1 0.354-0.537 on SpamAssassin)
- After explanation-augmented fine-tuning, accuracies reached 0.860-0.968 and F1 scores 0.673-0.944 on test sets
- Fine-tuned small LLMs outperformed traditional ML models and even surpassed larger LLMs like LLaMA-3.1-70B-Instruct in many cases
- Model ensemble methods provided additional modest improvements (up to ~0.975 accuracy on SpamAssassin)
- Strong transferability demonstrated with accuracies above 0.85 and F1 scores above 0.8 on various phishing email datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explanation-augmented fine-tuning dramatically improves small LLM phishing detection compared to label-only fine-tuning.
- **Mechanism:** Adding generated explanations to training targets transforms the task from closed-form classification to open-ended generation, reducing the distribution gap between pretraining and fine-tuning.
- **Core assumption:** LLMs pretrained on next-token prediction perform better when fine-tuning targets resemble natural language generation rather than single-label prediction.
- **Evidence anchors:** After explanation-augmented fine-tuning, accuracies reached 0.860-0.968; without explanations, F1 score plummeted from 0.928 to 0.219.
- **Break condition:** If explanation quality from GPT-4o-mini is poor or inconsistent, the alignment benefit degrades.

### Mechanism 2
- **Claim:** Structured prompting with reasoning-before-answer reduces output format violations and classification bias.
- **Mechanism:** Requiring the model to generate reasoning before the final label leverages its generative strengths and anchors the classification in explicit analysis.
- **Core assumption:** Small instruction-tuned LLMs can follow multi-step output formatting when given explicit structure markers.
- **Evidence anchors:** When simply asking LLMs to output "Phishing" or "Safe" as a label, they often fail to strictly adhere to the requested format.
- **Break condition:** If sequence length limits truncate reasoning or model lacks instruction-following capability for complex formats.

### Mechanism 3
- **Claim:** Model ensemble provides modest additional improvements over single fine-tuned models.
- **Mechanism:** Different LLMs capture complementary patterns; confidence-weighted or majority voting averages out individual model errors.
- **Core assumption:** Fine-tuned models make uncorrelated errors that ensemble methods can correct.
- **Evidence anchors:** On SpamAssassin, both ensemble methods achieved accuracies reaching approximately 0.975; on CEAS_08, neither ensemble method significantly outperformed the best individual model.
- **Break condition:** If models make correlated errors or individual model performance is already near ceiling.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables fine-tuning 3B parameter models on consumer GPUs by training only low-rank decomposition matrices instead of full weights.
  - Quick check question: Can you explain why LoRA reduces memory requirements compared to full parameter fine-tuning?

- **Concept: Generative vs. Discriminative Task Alignment**
  - Why needed here: Core insight of the paper—LLMs pretrained for next-token prediction underperform when fine-tuned for single-label classification; performance recovers when targets include natural language explanations.
  - Quick check question: Why would asking a generative model to output only "Phishing" or "Safe" underperform compared to asking it to explain first?

- **Concept: Length-Normalized Confidence Scoring**
  - Why needed here: Enables ensemble methods by computing comparable confidence scores across models using geometric mean of token probabilities.
  - Quick check question: Why use the Nth root of probability product rather than raw probability sum for confidence?

## Architecture Onboarding

- **Component map:** Input Email → Prompt Template → Small LLM (LLaMA/Phi/Qwen) → Training: Email + Label → GPT-4o-mini → Explanation → LoRA Fine-tuning → Inference: Reasoning + Answer extraction → Optional Ensemble → Final Label

- **Critical path:** Explanation generation quality → LoRA fine-tuning data quality → downstream detection performance. The GPT-4o-mini explanation generation step is the highest-leverage component.

- **Design tradeoffs:**
  - Small LLM (3B) vs. Large LLM (70B): ~21% accuracy gain possible with fine-tuning on small models vs. zero-shot large models, at fraction of inference cost
  - Ensemble vs. single model: +0.7-0.8% accuracy gain on some datasets, but 3x inference cost
  - With vs. without explanation augmentation: ~40% accuracy difference (critical)

- **Failure signatures:**
  - Output format violations (missing ### delimiters): prompt engineering issue
  - Strong classification bias (always "Safe" or always "Phishing"): fine-tuning data imbalance or label-only training
  - Poor transfer to new datasets: overfitting to training distribution

- **First 3 experiments:**
  1. Replicate vanilla prompting baseline on SpamAssassin to confirm poor initial performance (target: <0.65 accuracy)
  2. Fine-tune single model (suggest Phi-4-mini) with explanation-augmented data on 1000-sample subset; measure accuracy gain (target: >0.90 accuracy)
  3. Evaluate fine-tuned model on held-out dataset (Enron or Ling) to test transferability without retraining (target: >0.85 accuracy)

## Open Questions the Paper Calls Out

- **Question:** Can advanced ensemble strategies, such as weighted voting based on validation performance, significantly outperform the simple majority vote and confidence ensemble methods used in this study?
- **Basis in paper:** The authors state in the Limitations section that "Beyond confidence ensemble and simple majority voting, more advanced methods... were not explored in this work."
- **Question:** What is the precise, quantifiable cost-benefit ratio of using these fine-tuned small LLMs compared to large LLMs regarding inference latency and computational expense?
- **Basis in paper:** The paper notes, "Although it is intuitively less expensive to use small LLMs for inference, we did not specifically quantify the cost savings from using small LLMs for detection."
- **Question:** Can specific training techniques be designed to explicitly enhance the cross-domain transferability of small LLMs, rather than just observing their natural generalization?
- **Basis in paper:** The authors admit, "While we examined the transferability of our method, we did not further propose methods to improve the model's performance across various datasets."

## Limitations
- Critical hyperparameters including LoRA rank, learning rate, batch size, and training epochs are not disclosed, making exact reproduction difficult
- GPT-4o-mini explanation generation process is underspecified with unclear prompt templates and quality control measures
- Computational cost analysis is qualitative rather than quantitative, lacking explicit comparisons of inference latency and memory usage
- Transferability claims are based on limited testing (2 additional datasets) without comprehensive domain analysis

## Confidence

**High Confidence:** The core finding that explanation-augmented fine-tuning dramatically improves small LLM performance (40% accuracy gain) is well-supported by direct experimental evidence with clear before/after comparisons and consistent results across multiple datasets.

**Medium Confidence:** The transferability results showing strong performance on unseen datasets are based on limited testing (2 additional datasets) without comprehensive domain analysis, requiring broader validation.

**Low Confidence:** The claim that model ensemble provides "modest additional improvements" has inconsistent support, showing significant gains on SpamAssassin but negligible improvements on CEAS_08, suggesting dataset dependency.

## Next Checks

1. **Replication of Fine-Tuning Impact:** Replicate the core ablation experiment showing the 40% accuracy drop when fine-tuning without explanations by fine-tuning the same small LLM model on identical training data with only labels (no explanations).

2. **Cross-Dataset Generalization Study:** Systematically test the fine-tuned models on 3-5 additional phishing email datasets from different sources and time periods to quantify true transferability limits.

3. **Ensemble Robustness Analysis:** Conduct a comprehensive ensemble study varying the number of models (2-5), ensemble method (majority vote vs. confidence-weighted), and model selection criteria to determine if gains are reproducible or dataset-specific artifacts.