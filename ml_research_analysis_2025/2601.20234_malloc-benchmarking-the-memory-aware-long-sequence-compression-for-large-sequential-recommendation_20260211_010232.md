---
ver: rpa2
title: 'MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large
  Sequential Recommendation'
arxiv_id: '2601.20234'
source_url: https://arxiv.org/abs/2601.20234
tags:
- memory
- recommendation
- methods
- sequence
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MALLOC addresses the computational bottleneck in large sequential
  recommendation systems, where self-attention mechanisms incur quadratic complexity
  with long user interaction sequences. The paper introduces a comprehensive benchmark
  for memory-aware long sequence compression, organizing existing techniques by memory
  allocation granularity: sequence-level (e.g., Reformer, Linformer), token-level
  (e.g., Longformer, H2O), head-level (e.g., MQA, GQA), and precision-level (e.g.,
  KIVI, IntactKV).'
---

# MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation

## Quick Facts
- **arXiv ID:** 2601.20234
- **Source URL:** https://arxiv.org/abs/2601.20234
- **Reference count:** 39
- **Primary result:** No universally optimal compression approach; method selection depends on deployment-specific memory constraints

## Executive Summary
MALLOC addresses the computational bottleneck in large sequential recommendation systems, where self-attention mechanisms incur quadratic complexity with long user interaction sequences. The paper introduces a comprehensive benchmark for memory-aware long sequence compression, organizing existing techniques by memory allocation granularity: sequence-level (e.g., Reformer, Linformer), token-level (e.g., Longformer, H2O), head-level (e.g., MQA, GQA), and precision-level (e.g., KIVI, IntactKV). MALLOC evaluates these methods across three real-world datasets using both ranking metrics (AUC, GAUC, Logloss) and system-level constraints (MACs, memory occupation). Results show that while Native and Reformer achieve high accuracy, they demand substantial computational resources (up to 17.34 G MACs). Head-level methods like MLA offer balanced trade-offs with Native-level accuracy but significantly reduced MACs (≈276 M). Token-level pruning methods (H2O, SnapKV) minimize memory usage (≈2 MB) but sacrifice accuracy, particularly on long sequences. The benchmark reveals no universally optimal approach, emphasizing the need for deployment-specific memory-aware design choices.

## Method Summary
MALLOC benchmarks memory-aware long sequence compression methods for large sequential recommendation systems using the HSTU backbone architecture with FuxiCTR framework. The benchmark evaluates compression techniques across four granularities: sequence-level (Reformer, Linformer), token-level (Longformer, H2O, SnapKV), head-level (MQA, GQA, MLA), and precision-level (KIVI, IntactKV). Three real-world datasets are used: Amazon-Electronic (max length 128), MicroVideo1.7M (max length 1024), and KuaiVideo (max length 1024). Methods are assessed using ranking metrics (AUC, GAUC, Logloss) and system-level constraints (MACs, memory occupation). The evaluation reveals distinct Pareto frontiers for each granularity level, with head-level methods providing the most balanced trade-offs between accuracy and computational efficiency.

## Key Results
- No compression method dominates across all memory and accuracy constraints
- Head-level methods (MLA, MQA, GQA) achieve Native-level accuracy with 15-20× reduction in MACs
- Token-level pruning (H2O, SnapKV) reduces memory to 2 MB but causes accuracy loss, especially on long sequences
- Sequence-level methods (Reformer) eliminate persistent memory usage but require up to 17.34 G MACs
- Precision-level methods (KIVI, IntactKV) show marked performance degradation on long-sequence datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-computing and caching key-value (KV) states avoids quadratic re-computation during inference, while compression at varying granularities reduces the associated memory overhead.
- **Mechanism:** Instead of recomputing attention over full sequence history for each request, methods cache key-value pairs from prior computations. Different compression granularities (sequence, token, head, precision) reduce this cache to balance memory occupation versus accuracy retention.
- **Core assumption:** Historical interaction states can be lossily compressed without critically degrading recommendation quality beyond acceptable thresholds.
- **Evidence anchors:**
  - [abstract]: "Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost"
  - [section 1]: Empirical study showing "Storage Explosion" (545 GB for full KV cache) versus "Computation Surge" (450× MACs increase without caching)
  - [corpus]: Neighbor paper mentions "latency and memory constraints" in sequential recommendation but does not directly validate KV cache compression mechanisms
- **Break condition:** When compression ratio exceeds information preservation threshold—observed in precision-level methods (KIVI, IntactKV) showing "marked performance degradation, particularly on long-sequence datasets" per Section 4.3.

### Mechanism 2
- **Claim:** Restructuring attention patterns to limit token interactions reduces computational complexity from O(N²) toward O(N·M) or O(N·logN), though effectiveness depends on preserving behaviorally-relevant connections.
- **Mechanism:** Reformer uses locality-sensitive hashing to route queries only to hash-matched keys; Longformer employs sliding-window plus global tokens; Linformer projects to lower-dimensional space along the sequence dimension.
- **Core assumption:** Not all token-to-token interactions are equally important for predicting user intent.
- **Evidence anchors:**
  - [section 4.3]: Reformer "identifies the most relevant historical states for computation through local hashing based on candidate items... scoring scale is determined solely by relevant items"
  - [section 4.4]: Reformer achieves 17.34 G MACs on KuaiVideo versus 57.1 G for no-memory approach
  - [corpus]: Weak validation; corpus papers reference quadratic complexity problem but do not test attention restructuring in recommendation context
- **Break condition:** When relevant historical signals are excluded from attention window—pruning methods (H2O, SnapKV) maintain "stable but relatively lower performance" suggesting aggressive eviction removes critical behavioral information (Section 4.3).

### Mechanism 3
- **Claim:** Sharing or compressing key-value representations across attention heads maintains near-Native accuracy while proportionally reducing memory and computation relative to head reduction ratio.
- **Mechanism:** MQA shares a single KV head across all query heads; GQA groups heads for shared KV; MLA applies learned low-rank joint compression across heads.
- **Core assumption:** Redundancy exists across attention head representations that can be consolidated without losing user behavior modeling capacity.
- **Evidence anchors:**
  - [abstract]: "Head-level methods like MLA offer balanced trade-offs with Native-level accuracy but significantly reduced MACs (≈276 M)"
  - [section 4.3]: "MLA, MQA, and GQA deliver performance comparable to the Native baseline across all datasets"
  - [section 4.4]: MLA reduces MACs from 4.36 G to 276.25 M on KuaiVideo while maintaining 15.98 MB memory (vs. 127.88 MB for Native)
  - [corpus]: No corpus papers validate multi-head sharing specifically for recommendation tasks
- **Break condition:** When head diversity is critical for heterogeneous user interests—paper notes these methods "generally fail to surpass full-memory or merging-based techniques on long sequences" (Section 4.3).

## Foundational Learning

- **Concept: KV Caching in Autoregressive Inference**
  - **Why needed here:** Understanding how caching intermediate K/V states enables inference acceleration is prerequisite to grasping why compression trade-offs matter.
  - **Quick check question:** For a user with 1024 historical interactions, explain why recomputing full attention at each new request is more expensive than reusing cached states.

- **Concept: Attention Complexity Scaling**
  - **Why needed here:** The O(N²) bottleneck for sequence length N motivates all compression approaches in this benchmark.
  - **Quick check question:** Calculate the ratio of attention operations for sequence length 1024 versus 128—why does this matter for billion-user systems?

- **Concept: Pareto Frontier Analysis**
  - **Why needed here:** The paper explicitly constructs accuracy-resource trade-off curves; interpreting these is essential for deployment decisions.
  - **Quick check question:** If your system has strict 500 MB memory budget per batch, which methods from Figure 4 remain viable?

## Architecture Onboarding

- **Component map:** User sequence → item embeddings + historical click labels → Self-attention with memory-aware KV access → Compressed state storage per user → Binary CTR prediction
- **Critical path:**
  1. User sequence S_u → item embeddings + historical click labels
  2. Self-attention with memory-aware KV access pattern
  3. Compressed state storage per user (persistent across inference requests)
  4. Final probability prediction ŷ_u,v
- **Design tradeoffs:**
  - **Memory vs. Accuracy:** Native preserves all info (127.88 MB/user on KuaiVideo) vs. H2O/SnapKV at 2 MB with accuracy loss (GAUC drops from 0.691 to ~0.647)
  - **Computation vs. Memory:** Reformer uses 17.34 G MACs with zero persistent memory vs. Native at 4.36 G MACs with 127.88 MB storage
  - **Engineering complexity:** MQA/GQA/MLA marked as low implementation cost; RWKV and Beacon require substantial architectural changes (Table 4)
- **Failure signatures:**
  - **Gradient explosion:** Longformer and Activation Beacon "fail to converge when the number of blocks exceeds moderate depth" (Section 4.6)
  - **Accuracy collapse on long sequences:** Pruning and precision-level methods show "marked performance degradation" on MicroVideo/KuaiVideo (≥1024 tokens)
  - **Data leakage risk:** Linformer anomaly on short sequences attributed to "compression along timesteps interacting with label-concatenated inputs" (Section 4.3)
- **First 3 experiments:**
  1. **Baseline calibration:** Replicate Native vs. w.o. Mem comparison on KuaiVideo to quantify the Memory-Latency Dilemma—expect ~450× computation difference per Section 1.
  2. **Head-level validation on varying sequence lengths:** Compare MQA/GQA/MLA across Amazon (short) vs. KuaiVideo (long) to verify the "balanced trade-off" claim holds across sequence regimes.
  3. **Pareto frontier mapping:** Plot GAUC against MACs with memory occupation as bubble size (following Figure 4 methodology) to identify which methods lie on the efficiency frontier for your specific resource constraints.

## Open Questions the Paper Calls Out
None

## Limitations
- Architecture specification gaps: HSTU backbone details underspecified beyond "tailored attention layers"
- Dataset preprocessing opacity: Negative sampling strategy and feature engineering pipeline unclear
- Implementation complexity underestimation: "Low implementation cost" methods may require substantial engineering for production deployment
- Long-sequence degradation mechanisms: Accuracy collapse causes not fully explored (information loss vs. distributional shift)

## Confidence
- **High confidence:** Computational complexity characterization and system-level resource measurements (MACs, memory occupation)
- **Medium confidence:** Head-level methods achieving Native-level accuracy with reduced resources across all datasets
- **Low confidence:** Linformer's short-sequence failure characterization and blanket "high implementation cost" labels without operational detail

## Next Checks
1. **Architecture ablation study:** Implement and compare multiple HSTU variants to isolate how backbone design choices affect relative performance of memory-aware compression methods.
2. **Data pipeline replication audit:** Reconstruct exact preprocessing pipeline and verify whether Linformer's short-sequence anomaly persists under alternative data configurations.
3. **Memory-accuracy Pareto frontier stress test:** Conduct systematic evaluation across varying sequence lengths (128, 512, 1024) and memory budgets (100MB, 500MB, 1GB) to validate whether MLA consistently occupies the efficiency frontier or if method dominance shifts with deployment constraints.