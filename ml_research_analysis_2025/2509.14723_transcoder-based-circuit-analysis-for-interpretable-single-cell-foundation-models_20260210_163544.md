---
ver: rpa2
title: Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation
  Models
arxiv_id: '2509.14723'
source_url: https://arxiv.org/abs/2509.14723
tags:
- features
- transcoders
- circuit
- transcoder
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work applies transcoders to single-cell foundation models
  (scFMs) for the first time, training them on the cell2sentence model to extract
  interpretable internal decision circuits. The transcoders successfully identify
  biologically meaningful features, with 35% of analyzed features corresponding to
  specific genes.
---

# Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models

## Quick Facts
- arXiv ID: 2509.14723
- Source URL: https://arxiv.org/abs/2509.14723
- Reference count: 20
- Primary result: Transcoder-based circuit extraction successfully identifies biologically meaningful features in single-cell foundation models, with 35% of features corresponding to specific genes

## Executive Summary
This work introduces transcoders as a novel approach for interpreting single-cell foundation models (scFMs) by extracting decision circuits that connect model predictions to biological mechanisms. The method trains transcoders on the MLP layers of a cell2sentence (C2S) model, achieving 35% gene-level interpretability in human evaluation. A case study on endothelial cell classification successfully identifies known biological markers (VWF, PTPRB, SPARCL1) in the extracted circuits, demonstrating the approach's potential to bridge the interpretability gap in scFMs. The paper establishes a foundation for mechanistic understanding of how these models make cell-type predictions.

## Method Summary
The method replaces MLP layers in the C2S transformer with transcoders that learn to approximate MLP(x) from MLP input using sparse autoencoders with 8× expansion. Each transcoder consists of an encoder (ReLU activation) and decoder, trained to minimize reconstruction loss plus L1 sparsity penalty. Circuit extraction traces computational subgraphs by iteratively computing attribution between transcoder features across layers using decoder-encoder dot products and attention head contributions via OV matrices. The approach is validated on Heart Cell Atlas v2 data, comparing model performance with and without transcoders.

## Key Results
- Transcoder training achieves validation loss of 4.63 vs. 2.48 for original model and 12.67 for no-MLP baseline
- Human evaluation shows 35% of sampled features (7/20) are gene-level interpretable, including known biological markers
- Endothelial cell classification circuit successfully identifies VWF, PTPRB, and SPARCL1 as key contributors to predictions
- Mean KL divergence of 2.406 indicates reasonable preservation of model behavior

## Why This Works (Mechanism)

### Mechanism 1: MLP Approximation via Sparse Expansion
- Claim: Transcoders extract input-invariant interpretable features by approximating MLP input-output mappings rather than reconstructing hidden states.
- Mechanism: Standard SAEs learn encoder-decoder pairs on the same hidden state (x → z → x̂), producing input-dependent features. Transcoders instead learn on MLP input-output pairs (x → z → MLP(x)̂), which decomposes polysemantic MLP neurons into interpretable components that represent stable functional units across different inputs.
- Core assumption: MLP computations can be faithfully approximated by sparse linear combinations without substantial information loss.
- Evidence anchors: Validation loss comparison shows transcoder-replaced model (4.63) substantially outperforms no-MLP baseline (12.67), confirming MLP functionality capture.

### Mechanism 2: Circuit Tracing via Attribution
- Claim: Computational subgraphs (circuits) can be traced by iteratively computing attribution between transcoder features across layers and through attention heads.
- Mechanism: Attribution decomposes into two factors: (1) input-dependent activation z(l,i)(x), and (2) input-independent connection strength via decoder-encoder dot products (f_dec · f_enc). Attention head contributions are computed via OV matrices to track cross-token information flow. Iteratively following top contributors yields sparse computational paths.
- Core assumption: The dot product of decoder/encoder weight vectors meaningfully represents causal influence between features.
- Evidence anchors: Endothelial cell case study successfully traces circuit from final prediction token back to VWF, PTPRB, and SPARCL1 features at specific token positions.

### Mechanism 3: Gene-Token Correspondence
- Claim: In gene-sequence tokenized scFMs, transcoder features can correspond to biologically meaningful genes when those features consistently activate on gene-name tokens.
- Mechanism: C2S represents cells as ranked gene sequences tokenized via natural language tokenizer. When a transcoder feature activates reliably on tokens comprising a specific gene name (e.g., "VWF" token) and that gene is biologically relevant to the prediction task, the feature-circuit pair captures a mechanistic link between input and output.
- Core assumption: Feature-token correspondence reflects learned biological relationships rather than tokenizer artifacts or statistical correlations in training data.
- Evidence anchors: Human evaluation: 7/20 randomly sampled features (35%) were "gene-level interpretable," including known gene families. Endothelial cell classification circuit contains canonical endothelial markers.

## Foundational Learning

- **Polysemanticity and Superposition**: The paper's central motivation is that individual neurons encode multiple concepts (e.g., "Japanese city names AND gene names"), preventing direct interpretation. Understanding superposition explains why sparse expansion (transcoders) is necessary.
  - Quick check: If a neuron activates on both "Tokyo" and "TP53," can you infer its function from activation patterns alone?

- **Transformer MLP and Residual Stream Architecture**: Transcoders specifically replace/approximate MLP layers; attribution flows through residual streams and attention. Without this mental model, you cannot interpret circuit diagrams or debug transcoder placement.
  - Quick check: In a transformer, does information flow: (a) attention → MLP → residual addition, or (b) residual → attention → MLP → residual?

- **Single-Cell Gene Expression Encoding (Rank-Value)**: scFMs like C2S convert gene expression vectors into ranked sequences. This differs fundamentally from NLP tokenization—interpretation requires understanding that token order reflects expression level, not semantic sequence.
  - Quick check: In C2S, what does the first token position represent versus the 128th position?

## Architecture Onboarding

- **Component map**:
  - Heart Cell Atlas v2 → gene ranking → cell sentence (128 genes) → tokenization → C2S model → transcoder layers → circuit extraction → biological interpretation

- **Critical path**:
  1. Load pre-trained C2S model (vandijklab/C2S-Pythia-410m-cell-type-prediction)
  2. Train transcoders on each MLP layer using cell sentences from Heart Cell Atlas v2 (90/10 train/val split)
  3. Validate by comparing: original model loss vs. transcoder-replaced model loss vs. no-MLP baseline
  4. For a target cell, run forward pass and identify highly activated features on final token
  5. Trace circuit by iterating upstream attribution until reaching input token features

- **Design tradeoffs**:
  - **Expansion factor (8x)**: Higher expansion → more granular features but higher compute and potential overfitting; lower expansion → risk of residual polysemanticity
  - **L1 coefficient (1.4×10⁻⁴)**: Controls sparsity; too high → dead features, too low → dense uninterpretable activations
  - **Training data (Heart-specific)**: Captures heart biology well but may miss general features; consider multi-tissue training for broader applicability
  - **Manual interpretation**: Currently required; scalable deployment needs automated feature-to-gene mapping

- **Failure signatures**:
  - **High reconstruction loss**: Transcoder fails to approximate MLP; circuits will be incomplete or misleading
  - **Very low L0 (active features per token)**: Most features dead; increase L1 or check data pipeline
  - **Circuits contain only generic tokens (spaces, numbers)**: Token-feature mapping failed; verify tokenizer alignment
  - **Biologically implausible circuits in known cases**: Method may capture spurious correlations; revisit attribution thresholds

- **First 3 experiments**:
  1. **Reproduce validation metrics**: Train transcoders on Heart Cell Atlas v2, confirm validation loss (~4.63) and KL divergence (~2.4) match paper before proceeding
  2. **Feature interpretability spot-check**: Sample 20 live features from layer 12, manually inspect activating tokens, verify ~35% gene-level interpretability
  3. **Known cell-type circuit extraction**: Run endothelial cell case study, confirm VWF/PTPRB/SPARCL1 appear in circuit with non-trivial attribution scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated interpretation methods systematically map transcoder features to biological concepts without the subjectivity of manual inspection?
- Basis in paper: The Conclusion states, "biological interpretation... was performed manually, which is time-consuming and potentially subjective. Future work should develop automated interpretation methods."
- Why unresolved: The current study relied on human evaluation of activating tokens, which is not scalable for the full feature space of large models.
- What evidence would resolve it: Development of an automated pipeline that maps transcoder features to biological ontologies (e.g., Gene Ontology) with validated accuracy comparable to human experts.

### Open Question 2
- Question: To what extent do the learned transcoder features and circuits generalize across different biological contexts and training datasets?
- Basis in paper: The Conclusion notes that "the choice of training dataset significantly impacts the features... exploring other datasets could reveal more general features."
- Why unresolved: The experiments were restricted to the Heart Cell Atlas v2, leaving the universality of the extracted biological circuits (e.g., endothelial markers) unknown.
- What evidence would resolve it: Comparative studies showing that transcoders trained on different cell atlases converge on similar biological circuits for shared cell types.

### Open Question 3
- Question: How can circuit extraction techniques be refined to produce sparser, more targeted subgraphs that isolate specific biological mechanisms?
- Basis in paper: Section 4.3 notes that the extracted circuit "remains large and complex, with many features difficult to interpret biologically," highlighting the "need for more refined circuit extraction methods."
- Why unresolved: Current tracing methods result in dense graphs where noise obscures the core biological decision pathway.
- What evidence would resolve it: An algorithm that yields a minimal sufficient circuit for a prediction that is strictly smaller and validated against known ground-truth pathways.

## Limitations
- Circuit extraction validity is limited to a single endothelial cell case study without broader validation across multiple cell types
- Interpretability assessment based on small sample size (20 features) and subjective human evaluation introduces uncertainty
- Model approximation quality shows substantial distributional shift (KL divergence 2.406) between original and transcoder-replaced models

## Confidence

**High Confidence Claims**:
- Transcoders can be successfully trained on MLP layers of C2S model
- The training procedure converges with reasonable reconstruction loss
- Circuit extraction methodology is implementable and produces structured results
- Endothelial cell case study demonstrates the technical feasibility of the approach

**Medium Confidence Claims**:
- ~35% of transcoder features are gene-level interpretable (based on limited human evaluation)
- The VWF/PTPRB/SPARCL1 circuit represents genuine biological mechanisms (single case study)
- Transcoders provide better interpretability than alternative methods (no direct SAE comparisons)

**Low Confidence Claims**:
- The attribution-based circuit extraction captures true causal mechanisms
- The approach generalizes across diverse cell types and biological processes
- Transcoders are superior to SAEs for scFM interpretability (no comparative experiments)

## Next Checks

1. **Replicate across diverse cell types**: Extract and validate circuits for at least 5 additional well-characterized cell types (e.g., cardiomyocytes, fibroblasts, B cells, T cells, smooth muscle cells) to assess generalizability beyond the single endothelial cell example.

2. **Compare transcoder vs. SAE interpretability**: Train standard SAEs on the same C2S model using identical evaluation protocols to directly compare the proportion of interpretable features and circuit quality, addressing the core methodological contribution.

3. **Establish ground truth circuit validation**: Use knockout experiments or pathway perturbation data to verify that disrupting genes identified in circuits (e.g., VWF, PTPRB) actually affects the model's cell-type classification predictions, providing causal evidence for biological relevance.