---
ver: rpa2
title: 'ReCode: Unify Plan and Action for Universal Granularity Control'
arxiv_id: '2510.23564'
source_url: https://arxiv.org/abs/2510.23564
tags:
- location
- recode
- agent
- code
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The key limitation in current LLM-based agents is their inability
  to dynamically adjust decision granularity, stemming from a rigid separation between
  high-level planning and low-level action. ReCode addresses this by unifying plan
  and action within a single code representation, treating high-level plans as placeholder
  functions that are recursively decomposed into finer-grained sub-functions until
  reaching primitive actions.
---

# ReCode: Unify Plan and Action for Universal Granularity Control

## Quick Facts
- arXiv ID: 2510.23564
- Source URL: https://arxiv.org/abs/2510.23564
- Reference count: 40
- Primary result: Up to 20.9% improvement in inference performance while using 3.7× fewer training data pairs

## Executive Summary
ReCode addresses a fundamental limitation in LLM-based agents: their inability to dynamically adjust decision granularity due to rigid separation between high-level planning and low-level action. The framework unifies plan and action within a single code representation, treating high-level plans as placeholder functions that are recursively decomposed into finer-grained sub-functions until reaching primitive actions. This recursive approach enables fluid control over decision granularity and inherently generates rich, multi-granularity training data. Extensive experiments demonstrate that ReCode significantly outperforms advanced baselines in both inference performance (achieving up to 20.9% improvement) and training data efficiency (using 3.7× fewer data pairs while achieving better results).

## Method Summary
ReCode represents both plans and actions as Python function calls, creating a common computational substrate for the agent's policy. High-level plans are abstract placeholder functions that the LLM recursively decomposes into finer-grained sub-functions until reaching primitive actions. The executor traverses the decision tree depth-first, invoking ReCode recursively when encountering placeholders with only the current function signature and available variables as context. This on-demand recursive expansion defers granularity decisions until relevant state is known. The framework uses a unified variable namespace serialized into prompts and includes a self-correction loop that re-invokes the policy with error traceback on execution failures. Training data is generated from hierarchical traces that capture the reasoning from root task to leaf primitives, providing richer learning signals per sample.

## Key Results
- Achieved up to 20.9% improvement in inference performance across ALFWorld, WebShop, and ScienceWorld benchmarks
- Demonstrated 3.7× training data efficiency by using fewer data pairs while achieving better results
- Showed consistent improvements across diverse benchmarks including embodied manipulation tasks
- Validated effectiveness of universal granularity control for building more adaptive and capable LLM-based agents

## Why This Works (Mechanism)

### Mechanism 1: Unified Code Representation
- Claim: Unifying plans and actions as code functions enables fluid transitions between decision granularities
- Mechanism: High-level plans are represented as placeholder functions (e.g., `prepare_breakfast()`), while primitive actions are executable calls (e.g., `run('crack egg')`). Both share the same syntax and execution pathway. The LLM expands placeholders into child code blocks containing either more placeholders or primitives, creating a single representation hierarchy rather than separate planning and action modules.
- Core assumption: LLMs can reliably generate syntactically valid Python code with function abstractions that respect variable scoping
- Evidence anchors: Abstract states ReCode treats high-level plans as abstract placeholder functions; Section 3.2 establishes Python function calls as common computational substrate
- Break condition: If LLM cannot consistently generate valid function syntax or mismanages variable scope, unified representation degrades into execution failures

### Mechanism 2: On-Demand Recursive Expansion
- Claim: On-demand recursive expansion with isolated context produces more appropriate granularity decisions than pre-planning
- Mechanism: Executor traverses decision tree depth-first. When encountering placeholder function, it invokes ReCode recursively with only current function signature and available variables—no full execution history or tree structure. LLM generates code for specific subtask based on local context, then execution continues. This defers granularity decisions until relevant state is known.
- Core assumption: Local variable context is sufficient for good decomposition; LLM can infer appropriate abstraction levels without seeing full trajectory
- Evidence anchors: Section 3.2 Algorithm 1 describes expansion triggering; Section 3.3 Context Management enforces explicit state management through context isolation
- Break condition: If critical information is lost between recursion levels because model fails to save it to variables, decomposition quality degrades

### Mechanism 3: Hierarchical Training Data Generation
- Claim: Hierarchical training data from recursive traces provides richer learning signals per sample than flat action sequences
- Mechanism: Each successful trajectory naturally encodes decision tree capturing reasoning from root task to leaf primitives. This preserves "how" of decomposition, not just "what." When training on this data, model learns hierarchical planning patterns from fewer examples because each example contains more structural information
- Core assumption: Hierarchical structure of decision trees encodes learnable patterns that generalize better than memorized flat sequences
- Evidence anchors: Abstract mentions recursive structure inherently generates rich, multi-granularity training data; Section 4.3 Table 5 shows ReCode achieves 55.34% on ScienceWorld seen tasks with 1,713 pairs vs. ReAct's 36.34% with 6,320 pairs (3.7× more)
- Break condition: If hierarchical patterns don't generalize across task types, training efficiency gains diminish on out-of-distribution tasks

## Foundational Learning

- Concept: **Hierarchical Task Decomposition**
  - Why needed here: ReCode's core operation is breaking goals into sub-goals. You must understand what makes good decomposition—subtasks should be coherent, ordered, and neither too abstract nor too granular
  - Quick check question: Given "make breakfast," what are three valid sub-goals, and how would you know if your decomposition is too shallow or too deep?

- Concept: **Recursive Tree Traversal (Depth-First)**
  - Why needed here: Executor walks decision tree depth-first, expanding each placeholder fully before moving to siblings. Understanding control flow in recursion is essential for debugging stuck or looping agents
  - Quick check question: In depth-first traversal of task tree, when does control return to parent node, and what state must be preserved?

- Concept: **Code-as-Policy Representation**
  - Why needed here: ReCode inherits from CodeAct and Code-as-Policies idea that actions are code statements. Unlike natural language actions, code has syntax constraints, variable scoping, and composability
  - Quick check question: Why might `run('go to kitchen')` be more robust than generating "I should walk to the kitchen" as free-form text?

## Architecture Onboarding

- Component map: Text-to-Code Initializer -> Policy Model (LLM) -> Executor -> Variable Namespace -> Error Handler -> Recursion Controller
- Critical path: 1) Task instruction → rule-based root placeholder 2) Policy expands root → code block 3) Executor processes code block sequentially: If primitive: execute in environment; If placeholder: recurse to step 2 with new placeholder as current node 4) Recursion unwinds as functions complete; task finishes when root fully resolved
- Design tradeoffs: Max recursion depth = 10 (conservative upper bound, optimal ~8 on ScienceWorld per Figure 3); Context isolation (no full history) reduces token costs and forces explicit state management but may lose patterns visible only in trajectory history; Rule-based initialization is task-agnostic but delegates all interpretation to policy
- Failure signatures: Over-decomposition (performance drops at depth >12, Figure 3 inverted-U); Under-decomposition (depth <6 yields suboptimal performance); Variable loss (model fails to capture observations in variables, causing downstream functions to operate without necessary context); Format errors (LLM generates `def` functions or places placeholders inside loops/conditionals); Infinite loops (without depth limit, placeholder that generates itself causes non-termination)
- First 3 experiments: 1) Recursion depth ablation: Run ReCode on ScienceWorld seen set with max depths [2, 4, 6, 8, 10, 12, 14]. Reproduce Figure 3 curve to validate optimal depth for your model 2) Training data efficiency: Using percentile filtering from Table 5, train Qwen2.5-7B on ReCode vs. ReAct data at p=10, 20, 30, 40. Plot performance vs. data volume to confirm ReCode's steeper learning curve 3) Cross-environment generalization: Test few-shot ReCode (GPT-4o mini) on ALFWorld unseen split. Compare to ReAct and CodeAct baselines. Analyze failure cases where ReCode's hierarchy doesn't match task structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning be integrated to explicitly reward and optimize the efficiency of hierarchical plan generation?
- Basis in paper: [explicit] The authors state a future direction is to "explore reinforcement learning signals that reward efficient hierarchical plans and reliable expansions."
- Why unresolved: Current framework relies on supervised fine-tuning (SFT) on filtered trajectories, which mimics expert behavior but does not explicitly optimize for structural efficiency or depth of generated plans
- What evidence would resolve it: A training regime combining RL with SFT that demonstrates higher success rates or reduced computational cost compared to SFT-only training

### Open Question 2
- Question: How can the framework be made robust to code generation errors and format deviations in weaker models?
- Basis in paper: [explicit] The authors acknowledge that "weaknesses in planning ability or deviations from the required format can lead to unstable performance" and suggest increasing robustness
- Why unresolved: ReCode relies on underlying LLM producing executable Python code with specific signatures; current error handling is limited to self-correction loop that may fail with syntax errors in smaller models
- What evidence would resolve it: Performance metrics on smaller models (e.g., 3B parameters) showing that proposed robustness mechanism significantly reduces execution failures compared to baseline ReCode

### Open Question 3
- Question: Does curriculum learning improve model's ability to generalize to complex tasks by gradually introducing reasoning patterns?
- Basis in paper: [explicit] Section 6 suggests it is "promising to... investigate curriculum style training procedures that gradually introduce more complex tasks"
- Why unresolved: Current training methodology filters for top 40% of data by reward but presents it statically, without structured progression from simple to complex hierarchical structures
- What evidence would resolve it: A study comparing static SFT against curriculum-based approach, showing faster convergence or superior performance on hardest benchmark tasks (e.g., ScienceWorld)

## Limitations
- Training data efficiency gains may not generalize beyond controlled ScienceWorld experiments
- Optimal recursion depth of 8-10 appears task-specific without ablation studies on other benchmarks
- Self-correction mechanism effectiveness is unclear due to unspecified retry limits and recovery success rates

## Confidence
- **High confidence**: ReCode's core mechanism (unifying plans/actions as code functions with recursive decomposition) is well-specified and experimentally validated for inference performance gains
- **Medium confidence**: Training data efficiency improvements are demonstrated on ScienceWorld but require replication across environments to confirm generality
- **Low confidence**: Claims about inherent superiority of hierarchical training data structure over flat sequences need more rigorous comparison

## Next Checks
1. Cross-environment depth optimization: Run ReCode on ALFWorld and WebShop with varying max recursion depths (2-14) to determine if optimal depth varies by environment or task complexity
2. Training efficiency validation: Generate ReCode and ReAct training data from identical DeepSeek-V3.1 trajectories on all three benchmarks. Train Qwen2.5-7B models and compare learning curves across p=10, 20, 30, 40 percentile thresholds
3. Variable state retention analysis: Instrument executor to log when variable updates are lost between recursion levels. Measure correlation between variable loss frequency and decomposition quality degradation