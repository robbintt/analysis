---
ver: rpa2
title: 'Self-Improvement as Coherence Optimization: A Theoretical Account'
arxiv_id: '2601.13566'
source_url: https://arxiv.org/abs/2601.13566
tags:
- coherence
- learning
- prior
- policy
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents coherence optimization as a unified framework
  explaining several feedback-free self-improvement methods in language models. The
  authors show that methods like debate, bootstrap, and internal coherence maximization
  are all special cases of optimizing a policy's coherence - defined as the joint
  predictability of behaviors across contexts.
---

# Self-Improvement as Coherence Optimization: A Theoretical Account

## Quick Facts
- **arXiv ID:** 2601.13566
- **Source URL:** https://arxiv.org/abs/2601.13566
- **Reference count:** 40
- **Primary result:** Coherence optimization unifies feedback-free self-improvement methods and explains their effectiveness through joint predictability of behaviors

## Executive Summary
This paper presents coherence optimization as a unified theoretical framework for understanding feedback-free self-improvement methods in language models. The authors demonstrate that methods like debate, bootstrap, and internal coherence maximization are all special cases of optimizing a policy's coherence - defined as the joint predictability of behaviors across contexts. The framework shows that coherence optimization is mathematically equivalent to description-length regularization and provides an optimal solution for semi-supervised learning when using a pretrained model as the prior.

Through theoretical analysis and preliminary experiments, the paper validates that coherence-based metrics can outperform LLM-as-a-judge at detecting deceptive solutions, while coherence optimization improves model accuracy on GSM8K tasks. The framework explains why unsupervised self-improvement methods work and provides guidance on when they should succeed or fail, though the authors acknowledge that full experimental validation against alternative regularizers remains future work.

## Method Summary
The paper introduces coherence optimization as a unified framework where self-improvement methods are viewed as maximizing the joint predictability of a model's behaviors across different contexts. The coherence of a policy π is defined as χ(π) = Σ log₂ σ(ϕ + Σ π(s_m), s_n)(π(s_n)), which measures how well the model can predict its own future behaviors given past contexts. This is shown to be equivalent to description-length regularization and optimal for semi-supervised learning with a pretrained model as the prior.

The authors provide a practical algorithm using Gibbs sampling to optimize coherence: sample a_n^t ~ σ_β(Σ_{m≠n} π_t(s_m), s_n) for N rounds, with a training-friendly variant using hold-out subsets of size γ|S|. The method is demonstrated on GSM8K with 100 examples per run, tracking accuracy improvements over 40 rounds, and on detecting deceptive solutions using pointwise mutual information (PMI) = coherence - sum of individual log probabilities.

## Key Results
- Coherence-based metrics outperform LLM-as-a-judge at detecting deceptive solutions in preliminary experiments
- Coherence optimization improves GSM8K accuracy through Gibbs sampling over d-policies
- The framework provides theoretical justification for why unsupervised self-improvement methods like debate and bootstrap work
- Coherence optimization is equivalent to description-length regularization and optimal for semi-supervised learning with pretrained priors

## Why This Works (Mechanism)
Coherence optimization works because it captures the fundamental principle that self-consistent behavior across contexts is a signature of truthful, reliable reasoning. By maximizing joint predictability of behaviors, the model learns to produce outputs that are mutually reinforcing and internally consistent, which naturally filters out deceptive or incoherent responses. The framework shows this is equivalent to finding the most compact description of the policy that fits both the data and the prior, which is optimal for semi-supervised learning.

## Foundational Learning
**Coherence as joint predictability:** Understanding how coherence measures the model's ability to predict its own future behaviors from past contexts. Why needed: This is the core concept that unifies different self-improvement methods. Quick check: Verify that χ(π) increases when responses become more mutually predictable.

**Description-length regularization equivalence:** The mathematical connection between coherence optimization and finding the shortest description that explains the data and prior. Why needed: Provides theoretical justification for why coherence optimization is optimal. Quick check: Confirm Theorem 1 holds under different prior distributions.

**Semi-supervised learning optimality:** How coherence optimization provides the optimal solution when using a pretrained model as the prior. Why needed: Explains the practical effectiveness of coherence-based methods. Quick check: Test accuracy improvements on held-out data as coherence increases.

**Gibbs sampling for coherence:** The practical algorithm for optimizing coherence through iterative resampling. Why needed: Provides an implementable method for coherence optimization. Quick check: Monitor convergence and diversity of outputs during sampling.

**Pointwise mutual information for deception detection:** Using PMI = coherence - sum of individual log probabilities to identify deceptive solutions. Why needed: Provides a practical metric for evaluating coherence. Quick check: Compare PMI scores between honest and deceptive outputs.

## Architecture Onboarding

**Component map:** Pretrained model (σ) -> Context construction (ϕ) -> Gibbs sampling (Algorithm 2) -> Coherence evaluation (χ) -> Accuracy/deception detection metrics

**Critical path:** σ(ϕ, s) evaluation → Gibbs sampling iterations → Final policy π* → Performance metrics (accuracy/PMI)

**Design tradeoffs:** Gibbs sampling provides theoretical guarantees but may be computationally expensive; training-based implementations (Appendix B.2) scale better but lose some theoretical properties. Mode collapse is prevented by mixture sampling but may slow convergence.

**Failure signatures:** Mode collapse (reduced diversity), saturation after ~20 rounds due to context limits, overoptimization producing unnatural outputs, unstable dynamics when sampling exclusively from current policy.

**First experiments:**
1. Run Algorithm 2 on GSM8K with |S|=100, γ=0.25, N=40 rounds, 100 random seeds; plot accuracy vs. round
2. Compute PMI for honest vs. deceptive outputs on HARDMath; compare with LLM-as-a-judge scores
3. Test Gibbs sampling with different temperatures (β values) to find optimal balance between exploration and exploitation

## Open Questions the Paper Calls Out

**Open Question 1:** Can coherence-based metrics detect naturally occurring or adversarially trained deception, or are they limited to synthetically generated deceptive solutions? Current experiments only tested deception generated via explicit prompting instructions, not sophisticated adversarial training.

**Open Question 2:** Does coherence optimization provably achieve optimality compared to alternative regularizers in semi-supervised learning? While theoretical optimality bounds were proven, no empirical comparison against competing regularization schemes was conducted.

**Open Question 3:** Are the signals from pretraining and environmental feedback sufficient to uniquely determine the globally optimal reflective equilibrium? While coherence optimization narrows the search space, uniqueness requires stronger theoretical conditions that have not been established.

## Limitations
- Empirical validation is preliminary with only two specific tasks tested and small sample sizes (100 examples per run)
- Theoretical connection relies on strong assumptions about policy distribution that may not hold in practice
- Gibbs sampling may not be the most efficient approach for large-scale applications; training-based implementations needed for larger datasets

## Confidence

**High Confidence:** Theoretical equivalence between coherence optimization and description-length regularization (Theorem 1); general framework explaining coherence's relation to semi-supervised learning

**Medium Confidence:** Practical effectiveness of Gibbs sampling for coherence optimization; claim that coherence-based metrics outperform LLM-as-a-judge for detecting deceptive solutions

**Low Confidence:** Scalability claims for larger models and datasets; assertion that coherence optimization will generalize to diverse self-improvement tasks beyond tested scenarios

## Next Checks

1. **Scaling Validation:** Test coherence optimization on larger model families (e.g., Llama 3, GPT-3.5) with datasets exceeding 1,000 examples to verify claims about training-based implementations being necessary for larger scales.

2. **Generalization Test:** Apply the coherence metric to a broader range of potentially deceptive or suboptimal outputs (e.g., hallucinations, biased responses, safety violations) to assess whether it consistently outperforms LLM-as-a-judge across different failure modes.

3. **Alternative Optimization Methods:** Compare Gibbs sampling against more efficient optimization approaches (e.g., gradient-based methods, reinforcement learning) on the same GSM8K task to quantify the practical trade-offs between theoretical guarantees and computational efficiency.