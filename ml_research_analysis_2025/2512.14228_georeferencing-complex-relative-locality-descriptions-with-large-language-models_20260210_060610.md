---
ver: rpa2
title: Georeferencing complex relative locality descriptions with large language models
arxiv_id: '2512.14228'
source_url: https://arxiv.org/abs/2512.14228
tags:
- georeferencing
- data
- locality
- descriptions
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies Large Language Models to automatically georeference\
  \ biological specimen collection records that contain complex, relative locality\
  \ descriptions (e.g., \u201C10 km north of Lake Wanaka, near Pipson Creek\u201D\
  ). Traditional georeferencing methods rely on place names and gazetteers, which\
  \ fail to capture spatial relationships embedded in such descriptions."
---

# Georeferencing complex relative locality descriptions with large language models

## Quick Facts
- **arXiv ID:** 2512.14228
- **Source URL:** https://arxiv.org/abs/2512.14228
- **Reference count:** 30
- **Primary result:** Large Language Models can accurately georeference complex relative locality descriptions in biodiversity records, achieving up to 85% within 10 km accuracy for New York data.

## Executive Summary
This paper addresses the challenge of georeferencing biological specimen collection records containing complex, relative locality descriptions (e.g., "10 km north of Lake Wanaka, near Pipson Creek"). Traditional georeferencing methods relying on place names and gazetteers fail to capture spatial relationships embedded in such descriptions. The authors propose using fine-tuned Large Language Models (LLMs) to parse and interpret these complex descriptions, extracting spatial relationships and distances to determine geographic coordinates. Their approach significantly outperforms traditional methods and demonstrates robustness across different regions and languages.

## Method Summary
The authors fine-tune a Mistral-7B LLM using quantized LoRA on biodiversity datasets from multiple regions and languages. They employ a context manager prompting pattern to help the model interpret complex spatial relationships in locality descriptions. The fine-tuned model is evaluated against traditional methods (GEOLocate and gazetteer-based approaches) using test datasets containing 25-97 records per region. The evaluation focuses on the percentage of records correctly georeferenced within 10 km and 1 km of the true location, measuring the model's ability to handle relative descriptions involving distances, directions, and spatial relationships to geographic features.

## Key Results
- The fine-tuned LLM achieves an average of 65% of records within 10 km of true location across datasets
- Best performance: 85% within 10 km and 67% within 1 km for New York data
- The model is robust to description length and performs well with multilingual data
- Transfer learning shows region-specific training data is crucial, with performance improving significantly even with small amounts of local data

## Why This Works (Mechanism)
Large Language Models excel at understanding complex language patterns and spatial relationships embedded in text. By fine-tuning on biodiversity locality data, the model learns to interpret relative spatial descriptions, parse quantitative distance information, and resolve geographic references. The context manager prompting pattern helps the model maintain awareness of spatial relationships throughout the interpretation process, enabling it to handle nested or compound locality descriptions that would confuse traditional gazetteer-based approaches.

## Foundational Learning
- **Fine-tuning LoRA on quantized models** - Reduces computational requirements while maintaining performance; quickly check by comparing training time and memory usage against full fine-tuning
- **Context manager prompting patterns** - Maintains spatial relationship awareness during inference; verify by testing with progressively longer locality descriptions
- **Spatial reasoning in language models** - LLMs can interpret directional and distance-based spatial relationships; validate by testing with descriptions containing explicit measurements
- **Multilingual model adaptation** - Models can be trained on diverse linguistic patterns; test with parallel locality descriptions in different languages
- **Geospatial accuracy metrics** - Distance-based evaluation provides practical assessment; confirm by comparing results against known ground truth coordinates
- **Transfer learning limitations** - Performance depends heavily on training data similarity; check by testing on geographically distant datasets

## Architecture Onboarding
**Component Map:** Mistral-7B -> LoRA fine-tuning -> Context Manager Prompting -> Georeferencing Output

**Critical Path:** Input locality description → Context manager processing → LLM inference → Coordinate extraction → Accuracy evaluation

**Design Tradeoffs:** Fine-tuning on smaller quantized models reduces computational requirements but may limit maximum accuracy compared to larger models; context manager adds complexity but improves interpretation of complex relationships; region-specific training improves accuracy but requires more diverse training data.

**Failure Signatures:** 
- Poor performance on geographically distant test regions
- Degradation with very short or very long locality descriptions
- Misinterpretation of quantitative distance measurements
- Failure to resolve ambiguous geographic references

**First 3 Experiments:**
1. Test model performance on progressively longer locality descriptions to identify length limitations
2. Evaluate transfer learning by training on one region and testing on geographically distant regions
3. Compare performance with and without context manager prompting on complex nested locality descriptions

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation based on relatively small test datasets (25-97 records per region) may not represent real-world biodiversity collections
- Significant regional performance variation suggests potential geographic and linguistic biases in training data
- Absence of uncertainty quantification for individual predictions limits practical application
- Model may be memorizing regional patterns rather than developing generalizable spatial reasoning capabilities

## Confidence
**High confidence:** The overall methodology and experimental design are sound, with clear evaluation metrics and reasonable baseline comparisons.

**Medium confidence:** The reported accuracy figures are likely reliable for the tested datasets, but generalizability to broader biodiversity collections remains uncertain.

**Low confidence:** The model's ability to handle temporal changes in geography, scale appropriately for different geographic features, and maintain performance across diverse biodiversity domains requires further validation.

## Next Checks
1. Conduct a larger-scale evaluation using comprehensive biodiversity databases with thousands of records across multiple geographic regions and taxonomic groups to assess real-world performance and identify systematic biases.

2. Implement and evaluate uncertainty quantification methods that provide confidence scores or probability distributions for each georeferencing prediction, allowing users to filter or weight results appropriately.

3. Test the model's robustness to temporal changes by evaluating performance on historical locality descriptions where geographic features or place names may have changed significantly since collection.