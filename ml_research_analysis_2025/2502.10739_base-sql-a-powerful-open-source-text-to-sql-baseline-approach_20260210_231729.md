---
ver: rpa2
title: 'BASE-SQL: A powerful open source Text-To-SQL baseline approach'
arxiv_id: '2502.10739'
source_url: https://arxiv.org/abs/2502.10739
tags:
- schema
- methods
- linking
- table
- revision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BASE-SQL, a pipeline-based Text-to-SQL method
  using open-source models. The method addresses the challenge of converting natural
  language to SQL queries for databases, which has broad applications in data analysis
  and business intelligence.
---

# BASE-SQL: A powerful open source Text-To-SQL baseline approach

## Quick Facts
- **arXiv ID:** 2502.10739
- **Source URL:** https://arxiv.org/abs/2502.10739
- **Reference count:** 40
- **Primary result:** 88.9% execution accuracy on Spider test set using Qwen2.5-Coder-32B-Instruct

## Executive Summary
BASE-SQL introduces a pipeline-based Text-to-SQL method that leverages open-source models to convert natural language queries into SQL. The approach addresses the challenge of database querying through a four-stage process: Schema Linking, Candidate SQL Generation, SQL Revision, and SQL Merge Revision. It achieves strong performance on benchmark datasets (67.47% on BIRD dev, 88.9% on Spider test) while requiring only five LLM calls per query, making it both efficient and practical for real-world deployment.

## Method Summary
BASE-SQL is a pipeline-based Text-to-SQL system that uses a fine-tuned open-source model (Qwen2.5-Coder-32B-Instruct) with LoRA adapters. The method operates in four stages: Schema Linking predicts relevant tables, Candidate Generation produces initial SQL, SQL Revision corrects the candidate using full schema context, and SQL Merge Revision resolves discrepancies between multiple outputs. The system achieves high accuracy while minimizing LLM calls, using "M-Schema With Sample" representation that includes column descriptions and sample rows. LoRA fine-tuning is applied for both table prediction and SQL generation tasks, with specific hyperparameters (rank=32, alpha=16, dropout=0.1, LR=4e-5).

## Key Results
- Achieves 88.9% execution accuracy on Spider test set
- Achieves 67.47% execution accuracy on BIRD development set
- Outperforms other open-source model approaches and even surpasses several GPT-4o-based methods
- Requires only five LLM calls per SQL generation, demonstrating high efficiency

## Why This Works (Mechanism)
The method's effectiveness stems from its staged approach that breaks down complex SQL generation into manageable subtasks. By first identifying relevant tables through fine-tuned schema linking, the system reduces the search space for subsequent SQL generation. The candidate generation and revision stages work iteratively, with the model correcting its own outputs using full schema context. The merge revision stage provides a final quality check by comparing execution results, ensuring logical consistency. The use of LoRA fine-tuning allows efficient adaptation of the base model while maintaining the efficiency benefits of the open-source architecture.

## Foundational Learning
- **Schema Representation ("M-Schema With Sample")**: Why needed - provides structured context about tables, columns, and sample data to guide SQL generation. Quick check - verify schema formatting matches Figure 2 specifications.
- **LoRA Fine-tuning**: Why needed - enables efficient model adaptation without full retraining, preserving computational efficiency. Quick check - confirm LoRA parameters (rank=32, alpha=16) are correctly applied.
- **Pipeline Architecture**: Why needed - decomposes complex SQL generation into simpler, more reliable subtasks. Quick check - validate each stage produces expected intermediate outputs.
- **Execution-based Evaluation**: Why needed - measures actual SQL correctness rather than syntactic similarity. Quick check - ensure database connections and query execution are properly implemented.

## Architecture Onboarding
**Component Map:** Schema Linking → Candidate Generation → SQL Revision → SQL Merge Revision
**Critical Path:** The pipeline flow where each stage's output becomes the next stage's input, with the Merge Revision stage providing the final output
**Design Tradeoffs:** Prioritizes efficiency (5 LLM calls) over exhaustive exploration, uses execution results for final validation rather than complex selection models
**Failure Signatures:** 
- Schema Linking misses required tables (check Table Linking recall)
- SQL Revision hallucinates columns (verify Full Schema formatting)
- Merge Revision fails to resolve conflicts (test execution consistency logic)
**Three First Experiments:**
1. Test Schema Linking recall on development set to verify >89% table identification accuracy
2. Validate SQL Revision stage by checking if outputs maintain schema consistency
3. Verify Merge Revision correctly resolves conflicts by comparing execution results of different SQL variants

## Open Questions the Paper Calls Out
- **Open Question 1:** How does BASE-SQL performance vary when applied to other state-of-the-art open-source models like Llama-3.3-70B-Instruct or Mistral-Large-Instruct-2411? The paper's experiments were limited to Qwen2.5-Coder and DeepSeek-Coder families due to resource constraints.
- **Open Question 2:** How can the pipeline handle databases with extremely wide schemas (thousands of columns) without context length issues? The current approach skips column linking to save LLM calls, which may fail in industrial scenarios with massive schemas.
- **Open Question 3:** Is the SQL Merge Revision strategy inherently superior to a fine-tuned selection model, or is it primarily a workaround for the lack of open-source selection datasets? The paper compares Merge Revision against un-fine-tuned baselines but not against an optimized selection model.

## Limitations
- The method's performance on extremely wide schemas (hundreds or thousands of columns) may degrade due to context length limitations
- Results are primarily validated on Spider and BIRD benchmarks; generalization to other domains needs verification
- The lack of column linking may miss important relationships in complex schemas

## Confidence
- **High confidence:** Overall pipeline architecture and LoRA fine-tuning approach are clearly specified
- **Medium confidence:** Reported accuracy scores are specific but depend on exact evaluation setup
- **Low confidence:** Implementation details of SQL Revision and Merge steps, particularly prompt templates and sampling parameters

## Next Checks
1. Verify Schema Linking model achieves >89% recall on development set to ensure accurate table identification
2. Test the execution consistency heuristic in Merge Revision to confirm it correctly resolves conflicts between SQL variants
3. Evaluate the complete pipeline on an additional Text-to-SQL benchmark (e.g., WikiSQL) to verify cross-dataset generalization and performance claims