---
ver: rpa2
title: Detecting Data Contamination in LLMs via In-Context Learning
arxiv_id: '2510.27055'
source_url: https://arxiv.org/abs/2510.27055
tags:
- contamination
- nvidia
- qwen
- codec
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDeC (Contamination Detection via Context),
  a method to detect and quantify training data contamination in large language models
  by leveraging in-context learning. The core idea is that adding in-context examples
  from a dataset improves model confidence for unseen data but reduces it for memorized
  training data, due to disrupted memorization patterns.
---

# Detecting Data Contamination in LLMs via In-Context Learning

## Quick Facts
- **arXiv ID:** 2510.27055
- **Source URL:** https://arxiv.org/abs/2510.27055
- **Reference count:** 40
- **Primary result:** CoDeC achieves 99.9% AUC in detecting training data contamination across diverse models and architectures

## Executive Summary
This paper introduces CoDeC (Contamination Detection via Context), a method to detect and quantify training data contamination in large language models by leveraging in-context learning. The core idea is that adding in-context examples from a dataset improves model confidence for unseen data but reduces it for memorized training data, due to disrupted memorization patterns. CoDeC measures this effect by comparing model confidence with and without context, producing interpretable percentage-based contamination scores. Experiments show that CoDeC cleanly separates seen from unseen datasets, achieving dataset-level AUC of 99.9% across diverse models and architectures, outperforming baseline methods. It is model-agnostic, parameter-free, and computationally efficient, making it practical for large-scale benchmark evaluations and fair model comparisons.

## Method Summary
CoDeC detects contamination by measuring how in-context examples affect model confidence. For each sample in a suspect dataset, the method computes the model's average log-likelihood with and without prepended context from the same dataset. The difference (Δ) is calculated for each sample, and the final contamination score is the percentage of samples where context reduces confidence. The method ignores the first 10 tokens to avoid transition noise, uses 5 random context samples per target for stability, and requires 1,000 samples per dataset for reliable detection. The approach is parameter-free, model-agnostic, and computationally efficient.

## Key Results
- Achieves 99.9% AUC in separating seen vs. unseen datasets across diverse models and architectures
- Outperforms baseline methods with interpretable percentage-based contamination scores
- Requires only 1,000 samples and 5 inference runs per sample for reliable detection
- Works across model sizes from 125M to 70B parameters with consistent performance

## Why This Works (Mechanism)

### Mechanism 1: Memorization Disruption via Context Interference
- **Claim:** If a model has memorized a dataset, providing in-context examples from that same dataset disrupts the model's retrieval of specific token sequences, thereby reducing confidence.
- **Mechanism:** Memorized data relies on specific sequential dependencies. When in-context samples (which are also memorized) are prepended, they interfere with the activation of the target sequence, creating a "distraction" effect that lowers log-probabilities.
- **Core assumption:** The model relies on rote memorization for contaminated data rather than generalizable reasoning, and this memorized state is fragile to context changes.
- **Evidence anchors:** [abstract] "...in-context examples typically boost confidence for unseen datasets but may reduce it when the dataset was part of training, due to disrupted memorization patterns."

### Mechanism 2: Simulation of Finetuning Dynamics (Remaining Capacity)
- **Claim:** In-context learning (ICL) approximates a single step of finetuning; contaminated models have "saturated" on the data and show no improvement (or degradation), whereas unseen data shows improvement.
- **Mechanism:** CoDeC measures the "remaining learning capacity." A model trained on the target dataset resembles a model finetuned to saturation—it cannot extract new signal from the in-context examples.
- **Core assumption:** The dynamics of adding in-context examples sufficiently correlate with the dynamics of gradient-based updates on the loss landscape.
- **Evidence anchors:** [section 2.5] "ICL simulates finetuning dynamics. Contaminated models resemble finetuned models near saturation..."

### Mechanism 3: Loss Landscape Geometry (Sharp vs. Flat Minima)
- **Claim:** Contaminated data places the model in sharp, narrow minima which are destabilized by the "perturbation" of in-context examples.
- **Mechanism:** Memorized samples reside in high-curvature regions of the loss landscape. The intervention (adding context) acts like a large learning rate update, pushing the model out of this sharp minimum. Unseen data resides in flatter regions where this perturbation is stable or beneficial.
- **Core assumption:** In-context additions behave like high-learning-rate gradient steps in terms of their geometric effect on the loss.
- **Evidence anchors:** [section 2.5] "Interventions expose loss landscape... Overfitted models sit in narrow local minima that are more easily destabilized by new context..."

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** This is the core intervention. You must understand that LLMs adapt predictions based on examples in the prompt window without weight updates to grasp how CoDeC probes the model.
  - **Quick check question:** If a model's weights are frozen, how does prepending a news article about sports change its prediction for the word "goalie"?

- **Concept: Log-Likelihood & Confidence**
  - **Why needed here:** CoDeC quantifies "memorization" via the model's assigned log-probability of the target tokens. Understanding that lower perplexity = higher confidence is essential.
  - **Quick check question:** If a model assigns a log-probability of -2.0 to a token with context and -4.0 without context, did the context increase or decrease confidence?

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here:** CoDeC is a variant of dataset-level MIA. Knowing this context helps frame *why* we are comparing "seen" vs. "unseen" behavior.
  - **Quick check question:** Why is determining if a specific sample was in the training set (sample-level MIA) generally harder for LLMs than determining if a dataset was used (dataset-level)?

## Architecture Onboarding

- **Component map:** Sampler -> Inference Engine -> Delta Calculator -> Aggregator
- **Critical path:** The stability of the Δ signal. The paper ignores the first 10 tokens of the target x to avoid "transition noise" from the context. Failing to exclude these tokens or using mismatched formatting will break the signal.
- **Design tradeoffs:**
  - **Context Size (n):** n=1 is fast but noisier; larger n strengthens separation but doubles compute roughly linearly.
  - **Dataset Size:** The paper suggests 100 samples are efficient, but 1,000 reduces variance below 1%.
  - **Formatting:** Strict matching between training format and evaluation format is required. Artificial labels (e.g., "Question:") in the probe that weren't in the training data can artificially lower scores (false negative).
- **Failure signatures:**
  - **Score ≈ 50% (Random):** Model is untrained, or the dataset is too diverse/noisy for the context to provide a consistent signal.
  - **High False Positives (Score > 60% on Unseen):** Occurs when the unseen dataset is extremely diverse (context acts as noise) or highly related to the training distribution (distributional contamination).
  - **Low False Negatives (Score < 20% on Seen):** Likely for very large models (e.g., 200B+ parameters) that generalize well and don't exhibit "narrow minima" behavior even on training data.
- **First 3 experiments:**
  1. **Sanity Check (AUC):** Run CoDeC on a model with known training data (e.g., Pythia) using subsets of its training corpus (Seen) vs. data released after its training cutoff (Unseen). Verify AUC ≈ 100%.
  2. **Ablation on Context:** Vary the number of in-context examples (n=0, 1, 3, 5) on a single dataset to visualize the divergence of confidence curves (Figure 9a).
  3. **Diversity Stress Test:** Run CoDeC on a highly diverse unseen dataset (e.g., a mix of Code and Novels). Observe if the score rises near the 50-60% "unseen ceiling" to understand baseline noise for your specific evaluation suite.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can CoDeC be effectively adapted for strict sample-level membership inference?
- **Basis in paper:** [explicit] Section 4 notes that "adapting CoDeC for strict membership inference at the sample level" is a necessary direction for future work.
- **Why unresolved:** The current method aggregates scores at the dataset level to achieve high reliability; individual sample signals are too noisy for fine-grained analysis.
- **What evidence would resolve it:** A modified CoDeC variant that maintains high AUC when classifying individual training samples against unseen samples without aggregation.

### Open Question 2
- **Question:** Is it possible to engineer training strategies that bypass CoDeC detection, and would such methods inherently improve generalization?
- **Basis in paper:** [explicit] Section 4 suggests that "it may be possible to engineer training strategies that bypass increases in CoDeC estimates," though the authors hypothesize this might force models to avoid memorization.
- **Why unresolved:** The authors have not tested specific training modifications designed to evade the metric while retaining the benefits of training on specific data.
- **What evidence would resolve it:** A model training setup that minimizes CoDeC score increases on a target dataset without degrading downstream performance on that distribution.

### Open Question 3
- **Question:** How can CoDeC be modified to robustly evaluate models optimized for chat or reasoning, which currently produce anomalous false positives?
- **Basis in paper:** [explicit] Appendix A.3.2 notes that GPT-OSS 20B produces false high scores due to optimization for chat/reasoning that "impairs standard language sequence modeling," adding that "addressing that issue remains for future work."
- **Why unresolved:** Heavy instruction-tuning appears to destabilize the baseline confidence measurements CoDeC relies upon.
- **What evidence would resolve it:** A normalization technique or confidence metric that yields stable, low CoDeC scores for uncontaminated, heavily instruction-tuned models.

## Limitations
- **Dataset Diversity Sensitivity:** Highly diverse unseen datasets can produce baseline scores of 50-60%, creating ambiguous regions for contamination detection.
- **Model Size Generalization:** The method is validated on models up to 70B parameters, but frontier-scale models (200B+) may show weaker signals due to better generalization.
- **Format Dependency:** Strict matching between training and evaluation formats is required, and the method's robustness to subtle format variations is unclear.

## Confidence
- **High Confidence Claims:**
  - CoDeC achieves 99.9% AUC in separating seen vs. unseen datasets for models in the tested size range (125M-70B parameters)
  - The method is parameter-free, model-agnostic, and computationally efficient
  - CoDeC scores are interpretable as contamination percentages

- **Medium Confidence Claims:**
  - The mechanism of memorization disruption via context interference is valid
  - CoDeC reliably detects contamination without false positives from distributional overlap
  - The method scales to large-scale benchmark evaluations

- **Low Confidence Claims:**
  - CoDeC will maintain performance on frontier-scale models (200B+ parameters)
  - The method is robust to format variations in real-world contamination scenarios
  - The baseline noise floor is well-characterized across all dataset types

## Next Checks
1. **Frontier Model Validation** - Test CoDeC on a 175B+ parameter model (e.g., LLaMA-2 70B or larger) with known contamination to verify the mechanism holds at scale and determine if larger models show weaker signals.

2. **Format Sensitivity Analysis** - Systematically vary dataset formatting (add/remove labels, change delimiters, alter whitespace) on a known contaminated dataset to quantify the method's sensitivity to format mismatches and establish guidelines for robust application.

3. **Diversity Baseline Characterization** - Run CoDeC on a spectrum of highly diverse datasets (code, novels, scientific papers, social media) to establish the true "unseen ceiling" and create a reference table for interpreting scores across different dataset types.