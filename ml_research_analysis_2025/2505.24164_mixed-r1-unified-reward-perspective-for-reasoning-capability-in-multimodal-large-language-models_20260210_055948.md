---
ver: rpa2
title: 'Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal
  Large Language Models'
arxiv_id: '2505.24164'
source_url: https://arxiv.org/abs/2505.24164
tags:
- reward
- arxiv
- data
- zhang
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Mixed-R1, a unified reinforcement learning
  framework that improves multimodal large language models (MLLMs) across diverse
  reasoning tasks. The method combines a mixed reward function design with a carefully
  curated post-training dataset (Mixed-45K) containing 45,000 high-quality examples
  spanning five task categories: yes/no questions, multiple-choice questions, chart/document
  analysis, visual grounding, and open-ended responses.'
---

# Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2505.24164
- Source URL: https://arxiv.org/abs/2505.24164
- Reference count: 40
- Introduces Mixed-R1, a unified reinforcement learning framework that improves MLLMs across diverse reasoning tasks with 2-5% gains on benchmarks

## Executive Summary
Mixed-R1 introduces a unified reinforcement learning framework that enhances multimodal large language models (MLLMs) across diverse reasoning tasks. The method combines a mixed reward function design with a carefully curated post-training dataset (Mixed-45K) containing 45,000 high-quality examples spanning five task categories. The unified reward system employs matching rewards for binary and multiple-choice tasks, chart rewards for numerical reasoning, IoU rewards for grounding tasks, and a novel tokenizer-based Bidirectional Max-Average Similarity (BMAS) reward for open-ended responses. Experiments on Qwen2.5-VL and InternVL models show improvements of 2-5% across benchmarks including MathVista, MathVision, MMMU, MMStar, and AI2D.

## Method Summary
Mixed-R1 builds on DeepSeek-R1's GRPO algorithm with task-decomposed reward functions routed based on task type. The framework uses four reward functions: matching reward for binary/multiple-choice, chart reward for numerical reasoning with 1e-2 tolerance, IoU reward for grounding, and BMAS for open-ended responses. Data is filtered using Qwen2.5-VL-7B to remove examples where all 8 generated responses receive identical rewards, ensuring reward variance for effective advantage computation. The unified framework trains with batch_size=64, G=8 responses per sample, temperature=1.0, KL coefficient=0.04, and learning_rate=3e-6.

## Key Results
- Achieves 49.2 average score across benchmarks, improving MathVista from 61.2 to 66.2 (Qwen2.5-VL-7B)
- BMAS open-ended reward achieves 44.7 average vs. 43.3-43.9 for alternatives including external LLM judges
- 45K filtered dataset outperforms both 20K (undersaturated) and 90K (noise from harder samples) on MathVista
- Improvements of 2-5% on MathVista, MathVision, MMMU, MMStar, and AI2D benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Task-Decomposed Reward Functions
- Decomposing MLLM tasks into distinct categories with tailored reward functions enables stable multi-task GRPO training
- The framework routes each sample to one of four reward functions based on task type, computing task-appropriate signals that share a common scale for advantage calculation in GRPO
- Core assumption: Different task types require semantically different reward computations, but their scalar outputs remain comparable for policy gradient updates
- Evidence anchors: Incremental ablation shows progressive improvement as reward types are added (44.2 → 49.2 avg); related work Omni-R1 similarly proposes unified generative paradigms for multimodal reasoning

### Mechanism 2: BMAS (Bidirectional Max-Average Similarity)
- Using the policy model's own tokenizer embeddings for open-ended reward computation outperforms external LLM judges and produces stable training
- BMAS computes pairwise cosine similarity between response and ground-truth token embeddings, then aggregates bidirectionally: average of max-similarity per response token + average of max-similarity per ground-truth token
- Core assumption: Tokenizer embeddings from the policy model share a semantic space with its generation capabilities, avoiding cross-model misalignment
- Evidence anchors: BMAS achieves 44.7 avg vs. 43.3-43.9 for alternatives including Qwen2.5-0.5B as judge; paper argues semantic space inconsistency causes external model performance drops

### Mechanism 3: Difficulty-Filtered Data Curation
- Filtering training examples using the base model to remove both too-easy and too-hard samples ensures reward variance for effective GRPO advantage computation
- Generate 8 responses per question with temperature 1.0 using Qwen2.5-VL-7B; discard samples where all rewards are identical (indicating uniform success or failure)
- Core assumption: The filtering model approximates the initial policy's capability distribution, identifying examples where learning signal exists
- Evidence anchors: 45K filtered dataset outperforms both 20K (undersaturated) and 90K (noise from harder samples) on MathVista; EvoCoT discusses exploration bottlenecks when rollout accuracy is low on hard problems

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Mixed-R1 builds directly on DeepSeek-R1's GRPO algorithm, which replaces critic models with group-based advantage computation from rule-based rewards
  - Quick check question: Can you explain why GRPO computes advantage as `(r_i - mean(rewards)) / std(rewards)` rather than using a value function?

- **Tokenizer/Word Embeddings**
  - Why needed here: The BMAS reward relies on understanding how tokenizer embeddings represent semantic meaning and how cosine similarity captures token-level relationships
  - Quick check question: Why might "maximum similarity per token" be preferred over "average similarity" for matching open-ended responses?

- **Reward Shaping for Multi-Task RL**
  - Why needed here: The unified framework must balance heterogeneous reward signals across task types without gradient conflicts
  - Quick check question: What could go wrong if IoU rewards (0-1 scale) were mixed with unnormalized chart error rewards without scaling?

## Architecture Onboarding

- **Component map:**
Input: (image, question, ground_truth) 
  → Task Classifier (by answer format)
  → Reward Router → one of: MatchingReward | ChartReward | IoUReward | BMASReward
  → GRPO Loss Computation (advantage from group rewards)
  → Policy Update (Qwen2.5-VL or InternVL backbone)

- **Critical path:**
  1. Data filtering pipeline (Section 3.1) — determines what enters training
  2. Reward function dispatch (Section 3.2, Figure 3) — correct reward per task type
  3. BMAS computation path (Equation 4) — most complex, verify embedding extraction and similarity aggregation
  4. GRPO advantage normalization (Equation 2) — ensures stable gradients

- **Design tradeoffs:**
  - External LLM judge vs. self-tokenizer embeddings: Paper shows external models (ModernBERT, SentenceBERT, Qwen2.5-0.5B) perform worse or add computational cost; tokenizer embeddings are free but may lack semantic depth
  - Dataset size: 45K chosen over 90K despite slight MathVision improvement, due to MathVista degradation and training cost (Table 3a)
  - Bipartite matching vs. max-average: Bipartite forces one-to-one token alignment, causing false positives; max-average allows flexible semantic matching

- **Failure signatures:**
  - Reward stagnation at high values: Indicates filter failure (too-easy samples) — check reward variance per batch
  - Completion length collapse: BMAS should increase length over training; if stable or decreasing, may indicate reward hacking or embedding issues
  - Cross-task interference: If MCQ performance drops when adding IoU data, check reward magnitude balancing

- **First 3 experiments:**
  1. **Reproduce single-reward baseline:** Train with only MCQ+Matching reward on Qwen2.5-VL-3B; verify ~47.2 avg (Table 2, row 3) before adding complexity
  2. **Ablate BMAS vs. tokenizer-bipartite:** Compare Table 4 conditions on a held-out caption dataset; confirm max-average (44.7) beats bipartite (43.7) and document reward variance and completion length curves
  3. **Data scale sensitivity:** Replicate Table 3a at 20K/45K/90K; identify which task types cause MathVista degradation at 90K

## Open Questions the Paper Calls Out

- **Extension to video and multi-image tasks:** The authors state they will extend the unified reward design for image, video, and multiple image inputs in future work, as current experiments are limited to image datasets due to computational constraints.

- **Semantic space consistency with external judges:** The paper finds that external LLM or BERT-based judges cause performance drops compared to tokenizer-based BMAS, arguing this is due to semantic space inconsistency between MLLM text tokenizer outputs and external model language embeddings.

- **Non-monotonic scaling behavior:** The paper observes that 90K training data underperforms 45K on MathVista while improving on other benchmarks, acknowledging this trade-off but not investigating whether task interference or reward conflict causes benchmark-specific degradation at larger scales.

## Limitations

- The BMAS reward design assumes tokenizer embeddings adequately capture semantic relationships for open-ended evaluation, but lacks benchmarking against alternative semantic representations or ablation studies on embedding quality
- The data filtering mechanism could systematically exclude edge cases important for generalization, though only aggregate performance changes across dataset sizes are reported
- The unified reward scale assumption is asserted but not empirically validated through reward magnitude analysis or cross-task training stability metrics

## Confidence

- **High confidence**: Task-decomposed reward architecture and incremental ablation results showing 44.2→49.2 average improvement with added reward types
- **Medium confidence**: Superiority of BMAS over external LLM judges is demonstrated (44.7 vs 43.3-43.9), but comparison lacks external semantic models as baselines
- **Medium confidence**: 45K dataset size selection is justified by MathVista performance trade-offs, but analysis doesn't explore whether task-specific optimal sizes exist

## Next Checks

1. **Reproduce BMAS vs. semantic baselines**: Implement BMAS alongside SentenceBERT and ModernBERT as open-ended judges on the same held-out dataset, measuring both reward variance and downstream task performance to confirm the claimed semantic space consistency advantage

2. **Reward magnitude analysis**: Extract and visualize the distribution of each reward type (matching, chart, IoU, BMAS) across training batches to verify that they operate on comparable scales and that no single reward dominates gradient updates

3. **Data filtering sensitivity**: Systematically vary the "identical rewards" threshold in the filtering pipeline (e.g., allow small variance, test different temperature values for response generation) and measure the impact on both MathVista performance and training stability metrics like reward variance over time