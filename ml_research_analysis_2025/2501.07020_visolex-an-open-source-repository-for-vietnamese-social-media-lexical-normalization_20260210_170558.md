---
ver: rpa2
title: 'ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical Normalization'
arxiv_id: '2501.07020'
source_url: https://arxiv.org/abs/2501.07020
tags:
- normalization
- lexical
- vietnamese
- nguyen
- visolex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ViSoLex is an open-source system for Vietnamese social media lexical
  normalization that provides Non-Standard Word (NSW) lookup and text normalization
  services. The system uses multitask learning to simultaneously detect and normalize
  NSWs, integrating pre-trained language models with weakly supervised learning techniques
  to address data scarcity.
---

# ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical Normalization

## Quick Facts
- arXiv ID: 2501.07020
- Source URL: https://arxiv.org/abs/2501.07020
- Reference count: 7
- Primary result: ViSoLex improves F1-score by 3.74% and maintains high integrity scores (98.27%) for NSW normalization

## Executive Summary
ViSoLex is an open-source system for Vietnamese social media lexical normalization that provides Non-Standard Word (NSW) lookup and text normalization services. The system uses multitask learning to simultaneously detect and normalize NSWs, integrating pre-trained language models with weakly supervised learning techniques to address data scarcity. ViSoLex improves F1-score by 3.74% and maintains high integrity scores (98.27%) for NSW normalization. The system offers both a researcher-focused framework with customizable components and a user-friendly Flask interface for non-experts. By publishing the source code, ViSoLex aims to advance Vietnamese NLP research and support broader applications in lexical normalization.

## Method Summary
ViSoLex employs a hybrid approach combining multitask learning, weakly supervised learning, and dynamic dictionary expansion. The system jointly optimizes NSW detection (binary classification) and normalization (sequence-to-sequence generation) using a shared encoder with task-specific heads. A Rule Attention Network (RAN) provides weak supervision through heuristic rules derived from dictionaries and regex patterns. The system maintains a growing dictionary of NSW definitions, consulting GPT-4o API for unknown words and caching results for future lookups. The framework is built on top of pre-trained Vietnamese language models (BARTpho and ViSoBERT) and provides both command-line and Flask web interfaces for different user needs.

## Key Results
- Multitask learning improves F1-score by 3.74% compared to single-task baselines
- Integrity Score remains high at 98.27%, indicating minimal over-normalization
- System achieves 94.76% Accuracy on the ViLexNorm test set
- ViSoBERT outperforms BARTpho in multitask configuration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask learning jointly optimizing NSW detection and normalization improves normalization accuracy compared to single-task approaches.
- Mechanism: A shared encoder extracts input features, then task-specific heads predict (1) binary NSW detection via BCE loss and (2) normalized token generation via cross-entropy loss. The combined loss L_Total = αL_Norm + βL_NSW allows gradient signals from detection to inform normalization boundaries.
- Core assumption: NSW detection and normalization share underlying representations; detecting "what is non-standard" helps predict "what is standard."
- Evidence anchors:
  - [abstract]: "integrates multitask learning capabilities to simultaneously detect and normalize NSWs"
  - [section 3.2.1]: "This multitask approach enhances efficiency and performance in normalizing noisy social media text"
  - [table 1]: ViSoBERT multitask shows +3.74% F1 improvement at p=1.0 diacritics removal
- Break condition: If detection and normalization require conflicting representations, multitask learning may degrade both; monitor for negative transfer via single-task baselines.

### Mechanism 2
- Claim: Weakly supervised learning with a Rule Attention Network reduces dependency on manually labeled data while maintaining normalization quality.
- Mechanism: The RAN acts as a teacher model, embedding weak rules derived from NSW dictionaries and regex patterns. It learns to assign context-dependent attention weights to each rule, dynamically adjusting influence based on prediction reliability.
- Core assumption: Heuristic rules capture meaningful NSW patterns; the attention mechanism can learn which rules to trust in which contexts.
- Evidence anchors:
  - [abstract]: "weakly supervised learning techniques to ensure accurate and efficient normalization, overcoming the scarcity of labeled data"
  - [section 3.2.2]: "learns to assign different levels of attention to these rules during the training process"
  - [corpus]: Limited direct evidence in corpus for RAN-specific mechanisms; related work (MultiLexNorm++) addresses lexical normalization but not weak supervision architectures
- Break condition: If weak rules are systematically noisy or contradictory without discernible reliability patterns, RAN attention weighting provides limited benefit over fixed rule ensembles.

### Mechanism 3
- Claim: Hybrid dictionary-LLM lookup enables dynamic vocabulary expansion while maintaining retrieval efficiency.
- Mechanism: NSW queries first check a static dictionary built from GPT-4o-generated definitions. If the NSW is absent, the system queries GPT-4o API for normalization suggestions, then adds results to the dictionary for future lookups—creating a growing cache.
- Core assumption: GPT-4o provides reliable NSW interpretations; dictionary growth improves over time without quality degradation.
- Evidence anchors:
  - [section 3.1.1]: "If the word is not in the dictionary, the system consults the OpenAI GPT-4o API to suggest a possible normalization, which is then added to the dictionary for future use"
  - [corpus]: No corpus evidence for this specific hybrid lookup mechanism
- Break condition: If GPT-4o produces inconsistent or incorrect normalizations for domain-specific slang, cached errors propagate; requires periodic dictionary audits.

## Foundational Learning

- Concept: **Multitask Learning with Shared Representations**
  - Why needed here: The system jointly learns detection and normalization; understanding how shared encoders with task-specific heads balance competing objectives is essential for debugging performance tradeoffs.
  - Quick check question: Can you explain why α and β hyperparameters in L_Total matter for balancing detection vs. normalization?

- Concept: **Weak Supervision and Programmatic Labeling**
  - Why needed here: ViSoLex uses heuristic rules and dictionary lookups as noisy supervision signals; understanding how to design, validate, and weight weak rules is critical for extending the system.
  - Quick check question: What makes a weak supervision rule "good" vs. "harmful" in the context of RAN attention learning?

- Concept: **Token-Level Alignment for Sequence-to-Sequence Tasks**
  - Why needed here: Lexical normalization requires mapping non-standard tokens to standard forms while preserving unchanged tokens; alignment tokenizer modifications are a key customization point.
  - Quick check question: How would you handle a token that maps to multiple standard forms depending on context?

## Architecture Onboarding

- Component map:
  - **Input Layer**: User input → service router (Lookup vs. Normalization)
  - **NSW Lookup Service**: Dictionary lookup → GPT-4o fallback → dictionary update
  - **Lexical Normalization Service**: Tokenizer → Multitask Model (Shared Encoder + Detection Head + Generation Head) → Post-processor → Output
  - **Training Pipeline**: Labeled/Unlabeled data → RAN (teacher) + Lexical Normalizer (student) → Weak supervision integration
  - **User Interfaces**: Flask app (/dict_lookup, /normalize_text endpoints)

- Critical path: Input sentence → tokenization → multitask model inference → post-processing (space removal, capitalization) → normalized output with confidence scores. For training: data preparation → weak rule definition → RAN + normalizer joint training → evaluation.

- Design tradeoffs:
  - BARTpho vs. ViSoBERT: ViSoBERT shows stronger multitask gains (+3.74% F1 at p=1.0) but may require more compute; BARTpho offers more modest improvements with potentially lower resource needs.
  - Static dictionary vs. dynamic LLM: Dictionary is fast but incomplete; LLM fallback is comprehensive but adds latency and API dependency.
  - Single-task vs. multitask: Multitask improves F1 but slightly degrades integrity for BARTpho (-0.05%); monitor both metrics for target use case.

- Failure signatures:
  - High F1 but low Integrity Score: Model is over-normalizing, incorrectly modifying standard words.
  - Low F1 with high Integrity: Model is under-detecting NSWs, leaving too many non-standard tokens unchanged.
  - Inconsistent dictionary lookups: Cache pollution from incorrect GPT-4o normalizations; requires manual dictionary review.
  - Post-processing artifacts: Residual spacing or capitalization errors suggest tokenizer/post-processor misalignment.

- First 3 experiments:
  1. **Baseline comparison**: Run single-task vs. multitask inference on held-out test set; compare F1, Integrity, and Accuracy to reproduce Table 1 results and validate system setup.
  2. **Ablation on α/β weighting**: Systematically vary α and β in L_Total (e.g., α ∈ {0.5, 1.0, 2.0}, β ∈ {0.5, 1.0, 2.0}) to identify optimal loss balance for target domain.
  3. **Dictionary coverage audit**: Sample 100 NSW lookups; measure cache hit rate vs. GPT-4o fallback rate, and manually evaluate fallback accuracy to assess dictionary quality and expansion needs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ViSoLex's weakly supervised multitask framework be effectively adapted for lexical normalization in other low-resource languages beyond Vietnamese?
- Basis in paper: [explicit] The abstract states: "Future directions include expanding the system's capabilities for additional languages." Section 6 reiterates: "handling additional languages" as a future direction.
- Why unresolved: While the framework is designed to be modular and customizable (with components like `aligned_tokenizer.py` and `project_variables.py` for adaptation), no experiments or validation have been conducted on non-Vietnamese languages.
- What evidence would resolve it: Cross-lingual evaluation on at least one other language's social media corpus, comparing F1-score, Integrity Score, and Accuracy metrics against language-specific baselines.

### Open Question 2
- Question: To what extent does ViSoLex normalization improve performance on downstream tasks such as sentiment analysis, hate speech detection, and machine translation?
- Basis in paper: [inferred] Section 1 explicitly states normalization is "essential for improving the performance of downstream tasks," but the evaluation (Section 4) only measures normalization metrics, not downstream task impact.
- Why unresolved: No experiments were conducted linking normalization quality to improvements in the downstream applications that motivated the work.
- What evidence would resolve it: Benchmark comparisons on standard Vietnamese sentiment analysis and hate speech detection datasets, with and without ViSoLex preprocessing, showing statistically significant performance differences.

### Open Question 3
- Question: How can the NSW dictionary coverage be expanded to handle rapidly evolving social media slang and emerging non-standard patterns?
- Basis in paper: [explicit] Section 6 identifies "expanding the NSW dictionary and refining the system's ability to predict social-contextual meanings" as a promising direction. Current dictionary relies on GPT-4o API for unknown words.
- Why unresolved: The dynamic nature of social media language means NSW patterns constantly evolve; static dictionaries and weak rules may become outdated quickly.
- What evidence would resolve it: Longitudinal evaluation showing normalization accuracy degradation over time, and experiments with automated dictionary update mechanisms using continuous learning or crowd-sourcing.

## Limitations
- System performance depends heavily on GPT-4o API availability and cost, creating scalability concerns
- The RAN mechanism lacks direct empirical validation for its specific implementation details
- No evaluation of downstream task performance improvements despite normalization being positioned as essential for such tasks

## Confidence

- **High Confidence**: The multitask learning framework and its F1 improvement (+3.74%) are well-supported by ablation results in Table 1
- **Medium Confidence**: The weakly supervised RAN mechanism is theoretically sound but lacks direct corpus validation for its specific implementation
- **Medium Confidence**: The dictionary-LLM hybrid lookup approach is clearly described but has limited empirical validation for cache quality over time

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary α and β in the multitask loss function (e.g., α ∈ {0.5, 1.0, 2.0}, β ∈ {0.5, 1.0, 2.0}) to identify optimal balance and verify the reported F1 improvements are robust to parameter choices

2. **Rule Attention Network Evaluation**: Conduct ablation studies comparing RAN with alternative weak supervision approaches (fixed rule ensembles, no weak supervision) to isolate the specific contribution of the attention mechanism to normalization quality

3. **Dictionary Cache Quality Audit**: Sample 200 GPT-4o API responses across diverse NSW categories, manually evaluate normalization accuracy, and track cache hit rate vs. API fallback rate over time to assess dictionary growth quality and identify potential error propagation patterns