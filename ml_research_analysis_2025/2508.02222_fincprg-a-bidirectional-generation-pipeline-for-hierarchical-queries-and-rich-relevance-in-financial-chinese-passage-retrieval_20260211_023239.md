---
ver: rpa2
title: 'FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and
  Rich Relevance in Financial Chinese Passage Retrieval'
arxiv_id: '2508.02222'
source_url: https://arxiv.org/abs/2508.02222
tags:
- queries
- retrieval
- query
- fincprg
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a bidirectional generation pipeline for creating
  hierarchical queries and rich relevance labels in financial passage retrieval. The
  core method combines bottom-up (intra-doc) and top-down (cross-doc) query generation
  approaches, using LLMs to generate structured queries at multiple granularity levels.
---

# FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval

## Quick Facts
- arXiv ID: 2508.02222
- Source URL: https://arxiv.org/abs/2508.02222
- Reference count: 29
- Primary result: FinCPRG dataset with 94,874 query-passage pairs across 5 subsets, demonstrating strong correlation (Pearson up to 0.9) with real financial benchmarks and effective fine-tuning performance

## Executive Summary
This paper introduces FinCPRG, a novel bidirectional generation pipeline for creating hierarchical queries and rich relevance labels in financial passage retrieval. The method combines bottom-up (intra-doc) and top-down (cross-doc) query generation using LLMs to capture both granular and abstract information needs. A key innovation is the indirect positives mining method, which uses a reranker to discover additional relevant query-passage pairs beyond direct mapping. The pipeline produces a dataset from 1,300 Chinese financial reports with 94,874 query-passage pairs across five subsets. Evaluation shows strong correlation with real financial benchmarks and demonstrates effectiveness as both an evaluation set and training data, with fine-tuned models achieving significant performance improvements on low-resource financial retrieval tasks.

## Method Summary
The FinCPRG pipeline employs a two-stage approach: (1) bottom-up generation chunks text into passages/sentences and uses GPT-4o to generate specific queries; (2) top-down generation clusters documents by industry/time and generates broad intents and sub-queries. The pipeline introduces indirect positives mining via symmetric query similarity using a reranker to discover additional relevant pairs. The dataset construction involves preprocessing 1,317 Chinese financial reports, generating hierarchical queries at three levels (sentence, passage, topic), and annotating relevance through direct mapping plus indirect mining with a 0.99 similarity threshold. The method uses domain-specific constraints (FinBERT2-IC for industry, Time2Vec for temporal encoding) to improve cross-document query quality.

## Key Results
- FinCPRG dataset contains 94,874 query-passage pairs across five subsets (sentence, passage, topic, sentence-mined, passage-mined)
- Pearson correlation with real financial benchmarks reaches up to 0.9
- Fine-tuned models show significant performance improvements on low-resource financial retrieval tasks
- Sentence-level indirect mining shows 24% false positive rate, while topic/passage levels show 10-14% FP rates

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Query Synthesis via Bidirectional Flow
The paper combines bottom-up (intra-doc) and top-down (cross-doc) generation to capture a wider spectrum of information needs than unidirectional approaches. Bottom-up chunks text into passages/sentences and prompts an LLM to generate specific queries, while top-down clusters documents by topic/industry/time and prompts an LLM to generate broad intents and sub-queries. The union creates a 3-level hierarchy (sentence, passage, topic). Core assumption: LLMs can accurately infer user intent both from granular text chunks (deductive) and abstract metadata clusters (inductive).

### Mechanism 2: Indirect Positives Mining via Symmetric Query Similarity
Direct mapping creates false negatives by only linking generated queries to their source passages. The method mines additional pairs via symmetric query-to-query similarity using a reranker with 0.99 threshold. Instead of checking if Passage A matches Query B (asymmetric), it checks if Query A is synonymous with Query B (symmetric). If Query A relates to Passage A, and Query A ≈ Query B, then Query B also relates to Passage A.

### Mechanism 3: Domain-Constrained Clustering for Cross-Doc Queries
Generic embedding models fail to distinguish financial nuances (e.g., similar reports for different industries). The pipeline uses a fine-tuned financial classifier (FinBERT2-IC) and temporal encoding (Time2Vec) to force clustering alignment along industry lines before prompting the LLM for topic-level queries. Core assumption: Financial research queries are fundamentally structured by industry verticals and temporal cycles.

## Foundational Learning

- **Concept: Bi-Encoder vs. Cross-Encoder (Reranker) Architectures**
  - Why needed: The pipeline uses a bi-encoder for retrieval but critically relies on a cross-encoder (reranker) for indirect positives mining. Understanding that rerankers are slower but more accurate is key to grasping why they're used for sensitive label-generation rather than broad retrieval.
  - Quick check: Why does the paper use a reranker (cross-encoder) for mining labels instead of a faster embedding model (bi-encoder)?

- **Concept: False Negatives in Synthetic Data**
  - Why needed: A core motivation is that synthetic datasets usually only link a generated query to its source passage (1-to-1), ignoring other relevant passages. This explains the necessity of indirect positives mining.
  - Quick check: In a standard synthetic dataset, if Passage A generates Query A, what is the status of Passage B (different chunk from same report) relative to Query A, and how does this paper attempt to fix it?

- **Concept: Hard vs. Soft Clustering Constraints**
  - Why needed: The top-down generation uses "Maximum Topic Subtree" selection based on industry dominance (a hard/semi-hard constraint).
  - Quick check: How does the system handle a document cluster where the "industry" label is ambiguous or mixed (e.g., a macro-economic report covering multiple sectors)?

## Architecture Onboarding

- **Component map:**
  Preprocessing: PDF → Text Chunker → BERT-based Quality Filter → Bottom-Up Branch: Chunk → LLM (Query Generation) → Entity Completion; Top-Down Branch: Titles → FinBERT2-IC (Industry) + Time2Vec → HDBSCAN (Clusters) → LLM (Intent Generation); Annotation Engine: Direct Mapping + Reranker (Symmetric Query Mining)

- **Critical path:** The Reranker Threshold (0.99). The entire "richness" of the dataset depends on this cutoff.

- **Design tradeoffs:**
  - Cost vs. Coverage: Localized traversal (within same doc/cluster) balances efficiency against high cost of full cross-encoder inference
  - Precision vs. Recall: 0.99 threshold prioritizes high-precision labels but risks missing valid indirect pairs, especially at sentence level

- **Failure signatures:**
  - High False Positive Rate (Sentence Level): 24% FP rate for sentence-level mining
  - Entity Drift: If "Query Completion" regex fails, queries remain ambiguous and unusable

- **First 3 experiments:**
  1. Threshold Sensitivity Analysis: Re-run mining step with thresholds [0.95, 0.97, 0.99] and manually inspect trade-off between volume and semantic correctness
  2. Ablation on Industry Clustering: Generate dataset using generic BERT embeddings (excluding FinBERT2-IC) and compare topic-level query coherence
  3. Overfitting Test: Fine-tune base model on only passage-mined data and evaluate on sentence data to verify generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the stability and reproducibility of the FinCPRG pipeline be improved to reduce variability caused by commercial LLM API dependencies and complex hyperparameter configurations?
- Basis: Section 7.2 "Limitations and Future Work" explicitly identifies "Stability of Pipeline" as a limitation, noting that multi-stage nature and reliance on commercial APIs introduce "inherent variability" affecting "cross-run reproducibility"
- Why unresolved: Sequential operations and lack of standardized benchmarking procedures make it difficult to ensure consistent outputs across runs or model versions
- Evidence needed: Study demonstrating consistent dataset statistics across multiple generation runs using different random seeds or LLM versions, with fixed configuration benchmarking protocol

### Open Question 2
- Question: Can the "indirect positives mining" method be optimized with adaptive thresholds to reduce the high false positive rate observed in sentence-level query pairs?
- Basis: Section 6.1 reveals disparity in label quality, with 24% FP rate for sentence-level mining vs 10-14% for topic/passage levels, attributed to fixed 0.99 threshold
- Why unresolved: Fixed threshold is not sensitive enough to semantic nuances of fine-grained sentence queries, leading to noise in training data
- Evidence needed: Experiments implementing granularity-specific or dynamic thresholds showing reduction in FP rate for sentence pairs (validated by human or LLM assessment) without significantly dropping recall

### Open Question 3
- Question: How can the pipeline be enhanced to better incorporate deep temporal context and document metadata during cross-document query synthesis?
- Basis: Section 7.2 lists "insufficient utilization of doc metadata and inadequate incorporation of temporal context" as specific "inadequacies in the design of the pipeline"
- Why unresolved: Current top-down approach uses temporal vectors mainly for clustering, but generation of topic-level intents may miss subtle time-bound financial narratives crucial for the domain
- Evidence needed: Ablation study or pipeline modification where metadata is explicitly injected into LLM prompts, resulting in queries demonstrating higher temporal accuracy in human evaluations

## Limitations
- High false positive rate (24%) in sentence-level indirect mining due to fixed similarity threshold
- Pipeline stability and reproducibility issues due to commercial LLM API dependencies and complex hyperparameter configurations
- Insufficient utilization of document metadata and inadequate incorporation of temporal context in cross-document query synthesis

## Confidence
- Claims about bidirectional query generation and indirect positives mining: **High confidence** for dataset construction methodology
- Claims about dataset quality and downstream effectiveness: **Medium confidence** due to limited comparison with alternative approaches and reliance on internal metrics

## Next Checks
1. Conduct blind human evaluation comparing semantic quality of mined indirect pairs against directly mapped pairs across all three granularity levels to quantify precision gains
2. Test FinBERT2-IC industry classifier on external benchmark of Chinese financial reports to measure labeling accuracy and assess impact on topic-level query generation
3. Perform threshold sweep analysis (0.95-0.99) on indirect mining step to map precision-recall trade-off and determine if 0.99 cutoff is optimal for sentence-level pairs where higher FP rates were observed