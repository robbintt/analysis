---
ver: rpa2
title: 'ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models
  through Procedural Plans'
arxiv_id: '2507.23135'
source_url: https://arxiv.org/abs/2507.23135
tags:
- step
- reasoning
- image
- visual
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ISO-Bench, a benchmark for testing whether
  vision-language models can understand causal relationships across visual and textual
  procedural steps. The benchmark uses images of task steps paired with text snippets
  from instructional plans, asking models to judge whether the visual step occurs
  before or after the textual ones.
---

# ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans

## Quick Facts
- **arXiv ID**: 2507.23135
- **Source URL**: https://arxiv.org/abs/2507.23135
- **Reference count**: 24
- **Primary result**: Best-performing VLMs achieve 0.57 F1 (zero-shot) on cross-modal causal reasoning, far below human 0.98 F1

## Executive Summary
ISO-Bench is a benchmark designed to evaluate whether vision-language models can understand causal relationships across visual and textual procedural steps. The benchmark uses images of task steps paired with text snippets from instructional plans, asking models to judge whether the visual step occurs before or after the textual ones. Evaluation of 10 state-of-the-art multimodal models reveals their performance is quite low, with best F1 scores around 0.57 in zero-shot settings. Chain-of-thought prompting only slightly improves results, still falling far behind human performance (0.98 F1). The results highlight a notable gap in multimodal causal reasoning capabilities and suggest directions for improving how models integrate visual and textual information in real-world planning contexts.

## Method Summary
ISO-Bench evaluates cross-modal causal reasoning by presenting models with an image of a task step and a text snippet from a procedural plan, asking whether the visual step must occur before or after the textual steps. The benchmark uses 764 examples from YouCook2 and CrossTask datasets covering cooking, car maintenance, and woodworking. Models are evaluated in zero-shot settings with two prompt modes: direct answer-only and explain-then-answer (chain-of-thought). Performance is measured using per-class and macro-averaged precision, recall, and F1 scores, with human baseline at 0.98 F1 and best model achieving 0.57 F1 (zero-shot) and 0.62 F1 (with CoT).

## Key Results
- VLMs achieve best F1 of 0.57 in zero-shot settings, significantly below human performance of 0.98 F1
- Chain-of-thought prompting provides only marginal improvements (0.02-0.06 F1 gain)
- Models exhibit systematic bias toward predicting dependencies, with high DEP recall but low NONDEP recall
- Error analysis reveals 62% of errors stem from causal reasoning failures, 16% from grounding issues, 14% from action progression errors, and 8% from perceptual errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-modal causal inference requires integrating visual state recognition with procedural dependency reasoning, which current VLMs do not perform reliably.
- **Mechanism:** The task presents an image depicting a task step and a text snippet from a procedural plan. To determine whether the visual step must occur before or after the textual steps, the model must (1) correctly identify the action and object states in the image, (2) map these to the procedural context, and (3) reason about preconditions and effects across modalities.
- **Core assumption:** Models have been trained on image-text pairs but lack explicit training on cross-modal dependency structures where causal relations must be inferred rather than retrieved.
- **Evidence anchors:**
  - [abstract] "Each example presents an image of a task step and a text snippet from a plan, with the goal of deciding whether the visual step occurs before or after the referenced text step."
  - [section 3] "Recognizing such dependencies involves reasoning over actions shown in images and inferring preconditions, causes, sub-goals, and effects."
  - [corpus] Related work (FABLE, MMPlanner) confirms procedural reasoning remains challenging even in text-only settings; multimodal integration compounds this.
- **Break condition:** If visual grounding fails (misidentifying the step in the image), downstream causal inference will fail regardless of reasoning capacity.

### Mechanism 2
- **Claim:** Chain-of-thought prompting provides only marginal improvements (0.02–0.06 F1 gain) because current VLMs lack training on generating reasoning chains that bridge visual and textual modalities.
- **Mechanism:** Explanation-augmented prompting asks models to generate reasoning before answering. This helps closed-source models slightly but hurts some open-source models. The paper hypothesizes that instruction tuning focuses on image description/analysis, not cross-modal reasoning chains.
- **Core assumption:** CoT effectiveness transfers from text-only reasoning tasks; this assumption appears violated for multimodal causal tasks.
- **Evidence anchors:**
  - [section 4.2] "reasoning leads to improvements (~0.02-0.06) for closed-source models... but gains are rather small"
  - [section 4.2] "for other open-source models, there is a notable drop in performance (~0.02-0.16) indicating a weakness in their CoT abilities"
  - [corpus] No direct corpus evidence on why CoT fails specifically for multimodal causal tasks; this remains underexplored.
- **Break condition:** If step distance increases, CoT should theoretically help more, but Figure 2 shows this does not consistently hold for GPT models.

### Mechanism 3
- **Claim:** Models exhibit systematic bias toward predicting dependencies (high recall on DEP, low recall on NONDEP), suggesting they default to "yes" rather than reasoning about independence.
- **Mechanism:** Non-dependent steps can occur in parallel. Recognizing independence requires understanding that effects of one step do not satisfy preconditions of another—a more subtle inference than detecting dependence.
- **Core assumption:** Training data contains more dependent than independent step pairs, or models learn a default "connected" bias.
- **Evidence anchors:**
  - [section 4.2] "most models have high precision but low recall for NONDEP. Models can reliably identify only some of the NONDEP dependencies."
  - [section 4.2] "the best performing models attain high recall and low precision on DEP, indicating a bias towards assessing most data points as having a dependence"
  - [section 5.3] Error analysis: "Causal Reasoning (62%) – Models fail to understand that the step in the image describes either a precondition or an effect"
  - [corpus] MMPlanner similarly identifies object-state consistency as a core challenge in procedural planning.
- **Break condition:** If the task were redesigned to balance precision/recall penalties differently, model behavior might shift but would not address the underlying causal understanding gap.

## Foundational Learning

- **Concept:** Visual grounding of procedural actions
  - **Why needed here:** 16% of errors involve misidentifying which step an image depicts; 8% involve perceptual errors (incorrect object identification). Without correct grounding, causal inference is impossible.
  - **Quick check question:** Given an image of chopped onions and text "sauté vegetables," can the model identify that chopping precedes sautéing?

- **Concept:** Temporal and causal dependency structures in plans
  - **Why needed here:** The task explicitly requires distinguishing dependent (sequential) from non-dependent (parallel) steps. Understanding preconditions and effects is central.
  - **Quick check question:** For "boil water" and "chop vegetables," can the learner explain why these are independent whereas "chop vegetables" must precede "add vegetables to pan"?

- **Concept:** Cross-modal integration beyond alignment
  - **Why needed here:** Existing benchmarks (RecipeQA, MM-ReS) focus on text-image alignment, not on inferring causal relations across modalities. ISO-Bench requires reasoning, not just matching.
  - **Quick check question:** Given an image and text from the same recipe but different steps, can the learner determine temporal ordering without seeing the intervening steps?

## Architecture Onboarding

- **Component map:** Image + text snippet + question template -> Model prediction (with/without explanation) -> Binary classification (DEP/NONDEP) -> Per-class and macro metrics

- **Critical path:**
  1. Load image and text snippet
  2. Model generates prediction (with or without explanation)
  3. Extract "YES"/"NO" from output
  4. Compute per-class and macro metrics

- **Design tradeoffs:**
  - **Direct answer (A) vs. explain-then-answer (E→A):** E→A slightly helps closed-source models but can hurt open-source models. Test both.
  - **Step distance:** Questions about distant steps (>3 apart) should be harder, but CoT gains do not consistently scale with distance.
  - **Direction (before/after):** Models perform slightly better on "after" questions (reasoning about effects) than "before" (reasoning about preconditions).

- **Failure signatures:**
  - High DEP recall + low NONDEP recall → model bias toward predicting dependence
  - Explanation contains correct visual description but wrong temporal conclusion → causal reasoning failure
  - Explanation misidentifies image content → grounding failure (16% of errors)
  - Explanation assumes action completed when it is in progress → action progression error (14%)

- **First 3 experiments:**
  1. **Baseline replication:** Run all 10 models (or a subset: GPT-4o, Claude Sonnet, LLaVA-1.5, Qwen2.5-VL) on ISO-Bench with both A and E→A settings. Confirm macro F1 ranges match reported 0.35–0.62.
  2. **Error stratification:** For a sampled 100 errors from the best-performing model, categorize into: causal reasoning, grounding, action progression, perception. Compare to paper's 62%/16%/14%/8% distribution.
  3. **Ablation on step distance:** Split test set by step distance (<3 vs. ≥3) and compare F1 scores. Verify whether CoT helps more for distant steps (hypothesized but not consistently observed).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-tuning or adapter-based methods significantly improve VLM performance on multimodal causal reasoning tasks?
- **Basis in paper:** [explicit] The authors state they "are unable to perform fine-tuning or adapter based experiments on ISO-BENCH. We leave this exploration to future work."
- **Why unresolved:** The benchmark evaluation only covers zero-shot settings; the potential gains from task-specific training remain unknown.
- **What evidence would resolve it:** Fine-tuning experiments on ISO-Bench showing performance improvements over zero-shot baselines.

### Open Question 2
- **Question:** Would training VLMs specifically on multimodal reasoning chains improve their chain-of-thought performance on causal tasks?
- **Basis in paper:** [inferred] Authors hypothesize poor CoT performance in open-source models is "due to the limitations of instruction tuning in vision-language modeling where models are mainly finetuned to describe or analyze images, not produce reasoning chains across modalities."
- **Why unresolved:** Current instruction tuning emphasizes description over cross-modal reasoning, but this hypothesis has not been tested.
- **What evidence would resolve it:** Comparison of models trained with vs. without multimodal reasoning chain data on ISO-Bench.

### Open Question 3
- **Question:** Why does chain-of-thought reasoning fail to provide larger gains for temporally distant steps compared to adjacent steps?
- **Basis in paper:** [inferred] Analysis (Section 5.1) finds that "As step distance increases... Explanations should provide larger gains for larger distance over smaller distance. Surprisingly, this does not hold true for GPT models."
- **Why unresolved:** The expected relationship between step distance and reasoning benefits was not observed; the mechanism remains unclear.
- **What evidence would resolve it:** Detailed analysis of reasoning patterns for close vs. distant step pairs, potentially with intermediate reasoning supervision.

### Open Question 4
- **Question:** How well do these findings generalize to non-English instructional plans?
- **Basis in paper:** [explicit] "Our work only investigates English-language documents (plans) and this limits the generalizability of our findings to other languages."
- **Why unresolved:** No multilingual evaluation was conducted; cross-lingual causal reasoning capabilities are untested.
- **What evidence would resolve it:** ISO-Bench evaluation on translated or natively non-English instructional content.

## Limitations

- The benchmark assumes single-image grounding is sufficient to represent a procedural step, which may not capture dynamic state changes visible across video frames.
- Error analysis percentages are derived from manual annotation of a small sample, introducing potential sampling bias.
- Cross-task generalization is not tested; models may perform differently on domains outside cooking, car maintenance, and woodworking.
- The effectiveness of chain-of-thought prompting is inconsistent across models, suggesting that improvements may be more reflective of model-specific instruction tuning than general reasoning capacity.

## Confidence

- **High confidence**: The observation that VLMs struggle with cross-modal causal reasoning (F1 ~0.57 vs. human 0.98) is well-supported by direct quantitative evidence.
- **Medium confidence**: The claim that models exhibit systematic bias toward DEP predictions is supported, but the underlying cause (training data imbalance vs. architectural limitation) remains speculative.
- **Low confidence**: The mechanism explaining why CoT helps closed-source models but hurts open-source ones is inferred rather than directly tested; no ablation on instruction-tuning data is provided.

## Next Checks

1. **Replication on held-out domains**: Evaluate the best-performing models on procedural plans from domains not represented in the training set (e.g., medical procedures, electronics repair) to test generalization of causal reasoning.
2. **Temporal ordering ablation**: Create a variant of ISO-Bench where models must order *all* steps (not just judge pairwise before/after), testing whether error patterns persist under more complex temporal reasoning demands.
3. **Cross-modal grounding probe**: Design a diagnostic task where models must first correctly label the action and object states in each image before making causal judgments, isolating grounding failures from reasoning failures.