---
ver: rpa2
title: Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray
  Report Generation
arxiv_id: '2502.20056'
source_url: https://arxiv.org/abs/2502.20056
tags:
- report
- mlrg
- reports
- multi-view
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of generating accurate radiology
  reports by integrating multi-view longitudinal data from chest X-rays. The proposed
  method, MLRG, uses a two-stage approach: first, it employs multi-view longitudinal
  contrastive learning to capture spatial and temporal features from current and historical
  images, and second, it introduces tokenized absence encoding to handle missing patient-specific
  prior knowledge.'
---

# Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation

## Quick Facts
- arXiv ID: 2502.20056
- Source URL: https://arxiv.org/abs/2502.20056
- Authors: Kang Liu; Zhuoqi Ma; Xiaolu Kang; Yunan Li; Kun Xie; Zhicheng Jiao; Qiguang Miao
- Reference count: 40
- One-line primary result: Achieves state-of-the-art performance on MIMIC-CXR, MIMIC-ABN, and Two-view CXR datasets with improvements in BLEU-4 (2.3%), F1 score (5.5%), and F1 RadGraph (2.7%) compared to existing methods.

## Executive Summary
This paper addresses the challenge of generating accurate radiology reports by integrating multi-view longitudinal data from chest X-rays. The proposed MLRG method uses a two-stage approach: first, it employs multi-view longitudinal contrastive learning to capture spatial and temporal features from current and historical images, and second, it introduces tokenized absence encoding to handle missing patient-specific prior knowledge. This allows the model to adapt flexibly to scenarios with or without such data, ensuring accurate report generation.

## Method Summary
The MLRG method consists of two stages. In Stage 1, a RAD-DINO vision encoder and CXR-BERT text encoder are pre-trained using multi-view longitudinal contrastive learning to align visual and textual representations. The MLF (Multi-view Longitudinal Fusion) network fuses current and prior image features using learnable view and temporal positional embeddings. In Stage 2, the pre-trained encoders initialize a multi-modal fusion network that combines visual features with text embeddings (including special tokens for missing data) to generate reports using DistilGPT2.

## Key Results
- Achieves state-of-the-art performance on MIMIC-CXR, MIMIC-ABN, and Two-view CXR datasets
- Notable improvements in BLEU-4 (2.3%), F1 score (5.5%), and F1 RadGraph (2.7%) compared to existing methods
- The tokenized absence encoding technique effectively handles missing patient-specific prior knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating multi-view longitudinal data with spatial and temporal contrastive learning improves visual feature consistency and diagnostic context.
- **Mechanism:** The model uses a vision encoder (RAD-DINO) to extract patch-level features. It then applies learnable view positional embeddings to differentiate between views (e.g., PA, AP, Lat). A multi-positive contrastive loss maximizes similarity between images from the same visit (anchors and auxiliary references) while minimizing similarity to images from different visits. This forces the encoder to produce consistent feature representations for the same patient visit, regardless of the specific view.
- **Core assumption:** The geometric and structural differences between views contain complementary diagnostic information that, when aligned via contrastive learning, creates a more robust visual embedding. Furthermore, the most recent previous visit provides the most relevant temporal context.
- **Evidence anchors:**
  - [abstract] "we introduce a multi-view longitudinal contrastive learning method that integrates spatial information from current multi-view images and temporal information from longitudinal data."
  - [section] "To identify differences among views, we incorporate learnable view positional embeddings Ev... into visual features. ... we employ multi-positive contrastive learning [47] to maximize the similarity between images from the same visit while minimizing the similarity to images from different visits."
  - [corpus] PriorRG [78898] also emphasizes the importance of patient-specific prior knowledge and contrastive pre-training, supporting the general direction but MLRG's specific multi-view multi-positive approach is distinct.
- **Break condition:** The mechanism relies on the availability of multiple views per visit. If the dataset is dominated by single-view studies, the multi-positive contrastive loss cannot be computed for those samples, potentially limiting the learned feature consistency. The mechanism also assumes that temporal changes are visually significant enough to be captured by the fusion network.

### Mechanism 2
- **Claim:** Supervising pre-training with the inherent spatiotemporal information of radiology reports (instance-wise cross-modal alignment) improves the alignment of visual features with complex clinical descriptions, including disease progression.
- **Mechanism:** Instead of aligning current images only with the current report, the model aligns fused spatiotemporal visual features (from the MLF network) with the textual features of the *current* report. The current report often contains comparisons to prior studies. The instance-wise cross-modal alignment loss treats image-text pairs from the same visit as positive pairs, but crucially, it *also* treats pairs from different visits with identical reports as positive pairs. This teaches the model that if the clinical description is unchanged, the visual representation should be similar, linking visual stability to textual consistency.
- **Core assumption:** The "FINDINGS" section of a report effectively summarizes the current visual state *and* the change from the past (temporal context). Aligning the fused visual feature to this comprehensive text description is more effective than aligning only to the current image features.
- **Evidence anchors:**
  - [abstract] "This method also utilizes the inherent spatiotemporal information of radiology reports to supervise the pre-training of visual and textual representations."
  - [section] "Given that radiology report content may be consistent across visits, we consider both spatiotemporal and textual features from the same visit as positive pairs, as well as those from different visits with identical reports."
  - [corpus] Corpus signals for "Phrase-grounded Fact-checking" [57867] and "Lunguage" [34942] highlight the importance of fine-grained clinical semantics and temporal dependencies in report evaluation, which this mechanism aims to capture during pre-training.
- **Break condition:** This mechanism assumes the textual report is an accurate and complete description of the visual evidence and temporal change. If reports are noisy, incomplete, or written with significant stylistic variation without clinical change, the alignment signal may be degraded.

### Mechanism 3
- **Claim:** Tokenized absence encoding allows the multi-modal fusion network to robustly handle missing prior knowledge, preventing error propagation from unavailable data.
- **Mechanism:** Patient-specific priors like "INDICATION" and "previous report" are often missing (e.g., first visit). The model uses special tokens "[NHI]" (No Human Indication) and "[NHPR]" (No Human Previous Report) to explicitly represent the absence of this information. These tokens are learned embeddings. During the multi-modal fusion stage, these placeholder tokens are passed through the network just like real text embeddings. This ensures the transformer's attention mechanism has a valid input to attend to (or ignore) regardless of data availability, preventing the model from crashing or generating nonsensical outputs due to missing inputs.
- **Core assumption:** Explicitly modeling "absence" as a distinct state is better than zero-padding or masking, as it provides a learnable context for the model to adjust its report generation strategy when historical data is unavailable.
- **Evidence anchors:**
  - [abstract] "we present a tokenized absence encoding technique to flexibly handle missing patient-specific prior knowledge, allowing the model to produce more accurate radiology reports based on available prior knowledge."
  - [section] "we propose a tokenized absence encoding technique to handle missing patient-specific prior knowledge... Specifically, for missing 'INDICATION' and 'previous report', we utilize special tokens, '[NHI]' and '[NHPR]', to simulate their presence."
  - [corpus] Weak direct corpus support for this specific tokenized absence technique. Related work in the paper mentions handling longitudinal data but doesn't explicitly detail this tokenization strategy for missing priors.
- **Break condition:** The mechanism relies on the training data containing a sufficient mix of samples with and without priors so the model can learn meaningful representations for the "[NHI]" and "[NHPR]" tokens. If the training set is biased heavily towards one case, the model may not generalize well to the other.

## Foundational Learning

- **Concept: Contrastive Learning (Vision-Language)**
  - **Why needed here:** This is the core pre-training technique. The model learns by pulling related image-text pairs closer in the embedding space and pushing unrelated pairs apart. Understanding that the loss function is based on similarity scores (e.g., cosine similarity scaled by temperature) is fundamental to grasping how the visual and textual encoders are trained.
  - **Quick check question:** How does the model know which image corresponds to which report in a batch, and what happens to the loss when they are incorrectly paired?

- **Concept: Multi-Head Self-Attention and Cross-Attention**
  - **Why needed here:** The architecture relies on transformers. The MLF (Multi-view Longitudinal Fusion) network uses cross-attention (current image as query, auxiliary images/history as key/value) to fuse information. The multi-modal fusion network uses cross-attention to combine visual features with text embeddings. Understanding Query, Key, and Value is non-negotiable for debugging these components.
  - **Quick check question:** In the MLF network, which input serves as the Query and which serves as the Key/Value, and what is the resulting shape of the output compared to the input?

- **Concept: Positional Embeddings**
  - **Why needed here:** Transformers are permutation-invariant. This model uses *two* types of learnable positional embeddings: view embeddings (to distinguish PA, AP, Lat) and temporal embeddings (to distinguish current vs. prior). This allows the model to inject structural and temporal awareness directly into the patch tokens before fusion.
  - **Quick check question:** If you remove the view positional embeddings but keep the temporal ones, how might the model's interpretation of a PA vs. an AP image change?

## Architecture Onboarding

- **Component map:**
  - Stage 1 (Pre-training):
    - RAD-DINO (vision encoder) -> Patch embeddings
    - CXR-BERT (text encoder) -> Text embeddings
    - MLF Network -> Fused spatiotemporal visual features
    - Projection Heads -> Common latent space
    - MPC and LG losses -> Encoder training
  - Stage 2 (Generation):
    - RAD-DINO and CXR-BERT (initialized) -> Visual and text features
    - Multi-modal Fusion Network -> Fused representation
    - DistilGPT2 (initialized) -> Report generation
    - Cross-entropy loss -> Fine-tuning

- **Critical path:**
  1. Data Preparation: A single training example must contain current multi-view images, a previous image (or null), an indication (or null), a previous report (or null), and the current reference report.
  2. Stage 1 Forward Pass: Pass images and text through encoders -> Apply positional embeddings -> Fuse via MLF -> Project -> Compute MPC and LG losses.
  3. Stage 2 Forward Pass: Load Stage 1 weights -> Process priors (tokenize absence) -> Fuse visual and text features -> Generate report with DistilGPT2 -> Compute CE loss.

- **Design tradeoffs:**
  - **Two-stage vs. End-to-End:** The paper uses a two-stage approach (pre-train, then fine-tune/generate). This decouples representation learning from generation, likely leading to more robust visual features but requiring more complex training management. An end-to-end approach might be simpler but could produce less generalized visual embeddings.
  - **Frozen vs. Trainable Encoders:** The text generator (DistilGPT2) is initialized from a pre-trained model (CGPT2) and the vision encoder (RAD-DINO) is pre-trained on chest X-rays. The paper fine-tunes projection heads and fusion layers extensively. Freezing the core encoders could speed up training but might limit adaptation; fine-tuning them (as they likely do, given the results) is more effective but computationally expensive.
  - **Complexity of Fusion:** The MLF network adds significant complexity to handle variable numbers of views and missing priors. A simpler concatenation approach might fail to capture the nuanced spatiotemporal relationships the paper targets.

- **Failure signatures:**
  - **High BLEU, Low F1 RadGraph:** The model generates fluent text but misses or hallucinates clinical entities. This suggests the cross-modal alignment in Stage 1 is weak or the generator is overfitting to language patterns.
  - **Crash on Missing Data:** The model fails if the tokenized absence encoding is not implemented correctly (e.g., not handling null inputs in the dataloader or not adding the special tokens to the vocabulary).
  - **View Confusion:** The model describes an AP view finding as a PA view finding. This points to a failure in the view positional embedding learning or MLF attention mechanism.
  - **Temporal Hallucination:** The model invents changes from a prior study that didn't happen. This could stem from noisy training data where reports mention comparisons not visible in the provided prior image.

- **First 3 experiments:**
  1. **Ablation on Stage 1 Pre-training:** Train the full Stage 2 pipeline from scratch (random initialization) vs. initializing from Stage 1 weights. Measure the performance gap on both NLG and CE metrics. This validates the entire premise of the pre-training strategy.
  2. **Input Modality Ablation:** Systematically remove one input source at a time (e.g., no prior image, no indication, no multi-view data) and measure performance drop. This quantifies the contribution of each component mentioned in Table 3.
  3. **Missing Data Robustness Test:** Create a test set where you artificially remove the indication and prior report for all samples. Compare performance of the full model (using the "[NHI]" and "[NHPR]" tokens) against a baseline model that wasn't trained with these tokens and receives zero-padded input. This directly tests the tokenized absence encoding mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can saliency maps and visual question answering (VQA) datasets be effectively integrated to enhance region-based feature learning and reduce diagnostic uncertainty?
- **Basis in paper:** [explicit] The authors state in the Conclusion and Appendix A.5 that future work will focus on "using saliency maps to learn region-based features and predict uncertainty" and are "exploring the integration of saliency maps and MIMIC-CXR-VQA data."
- **Why unresolved:** The current model sometimes fails to describe lesion attributes accurately (e.g., confusing "proximal" and "middle" parts of the stomach) because it has not fully learned region-level features.
- **Evidence:** Improved accuracy in describing lesion attributes in qualitative case studies and a reduction in clinically significant errors related to severity misassessment.

### Open Question 2
- **Question:** Can the tokenized absence encoding technique be refined to better mitigate the performance degradation that occurs when patient-specific prior knowledge is missing?
- **Basis in paper:** [inferred] While the paper introduces tokenized absence encoding to handle missing data, Table 4 shows a notable performance drop when data is absent (e.g., F1 RadGraph drops from 0.318 to 0.254 when "Indication" is missing).
- **Why unresolved:** The current method allows the model to run without crashing, but it does not fully compensate for the informational loss compared to scenarios with complete longitudinal data.
- **Evidence:** A narrowing of the performance gap between "w/ Indication" and "w/o Indication" subsets (and similar longitudinal data subsets) in ablation testing.

### Open Question 3
- **Question:** What architectural modifications are required to specifically reduce the rates of anatomical misidentification and severity misassessment in generated reports?
- **Basis in paper:** [inferred] Table A8 indicates that while MLRG lowers total errors, it increases errors in "Misidentification of anatomic location" (0.199 vs 0.154) and "Misassessment of severity" (0.284 vs 0.273) compared to the SEI baseline.
- **Why unresolved:** The current contrastive learning approach optimizes for general alignment and finding detection but lacks specific mechanisms to enforce precise spatial localization or severity grading.
- **Evidence:** A decrease in GREEN error categories (c) and (d) to levels below the best existing baselines, without sacrificing the number of matched findings.

## Limitations
- The effectiveness of the multi-positive contrastive learning mechanism is heavily dependent on the dataset containing sufficient multi-view studies per visit.
- The tokenized absence encoding technique, while innovative, lacks strong empirical validation in the paper - the performance gain from handling missing priors is not isolated in ablation studies.
- The two-stage training pipeline introduces significant engineering complexity and potential for error propagation between stages that is not fully explored.

## Confidence
- **High confidence:** The overall two-stage training framework (pre-training + generation) and the use of contrastive learning for vision-language alignment are well-established techniques in the literature, with clear implementation details provided.
- **Medium confidence:** The specific multi-view longitudinal contrastive learning formulation and the MLF network architecture are novel contributions, but the paper provides limited ablation studies to isolate their individual contributions to the performance gains.
- **Low confidence:** The exact impact of the tokenized absence encoding on handling missing patient history is unclear, as the paper does not provide comparative results showing performance degradation when this mechanism is removed.

## Next Checks
1. Create a controlled experiment where all indication and previous report inputs are artificially removed, then compare model performance using the special tokens versus zero-padding - this directly tests the tokenized absence encoding mechanism.
2. Analyze the distribution of view counts in the training data and evaluate model performance as a function of available views per visit to quantify the dependency on multi-view data.
3. Perform a systematic ablation of the MLF network architecture by removing temporal/positional embeddings one at a time to measure their individual contributions to the overall performance.