---
ver: rpa2
title: Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined
  Concepts
arxiv_id: '2509.04926'
source_url: https://arxiv.org/abs/2509.04926
tags:
- language
- levels
- definitions
- which
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of controlling conversational
  agents based on large language models to ensure predictable, user-personalized responses.
  The proposed ontology-based approach quantifies qualitatively-defined conversational
  features by leveraging linguistic descriptors, enabling their integration into an
  ontology for reasoning and consistency checking.
---

# Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts

## Quick Facts
- **arXiv ID**: 2509.04926
- **Source URL**: https://arxiv.org/abs/2509.04926
- **Reference count**: 11
- **Primary result**: Ontology-based method quantifies qualitative conversational features via decision trees, achieving 0.66 accuracy on CEFR classification and improving LLM control of generated content.

## Executive Summary
This paper presents an ontology-based approach to controlling conversational agents by quantifying qualitative conversational features through linguistic descriptors. The method uses decision tree classifiers to establish feature-based definitions of qualitative concepts, formalizes these in description logic, and incorporates them into an ontology. Applied to CEFR proficiency-level control, the approach demonstrates consistent and explainable proficiency-level definitions. Experimental results show improved distinction of readability metrics across CEFR levels after fine-tuning a language model with these definitions, validating the method's effectiveness in controlling generated content properties.

## Method Summary
The approach extracts linguistic descriptors (readability metrics, lexical, syntactic, and discourse features) from CEFR-annotated text datasets. A decision tree classifier (max depth 5, min samples 50, Gini impurity) maps these numerical descriptors to qualitative concept labels (CEFR levels A1-C2), extracting value ranges per class as formal membership rules. These ranges are translated into Manchester syntax (description logic) and incorporated into an ontology. The ontology guides controlled text generation through LLM fine-tuning using causal language modeling with LoRA adapters, where ontology-annotated prompts train the model to generate text matching specified concepts.

## Key Results
- Decision Tree Classifier achieved 0.66 accuracy and 0.42 MAE on CEFR-T dataset
- Fine-tuned Llama3-8B-Instruct showed improved Flesch-Kincaid Grade Level (FKGL) separation across CEFR levels
- Ontology-based definitions provided consistent and explainable proficiency-level classifications
- Generated content demonstrated clear differentiation in readability metrics when controlled by ontology definitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Qualitative conversational features can be systematically converted to quantitative definitions via decision tree classification on linguistic descriptors.
- Mechanism: A decision tree classifier maps numerical values of linguistic descriptors to qualitative concept labels, extracting value ranges per class as formal membership rules.
- Core assumption: The relationship between linguistic descriptors and qualitative concepts is learnable and the derived ranges generalize beyond training data.
- Evidence anchors:
  - [abstract] "The method employs a decision tree classifier to establish feature-based definitions of each level, which are then formalized in description logic and incorporated into an ontology."
  - [section 4] "Using Gini impurity as a selection criterion, we restrict class definitions to a limited number of features (descriptors) with minimal impact on performance."
  - [corpus] Weak/no direct corpus evidence for this specific DTC-to-ontology pipeline; related work focuses on ontology-based semantic similarity and process modeling, not descriptor-derived definitions.
- Break condition: If descriptor-to-concept mappings are highly non-linear or context-dependent, DTC-derived ranges may fail to capture true concept boundaries.

### Mechanism 2
- Claim: Feature-based definitions extracted from classifiers can be formalized in description logic and integrated into ontologies for reasoning and consistency checking.
- Mechanism: DTC-derived value ranges are translated into Manchester syntax (description logic), defining ontology sub-concepts with explicit membership conditions.
- Core assumption: Decision tree rules sufficiently approximate concept semantics and are expressible in description logic without information loss.
- Evidence anchors:
  - [abstract] "These definitions are then formalized in description logic and incorporated into an ontology, which guides controlled text generation of an LLM through fine-tuning."
  - [section 4] "From the DTC, we explore the tree to identify the feature value ranges for each class, treat redundancies, and translate these range constraints to a description logic syntax – namely the Manchester syntax."
  - [corpus] Corpus shows ontology-based approaches for consistency verification and concept matching, but no direct evidence for DTC-to-description-logic translation.
- Break condition: If concept definitions require expressive power beyond description logic (e.g., probabilistic constraints, temporal reasoning), formalization will be incomplete.

### Mechanism 3
- Claim: Ontology-based concept definitions can guide LLM fine-tuning to control generated content properties (e.g., proficiency level).
- Mechanism: Ontology-derived definitions annotate training data; LLM is fine-tuned via causal language modeling with LoRA adapters, learning to generate text matching specified concepts.
- Core assumption: The LLM can internalize concept definitions through fine-tuning and generalize to generate appropriate content at inference time.
- Evidence anchors:
  - [abstract] "Fine-tuning a language model using these definitions improved the distinction of readability metrics across CEFR levels, demonstrating the method's effectiveness in controlling generated content."
  - [section 6] "Figure 2 shows how the generated content differentiates along the readability metrics used in ontological definitions... with the fine-tuned model, the FKGL discriminates simple from medium and complex sentences."
  - [corpus] Weak corpus connection; related work addresses RAG and knowledge graphs for LLMs, but not ontology-guided fine-tuning for generation control.
- Break condition: If fine-tuning data is insufficient, noisy, or concept definitions conflict, the model may fail to discriminate between concepts or overfit to spurious patterns.

## Foundational Learning

- Concept: **Decision Tree Classifiers and Rule Extraction**
  - Why needed here: Core mechanism for deriving interpretable, feature-based definitions from annotated data.
  - Quick check question: Can you explain how a decision tree splits data and how to extract classification rules from its structure?

- Concept: **Description Logic and Ontology Formalization**
  - Why needed here: Required to translate quantitative definitions into a formal ontology representation for reasoning.
  - Quick check question: What is description logic, and how does it differ from propositional logic in expressiveness?

- Concept: **Causal Language Modeling and LoRA Fine-Tuning**
  - Why needed here: Method for adapting LLMs to incorporate ontology-based constraints during generation.
  - Quick check question: How does causal language modeling differ from masked language modeling, and what role do LoRA adapters play in parameter-efficient fine-tuning?

## Architecture Onboarding

- Component map:
  1. **Data Layer**: CEFR-annotated datasets (CEFR-T for training, CEFR-SP for control)
  2. **Feature Extraction**: Linguistic descriptors (readability metrics, lexical, syntactic, discourse features)
  3. **Classifier Module**: Decision Tree Classifier (max depth 5, min samples 50, Gini impurity)
  4. **Ontology Module**: Description logic definitions in Manchester syntax, implemented in Protégé
  5. **Fine-Tuning Module**: Llama3-8B-Instruct with LoRA adapters, causal language modeling
  6. **Evaluation**: Accuracy, MAE, confusion matrix, readability metric distributions

- Critical path:
  1. Prepare and validate CEFR-annotated dataset
  2. Extract linguistic features for all texts
  3. Train DTC, extract rules, and translate to description logic
  4. Build ontology with concept definitions
  5. Annotate fine-tuning data using ontology definitions
  6. Fine-tune LLM with ontology-annotated prompts
  7. Evaluate generation control via readability metric distributions

- Design tradeoffs:
  - **Interpretability vs. accuracy**: DTC chosen for explicit rules despite lower accuracy (0.66) vs. deep learning alternatives (0.75–0.95)
  - **Tree depth vs. generalization**: Limiting depth to 5 mitigates overfitting but may miss complex concept boundaries
  - **Training data choice**: CEFR-T chosen over CEFR-SP for training due to better generalization, but may bias toward news-style texts

- Failure signatures:
  - High confusion between adjacent CEFR levels (evident in confusion matrix)
  - Readability metric distributions overlapping across levels in pre-fine-tuning model
  - Ontology definitions failing to capture domain-specific nuances if features are incomplete

- First 3 experiments:
  1. Replicate DTC training on CEFR-T, validate accuracy and MAE against reported values (0.66 accuracy, 0.42 MAE)
  2. Extract and inspect DTC rules for one CEFR level, translate to Manchester syntax, and verify ontology consistency in Protégé
  3. Fine-tune Llama3-8B-Instruct on a small ontology-annotated subset, generate samples for each CEFR level, and compare FKGL distributions against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ontology-based control framework simultaneously manage multiple qualitative aspects of a conversation (e.g., proficiency level and user emotion) without generating conflicting constraints?
- Basis in paper: [explicit] The conclusion explicitly identifies the need to "enable control on several aspects at the same time by combining definitions" as a future direction.
- Why unresolved: The current study only validates the framework on a single concept (Proficiency Level). It is unclear if the conjunction of quantitative definitions from different domains (e.g., complexity vs. sentiment) would result in logical inconsistencies or optimization conflicts during LLM fine-tuning.
- What evidence would resolve it: An experiment where the LLM is fine-tuned using an ontology containing definitions for two distinct qualitative variables, followed by an evaluation of the model's ability to satisfy both constraints concurrently.

### Open Question 2
- Question: Is the proposed decision-tree-based quantification method effective for qualitative concepts that do not correlate strongly with standard linguistic metrics?
- Basis in paper: [inferred] The method relies on linguistic descriptors (readability, syntax) to define concepts. While the paper validates this for proficiency (which is structurally defined), it lists abstract concepts like "user emotion" and "conviction" as targets which may not map cleanly to syntactic features.
- Why unresolved: The paper does not demonstrate if the chosen descriptors (lexical, morphological) are sufficient to quantitatively define non-structural concepts like emotion, or if such definitions would result in low classifier accuracy.
- What evidence would resolve it: Application of the same pipeline to a dataset annotated for abstract qualitative features (e.g., emotion detection) to verify if the resulting definitions yield comparable classification accuracy and feature importance.

### Open Question 3
- Question: Can the derived quantitative definitions remain robust across diverse text domains without overfitting to specific structural characteristics?
- Basis in paper: [inferred] The paper notes that training the classifier on both available datasets (CEFR-T and CEFR-SP) failed because it produced "overly specific rules that failed to capture the distinctions" between the data types.
- Why unresolved: This failure suggests the definitions may be brittle or overly sensitive to the source domain (e.g., news text vs. isolated sentences), limiting the ontology's applicability to general conversational settings.
- What evidence would resolve it: A cross-domain generalization test where the rules learned from one text genre are applied to a different, unseen conversational dataset to measure the stability of the classification definitions.

### Open Question 4
- Question: Does the noise inherent in the Decision Tree Classifier's definitions limit the upper bound of the fine-tuned LLM's performance?
- Basis in paper: [inferred] The method uses a DTC with an accuracy of 0.66 to define the ground truth ontology. This implies that up to 34% of the rule-based definitions guiding the fine-tuning may be incorrect approximations.
- Why unresolved: The paper shows improved distinction in readability metrics but does not analyze if the 0.34 error rate in the "teacher" model (the ontology) imposes a ceiling on the "student" model's ability to align with true human-level proficiency.
- What evidence would resolve it: A comparison of LLM performance when fine-tuned against DTC-derived rules versus LLM performance when fine-tuned against "perfect" (human-verified) rule sets.

## Limitations

- The 0.66 accuracy of the decision tree classifier raises concerns about whether extracted rules capture true concept boundaries or merely dataset-specific patterns.
- The translation of DTC rules to description logic and ontology-guided fine-tuning methodology lack sufficient empirical validation and are underspecified.
- The method's effectiveness for qualitative concepts that don't correlate strongly with standard linguistic metrics (e.g., emotion, conviction) remains unproven.

## Confidence

- **High Confidence**: The overall pipeline design (descriptor extraction → DTC → ontology → fine-tuning) is methodologically sound and builds on established NLP and knowledge representation techniques.
- **Medium Confidence**: The DTC accuracy (0.66) and MAE (0.42) values are reported, but without significance testing or comparison to baselines beyond generic deep learning approaches.
- **Low Confidence**: The translation of DTC rules to description logic, ontology-guided fine-tuning methodology, and actual generation control effectiveness lack sufficient empirical validation.

## Next Checks

1. **Rule Generalization Test**: Apply DTC-derived ontology definitions to classify CEFR-SP sentences and measure performance drop from CEFR-T baseline. A substantial drop (>15%) would indicate overfitting to training domain.

2. **Ontology Consistency Verification**: Using Protégé, check for logical inconsistencies in the Manchester syntax definitions (e.g., unsatisfiable concepts, contradictory value ranges). Report any detected issues and their frequency.

3. **Controlled Generation Evaluation**: Generate 50 samples per CEFR level using the fine-tuned model, compute readability metrics (FKGL, Gunning-Fog, Dale-Chall), and perform statistical tests (ANOVA) to verify significant separation across levels. Report effect sizes and p-values.