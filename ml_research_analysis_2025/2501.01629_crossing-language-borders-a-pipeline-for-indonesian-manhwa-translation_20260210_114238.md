---
ver: rpa2
title: 'Crossing Language Borders: A Pipeline for Indonesian Manhwa Translation'
arxiv_id: '2501.01629'
source_url: https://arxiv.org/abs/2501.01629
tags:
- translation
- manhwa
- text
- language
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel pipeline for translating Indonesian
  Manhwa (comics) into English by integrating computer vision, optical character recognition
  (OCR), and machine translation techniques. The system uses a fine-tuned YOLOv5xu
  model for speech bubble detection, Tesseract OCR for text extraction, and a fine-tuned
  MarianMT model for translation.
---

# Crossing Language Borders: A Pipeline for Indonesian Manhwa Translation

## Quick Facts
- arXiv ID: 2501.01629
- Source URL: https://arxiv.org/abs/2501.01629
- Reference count: 14
- F1 score of 90.7% for speech bubble detection, BLEU score of 27% for translation

## Executive Summary
This paper presents a novel pipeline for translating Indonesian Manhwa into English by integrating computer vision, optical character recognition (OCR), and machine translation techniques. The system uses a fine-tuned YOLOv5xu model for speech bubble detection, Tesseract OCR for text extraction, and a fine-tuned MarianMT model for translation. By automating the traditionally manual process, the system enhances accessibility and efficiency, addressing challenges posed by low-resource languages like Indonesian. The results demonstrate the potential of domain-specific fine-tuning and multi-modal machine learning for translating image-based content.

## Method Summary
The pipeline processes Manhwa panels through four sequential stages: speech bubble detection using fine-tuned YOLOv5xu, OCR text extraction with Tesseract Indonesian model, machine translation via fine-tuned MarianMT on combined Identic and OpenSubtitles corpora, and translated text overlay using OpenCV/Pillow. The approach fine-tunes pre-trained models on domain-specific data to improve performance on specialized tasks, with the translation model benefiting from both formal and informal parallel corpora to capture conversational language patterns found in Manhwa dialogue.

## Key Results
- Speech bubble detection achieves F1 score of 90.7% and mAP@0.5:0.95 of 88.9%
- OCR achieves character error rate of 3.1% and word error rate of 8.6%
- Translation achieves BLEU score of 27% and Meteor score of 0.61
- Pipeline demonstrates effective automation of Manhwa translation from Indonesian to English

## Why This Works (Mechanism)

### Mechanism 1: Domain-specific fine-tuning
Fine-tuning pre-trained models on domain-specific data (Manhwa speech bubbles, Indonesian-English conversational pairs) reduces domain shift error and improves performance over off-the-shelf alternatives. The mechanism relies on transferable representations learned by pre-trained models being adaptable to specialized patterns with modest domain data.

### Mechanism 2: Modular sequential pipeline
The system decomposes translation into isolated vision, recognition, and translation tasks, allowing independent optimization and error attribution. Each module can be evaluated using task-specific metrics, though this assumes errors don't compound catastrophically across stages.

### Mechanism 3: Combined formal and informal corpora
Training the translation model on both formal (Identic) and informal (OpenSubtitles) parallel corpora exposes it to diverse registers, improving translation quality for conversational Manhwa dialogue by matching the varied language styles found in comics.

## Foundational Learning

- **Object detection evaluation metrics (Precision, Recall, F1, mAP)**: Essential for assessing YOLOv5xu's ability to localize speech bubbles and diagnosing detection failures. Quick check: If a model has high recall but low precision for bubble detection, what type of error dominates (false positives or false negatives)?

- **Character Error Rate (CER) and Word Error Rate (WER) for OCR**: Quantify Tesseract's transcription accuracy at character and word levels, guiding preprocessing and post-processing decisions. Quick check: Why might CER be low (3.1%) while WER is higher (8.6%) in the same OCR output?

- **BLEU and Meteor for machine translation evaluation**: Evaluate MarianMT's translation quality, with BLEU measuring n-gram overlap while Meteor incorporates semantic similarity and synonyms. Quick check: What does a higher Meteor score (0.61) compared to BLEU (0.27) suggest about the translation's characteristics?

## Architecture Onboarding

- **Component map**: Input (Indonesian Manhwa panel) → YOLOv5xu (fine-tuned) → Bounding boxes for speech bubbles → Tesseract OCR (Indonesian model) → Extracted text strings → MarianMT (fine-tuned) → English translation → OpenCV/Pillow overlay → Output (English Manhwa panel)

- **Critical path**: Bubble detection accuracy → OCR text quality → Translation fidelity. Errors in detection or OCR directly constrain translation quality.

- **Design tradeoffs**: Small Manhwa dataset (538 images) enables rapid experimentation but may limit generalization to diverse art styles; Tesseract is fast and open-source but may struggle with stylized fonts compared to learned OCR models; MarianMT is efficient for low-resource languages but may lack contextual nuance.

- **Failure signatures**: Missed small bubbles (YOLOv5xu fails on tiny or irregular speech regions); OCR word-level errors (CER low but WER higher indicates partial word misrecognition); translation awkwardness (moderate BLEU with higher Meteor suggests semantic preservation but less fluent phrasing); overlay misalignment (text may not fit naturally within original bubble shapes).

- **First 3 experiments**:
  1. Establish baseline metrics by running the full pipeline on a held-out test set to confirm reproducibility of reported results.
  2. Test YOLOv5xu on images with varying bubble sizes and styles to identify detection failure modes and visualize errors.
  3. Experiment with additional preprocessing (binarization, denoising, contrast enhancement) beyond grayscale to measure impact on CER and WER for stylized fonts.

## Open Questions the Paper Calls Out

### Open Question 1
How does pipeline performance scale with the availability of a dedicated, large-scale Manhwa dataset compared to the current limited set of 538 images? The authors hypothesize performance could improve with more data, but the current study was constrained by data scarcity.

### Open Question 2
Can targeted post-processing algorithms close the performance gap between the low Character Error Rate (3.1%) and the higher Word Error Rate (8.6%)? The authors successfully utilized Tesseract for character recognition but did not implement logic to correct word-level transcription errors.

### Open Question 3
Does processing entire chapters simultaneously, rather than single panels, improve semantic consistency and translation accuracy? The current pipeline processes images individually, potentially losing narrative context required for accurate translation of ambiguous terms.

## Limitations

- Extremely small training dataset (538 images) for speech bubble detection raises concerns about generalization to diverse Manhwa art styles and layouts
- Text overlay component lacks detailed algorithmic description for handling font selection, text sizing, and alignment within original bubble shapes
- Evaluation metrics reported without confidence intervals or statistical significance testing across multiple runs

## Confidence

**High Confidence**: Modular pipeline architecture is well-established and reported detection metrics are consistent with fine-tuned YOLOv5 performance on similar tasks.

**Medium Confidence**: Translation quality metrics are reasonable for low-resource language pairs, but actual user experience quality cannot be assessed without qualitative examples.

**Low Confidence**: Generalization claims are not supported due to limited dataset size and lack of cross-validation across different Manhwa genres or artists.

## Next Checks

1. **Dataset Diversity Validation**: Test the fine-tuned YOLOv5xu model on Manhwa panels from different artists, genres, and publication years to assess generalization beyond the Webcomics dataset and document failure modes.

2. **Ablation Study on Translation Corpus**: Conduct controlled experiments training MarianMT on only Identic (formal), only OpenSubtitles (informal), and various combinations to quantify the contribution of each corpus type to final BLEU/METEOR scores.

3. **End-to-End Pipeline Robustness**: Run the complete pipeline on a larger, diverse test set of Manhwa panels, measuring error propagation by correlating detection failures with OCR errors and subsequent translation quality degradation.