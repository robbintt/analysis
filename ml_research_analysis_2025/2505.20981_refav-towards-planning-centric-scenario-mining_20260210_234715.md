---
ver: rpa2
title: 'RefAV: Towards Planning-Centric Scenario Mining'
arxiv_id: '2505.20981'
source_url: https://arxiv.org/abs/2505.20981
tags:
- scenario
- objects
- track
- candidates
- dict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of spatio-temporal scenario
  mining in autonomous driving, aiming to identify complex multi-agent interactions
  from uncurated driving logs using natural language descriptions. The authors introduce
  RefAV, a large-scale dataset of 10,000 natural language queries derived from 1000
  Argoverse 2 Sensor logs, and propose RefProg, a program synthesis-based approach
  that combines off-the-shelf 3D tracks with large language models to precisely localize
  referred objects in both time and space.
---

# RefAV: Towards Planning-Centric Scenario Mining

## Quick Facts
- **arXiv ID:** 2505.20981
- **Source URL:** https://arxiv.org/abs/2505.20981
- **Reference count:** 40
- **Primary result:** RefProg achieves HOTA-Temporal of 50.1%, outperforming LLM APIs (37.2%) and ReferGPT (20.7%) on spatio-temporal scenario mining

## Executive Summary
This paper introduces RefAV, a large-scale dataset for spatio-temporal scenario mining in autonomous driving, containing 10,000 natural language queries over 1,000 Argoverse 2 Sensor logs. The authors propose RefProg, a program synthesis approach that combines off-the-shelf 3D object tracks with large language models to precisely localize referred objects in both time and space. Experimental results demonstrate that RefProg significantly outperforms baseline methods, achieving state-of-the-art performance on multiple metrics including HOTA-Temporal, HOTA-Track, and Log Balanced Accuracy.

## Method Summary
RefAV addresses the challenge of spatio-temporal scenario mining by introducing a program synthesis approach that bridges natural language queries with 3D tracking data. The method works by synthesizing Python code from natural language descriptions using an LLM, which is then executed on pre-computed 3D object tracks to filter and localize referred objects. The approach includes post-processing steps such as relation distance thresholding (50m), timestamp dilation (1.5s minimum duration), and downsampling to 2Hz. The method relies on a carefully designed API that provides functions for querying object attributes, spatial relationships, and temporal patterns, enabling compositional reasoning over complex multi-agent interactions.

## Key Results
- RefProg achieves HOTA-Temporal score of 50.1% on the RefAV test set
- Outperforms LLM APIs as a Black Box (37.2% HOTA-Temporal) by 12.9 percentage points
- Significantly outperforms ReferGPT baseline (20.7% HOTA-Temporal) by 29.4 percentage points
- Demonstrates strong performance across multiple metrics: HOTA-Track, Log Balanced Accuracy, and Timestamp Balanced Accuracy

## Why This Works (Mechanism)
The success of RefProg stems from its ability to decompose complex spatio-temporal reasoning into executable program components. By leveraging the compositional power of programming languages, the approach can precisely encode multi-step reasoning chains that involve both spatial relationships (e.g., "car behind bicycle") and temporal patterns (e.g., "accelerating before intersection"). The modular design allows for precise 3D grounding of natural language descriptions, while the post-processing steps help filter out noisy or ambiguous detections.

## Foundational Learning
- **Program Synthesis for Vision Tasks:** Converts natural language to executable code for precise spatial reasoning
  - *Why needed:* Enables compositional reasoning over complex multi-step queries
  - *Quick check:* Verify LLM can generate syntactically correct Python code from simple queries
- **Spatio-Temporal Scenario Mining:** Identifying specific object interactions across time and space from driving logs
  - *Why needed:* Critical for autonomous driving planning and safety analysis
  - *Quick check:* Confirm dataset contains diverse temporal patterns and spatial relationships
- **HOTA Metrics for Multi-Object Tracking:** Hierarchical tracking metrics that evaluate detection and association quality
  - *Why needed:* Provides comprehensive evaluation of spatio-temporal localization accuracy
  - *Quick check:* Validate HOTA calculations match paper's reported values on sample data

## Architecture Onboarding

**Component Map:** Natural Language Query -> LLM Code Synthesis -> Python Script Execution -> Track Filtering -> Referred Object Localization

**Critical Path:** The most performance-critical components are the LLM code generation accuracy and the quality of the underlying 3D tracks. Errors in code generation (e.g., inverted spatial relationships) directly impact localization accuracy, while noisy tracks can lead to incorrect motion classifications.

**Design Tradeoffs:** The modular approach trades computational efficiency for precision and interpretability. While end-to-end vision-language models could be faster, they currently lack the compositional reasoning ability needed for precise spatio-temporal localization.

**Failure Signatures:** Common failure modes include:
- Hallucinated API functions or incorrect imports in generated code
- Inverted spatial relationships (e.g., "behind" vs "in front of")
- Sensitivity to track noise, particularly for short-duration events
- Failure on semantic attributes not covered by the API (weather, lighting)

**First Experiments:**
1. Test code generation with simple spatial queries (e.g., "find the car") to verify basic functionality
2. Evaluate handling of temporal patterns with ground-truth tracks to isolate LLM performance
3. Assess sensitivity to relation distance threshold by varying the 50m parameter

## Open Questions the Paper Calls Out

**Open Question 1:** Can end-to-end vision-language models be effectively adapted to match the performance of modular program synthesis approaches for spatio-temporal scenario mining?
- *Basis:* The paper concludes that "Future work should develop models capable of reasoning over complex, multi-modal temporal data," noting that off-the-shelf VLMs yield poor performance compared to RefProg.
- *Evidence needed:* An end-to-end VLM achieving HOTA-Temporal scores comparable to RefProg (approx. 50%) on the RefAV test set without external program execution.

**Open Question 2:** How can the expressivity of scenario mining systems be extended to handle complex semantic concepts without hand-crafted atomic functions?
- *Basis:* Authors state "this strategy is not scalable" and identify failures on prompts involving attributes like weather or occlusions.
- *Evidence needed:* A method successfully mining scenarios based on semantic attributes not explicitly defined in the initial API, evaluated on RefAV's manually annotated weather and lighting attributes.

**Open Question 3:** To what extent does noise in the underlying 3D object tracks degrade the accuracy of programmatic scenario mining?
- *Basis:* The paper notes that "jittery tracks can lead to poor motion classification over short horizons."
- *Evidence needed:* An ablation study simulating varying levels of noise on ground-truth tracks and measuring the resulting drop in HOTA-Temporal and Log Balanced Accuracy for RefProg.

## Limitations
- Performance heavily dependent on quality of underlying 3D tracks and may degrade with tracking noise
- Limited to scenarios expressible through the provided API; struggles with semantic attributes like weather or lighting
- Requires specific LLM versions (Claude 3.7 Sonnet/GPT-5) that are not publicly available

## Confidence
- **High:** The overall methodological approach and relative performance ranking are well-supported by quantitative results
- **Medium:** Absolute performance numbers are less certain due to unavailability of specific LLM model and limited API detail
- **Low:** Generalizability to other datasets or domains beyond Argoverse 2 Sensor logs is not demonstrated

## Next Checks
1. Verify the provided API in Appendix J is complete and sufficient to generate the Python scripts used in reported experiments
2. Rerun RefProg pipeline with readily available LLM (GPT-4o or Claude 3.5 Sonnet) and quantify performance difference on RefAV subset
3. Systematically vary key post-processing parameters (relation distance threshold, timestamp dilation) and measure impact on final HOTA scores to assess robustness