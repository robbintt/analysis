---
ver: rpa2
title: Exploring Visual Embedding Spaces Induced by Vision Transformers for Online
  Auto Parts Marketplaces
arxiv_id: '2502.05756'
source_url: https://arxiv.org/abs/2502.05756
tags:
- data
- images
- clustering
- image
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Vision Transformer (ViT) for clustering images
  of auto parts from online marketplaces using a single-modality approach. ViT-Base
  was used to extract 768-dimensional embeddings from 85,000 images, which were reduced
  to 64 dimensions using UMAP before applying K-Means clustering (k=20).
---

# Exploring Visual Embedding Spaces Induced by Vision Transformers for Online Auto Parts Marketplaces

## Quick Facts
- arXiv ID: 2502.05756
- Source URL: https://arxiv.org/abs/2502.05756
- Authors: Cameron Armijo; Pablo Rivas
- Reference count: 4
- Key outcome: ViT-Base achieved Calinski-Harabasz Index of 942.4 and Davies-Bouldin Index of 4.164 on 85k auto parts images, with low silhouette score (0.015) indicating overlapping clusters

## Executive Summary
This study evaluates Vision Transformer (ViT) for unsupervised clustering of auto parts images from online marketplaces using a single-modality approach. The authors extract 768-dimensional embeddings from 85,000 images using pre-trained ViT-Base, reduce dimensionality to 64 dimensions via UMAP, and apply K-Means clustering. The approach successfully groups visually similar auto parts but struggles with outliers and ambiguous images, demonstrating ViT's capability for single-modal image clustering while highlighting limitations compared to multimodal approaches.

## Method Summary
The study uses pre-trained ViT-Base to extract 768-dimensional embeddings from auto parts images, then applies UMAP dimensionality reduction to 64 dimensions before K-Means clustering with k=20. The pipeline processes 85,000 images from online marketplaces, with manual inspection using KNN to identify centroid neighbors and validate cluster assignments. The method is designed as a single-modality baseline for comparing against multimodal approaches that incorporate textual context.

## Key Results
- Best clustering performance achieved Calinski-Harabasz Index of 942.4 and Davies-Bouldin Index of 4.164
- Low silhouette score of 0.015 indicated significant cluster overlap and mixed-content outliers
- UMAP 64-dimensional reduction provided optimal trade-off between computational efficiency and cluster purity
- ViT effectively grouped visually similar auto parts but struggled with distinguishing functionally different components that look similar

## Why This Works (Mechanism)

### Mechanism 1: Patch-Based Tokenization Enables Transformer Processing of Images
ViT converts images into sequences of learnable tokens through fixed-size patch division and linear projection, enabling transformer architectures originally designed for text to process visual data. Input images are divided into non-overlapping patches (16×16 pixels), each flattened into a vector and linearly projected into a 768-dimensional embedding space via learned projection matrix E ∈ R(P²·C)×D.

### Mechanism 2: Self-Attention Captures Global Dependencies Across Image Regions
The multi-head self-attention mechanism enables ViT to model long-range dependencies across all image patches simultaneously, unlike CNNs constrained by local receptive fields. Self-attention computes pairwise relationships using query (Q), key (K), and value (V) matrices with softmax normalization: Attention(Q,K,V) = softmax(QK^T/√D)V.

### Mechanism 3: Dimensionality Reduction Preserves Cluster Structure While Enabling Efficient Analysis
UMAP reduces 768-dimensional embeddings to 64 dimensions while preserving both local neighborhood relationships and global cluster structure, enabling effective K-Means clustering. UMAP approximates the manifold underlying the high-dimensional data, projecting embeddings into lower-dimensional space while preserving neighborhood relationships.

## Foundational Learning

### Concept: Transformer Self-Attention and Tokenization
- Why needed here: ViT's fundamental architecture is adapted from NLP transformers; understanding token-based processing of images is essential for debugging embedding quality and identifying failure modes.
- Quick check question: If you double the image resolution from 224×224 to 448×448 with 16×16 patches, how does the attention complexity change?

### Concept: Clustering Validation Metrics
- Why needed here: The paper reports three different clustering metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin) with apparently conflicting signals; understanding what each measures is critical for interpreting results.
- Quick check question: Why can the Calinski-Harabasz Index be high (942.4) while the Silhouette score is near zero (0.015)? What does this tell you about the cluster structure?

### Concept: Single-Modal vs. Multimodal Representation Learning
- Why needed here: The paper explicitly positions itself as a single-modal baseline against prior multimodal work; understanding why text context improves clustering helps identify when ViT-alone is sufficient.
- Quick check question: What types of auto-parts classification errors would you expect from visual-only analysis that text metadata could resolve?

## Architecture Onboarding

### Component Map:
Raw images -> Resize/normalize -> Patch extraction (16×16) -> ViT-Base Encoder -> Class token embedding (768-dim) -> UMAP reduction (64-dim) -> K-Means clustering (k=20) -> Cluster assignments

### Critical Path:
1. Verify pre-trained ViT weights are correctly loaded (ImageNet-21k initialization)
2. Confirm embedding extraction uses class token, not mean pooling
3. UMAP hyperparameters (n_neighbors, min_dist) significantly affect cluster geometry—document defaults
4. K-Means initialization sensitivity: use fixed random seed for reproducibility

### Design Tradeoffs:
- k=20 clusters: Balances granularity vs. cluster coherence; higher k increases overlap risk
- 64-dim UMAP: Best C-H/D-B trade-off per experiments, but 128-dim had slightly better C-H (944.3)
- K-Means vs. alternatives: K-Means assumes spherical clusters; DBSCAN or hierarchical clustering may better handle irregular cluster shapes
- Pre-trained vs. domain fine-tuning: Paper uses ImageNet pre-training; domain-specific pre-training could improve auto-parts discrimination

### Failure Signatures:
- Low silhouette score with high C-H: Indicates clusters are well-separated globally but have significant internal overlap
- Mixed-content outliers: Images containing multiple part types (e.g., engine + exterior) cluster poorly
- Visually similar but functionally different parts: Mirrors vs. body panels with reflective surfaces may miscluster without textual context

### First 3 Experiments:
1. **Baseline reproduction**: Extract ViT-Base embeddings from 5,000 image subset, reduce to 64-dim via UMAP with paper's hyperparameters, run K-Means (k=20), verify cluster assignments align with reported categories
2. **Ablation on embedding dimension**: Compare clustering metrics at 32-dim, 64-dim, and 128-dim UMAP reduction; confirm 64-dim offers optimal trade-off
3. **Outlier analysis**: Identify images with lowest silhouette scores, manually inspect to characterize failure modes—hypothesis: mixed-content and ambiguous-angle images dominate this set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can density-based clustering algorithms like DBSCAN improve cluster validity metrics by handling irregularly shaped clusters better than K-Means?
- Basis in paper: The authors note that "K-Means assumes spherical clusters... Alternative clustering methods, such as DBSCAN... could provide a better fit. Future work could explore these methods"
- Why unresolved: The study relied solely on K-Means, which forced spherical partitions on the data, likely contributing to the low silhouette score (0.015) and the detection of outliers
- What evidence would resolve it: Comparative results showing higher Silhouette or Calinski-Harabasz scores when applying density-based methods to the 64-dimensional UMAP projections

### Open Question 2
- Question: Does domain-specific pre-training of ViT on auto parts imagery significantly enhance clustering performance compared to general-purpose pre-training?
- Basis in paper: The conclusion suggests, "One promising direction is to fine-tune the ViT model with a domain-specific pre-training dataset, allowing it to better capture the nuances of auto part imagery"
- Why unresolved: The current model struggled to distinguish between visually similar but functionally different components, a limitation attributed to the generic nature of the pre-trained features
- What evidence would resolve it: A comparative analysis of embedding quality using a model fine-tuned specifically on auto parts versus the standard ViT-Base

### Open Question 3
- Question: To what extent does the integration of textual embeddings into a hybrid model reduce the high degree of cluster overlap observed in the single-modality approach?
- Basis in paper: The paper proposes "experimenting with hybrid models that incorporate textual embeddings while maintaining a primary focus on visual data"
- Why unresolved: The exclusion of textual data resulted in overlapping clusters (low silhouette score of 0.015) and difficulties in handling ambiguous images containing multiple components
- What evidence would resolve it: Experiments demonstrating that fusing text metadata with image embeddings increases cluster purity and resolves the "outlier" issues caused by mixed-content images

## Limitations
- Dataset accessibility: The 85,000-image dataset is proprietary and not publicly available, preventing independent verification
- Single-modal baseline without ablation: Lacks comparison of different ViT variants, dimensionality reduction methods, or clustering algorithms beyond K-Means
- Metric interpretation challenges: High Calinski-Harabasz with near-zero Silhouette creates ambiguity about actual cluster quality

## Confidence
- **High Confidence**: Core architectural claims about ViT's patch-based tokenization and self-attention mechanisms
- **Medium Confidence**: Clustering results methodology, though exact reproducibility limited by undisclosed hyperparameters
- **Low Confidence**: Comparative claims about superiority/inferiority to multimodal approaches without direct comparisons

## Next Checks
1. **Synthetic Data Validation**: Generate synthetic auto parts images with controlled variations and apply the exact pipeline to verify whether reported clustering behavior is reproducible and expected
2. **Multimodal Integration Test**: Implement minimal multimodal baseline using same visual embeddings plus simple metadata to quantify performance gap
3. **Outlier Characterization**: Systematically analyze lowest-silhouette images to determine if they cluster around specific failure modes and suggest architectural improvements