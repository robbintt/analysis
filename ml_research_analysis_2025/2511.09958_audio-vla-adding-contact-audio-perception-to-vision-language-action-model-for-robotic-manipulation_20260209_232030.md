---
ver: rpa2
title: 'Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model
  for Robotic Manipulation'
arxiv_id: '2511.09958'
source_url: https://arxiv.org/abs/2511.09958
tags:
- audio
- contact
- manipulation
- audio-vla
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of vision-only Vision-Language-Action
  (VLA) models in perceiving contact events and dynamic processes during robotic manipulation.
  The authors propose Audio-VLA, a multimodal manipulation policy that integrates
  contact audio perception using pre-trained encoders (DINOv2, SigLIP, AudioCLIP)
  and a Llama2 backbone with LoRA fine-tuning.
---

# Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation

## Quick Facts
- arXiv ID: 2511.09958
- Source URL: https://arxiv.org/abs/2511.09958
- Reference count: 40
- Audio-VLA integrates contact audio perception with VLA models, achieving significant performance improvements in robotic manipulation tasks

## Executive Summary
Audio-VLA addresses the fundamental limitation of vision-only Vision-Language-Action (VLA) models in perceiving contact events and dynamic processes during robotic manipulation. The model integrates contact audio perception using pre-trained encoders and a Llama2 backbone with LoRA fine-tuning, leveraging contact microphones to capture high-frequency acoustic signals during object interactions. Extensive experiments demonstrate Audio-VLA's superiority over vision-only baselines, achieving 97.6% success rate on LIBERO and 55.1% on RLBench in standard environments, with particularly strong performance under domain shift conditions.

## Method Summary
Audio-VLA is a multimodal manipulation policy that combines vision, contact audio, and proprioception inputs through pre-trained encoders (DINOv2, SigLIP, AudioCLIP) and a Llama2 backbone with LoRA fine-tuning. The model processes audio signals independently at each timestep using modified FBSP parameters for high temporal resolution, enabling precise contact event detection. A unified multimodal projector maps heterogeneous features into the LLM embedding space, while the action head generates continuous actions through parallel decoding of K timesteps. The architecture is trained end-to-end with LoRA adapters to efficiently extract manipulation-relevant acoustic features from pre-trained models.

## Key Results
- Audio-VLA achieves 97.6% success rate on LIBERO and 55.1% on RLBench in standard environments
- Under domain shift conditions, Audio-VLA maintains over 66% performance retention compared to vision-only baselines
- Real-world experiments show at least three-fold improvements in success rates and Task Completion Rate metrics compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Contact audio provides domain-invariant physical interaction signals that compensate for vision failures during occlusion and environmental variation. Audio signals penetrate occlusions and capture contact event frequencies at ~1000× higher sampling than tactile sensors, enabling maintained policy performance when visual features degrade under domain shift.

### Mechanism 2
Independent timestep-level audio processing with modified FBSP parameters enables precise temporal alignment for contact event detection. By reducing hop length and window length in the Frequency B-Spline Projection layer, the model achieves finer temporal resolution while preserving frequency information for instantaneous contact detection.

### Mechanism 3
LoRA fine-tuning of the audio encoder is necessary for extracting manipulation-relevant acoustic features from pre-trained AudioCLIP. Pre-trained AudioCLIP learns general audio-visual representations but lacks specialization for robotic contact sounds, requiring efficient domain transfer through LoRA adaptation.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Used to fine-tune all pre-trained encoders and the LLM backbone efficiently without full parameter updates
  - Quick check question: Can you explain why LoRA reduces memory costs compared to full fine-tuning, and what the "rank" parameter controls?

- **Concept: Multimodal Feature Alignment via Projection Layers**
  - Why needed here: Audio-VLA maps heterogeneous modality features into a unified LLM embedding space through separate projection functions
  - Quick check question: Why use different projection architectures (linear vs. MLP) for different modalities?

- **Concept: Spectrogram-based Audio Representation**
  - Why needed here: Audio is converted to complex-valued spectrograms via FBSP, then to power spectra for feature extraction
  - Quick check question: What does the hop length control in spectrogram computation, and why might a shorter hop length improve contact event detection?

## Architecture Onboarding

- **Component map:** Raw images/audio/proprio → Encoders (with LoRA) → Projectors → Sequence concatenation → Llama2 backbone → Action hidden states → Action head → Continuous actions (parallel decoding of K timesteps)

- **Critical path:** Multi-modal Encoder: DINOv2 + SigLIP (vision), AudioCLIP with ResNeXt (audio), MLP (proprioception) → Multi-modal Projector: Linear (vision), 3-layer MLP (audio), 2-layer MLP (proprio) → Language Module: Llama2 7B with LoRA fine-tuning → Action Head: 4-layer MLP generating K×D continuous action predictions

- **Design tradeoffs:** FBSP window/hop (1024/256): Smaller values improve temporal precision but may reduce frequency resolution; LoRA rank (32): Higher rank captures more complex adaptations but increases parameter count; Audio timestep processing: Independent per-timestep enables alignment but may lose longer temporal context; Action chunk size K: Larger K enables action smoothing but increases latency

- **Failure signatures:** Vision-only baselines fail at contact-dependent transitions; without LoRA, audio encoder cannot extract manipulation-relevant features (~50% performance drop); under domain shift without audio, vision features degrade with near-zero performance

- **First 3 experiments:** 1) Baseline comparison on LIBERO/RLBench: Train Audio-VLA vs. OpenVLA-OFT and π0-FAST with identical data; 2) Ablation: Audio modality removal: Train vision-only variant; 3) Ablation: LoRA necessity: Train without LoRA on audio encoder

## Open Questions the Paper Calls Out

## Limitations
- Dataset scope may not capture full complexity of real-world contact-rich manipulation
- Generalization uncertainty regarding material property changes and environmental noise conditions
- Real-world evaluation scale limited to two tasks, potentially not representative of broader manipulation skill space

## Confidence

- **High confidence:** Audio-VLA's architecture and training methodology are technically sound with verifiable performance improvements
- **Medium confidence:** Domain-invariance claims for contact audio are plausible but require more extensive validation
- **Medium confidence:** Task Completion Rate metric is reasonable but needs additional validation for practical task success correlation

## Next Checks
1. **Material property generalization test:** Evaluate Audio-VLA on tasks with varying material properties to validate robustness of contact audio signatures across material variations
2. **Noise robustness evaluation:** Systematically test performance degradation under controlled acoustic noise conditions to establish practical limits
3. **Long-horizon manipulation benchmark:** Test Audio-VLA on complex, multi-stage manipulation tasks requiring sustained contact awareness to validate effectiveness beyond single-contact scenarios