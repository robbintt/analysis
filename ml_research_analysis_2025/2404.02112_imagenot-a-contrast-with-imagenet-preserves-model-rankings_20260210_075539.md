---
ver: rpa2
title: 'ImageNot: A contrast with ImageNet preserves model rankings'
arxiv_id: '2404.02112'
source_url: https://arxiv.org/abs/2404.02112
tags:
- imagenet
- imagenot
- learning
- image
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ImageNot, a large-scale dataset designed
  to be drastically different from ImageNet while maintaining the same scale of 1000
  classes with 1000 examples each. Unlike ImageNet, ImageNot is created without human
  annotation, using web-crawled images and automated labeling.
---

# ImageNot: A contrast with ImageNet preserves model rankings

## Quick Facts
- arXiv ID: 2404.02112
- Source URL: https://arxiv.org/abs/2404.02112
- Reference count: 25
- Primary result: Model rankings and relative improvements over AlexNet are preserved when training on a drastically different dataset (ImageNot) compared to ImageNet

## Executive Summary
This paper introduces ImageNot, a large-scale image classification dataset constructed to be drastically different from ImageNet while maintaining the same scale of 1000 classes with 1000 examples each. The dataset is created without human annotation using web-crawled images from LAION-5B and automated labeling through CLIP similarity. The study trains nine key model architectures from scratch on ImageNot and finds that the relative performance improvements over AlexNet are remarkably well-preserved compared to ImageNet results, with high Pearson correlation. This finding suggests that relative model progress is robust across significantly different datasets, highlighting the importance of model rankings over absolute accuracy numbers in assessing benchmark effectiveness.

## Method Summary
The study constructs ImageNot by filtering LAION-5B metadata to select 1000 noun synsets, removing ImageNet and NSFW overlaps, filtering captions using RoBERTa similarity threshold of 0.82, and ranking classes by CLIP image-caption similarity. Nine image classification architectures (AlexNet, VGG-19, Inception V3, ResNet-152, DenseNet-161, MobileNet V3, EfficientNet V2, ViT-B-16, and ConvNeXt) are trained from scratch on this dataset using standard configurations with early stopping. The evaluation compares test accuracy and relative improvements over AlexNet between ImageNot and ImageNet, computing Pearson correlation and ordinal ranking preservation.

## Key Results
- Model rankings on ImageNot closely match those on ImageNet
- Relative improvements over AlexNet show high correlation (slope and R value) between datasets
- The preservation of relative performance suggests benchmark effectiveness can be measured by ranking stability rather than absolute accuracy

## Why This Works (Mechanism)
The mechanism behind this result appears to be that model architecture characteristics (capacity, inductive biases, optimization properties) that determine relative performance on one large-scale dataset generalize to other large-scale datasets, even when the specific visual content differs dramatically. The automated dataset construction using CLIP similarity ensures ImageNot maintains the same semantic diversity as ImageNet while having completely different image instances.

## Foundational Learning
- **LAION-5B dataset**: Large-scale web image-text pair collection; needed for source material; quick check: verify dataset contains 5B pairs
- **CLIP model**: Contrastive image-text embedding model; needed for automated class selection and filtering; quick check: confirm CLIP ViT-L/14 performance on ImageNet
- **RoBERTa similarity**: Text embedding model for caption-gloss matching; needed to filter relevant captions; quick check: verify RoBERTa-large performance on GLUE benchmark
- **WordNet synsets**: Lexical database for noun categorization; needed for systematic class selection; quick check: confirm WordNet 3.1 contains target noun synsets
- **Early stopping with patience**: Training regularization technique; needed to prevent overfitting; quick check: monitor validation loss for convergence patterns

## Architecture Onboarding

**Component Map:**
LAION-5B metadata -> WordNet synset selection -> RoBERTa caption filtering -> CLIP class ranking -> ImageNot dataset -> Model training -> Performance evaluation

**Critical Path:**
The most critical components are the automated dataset construction pipeline (RoBERTa filtering and CLIP ranking) and the consistent hyperparameter configuration across models, as these directly determine whether the ImageNot dataset is truly different from ImageNet and whether the relative performance measurements are comparable.

**Design Tradeoffs:**
The authors chose automated dataset construction over human annotation to ensure ImageNot is truly different from ImageNet, accepting potential label noise. They maintained strict hyperparameter tuning budgets to avoid biasing results toward ImageNet performance. The tradeoff is between dataset quality (potential noise) and dataset independence (true contrast with ImageNet).

**Failure Signatures:**
If models fail to train or show drastically different rankings on ImageNot, this could indicate either: (1) the dataset construction pipeline failed to create a coherent classification task, or (2) the relative improvements are not as robust as claimed. Monitoring validation loss and correlation coefficients helps diagnose these issues.

**First Experiments:**
1. Reconstruct ImageNot dataset using LAION-5B metadata and verify it has 1000 classes with 1000 train images each
2. Train AlexNet on reconstructed ImageNot and confirm basic convergence
3. Compute RoBERTa caption-gloss similarities for a sample of synset-caption pairs to verify the 0.82 threshold behavior

## Open Questions the Paper Calls Out
None

## Limitations
- The study only tests a fixed set of 9 architectures and may not generalize to all model families
- The exact implementation details of the WordNet subtree selection and overlap removal are not fully specified
- The random seed for test set sampling is not reported, which could affect correlation measurements

## Confidence

**High Confidence:**
- The methodological approach of constructing a non-overlapping dataset is sound
- The general observation that relative model rankings are preserved is well-supported

**Medium Confidence:**
- Specific numerical values of Pearson correlation and relative improvements depend on precise implementation details
- Results are based on a single dataset construction and may have variability

**Low Confidence:**
- Broader implications for universal benchmark design are not directly validated
- Claims about robustness across all possible dataset shifts remain untested

## Next Checks

1. **Dataset Construction Fidelity:** Re-implement the ImageNot dataset construction pipeline and verify the resulting dataset has the claimed properties (1000 classes, 1000 train images/class, 100 test images/class, no ImageNet overlap).

2. **Hyperparameter Sensitivity:** Train the nine models on the reconstructed ImageNot dataset using specified hyperparameters and confirm that relative improvements and model rankings match the paper's results.

3. **Correlation Robustness:** Perform an ablation study by varying the RoBERTa similarity threshold (e.g., 0.80, 0.84) and random seed for test set sampling to assess stability of Pearson correlation and ordinal ranking preservation.