---
ver: rpa2
title: 'The Effect of Explainable AI-based Decision Support on Human Task Performance:
  A Meta-Analysis'
arxiv_id: '2504.13858'
source_url: https://arxiv.org/abs/2504.13858
tags:
- performance
- support
- decision
- human
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This meta-analysis examined the impact of explainable AI (XAI)
  on human performance in binary classification tasks across 16 studies from 14 papers.
  The analysis found that XAI-based decision support improves task performance compared
  to no support (SMD = 0.48, p = 0.001), but explanations do not significantly enhance
  performance beyond AI-only decision support (SMD = 0.09, p = 0.074).
---

# The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis

## Quick Facts
- arXiv ID: 2504.13858
- Source URL: https://arxiv.org/abs/2504.13858
- Reference count: 32
- Primary result: XAI-based decision support improves task performance compared to no support (SMD = 0.48, p = 0.001), but explanations do not significantly enhance performance beyond AI-only decision support (SMD = 0.09, p = 0.074)

## Executive Summary
This meta-analysis examined the impact of explainable AI (XAI) on human performance in binary classification tasks across 16 studies from 14 papers. The analysis found that XAI-based decision support improves task performance compared to no support (SMD = 0.48, p = 0.001), but explanations do not significantly enhance performance beyond AI-only decision support (SMD = 0.09, p = 0.074). The studies' risk of bias was identified as a significant moderator, with high-risk studies showing artificially inflated effects. Analysis of explanation types (feature attribution, counterfactual, and example-based) revealed no significant differences in their impact on performance. The findings suggest that while XAI improves task performance, explanations themselves are not the decisive factor, and the development of effective XAI-based decision support systems requires broader considerations beyond explanation type.

## Method Summary
The study conducted a systematic literature search across three databases, screening 420 initial papers down to 16 studies from 14 papers that met inclusion criteria. Task performance was measured as the ratio of correct binary decisions to all decisions. Standardized Mean Difference (SMD) with Hedges' g correction was used as the primary metric, with random-effects regression models and RoB 2 tool for risk of bias assessment. Subgroup analyses examined explanation types (feature attribution, counterfactual, example-based) and risk of bias levels (low, some concerns, high).

## Key Results
- XAI-based decision support improves task performance compared to no support (SMD = 0.48, p = 0.001)
- Explanations do not significantly enhance performance beyond AI-only decision support (SMD = 0.09, p = 0.074)
- Risk of bias was a significant moderator, with high-risk studies showing artificially inflated effects
- No significant differences found between explanation types (feature attribution, counterfactual, example-based)

## Why This Works (Mechanism)

### Mechanism 1: AI Prediction as Primary Performance Driver
The AI prediction itself, not the accompanying explanation, primarily drives human task performance improvements in binary classification. AI-based decision support provides probabilistic or categorical guidance that reduces human decision uncertainty and anchors judgment toward model-informed outcomes. Explanations add information but do not significantly shift performance beyond this anchoring effect. The underlying AI model achieves accuracy above human baseline; otherwise, support may degrade performance.

### Mechanism 2: Study Quality Moderation via Risk of Bias
High risk of bias in experimental design inflates observed XAI effectiveness. Studies with methodological flaws (e.g., inadequate randomization, selective reporting, lack of pre-registered analysis plans) produce artificially elevated effect sizes. This creates a false impression that explanations drive performance gains. The Cochrane RoB 2 assessment accurately captures the methodological weaknesses that bias effect estimates.

### Mechanism 3: Explanation Type Independence from Task Performance
The specific explanation type (feature attribution, counterfactual, or example-based) does not significantly moderate human task performance. All three explanation types provide roughly equivalent informational value for binary classification tasks, or user cognitive processing limits mean that additional explanation structure does not translate to better decisions. Effective support may require more nuanced tailoring beyond type categorization.

## Foundational Learning

- Concept: Standardized Mean Difference (SMD) and effect size interpretation
  - Why needed here: The paper's claims rest on comparing SMD values (0.48 vs. 0.09); without understanding that 0.2 ≈ small, 0.5 ≈ medium, 0.8 ≈ large, you cannot evaluate practical significance.
  - Quick check question: If a new XAI intervention reports SMD = 0.15 with p = 0.04, is this practically meaningful?

- Concept: Risk of Bias (RoB 2) framework components
  - Why needed here: The meta-analysis shows that bias risk moderates effects; knowing what constitutes "high risk" helps you design better experiments and critically evaluate claims.
  - Quick check question: Name two experimental design choices that would increase a study's risk of bias rating.

- Concept: Explanation type taxonomy (Why/Why-not/How/How-to/What-else)
  - Why needed here: The paper tests FA (Why/How), CF (How-to), and EB (What-else) types; mapping these to user questions helps you select appropriate methods for specific decision contexts.
  - Quick check question: Which explanation type would best support a user asking "What would need to change for the AI to recommend approval instead of denial?"

## Architecture Onboarding

- Component map: Prediction Layer -> Explanation Module -> Presentation Layer -> Evaluation Layer
- Critical path:
  1. Validate that the AI prediction model exceeds human baseline accuracy on the target task
  2. Ensure explanation fidelity—generated explanations accurately reflect model reasoning
  3. Design presentation that does not induce cognitive overload (explanations should not confuse)
  4. Measure task performance with appropriate control conditions (no support, AI-only, XAI)

- Design tradeoffs:
  - AI-only vs. XAI: Adding explanations increases system complexity without proven performance gains; may be justified for trust calibration, debugging, or compliance rather than accuracy
  - Explanation type selection: No clear winner; choose based on task domain, user expertise, and interpretability requirements rather than expected performance lift
  - Experimental rigor vs. resource constraints: Low-bias designs (pre-registration, proper controls) require more planning but yield trustworthy estimates

- Failure signatures:
  - AI predictions below human baseline → support degrades performance
  - Explanations that contradict model behavior or are unfaithful → user confusion, inappropriate trust
  - Cognitive overload from explanation complexity → slower decisions without accuracy gain
  - High-bias evaluation (e.g., within-subject designs without washout, selective reporting) → inflated effect claims

- First 3 experiments:
  1. Establish baseline: Compare AI-only decision support vs. no support on your task to confirm predictions are helpful (SMD > 0 expected).
  2. Incremental explanation test: Add one explanation type (e.g., feature attribution) to AI support; measure whether performance improves beyond AI-only. Pre-register hypothesis and analysis.
  3. Bias sensitivity check: Run a high-quality replication of an existing study with proper randomization and pre-registration; compare your effect size to the original.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does task complexity moderate the effect of XAI-based decision support on human task performance? The authors state that future research "might also reveal other moderators (e.g., task complexity) for task performance, promoting a more nuanced understanding of human-XAI collaboration." This remains unresolved because the current meta-analysis was restricted to binary classification tasks and did not incorporate task complexity as a variable in its subgroup analysis.

### Open Question 2
How does user AI literacy influence the effectiveness of explanations in AI-assisted decision-making? The authors mention they "planned to include further subgroup analyses (e.g., to analyze the moderating effect of AI literacy), but were unable to do so due to the small number of studies that report respective measures." This remains unresolved because the existing body of literature lacks sufficient reporting on user demographics and AI expertise, preventing a statistical synthesis of this factor.

### Open Question 3
Do the effects of XAI-based decision support generalize to tasks with continuous performance outcomes? The authors identify a limitation regarding "generalizability... beyond task performance in binary classification tasks" and suggest research should address this by "analyzing studies that measure performance as a continuous outcome." This remains unresolved because the authors excluded studies without binary decision metrics to ensure statistical compatibility, leaving non-classification tasks unexplored.

## Limitations
- Reliance on aggregate effect sizes from 16 studies with heterogeneous tasks and populations
- Lack of significant difference between explanation types could reflect true equivalence or insufficient statistical power
- RoB 2 assessment's handling of "no analysis plan" as low risk introduces potential bias in moderation analysis
- Weak corpus of related work for validation of risk-of-bias findings

## Confidence
- **High confidence**: XAI improves task performance compared to no support (SMD = 0.48, p = 0.001)
- **Medium confidence**: Explanations do not significantly enhance performance beyond AI-only support (SMD = 0.09, p = 0.074)
- **Low confidence**: No significant differences between explanation types (all p > 0.05) due to limited statistical power

## Next Checks
1. Replication with pre-registered protocol: Replicate one high-bias study with proper randomization, pre-registration, and transparent reporting to test whether effect size inflation disappears.

2. Power analysis for explanation type differences: Conduct a formal power analysis to determine minimum detectable effect sizes for comparing FA, CF, and EB explanations, then design a study with adequate sample size.

3. Task-context sensitivity test: Select three binary classification tasks with distinct characteristics (e.g., medical diagnosis, fraud detection, content moderation) and test whether specific explanation types perform better for particular task domains.