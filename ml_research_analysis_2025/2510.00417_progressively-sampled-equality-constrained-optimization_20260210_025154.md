---
ver: rpa2
title: Progressively Sampled Equality-Constrained Optimization
arxiv_id: '2510.00417'
source_url: https://arxiv.org/abs/2510.00417
tags:
- algorithm
- which
- constraint
- sample
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses equality-constrained optimization problems
  where constraints are defined by expectations or averages over many terms. The proposed
  method solves a sequence of progressively larger sampled constraint problems, starting
  from a small sample set and expanding it iteratively.
---

# Progressively Sampled Equality-Constrained Optimization

## Quick Facts
- **arXiv ID**: 2510.00417
- **Source URL**: https://arxiv.org/abs/2510.00417
- **Reference count**: 11
- **Primary result**: Progressive sampling achieves better worst-case sample complexity than solving full-sample problem directly

## Executive Summary
This paper addresses equality-constrained optimization problems where constraints are defined by expectations or averages over many terms. The proposed method solves a sequence of progressively larger sampled constraint problems, starting from a small sample set and expanding it iteratively. Under assumptions about constraint functions and their derivatives, the authors prove that this progressive sampling approach yields better worst-case sample complexity bounds compared to solving the full-sample problem directly. Specifically, with a sufficiently large initial sample, the method can achieve approximate second-order stationarity with fewer total constraint gradient evaluations. The analysis relies on properties of least-squares multipliers and acute perturbations between constraint Jacobians.

## Method Summary
The algorithm solves equality-constrained optimization problems where constraints are expectations by progressively increasing sample sizes. It starts with an initial sample set $S_1$ of size $p_1$ (much smaller than total samples $N$), solves the sampled problem to approximate second-order stationarity, then doubles the sample size and uses the previous solution as a warm start. This continues until the full sample set is used. The method relies on the property that for large enough samples, the sampled constraint Jacobian is an acute perturbation of the full Jacobian, preserving constraint geometry and allowing warm starts to be effective. The analysis shows this approach reduces total gradient evaluations from $O(N\epsilon^{-2})$ to $O(|S_1|\epsilon_1^{-2} + \sum_{k=2}^K |S_k|\log(\epsilon_{k-1}/\epsilon_k))$.

## Key Results
- Progressive sampling achieves approximately 33% fewer individual constraint gradient evaluations compared to solving the full-sample problem directly while achieving comparable solution accuracy
- With sufficiently large initial sample, the method achieves improved worst-case sample complexity bounds: $O(|S_1|\max\{\epsilon_1^{-2}, \zeta_1^{-3}\} + \sum_{i=2}^K |S_k|\log(\epsilon_{k-1}/\epsilon_k))$ versus $O(N\max\{\epsilon_1^{-2}, \zeta_1^{-3}\})$ for one-shot approach
- Strong Morse property and acute perturbation conditions are preserved across sample sizes when initial sample is sufficiently large, enabling warm-start benefits
- Numerical experiments on both artificial problems and physics-informed neural network training demonstrate computational savings

## Why This Works (Mechanism)

### Mechanism 1: Warm-Start Transfer Through Progressive Sampling
- Claim: An approximate stationary point of a small-sample problem provides a computationally advantageous starting point for a larger-sample problem when samples are sufficiently large.
- Mechanism: The algorithm solves subproblems with increasing sample sizes $|S_k|$, where each $S_k \supseteq S_{k-1}$. When the full-sample problem is strongly Morse and $|S|$ is large enough, the sampled constraint Jacobian $\nabla c_S(x)$ is an acute perturbation of $\nabla c(x)$ (Lemma 3.3), ensuring stationary points of subproblems are close to those of the full problem. This proximity reduces the iterations needed for subsequent subproblems to $O(\log(\epsilon_{k-1}/\epsilon_k))$ rather than $O(\epsilon_k^{-2})$.
- Core assumption: The initial sample size $p_1$ must satisfy condition (24): $\xi_S \leq \min\{1/(3\kappa_1), \alpha/(2\kappa_2), 7\beta/(18\kappa_3)\}$ where $\xi_S = \sqrt{N(N-|S|)}/|S|$, ensuring subproblems remain $(\alpha_S, \beta_S)$-strongly Morse with $\alpha_S \geq \alpha/2$, $\beta_S \geq \beta/2$.
- Evidence anchors:
  - [abstract]: "solving a sequence of problems defined through progressive sampling yields a better worst-case sample complexity bound compared to solving a single problem with a full set of samples"
  - [section]: Theorem 3.1 proves total gradient evaluations scale as $O(|S_1|\max\{\epsilon_1^{-2}, \zeta_1^{-3}\} + \sum_{i=2}^K |S_k|\log(\epsilon_{k-1}/\epsilon_k))$ versus $O(N\max\{\epsilon_1^{-2}, \zeta_1^{-3}\})$ for the one-shot approach
  - [corpus]: No directly comparable corpus evidence on progressive sampling for equality-constrained optimization; related distributionally robust optimization papers (arXiv:2503.22923, arXiv:2503.20341) use nested sampling but for different objective classes
- Break condition: If $p_1$ is too small to satisfy (24), the subproblem may not be strongly Morse, and the warm-start benefit degrades; Figure 1 contour plots show this empirically—smaller samples exhibit weaker strong Morse properties

### Mechanism 2: Acute Perturbation Preservation of Constraint Geometry
- Claim: For sufficiently large sample sizes, sampled and full-sample constraint Jacobians are acute perturbations of each other, preserving rank and null-space structure critical for constrained optimization.
- Mechanism: Two matrices $A, B \in \mathbb{R}^{n \times m}$ are acute perturbations when $\text{rank}(AA^\dagger BA^\dagger A) = \text{rank}(A) = \text{rank}(B)$. Lemma 3.3 proves this holds when $|S| > 2N/(1 + \sqrt{1 + 4\sigma_{\min}^2/(\gamma_{\nabla c}\kappa_{\nabla c}^2)})$. This ensures the optimization direction remains well-defined across sample sizes.
- Core assumption: The constraint Jacobian maintains full column rank with $\sigma_m(\nabla c_S(x)^T) \geq \sigma_{\min} > 0$ for all $x$ and $|S| \geq p_1$ (Assumption 3.1(a))
- Evidence anchors:
  - [abstract]: "The analysis relies on properties of least-squares multipliers and acute perturbations between constraint Jacobians"
  - [section]: Definition 3.1 formally defines acute perturbations; Lemma 3.3 proves the condition; equation (13) shows $\|\nabla c(x)^\dagger(\nabla c_S(x) - \nabla c(x))\| < 1$ is the key inequality
  - [corpus]: Weak corpus connection—related papers discuss perturbation bounds in optimization contexts but not acute perturbations specifically for constraint Jacobians
- Break condition: If samples have high variance such that $\|\nabla c_S(x) - \nabla c(x)\|$ approaches $\sigma_{\min}/(\sqrt{\gamma_{\nabla c}}\kappa_{\nabla c})$, the acute perturbation condition fails; constraint qualifications may be violated at sampled points

### Mechanism 3: Strong Morse Property Inheritance
- Claim: If the full-sample problem (2) is $(\alpha, \beta)$-strongly Morse, sampled subproblems (3) inherit $(\alpha_S, \beta_S)$-strong Morse properties with slightly weakened constants.
- Mechanism: Strong Morse means: if $\|\nabla L(x, y(x))\| \leq \alpha$, then the reduced Hessian satisfies $|d^T \nabla^2_{xx} L(x,y(x))d| \geq \beta\|d\|^2$ for all $d \in \text{null}(\nabla c(x)^T)$. Lemma 3.6 shows this transfers to sampled problems because: (i) gradient perturbations are bounded by $\xi_S \kappa_2$, and (ii) Hessian perturbations in the null space are controlled by $\kappa_3 \xi_S$. When $\xi_S$ is small enough, $\alpha_S \geq \alpha/2$ and $\beta_S \geq \beta/2$.
- Core assumption: The full problem is $(\alpha, \beta)$-strongly Morse (Assumption 3.1(c))—this is a non-trivial structural assumption about the optimization landscape
- Evidence anchors:
  - [section]: Lemma 3.6 provides the complete proof; equations (33) and (40) show how the reduced Hessian bound degrades controllably; Figure 1 empirically validates that when $\|\nabla L_S\| \leq 0.6$, the reduced Hessian eigenvalue $\lambda_S \geq 0.8$ for both $|S|=64$ and $|S|=2048$
  - [corpus]: No corpus papers discuss strong Morse properties in constrained optimization
- Break condition: Highly variable constraint functions (large $\gamma_{\nabla c}$, $\gamma_{\nabla^2 c}$) increase $\kappa_1$, $\kappa_2$, $\kappa_3$, requiring larger $|S|$ to maintain the bounds; in extreme cases, no practical $p_1$ may satisfy (24)

## Foundational Learning

### Concept: Second-Order Stationarity for Equality-Constrained Optimization
- Why needed here: The algorithm targets $(\epsilon, \zeta)$-stationary points (equation 6) satisfying both $\|\nabla L_S(x, y_S(x))\| \leq \epsilon$ and $d^T \nabla^2_{xx} L_S(x, y_S(x))d \geq -\zeta\|d\|^2$ for all $d \in \text{null}(\nabla c_S(x)^T)$. This second-order condition is essential for the complexity guarantees—Remark 3.1 notes that first-order stationarity alone would not yield the improved sample complexity bound.
- Quick check question: Given a point $x^*$ where $\nabla c(x^*)$ has full row rank and $\nabla_x L(x^*, y(x^*)) = 0$, explain why checking only $\nabla^2 f(x^*) \succeq 0$ is insufficient for optimality. What subspace must the Hessian be positive semidefinite on?

### Concept: Least-Squares Lagrange Multipliers
- Why needed here: The algorithm uses $y_S(x) = -(\nabla c_S(x)^T \nabla c_S(x))^{-1}\nabla c_S(x)^T \nabla f(x) = -\nabla c_S(x)^\dagger \nabla f(x)$ (equation 7) to define multipliers at any point $x$, not just stationary points. This enables the termination criterion (6) to be evaluated without solving a separate dual problem. Lemma 3.4 bounds $\|y(x) - y_S(x)\|$ in terms of $\xi_S$.
- Quick check question: For a point $x$ with $c_S(x) = 0$ and $\nabla c_S(x)$ full rank, if $\nabla f(x)$ lies entirely in the range of $\nabla c_S(x)$, what is $y_S(x)$? If $\nabla f(x)$ has a component orthogonal to this range, what does that imply about $x$?

### Concept: Sample Average Approximation (SAA) Consistency
- Why needed here: The paper addresses constrained problems where $c(x) = \mathbb{E}[C(x, \omega)]$. Understanding why $c(x) \approx \frac{1}{N}\sum_{i=1}^N c_i(x)$ converges and how the error propagates through derivatives is foundational to Assumptions 3.2–3.3.
- Quick check question: If each $c_i(x)$ is an i.i.d. unbiased estimator of $c(x)$ with variance $\sigma^2$, how does the error $\|\bar{c}_N(x) - c(x)\|$ scale with $N$? What additional assumptions are needed for similar bounds on $\|\nabla \bar{c}_N(x) - \nabla c(x)\|$?

## Architecture Onboarding

### Component map:
1. **Outer Loop (Algorithm 1, lines 2–6)**: Manages progressive sample expansion $|S_k| = \min\{2|S_{k-1}|, N\}$, sets per-iteration tolerances $(\epsilon_k, \zeta_k)$
2. **Sample Set Selector**: Chooses $S_k \supseteq S_{k-1}$ with $|S_k| = p_k$; paper uses random supersets but deterministic inclusion would also work
3. **Subproblem Solver**: Two validated options—[4, Algorithm 1] (Fletcher's augmented Lagrangian with second-order guarantees) or [1, Algorithm 2.2] (SQP method); Lemma 3.8–3.10 analyze complexity
4. **Multiplier Oracle**: Computes $y_{S_k}(x) = -\nabla c_{S_k}(x)^\dagger \nabla f(x)$ via equation (7) for termination checks
5. **Stationarity Verifier**: Tests conditions (6a) and (6b)—requires computing $\nabla^2_{xx} L_{S_k}$ projected onto $\text{null}(\nabla c_{S_k}(x)^T)$

### Critical path:
$x_0$ (arbitrary) $\to$ initialize $S_1$ with $|S_1| = p_1$ $\to$ run subproblem solver to $(\epsilon_1, \zeta_1)$-stationarity $\to$ obtain $x_1$ $\to$ double sample size $\to$ warm-start solver from $x_{k-1}$ $\to$ repeat until $|S_K| = N$

### Design tradeoffs:
- **Initial sample size $p_1$**: Larger $p_1$ reduces outer iterations but increases per-subproblem cost; smaller $p_1$ maximizes warm-start benefit but risks violating (24)—Section 4.2 experiments suggest $p_1 \approx N/4$ works well empirically
- **Tolerance schedule $(\epsilon_k, \zeta_k)$**: Equation (42) proposes $\epsilon_k = \tau_1 \xi_{S_k}$, $\zeta_k = \tau_2 \xi_{S_k}$ where $\xi_{S_k} = \sqrt{N(N-|S_k|)}/|S_k|$; tighter early tolerances increase cost but may improve final accuracy
- **Subproblem solver choice**: Fletcher's AL ([4, Algorithm 1]) provides proven second-order convergence (Lemma 3.8) but requires Hessian computations; SQP ([1, Algorithm 2.2]) is first-order and faster per-iteration but lacks second-order guarantees—Figure 3 shows it still works well empirically

### Failure signatures:
1. **Singular constraint Jacobian**: $\sigma_m(\nabla c_{S_k}(x)^T) \to 0$ violates Assumption 3.1(a); manifests as multiplier explosion $\|y_{S_k}(x)\| \to \infty$
2. **Strong Morse violation**: If $\xi_{S_k}$ exceeds bounds in (24), reduced Hessian may become negative in null space despite $\|\nabla L_{S_k}\| \leq \epsilon_k$; algorithm may converge to saddle points
3. **No sample-to-sample improvement**: If $\|\nabla L_{S_{k+1}}(x_k, y_{S_{k+1}}(x_k))\| \gg \epsilon_k$, the warm-start failed; indicates samples are too uncorrelated or $p_k$ too small

### First 3 experiments:
1. **2D artificial problem validation** (Section 4.1): Implement with $N=2048$, $a=10^{-4}$, $\phi=100$; generate contour plots analogous to Figure 1 for $|S| \in \{64, 256, 1024, 2048\}$ to verify strong Morse inheritance; measure gradient evaluations for $p_1 = 64$ vs $p_1 = 2048$; target: confirm ~33% reduction as reported
2. **Sample size sensitivity sweep**: For the PINN problem (equation 106), run Algorithm 1 with $p_1 \in \{32, 64, 128, 256\}$; plot (a) total gradient evaluations vs $p_1$, (b) final $\|\nabla L(x_K, y(x_K))\|$ vs $p_1$; identify the smallest viable $p_1$ that maintains convergence
3. **Solver comparison on constrained test**: On both artificial and PINN problems, compare [4, Algorithm 1] vs [1, Algorithm 2.2] across metrics: (a) gradient evaluations, (b) achieved second-order stationarity (check if (6b) actually holds), (c) robustness to $p_1$ choice; determine if second-order solver overhead is justified

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive mechanism be developed to select the initial sample size $p_1$ that guarantees the strong Morse property without relying on unknown problem constants?
- Basis in paper: [explicit] The authors state that a value of $p_1$ satisfying the theoretical requirements "is not necessarily known in practice," though they note it can be estimated.
- Why unresolved: The theoretical bounds depend on $p_1$ being sufficiently large to satisfy specific inequality constraints involving unknown constants (e.g., $\sigma_{\min}$, $\kappa_{\nabla c}$), leaving a gap between theory and practical implementation.
- What evidence would resolve it: An adaptive algorithm that provably identifies a sufficient $p_1$ on-the-fly while maintaining the improved worst-case sample complexity.

### Open Question 2
- Question: Can the progressive sampling framework be extended to optimization problems involving inequality constraints?
- Basis in paper: [inferred] The problem formulation (1) and the entirety of the theoretical analysis are restricted to nonlinear equality constraints ($c(x) = 0$).
- Why unresolved: The analysis relies heavily on properties of the constraint Jacobian and null-space projections (e.g., acute perturbations) that become significantly more complex in the presence of active-set changes typical of inequality constraints.
- What evidence would resolve it: A derivation of sample complexity bounds for an algorithm handling inequalities, likely requiring modifications to the definition of stationarity and the perturbation analysis.

### Open Question 3
- Question: Does the sample complexity advantage persist if stochastic subproblem solvers are used instead of deterministic ones?
- Basis in paper: [inferred] The theoretical guarantees (Lemma 3.8, Theorem 3.2) rely on the worst-case iteration complexity of deterministic methods like Fletcher's augmented Lagrangian, assuming exact gradient evaluations for the subsampled problems.
- Why unresolved: Stochastic solvers introduce noise that may prevent the algorithm from reaching the precise approximate stationarity required to seed the next subproblem effectively, potentially destabilizing the progressive sampling sequence.
- What evidence would resolve it: A convergence proof or empirical study demonstrating that the progressive scheme remains stable and efficient when subproblems are solved via stochastic gradient methods.

## Limitations
- Strong structural assumptions required, particularly that full-sample problem must be $(\alpha, \beta)$-strongly Morse, which may not hold for general equality-constrained problems
- Theoretical bounds require impractically large initial sample sizes when constraint functions have high variance, creating gap between theory and practice
- Limited empirical validation beyond two specific problem instances, with essentially no corpus evidence for progressive sampling in equality-constrained optimization
- Analysis assumes idealized solver behavior that may not hold with finite-precision arithmetic or near boundary cases of assumptions

## Confidence

- **High Confidence**: The warm-start benefit mechanism (Mechanism 1) is well-supported by both theoretical analysis (Theorem 3.1) and numerical experiments (Figure 3 shows ~33% gradient evaluation reduction). The acute perturbation framework (Mechanism 2) is rigorously defined and proven under the stated assumptions.
- **Medium Confidence**: Strong Morse property inheritance (Mechanism 3) follows logically from the perturbation analysis, but the assumption that full problems are strongly Morse is restrictive and not empirically validated across diverse problem classes. The multiplier computation via least-squares (equation 7) is standard but the perturbation bounds (Lemma 3.4) depend critically on the acute perturbation condition.
- **Low Confidence**: The sample complexity bounds rely on idealized solver behavior (Lemma 3.8–3.10) that may not hold with finite-precision arithmetic or when subproblems approach boundary cases of the assumptions.

## Next Checks

1. **Assumption Robustness Test**: Systematically vary constraint function variance and compute the minimum $p_1$ required to satisfy (24); determine failure thresholds and document how constraint geometry affects the warm-start benefit.

2. **Solver-Independent Verification**: Implement a simpler first-order subproblem solver (projected gradient with Armijo line search) and verify whether the progressive sampling benefit persists without second-order machinery.

3. **Generalization Benchmark**: Apply the method to diverse equality-constrained problems (e.g., constrained logistic regression, portfolio optimization) to test whether the strong Morse assumption holds broadly or is problem-specific.