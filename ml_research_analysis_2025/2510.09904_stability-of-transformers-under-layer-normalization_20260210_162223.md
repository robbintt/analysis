---
ver: rpa2
title: Stability of Transformers under Layer Normalization
arxiv_id: '2510.09904'
source_url: https://arxiv.org/abs/2510.09904
tags:
- xperi
- training
- layer
- stability
- xpre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a principled theoretical analysis of forward
  and backward stability in Transformers under different layer normalization placements.
  Using optimal control theory, it explains why Pre-LN Transformers suffer from unbounded
  hidden state growth and gradient explosion, while Peri-LN maintains controlled growth
  and stable gradients.
---

# Stability of Transformers under Layer Normalization

## Quick Facts
- arXiv ID: 2510.09904
- Source URL: https://arxiv.org/abs/2510.09904
- Reference count: 40
- Primary result: Peri-LN theoretically bounds hidden state growth to linear rates, preventing the exponential explosion seen in Pre-LN

## Executive Summary
This paper provides a principled theoretical analysis of forward and backward stability in Transformers under different layer normalization placements. Using optimal control theory, it explains why Pre-LN Transformers suffer from unbounded hidden state growth and gradient explosion, while Peri-LN maintains controlled growth and stable gradients. The analysis reveals that Pre-LN can lead to exponential hidden state growth (O(e^D)) even with weight decay, while Peri-LN ensures linear (O(D)) growth for mean absolute values and quadratic (O(D^2)) for variance. For backward stability, Pre-LN gradients grow proportionally with activations leading to instability, whereas Peri-LN gradients remain invariant to activation magnitude.

## Method Summary
The paper compares Pre-LN and Peri-LN architectures using GPT-2 models (100M to 1.5B parameters) trained on OpenWebText dataset. Peri-LN applies Layer Normalization to both input and output of attention/feedforward modules, with an optional residual step scaling (∆t < 1). The theoretical analysis uses optimal control theory to derive growth bounds and gradient properties, while experiments validate these predictions through training stability tests and hidden state profiling across different model scales.

## Key Results
- Pre-LN leads to exponential hidden state growth (O(e^D)) even with weight decay, while Peri-LN ensures linear (O(D)) growth for mean absolute values and quadratic (O(D^2)) for variance
- Pre-LN gradients grow proportionally with activations leading to instability, whereas Peri-LN gradients remain invariant to activation magnitude
- Residual step scaling (∆t < 1) improves stability and performance without additional computational cost, with ∆t=0.1 showing consistent improvements across model sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Peri-LN theoretically bounds hidden state growth to linear rates, preventing the exponential explosion seen in Pre-LN.
- **Mechanism:** Pre-LN normalizes inputs but leaves the residual output unbounded. Theoretically, this allows the "velocity field" of the continuous-time dynamical system to become infinite, leading to unbounded Hamiltonian and exponential growth O(e^D) of hidden states. Peri-LN restricts the output of the sublayer to a fixed ellipsoid (via output LN), ensuring the Hamiltonian is well-defined and bounding the mean absolute value growth to O(D) and variance to O(D^2).
- **Core assumption:** The analysis assumes the continuous-time ODE formulation (Eq. 4) is a valid model for the discrete Transformer dynamics, allowing the application of optimal control theory to trained weights.
- **Evidence anchors:**
  - [abstract]: "Pre-LN can lead to exponential hidden state growth (O(e^D)) even with weight decay, while Peri-LN ensures linear (O(D)) growth..."
  - [section 3.3]: "Theorem 4... Peri-LN guarantees only linear growth... Theorem 3... Pre-LN... O(e^D)."
  - [corpus]: Neighbor paper "Peri-LN: Revisiting Normalization Layer..." (Kim et al. 2025) is cited extensively for the empirical observation of regular hidden states which this paper explains theoretically.
- **Break condition:** If the network depth D is very small, the exponential penalty in Pre-LN may not manifest numerically; conversely, extremely high weight decay is required to partially counteract Pre-LN growth (Theorem 3).

### Mechanism 2
- **Claim:** Peri-LN decouples gradient magnitude from activation magnitude, preventing gradient explosion caused by large weights or activations.
- **Mechanism:** During backpropagation, the gradient involves a product of Jacobians (Eq. 16). In Pre-LN, the local sensitivity ∇f_{Pre} grows linearly with activation magnitude and weights; if activations explode, gradients explode. In Peri-LN, the normalization operation induces a 1/magnitude factor in the gradient (via σ^{-1}), which exactly cancels the scaling from the activation magnitude. This makes the sensitivity invariant to the scale of the hidden states.
- **Core assumption:** Standard backpropagation dynamics apply, and the "local sensitivity" dominates the product term in deep networks (identity matrices in residual connections handle the rest).
- **Evidence anchors:**
  - [abstract]: "Pre-LN gradients grow proportionally with activations... whereas Peri-LN gradients remain invariant to activation magnitude."
  - [section 4]: "Proposition 8... sensitivity... is invariant to the magnitude of the activation. This invariance is especially critical in deep networks..."
  - [corpus]: Evidence in corpus is limited regarding the specific theoretical gradient invariance; this paper provides the derivation.
- **Break condition:** This stability relies on the Peri-LN structure; if the output LN is removed (reverting to Pre-LN), the invariance breaks.

### Mechanism 3
- **Claim:** Introducing a residual step scaling parameter ∆t < 1 improves stability by dampening the contribution of each layer's update in both forward and backward passes.
- **Mechanism:** Viewing the residual connection as a discretization of an ODE with step size ∆t, setting ∆t=1 (standard) allows large steps. Reducing ∆t (e.g., to 0.1) reduces the "velocity" of hidden state change per layer. Theoretically, this sharpens the bound on output uncertainty (Eq. 17) and reduces the local sensitivity in the backward pass (Eq. 18), preventing the product of Jacobians from exploding.
- **Core assumption:** The "velocity" of the residual function is roughly consistent, so dampening it prevents overshooting or numerical instability without stifling learning capacity.
- **Evidence anchors:**
  - [section 5]: "With the modification... the matrix J_{i:D} is given by (18)... The factor ∆t < 1 scales the local sensitivity... mitigating the potential for gradient explosion."
  - [table 2]: Shows that adding ∆t=0.1 to Peri-LN yields consistent performance improvements (e.g., Val Loss 3.10 vs 3.12) across model sizes.
  - [corpus]: Corpus evidence for specific scaling parameters is weak; this appears to be a specific derivation from this paper's control theory framework.
- **Break condition:** If ∆t is too small, the network may fail to propagate signals effectively over limited depth D, behaving like a shallower network.

## Foundational Learning

- **Concept:** **Hamilton-Jacobi-Bellman (HJB) Equation & Optimal Control**
  - **Why needed here:** The authors use the existence of a solution to the HJB PDE as a certificate for whether a Transformer architecture is "well-posed" (bounded). If the Hamiltonian is unbounded (as in Pre-LN), the training solution is theoretically degenerate.
  - **Quick check question:** Does the "cost-to-go" function remain finite if the system's velocity can be infinite?

- **Concept:** **Euler Discretization of ODEs**
  - **Why needed here:** The paper maps Transformer layers to time-steps in an ODE. The residual scaling ∆t is literally the step size in this discretization. Understanding this is key to grasping why ∆t < 1 stabilizes the "integration".
  - **Quick check question:** In y_{t+1} = y_t + ∆t · f(y_t), what happens if f is large and ∆t=1 vs ∆t=0.1?

- **Concept:** **Jacobian of Layer Normalization**
  - **Why needed here:** The backward stability relies on the specific gradient property of LN: ∇_x LN(cx) = 1/c · ∇_x LN(x). This scale invariance is the mathematical "magic" that makes Peri-LN gradients stable.
  - **Quick check question:** If you scale the input to a normalized layer by 10, how does the gradient change?

## Architecture Onboarding

- **Component map:** Input -> LN -> [Attn/FFN] -> LN -> Add(∆t ×) -> Output
- **Key Diff:** Adds one LN layer per sub-block compared to Pre-LN, and a scalar multiplier on the residual path.

- **Critical path:**
  1. Implementing the second LN (output LN) before the residual addition.
  2. Adding the residual scaling factor (∆t) inside the residual addition, treating it as a hyperparameter (suggested ∆t ∈ [0.1, 1.0]).

- **Design tradeoffs:**
  - Compute vs. Stability: Peri-LN requires extra normalization ops (minimal FLOPs, some memory overhead). It trades this for the ability to train deep models without "warm-up" heuristics or fearing divergence.
  - ∆t Tuning: Smaller ∆t increases stability but may require more layers (D) to achieve the same representational capacity (theoretically, to traverse the same "distance" in latent space).

- **Failure signatures:**
  - Pre-LN in Deep Nets: Divergence (NaN loss) late in training; or hidden state norms growing exponentially with depth (verify by logging norm(hidden) per layer).
  - Pre-LN Gradients: Exploding gradient norms specifically correlated with large activations.

- **First 3 experiments:**
  1. **Hidden State Profiling:** Train equivalent Pre-LN and Peri-LN models; plot the Frobenius norm of hidden states across layer depth. Verify if Pre-LN grows exponentially while Peri-LN grows linearly.
  2. **Stress Test Divergence:** Attempt to train Pre-LN and Peri-LN models with high learning rates or low weight decay (Table 1 setup). Count divergence instances.
  3. **Residual Scaling Ablation:** Train Peri-LN with ∆t ∈ {1.0, 0.5, 0.1}. Compare validation loss and gradient norm stability to verify if dampening improves convergence.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Can the proposed optimal control framework accurately predict the stability of novel Transformer architectures (e.g., mixture of experts or linear attention) before training?
  - Basis in paper: [explicit] The authors state their framework "serves as theoretical criteria for screening architectures before expensive empirical training."
  - Why unresolved: The paper validates the framework on existing placements (Pre-LN, Peri-LN) but does not demonstrate its predictive power on untested, future architectural modifications.
  - What evidence would resolve it: Application of the stability diagnostic workflow to new architectural variants followed by empirical training results that confirm the theoretical predictions.

- **Open Question 2**
  - Question: Do the theoretical stability guarantees of Peri-LN and residual scaling persist at model scales significantly larger than the 1.5B parameters tested?
  - Basis in paper: [inferred] Experiments are limited to 100M–1.5B parameters, while industrial models are often orders of magnitude larger.
  - Why unresolved: While bounds are theoretically depth-dependent (O(D)), practical numerical instabilities might manifest differently in hardware-constrained, distributed training environments of massive models.
  - What evidence would resolve it: Empirical training stability analysis and hidden state growth measurements on models exceeding 70B parameters.

- **Open Question 3**
  - Question: How much performance gain can Peri-LN achieve with hyperparameters tuned specifically for it, rather than using settings optimized for Pre-LN?
  - Basis in paper: [inferred] The authors admit to using hyperparameters tuned for Pre-LN for all models to isolate architectural effects.
  - Why unresolved: The reported Peri-LN performance may be conservative; specific tuning (e.g., learning rate, weight decay) could widen the performance gap.
  - What evidence would resolve it: A dedicated hyperparameter search for Peri-LN models followed by a performance comparison against the baselines.

## Limitations

- The theoretical analysis relies on continuous-time ODE approximations which may not capture all numerical effects in practice
- Experimental validation is limited to language modeling on a single dataset (OpenWebText) without exploring other modalities or tasks
- The assertion that Peri-LN is universally superior for all architectures and tasks lacks sufficient empirical validation beyond the GPT-2 setup

## Confidence

**High Confidence:**
- The theoretical framework connecting optimal control theory to Transformer stability is sound and the mathematical proofs are rigorous
- The empirical observation that Pre-LN can diverge while Peri-LN remains stable is consistently demonstrated across model scales

**Medium Confidence:**
- The specific growth rate bounds (O(e^D) vs O(D)) are theoretically proven but may vary with architectural details like attention mechanisms and activation functions
- The claim that residual scaling improves performance is supported by experiments, but the magnitude of improvement may depend on specific hyperparameters

**Low Confidence:**
- The assertion that Peri-LN is universally superior for all architectures and tasks lacks sufficient empirical validation beyond the GPT-2 language modeling setup
- The optimal range for the residual scaling parameter (∆t ∈ [0.1, 1.0]) is suggested but not thoroughly explored

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate Peri-LN with residual scaling on vision transformers and multimodal models to verify the theoretical stability benefits extend beyond language modeling.

2. **Extreme Depth Scaling**: Train models with 100+ layers to empirically verify if Pre-LN's exponential growth manifests more severely at extreme depths, as predicted by theory.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates, weight decay, and batch sizes for both Pre-LN and Peri-LN to quantify the stability margin and identify break points for each architecture.