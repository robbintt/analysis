---
ver: rpa2
title: Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived
  Priors Across Discrete and Continuous Domains
arxiv_id: '2505.07274'
source_url: https://arxiv.org/abs/2505.07274
tags:
- cache
- performance
- learning
- policy
- caching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a cache-efficient framework for integrating
  large language models (LLMs) as priors in reinforcement learning (RL). The core
  method uses an adaptive caching mechanism where cache parameters are meta-optimized
  using surrogate gradients from policy performance.
---

# Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains

## Quick Facts
- arXiv ID: 2505.07274
- Source URL: https://arxiv.org/abs/2505.07274
- Reference count: 40
- Reduces LLM queries by 3.8-4.7× and latency by 4.0-12.0× while retaining 96-98% of uncached performance

## Executive Summary
This paper presents a cache-efficient framework for integrating large language models (LLMs) as priors in reinforcement learning across discrete and continuous domains. The core innovation is an adaptive caching mechanism where cache parameters are meta-optimized using surrogate gradients from policy performance, achieving 3.8-4.7× reduction in LLM queries while maintaining 96-98% of uncached performance. The framework extends to offline RL with CQL-Prior, improving performance by 14-29% and reducing training time by 38-40%. Theoretical KL divergence bounds validate the approximation quality, and the method achieves success rates of 92.5-95.6% on text-based tasks and returns of 480.2-684.2 on continuous control tasks.

## Method Summary
The framework combines state abstraction, adaptive caching, and posterior sampling to efficiently use LLM-derived priors in RL. A three-stage pipeline converts continuous states to textual descriptions through human annotation, contrastive expansion, and joint optimization. The adaptive cache stores (embedding, LLM prior) pairs and is meta-optimized via surrogate gradients from policy performance. Posterior sampling combines cached priors with Q-values using an adaptive temperature schedule that shifts exploration to exploitation as cache reliability increases. The method extends SAC to factorized policies that bridge symbolic LLM outputs and continuous control actions.

## Key Results
- Achieves 3.8-4.7× reduction in LLM queries and 4.0-12.0× latency reduction while retaining 96-98% of uncached performance
- Success rates of 92.5-95.6% on text-based tasks (TextWorld, ALFWorld, BabyAI, WebShop)
- Average returns of 480.2-684.2 on continuous control tasks (MuJoCo HalfCheetah, Walker2d, Ant, MetaWorld)
- Offline RL extension (CQL-Prior) improves performance by 14-29% and reduces training time by 38-40%

## Why This Works (Mechanism)

### Mechanism 1: Meta-Learned Adaptive Caching Amortizes LLM Inference
Cache parameters (capacity K, similarity threshold δ, refresh rate r) adapt online via surrogate gradients from policy performance, reducing LLM queries by 3.8-4.7× while retaining 96-98% of uncached performance. A state encoder maps states to embeddings; cache stores (embedding, LLM prior) pairs with cosine similarity retrieval. Parameters update via heuristics: larger K when hit rate low, lower δ when TD error high, higher r when policy variability high. The meta-reward balances performance and efficiency.

### Mechanism 2: Posterior Sampling with Adaptive Temperature Balances Prior Fidelity and Q-Value Exploitation
The policy π(a|s) ∝ p̂prior(asym|s) · exp(Q(s,a)/τ(t)) with τ(t) = 0.8e^(-2.0h(t)) dynamically shifts exploration→exploitation as cache hit rate h(t) increases. Sample k symbolic action candidates from cached LLM prior, reweight by Q-values using temperature-modulated softmax, then sample continuous control u ∼ πθ(u|s, asym). As h(t) rises (cache reliable), τ(t) falls → exploit Q-informed actions.

### Mechanism 3: State Abstraction Pipeline Enables Cross-Domain LLM Priors
A three-stage pipeline (annotation → contrastive expansion → joint optimization) converts continuous states to textual descriptions, allowing unified LLM prior generation for both discrete text and continuous control domains. Human annotation (200-300 pairs) seeds a contrastive learner that propagates labels via embedding similarity; final ϕ is fine-tuned to ensure descriptions are both accurate and RL-useful.

## Foundational Learning

- **Control-as-Inference / KL-regularized RL**: The framework models LLM outputs as structured priors p(a|s) and the policy as a posterior p(a|s,O=1); understanding this Bayesian framing is essential to grasp why KL bounds matter. Quick check: Can you explain why the posterior π*(a|s) ∝ p(a|s)·exp(Q*(s,a)/α) balances prior fidelity against reward maximization?

- **Meta-Learning (MAML-style)**: Cache parameters are meta-optimized via surrogate gradients; distinguishing this from standard hyperparameter tuning clarifies why the cache adapts online to task phases. Quick check: How does meta-learning cache parameters via Rmeta differ from pre-setting fixed K, δ, r values?

- **Soft Actor-Critic (SAC) and Hybrid Action Spaces**: The method extends SAC to factorized policies π(asym, u|s) = π(asym|s)·πθ(u|s,asym), bridging symbolic LLM outputs and continuous control. Quick check: Why does factorizing the policy enable symbolic guidance while preserving differentiable continuous control?

## Architecture Onboarding

- **Component map**: Raw state → State Encoder fψ → d-dimensional embedding → Cache C → (if hit) cached prior, (if miss) LLM query → abstraction ϕ(s) → text description → LLM prior generator → posterior sampler → SAC extension

- **Critical path**: 1. State s → abstraction ϕ(s) → text description. 2. Encode: z = fψ(s); query cache. 3. If miss: LLM query → cache store; if hit: retrieve p̂prior. 4. Sample k symbolic candidates → Q-reweight → select asym. 5. Sample u ∼ πθ(u|s,asym) → execute (asym, u). 6. Observe (s, a, r, s′) → update Q, policy, cache parameters via Algorithm 1.

- **Design tradeoffs**: Cache size K (larger → higher hit rate but more memory), similarity threshold δ (higher → stricter retrieval but lower hit rate), temperature schedule (aggressive decay speeds convergence but may premature-exploit).

- **Failure signatures**: Cache hit rate stuck <60% → embedding or abstraction failure; KL divergence between cached/fresh priors consistently exceeds κ′ bound → cache stale or state distribution shifted; performance gap vs. uncached >5% → δ too low (bad retrievals) or abstraction misleading; latency spikes on >25% of steps → cache miss rate too high.

- **First 3 experiments**: 1. Ablate caching: Run full system vs. "Simple LRU Only" on ALFWorld—expect ~8% performance drop. 2. Sweep δ: Test δ ∈ {0.5, 0.7, 0.8, 0.9, 0.95} on HalfCheetah—plot hit rate vs. return to find Pareto point. 3. Validate KL bound: Inject controlled noise into abstraction on MuJoCo; measure empirical KL vs. theoretical bound.

## Open Questions the Paper Calls Out
None

## Limitations
- The state abstraction pipeline relies heavily on human annotation quality and the contrastive learning assumption that semantically similar states cluster in embedding space, without independent validation of abstraction quality.
- Cache meta-optimization uses heuristic updates rather than gradient-based methods, which may not generalize across diverse task structures.
- The 5-shot fine-tuning approach is sensitive to prompt engineering and example selection, neither of which are fully specified in the paper.

## Confidence
- **High**: Cache efficiency gains (3.8-4.7× reduction in LLM queries, 4.0-12.0× latency reduction) and theoretical KL divergence bounds are well-supported by empirical results and mathematical derivation.
- **Medium**: Performance retention (96-98% of uncached) is demonstrated but depends on abstraction quality and cache meta-optimization, which have higher uncertainty.
- **Low**: Claims about generalization across unseen domains are based on limited ablation studies and transfer experiments without systematic evaluation of abstraction failure modes.

## Next Checks
1. **Abstraction Quality Validation**: Measure KL divergence between LLM priors from cached vs. fresh abstractions on held-out states to quantify staleness independent of RL performance.
2. **Cache Robustness Test**: Systematically vary annotation data quantity (50, 100, 200, 300 pairs) and measure performance degradation to establish minimum viable abstraction quality.
3. **Meta-Optimization Ablation**: Compare adaptive cache (Algorithm 1) against fixed K/δ/r baselines across multiple seeds to isolate contribution of online meta-learning vs. good initial hyperparameters.