---
ver: rpa2
title: 'XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation
  Detection with Multimodal LLMs'
arxiv_id: '2508.09999'
source_url: https://arxiv.org/abs/2508.09999
tags:
- evidence
- news
- misinformation
- caption
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XFacta, a contemporary, real-world dataset
  for multimodal misinformation detection that addresses evaluation bias in existing
  benchmarks by including only content from January 2024 onward and sourced from actual
  social media posts. The dataset enables systematic analysis of multimodal large
  language model (MLLM) design strategies for evidence retrieval and reasoning, using
  eight evidence retrieval methods and four reasoning techniques.
---

# XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs

## Quick Facts
- arXiv ID: 2508.09999
- Source URL: https://arxiv.org/abs/2508.09999
- Authors: Yuzhuo Xiao; Zeyu Han; Yuhan Wang; Huaizu Jiang
- Reference count: 20
- Primary result: GPT-4o achieves 88.8% overall accuracy on XFacta using T→Et and I→Et evidence retrieval with multi-step reasoning

## Executive Summary
This paper introduces XFacta, a contemporary dataset for multimodal misinformation detection that addresses evaluation bias in existing benchmarks by including only content from January 2024 onward. The dataset consists of 2,400 real-world social media posts with systematic annotations, enabling evaluation of multimodal large language models (MLLMs) for evidence retrieval and reasoning. Experiments demonstrate that combining text-to-text and image-to-text evidence retrieval strategies with multi-step reasoning yields the best performance, with GPT-4o achieving 88.8% accuracy compared to 68.2% by previous state-of-the-art methods. The dataset is continuously updated through a semi-automatic detection-in-the-loop framework.

## Method Summary
XFacta uses a zero-shot MLLM inference approach with evidence retrieval and reasoning. The method combines two evidence retrieval strategies: T→Et (text query to retrieve supporting webpages) and I→Et (image query via reverse image search to trace provenance). Retrieved evidence undergoes domain filtering to remove low-credibility sources, followed by multi-step reasoning where the MLLM separately evaluates caption authenticity and image provenance before aggregating the final judgment. The best configuration uses both T→Et and I→Et evidence with domain filtering, processed through multi-step reasoning using GPT-4o.

## Key Results
- GPT-4o achieves 88.8% overall accuracy on XFacta, significantly outperforming previous SOTA methods at 68.2%
- T→Et and I→Et evidence retrieval strategies combined with multi-step reasoning yield optimal performance
- Smaller MLLMs (Qwen-7B) show severe real/fake imbalance, with accuracy heavily skewed toward "real" predictions
- Contemporary dataset design prevents memorization, with GPT-4o dropping from 80-92% on older benchmarks to 70.8% on XFacta without evidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining T→Et and I→Et evidence retrieval improves multimodal misinformation detection accuracy
- Mechanism: T→Et retrieves webpages supporting/refuting text claims, while I→Et traces images to original context via reverse image search. The complementary coverage addresses both text manipulation and image out-of-context misuse.
- Core assumption: Misinformation frequently involves either false textual claims OR authentic images misused from unrelated events—rarely both perfectly fabricated.
- Evidence anchors:
  - "T→Et and I→Et evidence retrieval strategies combined with multi-step reasoning yield the best performance"
  - "I → Et (strategy 3) provides consistently informative evidence especially for identifying fake posts... achieves high accuracy but also consistently maintains high confidence across Deepfakes and Image OOC categories"
  - FMR=0.47 from HiEAG paper showing evidence-augmented generation improves OOC detection

### Mechanism 2
- Claim: Multi-step reasoning outperforms single-step chain-of-thought by isolating evidence types before aggregation
- Mechanism: Multi-step reasoning evaluates caption authenticity and image misuse separately, then combines judgments, preventing strong textual evidence from overwhelming image-mismatch signals.
- Core assumption: Models struggle to weight conflicting evidence appropriately when presented jointly.
- Evidence anchors:
  - "For GPT-4o, multi-step reasoning has the overall best balanced accuracy... accuracy in detecting fake posts is superior to other methods"
  - Figure 4 shows multi-step reasoning correctly flags CNN post where image doesn't match Oklahoma event described in text
  - Weak corpus signal on reasoning decomposition; related work focuses more on evidence retrieval

### Mechanism 3
- Claim: Contemporary dataset design (post-January 2024) forces models to rely on evidence retrieval rather than memorization
- Mechanism: By excluding events likely in MLLM training data (pre-October 2023 cutoff), the dataset eliminates shortcut where models classify based on parametric knowledge alone.
- Core assumption: Existing benchmarks contain events memorized during training, inflating zero-shot performance without evidence.
- Evidence anchors:
  - "existing benchmarks either contain outdated events, leading to evaluation bias due to discrepancies with contemporary social media scenarios as MLLMs can simply memorize these events"
  - Table 8: GPT-4o achieves 80-92% accuracy on older datasets without evidence, but only 70.8% on XFacta without evidence
  - TrueFake dataset (FMR=0.53) similarly emphasizes real-world in-the-wild testing

## Foundational Learning

- Concept: **Out-of-Context (OOC) Misinformation**
  - Why needed here: Distinguishing OOC (authentic image, wrong event) from deepfakes (synthetic image) and text-misleading (false caption) determines which evidence retrieval strategy will succeed.
  - Quick check question: If I→Et returns the image's original source but T→Et supports the caption, what misinformation type is likely? (Answer: Image OOC)

- Concept: **Cross-Modal Evidence Retrieval**
  - Why needed here: Understanding that T→Ei and I→Et are asymmetric—image-to-text produces more precise provenance information for OOC detection.
  - Quick check question: Why does I→Et outperform T→Ei for detecting image misuse? (Answer: I→Et finds exact image matches with context; T→Ei returns semantically similar but visually unmatched images)

- Concept: **Evidence Post-Processing (Domain Filtering & Extraction)**
  - Why needed here: Raw web search results include low-credibility sources that can mislead models; filtering by domain trustworthiness and extracting relevant passages reduces noise.
  - Quick check question: What tradeoff does LLM-based evidence extraction introduce? (Answer: Token overhead vs. precision improvement)

## Architecture Onboarding

- Component map:
  Input Post (Image I, Text T) -> Evidence Retrieval Layer (T→Et via Google/DuckDuckGo, I→Et via Reverse Image Search, Post-processing via Domain Filter) -> Reasoning Layer (MLLM) -> Output: Real/Fake + Explanation

- Critical path:
  1. Retrieve T→Et and I→Et evidence (both strategies, not just one)
  2. Apply domain filtering to remove low-credibility sources
  3. Run multi-step reasoning: first assess caption against T→Et, then assess image provenance against I→Et, then aggregate
  4. Flag posts where caption is true but image is mismatched as "misleading" (overly strict but beneficial per authors)

- Design tradeoffs:
  - Google vs. DuckDuckGo: Google returns higher-quality evidence (87.1% vs 84.2% for T→Et) but may have API restrictions
  - Evidence extraction: Improves accuracy but adds token costs; skip for cost-sensitive deployments
  - CoT vs. multi-step: Multi-step better for large models (GPT-4o), prompt ensembles better for smaller models (Qwen-7B)

- Failure signatures:
  - High confidence on wrong predictions when T→Et strongly supports caption but image is OOC (mitigate by always including I→Et)
  - LLM-generated queries (strategies 4-5) underperform because they paraphrase rather than interrogate
  - Domain filter may exclude legitimate evidence if source is misclassified

- First 3 experiments:
  1. Baseline comparison: Run zero-shot detection on XFacta Dev set with no evidence, T→Et only, I→Et only, and both combined. Expect ~70% → 85-90% accuracy jump with combined evidence.
  2. Reasoning method ablation: Compare CoT vs. multi-step reasoning on GPT-4o using both T→Et and I→Et. Expect multi-step to improve fake-post accuracy by ~10 percentage points.
  3. Error analysis on fake types: Stratify Dev set by misinformation type (Deepfake/OOC/Text Misleading) and identify which evidence type best predicts each. Expect I→Et to dominate OOC detection.

## Open Questions the Paper Calls Out

- Question: What is the optimal combination of evidence retrieval strategies and reasoning methods, and how do interactions between strategies affect overall detection performance?
  - Basis: "due to the limited token usage and the high cost of closed-source MLLMs, we cannot benchmark all combinations of different strategies, so our evaluation may not yield the optimal solution due to potential interactions between different strategies"
  - Why unresolved: Authors only tested evidence retrieval and reasoning strategies independently due to computational cost
  - What evidence would resolve it: Systematic grid search across all evidence-retrieval and reasoning method combinations on XFacta, reporting both accuracy and computational cost metrics

- Question: How can real-world misinformation datasets be scaled up while maintaining annotation quality and contemporary relevance?
  - Basis: "the dataset is inevitably smaller in scale compared to misinformation datasets collected through automated approaches, and it is also less suitable for supporting model training"
  - Why unresolved: Real-world collection requires manual verification, creating trade-off between scale and authenticity
  - What evidence would resolve it: Development and evaluation of semi-automated annotation pipelines that significantly increase dataset size without sacrificing label reliability

- Question: How will XFacta remain a valid benchmark as MLLMs' training data expands to include 2024+ content?
  - Basis: "As MLLMs continue to evolve, this dataset may eventually become outdated or overlap with the training data of future MLLMs"
  - Why unresolved: Detection-in-the-loop framework adds new content, but no mechanism prevents future MLLM training from ingesting that content
  - What evidence would resolve it: Periodic re-evaluation of models on XFacta as newer MLLM versions release, tracking when performance gains shift from evidence-based reasoning to memorization

## Limitations

- Effectiveness depends on assumption that January 2024+ content is truly unseen by contemporary MLLMs, but exact training cutoffs and data leakage risks remain uncertain
- Domain filtering mechanism excludes low-credibility sources but lacks transparency about which domains are trusted versus untrusted, potentially introducing bias
- Study focuses on English-language content from specific social media sources, limiting generalizability to other languages and platforms

## Confidence

- **High confidence**: Dataset design methodology (post-2024 filtering) and its impact on preventing memorization; complementary nature of T→Et and I→Et evidence retrieval strategies; superiority of multi-step reasoning for large models
- **Medium confidence**: Domain filtering effectiveness and its impact on evidence quality; relative performance of different reasoning methods across model scales; generalizability to non-English content
- **Low confidence**: Long-term viability of detection-in-the-loop update framework; optimal evidence retrieval parameters (number of results, search operators); exact prompt templates used for reasoning

## Next Checks

1. **Cross-platform validation**: Test XFacta's methodology on Facebook and Instagram posts to verify whether the evidence retrieval and reasoning strategies generalize beyond X/Twitter content.

2. **Domain filtering ablation**: Conduct controlled experiments removing the domain filter to quantify its contribution to accuracy gains and identify potential false positives from low-credibility sources.

3. **Small model optimization**: Systematically evaluate prompt engineering and few-shot examples for Qwen-7B and similar small MLLMs to close the performance gap observed in the current study.