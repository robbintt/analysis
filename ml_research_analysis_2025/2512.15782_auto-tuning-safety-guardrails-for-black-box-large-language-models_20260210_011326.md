---
ver: rpa2
title: Auto-Tuning Safety Guardrails for Black-Box Large Language Models
arxiv_id: '2512.15782'
source_url: https://arxiv.org/abs/2512.15782
tags:
- prompts
- safety
- jailbreak
- malware
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a practical approach to hardening safety guardrails
  for black-box large language models by framing guardrail configuration as a hyperparameter
  optimization problem. The method wraps Mistral-7B-Instruct with modular jailbreak
  and malware system prompts and a ModernBERT-based harmfulness classifier, then searches
  for optimal prompt-filter combinations to minimize malware and jailbreak attack
  success rates while reducing over-refusal on benign queries and maintaining low
  latency.
---

# Auto-Tuning Safety Guardrails for Black-Box Large Language Models

## Quick Facts
- arXiv ID: 2512.15782
- Source URL: https://arxiv.org/abs/2512.15782
- Reference count: 3
- Primary result: Optuna HPO rediscovers best guardrail configs using ~10× fewer evaluations than grid search

## Executive Summary
This paper introduces a practical approach to hardening safety guardrails for black-box large language models by framing guardrail configuration as a hyperparameter optimization problem. The method wraps Mistral-7B-Instruct with modular jailbreak and malware system prompts and a ModernBERT-based harmfulness classifier, then searches for optimal prompt-filter combinations to minimize malware and jailbreak attack success rates while reducing over-refusal on benign queries and maintaining low latency. A 48-point grid search baseline is outperformed by Optuna, which reliably rediscovers the best configurations using roughly 10× fewer evaluations and 8× less wall-clock time.

## Method Summary
The study treats safety guardrail configuration as a hyperparameter optimization problem, using Mistral-7B-Instruct as the base model and a ModernBERT-based harmfulness classifier for content filtering. The guardrail space includes four toggleable system prompt snippets (two for jailbreak prevention, two for malware refusal) and three filter modes (none, mild, strict) based on classifier probability thresholds. A 48-point grid search baseline is compared against Optuna HPO with a scalarized objective that balances malware ASR, jailbreak ASR, benign harmful-response rate, and latency. All evaluations use 50 prompts per dataset across malware, jailbreak, and benign categories.

## Key Results
- Optuna reliably rediscovers near-optimal guardrail configurations using roughly 10× fewer evaluations and 8× less wall-clock time than exhaustive grid search
- Adding classifier-based filtering reduces malware attack success by up to 10 percentage points
- Combining system prompts with classifier filtering yields better benign performance than filtering alone
- Jailbreak attack success rates remain high (0.88-1.0) across all configurations, indicating prompt-based defenses are insufficient for sophisticated attacks

## Why This Works (Mechanism)

### Mechanism 1: Classifier-based content filtering intercepts harmful outputs post-generation
- Adding a harmfulness classifier with threshold-based filtering reduces malware attack success rates by detecting and blocking harmful responses before they reach users
- The ModernBERT-based classifier scores (prompt, response) pairs for harmfulness probability (p_harm), with filter modes overriding responses when p_harm exceeds thresholds (0.5 for "mild", 0.8 for "strict")
- Core assumption: The classifier generalizes sufficiently to detect harmful patterns in malware and jailbreak responses that the base model produces

### Mechanism 2: System prompt + filter combination provides defense-in-depth
- Combining safety-oriented system prompts with classifier filtering yields better benign performance than filtering alone by addressing attacks at two stages
- System prompts instruct the model to refuse unsafe requests during generation while the classifier catches responses that slip through, reducing both attack success AND over-refusal on benign queries
- Core assumption: System prompts can shift model behavior without degrading helpfulness on legitimate requests

### Mechanism 3: Black-box hyperparameter optimization efficiently searches discrete guardrail configurations
- Standard HPO tools (Optuna) rediscover near-optimal guardrail configurations with ~10× fewer evaluations than exhaustive grid search
- Optuna treats guardrail configuration (4 binary prompt toggles + 3 filter modes = 48 combinations) as a discrete hyperparameter space, sampling promising configurations using Bayesian optimization
- Core assumption: The guardrail configuration space has exploitable structure (smoothness, correlations) that HPO can leverage

## Foundational Learning

- Concept: **Hyperparameter optimization (HPO) fundamentals**
  - Why needed here: The paper's core contribution is reframing guardrail design as an HPO problem. Understanding objective scalarization, search efficiency, and Pareto frontiers is essential.
  - Quick check question: Given 4 binary prompt toggles and 3 filter modes, why does Optuna require fewer than 48 evaluations to find good configurations?

- Concept: **Harmfulness classifiers and threshold calibration**
  - Why needed here: The ModernBERT classifier's probability scores and threshold choices directly determine false positive (over-refusal) and false negative (attack success) rates.
  - Quick check question: If you lower the filter threshold from 0.8 to 0.5, what happens to malware ASR vs. benign helpfulness?

- Concept: **Black-box LLM safety constraints**
  - Why needed here: The paper targets frozen models where weight modification is infeasible. Understanding what can/cannot be tuned at deployment time is critical.
  - Quick check question: Name three guardrail components you CAN tune on a frozen LLM vs. one you CANNOT.

## Architecture Onboarding

- Component map: User prompt → System prompt assembler → Base LLM generation → Classifier scoring → Filter override (if p_harm > threshold) → Final response

- Critical path: User prompt → System prompt assembler → Base LLM generation → Classifier scoring → Filter override (if p_harm > threshold) → Final response

- Design tradeoffs:
  - Strict filtering lowers ASR but increases over-refusal and latency (~0.1s classifier overhead)
  - More system prompt snippets reduce generation latency (shorter responses) but may conflict with each other
  - HPO efficiency vs. guarantee: Optuna is fast but may miss global optimum; grid search is exhaustive but costly

- Failure signatures:
  - Jailbreak ASR remains high (~0.88-1.0) across ALL configurations → prompt-based defenses insufficient for sophisticated attacks
  - Benign harmful-rate >0.4 with no filter → classifier flags many safe outputs as harmful (over-sensitive)
  - Optuna converges to different configs on repeated runs → objective has flat regions or high variance

- First 3 experiments:
  1. **Baseline vulnerability**: Run bare model (no prompts, filter-none) on all 3 datasets to establish worst-case ASR and latency.
  2. **Filter-only ablation**: Test all 3 filter modes with NO system prompts to isolate classifier contribution.
  3. **Single-snippet sweep**: Enable one prompt snippet at a time (JB1, JB2, MW1, MW2) with mild filtering to identify which prompts help which attack types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the auto-tuning framework maintain robustness when extended to multi-turn jailbreaking and persuasion attacks?
- Basis in paper: [explicit] The paper explicitly states that "All evaluations are single-turn; multi-turn jailbreaking and persuasion attacks are out of scope."
- Why unresolved: The current study only validates guardrails against single-prompt attacks; it is unknown if these configurations hold up against adversaries who decompose malicious intent across multiple conversational turns.
- What evidence would resolve it: Evaluating the optimized configurations on multi-turn adversarial benchmarks (e.g., AgentInstruct) to measure resilience.

### Open Question 2
- Question: How does decoupling the harmfulness classifier used for evaluation from the classifier used for filtering impact the reported performance?
- Basis in paper: [explicit] The authors identify the limitation that "The same harmfulness classifier is used to both block content and evaluate the system," introducing obvious biases.
- Why unresolved: The reported success rates may be artificially inflated or distorted because the "judge" is identical to the "guard," potentially missing blind spots shared by both.
- What evidence would resolve it: Re-running the optimization loop using a distinct, separate model (or human evaluation) for the objective function scoring.

### Open Question 3
- Question: Can this hyperparameter optimization approach effectively generalize to cover a wider range of harm categories (e.g., hate speech, self-harm) and larger configuration spaces?
- Basis in paper: [explicit] The Discussion notes "Limited data and coverage" (only malware/jailbreak) and a "Narrow configuration space," suggesting the need for expansion.
- Why unresolved: It is unclear if the efficiency gains observed (roughly 10x fewer evaluations) persist when the search space becomes high-dimensional or continuous.
- What evidence would resolve it: Applying the Optuna framework to a broader set of safety benchmarks and a richer parameter space (e.g., continuous thresholds).

## Limitations

- **Dataset representativeness**: The study uses 50 prompts per dataset, which may not capture the full diversity of real-world attack patterns or benign queries
- **Jailbreak ASR ceiling**: All configurations show jailbreak ASR between 0.88-1.0, suggesting prompt-based defenses are fundamentally insufficient against sophisticated jailbreaks
- **Unknown prompt snippet text**: The exact system prompt snippets (JB1, JB2, MW1, MW2) are only conceptually described, not provided verbatim

## Confidence

- **HPO efficiency claims** (Optuna finds near-optimal configs with ~10× fewer evaluations): High
- **Classifier effectiveness claims** (10 percentage point ASR reduction): Medium
- **Defense-in-depth claims** (combined prompts + filtering outperform either alone): Medium
- **Black-box guardrail tunability claims**: High

## Next Checks

1. **Independent classifier validation**: Re-run the full evaluation pipeline using a different harmfulness classifier (e.g., OpenAI moderation API or another BERT variant) to verify that the 10 percentage point malware ASR reduction is classifier-independent

2. **Larger dataset evaluation**: Scale up prompt evaluation from 50 to 500+ prompts per dataset and recompute confidence intervals to determine if observed performance differences are statistically significant

3. **Prompt snippet isolation test**: Systematically disable each individual system prompt snippet (JB1, JB2, MW1, MW2) across all configurations to quantify their individual contributions to ASR reduction vs. over-refusal, isolating which components drive the defense-in-depth effect