---
ver: rpa2
title: 'TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation'
arxiv_id: '2512.04694'
source_url: https://arxiv.org/abs/2512.04694
tags:
- station
- site
- data
- latent
- stations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TimesNet-Gen, a deep learning model for generating
  site-specific strong ground motion waveforms conditioned on station identifiers.
  The authors address the challenge of simulating realistic seismic signals by learning
  station-dependent spectral patterns from real accelerometer data.
---

# TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation

## Quick Facts
- **arXiv ID:** 2512.04694
- **Source URL:** https://arxiv.org/abs/2512.04694
- **Reference count:** 35
- **Primary result:** TimesNet-Gen generates site-specific strong ground motion waveforms with better HVSR and fundamental frequency alignment than a conditional VAE baseline

## Executive Summary
TimesNet-Gen is a deep learning model for generating site-specific strong ground motion waveforms conditioned on station identifiers. The model learns station-dependent spectral patterns from real accelerometer data using a TimesNet architecture extended with station ID conditioning. By learning latent representations that capture site-specific response properties, TimesNet-Gen can generate diverse, physically plausible waveforms that preserve station-specific frequency characteristics. The model is evaluated against a conditional VAE baseline using horizontal-to-vertical spectral ratio (HVSR) curves and fundamental site frequency distributions, demonstrating superior alignment with real data in both metrics.

## Method Summary
TimesNet-Gen uses a two-phase training approach: Phase 0 performs unsupervised pretraining on the full AFAD strong-motion database, learning general seismic representations without conditioning; Phase 1 fine-tunes the model with station-ID conditioning via one-hot encoding and channel-wise feature modulation. The TimesNet backbone applies FFT-based period selection, reshapes sequences into 2D grids, and uses Inception-style 2D convolutions. Generation uses k-sample latent averaging (k=5) from station-specific record pools with Gaussian noise injection. The model is evaluated using HVSR curve alignment, fundamental site frequency distributions, and confusion matrices, showing better performance than a spectrogram-based conditional VAE baseline.

## Key Results
- TimesNet-Gen achieves better HVSR curve alignment and f0 distribution matching than the conditional VAE baseline
- The model captures station-specific characteristics beyond fundamental frequency, as shown by confusion matrix analysis
- Zero-shot generation experiments demonstrate that station-specific patterns are encoded in the latent space without explicit supervision
- TimesNet-Gen achieves NCC alignment score of 0.93 compared to VAE's 0.81 on f0 confusion matrices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reshaping 1D seismic waveforms into period-aligned 2D grids enables TimesNet to capture multi-scale temporal patterns through 2D convolutions, which is particularly effective for signals with inherent periodic structure.
- **Mechanism:** The model applies FFT to identify top-k dominant periods, then reshapes each 1D sequence into a (T/p, p) grid where intra-period variation appears along columns and inter-period variation along rows. Inception-style 2D convolutions then process these grids with multiple kernel sizes.
- **Core assumption:** Strong-motion waveforms exhibit meaningful periodic structure that can be exploited via 2D convolution patterns; when clear periodicity is absent, intra-period structure dominates as a limiting case.
- **Evidence anchors:** TimesNet has demonstrated strong performance on complex, multi-periodic time signals by extracting multi-scale temporal patterns. We note that seismic strong-motion waveforms exhibit similar periodic structure. For each selected period p, the sequence is reshaped into a (T/p, p) grid so that intraperiod-variation (within a period) appears along columns and interperiod-variation (across periods at the same phase) appears along rows.

### Mechanism 2
- **Claim:** Station-specific latent representations emerge naturally from waveform features without requiring explicit station ID embeddings for discrimination.
- **Mechanism:** The encoder compresses waveforms into latent codes that implicitly encode site-specific response properties. A zero-shot experiment showed that feeding waveforms to an unconditioned encoder still produced station-distinguishable outputs, suggesting the latent space self-organizes by site characteristics.
- **Core assumption:** Waveforms from the same station share spectral signatures (e.g., fundamental site frequency, HVSR peaks) that are encoded into similar latent regions without explicit supervision.
- **Evidence anchors:** A single-phase, fully unsupervised TimesNet-Gen model was trained without any station conditioning, and generation for the five target stations was performed in a zero-shot manner. The results remained consistent with the two-phase conditioned approach. The latent representations naturally encode site-specific response properties in an unsupervised manner, and the station-specific sampling strategy effectively leverages this implicit clustering.

### Mechanism 3
- **Claim:** K-sample latent averaging with station-specific noise injection produces diverse yet station-consistent waveforms without requiring a VAE-style KL prior.
- **Mechanism:** Instead of sampling from a learned prior distribution, the model averages N=5 encoder outputs from a station's record pool and adds Gaussian noise with σ computed from that station's latent codes. This preserves station characteristics while introducing controlled variation.
- **Core assumption:** Averaging multiple station-specific latent codes captures the "station prototype" while added noise generates diversity; the decoder has learned to map perturbed latent codes to plausible waveforms.
- **Evidence anchors:** To introduce controlled diversity while preserving station characteristics, we average N (selected as 5 in our experiments) encoder features from a specific station record pool and decode the mixed representation under the target station label. We also add gaussian noise of unit standard deviation σ, where σ is calculated from all latent codes of a specific station's records.

## Foundational Learning

- **Concept:** FFT-based period detection and 2D temporal reshaping
  - **Why needed here:** TimesNet's core operation transforms 1D sequences into period-aligned 2D grids. Without understanding how FFT identifies dominant periods and how reshaping creates intra/inter-period structure, the architecture appears arbitrary.
  - **Quick check question:** Given a sequence of length 1000 with a dominant period of 50, what would the reshaped 2D grid dimensions be?

- **Concept:** Horizontal-to-Vertical Spectral Ratio (HVSR) and fundamental site frequency (f₀)
  - **Why needed here:** All evaluation metrics in the paper (HVSR curves, f₀ distributions, confusion matrices) assume familiarity with how site response is characterized spectrally. The physics of why different stations have different f₀ values is essential context.
  - **Quick check question:** If a station has a fundamental site frequency of 5 Hz, what would you expect to observe in its HVSR curve?

- **Concept:** Autoencoder reconstruction vs. generation
  - **Why needed here:** TimesNet-Gen is first trained as an autoencoder (reconstruction), then converted to a generator via latent bottleneck and sampling. Understanding this progression is critical to parsing the two-phase training strategy.
  - **Quick check question:** Why does a standard autoencoder not generate new samples, and what modifications enable generation?

## Architecture Onboarding

- **Component map:**
  Input (T×3) → TimesBlock × N → Encoder → Latent Bottleneck → Decoder → TimesBlock × N → Output (T×3)
                      ↓                                                      ↑
               Station ID (one-hot) ────────────→ Channel-wise modulation ─────────────────┘
  
  TimesBlock: FFT → Period selection → Reshape to 2D grids → Inception 2D conv → Aggregate → Reshape back

- **Critical path:**
  1. Understand TimesBlock's FFT-based period extraction and 2D reshaping (this is where multi-scale temporal patterns are captured).
  2. Trace how station ID is injected: one-hot encoded, concatenated to 2D feature maps, projected via 1×1 conv.
  3. Understand the latent bottleneck: no variational sampling, just direct propagation during training.
  4. Understand generation-time sampling: k-sample averaging from station pool + noise injection.

- **Design tradeoffs:**
  - **Latent space design:** No KL regularization means better reconstruction but requires custom sampling strategy (averaging). VAE baseline uses KL but showed worse f₀ alignment.
  - **Two-phase training:** Pretraining on full corpus learns general seismic representations; fine-tuning on 5 stations learns station specificity. Trade-off is complexity vs. potentially better station discrimination.
  - **Spectrogram vs. time-domain:** VAE baseline operates on STFT spectrograms (amplitude + phase); TimesNet-Gen operates directly on time-domain. Paper shows time-domain approach achieves better HVSR alignment, but spectrogram approach may capture frequency content more explicitly.

- **Failure signatures:**
  - **f₀ distribution mismatch:** Generated samples have f₀ distribution that doesn't match real data for a station (check HVSR curves and f₀ histograms).
  - **Station confusion:** Similar stations (close f₀ values) produce indistinguishable outputs (check confusion matrix off-diagonal blocks).
  - **Mode collapse:** Generated samples from same station are too similar (inspect waveform diversity and P/S-wave arrival time variation).
  - **Spectral artifacts:** Fourier amplitude spectra show unrealistic peaks or missing energy (compare real vs. generated spectra).
  - **Reconstruction-quality gap:** Large gap between reconstructed samples (autoencoder pathway) and generated samples (sampling pathway) indicates sampling strategy issues.

- **First 3 experiments:**
  1. **Reconstruction sanity check:** Train only Phase 0 (unconditioned autoencoder), feed real waveforms through encoder-decoder, verify reconstruction loss converges and HVSR curves of reconstructed signals match originals. This validates the backbone before introducing conditioning.
  2. **Ablate sampling strategy:** Compare generation using (a) single latent code from one record, (b) k-sample averaging without noise, (c) k-sample averaging with noise. Measure f₀ distribution JSD against real data for each. This isolates the contribution of the sampling mechanism.
  3. **Station confusion probe:** Train on all 5 stations but evaluate generation for the pair with closest f₀ values (stations 0205 at 2.6 Hz and 4628 at 1.8 Hz). Generate samples conditioned on each, compute confusion matrix. High off-diagonal similarity indicates the model cannot distinguish stations beyond f₀; low off-diagonal suggests it captures additional site characteristics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can physics-based constraints or hybrid modeling approaches improve the physical consistency of TimesNet-Gen's generated waveforms beyond data-driven learning alone?
- **Basis in paper:** The authors state: "the model can be extended toward physics-guided or simulation-oriented earthquake generation."
- **Why unresolved:** The current architecture learns purely from data without incorporating physical laws or seismological theory, which may limit extrapolation to scenarios not well-represented in the training corpus.
- **What evidence would resolve it:** Comparative experiments where physics-informed loss terms or hybrid architectures (combining TimesNet-Gen with ground-motion equations) are evaluated against the purely data-driven baseline using standard engineering intensity measures.

### Open Question 2
- **Question:** How robust is station-conditioned generation for stations with very limited training records, and what is the minimum number of records needed per station for reliable site-specific generation?
- **Basis in paper:** The authors note: "most sites have only a limited number of available records, and traditional conditioning methods often fail to capture the underlying site-dependent patterns reliably."
- **Why unresolved:** The study used five stations with 31–110 records each, but real-world deployment may involve stations with far fewer recordings, and the generalization threshold remains unknown.
- **What evidence would resolve it:** Systematic experiments varying training set size per station (e.g., 5, 10, 20, 50 records) and measuring degradation in HVSR alignment and $f_0$ distribution similarity scores.

### Open Question 3
- **Question:** How do TimesNet-Gen generated waveforms perform in downstream seismic engineering tasks compared to real records or classical simulation methods?
- **Basis in paper:** The authors mention potential for "downstream seismic tasks such as phase picking, ground-motion parameter estimation, or event classification," but evaluate only spectral properties (HVSR, $f_0$), not downstream task performance.
- **Why unresolved:** Spectral fidelity does not guarantee utility for engineering applications such as structural response analysis or seismic hazard quantification, which require accurate intensity measures and temporal characteristics.
- **What evidence would resolve it:** Use generated waveforms as input to structural response simulations or as training data for ground-motion parameter estimation models, comparing outputs against those from real recordings.

## Limitations

- The model is evaluated on only 5 stations from a single regional database (AFAD), limiting generalization claims to other geological settings
- The k-sample averaging approach assumes smooth latent spaces without direct validation of latent space continuity
- The VAE baseline comparison is complicated by fundamental differences in input representation (time vs. frequency domain)

## Confidence

- **High confidence:** The core architectural claims about TimesNet's FFT-based period extraction and 2D reshaping mechanism are well-supported by the paper's descriptions and the original TimesNet work. The HVSR and f0 evaluation metrics are standard in site characterization and their implementation appears sound.
- **Medium confidence:** The station conditioning mechanism and its effectiveness are reasonably supported, though the paper could provide more rigorous ablation studies. The claim that TimesNet-Gen captures "site-specific" behavior beyond f0 is supported by confusion matrix analysis but would benefit from additional validation on stations with similar f0 but different site characteristics.
- **Low confidence:** The zero-shot experiment's implications for latent space structure are intriguing but not thoroughly validated. The claim that averaging produces meaningful intermediate station representations assumes latent space continuity that is not directly demonstrated.

## Next Checks

1. **Latent space continuity test:** Generate interpolated latent codes between station pairs and decode them. If the decoded waveforms show smooth transitions in spectral characteristics and f0 values, this validates the continuity assumption underlying the averaging approach.

2. **Cross-database validation:** Train TimesNet-Gen on AFAD data, then evaluate generation quality on a different seismic database (e.g., K-NET/KiK-net in Japan). Compare HVSR alignment and f0 distribution matching to assess generalization beyond the training region.

3. **Site characteristic disentanglement:** Select station pairs with similar f0 values but known differences in other site parameters (e.g., Vs30, depth to bedrock). Generate samples conditioned on each station and analyze whether the model captures differences beyond f0, such as duration characteristics or high-frequency content.