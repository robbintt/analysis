---
ver: rpa2
title: 'ExpliCIT-QA: Explainable Code-Based Image Table Question Answering'
arxiv_id: '2507.11694'
source_url: https://arxiv.org/abs/2507.11694
tags:
- table
- code
- reasoning
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExpliCIT-QA addresses the challenge of creating explainable systems
  for visual question answering over complex table images. It builds on the MRT approach,
  using a modular pipeline combining multimodal table understanding, chain-of-thought
  reasoning, automatic code generation, code execution, and natural language explanation.
---

# ExpliCIT-QA: Explainable Code-Based Image Table Question Answering

## Quick Facts
- **arXiv ID:** 2507.11694
- **Source URL:** https://arxiv.org/abs/2507.11694
- **Authors:** Maximiliano Hormazábal Lagos; Álvaro Bueno Sáez; Pedro Alonso Doval; Jorge Alcalde Vesteiro; Héctor Cerezo-Costas
- **Reference count:** 30
- **Primary result:** 27.69% average accuracy on TableVQA-Bench using Qwen3-4B, compared to 60.7% by GPT-4V+GPT-4

## Executive Summary
ExpliCIT-QA addresses the challenge of creating explainable systems for visual question answering over complex table images. It builds on the MRT approach, using a modular pipeline combining multimodal table understanding, chain-of-thought reasoning, automatic code generation, code execution, and natural language explanation. The system uses vision models to extract structured table data, LLMs to generate reasoning steps, and Python/Pandas scripts to compute answers, enabling full auditability of intermediate outputs. Evaluated on TableVQA-Bench, ExpliCIT-QA achieved 27.69% average accuracy across four subdatasets using Qwen3-4B, compared to 60.7% by GPT-4V+GPT-4, with performance improving to 31.50% using Qwen3-14B. While accuracy lags behind state-of-the-art end-to-end models, ExpliCIT-QA offers superior interpretability, making it suitable for high-stakes domains like finance and healthcare where transparency is critical.

## Method Summary
ExpliCIT-QA is a five-step pipeline for explainable visual question answering over table images. It first uses a vision model (Qwen-2.5-VL) with chain-of-thought prompting to extract structured table data as CSV. The extracted table and question are then passed to a reasoning LLM (Qwen 3) to generate step-by-step logic. This reasoning is translated into Python/Pandas code by an automatic code generation module, which is executed in a Python environment with up to three retries for error handling. Finally, the code and execution result are converted into natural language explanations. The system prioritizes interpretability by making all intermediate outputs (extracted CSV, generated code, reasoning steps) available for inspection.

## Key Results
- Achieved 27.69% average accuracy on TableVQA-Bench using Qwen3-4B, significantly below GPT-4V+GPT-4's 60.7%
- Performance improved to 31.50% using Qwen3-14B, showing model size impact
- Extremely low accuracy on VTabFact (under 4.9%) and FinTabNetQA (2-5%) due to complex layout challenges
- All intermediate outputs (CSV, code, reasoning) are available for auditability and explanation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deconstructing the TableVQA process into a sequential pipeline allows for granular inspection where end-to-end models fail.
- **Mechanism:** The system separates visual parsing (Table Understanding), logical planning (Reasoning), and calculation (Code Execution). By forcing the model to externalize intermediate states—specifically the extracted CSV and the generated Python code—the system exposes failure points that would otherwise be hidden in a black-box's latent space.
- **Core assumption:** The accuracy of the final answer depends less on a single model's internal weights and more on the successful execution of distinct, verifiable sub-tasks.
- **Evidence anchors:**
  - [abstract] "ExpliCIT-QA follows a modular design... The system is built for transparency and auditability: all intermediate outputs... are available for inspection."
  - [section 3.4] "Generated code is executed in a Python environment... It has a max_tries parameter set to 3... [for] error feedback."
  - [corpus] Related work (e.g., "Reasoning by Commented Code") supports the general efficacy of decomposition but does not confirm this specific 5-step architecture.
- **Break condition:** If the Multimodal Table Understanding (Step 1) fails to normalize a complex layout (e.g., merged cells) into a standard CSV, the subsequent reasoning and code steps will operate on malformed data, rendering the final answer incorrect despite executable code.

### Mechanism 2
- **Claim:** Offloading arithmetic and logical operations to a deterministic Python interpreter reduces hallucination errors common in LLMs.
- **Mechanism:** Instead of predicting the next token (the answer), the LLM predicts a syntactically correct program (Pandas code). This leverages the Program-aided Language Models (PAL) paradigm, where the formal logic of the code constrains the solution space, preventing the model from "guessing" numbers.
- **Core assumption:** The reasoning model (Qwen 3) can reliably translate natural language logic into syntactically correct Python/Pandas syntax.
- **Evidence anchors:**
  - [section 3.3] "The final answer is not just whatever the LLM guesses, but is the output of a deterministic computation, eliminating occasional arithmetic or logical mistakes made by LLMs."
  - [section 2.4] "The code serves as a formal specification of the reasoning eliminating ambiguity and arithmetic mistakes by delegating those to a Python/SQL interpreter system."
  - [corpus] "General Table Question Answering via Answer-Formula Joint Generation" reinforces that formula/code-based approaches are standard for improving complex reasoning.
- **Break condition:** If the generated code contains syntax errors that exceed the retry limit (3 tries) or logical errors that execute without crashing (e.g., summing the wrong column), the mechanism fails to correct the model's underlying misunderstanding.

### Mechanism 3
- **Claim:** Chain-of-Thought (CoT) prompting applied to visual inputs aids in restructuring complex table layouts before data extraction.
- **Mechanism:** The system uses a Vision-LLM (Qwen-2.5-VL) not just to OCR, but to generate a "to-do list" of spatial reasoning steps. This forces the model to explicitly plan how to handle merged cells or irregular schemas before attempting to serialize the data into a CSV format.
- **Core assumption:** The VLM possesses sufficient spatial reasoning capabilities to decompose visual complexity into sequential textual instructions.
- **Evidence anchors:**
  - [section 3.1] "We decompose this step into two substeps. First we use the VLLM spatial reasoning to generate a CoT of the 'to-do inner steps'... [then] use the 'to-do list'... to generate a csv like data."
  - [corpus] Corpus evidence on CoT for *visual* layout restructuring specifically is limited/weak compared to textual CoT.
- **Break condition:** If the table image is too ambiguous or low-resolution for the VLM to resolve spatial relationships, the generated "to-do list" will be flawed, leading to the extraction of a structurally unsound table.

## Foundational Learning

- **Concept: Program-Aided Language Models (PAL)**
  - **Why needed here:** The core of the architecture relies on the LLM generating executable Python/Pandas code rather than text. Engineers must understand how to prompt for code and how to safely execute it.
  - **Quick check question:** Can you distinguish between a standard text completion prompt and a prompt designed to output a valid Python function signature?

- **Concept: Visual Document Understanding (VDU)**
  - **Why needed here:** Step 1 requires transforming an image of a table into a structured dataframe. This requires knowledge of how VLMs handle spatial coordinates and text extraction (OCR-free vs. OCR-based).
  - **Quick check question:** How does a VLM typically represent a table's 2D structure within a 1D token stream (e.g., markdown, HTML, or specialized tokens)?

- **Concept: Error Feedback Loops**
  - **Why needed here:** The system includes a retry mechanism (Section 3.4) where execution errors are fed back to the coder. Understanding how to format stack traces as prompts is crucial for debugging this pipeline.
  - **Quick check question:** If a generated script throws a `KeyError`, how would you construct the prompt to help the model self-correct without confusing the previous instruction?

## Architecture Onboarding

- **Component map:** Input: Table Image + Question -> Visual Encoder (Qwen-2.5-VL) -> CSV Data -> Reasoner (Qwen 3) -> CoT Steps -> Coder (Qwen 3) -> Python Script -> Executor -> Result -> Explainer -> Natural Language Text

- **Critical path:** The **Multimodal Table Understanding (Step 1)** is the primary bottleneck. As noted in the Discussion, the accuracy drops significantly (e.g., on VTabFact and FinTabNetQA) when visual parsing fails to handle complex layouts. If the CSV is malformed, no amount of code reasoning can recover the correct answer.

- **Design tradeoffs:**
  - **Latency vs. Explainability:** The 5-step pipeline introduces significant latency compared to a single-pass end-to-end model, traded for the ability to audit the intermediate CSV and code.
  - **Precision vs. Recall in Parsing:** The system attempts to "replicate and redundant data" to flatten complex layouts, prioritizing machine readability over preserving the exact visual schema.

- **Failure signatures:**
  - **Format Mismatch:** The system may compute the correct numerical value (e.g., `44517`) but fail the benchmark due to strict string matching against the ground truth (e.g., `$44,517`).
  - **VTabFact Drop:** Extremely low accuracy (~2-5%) on fact-verification tasks suggests the code-based approach struggles with semantic alignment or boolean verification compared to numeric retrieval.

- **First 3 experiments:**
  1. **Unit Test the Visual Parser:** Isolate Step 1. Feed diverse table images (simple vs. merged cells) to Qwen-2.5-VL and manually inspect the generated CSV structure to establish a baseline extraction error rate.
  2. **Code Execution Sandbox:** Validate the "max_tries=3" feedback loop. Intentionally introduce errors into the generated code to verify that the system correctly catches the exception, feeds it back to the LLM, and generates a fix.
  3. **Ablation on CoT:** Run the pipeline with the CoT step disabled (Image -> Direct Code Generation) to measure the performance delta attributable to the explicit reasoning step.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the ExpliCIT-QA pipeline be adapted to handle semantic alignment tasks, such as fact verification, which currently underperform with code-based reasoning?
- **Basis in paper:** [explicit] The authors note that VTabFact accuracy was extremely low (under 4.9%) because "fact verification tasks... are less compatible with our current code-based reasoning approach."
- **Why unresolved:** The system translates reasoning into executable Python/Pandas code, a paradigm better suited for arithmetic and logical filtering than for semantic truth verification.
- **What evidence would resolve it:** A modified version of the pipeline that utilizes semantic similarity checks or non-arithmetic verification steps, demonstrating significantly improved accuracy on the VTabFact subset.

### Open Question 2
- **Question:** How can the internal coherence of the Chain-of-Thought (CoT) reasoning and the fidelity of the generated explanations be quantitatively measured?
- **Basis in paper:** [explicit] Section 6 states that it is a priority "to be able to measure the internal coherence of the CoT and the fidelity of the generated explanations."
- **Why unresolved:** Current evaluation relies on final answer accuracy (exact match), which does not verify if the intermediate reasoning steps logically justify the result or if the explanation faithfully reflects the executed code.
- **What evidence would resolve it:** The development of a specific evaluation metric or benchmark protocol that scores the logical consistency between the generated natural language reasoning and the corresponding Python script.

### Open Question 3
- **Question:** Do intermediate graph representations improve the extraction accuracy of tables with complex layouts compared to the current CSV-like serialization?
- **Basis in paper:** [explicit] The authors list "intermediate representations such as graphs to facilitate image conversion to coherent data structures" as a specific avenue for future work.
- **Why unresolved:** The current methodology struggles with "irregular layouts such as multilevel headers, merged cells and footnotes" (Section 5), leading to cell identification errors.
- **What evidence would resolve it:** An ablation study replacing the CSV-based table understanding module with a graph-based module, showing improved performance on the FinTabNetQA subset.

## Limitations

- **Visual parsing bottleneck:** The system shows dramatically reduced performance on FinTabNetQA and VTabFact datasets (2-5% accuracy) due to struggles with complex layouts and fact verification tasks.
- **Exact-match evaluation barrier:** Minor formatting differences between generated and ground truth answers (e.g., "$44,517" vs "44517") result in incorrect scores despite correct numerical computation.
- **Model dependency:** Reliance on specific Qwen model variants without ablation studies on alternative VLMs or LLMs limits generalizability of results.

## Confidence

- **High Confidence:** The mechanism of using code execution to eliminate LLM arithmetic errors is well-supported by established Program-aided Language Models literature and directly validated through the error feedback loop described in Section 3.4.
- **Medium Confidence:** The modular decomposition approach providing superior interpretability is logically sound but not empirically proven to be the sole cause of performance gaps versus end-to-end models, as the paper doesn't include direct ablation comparisons.
- **Low Confidence:** The effectiveness of visual Chain-of-Thought for complex table layout restructuring lacks strong empirical validation in the paper, with only qualitative description in Section 3.1 and limited evidence from the related work corpus.

## Next Checks

1. **Extract and Evaluate Visual Parsing Accuracy:** Isolate the Multimodal Table Understanding module by testing it on a diverse set of table images from all four subdatasets, measuring CSV extraction accuracy separately from downstream reasoning to quantify the exact contribution of visual parsing errors to overall performance degradation.

2. **Format Normalization Impact Analysis:** Implement and test the "relieved accuracy" normalization (removing currency symbols, commas, etc.) on the exact-match evaluation to determine whether the reported accuracy gaps are primarily due to formatting mismatches rather than substantive reasoning errors.

3. **Alternative VLM Ablation Study:** Replace Qwen-2.5-VL with alternative visual language models (e.g., GPT-4V, Gemini Pro Vision) in the Table Understanding step while keeping all other components constant to isolate the contribution of the visual parsing model to the overall accuracy differences.