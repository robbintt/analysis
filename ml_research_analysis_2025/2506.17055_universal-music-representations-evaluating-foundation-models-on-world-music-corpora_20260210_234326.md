---
ver: rpa2
title: Universal Music Representations? Evaluating Foundation Models on World Music
  Corpora
arxiv_id: '2506.17055'
source_url: https://arxiv.org/abs/2506.17055
tags:
- music
- across
- audio
- foundation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether foundation models can achieve universal
  music representations across diverse cultural traditions. The authors present a
  comprehensive assessment of five state-of-the-art audio foundation models across
  six musical corpora spanning Western popular, Greek, Turkish, and Indian classical
  traditions.
---

# Universal Music Representations? Evaluating Foundation Models on World Music Corpora

## Quick Facts
- arXiv ID: 2506.17055
- Source URL: https://arxiv.org/abs/2506.17055
- Reference count: 0
- Five state-of-the-art audio foundation models achieve state-of-the-art performance on 5 out of 6 world music corpora spanning Western, Greek, Turkish, and Indian traditions

## Executive Summary
This paper evaluates whether foundation models can achieve universal music representations across diverse cultural traditions. The authors present a comprehensive assessment of five state-of-the-art audio foundation models across six musical corpora spanning Western popular, Greek, Turkish, and Indian classical traditions. They employ three complementary methodologies: probing (frozen feature extraction with trainable classifier), supervised fine-tuning (adapting 1-2 layers), and multi-label few-shot learning for low-resource scenarios. The evaluation reveals that while larger models generally outperform on non-Western music, performance consistently declines for culturally distant traditions, particularly in few-shot learning scenarios.

## Method Summary
The study evaluates five foundation models (MERT-95M, MERT-330M, CLAP-Music, CLAP-Music&Speech, Qwen2-Audio) across six datasets using three methodologies. Probing extracts frozen representations with an MLP classifier (200 epochs, 5 seeds). Supervised fine-tuning adapts 1-2 layers with AdamW optimizer and cosine scheduling. Multi-label few-shot learning employs optimized LC-Protonets with unique prototype dictionaries for 10-100× speedup. Models are evaluated on ROC-AUC, mAP, and macro/micro F1 metrics. Hardware constraints limited SFT to 1-2 layers across models.

## Key Results
- Larger models (MERT-330M, Qwen2-Audio) generally outperform smaller ones on non-Western music corpora
- Performance declines consistently for culturally distant traditions, especially in few-shot learning scenarios
- Probing often matches or exceeds supervised fine-tuning performance across all settings
- Optimized few-shot learning achieves 10-100× speedup while maintaining identical classification results
- State-of-the-art performance achieved on 5 out of 6 datasets

## Why This Works (Mechanism)
Foundation models trained on diverse audio data encode generalizable musical features that transfer across cultural boundaries. The frozen feature extraction (probing) approach leverages pre-trained representations without additional training, while targeted fine-tuning adapts model parameters to specific cultural contexts. The multi-label few-shot learning methodology enables effective adaptation to low-resource scenarios by optimizing prototype-based classification with unique label dictionaries.

## Foundational Learning
- **Audio foundation models**: Pre-trained architectures for general audio understanding, needed for cross-cultural music analysis; quick check: verify model weights load correctly from HuggingFace
- **Probing methodology**: Frozen feature extraction with trainable classifier, needed to assess pre-trained knowledge; quick check: confirm frozen layers remain unchanged during training
- **Supervised fine-tuning**: Parameter adaptation for domain-specific tasks, needed to evaluate cultural adaptation capability; quick check: verify gradient flow only to specified layers
- **Few-shot learning**: Learning from limited examples, needed for low-resource world music datasets; quick check: confirm prototype dictionary contains unique entries only
- **Multi-label classification**: Predicting multiple tags per sample, needed for realistic music tagging scenarios; quick check: verify sigmoid outputs in range [0,1]

## Architecture Onboarding
- **Component map**: Audio input -> Foundation model -> Representation extraction -> Classifier head (MLP/ProtoNet)
- **Critical path**: Input preprocessing -> Model inference -> Feature aggregation -> Classification
- **Design tradeoffs**: Large models offer better performance but higher computational cost; probing is efficient but may underfit; fine-tuning risks overfitting on small datasets
- **Failure signatures**: OOM errors with large models, underperformance with cultural distance, slow inference in few-shot scenarios
- **First experiments**: 1) Load all five models and verify basic inference works, 2) Run probing on MagnaTagATune to establish baseline performance, 3) Test LC-Protonets optimization on small dataset to verify 10-100× speedup claim

## Open Questions the Paper Calls Out
- Can LoRA and broader supervised fine-tuning achieve better cross-cultural adaptation than targeted 1-2 layer fine-tuning?
- How do foundation models perform on music understanding tasks beyond auto-tagging, such as mode estimation and raga recognition?
- What causes the unexpected performance inversion where MERT-95M outperforms MERT-330M on world music tagging?
- How effectively do current foundation models generalize to musical traditions absent from this evaluation, such as African and East Asian music?

## Limitations
- Evaluation focuses on classification metrics without qualitative analysis of cultural feature representation
- Input duration differences (30s vs 10s) across models may confound performance comparisons
- Limited to six corpora, leaving substantial global musical diversity unexplored
- No investigation of model behavior on hybrid musical traditions blending multiple cultural influences

## Confidence
- Representation extraction methodology: High confidence - exact layer combinations and averaging operations specified
- Larger models outperforming on non-Western music: Medium confidence - consistent effect but dataset-specific factors possible
- Probing matching or exceeding fine-tuning: Medium confidence - depends on specific fine-tuning strategy employed
- 10-100× speedup in few-shot learning: High confidence - technical implementation improvement with verifiable metrics

## Next Checks
1. Conduct qualitative analysis of model representations to identify how culturally-specific musical features are encoded across different foundation models
2. Test model performance on hybrid musical genres that blend Western and non-Western traditions to assess true cross-cultural generalization
3. Evaluate the impact of input duration differences (30s vs 10s) on model performance by standardizing input windows across all models while maintaining their architectural constraints