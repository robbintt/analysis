---
ver: rpa2
title: Evaluation of LLMs-based Hidden States as Author Representations for Psychological
  Human-Centered NLP Tasks
arxiv_id: '2503.00124'
source_url: https://arxiv.org/abs/2503.00124
tags:
- user
- states
- representations
- token
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically evaluates how different Transformer-based\
  \ language model (LM) representations\u2014including those from traditional models\
  \ (BERT, RoBERTa, GPT-2) and human language models (HaRT variants)\u2014perform\
  \ for predicting psychological human-centered NLP tasks. The study compares document,\
  \ wave, and user-level representations derived from various hidden state types (CLS,\
  \ last token, averaged tokens, and user states) to predict valence, arousal, empathy,\
  \ and distress."
---

# Evaluation of LLMs-based Hidden States as Author Representations for Psychological Human-Centered NLP Tasks

## Quick Facts
- **arXiv ID**: 2503.00124
- **Source URL**: https://arxiv.org/abs/2503.00124
- **Reference count**: 9
- **Primary result**: Averaged token embeddings outperform special tokens for predicting psychological attributes, with HuLMs improving performance when user context is included

## Executive Summary
This paper systematically evaluates how different Transformer-based language model representations perform for predicting psychological human-centered NLP tasks. The study compares document, wave, and user-level representations derived from various hidden state types to predict valence, arousal, empathy, and distress. Averaged token embeddings consistently outperformed other representations, particularly for document- and user-level predictions. While user states from HuLMs rarely outperformed averaged token representations on their own, their inclusion consistently improved the performance of token and document embeddings, making HuLMs the best-performing models overall. The results highlight that averaging token embeddings is the most effective approach for capturing both stable traits and changing states in human-centered NLP tasks.

## Method Summary
The paper evaluates psychological attribute prediction using hidden states from BERT, RoBERTa, GPT-2, and HaRT variants across document, wave, and user levels. For each document, embeddings are extracted using CLS, last token (LT), averaged tokens (AT), LTinsep, or user state (U) representations. Document embeddings are hierarchically averaged for wave and user levels following Classical Test Theory. Ridge regression with 10-fold stratified group cross-validation predicts valence, arousal, empathy, and distress using DLATK with alpha values ranging from 1e0 to 1e6. AT embeddings use the second-to-last layer for auto-encoders and last layer for others, while special tokens use the last layer.

## Key Results
- Averaged token embeddings consistently outperformed CLS and LT representations across all tasks and levels
- HuLM user states rarely outperformed averaged tokens alone but strengthened token/document embeddings when included
- HuLMs achieved the best overall performance by combining user-contextualized embeddings with hierarchical averaging
- Arousal prediction remained challenging with consistently low correlations (r < 0.36) across all models

## Why This Works (Mechanism)

### Mechanism 1
Averaging token embeddings captures psychological attributes more effectively than single special tokens because it preserves distributed signals across the semantic content of an entire document rather than compressing them into a bottleneck. Psychological constructs like valence and empathy are lexically and semantically distributed, making AT embeddings better at capturing nuanced, context-dependent details from each token rather than filtering them through summary positions.

### Mechanism 2
Explicit user-state vectors from Human Language Models serve as effective context providers for token embeddings, even if the vectors themselves are weak standalone predictors. HuLMs maintain a recurrent user state updated by author history, and these user-state-informed embeddings effectively capture the author's context, resulting in higher fidelity representations than models processing text in isolation.

### Mechanism 3
Stable trait-like attributes are best estimated by hierarchically averaging dynamic state representations. Trait attributes are statistically defined as the central tendency of fluctuating states over time (Classical Test Theory). By generating high-quality document-level embeddings and averaging them hierarchically to create user-level representations, the model smooths out situational noise and approximates stable traits.

## Foundational Learning

### Concept: Representation Pooling (CLS vs. Average vs. Last Token)
- **Why needed here**: The paper fundamentally compares how to extract a fixed-size vector from a variable-length Transformer output
- **Quick check question**: Why would averaging all tokens potentially lose less information about "arousal" than taking the final token?

### Concept: Classical Test Theory (Traits vs. States)
- **Why needed here**: The paper relies on the assumption that user-level labels are the aggregates of document-level labels
- **Quick check question**: If a user writes 10 documents, why is the average of the 10 embeddings considered a "trait" representation?

### Concept: HuLM (Human Language Model) Architecture
- **Why needed here**: Unlike standard LLMs, HuLMs inject a persistent user-state vector into the attention mechanism, changing the nature of the embeddings
- **Quick check question**: How does the HaRT model's handling of a document differ from BERT's when the same user writes multiple documents?

## Architecture Onboarding

### Component map
Input: Temporally ordered user documents -> Backbone: Transformer (BERT/RoBERTa/GPT-2) or Recurrent Transformer (HaRT) -> Extraction Layer: Last layer (for special tokens) or Second-to-last layer (for AT in auto-encoders) -> Pooling: CLS (auto-encoders), LT (autoregressive), or AT (Average Tokens) -> Aggregator: Hierarchical averaging for User/Wave levels -> Predictor: Ridge Regression (Linear probe)

### Critical path
Tokenize -> Forward Pass -> Extract Hidden States (Layer Selection) -> Pool (AT/CLS/LT) -> Aggregate (if user-level) -> Ridge Regression

### Design tradeoffs
- AT vs. CLS: AT has higher dimensionality ($N \times D$ reduction) and computational overhead but yields higher correlation with labels
- HaRT vs. RoBERTa: HaRT requires user-grouped data and history tracking (complexity) but offers user-contextualized embeddings; RoBERTa is stateless but simpler
- Layer Selection: Auto-encoders perform better with AT from the second-to-last layer; special tokens prefer the last layer

### Failure signatures
- Arousal prediction: Consistently low correlation across all models (r < 0.36), indicating the feature space struggles to capture high-energy states
- Direct User State usage: Using the HuLM's user-state vector ($U$) alone as features results in poor performance (e.g., Valence r=0.59 vs 0.73 for AT), failing to beat simpler baselines

### First 3 experiments
1. **Baseline Pooling Check**: Run RoBERTa on the document-level task, comparing CLS (last layer) vs. Averaged Tokens (second-to-last layer) to verify the AT advantage
2. **Hierarchical Validation**: Take the best document embeddings from Experiment 1, average them by user, and predict user-level traits to confirm the hierarchical aggregation hypothesis
3. **Context Ablation**: Compare HaRT (user-state-informed) AT embeddings against GPT-2 (stateless) AT embeddings on the Empathy task to quantify the value of added user context

## Open Questions the Paper Calls Out

### Open Question 1
Why do averaged token embeddings (AT) consistently outperform single special tokens (CLS, LT) in capturing stable traits and changing states? The authors state they "do not have evidence to prove the reasoning" for AT's superiority over compression into single tokens. The study is empirical; the theoretical explanation regarding information loss during compression was not tested. Probing experiments analyzing information density and distribution across layers for AT versus special tokens would resolve this.

### Open Question 2
How can the natural user state representations in HuLMs be improved to function as effective standalone representations? The authors note user states are rarely the best alone and call for future work to "improve the natural representations of user states." Current HuLM architectures produce user states that underperform compared to hierarchical averaging. Architectural modifications to recurrent user state updates that increase correlation with ground-truth traits without relying on token averaging would resolve this.

### Open Question 3
Do the performance benefits of HuLM-based representations hold when scaling model parameters to sizes comparable to modern LLMs? The authors acknowledge using smaller models (GPT-2/BERT size) due to the lack of larger HuLMs and encourage exploring this scaling. The study is limited to smaller architectures (HaRT), leaving the scaling dynamics of user states unknown. Pre-training and evaluating larger Human Language Models on the same psychological tasks would resolve this.

## Limitations
- Primary dataset (DS4UD) is not publicly available due to privacy constraints, limiting reproducibility of valence and arousal results
- Model training details lack specifications for HaRT variants, particularly user state initialization and update mechanisms
- Arousal prediction performance remains consistently low (r < 0.36) across all models, suggesting fundamental limitations in feature space or annotation noise

## Confidence

**High Confidence**: Averaged token embedding approach consistently outperforming special token representations is well-supported by results across multiple models and tasks. The mechanism (preserving distributed signals vs. information bottleneck) is theoretically sound and empirically validated.

**Medium Confidence**: Claim that user-state vectors from HuLMs strengthen token/document embeddings when included, despite being poor standalone predictors, is supported but requires more direct ablation studies. The paper shows improved performance but doesn't isolate the specific contribution of the user state conditioning mechanism.

**Medium Confidence**: Hierarchical averaging approach for deriving trait-like representations from state-level embeddings follows Classical Test Theory principles, but the paper doesn't validate whether the temporal span and frequency of documents provide sufficient data to approximate stable traits rather than noisy aggregates.

## Next Checks

1. **User State Ablation Study**: Replicate the empathy prediction task using GPT-2 AT embeddings, then compare against HaRT AT embeddings with the user state vector removed (i.e., use only the token embeddings). This will isolate the specific contribution of user-state conditioning versus the token embeddings themselves.

2. **Layer Sensitivity Analysis**: For BERT and RoBERTa, systematically compare averaged token embeddings from layers 10, 11 (second-to-last), and 12 (last) on the WASSA dataset. This will verify whether the second-to-last layer selection is optimal or if the improvement is consistent across multiple layers.

3. **Temporal Stability Test**: Using the DS4UD dataset (if accessible), analyze whether user-level embeddings computed from earlier vs. later waves show stronger correlations with ground truth traits. This would validate whether the hierarchical averaging approach captures stable traits rather than averaging transient states.