---
ver: rpa2
title: 'ECO: Quantized Training without Full-Precision Master Weights'
arxiv_id: '2601.22101'
source_url: https://arxiv.org/abs/2601.22101
tags:
- weights
- training
- master
- quantization
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ECO (Error-Compensating Optimizer), a method
  for training quantized neural networks without requiring high-precision master weights.
  The core idea is to apply gradient updates directly to quantized weights and inject
  the resulting quantization error into the optimizer's momentum buffer, creating
  an error-feedback loop that compensates for lost updates without additional memory
  overhead.
---

# ECO: Quantized Training without Full-Precision Master Weights

## Quick Facts
- arXiv ID: 2601.22101
- Source URL: https://arxiv.org/abs/2601.22101
- Authors: Mahdi Nikdan; Amir Zandieh; Dan Alistarh; Vahab Mirrokni
- Reference count: 40
- Primary result: Achieves near-lossless accuracy compared to master-weight baselines while reducing static memory usage by up to 25%

## Executive Summary
This paper presents ECO (Error-Compensating Optimizer), a method for training quantized neural networks without requiring high-precision master weights. The core innovation is applying gradient updates directly to quantized weights and injecting the resulting quantization error into the optimizer's momentum buffer, creating an error-feedback loop that compensates for lost updates without additional memory overhead. Theoretical analysis shows that ECO converges to a constant-radius neighborhood of the optimum, with a noise floor that remains bounded as the learning rate decays, unlike naive approaches which suffer from error that grows inversely with learning rate.

## Method Summary
ECO operates by quantizing weights directly and applying updates to these quantized values without maintaining a separate high-precision copy. During training, after computing gradients on quantized weights, the optimizer applies the update to get a tentative new weight value. This value is then quantized, and the quantization error (difference between the unquantized and quantized values) is injected back into the momentum buffer with a specific coefficient. This error-feedback mechanism allows the optimizer to recover the lost update over time, effectively compensating for the quantization without needing additional memory for master weights.

## Key Results
- ECO achieves near-lossless accuracy compared to master-weight baselines while reducing static memory usage by up to 25%
- Across scaling law studies on small transformers (30-800M parameters), pre-training of Gemma-3 1B and SMoE 2.1B models, and fine-tuning of DeepSeek-MoE-16B, ECO matches or nearly matches master-weight baseline performance
- Theoretical analysis proves ECO converges to a constant-radius neighborhood of the optimum, with noise floor bounded as learning rate decays

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ECO converges to a constant-radius neighborhood of the optimum as learning rate decays, preventing the $1/\eta$ error explosion seen in naive master-weight removal.
- **Mechanism:** ECO injects the quantization error $e_{t+1} = \tilde{\theta}_{t+1} - \hat{\theta}_{t+1}$ into the momentum buffer with coefficient $\alpha = \frac{1}{\eta}(1 - 1/\beta)$. This creates an error-feedback loop that carries the lost update forward to subsequent steps without additional memory, allowing the effective update to be recovered over time.
- **Core assumption:** The heuristic approximation $e_t \approx e_{t+1}$ holds sufficiently well (errors change slowly between steps), and with stochastic rounding, the quantization error is zero-mean with bounded variance.
- **Evidence anchors:**
  - [abstract]: "Theoretical analysis shows that ECO converges to a constant-radius neighborhood of the optimum, with a noise floor that remains bounded as the learning rate decays, unlike naive approaches which suffer from error that grows inversely with learning rate."
  - [section 3.3.5]: "As $\eta \to 0$, the noise floor $\sigma^2_{quant}$ becomes $4L^2\sigma^2/(1-\beta)^2$"
  - [corpus]: Weak direct supportâ€”corpus mentions error compensation in quantization contexts but does not validate ECO's specific injection formula.
- **Break condition:** If consecutive quantization errors become decorrelated (e.g., due to highly variable learning rates or unstable training dynamics), the $e_t \approx e_{t+1}$ approximation fails, and convergence guarantees may degrade.

### Mechanism 2
- **Claim:** Eliminating master weights reduces static memory by approximately 25% without significant accuracy loss.
- **Mechanism:** By applying updates directly to quantized weights (FP8 or INT4) and using the optimizer's existing momentum buffer to store error information, ECO removes the need for a separate FP32 master-weight buffer. This is particularly beneficial for SMoE models where parameter memory dominates.
- **Core assumption:** The forward and backward passes can operate effectively on quantized weights without destabilizing gradient computation.
- **Evidence anchors:**
  - [abstract]: "ECO achieves near-lossless accuracy compared to master-weight baselines while reducing static memory usage by up to 25%."
  - [section 4.4]: "Reducing master weight precision from FP32 to FP8 therefore lowers peak memory consumption from 12 bytes per parameter to 9, a reduction of approximately 25%."
  - [corpus]: Corpus papers discuss quantization for memory efficiency generally but do not specifically validate ECO's master-weight-free approach.
- **Break condition:** If quantization granularity is too coarse (e.g., aggressive INT4), gradient signal may be corrupted, leading to divergence or significant accuracy loss.

### Mechanism 3
- **Claim:** Stochastic rounding (SR) is critical for optimal ECO performance; deterministic rounding leads to higher noise floors.
- **Mechanism:** SR provides unbiased quantization error, allowing the momentum buffer to average out noise over time. Deterministic round-to-nearest introduces systematic bias that accumulates in momentum, resulting in a larger stationary error bound ($O(L^2\delta^2/(1-\beta)^2)$ vs. $O(L^2\sigma^2/(1-\beta)^2)$).
- **Core assumption:** Stochastic rounding is available in hardware or can be efficiently implemented.
- **Evidence anchors:**
  - [section 3.3.4]: "Assuming $\sigma \approx \delta$, the deterministic bound is significantly larger due to the $(1-\beta)^{-2}$ dependence"
  - [section 5]: "Both theory and experiments indicate that ECO performs best with stochastic rounding (SR)... ECO relies on the unbiasedness of SR for its strongest guarantees."
  - [corpus]: Weak direct evidence; corpus mentions stochastic rounding generally but not specifically for ECO's error injection.
- **Break condition:** If hardware lacks efficient SR support and RTN must be used, expect higher noise floors and potentially reduced accuracy compared to master-weight baselines.

## Foundational Learning

- **Concept: Quantization and Rounding Modes**
  - **Why needed here:** ECO operates on quantized weights; understanding the difference between round-to-nearest (biased, deterministic) and stochastic rounding (unbiased, probabilistic) is essential for interpreting convergence behavior.
  - **Quick check question:** Given a value 2.3 quantizing to integers, how would RTN vs SR differ in their output and expected error?

- **Concept: Momentum-based Optimization (SGDM/Adam)**
  - **Why needed here:** ECO injects quantization error directly into the momentum buffer; you must understand how momentum accumulates gradients to see why this recovers lost updates.
  - **Quick check question:** In SGDM with $\beta = 0.9$, approximately what fraction of the current update comes from gradients versus historical momentum?

- **Concept: Error Feedback in Distributed/Compressed Optimization**
  - **Why needed here:** ECO adapts error-feedback principles (traditionally used for gradient compression) to weight quantization, reusing momentum as the error buffer.
  - **Quick check question:** Why does storing error feedback traditionally require extra memory, and how does ECO avoid this?

## Architecture Onboarding

- **Component map:** Quantized weight storage $\hat{\theta}_t$ (FP8/INT4, no master weights) -> Optimizer state (Momentum buffer $m_t$ and $v_t$ for Adam) -> Error injection module (computes $e_{t+1} = \tilde{\theta}_{t+1} - \hat{\theta}_{t+1}$ and applies injection rule) -> Quantization function $q(\cdot)$ (supports both RTN and SR modes)

- **Critical path:**
  1. Forward/backward pass with quantized weights $\hat{\theta}_t$
  2. Compute gradient $g_t = \nabla f(\hat{\theta}_t)$
  3. Update momentum: $\tilde{m}_{t+1} = \beta \hat{m}_t + (1-\beta)g_t$
  4. Compute tentative update: $\tilde{\theta}_{t+1} = \hat{\theta}_t - \eta \tilde{m}_{t+1}$
  5. Quantize: $\hat{\theta}_{t+1} = q(\tilde{\theta}_{t+1})$
  6. Compute error: $e_{t+1} = \tilde{\theta}_{t+1} - \hat{\theta}_{t+1}$
  7. Inject into momentum: $\hat{m}_{t+1} = \tilde{m}_{t+1} + \frac{1}{\eta}(1 - \frac{1}{\beta})e_{t+1}$

- **Design tradeoffs:**
  - **SR vs RTN:** SR provides stronger guarantees but may not be hardware-supported; RTN works but has higher noise floor
  - **Quantization precision:** Lower bits (INT4) increase memory savings but risk divergence; FP8 is safer for pre-training
  - **Granularity:** Row-wise scaling is standard; finer granularity (group-wise) may improve accuracy at cost of complexity

- **Failure signatures:**
  - **Divergence with naive no-MW:** Training loss explodes or plateaus at high values (see Table 1 "dvg" entries)
  - **Elevated noise floor with RTN:** Converges but final loss higher than master-weight baseline by noticeable margin
  - **Instability at low learning rates:** Without ECO, error grows as $1/\eta$; with ECO, should remain bounded

- **First 3 experiments:**
  1. **Scaling law validation:** Train 30M-800M parameter transformers with FP8 ECO+SR vs BF16 master-weight baseline; plot validation loss vs model size to confirm near-lossless behavior
  2. **Ablation on rounding mode:** Compare ECO+RTN vs ECO+SR on Gemma-3 1B to quantify noise floor difference
  3. **Memory-accuracy Pareto:** Measure static memory usage and final validation loss for FP8 w/ MW, FP8 w/o MW+SR, and FP8 w/o MW ECO+SR across batch sizes to verify frontier shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ECO's performance gap with round-to-nearest (RTN) quantization be closed through alternative error-feedback mechanisms, achieving parity with stochastic rounding baselines?
- Basis in paper: [explicit] "Both theory and experiments indicate that ECO performs best with stochastic rounding (SR)... ECO still outperforms naive approaches but can exhibit a higher noise floor [with RTN]... This introduces a slight accuracy ceiling relative to the best RTN-based master-weight baselines."
- Why unresolved: The bias in deterministic RTN quantization violates the zero-mean error assumption (Assumption 3.2), leading to a noise floor with $(1-\beta)^{-2}$ dependence versus $(1-\beta^2)^{-1}$ for SR, but no bias-correction mechanism is proposed.
- What evidence would resolve it: A modified ECO variant that explicitly models or corrects RTN bias, tested against master-weight RTN baselines on the Gemma-3 1B and SMoE 2.1B benchmarks.

### Open Question 2
- Question: Can ECO be combined with optimizer state quantization methods (e.g., 4-bit or 8-bit momentum buffers) without degrading convergence?
- Basis in paper: [explicit] "ECO targets a different bottleneck: the master-weight copy. This provides memory savings comparable to optimizer-state quantization, while remaining largely unexplored."
- Why unresolved: ECO injects quantization error directly into momentum buffers; if momentum is itself quantized, the error-feedback signal could be corrupted, potentially breaking the bounded-momentum guarantee (Lemma 3.7).
- What evidence would resolve it: Experiments combining ECO with quantized momentum (e.g., FP8 or INT4 first moment) on the scaling law benchmarks, measuring validation loss degradation versus memory savings.

### Open Question 3
- Question: Does the heuristic approximation $e_t \approx e_{t+1}$ degrade ECO's convergence under highly non-stationary training dynamics, such as rapid learning rate warmup or sharp gradient shifts?
- Basis in paper: [inferred] The heuristic is motivated by the observation that consecutive errors are typically close because momentum changes slowly, but Figure 2 only validates this on a single 30M parameter model. The theoretical analysis assumes the simplified injection rule without proving its robustness under violation of the approximation.
- Why unresolved: The exact injection rule (requiring storage of $e_t$) guarantees equivalence to master-weight SGDM (Appendix A), but the memory-free heuristic trades this guarantee for practicality without formal bounds on the approximation error's impact.
- What evidence would resolve it: Ablation studies on models with aggressive learning rate schedules, measuring divergence between the heuristic and exact injection, and theoretical bounds on approximation-induced error accumulation.

## Limitations

- ECO's convergence guarantees depend critically on the approximation $e_t \approx e_{t+1}$ holding across training steps, which may break down under highly variable learning rates or unstable training dynamics
- The theoretical analysis assumes stochastic rounding is available and effective; in practice, many hardware platforms only support deterministic round-to-nearest, leading to higher noise floors
- ECO's memory savings are most pronounced for models where parameter memory dominates; for smaller models or those with large activations, the 25% reduction may be less significant
- The experiments focus primarily on transformers; applicability to other architectures (CNNs, MLPs) remains to be validated

## Confidence

- **High confidence**: ECO converges to a constant-radius neighborhood of the optimum (theoretical proof is rigorous)
- **Medium confidence**: ECO achieves near-lossless accuracy compared to master-weight baselines (experimental evidence is strong but architecture-specific)
- **Low confidence**: The 25% memory reduction applies universally across all model sizes and architectures (empirical validation is limited to specific transformer variants)

## Next Checks

1. **Hardware validation**: Implement ECO with only deterministic round-to-nearest on a GPU/TPU platform and measure the actual noise floor degradation compared to SR on the same hardware
2. **Architecture generalization**: Apply ECO to ResNet-50/ImageNet training and measure convergence behavior and accuracy compared to FP32 master weights
3. **Mixed-precision scaling**: Experiment with ECO using different quantization granularities (group-wise vs row-wise scaling) and measure the Pareto frontier between memory usage and final accuracy