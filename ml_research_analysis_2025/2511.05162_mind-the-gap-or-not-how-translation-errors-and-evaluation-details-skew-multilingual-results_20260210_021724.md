---
ver: rpa2
title: Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew
  Multilingual Results
arxiv_id: '2511.05162'
source_url: https://arxiv.org/abs/2511.05162
tags:
- languages
- answer
- english
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers found significant performance gaps in multilingual
  math reasoning models across languages. Initial experiments suggested high-resource
  languages like French performed worse than low-resource ones, which was unexpected.
---

# Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results

## Quick Facts
- arXiv ID: 2511.05162
- Source URL: https://arxiv.org/abs/2511.05162
- Reference count: 40
- High-resource languages initially underperformed low-resource ones due to benchmark flaws

## Executive Summary
Researchers discovered that multilingual math reasoning models' performance gaps across languages were largely artifacts of translation errors and inconsistent evaluation methods rather than genuine capability differences. Initial experiments showed unexpectedly poor performance for high-resource languages like French compared to low-resource ones. Through systematic quality assurance of the MGSM benchmark, researchers corrected mistranslations and standardized answer extraction methods. After these corrections, performance gaps nearly disappeared, with most models achieving near-perfect accuracy across all languages. The study highlights the critical importance of clean, standardized test data for fair multilingual model evaluation.

## Method Summary
The researchers implemented a semi-automated quality assurance process to identify and correct translation errors in the MGSM benchmark. They focused on fixing mistranslations and ambiguous questions, then standardized answer parsing methods to account for language-specific formatting such as decimal separators. This systematic approach involved both automated detection of potential issues and human review to ensure corrections maintained the original problem's intent while making it linguistically accurate and evaluable across all target languages.

## Key Results
- Translation errors in the MGSM benchmark initially caused high-resource languages like French to underperform low-resource languages
- After corrections, performance gaps between languages nearly disappeared, with most models achieving near-perfect accuracy across all languages
- Standardized answer parsing accounting for language-specific formatting (e.g., decimal separators) was crucial for fair evaluation

## Why This Works (Mechanism)
The study demonstrates that multilingual model evaluation is highly sensitive to data quality and evaluation methodology. Translation errors can systematically disadvantage certain languages by introducing ambiguities or changing problem semantics. Inconsistent answer extraction methods compound these issues by failing to account for language-specific conventions. When these artifacts are removed through careful quality assurance, the underlying model capabilities become apparent, revealing that well-trained multilingual models can achieve comparable performance across languages when given properly translated and formatted problems.

## Foundational Learning

1. **Translation fidelity in benchmarks** - why needed: Ensures problems maintain semantic equivalence across languages; quick check: Expert linguists verify problem meaning preservation

2. **Language-specific formatting conventions** - why needed: Different languages use varying decimal separators, number formats, and measurement units; quick check: Cross-reference with official language style guides

3. **Answer extraction standardization** - why needed: Consistent evaluation requires uniform parsing of numerical answers across languages; quick check: Test parser with edge cases from each language

## Architecture Onboarding

**Component map:** Quality Assurance System -> Translation Correction Module -> Answer Parser -> Evaluation Engine

**Critical path:** Error detection → Human review → Translation correction → Answer format standardization → Model evaluation

**Design tradeoffs:** Automated detection provides scalability but requires human oversight to maintain semantic integrity; strict standardization ensures fairness but may not capture all linguistic nuances

**Failure signatures:** Inconsistent performance across languages, unexpected underperformance of high-resource languages, answer format mismatches

**3 first experiments:** 1) Run initial evaluation to identify performance gaps across languages, 2) Sample and manually review translated problems for quality issues, 3) Test answer parser with language-specific formatting variations

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Semi-automated correction process introduces potential subjectivity in error identification
- Findings based on a single math reasoning benchmark (MGSM) may not generalize to other multilingual tasks
- Focus on math word problems limits applicability to domains with more substantial language-specific conventions

## Confidence
- High confidence: Existence of translation errors and inconsistent answer extraction in original benchmark
- Medium confidence: Magnitude of performance improvement and claim that gaps "nearly disappeared"
- Medium confidence: Generalizability of findings beyond math word problems

## Next Checks
1. Independent replication of translation correction process by different researchers to verify consistency in error identification
2. Testing corrected benchmark across additional multilingual reasoning datasets to assess performance parity beyond math problems
3. Human evaluation of sample original and corrected translations to quantify impact of correction methodology on benchmark validity