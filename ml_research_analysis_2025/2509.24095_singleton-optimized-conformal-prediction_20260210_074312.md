---
ver: rpa2
title: Singleton-Optimized Conformal Prediction
arxiv_id: '2509.24095'
source_url: https://arxiv.org/abs/2509.24095
tags:
- socop
- ours
- prediction
- size
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to construct conformal prediction
  sets that prioritize producing singleton sets (unambiguous predictions) over minimizing
  average set size. The authors formulate an optimization problem combining singleton
  probability and expected set length, derive a nonconformity score from its Lagrangian
  relaxation, and show it reduces to finding the lower convex hull of K 2D points
  with O(K) complexity.
---

# Singleton-Optimized Conformal Prediction

## Quick Facts
- arXiv ID: 2509.24095
- Source URL: https://arxiv.org/abs/2509.24095
- Reference count: 40
- Primary result: Method increases singleton prediction set frequency by up to 20% vs. standard conformal scores with minimal impact on average set size

## Executive Summary
This paper introduces a method to construct conformal prediction sets that prioritize producing singleton sets (unambiguous predictions) over minimizing average set size. The authors formulate an optimization problem combining singleton probability and expected set length, derive a nonconformity score from its Lagrangian relaxation, and show it reduces to finding the lower convex hull of K 2D points with O(K) complexity. Their algorithm computes nested conformal prediction sets efficiently. Experiments on image classification (ImageNet variants, TissueMNIST) and LLM multiple-choice QA (MMLU) demonstrate their method (SOCOP) increases singleton frequency by up to 20% compared to standard scores (plug-in, RAPS, least ambiguous sets), with minimal impact on average set size. SOCOP also achieves strong adaptivity with low size-stratified coverage violations.

## Method Summary
SOCOP builds on split conformal prediction by optimizing for singleton prediction sets rather than minimal average set size. The method computes a nonconformity score by sorting predicted probabilities descending, constructing points P_k = (Γ_k, g_k) where Γ_k is the cumulative sum of probabilities and g_k encodes set size penalty, then finding the lower convex hull of these points. The edge slopes of this hull become the nonconformity scores. During calibration, nonconformity scores are computed for the calibration set and a (1-α)(1+1/n)-th quantile is taken. For prediction, labels with scores below this threshold are included in the set. The regularization parameter λ controls the singleton vs. size trade-off and is selected via a kneedle algorithm on a tuning set.

## Key Results
- SOCOP increases singleton frequency by up to 20% compared to plug-in, RAPS, and least ambiguous sets
- Maintains coverage above target (typically 0.95) across all tested datasets
- Achieves average set size comparable to or slightly better than LAS while dramatically reducing non-singleton sets
- Demonstrates strong adaptivity with low size-stratified coverage violations (SSCV)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing for singleton frequency can substantially reduce non-singleton probability (sometimes >20%) with minimal average set size increase.
- Mechanism: The nonconformity score r(x,y) = inf{τ ≥ 0 : y ∈ S_τ,λ} encodes a Lagrangian trade-off between singleton penalty and set size. As the Lagrange multiplier η varies, optimal prediction sets S_η,γ are nested top-j label sets, enabling a single scalar to control inclusion.
- Core assumption: The true conditional probability p(·|x) can be approximated well by model outputs p̂(·|x); exchangeability holds for calibration and test data.
- Evidence anchors:
  - [abstract] "SOCOP increases singleton frequency (sometimes by over 20%) compared to the above scores, with minimal impact on average set size."
  - [section 3.1, Table 1] Shows ResNet152-v2: SOCOP achieves P(size>1)=0.370 vs LAS 0.466 at near-identical avg size.
  - [corpus] Related work on efficient prediction sets (FMR ~0.40-0.53) addresses average size but not singleton optimization directly.
- Break condition: If model probability estimates are poorly calibrated or distribution shift violates exchangeability, the singleton vs. size trade-off curve may degrade.

### Mechanism 2
- Claim: The nonconformity score can be computed in O(K) time via a geometric lower convex hull construction.
- Mechanism: Points P_k = (Γ_k, g_k) where Γ_k = Σ_{i=1}^k γ_{y_i} and g_k = I(k>1) + λk form a 2D point set. The lower convex hull vertices correspond to optimal subset sizes; edge slopes η_i are precisely the nonconformity scores for labels between vertices.
- Core assumption: Labels can be sorted by decreasing probability; the cumulative sum structure yields a convex geometry.
- Evidence anchors:
  - [section 2.1] "Our algorithm will leverage the convex hull of P... The lower convex hull is the lower boundary of this polygon."
  - [Theorem 2.5] "The range of κ(η;γ) is precisely the set {v_0, v_1, ..., v_m} of indices of the vertices of the lower convex hull."
  - [corpus] No directly comparable O(K) convex hull approach for conformal scores found; most related work uses different scoring schemes.
- Break condition: If K is extremely large and probability sorting dominates, practical gains over O(K log K) may diminish; ties in probabilities require careful tie-breaking.

### Mechanism 3
- Claim: The λ→∞ and λ→0 limits recover known baselines (Least Ambiguous Sets and Pure Singleton), providing interpolation control.
- Mechanism: At λ→∞, the score converges to r_las(x, y_i) = 1/p̂(y_i|x). At λ=0, r_singleton(x, y_i) = I(i≥2)/(1 - p̂(y_1|x)), yielding either {y_1} or all Y.
- Core assumption: The regularization parameter λ meaningfully interpolates between the two extremes.
- Evidence anchors:
  - [Corollary 2.6] Explicitly derives both limits with proof in Appendix B.4.
  - [Figure 2] Shows empirical trade-off curve as λ varies from 0.01 to 1.0.
  - [corpus] Weak direct evidence; related papers focus on length optimization rather than singleton interpolation.
- Break condition: If λ selection via "kneedle" algorithm on tuning data is unstable or the tuning set is unrepresentative, the chosen operating point may be suboptimal.

## Foundational Learning

- Concept: **Split Conformal Prediction**
  - Why needed here: SOCOP builds on split conformal calibration; understanding quantile computation and exchangeability guarantees is prerequisite.
  - Quick check question: Given n calibration scores, what quantile yields 1-α coverage?

- Concept: **Convex Hull Algorithms (Monotone Chain)**
  - Why needed here: The O(K) score computation relies on lower convex hull construction via the monotone chain algorithm (Algorithm 2).
  - Quick check question: What is the time complexity of computing the convex hull of K points in 2D?

- Concept: **Lagrangian Duality for Discrete Optimization**
  - Why needed here: The singleton-optimized set S_η,γ emerges from relaxing a constrained discrete optimization; understanding separability is key.
  - Quick check question: How does Lagrangian relaxation convert a constrained problem into an unconstrained one?

## Architecture Onboarding

- Component map:
  - Pre-trained model p̂ -> Probability sorting -> Cumulative sum computation -> Lower convex hull construction -> Nonconformity score assignment -> Calibration quantile computation -> Prediction set construction

- Critical path:
  1. Probability sorting (O(K log K) or O(K) with counting sort for bounded K)
  2. Lower convex hull construction (O(K) via monotone chain)
  3. Threshold comparison against q̂ (O(m) where m ≤ K vertices)

- Design tradeoffs:
  - **λ selection**: Smaller λ → more singletons but larger average size; larger λ → smaller sets but more non-singletons. Use tuning set with kneedle algorithm or domain-specific preference.
  - **Empty set handling**: Occurs <0.01% in experiments; negligible practical impact but monitor in deployment.
  - **Tie-breaking**: When multiple labels have equal probability, hull construction handles collinear points; ensure consistent ordering.

- Failure signatures:
  - **Undercoverage**: Check exchangeability violation (distribution shift between calibration and test).
  - **Excessive large sets**: Model probabilities may be poorly calibrated; consider temperature scaling.
  - **High SSCV**: Method may not adapt well to hard examples; consider λ tuned for SSCV (Section 3.1.2).

- First 3 experiments:
  1. **Reproduce Table 1 on a subset**: Take ImageNet-Val with ResNet152-v2, 20K calibration / 20K evaluation, compare SOCOP vs LAS vs RAPS on coverage, avg size, P(size>1).
  2. **Sweep λ values**: Generate trade-off curves similar to Figure 2; verify knee point selection aligns with desired operating point.
  3. **Test on distribution shift**: Apply ImageNet-V2 protocol (Table 11) to assess robustness; expect higher variance and larger sets but maintain coverage.

## Open Questions the Paper Calls Out

- Can a nonconformity score be designed intrinsically for conditional coverage by deriving SOCOP from a conditional-aware optimization objective?
  - Basis in paper: Authors state: "a challenging but interesting theoretical direction is to design a nonconformity score intrinsically targeted for conditional coverage. This would likely entail retracing the derivation of SOCOP starting from a conditional-aware optimization objective, such as the one from Gibbs et al. [2025]."
  - Why unresolved: The current derivation starts from a marginal coverage objective, and it is unclear whether the geometric lower-convex-hull approach transfers to conditional formulations.
  - What evidence would resolve it: A modified SOCOP score with provable conditional coverage guarantees and empirical demonstration of improved size-stratified coverage.

- Can data-dependent selection of λ using the calibration set directly improve data efficiency while accounting for tuning bias?
  - Basis in paper: Authors note: "while our current protocol uses a separate tuning set to maintain validity, future work could investigate data-dependent selection of λ using the calibration set directly to improve data efficiency, while accounting for the resulting tuning bias [Zeng et al., 2025]."
  - Why unresolved: Using the same data for λ selection and calibration may violate exchangeability assumptions underlying coverage guarantees.
  - What evidence would resolve it: A debiasing procedure for λ selection that maintains valid coverage with theoretical guarantees and empirical validation.

- Can SOCOP be extended to label-conditional or Mondrian conformal prediction while preserving its singleton-optimizing properties?
  - Basis in paper: Authors state: "it would be of interest to extend this method to more advanced conformal prediction methods, such as label-conditional or Mondrian conformal prediction."
  - Why unresolved: These methods have different calibration procedures that partition data by label or region, and it is unclear how the nested set structure of SOCOP interacts with per-class quantile estimation.
  - What evidence would resolve it: Derivation of label-conditional SOCOP with O(K) algorithm and experiments showing maintained singleton-frequency gains across classes.

## Limitations

- The singleton optimization trade-off heavily depends on the quality of model probability estimates. If the model's softmax probabilities are poorly calibrated, the resulting nonconformity scores may not reflect true uncertainty, degrading the singleton vs. size trade-off.
- The method requires computing the convex hull of K points, which, while O(K), still depends on accurate probability sorting. For extremely large K, the practical benefit over O(K log K) methods may diminish.
- The λ selection via "kneedle" algorithm on tuning data introduces additional variability and potential instability if the tuning set is small or unrepresentative of the deployment environment.

## Confidence

- **High**: The O(K) complexity claim for nonconformity score computation is theoretically sound given the convex hull structure and supported by the algorithmic description and Theorem 2.5.
- **Medium**: The claim of 20% improvement in singleton frequency is supported by Table 1 results, but the magnitude may vary depending on model calibration and dataset characteristics.
- **Low**: The robustness of the method under significant distribution shift (e.g., ImageNet-V2) is uncertain, as the paper notes higher variance and potentially larger sets, but the underlying mechanism's effectiveness in such settings requires further validation.

## Next Checks

1. **Reproduce the Convex Hull Construction**: Implement Algorithm 2 and verify that the nonconformity scores computed match the edge slopes of the lower convex hull for various λ values and probability distributions. Test edge cases like ties in probabilities.

2. **Evaluate Coverage Stability**: Apply the method to a synthetic distribution shift scenario (e.g., adding noise to calibration vs. test data) and measure coverage degradation. Compare against standard conformal methods to isolate the impact of the singleton optimization.

3. **Analyze λ Selection Robustness**: Run the kneedle algorithm on multiple random seeds for the tuning set splits. Measure the variance in selected λ and resulting singleton frequency to assess stability.