---
ver: rpa2
title: The 2025 Planning Performance of Frontier Large Language Models
arxiv_id: '2511.09378'
source_url: https://arxiv.org/abs/2511.09378
tags:
- planning
- tasks
- llms
- performance
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the end-to-end planning performance of three
  frontier LLMs (DeepSeek R1, Gemini 2.5 Pro, and GPT-5) on PDDL-based planning tasks,
  comparing them to the classical planner LAMA. The models are tested on standard
  and obfuscated versions of tasks from the IPC 2023 Learning Track, with the latter
  designed to assess pure symbolic reasoning by replacing all symbolic names with
  random strings.
---

# The 2025 Planning Performance of Frontier Large Language Models

## Quick Facts
- **arXiv ID**: 2511.09378
- **Source URL**: https://arxiv.org/abs/2511.09378
- **Reference count**: 28
- **Primary result**: GPT-5 solves 205 out of 360 PDDL planning tasks, matching classical planner LAMA's performance on standard tasks while showing substantial computational inefficiency.

## Executive Summary
This paper evaluates the end-to-end planning capabilities of three frontier LLMs (DeepSeek R1, Gemini 2.5 Pro, and GPT-5) on PDDL-based planning tasks, comparing them to the classical planner LAMA. The models are tested on both standard and obfuscated versions of tasks from the IPC 2023 Learning Track, with obfuscation designed to assess pure symbolic reasoning by replacing all symbolic names with random strings. Results show GPT-5 achieves competitive performance with LAMA on standard tasks (205 vs. 205 solved), while Gemini 2.5 Pro demonstrates the most robust symbolic reasoning on obfuscated tasks (146 vs. 155 solved). DeepSeek R1 shows the largest performance degradation on obfuscated tasks (93 vs. 157 solved). The study reveals that frontier LLMs have made substantial progress in planning but still rely heavily on semantic information from natural language tokens rather than pure symbolic manipulation, and remain orders of magnitude less computationally efficient than classical planners.

## Method Summary
The study evaluates three frontier LLMs (DeepSeek R1, Gemini 2.5 Pro, GPT-5) on PDDL planning tasks from the IPC 2023 Learning Track, using both standard and obfuscated versions. Models receive PDDL domain and task descriptions via few-shot prompting with a specific template containing instructions, checklist of common pitfalls, and 2 examples. Generated plans are validated by the VAL tool for correctness. The evaluation includes 8 domains (Blocksworld, Childsnack, Floortile, Miconic, Rovers, Sokoban, Spanner, Transport) with 45 tasks each (360 total). Performance is measured by coverage (number of solved tasks) and plan length, with LAMA-first serving as the baseline classical planner. Obfuscation is achieved by replacing all symbols with random strings to test semantic reasoning.

## Key Results
- GPT-5 solves 205/360 tasks, matching LAMA's performance on standard tasks
- Gemini 2.5 Pro shows smallest performance drop on obfuscated tasks (146 vs. 155 solved)
- DeepSeek R1's performance degrades most significantly on obfuscated tasks (93 vs. 157 solved)
- All LLMs generate plans that are orders of magnitude less computationally efficient than LAMA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frontier LLMs plan effectively partly by leveraging semantic priors from natural language tokens rather than purely symbolic manipulation
- **Mechanism:** Models map familiar keywords (e.g., "truck," "drive") to pre-trained behavioral patterns, bypassing explicit state-space search
- **Core assumption:** Models have encountered similar domain semantics during pre-training, enabling pattern matching over de novo reasoning
- **Evidence anchors:** Performance degrades when PDDL domains/tasks are obfuscated; models still depend on semantic information in predicate and action names

### Mechanism 2
- **Claim:** Increased computational budget allows models to compensate for missing semantic information or increased problem complexity
- **Mechanism:** Models generate internal reasoning tokens to explore PDDL operator logic when semantic clues are removed
- **Core assumption:** Inference infrastructure allows long contexts and sustained attention over reasoning tokens
- **Evidence anchors:** Gemini 2.5 Pro uses significantly more reasoning tokens for obfuscated tasks; DeepSeek R1 exceeds 30 minutes on complex tasks

### Mechanism 3
- **Claim:** Valid long-horizon planning is achieved through external validation rather than internal guarantees
- **Mechanism:** LLM generates candidate plans validated by sound symbolic tool (VAL), decoupling generation from verification
- **Core assumption:** PDDL syntax generated is formal enough for VAL parsing
- **Evidence anchors:** All LLM plans verified by VAL; LLM-based planning lacks soundness/completeness guarantees

## Foundational Learning

- **Concept: PDDL (Planning Domain Definition Language)**
  - **Why needed here:** Input format for benchmarks; understanding domain/task split is crucial for interpreting semantic reliance
  - **Quick check question:** Can you distinguish between predicate definition in a domain file and initial state assignment in a task file?

- **Concept: Satisficing vs. Optimal Planning**
  - **Why needed here:** Study evaluates "satisficing" planning (finding any valid plan), explaining variable plan lengths
  - **Quick check question:** Does a plan with 500 steps count as success if shortest possible plan is 10 steps?

- **Concept: Soundness vs. Completeness**
  - **Why needed here:** Paper explicitly states LLMs lack these properties, explaining why VAL validator is mandatory
  - **Quick check question:** If an LLM fails to find a plan for solvable task, is it violating soundness or completeness?

## Architecture Onboarding

- **Component map:** Input Formatter -> Generator (LLM) -> Validator (VAL) -> Compute Interface
- **Critical path:** Prompt construction is highest leverage point; specific structure (Instructions -> PDDL -> Checklist -> Examples) is critical
- **Design tradeoffs:** LLMs offer semantic flexibility and few-shot generalization but cost ~1000x more and lack guarantees; LAMA is cheaper and sound but brittle to domain variations
- **Failure signatures:** Semantic over-reliance (accuracy drops on obfuscation), resource exhaustion (timeouts on complex tasks), syntax drift (invalid predicates failing VAL)
- **First 3 experiments:**
  1. Run LAMA-first and GPT-4o on 10 standard IPC tasks to establish semantic baseline
  2. Apply random-string obfuscation to same 10 tasks and measure performance drop and latency/token usage increase
  3. Connect LLM output to VAL tool; if rejected, feed error message back for one retry attempt to test reflexion improvement

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the substantial computational efficiency gap between LLMs and classical planners be closed without sacrificing reasoning capability?
  - **Basis:** Authors explicitly state LLMs remain "orders of magnitude less efficient" regarding hardware, energy, and cost
  - **Why unresolved:** Study quantifies performance parity but highlights critical resource cost trade-off
  - **What evidence would resolve it:** Comparative analysis showing LLMs matching classical planners' latency and energy consumption on standard hardware

- **Open Question 2:** What specific architectural or training factors enable Gemini 2.5 Pro's relative robustness to PDDL obfuscation?
  - **Basis:** Paper notes Gemini 2.5 Pro was "the only LLM that is not severely impacted by obfuscation"
  - **Why unresolved:** Authors report empirical observation but don't investigate underlying mechanisms
  - **What evidence would resolve it:** Mechanistic interpretability study isolating features allowing Gemini to succeed on purely symbolic identifiers

- **Open Question 3:** How does plan quality (length and optimality) of frontier LLMs compare to classical planners when both succeed?
  - **Basis:** Paper focuses on coverage and acknowledges LLMs can generate very long plans (>500 steps)
  - **Why unresolved:** While correctness is validated, study doesn't rigorously compare plan lengths to baseline
  - **What evidence would resolve it:** Reporting ratio of plan lengths generated by LLMs versus LAMA for solved task intersections

## Limitations
- Results highly sensitive to specific few-shot prompt template structure
- Compute resource variation may affect results due to API parameter differences
- Obfuscation scheme implementation details not fully specified

## Confidence
- **High confidence:** LLMs show significant performance degradation on obfuscated PDDL tasks, confirming semantic reliance
- **Medium confidence:** GPT-5 matches LAMA's performance on standard tasks, suggesting LLMs have closed gap with classical planners
- **Medium confidence:** Gemini 2.5 Pro demonstrates most robust symbolic reasoning (smallest drop on obfuscated tasks)

## Next Checks
1. Test exact prompt template on small subset (10 tasks) of standard IPC domains to establish baseline performance
2. Measure actual token usage and latency for each model on obfuscated vs. standard tasks to confirm 1000x computational overhead
3. Create controlled ablation study with varying degrees of semantic obfuscation to quantify semantic information's contribution to planning performance