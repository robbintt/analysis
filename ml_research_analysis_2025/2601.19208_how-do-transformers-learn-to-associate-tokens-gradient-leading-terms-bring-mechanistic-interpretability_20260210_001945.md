---
ver: rpa2
title: 'How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring
  Mechanistic Interpretability'
arxiv_id: '2601.19208'
source_url: https://arxiv.org/abs/2601.19208
tags:
- have
- then
- each
- token
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes how semantic associations emerge in attention-based
  language models during training. The authors develop a theoretical framework using
  gradient leading-term approximations to characterize weight matrices at early training
  stages as compositions of three basis functions: bigram, interchangeability, and
  context mappings.'
---

# How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2601.19208
- Source URL: https://arxiv.org/abs/2601.19208
- Reference count: 40
- Key outcome: Transformer weights at early training stages decompose into compositions of three basis functions (bigram, interchangeability, context mappings) that reflect corpus statistics, with cosine similarities above 0.7 with theoretical predictions even after 100 epochs.

## Executive Summary
This paper develops a theoretical framework for understanding how semantic associations emerge in attention-based language models during training. By leveraging gradient leading-term approximations, the authors derive closed-form expressions for transformer weight matrices that decompose into three corpus-derived basis functions. The analysis reveals how different transformer components collaborate to capture semantic relationships between tokens, providing mechanistic interpretability for representation learning in transformers.

## Method Summary
The method uses gradient leading-term approximations to characterize weight matrices during early training stages. The analysis assumes small learning rates and limited training duration, deriving closed-form expressions for each weight matrix as compositions of three basis functions derived from corpus statistics: bigram mappings (B̄), interchangeability mappings (ΣB̄), and context mappings (Φ̄). The framework is validated on both toy transformers trained on TinyStories and practical LLMs (Pythia-1.4B), showing that learned weights closely match theoretical predictions with cosine similarities above 0.7.

## Key Results
- All major weight matrices in attention-only transformers can be expressed as closed-form compositions of three corpus-derived basis functions
- Learned weights maintain cosine similarities above 0.7 with theoretical predictions even after 100 training epochs
- Different transformer components collaborate to capture semantic associations: output matrices learn bigram associations, value matrices learn context-context associations, and query-key matrices learn token similarity
- Layer specialization emerges during training, with early layers maintaining high similarity to leading-term predictions longer than middle layers

## Why This Works (Mechanism)
The leading-term approximation works because early training dynamics are dominated by the initial gradient directions, which encode corpus statistics in a structured way. The three basis functions (bigram, interchangeability, and context mappings) emerge from the structure of the loss function and data distribution. The mechanism relies on the assumption that gradients are approximately constant during early training, allowing closed-form solutions. The collaboration between different weight matrices emerges because each component captures a different aspect of token relationships: query-key matrices learn token similarity, value matrices learn context representations, and output matrices learn direct token associations.

## Foundational Learning
The theoretical foundation rests on analyzing gradient flow during early training stages when weight updates are small enough that linear approximations hold. The authors leverage the fact that for small learning rates and limited training duration, the gradient at initialization dominates the learning trajectory. This allows derivation of closed-form expressions for weight matrices as functions of corpus statistics. The framework builds on prior work in mechanistic interpretability by providing a rigorous mathematical connection between training dynamics and learned representations.

## Architecture Onboarding
The analysis applies to attention-only transformers with standard architecture. The framework decomposes each weight matrix into interpretable components that capture different aspects of token relationships. The method reveals that transformer components work synergistically: query-key matrices establish token similarity, value matrices build context representations, and output matrices create direct token associations. This decomposition provides insights into how transformers build hierarchical representations of language through layer specialization during training.

## Open Questions the Paper Calls Out
The authors acknowledge several open questions regarding the limits of their leading-term approximation, particularly regarding when and why it breaks down during training. They note that while cosine similarities remain high for early layers even after 100 epochs, the theoretical predictions become less accurate over time. The paper calls for investigation into how non-linearities and long-range dependencies affect the validity of their framework. Additionally, they highlight the need to extend their analysis to more complex architectures and tasks beyond next-token prediction.

## Limitations
The leading-term approximation is only valid during early training stages with small learning rates, limiting its applicability to later training phases. The framework assumes linear dynamics and may not capture the full complexity of transformer learning, particularly regarding non-linear interactions and long-range dependencies. The analysis focuses on attention-only transformers and may not generalize to architectures with MLPs or other components. The theoretical predictions show decreasing accuracy over training time, with cosine similarities dropping below the 0.7 threshold after approximately 100 epochs in some cases.

## Confidence
High confidence in the theoretical framework's validity for early training stages, supported by empirical validation on both toy and practical models. The closed-form expressions for weight matrices are mathematically rigorous and the experimental results demonstrate strong agreement (cosine similarities > 0.7) with theoretical predictions. However, confidence decreases for later training stages where the leading-term approximation becomes less accurate.

## Next Checks
Future work should investigate the breakdown of leading-term approximations during later training stages and identify the specific mechanisms that cause divergence from theoretical predictions. Additional validation is needed on more diverse architectures, including those with MLPs and different attention mechanisms. The framework should be extended to analyze multi-task learning scenarios and more complex language understanding tasks beyond next-token prediction.