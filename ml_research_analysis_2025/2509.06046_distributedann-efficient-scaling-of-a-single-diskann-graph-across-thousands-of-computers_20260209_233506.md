---
ver: rpa2
title: 'DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands
  of Computers'
arxiv_id: '2509.06046'
source_url: https://arxiv.org/abs/2509.06046
tags:
- search
- index
- distributedann
- graph
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DISTRIBUTEDANN addresses the challenge of serving large-scale vector
  search on datasets that cannot fit in a single machine's memory by distributing
  a single logical graph index across thousands of machines using a distributed key-value
  store. The approach modifies DISKANN's graph index layout to enable efficient distributed
  serving, including duplicating compressed vectors into graph nodes, introducing
  an in-memory head index for fast traversal initiation, and implementing near-data
  computation for scoring candidates.
---

# DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers

## Quick Facts
- arXiv ID: 2509.06046
- Source URL: https://arxiv.org/abs/2509.06046
- Reference count: 9
- Primary result: DISTRIBUTEDANN achieves 26ms median latency and 100k QPS on 50B vectors with 6x efficiency improvement over partitioned approaches

## Executive Summary
DISTRIBUTEDANN addresses the challenge of serving massive-scale vector search on datasets that cannot fit in a single machine's memory by distributing a single logical graph index across thousands of machines. The system modifies DISKANN's graph index layout to enable efficient distributed serving, including duplicating compressed vectors into graph nodes, introducing an in-memory head index for fast traversal initiation, and implementing near-data computation for scoring candidates. Evaluated on a 50-billion vector web search dataset, DISTRIBUTEDANN achieved 26ms median query latency and processed over 100,000 queries per second, representing 6x more efficiency than conventional partitioning approaches while delivering significant recall improvements.

## Method Summary
DISTRIBUTEDANN modifies the DiskANN index layout by duplicating compressed vectors (OPQ codes) into graph nodes to avoid network lookups during scoring. The system uses a "stitching" approach where independent partition indices are built and merged by taking the union of neighbor lists for overlapping vectors. The serving architecture consists of an orchestration service that manages multi-hop searches across distributed nodes, a head index (in-memory ANN index over top graph layers) for fast entry-point location, a distributed key-value store holding the stitched graph nodes, and a node scoring service running on KV hosts to score candidates near-data. The system uses beam search with 5 hops and a bandwidth of 128, with the orchestration service maintaining result and candidate heaps throughout traversal.

## Key Results
- Achieved 26ms median query latency and over 100,000 queries per second on 50-billion vector dataset
- Delivered 7.8 and 4.5 percentage point improvements in recall@5 and recall@200 respectively compared to previous production systems
- Demonstrated 6x more efficiency than conventional partitioning approaches in terms of recall per IO operation
- Showed graceful degradation under partial failures, maintaining high recall even with node unavailability

## Why This Works (Mechanism)

### Mechanism 1: Unified Graph Traversal over Partitioned Routing
DISTRIBUTEDANN achieves higher recall per IO operation than clustered partitioning because it dynamically allocates the IO budget to the most relevant regions of the dataset rather than distributing it uniformly across pre-selected partitions. By treating the entire 50B vector dataset as a single logical graph, the beam search algorithm can traverse "cluster" boundaries naturally, spending IO hops where the graph density is highest for the specific query. This optimizes the recall/IO frontier by allowing the search to go deeper in relevant clusters rather than being limited to fixed partitions.

### Mechanism 2: Near-Data Computation (Compute-Storage Coupling)
Moving the distance calculation (scoring) to the storage node reduces network bandwidth consumption and parallelizes CPU load, preventing the orchestration node from becoming a serialization bottleneck. Instead of fetching full graph nodes across the network to the orchestrator for scoring, DISTRIBUTEDANN deploys a "Node Scoring Service" on the KV store hosts. These hosts read the node, score it locally against the query, and return only the compact results (IDs and distances). This reduces the payload size per hop (approximately 6x savings) and distributes the expensive distance computations across the cluster.

### Mechanism 3: Latency Hiding via In-Memory Head Index
Separating the top layers of the graph into a dedicated in-memory index ("Head Index") mitigates the high latency of distributed graph initialization. A standard distributed graph would require multiple network hops just to traverse the sparse upper layers to find an entry point near the query. DISTRIBUTEDANN extracts the top layers (2.5B vectors) into a separate, sharded in-memory index. The search starts here, allowing the query to "jump" close to the target cluster before initiating the slower, storage-bound graph traversal.

## Foundational Learning

- **Concept: DiskANN / Graph-based ANN (HNSW/NSW principles)**
  - **Why needed here:** The paper modifies the standard DiskANN layout. You must understand how graph-based search uses "beam search" (navigating via neighbors) to distinguish between "Full Vector" reads (precise, expensive) and "Compressed Vector" (OPQ) reads (approximate, cheap filtering).
  - **Quick check question:** In a standard DiskANN layout, which component determines the *ordering* of nodes to visit next: the SSD-resident full vector or the memory-resident compressed vector?

- **Concept: Product Quantization (OPQ)**
  - **Why needed here:** The paper relies on "duplicating compressed vectors" into graph nodes to function. You need to understand that OPQ allows comparing a query to a vector using a lookup table (SDC) without loading the full vector dimensions (d=384) from disk.
  - **Quick check question:** Why does storing the OPQ code inside the graph node (on SSD) save network bandwidth compared to fetching it from a remote memory store?

- **Concept: Distributed Key-Value Store Sharding**
  - **Why needed here:** The system treats the graph as a KV store where keys are Vector IDs. Understanding how consistent hashing or sharding maps these IDs to physical machines is required to debug "hot spots" or latency variance.
  - **Quick check question:** If a graph traversal path consistently lands on the same physical machine, what system-level bottleneck might occur despite the distributed architecture?

## Architecture Onboarding

- **Component map:**
  - Orchestration Service -> Head Index -> KV Store/Node Scoring Service -> SSD storage

- **Critical path:**
  1. Query Ingest: Orchestrator encodes query (OPQ)
  2. Warm Start: Orchestrator queries Head Index -> gets K_head entry points
  3. Traversal Loop (Hops): Orchestrator sends batch of Node IDs to KV Store/Scoring Service
  4. Near-Data Compute: KV Service reads nodes from SSD, scores locally, prunes candidates
  5. Merge: Orchestrator updates heaps with returned scores. Repeat loop H times

- **Design tradeoffs:**
  - Space vs. Latency: DISTRIBUTEDANN trades ~2.9x storage overhead (duplicating OPQ codes and storing graph edges) for 6x throughput improvement
  - Parallelism vs. Tail Latency: The system uses "hedged requests" to manage stragglers. Aggressive timeouts reduce tail latency but risk lower recall (graceful degradation)

- **Failure signatures:**
  - High Tail Latency (p99): Likely caused by "head-of-line blocking" in the KV store or network oversubscription
  - Recall Drop without Error: Indicates the Node Scoring Service is timing out too aggressively (dropping valid candidates) or the Head Index is under-provisioned
  - CPU Bound on Head Index: Explicitly mentioned as a bottleneck; requires scaling head-index replicas independently of the KV store

- **First 3 experiments:**
  1. Baseline IO Profile: Measure IO distribution per query to verify the "flexible traversal" claim is active (variable IO per cluster) vs. the fixed IO of partitioned systems
  2. Failure Injection: Simulate 1-5% packet loss or node unavailability in the KV store to validate the "graceful degradation" curve and tune timeout thresholds
  3. Head Index Sizing: Ablation study on the Head Index size (e.g., 1B vs 2.5B vectors) to measure the tradeoff between entry-point quality (recall) and memory/CPU cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DISTRIBUTEDANN perform empirically compared to advanced partitioning schemes that minimize cross-partition edges?
- Basis in paper: Section 4.4 notes that "Further work is needed to compare the empirical performance of these approaches and DISTRIBUTEDANN."
- Why unresolved: The evaluation primarily compares DISTRIBUTEDANN against clustered partitioning, but newer graph-partitioning methods (e.g., Gottesb√ºren et al., 2024) were not benchmarked.
- What evidence would resolve it: A direct comparison of recall, latency, and throughput on the same 50-billion vector dataset against systems using advanced graph partitioning.

### Open Question 2
- Question: Can the 10x storage amplification be reduced by placing multiple nearby full-dimension vectors into a single graph node?
- Basis in paper: Section 5.1 suggests this approach is likely possible but requires "further experimentation is needed to understand the tradeoffs."
- Why unresolved: The current design duplicates compressed vectors into every neighbor list to reduce network hops, incurring significant space overhead.
- What evidence would resolve it: Analysis of the space savings versus the impact on graph connectivity and IO complexity when implementing multi-vector nodes.

### Open Question 3
- Question: To what extent can kernel-bypass networking or computational storage devices reduce tail latency in the node scoring service?
- Basis in paper: Section 5.1 states that "Kernel-bypass networking would likely reduce the tail latency" and suggests the "node scoring service logic could be implemented in a computational storage device."
- Why unresolved: The current implementation relies on standard kernel-based TCP networking, which introduces overhead in the multi-hop architecture.
- What evidence would resolve it: End-to-end latency measurements (especially p99) refactoring the orchestration or scoring service to use these hardware optimizations.

## Limitations

- Storage amplification: The system requires approximately 10x storage overhead due to duplicating compressed vectors into every neighbor list
- Under-specified implementation details: Critical components like the Head Index algorithm and KV store configuration are not fully detailed
- Graph stitching complexity: The process for merging overlapping partition indices and ensuring robust cross-cluster edges is referenced but not fully explained

## Confidence

- **High confidence:** The core architectural approach (distributed KV store + near-data computation + head index) is well-described and technically sound
- **Medium confidence:** The 6x efficiency improvement claim over partitioned approaches is supported by the mechanism explanation, but the exact comparison baseline and conditions could be clearer
- **Medium confidence:** The recall improvements (7.8 and 4.5 percentage points) are demonstrated on a specific dataset but may vary with different data distributions or failure scenarios

## Next Checks

1. **IO profile validation:** Measure actual IO distribution per query to verify DISTRIBUTEDANN achieves the claimed flexible traversal (variable IO per cluster) versus fixed IO of partitioned systems

2. **Failure injection testing:** Simulate 1-5% node failures in the KV store to validate the graceful degradation curve and measure actual recall loss under partial failures

3. **Head index sizing study:** Perform an ablation study on Head Index size (e.g., 1B vs 2.5B vectors) to quantify the tradeoff between entry-point quality and memory/CPU overhead