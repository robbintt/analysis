---
ver: rpa2
title: Single-Branch Network Architectures to Close the Modality Gap in Multimodal
  Recommendation
arxiv_id: '2509.18807'
source_url: https://arxiv.org/abs/2509.18807
tags:
- modalities
- modality
- item
- user
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes single-branch network architectures for multimodal
  recommendation to address the "modality gap" problem in hybrid recommender systems.
  The key idea is to use a shared neural network to encode all modalities of the same
  item into similar regions of a joint embedding space, supplemented with weight sharing,
  modality sampling, and a contrastive loss function.
---

# Single-Branch Network Architectures to Close the Modality Gap in Multimodal Recommendation

## Quick Facts
- **arXiv ID:** 2509.18807
- **Source URL:** https://arxiv.org/abs/2509.18807
- **Reference count:** 40
- **Primary result:** Single-branch SiBraR outperforms multi-branch models in missing modality settings and matches them in warm-start scenarios.

## Executive Summary
This paper introduces single-branch neural network architectures (SiBraR) for multimodal recommendation, designed to address the "modality gap" problem where different data types occupy separate regions in embedding space. The core innovation is using a shared neural network to encode all modalities of the same item into similar embedding regions, supplemented with weight sharing, random modality sampling during training, and a symmetric InfoNCE contrastive loss. Across three datasets (MovieLens, Music4All-Onion, and Amazon Video Games), SiBraR achieves competitive accuracy in warm-start scenarios and significantly better performance in cold-start and missing modality settings compared to multi-branch alternatives.

## Method Summary
The method employs a single shared neural network $g$ that takes any modality (text, audio, image, interactions) as input and produces a unified embedding. Modality-specific shallow projection layers map raw inputs to a common dimension before the shared branch. During training, random modality sampling selects subsets of available modalities, forcing the model to handle incomplete data. A combined loss function optimizes both recommendation ranking (BPR) and modality alignment (symmetric InfoNCE). The user and item embeddings are computed by averaging the embeddings of available modalities.

## Key Results
- SiBraR outperforms multi-branch models in cold-start and missing modality scenarios across all three datasets
- The contrastive loss and modality sampling effectively reduce the modality gap, making embeddings more interchangeable
- SiBraR maintains robust performance when progressively fewer modalities are available, while multi-branch models degrade significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single shared network branch enforces tighter modality gap than multi-branch alternatives by projecting heterogeneous inputs into unified embedding regions.
- **Mechanism:** The structural constraint of shared weights forces distinct modalities through the same transformation, preventing them from drifting into separate subspaces and making representations interchangeable.
- **Core assumption:** Different modalities of the same item share an underlying semantic core that a single non-linear function can approximate.
- **Evidence anchors:** Abstract states SiBraR "narrows the modality gap"; Section 3.2 describes shared weights for all modalities.
- **Break condition:** If modalities contain fundamentally contradictory semantic information that cannot be reconciled by a single projection, the mechanism may degrade into an averaging effect that loses nuance.

### Mechanism 2
- **Claim:** Symmetric InfoNCE contrastive loss acts as explicit alignment signal, pulling representations of different modalities from the same item closer together than to other items.
- **Mechanism:** The contrastive loss maximizes similarity between $g(m_1)$ and $g(m_2)$ for the same item while pushing them away from other items' modalities, creating "gravity" that collapses the modality gap.
- **Core assumption:** Proximity in joint embedding space correlates with semantic equivalence and recommendation relevance.
- **Evidence anchors:** Section 5.3.1 shows contrastive loss encourages independent branches to map modalities to same region; Section 5.2.4 demonstrates effect of temperature and weight on modality proximity.
- **Break condition:** If temperature $\tau$ is too low or weight $\alpha$ too high, contrastive loss may dominate and collapse all representations into a single point.

### Mechanism 3
- **Claim:** Random modality sampling during training regularizes the model, preventing over-reliance on any single dominant modality.
- **Mechanism:** Randomly masking modalities during forward pass forces the model to learn robust representations that function even when inputs are sparse, simulating inference-time missing modality conditions.
- **Core assumption:** The distribution of missing modalities at inference is unpredictable, requiring the model to be agnostic to input cardinality.
- **Evidence anchors:** Section 3.2.1 describes modality sampling to improve robustness; Section 5.1.3 shows SiBraR maintains advantage in cold start while only suffering slight performance decline.
- **Break condition:** If a specific modality is critical for the recommendation task and sampled too infrequently, the model may fail to learn specific features required, trading robustness for accuracy.

## Foundational Learning

- **Concept: Bayesian Personalized Ranking (BPR)**
  - **Why needed here:** This is the base recommendation loss used to train the system. Understanding that the model optimizes for relative ordering rather than absolute scores is crucial for tuning the interaction between recommendation loss and contrastive loss.
  - **Quick check question:** How does BPR handle implicit feedback, and what happens if we weight the contrastive loss higher than the BPR loss?

- **Concept: The Modality Gap**
  - **Why needed here:** The paper defines this as the phenomenon where different data types occupy distinct regions in a shared vector space, making them incompatible. SiBraR is explicitly designed to close this gap.
  - **Quick check question:** If you plot embeddings of text and images from a standard multi-branch model, would you expect them to overlap or form separate clusters? Why?

- **Concept: Symmetric Noise Contrastive Estimation (InfoNCE)**
  - **Why needed here:** The paper uses a specific variant to align modalities. You need to understand why it is "symmetric" (optimizing $m_1 \to m_2$ AND $m_2 \to m_1$) and the role of temperature parameter in controlling the "hardness" of negatives.
  - **Quick check question:** In the context of this paper, what constitutes a "positive" pair and a "negative" pair for the contrastive loss?

## Architecture Onboarding

- **Component map:** Input modalities (audio, text, image, interactions) -> Modality-specific projections ($f_k$) -> Single shared branch ($g$) -> Aggregator ($\mu$) -> Loss Head (BPR + Symmetric InfoNCE)
- **Critical path:** The Single Branch ($g$) is the critical bottleneck. Unlike multi-branch architectures where failure in one branch might be compensated by others, here $g$ must successfully transform text, audio, and interactions into the same manifold.
- **Design tradeoffs:**
  - **Robustness vs. Specificity:** The single branch sacrifices modality-specific depth for unified robustness, gaining ability to handle missing data but potentially losing fine-grained feature extraction for rich modalities.
  - **Efficiency:** Inference is efficient as only available modalities need to run through the shared network, rather than maintaining parallel branches.
- **Failure signatures:**
  - **Warm-Start Degradation:** If model performs worse than standard MF in warm-start scenarios, contrastive loss weight is likely too high.
  - **Modality Collapse:** If embedding space shows all items clustered into a single ball, contrastive temperature is likely too low or weight too high.
  - **Cold-Start Failure:** If performance drops to random levels when interactions are missing, modality sampling was likely insufficient or shared network failed to align content modalities.
- **First 3 experiments:**
  1. **Gap Analysis:** Train SiBraR and MuBraR on same data. Plot t-SNE visualizations of embeddings to verify SiBraR overlaps modalities while MuBraR separates them.
  2. **Hyperparameter Sensitivity (Temperature/Weight):** Run grid search on $\alpha$ and $\tau$. Plot NDCG against ratio $\alpha/\tau$ to find optimal "sweet spot" diagonal.
  3. **Ablation on Availability:** Evaluate trained model on test set while progressively removing modalities (5 -> 1). Confirm graceful degradation compared to baseline that crashes when inputs are missing.

## Open Questions the Paper Calls Out
None

## Limitations
- The single-branch architecture may underperform in domains where certain modalities contain highly specialized information that generic projections cannot capture.
- Hyperparameter sensitivity to the contrastive loss weight and temperature ratio is not fully explored across all datasets.
- The ablation study on modality sampling is limited to only two values, and training convergence diagnostics are not reported.

## Confidence

- **High Confidence:** Claims about superior cold-start performance and robustness to missing modalities are well-supported by ablation studies and cross-dataset evaluations.
- **Medium Confidence:** The assertion that single-branch architecture inherently reduces modality gap is plausible but relies on indirect evidence rather than direct visualization of all datasets.
- **Low Confidence:** The paper does not thoroughly investigate failure modes or provide extensive hyperparameter sensitivity analysis for contrastive loss components.

## Next Checks
1. **Modality Gap Visualization:** Generate t-SNE plots of final embeddings for all three datasets, comparing SiBraR vs. MuBraR, to directly confirm SiBraR collapses modality clusters while MuBraR separates them.
2. **Hyperparameter Sensitivity Grid:** Conduct systematic grid search over ratio $\alpha / \tau$ for contrastive loss, plotting NDCG@10 against this ratio to identify optimal operating regions and potential collapse thresholds.
3. **Extreme Modality Removal:** Evaluate model performance when only a single, non-interaction modality (e.g., only text or only images) is available at inference, to test limits of single-branch architecture's robustness.