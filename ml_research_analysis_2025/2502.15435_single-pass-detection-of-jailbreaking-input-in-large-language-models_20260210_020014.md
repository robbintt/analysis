---
ver: rpa2
title: Single-pass Detection of Jailbreaking Input in Large Language Models
arxiv_id: '2502.15435'
source_url: https://arxiv.org/abs/2502.15435
tags:
- attacks
- attack
- language
- llama
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Single Pass Detection (SPD), a method to detect
  jailbreaking attacks in Large Language Models (LLMs) with just one forward pass
  by analyzing the output logits. SPD identifies harmful prompts by detecting shifts
  in the logit distributions compared to benign inputs, using a feature matrix based
  on the negative log probabilities of the first few tokens.
---

# Single-pass Detection of Jailbreaking Input in Large Language Models

## Quick Facts
- arXiv ID: 2502.15435
- Source URL: https://arxiv.org/abs/2502.15435
- Reference count: 40
- One-line primary result: Achieves >90% TP rate and <1% FP rate on jailbreaking detection using only one forward pass

## Executive Summary
This paper introduces Single Pass Detection (SPD), a highly efficient method for detecting jailbreaking attacks in Large Language Models (LLMs) by analyzing the output logits from just one forward pass. SPD identifies harmful prompts by detecting characteristic shifts in the logit distributions compared to benign inputs, using a feature matrix based on the negative log probabilities of the first few tokens. An SVM classifier is trained to distinguish between attacked and benign sentences. The method achieves high true positive rates (>90%) and low false positive rates (<1%) across multiple open-source models (Llama 2, Vicuna) and proprietary models (GPT-3.5, GPT-4, GPT-4o-mini), even with limited logit access. SPD is 12-80 times faster than existing perturbation-based or auxiliary LLM-based defenses, offering a highly efficient solution for safeguarding LLMs against jailbreaking attacks.

## Method Summary
SPD works by intercepting the LLM's output at the logit level before text generation occurs. For a given prompt, the method collects the logits for the first r output tokens (typically r=5), extracts the top-k logits for each token, and converts them to negative log-probabilities to form a feature matrix H. This H matrix is then flattened and fed to an SVM classifier (with RBF kernel) that was trained on labeled data to distinguish between benign and attacked inputs. The core insight is that optimization-based attacks (like GCG) cause a characteristic negative shift in the logit distribution compared to benign inputs, which can be detected statistically through these features.

## Key Results
- Achieves >90% true positive rate and <1% false positive rate on Llama 2 and Vicuna for optimization-based attacks
- Maintains effectiveness on proprietary models (GPT-3.5, GPT-4, GPT-4o-mini) with adjusted parameters
- 12-80 times faster than existing perturbation-based or auxiliary LLM-based defenses
- Works with limited logit access (top-5 only) when full logit distributions are unavailable

## Why This Works (Mechanism)

### Mechanism 1: Optimization-Induced Logit Shift
Optimization-based attacks like GCG minimize cross-entropy loss to force specific target outputs, creating a characteristic negative shift in the logit values of output tokens compared to benign inputs. This optimization gradient pushes the logit of the target token up while suppressing others, creating a distinct distribution anomaly. The core assumption is that the optimization process leaves a consistent "fingerprint" on the logit landscape that differs statistically from benign inputs. If an attack method is developed that does not rely on gradient-based optimization of specific tokens (e.g., purely semantic human-engineered prompts), this specific statistical shift may not occur.

### Mechanism 2: Entropy Divergence in Early Tokens
Attacked inputs exhibit higher entropy in the logits of the first few output tokens compared to benign inputs. Aligned models typically respond to benign safety prompts with high certainty (low entropy), often favoring a specific refusal token. Adversarial inputs disrupt this certainty, spreading probability mass across multiple candidates, thereby increasing entropy. The core assumption is that the model's internal conflict between alignment (refusal) and the adversarial instruction (compliance) manifests as increased uncertainty in immediate next-token prediction. If an attack perfectly suppresses the refusal option without introducing competing high-probability tokens, the entropy might remain low, potentially evading this signal.

### Mechanism 3: Separability via Negative Log-Probability Features
The feature matrix constructed from the negative log-probabilities of the top-k logits allows for the linear (or kernel-based) separation of attacked and benign inputs. By converting raw logits to negative log-probabilities for the top tokens, the method normalizes the distribution into a fixed-size feature vector that an SVM can map to a high-dimensional space where the two classes are distinct. The core assumption is that the "shape" of the probability distribution (defined by the top-k tokens) is the primary distinguisher, rather than the semantic content of the prompt. If the model architecture changes such that logit distributions for safety and harm overlap significantly, the SVM decision boundary may fail.

## Foundational Learning

- **Concept: Logits and Softmax Distribution**
  - Why needed here: SPD relies on the statistical properties of logits (the raw scores before probability conversion). Understanding how logits translate to probabilities (via Softmax) and how they relate to model "confidence" is essential to grasp why a shift in logit values signals an attack.
  - Quick check question: If a model has a vocabulary of 50k tokens, and the top logit is significantly higher than the second, does this imply high or low entropy in the output distribution?

- **Concept: Adversarial Suffixes (e.g., GCG)**
  - Why needed here: The mechanism detects *optimization-based* attacks. You need to understand that these attacks append seemingly random characters ("adversarial suffixes") optimized via gradients to maximize the probability of a harmful output, which is what creates the detectable logit shift.
  - Quick check question: Why does optimizing a suffix to produce the token "Sure" affect the logits of unrelated tokens in the vocabulary?

- **Concept: Support Vector Machines (SVM) & Kernels**
  - Why needed here: The paper selects an SVM with an RBF kernel as the classifier. You must understand that SVMs seek a hyperplane to separate data and that "RBF" allows it to handle non-linear relationships in the logit features.
  - Quick check question: Why might a linear classifier fail to separate benign from attacked logit distributions if the boundary is complex (non-linear)?

## Architecture Onboarding

- **Component map:** Input Prompt -> Target LLM (Frozen) -> Logits of first r tokens -> Feature Extractor (top-k, negative log-probabilities) -> Feature Matrix H -> SVM Classifier -> {Benign, Attacked}

- **Critical path:** The efficiency hinges on interrupting the generation process after just 1 forward pass. The system does not wait for the full sentence generation; it acts immediately upon receiving the logits for the first token.

- **Design tradeoffs:**
  - Full vs. Limited Logit Access: Open-source models allow k=50 (higher accuracy); Closed APIs (GPT-4) often expose only top 5 (k=5), requiring adjusted hyperparameters (r=25) to maintain performance.
  - Token Count (r): Increasing r (number of tokens analyzed) captures more context but increases latency and feature dimensionality. The paper finds r=5 is the "sweet spot" for open models.

- **Failure signatures:**
  - High False Positives on Complex Benign Data: As seen in Appendix E.2 (CodeXGLUE), complex benign tasks (like coding) might naturally exhibit higher entropy, leading to misclassification as "Attacked."
  - Transferability Limits: An SVM trained on GCG (optimization-based) may fail to detect PAIR/PAP (semantic/persuasion-based) attacks if those attacks do not induce the same logit shift (Appendix E.1).

- **First 3 experiments:**
  1. Validation of Logit Shift: Replicate Figure 2(a/b). Feed 100 benign and 100 GCG-attacked prompts into an open model (e.g., Llama 2). Plot the histogram of the first token's logits and entropy to confirm the distributional shift exists in your specific model version.
  2. Hyperparameter Sensitivity (k vs r): Ablate k (top logits) and r (token positions). Train SVMs on a small dataset and measure AUROC to verify the paper's claim that r=5, k=50 is optimal for open models.
  3. Generalization Test: Train the SVM on *only* GCG data. Test it against a semantic attack (like PAIR or manual jailbreaks). This tests the hypothesis that the "optimization fingerprint" is the primary detection signal versus a general "harm" signal.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptive attacks successfully bypass SPD by optimizing for harmful outputs while simultaneously matching the logit distributions of benign inputs? The study only tested a specific constrained optimization setup; robustness against more sophisticated adaptive adversaries remains unverified. A demonstrated adaptive attack that maintains a high attack success rate while producing a logit distribution that SPD classifies as benign would resolve this.

- **Open Question 2:** Is the observed negative shift in logit values a fundamental property of alignment, or merely an empirical artifact? The paper relies on the empirical observation of logit shifts but lacks a theoretical proof explaining why this phenomenon necessarily occurs during jailbreaking. A theoretical framework proving the shift is intrinsic to alignment fine-tuning, or an analysis of successful attacks that do not exhibit this shift, would resolve this.

- **Open Question 3:** How can the method generalize to entirely unseen attack families without requiring training samples from those specific categories? The SVM classifier is currently trained on specific attack data, making it brittle to novel attack taxonomies. A modification of SPD that achieves high detection rates on zero-shot attack families excluded from the training data would resolve this.

## Limitations

- The method is highly dependent on the specific architecture and training of the target LLM; changes in model design or alignment procedure could invalidate the detection mechanism.
- Poor generalization to semantic attack families (PAIR, PAP) that do not induce the characteristic logit shift, limiting detection to optimization-based attacks only.
- High false positive rates on complex benign tasks that naturally exhibit high entropy in early token predictions, requiring careful curation of benign training data.

## Confidence

- **High Confidence:** SPD achieves >90% TP rate and <1% FP rate on Llama 2 and Vicuna for optimization-based attacks (GCG, AutoDAN). The experimental results are clear and reproducible.
- **Medium Confidence:** Transfer to proprietary models (GPT-3.5, GPT-4, GPT-4o-mini) shows reduced performance and requires parameter tuning, suggesting less robust mechanism on these architectures.
- **Low Confidence:** The claim of being "12-80 times faster" than existing methods is based on limited comparison and does not account for potential overhead of attack-specific defenses.

## Next Checks

1. **Cross-Model Generalization Test:** Train an SVM on Llama 2 using GCG data. Test its performance on *all* attack types (GCG, AutoDAN, PAIR, PAP) on a different model (e.g., Vicuna or GPT-3.5). This will validate whether the "optimization fingerprint" is the true detection signal or if the SVM is simply learning a general "harm" pattern.

2. **Adversarial Training Test:** Use a small set of adversarial examples (e.g., prompts specifically crafted to have high entropy but are benign, like complex reasoning questions) to attempt to "break" the SPD classifier. Measure the increase in false positive rate. This will test the robustness of the entropy-based detection mechanism against deliberate evasion.

3. **Architectural Sensitivity Test:** Repeat the SPD training and evaluation process on a significantly different LLM architecture (e.g., a model with a different tokenizer, a different decoder design, or a model trained with a different alignment procedure). Compare the performance drop to the paper's reported results. This will quantify the true model-dependence of the detection mechanism.