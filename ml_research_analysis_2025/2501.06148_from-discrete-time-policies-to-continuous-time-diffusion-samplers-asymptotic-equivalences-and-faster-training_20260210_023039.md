---
ver: rpa2
title: 'From discrete-time policies to continuous-time diffusion samplers: Asymptotic
  equivalences and faster training'
arxiv_id: '2501.06148'
source_url: https://arxiv.org/abs/2501.06148
tags:
- learning
- training
- time
- discretization
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges continuous-time neural SDEs and discrete-time
  Markov decision processes for sampling from Boltzmann distributions without target
  samples. The authors prove asymptotic equivalences between discrete-time objectives
  (detailed balance, trajectory balance) and their continuous-time counterparts (Fokker-Planck
  equations, path measure divergences) as discretization becomes finer.
---

# From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training

## Quick Facts
- arXiv ID: 2501.06148
- Source URL: https://arxiv.org/abs/2501.06148
- Authors: Julius Berner; Lorenz Richter; Marcin Sendera; Jarrid Rector-Brooks; Nikolay Malkin
- Reference count: 40
- Primary result: Bridging continuous-time neural SDEs and discrete-time Markov decision processes for sampling from Boltzmann distributions without target samples

## Executive Summary
This paper establishes theoretical connections between discrete-time Markov decision processes and continuous-time neural stochastic differential equations for sampling from unnormalized target distributions. The authors prove that discrete-time training objectives (Trajectory Balance, Detailed Balance) converge to their continuous-time counterparts (path measure divergences, Fokker-Planck equations) as discretization becomes finer. They demonstrate that training with coarse, non-uniform time discretizations achieves performance competitive with fine uniform schemes while significantly reducing computational cost.

## Method Summary
The method trains neural stochastic differential equations (SDEs) to sample from Boltzmann distributions using discrete-time Markov decision processes. The SDE drift network is trained using either global objectives (Trajectory Balance/Path KL) that accumulate rewards along full trajectories or local objectives (Detailed Balance) that compute time-local discrepancies. Three discretization schemes are compared: uniform, random (non-uniform), and equidistant. The method operates without target samples, using only the energy function to compute rewards and gradients.

## Key Results
- Theoretical equivalence proven between discrete-time Trajectory Balance and continuous-time KL divergence between path measures
- Detailed Balance objectives converge to Fokker-Planck equations in the continuous limit
- Coarse random discretization achieves competitive performance with fine uniform discretization while reducing training steps by ~10×
- Local objectives (Detailed Balance) provide significant efficiency gains over global objectives (Trajectory Balance)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing discrete-time Trajectory Balance (TB) approximates minimization of KL divergence between continuous path measures
- **Mechanism:** The Radon-Nikodym derivative computed using Euler-Maruyama discretization converges to its continuous-time counterpart defined by Girsanov's theorem
- **Core assumption:** Bounded drift functions and derivatives with mesh approaching zero
- **Break condition:** Large time steps or unbounded drift cause discrete loss to diverge from continuous geometric objective

### Mechanism 2
- **Claim:** Detailed Balance objective forces model to satisfy Fokker-Planck equation in continuous limit
- **Mechanism:** DB discrepancy involving log-ratios of forward/backward transition probabilities converges to Nelson's identity and FPE as Δt → 0
- **Core assumption:** Smooth drift, density, and diffusion coefficients for Taylor expansions
- **Break condition:** Coarse time steps or insufficient smoothness weaken PDE connection

### Mechanism 3
- **Claim:** Coarse non-uniform discretization acts as stochastic regularization preventing overfitting to specific time embeddings
- **Mechanism:** Random steps ensure network sees broader distribution of time inputs, learning more robust continuous trajectory approximation
- **Core assumption:** Time-conditioned drift network
- **Break condition:** Too few steps (<5 for complex distributions) or poorly clustered random intervals degrade Riemann sum approximation

## Foundational Learning

**Concept: Euler-Maruyama Discretization**
- **Why needed here:** Bridge between continuous SDE and discrete MDP policy; explains noise injection and Δt scaling
- **Quick check question:** How does variance of transition kernel change as Δt decreases?

**Concept: Fokker-Planck Equation (FPE)**
- **Why needed here:** Explains why local objectives work; physical law ensuring probability conservation and correct transport
- **Quick check question:** If drift is zero, how does FPE describe change in p(x,t)?

**Concept: Radon-Nikodym Derivative**
- **Why needed here:** Mathematical object estimated by TB/VarGrad losses; represents trajectory weight under forward vs reverse measure
- **Quick check question:** Why is computing this derivative difficult in high dimensions without reparameterization or variational approximations?

## Architecture Onboarding

**Component map:** Energy Function -> Drift Network (μ_θ(x,t)) -> Euler-Maruyama Discretization -> Trajectory Generation -> Loss Computation -> Backpropagation

**Critical path:**
1. Sample time schedule (Uniform, Random, or Equidistant)
2. Run Euler-Maruyama forward pass to generate trajectory X̂
3. Compute losses: Global (accumulate rewards for TB) or Local (DB error per step)
4. Backpropagate: Through time for on-policy (PIS) or via log-prob for off-policy (TB/DB)

**Design tradeoffs:**
- **Global (TB/PIS) vs. Local (DB):** Global captures full path but suffers high variance and memory costs; Local is memory-efficient but requires learning marginal densities
- **Uniform vs. Random Steps:** Uniform is standard but brittle with coarse steps; Random is robust but introduces gradient estimator stochasticity

**Failure signatures:**
- **Periodic ELBO:** ELBO varies wildly with evaluation steps at multiples of training steps indicates overfitting to discrete grid
- **Mode Collapse:** Coarse steps in PIS can collapse path to single mode due to gradient variance
- **Density Explosion:** Incorrect normalization of p̂_n in DB causes numerical instability

**First 3 experiments:**
1. **Baseline Stability:** Train PIS model on 2D GMM with 100 uniform steps for ELBO baseline
2. **Coarse Ablation:** Train same model with 10 steps (Uniform vs. Random); plot ELBO gap vs evaluation steps to verify periodic failure in Uniform and smooth robustness in Random
3. **Objective Switch:** Compare DB vs TB on 10D target with 20 steps; measure wall-clock time and memory usage

## Open Questions the Paper Calls Out

**Open Question 1:** Can asymptotic equivalences extend to diffusion processes on general Riemannian manifolds?
- **Basis:** Conclusion states "Future theoretical work could generalize our results to diffusions on general Riemannian manifolds"
- **Why unresolved:** Current proofs rely on Euclidean space R^d and standard Itô calculus
- **What evidence would resolve it:** Proof showing discrete-time objectives converge to Riemannian FPE accounting for geometric structure

**Open Question 2:** Do asymptotic limits hold for non-Markovian continuous-time processes like fractional noise?
- **Basis:** Conclusion identifies "non-Markovian continuous-time processes, such as those studied in Daems et al. (2024); Nobis et al. (2024)" as target for generalization
- **Why unresolved:** Current framework relies on Markov property for standard FPE; non-Markovian processes need different tools
- **What evidence would resolve it:** Derivation showing discrete-time local objectives converge to fractional PDEs or path measures for non-Markovian SDEs

**Open Question 3:** Can connection between diffusion samplers and discrete policies be established for discrete-space sampling?
- **Basis:** Conclusion lists "discrete-space sampling problems" as area for future work
- **Why unresolved:** Current analysis restricts to continuous SDEs and state spaces; discrete spaces lack continuous trajectories and differentiability
- **What evidence would resolve it:** Theoretical results bridging discrete MDPs and continuous limits without spatial continuity, or empirical validation in discrete domains

## Limitations

- Theoretical results require Δt → 0 limit never achieved in practice; finite step performance not guaranteed
- Random discretization shows empirical robustness but lacks theoretical justification for why it better preserves continuous-time objective
- Experiments focus on synthetic benchmarks and one VAE application; generalization to complex real-world distributions untested

## Confidence

**High Confidence:** Asymptotic equivalence results (Prop 3.2-3.4) are mathematically rigorous under stated assumptions; empirical observation that coarse Random discretization outperforms coarse Uniform is clearly demonstrated

**Medium Confidence:** Claim that Local objectives are more efficient than Global is supported but depends on implementation details not fully specified; assertion that nonuniform discretization acts as regularization lacks theoretical grounding

**Low Confidence:** Claim of "faster training without sacrificing accuracy" primarily demonstrated through step-count reduction rather than wall-clock time including implementation factors

## Next Checks

1. **Time-discretization ablation:** Systematically vary N_train (5, 10, 20, 50) and discretization scheme (Uniform, Random, Equidistant) on 10D Funnel benchmark, measuring ELBO gap and training stability

2. **Wall-clock time comparison:** Implement Local (DB) and Global (TB) objectives with identical architectures; measure actual training time and memory usage on fixed GPU budget

3. **Real-world distribution test:** Apply method to high-dimensional structured distribution from real application (e.g., molecular dynamics energy landscape or natural images) beyond synthetic benchmarks and VAE latent space