---
ver: rpa2
title: 'Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit
  and $\ell_1$ Relaxations'
arxiv_id: '2601.00329'
source_url: https://arxiv.org/abs/2601.00329
tags:
- coalition
- bgcp
- structure
- probabilistic
- coalitions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes sparse probabilistic coalition structure generation\
  \ (CSG) to handle episodic observations of coalition payoffs, where the underlying\
  \ characteristic function is sparse in a chosen coalition basis. The main idea is\
  \ to estimate the sparse parameter vector \u03B8\u22C6 governing coalition contributions\
  \ using two statistical approaches: (1) a Bayesian greedy pursuit (BGCP) algorithm\
  \ mimicking orthogonal matching pursuit, and (2) an \u21131-penalised Lasso estimator."
---

# Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and $\ell_1$ Relaxations

## Quick Facts
- **arXiv ID**: 2601.00329
- **Source URL**: https://arxiv.org/abs/2601.00329
- **Reference count**: 34
- **Primary result**: Sparse probabilistic CSG using BGCP and Lasso achieves exact support recovery and welfare guarantees under coherence/RE conditions when T ≳ K log m

## Executive Summary
This paper addresses coalition structure generation (CSG) from episodic observations of coalition payoffs under a sparse underlying characteristic function. The key insight is that the true coalition contributions θ* are sparse in a chosen coalition basis, enabling statistical estimation to reduce the combinatorial search space. Two estimation approaches are proposed: (1) Bayesian Greedy Pursuit (BGCP), a greedy algorithm inspired by OMP that achieves exact support recovery under mutual incoherence conditions, and (2) an ℓ₁-penalized Lasso estimator that achieves prediction error bounds under restricted eigenvalue conditions. The paper establishes non-asymptotic guarantees for both methods and extends them to random episodic design settings, showing these conditions hold with high probability when sample size T ≥ C₀K log m. The results identify regimes where sparse estimation outperforms dense baselines and provide a principled framework for scalable CSG in high-dimensional settings.

## Method Summary
The method operates in two phases: (1) statistical estimation of the sparse coalition contribution vector θ* from T episodic observations (Y_t, X_t) where Y = Xθ* + ε, using either BGCP (greedy support recovery) or Lasso (ℓ₁-regularized estimation), followed by (2) running a classical CSG solver on the reduced coalition set. The sparse estimation layer exploits the fact that |supp(θ*)| = K ≪ m to achieve sample complexity T ≳ K log m rather than T ≳ m. BGCP iteratively selects coalitions maximizing correlation with residuals and refits via least squares, achieving exact recovery under mutual coherence μ(X) < 1/(2K-1). Lasso uses regularization λ = c₀σ√(log m/T) and achieves ℓ₂ error bounds under restricted eigenvalue conditions. The CSG solver then finds the optimal partition from the inferred coalition set, with welfare guarantees depending on the estimation accuracy.

## Key Results
- BGCP achieves exact support recovery P(Ŝ = S*) ≥ 1 - c₂m^(-c₃) when T ≳ K log m under mutual coherence μ(X) < 1/(2K-1)
- Lasso achieves ℓ₂ error ∥θ̂ - θ*∥₂ ≤ C₃σK^(1/2)√(log m/T)/κ* under restricted eigenvalue condition
- Random episodic designs satisfy coherence and RE conditions with high probability when T ≥ C₀K log m
- Sparse methods outperform dense baselines (EPC, DLS) in high-dimensional regimes (K ≪ m), while dense methods are competitive when K/m is large

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BGCP achieves exact support recovery with probability ≥ 1 - c₂m^(-c₃) when T ≳ K log m
- Mechanism: Greedy selection identifies true coalition indices by maximizing correlation |X_j^T r_k|/T at each iteration; the orthogonal projection step ensures residuals preserve signal from remaining true atoms. Under coherence μ(X) < 1/(2K-1), the correlation separation inequality guarantees true indices dominate false ones at every step.
- Core assumption: Mutual coherence bound and minimum signal strength θ_min ≥ C₀σ√(log m/T) dominate noise correlations.
- Evidence anchors: Abstract states BGCP recovers true coalitions with high probability once T ≳ K log m; Section 3.5 proves P(S_BGCP = S^*) ≥ 1 - c₂m^(-c₃) via induction.

### Mechanism 2
- Claim: Lasso achieves ℓ₂ error ∥θ̂ - θ*∥₂ ≤ C₃σK^(1/2)√(log m/T)/κ* under restricted eigenvalue condition
- Mechanism: The basic inequality forces estimation error Δ onto the cone {u : ∥u_{(S*)^c}∥₁ ≤ 3∥u_S*∥₁}; RE condition then lower-bounds ∥XΔ∥₂ in terms of ∥Δ_S*∥₂, yielding error control. Regularization λ = c₀σ√(log m/T) dominates dual noise with high probability.
- Core assumption: Population RE constant κ_Σ > 0 and regularization sufficiently large to satisfy dual feasibility.
- Evidence anchors: Abstract states ℓ₁ estimator achieves ℓ₂ and prediction error bounds of order σ√K log m/T; Section 4.4 derives bounds (30)-(33) with explicit probability 1 - C₁m^(-C₂).

### Mechanism 3
- Claim: Random episodic design satisfies coherence and RE conditions with high probability when T ≥ C₀K log m
- Mechanism: Empirical Gram matrix Σ̂ = X^TX/T concentrates entrywise around population Σ via Hoeffding/Bernstein. If population satisfies μ(Σ) < 1/(4K) and κ_Σ² > 0, then deviations of order √(log m/T) preserve empirical coherence < 1/(2K) and RE constant ≥ κ_Σ/√2.
- Core assumption: Rows X_t are i.i.d., ∥X_t∥₀ ≤ q (bounded sparsity), and population covariance has diagonal ≥ ν_min and restricted eigenvalue κ_Σ².
- Evidence anchors: Section 5.4 states with probability at least 1 - c₇m^(-c₈) the following hold simultaneously: μ(X) < 1/(2K-1), κ(S*,3) ≥ κ_Σ/√2; Section 5.2 provides entrywise concentration.

## Foundational Learning

- **Concept: Mutual coherence** μ(X) = max_{i≠j} |X_i^T X_j|/(∥X_i∥₂∥X_j∥₂)
  - Why needed here: Determines whether greedy selection can distinguish true from false coalitions; bound μ(X) < 1/(2K-1) ensures exact recovery.
  - Quick check question: Given normalized columns, what is the maximum allowable coherence for K=5 sparse signals to guarantee OMP recovery?

- **Concept: Restricted eigenvalue (RE) condition**
  - Why needed here: Replaces global invertibility in high dimensions; guarantees Lasso error scales with sparsity K rather than dimension m.
  - Quick check question: For a design with collinear columns outside the true support, does RE necessarily fail?

- **Concept: Support recovery vs. prediction error**
  - Why needed here: BGCP targets exact support (structural identification); Lasso targets prediction (welfare gap). These require different assumptions and yield different guarantees.
  - Quick check question: Can a Lasso estimator with small prediction error still fail to recover the true support?

## Architecture Onboarding

- **Component map**: Data collection -> Sparsification (BGCP/Lasso) -> Coalition set reduction -> CSG solver -> Welfare evaluation
- **Critical path**: Data collection → Sparsification (BGCP/Lasso) → Coalition set reduction → CSG solver → Welfare evaluation
  - Assumption: CSG solver is exact or α-approximate per Remark 5.7
- **Design tradeoffs**:
  - BGCP: Exact recovery under strong coherence; greedy, non-convex; cost O(TmK)
  - Lasso: Approximate welfare under weaker RE; convex, robust; cost O(TmI_ℓ₁); admits O(K) false positives
  - EPC baseline: Cheapest (O(Tq)), no sparsity exploitation, welfare gap ∼ √(K/T)
  - DLS baseline: Requires T ≳ m, optimal in dense regime
- **Failure signatures**:
  - BGCP stalls: Residual norm stops decreasing but support size < K (coherence violation or θ_min too small)
  - Lasso over-selects: |supp(θ̂)| ≫ K (λ under-tuned or RE weak)
  - Welfare gap persists despite low estimation error: CSG solver suboptimal or reduced set excludes optimal coalitions
- **First 3 experiments**:
  1. **Synthetic coherence stress test**: Generate X with controlled μ(X); sweep T from K log m to 10K log m; measure BGCP support recovery rate and Lasso false positive count. Verify Theorems 3.7 and 4.6 scaling.
  2. **Random design validation**: Sample rows X_t from Assumption 5.1 with varying q and T; empirically estimate P(μ(X) < 1/(2K-1)) and P(κ(S*,3) ≥ threshold). Compare to Theorem 5.5 predictions.
  3. **End-to-end welfare comparison**: On sparse ground truth (K ≪ m), compare BGCP→CSG, Lasso→CSG, EPC→CSG, and oracle CSG. Measure ∆_Alg(T) and verify Corollary 6.4 regime separation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the ℓ₁ Lasso estimator achieve exact support recovery (supp(θ̂) = S*) in the probabilistic CSG setting under irrepresentable or mutual incoherence conditions?
- **Basis in paper**: Remark 4.7 states: "To obtain exact support recovery supp(θ̂) = S*, one typically requires stronger conditions... We do not spell out these additional conditions here, as our probabilistic coalition structure generation analysis in later sections only requires control of the error and the number of spurious coalitions."
- **Why unresolved**: The paper provides only O(K) false positive control under the RE condition, not exact recovery, which would tighten welfare guarantees.
- **What evidence would resolve it**: Deriving explicit irrepresentable conditions for the random episodic design model and proving that they hold with high probability when T ≳ K log m.

### Open Question 2
- **Question**: What is the precise sparsity threshold ρ* = K/m at which one should transition from sparse (BGCP/ℓ₁) to dense (DLS) estimation methods for optimal welfare guarantees?
- **Basis in paper**: Section 6.6 identifies distinct sparse (K ≪ m) and dense (K/m → ρ ∈ (0,1]) regimes, but does not characterize the crossover point where methods have comparable performance.
- **Why unresolved**: The welfare gap rates differ by factors involving K√(log m/T) versus √(m/T), leaving the transition region unspecified.
- **What evidence would resolve it**: Computing the critical ρ* where σK√(log m/T)/κ²Σ ≈ σ√(m/T) and verifying empirically where each method dominates.

### Open Question 3
- **Question**: Can probabilistic CSG be solved in an online/sequential setting where coalition structures must be committed after each episode rather than after collecting all T observations?
- **Basis in paper**: The paper assumes all T episodes are observed before running CSG (Section 2.5), but real applications like repeated auctions may require immediate decisions.
- **Why unresolved**: The batch estimation framework does not address exploration-exploitation trade-offs inherent in sequential decision-making.
- **What evidence would resolve it**: Designing an online algorithm with regret bounds relative to the optimal batch solution, or proving that online regret scales similarly to the batch welfare gap.

### Open Question 4
- **Question**: Do the theoretical guarantees hold under heavy-tailed noise (e.g., sub-exponential or bounded-moment) rather than sub-Gaussian noise?
- **Basis in paper**: Assumption 3.2(A2) requires sub-Gaussian noise for concentration bounds in Lemma 3.3, but many economic/payoff distributions exhibit heavier tails.
- **Why unresolved**: The proof techniques rely on Gaussian-style tail bounds (exp(-ct²)) which fail for heavier-tailed distributions.
- **What evidence would resolve it**: Deriving analogous noise correlation bounds using robust estimators (e.g., median-of-means) and showing welfare guarantees degrade gracefully with tail heaviness.

## Limitations

- The coherence and restricted eigenvalue assumptions may not hold in realistic coalition formation scenarios with structured correlations
- Guarantees assume known noise variance σ for regularization tuning, requiring data-driven estimation in practice
- The combinatorial CSG solver's performance is treated abstractly without considering specific algorithms' runtime or approximation quality
- The paper lacks empirical validation to demonstrate theoretical scaling predictions

## Confidence

- **High confidence**: The theoretical framework for BGCP and Lasso estimation is sound, following established compressed sensing literature
- **Medium confidence**: The extension to the CSG context is mathematically rigorous, but practical applicability depends on whether real-world coalition structures exhibit sufficient sparsity
- **Low confidence**: Without empirical validation, the predicted performance gaps between methods remain theoretical

## Next Checks

1. **Synthetic coherence stress test**: Generate controlled random designs with varying coherence μ(Σ) and verify the exact recovery threshold for BGCP (μ < 1/(2K-1)) and the breakdown point for Lasso under different RE conditions

2. **Random design concentration validation**: Sample multiple random episodic designs and empirically measure P(μ(X) < 1/(2K-1) ∧ κ(S*,3) ≥ κ_Σ/√2) as a function of T/K log m to validate Theorem 5.5 predictions

3. **End-to-end welfare performance sweep**: Implement all four methods (BGCP→CSG, Lasso→CSG, EPC→CSG, DLS→CSG) on synthetic sparse ground truth across the full regime space (K/m, T/K log m) to empirically identify the transition boundaries between sparse and dense method dominance