---
ver: rpa2
title: 'PSDNorm: Test-Time Temporal Normalization for Deep Learning in Sleep Staging'
arxiv_id: '2503.04582'
source_url: https://arxiv.org/abs/2503.04582
tags:
- psdnorm
- normalization
- datasets
- sleep
- batchnorm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PSDNorm is a temporal normalization layer that leverages Monge
  mapping and power spectral density (PSD) alignment to address distribution shifts
  in deep learning for signals, particularly in sleep staging. Unlike standard normalization
  layers (BatchNorm, LayerNorm, InstanceNorm), which normalize feature maps without
  considering temporal correlations, PSDNorm aligns each feature map's PSD to a running
  Riemannian barycenter using geodesic interpolation.
---

# PSDNorm: Test-Time Temporal Normalization for Deep Learning in Sleep Staging

## Quick Facts
- **arXiv ID:** 2503.04582
- **Source URL:** https://arxiv.org/abs/2503.04582
- **Reference count:** 40
- **Primary result:** PSDNorm achieves 79.51% balanced accuracy on unseen sleep datasets, requiring 4× less labeled data than baselines.

## Executive Summary
PSDNorm is a novel temporal normalization layer designed to address distribution shifts in deep learning for sleep staging. Unlike standard normalization layers that normalize feature maps without considering temporal correlations, PSDNorm aligns each feature map's power spectral density (PSD) to a running Riemannian barycenter using geodesic interpolation and Monge mapping. This enables effective normalization of temporal correlations within the network architecture. Evaluated on 10 sleep datasets covering 10K subjects, PSDNorm consistently outperformed baselines, achieving state-of-the-art performance on unseen datasets while demonstrating superior data efficiency.

## Method Summary
PSDNorm is a temporal normalization layer that leverages Monge mapping and power spectral density (PSD) alignment to address distribution shifts in deep learning for signals, particularly in sleep staging. Unlike standard normalization layers (BatchNorm, LayerNorm, InstanceNorm), which normalize feature maps without considering temporal correlations, PSDNorm aligns each feature map's PSD to a running Riemannian barycenter using geodesic interpolation. This enables effective normalization of temporal correlations within the network architecture.

## Key Results
- Achieved 79.51% balanced accuracy across subjects on unseen datasets
- Required 4× less labeled data to match baseline accuracy
- Improved performance for 91% of subjects in challenging datasets like CHAT
- Architecture-agnostic, working well with both U-Sleep and CNNTransformer models

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Alignment via Monge Mapping
Normalizing temporal correlations in the frequency domain (Power Spectral Density) appears to address distribution shifts more effectively than time-domain statistics used in BatchNorm or InstanceNorm. The layer estimates the PSD of feature maps and computes a Monge map (optimal transport filter) H, which aligns the source PSD to a target PSD (the barycenter) via a convolution operation, effectively whitening and re-coloring the signal to match a canonical spectral template. The core assumption is that distribution shifts in sleep EEG are primarily captured by second-order statistics (covariance/PSD), and the underlying signal structure can be locally approximated by centered Gaussian distributions.

### Mechanism 2: Geodesic Barycenter Aggregation
Maintaining a running Riemannian barycenter of PSDs stabilizes training and creates a robust target domain for alignment. Instead of normalizing to a zero-mean/unit-variance (Euclidean), the method computes a Wasserstein barycenter on the Riemannian manifold of PSDs. This creates a geometrically valid "average" spectral signature across subjects. The core assumption is that the manifold of PSDs is a valid geometric space where interpolation via geodesics preserves physical signal characteristics better than linear averaging.

### Mechanism 3: Intra-Network Spectral Normalization
Applying spectral normalization as a learnable layer within the deep architecture corrects for internal covariate shifts that simple input preprocessing cannot. Unlike TMA (Temporal Monge Alignment) which is a fixed preprocessing step, PSDNorm operates on intermediate feature maps. This allows the model to correct spectral distortions induced by prior convolution layers. The core assumption is that intermediate feature maps in sleep CNNs retain sufficient temporal auto-correlation structure to make PSD estimation meaningful.

## Foundational Learning

- **Concept:** Riemannian Manifolds & Bures Metric
  - **Why needed here:** Standard Euclidean averages are invalid for PSD matrices (which must remain positive semi-definite). Understanding that PSDs lie on a curved manifold explains why the paper uses "geodesic interpolation" rather than simple averaging.
  - **Quick check question:** Why does the paper use √P in Equation 6 instead of just averaging the PSD matrices directly?

- **Concept:** Optimal Transport & Monge Mapping
  - **Why needed here:** This is the mathematical engine driving the alignment. It explains how the layer derives the exact filter H needed to morph one distribution into another with minimal distortion.
  - **Quick check question:** How does the f-Monge mapping (Equation 4) differ from simply scaling the signal by its standard deviation?

- **Concept:** Power Spectral Density (PSD) & Welch's Method
  - **Why needed here:** The mechanism relies entirely on accurately estimating the "frequency color" of the signal. Welch's method (averaging periodograms) is used to trade off frequency resolution for variance reduction in the estimate.
  - **Quick check question:** Why does the paper estimate PSD using overlapping segments (Welch) rather than a single Fourier Transform of the whole window?

## Architecture Onboarding

- **Component map:** Centering -> PSD Estimator -> Barycenter Update -> Monge Filter -> Alignment
- **Critical path:** The stability of the Running Barycenter. If the initial P̂ is poorly initialized or the momentum α is wrong, the target domain will oscillate, preventing convergence. The filter size f is also critical; it defines the frequency resolution of the normalization.
- **Design tradeoffs:** Filter Size f: Small f (≈1) recovers InstanceNorm (scalar scaling); Large f (≈17) offers fine-grained spectral shaping but increases compute cost O(N c ℓ f log f) and may overfit to specific noise patterns. Layer Placement: Paper applies it to the first 3 conv layers. Replacing all BatchNorms may destroy useful learned spectral features deep in the network.
- **Failure signatures:** Spectral Collapse: Barycenter P̂ trends to zero or infinity (divergence). N1 Class Confusion: Over-smoothing transients (like K-complexes) if f is too large, as the filter acts as a strong frequency notch. High Variance: Standard deviation across seeds increases (observed in Table 2 for some datasets like CHAT with InstanceNorm, but PSDNorm generally reduces this).
- **First 3 experiments:**
  1. Sanity Check (Ablation on f): Train U-Sleep on a single dataset (e.g., SHHS) with f ∈ {1, 5, 17}. Verify that f=1 mimics InstanceNorm and f=5 provides a measurable bump in balanced accuracy on a validation set.
  2. Domain Generalization (LODO): Train on 9 datasets, test on the 10th (e.g., CHAT). Compare PSDNorm vs. BatchNorm. Look specifically for improvement in the hardest classes (N1/Wake transition).
  3. Low-Data Regime: Train using only 40 subjects per dataset (balanced@40). The paper claims PSDNorm is 4x more data efficient; verify if the model converges faster or to a higher accuracy than BatchNorm with limited labels.

## Open Questions the Paper Calls Out

### Open Question 1
Can the filter size hyperparameter (f) be adapted automatically during training instead of being manually tuned? The authors state in the Conclusion that "selecting it automatically in adaptive settings could be challenging." Currently, f is a fixed hyperparameter requiring sensitivity analysis, with the paper identifying a range (5 to 11) that works well but no mechanism for dynamic adaptation. What evidence would resolve it: A method that adapts f per layer or per dataset dynamically, achieving comparable or superior performance without manual grid search.

### Open Question 2
Is PSDNorm effective for distribution shift in non-biological time-series domains, such as audio or financial data? The Conclusion lists "extending it to other signals such as audio" as a specific direction for future use. The experimental validation is restricted to sleep staging (EEG/EOG), leaving its efficacy on other signal types with different spectral properties unproven. What evidence would resolve it: Benchmarks on standard audio classification or speech recognition datasets showing that PSDNorm outperforms InstanceNorm or BatchNorm under domain shift.

### Open Question 3
What is the optimal strategy for placing PSDNorm layers within deep network architectures? The authors apply PSDNorm to the first three layers and note in Appendix A.14 that performance plateaus after 3 layers, but provide no theoretical justification for this specific depth placement. It is unclear if PSDNorm is universally beneficial only in early stages or if its utility depends on the frequency content of specific feature maps at different depths. What evidence would resolve it: An analysis correlating the spectral shift of feature maps at various depths with the performance gain of inserting PSDNorm at those specific points.

## Limitations
- Effectiveness predicated on distribution shifts being primarily captured by second-order statistics (PSD); may not resolve shifts involving phase information or higher-order temporal patterns
- Introduces computational overhead proportional to filter size, which could be prohibitive for very large models or high sampling rates
- Barycenter update mechanism's robustness to non-stationary or multimodal spectral distributions across subjects is not explicitly tested

## Confidence
- **High Confidence:** Data efficiency claims (4× less labeled data) and cross-dataset performance (79.51% BACC on unseen datasets) are well-supported by Table 2 and Figure 3
- **Medium Confidence:** The mechanism of frequency-domain alignment via Monge mapping is theoretically sound, but its superiority over simpler spectral whitening methods needs direct comparison
- **Medium Confidence:** The claim that PSDNorm corrects internal covariate shifts better than TMA is plausible given the architecture, but the ablation study comparing different normalization layers is limited to 5 methods

## Next Checks
1. **Direct Baseline Comparison:** Implement and compare against a simpler spectral normalization method (e.g., fixed bandpass filtering or InstanceNorm) to isolate the benefit of the Monge mapping and Riemannian barycenter
2. **Phase Sensitivity Analysis:** Evaluate model performance on synthetic datasets where only phase information is shifted (not spectral power) to test the method's sensitivity to phase vs. amplitude changes
3. **Deep Network Stress Test:** Apply PSDNorm to all BatchNorm layers in a deeper architecture (e.g., beyond the first 3 layers) to test if it can correct spectral distortions induced by complex feature transformations, or if it degrades performance by over-normalizing learned features