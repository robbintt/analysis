---
ver: rpa2
title: 'MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational
  Question Generation'
arxiv_id: '2511.18714'
source_url: https://arxiv.org/abs/2511.18714
tags:
- image
- angle
- text
- multimodal
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MAGMA-Edu introduces a training-free multi-agent framework for\
  \ generating multimodal educational questions with text\u2013diagram pairs. It addresses\
  \ the challenge of creating pedagogically coherent and semantically consistent educational\
  \ visuals by decomposing the task into a two-stage process: text refinement (Stage\
  \ 1) and code-based diagram generation (Stage 2), each guided by iterative generation\u2013\
  verification\u2013reflection loops."
---

# MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational Question Generation

## Quick Facts
- arXiv ID: 2511.18714
- Source URL: https://arxiv.org/abs/2511.18714
- Reference count: 40
- Multi-agent framework generates multimodal educational questions with text-diagram pairs, achieving Avg-Text 96.20 and ITC 99.12

## Executive Summary
MAGMA-Edu addresses the challenge of generating pedagogically coherent and semantically consistent educational visuals by decomposing the task into two stages: text refinement and code-based diagram generation. Each stage employs iterative generation-verification-reflection loops guided by specialized agents. The framework improves average textual metrics from 57.01 to 92.31 (+35.3 pp) and boosts image-text consistency from 13.20 to 85.24 (+72 pp) compared to GPT-4o, establishing a new state of the art for multimodal educational content generation.

## Method Summary
MAGMA-Edu is a training-free multi-agent framework that generates multimodal educational questions through a two-stage pipeline. Stage 1 refines problem statements and image descriptions using three specialized agents (Generator, Validator, Reflector) in iterative loops until quality thresholds are met. Stage 2 translates these descriptions into executable Python/Matplotlib code through another three-agent loop (Code Generator, Code Executor, Code Reflector) to ensure geometric fidelity and semantic alignment. The framework uses DeepSeek-V3.1 as default backbone but is tested across multiple models, operating on a custom K-12 math dataset with 78 knowledge points.

## Key Results
- Achieves Avg-Text 96.20 and ITC 99.12, surpassing GPT-4o's 57.01 Avg-Text and 13.20 ITC
- Stage 2 code generation alone improves ITC from 13.20 to 75.65 (+62.45 pp) for GPT-4o
- Text refinement stage improves Avg-Text by 35.3 pp compared to baseline
- Optimal iteration count found to be 7-8 for maximum quality gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative Generate-Validate-Reflect loops substantially improve text quality and cross-modal consistency compared to single-pass generation.
- Mechanism: Three specialized agents operate in closed loops with the Validator evaluating outputs against multi-dimensional metrics, the Reflector synthesizing diagnostic feedback into actionable revision signals, and the Generator incorporating these in next iterations until quality thresholds or iteration limits are reached.
- Core assumption: LLMs can reliably identify errors in their own outputs when prompted with structured evaluation criteria and can translate feedback into effective corrections.
- Evidence anchors: Abstract mentions "generation-verification-reflection loop that iteratively refines question statements," Section 4.1 provides formal update rules, and Figure 3 shows steady score gains with iterations.

### Mechanism 2
- Claim: Using executable code as intermediate representation dramatically improves image-text consistency compared to direct MLLM image synthesis.
- Mechanism: Instead of generating images directly (which MLLMs do poorly for geometric diagrams), the system generates Python/Matplotlib code that is executed to render diagrams, serving as a verifiable "lingua franca" where geometric constraints are explicit.
- Core assumption: The mapping from natural language descriptions to executable drawing code preserves geometric semantics with high fidelity, and code execution produces visually correct outputs.
- Evidence anchors: Abstract mentions "code-based intermediate representation that enforces geometric fidelity," Table 2 shows Stage 2 ablation improves ITC from 13.20 to 75.65 for GPT-4o, and Section 4.2 describes four specialized agents ensuring code validity.

### Mechanism 3
- Claim: Stage separation with bounded iteration prevents error propagation while enabling cross-modal co-optimization.
- Mechanism: Stage 1 optimizes text to convergence before Stage 2 begins image generation, creating a stable textual specification for visual synthesis while preventing premature feedback loops.
- Core assumption: Text quality reaches an acceptable plateau before image generation begins, and errors in Stage 1 can be detected without visual context.
- Evidence anchors: Section 4 describes "two-stage co-evolutionary pipeline" with independent loops, Table 2 shows both stages contribute (Stage 1 improves Avg-Text, Stage 2 improves ITC), and related work shows stage decomposition as a broader design pattern.

## Foundational Learning

- **Multi-Agent Orchestration Patterns**
  - Why needed here: MAGMA-Edu coordinates 7 agents across 2 stages with specific data contracts. Without understanding role separation and iteration control, implementation will devolve into unstructured prompting.
  - Quick check question: If the Text Reflector Agent detects a conflict between question text and image description, which should it prioritize revising, and where is this rule specified?

- **Code-as-Intermediate-Representation (Code-IR)**
  - Why needed here: The core innovation is generating executable Python rather than images. This requires understanding how to prompt for code generation, validate syntax/semantics, and handle execution failures.
  - Quick check question: What three validation stages must pass for a diagram to achieve positive Image-Text Consistency (ITC)?

- **Bounded Optimization with LLM Agents**
  - Why needed here: Unlike gradient descent, termination here is rule-based (threshold or max iterations). Understanding when to stop is critical for both quality and cost.
  - Quick check question: What happens if Q_text(T^(i)) never reaches τ_text before I_max iterations?

## Architecture Onboarding

- **Component map:**
  Stage 1: [User Prompt] → Text Generator → Text Validator (6 checks) → Text Reflector → back to Generator (if not converged)
  Stage 2: [T*, image_desc] → Code Generator → Code Executor → Image Validator (3 checks) → Image Reflector → back to Code Generator (if not converged)
  Final Output: (T*, G*) as multimodal question

- **Critical path:** Stage 1 convergence → Stage 2 code generation → Code execution → Image validation. The longest latency path is iterative refinement in Stage 2 if code repeatedly fails validation (up to I_max=5 iterations default, but experiments show T=7-8 may be optimal).

- **Design tradeoffs:**
  - Higher I_max improves quality (Figure 3 shows gains through iteration 7-8) but increases API costs and latency linearly
  - Threshold tuning (τ_text, τ_visual) controls precision-recall: tighter thresholds improve output quality but may cause more non-convergence cases
  - Code-IR trades generative flexibility for verifiability: can't render arbitrary artistic styles, but guarantees geometric precision

- **Failure signatures:**
  1. Infinite loop without convergence: Validator repeatedly fails but Reflector provides ineffective feedback. Detect via iteration count hitting I_max with low final scores.
  2. Code execution errors: Generated Python has syntax errors or runtime exceptions. Executor captures these; check Code Executor logs.
  3. Semantic drift: Iterations improve validator scores but output diverges from original user intent. Monitor User Orientation (UO) metric across iterations.

- **First 3 experiments:**
  1. Reproduce Stage 2 ablation: Take a single knowledge point (e.g., "Pythagorean theorem"), run code generation without reflection loop (I_max=1), then with reflection (I_max=5), compare ITC scores to validate core mechanism.
  2. Agent prompt sensitivity: Swap Text Validator prompts for one metric (e.g., AA—"Accurate Analysis") with a simpler version, run 20 problems, measure if AA scores degrade relative to baseline.
  3. Cross-backbone validation: Replace DeepSeek-V3.1 with GPT-4o-mini as agent backbone for Stage 1 only, keeping Stage 2 fixed. Compare Avg-Text scores to understand backbone dependency vs. framework contribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Model hyperparameters (temperature, top_p, max_tokens) for each agent are unspecified, making exact replication difficult.
- Quality thresholds (τ_text, τ_visual) for early stopping are not provided, affecting convergence behavior.
- Evaluation rubric for 6 textual metrics and ITC is conceptually described but not fully specified.
- Full dataset with 78 knowledge points and prompts is not released.

## Confidence
- **High Confidence**: Stage 2 (Code-IR) mechanism for ITC improvement (evidence from ablation showing 62.45 pp gain for GPT-4o)
- **Medium Confidence**: Stage 1 (Text Refinement) effectiveness for Avg-Text improvement (35.3 pp gain)
- **Medium Confidence**: Iterative refinement loops' effectiveness (Figure 3 shows gains through iterations 7-8)

## Next Checks
1. Code-IR Mechanism Validation: Run Stage 2 ablation with identical input (e.g., Pythagorean theorem) comparing I_max=1 vs I_max=5, measuring ITC scores to confirm 62.45 pp improvement claim.
2. Evaluation Rubric Clarity: Implement one textual metric (e.g., Accurate Analysis/AA) using paper's conceptual description, run on 20 generated problems, compare scores to published range to validate rubric interpretation.
3. Cross-Backbone Comparison: Replace DeepSeek-V3.1 with GPT-4o-mini for Stage 1 only, keeping Stage 2 fixed, measure Avg-Text score change to isolate framework contribution from backbone quality.