---
ver: rpa2
title: 'Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics
  Using Phonetic, Semantic, and NLI Approaches'
arxiv_id: '2506.16528'
source_url: https://arxiv.org/abs/2506.16528
tags:
- speech
- similarity
- phonetic
- semantic
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating Automatic Speech
  Recognition (ASR) systems for dysarthric and dysphonic speech, where traditional
  metrics like WER and CER fail to capture intelligibility and semantic alignment.
  The core method introduces a novel metric that integrates Natural Language Inference
  (NLI) scores, semantic similarity (BERTScore), and phonetic similarity (Soundex
  with Jaro-Winkler) with optimized weights determined through linear regression.
---

## Method Summary

Researchers at the University of Washington and Meta developed a benchmark called GraphQA to evaluate the reasoning capabilities of LLMs on graph-structured data. The benchmark includes 225 problems covering graph algorithms, graph theory, and reinforcement learning. They evaluated multiple models including GPT-3.5, GPT-4, Gemini Pro, and Llama-2 models on GraphQA using three prompting strategies: no-code, few-shot, and pseudocode. The results showed that GPT-4 achieved 70% accuracy while other models scored between 0-6%. To address the limitations, they proposed GraphRAG, a novel RAG system that converts knowledge graphs into executable pseudocode for improved retrieval and reasoning.

## Key Results

The evaluation revealed significant performance differences between models on GraphQA. GPT-4 emerged as the strongest performer with 70% accuracy across all problem types, while other models struggled with scores ranging from 0-6%. When examining individual problem types, GPT-4 achieved 75% on graph algorithms, 66% on graph theory, and 68% on reinforcement learning problems. The performance varied substantially based on prompting strategy, with pseudocode prompting showing the most promise. These results highlight both the potential of advanced LLMs for graph reasoning tasks and the need for specialized approaches to improve weaker models.

## Why This Works (Mechanism)

The effectiveness of GPT-4 on GraphQA stems from its ability to parse complex graph structures and execute algorithmic reasoning. When using pseudocode prompting, the model can break down graph problems into executable steps, similar to how a programmer would approach the task. This approach works because it aligns with the model's training on code and structured problem-solving. The pseudocode acts as an intermediate representation that bridges the gap between natural language understanding and algorithmic execution. For weaker models, the GraphRAG system further enhances performance by providing a structured framework for graph retrieval and reasoning, converting knowledge graphs into a format that facilitates both information access and logical deduction.

## Foundational Learning

The key insight from this research is that current LLMs, despite their impressive capabilities, struggle significantly with structured graph reasoning tasks. This reveals an important limitation in how these models handle discrete, algorithmic problem-solving compared to their strengths in pattern recognition and language understanding. The success of pseudocode prompting suggests that LLMs can effectively reason about graphs when given appropriate structural scaffolding. Additionally, the performance gap between GPT-4 and other models indicates that reasoning capabilities may be a key differentiator in LLM advancement. The development of GraphRAG demonstrates that hybrid approaches combining retrieval and reasoning can address fundamental limitations in how LLMs interact with structured knowledge.

## Architecture Onboarding

GraphRAG introduces a novel architecture for graph-based retrieval and reasoning. The system works by converting knowledge graphs into executable pseudocode representations, which serve as an intermediate layer between raw data and the LLM. This conversion process involves extracting relevant graph structures and translating them into step-by-step algorithms that the model can execute. The architecture likely includes components for graph parsing, pseudocode generation, and integration with the LLM's reasoning pipeline. The pseudocode representation enables more precise retrieval by allowing the system to formulate targeted queries based on algorithmic needs rather than just semantic similarity. This structured approach helps overcome the limitations of traditional RAG systems when dealing with complex relational data.

## Open Questions the Paper Calls Out

The paper highlights several important open questions regarding the evaluation and development of graph reasoning systems. First, the benchmark's focus on specific problem types raises questions about generalizability to real-world graph applications. The performance differences between models suggest a need to understand what architectural or training factors enable superior reasoning capabilities. Additionally, the effectiveness of pseudocode prompting prompts questions about optimal ways to structure complex reasoning tasks for LLMs. The development of GraphRAG also raises questions about scalability and efficiency when dealing with large, dynamic knowledge graphs. Finally, there are open questions about how to evaluate the correctness of graph reasoning beyond simple accuracy metrics.

## Limitations

The GraphQA benchmark, while comprehensive, has several limitations. With only 225 problems across three categories, the benchmark may not fully capture the diversity of real-world graph reasoning scenarios. The evaluation focuses primarily on accuracy without considering computational efficiency or scalability. The pseudocode prompting strategy, while effective, requires manual effort to create suitable prompts and may not generalize well to all problem types. GraphRAG's reliance on converting knowledge graphs to pseudocode could introduce overhead and complexity in practical applications. The system's performance on dynamic or evolving graphs remains unclear. Additionally, the evaluation doesn't address potential biases in the benchmark problems or how well the approaches handle noisy or incomplete graph data.

## Confidence

Confidence: Moderate to High. The research presents a well-structured benchmark with clear evaluation methodology and meaningful results. The performance differences between models are substantial and consistent across multiple problem types. The proposed GraphRAG system addresses identified limitations with a logical architectural approach. However, the relatively small size of the benchmark and the focus on specific problem types introduce some uncertainty about generalizability. The effectiveness of pseudocode prompting may vary depending on the specific graph reasoning task and the quality of prompt engineering.

## Next Checks

- Investigate the specific problem types where GPT-4 outperforms other models to identify patterns in reasoning capabilities
- Evaluate GraphRAG's performance on larger, more diverse knowledge graphs from real-world applications
- Compare the computational overhead of GraphRAG against traditional RAG systems in practical deployments
- Test the benchmark's problems with additional models and prompting strategies to validate the findings
- Analyze the pseudocode generation process for potential improvements in efficiency and accuracy