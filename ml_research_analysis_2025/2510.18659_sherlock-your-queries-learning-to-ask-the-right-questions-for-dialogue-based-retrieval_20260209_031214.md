---
ver: rpa2
title: 'Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based
  Retrieval'
arxiv_id: '2510.18659'
source_url: https://arxiv.org/abs/2510.18659
tags:
- target
- number
- answer
- have
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SherlockLLM introduces a reinforcement learning-based framework
  for dialogue-driven information retrieval. The core idea is to train an agent to
  ask optimally informative yes/no questions, thereby efficiently narrowing the search
  space.
---

# Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval

## Quick Facts
- arXiv ID: 2510.18659
- Source URL: https://arxiv.org/abs/2510.18659
- Authors: Dong Yun; Marco Schouten; Dim Papadopoulos
- Reference count: 40
- Key outcome: A 7B model outperforms 96× larger models in dialogue-driven retrieval tasks

## Executive Summary
SherlockLLM introduces a reinforcement learning-based framework for dialogue-driven information retrieval where an agent learns to ask optimally informative yes/no questions to efficiently identify a target item from a candidate set. The core innovation is training a policy via GRPO to maximize expected information gain per question, thereby minimizing dialogue turns. Tested on structured tasks (Guess Number, Guess Who) and unstructured image retrieval (CelebA), the approach achieves near-perfect success rates on structured tasks and significantly outperforms much larger models on image retrieval, demonstrating robust performance across diverse retrieval scenarios.

## Method Summary
SherlockLLM trains a question-asking agent via reinforcement learning to identify target items through multi-turn yes/no dialogues. The agent uses a fine-tuned LLM (Qwen2.5-7B-Instruct) with LoRA to generate questions based on augmented dialogue history, which includes candidate summaries from a domain-specific retriever. Training employs GRPO with rewards based on expected information gain for structured tasks and rank improvement for image retrieval. The framework operates in a loop: retrieve candidates, augment history with summary information, generate question, receive answer from user simulator, and repeat until identification or turn limit. The approach outperforms both larger models and fine-tuned SFT baselines across multiple benchmarks.

## Key Results
- Near-perfect success rates (0.99-1.00) on structured Guess Number and Guess Who tasks with minimal turns
- On CelebA image retrieval, SherlockLLM achieves substantially higher success rates and drastically fewer turns than DeepSeek-V3.1 (96× more parameters)
- GRPO training yields significantly better efficiency than supervised fine-tuning, producing more optimal questioning strategies
- Performance approaches theoretical optimum defined by binary search on structured tasks

## Why This Works (Mechanism)
SherlockLLM frames dialogue-based retrieval as a sequential decision-making problem where the agent learns to maximize expected information gain per question. By modeling question generation as a policy to be optimized via reinforcement learning, the system can learn to ask questions that optimally split the candidate space rather than simply imitating expert demonstrations. The GRPO algorithm enables efficient policy optimization by reducing variance in gradient estimates, while the augmented history mechanism provides the agent with explicit feedback about candidate distributions. This combination allows the agent to develop sophisticated questioning strategies that adapt to the current state of uncertainty, rather than following fixed patterns.

## Foundational Learning
- **Concept: Expected Information Gain (EIG)**
  - Why needed here: This is the core mathematical signal used to guide the agent's questioning strategy in structured tasks. Without understanding EIG, one cannot interpret the reward function or the "optimality" of the agent's questions.
  - Quick check question: Given a candidate set of 8 items, what is the EIG of a question that splits the set into 2 and 6 items? (Answer: High, but not maximal. A 4/4 split yields 1 bit of information, which is maximal).

- **Concept: Policy Gradient Methods (specifically GRPO)**
  - Why needed here: This is the learning algorithm that actually optimizes the agent's behavior. Understanding the basics of policy gradients explains how the model improves from reward signals without direct supervision.
  - Quick check question: How does GRPO differ from standard Reinforce-style policy gradient? (Clue: Think about how it estimates the baseline to reduce variance).

- **Concept: Vision-Language Retrieval (CLIP)**
  - Why needed here: The image retriever is a critical component of the architecture for unstructured tasks. Its mechanics (embedding text and images into a shared space) and limitations (e.g., with negation) are essential for understanding the system's performance and failure modes.
  - Quick check question: How does the proposed retriever handle negative keywords (e.g., "a man *without* glasses") differently from a standard CLIP model? (Clue: Recall the discount mechanism).

## Architecture Onboarding

### Component Map
User Simulator/Target -> Retriever (Tabular/CLIP) -> Augmented History -> Questioner (Qwen2.5-7B-Instruct) -> Question -> User Simulator/Target (loop)

### Critical Path
The core execution loop is:
1. **Initialization:** A target item is selected, and the user provides an initial vague request.
2. **Turn Execution:**
   a. **Retrieve:** The Retriever takes the dialogue history and produces a candidate list and a summary (e.g., attribute distribution).
   b. **Augment:** The retriever's summary is appended to the user's last answer, forming the `augmented_answer`.
   c. **Generate Question:** The Questioner LLM receives the full augmented history and generates a new yes/no question.
   d. **Get Answer:** The User Simulator (or real user) answers the question.
3. **Loop/Exit:** The process repeats until the target is correctly identified or a turn limit is reached.

### Design Tradeoffs
- **GRPO vs. SFT:** GRPO learns a more efficient policy (fewer turns) from scratch but requires a simulator. SFT is faster but produces a less optimal, more imitative policy. The paper shows GRPO outperforms SFT.
- **Fixed vs. Dynamic Question Pool:** The authors use an open-ended LLM to generate questions on the fly, which is more flexible but harder to control than selecting from a pre-defined pool.
- **Rank vs. Similarity Reward:** The paper chose rank-based reward for image retrieval, prioritizing typical-case performance and efficiency (fewer turns) over the higher mean rank but lower efficiency of a similarity-based reward.

### Failure Signatures
- **Repetitive Questioning:** A common failure mode for smaller or untrained models, where the agent gets stuck in a loop asking the same or semantically similar questions.
- **Reward Hacking:** The agent could learn to generate questions that maximize the reward signal (e.g., easy splits) but are not helpful for finding the target.
- **Retriever Hallucination:** In image tasks, if the CLIP-based retriever is misled by complex or negated queries, the agent receives incorrect feedback, leading to poor subsequent questions.

### First 3 Experiments
1. **Baseline vs. SherlockLLM:** Run the trained SherlockLLM agent against baseline LLMs (e.g., the untrained base model, or a large model like DeepSeek) on the Guess Who task. Measure Success Rate and Mean Turns to verify the performance claims.
2. **Ablation on Feedback:** Retrain the agent on the CelebA task without providing the top-K image captions in the augmented history. Compare the final Success Rate and Mean Rank to quantify the value of explicit retriever feedback.
3. **Reward Function Swap:** Train two separate agents on the Guess Number task—one with only the EIG reward and one with only the step-length penalty. Compare their dialogue strategies (turn count vs. success rate) to understand the impact of different reward components.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several natural questions emerge:
- Can the dialogue policy learned via a simulated user generalize effectively to human users who may provide erroneous or inconsistent answers?
- How does SherlockLLM's performance scale when the candidate pool size increases by orders of magnitude?
- Does restricting the action space to binary questions limit efficiency compared to agents capable of asking open-ended questions?

## Limitations
- Evaluation relies on simulated user interactions rather than real human responses, raising questions about real-world generalization
- Performance on truly large-scale retrieval tasks (>500 items) remains untested
- The binary question constraint may limit efficiency for complex queries requiring multiple attributes
- Image retrieval evaluation uses synthetic captions rather than natural language queries

## Confidence
**High Confidence:**
- The SherlockLLM framework architecture and training methodology are clearly described and technically sound
- Performance improvements over baseline models on the structured Guess Number and Guess Who tasks are robust and well-supported
- The GRPO training approach and its superiority over SFT are convincingly demonstrated through controlled experiments

**Medium Confidence:**
- The 96× parameter efficiency claim relative to DeepSeek-V3.1 is accurate but should be interpreted with context (different model families, training objectives)
- The image retrieval results on CelebA show strong performance, but the synthetic caption generation via GPT-4o may not represent real-world complexity
- The generality claims across structured and unstructured domains are supported but could benefit from additional diverse test cases

**Low Confidence:**
- Real-world deployment performance with actual human users (not simulated)
- Scalability to significantly larger candidate sets (>500 items)
- Performance on truly open-domain queries without predefined attribute structures

## Next Checks
1. **User Simulator Validation:** Replace the DeepSeek-V3.1 user simulator with human annotators for 100 test cases on the Guess Who task. Compare success rates and mean turns between simulator-based and human-based evaluations to quantify any performance gap.

2. **Open-Domain Query Testing:** Apply SherlockLLM to a real-world image retrieval scenario using natural language queries from a dataset like Flickr30k without synthetic caption conversion. Measure success rates and dialogue efficiency compared to standard text-to-image retrieval baselines.

3. **Parameter Scaling Study:** Train SherlockLLM with varying model sizes (1B, 3B, 7B, 13B parameters) on the CelebA task. Plot success rate and mean turns against parameter count to identify whether the claimed 96× efficiency holds across different scales or is specific to the 7B model configuration.