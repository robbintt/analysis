---
ver: rpa2
title: Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback
arxiv_id: '2512.24818'
source_url: https://arxiv.org/abs/2512.24818
tags:
- dual
- convergence
- lemma
- preprint
- ptqq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the convergence properties of the Optimistic
  Multiplicative Weights Update (OMWU) algorithm for solving non-transitive preference
  games in the context of aligning large language models (LLMs) with human preferences.
  While existing methods typically rely on regularization, which introduces bias when
  computing the duality gap in the original game, OMWU offers a regularization-free
  approach.
---

# Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback

## Quick Facts
- **arXiv ID**: 2512.24818
- **Source URL**: https://arxiv.org/abs/2512.24818
- **Reference count**: 40
- **Primary result**: OMWU achieves last-iterate linear convergence after burn-in for zero-sum games without regularization bias, removing uniqueness assumptions from prior work

## Executive Summary
This work studies the convergence properties of the Optimistic Multiplicative Weights Update (OMWU) algorithm for solving non-transitive preference games in the context of aligning large language models (LLMs) with human preferences. While existing methods typically rely on regularization, which introduces bias when computing the duality gap in the original game, OMWU offers a regularization-free approach. The authors prove that OMWU achieves last-iterate linear convergence after a burn-in phase whenever a full-support Nash equilibrium exists, with an instance-dependent linear convergence rate to the original Nash equilibrium measured by duality gaps. This result improves upon prior work by removing the assumption of Nash equilibrium uniqueness and achieves polynomial (rather than exponential) dependence on instance-dependent constants for both convergence rate and burn-in time. The analysis introduces a novel framework for understanding how OMWU escapes undesirable regions of the strategy space, identifying a "marginal convergence behavior" where the probability of rarely played actions grows exponentially from exponentially small values.

## Method Summary
The method applies OMWU to find Nash equilibria in non-transitive preference games formulated as two-player zero-sum games. The algorithm uses an optimistic update rule that incorporates predictions of future gradients: θ^(t) = θ^(t-1) + ηPπ̂^(t) with π̂^(t+1) = θ^(t) + ηPπ(t). The learning rate must satisfy ηL < 1/2 where L = max|P_{a,a'}|. The key innovation is avoiding regularization while maintaining linear convergence guarantees. For neural policies, the authors use a 3-layer MLP with ReLU activations and Gaussian noise inputs. The convergence is measured by KL divergence to the Nash equilibrium and duality gap, with the algorithm showing a characteristic two-phase behavior: an initial burn-in phase with oscillatory dynamics followed by exponential convergence to equilibrium.

## Key Results
- OMWU achieves last-iterate linear convergence to the original Nash equilibrium (measured by duality gaps) after a burn-in phase, without requiring regularization
- The burn-in time and convergence rate depend polynomially (rather than exponentially) on instance-dependent constants ε and C_P
- The analysis removes the assumption of Nash equilibrium uniqueness required by prior work, while maintaining theoretical guarantees
- Experimental validation on both tabular and neural policy classes corroborates the theoretical convergence properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OMWU's iterates maintain a constant KL-projection target throughout training, enabling convergence prediction without requiring equilibrium uniqueness.
- Mechanism: The update rule `log ˆπ(t+1) = log ˆπ(t) + ηPπ(t) + ˆM(t+1)` preserves the KL-projection of iterates onto the Nash equilibrium set M. This follows because `xπ*, Pπ(t)y = 0` for any equilibrium π*, making the projection target invariant to the multiplicative update.
- Core assumption: A full-support Nash equilibrium exists (every action has positive probability in some equilibrium).
- Evidence anchors:
  - [abstract] "we do not require the assumption of NE uniqueness"
  - [Section 3.4, Lemma 2] "pp(ˆπ(1)) = pp(ˆπ(2)) = ... = pp(ˆπ(t))"
  - [corpus] Related work (Wei et al. 2020) required uniqueness; this work relaxes it.
- Break condition: If no full-support equilibrium exists, ε = 0 and the polynomial dependence bounds become undefined.

### Mechanism 2
- Claim: OMWU exhibits a two-phase convergence: a burn-in phase with potentially oscillatory behavior, followed by linear (exponential) convergence to the equilibrium.
- Mechanism: Define `Θ_t = D_KL(π*||ˆπ(t)) + 4η²L² D_KL(ˆπ(t)||π(t-1))`. This potential decreases monotonically when ηL < 1/2. During burn-in, the algorithm must "escape" regions where policy mass concentrates on action subsets that form subgame equilibria. The potential `Φ_t` captures this escape dynamics through correlation between log-policy deviation and preference gradients.
- Core assumption: Learning rate satisfies ηL < 1/2 where L = max|P_{a,a'}|.
- Evidence anchors:
  - [Section 4.2] "The dynamics naturally split into two phases: a burn-in stage, with oscillatory behavior; a convergence stage, with nearly linear decay"
  - [Figure 1] Shows Θ_t - Θ_{t+1} evolution with clear phase transition
  - [corpus] Related work shows similar two-phase behavior is common in game dynamics (limited direct corpus on OMWU specifically).
- Break condition: If ηL ≥ 1/2, the potential may not decrease monotonically; burn-in time becomes unbounded.

### Mechanism 3
- Claim: Rarely played actions grow exponentially from exponentially small values, enabling polynomial (rather than exponential) dependence on instance constants.
- Mechanism: The "marginal case" occurs when current policy is near-equilibrium on a restricted action subset A' but not on full action space A. Actions in A\A' have small probability but large preference gradient magnitude. The update `θ(t+1) = θ(t) + ηPˆπ(t)` drives these probabilities upward exponentially fast relative to their current magnitude, until full-support is restored.
- Core assumption: The instance constant C_P > 0, which requires the preference matrix restricted to the equilibrium subspace to have non-zero smallest singular value.
- Evidence anchors:
  - [abstract] "probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants"
  - [Section 4.2, Remark 4] "it takes roughly min_{a:(Pπ')_a>0} -log ˆπ(t)_a / (Pπ')_a steps to escape the marginal case"
  - [corpus] This "marginal convergence" analysis appears novel; no direct corpus precedent found.
- Break condition: If preference matrix has rank deficiency in equilibrium subspace (C_P = 0), convergence may be arbitrarily slow.

## Foundational Learning

- Concept: **Two-player zero-sum games and Nash equilibria**
  - Why needed here: NLHF formulates preference alignment as finding Nash equilibrium where no policy can be improved against an optimal opponent.
  - Quick check question: Given a skew-symmetric preference matrix P, what does π* = argmax_π min_{π'} π^T P π' mean?

- Concept: **Last-iterate vs average-iterate convergence**
  - Why needed here: Last-iterate convergence ensures the final trained policy is near-equilibrium; average-iterate only guarantees the time-averaged policy is good, requiring storage overhead.
  - Quick check question: Why does average-iterate convergence require storing all historical policies?

- Concept: **KL divergence and its projection properties**
  - Why needed here: The proof uses KL divergence as the primary convergence metric and relies on KL-projection invariance onto equilibrium sets.
  - Quick check question: Why is D_KL(π*||ˆπ) used rather than D_KL(ˆπ||π*)?

## Architecture Onboarding

- Component map: Preference matrix P -> OMWU update θ^(t) = θ^(t-1) + ηPπ̂^(t) -> Policy π(t) -> Duality gap computation
- Critical path:
  1. Initialize ˆπ(1) (uniform is typical)
  2. Compute π(t) using predicted gradient: θ(t) = θ(t-1) + ηPˆπ(t)
  3. Update prediction: ˆθ(t+1) = θ(t) + ηPπ(t)
  4. Monitor D_KL(π*||ˆπ(t)) and duality gap until convergence
- Design tradeoffs:
  - Learning rate η: Smaller η gives more stable burn-in but slower convergence; must satisfy ηL < 1/2
  - Initialization: Uniform initialization leads to minimal-entropy equilibrium; non-uniform can bias toward specific equilibria
  - Regularization: OMWU avoids regularization (no bias), but alternative algorithms (MPO, EGPO) use regularization for different convergence guarantees
- Failure signatures:
  - Divergence or oscillation: Likely η too large (ηL ≥ 1/2)
  - Slow convergence with plateau: May indicate burn-in phase; check if action space has subgame equilibria
  - Convergence to biased equilibrium: Check initialization; π* depends on KL-projection of initial policy
  - Nested optimization failures (for baselines): SPPO/MPO/ONPO require inner optimization; insufficient steps cause instability
- First 3 experiments:
  1. Validate on small cyclic preference matrix (e.g., rock-paper-scissors style) with uniform initialization; verify two-phase dynamics and measure burn-in time vs. theoretical bound.
  2. Compare OMWU against regularized methods (OMD-regularized, EGPO) on matrices with varying C_P values; confirm OMWU avoids regularization bias in duality gap.
  3. Test neural policy parameterization on larger action spaces (|A| = 100); verify linear convergence phase holds and compare runtime against methods requiring nested optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical bounds for burn-in time and convergence rate be proven tight?
- Basis in paper: [explicit] The Conclusion states, "the burn-in time and convergence rate are not proved to be of tightest order (and we believe they are not tight)."
- Why unresolved: The analysis relies on conservative bounds during the transition region of the burn-in phase, and the convergence rate may be loose depending on the alignment of specific probability arguments.
- What evidence would resolve it: Establishing matching lower bounds for OMWU in this setting or deriving an analysis that yields orders of magnitude consistent with empirical observations.

### Open Question 2
- Question: Does OMWU converge for preference matrices lacking a full-support Nash equilibrium?
- Basis in paper: [explicit] The Conclusion notes, "the result still does not hold for any preference matrix," specifically referencing the necessity of Assumption 1 (existence of a full-support NE).
- Why unresolved: The proof requires $\epsilon = \min_a \pi^*_a > 0$; if $\pi^*_a = 0$ for some action, the logarithmic terms in the KL-divergence analysis become unbounded.
- What evidence would resolve it: A theoretical extension proving last-iterate convergence for non-full-support equilibria or a counter-example showing divergence in such cases.

### Open Question 3
- Question: Does OMWU maintain linear convergence when scaled to practical Large Language Model (LLM) fine-tuning?
- Basis in paper: [explicit] The Conclusion admits the authors are "currently not able to reproduce our result on a fine-tuning problem of large language models due to resource constraints."
- Why unresolved: The theoretical guarantees assume exact updates, whereas LLM implementations rely on stochastic gradients and function approximation, which introduce errors not accounted for in the tabular proof.
- What evidence would resolve it: Empirical validation of OMWU on standard LLM alignment benchmarks (e.g., summarization or dialogue) showing convergence comparable to the tabular setting.

## Limitations

- The theoretical guarantees require the existence of a full-support Nash equilibrium; convergence may fail or degrade when min_a π*_a = 0
- The "marginal convergence" mechanism, while theoretically compelling, represents a novel analytical framework with limited empirical validation outside synthetic game matrices
- The neural policy experiments demonstrate feasibility but don't fully explore the algorithm's behavior on truly large-scale preference learning problems with continuous or extremely high-dimensional action spaces

## Confidence

- **High Confidence**: OMWU achieves last-iterate linear convergence after burn-in when full-support Nash equilibrium exists (Section 4.2, Theorem 11). The polynomial dependence on 1/ε and 1/C_P is mathematically established and improves upon exponential dependence in prior work.
- **Medium Confidence**: The two-phase convergence behavior (burn-in followed by linear convergence) is observed experimentally but the theoretical characterization of burn-in time remains relatively loose. The marginal convergence mechanism explains escape from subgame equilibria but requires further empirical validation.
- **Medium Confidence**: The claim that OMWU avoids regularization bias in duality gap computation is theoretically sound but the practical significance depends on the specific preference alignment task and how regularization is implemented in alternative methods.

## Next Checks

1. **Stress-test the marginal convergence mechanism**: Construct preference matrices with increasingly small ε values (min_a π*_a) and measure whether OMWU's convergence rate degrades polynomially as predicted or exhibits unexpected behavior. Compare against regularized methods to quantify the practical impact of avoiding regularization bias.

2. **Validate on continuous action spaces**: Extend experiments beyond tabular games to settings with continuous or high-dimensional action spaces (e.g., using OMWU for preference optimization in continuous control tasks or language generation with continuous parameter spaces). Verify that the core convergence properties generalize beyond discrete games.

3. **Benchmark against state-of-the-art alignment methods**: Conduct head-to-head comparisons between OMWU and established preference optimization algorithms (PPO, DPO, MPO, EGPO) on realistic LLM alignment tasks using actual human preference data. Measure not just convergence speed but also the quality of aligned outputs and computational efficiency, particularly accounting for OMWU's avoidance of nested optimization.