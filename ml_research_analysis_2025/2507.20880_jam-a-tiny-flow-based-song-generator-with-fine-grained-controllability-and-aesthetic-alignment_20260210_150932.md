---
ver: rpa2
title: 'JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and
  Aesthetic Alignment'
arxiv_id: '2507.20880'
source_url: https://arxiv.org/abs/2507.20880
tags:
- song
- duration
- generation
- music
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JAM introduces a lightweight, flow-matching-based song generator
  with word-level timing and duration control, addressing the lack of fine-grained
  controllability in existing lyrics-to-song models. By combining rectified flow with
  explicit word and phoneme timing inputs, JAM enables precise prosodic alignment
  while supporting global duration and style conditioning.
---

# JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment

## Quick Facts
- arXiv ID: 2507.20880
- Source URL: https://arxiv.org/abs/2507.20880
- Authors: Renhang Liu; Chia-Yu Hung; Navonil Majumder; Taylor Gautreaux; Amir Ali Bagherzadeh; Chuan Li; Dorien Herremans; Soujanya Poria
- Reference count: 11
- Key outcome: Achieves state-of-the-art results with WER of 0.151, PER of 0.101, and highest scores in musical enjoyment and style adherence on JAME benchmark, while being only 530M parameters

## Executive Summary
JAM introduces a lightweight, flow-matching-based song generator with word-level timing and duration control, addressing the lack of fine-grained controllability in existing lyrics-to-song models. By combining rectified flow with explicit word and phoneme timing inputs, JAM enables precise prosodic alignment while supporting global duration and style conditioning. Aesthetic alignment is achieved through iterative direct preference optimization using automated quality metrics, eliminating manual annotation. On a newly curated, genre-diverse benchmark (JAME), JAM achieves state-of-the-art results with WER of 0.151, PER of 0.101, and highest scores in musical enjoyment and style adherence, despite being only 530M parameters—less than half the size of the next smallest competitor.

## Method Summary
JAM uses a VAE-based flow-matching architecture with a 530M parameter DiT backbone. Audio is encoded into latent representations using Stable Audio Open's VAE, then conditioned on lyrics, style, and duration embeddings through multi-scale guidance. Phoneme sequences are aligned to latent frame positions using explicit word-level timestamps and injected as residuals into early transformer layers. The model is trained using rectified flow with straight-line trajectories between noise and target distributions. Aesthetic alignment is achieved through iterative direct preference optimization using SongEval metrics without manual annotation.

## Key Results
- Achieves WER of 0.151 and PER of 0.101 on JAME benchmark
- Highest scores in musical enjoyment and style adherence metrics
- Only 530M parameters—less than half the size of competitors (1.1B-7B)
- Maintains quality with explicit word-level timing control (WER 0.151 vs 0.370 with GPT-4o timing)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit word-level timing inputs dramatically improve lyrical fidelity and prosody.
- Mechanism: Phoneme sequences are aligned to latent frame positions using start/end timestamps, then upsampled and injected as residual signals into early transformer layers. This creates strong temporal supervision at the exact positions where vocals occur.
- Core assumption: Accurate word-level timestamps are available during inference; imprecise timing degrades output quality.
- Evidence anchors:
  - [abstract] "combining rectified flow with explicit word and phoneme timing inputs, JAM enables precise prosodic alignment"
  - [section 3.6] Algorithm 1 describes phoneme-to-frame alignment with VOCAL FILLER and SONG FILLER tokens
  - [corpus] Related work "Muse" also targets fine-grained style control, suggesting timing controllability is an emerging research direction
- Break condition: If timing inputs are noisy or absent (e.g., naive GPT-4o predictions), WER increases from 0.151 to 0.370 and PER from 0.101 to 0.320 (Table 9).

### Mechanism 2
- Claim: Rectified flow enables compact model size while maintaining generation quality.
- Mechanism: Straight-line trajectories between noise and target distributions reduce sampling steps and improve training stability compared to score-based diffusion. The 530M parameter DiT backbone learns velocity fields conditioned on concatenated lyric, style, and duration embeddings.
- Core assumption: The VAE latent space is sufficiently expressive to represent 44.1kHz stereo audio up to 3m50s.
- Evidence anchors:
  - [section 3.4] "Rectified flows have been shown empirically to be more sample-efficient and to degrade less than other approaches"
  - [Table 2] JAM achieves competitive results at 0.53B parameters vs. competitors ranging 1.1B–7B
  - [corpus] Weak corpus evidence on rectified flow specifically for songs; mostly diffusion-based approaches noted
- Break condition: Insufficient model capacity may manifest as artifacts in long-form generation; not directly tested in ablations.

### Mechanism 3
- Claim: Iterative DPO with automated aesthetic metrics improves musicality without manual annotation.
- Mechanism: SongEval scores (averaged across 5 criteria) select win/loss pairs from model-generated candidates. DPO loss encourages the model toward higher-scoring outputs. Multiple rounds iteratively refine the policy.
- Core assumption: SongEval metrics correlate with human aesthetic preferences; over-optimization may drift from ground-truth style.
- Evidence anchors:
  - [abstract] "Aesthetic alignment is achieved through iterative direct preference optimization using automated quality metrics"
  - [Table 4] DPO Round-3 improves MuQ from 0.7473 (SFT) to 0.7593; PQ improves from 7.59 to 8.06
  - [corpus] No direct corpus evidence on SongEval-based DPO for songs
- Break condition: Without ground-truth reconstruction loss (λ=0.2), FAD worsens from 0.148 to 0.204 (Table 4), indicating distributional drift.

## Foundational Learning

- **Flow Matching / Rectified Flow**
  - Why needed here: Core generative framework; understanding ODE-based sampling and velocity prediction is essential for debugging convergence and sampling issues.
  - Quick check question: Can you explain why rectified flow uses straight-line interpolation z_t = (1-t)z_1 + t·z_0 instead of the noise schedules typical in DDPM?

- **Conditional Diffusion/Flow Models (CFG)**
  - Why needed here: Multi-condition CFG with separate guidance scales for style (α_s) and lyrics (α_l) controls output characteristics.
  - Quick check question: If α_s is set too high, what artifact would you expect in the generated audio?

- **Direct Preference Optimization (DPO)**
  - Why needed here: Post-training alignment stage; understanding the loss formulation helps diagnose overfitting to preference data.
  - Quick check question: In Eq. 8, why does DPO optimize the margin between winning and losing samples rather than their absolute likelihoods?

## Architecture Onboarding

- **Component map:**
  VAE Encoder (Stable Audio Open) -> VAE Decoder (DiffRhythm) -> DiT Backbone (530M params) -> Conditioning (Lyrics, Style, Duration) -> Alignment (DPO with SongEval)

- **Critical path:**
  1. Audio → VAE encode → latent z ∈ R^{l×c}
  2. Lyrics → phonemize → upsample to latent frame rate → embed as c_lyric
  3. Concatenate [z_t || c_lyric || c_style || c_dur] → linear fusion → ConvPosEmbed
  4. 16 DiT layers with residual lyric injection in first 8 layers
  5. Project to velocity → Euler ODE solver → VAE decode → audio

- **Design tradeoffs:**
  - Compact size (530M) vs. long-form capacity: truncation at Tmax=230s for full-song SFT
  - Quantized timing simplifies user input but introduces slight "electronic" vocal character (Table 9)
  - DPO-GT variant preserves genre fidelity but trades off peak aesthetic scores

- **Failure signatures:**
  - Missing/inaccurate word timings → vocal-accompaniment mismatch, robotic phrasing
  - Token-level duration control omitted → non-silent content beyond T_target (Table 7: RMS 33-36% of reference without TDC vs. 0.41% with)
  - DPO over-optimization → increased FAD, style drift

- **First 3 experiments:**
  1. Validate VAE reconstruction: Encode/decode 10 diverse songs; compute spectral loss and listen for artifacts.
  2. Test lyric conditioning ablation: Generate with random vs. aligned phoneme positions; measure WER/PER delta.
  3. DPO scaling check: Run 1 vs. 3 rounds with and without GT reconstruction loss; plot MuQ, FAD, and genre accuracy trajectories.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a duration predictor trained jointly with the song generator effectively replace manual word-level timing inputs for non-expert users?
- Basis in paper: [explicit] Section 7 states that "generating [temporal information] during inference remains a significant challenge" and proposes "train[ing] the duration predictor jointly... in an end-to-end fashion" as a promising future direction.
- Why unresolved: The authors experimented with GPT-4o for duration prediction (Section 6.1), which yielded robotic results, but have not yet implemented or evaluated the proposed joint end-to-end training approach.
- What evidence would resolve it: Performance metrics (WER, MuQ) of a model trained end-to-end with an integrated predictor, showing comparable quality to oracle timing inputs.

### Open Question 2
- Question: Does incorporating phoneme-level alignment data significantly improve pronunciation accuracy and expressive granularity?
- Basis in paper: [explicit] Section 7 explicitly identifies the "lack of phoneme-level duration control" as a limitation that "restricts the model's expressive granularity," particularly for "fast rap segments or melismatic vocal runs."
- Why unresolved: The current architecture (Algorithm 1) distributes phonemes evenly within word segments but does not allow explicit user control or model prediction of specific phoneme durations.
- What evidence would resolve it: A comparative study evaluating synthesis quality on datasets featuring melisma or rapid rap flows using a modified model with phoneme-level conditioning.

### Open Question 3
- Question: Does training the song generation model specifically on beat-aligned quantized annotations mitigate the artificial "electronic character" observed during quantized inference?
- Basis in paper: [inferred] Section 6.3 notes that using quantized inputs during inference creates an "electronic character" because the model was trained on continuous data. The authors suggest that training on "similarly quantized annotations" might allow the model to "recover... subtle temporal nuances."
- Why unresolved: The authors evaluated inference with quantized inputs against a continuously trained model but did not train a separate model on quantized ground-truth data to test the hypothesis.
- What evidence would resolve it: An ablation study training two JAM models (continuous vs. quantized targets) and evaluating their naturalness when fed quantized input timings.

## Limitations

- Sampling stability and long-form generation at full VAE capacity (3m50s) not empirically validated
- Fragile dependency on high-quality word-level timestamps with dramatic degradation using naive predictions
- Potential DPO over-optimization risks and distributional drift without ground-truth reconstruction loss

## Confidence

**High Confidence**: The core mechanism of phoneme-to-frame alignment with explicit timing injection is well-documented and produces measurable improvements in lyrical fidelity (WER/PER). The VAE-based flow architecture and multi-condition CFG formulation are standard and well-supported.

**Medium Confidence**: The rectified flow training efficiency claims and compact model size advantages are supported by comparisons but lack ablation studies isolating rectified flow's contribution. The SongEval-based DPO alignment shows quantitative improvements but lacks qualitative validation against human preferences.

**Low Confidence**: The long-form generation capability at full VAE capacity is asserted but not empirically validated. The robustness of timing inputs and the long-term stability of iterative DPO are not rigorously tested.

## Next Checks

1. **Long-form generation stability test**: Generate and evaluate 10 full-length songs (3m50s) with diverse styles; measure FAD, spectral consistency, and listener preference compared to truncated versions.

2. **Timing noise robustness**: Systematically degrade word-level timestamps (jitter, missing words) and measure WER/PER degradation curves; test alternative timing extraction methods (forced alignment, RVG-T).

3. **DPO over-optimization monitoring**: Run extended DPO (5+ rounds) with and without GT reconstruction loss; track MuQ, FAD, genre accuracy, and conduct human preference studies to detect aesthetic drift.