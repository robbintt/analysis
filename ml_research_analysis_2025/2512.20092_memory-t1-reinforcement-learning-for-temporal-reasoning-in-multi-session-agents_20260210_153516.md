---
ver: rpa2
title: 'Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session
  Agents'
arxiv_id: '2512.20092'
source_url: https://arxiv.org/abs/2512.20092
tags:
- uni00000011
- temporal
- uni00000014
- uni00000013
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of temporal reasoning over long,
  multi-session dialogues, where existing long-context models struggle to accurately
  identify and reason about temporally pertinent information. To solve this, the authors
  propose Memory-T1, a reinforcement learning-based framework that uses a coarse-to-fine
  retrieval strategy combined with a multi-level reward function.
---

# Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents

## Quick Facts
- arXiv ID: 2512.20092
- Source URL: https://arxiv.org/abs/2512.20092
- Reference count: 40
- Establishes new state-of-the-art for open-source models on the Time-Dialog benchmark (67.0% overall score)

## Executive Summary
This paper tackles the challenge of temporal reasoning over long, multi-session dialogues where existing long-context models struggle to accurately identify and reason about temporally pertinent information. The authors propose Memory-T1, a reinforcement learning-based framework that uses a coarse-to-fine retrieval strategy combined with a multi-level reward function. Evaluated on the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0%, establishing a new state-of-the-art for open-source models and outperforming a 14B baseline by 10.2%.

## Method Summary
Memory-T1 employs a coarse-to-fine strategy to address temporal reasoning in multi-session dialogues. First, it prunes the dialogue history using temporal and relevance filters to create a candidate set. Then, an RL agent selects precise evidence sessions guided by rewards for answer accuracy, evidence grounding, and temporal consistency. The framework uses GRPO (Group Relative Policy Optimization) for stable training, computing advantages from batch-average rewards rather than learned value functions.

## Key Results
- Memory-T1 achieves 67.0% overall score on Time-Dialog, establishing SOTA for open-source models
- Outperforms 14B baseline by 10.2% overall
- Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0% performance gain
- Maintains robustness on contexts up to 128k tokens where baseline models collapse

## Why This Works (Mechanism)

### Mechanism 1: Coarse-to-Fine Retrieval Cascade
Pruning the dialogue history before fine-grained selection improves evidence recall while reducing attentional noise. A temporal filter (LLM-predicted query scope) followed by relevance-based retrieval (BM25) reduces the candidate pool from thousands of sessions to a manageable top-k subset before the RL agent acts.

### Mechanism 2: Dense Multi-Level Reward Signal
Combining accuracy, evidence grounding, and temporal consistency rewards provides richer supervision than sparse answer-only feedback. R_a enforces task success; R_g (Jaccard-indexed session matching) forces correct evidence selection; R_t (session-level proximity + utterance-level fidelity) aligns selected sessions temporally with the query scope.

### Mechanism 3: GRPO Stabilization via Group-Average Baseline
GRPO reduces reward variance and stabilizes policy updates compared to PPO for this retrieval-then-reason task. Advantage is computed as R_j - mean(R_batch) rather than relying on a learned value function; KL divergence penalty prevents drift from the reference policy.

## Foundational Learning

- **Concept**: Reinforcement Learning from Verifiable Rewards
  - Why needed here: The agent learns to select evidence sessions via reward signals computed from ground-truth annotations, not human preference.
  - Quick check question: Can you compute a scalar reward from parsed model output without a learned reward model?

- **Concept**: Jaccard Index / Set Overlap
  - Why needed here: The evidence grounding reward R_g quantifies how well selected sessions match gold sessions using intersection-over-union.
  - Quick check question: If predicted sessions are {1, 3} and gold sessions are {3, 5}, what is the Jaccard score?

- **Concept**: Logistic Penalty Functions for Temporal Alignment
  - Why needed here: R_s uses a logistic curve to softly penalize temporal distance, providing a tolerance window rather than hard cutoffs.
  - Quick check question: How does adjusting the margin parameter `m` affect the reward for a session 5 days outside the query scope?

## Architecture Onboarding

- **Component map**: LLM temporal predictor → hard session filter → BM25 ranker → top-k selection → Policy model (Qwen-7B-Instruct) → structured output → Reward Verifier → GRPO update

- **Critical path**: Temporal prediction accuracy (if wrong, recall collapses) → Evidence grounding annotations (required for R_g and R_f) → Reward weight tuning (w_a=0.6, w_g=0.2, w_t=0.2)

- **Design tradeoffs**:
  - BM25 vs. dense retriever: BM25 is fast and sufficient; dense retrievers may improve recall but add latency.
  - R_f granularity: Utterance-level event scoring provides finer signal but requires more annotation effort and is noisier (session-level used for R_g to mitigate this).
  - Group size G: Larger G improves baseline estimation but increases compute.

- **Failure signatures**:
  - Parsing failures: Output not in JSON format → fixed -0.5 penalty
  - Temporal prediction drift: Systematic errors in query time scope → low recall, high drop in Category C (timeline tasks)
  - Reward hacking: Model selects sessions that maximize R_t without grounding in R_g → high R_t but low R_a

- **First 3 experiments**:
  1. Validate temporal filter recall: Run candidate generation with and without temporal filtering; plot recall@k across different top-k values.
  2. Ablate reward components: Train with R_a-only, R_a+R_g, and full reward; measure per-category degradation.
  3. Noise sensitivity test: Corrupt 5-20% of time labels; observe R_t stability and overall F1 drop.

## Open Questions the Paper Calls Out
None

## Limitations

- The coarse-to-fine retrieval cascade relies on accurate LLM temporal predictions; systematic prediction errors for ambiguous queries could exclude relevant sessions and cause unrecoverable recall loss.
- Performance gains depend heavily on the availability of gold-standard annotations for evidence sessions and temporal spans, which are labor-intensive to produce and may not generalize to domains lacking such detailed supervision.
- The framework's robustness claim for contexts up to 128k tokens is demonstrated only on the Time-Dialog benchmark; generalization to other long-context domains is untested.

## Confidence

- **High Confidence**: The overall performance improvement (67.0% score, 10.2% over 14B baseline) and the contribution of multi-level rewards (15.0% gain from ablation) are well-supported by reported results and ablation studies.
- **Medium Confidence**: The coarse-to-fine retrieval mechanism's effectiveness is supported by recall curves but hinges on the LLM's temporal prediction accuracy, which is not exhaustively validated across query types.
- **Medium Confidence**: GRPO's superiority over PPO (18.5% gain) is shown, but lacks direct comparison in the broader temporal reasoning literature, limiting external validation.
- **Low Confidence**: The claim of robustness to 128k-token contexts is based solely on Time-Dialog; no evidence is provided for other long-context domains.

## Next Checks

1. **Temporal Filter Robustness**: Systematically test the coarse-to-fine retrieval with ambiguous or edge-case temporal queries (e.g., "the week before that") to quantify recall loss when the LLM prediction is incorrect.
2. **Annotation Dependency Stress Test**: Corrupt 5-20% of time labels in the training data to measure the impact on R_t stability and overall F1 score, assessing sensitivity to annotation noise.
3. **Generalization to Other Long-Context Domains**: Evaluate Memory-T1 on a different long-context benchmark (e.g., multi-session customer support logs or medical dialogues) to verify robustness claims beyond Time-Dialog.