---
ver: rpa2
title: Computationally efficient Gauss-Newton reinforcement learning for model predictive
  control
arxiv_id: '2508.02441'
source_url: https://arxiv.org/abs/2508.02441
tags:
- policy
- hessian
- optimization
- learning
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of second-order
  policy optimization in reinforcement learning (RL) when using model predictive control
  (MPC) policies. Traditional second-order methods require expensive second-order
  policy derivatives, making them impractical for MPC.
---

# Computationally efficient Gauss-Newton reinforcement learning for model predictive control

## Quick Facts
- arXiv ID: 2508.02441
- Source URL: https://arxiv.org/abs/2508.02441
- Reference count: 13
- Second-order policy optimization using Gauss-Newton approximation achieves superlinear convergence for MPC-based RL without expensive second-order derivatives

## Executive Summary
This paper addresses the high computational cost of second-order policy optimization in reinforcement learning (RL) when using model predictive control (MPC) policies. Traditional second-order methods require expensive second-order policy derivatives, making them impractical for MPC. The authors propose a Gauss-Newton approximation of the deterministic policy Hessian that avoids these costly derivatives while maintaining superlinear convergence. They also introduce a momentum-based Hessian averaging scheme to improve robustness against noisy estimates. The method is tested on a nonlinear continuously stirred tank reactor (CSTR) and shows faster convergence and improved data efficiency compared to first-order methods like Adam.

## Method Summary
The proposed method combines Gauss-Newton approximation of the deterministic policy Hessian with momentum-based Hessian averaging for stable training under noisy estimates. The Gauss-Newton approximation M₂(θ) = E[∇_θπ_θ^T ∇_a²Q ∇_θπ_θ] eliminates the need for second-order policy derivatives while maintaining superlinear convergence properties. A momentum-based averaging scheme with bias correction (D̂ₖ = Dₖ/(1-η^{k+1})) smooths noisy Hessian estimates during training. The approach leverages MPC's sparse parameterization to make second-order methods computationally tractable while benefiting from model-based initialization. The method is implemented using do-mpc with CasADi/IPOPT and tested on a nonlinear CSTR system with 2 learnable parameters.

## Key Results
- The Gauss-Newton approximation achieves superlinear convergence without requiring second-order policy derivatives, reducing per-iteration computation time from 1.98 to 1.85 minutes
- Momentum-based Hessian averaging with bias correction stabilizes training under noisy estimates, with η = 0.9 and β = 0.75 showing optimal performance
- The method converges in 65 iterations (~120 minutes total) compared to first-order Adam methods that require significantly more iterations and data
- Sparse MPC parameterization makes full Hessian storage feasible while providing good initialization through model-based control

## Why This Works (Mechanism)

### Mechanism 1
The Gauss-Newton approximation M₂(θ) achieves superlinear convergence without requiring second-order policy derivatives. At optimal parameters θ*, the action-value gradient ∇ₐQ(s,a)|_{a=π*(s)} = 0, causing the term M₁(θ) = E[∇²₍θ₎π ⊗ ∇ₐQ] to vanish as θ → θ*. This means M₂(θ) alone converges to the true Hessian, satisfying the Dennis-Moré condition for superlinear convergence. Core assumptions include the parameterized policy being able to represent the optimal policy and gradients/Hessians remaining bounded near optimum.

### Mechanism 2
Momentum-based Hessian averaging with bias correction stabilizes training under noisy estimates. Local Hessian samples B̃ₖ are noisy due to finite-sample Q-estimation. Exponential moving average Dₖ = ηDₖ₋₁ + (1-η)B̃ₖ smooths eigenvalue estimates, while bias correction D̂ₖ = Dₖ/(1-η^{k+1}) prevents early-iteration underweighting of new information. Initial D₀ = -ω⁻¹I provides regularization when local estimates are unreliable. Core assumptions include Hessian noise being zero-mean and momentum factor η being appropriately tuned.

### Mechanism 3
Sparse MPC parameterization makes second-order methods tractable while providing good initialization. MPC policies typically have few tunable parameters (2 parameters in CSTR case vs. thousands in NNs), making full Hessian storage feasible. MPC's model-based initialization provides reasonable starting performance, reducing data requirements vs. random NN initialization. Core assumptions include the system model in MPC being sufficiently accurate and the parameterization capturing relevant uncertainties.

## Foundational Learning

- **Concept:** Newton and quasi-Newton optimization convergence theory
  - **Why needed here:** Understanding why Gauss-Newton achieves superlinear convergence requires knowing the Dennis-Moré condition and difference between linear/quadratic/superlinear rates.
  - **Quick check question:** Explain why the condition lim ||(Bₖ - H*)pₖ||/||pₖ|| = 0 guarantees superlinear convergence.

- **Concept:** NLP sensitivities and implicit differentiation
  - **Why needed here:** MPC policy gradient requires differentiating through an optimization problem; understanding why second-order sensitivities are expensive motivates the approximation.
  - **Quick check question:** Why does computing ∇²₍θ₎π₍θ₎ require third-order derivatives of the MPC Lagrangian?

- **Concept:** Deterministic policy gradient theorem
  - **Why needed here:** The gradient and Hessian formulas derive from this; understanding the Q-function's role is essential.
  - **Quick check question:** Write the deterministic policy gradient formula and identify what quantities must be estimated empirically.

## Architecture Onboarding

- **Component map:**
  Environment (CSTR ODEs + noise) -> MPC Policy π₍θ₎(s) (solves NLP) -> action a -> Q-Function Approximator (NN) -> Policy Optimizer (computes gradient, Hessian, updates θ)

- **Critical path:** The dominant computational cost is solving the MPC NLP at each time step. The Gauss-Newton approximation avoids the secondary expensive step (second-order NLP sensitivities), reducing per-iteration time from 1.98 → 1.85 min vs. approximate Newton.

- **Design tradeoffs:**
  - Learning rate α: Second-order methods use larger α (0.1) vs. Adam (0.001-0.01); too large causes instability
  - Gradient momentum β: Controls gradient smoothing; 0.75-0.99 range works
  - Hessian momentum η: 0.9 balances stability and adaptivity; 0.99 is too conservative
  - Initial eigenvalue estimate ω⁻¹: Should approximate largest |eigenvalue|; 10²-10³ typical for process control

- **Failure signatures:**
  - Eigenvalue estimates off by >2 orders of magnitude → updates overshoot/undershoot dramatically
  - η too low (<0.5) → divergence after ~20 iterations
  - Q-function approximation poor → Hessian estimates unreliable
  - MPC becomes infeasible mid-training → θ updated to region where constraints cannot be satisfied

- **First 3 experiments:**
  1. Validate on analytical case: Replicate Figure 3 on linear system to verify superlinear convergence; compare error decay rates for exact Hessian vs. Gauss-Newton vs. gradient ascent.
  2. Ablate momentum parameters: Run CSTR training with η ∈ {0, 0.5, 0.9, 0.99} and β ∈ {0.75, 0.9, 0.99}; plot convergence curves to identify stable region.
  3. Compare wall-clock time: Measure per-iteration time for Gauss-Newton vs. approximate Newton vs. Adam on CSTR; verify 65-iteration convergence and ~120 min total time for proposed method.

## Open Questions the Paper Calls Out

### Open Question 1
How does the Gauss-Newton approximation scale to higher-dimensional MPC policies with significantly more tunable parameters? The CSTR demonstration uses only 2 parameters; computational and memory demands may grow non-trivially with parameter dimension.

### Open Question 2
Can the proposed method be integrated with robust MPC formulations that account for bounded uncertainty explicitly? Robust MPC introduces min-max optimization structures; it is unclear whether the Gauss-Newton Hessian approximation remains valid and computationally tractable under such formulations.

### Open Question 3
Does the approach extend to constrained Markov decision processes where constraints must be satisfied during learning, not just at convergence? The current method optimizes expected cumulative reward without explicit constraint handling during policy updates.

## Limitations
- The empirical nature of the momentum-based Hessian averaging scheme lacks comprehensive theoretical justification for stability across all parameter ranges
- Success heavily depends on MPC's sparse parameterization, which may not generalize to other policy architectures
- Reliance on accurate Q-function approximation for Hessian estimation introduces potential compounding errors in high-dimensional state spaces

## Confidence
- **High confidence:** The Gauss-Newton approximation mechanism and its superlinear convergence properties are well-supported by theoretical analysis and numerical validation
- **Medium confidence:** The momentum-based Hessian averaging scheme shows empirical success but lacks comprehensive theoretical justification for stability across all parameter ranges
- **Medium confidence:** The claim about MPC's sparse parameterization making second-order methods tractable is well-supported for the CSTR case but may not generalize to all MPC applications

## Next Checks
1. **Theoretical extension:** Prove stability conditions for the momentum-based Hessian averaging scheme across different ranges of η and β parameters, extending beyond empirical observations
2. **Architecture generalization:** Test the Gauss-Newton approach on a different policy parameterization (e.g., low-dimensional NN with fewer parameters) to verify the claim about sparse parameterization benefits
3. **Model mismatch scenarios:** Evaluate performance when the MPC model has significant errors compared to the true system dynamics to test robustness claims about model-based initialization benefits