---
ver: rpa2
title: 'ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory'
arxiv_id: '2509.25140'
source_url: https://arxiv.org/abs/2509.25140
tags:
- memory
- scaling
- agent
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ReasoningBank, a memory framework that enables
  LLM agents to learn from both successful and failed experiences by distilling generalizable
  reasoning strategies. Unlike prior work focused on raw trajectories or successful
  routines, ReasoningBank stores actionable principles that agents retrieve during
  test-time to guide decisions.
---

# ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory

## Quick Facts
- arXiv ID: 2509.25140
- Source URL: https://arxiv.org/abs/2509.25140
- Reference count: 40
- Key outcome: ReasoningBank improves agent effectiveness by 34.2% and efficiency by 16.0% across web browsing and software engineering tasks

## Executive Summary
ReasoningBank introduces a memory framework that enables LLM agents to learn from both successful and failed experiences by distilling generalizable reasoning strategies rather than raw trajectories. The framework uses memory-aware test-time scaling (MaTTS) to create synergy between better memory guiding exploration and diverse rollouts enriching memory. Across web browsing (WebArena, Mind2Web) and software engineering (SWE-Bench-Verified) benchmarks, ReasoningBank consistently improves effectiveness (up to 34.2% relative gain) and efficiency (16.0% fewer steps). MaTTS amplifies these gains, with memory-aware scaling achieving up to 62.1% pass@k versus 52.4% without memory. The work establishes memory-driven experience scaling as a new dimension for self-evolving agents, enabling emergent reasoning behaviors over time.

## Method Summary
ReasoningBank operates through a streaming learning pipeline where agents extract structured memory items from completed trajectories using LLM-based extraction. These items (title, description, content) are stored in a memory bank and retrieved via embedding similarity for future tasks. The framework uses LLM-as-a-judge for self-evaluation without ground-truth labels. MaTTS extends this by running multiple parallel trajectories with self-contrast reasoning or sequential self-refinement steps, aggregating across rollouts rather than treating them independently. The agent core uses ReAct-style loops with memory-injected system prompts, maintaining a maximum of 30 steps per task.

## Key Results
- ReasoningBank achieves 49.7% success rate in WebArena-Shopping vs 44.4% baseline, a 34.2% relative improvement
- MaTTS with memory-aware scaling reaches 62.1% pass@k versus 52.4% without memory
- Including failure-derived memory items improves performance from 46.5% to 49.7% success rate
- Memory-aware scaling reduces average steps by 16.0% while maintaining or improving success rates

## Why This Works (Mechanism)

### Mechanism 1: Learning from Both Successes and Failures
Distilling reasoning strategies from both successful and failed experiences improves generalization over storing raw trajectories or success-only routines. After each task, an LLM-based extractor converts the trajectory into structured memory items. Successes contribute validated strategies while failures provide counterfactual signals that sharpen guardrails. These items are retrieved via embedding similarity and injected into the agent's system prompt for future tasks.

### Mechanism 2: Memory-Aware Test-Time Scaling Synergy
Memory-aware test-time scaling creates synergy where better memory guides exploration and diverse rollouts enrich memory. In parallel scaling, multiple trajectories are generated for the same query, with self-contrast reasoning identifying consistent patterns and filtering spurious solutions. In sequential scaling, iterative self-refinement generates intermediate reasoning signals for memory extraction. The key insight is aggregating across rollouts rather than treating them independently.

### Mechanism 3: Structured Memory Items Enable Transfer
Structured memory items (title, description, content) enable more transferable guidance than raw trajectory storage. Each memory item abstracts away low-level execution details while preserving reasoning patterns. The title serves as a concise identifier, description provides a one-sentence summary, and content records distilled steps or rationales. This schema is both human-interpretable and machine-usable.

## Foundational Learning

- **Test-Time Scaling (Best-of-N, verifiers)**: MaTTS extends standard TTS to interactive agents; understanding baseline TTS is prerequisite. Quick check: Can you explain why Best-of-N improves pass@k but not necessarily pass@1?

- **LLM-as-a-judge for self-evaluation**: ReasoningBank relies on self-judged success/failure signals without ground-truth labels. Quick check: What failure modes occur when a judge model is biased toward certain output formats?

- **Embedding-based semantic retrieval**: Memory retrieval uses cosine similarity over query embeddings; understanding retrieval quality is essential. Quick check: How does embedding similarity fail when queries are semantically related but lexically distinct?

## Architecture Onboarding

- **Component map**: Query arrives → embed query → retrieve top-k memory items → inject items into system prompt → agent executes trajectory (max 30 steps) → LLM-as-a-judge labels outcome → extractor generates 0-3 memory items → append items to bank

- **Critical path**: 1) Query arrives → embed query → retrieve top-k memory items; 2) Inject items into system prompt → agent executes trajectory (max 30 steps); 3) LLM-as-a-judge labels outcome → extractor generates 0-3 memory items; 4) Append items to bank → persist for future queries

- **Design tradeoffs**: Simplicity vs. sophistication - authors intentionally use simple retrieval/consolidation to isolate content quality effects; advanced routers could improve but introduce confounds. k=1 retrieval default - higher k degrades performance, suggesting relevance beats quantity. Max 3 items per trajectory prevents redundancy but may miss edge-case insights.

- **Failure signatures**: Judge mislabels trajectory → incorrect memory type (success principle vs. failure lesson); Retrieval returns irrelevant items → agent follows wrong strategy path; Scaling without aggregation → diverse rollouts add noise instead of signal

- **First 3 experiments**: 1) Reproduce WebArena-Shopping baseline: No Memory vs. Synapse vs. ReasoningBank (Table 1) to validate implementation; 2) Ablate failure inclusion: Compare success-only vs. success+failure extraction (Figure 7) to verify failure signal value; 3) Test scaling factor k: Run parallel MaTTS with k=1,3,5 to confirm synergy curve (Figure 4a) and identify compute-quality tradeoff point

## Open Questions the Paper Calls Out

- **Open Question 1**: Can memory items be composed into higher-level strategies, and would composition-aware retrieval and consolidation enable richer strategies for long-horizon tasks? The current framework treats memory items independently during retrieval without mechanisms to combine complementary items or form reusable macro-strategies.

- **Open Question 2**: How robust is the framework under noisy or erroneous LLM-as-judge signals, and would stronger verifiers or human-in-the-loop feedback significantly improve memory induction quality? The automatic labeling for success/failure may introduce noise when tasks are ambiguous or when the judge model errs.

- **Open Question 3**: Would reasoning-intensive retrieval controllers that decompose queries and plan multi-hop lookups improve performance over simple embedding-based similarity search? The paper intentionally uses simple embedding-based retrieval to isolate content quality effects, leaving advanced retrieval unexplored.

## Limitations

- Grounding of LLM-as-a-Judge: The framework relies entirely on self-judged success/failure signals without ground-truth validation, raising concerns about potential systematic bias in failure-derived memory items.

- Generalizability across domains: All experiments use specific benchmarks (WebArena, Mind2Web, SWE-Bench-Verified). The framework's performance on open-ended or multimodal tasks remains untested.

- Compute-quality tradeoffs: MaTTS shows consistent improvements, but the paper doesn't explore diminishing returns at higher k values or provide cost-benefit analysis for different budget constraints.

## Confidence

- **High confidence**: Memory extraction and consolidation mechanisms are well-specified and directly testable. The core architectural components (retrieval, injection, scaling) follow established patterns.
- **Medium confidence**: Claims about failure-derived memory improving performance are supported by ablation studies, but the magnitude of improvement (2.2 percentage points) may be sensitive to judge accuracy.
- **Low confidence**: The synergistic scaling claim (MaTTS outperforming both memory and scaling separately) requires deeper investigation - the paper shows aggregate results but doesn't isolate whether memory guidance or aggregation explains the gains.

## Next Checks

1. **Judge accuracy validation**: Run LLM-as-a-judge on a held-out subset of tasks with ground-truth labels to quantify error rates and their impact on memory quality.

2. **Cross-domain transfer test**: Apply ReasoningBank trained on WebArena to a different interactive domain (e.g., robotic manipulation) to assess domain generalization beyond benchmark-specific patterns.

3. **Scaling saturation analysis**: Systematically vary k from 1 to 10 in MaTTS to identify the point of diminishing returns and determine optimal compute allocation for different task complexities.