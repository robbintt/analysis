---
ver: rpa2
title: 'OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models'
arxiv_id: '2509.26140'
source_url: https://arxiv.org/abs/2509.26140
tags:
- audio
- reasoning
- sound
- spatial
- binaural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of spatial reasoning in audio
  large language models (ALLMs) by introducing OWL, a geometry-aware spatial reasoning
  framework. The core method involves SAGE, a novel encoder that aligns binaural audio
  with 3D spatial structure using panoramic depth images and room impulse responses
  during training, while requiring only audio at inference.
---

# OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models

## Quick Facts
- **arXiv ID**: 2509.26140
- **Source URL**: https://arxiv.org/abs/2509.26140
- **Reference count**: 20
- **Primary result**: OWL achieves 11° lower mean DoA error and 25% higher spatial reasoning accuracy than BAT baseline

## Executive Summary
OWL addresses spatial reasoning in audio large language models (ALLMs) by introducing SAGE, a geometry-aware encoder that learns spatial features through RIR reconstruction during training. The framework integrates this encoder with a curriculum-based chain-of-thought reasoning mechanism to decompose complex acoustic queries into interpretable steps. OWL demonstrates state-of-the-art performance on the BiDepth dataset, improving spatial localization accuracy and reasoning capabilities while requiring only audio input at inference time.

## Method Summary
OWL uses SAGE, a 12-layer Transformer encoder initialized from AudioMAE, which processes 4-channel binaural audio (Mel-spectrograms + IPD features) and is trained to reconstruct Room Impulse Responses using depth images as privileged information. A Q-Former projector bridges SAGE features to LLaMA-2-7B LLM embeddings, enabling spatial reasoning through a 3-stage curriculum: perception warmup, relational reasoning, and CoT generation. The BiDepth dataset of 1.1M QA pairs combining binaural audio, panoramic depth images, and RIRs provides supervision for both spatial grounding and reasoning tasks.

## Key Results
- SAGE reduces mean direction-of-arrival error by 11° compared to BAT baseline
- Spatial reasoning accuracy improves by up to 25% over baseline models
- 3-stage curriculum learning significantly outperforms end-to-end training approaches

## Why This Works (Mechanism)

### Mechanism 1: Geometry-Regularized Audio Encoding
The RIR reconstruction task during training forces the audio encoder to capture implicit geometric cues like distance and reverb, which transfer to improved localization without depth at inference. This works because acoustic features learned via RIR reconstruction internalize spatial properties like Direct-to-Reverberant Ratio that correlate with distance.

### Mechanism 2: Curriculum-Based CoT Distillation
Training progresses from single-source detection to relative comparisons and finally to explicit rationale generation, preventing the model from learning shortcuts. This ensures the LLM first learns accurate spatial grounding before attempting relational reasoning.

### Mechanism 3: Binaural-Spatial Feature Decoupling
Feeding the LLM fused representations of Mel-spectrograms (content) and IPD (spatial cues) allows the architecture to disentangle "what" from "where" more effectively than single-channel audio.

## Foundational Learning

- **Room Impulse Response (RIR) & EDC Loss**: This models how rooms modify sound (reverb). The EDC measures sound decay rate, correlating with room size and distance. *Quick check*: Does an RIR capture semantic information (e.g., "this is a dog bark") or spatial-acoustic information (e.g., "this room has glass walls")?

- **Interaural Phase Difference (IPD)**: This is the primary signal for Direction of Arrival. The model relies on phase differences between left/right channels to compute azimuth. *Quick check*: If a sound source moves from your left to your right (at the same distance), does the IPD frequency increase, decrease, or shift sign?

- **Chain-of-Thought (CoT) Grounding**: OWL forces the model to output intermediate coordinates before answering relational questions. *Quick check*: In the context of OWL, is the CoT generated by an external planner, or is it a generative output of the LLM itself trained on the BiDepth dataset?

## Architecture Onboarding

- **Component map**: Binaural Audio (2-ch) -> Feature Extractor (STFT/Mel/IPD) -> SAGE Encoder (Transformer, 85M params) -> Q-Former Projector (8 layers, 64 queries) -> LLaMA-2-7B (LoRA adapted) -> Text Output

- **Critical path**: The alignment between the Q-Former tokens and the LLaMA-2 embedding space. If the projector is under-trained, the LLM receives "noisy" spatial tokens and hallucinates coordinates.

- **Design tradeoffs**: Synthetic vs. Real Data - relies heavily on synthetic BiDepth dataset, risking sim-to-real gap. Auxiliary Task vs. Inference Speed - SAGE requires heavy RIR reconstruction head during training (discarded at inference).

- **Failure signatures**: "Coarse Label Collapse" (reverts to simple left/right), Semantic-Spatial Disassociation (correct semantic but wrong spatial), CoT Hallucination (logical errors in generated reasoning).

- **First 3 experiments**: 1) Train SAGE only on RIR reconstruction task to verify latent space clustering by distance/azimuth. 2) Overfit sanity check on single room from BiDepth. 3) Ablation using only Mel-spectrogram channels to quantify IPD contribution.

## Open Questions the Paper Calls Out

- How well does OWL generalize to real-world acoustic environments given that BiDepth is entirely simulation-based?
- Can OWL's reasoning capabilities extend to multi-turn dialogues about spatial scenes rather than single-turn QA?
- How does performance degrade for sources at extreme elevation angles given the dataset's horizontal-plane bias?

## Limitations
- Relies on synthetic BiDepth dataset, leaving sim-to-real transfer validation open
- Geometry loss hyperparameters (η₂, λ) not systematically analyzed
- No direct evidence that CoT outputs improve reasoning versus providing surface-level explanations

## Confidence

**High Confidence**:
- SAGE's 11° improvement in DoA estimation over BAT baseline
- The 25% spatial reasoning accuracy gain from geometry-aware training
- The necessity of Stage 2 intermediate reasoning training

**Medium Confidence**:
- The claim that RIR reconstruction "internalizes geometry-aware features" without depth at inference
- The assertion that IPD + Mel fusion provides optimal spatial-semantic decoupling

**Low Confidence**:
- Generalization to real-world audio without synthetic depth supervision
- The relative contribution of L_EDC vs. L₁ in the geometry loss
- Whether CoT outputs reflect genuine reasoning or pattern matching

## Next Checks
1. **Real-World Transfer Test**: Evaluate OWL on held-out AudioSet recordings captured with actual binaural microphones to quantify sim-to-real gap.
2. **Geometry Loss Ablation**: Systematically vary η₂ and λ to identify optimal balance between RIR reconstruction and EDC supervision.
3. **CoT Grounding Verification**: Extract intermediate coordinates claimed in CoT and compare against actual DoA predictions to calculate logical consistency rate.