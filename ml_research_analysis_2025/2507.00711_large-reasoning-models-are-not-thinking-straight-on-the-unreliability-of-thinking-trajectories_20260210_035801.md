---
ver: rpa2
title: 'Large Reasoning Models are not thinking straight: on the unreliability of
  thinking trajectories'
arxiv_id: '2507.00711'
source_url: https://arxiv.org/abs/2507.00711
tags:
- reasoning
- arxiv
- preprint
- tokens
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of chain-of-thought (CoT)
  reasoning in large reasoning models (LRMs) trained via reinforcement learning. While
  LRMs achieve strong performance on reasoning benchmarks, the authors question whether
  these gains reflect genuine reasoning improvements or overfitting to specific patterns.
---

# Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories

## Quick Facts
- **arXiv ID:** 2507.00711
- **Source URL:** https://arxiv.org/abs/2507.00711
- **Reference count:** 5
- **Primary result:** LRMs frequently disregard externally provided correct solutions and engage in "overthinking"

## Executive Summary
This paper investigates whether gains in chain-of-thought reasoning by large reasoning models reflect genuine reasoning improvements or overfitting to specific patterns. The authors present evidence of "overthinking" behavior where models generate unnecessary reasoning steps that often lead to incorrect conclusions, even when correct solutions are explicitly provided. Using the AIME2024 math benchmark with three state-of-the-art models (LLaMA-70B, Qwen-7B, and DeepScalR-1.5B), the study reveals that models frequently fail to integrate corrective information from ground-truth solutions injected into their reasoning trajectories.

The research challenges the assumed relationship between CoT length and reasoning quality, suggesting that current LRM reasoning dynamics have critical limitations. The phenomenon of overthinking appears systematic across different model architectures, indicating potential fundamental issues in how these models process and integrate reasoning information rather than isolated implementation failures.

## Method Summary
The authors conducted experiments using the AIME2024 math benchmark with three state-of-the-art large reasoning models: LLaMA-70B, Qwen-7B, and DeepScalR-1.5B. They employed a methodology where ground-truth solutions were explicitly injected into the models' reasoning trajectories to test whether the models would integrate this corrective information. The study measured the models' ability to incorporate externally provided correct solutions and analyzed the reasoning dynamics when models had access to both their own generated reasoning and externally injected ground-truth reasoning.

## Key Results
- Models frequently fail to integrate externally provided correct solutions even when these are explicitly injected into reasoning trajectories
- Evidence of "overthinking" behavior where models generate unnecessary reasoning steps leading to incorrect conclusions
- Systematic failure patterns observed across multiple model architectures (LLaMA-70B, Qwen-7B, DeepScalR-1.5B)
- Challenges the assumed relationship between chain-of-thought length and reasoning quality

## Why This Works (Mechanism)
The paper does not provide a definitive mechanism explaining why LRMs exhibit overthinking behavior and fail to integrate externally provided solutions. The authors suggest that the phenomenon may stem from training data patterns, architectural constraints, or optimization objectives, but do not conclusively establish the underlying causes. The observed behavior appears to be a fundamental limitation in how these models process and integrate reasoning information rather than an isolated implementation issue.

## Foundational Learning
**Chain-of-Thought Reasoning** - Sequential reasoning steps that models generate to solve problems; needed to understand the core methodology being evaluated and how overthinking manifests.
**Reinforcement Learning in LRMs** - Training approach using reward signals to shape reasoning behavior; needed to contextualize why models might develop suboptimal reasoning patterns.
**Ground-truth Solution Injection** - Experimental technique of providing correct answers within reasoning trajectories; needed to understand the experimental design and what constitutes model failure.
**Overthinking Phenomenon** - When models generate excessive or incorrect reasoning steps despite having access to correct information; needed to grasp the central problem being investigated.
**Benchmark Evaluation** - Standardized testing methodology using AIME2024; needed to understand how the authors measure and validate their findings.

## Architecture Onboarding
**Component Map:** Input -> Chain-of-Thought Generator -> Reasoning Processor -> Output
**Critical Path:** Input → Chain-of-Thought Generation → Solution Formation → Output
**Design Tradeoffs:** Longer reasoning chains may improve accuracy but risk overthinking; shorter chains may miss necessary steps but avoid integration failures.
**Failure Signatures:** Models generate reasoning that contradicts injected ground-truth solutions; unnecessary intermediate steps that lead away from correct answers.
**First 3 Experiments:** 1) Test model response to ground-truth solution injection at different positions in reasoning chain. 2) Vary the format and presentation of externally provided solutions. 3) Compare performance across different reasoning benchmark datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus primarily on AIME2024 benchmark and three specific model architectures, limiting generalizability
- Underlying mechanisms driving overthinking behavior remain unclear and unexplained
- Does not conclusively prove that all reasoning benchmark gains are illusory rather than genuine capability improvements

## Confidence
- **High:** Core finding that LRMs frequently disregard externally provided correct solutions is well-supported by systematic experimental design
- **Medium:** Broader claim about the relationship between CoT length and reasoning quality, as alternative explanations are not comprehensively tested

## Next Checks
1. Replicate experiments across a more diverse set of reasoning benchmarks and model architectures to assess generalizability
2. Conduct ablation studies varying the format and positioning of externally injected solutions to understand model integration mechanisms
3. Analyze training data distributions of these LRMs to identify potential patterns that might encourage overthinking behaviors