---
ver: rpa2
title: 'SEMA: a Scalable and Efficient Mamba like Attention via Token Localization
  and Averaging'
arxiv_id: '2506.08297'
source_url: https://arxiv.org/abs/2506.08297
tags:
- attention
- sema
- linear
- window
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proves that generalized attention mechanisms, including
  softmax and linear attention, inherently disperse as the number of tokens grows,
  leading to equal weighting of all keys. To address this, the authors propose SEMA,
  a scalable and efficient Mamba-like attention mechanism that combines localized
  window attention with arithmetic averaging to maintain focus while capturing global
  information.
---

# SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging
## Quick Facts
- arXiv ID: 2506.08297
- Source URL: https://arxiv.org/abs/2506.08297
- Reference count: 40
- 83.7% top-1 accuracy on ImageNet-1K, outperforming MILA and other Mamba-like models at similar parameter sizes

## Executive Summary
This paper addresses the inherent dispersion problem in generalized attention mechanisms as token count grows, where all keys become equally weighted. The authors propose SEMA (Scalable and Efficient Mamba-like Attention) that combines localized window attention with arithmetic averaging to maintain focus while capturing global information. SEMA achieves state-of-the-art performance among Mamba-like models, reaching 83.7% top-1 accuracy on ImageNet-1K while scaling effectively to larger image resolutions with 0.5-0.7% accuracy improvements over VMamba. The method also demonstrates competitive results on COCO instance segmentation, showing versatility across computer vision tasks.

## Method Summary
SEMA addresses the token dispersion problem in attention mechanisms by implementing a two-stage approach: localized window attention followed by arithmetic averaging. The method first applies window-based attention to capture local features within a defined neighborhood, then uses averaging to aggregate global context without the quadratic complexity of traditional attention. This design maintains the efficiency benefits of Mamba-like architectures while preserving the ability to capture both local and global information. The approach is particularly effective for vision tasks where spatial relationships matter but full global attention is computationally prohibitive.

## Key Results
- Achieves 83.7% top-1 accuracy on ImageNet-1K, outperforming MILA and other Mamba-like models at similar parameter sizes
- Scales effectively to larger image resolutions, improving accuracy by 0.5-0.7% over VMamba
- Demonstrates competitive performance on COCO instance segmentation, validating cross-task effectiveness

## Why This Works (Mechanism)
SEMA works by addressing the fundamental limitation of attention mechanisms where increasing token count leads to uniform weighting across all keys. The window attention component localizes the computation to relevant neighborhoods, preventing the dilution of important features. The subsequent averaging stage efficiently captures global context without the computational burden of full attention. This two-stage process preserves the essential information flow while maintaining linear complexity, making it scalable for high-resolution vision tasks where traditional attention becomes prohibitively expensive.

## Foundational Learning
- **Token dispersion in attention**: As token count increases, attention weights spread uniformly across all keys, reducing discriminative power - needed to understand why standard attention fails at scale, check by visualizing attention weight distributions
- **Window-based attention**: Local attention computation within defined spatial neighborhoods - needed for computational efficiency, verify by measuring complexity reduction
- **Arithmetic averaging for global context**: Simple aggregation method to capture global information without quadratic complexity - needed for efficiency, test by comparing with other aggregation methods
- **Mamba architecture principles**: State-space models for sequence processing with linear complexity - needed as foundation, validate by reproducing baseline results
- **Vision transformer scaling**: How attention mechanisms perform as image resolution increases - needed for practical application, measure accuracy scaling with resolution
- **Computational complexity trade-offs**: Balance between model expressiveness and efficiency - needed for practical deployment, analyze FLOPs and accuracy trade-offs

## Architecture Onboarding
- **Component map**: Input tokens -> Window Attention -> Arithmetic Averaging -> Output tokens
- **Critical path**: Token embedding → Localized window processing → Global averaging → Final representation
- **Design tradeoffs**: Window size vs. computational cost, averaging scheme vs. information preservation, model depth vs. accuracy
- **Failure signatures**: Loss of fine-grained local details with overly large windows, insufficient global context with aggressive averaging, accuracy drop when scaling beyond training resolution
- **First experiments**: 1) Ablation study on window size impact on accuracy and FLOPs, 2) Comparison of different averaging methods (mean, max, attention-weighted), 3) Scaling analysis on progressively larger image resolutions

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical SEMA mechanism lacks theoretical grounding for why token localization and averaging preserves global context effectively
- Limited ablation studies fail to isolate individual component contributions (window size, averaging scheme, scaling factor)
- Comparison restricted to specific parameter scales with unclear scaling behavior beyond 10M parameters
- No analysis of performance on non-natural image domains or sequences with variable token importance

## Confidence
- Experimental results showing SEMA's competitive performance on ImageNet-1K and COCO: High
- Theoretical justification for SEMA's design choices: Medium
- Claim that SEMA "maintains focus while capturing global information": Low

## Next Checks
1. Conduct ablation studies varying window size, averaging methods, and scaling factors to quantify each component's contribution to final accuracy
2. Test SEMA on out-of-distribution datasets and non-natural image domains (medical imaging, satellite imagery) to evaluate robustness beyond standard benchmarks
3. Analyze the information bottleneck introduced by averaging through techniques like attention visualization or probing classifiers to verify global context preservation