---
ver: rpa2
title: The Cost of Local and Global Fairness in Federated Learning
arxiv_id: '2503.22762'
source_url: https://arxiv.org/abs/2503.22762
tags:
- fairness
- local
- global
- predictor
- outcome
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the cost of enforcing both local and global
  fairness in federated learning for multi-class problems. The authors formulate the
  problem as a convex program to find the optimal outcome predictor satisfying fairness
  constraints while minimizing accuracy loss.
---

# The Cost of Local and Global Fairness in Federated Learning

## Quick Facts
- arXiv ID: 2503.22762
- Source URL: https://arxiv.org/abs/2503.22762
- Reference count: 40
- Authors: Yuying Duan; Gelei Xu; Yiyu Shi; Michael Lemmon
- Primary result: Framework achieves comparable/better fairness than state-of-the-art methods while reducing accuracy loss by 0.5%-4%, communication costs by 20%, and computational costs by 33%.

## Executive Summary
This paper proposes a post-processing framework for enforcing both local (within-client) and global (population-wide) fairness in federated learning for multi-class problems. The method trains a standard model using FedAvg, then solves a linear program to find target true positive rates that satisfy fairness constraints while minimizing accuracy loss. By using a simplex to approximate the Region Under the ROC Surface (RUS), the authors transform an intractable convex program into a tractable linear program. Experiments on three real-world datasets demonstrate that the framework achieves state-of-the-art fairness with reduced accuracy loss and computational costs compared to existing methods.

## Method Summary
The framework operates in four steps: (1) train a Bayesian optimal score function using FedAvg on local data; (2) clients compute and upload local statistics without sharing raw data; (3) the server constructs and solves a linear program to find optimal true positive rates satisfying fairness constraints; (4) clients solve a linear algebraic equation to compute randomized mapping functions that transform the model output. The approach uses a simplex approximation of the RUS to make the optimization tractable, allowing simultaneous enforcement of both local and global fairness through post-processing rather than modifying the training loop.

## Key Results
- Achieves comparable or better fairness than state-of-the-art methods (FCFL, FairFed, Fair-FATE, EquiFL)
- Reduces accuracy loss by 0.5%-4% compared to baselines
- Cuts communication costs by 20% and computational costs by 33%
- Demonstrates that local and global fairness impact accuracy differently across datasets and heterogeneity levels
- Shows trade-off between privacy (via differential privacy) and fairness performance

## Why This Works (Mechanism)

### Mechanism 1: Simplex Approximation of the ROC Region
The framework makes a convex optimization tractable by linearizing the feasible region of true positive rates. The paper formulates the fair predictor problem as a convex program over the "Region Under the ROC Surface" (RUS). Because characterizing the exact RUS for multi-class problems is computationally intractable, the authors approximate this convex set with a simplex ($\hat{D}_{ac}$). This simplex is defined by the vertices of the unit vectors and the true positive vector of a derived "costless" predictor ($e_{Y_1}$). This geometric approximation allows the constraints to be expressed linearly. The core assumption is that the simplex $\hat{D}_{ac}$ is a sufficiently tight inner approximation of the true convex set $D_{ac}$ so that the optimal solution remains meaningful.

### Mechanism 2: Decoupled Post-Processing via Linear Programming
Fairness can be enforced by solving a global Linear Program (LP) based on local statistics rather than modifying the training loop. The algorithm first trains a standard model using FedAvg. It then decouples the fairness enforcement: clients compute and transmit local statistics (confusion rates), and the server solves a global LP to find target true positive rates ($z_{ac}$) that satisfy fairness constraints. Finally, clients solve a Linear Algebraic Equation (LAE) to find a randomized mapping function (a probability vector $\beta_{ac}$) that transforms the original model's output to match the LP targets. The core assumption is that the Bayesian optimal score function learned in the first phase is accurate enough that the post-hoc linear algebraic solution can effectively "shift" the decision boundary via randomization without breaking model utility.

### Mechanism 3: Joint Local and Global Constraint Formulation
Simultaneous enforcement of local and global fairness is achieved by modeling them as coupled linear inequalities in a single objective function. The framework defines global fairness (across the population) and local fairness (within a client) as constraints on true positive differences between sensitive groups. These are encoded in a matrix $A$ and vector $b$ (Eq. 8 and 12). By adjusting the fairness budget vector $\epsilon$ (e.g., $\epsilon_0$ for global, $\epsilon_c$ for local), the LP minimizes accuracy loss subject to these bounds. The core assumption is that the accuracy loss is a linear combination of true positive rates, allowing it to be minimized linearly.

## Foundational Learning

- **Concept:** Linear Programming (LP)
  - **Why needed here:** The core contribution relies on reformulating a complex fairness optimization into a set of linear constraints and a linear objective function. Understanding objective functions, constraint matrices, and feasibility is required to modify the fairness bounds ($\epsilon$).
  - **Quick check question:** Can you explain why approximating a convex region with a simplex allows us to use standard LP solvers?

- **Concept:** Group Fairness Metrics (Equalized Odds & Statistical Parity)
  - **Why needed here:** The method optimizes for specific definitions of fairness. The user must distinguish between Statistical Parity (independence of $Y$) and Equalized Odds (independence conditional on $Y$) to choose the correct constraint configuration (Appendix D vs Section 5).
  - **Quick check question:** Does Equalized Odds allow for differences in False Positive Rates between groups, and how is this captured in the paper's constraints?

- **Concept:** Federated Averaging (FedAvg)
  - **Why needed here:** The framework uses FedAvg as the "Step 1" generator of the Bayesian score function. Understanding the communication rounds and local update epochs is necessary to diagnose if the base model has converged before the post-processing step.
  - **Quick check question:** If FedAvg fails to converge to a reasonable accuracy in Step 1, can the LP in Step 3 still enforce fairness? (Answer: It can enforce fairness, but accuracy will be low).

## Architecture Onboarding

- **Component map:** FedAvg Trainer -> Statistics Collector -> Global LP Solver (Server) -> Local LAE Solver (Client) -> Randomized Post-processing
- **Critical path:** The Local Statistic Computation (Eq. 15) is the pivot point. If these statistics are noisy or sparse (e.g., a client has zero examples of a certain class/group), the global LP will be ill-conditioned.
- **Design tradeoffs:**
  - Randomization vs. Determinism: The fair predictor (Algorithm 1) is randomized. It sometimes ignores the model output and returns a random class to satisfy fairness stats. This reduces accuracy cost compared to "reject option" classification but introduces non-determinism.
  - Privacy vs. Utility: The paper notes a trade-off with Differential Privacy (DP). Adding noise to local statistics (Laplace mechanism) degrades the fairness-accuracy trade-off (Section 7, Table 4).
- **Failure signatures:**
  - Infeasible LP: If fairness constraints ($\epsilon$) are set too tight relative to the base model's capability, the LP solver will return "infeasible."
  - High Variance in Minority Clients: If a client has very few samples for a specific sensitive group, the estimated statistics are high variance, leading to unstable fairness enforcement.
- **First 3 experiments:**
  1. Baselines Reproduction: Implement the LP solver on a centralized dataset (e.g., Adult) to verify that the simplex approximation actually captures the convex region and reduces accuracy loss compared to a threshold-based approach.
  2. Heterogeneity Stress Test: Vary the data distribution across simulated clients (e.g., changing the proportion of sensitive attributes per client) to observe the "accuracy cost" curve shown in Figure 2.
  3. DP Integration: Run the Laplace mechanism on the local statistics as described in Section 7 to verify the trade-off between the privacy budget $\epsilon$ and the resulting global/local disparity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be adapted to provide formal fairness guarantees when local differential privacy (DP) noise is injected into the client statistics?
- Basis in paper: [explicit] Section 7 investigates the "Privacy Protection of the Local Statistics" using a Laplace mechanism. The authors conclude, "It is clear that there is a trade-off between privacy and fairness," noting that disparity increases as privacy protection strengthens ($\epsilon$ decreases), but they offer no solution to mitigate this.
- Why unresolved: The current method applies noise post-hoc to the statistics used to construct the Linear Program (LP). This noise perturbs the constraints, causing the optimal solution to deviate from the intended fairness bounds, yet the framework does not account for this uncertainty in the optimization process.
- What evidence: A modified optimization formulation that incorporates the Laplace noise distribution into the constraints (e.g., chance-constrained programming) or empirical results showing bounded disparity under specific DP budgets.

### Open Question 2
- Question: What is the theoretical sub-optimality gap between the simplex-based Linear Program (LP) approximation and the true convex program for the Region Under the ROC Surface (RUS)?
- Basis in paper: [inferred] Section 5 states that solving the convex program is "computationally intractable," so the authors "approximate that surface using a simplex." The paper relies on this approximation to achieve polynomial time complexity but does not quantify the approximation error or the potential accuracy loss compared to the theoretical optimum.
- Why unresolved: The simplex $\hat{D}_{ac}$ is an inner approximation ($\hat{D}_{ac} \subset D_{ac}$), meaning the LP solution is feasible but may be strictly sub-optimal in terms of accuracy compared to the convex program's solution.
- What evidence: A theoretical derivation of the approximation error bound relative to the number of classes $N$, or a comparative analysis on small-scale datasets where the convex program is tractable to measure the exact gap.

### Open Question 3
- Question: Under what specific conditions of data heterogeneity do local fairness constraints conflict with global fairness constraints, causing one to degrade when optimizing for the other?
- Basis in paper: [explicit] In Section 7, the authors observe that "enforcing global fairness reduces local disparity by 45% for Adult... but increases it by 58% for ACSPublicCoverage." They state the impact varies by dataset but do not theoretically characterize the underlying distributional properties that cause this divergence.
- Why unresolved: While the paper demonstrates that the framework can handle both constraints simultaneously, it does not explain why the interaction between local and global fairness is synergistic in some datasets and antagonistic in others.
- What evidence: A theoretical analysis relating the divergence in local vs. global fairness performance to the statistical distance (e.g., total variation) between client data distributions.

## Limitations
- The simplex approximation's tightness is not empirically validated across all datasets, leaving uncertainty about whether reported accuracy-fairness trade-offs are optimal.
- The DP integration uses Laplace noise without fully specifying sensitivity analysis, making the privacy-utility trade-off hard to verify.
- The framework's performance on higher-dimensional class spaces beyond the three tested datasets is unknown.

## Confidence
- **High Confidence:** The post-processing framework's mechanism (LP + LAE) is well-defined and reproducible. The reported reductions in communication (20%) and computation (33%) are plausible given the shift from in-processing to post-processing.
- **Medium Confidence:** The accuracy loss reduction of 0.5%-4% compared to state-of-the-art is based on three datasets. The result is credible but may not generalize to other domains or higher-dimensional class spaces.
- **Low Confidence:** The claim that the simplex approximation is "sufficiently tight" is theoretical. No empirical bound or error analysis is provided.

## Next Checks
1. **Simplex Tightness Verification:** For a small-scale dataset (e.g., Adult), compute the exact RUS (e.g., via brute-force sampling) and compare it to the simplex. Report the volume ratio or Hausdorff distance to quantify approximation quality.
2. **DP Sensitivity Analysis:** Reproduce the Laplace mechanism experiment (Table 4) with different sensitivity values and noise scales. Plot the accuracy-fairness curve as a function of the privacy budget to confirm the trade-off shape.
3. **Stress Test on Heterogeneous Data:** Simulate extreme data heterogeneity (e.g., clients with disjoint class distributions) and measure the maximum achievable fairness under the given constraints. This will reveal if the LP becomes infeasible or if accuracy drops sharply.