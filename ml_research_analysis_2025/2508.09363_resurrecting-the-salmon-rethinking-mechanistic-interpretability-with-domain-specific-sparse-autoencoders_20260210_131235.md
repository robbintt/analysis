---
ver: rpa2
title: 'Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific
  Sparse Autoencoders'
arxiv_id: '2508.09363'
source_url: https://arxiv.org/abs/2508.09363
tags:
- saes
- sparse
- features
- interpretability
- autoencoders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders

## Quick Facts
- **arXiv ID**: 2508.09363
- **Source URL**: https://arxiv.org/abs/2508.09363
- **Reference count**: 40
- **Primary result**: Domain-specific SAEs substantially outperform broad-domain SAEs in medical interpretability while maintaining strong reconstruction fidelity.

## Executive Summary
This paper re-examines sparse autoencoder (SAE) approaches to mechanistic interpretability by demonstrating that domain-specific SAEs trained on medical corpora significantly outperform broad-domain SAEs on the same tasks. The authors argue that the "salmon" of interpretability is not dead but requires rethinking through domain adaptation. Using JumpReLU SAEs on Llama-3.2-3B activations, they show that medical-domain SAEs achieve higher F1 scores, better variance explained, and superior feature interpretability compared to models trained on general internet text.

## Method Summary
The authors developed domain-specific sparse autoencoders by fine-tuning Llama-3.2-3B on curated medical corpora and training SAEs on the resulting activation patterns. They used the JumpReLU architecture with L0 sparsity penalties and compared medical-domain SAEs against broad-domain baselines using interpretability metrics including feature localization accuracy, reconstruction fidelity, and F1 scores. The approach involved first domain-adaptive pre-training of the base LLM, then training SAEs on the medical-specific activations, and finally evaluating both reconstruction quality and interpretability through automated and human evaluation methods.

## Key Results
- Medical-domain SAEs achieved substantially higher F1 scores compared to broad-domain SAEs on the same medical activation data
- Domain-specific SAEs demonstrated superior feature interpretability while maintaining strong reconstruction fidelity
- The approach validated that domain adaptation significantly enhances SAE performance for mechanistic interpretability tasks

## Why This Works (Mechanism)
Domain-specific SAEs work better because they learn features aligned with the statistical and semantic patterns of the target domain. Medical text contains specialized terminology, domain-specific reasoning patterns, and distinct activation distributions that general-purpose SAEs fail to capture effectively. By training both the base model and SAEs on medical corpora, the system develops latent representations that directly map to clinically meaningful concepts and reasoning pathways, enabling more accurate feature localization and interpretation.

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural networks that learn compressed representations by enforcing sparsity in latent features; needed to understand the core interpretability mechanism
- **JumpReLU activation**: A specific activation function used in SAEs that helps maintain sparsity while enabling gradient flow; critical for training stability
- **L0 sparsity penalties**: Regularization that directly counts active features rather than squared penalties; important for controlling feature cardinality
- **Mechanistic interpretability**: The field focused on understanding how neural networks implement specific behaviors through feature decomposition
- **Domain adaptation**: The process of fine-tuning models on specialized corpora to capture domain-specific patterns
- **Variance explained**: A metric measuring how much of the input signal is captured by reconstructed outputs; indicates reconstruction quality

## Architecture Onboarding
- **Component map**: Base LLM -> Activation extraction -> SAE encoder -> Sparse latent features -> SAE decoder -> Reconstructed activations
- **Critical path**: Input medical text → Llama-3.2-3B → Layer activations → JumpReLU SAE → Sparse features → Feature interpretation
- **Design tradeoffs**: Domain specificity vs. general applicability, reconstruction fidelity vs. interpretability sparsity, computational cost vs. feature granularity
- **Failure signatures**: Poor domain alignment leading to irrelevant features, excessive sparsity killing meaningful patterns, reconstruction collapse from over-regularization
- **First experiments**: 1) Train broad-domain SAE on general Llama-3.2-3B, 2) Train medical-domain SAE on medical Llama-3.2-3B, 3) Compare F1 scores and variance explained on held-out medical data

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Does domain-specific SAE training transfer to even larger medical corpora (e.g., bioarXiv papers, clinical textbooks, 2B+ tokens), and does feature granularity improve commensurately?
- **Basis in paper**: [explicit] Future Work section explicitly proposes training on "bioarXiv papers, extensive clinical textbooks, and upwards of 2 billion medicine-specific tokens" to enhance feature granularity and robustness.
- **Why unresolved**: Current experiments used only ~195k examples (~50M tokens); scalability to billion-token medical corpora is untested.
- **What evidence would resolve it**: Training identical SAE architectures on expanded medical corpora and measuring whether interpretability scores (F1) and variance explained continue to improve.

### Open Question 2
- **Question**: Would alternative optimization objectives such as minimum description length (MDL-SAEs) yield more interpretable latent decompositions than sparsity-constrained training?
- **Basis in paper**: [explicit] Future Work proposes "alternative optimisation targets (e.g., minimising description length rather than solely enforcing sparsity)" as a methodological extension.
- **Why unresolved**: All experiments used L0 sparsity penalties; MDL-based objectives are mentioned but not tested.
- **What evidence would resolve it**: Comparative study of MDL-SAEs vs. JumpReLU SAEs on the same medical domain, measuring interpretability and reconstruction fidelity.

### Open Question 3
- **Question**: Do domain-specific SAE features support downstream tasks such as constructing causal subcircuits via sparse feature circuits?
- **Basis in paper**: [explicit] Future Work states that "further experiments are needed to assess the downstream benefits of these refined latent features — such as in the construction of causal subcircuits using sparse feature circuits."
- **Why unresolved**: The paper evaluates reconstruction and interpretability but does not test whether features enable causal circuit discovery or model editing.
- **What evidence would resolve it**: Demonstrating that medical-domain SAE features can be composed into sparse feature circuits that causally mediate specific clinical reasoning behaviors.

### Open Question 4
- **Question**: Do domain-confined SAE benefits generalize to non-text modalities and crosscoder architectures?
- **Basis in paper**: [explicit] Future Work mentions "significant potential in applying domain-specific SAE methods to other modalities or in the context of crosscoders."
- **Why unresolved**: Experiments were limited to LLM text activations on a single layer.
- **What evidence would resolve it**: Training domain-specific SAEs on vision, protein, or multimodal models and comparing to broad-domain baselines on analogous interpretability metrics.

## Limitations
- Experiments were limited to text-based LLMs and single-layer activation analysis
- Current domain adaptation scope restricted to medical corpora (~195k examples, ~50M tokens)
- No evaluation of cross-modal or cross-architectural generalization
- Limited exploration of alternative SAE architectures beyond JumpReLU with L0 penalties

## Confidence
- **Medical domain superiority**: High - Clear quantitative improvements in F1 scores and interpretability metrics
- **Scalability claims**: Medium - Theoretical justification strong but empirical validation on large corpora pending
- **Generalizability**: Low - Limited to medical text and single architecture; broader claims remain untested

## Next Checks
1. Replicate domain-specific SAE training on expanded medical corpora (bioarXiv, clinical textbooks) to verify scalability claims
2. Implement and compare MDL-SAE optimization against current L0 sparsity approach on identical medical datasets
3. Test whether medical-domain SAE features can be composed into sparse feature circuits that causally mediate clinical reasoning behaviors