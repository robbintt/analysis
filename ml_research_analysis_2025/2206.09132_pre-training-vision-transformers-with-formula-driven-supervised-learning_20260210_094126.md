---
ver: rpa2
title: Pre-training Vision Transformers with Formula-driven Supervised Learning
arxiv_id: '2206.09132'
source_url: https://arxiv.org/abs/2206.09132
tags:
- pre-training
- fdsl
- images
- datasets
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces formula-driven supervised learning (FDSL)
  as an alternative to conventional real-image pre-training methods. FDSL trains vision
  transformers using mathematically generated images, avoiding the privacy, copyright,
  and labeling issues associated with real-image datasets.
---

# Pre-training Vision Transformers with Formula-driven Supervised Learning

## Quick Facts
- arXiv ID: 2206.09132
- Source URL: https://arxiv.org/abs/2206.09132
- Reference count: 40
- Primary result: FDSL matches or exceeds ImageNet-21k pre-training performance while using 14.2× fewer images

## Executive Summary
This paper introduces Formula-Driven Supervised Learning (FDSL) as an alternative to conventional real-image pre-training methods. FDSL trains vision transformers using mathematically generated images, avoiding the privacy, copyright, and labeling issues associated with real-image datasets. The authors demonstrate that FDSL can match or exceed the performance of ImageNet-21k pre-training and approach JFT-300M levels while using 14.2× fewer images. Two key hypotheses are validated: object contours are the critical feature for effective pre-training, and increasing the complexity of mathematically generated images improves performance.

## Method Summary
The FDSL approach uses mathematical formulas to generate synthetic images with automatic label assignment. The authors develop Radial Contour Database (RCDB) and Extended Fractal Database (ExFractalDB) as FDSL datasets. Images are generated using Iterated Function Systems (IFS) and radial contour polygons, with labels derived from the generative parameters. Standard Vision Transformers are pre-trained on these synthetic datasets using supervised classification, then fine-tuned on downstream tasks like ImageNet-1k classification and COCO detection.

## Key Results
- FDSL (FractalDB) achieves 97.0% accuracy on CIFAR-10, surpassing self-supervised learning (94.8%)
- ExFractalDB-21k with ViT-Base achieves 82.7% top-1 accuracy on ImageNet-1k, matching JFT-300M's 84.1%
- Contour corruption drops performance from 95.5% to 46.2% on CIFAR-10, validating the contour hypothesis
- RCDB with 3-202 vertices outperforms simpler shapes, demonstrating optimal complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Object contours are the critical visual feature for effective ViT pre-training using synthetic images.
- Mechanism: Mathematical formulas generate images with clear, learnable boundaries (contours). ViT self-attention mechanisms focus on these outer contours during pre-training, learning boundary-detection representations that transfer to real-world object recognition where contours similarly define object boundaries.
- Core assumption: The contour-attention behavior learned from synthetic shapes generalizes to natural images where objects also have defining boundaries.
- Evidence anchors: Abstract confirms "object contours are what matter in FDSL datasets"; Table 7 shows RCDB with object contours yields higher scores; section 5.3 demonstrates contour corruption severely degrades accuracy across multiple datasets.

### Mechanism 2
- Claim: Increasing formula parameter complexity creates harder pre-training tasks that improve downstream transfer.
- Mechanism: More parameters create finer-grained class distinctions. The model must learn to discriminate between more subtle parameter variations, forcing richer feature representations.
- Core assumption: Task difficulty during pre-training correlates with representation quality, following a curriculum-like effect.
- Evidence anchors: Abstract states "increased number of parameters for label creation improves performance"; Table 9 shows BezierCurveDB, RCDB, and ExFractalDB gains when parameters increased; Table 10 demonstrates ExFractalDB (3D IFS) outperforms FractalDB (2D IFS).

### Mechanism 3
- Claim: Formula-derived labels provide effective supervision without real images or human annotation.
- Mechanism: Each unique parameter configuration automatically defines a class label. The model learns to map visual patterns to their generative parameters, creating a discriminative task that transfers to real-world classification.
- Core assumption: The mapping between visual appearance and generative parameters shares computational similarities with mapping visual appearance to semantic categories.
- Evidence anchors: Abstract notes FDSL "avoids privacy and copyright issues, labeling costs and errors"; section 3.2 explains automatic label generation; Table 1 shows FDSL (Fractal) 97.0 > SSL (SimCLRV2) 94.8 on CIFAR-10.

## Foundational Learning

- **Iterated Function Systems (IFS) for Fractal Generation**
  - Why needed here: ExFractalDB uses 3D-IFS to generate complex fractal images. Understanding how affine transformations compose to create self-similar patterns is essential for reproducing or extending the datasets.
  - Quick check question: Can you explain why a 3D affine transformation projected to 2D creates more visual diversity than a 2D IFS directly?

- **Vision Transformer Self-Attention Mechanisms**
  - Why needed here: The paper relies on attention map analysis to validate hypothesis 1. Understanding how patch embeddings and multi-head attention distribute focus across images is necessary to interpret these visualizations.
  - Quick check question: Why would ViT attention naturally focus on contours rather than texture or color?

- **Transfer Learning from Synthetic to Real Domains**
  - Why needed here: The core claim is that pre-training on synthetic contours transfers to real image classification. Understanding domain shift, feature transferability, and fine-tuning dynamics is critical.
  - Quick check question: Why might learning from 21M synthetic images transfer better than 300M real images (JFT-300M achieved 84.1% vs ExFractalDB's 83.8%)?

## Architecture Onboarding

- **Component map:**
  - Formula-driven image synthesis -> Automatic parameter-to-label mapping -> ViT Backbone -> Supervised pre-training -> Fine-tuning interface -> Downstream task

- **Critical path:**
  1. Generate FDSL dataset (ExFractalDB-21k: 21M images, 21k classes, 1k instances/class)
  2. Pre-train ViT with supervised cross-entropy loss on synthetic data
  3. Transfer pre-trained weights to downstream task
  4. Fine-tune with task-specific head and hyperparameters

- **Design tradeoffs:**
  - Contour complexity vs. learnability: Too few vertices under-train; too many (>302) create noise. Optimal: 3-202 vertices
  - Dataset scale vs. saturation: ExFractalDB-21k (82.7%) ≈ ExFractalDB-100k (82.7%), showing saturation
  - Batch size vs. stability: Optimal batch 1,024 for FDSL vs. 4,096+ typical for real-image SSL
  - Resolution augmentation: Fine-tuning at 384² vs. 224² provides +1.1% gain

- **Failure signatures:**
  - Broken contours: Adding random background-colored lines drops Cars accuracy from 78.4% to 1.1%
  - Insufficient rendering: FractalDB with <50k points fails to form coherent contours
  - Over-complexity: RCDB with 403-502 vertices achieves only 0.8% on Cars vs. 11.6% from scratch
  - Random labels: LineDB-512 with permuted labels: 0.8% Cars vs. 71.9% with correct labels
  - Detection task mismatch: FDSL underperforms ImageNet-21k on COCO detection (47.3 vs. 49.0 AP)

- **First 3 experiments:**
  1. **Reproduce RCDB-1k baseline**: Generate 1k classes with 1k instances each (vertices 3-202). Pre-train ViT-Tiny for 300 epochs, batch 1,024. Fine-tune on CIFAR-10. Target: ~96.5% accuracy.
  2. **Contour corruption ablation**: Take trained RCDB model, generate corrupted test images with random background-colored lines. Measure attention map degradation and fine-tuning performance drop.
  3. **Complexity scaling test**: Train three ViT-Base models on ExFractalDB with 10k, 21k, 50k classes (fixed 1k instances/class). Fine-tune on ImageNet-1k at 384² resolution. Verify saturation pattern.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a specifically tailored pre-training task be designed to close the performance gap between FDSL and real-image pre-training for object detection and instance segmentation? The authors state that current approach "did not reach the performance level of ImageNet-1k/21k" on localization tasks and conclude that "we should prepare a more suitable pre-training task for object detection and instance segmentation."

- **Open Question 2**: What hyperparameter adjustments are required to effectively pre-train larger architectures, such as ViT-Large, using FDSL without suffering performance degradation? The paper notes that ViT-Large achieved lower accuracy (81.0%) on ExFractalDB-50k compared to ViT-Base (82.7%), explicitly suggesting this was "likely due to a need to adjust the hyperparameters."

- **Open Question 3**: Does increasing the scale of FDSL datasets beyond 21k classes yield diminishing returns for all model architectures, or can further scaling be made effective by combining it with higher input resolutions? The authors observed that scaling the dataset from ExFractalDB-21k to 50k/100k resulted in saturated accuracy for ViT-Base, while increasing fine-tuning resolution to 384px significantly boosted performance to near JFT-300M levels.

## Limitations

- The approach's reliance on specific mathematical patterns (contours) may limit generalizability to tasks requiring texture or context understanding
- The synthetic-to-real transfer mechanism remains incompletely understood despite contour attention map evidence
- Ablation studies focus on ViT architectures, leaving open questions about effectiveness across different model families

## Confidence

- **High**: RCDB/ExFractalDB performance matching ImageNet-21k on ImageNet-1k fine-tuning (83.8% vs 84.1%)
- **Medium**: Contour importance hypothesis (strong ablation evidence but limited to ViT architectures)
- **Low**: Claims about computational efficiency and scalability beyond tested configurations (no analysis of memory/compute trade-offs for very large datasets)

## Next Checks

1. Test FDSL pre-training on non-ViT architectures (ResNet, ConvNeXt) to verify cross-architecture transfer effectiveness
2. Evaluate performance on downstream tasks requiring texture understanding (e.g., material classification) where contours alone may be insufficient
3. Conduct ablation on the exact relationship between contour complexity and feature learning capacity to determine optimal parameter ranges systematically