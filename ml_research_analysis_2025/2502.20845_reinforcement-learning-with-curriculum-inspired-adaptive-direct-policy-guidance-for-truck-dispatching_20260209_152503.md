---
ver: rpa2
title: Reinforcement Learning with Curriculum-inspired Adaptive Direct Policy Guidance
  for Truck Dispatching
arxiv_id: '2502.20845'
source_url: https://arxiv.org/abs/2502.20845
tags:
- policy
- learning
- dispatching
- time
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a curriculum learning method for policy-based
  reinforcement learning in truck dispatching for open-pit mining. The method uses
  adaptive direct policy guidance with a shortest processing time teacher policy to
  improve convergence speed and robustness to reward function design.
---

# Reinforcement Learning with Curriculum-inspired Adaptive Direct Policy Guidance for Truck Dispatching

## Quick Facts
- arXiv ID: 2502.20845
- Source URL: https://arxiv.org/abs/2502.20845
- Authors: Shi Meng; Bin Tian; Xiaotong Zhang
- Reference count: 22
- Key outcome: RL method with adaptive curriculum learning and teacher guidance achieves 10% performance gain over PPO across sparse/dense rewards in truck dispatching

## Executive Summary
This paper addresses the challenge of truck dispatching in open-pit mining using reinforcement learning. The authors propose a curriculum learning method that combines time-delta modified temporal difference learning with adaptive direct policy guidance from a shortest processing time (SPT) teacher. The method aims to improve convergence speed and robustness to reward function design by injecting domain knowledge through teacher guidance while automatically adjusting the strength of this guidance based on the agent's progress.

## Method Summary
The method modifies Proximal Policy Optimization (PPO) for mine dispatching's uneven decision intervals by incorporating time deltas into Temporal Difference and Generalized Advantage Estimation calculations. A Shortest Processing Time heuristic provides action suggestions that are incorporated through a regularization loss term, with an adaptive guidance coefficient that decays as the policy's alignment with the teacher increases. The approach is evaluated in the OpenMines simulation environment using both sparse and dense reward settings, showing improved convergence and robustness compared to standard PPO.

## Key Results
- Achieves 10% performance gain over standard PPO across sparse and dense reward settings
- Demonstrates faster convergence with adaptive guidance mechanism
- Shows improved robustness to reward function design choices
- Maintains effectiveness across both sparse and dense reward formulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-delta modified temporal difference learning enables stable value estimation under uneven decision intervals.
- Mechanism: Raises discount factor γ to power of Δt and modifies λ in GAE to correspond to actual elapsed time rather than arbitrary step counts.
- Core assumption: Uneven intervals distort standard TD learning by over/under-discounting rewards relative to real time progression.
- Evidence anchors: [abstract] "time deltas in Temporal Difference and Generalized Advantage Estimation"; [section III-B-1] Equations 1-3 show γ^(Δt) and λ^(Δt) modifications.

### Mechanism 2
- Claim: Teacher policy regularization accelerates early-stage exploration by injecting domain knowledge.
- Mechanism: SPT heuristic provides suggested actions a_sug, with log-probability of taking these actions added as GuideLoss to encourage alignment during early training.
- Core assumption: Simple heuristic provides meaningful directional guidance before policy discovers effective behaviors through random exploration.
- Evidence anchors: [abstract] "Shortest Processing Time teacher policy for guided exploration via policy regularization"; [section III-B-2] Equation 4 defines GuideLoss.

### Mechanism 3
- Claim: Adaptive guidance coefficient enables automatic curriculum progression by measuring policy-teacher alignment.
- Mechanism: Computes c_teacher as mean exp(log π(a_sug|s)) measuring policy-teacher agreement, with guide_coef proportional to (1 - c_teacher), decaying guidance as alignment increases.
- Core assumption: Policy-teacher agreement correlates with having absorbed useful behaviors; exceeding baseline indicates readiness for independent exploration.
- Evidence anchors: [abstract] "adaptive guidance" mentioned as key contribution; [section III-B-3] Equations 5-6 define adaptive guide_coef.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) fundamentals**
  - Why needed here: Method modifies PPO's TD and GAE calculations; understanding baseline required to reason about time-delta modifications
  - Quick check question: Can you explain why PPO clips policy updates and how GAE balances bias-variance in advantage estimation?

- Concept: **Curriculum learning and teacher-student frameworks**
  - Why needed here: Core contribution frames guidance as curriculum; knowing how curriculum schedules typically decay helps evaluate adaptive coefficient design
  - Quick check question: What is the risk of a curriculum that removes teacher supervision too slowly versus too quickly?

- Concept: **Open-pit mine dispatching constraints**
  - Why needed here: Domain has uneven decision timing, heterogeneous equipment, and stochastic events; these motivate all three mechanisms
  - Quick check question: Why would a fixed optimization schedule fail in an environment with random shovel maintenance and road closures?

## Architecture Onboarding

- Component map:
  State encoder -> Policy network (outputs action distribution) -> Teacher policy module (SPT heuristic) -> Adaptive coefficient controller -> Value network (Δt-modified TD/GAE) -> Environment

- Critical path:
  1. Environment emits state when truck requests dispatch
  2. Policy network samples action; teacher provides a_sug
  3. Compute standard PPO loss + GuideLoss × guide_coef
  4. Update value network using Δt-modified TD/GAE
  5. After each episode, compare production to baseline; update guide_coef

- Design tradeoffs:
  - Dense vs. sparse rewards: Dense speeds convergence but requires engineering; sparse is cleaner but needs guidance to avoid cold-start failure
  - Teacher strength (α in Equation 6): Higher α accelerates early learning but risks overfitting to SPT heuristic
  - Baseline threshold: Controls when guidance stops; domain-specific tuning required

- Failure signatures:
  - Training stagnates at SPT performance level → guidance not decaying; check c_teacher computation and baseline setting
  - Sparse reward runs show no progress for extended periods → teacher guidance may be too weak or disabled prematurely
  - Value loss diverges → verify time-delta scaling in TD/GAE; very large Δt may cause numerical instability

- First 3 experiments:
  1. Ablate time-delta modification: Run standard TD/GAE vs. Δt-modified version on same environment; compare convergence stability and final production
  2. Vary guidance coefficient schedule: Test fixed guide_coef values (0.0, 0.5, 1.0) vs. adaptive; measure convergence speed and robustness across sparse/dense rewards
  3. Teacher policy comparison: Replace SPT with alternative heuristics (NearestDispatcher, SQDispatcher) to assess sensitivity to teacher quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can neural network architectures based on fine-tuned end-side large language models (LLMs) be integrated to enable agents to follow instructions described in natural language?
- Basis in paper: [explicit] The conclusion explicitly identifies future work involving "neural network architectures based on fine-tuning end-side large language models" to introduce instruction-following capabilities.
- Why unresolved: Current implementations use standard policy networks without natural language understanding modules.
- What evidence would resolve it: Successful deployment of an LLM-based policy that processes textual commands and maintains production efficiency.

### Open Question 2
- Question: How can the performance degradation of RL dispatchers in small-scale fleets be mitigated?
- Basis in paper: [explicit] The authors observe that "in smaller fleet sizes, the performance of reinforcement learning algorithms is not as good as traditional rule-based methods," hypothesizing that sparse random events cause generalization issues.
- Why unresolved: The paper identifies the limitation but does not propose or test solutions for small fleet scenarios.
- What evidence would resolve it: A modified training curriculum or architecture that matches or exceeds heuristic performance in small fleet simulations.

### Open Question 3
- Question: Is the adaptive direct policy guidance method robust to suboptimal teacher policies?
- Basis in paper: [inferred] The method relies on a specific "Shortest Processing Time" teacher. While the method is claimed to be general, the sensitivity of the final policy to the teacher's optimality is not tested.
- Why unresolved: If the teacher policy is flawed, the adaptive guidance coefficient might reinforce suboptimal behaviors before the agent surpasses the baseline.
- What evidence would resolve it: Ablation studies using intentionally noisy or suboptimal teacher policies to measure convergence degradation.

## Limitations
- Performance degradation in small-scale fleets where RL underperforms traditional heuristics due to sparse random events
- Unknown sensitivity to teacher policy quality - success may depend on having a reasonably good heuristic
- Method effectiveness relies on specific environmental dynamics (uneven decision intervals) that may not generalize to all RL applications

## Confidence

- **High confidence**: 10% performance improvement over standard PPO and faster convergence under both sparse and dense reward settings are well-supported by experimental results
- **Medium confidence**: Time-delta modifications stabilizing value estimation is plausible but relies on specific environmental dynamics
- **Medium confidence**: Adaptive guidance coefficient effectively balances exploration and exploitation, though correlation between production baseline and policy readiness is assumed rather than proven

## Next Checks

1. **Ablate time-delta modification**: Run standard TD/GAE vs. Δt-modified version on the same environment; compare convergence stability and final production to isolate the impact of temporal adjustments

2. **Teacher policy sensitivity**: Replace SPT with alternative heuristics (NearestDispatcher, SQDispatcher) to assess whether the method's success depends on specific teacher quality or is robust to heuristic choice

3. **Baseline threshold tuning**: Systematically vary the production baseline that triggers guidance termination; evaluate the tradeoff between premature independence and over-reliance on the teacher