---
ver: rpa2
title: Deep Reinforcement Learning Agents are not even close to Human Intelligence
arxiv_id: '2505.21731'
source_url: https://arxiv.org/abs/2505.21731
tags:
- agents
- learning
- game
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep RL agents achieve superhuman performance in Atari games but
  fail to generalize to simpler task variations, revealing reliance on shortcuts rather
  than robust reasoning. We introduce HackAtari, a suite of over 200 environment modifications
  that simplify or minimally alter Atari tasks, allowing systematic evaluation of
  generalization.
---

# Deep Reinforcement Learning Agents are not even close to Human Intelligence

## Quick Facts
- arXiv ID: 2505.21731
- Source URL: https://arxiv.org/abs/2505.21731
- Reference count: 40
- Key outcome: Deep RL agents achieve superhuman performance in Atari games but fail to generalize to simpler task variations, revealing reliance on shortcuts rather than robust reasoning.

## Executive Summary
Deep RL agents achieve superhuman performance in Atari games but fail to generalize to simpler task variations, revealing reliance on shortcuts rather than robust reasoning. We introduce HackAtari, a suite of over 200 environment modifications that simplify or minimally alter Atari tasks, allowing systematic evaluation of generalization. Experiments across 17 games show that deep RL agents (DQN, PPO, C51, i-DQN, IMPALA) consistently experience severe performance drops on these simplifications, unlike humans who easily adapt. Object-centric agents improve slightly but still struggle. Our results highlight that current RL agents lack human-like intelligence, as they do not learn to select actions for the right reasons. HackAtari enables rigorous testing of generalization and alignment, calling for new benchmarks and methodologies to develop agents with human-like reasoning capabilities.

## Method Summary
The study evaluates RL agents' zero-shot generalization to simplified variants of Atari games using HackAtari modifications (RAM-based environment perturbations). Agents are trained for 200M frames on original ALE games with standard hyperparameters (frameskip=4, repeat action probability=0.25), then evaluated zero-shot on HackAtari variations. Performance is measured using Expert-Human Normalized Score (E-HNS) with Interquartile Mean (IQM) aggregation and 95% stratified bootstrap confidence intervals. The framework includes 17 games with 30 total variations, testing both visual perturbations (color changes, visibility) and gameplay simplifications (static enemies, removed hazards, altered mechanics).

## Key Results
- Deep RL agents (DQN, PPO, C51, i-DQN, IMPALA) experience severe performance drops on simplified Atari variations, with PPO showing only 13% average performance drop versus 80-95% for others
- Object-centric agents (SCoBots, OC-NN) improve slightly on visual perturbations but still struggle with gameplay alterations, indicating structural biases help but don't solve fundamental misalignment
- Humans easily adapt to simplified variations, confirming these modifications are genuinely easier while exposing agent reliance on spurious correlations rather than causal understanding

## Why This Works (Mechanism)

### Mechanism 1
Deep RL agents achieve high performance by exploiting spurious correlations (shortcuts) rather than learning causal task structures. Agents optimize for reward signals in fixed distributions. If a proxy variable (e.g., enemy paddle position in Pong) correlates perfectly with the optimal action during training, the agent converges on this low-complexity solution. It fails when the proxy is decoupled from the causal mechanism (e.g., the ball trajectory) in modified environments. Evidence: "Delfosse et al. (2024b) have thus exposed that deep and symbolic RL agents learn to rely on the enemy's position... Hiding the enemy... leads to a performance drop."

### Mechanism 2
Direct RAM manipulation creates targeted diagnostic variations to isolate specific agent dependencies. The framework (HackAtari) bypasses proprietary source code by modifying memory addresses during emulation. By freezing specific game objects (e.g., "StoppedCars") or altering colors, the system forces the agent to rely on specific features, exposing whether it understands the game rules or just reacts to pixel patterns. Evidence: "modifications are implemented through direct alterations of the game's RAM... allowing diverse controlled perturbations."

### Mechanism 3
Object-centric inductive biases improve robustness to visual perturbations but are insufficient for game-logic misalignment. Object-centric agents (e.g., SCoBots, OC-NN) extract structured representations (masks, vectors) from pixels. This preprocessing layer filters noise, making the policy invariant to visual changes (e.g., color). However, the downstream policy learner still treats the object states as a fixed distribution and can learn spurious object-object correlations rather than causal rules. Evidence: "As expected, these agents are insensitive to visual perturbations... however, even on most tasks with gameplay alterations, all object-centric agents exhibit high performance drops."

## Foundational Learning

- **Concept: Shortcut Learning**
  - Why needed here: The paper redefines "superhuman" performance as potentially hollow, driven by dataset-specific shortcuts rather than generalizable intelligence. Understanding this is critical to interpreting the failure modes.
  - Quick check question: If an agent solves a maze by reading the exit sign rather than mapping walls, will it transfer to a maze without signs?

- **Concept: Zero-Shot Generalization**
  - Why needed here: The evaluation protocol tests zero-shot adaptation—applying a fixed policy to new environment dynamics without weight updates.
  - Quick check question: Does the agent need to retrain on the "LazyEnemy" variant to succeed, or should its original training suffice?

- **Concept: Inductive Biases (Object-Centricity)**
  - Why needed here: The paper compares "Nature-CNN" (pixel-biased) against "Object-Centric" (entity-biased) architectures to test if structuring the input space creates more human-like reasoning.
  - Quick check question: Does forcing an agent to see "a paddle" and "a ball" rather than "pixels 0-255" guarantee it understands the paddle *hits* the ball?

## Architecture Onboarding

- **Component map:** Arcade Learning Environment (ALE) wrapped with HackAtari (RAM-modifier) -> Nature-CNN or Object-centric wrappers (Binary Masks, Planes, Symbolic Vectors) -> PPO/C51/DQN policy
- **Critical path:** 1. Select standard agents and object-centric variants. 2. Train exclusively on Original ALE environments (200M frames). 3. Evaluate zero-shot on HackAtari variations. 4. Compute Performance Drop: (Score_Mod - Score_Orig) / Score_Orig.
- **Design tradeoffs:** RAM Modification vs. Code Modification: RAM hacking allows modification of proprietary games without source access but requires reverse-engineering memory addresses. Simplification vs. Complexification: Testing on simplifications distinguishes "can't adapt" from "task is too hard," unlike benchmarks that increase difficulty.
- **Failure signatures:** Visual Overfitting: High performance on Original, zero score on "Color Change" variants. Logic Shortcut: High performance on Original, zero score on "StoppedCars" (where the agent should easily win but fails because it relied on car movement timing).
- **First 3 experiments:** 1. Pong Sanity Check: Train DQN on Pong. Test on `lazy_enemy`. If score drops > 50%, confirm shortcut reliance on enemy position. 2. Object-Centric Robustness Test: Train SCoBots on `Breakout`. Test on `color_all_blocks_red`. Compare performance drop against standard PPO. 3. Human Baseline: Run user study (n=10) on `Freeway` (Original vs. `stop_all_cars`). Confirm human performance increases while agent performance stays flat or drops.

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms within the IMPALA architecture (e.g., V-trace correction, distributed data collection) enable it to maintain superhuman performance under task variations better than other algorithms? The paper identifies IMPALA's superior robustness empirically but does not ablate the specific components responsible for this generalization. Evidence: Section 7 states, "Understanding how these elements support generalization may guide the development of more task-aligned deep RL agents."

### Open Question 2
Can combining object-centric representations with advanced human-like inductive biases—such as causal world models or hierarchical sub-policies—enable robust generalization to gameplay alterations? Current object-centric agents still fail on logic modifications, and the paper calls for integrating these further human biases. Evidence: Sections 6 and 7 suggest that while object-centricity helps, it is insufficient alone; the authors list causal world models and hierarchical RL as necessary, untested next steps.

### Open Question 3
Does the systematic failure of agents on task simplifications demonstrate that "reward is enough" only when combined with specific structural biases? The paper challenges the hypothesis but leaves open the exact formulation of necessary biases versus reward signals. Evidence: Section 8 concludes, "Silver et al. (2021) have hypothesized that reward is enough, but our results suggest that without the right biases and evaluations, reward alone is not enough to build agents that truly understand."

## Limitations
- Evaluation scope limited to 17 games and 30 variations, potentially missing broader generalization patterns
- RAM modification implementation details could influence agent behavior in ways not fully characterized
- Object-centric agents show only marginal improvements, suggesting structural biases help with visual changes but don't address fundamental policy misalignment

## Confidence

- **High Confidence**: Agent performance drops on simplified variations are real and significant across multiple architectures and games
- **Medium Confidence**: The characterization of these drops as "shortcut learning" rather than "incomplete learning" - while strongly supported, alternative explanations (e.g., insufficient training diversity) cannot be fully excluded
- **Medium Confidence**: Object-centric agents' slight improvements indicate partial robustness, but the mechanism by which they fail on gameplay alterations remains incompletely characterized

## Next Checks

1. **Control for Training Diversity**: Re-run key experiments (DQN on Pong, PPO on Breakout) with curriculum-based exposure to variations during training to distinguish shortcut learning from insufficient exploration
2. **Cross-Platform Generalization**: Apply HackAtari methodology to non-Atari environments (e.g., Procgen, DM Control) to test whether shortcut learning is platform-specific or a fundamental RL limitation
3. **Human-Aligned Reward Design**: Test agents with reward shaping that penalizes reliance on specific visual features (e.g., enemy position) to determine if alignment can be engineered rather than requiring architectural changes