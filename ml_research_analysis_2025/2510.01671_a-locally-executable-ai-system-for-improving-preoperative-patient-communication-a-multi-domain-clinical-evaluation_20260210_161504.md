---
ver: rpa2
title: 'A Locally Executable AI System for Improving Preoperative Patient Communication:
  A Multi-Domain Clinical Evaluation'
arxiv_id: '2510.01671'
source_url: https://arxiv.org/abs/2510.01671
tags:
- clinical
- extraction
- medical
- casual
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LENOHA uses a sentence-transformer classifier to route pre-procedural\
  \ questions to a clinician-curated FAQ for verbatim, non-generative answers, while\
  \ routing casual conversation to a local small language model. Evaluated in tooth\
  \ extraction and gastroscopy domains, E5-large-instruct (560M parameters) achieved\
  \ 0.983 accuracy (95% CI 0.964\u20130.991) and AUC 0.996 on held-out test sets,\
  \ with performance statistically indistinguishable from GPT-4o and better than other\
  \ tested models."
---

# A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation

## Quick Facts
- arXiv ID: 2510.01671
- Source URL: https://arxiv.org/abs/2510.01671
- Reference count: 0
- One-line primary result: LENOHA achieves 0.983 accuracy and 0.996 AUC on routing clinical vs casual queries using a 560M-parameter sentence transformer, with near-GPT-4o performance and safe verbatim retrieval.

## Executive Summary
LENOHA is a locally executable AI system designed to improve preoperative patient communication by routing queries to either a clinician-curated FAQ for verbatim answers or a local small language model for casual conversation. Evaluated in tooth extraction and gastroscopy domains, the system demonstrates high accuracy (0.983) and low energy consumption (~1.0 mWh per clinical query) while eliminating hallucination risk in medical responses. The architecture achieves near-frontier performance without requiring cloud-based inference, making it suitable for resource-constrained environments.

## Method Summary
LENOHA uses a sentence-transformer classifier to route pre-procedural questions to a clinician-curated FAQ for verbatim, non-generative answers, while routing casual conversation to a local small language model. The E5-large-instruct (560M parameters) model achieved 0.983 accuracy (95% CI 0.964–0.991) and AUC 0.996 on held-out test sets. The system was evaluated on synthetic validation and test sets (200 utterances/domain for validation, 200 for testing) in tooth extraction and gastroscopy domains, with thresholds fixed via Youden's J on ROC curves.

## Key Results
- E5-large-instruct (560M parameters) achieved 0.983 accuracy (95% CI 0.964–0.991) and AUC 0.996 on held-out test sets
- Performance was statistically indistinguishable from GPT-4o on classification tasks
- Non-generative clinical path consumed ~1.0 mWh per query versus ~168 mWh for small-talk replies
- System maintained ~0.10 s latency on a single on-prem GPU (RTX 3080)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a system structurally decouples clinical queries from generative models, it eliminates the causal pathway for hallucination in high-stakes responses.
- **Mechanism:** A sentence-transformer classifier acts as a switch. If the input is "Clinical," the system retrieves a verbatim answer from a static, clinician-curated FAQ. If "Casual," it routes to a local SLM. By removing the generative loop for medical data, the probability of hallucinated medical facts drops to near zero (conditional on FAQ accuracy).
- **Core assumption:** The classifier can reliably distinguish clinical intent from casual chatter, and the FAQ coverage is sufficient for the domain.
- **Evidence anchors:** [abstract], [section 2.2], [corpus]
- **Break condition:** If the classifier produces a False Negative (labeling a clinical query as casual), the query hits the generative model, reintroducing hallucination risk.

### Mechanism 2
- **Claim:** A compact 560M-parameter encoder can match frontier-model discrimination performance on specialized clinical intent tasks while running on consumer hardware.
- **Mechanism:** The E5-large-instruct model maps input to embeddings. The system compares these against domain-specific thresholds (optimized via Youden's J) rather than relying on the generalized reasoning of massive LLMs. This reduces the task to a geometric similarity problem rather than a generative one.
- **Core assumption:** The semantic boundary between "clinical" and "casual" is stable and learnable via vector similarity.
- **Evidence anchors:** [abstract], [section 4.3], [corpus]
- **Break condition:** If domain vocabulary shifts (e.g., new slang or novel symptoms), the static embedding space may fail to separate classes, requiring fine-tuning or threshold recalibration.

### Mechanism 3
- **Claim:** Routing queries to a non-generative path significantly reduces per-query energy consumption (claimed ~170x reduction).
- **Mechanism:** The clinical path performs a single forward pass for embedding and a vector lookup (cheap operations). The casual path initiates a full generative inference cycle on an 8B model (expensive operation).
- **Core assumption:** The proportion of clinical-to-casual queries remains stable; if users engage in lengthy casual conversations, the energy savings dilute.
- **Evidence anchors:** [abstract], [section 4.5], [corpus]
- **Break condition:** If batch processing or concurrent users increase peak load, the memory bandwidth may become the bottleneck rather than the computation itself, changing the energy profile.

## Foundational Learning

**Concept: Sentence Embeddings (Vector Space Models)**
- **Why needed here:** You must understand how text is converted into vectors to debug why a user input matched a specific FAQ entry or why it was routed to the wrong path.
- **Quick check question:** Can you explain why "Can I drink water?" and "Is hydration allowed?" might have high cosine similarity despite different words?

**Concept: Hallucination in LLMs vs. Deterministic Retrieval**
- **Why needed here:** The core value proposition is safety. You need to understand *why* generative models hallucinate (probability sampling) and *why* retrieval does not (lookup).
- **Quick check question:** If the FAQ contains a typo, will the verbatim retrieval fix it or repeat it?

**Concept: Precision-Recall Trade-off**
- **Why needed here:** The "operating threshold" determines how aggressive the router is. You need to know the cost of False Positives (Casual treated as Clinical) vs. False Negatives (Clinical treated as Casual).
- **Quick check question:** In this safety-critical context, is it better to have a False Negative (sending a clinical question to the casual bot) or a False Positive (answering a casual question with a clinical FAQ)?

## Architecture Onboarding

**Component map:** Input -> Router (E5-large-instruct) -> Threshold Logic -> Path A (Vector Search -> FAQ Database -> Verbatim Output) or Path B (Local 8B SLM -> Generative Output)

**Critical path:** The threshold setting (Section 3.7). If this is wrong, the routing fails. You must understand how ROC curves and Youden's Index were used to freeze these boundaries.

**Design tradeoffs:**
- **Safety vs. UX:** Strictly verbatim answers are safe but can feel robotic.
- **Cost vs. Capability:** A 560M router is cheap but may fail on edge cases that GPT-4o would catch (though the paper claims statistical parity).
- **Assumption:** The paper claims statistical indistinguishability from GPT-4o on *this specific test set*; do not assume this generalizes to all medical domains without testing.

**Failure signatures:**
- **The "Small Talk" Contamination:** Clinical questions misclassified as casual (False Negative) generate empathetic but potentially misleading non-medical replies.
- **The "Rigid FAQ":** Valid clinical questions that are not in the database may be forced into a "best match" that is technically wrong (semantic mismatch).

**First 3 experiments:**
1. **Threshold Sensitivity Analysis:** Re-run the validation set with slightly adjusted thresholds to observe the shift in False Positives/Negatives. Confirm the "safe" operating range.
2. **Latency Baseline:** Measure end-to-end latency for the clinical path (embedding + retrieval) vs. the casual path (generation) on the target hardware to verify the <0.5s constraint.
3. **Adversarial Routing:** Input edge-case queries (e.g., "I'm scared about the anesthesia" - technically clinical/anxiety but phrasing is emotive) to see which path triggers.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** How does the classifier perform on authentic patient utterances compared to the synthetic LLM-generated test sets?
- **Basis in paper:** [explicit] The authors identify the "distribution shift to real-world phrasing" as a key limitation, noting that validation and test sets were drafted by LLMs rather than sourced from real patient logs.
- **Why unresolved:** Synthetic queries may lack the linguistic variability, colloquialisms, or error patterns found in actual patient speech, potentially inflating performance metrics (0.983 accuracy) and threatening external validity.
- **Evidence:** Prospective evaluation using transcripts of real patient-clinician interactions to measure performance drift and accuracy.

**Open Question 2**
- **Question:** What is the optimal text or voice interface for clinical environments using this architecture?
- **Basis in paper:** [explicit] The authors explicitly ask, "What is the optimal text/voice (or hybrid) interface for clinical environments?" in the limitations section.
- **Why unresolved:** The current system is text-only to ensure fidelity and privacy, but this limits accessibility (e.g., for visually impaired users or those with low literacy); adding voice introduces risks from ASR errors and accents.
- **Evidence:** A comparative usability study measuring latency, error rates (ASR vs. intent classification), and patient satisfaction across text, voice, and hybrid modes.

**Open Question 3**
- **Question:** Can LENOHA effectively reduce clinician workload and maintain safety in real-world, low-resource deployment settings?
- **Basis in paper:** [explicit] The paper concludes that "further prospective evaluation is needed" specifically in "a real-world, low-resource setting" to validate the system's utility beyond technical metrics.
- **Why unresolved:** The study focuses on offline performance (AUC, energy consumption) on curated test sets, which does not guarantee operational efficiency or safety when integrated into live clinical workflows.
- **Evidence:** A field deployment study in a remote or bandwidth-limited clinic tracking metrics such as clinician time saved, query resolution rates, and user satisfaction.

**Open Question 4**
- **Question:** How sensitive is the system's safety profile to the fixed operating thresholds and lack of probability calibration?
- **Basis in paper:** [explicit] The authors note that "probability calibration and threshold-sensitivity analyses were not evaluated and remain to be performed."
- **Why unresolved:** Thresholds were fixed via Youden's Index on a validation set; it is unknown if these thresholds remain optimal or safe under varying query prevalences or input styles (e.g., shorter or ambiguous inputs).
- **Evidence:** Experiments analyzing calibration reliability and threshold sensitivity across different domains to quantify false-negative risks.

## Limitations
- **Knowledge Base Dependence:** Safety guarantees rest entirely on FAQ comprehensiveness and accuracy, which is not fully validated beyond the test set.
- **Domain Specificity:** Performance validated only on tooth extraction and gastroscopy domains; generalizability to other medical specialties is unproven.
- **Classifier Robustness:** Threshold calibration assumes representative validation data; system may fail under adversarial or out-of-distribution inputs.

## Confidence

**High Confidence:** The architectural design (retrieval vs. generation separation) is clearly specified and logically sound for reducing hallucination risk. The energy consumption measurements are directly reported from hardware logs.

**Medium Confidence:** The classification performance (0.983 accuracy, AUC 0.996) is statistically validated on the test set, but the reliance on synthetic validation data and the lack of real-world deployment data introduce uncertainty about generalizability.

**Low Confidence:** The claim of "near-frontier accuracy" is based on a single comparison with GPT-4o on a specific test set. Without broader benchmarking across diverse medical domains and query types, this claim is overstated.

## Next Checks
1. **Real-World Query Testing:** Deploy the system with a live FAQ and capture actual patient queries (with consent) to test routing accuracy and answer relevance. Compare the similarity score distributions from real queries against the validation set to detect potential drift.

2. **Adversarial Query Benchmark:** Construct a test suite of edge-case and intentionally misleading queries (e.g., "Can I eat glass before surgery?") to evaluate the classifier's robustness and the system's ability to fail safely (e.g., by routing to a human or providing a generic safety message).

3. **FAQ Coverage Gap Analysis:** For each clinical query that scores below the routing threshold, categorize the type of question (e.g., procedure-specific, medication-related, psychological) to identify systematic gaps in FAQ coverage. This will inform whether the knowledge base needs expansion or if the classifier thresholds need adjustment.