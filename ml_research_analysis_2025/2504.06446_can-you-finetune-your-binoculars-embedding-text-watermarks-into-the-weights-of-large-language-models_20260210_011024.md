---
ver: rpa2
title: Can you Finetune your Binoculars? Embedding Text Watermarks into the Weights
  of Large Language Models
arxiv_id: '2504.06446'
source_url: https://arxiv.org/abs/2504.06446
tags:
- text
- watermark
- watermarking
- arxiv
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes embedding watermarks into the weights of large
  language models (LLMs) by fine-tuning a pair of low-rank adapters using the binoculars
  score for detectability. One adapter generates watermarked text, while the other
  detects it.
---

# Can you Finetune your Binoculars? Embedding Text Watermarks into the Weights of Large Language Models

## Quick Facts
- arXiv ID: 2504.06446
- Source URL: https://arxiv.org/abs/2504.06446
- Reference count: 7
- One-line primary result: Fine-tuning LoRA adapters with exponential barriers embeds watermarks into LLM weights, achieving ROC-AUC ~0.968 while improving reasoning task performance.

## Executive Summary
This paper proposes embedding watermarks into the weights of large language models (LLMs) by fine-tuning a pair of low-rank adapters using the binoculars score for detectability. One adapter generates watermarked text, while the other detects it. The approach uses a regularized min-max optimization framework to balance watermark robustness, naturalness, and task performance during instruction tuning. Results show that fine-tuning with exponential barriers (λ=1e-5, 1e-2) improves watermark detection (ROC-AUC ~0.968) while maintaining text fluency and achieving gains in reasoning tasks like GSM8K (+8%) and MMLU (+4.7%). The watermark-embedded models outperform baselines in distinguishing generated from human text, demonstrating a scalable approach for model-level watermarking without sacrificing performance.

## Method Summary
The method trains two LoRA-adapted models from a shared base (LLaMA 3.1 8B) to embed watermarks into LLM weights. The performer adapter (r=16, α=32) generates watermarked text, while the observer adapter (r=32, α=128) detects it using the binoculars score. The optimization jointly maximizes watermark detectability while minimizing task loss, using exponential barriers to prevent over-optimization. Training uses UltraChat 200k instruction pairs, with evaluation on detection metrics (ROC-AUC, PR-AUC) and task performance (accuracy, F1) across reasoning benchmarks.

## Key Results
- Watermark detection ROC-AUC reaches 0.968 using exponential barrier with λ=1e-5 or 1e-2
- Task performance remains strong: accuracy >0.91 across benchmarks while embedding watermarks
- GSM8K reasoning accuracy improves by 8%, MMLU by 4.7% compared to baseline
- Exponential barriers prevent over-optimization that degrades fluency at higher λ values

## Why This Works (Mechanism)

### Mechanism 1: Binoculars Score as Learnable Detection Signal
- Claim: The binocular score, which measures perplexity discrepancy between two models, can be repurposed from a detection metric into a training objective that shapes model behavior.
- Mechanism: The score B(s) = log PPL(s) / log XPPL(s) compares how an "observer" model predicts tokens versus how a "performer" model's distribution aligns with those predictions. By maximizing this score on generated text while minimizing it on human text, the performer learns to emit detectable statistical signatures without explicit token-level rules.
- Core assumption: The statistical separation learned during training generalizes to unseen text and persists across diverse generation contexts.

### Mechanism 2: Dual-Adapter Min-Max Optimization
- Claim: Jointly training performer and observer adapters creates a stable adversarial dynamic where each model improves the other's objective.
- Mechanism: The performer minimizes task loss while maximizing binocular detectability; the observer refines detection boundaries. The formulation L_MP(s_real) - λ(B(s_real) + B(s_gen)) creates pressure for the performer to make generated text statistically distinguishable from human text according to the observer's lens.
- Core assumption: The shared base model initialization provides sufficient common ground for the observer to meaningfully evaluate the performer's outputs.

### Mechanism 3: Barrier Regularization Prevents Over-Optimization
- Claim: Exponential barrier functions stabilize training by preventing excessive watermark strength that would harm fluency.
- Mechanism: The exponential barrier L_barrier = exp(L_task - L_0) applies steep penalties when task loss exceeds a threshold L_0, dampening updates that would sacrifice text quality for detectability. This transforms an unbounded maximization into a constrained optimization problem.
- Core assumption: The constraint threshold L_0 can be set to preserve acceptable fluency while still allowing meaningful watermark signal.

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: The entire architecture depends on efficiently fine-tuning adapters rather than full model weights. Understanding rank (r) and alpha (α) parameters is essential for configuring performer vs. observer capacity.
  - Quick check question: Why would you give the observer model a larger LoRA rank (r=32) than the performer (r=16)?

- Concept: **Perplexity and Cross-Perplexity**
  - Why needed here: The binocular score fundamentally measures how two models' probability distributions relate. Without understanding perplexity as "how surprised a model is by text," the objective function is opaque.
  - Quick check question: If PPL measures observer surprise at tokens, what does XPPL (cross-perplexity) measure about the performer-observer relationship?

- Concept: **Barrier Methods in Constrained Optimization**
  - Why needed here: The paper's key innovation is using exponential/quadratic barriers to balance competing objectives. Understanding how barriers penalize constraint violation explains why λ = 1e-2 works but λ = 0.1 fails.
  - Quick check question: Why would an exponential barrier (steep penalty near violation) outperform a quadratic barrier for this application?

## Architecture Onboarding

- Component map: Base Model (LLaMA 3.1 8B) -> Performer Adapter (LoRA r=16, α=32) -> generates watermarked text; Base Model -> Observer Adapter (LoRA r=32, α=128) -> computes binocular scores

- Critical path: Initialize both adapters from base → sample batch → generate s_gen from performer → compute binocular scores on both s_real and s_gen → apply barrier if L_task approaches threshold → backprop through both adapters

- Design tradeoffs:
  - Larger performer LoRA = stronger watermark but fluency risk
  - Larger observer LoRA = better detection but slower training
  - Higher λ = more detectable watermark but potential fluency collapse
  - Exponential vs. quadratic barrier: exponential provides sharper control but less forgiving of misconfiguration

- Failure signatures:
  - Oscillating training: Min-max instability; reduce λ or check barrier threshold
  - High perplexity on outputs: Watermark too aggressive; decrease λ or tighten barrier
  - ROC-AUC < 0.92: Watermark too weak; increase λ or expand observer LoRA rank
  - Accuracy drops on reasoning tasks: Over-regularization; the paper shows GSM8K improved, so degradation signals misconfiguration

- First 3 experiments:
  1. Replicate the λ = 1e-5 exponential barrier configuration on a small subset of UltraChat to verify ROC-AUC reaches ~0.96 before full training
  2. Ablate the observer LoRA size (r=32 vs r=16) to confirm the paper's claim that larger observer capacity improves detection without harming fluency
  3. Test paraphrase robustness: generate watermarked text, apply a simple paraphraser, and measure if binocular scores remain detectable—the paper explicitly flags this as untested future work

## Open Questions the Paper Calls Out

- **Question**: How well does the watermarking framework generalize across different LLM architectures beyond LLaMA 3.1 8B?
  - Basis in paper: The conclusion states: "Future work should explore how this framework generalizes across different LLM architectures and its resistance to text transformations such as paraphrasing and adversarial perturbations."
  - Why unresolved: All experiments use a single architecture (LLaMA 3.1 8B), leaving transferability untested.
  - What evidence would resolve it: Apply the same LoRA-based binocular watermarking to diverse architectures (e.g., Mistral, Gemma, GPT-style models) and compare ROC-AUC and fluency metrics.

- **Question**: How robust are the embedded watermarks against adversarial attacks such as paraphrasing, substitution, or character-level perturbations?
  - Basis in paper: The conclusion explicitly calls for testing "resistance to text transformations such as paraphrasing and adversarial perturbations."
  - Why unresolved: The paper evaluates clean detection performance but does not subject watermarked outputs to attack simulations.
  - What evidence would resolve it: Measure ROC-AUC degradation after applying paraphrasing models, synonym swaps, or character noise to watermarked text.

- **Question**: Can watermarks be detected by third parties without access to the jointly-trained observer model?
  - Basis in paper: Detection relies on the co-trained observer, but real-world accountability may require third-party verification without proprietary model access.
  - Why unresolved: The paper assumes both performer and observer are available for detection; external detectability is unexplored.
  - What evidence would resolve it: Test detection using off-the-shelf LLMs or alternative detection metrics instead of the paired observer.

- **Question**: Does the watermark persist after subsequent fine-tuning on downstream tasks?
  - Basis in paper: The method embeds watermarks during instruction tuning, but related work (Gu et al. 2023) found that "fine-tuning on normal text weakens watermarking"; the paper does not test durability under continued training.
  - Why unresolved: Models in practice undergo additional fine-tuning, which could overwrite learned watermarking patterns.
  - What evidence would resolve it: Continue fine-tuning the watermarked performer on new datasets and measure whether ROC-AUC and binocular separation degrade.

## Limitations

- **Training Configuration Gaps**: Critical hyperparameters including learning rate, batch size, weight decay, training epochs/steps, gradient accumulation settings, and the barrier threshold L_0 are omitted, preventing exact replication.
- **Generation Process Ambiguity**: The paper does not specify how s_gen is produced for training—temperature, top-p, max tokens, or decoding strategy—which could substantially affect both training dynamics and downstream detection performance.
- **Evaluation Scope Constraints**: Results demonstrate effectiveness on classification-style detection but do not validate watermark persistence under paraphrasing, style transfer, or other natural transformations, leaving a critical real-world vulnerability untested.

## Confidence

- **High Confidence**: The core technical contribution—embedding watermarks via weight-level fine-tuning using a dual-adapter min-max framework—is well-specified and the reported detection metrics (ROC-AUC ~0.968) are internally consistent with the described methodology.
- **Medium Confidence**: The claimed reasoning task improvements (GSM8K +8%, MMLU +4.7%) are surprising and warrant skepticism, as the mechanism linking watermark training to reasoning gains is not explained.
- **Low Confidence**: The scalability and robustness claims extend beyond what is directly demonstrated, as the paper provides limited comparison to state-of-the-art watermarking methods and generalizability to different model sizes, domains, or languages remains untested.

## Next Checks

- **Check 1: Hyperparameter Sensitivity Analysis** - Systematically vary λ across orders of magnitude (1e-6 to 1e-1) and measure ROC-AUC, task accuracy, and perplexity on a fixed validation set to reveal whether the reported λ = 1e-5/1e-2 sweet spot is robust or an artifact of specific training conditions.

- **Check 2: Paraphrase Robustness Test** - Generate watermarked text using the λ = 1e-5 configuration, then apply automated paraphrasing and measure detection ROC-AUC to directly test the acknowledged vulnerability and quantify operational risk.

- **Check 3: Observer Adapter Ablation** - Train otherwise identical configurations with observer LoRA ranks of 16, 32, and 64 to empirically validate the paper's implicit claim that larger observer capacity improves detection without harming fluency.