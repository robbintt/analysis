---
ver: rpa2
title: 'Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries'
arxiv_id: '2510.14751'
source_url: https://arxiv.org/abs/2510.14751
tags:
- future
- tokens
- prediction
- summaries
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Future Summary Prediction (FSP) was proposed to address the limitation
  of Next-Token Prediction (NTP) and Multi-Token Prediction (MTP) in capturing long-range
  dependencies. FSP trains a single auxiliary head to predict a compact representation
  of the future sequence, rather than individual tokens.
---

# Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries

## Quick Facts
- arXiv ID: 2510.14751
- Source URL: https://arxiv.org/abs/2510.14751
- Reference count: 18
- Proposed Future Summary Prediction (FSP) to address NTP/MTP limitations in capturing long-range dependencies, achieving up to 5% improvement on reasoning benchmarks.

## Executive Summary
This paper addresses the limitations of Next-Token Prediction (NTP) and Multi-Token Prediction (MTP) in capturing long-range dependencies for large language models. The authors propose Future Summary Prediction (FSP), which trains a single auxiliary head to predict a compact representation of the long-term future sequence rather than individual tokens. Two variants are explored: handcrafted summaries using bag-of-words and learned summaries using a reverse language model. Large-scale pretraining experiments (3B and 8B parameters) demonstrate that FSP consistently outperforms NTP and MTP on math, reasoning, and coding benchmarks, particularly for tasks requiring long-horizon reasoning and planning.

## Method Summary
The method trains a transformer backbone with an additional lightweight auxiliary head that predicts a summary of the future sequence. For FSP-BCE, the target is a multi-hot vector of future tokens in a window τ. For FSP-RevLM, a separate reverse language model is trained on reversed sequences, and its hidden states serve as summary targets. The total loss combines the standard NTP loss with the auxiliary summary loss (BCE for handcrafted, ℓ₂ for learned). The approach aims to reduce teacher forcing dependence by requiring the model to reason about global trajectory properties rather than local next-token patterns.

## Key Results
- FSP consistently outperforms NTP and MTP on math, reasoning, and coding benchmarks at both 3B and 8B scales
- FSP-BCE achieves perfect accuracy on synthetic path-star graph tasks where NTP/MTP fail due to shortcut learning
- FSP-RevLM shows better scaling properties than FSP-BCE at 8B parameters
- Improvements range from 1-5% on tasks requiring long-horizon reasoning, with τ=100 window size showing particular benefit for complex reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Reduced Teacher Forcing via Long-Range Summary Prediction
Predicting a compact summary of the long-term future reduces dependence on ground-truth prefixes more effectively than predicting individual future tokens. At each timestep t, FSP trains an auxiliary head to predict a summary vector representing tokens (x_{t+2}, ..., x_{t+τ}), forcing the model to encode global trajectory information into the prefix representation rather than exploiting local cues. This contrasts with NTP (maximum teacher forcing) and MTP (partial reduction via short blocks).

### Mechanism 2: Adaptive Filtering of Irrelevant Future Tokens via Learned Summaries
Learned summaries from a reverse language model selectively encode informative future content while ignoring irrelevant tokens. The reverse LM Q_ψ is trained on right-to-left sequences to predict x_{t+1} from x_{≥t+2}. Its hidden state naturally emphasizes tokens most predictive of the "next" (structurally earlier) token, suppressing irrelevant context. The forward model's auxiliary head learns to match this filtered representation.

### Mechanism 3: Shortcut Prevention via Long-Horizon Supervision
FSP-BCE's bag-of-words summaries prevent "Clever Hans" shortcut learning on path-star graphs by requiring the model to encode the full future path trajectory, not just local adjacency information. In path-star tasks, NTP can predict v_{i+1} from v_i via local adjacency lookup without learning the global path. FSP-BCE requires predicting all future tokens in the path simultaneously, making local shortcuts insufficient.

## Foundational Learning

- **Concept: Teacher Forcing and Exposure Bias**
  - Why needed here: FSP is motivated as a practical alternative to teacherless training that reduces the train-inference mismatch. Understanding why teacher forcing causes compounding errors and shortcut learning is essential to grasp FSP's design rationale.
  - Quick check question: Can you explain why a model trained with teacher forcing might fail at inference even if it achieves low training loss on path-planning tasks?

- **Concept: Multi-Token Prediction (MTP) Architecture**
  - Why needed here: FSP is positioned as an extension/refinement of MTP. MTP uses multiple auxiliary heads (one per future token); FSP uses a single head for a summary. Understanding MTP's limitations (short horizon, independence assumptions) clarifies why FSP's single-summary approach is proposed.
  - Quick check question: Why does standard MTP assume independence among predicted future tokens, and what problem does this create for long-range dependencies?

- **Concept: Reverse Language Models and Bidirectional Representations**
  - Why needed here: FSP-RevLM uses a reverse LM trained right-to-left to generate summary vectors. Understanding how reverse LMs encode context—and why their hidden states might capture predictive structure—is necessary to implement and debug FSP-RevLM.
  - Quick check question: How does a reverse LM trained on x_T → x_1 differ from a forward LM in what its hidden states encode at position t?

## Architecture Onboarding

- **Component map**: Forward Transformer Backbone (f_s) -> Main Prediction Head (f_h → f_u) -> Auxiliary Summary Head (f'_{ha}) -> Loss Combiner
- **Critical path**: 
  1. Implement FSP-BCE first (simpler, no additional model needed): For each training sequence, precompute multi-hot vectors for each position t over window τ. Add single auxiliary head with BCE loss.
  2. For FSP-RevLM: Train reverse LM on same corpus (can reuse compute later). Extract hidden states as targets. Train forward model with ℓ₂ matching loss.
  3. During inference: Discard auxiliary head entirely; use only standard next-token head.

- **Design tradeoffs**:
  1. Hand-crafted vs. Learned summaries: FSP-BCE is computationally cheaper (no reverse LM) but noisier on tasks with irrelevant future content. FSP-RevLM is more expensive upfront (reverse LM training) but provides adaptive summaries.
  2. Window size τ: Shorter windows (τ=12) are easier to predict but may miss long-range signal. Longer windows (τ=100) capture more signal but introduce noise.
  3. Summary head depth: Paper experiments with different reverse LM layers; last layer generally works best but not universally.
  4. TF-IDF reweighting for FSP-BCE: Helps at 8B but not consistently at 3B.

- **Failure signatures**:
  1. Auxiliary loss converges but main task doesn't improve: Summary may be capturing predictable but irrelevant information. Try learned summaries or shorter windows.
  2. Performance degrades on some tasks: Hand-crafted summaries may include distracting future tokens. Switch to FSP-RevLM.
  3. FSP-RevLM underperforms FSP-BCE: Reverse LM may not have converged, or its representations don't transfer. Check reverse LM perplexity; try different layers.
  4. No improvement over MTP on short-horizon tasks: Expected—FSP is designed for long-range reasoning.

- **First 3 experiments**:
  1. Synthetic validation on path-star graph: Implement FSP-BCE on G(2,6) and G(2,8). Confirm 100% accuracy vs. NTP/MTP failure.
  2. Small-scale (3B or smaller) ablation of summary types: Compare FSP-BCE (τ=12, τ=100), FSP-RevLM, and vanilla MTP on GSM8K, MATH.
  3. Reverse LM quality check: Train reverse LM on held-out validation split. Report perplexity and visualize hidden states to ensure representations cluster meaningfully.

## Open Questions the Paper Calls Out

- **Question**: Does FSP improve performance on open-ended creative writing tasks?
  - Basis: The abstract and introduction identify "creative writing" as a domain where NTP struggles, but experimental evaluation is restricted to math, reasoning, and coding benchmarks.

- **Question**: Does FSP-RevLM retain its performance advantage under strict iso-compute constraints that include the cost of the ReverseLM teacher?
  - Basis: Section 4.1 states iso-compute comparisons that include the teacher model's cost were not performed, treating it as a "one-time overhead."

- **Question**: Why does FSP-RevLM underperform Next-Token Prediction on the GSM8K benchmark at the 8B parameter scale?
  - Basis: Table 1 shows FSP-RevLM achieves 70.5% on GSM8K at 8B, lagging behind NTP baseline (71.6%) despite outperforming on other math and reasoning tasks.

## Limitations
- Summary quality directly determines whether FSP provides meaningful gradients—poor summaries can mislead training
- The optimal window size τ is task-dependent and not fully characterized, with mixed results across different benchmarks
- Computational overhead of training a separate reverse LM is substantial and not fully accounted for in efficiency comparisons

## Confidence
- **High Confidence**: Synthetic experiments on path-star graphs and sibling discovery tasks demonstrating FSP-BCE's superiority
- **Medium Confidence**: Scaling results on real-world math, reasoning, and coding benchmarks (GSM8K, MATH, ARC, HumanEval+)
- **Low Confidence**: Learned summaries via reverse LM (FSP-RevLM) scaling advantage at 8B parameters due to limited ablation and transfer uncertainty

## Next Checks
1. **Transfer Learning Validation for FSP-RevLM**: Train a reverse LM on a small validation subset and verify its representations cluster meaningfully before using as targets.
2. **Window Size Sensitivity Analysis**: Systematically vary τ on representative tasks to characterize the tradeoff between signal capture and noise inclusion.
3. **Gradient Analysis on Synthetic Tasks**: Analyze gradient flow from auxiliary loss on path-star graphs to validate whether FSP gradients emphasize global trajectory information versus local adjacency patterns.