---
ver: rpa2
title: Disrupt Your Research Using Generative AI Powered ScienceSage
arxiv_id: '2502.18479'
source_url: https://arxiv.org/abs/2502.18479
tags:
- index
- knowledge
- vector
- query
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ScienceSage is a web application that leverages generative AI to
  help researchers build, store, update, and query knowledge bases for scientific
  research. It addresses challenges in retrieving relevant, up-to-date information
  from multimodal data (text, images, audio, video) and generating structured research
  reports.
---

# Disrupt Your Research Using Generative AI Powered ScienceSage

## Quick Facts
- arXiv ID: 2502.18479
- Source URL: https://arxiv.org/abs/2502.18479
- Reference count: 26
- ScienceSage demonstrates hybrid (vector + knowledge graph) indexing outperforms single-index approaches for correctness, relevance, and faithfulness across 2,295 queries.

## Executive Summary
ScienceSage is a web application leveraging generative AI to help researchers build, store, update, and query knowledge bases for scientific research. It addresses challenges in retrieving relevant, up-to-date information from multimodal data and generating structured research reports. The application integrates three core functions—generating research reports, chatting with documents, and chatting with any multimodal data—using a common set of knowledge bases. These knowledge bases are indexed using vector, knowledge graph, and custom (combined) indices to enable efficient information retrieval and querying. The application uses LLMs like ChatGPT4 and Mixtral 8X7B, with retrieval-augmented generation (RAG) for enhanced accuracy. Evaluation of RAG performance across three indexing methods showed that the custom index consistently outperformed others in correctness, relevance, and faithfulness across queries of varying difficulty and keyword occurrences.

## Method Summary
ScienceSage employs retrieval-augmented generation with three indexing methods: vector index using Hugging Face all-distilroberta-v1 embeddings, knowledge graph index using Nebula Graph with triples, and a custom hybrid index combining both. The system uses 2,295 queries across three difficulty levels and three keyword occurrence levels to evaluate indexing performance using LlamaIndex evaluators (CorrectnessEvaluator, RelevancyEvaluator, FaithfulnessEvaluator) with GPT-4 as judge. Knowledge bases are built incrementally from 85 documents, enabling progressive updates without reprocessing. The application supports research report generation through multi-stage prompting with web search integration, and multimodal chat functionality with image, audio, and video processing.

## Key Results
- Hybrid indexing consistently outperformed vector and knowledge graph indexing across all query types and difficulty levels.
- Custom index achieved highest correctness scores (1-5 scale), relevance (0-1 scale), and faithfulness (F = |V|/|S|, 0-1 scale).
- Incremental knowledge base construction enabled efficient updates without reprocessing, maintaining retrieval quality.
- System successfully deployed internally for accelerating research projects through quick iteration and information retrieval.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid (vector + knowledge graph) indexing improves retrieval quality over single-index approaches, particularly for complex queries.
- Mechanism: Vector index captures broad semantic context through embeddings; knowledge graph index anchors responses in specific entities and factual relationships. The custom index retrieves from both sources independently, then synthesizes a combined response.
- Core assumption: Semantic similarity and factual precision provide complementary signals that jointly improve retrieval when properly combined.
- Evidence anchors:
  - [abstract]: "A comprehensive evaluation using 2,295 queries across three difficulty levels... demonstrated that the hybrid (vector + knowledge graph) indexing method outperformed single-index approaches"
  - [section]: "This can be attributed to the integration of two complementary strengths: the vector index's ability to capture broad semantic context and the KG index's ability to anchor responses in specific entities and factual information"
- Break condition: When vector and KG indices retrieve contradictory information, synthesis may produce confused or inconsistent responses; computational overhead may not justify marginal gains for simple queries.

### Mechanism 2
- Claim: Incremental knowledge base construction reduces computational burden while maintaining retrieval quality.
- Mechanism: Users build and update KBs progressively rather than batch-processing all documents at once. Indices persist across sessions, allowing cumulative knowledge accumulation without reprocessing.
- Core assumption: Incremental updates preserve index coherence; stale or conflicting entries don't significantly degrade retrieval.
- Evidence anchors:
  - [abstract]: "ScienceSage enables researchers to build, store, update and query a knowledge base (KB)"
  - [section]: "Practically, it is good to build and update the KB incrementally as it both smooth computational burden and ease user's effort to curate all data at once"
- Break condition: Accumulated index fragmentation or conflicting entities across updates may degrade knowledge graph quality over time.

### Mechanism 3
- Claim: Multi-stage prompting with web search integration enables up-to-date, structured research report generation.
- Mechanism: Question decomposition → sub-query generation → internet/scientific database search → content scraping → summarization → hierarchical report synthesis with references.
- Core assumption: LLM can reliably decompose questions, identify relevant sources, and synthesize coherent structured output from multiple summaries.
- Evidence anchors:
  - [section]: "ScienceSage first decomposes the question into a number of queries and then searches the internet... The scraped websites are then summarized. Finally a comprehensive and structural report is generated"
  - [section]: "For generating a research report, ScienceSage always searches the internet or scientific databases to get the latest information"
- Break condition: Low-quality or contradictory scraped sources produce unreliable syntheses; web search may return irrelevant or outdated content despite recency intentions.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture for "Chat With Your Documents" and "Chat With Anything" functions; understanding RAG is prerequisite for debugging retrieval quality issues.
  - Quick check question: Can you explain how RAG differs from fine-tuning an LLM, and what tradeoffs each approach involves?

- Concept: **Vector Embeddings and Semantic Similarity**
  - Why needed here: Foundation for vector index; text embedding quality directly affects retrieval relevance and faithfulness scores.
  - Quick check question: Given two text embeddings, how would you compute their similarity, and what does a high similarity score actually represent?

- Concept: **Knowledge Graphs and Entity-Relationship Triples**
  - Why needed here: KG index provides factual grounding; understanding (Subject, Predicate, Object) structure is necessary for interpreting and debugging graph-based retrieval.
  - Quick check question: What is a triple in knowledge graph terminology, and how might a graph traversal differ from a vector similarity search?

## Architecture Onboarding

- Component map:
  Frontend: Streamlit UI (three tabs: Generate Research Report, Chat With Your Documents, Chat With Anything)
  LLM Layer: GPT-4/GPT-4-Vision via Azure (commercial) or Mixtral 8X7B on-premise (open source)
  Embedding Models: Hugging Face all-distilroberta-v1 (text), CLIP (image)
  Orchestration: Langchain (report generation agent), LlamaIndex (RAG pipelines)
  Storage: Weaviate (vector index), Nebula Graph (knowledge graph), LanceDB (multimodal—under testing)
  External Search: DuckDuckGoSearchAPI, ArxivRetriever

- Critical path:
  User query → KB selection → Index type selection (vector/KG/hybrid) → Retrieval from indices → Context assembly → LLM synthesis → Response with references

- Design tradeoffs:
  - Hybrid vs. single index: Superior correctness/relevance/faithfulness at cost of latency (per paper's evaluation)
  - On-premise Mixtral vs. Azure GPT-4: Data privacy and response guarantees vs. potential capability differences
  - Multiple specialized databases (Weaviate, Nebula, LanceDB) vs. unified storage: Flexibility vs. operational complexity

- Failure signatures:
  - Vector index: High hallucination rates on hard queries with low keyword occurrence (semantic drift without factual anchors)
  - KG index: Lower relevance on context-heavy queries lacking specific named entities
  - Hybrid synthesis: Contradictory retrieved contexts may produce inconsistent outputs
  - Report generation: Scraped web content quality directly impacts final report accuracy

- First 3 experiments:
  1. Run the provided evaluation benchmark (or subset) comparing vector vs. KG vs. hybrid indices on domain-specific queries to establish baseline performance for your use case.
  2. Test incremental KB building: upload documents in batches over multiple sessions, querying after each update to verify index coherence and retrieval quality.
  3. Generate research reports on topics with known ground truth; manually verify reference quality and factual accuracy against source documents.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the STORM framework be adapted to generate knowledge base grounded long documents in specific formats like product feasibility reports?
- Basis in paper: [explicit] Authors state they "intend to adapt STORM to generate knowledge base grounded long documents with specific formats such as product feasibility and quality reports."
- Why unresolved: The current system generates Wiki-style articles or generic reports, but lacks the specialized formatting required for specific industry documentation.
- What evidence would resolve it: A successful demonstration of the system generating a structured product feasibility report derived directly from the knowledge base.

### Open Question 2
- Question: What methodologies are required to effectively capture tacit knowledge from domain experts for integration into the ScienceSage knowledge base?
- Basis in paper: [explicit] The improvement plan explicitly includes "capturing tacit knowledge from domain experts."
- Why unresolved: The current system extracts knowledge only from explicit multimodal data (documents, video); mechanisms for extracting unrecorded expert intuition remain undefined.
- What evidence would resolve it: A defined workflow or interface that successfully converts expert interactions into indexed knowledge base entities.

### Open Question 3
- Question: Can a single open-source database efficiently store and query vector, knowledge graph, and multimodal indices simultaneously?
- Basis in paper: [inferred] The authors note the challenge of managing three separate databases and state, "To our best knowledge, we don't have an open source embedding based database to efficiently store and query all these indices yet."
- Why unresolved: The current architecture relies on three distinct databases (Weaviate, Nebula Graph, LanceDB), creating management overhead and complexity.
- What evidence would resolve it: Identification or development of a unified database architecture supporting all three index types with comparable retrieval performance.

### Open Question 4
- Question: How can the SciSafeEval framework be utilized to evaluate and mitigate safety risks and potential misuse of the AI in scientific tasks?
- Basis in paper: [explicit] The authors list "using framework like SciSafeEval to evaluate the safety and risk for potential misuse of AI in science" as further research.
- Why unresolved: While the tool is deployed internally, a formal safety alignment and risk evaluation specific to scientific tasks has not yet been conducted.
- What evidence would resolve it: Benchmark results showing the system's performance on safety metrics and a reduction in potential misuse scenarios.

## Limitations

- Evaluation corpus (85 documents, 2,295 queries) is relatively small and domain-specific, potentially limiting external validity.
- Paper does not report statistical significance testing between indexing methods, making it unclear whether observed performance differences are robust.
- Custom hybrid index implementation details—specifically how vector and KG retrievals are fused before synthesis—remain underspecified, preventing independent replication.

## Confidence

- **High confidence**: The hybrid indexing approach consistently outperformed single-index methods in correctness, relevance, and faithfulness across difficulty levels and keyword occurrences.
- **Medium confidence**: The claim that incremental KB construction reduces computational burden while maintaining retrieval quality. Lacks comparative analysis of incremental vs. batch processing overhead.
- **Medium confidence**: The multi-stage prompting mechanism for research report generation. Architecture described but evaluation focuses on retrieval quality rather than report generation accuracy.

## Next Checks

1. **Statistical Significance Testing**: Conduct paired t-tests or Wilcoxon signed-rank tests on correctness scores across the 2,295 queries to determine whether hybrid indexing improvements over vector and KG methods are statistically significant.

2. **Latency and Cost Analysis**: Measure end-to-end query latency (time-to-first-token and time-to-complete) for each indexing method across easy, medium, and hard queries. Calculate computational cost per query type to quantify the practical overhead of hybrid indexing.

3. **Domain Transferability**: Apply the three indexing methods to a different scientific domain (e.g., biology or materials science) with comparable document count and query sets. Compare performance consistency to assess whether hybrid indexing advantages generalize beyond the original evaluation corpus.