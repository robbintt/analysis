---
ver: rpa2
title: 'Fairness in Survival Analysis: A Novel Conditional Mutual Information Augmentation
  Approach'
arxiv_id: '2502.02567'
source_url: https://arxiv.org/abs/2502.02567
tags:
- survival
- fairness
- time
- data
- cmia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fairness in survival analysis,
  specifically focusing on achieving equitable prediction accuracy across predefined
  time intervals for different demographic groups. Existing methods overlook this
  crucial aspect, leading to biased predictions in real-world applications like healthcare
  and criminal justice.
---

# Fairness in Survival Analysis: A Novel Conditional Mutual Information Augmentation Approach

## Quick Facts
- arXiv ID: 2502.02567
- Source URL: https://arxiv.org/abs/2502.02567
- Reference count: 12
- Primary result: Introduces CMIA approach achieving up to 93.78% adTPR and 92.75% adFPR mitigation on COMPAS dataset

## Executive Summary
This paper addresses fairness in survival analysis by introducing a novel Conditional Mutual Information Augmentation (CMIA) approach. The method tackles the problem of achieving equitable prediction accuracy across different demographic groups at predefined time intervals. Traditional survival analysis methods overlook fairness considerations, leading to biased predictions in critical applications like healthcare and criminal justice. The authors propose a new fairness concept called equalized odds (EO) for survival analysis and demonstrate that their CMIA approach consistently reduces prediction disparity while maintaining good accuracy across multiple datasets.

## Method Summary
The CMIA approach features two key innovations: a fairness regularization term based on conditional mutual information and a novel censored data augmentation technique. The fairness regularization term ensures the survival model adheres to EO fairness criteria by minimizing conditional mutual information between predictions and sensitive attributes given true labels. The censored data augmentation module balances prediction accuracy and fairness during model training by learning additional time offsets for censored observations. The method is evaluated across multiple datasets and survival models, showing consistent improvements in fairness metrics while maintaining competitive accuracy.

## Key Results
- CMIA achieves up to 93.78% and 92.75% mitigation in adTPR and adFPR respectively on COMPAS dataset for deep survival model
- Consistent reduction in prediction disparity across multiple datasets including FLC, Consumer Loan, and COMPAS
- Ablation study validates effectiveness of both CMI regularization and censored data augmentation components
- Maintains competitive accuracy metrics (aAUC, aBrier) while improving fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing conditional mutual information to zero enforces equalized odds fairness at pre-defined time points
- Mechanism: The regularization term CMI_t quantifies shared information between predictions Ŷ_t and sensitive attributes Z given true labels Y_t. Because CMI_t ≥ 0 with equality if and only if Ŷ_t ⊥ Z | Y_t (proven via Jensen's inequality), gradient descent on this term asymptotically enforces TPR and FPR parity across groups at each evaluation time t ∈ Q
- Core assumption: The approximation ĈMI_m,t converges to true CMI as m → ∞ (relies on strong law of large numbers and kernel density estimation with Gaussian kernels)
- Evidence anchors: [abstract] "fairness regularization term based on conditional mutual information", [section 3.3.1] Eq. (7)-(11) formalize CMI_t and the approximation ĈMI_m,t; property (10) establishes non-negativity and implication to EO

### Mechanism 2
- Claim: Censored data augmentation creates a "mediation zone" that reduces conflict between accuracy and fairness optimization landscapes
- Mechanism: For censored observations (δ=0), the method learns additional time offsets Δ_j to synthesize pseudo-event times T̃_j = T_j + Δ_j. These augmented samples are included in the joint loss, which changes the geometry of the loss contours so that parameter updates can improve fairness without large accuracy sacrifices
- Core assumption: The learned Δ values reflect plausible event times and do not introduce systematic bias; the augmentation helps align the optima of the accuracy loss and fairness regularization
- Evidence anchors: [abstract] "censored data augmentation technique... balances prediction accuracy and fairness", [section 3.3.2] Formalizes augmentation as Eq. (12); section 4.6 visualization shows contour shift after augmentation (Figure 2)

### Mechanism 3
- Claim: Equalized odds in survival analysis requires conditional independence at each pre-specified time point, not across all continuous time
- Mechanism: EO is defined as Ŷ_t ⊥ Z | Y_t for all t ∈ Q (Eq. 6). The necessary and sufficient condition is equality of TPR and FPR across groups at each t. This differs from prior work (e.g., concordance imparity) that averages over continuous time
- Core assumption: Pre-defined evaluation times Q are correctly chosen for the application domain (e.g., 730 days for recidivism)
- Evidence anchors: [abstract] "equalized odds (EO) for survival analysis, which requires conditional independence... across all predefined time points", [section 3.2] Eq. (6) and the TPR/FPR equivalence proof in Appendix A

## Foundational Learning

- Concept: Survival function S(t|X) and hazard function λ(t|X) relationship
  - Why needed here: The paper's models (COX, AFT) estimate risk via g_θ(X) in the hazard; understanding S(t|X) = exp(-∫λ) is essential to interpret predictions and binarize them at threshold 0.5
  - Quick check question: Given a hazard function λ(t|X) = λ_0(t)·exp(g_θ(X)), can you derive the survival probability at t=730 days?

- Concept: Conditional mutual information
  - Why needed here: The core regularization term is a CMI estimator; you must understand that CMI(X;Y|Z)=0 iff X⊥Y|Z to see why minimizing it enforces EO
  - Quick check question: If P(Ŷ|Z,Y) = P(Ŷ|Y) for all Z, what is the value of CMI(Ŷ;Z|Y)?

- Concept: Censoring in time-to-event data
  - Why needed here: The augmentation module specifically targets censored observations (δ=0) where event times are unknown; this is unique to survival analysis and central to the method's design
  - Quick check question: In a dataset with 30% events and 70% censored, which subset does CMIA's augmentation modify?

## Architecture Onboarding

- Component map:
  Input -> Base model -> Fairness regularizer -> Joint objective
  {(X, T, δ, Z)}_i -> g_θ(X) -> R_EO(g_θ, Δ) = Σ_t ĈMI_m,t -> L(g_θ, Δ) + λ_1·R_EO(g_θ, Δ) + λ_2·||θ||²_2

- Critical path:
  1. Initialize θ (model weights) and Δ (augmentation offsets, typically zeros or small positive values)
  2. Forward pass: compute g_θ(X) for all samples; apply augmentation to censored subset
  3. Compute base loss L (COX or AFT) on augmented dataset
  4. Compute fairness regularizer R_EO via Eq. (9) — requires grouping by (Y_t, Z) and kernel evaluations
  5. Backpropagate through θ and Δ jointly using Adam
  6. Evaluate EO metrics (adTPR, adFPR) at each t ∈ Q on validation set

- Design tradeoffs:
  - m (number of noise samples): larger m improves approximation quality but increases compute; paper uses predetermined large m
  - τ (kernel bandwidth): controls smoothness of density estimation; must be tuned
  - λ_1 (fairness weight): higher values enforce EO more strictly but may hurt accuracy; tuning on validation set is critical
  - Augmentation scope: only censored observations are augmented; if censoring rate is low, the mediation effect is limited

- Failure signatures:
  - ĈMI_m,t remains high despite training: check m, τ, and learning rate; approximation may be unstable
  - Accuracy collapses (aAUC ≈ 0.5): λ_1 may be too large, or augmentation Δ values diverged
  - adTPR/adFPR disparities persist: Q may be misaligned with data distribution, or group sizes are too imbalanced for stable CMI estimation
  - Training instability: Δ gradients may be sparse if few censored samples exist; consider reducing augmentation influence

- First 3 experiments:
  1. **Baseline sanity check**: Train vanilla COX and AFT models (linear and deep) on one dataset (e.g., COMPAS); report aAUC, aBrier, adTPR, adFPR to quantify initial unfairness
  2. **Ablation of components**: Run CMIA-Aug (no augmentation) and CMIA-Reg (no regularization) on the same dataset; compare to full CMIA to isolate each component's contribution to accuracy and fairness
  3. **Hyperparameter sweep on λ_1 and τ**: Fix m to a large value; vary λ_1 ∈ {0.01, 0.1, 1.0} and τ ∈ {0.1, 1.0, 10.0}; plot accuracy vs. adTPR/adFPR to identify the Pareto frontier and validate the tradeoff visualization in Figure 2

## Open Questions the Paper Calls Out
- How does the CMIA approach perform when applied to non-parametric survival models, such as Random Forests, or other complex objective functions not tested in the study?
- What are the long-term impacts of deploying the CMIA approach on different stakeholders in real-world high-stakes environments?
- How sensitive is the model's fairness performance to the specific selection and granularity of the pre-defined evaluation time points (Q)?
- How does the censored data augmentation strategy affect model performance when the censoring mechanism is informative rather than random?

## Limitations
- The approximation quality of ĈMI_m,t depends heavily on hyperparameters m and τ, which are not fully specified in the paper
- The censored data augmentation mechanism's success relies on the learned Δ values reflecting plausible event times, but this is not validated
- Group size imbalances could lead to unstable CMI estimates and unreliable fairness metrics in minority groups

## Confidence
- High: The theoretical foundation that minimizing CMI enforces EO (non-negativity and conditional independence properties are mathematically proven)
- Medium: Empirical results showing reduced adTPR/adFPR disparities across datasets, though the magnitude of improvement varies
- Low: The general applicability to datasets with extreme censoring rates or very small minority groups

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary m, τ, and λ_1 to identify stable regions where CMI approximation is accurate and fairness gains are robust
2. **Synthetic Data Stress Test**: Create synthetic survival datasets with controlled censoring rates (0-90%) and group imbalances to evaluate CMIA's performance boundaries
3. **Augmentation Realism Validation**: For censored samples, compare the distribution of learned Δ values to the observed event time distribution in uncensored samples to verify that augmentation produces plausible synthetic event times