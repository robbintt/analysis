---
ver: rpa2
title: Wasserstein distance based semi-supervised manifold learning and application
  to GNSS multi-path detection
arxiv_id: '2512.05567'
source_url: https://arxiv.org/abs/2512.05567
tags:
- data
- learning
- samples
- distance
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a semi-supervised learning method for image
  classification when labelled data are scarce. The core idea is to use the Wasserstein
  distance as a similarity metric in a graph-based transductive label propagation
  framework, enabling a CNN to learn from both labelled and unlabelled samples.
---

# Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection

## Quick Facts
- **arXiv ID:** 2512.05567
- **Source URL:** https://arxiv.org/abs/2512.05567
- **Reference count:** 7
- **Primary result:** Up to 5% classification accuracy improvement using semi-supervised learning with Wasserstein distance regularization on GNSS multipath detection

## Executive Summary
This study proposes a semi-supervised learning method for image classification when labelled data are scarce. The core idea is to use the Wasserstein distance as a similarity metric in a graph-based transductive label propagation framework, enabling a CNN to learn from both labelled and unlabelled samples. The method is applied to GNSS multipath detection, where input signals are represented as 2D I/Q images. Experiments across various signal-to-noise conditions show that introducing unlabelled data can improve classification accuracy by up to 5% compared to fully supervised training. The approach demonstrates that exploiting data regularity can enhance model performance while reducing dependency on labelled datasets. No clear hyperparameter selection rule was identified, suggesting a need for further investigation.

## Method Summary
The method applies semi-supervised learning to binary classification of GNSS multipath detection using I/Q correlator output images. A VGG-style CNN processes 26×26 I/Q image pairs, with outputs passed through sigmoid for binary classification. The training loss combines supervised BCE loss on labelled samples with a Wasserstein-regularized smoothness term. The smoothness term penalizes differences in model predictions for sample pairs, weighted by a Gaussian function of their Wasserstein distance. The Wasserstein distance is computed separately for I and Q channels using optimal transport solvers, then summed. The regularization excludes pairs where both samples have labels to prevent conflicting objectives. A grid search over hyperparameters λ (regularization weight) and σ (bandwidth) is performed across multiple signal-to-noise conditions.

## Key Results
- Up to 5% accuracy improvement over fully supervised baseline when using unlabelled data
- Best performance achieved with N_SUP=60 labelled samples out of 200 total training samples
- Performance degrades with too few labelled samples (N_SUP=25) or at low signal-to-noise ratios (C/N0=37 dBHz)
- No consistent hyperparameter selection rule identified across different C/N0 conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Wasserstein distance provides a perceptually meaningful similarity metric for comparing I/Q signal images, enabling effective label propagation between samples.
- **Mechanism:** The Wasserstein (Earth Mover's) distance computes the minimum energy cost to transform one image distribution into another. For GNSS I/Q images, this captures structural similarity better than pixel-wise metrics, as it respects the spatial organization of signal features (correlation peaks, phase patterns). The distance is computed separately for I and Q channels then summed.
- **Core assumption:** Signal images can be treated as probability distributions where pixel intensities represent mass, and optimal transport between distributions correlates with perceptual/signal similarity.
- **Evidence anchors:**
  - [section 3.2] "an efficient metric to compare two images is to consider the Wasserstein distance that computes the minimum energy cost to map one image seen as a probability distribution to the other image"
  - [section 3.2] "we propose to compute the Wasserstein distance between two pairs (I, Q) and (I', Q') of images by independently computing the optimal transportation cost between the pair (I, I') first and then (Q, Q')"
  - [corpus] Limited direct corpus validation for GNSS-specific application; "Laplace Learning in Wasserstein Space" provides theoretical grounding for Wasserstein-based SSL but not signal processing specifics.
- **Break condition:** If pixel intensities don't meaningfully represent probability mass (e.g., negative values, sparse signals with little structure), the transport interpretation degrades. Also breaks if the linear programming solver fails to converge for large images.

### Mechanism 2
- **Claim:** Smoothness-enforcing regularization via Gaussian-weighted pairwise prediction differences propagates label information from labelled to unlabelled samples along the data manifold.
- **Mechanism:** The regularization term L_U penalizes differences in model predictions for sample pairs, weighted by W_ij = exp(-d(x_i, x_j)²/σ). Samples with small Wasserstein distance receive high weight, forcing their predictions toward similarity. This creates an implicit graph where labels flow from annotated nodes to neighbours, respecting manifold geometry.
- **Core assumption:** The smoothness assumption: "the label function of the data varies smoothly with the distance between samples and even more so when in high density regions" [section 2.1]. Also assumes the manifold hypothesis holds for GNSS signal images.
- **Evidence anchors:**
  - [section 2.2] "L_U(f(x_i, θ), f(x_j, θ)) = W_ij ||f(x_i, θ) - f(x_j, θ)||²"
  - [section 2.2] "W_ij ∈ [0,1] is a weight parameter that is close to 1 if the distance between x_i and x_j is small and near 0 otherwise"
  - [corpus] "Laplace Learning in Wasserstein Space" examines graph-based SSL in Wasserstein space with FMR=0.55, supporting theoretical foundations.
- **Break condition:** When labelled samples are too sparse (N_SUP = 25 showed degraded performance), the manifold is poorly characterized and label propagation becomes unreliable. High noise conditions (low C/N0) also stress this mechanism.

### Mechanism 3
- **Claim:** Excluding labelled-labelled pairs from the regularization term preserves ground-truth signal and prevents conflicting objectives.
- **Mechanism:** The XOR condition (i≤L)⊕(j≤L) excludes pairs where both samples have labels. This prevents the regularization from forcing similar labelled samples toward the same prediction when they have different true labels (e.g., both multipath-free and multipath-contaminated signals might have similar visual features but different classes).
- **Core assumption:** Ground-truth labels are more reliable than smoothness-based inference for annotated samples.
- **Evidence anchors:**
  - [section 2.2] "the operator ⊕ indicates that a pair where both samples have labels is excluded from the sum"
  - [section 2.2] "this specific handling of pairs of samples ensures that we are not computing any smoothing term when both samples have labels as this might be a conflicting objective"
  - [corpus] No direct corpus validation found for this specific design choice.
- **Break condition:** If labelled samples contain annotation errors, this mechanism protects those errors from correction via smoothness-based regularization.

## Foundational Learning

- **Concept: Optimal Transport / Wasserstein Distance**
  - **Why needed here:** Core similarity metric. Understanding that it solves a linear program to find minimum-cost mass transport between distributions is essential for debugging distance computations and interpreting σ parameter effects.
  - **Quick check question:** Given two histograms [0.5, 0.5, 0, 0] and [0, 0, 0.5, 0.5] with unit spacing, what's the minimum transport cost? (Answer: 1.0)

- **Concept: Transductive vs. Inductive SSL**
  - **Why needed here:** This method is transductive—it propagates labels through a fixed graph structure rather than learning a general label function. Affects how you think about test-time inference and data augmentation.
  - **Quick check question:** Does a transductive method require unlabelled test samples during training? (Answer: Yes, typically)

- **Concept: Manifold Hypothesis and Smoothness Assumption**
  - **Why needed here:** Justifies why label propagation should work. If data doesn't lie on a low-dimensional manifold or labels aren't smooth over it, regularization provides no benefit.
  - **Quick check question:** In high-dimensional space with sparse data, why might k-nearest neighbours fail but manifold-based methods succeed? (Answer: Geodesic distance vs. Euclidean distance on curved manifolds)

## Architecture Onboarding

- **Component map:**
  Input: I/Q image pairs (2×26×26) -> CNN Backbone (VGG-style: Conv→Conv→Pool→FC→FC→Sigmoid) -> Output: Probability p(multipath) -> Loss = BCE(labelled) + λ × Σ W_ij × (p_i - p_j)²

- **Critical path:**
  1. Pre-compute Wasserstein distance matrix W for all training pairs (expensive, O(n²) LP solves)
  2. Convert distances to weights: W_ij = exp(-d²/σ)
  3. During each batch: sample pairs, compute BCE + weighted prediction differences
  4. Backprop through both terms

- **Design tradeoffs:**
  - **VGG vs. modern architectures:** Authors chose VGG for methodological clarity; ResNet/efficient architectures may improve absolute performance but obscure SSL contribution.
  - **I/Q channel separation:** Computing Wasserstein separately per channel then summing is a simplification; joint complex-valued transport could capture phase relationships better but lacks off-the-shelf solvers.
  - **Batch-compatible regularization:** The pairwise sum is restricted to batch-internal pairs, enabling SGD but approximating full-graph regularization.

- **Failure signatures:**
  - N_SUP = 25: Underperforms fully supervised (Figure 10)—insufficient labelled samples to characterize manifold.
  - Low C/N0 (37 dBHz) + high σ: Noise overwhelms signal structure, smoothness regularization propagates noise.
  - λ too high: Model ignores labelled data, produces uniform predictions.
  - σ too low: Only near-identical samples are smoothed, minimal propagation.
  - σ too high: Dissimilar samples forced together, prediction collapse.

- **First 3 experiments:**
  1. **Baseline calibration:** Train fully supervised CNN with N_SUP ∈ {25, 50, 75, 100, 200} at C/N0 = 40 dBHz. Plot accuracy curve to identify scarcity regime (should match Figure 3 shape).
  2. **Hyperparameter grid search:** Fix N_SUP = 60, C/N0 = 40 dBHz. Grid search λ ∈ {1, 10, 100, 1000} × σ ∈ {0.01, 0.1, 1.0, 10.0}. Report accuracy gain over baseline. Expect optimum near λ = 10-100, σ = 0.01-10 (per paper's Figure 7).
  3. **Ablation on distance metric:** Replace Wasserstein with L2 pixel distance. Compare accuracy gains. If Wasserstein advantage is real, L2 should show smaller or negative gains, particularly at lower C/N0 where structural similarity matters more.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a deterministic or heuristic rule for selecting the optimal hyperparameters $\lambda$ (regularization weight) and $\sigma$ (bandwidth) based on the dataset characteristics ($N_{SUP}$ and $C/N_0$)?
- Basis in paper: [explicit] The conclusion states, "no clear rule has been found to select the optimum values for the hyperparameters involved in the method and further investigation in this direction should be conducted."
- Why unresolved: The grid search results showed high variability in optimal pairs depending on the scenario (e.g., $(1, 1.0)$ vs $(1000, 0.5)$), preventing the identification of a consistent pattern.
- What evidence would resolve it: A theoretical analysis linking the bandwidth $\sigma$ to the signal noise floor or an adaptive meta-learning algorithm that predicts $\lambda$ without exhaustive grid search.

### Open Question 2
- Question: Does the proposed semi-supervised learning approach maintain its effectiveness when applied to modern, deeper neural network architectures (e.g., ResNet, Inception) compared to the VGG baseline?
- Basis in paper: [inferred] The authors acknowledge using an "obsoleted" VGG architecture to isolate methodological effects, noting that "using alternative architectures the proposed method would be exactly identical... however, this would not highlight the role of SSL."
- Why unresolved: The study intentionally restricted experiments to a baseline CNN to avoid conflating the gains from the SSL method with gains from architectural optimization.
- What evidence would resolve it: Benchmarking the Wasserstein-SSL loss function on state-of-the-art architectures within the GNSS multipath detection context.

### Open Question 3
- Question: Can a direct computational procedure for the Wasserstein distance on complex measures be developed to replace the current approximation of summing independent transport costs for I and Q channels?
- Basis in paper: [inferred] The text notes that regarding computing the metric on complex images, "to the best of our knowledge, there is no computational procedure... for this reason, we propose to compute the Wasserstein distance... by independently computing the optimal transportation cost."
- Why unresolved: Current optimal transport solvers typically operate on real-valued probability distributions, lacking a standard implementation for complex-valued image data common in signal processing.
- What evidence would resolve it: Derivation of a complex-valued optimal transport metric and a demonstration that it provides better semantic similarity for GNSS signals than the summed independent approach.

## Limitations
- Performance gains are demonstrated only in synthetic GNSS multipath detection setting, not validated on real-world data
- No principled hyperparameter selection rule exists, requiring expensive grid search for each new condition
- The method's robustness to annotation errors or domain shifts remains untested

## Confidence
- **High confidence:** The core SSL mechanism (Wasserstein-based smoothness regularization) is technically sound and correctly implemented. The transductive framework and pairwise exclusion logic are clearly specified.
- **Medium confidence:** The claimed 5% accuracy improvement is statistically demonstrated within the synthetic dataset but may not transfer to real GNSS signals or other domains. The lack of a hyperparameter selection rule is a practical limitation.
- **Low confidence:** The assertion that Wasserstein distance is "perceptually meaningful" for GNSS I/Q images lacks independent validation beyond the synthetic results. The method's robustness to annotation errors or domain shifts is untested.

## Next Checks
1. **Real-world data test:** Apply the trained model to a small set of real GNSS I/Q captures (if available) and compare performance degradation against synthetic test sets.
2. **Hyperparameter transfer study:** Fix a good (λ,σ) pair from the C/N0=40 dBHz setting and evaluate its performance across all C/N0 conditions without re-tuning. Report sensitivity.
3. **Ablation on label error:** Intentionally inject 5-10% label noise into the N_SUP samples and measure how much the SSL regularization helps correct these errors versus the fully supervised baseline.