---
ver: rpa2
title: The Challenge of Achieving Attributability in Multilingual Table-to-Text Generation
  with Question-Answer Blueprints
arxiv_id: '2503.23204'
source_url: https://arxiv.org/abs/2503.23204
tags:
- blueprints
- blueprint
- english
- table
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether using Question-Answer blueprints improves
  the attributability (faithfulness to source tables) of multilingual Table-to-Text
  generation. QA blueprints are created by extracting propositions from reference
  sentences, generating QA pairs from these propositions, and concatenating them into
  a structured blueprint that models use as an intermediate planning step before generating
  the final verbalisation.
---

# The Challenge of Achieving Attributability in Multilingual Table-to-Text Generation with Question-Answer Blueprints

## Quick Facts
- **arXiv ID**: 2503.23204
- **Source URL**: https://arxiv.org/abs/2503.23204
- **Reference count**: 30
- **Key outcome**: QA blueprints improve attributability for English Table-to-Text but fail in multilingual settings due to translation inaccuracies and models not relying closely on blueprints.

## Executive Summary
This paper investigates whether Question-Answer (QA) blueprints can improve the attributability (faithfulness to source tables) of multilingual Table-to-Text generation. QA blueprints are generated by extracting propositions from reference sentences, creating QA pairs from these propositions, and using them as intermediate planning steps before generating the final verbalisation. While QA blueprints improve attributability for English-only models, they perform worse than no blueprints in the multilingual setting due to translation inaccuracies and models failing to closely rely on their generated blueprints. The study uses the multilingual TaTA dataset (including African languages) and mT5 models, revealing fundamental challenges in multilingual blueprint-based generation.

## Method Summary
The method involves extracting propositions from reference sentences using a propositionizer, generating QA pairs for each proposition using a QA generator, and filtering these pairs to remove hallucinated content. The resulting QA pairs are formatted as blueprints and used as intermediate representations during training. Models learn to first predict a blueprint from the linearised table, then generate the verbalisation conditioned on both. Two setups are tested: English-only blueprints and blueprints machine-translated into target languages. The mT5 model (both Small and Large variants) is fine-tuned on this data with repetition penalty applied during inference. STATA metric (mT5-Large fine-tuned for regression on human annotations) is used to evaluate attributability.

## Key Results
- QA blueprints improve attributability for English-only models (STATA +0.010)
- In multilingual setting, blueprints perform worse than no blueprints due to translation inaccuracies
- Models fail to rely closely on generated blueprints (CHRF 0.39 vs 0.61 in gold data)
- Larger models (mT5-Large) benefit low-resource languages but may overfit on small datasets
- Translation quality degrades significantly for low-resource languages (Yorùbá CHRF 0.19 vs French 0.80)

## Why This Works (Mechanism)

### Mechanism 1: QA Blueprint as Intermediate Planning
QA blueprints improve attributability by decomposing generation into explicit content selection followed by verbalisation. The model learns to first predict a structured blueprint p(b|t) from the linearised table, then generates the verbalisation conditioned on both: p(v|t, b). This forces the model to commit to specific propositions before generating fluent text. The core assumption is that the model treats the blueprint as a binding intermediate representation rather than ignoring it during verbalisation. If the blueprint contains hallucinated numbers or propositions not in the source table, errors propagate to the final output.

### Mechanism 2: Translation-Induced Blueprint Degradation
Machine-translated blueprints introduce semantic drift that undermines multilingual effectiveness. Blueprints are generated in English then machine-translated to target languages. Translation errors (especially for low-resource languages) break the alignment between blueprint content and source table, training models on noisy intermediate representations. The core assumption is that translation quality varies predictably by language resource level, and semantic drift in blueprints directly harms downstream verbalisation quality. When translation quality falls below a threshold, the blueprint may introduce contradictory or nonsensical content.

### Mechanism 3: Blueprint-Verbalisation Dependency Failure
Fine-tuned models underutilize their own generated blueprints during verbalisation. Analysis shows low lexical overlap between generated blueprints and verbalisations (CHRF 0.39 for English model vs. 0.61 in gold data), indicating models do not strongly condition verbalisation on blueprint content. The core assumption is that the training objective does not enforce tight coupling between blueprint and verbalisation; models learn to generate valid-looking but decoupled outputs. If verbalisation generation ignores the blueprint, the planning step provides no constraint on hallucination.

## Foundational Learning

- **Linearised table representation**
  - Why needed here: Tables must be converted to text format for seq2seq models; understanding this transformation is critical for debugging attribution failures.
  - Quick check question: Can you explain how a multi-column table is flattened into a single input sequence?

- **Attributability vs. fluency trade-off**
  - Why needed here: The paper measures success by faithfulness to source data, not output quality alone; optimizing only for fluency can increase hallucination.
  - Quick check question: Why might a fluent, grammatical output still receive a low STATA score?

- **Constrained decoding**
  - Why needed here: The paper identifies this as a potential solution for enforcing blueprint-verbalisation coupling.
  - Quick check question: How might constrained decoding force a model to use numbers that appear in its generated blueprint?

## Architecture Onboarding

- **Component map**: Linearised table -> Proposition extraction -> QA generation -> Blueprint creation -> Blueprint generation p(b|t) -> Verbalisation generation p(v|t, b) -> Output with repetition penalty

- **Critical path**: 1) Extract propositions using FLAN-T5-Large propositionizer; 2) Generate 5 QA pairs per proposition with T5-Large finetuned on SQuAD; 3) Filter pairs (drop hallucinated numbers, empty strings, those without question marks); 4) Select best QA pair per proposition by word-level F1 vs reference; 5) Blueprint format: "a1.|a2.|..." with "Blueprint:" and "Verbalisation:" prefixes; 6) For multilingual: Machine-translate blueprints OR keep English with language tag; 7) Fine-tune mT5 with combined "Blueprint: ... Verbalisation: ..." target; 8) At inference: Generate full sequence, split on "Verbalisation:" token

- **Design tradeoffs**: English blueprints vs. translated blueprints (English avoids translation errors but causes language mixing; translated is better but introduces noise); mT5-Small vs. mT5-Large (Large benefits low-resource languages but may overfit on small datasets); Repetition penalty strength (higher values reduce repetition but may alter normal outputs)

- **Failure signatures**: Repetitive outputs (e.g., "juu cha juu cha juu..."); Language mixing when using English blueprints (Swahili phrases in English sentences); Hallucinated numbers in blueprints that propagate to verbalisation; Incomplete verbalisations (sentence fragments ending in periods); Identical numbers in comparative statements ("increased from 15% to 15%")

- **First 3 experiments**: 1) Establish baseline: Fine-tune mT5-Small on English-only subset without blueprints; measure STATA, CHRF, BLEU; 2) Validate blueprint mechanism: Add blueprints to English-only setup; confirm STATA increases (+0.010); 3) Diagnose multilingual failure: Fine-tune on full multilingual dataset with translated blueprints; compute Table→Blueprint and Blueprint→Verbalisation CHRF to quantify coupling strength

## Open Questions the Paper Calls Out

- **Open Question 1**: Can constrained decoding techniques effectively force models to rely more heavily on generated blueprints during multilingual Table-to-Text generation?
  - Basis in paper: Authors state constrained decoding could be explored to make verbalisations utilize blueprints more
  - Why unresolved: The paper identifies that verbalisations draw less from blueprints than gold references (CHRF 0.39 vs 0.61 for English), but does not test whether constrained decoding can address this
  - What evidence would resolve it: Experiments applying constrained beam search to multilingual blueprint models, measuring blueprint-to-verbalisation alignment scores and STATA attributability

- **Open Question 2**: Can LLM-generated synthetic multilingual training data improve blueprint-based Table-to-Text performance for low-resource African languages?
  - Basis in paper: Authors recommend trial using LLMs to generate more synthetic training data in several languages
  - Why unresolved: The TaTA dataset is small (902 English training examples), and models overfit quickly. No synthetic data experiments were conducted.
  - What evidence would resolve it: Comparison of models trained on original vs. LLM-augmented datasets, with human evaluations of fluency and attributability across languages, particularly Yorùbá which had the worst translation quality (BLEU 0.06)

- **Open Question 3**: Would annotating cell-level table-to-reference alignments in TaTA improve blueprint quality and model performance?
  - Basis in paper: Authors suggest TaTA can be turned into a more constrained task by having human annotators highlight cells used in each reference verbalisation
  - Why unresolved: The valid answer space for large tables is vast, and models struggle with content selection. Current setup lacks explicit table-reference alignment.
  - What evidence would resolve it: Annotation of cell highlights for a subset of TaTA, followed by experiments comparing models trained with and without alignment information, measuring linearised-input-to-blueprint CHRF scores

## Limitations

- QA blueprints fail to improve attributability in multilingual settings despite benefits for English-only models
- The TaTA dataset is relatively small (902 English training examples), limiting model learning and increasing overfitting risks
- STATA metric has not been validated across diverse languages, raising questions about its cross-linguistic reliability
- Translation quality degrades significantly for low-resource languages, introducing noise in blueprint-based training

## Confidence

- **High confidence**: English-only blueprint effectiveness (STATA +0.010 improvement), translation quality degradation for low-resource languages (measured CHRF/BLEU), blueprint-verbalisation dependency failure (CHRF 0.39 vs 0.61 in gold data)
- **Medium confidence**: mT5-Large benefits for low-resource languages, overall negative impact of blueprints in multilingual setting, repetition penalty effectiveness
- **Low confidence**: Cross-linguistic validity of STATA metric, generalization of findings to other multilingual NLG tasks, specific threshold where translation quality becomes problematic

## Next Checks

1. **Cross-linguistic STATA validation**: Evaluate STATA metric reliability on human-annotated multilingual data to confirm it maintains 0.59 correlation across languages beyond English.

2. **Translation quality threshold analysis**: Systematically vary translation quality (e.g., using different MT systems or human translations) to identify the specific threshold where blueprint degradation impacts attributability.

3. **Constrained decoding implementation**: Implement and evaluate constrained decoding that forces verbalisation to reuse numbers from generated blueprints, testing whether this closes the 0.22 CHRF gap between current models and gold data.