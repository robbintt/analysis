---
ver: rpa2
title: Hierarchical Knowledge Structuring for Effective Federated Learning in Heterogeneous
  Environments
arxiv_id: '2504.03505'
source_url: https://arxiv.org/abs/2504.03505
tags:
- data
- global
- knowledge
- local
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of balancing personalization
  and generalization in federated learning under non-IID data distributions. The proposed
  Hierarchical Knowledge Structuring (HKS) framework introduces a bottom-up clustering
  mechanism on the server to organize sample logits into multi-granularity codebooks,
  ranging from per-sample to per-class knowledge.
---

# Hierarchical Knowledge Structuring for Effective Federated Learning in Heterogeneous Environments

## Quick Facts
- arXiv ID: 2504.03505
- Source URL: https://arxiv.org/abs/2504.03505
- Reference count: 40
- Primary result: HKS achieves MAUA of 90.79% and 90.56% with competitive global accuracy under high data heterogeneity

## Executive Summary
This paper introduces Hierarchical Knowledge Structuring (HKS), a federated learning framework that addresses the fundamental challenge of balancing personalization and generalization in non-IID data environments. The method organizes sample logits into multi-granularity codebooks using bottom-up clustering on the server, enabling clients to access aggregated knowledge at varying specificity levels during local training. By combining supervised learning with knowledge distillation from the hierarchical codebook, HKS achieves strong personalization performance while maintaining competitive global accuracy, outperforming baseline methods like FedDistill and FedCache.

## Method Summary
HKS operates through a client-server architecture where clients train local models and upload sample logits to a central server. The server performs Agglomerative Hierarchical Clustering (AHC) on these logits to create a hierarchy ranging from per-sample to per-class knowledge. During local training, clients download aggregated logits from their sample's cluster path at a specified granularity level and apply knowledge distillation alongside supervised learning. The framework uses a warm-up phase for initial logit generation, followed by iterative communication rounds where clients receive hierarchical knowledge and update their models using a combined loss function.

## Key Results
- HKS achieves MAUA values of 90.79% and 90.56% under high data heterogeneity (α=0.5)
- Maintains competitive global accuracy of 59.75% and 56.89% respectively
- Outperforms baseline methods FedDistill and FedCache in personalization-generalization tradeoff
- "Middle" granularity setting provides optimal balance for heterogeneous settings

## Why This Works (Mechanism)
HKS addresses the personalization-generalization tradeoff by creating a hierarchical knowledge structure that captures information at multiple levels of specificity. The bottom-up clustering algorithm groups similar sample logits, creating clusters that represent different levels of abstraction. During training, clients can access knowledge at their desired granularity - from highly specific per-sample information to more generalized per-class representations. This allows the framework to provide personalized knowledge while maintaining connections to the global data distribution through the hierarchical structure. The knowledge distillation component ensures that clients' local models are regularized by the aggregated knowledge, preventing overfitting while still allowing specialization.

## Foundational Learning
- **Concept: Knowledge Distillation (KD)**
  - Why needed here: HKS uses KD as its core engine for transferring knowledge from the server's hierarchical codebook to the client's local model. The `L_distill` term (KL divergence) aligns the client's output with the aggregated logits, acting as a learned regularization term.
  - Quick check question: Can you explain how Kullback-Leibler (KL) divergence is used to align the output of a student model with a teacher model's softened probability distribution?

- **Concept: Non-IID Data & Client Drift**
  - Why needed here: The entire HKS framework is designed to solve the problem of performance degradation caused by non-IID data. Client drift—where local models diverge from the global optimum due to training on skewed local data—is the primary issue HKS seeks to mitigate through personalized distillation.
  - Quick check question: In a federated setting, how does a highly skewed local data distribution (e.g., a client having only two classes out of ten) cause a globally averaged model to perform poorly on that client's data?

- **Concept: Agglomerative Hierarchical Clustering (AHC)**
  - Why needed here: This is the unsupervised algorithm HKS uses on the server to build the multi-granularity codebook. Understanding AHC is critical to understanding how the "bottom-up" structure is formed and how a sample's "cluster path" is determined.
  - Quick check question: How does a bottom-up hierarchical clustering algorithm iteratively group data points, and what does a dendrogram represent in this context?

## Architecture Onboarding
- **Component map:** Clients -> Local Models -> Logits -> Server -> KnowledgeCache -> AHC -> Hierarchical Codebook -> Clients
- **Critical path:**
    1. **Initialization:** The `KnowledgeCache` is initialized. Clients use the shared encoder `E` to generate and upload feature hashes for their samples. The server builds an initial HNSW index.
    2. **Warm-up:** Clients perform local training for `W` warm-up epochs using only their local supervised loss (`L_local = L_CrossEntropy`).
    3. **Logit Exchange:** After warm-up, clients upload their sample logits (`ℓi`) to the server. The server updates the `KnowledgeCache`.
    4. **Hierarchical Clustering:** The server performs AHC on the logits in `K` to build a hierarchy of clusters from fine-grained (bottom) to coarse-grained (top, e.g., one cluster per class).
    5. **Personalized Distillation:** Clients download aggregated logits from their sample's "cluster path" at the chosen granularity (e.g., "middle"). Local training continues with the combined loss: `L_total = L_local + α * L_distill`.

- **Design tradeoffs:**
    *   **Granularity Selection:** A key hyperparameter. "Bottom" clusters favor personalization but may overfit to local noise. "Top" clusters favor generalization but may not address specific client skew. The "middle" level is proposed as a strong default for heterogeneous settings.
    *   **Communication vs. Specificity:** The framework exchanges per-sample logits, which can be high-dimensional. While more efficient than exchanging full model parameters, this could be a bottleneck compared to methods that only exchange summary statistics or class-level prototypes.
    *   **Clustering Complexity:** Performing AHC on a large number of samples from many clients can be computationally expensive on the server. The paper does not deeply analyze the scalability of the clustering component.

- **Failure signatures:**
    *   **Collapse to Generic Model:** If the distillation weight `α` is too high or only the "top" granularity is used, local models may lose their ability to specialize, effectively behaving like a single global model and failing on personalized tasks.
    *   **Poor Clustering:** If the initial encoder `E` is not representative or if client logits are of very low quality (e.g., from extremely limited data), the AHC may form meaningless clusters, causing the distilled knowledge to be irrelevant or harmful.
    *   **Overfitting to Local Data:** If `α` is too low or only "bottom" granularity is used, clients may overfit to their local data and fail to generalize.

- **First 3 experiments:**
    1.  **Baseline & Heterogeneity Test:** Replicate the core result from Table I/II. Train HKS on a non-IID dataset (e.g., FashionMNIST with Dirichlet α=0.5). Compare the personalized accuracy (MAUA) and global test accuracy against a simple FedAvg baseline and a basic distillation method (FedDistill) to validate the personalization-generalization tradeoff.
    2.  **Granularity Ablation:** With a fixed non-IID setting (α=0.5), run HKS four times using each granularity setting ("top", "middle", "bottom", "all"). Plot the personalized vs. global accuracy for each run to empirically verify the trade-off curve described in section IV.C and identify the optimal default setting for that level of heterogeneity.
    3.  **Model Heterogeneity Validation:** Configure a subset of clients with a different model architecture (e.g., some clients use a CNN, others use a simple MLP). Run HKS to confirm that the framework functions correctly and improves performance over baselines, demonstrating that the logit-based exchange successfully decouples knowledge from model architecture.

## Open Questions the Paper Calls Out
- **Dynamic hyperparameter tuning:** Future work should focus on implementing mechanisms to automatically adjust knowledge granularity levels in response to real-time shifts in data heterogeneity, as the current framework relies on manually selected granularity levels fixed before training begins.

- **Computational scalability:** The computational limits of server-side Agglomerative Hierarchical Clustering (AHC) when scaling to large federated networks with millions of sample logits remain unexplored, as the experimental evaluation is restricted to 20 clients and FashionMNIST.

- **Domain specificity of encoder:** The performance dependence on the domain specificity of the pre-trained encoder used for feature hashing is unclear, as the paper does not analyze the impact of using encoders pre-trained on different domains relative to the client data.

## Limitations
- Server-side computational complexity of AHC scales with number of samples and clients, with scalability concerns not thoroughly analyzed
- Performance significantly depends on proper selection of granularity level, requiring empirical tuning for different heterogeneity levels
- Method's effectiveness relies heavily on quality of pre-trained encoder E for feature hashing, which is not fully specified

## Confidence
- **High Confidence:** The core methodology of using hierarchical knowledge structuring with knowledge distillation is technically sound and experimental results are reproducible
- **Medium Confidence:** Performance advantages in Table I and II are credible but require careful reproduction, particularly regarding unspecified distillation temperature T and pre-trained encoder E implementation
- **Medium Confidence:** Generalizability claims to other datasets and model architectures are reasonable but would benefit from additional validation beyond FashionMNIST

## Next Checks
1. **Granularity Trade-off Validation:** Replicate experiments from section IV.C to empirically verify the personalization-generalization tradeoff curve across different granularity levels under varying Dirichlet parameters (α=0.5 and α=1.0).

2. **Scalability Analysis:** Measure computational overhead and memory requirements of server-side AHC clustering as number of clients and samples increases, comparing performance with 10 vs 50 vs 100 clients.

3. **Encoder Dependency Test:** Systematically evaluate impact of using different pre-trained encoders (or no pre-trained encoder) on quality of hierarchical clustering and subsequent distillation performance to quantify dependency on this component.