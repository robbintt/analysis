---
ver: rpa2
title: Next Steps in LLM-Supported Java Verification
arxiv_id: '2502.01573'
source_url: https://arxiv.org/abs/2502.01573
tags:
- verification
- specification
- annotation
- code
- java
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates different prompting strategies for using Large
  Language Models (LLMs) to generate JML specifications for Java programs, with a
  focus on reliable error recovery during verification. The authors compare feedback-based
  approaches, where KeY's error messages guide corrections, against sampling-based
  approaches that restart the process from scratch.
---

# Next Steps in LLM-Supported Java Verification

## Quick Facts
- arXiv ID: 2502.01573
- Source URL: https://arxiv.org/abs/2502.01573
- Reference count: 23
- This paper evaluates prompting strategies for LLMs to generate JML specifications for Java programs, comparing feedback-based refinement against sampling-based restarts.

## Executive Summary
This paper evaluates different prompting strategies for using Large Language Models (LLMs) to generate JML specifications for Java programs, with a focus on reliable error recovery during verification. The authors compare feedback-based approaches, where KeY's error messages guide corrections, against sampling-based approaches that restart the process from scratch. Experiments show that iterative methods improve success rates, but current feedback from KeY does not significantly outperform sampling. A mixed approach combining both strategies solves more benchmarks than either alone. Token efficiency favors sampling, though feedback eventually becomes necessary for scaling. The work highlights the potential of intersymbolic AI for program verification while identifying limitations in current LLM feedback mechanisms.

## Method Summary
The study compares three approaches for LLM-supported JML specification generation: feedback-based refinement using KeY verifier error messages, sampling-based restarts without feedback, and a mixed strategy combining both. The pipeline involves generating JML annotations with GPT-4o, verifying them with KeY, and using the verifier's feedback (syntactic errors or proof branch labels) to guide corrections. Experiments run on 27 invariant synthesis tasks and 14 subcontract tasks, measuring success rates across 1-10 verification attempts. The mixed approach uses 5 initial samples followed by up to 10 feedback iterations per sample.

## Key Results
- Iterative attempts improve JML specification synthesis success rates compared to single-shot generation
- Sampling-based restart outperforms feedback-based refinement in token efficiency while matching effectiveness
- Mixed strategy (multiple samples with limited feedback iterations) achieves higher coverage than pure approaches

## Why This Works (Mechanism)

### Mechanism 1
Iterative attempts improve JML specification synthesis success rates compared to single-shot generation. LLMs exhibit variance in output quality across probabilistic samples, and by generating multiple candidates and validating each against KeY, the system exploits this variance—the probability of finding at least one correct specification increases with attempts. The bottleneck is elicitation reliability rather than fundamental LLM capability.

### Mechanism 2
Sampling-based restart outperforms feedback-based refinement in token efficiency and matches effectiveness because current verifier feedback does not meaningfully guide the LLM toward correct solutions. Sampling generates independent candidates at linear token cost while feedback accumulates conversation history at quadratic cost. When feedback content lacks actionable semantic information, the LLM makes only marginal changes rather than structural pivots.

### Mechanism 3
A mixed strategy achieves higher coverage than pure sampling or pure feedback alone by combining diverse starting points with local refinement. Some problems require the LLM to "land" near a correct solution before local refinement can succeed, while others benefit from short refinement chains. The mixed approach captures both scenarios.

## Foundational Learning

- **Contract-based specification (JML)**: Understanding preconditions, postconditions, loop invariants, and assignable clauses is essential for interpreting KeY errors and evaluating LLM outputs. Quick check: Given a method that reverses an array in-place, what JML annotations would you need to verify it?

- **Deductive verification vs. testing**: The paper's value proposition hinges on formal verification providing sound guarantees—unlike testing, a successful KeY proof means the specification holds for all inputs. Quick check: If KeY proves a method satisfies its contract, what residual possibilities for bugs remain?

- **LLM sampling temperature and variance**: The sampling strategy exploits LLM output variance. Understanding how temperature, top-p, and repeated sampling affect output diversity is critical for tuning the approach. Quick check: If you set temperature=0 for all sampling attempts, would the sampling-based strategy still work? Why or why not?

## Architecture Onboarding

- **Component map**: Partially Annotated Java File → LLM: GPT-4o with structured prompt → generates JML annotation candidates → KeY Verifier → attempts proof, returns: SUCCESS | PARSE_ERROR | PROOF_FAILURE(open branches) → Error Recovery Strategy → Loop until: success OR max_attempts OR context_window_exceeded

- **Critical path**: The verification call to KeY is the ground-truth gate. All reliability depends on KeY's correctness; if KeY has soundness bugs, the system could accept incorrect specifications. The LLM prompt design is the second critical path—poor prompts yield syntactically invalid JML that wastes iterations.

- **Design tradeoffs**: Sampling vs. Feedback (sampling is token-efficient but doesn't learn from failures; feedback accumulates context and can refine but risks stagnation). Mixed strategy budget allocation (5 samples × 10 feedback steps = 50 total attempts). Verifier choice (KeY provides detailed proof trees but terse error labels).

- **Failure signatures**: Syntactic loop (LLM repeatedly generates invalid JML—mitigation: few-shot examples in prompt). Semantic stagnation (LLM makes trivial changes to near-miss specifications—mitigation: switch to fresh sample). Context overflow (feedback-based approach hits context window limits—mitigation: summarize or truncate early conversation history).

- **First 3 experiments**: 1) Baseline replication: Run feedback-based approach on 27 invariant + 14 subcontract benchmarks with GPT-4o, measuring success rate at each step. 2) Token-cost instrumentation: Log input/output tokens for each successful verification, stratified by problem complexity. 3) Error message ablation: Replace KeY's proof branch labels with no feedback, generic "verification failed" message, and full proof tree dump.

## Open Questions the Paper Calls Out

1. **How can failed proof attempts be translated into textual representations that effectively guide LLMs toward correct JML specifications?** Current KeY feedback consists of raw proof branch labels that often confuse the LLM or cause it to get "stuck" on incorrect solutions. A new feedback mechanism that extracts semantic meaning from proof trees could demonstrate significant improvement over sampling-based baselines.

2. **Can a verification strategy successfully generate all necessary auxiliary specifications simultaneously rather than filling a single gap?** The current methodology is restricted to files with a single missing annotation; scaling to multiple gaps introduces dependencies where one incorrect annotation might block verification of others. An automated pipeline that synthesizes all dependent annotations without human intervention is needed.

3. **Does pre-filtering specification candidates using testing or fuzzing improve the efficiency of LLM-supported verification?** Deductive verification is computationally expensive; lightweight dynamic checks might reliably filter out incorrect LLM hallucinations faster than the current mixed sampling/feedback approach. Integrating a fuzzing step could reduce overall token cost and verification time.

## Limitations

- The underlying LLM capability for JML synthesis remains uncertain—experiments show variance but don't establish capability ceilings
- Benchmark representativeness is limited—41 total benchmarks may not capture the full difficulty spectrum of real-world verification tasks
- Token efficiency model assumes fixed token usage per attempt without empirical validation of actual token usage patterns

## Confidence

- **High confidence**: Iterative approaches improve success rates; sampling-based restart is more token-efficient than feedback-based refinement; mixed strategy outperforms both pure approaches
- **Medium confidence**: Current KeY feedback doesn't significantly improve LLM performance; LLM gets "stuck" on near-miss solutions; token efficiency favors sampling
- **Low confidence**: Underlying LLM capability assessment; generalizability to larger problem sets; impact of alternative feedback representations

## Next Checks

1. **Feedback ablation study**: Systematically replace KeY's proof branch labels with (a) no feedback, (b) generic failure messages, (c) full proof tree dumps, and (d) natural language explanations of verification failures. Measure whether any semantic content in current feedback is actually used by the LLM.

2. **Token usage profiling**: Instrument the pipeline to log exact input/output token counts for each attempt, stratified by problem complexity (lines of code, annotation density). Validate whether context accumulation provides efficiency benefits that offset quadratic costs.

3. **Capability ceiling assessment**: Design experiments with progressively harder verification problems where single-attempt success rate approaches zero. Measure whether iterative approaches can accumulate probability mass or whether this indicates fundamental capability gaps rather than elicitation failures.