---
ver: rpa2
title: Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's
  Disease Diagnosis
arxiv_id: '2511.02228'
source_url: https://arxiv.org/abs/2511.02228
tags:
- feature
- features
- fusion
- disease
- alzheimer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal fusion framework for Alzheimer's
  disease diagnosis that integrates MRI and PET imaging. The method addresses the
  challenge of effectively combining modality-specific and shared features while reducing
  cross-modal discrepancies.
---

# Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis

## Quick Facts
- arXiv ID: 2511.02228
- Source URL: https://arxiv.org/abs/2511.02228
- Authors: Delin Ma; Menghui Zhou; Jun Qi; Yun Yang; Po Yang
- Reference count: 25
- Primary result: 94.40% accuracy for CN vs. AD classification on ADNI dataset

## Executive Summary
This paper proposes a multimodal fusion framework that integrates MRI and PET imaging for Alzheimer's disease diagnosis. The method addresses the challenge of effectively combining modality-specific and shared features while reducing cross-modal discrepancies. The framework introduces a triple-channel attention module to capture spatial, channel, and pixel-level dependencies, along with a cross-modal consistent feature enhancement mechanism that uses learnable parameter representations and feature-level cross-correlation to align latent distributions. Experimental results demonstrate superior performance compared to existing approaches on the ADNI dataset.

## Method Summary
The proposed framework integrates MRI and PET imaging through a collaborative attention mechanism and consistent-guided fusion strategy. The approach employs a triple-channel attention module that captures spatial, channel, and pixel-level dependencies within each modality. A cross-modal consistency enhancement mechanism then aligns the feature distributions using learnable parameters and cross-correlation operations. Finally, a shared-specific feature fusion strategy combines the enhanced shared features with modality-specific information to produce the final classification output. The method was evaluated on the ADNI dataset with promising results for both CN vs. AD and AD vs. MCI classification tasks.

## Key Results
- Achieved 94.40% accuracy for CN vs. AD classification on the ADNI dataset
- Obtained 80.07% accuracy for AD vs. MCI classification task
- Ablation studies confirmed the effectiveness of each component, particularly the complementary benefits of triple-channel attention and cross-modal consistency modules

## Why This Works (Mechanism)
The framework succeeds by addressing the fundamental challenge of multimodal fusion: effectively integrating complementary information while accounting for modality-specific characteristics and reducing cross-modal discrepancies. The triple-channel attention mechanism allows the model to capture different types of dependencies within each modality, enabling more comprehensive feature representation. The cross-modal consistency enhancement aligns the latent distributions of MRI and PET features, reducing the impact of modality-specific variations. By preserving both shared and modality-specific features, the method can leverage the complementary strengths of both imaging modalities while maintaining their unique diagnostic information.

## Foundational Learning
- Multimodal fusion fundamentals: Combines information from multiple data sources to improve diagnostic accuracy; needed to integrate complementary information from MRI and PET; quick check: verify that fused features contain information from both modalities
- Attention mechanisms: Allows the model to focus on relevant features while suppressing noise; needed to capture important spatial, channel, and pixel-level dependencies; quick check: ensure attention weights highlight disease-relevant regions
- Cross-modal consistency: Aligns feature distributions across different modalities; needed to reduce discrepancies between MRI and PET representations; quick check: verify that aligned features show reduced modality-specific variations
- Feature fusion strategies: Methods for combining different feature representations; needed to integrate shared and modality-specific information effectively; quick check: confirm that fused features contain both shared and unique information
- Cross-correlation operations: Measures similarity between feature distributions; needed for cross-modal alignment and consistency enhancement; quick check: verify that correlation values improve after alignment

## Architecture Onboarding

Component map: Input MRI/PET -> Triple-Channel Attention -> Cross-Modal Consistency Enhancement -> Shared-Specific Feature Fusion -> Classification

Critical path: The most important components are the triple-channel attention module and cross-modal consistency enhancement, as they directly address the core challenges of multimodal feature representation and alignment. The shared-specific fusion layer is critical for preserving complementary information while integrating the modalities effectively.

Design tradeoffs: The framework balances between preserving modality-specific information and integrating shared features. Using learnable parameters for cross-modal alignment provides flexibility but may introduce additional optimization challenges. The triple-channel attention increases computational complexity but captures more comprehensive feature dependencies compared to single attention mechanisms.

Failure signatures: Poor performance may result from inadequate attention mechanism training, leading to insufficient feature extraction. Cross-modal alignment failures could cause modality-specific noise to dominate the fusion process. If the shared-specific fusion strategy is not properly balanced, important modality-specific diagnostic information might be lost during integration.

First experiments to run:
1. Visualize attention weight distributions across spatial, channel, and pixel dimensions to verify effective feature selection
2. Measure cross-modal correlation before and after consistency enhancement to confirm alignment effectiveness
3. Perform modality ablation tests to quantify the contribution of MRI and PET individually to final performance

## Open Questions the Paper Calls Out
None

## Limitations
- Validation on single dataset (ADNI) with specific demographic characteristics may limit generalizability to more diverse populations
- Performance differences between CN vs. AD (94.40%) and AD vs. MCI (80.07%) tasks suggest varying effectiveness across disease stages
- Reliance on learnable parameters for cross-modal alignment may exhibit sensitivity to initialization or require careful tuning across different datasets

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Technical implementation and mathematical formulation | High |
| Comparative performance claims on ADNI dataset | Medium |
| Generalizability of triple-channel attention mechanism | Medium |
| Robustness across different demographic distributions | Low |

## Next Checks
1. Evaluate the framework on independent, multi-site datasets with different demographic compositions to assess generalizability
2. Conduct ablation studies isolating the contribution of each attention channel to determine their relative importance
3. Test the method's performance with varying degrees of cross-modal noise to evaluate robustness in real-world clinical scenarios