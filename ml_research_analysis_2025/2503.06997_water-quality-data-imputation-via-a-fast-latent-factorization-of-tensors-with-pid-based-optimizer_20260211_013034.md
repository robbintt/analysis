---
ver: rpa2
title: Water Quality Data Imputation via A Fast Latent Factorization of Tensors with
  PID-based Optimizer
arxiv_id: '2503.06997'
source_url: https://arxiv.org/abs/2503.06997
tags:
- data
- ieee
- latent
- water
- factorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of missing water quality data
  imputation, which is critical for accurate environmental governance and decision
  support. The authors propose a Fast Latent Factorization of Tensors (FLFT) model
  that improves upon standard Latent Factorization of Tensors (LFT) by incorporating
  a nonlinear PID controller into the Stochastic Gradient Descent (SGD) optimization
  process.
---

# Water Quality Data Imputation via A Fast Latent Factorization of Tensors with PID-based Optimizer

## Quick Facts
- arXiv ID: 2503.06997
- Source URL: https://arxiv.org/abs/2503.06997
- Reference count: 40
- Primary result: FLFT model achieves lower RMSE (0.066 vs 0.098 for WSPred) and faster convergence (321 vs 461 iterations) on water quality datasets

## Executive Summary
This paper addresses the critical problem of missing water quality data imputation, which is essential for accurate environmental governance and decision support. The authors propose a Fast Latent Factorization of Tensors (FLFT) model that improves upon standard Latent Factorization of Tensors (LFT) by incorporating a nonlinear PID controller into the Stochastic Gradient Descent (SGD) optimization process. The PID controller dynamically adjusts the learning rate by incorporating past, current, and future error information, which accelerates convergence while maintaining high accuracy. Experiments on real-world water quality datasets (Victoria Harbor marine data) demonstrate that FLFT achieves lower RMSE and faster convergence compared to state-of-the-art models.

## Method Summary
FLFT is a tensor completion method that represents water quality data as a 3-way tensor Y (stations × metrics × time) and approximates it via rank-R factorization using three latent factor matrices S, M, T. Missing entries are predicted as ŷ_ijk = Σ(s_ir · m_jr · t_kr). The key innovation is incorporating a nonlinear PID controller into the SGD optimization process, which dynamically adjusts the effective learning signal by combining proportional, integral, and derivative error terms with non-linear amplification. The model uses Tikhonov regularization and linear bias correction, with termination based on convergence threshold or maximum iterations (500). The method is evaluated on Victoria Harbor marine water quality datasets with random missing data patterns.

## Key Results
- FLFT achieves lower RMSE (0.066 vs 0.098 for WSPred) on water quality datasets
- Faster convergence with fewer iterations (321 vs 461 for WSPred)
- Reduced computational time (4.91 vs 6.96 seconds for WSPred)
- Superior performance compared to baselines including matrix factorization, tensor factorization, and deep learning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor factorization captures latent structure in sparse water quality monitoring data to impute missing values.
- Mechanism: The model represents water quality data as a 3-way tensor Y (stations × metrics × time) and approximates it via rank-R factorization using three latent factor matrices S, M, T. Missing entries are predicted as ŷ_ijk = Σ(s_ir · m_jr · t_kr). This exploits low-rank structure inherent in spatiotemporal environmental data where correlations exist across monitoring stations, water quality metrics, and temporal patterns.
- Core assumption: Water quality data exhibits sufficient latent structure (correlations across stations, metrics, time) that can be captured by low-rank approximation.
- Evidence anchors:
  - [abstract] "A Latent Factorization of Tensors (LFT) with Stochastic Gradient Descent (SGD) proves to be an efficient imputation method."
  - [section] Page 2, Definition 2 describes rank-R approximation and the objective function (Equations 1-3).
  - [corpus] Related paper "Multiple Linked Tensor Factorization" confirms tensor approaches are used for multi-way biomedical/environmental data integration. "A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data" validates low-rank representation for water quality specifically.
- Break condition: If the data tensor has insufficient observed entries (very low density) or lacks correlation structure across dimensions, factorization may fail to generalize.

### Mechanism 2
- Claim: Non-linear PID controller dynamically adjusts the effective learning signal, accelerating convergence compared to standard SGD.
- Mechanism: The PID controller refines the instance error used in gradient updates by combining: (1) proportional term KP·e^n (current error acting as adaptive learning rate), (2) integral term KI·Σe^f (accumulated past error for steady-state reduction), and (3) derivative term KD·(e^n - e^(n-1)) (error trend for overshoot prevention). Non-linear functions f(x,α) = x^α·sign(x) with 0<α<1 amplify small errors, enabling larger effective steps when approaching convergence.
- Core assumption: The error trajectory during optimization follows patterns amenable to PID control dynamics from control theory.
- Evidence anchors:
  - [abstract] "It constructs an adjusted instance error into SGD via leveraging a nonlinear PID controller to incorporates the past, current and future information of prediction error for improving convergence rate."
  - [section] Page 3-4, Equations 5-8 and Figure 3 describe the non-linear PID formulation. Equation 9 shows the refined update scheme replacing standard error with ê_ijk.
  - [corpus] "Latent Tensor Factorization with Nonlinear PID Control for Missing Data Recovery in Non-Intrusive Load Monitoring" applies identical PID-SGD mechanism to NILM data, suggesting generalizability. Reference [38] Li et al. 2020 (cited in paper) provides theoretical foundation for PID-incorporated SGD.
- Break condition: If hyperparameters (KP, KI, KD, αi, αd) are poorly tuned for the error landscape, the controller may cause oscillation, overshoot, or slow convergence.

### Mechanism 3
- Claim: Non-linear gain modulation prevents overshoot while maintaining fast convergence through asymmetric error amplification.
- Mechanism: The non-linear function f(x,α) maps errors such that smaller errors receive proportionally larger amplification (e.g., 0.1→0.3 when α=0.5). This allows the integral term to dominate when error is large (fast approach), while the derivative term increases influence as error decreases (smooth landing). This replaces the fixed learning rate in standard SGD with an error-dependent adaptive signal.
- Core assumption: Different phases of optimization benefit from different balances between exploration (integral-driven) and stabilization (derivative-driven).
- Evidence anchors:
  - [abstract] "incorporates the past, current and future information of prediction error"
  - [section] Page 4 states: "The integral item can lead fast convergence but cause overshoot while the derivative can avoid it... the derivative term is set small at the beginning and increases when error decreases, in order to prevent overshoot." Figure 3 visualizes the non-linear amplification.
  - [corpus] Corpus evidence is limited on this specific non-linear gain design. Related papers on PID-optimizers (cited references [37-40]) suggest similar principles but the specific f(x,α) formulation appears novel to this work.
- Break condition: If the non-linearity exponent α is too aggressive (too small), small errors may be over-amplified causing instability near convergence.

## Foundational Learning

- Concept: **Tensor decomposition / CANDECOMP-PARAFAC (CP) factorization**
  - Why needed here: The entire model is built on expressing a 3-way tensor as a sum of rank-1 outer products. Without understanding how latent factors S, M, T reconstruct the tensor, you cannot debug imputation quality.
  - Quick check question: Given a 3×4×5 tensor with rank-3 approximation, what are the dimensions of the three factor matrices?

- Concept: **PID control theory (Proportional-Integral-Derivative)**
  - Why needed here: The optimizer replaces standard gradient descent error with a PID-refined error. Understanding the role of each term (P for immediate response, I for accumulated correction, D for trend/smoothing) is essential for hyperparameter tuning.
  - Quick check question: If your model oscillates around the minimum without converging, which PID component should you likely reduce?

- Concept: **Stochastic Gradient Descent with regularization (Tikhonov/L2)**
  - Why needed here: The base LFT model uses SGD with L2 regularization to prevent overfitting on sparse observed data. The FLFT modification augments this learning scheme.
  - Quick check question: In Equation 4, what term prevents the latent factors from growing unboundedly, and what hyperparameter controls its strength?

## Architecture Onboarding

- Component map:
  - Input Layer: HDI tensor Y of shape |I|×|J|×|K| (stations×metrics×time) with known entries Λ
  - Latent Factor Matrices: S (|I|×R), M (|J|×R), T (|K|×R), initialized randomly
  - Bias Vectors: a (|I|), b (|J|), c (|K|) for linear bias correction
  - PID Controller Module: Maintains error history for integral term, previous error for derivative term, applies non-linear gain f(x,α)
  - Optimizer: SGD with PID-refined error, L2 regularization λ
  - Output: Completed tensor Ŷ via Equation 1

- Critical path:
  1. Initialize S, M, T, a, b, c (small random values)
  2. For each known entry (i,j,k) ∈ Λ:
     - Compute prediction ŷ_ijk = Σ(s_ir·m_jr·t_kr) + a_i + b_j + c_k
     - Compute raw error e_ijk = y_ijk - ŷ_ijk
     - Apply PID controller to get refined error ê_ijk (Equation 8)
     - Update all factors touching this entry via Equation 9
  3. Evaluate on validation set; terminate when convergence criterion met
  4. Apply trained model to impute missing entries

- Design tradeoffs:
  - **Rank R**: Higher R captures more complex patterns but risks overfitting sparse data and increases computation O(|Λ|·R). Paper uses R=10.
  - **PID gains (KP, KI, KD)**: Higher KI accelerates early convergence but risks overshoot; higher KD stabilizes but may slow learning.
  - **Non-linear exponent α**: Lower α (e.g., 0.3) provides stronger amplification of small errors but may cause instability; higher α (e.g., 0.7) is more conservative.
  - **Regularization λ**: Higher λ prevents overfitting on sparse data but may underfit valid patterns.

- Failure signatures:
  - **RMSE plateaus early without further descent**: Learning rate too low or PID gains poorly scaled. Check KP baseline.
  - **Oscillating RMSE**: Integral gain KI too high relative to derivative KD. Reduce KI or increase KD.
  - **Imputed values are physically implausible** (e.g., negative dissolved oxygen): Model lacks domain constraints; consider adding non-negativity constraints or post-hoc clipping.
  - **Converges faster than baseline but to worse RMSE**: Non-linear gains over-amplifying noise. Increase α values.
  - **Training diverges (NaN values)**: Learning rate or PID gains too aggressive. Reduce all gains proportionally.

- First 3 experiments:
  1. **Reproduce baseline comparison**: Implement standard LFT with SGD on one dataset (D1), verify RMSE and convergence iterations approximately match paper (RMSE ~0.068, iterations ~500). This validates your base implementation.
  2. **Ablate PID components**: Run FLFT with only P-term, only P+I, only P+D, and full PID. Plot convergence curves to understand each component's contribution. Expect: P-only similar to standard SGD, P+I faster but possible overshoot, P+D smoother, full PID best.
  3. **Sensitivity analysis on non-linear gain**: Vary αi and αd independently across {0.3, 0.5, 0.7, 0.9} on validation set. Report RMSE and iterations. Identify optimal ranges for your data characteristics.

## Open Questions the Paper Calls Out

- **Can newly designed forms of non-linear functions further enhance the convergence rate and prediction performance of the FLFT model?**
  - Basis: The Conclusion states, "Further, there are multiple forms of non-linear functions, we consider design new forms of non-linear function to improve convergence rate and prediction performance in future work."
  - Why unresolved: The current study utilizes a specific non-linear function (power and sign combination) defined in Equation (7), but the authors acknowledge that alternative function forms remain unexplored.
  - What evidence would resolve it: Comparative experiments integrating alternative non-linear gain functions into the PID controller, demonstrating improved RMSE or iteration counts over the current $f(x, \alpha)$ implementation.

- **How does the FLFT model perform when imputing non-random, consecutive blocks of missing data typical of prolonged sensor failures?**
  - Basis: The Introduction cites "sensor malfunctions" as a cause for missing data, but the Methodology section notes datasets were "randomly divided" into training, validation, and testing sets.
  - Why unresolved: Randomly removing data points does not simulate the temporal continuity of real-world sensor outages (e.g., a sensor offline for a week), leaving the model's robustness to structured missingness untested.
  - What evidence would resolve it: Experiments utilizing "block-wise" missing data masks (removing continuous time slots) rather than random entry removal to evaluate the model's stability in realistic failure scenarios.

- **What is the sensitivity of the non-linear PID parameters ($\alpha_i$, $\alpha_d$, and gains) across heterogeneous water quality datasets?**
  - Basis: The paper tests on three datasets from the same location (Victoria Harbor) and mentions fixing the rank, but does not provide an ablation study on how changes in the non-linear gains ($\alpha_i$, $\alpha_d$) affect performance.
  - Why unresolved: While the paper demonstrates superior performance on marine data, it is unclear if the specific PID tuning is robust or if it requires extensive manual recalibration for different water types (e.g., freshwater vs. marine) or noise levels.
  - What evidence would resolve it: A parameter sensitivity analysis showing the fluctuation in RMSE and convergence speed as $\alpha_i$ and $\alpha_d$ vary, or tests on out-of-domain water quality datasets to verify generalizability.

## Limitations

- The model's performance depends heavily on careful hyperparameter tuning, particularly for PID gains and non-linear exponents, which may require extensive manual calibration for different datasets.
- The computational efficiency improvements are modest (2.05x speedup) and may not scale proportionally for larger tensors or different hardware configurations.
- The method assumes water quality data exhibits low-rank structure, which may not hold for all environmental monitoring scenarios or locations.

## Confidence

- **High confidence** in the mechanism by which tensor factorization captures spatiotemporal correlations in water quality data (Mechanism 1). This is well-established in the literature and the mathematical formulation is standard.
- **Medium confidence** in the PID controller's contribution to convergence acceleration (Mechanism 2). While the theoretical foundation exists, the specific non-linear gain design appears novel and lacks extensive validation.
- **Medium confidence** in the overall method's superiority over baselines. The reported improvements are statistically significant on the tested datasets, but generalization to other environmental monitoring contexts requires further validation.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary PID gains (KP, KI, KD) and non-linear exponents (αi, αd) across multiple random seeds to quantify robustness and identify stable operating regions.

2. **Cross-domain evaluation**: Apply FLFT to a different environmental monitoring dataset (e.g., air quality or soil moisture) to test generalizability beyond marine water quality data.

3. **Ablation study on tensor rank**: Evaluate FLFT performance across different rank values (R=5, 10, 15, 20) to establish the optimal trade-off between accuracy and computational efficiency for varying data densities.