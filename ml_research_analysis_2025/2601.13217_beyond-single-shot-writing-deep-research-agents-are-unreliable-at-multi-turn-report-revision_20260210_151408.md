---
ver: rpa2
title: 'Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn
  Report Revision'
arxiv_id: '2601.13217'
source_url: https://arxiv.org/abs/2601.13217
tags:
- feedback
- report
- content
- revision
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRDRE evaluates deep research agents on multi-turn report revision,
  revealing that while agents address over 90% of user feedback, they regress on 16-27%
  of previously covered content and degrade citation quality. Current DRAs struggle
  to reliably improve reports across comprehensiveness, factuality, and presentation
  metrics, even after multiple revision turns.
---

# Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision

## Quick Facts
- arXiv ID: 2601.13217
- Source URL: https://arxiv.org/abs/2601.13217
- Reference count: 40
- Primary result: MRDRE evaluation reveals DRAs regress on 16-27% of previously correct content during multi-turn revision despite addressing 90%+ of feedback.

## Executive Summary
This paper evaluates deep research agents on their ability to revise research reports across multiple turns of user feedback. While agents successfully address over 90% of specific feedback requests, they consistently degrade previously correct content (16-27% regression rate) and significantly harm citation quality. The study finds that even after four revision turns, agents fail to maintain improvements from earlier feedback, with all-history incorporation rates dropping from 90% to 66%. Simple inference-time fixes like prompt engineering and dedicated sub-agents provide limited improvement, suggesting fundamental limitations in current DRA architectures for reliable multi-turn revision.

## Method Summary
The MRDRE benchmark evaluates five deep research agents on multi-turn report revision using a checklist-based protocol. Agents first generate initial reports on research questions from three datasets (ResearchRubrics, RigorousBench, ResearcherBench). Reports are evaluated for comprehensiveness, factuality, and presentation, then user feedback is simulated for missed checklist criteria. Agents revise reports conditioned on previous versions and feedback history. The evaluation measures incorporation rate (feedback addressed), break rate (previously correct content degraded), and all-history incorporation rate (cumulative feedback satisfaction). Citation quality is assessed via faithfulness and groundedness metrics using an adapted VeriScore protocol with external verification.

## Key Results
- DRAs address over 90% of user feedback but regress on 16-27% of previously correct content
- Citation quality degrades significantly during revision, with Sonar DR showing faithfulness drops up to -67.4%
- All-history incorporation rate falls from 90% at Turn 2 to 66% by Turn 4
- Inference-time fixes reduce break rate from 31% to 10% but don't resolve citation degradation
- Revision fails to reliably improve reports across comprehensiveness, factuality, and presentation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High instruction-following capability causes agents to make aggressive, localized edits that disrupt unrelated content when addressing feedback.
- Mechanism: When DRAs receive specific feedback, they focus computational resources on satisfying the requested changes, treating revision as a targeted edit task rather than a preservation-aware rewrite. This narrow focus leads to "over-editing" where content outside the feedback scope is inadvertently modified or deleted.
- Core assumption: The underlying LLM optimizes for immediate feedback satisfaction without an explicit preservation objective in its objective function.
- Evidence anchors:
  - [abstract] "while agents address over 90% of user feedback, they regress on 16–27% of previously covered content and citation quality"
  - [section 5.1] "While DRAs can follow most of the feedback instructions, they fail to preserve content outside the feedback's scope... break rates average 31% under content feedback"
  - [corpus] Related work on LLM revision (Voice Interaction paper) suggests dialogic feedback improves reflection but does not address preservation explicitly

### Mechanism 2
- Claim: Multi-turn revision fails because agents cannot maintain cumulative awareness of all previous feedback targets across conversation history.
- Mechanism: As context accumulates across turns, earlier feedback receives less attention due to recency bias in LLMs. The agent prioritizes the most recent instruction, treating prior feedback as background context rather than active constraints. This leads to "forgetting" earlier edits while addressing new requests.
- Core assumption: Current DRAs lack an explicit external memory or state-tracking mechanism for tracking satisfied vs. unsatisfied criteria across turns.
- Evidence anchors:
  - [section 5.2] "all-history incorporation rate falls from 90% at Turn 2 to 66% by Turn 4" for Sonar DR, "while the current-turn incorporation rate stays stable around 90%"
  - [section 5.2] "agents fail to preserve previous fixes while addressing new feedback, even though earlier feedback remains in the input context"
  - [corpus] FS-Researcher (arxiv:2602.01566) addresses long-horizon context limits via file-system-based agents but does not specifically target feedback memory

### Mechanism 3
- Claim: Citation degradation occurs because revision models are not explicitly trained to preserve citation-to-claim bindings during text modification.
- Mechanism: When agents revise text to incorporate feedback, they regenerate claims without re-grounding them to original sources or verifying that new/modified claims maintain proper citation support. This causes citation faithfulness and claim groundedness to drop independently of content coverage changes.
- Core assumption: Citation generation is treated as an append-only behavior during initial report writing, not as a constraint that must persist through revision.
- Evidence anchors:
  - [section 5.1] "Revision significantly degrades citation faithfulness and claim groundedness... Sonar DR exhibits the most severe degradation, with faithfulness plummeting by up to -67.4%"
  - [appendix E.1] "For OpenAI DR, degradation stems primarily from reductions in both supported claims and overall citation counts"
  - [corpus] Weak direct evidence in neighbors; Dr. Bench (arxiv:2510.02190) evaluates factuality but not revision dynamics

## Foundational Learning

- Concept: **Deep Research Agent (DRA) Architecture**
  - Why needed here: The paper evaluates five DRAs with different scaffolds (proprietary, open scaffold + proprietary models, fully open). Understanding that DRAs are multi-component systems (retrieval, synthesis, writing agents) is prerequisite to diagnosing where revision failures originate.
  - Quick check question: Can you distinguish between a DRA's retrieval component and its report-writing component, and explain which is responsible for citation quality?

- Concept: **Checklist-Based Evaluation with Ternary Scoring**
  - Why needed here: The paper's core metrics (coverage, incorporation rate, break rate) all depend on checklist criteria scored as {0, 0.5, 1}. Without understanding this, the quantitative results are uninterpretable.
  - Quick check question: If a report partially addresses a checklist criterion, what score does it receive, and how does this affect the weighted coverage formula?

- Concept: **Citation Faithfulness vs. Claim Groundedness**
  - Why needed here: These are distinct factuality metrics. Faithfulness measures whether cited claims are supported by their sources; groundedness measures whether all claims (cited or not) are verifiable. The paper shows they degrade differently across agents.
  - Quick check question: If a report makes 10 claims, 6 with citations, and 4 of those 6 are supported by their sources, what are the faithfulness and groundedness scores?

## Architecture Onboarding

- Component map:
  MRDRE Evaluation Suite -> Three-dimension protocol (Comprehensiveness, Factuality, Presentation) + Feedback Simulation Pipeline -> Checklist evaluator J_cov -> Claim extractor -> Citation verifier J_fact -> Presentation evaluator J_pres -> Inference-time Fixes (Prompt Engineering, Reviser Sub-agent)

- Critical path:
  1. Generate initial report via DRA
  2. Evaluate with checklist to identify uncovered criteria
  3. Sample k uncovered criteria → simulate feedback via LLM
  4. DRA revises report conditioning on (query, previous reports, feedback history)
  5. Re-evaluate: compute coverage delta, incorporation rate (current turn targets), break rate (previously satisfied criteria that degraded)

- Design tradeoffs:
  - **Proprietary DRAs (OpenAI DR, Sonar DR)**: Best initial performance but largest degradation after revision; black-box debugging
  - **Open scaffolds (LC ODR)**: More controllable but requires careful model selection; GPT-4.1-mini backbone shows moderate break rates
  - **Post-trained DRAs (Tongyi DR, DR Tulu)**: Most stable citation quality but lower initial coverage; Tongyi omits citations entirely
  - **Inference-time fixes**: Reviser sub-agent reduces break rate (31% → 10%) but does not fix citation degradation; prompt engineering provides smaller gains

- Failure signatures:
  - **High break rate (>25%)**: Agent making destructive edits; check if scaffold has any content preservation mechanism
  - **All-history incorporation dropping while current-turn stays high**: Context overload or recency bias; agent not maintaining explicit state
  - **Citations dropping to zero (Sonar DR self-reflection: 68% of reports)**: Agent treating revision as regeneration without citation constraint; scaffold-level issue
  - **Coverage not improving across turns despite high incorporation**: Break rate offsetting gains; fundamental training/architecture issue

- First 3 experiments:
  1. **Establish baseline**: Run your DRA on MRDRE Core Set (75 questions), measure initial coverage and break rate under Content_1 feedback to identify which failure mode is most severe
  2. **Test inference-time mitigation**: Implement the Prompt Engineering fix (structured edit plans with hard constraints) and measure reduction in break rate; expect ~50% reduction based on paper results
  3. **Diagnose citation degradation**: For your worst-performing feedback type, manually inspect 5-10 cases where citation faithfulness dropped; classify whether the issue is (a) claims removed, (b) citations removed, or (c) new unsupported claims added—this determines whether the fix requires retrieval re-grounding or citation-preserving text editing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does scaling the backbone model size (e.g., from GPT-4.1-mini to larger variants) impact the reliability and regression rates of multi-turn report revision?
- Basis in paper: [explicit] The authors state in the Limitations section that "How model scaling affects revision ability remains unclear and warrants further investigation" due to the high cost of running proprietary DRAs.
- Why unresolved: The study primarily evaluated cost-efficient models (e.g., o4-mini, GPT-4.1-mini) and open-weights models, leaving the performance ceiling of the largest proprietary models untested.
- What evidence would resolve it: A comparative evaluation on the MRDRE benchmark using the strongest available backbone models (e.g., GPT-4.1 or o3) against the smaller variants to observe if regression rates decrease.

### Open Question 2
- Question: What are the underlying mechanistic causes that drive Deep Research Agents to hallucinate citations or regress on previously covered content during revision?
- Basis in paper: [explicit] The Limitations section notes that "the causes of the high break rate, imperfect incorporation rate, and citation degradation are not yet fully understood."
- Why unresolved: The paper identifies and quantifies these failure modes but does not perform internal analysis (e.g., attention head inspection) to explain why the context is lost or corrupted during the revision step.
- What evidence would resolve it: An interpretability study or error analysis that correlates specific architectural components or attention mechanisms with the "break rate" metric.

### Open Question 3
- Question: What specific training algorithms or reward structures can optimize DRAs to maximize feedback incorporation while minimizing content regression?
- Basis in paper: [explicit] The Conclusion states that reliable revision will require "more fundamental advances in training and scaffold design," as inference-time fixes like prompt engineering provided limited improvement.
- Why unresolved: Current DRAs appear optimized for single-shot generation; the paper shows they lack the intrinsic capability to balance local edits against global consistency without explicit architectural or optimization changes.
- What evidence would resolve it: Developing a DRA fine-tuned with a multi-objective loss function that penalizes the deletion of previously established facts and testing it on MRDRE.

### Open Question 4
- Question: How can evaluation protocols robustly normalize for report length and verify checklist quality to prevent rewarding verbosity?
- Basis in paper: [explicit] The Limitations section highlights that MRDRE "does not penalize excessive report length" and assumes checklists are high-quality, noting that Sonar DR achieved higher coverage partially due to generating much longer reports.
- Why unresolved: The current protocol allows agents to "game" coverage metrics by generating excessive text, and it lacks a mechanism to validate the simulation pipeline against potentially flawed ground-truth checklists.
- What evidence would resolve it: Introducing a length-controlled metric or an LLM-based checklist validator into the MRDRE suite to see if the performance gap between concise and verbose agents narrows.

## Limitations

- The evaluation relies on proprietary deep research agents (OpenAI DR, Sonar DR) whose internal behaviors cannot be directly inspected, making it difficult to distinguish between scaffold-level architectural failures and model-specific limitations.
- The paper does not explore more sophisticated preservation mechanisms beyond simple inference-time fixes, which showed limited improvement (break rate reduced from 31% to 10%).
- Citation degradation patterns vary significantly across agents—Sonar DR shows extreme faithfulness drops (-67.4%) while Tongyi DR omits citations entirely—suggesting the underlying causes may be fundamentally different rather than a unified failure mode.

## Confidence

- **High confidence**: The quantitative finding that 16-27% of previously correct content degrades during revision is robust, supported by consistent measurements across multiple agents and feedback types.
- **Medium confidence**: The claim that simple inference-time fixes are insufficient is supported by experimental results, though the paper does not test more sophisticated preservation mechanisms that might address the core issues.
- **Low confidence**: The specific mechanism attributions (e.g., "over-editing due to narrow feedback focus") are plausible but not directly validated; the paper demonstrates what happens but not precisely why each agent fails in its particular way.

## Next Checks

1. **Preservation mechanism ablation**: Implement an explicit content preservation module that maintains a separate edit buffer and only modifies regions explicitly referenced in feedback, then measure whether break rates decrease below 10%.

2. **Citation re-grounding verification**: After each revision, automatically re-crawl and verify that all existing citations still support their claims, isolating whether citation degradation stems from claim removal vs. citation removal vs. unsupported new claims.

3. **Memory state tracking experiment**: Implement an external checklist satisfaction tracker that explicitly maintains which criteria were satisfied in previous turns, then measure whether all-history incorporation rates improve beyond the current 66% at Turn 4.