---
ver: rpa2
title: 'Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer
  MLPs'
arxiv_id: '2509.23684'
source_url: https://arxiv.org/abs/2509.23684
tags:
- coalitions
- neurons
- coalition
- hedonic
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hedonic Neurons, a game-theoretic framework
  for identifying and tracking synergistic neuron groups (coalitions) in transformer
  MLP layers. The authors model neurons as agents in a hedonic game, where utilities
  are based on pairwise synergies, and apply the PAC-Top-Cover algorithm to find stable
  coalitions whose joint ablation has non-additive effects.
---

# Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs

## Quick Facts
- **arXiv ID:** 2509.23684
- **Source URL:** https://arxiv.org/abs/2509.23684
- **Reference count:** 37
- **Primary result:** Introduces a game-theoretic framework for identifying synergistic neuron groups in transformer MLPs via hedonic games, showing these groups exhibit non-additive ablation effects and distinct cross-layer dynamics.

## Executive Summary
This paper presents Hedonic Neurons, a novel framework for discovering and tracking stable, synergistic neuron coalitions in transformer MLP layers. By modeling neurons as agents in a hedonic game, where utilities are based on pairwise synergies, the authors apply the PAC-Top-Cover algorithm to identify groups whose joint ablation has non-additive effects on model outputs. Applied to LoRA-tuned LLMs, the method reveals heavy-tailed coalition size distributions and dynamic patterns suggesting deeper MLPs act as feature filters. The framework offers a new perspective on functional organization within transformer architectures.

## Method Summary
The Hedonic Neurons framework identifies stable neuron coalitions by treating neurons as agents in a hedonic game with utilities based on pairwise synergies (measured via OCA or PAS metrics). The PAC-Top-Cover algorithm samples candidate coalitions and finds sink strongly connected components in the preference graph to determine stable groups. These coalitions are then tracked across layers using interaction mass and bipartite matching to classify dynamics as persistence, splitting, merging, or disappearance. The method focuses on MLP layers 7-14 of LoRA-tuned LLMs, using layer-local logits as the target for ablation-based synergy estimation.

## Key Results
- PAC-Top-Cover discovers coalitions with consistently higher pairwise and ratio synergy than clustering baselines across 26/27 model-metric cells
- Coalition sizes follow heavy-tailed Zipfian distributions with a few large macro groups and many small specialists
- Cross-layer tracking shows high vanish (60-75%) and split (20-50%) rates, supporting a feature filtering/refinement hypothesis
- Joint ablation of hedonic coalitions produces non-additive performance degradation on OOD retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1: Non-additive Joint Ablation Effects (Synergy-Based Coalition Detection)
Groups identified by the hedonic game framework exhibit non-additive contributions to model outputs; their joint ablation degrades performance more than the sum of individual ablations. Pairwise synergy scores quantify interaction effects, and the PAC-Top-Cover algorithm converges on groups where each neuron's top-k preferred partners are within the same coalition, forming stable cooperating units.

### Mechanism 2: Layer-Local Logit-Driven Utility Estimation
Using layer-local logits as the target for ablation-based synergy estimation allows discovery of coalitions intrinsically important at the point of formation within each MLP layer. The logit is computed immediately after the layer-MLP (including residual) but before subsequent layers, making the utility directly tied to the coalition's contribution to immediate local computation.

### Mechanism 3: Heavy-Tailed Coalition Size Distribution and Dynamics (Filtering/Refinement Hypothesis)
Coalition sizes follow a heavy-tailed distribution, with most coalitions splitting or vanishing across layers. This pattern supports the hypothesis that deeper MLPs act primarily as feature filters/refiners, with high disappearance rates after layer 12 suggesting less feature creation and more filtering in deeper layers.

## Foundational Learning

**Concept: Hedonic Games & Core Stability**
- Why needed: The paper models neurons as agents in a hedonic game to define "stable" groups; understanding core stability and PAC-approximations is essential to grasp why the algorithm outputs these particular coalitions
- Quick check: Explain, in one sentence, what makes a coalition structure "core stable" and why PAC-stability is used instead

**Concept: Transformer MLP Layer Function & Gating**
- Why needed: The paper hypothesizes MLPs encode/refine features; knowledge of gated MLP architectures (e.g., SwiGLU) is needed to interpret how ablations affect the up/gate/down projections and residual stream
- Quick check: In a SwiGLU-style MLP, which pathway is ablated when "neuron i" is reset to pre-LoRA weights?

**Concept: Pairwise Interaction Effects & Ablation Studies**
- Why needed: The central metric (PAS, synergy scores) relies on measuring second-order interaction effects via ablation; distinguishing marginal from joint effects is critical for evaluating the framework's claims
- Quick check: Write the formula for PAS(i, j) and state what a positive value signifies about the relationship between neurons i and j

## Architecture Onboarding

**Component map:** Pairwise Valuation Module -> Coalition Sampler -> PAC-Top-Cover Engine -> Cross-Layer Tracker

**Critical path:** The accuracy of the Pairwise Valuation Module directly determines the quality of preferences. The PAC-Top-Cover Engine then relies on sufficient samples and proper closure checks to guarantee stability.

**Design tradeoffs:** PAS is more computationally expensive but empirically stronger than OCA. Higher sample counts improve stability guarantees but increase runtime. Focusing on MLP layers 7-14 captures task-specific updates but may miss attention-head contributions.

**Failure signatures:**
1. Degenerate Partitions: Many single-neuron coalitions or one giant coalition, indicating poor preference data or sampling
2. No Correlation with OOD Drop: Coalitions show high synergy but ablation causes no downstream performance change, suggesting layer-local logits are poor proxies
3. Trivial Dynamics: Near 100% persistence or vanishing across all layers, suggesting the tracking thresholds are poorly tuned

**First 3 experiments:**
1. Baseline Synergy Check: Run PAC-Top-Cover with random pairwise valuations. Verify that output coalitions show no significant Pairwise/Ratio synergy and OOD drop comparable to random subsets
2. Valuation Method Ablation: Compare OCA vs. PAS on a single model/task. Measure runtime, intrinsic synergy scores, and extrinsic OOD drop to confirm the reported performance gap
3. Cross-Layer Dynamics Sensitivity: Vary the tracking thresholds and quantify how the percentages of persist/split/vanish events change. Check if the "filtering" pattern is robust to reasonable parameter changes

## Open Questions the Paper Calls Out

**Open Question 1:** Can fractional hedonic games effectively capture polysemantic neurons by allowing single neurons to participate in multiple stable coalitions simultaneously?
- Basis: Current formulation yields disjoint coalitions despite early-layer polysemy; paper proposes extending to overlapping coalitions via fractional hedonic games
- Evidence needed: Implementing fractional utilities and demonstrating that individual neurons form stable, overlapping memberships that correlate with distinct, interpretable features

**Open Question 2:** How does integrating attention heads into the hedonic game framework alter the formation and stability of identified coalitions?
- Basis: Current method omits attention mechanisms; paper suggests future work should "integrate attention heads for joint sub-module analysis"
- Evidence needed: A modified utility function including attention weights that yields coalitions with higher functional importance or semantic alignment than MLP-only groups

**Open Question 3:** Can low-variance estimators reduce the O(n^2) computational cost of ablation-based synergy calculations without sacrificing coalition stability?
- Basis: Paper identifies need to "design low-variance estimators to reduce O(n^2) ablation costs" as necessary for scalability
- Evidence needed: A proposed estimator that runs in linear or sub-quadratic time while maintaining the high Pairwise and Ratio synergy scores reported in results

## Limitations
- Layer-local logit utility estimation may not reliably correlate with final task performance across diverse tasks or model families
- PAC-Top-Cover algorithm sensitivity to hyperparameters and choice set stability can produce degenerate partitions
- All experiments focus on fine-tuned models for information retrieval, limiting generalization to other NLP tasks or modalities

## Confidence

**High Confidence:**
- PAC-Top-Cover can discover coalitions with higher pairwise and ratio synergy than clustering baselines on tested retrieval tasks
- Coalitions exhibit non-additive ablation effects as measured by PAS and OCA synergy metrics
- The hedonic game framework is a valid theoretical approach for modeling neuron cooperation in MLPs

**Medium Confidence:**
- Layer-local logits are a reliable proxy for detecting task-relevant coalitions within MLP layers
- Heavy-tailed coalition size distribution and high vanish/split rates reflect general feature filtering/refinement function
- Cross-layer tracking accurately captures coalition dynamics (persistence, splitting, merging, vanishing)

**Low Confidence:**
- Discovered coalitions are the primary computational units responsible for task performance versus other mechanisms
- Specific coalition dynamics (e.g., spike in vanish rates after layer 12) are fundamental architectural principles versus model/task artifacts

## Next Checks

1. **Cross-Task Generalization:** Apply the hedonic framework to a non-retrieval task (e.g., commonsense reasoning or summarization). Verify if the same coalition size distributions, high synergy scores, and filtering dynamics are observed.

2. **Layer-Local Ablation vs. Final Performance:** For a held-out test set, compare the OOD Drop when ablating coalitions discovered via layer-local logits versus ablations discovered via direct final-layer logit changes. A strong correlation validates the layer-local proxy assumption.

3. **Baselines Ablation Study:** Run the PAC-Top-Cover algorithm with random pairwise valuations (null model). If discovered coalitions still show high synergy or OOD drop, the framework may be overfitting to noise. Confirm that hedonic coalitions significantly outperform this null model on intrinsic and extrinsic metrics.