---
ver: rpa2
title: 'Temporal Conformal Prediction (TCP): A Distribution-Free Statistical and Machine
  Learning Framework for Adaptive Risk Forecasting'
arxiv_id: '2507.05470'
source_url: https://arxiv.org/abs/2507.05470
tags:
- coverage
- prediction
- conformal
- tcp-rm
- intervals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TCP provides a distribution-free, adaptive framework for constructing
  well-calibrated prediction intervals in nonstationary financial time series. It
  combines a modern quantile forecaster with a rolling split-conformal calibration
  layer, and in its TCP-RM variant, adds an online Robbins-Monro offset to steer coverage
  toward the target level.
---

# Temporal Conformal Prediction (TCP): A Distribution-Free Statistical and Machine Learning Framework for Adaptive Risk Forecasting

## Quick Facts
- arXiv ID: 2507.05470
- Source URL: https://arxiv.org/abs/2507.05470
- Reference count: 30
- Primary result: TCP achieves near-nominal 95% coverage with competitive interval sharpness across equities, cryptocurrency, and commodities, outperforming static ML baselines.

## Executive Summary
TCP provides a distribution-free, adaptive framework for constructing well-calibrated prediction intervals in nonstationary financial time series. It combines a modern quantile forecaster with a rolling split-conformal calibration layer, and in its TCP-RM variant, adds an online Robbins-Monro offset to steer coverage toward the target level. Evaluated across equities (S&P 500), cryptocurrency (Bitcoin), and commodities (Gold), TCP achieves near-nominal 95% coverage while maintaining competitive interval sharpness, outperforming static ML baselines (tree-based QR, linear QR) and partially adaptive methods (ACI) that remain under-calibrated. The Robbins-Monro update has negligible effect at default hyperparameters but provides a principled online mechanism for coverage drift correction. Sensitivity analysis confirms robustness to window size and step-size choices. During the March 2020 crash, TCP intervals expand and contract promptly with volatility spikes, with miscoverage events marked visually. TCP delivers distribution-free, time-adaptive intervals that align with backtesting and governance needs in risk management.

## Method Summary
TCP uses rolling window conformal prediction with a quantile regression base model. For each day, it partitions a rolling window into training and calibration sets, fits a gradient-boosted quantile regressor on the training set, computes nonconformity scores on the calibration set, and sets the threshold as the (1-Î±)-quantile of these scores. TCP-RM adds an online Robbins-Monro offset to correct coverage drift. The method evaluates on daily log-returns of S&P 500, Bitcoin, and Gold from Nov 2017-May 2025, using features including lagged returns, rolling volatility, and return transformations.

## Key Results
- TCP achieves near-nominal 95% coverage across all three asset classes while maintaining competitive interval sharpness
- TCP outperforms static ML baselines (tree-based QR, linear QR) and partially adaptive methods (ACI) that remain under-calibrated
- TCP intervals expand and contract promptly with volatility spikes during the March 2020 crash, with miscoverage events clearly marked
- TCP-RM's Robbins-Monro update has negligible effect at default hyperparameters but provides a principled online mechanism for coverage drift correction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rolling split-conformal calibration adapts prediction intervals to local volatility regimes without assuming global distributional properties.
- **Mechanism:** The algorithm partitions a rolling window of size $w$ into a training set $T_t$ and a calibration set $C_t$. A quantile forecaster is fit on $T_t$. Nonconformity scores (residuals relative to predicted quantiles) are computed on $C_t$. The $(1-\alpha)$-quantile of these scores, $C_t$, serves as a dynamic margin added to the forecaster's output. This resets the calibration state at every time step, allowing the interval width to track recent volatility.
- **Core assumption:** Local exchangeability or $\beta$-mixing (Assumption A2) holds within the rolling window, such that recent history approximates the current data distribution.
- **Evidence anchors:**
  - [Section 4.3] Describes TCP leveraging principles relaxed to hold only within a local time window.
  - [Algorithm 1] Formalizes the rolling split, score computation, and threshold definition $C_t = s_{(k)}$.
  - [Corpus] Weak direct evidence; neighbors focus on general conformal frameworks rather than the specific rolling-window financial application.
- **Break condition:** If the time series exhibits sudden, structural breaks that render the window $W_t$ unrepresentative of $t+1$ (violating mixing), calibration may temporarily lag, leading to under-coverage until the window slides past the break point.

### Mechanism 2
- **Claim:** The Robbins-Monro (RM) offset in TCP-RM provides asymptotic coverage correction by treating calibration as a stochastic root-finding problem.
- **Mechanism:** TCP-RM adds a scalar offset $C_{RM,t}$ to the conformal threshold. This offset is updated online via $C_{RM,t+1} = C_{RM,t} + \gamma_t(\mathbb{1}\{r_t \notin [\ell_t, u_t]\} - \alpha)$. If empirical coverage deviates from the target $1-\alpha$, the gradient term nudges the threshold wider (if under-covered) or tighter (if over-covered).
- **Core assumption:** Assumption A3 (monotone coverage in $C$) and A4 (step size decay) ensure convergence to a fixed point where expected coverage equals $1-\alpha$.
- **Evidence anchors:**
  - [Section 4.4] Defines the update rule Eq (1) and theoretical guarantees.
  - [Section 6] Notes that at default hyperparameters, the RM update changes calibration only marginally, but sensitivity analysis confirms robustness.
  - [Corpus] No specific evidence for RM in time series; standard stochastic approximation theory is implied.
- **Break condition:** If the learning rate $\gamma_0$ is set too high relative to the noise variance, the offset $C_{RM}$ may oscillate or diverge, causing unstable interval widths (overshooting/undershooting).

### Mechanism 3
- **Claim:** Gradient-boosted quantile regression provides a "sharp" base estimate by capturing nonlinear dependencies, which the conformal layer subsequently "calibrates."
- **Mechanism:** The base model (e.g., LightGBM) minimizes pinball loss to estimate conditional $\alpha/2$ and $1-\alpha/2$ quantiles. Unlike parametric models (GARCH), this captures complex feature interactions (e.g., lagged returns, volatility). The conformal layer corrects the systematic under-coverage typical of raw ML outputs by adding the data-driven margin $C_t$.
- **Core assumption:** The feature set (lagged returns, rolling volatility) is sufficient to describe the conditional distribution's shape, if not its exact scale.
- **Evidence anchors:**
  - [Section 5.2.1] Details the use of gradient-boosted trees for quantile forecasting.
  - [Section 6] Results show QR yields the sharpest intervals but is materially under-calibrated; TCP adds width to fix this.
  - [Corpus] "Extreme Conformal Prediction" and "Epistemic Uncertainty" papers suggest conformal methods often rely on strong base learners to achieve efficiency.
- **Break condition:** If the feature space becomes uninformative (e.g., "black swan" events driven by exogenous factors not in lags), the base quantiles will be inaccurate, and the conformal layer may over-widen intervals to maintain coverage, sacrificing utility.

## Foundational Learning

- **Concept: Quantile Regression & Pinball Loss**
  - **Why needed here:** TCP does not predict a point estimate but an interval. You must understand that the base model minimizes an asymmetric loss function (pinball) to target specific percentiles (e.g., 5th and 95th), rather than minimizing Mean Squared Error.
  - **Quick check question:** If the model perfectly minimizes pinball loss for $\tau=0.05$, what proportion of true values should fall below the prediction?

- **Concept: Exchangeability vs. Mixing in Time Series**
  - **Why needed here:** Standard conformal prediction requires data to be exchangeable (permutable). Financial time series violate this (today depends on yesterday). You need to understand why TCP relaxes this to "local mixing" (recent data is approximately exchangeable) to justify the rolling window.
  - **Quick check question:** Why would a global conformal threshold (calculated on all history) fail for a time series with shifting volatility (e.g., 2008 vs. 2017)?

- **Concept: Stochastic Approximation (Robbins-Monro)**
  - **Why needed here:** The TCP-RM variant adjusts the interval width iteratively. You need to grasp that this is a root-finding algorithm where the "gradient" is the noisy signal of whether a return fell inside or outside the interval.
  - **Quick check question:** In the update equation $C_{t+1} = C_t + \gamma_t e_t$, why must the step size $\gamma_t$ decay to zero for convergence?

## Architecture Onboarding

- **Component map:** Data Ingest -> Base Learner (LightGBM Quantile) -> Calibration Engine -> (TCP-RM: Adaptive Layer) -> Output
- **Critical path:** The split between Training ($T_t$) and Calibration ($C_t$). If data leaks between these (e.g., fitting the base model on the calibration set), the nonconformity scores will be artificially small, leading to overconfident (narrow) intervals and failed coverage.
- **Design tradeoffs:**
  - **Window Size ($w$):** Larger $w$ increases statistical stability of the threshold but reacts slower to regime changes (volatility spikes). Smaller $w$ adapts faster but has higher variance.
  - **TCP vs. TCP-RM:** TCP is simpler and robust to defaults. TCP-RM offers theoretical drift correction but introduces a hyperparameter ($\gamma_0$) that can cause instability if misconfigured.
- **Failure signatures:**
  - **Persistent Under-coverage:** Likely a leak in the train/calibration split or non-stationarity exceeding the window's adaptability.
  - **Exploding Intervals (TCP-RM):** Learning rate $\gamma_0$ is too high; the threshold spirals upward after a cluster of misses.
  - **Constant Width:** Base features may be insufficient, forcing the conformal layer to rely entirely on the marginal distribution.
- **First 3 experiments:**
  1. **Coverage Validity Check:** Replicate the Kupiec and Christoffersen backtests on S&P 500 data. Verify that TCP yields p-values > 0.05 (non-rejection of null) while raw Quantile Regression fails.
  2. **Hyperparameter Stress Test:** Run TCP-RM with aggressive step sizes ($\gamma_0=0.1$) on the March 2020 window. Observe if the adaptive offset stabilizes or oscillates wildly compared to the default ($\gamma_0=0.01$).
  3. **Ablation Study:** Remove the rolling volatility feature from the input set. Measure the increase in interval width (loss of sharpness) to quantify the value of the base learner's feature engineering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the TCP framework be extended to multivariate settings for portfolio risk management?
- Basis in paper: [explicit] The conclusion explicitly states it is "natural to develop multivariate and portfolio versions of TCP."
- Why unresolved: The current study strictly evaluates univariate time series (single assets) and does not model cross-asset dependencies or joint prediction regions.
- What evidence would resolve it: A modified conformal mechanism that produces valid joint prediction regions for a portfolio of assets, likely requiring adjustments to the exchangeability assumptions to handle cross-sectional dependence.

### Open Question 2
- Question: Can TCP be modified to guarantee coverage conditional on specific market regimes, such as high volatility states?
- Basis in paper: [explicit] The authors list targeting "conditional or localized coverage (for example by volatility state)" as a primary direction for future modeling work.
- Why unresolved: Standard TCP ensures marginal (time-averaged) coverage, which may theoretically mask temporary under-coverage during distinct volatility regimes or crisis periods.
- What evidence would resolve it: A TCP variant that formally satisfies conditional coverage guarantees (passing conditional independence tests) across defined volatility clusters, rather than just marginal coverage.

### Open Question 3
- Question: Do Extreme Value Theory (EVT) inspired or asymmetric nonconformity scores improve TCP robustness during heavy-tailed market regimes?
- Basis in paper: [explicit] The paper suggests studying "heavy-tail robustness through EVT-inspired or asymmetric nonconformity scores" to address limitations of the current symmetric scores.
- Why unresolved: The current implementation uses a symmetric "max" nonconformity score, which may be suboptimal for the asymmetric, fat-tailed distributions typical of financial crashes (e.g., distinct left-tail risk).
- What evidence would resolve it: Empirical results showing that an EVT-based score yields sharper (narrower) intervals with valid coverage during crisis periods compared to the standard score used in the paper.

## Limitations

- **Window Size Sensitivity:** TCP performance depends critically on the rolling window $w=252$, with an implicit tension between statistical stability and local exchangeability that is not fully resolved.
- **Base Model Dependence:** TCP's sharpness is downstream of the base quantile forecaster's feature learning; insufficient features force the conformal layer to widen intervals, sacrificing utility.
- **Robbins-Monro Convergence:** TCP-RM's practical benefit is modest at default hyperparameters, and its convergence properties in non-stationary financial series are inferred from standard stochastic approximation theory rather than direct empirical validation.

## Confidence

- **High Confidence:** The core TCP mechanism (rolling split-conformal calibration) is well-defined and theoretically grounded in Assumption A2. The empirical results (near-nominal 95% coverage, improved sharpness over QR baselines) are robust across three asset classes. The distinction between TCP and TCP-RM is clearly articulated.
- **Medium Confidence:** The Robbins-Monro adaptation's practical benefit is modest at stated hyperparameters, and its convergence properties in non-stationary financial series are inferred from standard stochastic approximation theory rather than direct empirical validation. The paper does not quantify the long-run cost of potential oscillations.
- **Low Confidence:** The paper does not address the computational cost of retraining the base model daily, nor does it benchmark against deep learning quantile forecasters (e.g., QRNNs) that might capture richer temporal dependencies. The choice of 20-day rolling volatility as a feature is not justified beyond convention.

## Next Checks

1. **Coverage Robustness Across Regimes:** Replicate the TCP-RM update with aggressive step sizes ($\gamma_0=0.1$) during both calm (e.g., 2019) and volatile (e.g., March 2020) windows. Measure whether the adaptive offset stabilizes coverage in drifting regimes without causing oscillations in stable ones.

2. **Feature Ablation Study:** Systematically remove each feature (lagged returns, rolling volatility, $r_{t-1}^2$, sign) from the input set and measure the degradation in interval sharpness and coverage. Quantify the marginal contribution of the rolling volatility feature, which is unique to the financial application.

3. **Long-Horizon Drift Test:** Extend the evaluation period (e.g., to 2030) and measure TCP's coverage over rolling 5-year windows. Test whether TCP-RM corrects for long-term coverage drift that TCP alone cannot address, or whether both methods degrade similarly.