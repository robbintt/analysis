---
ver: rpa2
title: Probing RLVR training instability through the lens of objective-level hacking
arxiv_id: '2602.01103'
source_url: https://arxiv.org/abs/2602.01103
tags:
- training
- discrepancy
- step
- token
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a principled framework for understanding training
  instability in RLVR through objective-level hacking, where token-level credit misalignment
  creates spurious optimization signals that destabilize training. By analyzing the
  abnormal growth of training-inference discrepancy in MoE models, the authors show
  that this phenomenon stems from biased perturbations in the optimization objective
  rather than transient noise.
---

# Probing RLVR training instability through the lens of objective-level hacking

## Quick Facts
- arXiv ID: 2602.01103
- Source URL: https://arxiv.org/abs/2602.01103
- Reference count: 40
- This work introduces a principled framework for understanding training instability in RLVR through objective-level hacking, where token-level credit misalignment creates spurious optimization signals that destabilize training.

## Executive Summary
This paper identifies a novel mechanism for RLVR training instability called "objective-level hacking," which emerges from token-level credit misalignment in MoE models. The authors demonstrate that biased perturbations to the optimization objective—rather than transient noise—create persistent spurious signals that drive pathological behavior. Through extensive experiments on a 30B MoE model, they show that token-level clipping and injected weight distortions accelerate training-inference discrepancy growth and degrade model performance. The study reveals that variance-based perturbations don't trigger instability, while biased distortions do, establishing a causal link between objective-level hacking and training collapse. These findings provide concrete mechanistic insights into RLVR instability and suggest design principles for more stable algorithms.

## Method Summary
The authors investigate RLVR instability using GRPO/GSPO on a Qwen3-30B-A3B MoE model with the verl framework. Training uses DAPO-Math-17k dataset with AIME24 validation, while vLLM handles inference rollout with potentially lower numerical precision (BF16). The core experimental design monitors training-inference discrepancy through importance weights ρᵢ,ₜ = π_train/π_infer, tracking their standard deviation across training steps. Three key ablation conditions compare vanilla GRPO with token-level clipping, GRPO with TIS correction, and GSPO with sequence-level clipping. The authors also inject controlled weight perturbations (biased vs. variance-only) to test their differential effects on discrepancy growth and model performance.

## Key Results
- Training-inference discrepancy grows abnormally during instability, with early detection enabling prevention of irreversible collapse
- Biased weight distortions trigger discrepancy growth while variance-only perturbations do not, establishing objective-level hacking as the primary instability mechanism
- Token-level clipping accelerates instability by introducing spurious optimization signals through credit misalignment
- A positive feedback loop exists between discrepancy growth and objective-level hacking that leads to irreversible model collapse

## Why This Works (Mechanism)

### Mechanism 1: Objective-Level Hacking via Token-Level Credit Misalignment
- **Claim:** Training instability in RLVR for MoE models arises from biased perturbations to the optimization objective, not merely transient noise or variance.
- **Mechanism:** When training-inference implementation discrepancies or token-level clipping redistribute token weights, they introduce a systematic bias term ∆J(θ) = Σᵢ,ₜ Cov(Xᵢ,ₜ(θ), ρ⁻¹ᵢ,ₜ) into the optimization objective. This covariance term creates exploitable spurious signals that the model optimizes toward, driving pathological behavior. The model learns unintended correlations between importance weights and token probabilities rather than the intended reward signal.
- **Core assumption:** The covariance between token-level optimization targets and distorted weights persists across training steps rather than averaging to zero.
- **Evidence anchors:** [abstract] "objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective"; [section 3.1] "∆J(θ) ≃ Σ Cov_train(Xᵢ,ₜ(θ), ρ⁻¹ᵢ,ₜ)"

### Mechanism 2: Biased vs. Variance-Based Perturbations
- **Claim:** Only biased weight distortions trigger discrepancy growth; variance-only perturbations do not cause instability.
- **Mechanism:** When token weights are perturbed by mean-zero Gaussian noise ξᵢ,ₜ ~ N(1, σ²), the expected additional objective term E[∆Jᵥₐᵣ(θ)] ≈ 0 because independence between ξᵢ,ₜ and Yᵢ,ₜ(θ) causes the perturbation to cancel in expectation. However, probability-correlated weight distortions (e.g., weighting low-probability tokens differently) create persistent bias that accumulates.
- **Core assumption:** The perturbation distribution is independent of the optimization target for variance-only cases; bias creates correlation that survives expectation.
- **Evidence anchors:** [section 5.1] "injecting the token-level weight variance noise does not trigger an abnormal increase in the training-inference discrepancy"; [section 5.1] "biased distortion is the primary driver of discrepancy growth"

### Mechanism 3: Positive Feedback Loop Between Discrepancy and Hacking
- **Claim:** Training collapse is irreversible because training-inference discrepancy and objective-level hacking reinforce each other.
- **Mechanism:** Low-probability tokens exhibit wider dispersion from ρᵢ,ₜ = 1 due to survival bias in sampling. As discrepancy grows, survival bias intensifies, causing ρᵢ,ₜ for low-probability tokens to drift further from 1. This increased distortion amplifies objective-level hacking, which drives further discrepancy growth—a self-reinforcing cycle that eventually collapses the inference pattern.
- **Core assumption:** Sampling from π_infer creates systematic over-representation of tokens with higher π_infer relative to π_train.
- **Evidence anchors:** [section 5.3] "The divergence of ρᵢ,ₜ across different probability ranges further exacerbates objective-level hacking, which in turn drives the continued growth of the discrepancy"

## Foundational Learning

- **Concept: Importance Sampling in Off-Policy RL**
  - **Why needed here:** The paper's core mathematical framework relies on understanding how importance sampling fails when covariance terms don't cancel. Without this foundation, the distinction between bias and variance perturbations is opaque.
  - **Quick check question:** Why does E[f(x) · q(x)/p(x)] = E_p[f(x)] require independence assumptions that may fail in GRPO's group-normalized advantages?

- **Concept: Mixture-of-Experts (MoE) Routing Sensitivity**
  - **Why needed here:** The paper specifically targets MoE instability, noting that MoE inference mechanisms are "inherently more complex and sensitive." Understanding why MoE amplifies these issues is critical for anticipating failure modes.
  - **Quick check question:** How does expert routing during inference differ from training in ways that could amplify token-distribution mismatches?

- **Concept: Clipping in Policy Optimization (PPO/GRPO)**
  - **Why needed here:** The paper shows token-level clipping—a stability mechanism—can paradoxically introduce objective-level hacking. Understanding clipping's intended role clarifies why this is counterintuitive.
  - **Quick check question:** In GRPO, does clipping at the token level versus sequence level change which tokens receive gradient updates, and how might this create credit misalignment?

## Architecture Onboarding

- **Component map:** Rollout engine (vLLM) -> Training backend (Megatron) -> GRPO objective computation -> Gradient update
- **Critical path:** 1. Rollout generation → 2. Token probability extraction (both train and infer modes) → 3. Importance weight calculation → 4. Advantage-weighted objective computation → 5. Gradient update
  - **Vulnerable junction:** Steps 2-4, where numerical precision differences and clipping distort token-level credit assignment
- **Design tradeoffs:**
  - Token-level vs. sequence-level clipping: Token-level provides finer control but introduces credit misalignment risk; sequence-level is more stable for MoE per experimental results
  - Numerical precision in rollout: Lower precision (BF16/FP8) improves throughput but introduces discretization artifacts in probability distributions
  - Importance sampling correction (TIS): Reduces objective bias but adds computational overhead and doesn't fully eliminate discrepancy growth
- **Failure signatures:**
  - **Early indicator:** Training-inference discrepancy (mismatch) begins rising before token entropy or gradient norm anomalies
  - **Mid-stage:** PCC between π_train and π_infer drops; abnormal clustering visible in scatter plots
  - **Collapse:** Token entropy diverges (collapse or surge), validation accuracy degrades irreversibly
- **First 3 experiments:**
  1. **Baseline mismatch monitoring:** Run vanilla GRPO on MoE, log ρᵢ,ₜ standard deviation per step. Confirm discrepancy growth correlates with clip strength.
  2. **TIS ablation:** Compare vanilla GRPO vs. TIS-corrected GRPO on same data. Quantify reduction in discrepancy growth rate and correlation with validation accuracy.
  3. **Controlled bias injection:** Start with stable sequence-level clipping (GSPO), inject probability-correlated weight distortion (δ = 1.2, 2.0) for low-probability tokens. Verify discrepancy growth reappears and measure time-to-collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can token-level clipping strategies be redesigned to prevent objective-level hacking while preserving their stabilization benefits in MoE training?
- **Basis in paper:** [explicit] "This finding provides a new perspective on the stability analysis of RLVR algorithms, suggesting that we need to carefully consider the potential side effects of token-level modulations."
- **Why unresolved:** The paper demonstrates that token-level clipping introduces spurious signals but does not propose alternative designs that avoid this dual effect.
- **What evidence would resolve it:** Demonstration of a modified clipping mechanism that maintains training stability without accelerating training-inference discrepancy growth.

### Open Question 2
- **Question:** What other mechanisms beyond objective-level hacking contribute to training collapse in MoE models under RLVR?
- **Basis in paper:** [explicit] "Therefore, our theoretical explanation does not represent the full picture of model collapse; diverse mechanisms and processes may also contribute to this phenomenon."
- **Why unresolved:** The paper establishes objective-level hacking as one mechanism but acknowledges training instability is a multi-system interaction with potentially multiple causes.
- **What evidence would resolve it:** Systematic ablation studies identifying additional independent causal factors for MoE training instability.

### Open Question 3
- **Question:** How can the positive feedback loop between training-inference discrepancy and objective-level hacking be interrupted once initiated?
- **Basis in paper:** [inferred] The paper demonstrates irreversibility of collapse and identifies a positive feedback loop mechanism, but does not explore intervention strategies after discrepancy growth begins.
- **Why unresolved:** The paper characterizes the feedback loop that leads to irreversible collapse but provides no method to break this cycle mid-training.
- **What evidence would resolve it:** Experiments showing successful recovery of training stability after detecting anomalous discrepancy growth, through algorithmic or architectural interventions.

### Open Question 4
- **Question:** To what extent do the objective-level hacking dynamics observed in MoE models generalize to dense model architectures?
- **Basis in paper:** [inferred] Experiments are conducted exclusively on a 30B MoE model; the paper notes MoE models have "inherently more complex and sensitive inference mechanisms" but does not test dense models.
- **Why unresolved:** The theoretical framework applies broadly, but empirical validation is limited to MoE architectures where sensitivity may amplify effects.
- **What evidence would resolve it:** Comparative experiments on equivalent-sized dense models showing whether discrepancy growth rates and instability patterns differ qualitatively or quantitatively from MoE behavior.

## Limitations
- Model Scale and Generalizability: Findings based exclusively on 30B MoE model, leaving uncertainty about generalization to dense or smaller architectures
- Numerical Precision Artifacts: Exact contribution of precision differences between training and inference backends remains unclear
- Dataset and Task Specificity: Results based on mathematical problem-solving may not generalize to other RLVR domains

## Confidence
- **High Confidence:** Empirical observation that training-inference discrepancy grows abnormally during instability, and that early intervention prevents irreversible collapse
- **Medium Confidence:** Theoretical framing of objective-level hacking as stemming from token-level credit misalignment
- **Low Confidence:** Claim that TIS correction is ineffective because it doesn't eliminate discrepancy growth

## Next Checks
1. **Architecture Ablation Study:** Replicate core experiments on dense models (e.g., LLaMA-7B, LLaMA-70B) to determine whether MoE routing complexity is essential to the instability mechanism or merely amplifies it.
2. **Precision Sensitivity Analysis:** Systematically vary numerical precision in rollout generation (FP16, BF16, FP32) while keeping training precision constant to quantify the contribution of discretization artifacts to discrepancy growth.
3. **Cross-Domain Validation:** Apply the same analysis framework to RLVR tasks with different reward structures (dialogue, code generation, instruction following) to test whether the objective-level hacking mechanism is domain-agnostic or specific to mathematical reasoning tasks.