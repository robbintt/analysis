---
ver: rpa2
title: 'Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel Technique
  using Tensor Data and Bayesian Regression'
arxiv_id: '2506.11627'
source_url: https://arxiv.org/abs/2506.11627
tags:
- skin
- color
- fairness
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to evaluate and mitigate bias in ML
  models dealing with skin color, which is represented as tensor data. Unlike traditional
  approaches that use categorical groupings, the authors convert skin color into probability
  distributions and measure differences using statistical distance.
---

# Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel Technique using Tensor Data and Bayesian Regression

## Quick Facts
- arXiv ID: 2506.11627
- Source URL: https://arxiv.org/abs/2506.11627
- Reference count: 20
- Primary result: Method detects latent bias within traditionally fair groups and reduces correlation between skin color nuances and model performance

## Executive Summary
This paper introduces a novel technique to evaluate and mitigate bias in machine learning models dealing with skin color, using tensor data without requiring categorical annotations. The method converts skin color into probability distributions using Individual Typology Angle (ITA) histograms and measures differences using statistical distance (Wasserstein Distance). A Bayesian regression model predicts performance based on these distances, and a weighted loss function is used to mitigate bias during training. Experiments on three datasets (UTKFace, CelebA, HAM) demonstrate the approach can detect nuanced bias within traditionally fair groups and reduce performance disparities.

## Method Summary
The method extracts skin pixels from images using Dlib landmarks for faces and morphological transformations for lesions, then converts these to ITA probability distributions. It calculates Wasserstein distance between each image and a baseline reference to quantify skin color nuance. A Bayesian polynomial regression is trained on validation data to predict performance from distance metrics. During training, a weighted loss function applies penalties inversely proportional to the predicted performance, mitigating bias by re-weighting samples where skin color is likely to cause errors.

## Key Results
- Method detects latent bias within traditionally "fair" groups that categorical approaches miss
- Achieves fairer predictions without requiring skin color annotations
- Successfully reduces correlation between skin color nuances and model performance across three datasets

## Why This Works (Mechanism)

### Mechanism 1
Representing skin color as a probability distribution of pixel tones (rather than categorical labels) exposes intra-group bias. The architecture converts raw image tensors into probability distributions via ITA histograms, retaining nuance that averaging or categorizing loses. Core assumption: Bias manifests in the specific pixel-level distribution and variance of skin tones present. Evidence anchors: Abstract mentions converting to probability distributions and applying statistical distance measures; Section 3.3 details conversion to CIELab space and ITA scores. Break condition: If skin segmentation fails to isolate true skin pixels, the resulting distribution will be noisy and the fairness signal invalid.

### Mechanism 2
Wasserstein Distance provides a directional metric for "fairness distance" between an individual and a baseline reference. Unlike standard distance metrics, WD captures the geometric shift in probability mass, using the sign of the median difference to determine if skin is lighter or darker. Core assumption: A single baseline image is sufficient to serve as a stable reference point for calculating relative bias across the entire dataset. Evidence anchors: Section 3.3.1 defines Signed Wasserstein Distance; section 3.1.3 states WD is recognized as one of the best approaches for capturing changes in distribution geometry. Break condition: If the baseline image is an outlier (extreme lighting), all relative distances will be skewed, making fairness assessment relative to a non-representative standard.

### Mechanism 3
A Bayesian Regression performance estimator generates sample weights to decorrelate skin tone from prediction error. A Prior Training phase fits a Bayesian polynomial regression to map [Skin Distance] -> [Validation Performance]. During Posterior Training, this estimator predicts expected error for a sample, and the loss function is weighted inversely to this prediction via softmax, penalizing the model more for samples where bias is expected to cause failure. Core assumption: The relationship between skin color distance and performance degradation follows a polynomial function generalizable from validation to training data. Evidence anchors: Section 3.4 states the visualization of observed performance suggests the regression model assumed polynomial features; Section 3.5 modifies Binary Cross Entropy with the penalty derived from the estimator. Break condition: If the validation set is too small or lacks diversity, the Bayesian estimator will have high uncertainty, leading to noisy or unhelpful loss weighting.

## Foundational Learning

**Concept: Wasserstein Distance (Earth Mover's Distance)**
Why needed here: You cannot use simple Euclidean distance to compare histograms (distributions) effectively because it doesn't account for the "shape" of the distribution shift. WD is required to quantify how much "work" it takes to transform one skin tone distribution into another.
Quick check question: Can you explain why two images with the same average skin color pixel might have vastly different Wasserstein distances from a baseline?

**Concept: Individual Typology Angle (ITA)**
Why needed here: This is the specific domain representation used to convert RGB pixels into a standardized skin-tone metric. Understanding this is critical for preprocessing and interpretation of the "distance" axis.
Quick check question: How does mapping RGB pixels to ITA values in CIELab space help isolate skin color from lighting artifacts?

**Concept: Bayesian Regression (Polynomial)**
Why needed here: The mitigation strategy relies on a probabilistic model of performance. Understanding Bayesian regression is necessary to interpret the "uncertainty" of the bias prediction and how the penalty weight is derived.
Quick check question: Why might a Bayesian approach be preferred over a deterministic regression for estimating performance "priors" in this context?

## Architecture Onboarding

**Component map:**
1. Preprocessing: Image -> Skin Segmentation (Dlib/OpenCV) -> Masked Image
2. Representation: Masked Image -> CIELab/ITA Conversion -> ITA Probability Distribution
3. Metric: ITA Dist + Baseline Dist -> Signed Wasserstein Distance
4. Estimation (Prior): Distance + Validation Performance -> Bayesian Polynomial Regressor
5. Mitigation (Posterior): Train Batch -> Predict Performance -> Calc Softmax Penalty -> Weighted Loss

**Critical path:** The Skin Segmentation is the most critical dependency. If landmarks or thresholds are wrong (e.g., including hair or background in the "skin" mask), the ITA distribution will be corrupted, rendering the distance metric and subsequent bias mitigation ineffective.

**Design tradeoffs:**
- Robustness vs. Simplicity: The paper uses heuristic landmarking for faces and morphological transforms for lesions. More complex segmentation models could improve robustness but add computational overhead.
- Single vs. Multi-baseline: The method relies on a single random baseline image. While efficient, this makes the system sensitive to that specific choice of baseline.

**Failure signatures:**
- Flat Estimator: If the Bayesian regression shows high variance or a flat line, it indicates no correlation between skin distance and performance (or the validation set is too small). Do not proceed to Posterior training.
- VGG/HAM Divergence: The paper notes VGG on HAM showed increased negative correlation after mitigation. Monitor for this "backfire" effect where re-weighting amplifies bias on specific model backbones.

**First 3 experiments:**
1. Baseline Sanity Check: Run the Skin Identifier on a subset of images. Visually verify that the "Skin Distribution" aligns with human perception of the image's skin tone nuances.
2. Correlation Detection: Train the Prior model and plot the Bayesian regression curve. Confirm that the model actually captures a trend (e.g., performance drops for darker/lighter distances) before attempting mitigation.
3. Ablation on Penalty Start: The paper varies when the penalty starts (Epoch 16, 17, 1, etc.). Run a sweep on the Penalty Start epoch to find the optimal point where the model has learned general features but has not yet overfit to biased ones.

## Open Questions the Paper Calls Out

**Open Question 1**
Can alternative statistical distance metrics outperform Wasserstein Distance in reducing performance differences across skin tones?
Basis in paper: [explicit] The authors state in Section 6.1 that "it is possible to reduce further performance differences... by investigating various statistical distance methods" beyond Wasserstein Distance.
Why unresolved: The current study restricted its implementation and evaluation solely to the Wasserstein metric.
What evidence would resolve it: Comparative experiments using metrics like KL-divergence or Jensen-Shannon divergence on the UTKFace and HAM datasets to measure correlation reduction.

**Open Question 2**
How can the dependency on pre-existing skin segmentation masks be removed to allow application on datasets like Fitzpatrick17K?
Basis in paper: [inferred] Section 6.2 lists a key limitation: the method "cannot be applied to datasets lacking skin detection methods" or those with tiny skin areas, as it relies on external segmentation.
Why unresolved: The current framework requires specific pixel isolation (via landmarks or segmentation) before converting skin color to probability distributions.
What evidence would resolve it: An extension of the method that integrates robust, automated skin detection capable of handling varied clinical imagery without manual annotation.

**Open Question 3**
Is there a theoretical framework for determining the optimal "penalty start epoch" and "penalty weight" for the weighted loss function?
Basis in paper: [inferred] Table 2 and Section 6 note that the epoch to start penalizing "differs depending on the combination of the model and dataset" and was largely determined heuristically (often around 30% of training).
Why unresolved: The paper provides no automated mechanism or rule of thumb for selecting these hyperparameters for new architectures.
What evidence would resolve it: An ablation study identifying a relationship between model convergence behavior and the optimal timing for introducing the fairness penalty.

## Limitations
- Method relies on a single randomly selected baseline image, introducing sensitivity to outliers
- Cannot be applied to datasets lacking skin detection methods or with tiny skin areas
- Requires external skin segmentation, limiting applicability to certain clinical datasets

## Confidence
- Core mechanisms (Wasserstein distance, Bayesian regression, weighted loss): High
- Implementation details and specific hyperparameters: Medium
- Generalizability to new datasets and architectures: Medium

## Next Checks
1. **Baseline Sensitivity Analysis**: Systematically vary the baseline image and measure the impact on Wasserstein distances and subsequent fairness metrics.
2. **Segmentation Robustness**: Test the skin segmentation algorithm across diverse datasets with varying lighting, backgrounds, and image qualities to ensure ITA distributions are consistently reliable.
3. **Polynomial Degree Optimization**: Experiment with different polynomial degrees in the Bayesian regression to determine the optimal balance between model complexity and generalization.