---
ver: rpa2
title: Convergence of Actor-Critic Learning for Mean Field Games and Mean Field Control
  in Continuous Spaces
arxiv_id: '2511.06812'
source_url: https://arxiv.org/abs/2511.06812
tags:
- distribution
- mean
- learning
- field
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes convergence of deep actor-critic reinforcement\
  \ learning algorithms for mean field games and mean field control in continuous\
  \ state and action spaces with infinite time horizons. The method uses two neural\
  \ networks\u2014one for the policy (actor) and one for the value function (critic)\u2014\
  with relative learning rates distinguishing between MFG and MFC solutions."
---

# Convergence of Actor-Critic Learning for Mean Field Games and Mean Field Control in Continuous Spaces

## Quick Facts
- **arXiv ID:** 2511.06812
- **Source URL:** https://arxiv.org/abs/2511.06812
- **Reference count:** 40
- **Primary result:** Establishes convergence of deep actor-critic RL algorithms for MFG and MFC in continuous spaces using relative learning rates and discretization

## Executive Summary
This paper develops a unified actor-critic reinforcement learning framework that can solve both Mean Field Games (competitive equilibrium) and Mean Field Control (social optimum) problems in continuous state and action spaces. The key innovation is using a single architecture with different relative learning rates to distinguish between MFG and MFC solutions. The method introduces discretization via binning for MFC to rigorously identify the limit, and proves convergence using a generalization of the two-timescale stochastic approximation framework. Numerical experiments on linear-quadratic problems demonstrate the algorithm accurately recovers analytical solutions.

## Method Summary
The method uses two neural networks—actor (policy) and critic (value function)—with relative learning rates distinguishing between MFG and MFC solutions. For MFG, the mean field distribution updates slowly while actor-critic updates quickly, treating population as quasi-static to find Nash equilibrium. For MFC, mean field updates quickly relative to actor, allowing optimization over adaptive population distribution. MFC introduces discretization of state/action spaces into bins, maintaining separate population distributions and critic paths for each bin. Convergence is proven using three-timescale stochastic approximation, showing learned policies and distributions converge to optimal solutions.

## Key Results
- Single actor-critic architecture can solve both MFG (competitive) and MFC (cooperative) problems by tuning value-function to mean-field learning rate ratios
- Introduces binning discretization for MFC that achieves significantly better population distribution identification compared to prior work
- Numerical experiments on linear-quadratic problems in 1D and 2D accurately recover analytical solutions
- Method extends to mean field control games with cooperative groups and competitive interactions

## Why This Works (Mechanism)

### Mechanism 1: Relative Learning Rate Distinguishability
The algorithm utilizes a three-timescale stochastic approximation framework. For MFG, mean field distribution updates slowly while actor-critic updates quickly, treating population distribution as quasi-static to find Nash equilibrium. For MFC, mean field updates quickly relative to actor, allowing policy to optimize over fully adaptive population distribution. The strict ordering of timescales ($\rho_\mu \ll \rho_\Pi \ll \rho_V$ for MFG vs $\rho_\Pi \ll \rho_V \ll \rho_\mu$ for MFC) is required to track respective ODEs.

### Mechanism 2: Discretization of Continuous Spaces (Binning)
MFC algorithm partitions continuous space into bins, maintaining separate population distribution and critic path for each bin, forcing action to bin's midpoint when state falls within it. This approximates continuous McKean-Vlasov control problem with finite set of controlled dynamics. The discretization resolution must be sufficient to approximate continuous optimal policy.

### Mechanism 3: Linear Parameterization and Ergodicity
Convergence of value function approximation guaranteed under linear parameterization of critic. The critic is linear combination of basis functions ($V_\theta(x) = \sum \phi_k(x)^\top \theta$), ensuring Mean Squared Bellman Error objective has unique global minimum that stochastic gradient descent can track. Proof relies on linear independence of basis functions and Dobrushin condition for ergodicity.

## Foundational Learning

- **Concept:** **Two-Timescale Stochastic Approximation**
  - **Why needed here:** Mathematical engine of the paper; without understanding one process must converge "infinitely faster" than another, logic of switching between MFG and MFC modes is opaque
  - **Quick check question:** If $\rho_\mu$ (mean field rate) is larger than $\rho_V$ (critic rate), does the algorithm see a static or dynamic population during value function updates?

- **Concept:** **McKean-Vlasov (MKV) Dynamics**
  - **Why needed here:** MFC problems involve MKV dynamics where evolution of state depends on distribution of state itself, distinguishing MFC from standard control
  - **Quick check question:** In standard control, $X_{t+1}$ depends on $X_t$ and $A_t$. In MKV dynamics, what third term does $X_{t+1}$ depend on?

- **Concept:** **Nash Equilibrium vs. Social Optimum**
  - **Why needed here:** Paper unifies these two distinct game-theoretic solutions; must understand MFG seeks fixed point where no individual benefits from deviating (Nash), while MFC seeks to minimize total cost (Social Optimum)
  - **Quick check question:** Does the MFC solution minimize the cost for a representative agent or the average cost for entire population?

## Architecture Onboarding

- **Component map:**
  - Actor Network ($\Pi_\psi$) -> Maps state $x \to$ action distribution (Gaussian)
  - Critic Network ($V_\theta$) -> Maps state $x \to$ scalar value (Linear basis functions in proof, Feedforward NN in experiments)
  - Mean Field Vector ($\mu$) -> Empirical distribution stored as weighted list (or binned histograms for MFC)
  - Environment -> Oracle providing rewards $f$ and next states $p(x'|x,\mu,a)$

- **Critical path:**
  1. Sample: Draw action $A_n$ from Actor; observe reward and next state
  2. TD Update: Compute TD error $\delta_n$ and update Critic parameters $\theta$
  3. Actor Update: Use $\delta_n$ as advantage estimate to update Actor parameters $\psi$
  4. Distribution Update: Update $\mu$ using $\mu_{n+1} = \mu_n - \rho_n(\mu_n - \delta_{X_{n+1}})$
  5. Rate Check: Ensure $\rho$ ratios adhere to MFG (slow $\mu$) or MFC (fast $\mu$) regime

- **Design tradeoffs:**
  - Binning Granularity: Higher bin count improves MFC accuracy but linearly increases number of parallel critics/paths to manage
  - Linear vs. Non-Linear Critic: Theory requires linear features for rigorous convergence, experiments use tanh-based NNs for better representation power at cost of theoretical guarantees
  - Learning Rate Ratios: Ratio $\rho_\mu / \rho_\Pi$ is "knob" to switch modes, but tuning these rates to satisfy Robbins-Monro while maintaining separation can be unstable

- **Failure signatures:**
  - Mode Collapse: Algorithm converges to MFC solution when aiming for MFG (or vice versa), likely due to incorrect learning rate ratios
  - Oscillating Distribution: In MFG, if $\rho_\mu$ is too high, distribution $\mu$ chases policy too aggressively, preventing critic from settling
  - Binning Saturation: In MFC, if bins are too small, samples per bin drop, leading to high variance in specific critic updates

- **First 3 experiments:**
  1. LQ Benchmark Reproduction: Implement Linear-Quadratic example from Section 7.1.1. Plot learned distribution against analytic Gaussian to verify MFG/MFC switch works
  2. Learning Rate Ablation: Fix problem configuration. Sweep ratio $\rho_\mu / \rho_\Pi$ from $10^{-5}$ to $10^{-1}$. Observe transition of solution from Nash (MFG) to Social Optimum (MFC)
  3. Binning Sensitivity (2D): Run 2D MFC experiment (Section 7.2) with varying grid sizes ($3\times3$ vs $9\times4$ bins). Quantify error in population mean ($e_\mu$) against computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence of the unified actor-critic framework be rigorously extended to finite-horizon mean field game and control settings?
- **Basis in paper:** [explicit] The conclusion states: "Promising directions for future research include extending the method to finite-horizon settings."
- **Why unresolved:** Current theoretical proofs rely heavily on infinite-horizon assumptions, specifically existence of stationary distributions and ergodicity, which do not translate directly to time-dependent finite horizon problems
- **What evidence would resolve it:** A convergence proof (similar to Theorems 5.10 and 5.16) or algorithm modification that handles time-dependent value functions and policies without relying on asymptotic stationary distributions

### Open Question 2
- **Question:** Can the theoretical convergence guarantees be extended to non-linear (deep) neural network function approximation?
- **Basis in paper:** [inferred] While title and numerical experiments utilize "deep" actor-critic methods, theoretical convergence proof in Section 5.1.3 explicitly restricts value function parametrization to linear form
- **Why unresolved:** Proof relies on linear parametrization properties to ensure existence of Globally Asymptotically Stable Equilibrium (GASE) for critic ODE; extending this to non-linear approximators introduces non-convexity not covered by current theorems
- **What evidence would resolve it:** Theoretical analysis incorporating approximation errors for over-parameterized neural networks or proof of convergence for non-linear feature maps under specific regularity conditions

### Open Question 3
- **Question:** How does the MFC algorithm's discretization (binning) strategy scale with dimensionality of state and action spaces?
- **Basis in paper:** [inferred] Section 4.2 introduces partition of state and action spaces into bins to identify MFC limit, but numerical experiments in Section 7 are limited to 1D and 2D linear-quadratic examples
- **Why unresolved:** Binning strategy suffers from curse of dimensionality; as number of bins grows exponentially with dimensions, method may become computationally intractable or statistically inefficient in high-dimensional settings
- **What evidence would resolve it:** Numerical experiments demonstrating algorithm's performance and sample efficiency on benchmarks with state/action dimensions significantly greater than 2, or theoretical analysis on discretization error relative to dimension

## Limitations

- Theoretical convergence proofs rely on linear function approximation while experiments use non-linear neural networks without clear justification for this gap
- Robbins-Monro conditions required for stochastic approximation are stated but not empirically verified in experimental section
- Binning discretization in MFC introduces approximation errors not quantified beyond reported metrics
- Method's scalability to higher dimensions remains untested due to exponential growth in number of bins

## Confidence

- **High confidence:** Mechanism linking learning rate ratios to solution type (MFG vs MFC) is well-grounded in two-timescale framework and supported by both theory and experiments
- **Medium confidence:** Numerical results accurately demonstrate algorithm's ability to recover known solutions for linear-quadratic problems, though linear critic assumption in theory vs neural network critic in practice creates uncertainty
- **Low confidence:** Claims about method's ability to handle competitive-cooperative mixed game structures are based on single example without systematic validation of general approach

## Next Checks

1. Verify Robbins-Monro conditions empirically during training by monitoring learning rate schedules and parameter changes
2. Conduct systematic ablation studies on bin resolution in 2D MFC problems to quantify tradeoff between discretization error and computational cost
3. Test algorithm on non-linear problems with known solutions to validate that neural network function approximation maintains convergence properties outside linear setting