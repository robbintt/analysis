---
ver: rpa2
title: Bridging Domain Gaps between Pretrained Multimodal Models and Recommendations
arxiv_id: '2502.15542'
source_url: https://arxiv.org/abs/2502.15542
tags:
- recommendation
- multimodal
- domain
- pre-trained
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively leveraging pre-trained
  multimodal models for recommendation systems while managing computational costs.
  Existing approaches struggle with domain gaps between pre-training and personalized
  recommendation, leading to performance degradation when using joint training.
---

# Bridging Domain Gaps between Pretrained Multimodal Models and Recommendations

## Quick Facts
- arXiv ID: 2502.15542
- Source URL: https://arxiv.org/abs/2502.15542
- Reference count: 40
- Key outcome: 10.6% average gain in Recall@10 on Amazon datasets using knowledge-guided dual-stage training

## Executive Summary
This paper addresses the challenge of effectively leveraging pre-trained multimodal models for recommendation systems while managing computational costs. Existing approaches struggle with domain gaps between pre-training and personalized recommendation, leading to performance degradation when using joint training. The authors propose PTMRec, a knowledge-guided dual-stage parameter-efficient training framework. In the first stage, pre-trained CLIP features are used to train a lightweight recommendation model; in the second stage, knowledge transfer optimization guides parameter-efficient fine-tuning through in-batch knowledge transfer. Experiments on three Amazon datasets (Baby, Sports, Clothing) show significant performance improvements while training only a fraction of parameters compared to full model fine-tuning.

## Method Summary
The proposed PTMRec framework employs a two-stage training approach to bridge domain gaps between pre-trained multimodal models and recommendation systems. In Stage 1, CLIP features are frozen and used to train a lightweight recommendation model that learns user-item interaction patterns through ID embeddings. In Stage 2, parameter-efficient fine-tuning is performed using knowledge transfer optimization, where in-batch KL divergence aligns modal features to the interaction distributions learned in Stage 1. The framework supports various parameter-efficient tuning methods, with prompt tuning showing superior performance due to lower memory footprint and competitive accuracy.

## Key Results
- Achieves 10.6% average gain in Recall@10 across three Amazon datasets
- Prompt tuning outperforms adapters and LoRA with 3-4x faster training and 5-18x fewer parameters
- Two-stage training prevents domain gap corruption compared to joint training approaches
- Maintains computational efficiency by training only 9M-28M parameters vs 160M-188M for alternatives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Two-stage training decouples recommendation pattern learning from multimodal feature adaptation, preventing domain gap corruption.
- **Mechanism:** Stage 1 trains a lightweight recommendation model using frozen CLIP features, allowing ID embeddings to capture personalized user-item matching patterns without interference from misaligned CLIP objectives. Stage 2 then uses this learned knowledge to guide parameter-efficient CLIP adaptation via prompts.
- **Core assumption:** ID-based interaction patterns learned in Stage 1 contain transferable knowledge that can guide modal feature refinement without requiring full joint optimization.
- **Evidence anchors:** [abstract] "in the first stage, pre-trained CLIP features are used to train a lightweight recommendation model; in the second stage, knowledge transfer optimization guides parameter-efficient fine-tuning"; [section 3.3.2] "joint training with pre-prompt leads to performance degradation, indicating that simple prompts struggle to bridge the domain gap".
- **Break condition:** If Stage 1 recommendation model fails to converge (e.g., sparse interactions), transferred knowledge will be noisy, potentially worsening Stage 2 adaptation.

### Mechanism 2
- **Claim:** In-batch knowledge transfer via KL divergence aligns modal features to user-item interaction distributions without additional computational overhead.
- **Mechanism:** The KL loss uses stopgrad-protected ID embedding similarities as target distributions, then trains modal features to match these distributions. The stopgrad operation prevents gradients from corrupting ID embeddings, avoiding representation collapse.
- **Core assumption:** ID-based collaborative signals capture recommendation-relevant patterns that modal features should emulate, and soft label matching via KL divergence is sufficient for cross-modal transfer.
- **Evidence anchors:** [section 2.4] "the user-item ID interaction distributions learned in the first stage contain core recommendation patterns... effectively transfer these patterns to modal feature learning"; [section 2.4] "stopgrad operation prevents gradients from flowing through ID features, thus maintaining a stable target distribution and avoiding representation collapse".
- **Break condition:** If batch size is too small, in-batch negative sampling provides insufficient contrastive signal; if too large, memory constraints limit practical deployment.

### Mechanism 3
- **Claim:** Prompt tuning outperforms adapters and LoRA for multimodal recommendation due to lower memory footprint and competitive accuracy.
- **Mechanism:** Prompts are inserted into early CLIP encoder layers, with modality-specific prompts for visual and text encoders. Only 9M-28M parameters are trained vs 160M-188M for adapters/LoRA.
- **Core assumption:** Modifying transformer attention patterns via prompts is sufficient for domain adaptation without explicitly modulating feed-forward layers.
- **Evidence anchors:** [table 2] Prompt tuning achieves comparable Recall@20 with 8m/epoch vs 20.5m/23.5m and 9M params vs 169M/160M; [section 2.3] "we remove the prompt coupling function for modality alignment since recommendation systems focus more on personalized perception".
- **Break condition:** If prompt length or layer depth is misconfigured, prompts may fail to influence deeper representations, limiting adaptation capacity.

## Foundational Learning

- **Concept: Bayesian Personalized Ranking (BPR) Loss**
  - **Why needed here:** The paper's recommendation models optimize BPR (Eq. 1), which learns to rank interacted items higher than non-interacted items using pairwise comparisons. Understanding this clarifies why joint training with CLIP's InfoNCE objective causes misalignment.
  - **Quick check question:** Given a user u, interacted item i, and non-interacted item j, what does BPR optimize: (a) absolute scores of i and j, or (b) the relative difference f_u(i) - f_u(j)?

- **Concept: InfoNCE / Contrastive Learning**
  - **Why needed here:** CLIP pre-training uses InfoNCE (Eq. 2) for image-text alignment, optimizing for general semantic matching rather than personalized preferences. The domain gap stems from this objective mismatch.
  - **Quick check question:** In InfoNCE, what role does the temperature parameter τ play: (a) controls gradient sharpness, or (b) defines negative sample count?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** The framework compares prompt tuning, LoRA, and adapters. Understanding how each injects learnable parameters into frozen models is essential for architectural decisions.
  - **Quick check question:** Which PEFT method modifies attention weights via low-rank decomposition: (a) prompt tuning, (b) LoRA, or (c) adapters?

## Architecture Onboarding

- **Component map:**
  [CLIP Visual Encoder] + [Visual Prompts p_v] → e_v (visual features)
  [CLIP Text Encoder] + [Text Prompts p_t] → e_t (text features)
  [ID Embeddings] → e_id (user/item ID features)
  [Linear Projection] → dimension alignment for e_v, e_t
  [Recommendation Model] → VBPR/Freedom/MGCN/SMORE backbone
  [Knowledge Transfer Module] → KL divergence loss (Eq. 13)

- **Critical path:**
  1. Stage 1: Freeze CLIP, train recommendation model (ID + frozen modal features) with BPR loss
  2. Stage 2: Initialize prompts, load Stage 1 ID embeddings, train prompts + linear projections with L_KT + task loss
  3. Inference: Use prompt-enhanced CLIP features with trained recommendation model

- **Design tradeoffs:**
  - Prompt depth (i layers): More layers = stronger adaptation but higher memory; paper uses first i layers (configurable, MaPLe default ~2-3)
  - Batch size: Stage 1 uses 2048 for efficiency; Stage 2 uses 128 with gradient accumulation 12 to fit memory while maintaining effective batch size
  - Prompt vs LoRA/Adapter: Prompts chosen for 3-4x faster training and 5-18x fewer parameters with comparable accuracy

- **Failure signatures:**
  - Stage 1 underfitting: Low Recall@10 on validation → check ID embedding learning rate, increase epochs
  - Stage 2 divergence: KL loss spikes → reduce learning rate, verify stopgrad is applied correctly
  - Representation collapse: All modal features converge to similar vectors → ensure stopgrad protects ID features, check batch diversity
  - Memory overflow: Reduce batch size or prompt depth, increase gradient accumulation steps

- **First 3 experiments:**
  1. **Baseline sanity check:** Run CLIP-frozen (Stage 1 only) on single dataset (Baby) to verify recommendation model converges; expect ~0.064 Recall@10 (Table 1, Freedom baseline).
  2. **PEFT method comparison:** Test prompt vs adapter vs LoRA on Baby dataset; measure Recall@20, time/epoch, memory; expect prompt to match adapter with ~3x speedup.
  3. **Ablation of knowledge transfer:** Run PTMRec with and without L_KT loss on Sports dataset; expect ~2-3% Recall drop without transfer (Fig. 3 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the knowledge-guided dual-stage strategy be effectively generalized to generative Large Language Models (LLMs) or diffusion-based recommendation backbones, or is it specific to the discriminative CLIP architecture?
- Basis: [inferred] The paper evaluates the framework exclusively using the CLIP model. While the authors claim the framework is "general" and "flexible," the efficacy of transferring ID-based interaction knowledge via KL divergence to generative architectures with vastly different parameter spaces remains unverified.
- Why unresolved: The mechanism relies on aligning frozen CLIP features with ID embeddings. Generative models may have different feature distribution properties that make the "stopgrad" or linear projection assumptions less effective.
- What evidence would resolve it: Experiments applying PTMRec to generative multimodal backbones (e.g., LLaVA or Stable Diffusion features) to observe if the 10.6% performance gain is maintained.

### Open Question 2
- Question: How robust is the Stage 2 knowledge transfer mechanism in strict cold-start scenarios where interaction history is too sparse to form a reliable target distribution?
- Basis: [inferred] The Knowledge Transfer Optimization (Eq. 13) relies on probs_id (target distribution) derived from Stage 1 training on interaction records. The paper does not analyze performance on items/users with zero or extremely few interactions.
- Why unresolved: If the Stage 1 model cannot learn meaningful ID embeddings due to data sparsity, the "knowledge" used to guide the fine-tuning in Stage 2 will be noisy or undefined, potentially degrading the pre-trained model's native capabilities.
- What evidence would resolve it: A breakdown of performance metrics specifically for long-tail or cold-start items comparing PTMRec against a frozen-feature baseline.

### Open Question 3
- Question: Does the removal of the prompt coupling function (present in MaPLe) limit the model's ability to capture cross-modal synergies in domains where visual and textual signals are heavily intertwined?
- Basis: [inferred] The authors state they removed the prompt coupling function because "recommendation systems focus more on personalized perception." However, they do not provide an ablation study comparing coupled vs. decoupled prompts to prove that decoupling is universally better.
- Why unresolved: While decoupling might simplify personalized matching, it ignores the potential that aligning visual and textual prompts could help bridge the domain gap for complex items where the image and text are mutually dependent.
- What evidence would resolve it: An ablation study on the Sports or Clothing datasets specifically testing the "coupled prompt" variant to see if it hurts or helps the Recall/NDCG metrics.

## Limitations

- Narrow experimental scope: Only three Amazon dataset categories tested from similar product domains
- Computational cost comparison focuses on parameter count rather than wall-clock time or energy consumption
- Knowledge transfer mechanism relies on in-batch sampling, which may not scale well to sparse datasets

## Confidence

- **High Confidence:** The two-stage training framework design and its basic implementation are well-specified. The observation that joint training degrades performance while sequential training improves it is directly measurable and supported by ablation experiments.
- **Medium Confidence:** The performance improvements (10.6% Recall@10 gain) are reported with statistical significance from the ablation studies, but the generalizability across domains remains uncertain given the limited dataset diversity.
- **Low Confidence:** The claimed superiority of prompt tuning over other PEFT methods is based on a single dataset comparison without comprehensive hyperparameter tuning across all methods.

## Next Checks

1. **Domain Generalization Test:** Evaluate PTMRec on a non-Amazon dataset (e.g., MovieLens, Yelp) to verify whether the 10.6% average improvement holds across different recommendation domains and interaction patterns.

2. **Knowledge Transfer Robustness:** Systematically vary batch sizes in Stage 2 to determine the minimum batch size required for effective knowledge transfer, and test performance on datasets with varying levels of sparsity to identify break conditions.

3. **PEFT Method Sensitivity:** Conduct a comprehensive ablation comparing prompt tuning, LoRA, and adapters across all three datasets with consistent hyperparameter tuning (learning rates, training epochs, prompt dimensions) to isolate whether the observed performance differences are method-specific or implementation-dependent.