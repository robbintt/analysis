---
ver: rpa2
title: 'DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code
  Generation via Context Poisoning'
arxiv_id: '2601.20615'
source_url: https://arxiv.org/abs/2601.20615
tags:
- code
- generation
- arxiv
- wang
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DrainCode, the first adversarial attack targeting
  computational efficiency of retrieval-augmented code generation systems. By strategically
  poisoning retrieval contexts through a mutation-based approach, DrainCode forces
  LLMs to produce significantly longer outputs, thereby increasing GPU latency and
  energy consumption.
---

# DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning

## Quick Facts
- arXiv ID: 2601.20615
- Source URL: https://arxiv.org/abs/2601.20615
- Reference count: 40
- Key outcome: Introduces the first adversarial attack on RAG code generation systems that increases GPU latency and energy consumption by poisoning retrieval contexts to produce longer outputs while maintaining 95–99% functional accuracy

## Executive Summary
DrainCode introduces a novel adversarial attack targeting the computational efficiency of retrieval-augmented code generation (RAG) systems. By strategically poisoning the retrieval corpus with adversarial triggers, DrainCode forces large language models to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. The attack achieves up to 85% increase in latency, 49% increase in energy consumption, and more than a 3x increase in output length compared to baseline, while maintaining high functional accuracy. DrainCode employs a mutation-based approach that optimizes triggers to suppress the EOS token probability, uses hypothetical query generation for query-agnostic poisoning, and maintains functional correctness through KL-divergence constraints.

## Method Summary
DrainCode generates adversarial triggers through a gradient-guided mutation process that optimizes for three objectives: suppressing EOS token probability to extend output length, maintaining token diversity to avoid detection, and constraining output distribution via KL-divergence to preserve functional correctness. The attack uses a hypothetical query generation mechanism to enable query-agnostic poisoning without requiring knowledge of specific user prompts. Triggers are iteratively mutated across multiple token positions using a buffer pool to improve search efficiency. The poisoned contexts are injected into the retrieval corpus, and when retrieved, cause the LLM to generate verbose but functionally correct code. The attack is evaluated on multiple models including DeepSeek-Coder-7B, CodeQwen1.5-7B, and Llama3-8B across benchmarks like RepoEval and Odex.

## Key Results
- Achieves up to 85% increase in GPU latency and 49% increase in energy consumption
- Increases output length by more than 3x compared to baseline while maintaining 95-99% functional accuracy (Pass@1)
- Maintains effectiveness across different prompting strategies (zero-shot, few-shot, chain-of-thought)
- Demonstrates strong black-box transferability across different model architectures
- Evades both classifier-based and perplexity-based detection methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Delaying the End-of-Sequence (EOS) token emission extends generation length, which directly increases GPU latency and energy consumption.
- **Mechanism:** DrainCode optimizes adversarial triggers to suppress the probability of the EOS token at all generation positions (EOS Loss), preventing the LLM from terminating output. This exploits the autoregressive generation process, which continues until an EOS token is emitted or a preset token limit is reached.
- **Core assumption:** The model's inference cost is primarily driven by output token count, and the target LLM's generation process is sensitive to the triggers embedded in the retrieved context.
- **Evidence anchors:**
  - [abstract]: "DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption."
  - [section IV-B, V-C]: The EOS loss is formally defined as minimizing the probability of the EOS token at all positions.
  - [corpus]: Related work such as "Energy-Latency Attacks: A New Adversarial Threat to Deep Learning" and "Sponge Attacks on Sensing AI" establishes the broader viability of energy-latency attacks on DNNs, supporting the general attack concept but not this specific RAG-based mechanism.
- **Break condition:** If the target LLM enforces a hard output token limit shorter than the attack-induced length, the impact on latency/energy is capped. If the model's termination condition is not governed by the EOS probability, this mechanism fails.

### Mechanism 2
- **Claim:** A hypothetical query construction mechanism enables query-agnostic poisoning, allowing an attack without knowledge of the victim's specific prompt.
- **Mechanism:** DrainCode uses an LLM to generate a plausible "unfinished code" query based on a candidate context from the retrieval corpus. This simulated query is used during trigger mutation, removing the need for the attacker to predefine or access actual user queries. The trigger is optimized to generalize across a range of potential queries related to the poisoned context.
- **Core assumption:** The LLM can generate a hypothetical query that is sufficiently representative of the distribution of real user queries for the attack trigger to generalize.
- **Evidence anchors:**
  - [abstract]: The paper introduces a "hypothetical query construction mechanism" to enable query-agnostic poisoning.
  - [section IV-B1]: Describes the few-shot prompting method for the LLM to generate the hypothetical unfinished code.
  - [corpus]: No direct corpus evidence for this specific query-agnostic mechanism.
- **Break condition:** If the hypothetical query is not representative of real user queries for a given context, the generated trigger may not be activated during a real attack.

### Mechanism 3
- **Claim:** A constraint on the output token distribution (via KL-divergence) preserves functional correctness, making the attack stealthy by avoiding detection via code correctness tests.
- **Mechanism:** During gradient-based trigger optimization, a KL-divergence loss term is minimized. This constraint ensures that for non-trigger-affected token positions, the output distribution under attack remains close to the clean distribution, steering the model to generate verbose but functionally correct code.
- **Core assumption:** The model's output distribution at non-trigger positions is a reliable proxy for overall functional correctness, and a small KL divergence is sufficient to maintain it.
- **Evidence anchors:**
  - [section IV-B2]: Formally defines the KL-divergence constraint used to maintain output distribution stealthiness.
  - [table I]: Pass@1 scores remain high (e.g., 31.9 vs. 33.1 baseline), demonstrating that functional correctness is preserved.
  - [corpus]: No direct corpus evidence for this specific KL-divergence technique in this context.
- **Break condition:** If the KL-divergence constraint is not sufficiently tight, the attack could corrupt the core logic of the generated code, leading to syntax errors or test failures, which would make the attack easily detectable. If it is too tight, it may conflict with the goal of increasing output length.

## Foundational Learning

- **Concept:** Autoregressive Generation & EOS Token
  - **Why needed here:** The core attack vector (Mechanism 1) manipulates the model's generation termination condition. Understanding that an LLM generates one token at a time until a specific stop token is produced is essential.
  - **Quick check question:** In a standard LLM, what is the primary signal that causes the model to stop generating more tokens for a given request?

- **Concept:** RAG (Retrieval-Augmented Generation) Attack Surface
  - **Why needed here:** This entire paper is about a novel attack vector via the retrieval corpus. Understanding the data flow—user query → retriever → retrieval corpus → context → LLM—clarifies where the attacker intervenes (the corpus) and how the payload reaches the target (the context).
  - **Quick check question:** In a RAG system, which component is the attacker targeting for poisoning in this paper, and how does the poisoned data reach the LLM?

- **Concept:** Gradient-Guided Mutation for Adversarial Examples
  - **Why needed here:** The method for creating the attack trigger relies on using gradients from a surrogate model to identify which tokens to swap to maximize the attack's objective (EOS loss, diversity loss). This is a standard technique in adversarial machine learning, applied here to a new domain.
  - **Quick check question:** What is the primary signal used by DrainCode to determine which token in the adversarial trigger should be replaced in each optimization step?

## Architecture Onboarding

- **Component map:** Hypothetical Query Generator -> Trigger Optimizer -> Attack Buffer Pool -> Target RAG System
- **Critical path:** Trigger Optimization Loop. Hypothetical Query Generator creates a query → Trigger Optimizer initializes a trigger and begins the mutation loop. Inside the loop, gradients are computed, candidate tokens are selected, a new trigger is formed, and the loss is evaluated. The best-performing trigger is stored in the Attack Buffer Pool for the next iteration. This loop runs offline to produce the final poisoned context.
- **Design tradeoffs:**
  - **Stealthiness vs. Overhead:** The KL-divergence loss is tuned to balance preserving functional correctness (stealthiness) while allowing for increased output length (overhead). A poorly tuned constraint could break the code (too weak) or fail to increase length (too strong).
  - **Query-Agnosticism vs. Specificity:** The hypothetical query mechanism allows for a general attack but may be less effective than a query-specific attack if the simulated queries are not representative.
  - **Optimization Speed vs. Effectiveness:** The multi-position mutation and attack buffer pool speed up trigger generation but may converge to a local optimum compared to a more exhaustive (but slower) single-point mutation search.
- **Failure signatures:**
  - **Low Latency/Energy Increase:** The attack trigger is ineffective, possibly due to a failure in the trigger optimization (e.g., getting stuck in a local minimum) or poor generalization from the surrogate model.
  - **Drop in Pass@1:** The generated code is functionally incorrect, indicating the KL-divergence constraint was not effective or the trigger corrupted the core logic. This would break stealth.
  - **Attack Detected by Perplexity:** The poisoned context has an unusually high perplexity, which might be detected by a defense system (though the paper claims its method evades this).
- **First 3 experiments:**
  1. **Baseline Attack Validation:** Implement the core trigger optimization loop (Mechanism 1) without the hypothetical query or efficiency improvements. Test on a single LLM (e.g., DeepSeekCoder) with a known query to verify the core ability to delay EOS and increase output length.
  2. **HypoQuery Ablation:** Run the full optimization with the hypothetical query module and compare its effectiveness (length/energy increase) against a baseline that uses an empty or random query. This validates Mechanism 2.
  3. **Functional Correctness Test:** Run the attack on a code generation benchmark (e.g., RepoEval) and measure both Pass@1 and output length. Compare against the baseline to quantify the trade-off between overhead and correctness, validating Mechanism 3.

## Open Questions the Paper Calls Out
- Can advanced detection techniques effectively identify DrainCode attacks by analyzing the generated code output rather than just the retrieval corpus?
- To what extent do larger, proprietary, or mixture-of-experts (MoE) architectures mitigate or amplify the energy-latency impacts of DrainCode?
- What is the full system-level energy cost (CPU and memory) of DrainCode attacks beyond the isolated GPU measurements?

## Limitations
- The attack's effectiveness may degrade when scaled to larger models or different retrieval systems beyond the tested 7-8B parameter models and BM25 retrievers
- The reported stealthiness is validated only through limited defense methods (classifier- and perplexity-based), leaving effectiveness against more sophisticated anomaly detection unknown
- The functional correctness claims are based on static benchmarks rather than live user environments with diverse query distributions

## Confidence
- **High Confidence**: The core claim that strategic context poisoning can increase LLM output length, latency, and energy consumption is well-supported by direct measurements and controlled experiments. The effectiveness of the EOS token suppression mechanism is demonstrated through quantitative metrics.
- **Medium Confidence**: The claim that the attack maintains functional correctness while increasing overhead is plausible given the KL-divergence constraint and empirical Pass@1 scores, but the robustness of this trade-off across diverse query types and models remains uncertain.
- **Low Confidence**: The claim of strong black-box transferability is the least substantiated, as the paper only demonstrates transfer between models of similar size and architecture. The attack's performance on larger models or different retrieval systems is untested.

## Next Checks
1. **Live Environment Test**: Deploy the attack on a live code completion system with real user queries over a week. Measure not only Pass@1 and overhead metrics, but also user experience indicators (e.g., abandonment rates, reported issues) to assess true stealth.

2. **Defense Robustness Test**: Implement and evaluate DrainCode against a broader set of defenses, including runtime monitoring for anomalous token distributions, adaptive decoding strategies, and anomaly detection on retrieved contexts. Report false positive and false negative rates.

3. **Cross-Model Transferability Test**: Systematically test the attack's effectiveness when the surrogate model used for trigger generation differs significantly in size, architecture, or training data from the target model (e.g., using a 7B model to attack a 70B model). Quantify the drop in effectiveness and identify architectural factors that influence transferability.