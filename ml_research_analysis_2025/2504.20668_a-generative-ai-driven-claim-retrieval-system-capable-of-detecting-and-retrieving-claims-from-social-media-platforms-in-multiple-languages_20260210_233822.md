---
ver: rpa2
title: A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving
  Claims from Social Media Platforms in Multiple Languages
arxiv_id: '2504.20668'
source_url: https://arxiv.org/abs/2504.20668
tags:
- fact-checks
- claim
- article
- llms
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multilingual claim retrieval system that
  uses large language models to identify and filter previously fact-checked claims
  from social media. The pipeline retrieves similar fact-checks using multilingual
  embeddings, filters irrelevant ones via LLM explanations, and summarizes relevant
  articles.
---

# A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages

## Quick Facts
- arXiv ID: 2504.20668
- Source URL: https://arxiv.org/abs/2504.20668
- Reference count: 26
- Primary result: Multilingual claim retrieval achieves S@10 scores up to 0.84 using E5 embeddings

## Executive Summary
This paper introduces a multilingual claim retrieval system that uses large language models to identify and filter previously fact-checked claims from social media. The pipeline retrieves similar fact-checks using multilingual embeddings, filters irrelevant ones via LLM explanations, and summarizes relevant articles. Evaluation on 20+ languages shows multilingual E5 embeddings achieve S@10 scores up to 0.84, while Mistral Large best balances precision and recall in filtration (TNR 90.23%, FNR 15.38%). Summarization experiments indicate larger models like Mistral Large and Llama3.3 70B outperform smaller variants, especially when articles precede instructions. The system also predicts claim veracity using retrieved fact-checks and summaries, with Mistral Large achieving the highest macro F1 of 63.05%. A web-based tool integrates the pipeline for practical use by fact-checkers.

## Method Summary
The system implements a multi-stage pipeline: (1) retrieves top-K fact-checks using Multilingual E5 Large embeddings with cosine similarity, (2) filters candidates with LLM-based relevance assessment providing explanations, (3) summarizes fact-check articles using LLM with article-first prompt ordering, and (4) predicts veracity by synthesizing retrieved evidence. The approach is evaluated on the MultiClaim dataset (206K fact-checks, 28K posts, 31K pairs across 39 languages) and AFP-Sum dataset (~19K articles with summaries in 23 languages), using metrics including S@10, Macro F1, ROUGE-L, and BERTScore.

## Key Results
- Multilingual E5 Large embeddings achieve S@10 of 0.84 across 20+ languages
- Mistral Large achieves optimal filtration trade-off (TNR 90.23%, FNR 15.38%)
- Larger LLMs (Mistral Large, Llama3.3 70B) significantly outperform smaller models in summarization quality
- Veracity prediction improves from 26.53% to 63.05% macro F1 when using retrieved information

## Why This Works (Mechanism)

### Mechanism 1: Semantic Alignment via Multilingual Embeddings
Multilingual E5 Large embeddings create language-agnostic semantic representations that enable cross-lingual claim matching. The TEM encodes both input claims and fact-check database entries into a shared dense vector space where cosine similarity approximates semantic equivalence. Multilingual E5 Large achieved S@10 of 0.84 across 20 languages, outperforming English-only TEMs on translated text. Core assumption: Claims with similar meaning across languages map to proximate regions in the embedding space, even with syntactic variation. Break condition: Embedding quality degrades for low-resource languages with limited pretraining data (e.g., Thai) or when claims undergo adversarial edits.

### Mechanism 2: LLM-Based Relevance Filtering with Instruction Following
Large language models can filter retrieved fact-checks by assessing semantic entailment between input claims and candidates, reducing false positives. After TEM retrieval of top-50 candidates, the LLM is prompted to identify directly relevant fact-checks and provide explanations. Mistral Large achieved TNR 90.23% (correctly filtering irrelevant items) while maintaining FNR 15.38% (incorrectly filtering relevant items). Core assumption: LLMs possess sufficient multilingual reasoning to distinguish semantic implication from topical overlap. Break condition: Smaller models show higher FNR (21.16%), and instruction-following failures occur when prompts position articles after instructions.

### Mechanism 3: Retrieval-Augmented Veracity Inference
Providing LLMs with retrieved fact-checks, summaries, and ratings improves veracity prediction over zero-shot baselines. The pipeline aggregates retrieved evidence as structured context for the LLM to infer veracity. Mistral Large with retrieved information achieved macro F1 of 63.05% versus 26.53% without retrieval. Core assumption: Retrieved fact-checks accurately represent the ground truth and the LLM can synthesize potentially conflicting evidence. Break condition: Error analysis shows 27% of failures stem from missing context in claims, and 25% of posts had no ground-truth relevant fact-checks retrieved.

## Foundational Learning

- **Concept: Dense Retrieval and Semantic Similarity**
  - Why needed here: The pipeline's first stage relies entirely on embedding-based retrieval; understanding how TEMs encode semantic meaning into vectors is essential for diagnosing retrieval failures.
  - Quick check question: Why would two claims with different words but similar meaning cluster in embedding space?

- **Concept: Instruction Following and Prompt Design**
  - Why needed here: LLM performance varies dramatically with prompt structure (Article first vs. Article last); smaller models fail when instructions precede content.
  - Quick check question: What might cause an LLM to generate summaries in the wrong language when instructions are positioned after content?

- **Concept: Precision-Recall Trade-offs in Filtration**
  - Why needed here: The filtration stage must balance removing irrelevant fact-checks (high TNR) against preserving relevant ones (low FNR); different models optimize differently.
  - Quick check question: If TNR is 90% and FNR is 15%, what proportion of retrieved items are incorrectly kept versus incorrectly discarded?

## Architecture Onboarding

- **Component map:** Vector Database (Milvus) -> Retrieval Module (TEM-based cosine similarity) -> Filtration Module (LLM relevance assessment) -> Summarization Module (LLM with article-first prompts) -> Veracity Predictor (LLM with retrieved context)

- **Critical path:** Retrieval quality (S@10) determines upper bound for all downstream tasks; filtration errors cascade to veracity prediction (25% of posts missing relevant fact-checks in retrieved context).

- **Design tradeoffs:** Larger LLMs improve summarization and veracity but increase latency and cost; "Article first" prompt ordering improves summarization quality but requires longer context windows; multilingual TEMs reduce translation overhead but underperform on certain languages (Thai) compared to English TEMs with translation.

- **Failure signatures:** Retrieval gaps (low S@10 for specific languages indicates embedding coverage issues); Filtration over-pruning (high FNR suggests model cannot distinguish semantic implication from topical similarity); Instruction-following failures (summaries generated in original language instead of English); Missing context errors (veracity prediction fails when claims lack sufficient detail).

- **First 3 experiments:** 1) Baseline retrieval benchmark: Evaluate Multilingual E5 Large S@10 on target language distribution against BM25 and English TEMs with translation; 2) Filtration precision-recall calibration: Test LLM filtration on sampled set (10-50 posts per language) with manual relevance annotations; 3) Prompt ordering A/B test: Compare "Article first" vs. "Article last" summarization on language mix to quantify instruction-following failure rates.

## Open Questions the Paper Calls Out

- **Question:** How does the system perform when evaluated by professional fact-checkers rather than journalism students, and does the tool's usability and effectiveness differ across user expertise levels?
  - Basis: Authors state "professional fact-checkers would have been more appropriate evaluators" and "consider evaluation with professional fact-checkers as an important direction for future work."
  - Why unresolved: Human evaluation included only six participants (five journalism students, one academic) due to time constraints and limited availability of professionals.
  - What evidence would resolve it: A comparative user study with professional fact-checkers using standard usability metrics and task completion assessments.

- **Question:** Can criteria-based retrieval using TEMs be improved to handle natural language filtering instructions, particularly for temporal constraints like date ranges?
  - Basis: Authors report that TEMs "struggled with natural language instructions, especially when filtering by date range" with negative Spearman correlations.
  - Why unresolved: The embedding approach encodes exact dates rather than date ranges, creating a fundamental mismatch between query intent and representation.
  - What evidence would resolve it: Experiments with alternative representations (e.g., date range tokenization, metadata-aware embeddings) and evaluation on temporal filtering tasks.

- **Question:** How can the filtration step balance the precision-recall trade-off to minimize false negatives while maintaining high true negative rates?
  - Basis: Authors note "the trade-off between precision and recall remains challenging, as some useful fact-checks may be excluded" and report FNR of 15.38% for the best model.
  - Why unresolved: Current LLM-based filtration reduces irrelevant results but sacrifices recall, which is problematic for fact-checkers who need comprehensive coverage.
  - What evidence would resolve it: Experiments with confidence thresholds, multi-stage filtration, or ensemble approaches that jointly optimize TNR and FNR.

## Limitations

- Multilingual TEMs underperform on low-resource languages like Thai where embedding coverage remains incomplete
- Filtration mechanism sacrifices recall (15.38% FNR) to achieve high precision, potentially excluding relevant fact-checks
- Veracity prediction fails when relevant fact-checks are missing from retrieved context (25-49% of test cases)

## Confidence

- **High confidence:** English and high-resource language retrieval (S@10 0.84), LLM-based filtration (TNR 90.23%)
- **Medium confidence:** Multilingual retrieval generalization, summarization improvements, veracity prediction gains
- **Low confidence:** Low-resource language performance, external validation beyond MultiClaim dataset

## Next Checks

1. **Cross-dataset validation:** Test the pipeline on a separate fact-checking corpus to assess generalization beyond MultiClaim
2. **Low-resource language audit:** Manually evaluate retrieval and filtration performance on Thai and other underrepresented languages to identify embedding quality gaps
3. **Latency-cost analysis:** Measure inference time and token costs for Mistral Large across the full pipeline to quantify real-world deployment constraints