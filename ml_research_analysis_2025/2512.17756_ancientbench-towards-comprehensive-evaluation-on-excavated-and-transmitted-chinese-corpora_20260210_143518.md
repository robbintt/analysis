---
ver: rpa2
title: 'AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted
  Chinese Corpora'
arxiv_id: '2512.17756'
source_url: https://arxiv.org/abs/2512.17756
tags:
- ancient
- chinese
- comprehension
- language
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AncientBench addresses the gap in evaluating large language models\
  \ on excavated Chinese documents, which are historically authentic but difficult\
  \ to process. The benchmark introduces four competencies\u2014glyph, pronunciation,\
  \ meaning, and contextual comprehension\u2014across ten tasks, using a standardized\
  \ multiple-choice format with 28,707 questions."
---

# AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora

## Quick Facts
- arXiv ID: 2512.17756
- Source URL: https://arxiv.org/abs/2512.17756
- Reference count: 7
- Best model accuracy: 51.00% (Qwen-14B-Chat)

## Executive Summary
AncientBench addresses the gap in evaluating large language models on excavated Chinese documents, which are historically authentic but difficult to process. The benchmark introduces four competencies—glyph, pronunciation, meaning, and contextual comprehension—across ten tasks, using a standardized multiple-choice format with 28,707 questions. Experimental evaluation shows human performance significantly exceeds current models: humans achieve 55.13% average accuracy, while the best model (Qwen-14B-Chat) reaches only 51.00%. Models perform poorly on glyph and pronunciation comprehension (around 30%), but better on meaning and contextual tasks (up to 69.71%). Fine-tuning an ancient model improves performance, but gaps remain. AncientBench thus highlights the need for better ancient Chinese language understanding in LLMs.

## Method Summary
The AncientBench benchmark is constructed through a three-stage digitization pipeline that processes ancient Chinese characters lacking standardized Unicode encoding. The pipeline extracts radical features and spatial relationships from character images to create a knowledge graph, applies unified font encoding with deduplication to create a character-encoding table, and assigns new encoding to previously unrecorded characters. The benchmark comprises 28,707 multiple-choice questions across ten tasks evaluating four competencies: glyph, pronunciation, meaning, and contextual comprehension. Evaluation uses zero-shot and 5-shot prompting with logit-based answer selection comparing probabilities for options A/B/C/D. A fine-tuned baseline model (Yi1.5-9B-Ancient) is created through full fine-tuning on Yi-1.5-9B-Chat with instruction-input-output format, batch size 2, and learning rate 1e-5.

## Key Results
- Human experts achieve 55.13% average accuracy, significantly exceeding the best model (Qwen-14B-Chat at 51.00%)
- Models score near-random (~30%) on glyph and pronunciation comprehension tasks but reach 69.71% on meaning comprehension
- Fine-tuned Yi1.5-9B-Ancient improves glyph comprehension by 8.07% but degrades contextual understanding by 5.98%
- Pronunciation task performance is lowest (~25-30%), suggesting pre-training corpora lack phonological knowledge of ancient characters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A three-stage digitization pipeline enables systematic processing of ancient characters that lack standardized Unicode encoding.
- **Mechanism:** Ancient characters are processed through: (1) image processing to extract radical features and spatial relationships, generating a knowledge graph; (2) unified font encoding with deduplication to create a character-encoding table; (3) new encoding assignment for previously unrecorded characters. This creates a complete and unified character encoding table.
- **Core assumption:** The radical-level feature extraction from images accurately captures the structural properties needed for downstream comprehension tasks.
- **Evidence anchors:** [section] "We propose a three-stage method for digitization. With this method, we can process excavated documents, which facilitates the training and evaluation of ancient LLMs." [section] Details the three phases: ancient character image processing, unified font encoding, and new encoding of missing characters. [corpus] Neighbor paper InteChar addresses similar encoding challenges for oracle bone characters, suggesting this is a recognized problem in the field.
- **Break condition:** If ancient character images are too degraded or noisy for reliable radical extraction, the pipeline fails at stage one. The paper notes excavated documents have "complex noises" that complicate processing.

### Mechanism 2
- **Claim:** The four-competency evaluation framework (glyph → pronunciation → meaning → context) reveals differential model weaknesses that correlate with knowledge depth in pre-training corpora.
- **Mechanism:** Tasks are structured by difficulty from character-level (glyph, pronunciation) to word-level (meaning) to sentence-level (context). Models score near-random (~30%) on glyph/pronunciation but reach 69.71% on meaning tasks, suggesting pre-training corpora contain more semantic than orthographic/phonological knowledge about ancient characters.
- **Core assumption:** The performance differential across competencies reflects underlying knowledge gaps rather than task design artifacts.
- **Evidence anchors:** [section] "Models perform poorly on glyph and pronunciation comprehension (around 30%), but better on meaning and contextual tasks (up to 69.71%)." [section] Results table shows humans outperform models on glyph (76.66% vs 53.01% best) but models exceed humans on meaning comprehension (69.71% vs 38.33%). [corpus] Fùxì benchmark paper similarly finds comprehension-focused evaluation gaps, supporting the multi-competency approach.
- **Break condition:** If the multiple-choice format introduces systematic bias (e.g., confusing options are poorly designed), competency scores may not reflect actual understanding. The paper claims careful option design but doesn't validate this systematically.

### Mechanism 3
- **Claim:** Fine-tuning on ancient Chinese knowledge improves glyph comprehension but may degrade contextual understanding through embedding interference.
- **Mechanism:** Yi1.5-9B-Ancient (fine-tuned) achieves 50.77% on glyph comprehension vs. 42.70% baseline (+8.07%), but drops 5.98% on contextual comprehension. Assumption: Fine-tuning introduces ancient character knowledge but disrupts learned representations for transmitted document contexts.
- **Core assumption:** The trade-off between glyph improvement and context degradation is caused by embedding interference rather than overfitting or dataset quality issues.
- **Evidence anchors:** [section] "Yi1.5-9B-Ancient performed poorly on the contextual comprehension task, even 5.98% lower than Yi1.5-9B-Chat. This maybe because the ancient knowledge introduced by fine-tuning caused some damage to the original embedding." [section] Fine-tuning setup: batch size 2, learning rate 1e-5, full fine-tuning on Yi1.5-9B-Chat. [corpus] WenyanGPT paper reports improved classical Chinese performance through domain-specific training, but doesn't analyze embedding trade-offs—mechanism remains underexplored.
- **Break condition:** If contextual comprehension tasks disproportionately test transmitted documents (which the paper notes may conflict with excavated document knowledge), the degradation may be task-distribution mismatch rather than embedding interference.

## Foundational Learning

- **Concept: Excavated vs. Transmitted Documents (TraDoc vs. ExcDoc)**
  - **Why needed here:** The benchmark specifically addresses the gap in evaluating excavated documents (oracle bones, bronze inscriptions, bamboo slips), which are more authentic but underrepresented in LLM training data compared to transmitted documents (classics like Analects that have been copied and standardized over centuries).
  - **Quick check question:** Given an ancient Chinese text, can you identify whether it likely comes from an excavated source (original, unedited, with clear archaeological provenance) or a transmitted source (standardized through generations of copying)?

- **Concept: Chinese Character Radical Structure (Semantic + Phonetic)**
  - **Why needed here:** Two of the ten tasks (Radical, Phonetic Radical) require understanding that Chinese characters are composed of semantic radicals (indicating meaning category) and phonetic radicals (indicating pronunciation), and the paper explicitly tests recognition of these components.
  - **Quick check question:** For the character 河 (river), can you identify the semantic radical (氵= water) and explain how it signals the character's meaning category?

- **Concept: Phonetic Loan Characters (Jiajie 假借)**
  - **Why needed here:** One of the ten tasks specifically tests phonetic loan character recognition—characters borrowed for their sound value to represent homophonous words. This requires contextual reasoning since the written character differs from the intended meaning.
  - **Quick check question:** If an ancient text uses the character 来 (originally "wheat") to represent the verb "to come" because of similar pronunciation, what type of character usage is this?

## Architecture Onboarding

- **Component map:**
  - Digitization pipeline: Image processing → radical extraction → unified encoding → new character assignment
  - Benchmark construction: Competency definition → task design → question/option generation → expert validation
  - Evaluation framework: Multiple-choice format → zero-shot/few-shot prompting → logit-based answer selection (compare A/B/C/D probabilities)
  - Baseline model: Yi1.5-9B-Chat → fine-tuning dataset construction → full fine-tuning → Yi1.5-9B-Ancient

- **Critical path:** The digitization pipeline is the bottleneck—without standardized encoding, ancient characters cannot be consistently evaluated. The three-stage process must complete before benchmark construction begins.

- **Design tradeoffs:**
  - Multiple-choice vs. open-ended: Standardized evaluation and automated scoring, but may not capture nuanced understanding
  - Fine-tuning vs. in-context learning: Fine-tuning shows strong glyph improvements (+8.07%) but risks embedding interference (−5.98% on context); few-shot shows mixed results (some models improve, Baichuan/Qwen degrade)
  - Excavated vs. transmitted document balance: ExcDoc Word task has only 365 questions vs. TraDoc Word's 1,504; paper focuses on excavated documents but translational understanding may be underrepresented

- **Failure signatures:**
  - Near-random pronunciation scores (~25-30%): Indicates model has insufficient phonological knowledge of ancient characters—pre-training corpus lacks this information
  - Large gap between glyph and meaning comprehension: Models can infer meaning from context but cannot recognize visual character forms (humans are multimodal, models are text-only)
  - Few-shot performance degradation: Baichuan2-7B drops 2.5% average in few-shot vs. zero-shot; may indicate context window confusion or prompt format mismatch

- **First 3 experiments:**
  1. Reproduce digitization pipeline on a small subset (e.g., 100 oracle bone characters): Verify that image processing correctly extracts radicals and that new encoding assignments are consistent. Check against the paper's character encoding table if available.
  2. Run zero-shot evaluation on Qwen-7B-Chat across all 10 tasks: Establish baseline accuracy per task. Compare against paper's reported 43.83% average to validate your evaluation setup.
  3. Test fine-tuning learning rate sensitivity: Retrain Yi1.5-9B-Ancient with learning rates [1e-6, 5e-6, 1e-5, 5e-5] to determine if the embedding interference trade-off can be reduced while preserving glyph comprehension gains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can multi-modal inputs (e.g., character images) bridge the performance gap between text-only LLMs and human experts in glyph comprehension tasks?
- **Basis in paper:** [explicit] The conclusion states an intent to "extend it to multiple modalities" in future work.
- **Why unresolved:** The authors observe humans significantly outperform models (76.66% vs ~42-53%) on glyph tasks, likely because humans process visual features while current models rely on single-modality text.
- **What evidence would resolve it:** Evaluating models using image-based inputs on AncientBench glyph tasks to see if performance aligns with human visual processing.

### Open Question 2
- **Question:** What training methodologies allow models to acquire ancient Chinese knowledge without degrading their pre-existing contextual comprehension embeddings?
- **Basis in paper:** [explicit] The authors note that while fine-tuning improved Yi1.5-9B-Ancient's glyph knowledge, it caused a performance drop (-5.98%) in contextual comprehension, concluding it is "necessary to further propose better methods... without affecting the original embedding."
- **Why unresolved:** Standard fine-tuning creates a trade-off where new domain knowledge overwrites or interferes with the model's general reasoning capabilities.
- **What evidence would resolve it:** A training method (e.g., parameter-efficient fine-tuning) that improves glyph/pronunciation scores without lowering the contextual comprehension baseline.

### Open Question 3
- **Question:** How does model performance scale or vary with the expansion of data sources beyond the current pre-Qin excavated documents?
- **Basis in paper:** [explicit] The authors state in the conclusion, "In future work, we will further expand the sources of data."
- **Why unresolved:** The current benchmark focuses specifically on Oracle Bone, Bronze, and Bamboo scripts; it is unknown if these findings generalize to other historical eras or document types.
- **What evidence would resolve it:** Evaluation results on an extended benchmark incorporating documents from dynasties beyond the pre-Qin period.

## Limitations

- Dataset availability: The AncientBench dataset and character encoding table are not yet publicly accessible, creating a critical dependency for reproduction. The digitization pipeline is described at a high level but lacks implementation details that would be necessary to reconstruct the encoding system independently.
- Evaluation scope: The benchmark focuses heavily on excavated documents (7/10 tasks) while transmitted document understanding is less represented. This creates an incomplete picture of ancient Chinese comprehension and may overstate the gap between human and model performance in practical applications where both document types appear.
- Prompt format sensitivity: The paper reports that some models (Baichuan2-7B, Qwen-14B-Chat) degrade in few-shot settings, but doesn't systematically investigate whether this is due to prompt format, context window limitations, or model-specific biases. The evaluation framework may be less robust than reported.

## Confidence

- **High confidence:** The core claim that current LLMs underperform humans on ancient Chinese character comprehension is well-supported by the comparative results (55.13% human vs 51.00% best model). The digitization pipeline concept is plausible given the known challenges of ancient character encoding in the field.
- **Medium confidence:** The performance differential across competencies (glyph/pronunciation ~30% vs meaning/contextual up to 69.71%) is reported clearly, but the paper doesn't validate whether this reflects actual knowledge gaps versus task design artifacts. The fine-tuning trade-off (glyph +8.07% vs context -5.98%) is reported but the mechanism (embedding interference) is speculative.
- **Low confidence:** The exact impact of the digitization pipeline on model performance cannot be assessed without access to the encoded character representations and their quality metrics. The human baseline of 55.13% appears precise but the methodology for establishing this benchmark is not detailed.

## Next Checks

1. **Digitization pipeline validation:** Reconstruct the character encoding process on a small test set (100 oracle bone characters) and verify that radical extraction, deduplication, and new encoding assignments are consistent and produce usable representations for downstream tasks.
2. **Prompt format sensitivity analysis:** Systematically vary the few-shot exemplar count (1, 3, 5 shots) and format across the priority models to determine whether the reported performance degradation is robust or an artifact of the specific prompt template used.
3. **Cross-dataset generalization test:** Evaluate models on both AncientBench and the related Fùxì benchmark (classical Chinese comprehension) to determine whether performance gaps are specific to excavated documents or reflect broader ancient Chinese language understanding deficits.