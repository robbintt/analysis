---
ver: rpa2
title: 'VIVAT: Virtuous Improving VAE Training through Artifact Mitigation'
arxiv_id: '2506.07863'
source_url: https://arxiv.org/abs/2506.07863
tags:
- training
- arxiv
- https
- latent
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work systematically addresses common artifacts in KL-VAE\
  \ training by identifying five prevalent issues\u2014color shift, grid patterns,\
  \ blur, corner artifacts, and droplet artifacts\u2014and analyzing their root causes.\
  \ The authors propose straightforward modifications, including adjustments to loss\
  \ weights, padding strategies, and the integration of spatially conditional normalization,\
  \ to mitigate these artifacts without requiring radical architectural changes."
---

# VIVAT: Virtuous Improving VAE Training through Artifact Mitigation

## Quick Facts
- **arXiv ID:** 2506.07863
- **Source URL:** https://arxiv.org/abs/2506.07863
- **Reference count:** 40
- **Primary result:** Mitigates five common KL-VAE artifacts through targeted architectural changes, achieving state-of-the-art PSNR/SSIM on ImageNet benchmarks.

## Executive Summary
This work systematically addresses common artifacts in KL-VAE training by identifying five prevalent issues—color shift, grid patterns, blur, corner artifacts, and droplet artifacts—and analyzing their root causes. The authors propose straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of spatially conditional normalization, to mitigate these artifacts without requiring radical architectural changes. Their approach achieves state-of-the-art reconstruction quality, with PSNR and SSIM scores surpassing existing models on multiple benchmarks (e.g., 31.25 PSNR and 0.90 SSIM on ImageNet). Additionally, the improved VAE enhances text-to-image generation quality, outperforming the Flux VAE in CLIP score when integrated into a latent diffusion pipeline. By preserving the simplicity of the KL-VAE framework, this method offers actionable insights for optimizing VAE training and improving generative performance.

## Method Summary
The authors address five common KL-VAE artifacts through systematic analysis and targeted fixes. They base their architecture on SBER-MoVQGAN, replacing the VQ-bottleneck with a KL-based encoder that outputs mean and log-variance for a 16-dimensional latent space. Key modifications include switching from zero padding to reflect padding in convolutional layers and incorporating Spatially Conditional Normalization (SCN) in decoder blocks. The training objective combines L2, LPIPS, adversarial, and KL losses with specific weightings. A two-stage training procedure is employed: initial full VAE training followed by decoder-only finetuning with a frozen encoder. Data preprocessing involves bicubic resizing of LAION HighRes images from HD to 480p, then cropping to 240p resolution.

## Key Results
- Achieves state-of-the-art reconstruction quality with 31.25 PSNR and 0.90 SSIM on ImageNet 256/512
- Successfully mitigates five common VAE artifacts: color shift, grid patterns, blur, corner artifacts, and droplet artifacts
- Improves text-to-image generation quality, outperforming Flux VAE in CLIP score when integrated into a latent diffusion pipeline
- Demonstrates that simple architectural modifications can significantly enhance VAE performance without complex redesigns

## Why This Works (Mechanism)
The proposed approach works by systematically addressing the root causes of common VAE artifacts. Reflect padding eliminates edge artifacts by avoiding zero-padding discontinuities, while Spatially Conditional Normalization prevents droplet artifacts by providing location-aware normalization. The carefully tuned loss weights balance reconstruction quality with latent regularization, preventing both excessive blur (from high KL weight) and grid patterns (from high adversarial weight). The two-stage training procedure allows the decoder to specialize in reconstruction after the encoder is fixed, improving overall image quality without destabilizing the learned latent space.

## Foundational Learning

**KL-VAE vs VQ-VAE**
- *Why needed:* Understanding the fundamental differences between continuous and discrete latent spaces in VAEs
- *Quick check:* KL-VAE uses continuous Gaussian latents with KL divergence, while VQ-VAE uses discrete codebook vectors

**Spatially Conditional Normalization (SCN)**
- *Why needed:* Provides location-aware normalization that prevents droplet artifacts in decoder
- *Quick check:* SCN modulates normalization parameters based on spatial position, unlike standard GroupNorm

**Adversarial Training in VAEs**
- *Why needed:* High adversarial loss weights create grid artifacts through excessive high-frequency emphasis
- *Quick check:* Monitor for checkerboard patterns when λ_adv exceeds 0.01

**Loss Weighting in Multi-Objective Training**
- *Why needed:* Proper balance between L2, LPIPS, adversarial, and KL losses is critical for artifact-free reconstruction
- *Quick check:* KL weight of 1e-4 prevents blur while maintaining compression efficiency

## Architecture Onboarding

**Component Map:** Input Images -> Encoder (Mean/LogVar) -> 16D Latent -> Decoder (SCN+Reflect Padding) -> Reconstructed Images -> Discriminator (PatchGAN)

**Critical Path:** The decoder is the critical path for reconstruction quality, particularly the integration of SCN and reflect padding in residual blocks

**Design Tradeoffs:** The approach prioritizes simplicity and artifact mitigation over architectural complexity, maintaining the KL-VAE framework while adding minimal modifications

**Failure Signatures:**
- Grid artifacts indicate excessive adversarial loss weight
- Blur indicates excessive KL regularization
- Droplet artifacts indicate missing SCN implementation
- Corner artifacts indicate zero padding usage

**First Experiments:**
1. Verify reflect padding implementation by comparing corner reconstruction between zero and reflect padding
2. Test SCN integration by measuring droplet artifact presence with and without SCN
3. Systematically vary λ_adv to observe grid artifact emergence and determine optimal threshold

## Open Questions the Paper Calls Out
None

## Limitations
- The exact spatial compression factor remains ambiguous despite its importance for fair comparisons against 8x models
- Specific discriminator architecture and precise training durations are unspecified, creating uncertainty about whether improvements stem from architectural changes or training scale
- Claims of "state-of-the-art" performance on COCO and FFHQ lack direct citations to specific benchmark results

## Confidence

**Artifact identification and mitigation strategies:** High - The methodology for diagnosing and addressing each artifact type is clearly articulated and technically sound

**Quantitative improvements on ImageNet:** Medium - PSNR/SSIM results are reported with specific values, but training details affecting these metrics are incomplete

**Enhancement of latent diffusion generation:** Medium - CLIP score improvements are claimed, but the exact pipeline configuration and comparison conditions are not fully specified

**Claims of "state-of-the-art" on COCO/FFHQ:** Low - Performance claims lack direct comparative citations and detailed benchmark configurations

## Next Checks

1. Implement a minimal reproduction with fixed compression factor f=4, verify that reflect padding and SCN are correctly applied, and measure reconstruction quality on a small subset of ImageNet

2. Systematically vary adversarial loss weight (λ_adv) to observe grid artifact emergence, confirming the proposed diagnostic threshold

3. Compare against a baseline SBER-MoVQGAN with identical training duration but without artifact mitigation modifications to isolate the contribution of each proposed change