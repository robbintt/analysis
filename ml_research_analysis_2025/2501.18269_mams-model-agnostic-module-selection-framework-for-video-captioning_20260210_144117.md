---
ver: rpa2
title: 'MAMS: Model-Agnostic Module Selection Framework for Video Captioning'
arxiv_id: '2501.18269'
source_url: https://arxiv.org/abs/2501.18269
tags:
- video
- mams
- tokens
- visual
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of fixed frame sampling in video
  captioning models, where using the same number of frames for all videos can miss
  crucial information or introduce redundancy. It proposes the first model-agnostic
  module selection framework (MAMS) that dynamically selects an appropriate caption
  generation module and constructs subsets of visual tokens for each video.
---

# MAMS: Model-Agnostic Module Selection Framework for Video Captioning

## Quick Facts
- arXiv ID: 2501.18269
- Source URL: https://arxiv.org/abs/2501.18269
- Authors: Sangho Lee; Il Yong Chun; Hogun Park
- Reference count: 10
- Key outcome: Introduces first model-agnostic module selection framework for video captioning, achieving new SOTA on YouCookII when applied to mPLUG-2 model with significant CIDEr improvements across three benchmarks.

## Executive Summary
This paper addresses the challenge of fixed frame sampling in video captioning models, where using the same number of frames for all videos can miss crucial information or introduce redundancy. It proposes the first model-agnostic module selection framework (MAMS) that dynamically selects an appropriate caption generation module and constructs subsets of visual tokens for each video. The framework also introduces an adaptive attention masking scheme to focus on more important visual tokens. Experiments on three benchmark datasets show that MAMS significantly improves the performance of three recent video captioning models, with notable gains in CIDEr scores.

## Method Summary
MAMS implements a two-module framework (large/small caption generators) with three key components: token significance scoring, module selection, and adaptive attention masking. Token significance scores combine attention weights with token magnitudes to identify important frames. A Gumbel-Softmax-based selector iteratively samples important frames, routing videos to either the large module (all frames) or small module (subset of frames). The framework generates video-specific binary attention masks that prioritize high-contribution tokens, outperforming fixed learnable masks. The approach is applied to three base models (SwinBERT, UniVL, mPLUG-2) on three datasets (MSVD, MSRVTT, YouCookII).

## Key Results
- MAMS improves CIDEr scores by 4.4 to 7.7 points across three benchmark datasets
- Achieves new state-of-the-art results on YouCookII when applied to mPLUG-2 model
- Outperforms fixed learnable attention masks by +2.2 CIDEr on MSVD
- Two-module design outperforms configurations with 3 or 4 modules

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Module Selection via Frame Significance
Adaptively selecting between caption generation modules of different capacities mitigates the saturation/degradation problem caused by fixed-frame sampling. A Gumbel-Softmax-based selector iteratively samples important frames using frame significance scores. If the number of selected frame indices |S_frm| ≤ T_small, a smaller module is selected; otherwise, the large module processes all frames.

### Mechanism 2: Token Significance Scoring with Attention-Norm Product
Combining attention weights with token magnitudes provides a better importance signal than attention alone for selecting contributing visual tokens. Token significance score t_i,p = (a_i,p · ||x^v_i,p||) / Σ(a_i,p · ||x^v_i,p||), where a_i,p is CLS-to-token attention from the first attention layer.

### Mechanism 3: Adaptive Attention Masking per Video
Video-specific binary attention masks that prioritize high-contribution tokens outperform fixed learnable masks that apply uniformly across all videos. Using Gumbel-Softmax over {t_i,p}, select a set of token indices S_tk. Construct M_large where mask entries are 1 for selected token pairs and 0 otherwise.

## Foundational Learning

- Concept: Multi-modal Transformer Attention Mechanisms
  - Why needed here: Token significance scores derive from CLS-to-visual-token attention; understanding how attention weights distribute across tokens is prerequisite for interpreting and debugging the selector.
  - Quick check question: Given a 12-layer transformer with 8 attention heads, can you trace how CLS token attention to visual tokens changes across layers?

- Concept: Gumbel-Softmax Differentiable Sampling
  - Why needed here: Both module/token selection and adaptive masking use Gumbel-Softmax for differentiable discrete selection; understanding the temperature parameter's effect on sampling stochasticity is critical.
  - Quick check question: What happens to the gradient variance and sample diversity as Gumbel-Softmax temperature approaches 0 vs. infinity?

- Concept: Video Token Redundancy from Consecutive Frames
  - Why needed here: The core problem MAMS addresses is redundancy from dense sampling of similar adjacent frames; recognizing this motivates the frame-level selection design.
  - Quick check question: For a 30fps video with minimal motion, how many consecutive frames would you expect to produce near-identical visual tokens using a standard ViT encoder?

## Architecture Onboarding

- Component map: Video Encoder → visual tokens (T_large × P tokens) → Token Significance Score Calculator → Module Selector (Gumbel-Softmax loop over frames) → Token Selector (while-loop for small module padding) → Caption Generation Modules (large: T_large frames, small: T_small frames) → Adaptive Mask Generator → M_large / M_small binary masks per attention layer

- Critical path: Token significance computation → module selection → token subset construction → mask generation → caption module forward pass. Errors in significance scores propagate to all downstream components.

- Design tradeoffs:
  - Two-module default vs. 3-4 modules: Table 7 shows default (2 modules) outperforms 3-4 candidates, as more modules fragment training data.
  - Adaptive mask vs. fixed mask: Adaptive provides +2.2 CIDEr on MSVD (Table 6) but adds per-video mask generation overhead.
  - First-layer vs. deeper-layer attention for scores: Paper uses first layer; assumption is early attention captures broad relevance, but this is not ablated.

- Failure signatures:
  - CIDEr stagnates despite increasing frame count → selector may be miscalibrated, routing most videos to small module.
  - Generated captions miss edge objects consistently → adaptive mask may be defaulting to center-biased patterns; check Gumbel-Softmax temperature.
  - Training loss NaN → check Gumbel-Softmax numerical stability at low temperatures.

- First 3 experiments:
  1. Reproduce Figure 2 saturation curve: Run baseline SwinBERT with frame counts [8, 16, 32, 64] on MSVD to confirm saturation pattern before integrating MAMS.
  2. Ablate module selection only: Disable adaptive masking, run MAMS with only module selection to isolate its contribution (target: ~+2.9 CIDEr per Table 3, row 2).
  3. Sanity check token selector: Implement swapped Eq.(3) conditions and inverted Eq.(1) scores to verify performance drops match Table 5 (~5-8 CIDEr degradation).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can extending the MAMS framework to video summarization and video question answering tasks achieve similar performance gains?
- **Basis in paper:** The conclusion states, "we aim to extend the underlying principles of the MAMS framework to other video understanding tasks, such as video summarization and video question answering."
- **Why unresolved:** The current study only validates the framework on video captioning benchmarks (MSVD, MSRVTT, YouCookII).
- **What evidence would resolve it:** Experimental results showing MAMS integration improves state-of-the-art models on standard video QA or summarization datasets.

### Open Question 2
- **Question:** Does incorporating importance weighting for textual tokens alongside visual tokens provide significant performance improvements?
- **Basis in paper:** The conclusion proposes, "focusing more on important textual tokens, as well as important visual tokens."
- **Why unresolved:** The current module and token selectors operate exclusively on visual tokens ($t_{i,p}$), ignoring the potential redundancy or significance of textual tokens in the multi-modal transformer.
- **What evidence would resolve it:** Ablation studies showing quantitative gains in CIDEr/BLEU scores from a modified MAMS framework that implements textual token selection.

### Open Question 3
- **Question:** Would increasing the training dataset size allow for effective scaling of MAMS with more than two candidate modules?
- **Basis in paper:** Table 7 shows performance degradation when using 3 or 4 modules, which the authors attribute to insufficient training samples for each module.
- **Why unresolved:** It is unclear if the binary (large/small) selection is the architectural limit or simply a constraint imposed by the size of current datasets.
- **What evidence would resolve it:** Experiments on larger-scale pre-training corpora demonstrating that models with 3 or 4 granular modules outperform the binary configuration.

## Limitations

- Implementation-specific parameter choices: Missing critical hyperparameters for Gumbel-Softmax sampling (temperature schedule, iterations) and exact frame count thresholds T_large and T_small for each dataset.
- Statistical validation gaps: No statistical significance testing between baseline and MAMS results to confirm reported improvements are not due to random variation.
- Single attention layer dependency: Token significance scoring relies exclusively on first attention layer, without validation of whether deeper layers might provide better signals.

## Confidence

- High Confidence: The adaptive attention masking mechanism improves performance over fixed masks (Tables 6, qualitative Figure 6). The token significance scoring formulation (attention-norm product) is validated through sanity tests (Table 5b showing 8.2 CIDEr degradation when inverted). The saturation problem with fixed frame counts is empirically demonstrated (Figure 2).
- Medium Confidence: The module selection framework provides consistent improvements across all three base models and datasets. The two-module design is optimal compared to three or four modules (Table 7). The claim that MAMS achieves new SOTA on YouCookII when applied to mPLUG-2 is supported but relies on the underlying mPLUG-2 performance.
- Low Confidence: The specific contribution of each component (module selection vs. adaptive masking vs. token significance scoring) to the overall improvement is not precisely quantified through ablation studies. The claim that MAMS is "the first model-agnostic module selection framework" in video captioning cannot be fully verified without exhaustive literature review.

## Next Checks

- Check 1: Implement MAMS with only module selection (disable adaptive masking) and run on MSVD with SwinBERT. Measure the isolated contribution of module selection alone to verify the ~+2.9 CIDEr improvement reported in Table 3.
- Check 2: Re-run the baseline and MAMS implementations on MSRVTT for all three base models with 5 different random seeds. Compute 95% confidence intervals for CIDEr scores and perform paired t-tests between baseline and MAMS.
- Check 3: Vary the Gumbel-Softmax temperature across [0.1, 0.5, 1.0, 2.0] while keeping other parameters fixed. For each temperature, measure the distribution of selected frames (|S_frm|) across the validation set and corresponding CIDEr scores.