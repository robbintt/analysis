---
ver: rpa2
title: Alignment with Fill-In-the-Middle for Enhancing Code Generation
arxiv_id: '2508.19532'
source_url: https://arxiv.org/abs/2508.19532
tags:
- code
- training
- zhang
- generation
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to enhance code generation
  capabilities of large language models by integrating Fill-In-the-Middle (FIM) with
  Direct Preference Optimization (DPO). The method addresses the challenge of limited
  training data with accurate test cases in code generation tasks by splitting code
  snippets into granular blocks using Abstract Syntax Tree (AST) parsing, constructing
  more diverse DPO pairs, and employing curriculum training based on block depth.
---

# Alignment with Fill-In-the-Middle for Enhancing Code Generation

## Quick Facts
- arXiv ID: 2508.19532
- Source URL: https://arxiv.org/abs/2508.19532
- Reference count: 17
- One-line primary result: Integrates FIM with DPO to improve code generation pass@1 accuracy by up to 2.6 pp on Qwen2.5-Coder-7B-Instruct.

## Executive Summary
This paper addresses the challenge of limited training data with accurate test cases in code generation by integrating Fill-In-the-Middle (FIM) with Direct Preference Optimization (DPO). The method splits code snippets into granular blocks using Abstract Syntax Tree (AST) parsing, constructs more diverse DPO pairs, and employs curriculum training based on block depth. Experiments demonstrate significant improvements across multiple benchmarks: HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench, with up to 2.6 percentage points improvement in pass@1 accuracy on Qwen2.5-Coder-7B-Instruct compared to standard DPO.

## Method Summary
The approach enhances code generation for LLMs by combining Fill-In-the-Middle (FIM) with Direct Preference Optimization (DPO). It addresses limited training data with test cases by creating more granular DPO pairs through AST-based code segmentation. The pipeline extracts blocks (if, for, while, function) from code, constructs FIM prompts with prefix/suffix/middle segments, generates preference pairs via model sampling and test case evaluation, and trains with curriculum ordering and mixed FIM/Chat formats using DPO loss restricted to the middle segment.

## Key Results
- Achieves up to +2.6 percentage points improvement in pass@1 accuracy on Qwen2.5-Coder-7B-Instruct vs standard DPO
- Demonstrates consistent gains across HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench benchmarks
- Ablation studies confirm the importance of AST-based splitting, curriculum learning, and suffix token masking in DPO loss

## Why This Works (Mechanism)

### Mechanism 1: FIM-Localized Preference Optimization
- **Claim:** Integrating Fill-In-the-Middle (FIM) with Direct Preference Optimization (DPO) may localize the reward signal to the specific erroneous code block, preventing the counter-productive penalization of correct suffix tokens that occurs in standard DPO.
- **Mechanism:** In standard DPO, if a preferred and dispreferred response differ only slightly (e.g., one line), the loss function penalizes the dispreferred suffix even if it is logically correct, simply because it follows an error. By converting the task to FIM (generating the middle given prefix and suffix), the authors isolate the target block. The DPO loss is calculated strictly on the generated middle segment ($g$), ensuring rewards are assigned only to the tokens causing the pass/fail outcome.
- **Core assumption:** The base model possesses sufficient FIM capability to generate coherent code given bidirectional context; the test cases validly check the functionality of the isolated block within the full code context.
- **Evidence anchors:**
  - [Section 2.3]: "It is inappropriate to calculate the DPO loss on the suffix... implies the need for a large dataset... to make up for the negative impact."
  - [Section 3.1]: "Only $g^{(j)}_i$ is regarded as the response and included in the DPO loss."
  - [corpus]: "Focal Preference Alignment" (related work) similarly addresses focusing optimization, but this paper uses structural splitting rather than purely probabilistic focal points.
- **Break condition:** If the model's FIM capability is weak (generating incoherent code), the isolated loss will optimize for hallucinated structures rather than logical correctness.

### Mechanism 2: AST-Based Semantic Segmentation
- **Claim:** Splitting code using Abstract Syntax Tree (AST) nodes likely produces more semantically coherent training targets than random character spans, reducing noise in preference pairs.
- **Mechanism:** Random splitting often interrupts logical flow (e.g., splitting inside an expression), making the "correct" completion ambiguous. By extracting specific node types (if, for, while, function blocks), the target represents a complete logical unit. This ensures that passing the test case depends on the logic of that specific block, making the "preferred" vs. "dispreferred" label cleaner and more deterministic.
- **Core assumption:** The logical correctness of the code is localized within these structural blocks; test cases effectively validate these sub-components.
- **Evidence anchors:**
  - [Section 3.2]: "Extract syntactically complete and functionally independent code blocks... reduces the risk of including partial or entangled code."
  - [Table 2]: The "w/o AST" variant (random selection) shows degraded performance compared to the full StructureCoder.
  - [corpus]: "Structure-Aware Fill-in-the-Middle Pretraining for Code" (corpus neighbor) supports the efficacy of AST-aware masking strategies.
- **Break condition:** If logic is tightly coupled across non-contiguous blocks (spaghetti code), isolating a single AST node may remove necessary context, making the task impossible.

### Mechanism 3: Curriculum Training by Block Depth/Length
- **Claim:** Organizing training data by block length (short/curriculum) likely stabilizes the optimization of token-level rewards compared to random ordering.
- **Mechanism:** The authors hypothesize that shorter blocks contain simpler logic and fewer dependencies. By training on these first, the model learns to attribute reward/penalty to specific tokens without the complexity of long-range dependencies. Progressing to longer blocks allows the model to generalize these learned rewards to more complex structures.
- **Core assumption:** Code block length serves as a reliable proxy for logical complexity and learning difficulty.
- **Evidence anchors:**
  - [Section 3.3]: "Curriculum learning strategy... orders the training data based on the length... first focus on mastering token-level rewards for shorter, simpler code snippets."
  - [Table 2]: The "w/o Curriculum" variant underperforms the full method, specifically noted as detrimental to HumanEval in the ablation.
- **Break condition:** If shorter blocks are semantically dense or rely on implicit external state while longer blocks are boilerplate, the "difficulty" ordering is inverted, potentially confusing the gradient descent.

## Foundational Learning

- **Concept: Fill-In-the-Middle (FIM)**
  - **Why needed here:** This is the core architectural change. You must understand how the causal mask is modified to allow the model to attend to both prefix (past) and suffix (future) tokens when generating the middle.
  - **Quick check question:** Given the sequence `<PRE> A <SUF> C <MID> B <EOT>`, does the attention mask allow B to attend to C?

- **Concept: Direct Preference Optimization (DPO) Implicit Reward**
  - **Why needed here:** The paper relies on the premise that DPO learns a hidden reward model. You need to understand why calculating DPO loss on a correct suffix (in a failed sample) is "unfair" and suppresses correct generation patterns.
  - **Quick check question:** In the DPO loss formula, what happens to the probability of the dispreferred response ($y_l$) relative to the preferred response ($y_w$)?

- **Concept: Abstract Syntax Tree (AST) Parsing**
  - **Why needed here:** This is the data curation mechanism. You need to know that AST represents code structure (loops, conditionals) rather than just text lines, ensuring splits happen at logical boundaries.
  - **Quick check question:** Why would splitting the text `x = y + z` randomly be worse than splitting at the statement level for a generation task?

## Architecture Onboarding

- **Component map:** Data Processor -> AST Parser -> Extract Blocks -> Prompt Constructor -> Generator -> Verifier -> Trainer
- **Critical path:** The **Verifier** is the bottleneck. If the test cases are flaky or the execution environment is unsafe, the DPO pairs will be noisy (label flipping), rendering the optimization useless.
- **Design tradeoffs:**
  - **Format Mixing ($\alpha=0.5$):** The paper mixes FIM and Chat formats. This trades off peak FIM performance for preserving the model's ability to act as a Chat Assistant.
  - **Block Types:** Restricting to 4 node types (if, for, while, function) reduces complexity but might ignore logic contained in `try/except` blocks or class definitions.
- **Failure signatures:**
  - **Syntax Regression:** Model starts generating code that passes tests but violates syntax in the "glue" areas between prefix/middle/suffix.
  - **Chat Collapse:** Model refuses standard instruction prompts (fix by increasing $\alpha$ for Chat format).
  - **Reward Hacking:** Model generates comments or "mock" code in the middle that tricks the verifier (if verifier checks output text rather than execution).
- **First 3 experiments:**
  1. **Sanity Check (FIM Base):** Verify the base model (without DPO) can actually solve AST-split FIM tasks. If accuracy is near 0%, DPO will fail.
  2. **Loss Isolation Ablation:** Replicate the "w/ suf loss" vs "StructureCoder" comparison (Table 2) to confirm that suppressing suffix loss is the primary driver of improvement.
  3. **Curriculum Validity:** Train two models—one with random data order, one with curriculum order—on the same FIM data to validate the curriculum hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the FIM-enhanced DPO approach be effectively transferred to non-code domains, such as mathematical reasoning or general instruction following?
- **Basis in paper:** [explicit] The Limitations section states, "our method is currently focused solely on the code generation domain... Further research would be required to explore the transferability to other closed-question tasks."
- **Why unresolved:** The method relies heavily on Abstract Syntax Tree (AST) parsing to create structured "middle" segments for training. It is unclear how to define equivalent, verifiable "middle" segments for natural language or mathematical reasoning chains where formal syntax trees are unavailable.
- **What evidence would resolve it:** Successful application of the StructureCoder pipeline (or an adapted version using semantic chunking) to benchmarks like GSM8K or MATH, showing statistically significant improvements over standard DPO.

### Open Question 2
- **Question:** How can the method be adapted for models that have suffered degradation in Fill-in-the-Middle (FIM) capabilities due to Supervised Fine-Tuning (SFT)?
- **Basis in paper:** [explicit] The Limitations section notes, "most current models, after supervised fine-tuning, may struggle to maintain high FIM performance... This reliance on FIM limits the applicability of our approach to models that have been specifically optimized for this task."
- **Why unresolved:** The data construction pipeline depends entirely on the model's ability to generate valid code for the middle segment given the prefix and suffix. If the base model cannot perform FIM effectively, it cannot generate the high-quality preference pairs required for the subsequent DPO training.
- **What evidence would resolve it:** A study analyzing the correlation between the base model's FIM accuracy (e.g., pass rates on FIM benchmarks) and the final performance gains of StructureCoder, or the proposal of a "FIM recovery" warm-up stage that enables this method for standard chat models.

### Open Question 3
- **Question:** What is the impact of more granular or alternative AST segmentation strategies on training efficiency and model performance?
- **Basis in paper:** [explicit] The Conclusion suggests, "Future work could explore further refinements, such as alternative segmentation strategies or additional data sources."
- **Why unresolved:** The current implementation restricts segmentation to only four node types (if, for, while, function). It is unknown if excluding other structures (e.g., class definitions, try-except blocks, or expression-level statements) limits the model's ability to learn fine-grained token-level rewards or introduces bias in the curriculum training.
- **What evidence would resolve it:** Ablation studies varying the AST node types used for segmentation (e.g., adding nested logic vs. keeping only top-level functions) and measuring the resulting changes in Pass@1 accuracy on complex benchmarks like BigCodeBench.

## Limitations
- **Family-specific bias:** Improvements are primarily demonstrated on Qwen2.5-Coder family models; efficacy on other base models is not established.
- **Test case dependency:** The quality of preference pairs depends critically on the reliability and coverage of the test cases used for verification.
- **Ablation incompleteness:** The paper does not isolate the impact of format mixing (FIM vs Chat), leaving open whether gains come from FIM optimization or architectural changes.

## Confidence
- **High Confidence:** The core mechanism of isolating DPO loss to the generated middle block is logically sound and supported by ablation.
- **Medium Confidence:** Curriculum learning by block length shows positive effect, but the assumption that length proxies difficulty is not rigorously tested.
- **Low Confidence:** "State-of-the-art" claims are based on limited baseline comparisons and may not reflect the method's absolute positioning.

## Next Checks
1. **Cross-Model Generalization:** Apply the full StructureCoder pipeline to a different base code model (e.g., CodeLlama-7B-Instruct) and evaluate on HumanEval/MBPP to confirm improvements are not family-specific.
2. **Format Isolation Ablation:** Train two models using only the FIM format (α=1.0) and only the Chat format (α=0.0) with the same AST splitting and curriculum strategy to isolate the source of gains.
3. **Noise Sensitivity Test:** Intentionally inject noise into preference pairs by flipping test case labels and measure performance degradation to quantify robustness to label noise.