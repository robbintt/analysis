---
ver: rpa2
title: Weights initialization of neural networks for function approximation
arxiv_id: '2510.08780'
source_url: https://arxiv.org/abs/2510.08780
tags:
- neural
- function
- networks
- approximation
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reusable weights initialization framework
  for neural function approximation, addressing challenges in training stability,
  generalization, and efficiency. The method pretrains neural networks to approximate
  polynomial basis functions on a reference domain, then reuses these learned weights
  to initialize more complex function approximators.
---

# Weights initialization of neural networks for function approximation

## Quick Facts
- **arXiv ID**: 2510.08780
- **Source URL**: https://arxiv.org/abs/2510.08780
- **Authors**: Xinwen Hu; Yunqing Huang; Nianyu Yi; Peimeng Yin
- **Reference count**: 27
- **Primary result**: Pretraining neural networks on polynomial basis functions and reusing learned weights substantially improves function approximation accuracy and convergence speed compared to random initialization.

## Executive Summary
This paper introduces a reusable weights initialization framework for neural function approximation that addresses training stability, generalization, and efficiency challenges. The method pretrains neural networks to approximate polynomial basis functions on a reference domain, then reuses these learned weights to initialize more complex function approximators. A domain mapping mechanism enables adaptation to arbitrary input domains while preserving generalization. Experiments demonstrate substantial improvements: one-dimensional functions achieve MSEs below 1e-6 and R2 values exceeding 0.9999, while two-dimensional approximations maintain similar accuracy. The framework enables efficient training with better extrapolation, modularity, and parameter reuse compared to standard random initialization approaches.

## Method Summary
The framework constructs a library of "basis networks" trained to approximate polynomial functions (x^0, x^1, ..., x^M) on a reference domain [-1, 1]. For a target function approximation, these pre-trained weights are used as initialization, and a domain mapping mechanism transforms inputs from arbitrary domains to the reference domain. The final approximation combines the basis networks using linear coefficients solved via least squares. The method uses progressive initialization where higher-degree polynomial networks are initialized with weights from lower-degree ones, and employs GELU activation functions with single hidden layers of 1024 neurons.

## Key Results
- One-dimensional function approximations achieve MSE < 1e-6 and R2 > 0.9999
- Two-dimensional function approximations maintain similar accuracy levels
- The framework demonstrates superior extrapolation capabilities compared to standard initialization
- Progressive initialization accelerates the construction of the basis library
- Domain mapping enables networks trained on small domains to generalize to much larger domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Initializing networks with weights pre-trained on polynomial basis functions improves convergence speed and stability compared to random initialization.
- **Mechanism:** The framework pre-trains basis networks to approximate fundamental polynomials (x^0, x^1, ..., x^M). When approximating a target function, these pre-learned weights serve as initialization, shifting optimization from a "cold start" to a warm start where the network already possesses basic functional primitives.
- **Core assumption:** Target functions can be effectively decomposed into or approximated by linear combinations of polynomial basis functions.
- **Evidence anchors:** The abstract describes training basis networks to approximate polynomials and reusing learned parameters. Section 2.1 describes the neural representation space and parameter reuse. General consensus suggests initialization dynamics critically dictate convergence.
- **Break condition:** If the target function contains features orthogonal to the polynomial basis, initialization offers little benefit over random initialization.

### Mechanism 2
- **Claim:** A logarithmic domain mapping mechanism enables networks trained on a small reference domain to generalize to significantly larger or arbitrary domains.
- **Mechanism:** Inputs from arbitrary domains [a, b] are mapped to a reference domain [-1, 1] using scaling transformation T(x) = x / 10^s. The network learns on the stable reference domain, and inverse mapping recovers the output, preventing blind extrapolation into unbounded regions.
- **Core assumption:** The structural properties of the function are preserved under logarithmic scaling transformation, and inverse mapping accurately recovers necessary dynamics.
- **Evidence anchors:** The abstract mentions a domain mapping mechanism that transforms inputs while preserving structural correspondence. Section 2.2 defines the mapping T(x) and shows improved inference on [-60, 60] for models trained on [-1, 1].
- **Break condition:** If the target domain involves extreme dynamical changes smoothed out or lost by logarithmic scaling compression.

### Mechanism 3
- **Claim:** Training basis networks progressively (using weights from degree k-1 to initialize degree k) accelerates basis library construction.
- **Mechanism:** This curriculum learning strategy applies to the pre-training phase. Since x^k shares structural similarities with x^(k-1), initializing higher-order polynomial training with lower-order weights provides a better starting point than random initialization.
- **Core assumption:** Adjacent polynomial degrees lie in a sufficiently smooth loss landscape such that their optimal weights are correlated.
- **Evidence anchors:** Section 2.1 states parameters from degree k-1 are used to initialize degree k training. General literature mentions special random initialization, but this specific progressive weight reuse is distinct.
- **Break condition:** If network capacity is insufficient, errors might accumulate through the progressive initialization chain.

## Foundational Learning

- **Concept: Polynomial Basis Expansion**
  - **Why needed here:** The entire framework relies on the premise that complex functions can be represented as linear combinations of simpler polynomials. Without this, the basis library concept is meaningless.
  - **Quick check question:** Can you explain why fitting sin(x) might benefit from initializing with weights trained on x, x^3, x^5?

- **Concept: Transfer Learning & Fine-Tuning**
  - **Why needed here:** The method creates a library of models and transfers their weights to new tasks. Understanding the difference between freezing weights and fine-tuning is critical for using the framework effectively.
  - **Quick check question:** In this framework, are the basis network weights fixed (frozen) when approximating a new target function, or are they updated?

- **Concept: Logarithmic Scaling/Normalization**
  - **Why needed here:** The paper introduces a specific non-linear normalization to handle extrapolation. Understanding how input scaling affects gradient stability is required to debug the domain mapper.
  - **Quick check question:** How does mapping an input x ∈ [-60, 60] to x̂ ∈ [-1, 1] affect the magnitude of gradients during backpropagation?

## Architecture Onboarding

- **Component map:** Offline Library -> Domain Mapper -> Combiner
- **Critical path:**
  1. Verify Offline Library quality (do basis networks actually output x^k accurately?)
  2. Implement Domain Mapper (ensure numerical stability of log scaling)
  3. Solve for coefficients α_k using Least Squares on the mapped target function
- **Design tradeoffs:**
  - GELU vs. ReLU: Paper recommends GELU for superior fitting accuracy despite ReLU being faster
  - Width vs. Depth: For basis functions, single wide layer (1024 neurons) preferred over deep architectures to avoid overfitting and optimization difficulties
- **Failure signatures:**
  - High MSE on training domain: Suggests basis library is insufficient (max degree M too low) or activation function is inappropriate
  - Poor extrapolation: Indicates failure in Domain Mapping logic or "spectral bias" limiting network from learning high-frequency components outside training range
  - Nan/Loss Explosion: Likely caused by instability in progressive initialization or domain mapping exponent calculation
- **First 3 experiments:**
  1. Basis Sanity Check: Train single network on φ(x) = x^3 on [-1, 1] and verify MSE < 10^-7
  2. Mapping Ablation: Approximate f(x) = x^3 on [-60, 60] with and without domain mapping module to quantify generalization gain
  3. Composition Test: Use pre-trained library to approximate composite function (e.g., sin(x)) by solving only for coefficients α_k while keeping basis weights frozen

## Open Questions the Paper Calls Out
- **Question:** Can the reusable weights initialization framework be effectively extended to solve partial differential equations (PDEs) with complex boundary conditions?
- **Basis in paper:** The conclusion identifies "a particularly compelling avenue is the extension of this framework to the numerical solution of partial differential equations."
- **Why unresolved:** Current experiments validate the framework only on explicit function approximation tasks rather than the implicit constraints or residual minimization required for PDEs.
- **What evidence would resolve it:** Demonstrating the framework's ability to solve benchmark PDEs (e.g., Burgers', Allen-Cahn) with faster convergence than Physics-Informed Neural Networks using random initialization.

- **Question:** Does the structured basis design mitigate the curse of dimensionality when scaling to high-dimensional function approximation?
- **Basis in paper:** Authors state that "higher-dimensional domains" are an important extension where the strategy "may help mitigate the curse of dimensionality."
- **Why unresolved:** Paper restricts numerical validation to one- and two-dimensional settings, leaving efficiency in higher dimensions (>5D) untested.
- **What evidence would resolve it:** Comparative experiments in 3D+ domains showing basis-initialized networks maintain low MSE without exponential parameter growth.

- **Question:** Is the method's accuracy limited when approximating non-smooth or discontinuous functions compared to smooth analytical ones?
- **Basis in paper:** In Table 7, piecewise function f_6 yields MSE of 5.38×10^-3, orders of magnitude worse than smooth functions (e.g., 10^-8), yet text claims performance "remains strong."
- **Why unresolved:** Authors don't analyze degradation for piecewise case or propose mechanisms to handle discontinuities within polynomial basis framework.
- **What evidence would resolve it:** Ablation studies on functions with sharp gradients or jump discontinuities using adaptive basis degrees or hybrid activation mappings.

## Limitations
- The framework's effectiveness depends critically on the assumption that target functions can be well-approximated by polynomial basis expansions, limiting applicability to functions with smooth, continuous features.
- The logarithmic domain mapping introduces numerical complexity through the discrete scaling parameter s, which could cause instability during gradient-based fine-tuning.
- The method may struggle with high-frequency components, discontinuities, or chaotic dynamics that are orthogonal to the polynomial basis.

## Confidence
- **High Confidence**: The core mechanism of pre-training basis networks and reusing weights for initialization is well-supported by experimental results and aligns with established transfer learning principles.
- **Medium Confidence**: The domain mapping mechanism's effectiveness is demonstrated but relies on a specific mathematical transformation whose robustness across diverse function classes requires further validation.
- **Medium Confidence**: The progressive initialization strategy is theoretically sound but may accumulate errors through the initialization chain for high-degree polynomials.

## Next Checks
1. **Break Case Analysis**: Systematically test the framework on functions containing discontinuities or high-frequency components (e.g., sin(20x), step functions) to quantify the breakdown point of the polynomial basis assumption.
2. **Scaling Robustness**: Evaluate the domain mapping performance across different scaling functions (linear vs. logarithmic) to isolate the contribution of the logarithmic transformation to generalization gains.
3. **Parameter Sensitivity**: Conduct ablation studies on the progressive initialization chain to determine the optimal degree range and assess error propagation through the initialization hierarchy.