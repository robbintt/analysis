---
ver: rpa2
title: A Novel Double Pruning method for Imbalanced Data using Information Entropy
  and Roulette Wheel Selection for Breast Cancer Diagnosis
arxiv_id: '2503.12239'
source_url: https://arxiv.org/abs/2503.12239
tags:
- class
- data
- samples
- minority
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of class imbalance in breast cancer
  diagnosis, which can hinder classifier performance and reliability. The proposed
  method, RE-SMOTEBoost, is an enhanced version of SMOTEBoost that tackles this issue
  through a novel double pruning process.
---

# A Novel Double Pruning method for Imbalanced Data using Information Entropy and Roulette Wheel Selection for Breast Cancer Diagnosis

## Quick Facts
- **arXiv ID:** 2503.12239
- **Source URL:** https://arxiv.org/abs/2503.12239
- **Reference count:** 13
- **Primary result:** RE-SMOTEBoost improves breast cancer diagnosis accuracy by 3.22% and reduces variance by 88.8% on imbalanced datasets.

## Executive Summary
This paper addresses class imbalance in breast cancer diagnosis, where minority malignant cases are often underrepresented. The proposed RE-SMOTEBoost method enhances SMOTEBoost through a novel double pruning strategy that combines targeted oversampling with information-theoretic filtering. By focusing synthetic sample generation on boundary regions using roulette wheel selection and pruning both majority and synthetic samples based on entropy, the method achieves significant improvements in classification reliability. Experimental results on nine breast cancer datasets demonstrate superior performance with both higher accuracy and dramatically reduced variance compared to existing sampling techniques.

## Method Summary
RE-SMOTEBoost modifies SMOTEBoost by implementing a double pruning process at each boosting iteration. For majority classes, it calculates Shannon entropy from Gaussian NaÃ¯ve Bayes posterior probabilities and retains only high-entropy (uncertain) samples. For minority classes, it uses roulette wheel selection based on inverse Mahalanobis distance to prioritize boundary samples, generates synthetic samples via SMOTE interpolation, applies a double regularization penalty ensuring synthetic samples remain closer to their minority origin than to majority neighbors, and finally filters generated samples using entropy to remove noise. This balancing is integrated into an AdaBoost framework with decision trees as base classifiers.

## Key Results
- Achieves 3.22% higher accuracy compared to top-performing sampling algorithms
- Reduces classification variance by 88.8%, indicating more stable predictions
- Outperforms standard SMOTEBoost, RUSBoost, and other sampling methods across multiple breast cancer datasets
- Demonstrates improved reliability in medical settings with limited samples and privacy constraints

## Why This Works (Mechanism)

### Mechanism 1
The method uses Roulette Wheel Selection (RWS) with Mahalanobis distance to target synthetic sample generation at overlapping boundary regions. By selecting minority samples closest to the majority class (highest inverse distance), it biases interpolation toward the decision boundary where synthetic data is most valuable for classifier training.

### Mechanism 2
Information entropy serves as a proxy for sample informativeness, enabling simultaneous reduction of redundancy and noise. High-entropy samples (uncertain/hard to classify) are retained as they represent borderline cases essential for learning, while low-entropy samples are pruned as redundant or noisy.

### Mechanism 3
A double regularization penalty applies spatial constraints during synthetic generation to prevent class overlap. Synthetic samples are kept only if they remain geometrically closer to their minority origin than to the majority class, enforcing a clear separation margin between classes.

## Foundational Learning

- **Concept:** SMOTE (Synthetic Minority Over-sampling Technique)
  - *Why needed:* RE-SMOTEBoost modifies the standard SMOTE pipeline; understanding linear interpolation between minority neighbors is essential.
  - *Quick check:* Does standard SMOTE check if the synthetic point it creates actually belongs to the minority class region?

- **Concept:** Information Entropy (Shannon Entropy)
  - *Why needed:* The core pruning logic relies on entropy; understanding high entropy means high uncertainty is crucial.
  - *Quick check:* Would a sample with 99% probability of being Class A have high or low entropy?

- **Concept:** AdaBoost (Adaptive Boosting)
  - *Why needed:* RE-SMOTEBoost is an enhancement of SMOTEBoost ensemble; understanding iterative re-weighting of misclassified samples is essential.
  - *Quick check:* In AdaBoost, does the algorithm focus more or less on samples that were misclassified in previous rounds?

## Architecture Onboarding

- **Component map:** Input Dataset -> DoublePruning (Majority: Entropy Ranking, Minority: RWS+SMOTE+Regularization+Entropy Filter) -> AdaBoost with Decision Trees -> Final Classifier

- **Critical path:** The Noise Filter (Entropy) in the Minority module is critical; if too strict, it may generate zero synthetic samples, causing the boosting loop to fail or overfit sparse minority data.

- **Design tradeoffs:**
  - Computational Cost vs. Quality: Calculating Mahalanobis distance and Information Entropy for every sample is significantly more expensive than standard random under-sampling.
  - Aggressiveness: High pruning rates reduce dataset size (speeding training) but increase risk of deleting useful prototypes (data starvation).

- **Failure signatures:**
  - Empty Batch Error: If DoublePruning returns empty due to over-aggressive regularization
  - Metric Collapse: If variance drops to 0, indicating potential memorization of tiny sample sets

- **First 3 experiments:**
  1. Run provided heuristic for Tmax against a fixed number to verify dynamic stopping criteria efficiency
  2. Disable Double Regularization Penalty to quantify its specific contribution to F1-score
  3. Vary pruning proportion k to find tipping point where variance reduction hurts accuracy

## Open Questions the Paper Calls Out

- How can the RE-SMOTEBoost algorithm be effectively adapted for multi-class imbalanced classification problems? The current binary formulation lacks mechanisms for handling multiple overlapping minority classes simultaneously.

- Can the double pruning methodology be integrated with deep learning techniques for high-dimensional medical image analysis? The current method relies on structured feature calculations that may not translate efficiently to deep learning latent spaces.

- What is the computational complexity overhead of the double pruning process compared to standard SMOTEBoost? The paper provides extensive classification metrics but lacks time-complexity analysis or runtime comparison.

## Limitations

- Does not specify exact pruning proportion k values used in experiments, hindering precise reproduction
- Noise filtering threshold for synthetic samples described as "proportion" without defining specific values
- Impact of double regularization penalty on class distribution stability not quantified in isolation

## Confidence

- **High confidence:** Core SMOTEBoost framework modification
- **Medium confidence:** Entropy-based pruning mechanism
- **Low confidence:** Mahalanobis distance calculation for boundary detection

## Next Checks

1. Implement algorithm with multiple k values (5%, 10%, 20% of minority class) to identify sensitivity to pruning rate
2. Compare RE-SMOTEBoost performance using Manhattan vs. Euclidean distance for both selection and regularization
3. Conduct ablation studies removing each component (regularization, entropy filtering, roulette selection) to quantify individual contributions to performance gains