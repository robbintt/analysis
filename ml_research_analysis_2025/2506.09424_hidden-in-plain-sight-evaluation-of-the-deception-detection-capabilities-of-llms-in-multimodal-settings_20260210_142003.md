---
ver: rpa2
title: 'Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities
  of LLMs in Multimodal Settings'
arxiv_id: '2506.09424'
source_url: https://arxiv.org/abs/2506.09424
tags:
- deception
- detection
- reasoning
- deceptive
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates large language models (LLMs) and large multimodal
  models (LMMs) for automated deception detection across three datasets: real-life
  trial interviews (RLTD), instructed deception in interpersonal scenarios (MU3D),
  and deceptive reviews (OpSpam). The research compares zero-shot and few-shot inference
  strategies, investigates the impact of auxiliary features like non-verbal gestures
  and video summaries, and assesses different prompting methods including direct label
  generation and post-hoc reasoning.'
---

# Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings

## Quick Facts
- arXiv ID: 2506.09424
- Source URL: https://arxiv.org/abs/2506.09424
- Reference count: 29
- This study evaluates large language models (LLMs) and large multimodal models (LMMs) for automated deception detection across three datasets: real-life trial interviews (RLTD), instructed deception in interpersonal scenarios (MU3D), and deceptive reviews (OpSpam).

## Executive Summary
This study systematically evaluates the capabilities of large language models (LLMs) and large multimodal models (LMMs) for automated deception detection across three diverse datasets. The research compares zero-shot and few-shot inference strategies, investigates the impact of auxiliary features like non-verbal gestures and video summaries, and assesses different prompting methods including direct label generation and post-hoc reasoning. Results demonstrate that fine-tuned LLMs achieve state-of-the-art performance on textual deception detection tasks, particularly on OpSpam, while LMMs struggle to leverage cross-modal cues effectively. The findings highlight both the potential and limitations of LLMs in real-world deception detection applications, especially in multimodal settings.

## Method Summary
The study evaluates deception detection using three datasets: RLTD (real-life trial interviews with transcripts and non-verbal cues), MU3D (interpersonal scenarios with video), and OpSpam (deceptive reviews). It employs various models including RoBERTa-base, Whisper, CLIP for baselines, and LLaMA3.1, Gemma2, GPT-4o, LLaVA-NeXT, and Qwen2VL for LLMs/LMMs. The evaluation compares zero-shot, few-shot (with semantic similarity retrieval), and fine-tuned approaches. Different prompting strategies are tested, including direct label prediction versus post-hoc reasoning. Performance is measured using accuracy and macro F1-score with stratified 4-fold cross-validation.

## Key Results
- Fine-tuned LLMs achieve state-of-the-art performance on textual deception detection, particularly for OpSpam reviews (approximately 88 F1).
- LMMs struggle to leverage cross-modal cues effectively, underperforming simple CNN baselines on video-based deception detection.
- Direct label prediction generally outperforms reasoning-based approaches, with reasoning often introducing performance-degrading noise.
- Semantic similarity-based in-context example selection (Sim-top) significantly improves few-shot performance, particularly in semantically homogeneous domains like OpSpam.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity Anchors In-Context Learning
Similar examples prime the model with relevant linguistic patterns, reducing the search space for the classification task. This works when datasets have semantic clustering, as shown by Sim-top's 4% F1 improvement on OpSpam over random selection. The mechanism fails when datasets lack semantic clustering or when the retriever cannot distinguish deceptive from truthful semantics.

### Mechanism 2: Fine-Grained Temporal Feature Extraction (CNN vs. LMM)
CNN baselines outperform LMMs in video-based deception detection due to superior temporal resolution and domain-specific training. Deception is encoded in high-frequency temporal visual cues that LMMs miss due to lower frame sampling rates and pre-training biases toward static descriptions. This gap may close if LMMs process higher frame rates or receive specialized micro-expression training.

### Mechanism 3: Direct Label Prediction Minimizes Reasoning Noise
Direct prediction avoids the noise introduced by generating post-hoc justifications that can contradict correct classifications. The model's internal classification logits are more reliable than its ability to verbalize causal chains for this subjective task. This mechanism might reverse if models are fine-tuned on high-quality, causal reasoning chains rather than post-hoc rationalization.

## Foundational Learning

- **Concept: Binary Classification with Class Imbalance**
  - **Why needed here:** Deception datasets have varying distributions; understanding F1 is crucial for skewed data.
  - **Quick check question:** Why does the paper report F1 score alongside accuracy for the MU3D and RLTD datasets?

- **Concept: Zero-Shot vs. Few-Shot Inference**
  - **Why needed here:** The paper benchmarks both strategies; understanding their differences is central to the proposed architecture.
  - **Quick check question:** How does the "Sim-top" strategy change the composition of the prompt compared to a standard 10-shot random prompt?

- **Concept: Modality Alignment**
  - **Why needed here:** The study highlights LMMs' failure to effectively align visual and textual cues for deception detection.
  - **Quick check question:** Why does a CLIP-based visual encoder outperform a video-native LLM like LLaVA-NeXT-Video in this specific task?

## Architecture Onboarding

- **Component map:** Transcripts/Audio/Video -> Sentence-transformers (Sim-top retrieval) -> LLaMA3.1/Gemma2/GPT-4o (Direct Label prediction) -> Binary classification (Truthful/Deceptive)
- **Critical path:**
  1. Ingest transcript `t`. If multimodal, extract frames `v` and audio `a`.
  2. Embed `t` and retrieve top-k similar examples from training split.
  3. Construct prompt with retrieved examples + query `t` using Direct Label format.
  4. Generate label.
- **Design tradeoffs:** Text-only fine-tuning is currently SOTA and cheaper; multimodal LMMs are computationally expensive and underperform CNN baselines. Reasoning generation aids interpretability but statistically hurts accuracy.
- **Failure signatures:** "Minimal Hand Gestures" hallucination in LMMs; Specificity Bias causing misclassification of detailed deceptive reviews as truthful.
- **First 3 experiments:**
  1. Run zero-shot inference on OpSpam using LLaMA 3.1 to verify ~59% F1 baseline.
  2. Compare Direct Label vs. Post-Hoc Reasoning on RLTD subset to confirm performance drop.
  3. Implement random k-shot vs. Sim-top k-shot retriever for MU3D to quantify in-context example impact.

## Open Questions the Paper Calls Out
- Can LLMs effectively generalize to detect AI-generated deceptive content, distinct from human deception?
- How do linguistic and cultural nuances impact the reliability of LLM-based deception detection?
- Can LMMs be optimized to capture the fine-grained temporal dynamics required for video-based deception detection?
- What prompting mechanisms can ensure that reasoning generation aids deception detection rather than degrading accuracy?

## Limitations
- The study focuses exclusively on English-language datasets, limiting generalizability to multilingual contexts.
- LMMs' poor multimodal performance may stem from limited temporal resolution rather than fundamental architectural constraints.
- Semantic similarity retrieval mechanism relies on an unspecified sentence-transformer model, creating reproducibility challenges.

## Confidence

**High Confidence**: Fine-tuned LLMs achieving state-of-the-art performance on textual deception detection, particularly for OpSpam reviews.

**Medium Confidence**: LMMs struggling to leverage cross-modal cues due to pre-training biases toward static descriptions.

**Low Confidence**: Direct label prediction universally outperforming reasoning-based approaches across all deception detection scenarios.

## Next Checks

1. **Temporal Resolution Experiment**: Test whether LMMs' poor multimodal performance persists when processing video at higher frame rates (10 fps) to isolate whether the limitation is temporal resolution or architectural.

2. **Prompt Format Sensitivity**: Systematically vary the direct label prediction prompt format to establish the robustness of the observed performance advantage over reasoning approaches.

3. **Dataset Diversity Validation**: Apply the best-performing Sim-top retrieval strategy to a more diverse deception dataset with less semantic clustering to verify whether performance gains generalize beyond semantically homogeneous domains.