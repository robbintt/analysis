---
ver: rpa2
title: 'SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level
  Detoxification'
arxiv_id: '2512.15052'
source_url: https://arxiv.org/abs/2512.15052
tags:
- multimodal
- toxic
- safety
- toxicity
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of toxicity in multimodal large
  language models (MLLMs), which can inherit harmful content from training data and
  produce unsafe outputs, especially under adversarial conditions. Existing detoxification
  methods mostly operate at the input or output level and struggle with internal multimodal
  fusion, prompting the need for white-box, neuron-level intervention.
---

# SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification

## Quick Facts
- arXiv ID: 2512.15052
- Source URL: https://arxiv.org/abs/2512.15052
- Authors: Hongbo Wang; MaungMaung AprilPyone; Isao Echizen
- Reference count: 40
- Key outcome: Reduces harmful output rates from 48.2% to 2.5% while preserving fluency and multimodal reasoning

## Executive Summary
This paper tackles the problem of toxicity in multimodal large language models (MLLMs), which can inherit harmful content from training data and produce unsafe outputs, especially under adversarial conditions. Existing detoxification methods mostly operate at the input or output level and struggle with internal multimodal fusion, prompting the need for white-box, neuron-level intervention. The authors propose SGM, a training-free method that identifies and softly suppresses toxic expert neurons in post-fusion layers via AUROC-based selection and expertise-weighted attenuation, without updating model parameters. To enable reliable evaluation, they construct MM-TOXIC-QA, a multimodal toxicity benchmark with high-quality toxic/non-toxic image-text pairs. Experiments on open-source MLLMs show SGM reduces harmful output rates from 48.2% to 2.5% while preserving fluency and multimodal reasoning. The method transfers across models and can be combined with existing approaches (SGM*) for stronger safety.

## Method Summary
SGM (Safety Glasses for MLLMs) is a training-free neuron-level detoxification method that operates on post-fusion layers of multimodal models. It identifies toxic expert neurons using AUROC-based selection from a curated multimodal toxic dataset, then applies expertise-weighted soft suppression to these neurons during inference. The method is architecture-agnostic and compatible with existing detoxification approaches. The authors also introduce MM-TOXIC-QA, a high-quality multimodal toxicity benchmark for evaluation.

## Key Results
- Reduces harmful output rates from 48.2% to 2.5% on tested MLLMs
- Preserves multimodal reasoning capabilities and fluency
- Transfers across different model architectures and can be combined with existing detoxification methods (SGM*)

## Why This Works (Mechanism)
SGM works by identifying and suppressing toxic expert neurons at the post-fusion stage of MLLMs. Unlike input/output-level methods that struggle with multimodal fusion, this neuron-level approach directly targets the internal representation where toxic patterns are encoded. The AUROC-based selection identifies neurons most strongly associated with toxic responses, while the expertise-weighted attenuation mechanism allows for soft suppression that preserves model capabilities on non-toxic tasks. The training-free nature enables application to any pre-trained model without fine-tuning overhead.

## Foundational Learning
- **AUROC-based neuron selection**: Why needed - to identify toxic expert neurons; Quick check - neurons with highest AUROC scores show strongest correlation with toxic outputs
- **Post-fusion layer targeting**: Why needed - toxic patterns are encoded after multimodal fusion; Quick check - suppressing pre-fusion neurons shows minimal effect on toxicity
- **Expertise-weighted soft suppression**: Why needed - to maintain model capabilities while reducing toxicity; Quick check - soft suppression preserves performance on non-toxic tasks
- **Multimodal toxicity benchmarking**: Why needed - existing benchmarks lack quality toxic/non-toxic pairs; Quick check - MM-TOXIC-QA shows clear separation between toxic and non-toxic samples
- **Training-free adaptation**: Why needed - to avoid computational overhead of fine-tuning; Quick check - SGM can be applied to any pre-trained model without parameter updates

## Architecture Onboarding

**Component Map:**
Multimodal Input -> Fusion Layer -> Post-Fusion Layer -> SGM Neuron Suppression -> Output Layer

**Critical Path:**
The critical path is the identification of toxic expert neurons through AUROC scoring, followed by their soft suppression during inference. This neuron-level intervention directly impacts the post-fusion representations before they reach the output generation stage.

**Design Tradeoffs:**
- Training-free approach trades potential performance gains from fine-tuning for immediate applicability
- Soft suppression balances toxicity reduction against capability preservation
- AUROC-based selection provides interpretable targeting but may miss complex toxic patterns
- Post-fusion targeting ensures effectiveness but requires access to model internals

**Failure Signatures:**
- Insufficient toxicity reduction indicates poor neuron identification or inadequate suppression strength
- Loss of multimodal reasoning suggests over-suppression of non-toxic neurons
- Inconsistent performance across different toxic categories suggests benchmark bias or limited neuron coverage

**First 3 Experiments to Run:**
1. AUROC-based neuron identification on MM-TOXIC-QA benchmark to verify toxic neuron detection
2. Soft suppression effectiveness test comparing toxic output rates before and after SGM application
3. Capability preservation test measuring performance on non-toxic multimodal reasoning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- AUROC-based neuron selection assumes identified neurons generalize across toxic scenarios, unverified for novel attacks
- Effectiveness demonstrated primarily on open-source MLLMs, scalability to proprietary models unclear
- MM-TOXIC-QA benchmark may not capture full diversity of real-world multimodal toxicity

## Confidence

**High:** Method's ability to reduce toxic outputs in controlled experimental settings
**Medium:** Claims about method's transferability across models and compatibility with other approaches
**Low:** Assertions about robustness to adversarial attacks or performance in dynamic user environments

## Next Checks
1. Evaluate SGM's robustness against adversarial prompts designed to bypass neuron-level suppression, including those targeting underrepresented