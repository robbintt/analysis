---
ver: rpa2
title: 'LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?'
arxiv_id: '2601.20009'
source_url: https://arxiv.org/abs/2601.20009
tags:
- language
- task
- layers
- consistency
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LinguaMap identifies and characterizes two failure modes in multilingual
  large language models: the multilingual transfer bottleneck (correct language, incorrect
  task response) and the language consistency bottleneck (correct task response, wrong
  language). The paper introduces a four-scenario evaluation protocol spanning MMLU,
  MGSM, and XQuAD benchmarks and uses logit lens analysis and semantic similarity
  evaluation to probe internal representations.'
---

# LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?

## Quick Facts
- arXiv ID: 2601.20009
- Source URL: https://arxiv.org/abs/2601.20009
- Reference count: 34
- Primary result: Selective fine-tuning of only the final 2-3% of parameters fixes multilingual language control failures, achieving >98% language consistency across six languages without sacrificing task accuracy.

## Executive Summary
LinguaMap addresses a critical failure in multilingual large language models: responses often default to English or mix languages incorrectly, even when prompted in another language. The paper introduces a layer-wise interpretability framework that reveals a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. This insight enables a selective fine-tuning method that targets only the final layers responsible for language control, achieving near-identical performance to full fine-tuning while using a fraction of the computational resources.

## Method Summary
The method combines interpretability analysis with selective fine-tuning. First, logit lens analysis projects hidden states through the unembedding matrix layer-by-layer to track language probabilities and semantic similarity across layers. This reveals when language-specific control emerges. Second, guided by these insights, selective SFT fine-tunes only the final 2-3 layers (3-5% of parameters) on MMLU Business data using code-switched prompts (English instruction, target-language question). The approach uses AdamW optimizer with LR=1e-5, batch size=16, 5 epochs, and masks loss to Q, R, A tokens only.

## Key Results
- Identified two failure modes: multilingual transfer bottleneck (correct language, incorrect task response) and language consistency bottleneck (correct task response, wrong language)
- Three-phase internal structure discovered: early layers align inputs into shared semantic space, middle layers perform task reasoning, late layers drive language-specific generation
- Selective fine-tuning of only final 2-3% of parameters achieves >98% language consistency across six languages while preserving task accuracy
- Method uses 95% fewer parameters than full fine-tuning but achieves near-identical performance

## Why This Works (Mechanism)
The three-phase structure explains why language control fails: task reasoning and language generation are separated in the network. Early layers handle semantic understanding in a language-agnostic way, middle layers reason about the task, and late layers must switch to the target language. When these late layers aren't properly trained for multilingual control, they default to English or mix languages. By selectively fine-tuning only these late layers with code-switched prompts, the model learns to control language generation without disrupting the well-trained task reasoning capabilities in earlier layers.

## Foundational Learning
- **Logit Lens Analysis**: Projects hidden states through unembedding matrix to decode token probabilities at each layer; needed to track language emergence across layers; quick check: compare projected distributions to ground truth tokens
- **Semantic Similarity Tracking**: Computes cosine similarity between mean-pooled hidden states across languages; needed to identify when inputs align into shared semantic space; quick check: plot similarity curves across layers
- **Code-Switched Prompting**: Uses English instructions with target-language questions; needed to teach language control without retraining task reasoning; quick check: verify language consistency on held-out data
- **Selective Layer Freezing**: Freezes all but final 2-3 layers during fine-tuning; needed to preserve task capabilities while learning language control; quick check: monitor task accuracy during fine-tuning
- **Language Detection via LangDetect**: Automated evaluation of response language consistency; needed for quantitative assessment of language control; quick check: validate with human annotation on sample
- **Chain-of-Thought Alignment**: Ensures reasoning traces match question language; needed for coherent multilingual reasoning; quick check: verify CoT language matches target language

## Architecture Onboarding

**Component Map**: Input Prompt -> Early Layers (Semantic Alignment) -> Middle Layers (Task Reasoning) -> Late Layers (Language Generation) -> Output Response

**Critical Path**: The language generation pathway through late layers is critical. Early layers handle semantic understanding, middle layers perform reasoning, but language-specific generation happens in the final layers. The critical insight is that these can be trained separately.

**Design Tradeoffs**: Full fine-tuning would fix language issues but requires 20-30x more computation and risks catastrophic forgetting of task capabilities. Selective fine-tuning preserves task performance while dramatically reducing compute, but requires precise identification of which layers control language.

**Failure Signatures**: 
- Random layer selection causes catastrophic task performance drop (near-zero accuracy)
- Insufficient fine-tuning epochs (less than 5) leads to poor language consistency
- Incorrect loss masking (including P and I tokens) slows convergence
- Wrong layer cutoff (not final 2-3) fails to learn language control

**First Experiments**:
1. Run logit lens analysis on base model to identify language control emergence layer
2. Implement selective SFT with final 2 layers frozen and monitor language consistency
3. Compare selective SFT against full fine-tuning on held-out language benchmarks

## Open Questions the Paper Calls Out

**Open Question 1**: How can explicit reasoning-level disambiguation strategies mitigate task performance degradation caused by English distractors?
The paper notes that task performance remains weak under English distractor prompts, requiring more explicit reasoning-level disambiguation strategies in future work.

**Open Question 2**: Does the three-phase multilingual structure persist at the token level?
The paper's mean-pooled analysis may hide token-level divergence, leaving the fine-grained structure unproven.

**Open Question 3**: Does selective late-layer fine-tuning enable zero-shot cross-lingual transfer to languages unseen during training?
The fine-tuning experiments are restricted to specific target languages; the model's ability to generalize control to other languages is not evaluated.

## Limitations
- Training data selection relies on "Claude 3.5 Sonnet verification" without releasing the filtered dataset or specifying random seeds
- Chain-of-thought augmentation method is described but not implemented in detail
- Layer cutoff criterion for language control emergence uses informal convergence thresholds rather than formalized stopping rules
- Evaluation focuses on MMLU Business subset with five languages; results may not extend to broader domains or language families

## Confidence
- Method framework (High): Layer-wise interpretability and four-scenario evaluation are well-documented and reproducible
- Selective fine-tuning effectiveness (High): Strong empirical evidence for tested models and languages
- Generalizability (Medium): Limited by lack of detailed training data specification and focus on specific model architectures
- Layer identification criteria (Medium): Informal thresholds reduce reproducibility across different model sizes

## Next Checks
1. Implement and test the exact training data selection pipeline using the same verification criteria to confirm the 2500-sample subset generation
2. Apply the selective fine-tuning method to additional model architectures (e.g., Llama, Mistral) and language families (e.g., Slavic, Turkic) to test generalizability
3. Develop and validate formalized criteria for identifying the language control emergence layer threshold across different model sizes and architectures