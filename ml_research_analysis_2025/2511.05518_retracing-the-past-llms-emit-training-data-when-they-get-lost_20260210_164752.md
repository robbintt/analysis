---
ver: rpa2
title: 'Retracing the Past: LLMs Emit Training Data When They Get Lost'
arxiv_id: '2511.05518'
source_url: https://arxiv.org/abs/2511.05518
tags:
- data
- training
- memorization
- first
- city
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Confusion-Inducing Attacks (CIA), a framework
  for extracting memorized training data from large language models by maximizing
  token-level prediction entropy. CIA identifies that memorized text emission during
  model divergence is preceded by sustained high-entropy spikes and optimizes input
  prompts to deliberately induce this state.
---

# Retracing the Past: LLMs Emit Training Data When They Get Lost

## Quick Facts
- arXiv ID: 2511.05518
- Source URL: https://arxiv.org/abs/2511.05518
- Reference count: 18
- Primary result: CIA achieves up to 22.2% verbatim extraction on unaligned LLAMA2 (70B) and up to 18.8% on aligned LLAMA3-INSTRUCT (70B) via entropy-optimized prompts

## Executive Summary
This paper introduces Confusion-Inducing Attacks (CIA), a novel framework for extracting memorized training data from large language models by deliberately inducing model confusion. The key insight is that memorized text emerges during model divergence, which is reliably preceded by sustained spikes in token-level prediction entropy. CIA optimizes input prompts to maximize this entropy, triggering the emission of memorized content. For aligned models, the framework is enhanced with mismatched Supervised Fine-tuning (SFT), where models are fine-tuned on irrelevant data to increase vulnerability to memorization attacks. The method requires no access to training data and outperforms existing baselines across multiple model sizes and alignments.

## Method Summary
CIA works by maximizing token-level prediction entropy through optimized prompts, identifying that high-entropy states precede memorized text emission during model confusion. The attack framework consists of entropy maximization followed by prompt optimization to trigger memorized content release. For aligned models, CIA+SFT first applies mismatched fine-tuning with irrelevant data before executing the attack, increasing susceptibility to memorization. The approach is tested across LLAMA2 (70B, 13B), LLAMA1 (65B), and LLAMA3-INSTRUCT (70B) models, comparing performance against baselines without requiring training data access.

## Key Results
- Unaligned LLAMA2 (70B): up to 22.2% verbatim extraction rate
- Unaligned LLAMA1 (65B): up to 16.0% verbatim extraction rate
- Aligned LLAMA3-INSTRUCT (70B): up to 18.8% extraction rate with CIA+SFT
- CIA significantly outperforms existing baselines across all tested models

## Why This Works (Mechanism)
CIA exploits the relationship between model confusion and memorization emergence. When models become uncertain about predictions (high entropy), they are more likely to revert to memorized training patterns. By optimizing prompts to maximize entropy, the attack deliberately induces this confused state, triggering the emission of memorized text. The mismatched SFT component increases vulnerability by disrupting the model's normal reasoning patterns through irrelevant fine-tuning, making it more susceptible to confusion-based attacks.

## Foundational Learning

- **Token-level prediction entropy**: Measures uncertainty in next-token predictions; needed to identify model confusion states; quick check: calculate entropy across token predictions in sample outputs
- **Memorization emergence**: Process where models revert to training data during uncertainty; needed to understand attack target; quick check: compare model outputs during high vs low entropy states
- **Prompt optimization**: Technique for crafting inputs that maximize desired model responses; needed to induce targeted confusion; quick check: evaluate different prompt formulations for entropy maximization
- **Mismatched fine-tuning**: Training on irrelevant data to disrupt normal model behavior; needed to increase aligned model vulnerability; quick check: compare memorization rates pre/post mismatched SFT
- **Model divergence**: State where models produce unexpected or uncharacteristic outputs; needed to identify memorization opportunities; quick check: measure output deviation from typical patterns
- **Entropy spikes**: Sustained periods of high prediction uncertainty; needed as attack indicators; quick check: detect and measure duration of high-entropy periods

## Architecture Onboarding

**Component map**: Prompt Optimizer -> Entropy Maximizer -> Model Input -> Output Monitor -> Memorization Detector

**Critical path**: CIA execution follows: (1) Prompt optimization to maximize entropy, (2) Model input with optimized prompt, (3) Output monitoring for entropy spikes, (4) Detection of memorized text emission

**Design tradeoffs**: CIA trades attack specificity for broad applicability, requiring no training data access but potentially producing noisy outputs. CIA+SFT increases effectiveness on aligned models but requires additional fine-tuning resources and introduces complexity in identifying suitable mismatched datasets.

**Failure signatures**: Attack failure manifests as sustained high entropy without memorized text emergence, or low entropy outputs that remain coherent but lack memorized content. Baseline comparisons help distinguish between inherent model resistance and attack execution issues.

**3 first experiments**: (1) Run baseline entropy measurement on clean model outputs, (2) Execute CIA on unaligned LLAMA2 with entropy monitoring, (3) Apply mismatched SFT to aligned model before CIA execution

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Generalizability uncertain across diverse model architectures beyond tested LLAMA series
- Theoretical explanation lacking for why entropy spikes reliably predict memorization
- Mechanisms of mismatched SFT enhancement require further investigation
- Operational significance of extraction rates in real-world privacy contexts not fully explored

## Confidence
- High: Experimental results showing CIA effectiveness on unaligned LLAMA models
- Medium: CIA+SFT combined attack results and entropy-memorization relationship
- Low: Broader claims about CIA applicability across all model types and training regimes

## Next Checks
1. Test CIA on broader range of model families including transformer variants, different parameter scales, and diverse training corpora
2. Conduct controlled experiments varying fine-tuning dataset characteristics to isolate mismatched SFT properties
3. Implement real-time monitoring during CIA execution to verify consistent entropy spike-memorization relationship across multiple instances