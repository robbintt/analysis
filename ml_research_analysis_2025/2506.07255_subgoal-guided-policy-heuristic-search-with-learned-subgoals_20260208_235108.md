---
ver: rpa2
title: Subgoal-Guided Policy Heuristic Search with Learned Subgoals
arxiv_id: '2506.07255'
source_url: https://arxiv.org/abs/2506.07255
tags:
- search
- policy
- problems
- subgoal
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently training policy-guided
  tree search algorithms in single-agent deterministic search problems, where complete
  solution trajectories are costly to obtain. The authors propose a novel method for
  learning subgoal-based policies that can utilize both successful and failed search
  attempts during online training.
---

# Subgoal-Guided Policy Heuristic Search with Learned Subgoals

## Quick Facts
- **arXiv ID**: 2506.07255
- **Source URL**: https://arxiv.org/abs/2506.07255
- **Reference count**: 40
- **Primary result**: Subgoal-guided policies learned from both successful and failed searches solve all test problems while requiring fewer node expansions than baselines like PHS*, LevinTS, and WA*.

## Executive Summary
This paper addresses the challenge of efficiently training policy-guided tree search algorithms when complete solution trajectories are costly to obtain. The authors propose a novel method that learns subgoal-based policies by utilizing both successful and failed search attempts during online training. Their approach uses a hierarchical policy structure with a VQVAE-based subgoal generator and low-level policies conditioned on subgoals, combined with a high-level policy over subgoals. Experiments demonstrate significant improvements in sample efficiency compared to existing methods, particularly in hard problem instances where other approaches fail to learn effective policies.

## Method Summary
The method learns subgoal-based policies through a hierarchical architecture where a VQVAE codebook discretizes the state space into latent subgoals. For each state, multiple low-level policies propose actions conditioned on potential subgoals, weighted by a high-level policy. The key innovation is utilizing failed search trees by clustering them with the Louvain algorithm to generate training pairs, extracting useful subgoal transitions even from unsuccessful attempts. The system trains online through a bootstrap process, doubling the search budget when no progress is made and halving it when significant improvement occurs.

## Key Results
- Solves all test problems in challenging environments where baselines like PHS*, LevinTS, and WA* struggle
- Requires fewer node expansions than existing methods due to more effective search priors
- Particularly effective in hard problem instances where other approaches fail to learn effective policies
- Robust to reconstruction inaccuracies in subgoal generation, preserving search completeness

## Why This Works (Mechanism)

### Mechanism 1: Utilization of Failed Search Data
If the system extracts training data from budget-limited failed search trees, sample efficiency may improve compared to relying solely on complete solution trajectories. The Louvain algorithm clusters the expanded nodes from failed searches into a hierarchy based on modularity. By sampling state pairs from adjacent clusters and finding paths between them, the system generates "successful" sub-trajectories for training, effectively recycling wasted computation. This works because the community structure identified by Louvain in partial search trees correlates with meaningful subgoals that generalize to unsolved problem instances.

### Mechanism 2: Hierarchical Policy Weighting
Replacing a flat policy with a weighted geometric mixture of subgoal-conditioned policies likely improves the search prior, reducing the effective depth of the search tree. A VQVAE codebook discretizes the state space into k latent subgoals. For a state s, k low-level policies propose actions conditioned on each potential subgoal. A high-level policy weights these proposals, decomposing complex navigation problems into "reach subgoal" sub-problems. This allows the search algorithm to leverage structured exploration by learning which subgoals are most relevant in different contexts.

### Mechanism 3: Robustness via Action-Space Search
Searching in the original action space while using subgoals only for conditioning likely preserves completeness better than searching directly in a latent subgoal space. Unlike subgoal-search methods where an expansion generates a latent state (risking unreachability), this method uses the VQVAE reconstruction only to bias the probability distribution of primitive actions. The underlying search retains the ability to select any valid action, ensuring the reconstructed subgoal need not be perfectly accurate to be useful. Imperfect subgoal reconstructions still provide a gradient signal better than random exploration.

## Foundational Learning

- **Concept: Policy-Guided Heuristic Search (PHS*)**
  - **Why needed here:** The proposed method acts as a drop-in replacement for the policy π in the PHS* evaluation function φPHS(n). Understanding how φ balances policy probability π(n) and heuristic cost h(n) is required to interpret the search behavior.
  - **Quick check question:** In PHS*, does a lower policy probability π(n) result in a higher or lower priority for expansion (assuming constant cost)?

- **Concept: Vector Quantized Variational Autoencoders (VQVAE)**
  - **Why needed here:** The subgoal generator relies on a discrete codebook to represent transitions. Understanding the quantization loss and commitment loss is necessary for debugging the subgoal generator.
  - **Quick check question:** What happens to the codebook vectors if the commitment loss (∥sg(ze) - ei∥22) is removed from the optimization?

- **Concept: Graph Clustering (Louvain Method)**
  - **Why needed here:** This is the structural engine for the "learning from failure" mechanism. It detects communities in the state-transition graph of failed searches to sample training pairs.
  - **Quick check question:** Does the Louvain algorithm maximize the number of edges inside clusters or minimize the edges between clusters?

## Architecture Onboarding

- **Component map:** Search Driver -> Neural Inference (VQVAE Encoder/Decoder + ResNet policies) -> Data Pipeline (Graph Builder -> Louvain Clusterer -> Trajectory Sampler) -> Trainer (Online Bootstrap loop)
- **Critical path:** 1. Search: Expand nodes using current πSG. 2. Failure: If budget exhausted, convert tree to graph G0. 3. Cluster: Run Louvain to find hierarchy (G0, …, GN). 4. Sample: Pick neighboring clusters, sample scur, star, find path in G0. 5. Update: Train VQVAE and Low-Level Policy on this path.
- **Design tradeoffs:**
  - **Codebook Size (k):** A small k (e.g., 4) forces the model to learn coarse transitions (faster, more general). A large k risks "mode collapse" where entries are unused, resulting in a near-uniform high-level policy.
  - **Clustering Level:** Sampling from G1 (fine-grained) yields short trajectories; G5 (coarse) yields long trajectories. The paper suggests intermediate levels (e.g., G3) strike the best balance for training.
- **Failure signatures:**
  - **Stagnant Loss:** VQVAE reconstruction loss decreases, but search node expansions do not improve (implies subgoals are learnable but not useful for navigation).
  - **Uniform High-Level Policy:** πhi outputs equal probabilities for all codebook entries, effectively ignoring the subgoal mechanism.
- **First 3 experiments:**
  1. **Overhead Analysis:** Run LevinTS(π) vs. LevinTS(πSG) with random weights to measure the raw inference overhead of the hierarchical architecture without learning.
  2. **Ablation (No Failure Learning):** Disable the Louvain pipeline. Train only on solution trajectories to quantify the contribution of the "failed tree" data (replicating Figure 3).
  3. **Reconstruction Stress Test:** Evaluate PHS*(πSG) on a domain with high visual complexity (e.g., BoulderDash) to confirm robustness to VQVAE error compared to HIPS-ε.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can dynamically removing unused codebook entries from the VQVAE during training improve the mixture policy's guidance quality? The current fixed codebook approach (k=4) was empirically selected, but no mechanism exists to prune or adapt codebook entries during training based on utilization.
- **Open Question 2:** Why does the Louvain clustering algorithm fail to discover helpful subgoal structures in Sokoban while succeeding in other domains? The modularity-based clustering may not capture Sokoban's deadlock-prone structure where box positioning creates irreversible states, unlike key-lock dependencies in other domains.
- **Open Question 3:** How does the computational overhead of running multiple neural networks per node expansion scale with problem complexity, and can it be reduced while maintaining sample efficiency gains? The paper acknowledges that PHS*(πSG) requires "a higher computational cost per node expansion due to the use of several networks" but does not quantify this overhead or explore efficiency optimizations.

## Limitations

- The Louvain clustering step's effectiveness depends on sufficient search budget to form meaningful clusters
- The method's performance in highly stochastic or partially observable environments remains untested
- The specific architectural parameters (codebook size, clustering parameters) may require domain-specific tuning
- The computational overhead of the hierarchical policy during inference is not thoroughly characterized

## Confidence

- **High confidence:** Overall approach and experimental methodology
- **Medium confidence:** Generalization of the "learning from failure" mechanism across diverse domains
- **Medium confidence:** Robustness of the hierarchical policy weighting under varying problem complexities
- **Low confidence:** Specific architectural choices (codebook size, clustering parameters) without domain-specific tuning

## Next Checks

1. **Ablation study**: Disable the Louvain failure-learning pipeline and train only on successful trajectories to quantify its contribution
2. **Reconstruction stress test**: Evaluate performance in domains with high visual complexity to verify robustness to VQVAE errors
3. **Parameter sensitivity analysis**: Systematically vary codebook size and clustering levels across domains to identify optimal configurations