---
ver: rpa2
title: 'From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives'
arxiv_id: '2509.11803'
source_url: https://arxiv.org/abs/2509.11803
tags:
- patient
- llms
- language
- clinical
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the Noisy Diagnostic Benchmark (NDB), a synthetic
  dataset simulating patient-generated narratives with varying levels of linguistic
  noise and ambiguity. The benchmark evaluates large language models' ability to infer
  diagnoses from free-form, noisy text, addressing a critical gap in clinical NLP
  research.
---

# From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives

## Quick Facts
- arXiv ID: 2509.11803
- Source URL: https://arxiv.org/abs/2509.11803
- Reference count: 20
- Primary result: Synthetic benchmark shows Flan-T5 achieves 97.1% accuracy on clean text vs. 87.1% on high-noise narratives, demonstrating superior robustness to linguistic degradation compared to BERT and ClinicalBERT

## Executive Summary
This study introduces the Noisy Diagnostic Benchmark (NDB), a synthetic dataset simulating patient-generated narratives with varying levels of linguistic noise and ambiguity. The benchmark evaluates large language models' ability to infer diagnoses from free-form, noisy text, addressing a critical gap in clinical NLP research. Fine-tuning BERT, ClinicalBERT, and Flan-T5 on clean clinical text, the study shows a measurable drop in accuracy as input noise increases: BERT falls from 98.3% to 79.2%, ClinicalBERT from 97.9% to 86.2%, and Flan-T5 from 97.1% to 87.1%. Flan-T5 demonstrated the highest robustness to noise. NDB provides a reproducible framework for stress-testing diagnostic models on realistic patient communication.

## Method Summary
The study generates 3,600 narrative triplets from the Symptom-based Disease Labeling Dataset (SDPD), creating parallel descriptions at three noise levels (clean, medium, heavy) using LLM prompting. Three models are fine-tuned on the clean subset only: BERT (general), ClinicalBERT (MIMIC-pretrained), and Flan-T5 (instruction-tuned encoder-decoder). Models are then evaluated on all noise tiers to measure degradation curves. The synthetic data enables controlled ablation of linguistic quality while preserving clinical content, though the study acknowledges the need for validation against real patient narratives.

## Key Results
- Flan-T5 demonstrated highest robustness with only 10% accuracy drop (97.1% → 87.1%) versus BERT's 19.1% drop (98.3% → 79.2%)
- ClinicalBERT achieved 97.9% on clean text but showed inconsistent degradation (83.8% medium → 86.2% high noise)
- Encoder-decoder architecture (Flan-T5) outperformed encoder-only models (BERT, ClinicalBERT) across all noise levels
- NDB enables reproducible stress-testing of diagnostic models on realistic patient communication patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoder-decoder architectures (Flan-T5) exhibit greater robustness to linguistic noise than encoder-only models (BERT, ClinicalBERT) in diagnostic classification tasks.
- Mechanism: Instruction-tuned encoder-decoder models employ a generative decoding process that can reconstruct corrupted input semantics during generation, whereas encoder-only models map directly from noisy representations to classification tokens without intermediate reconstruction opportunities.
- Core assumption: The generative pathway in T5-style architectures provides implicit denoising capability during sequence-to-sequence inference.
- Evidence anchors:
  - [abstract] "Flan-T5 demonstrated the highest robustness to noise" with only a 10% accuracy drop (97.1% → 87.1%) versus BERT's 19.1% drop (98.3% → 79.2%).
  - [results] Table 2 shows Flan-T5 achieving 92.5% and 87.1% on medium and high noise, outperforming both BERT variants.
  - [corpus] Related work on medical LLM benchmarking (MedArabiQ, CounselBench) does not directly address encoder-decoder vs. encoder-only noise robustness—evidence is limited to this study's three-model comparison.
- Break condition: If the noise distribution shifts from synthetic to real patient speech with acoustic artifacts, audio-ASR cascades may introduce error patterns not captured by text-only noise injection (see "Benchmarking Automatic Speech Recognition coupled LLM Modules" in corpus).

### Mechanism 2
- Claim: Domain-specific pre-training (ClinicalBERT) does not guarantee improved robustness to patient-generated linguistic noise compared to general-purpose pre-training.
- Mechanism: ClinicalBERT's pre-training on MIMIC notes optimizes for clinician-authored medical language patterns, which may not align with layperson symptom descriptions containing colloquialisms, disfluencies, and tangential narratives.
- Core assumption: Noise robustness depends more on exposure to diverse linguistic registers during pre-training than on domain-specific medical vocabulary.
- Evidence anchors:
  - [results] ClinicalBERT achieved 97.9% on clean text but dropped to 83.8% (medium) and 86.2% (high noise)—performing worse than Flan-T5 and showing inconsistent degradation patterns.
  - [literature review] Section II-A notes that LLMs pretrained on "enormous text corpora that include forums, Q&A sites" learn mappings between colloquial and medical language.
  - [corpus] No corpus papers directly compare ClinicalBERT vs. BERT on noisy patient text; this remains an underexplored area.
- Break condition: If real patient narratives contain domain-specific medical terminology mixed with noise (e.g., patients repeating clinician language incorrectly), ClinicalBERT's vocabulary knowledge may provide marginal benefits not observed with synthetic noise.

### Mechanism 3
- Claim: Synthetic noise generation via LLM prompting can produce controlled, reproducible stress tests that reveal model brittleness under realistic linguistic degradation.
- Mechanism: LLMs generate parallel narratives at multiple noise levels while preserving ground-truth diagnoses, enabling controlled ablation of linguistic quality while holding clinical content constant.
- Core assumption: LLM-generated "noisy" text accurately simulates the distribution of real patient self-descriptions.
- Evidence anchors:
  - [methodology] Section III-A describes generating 3,600 "narrative triplet examples" with "no-noise," "medium noise," and "heavy noise" levels using LLM prompting.
  - [table 1] Shows concrete examples: clean clinical description → colloquial narrative with digressions → highly fragmented speech with irrelevant details.
  - [corpus] Related work on simulated medical interviews (Fareez et al., 2022) validates synthetic dialogue approaches, but corpus papers do not specifically validate LLM-generated noise distributions against real patient corpora.
- Break condition: If real patient noise includes patterns not captured by LLM simulation (e.g., code-switching, speech recognition errors, dysarthria), benchmark performance may not generalize to deployment conditions.

## Foundational Learning

- **Concept: Noise dimensions in clinical text (lexical, syntactic, semantic)**
  - Why needed here: The NDB introduces "varying levels and types of noise" including misspellings (lexical), run-on sentences (syntactic), and irrelevant tangents (semantic). Understanding these categories is prerequisite to interpreting degradation patterns.
  - Quick check question: Given the heavy-noise example in Table 1, identify one lexical, one syntactic, and one semantic noise element.

- **Concept: Encoder-only vs. encoder-decoder architectures**
  - Why needed here: The study compares BERT-style bidirectional encoders against Flan-T5's sequence-to-sequence architecture. Architecture choice directly impacts noise handling behavior.
  - Quick check question: Why might a generative decoder provide implicit denoising that a classification head cannot?

- **Concept: Synthetic data validation gaps**
  - Why needed here: NDB is entirely synthetic; the paper does not validate noise realism against real patient corpora. Interpreting results requires understanding this limitation.
  - Quick check question: What validation steps would strengthen confidence that synthetic noise reflects real patient communication patterns?

## Architecture Onboarding

- **Component map:** SDPD symptom sets → LLM noise generator (3 levels) → 3,600 narrative triplets → BERT/ClinicalBERT/Flan-T5 fine-tuning (clean only) → accuracy evaluation across noise tiers

- **Critical path:**
  1. Sample symptom sets from SDPD
  2. Generate parallel narratives at 3 noise levels via LLM prompting
  3. Fine-tune each model on clean clinical subset only
  4. Evaluate on all noise tiers; measure degradation curves

- **Design tradeoffs:**
  - Training on clean text only vs. including noisy samples: The study trains on clean data to isolate robustness testing, but real deployment may benefit from noise-augmented training
  - Synthetic vs. real patient data: Synthetic enables controlled experiments but may not generalize; real patient data has privacy constraints and uncontrolled noise
  - Model selection: Flan-T5 shows best robustness but is computationally heavier than BERT for inference

- **Failure signatures:**
  - Sharp accuracy drop between clean and medium noise indicates poor generalization to colloquial language
  - Inconsistent degradation (e.g., ClinicalBERT: 83.8% medium → 86.2% high) suggests noise levels may not be linearly ordered or model is learning spurious correlations
  - Accuracy below ~85% on noisy inputs indicates models are unreliable for patient-facing triage without additional safeguards

- **First 3 experiments:**
  1. Reproduce the NDB evaluation on your local infrastructure using the released dataset (https://github.com/lielsheri/PatientSignal) to establish baseline metrics for your model of interest.
  2. Add noise-augmented samples to the fine-tuning set and measure whether robustness improves—this tests whether training distribution mismatch is the primary failure mode.
  3. Evaluate on real patient-generated text (e.g., MedDialog, AskDocs) to assess whether synthetic noise generalizes; expect performance gaps that reveal simulation limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does fine-tuning diagnostic models specifically on the Noisy Diagnostic Benchmark (NDB) recover the accuracy lost when models trained on clean data are exposed to noisy inputs?
- **Basis in paper:** [inferred] The authors identify a "critical blind spot" where models trained on clean data fail on noisy data and call for "noise-aware training protocols," yet their experiment only evaluates models trained on clean text.
- **Why unresolved:** The study measures the drop in accuracy (e.g., BERT falling to 79.2%) but does not test the remediation strategy of training on the noisy data itself.
- **What evidence would resolve it:** A comparison of accuracy scores between models fine-tuned on clean data versus models fine-tuned on the "medium" and "heavy" noise tiers of the NDB when evaluated on a held-out noisy test set.

### Open Question 2
- **Question:** Do the robustness patterns observed in synthetic noisy narratives transfer to real-world, transcribed patient speech?
- **Basis in paper:** [inferred] The study relies entirely on a synthetic dataset generated by an LLM to simulate patient narratives, which may not capture the full complexity of human speech disfluencies or dialectal variations mentioned in the literature review.
- **Why unresolved:** There is no validation comparing the model's performance on the synthetic NDB against performance on a corpus of actual human patient narratives (e.g., transcribed telehealth visits).
- **What evidence would resolve it:** A correlation analysis evaluating the same models on both the synthetic NDB and a ground-truth dataset of real, noisy patient descriptions.

### Open Question 3
- **Question:** To what extent do specific noise types (lexical errors vs. semantic ambiguity vs. syntactic run-ons) independently contribute to diagnostic failure?
- **Basis in paper:** [explicit] The authors state that future work will include "finer-grained noise categories," implying the current benchmark aggregates different noise types into broad levels (Medium/Heavy) without isolating their effects.
- **Why unresolved:** The results report accuracy by noise *level* (volume/intensity) rather than noise *modality*, leaving the specific sensitivities of models to different error types unclear.
- **What evidence would resolve it:** Ablation studies using datasets where only one category of noise (e.g., typos only, or vague language only) is introduced at a time to measure relative accuracy drops.

## Limitations

- The benchmark is entirely synthetic, with no validation against real patient-generated text to verify noise realism
- Fine-tuning procedure lacks complete specification of hyperparameters and model variants affecting reproducibility
- The study does not test whether training on noisy data itself can recover accuracy lost from clean-only training

## Confidence

- **High Confidence**: The comparative robustness ranking (Flan-T5 > ClinicalBERT > BERT) and general accuracy degradation patterns are likely reproducible given clear methodology
- **Medium Confidence**: Claims about encoder-decoder architectures providing implicit denoising are supported by results but need validation with diverse noise types and real patient data
- **Medium Confidence**: Finding that domain-specific pre-training doesn't guarantee improved robustness is plausible but limited by single comparison point and synthetic benchmark

## Next Checks

1. **Real Data Generalization**: Evaluate the three benchmarked models on actual patient-generated text from sources like MedDialog or AskDocs to measure performance gaps between synthetic and real noise patterns. This will reveal whether synthetic simulation captures clinically relevant linguistic degradation.

2. **Noise Type Ablation**: Create controlled subsets of NDB isolating specific noise dimensions (lexical only, syntactic only, semantic only) to determine which noise types drive the most significant accuracy drops. This would clarify whether Flan-T5's robustness stems from handling all noise types or specific categories.

3. **Training Distribution Expansion**: Fine-tune models on augmented datasets that include both clean clinical text and various noise levels, then re-evaluate on the benchmark. This tests whether training distribution mismatch is the primary cause of degradation, rather than fundamental model architecture limitations.