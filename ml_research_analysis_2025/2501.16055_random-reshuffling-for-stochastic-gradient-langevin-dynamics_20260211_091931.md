---
ver: rpa2
title: Random Reshuffling for Stochastic Gradient Langevin Dynamics
arxiv_id: '2501.16055'
source_url: https://arxiv.org/abs/2501.16055
tags:
- stochastic
- gradient
- sgld
- random
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Random Reshuffling (RR) as a stochastic
  gradient strategy for sampling problems, specifically applied to Stochastic Gradient
  Langevin Dynamics (SGLD). While RR is well-known to improve optimization performance
  compared to Robbins-Monro (RM), its benefits for sampling have not been systematically
  studied.
---

# Random Reshuffling for Stochastic Gradient Langevin Dynamics

## Quick Facts
- arXiv ID: 2501.16055
- Source URL: https://arxiv.org/abs/2501.16055
- Reference count: 40
- Authors: Luke Shaw; Peter A. Whalley
- One-line primary result: SGLD-RR achieves improved convergence rates (O(1/ϵ) vs O(1/ϵ²)) and reduced bias compared to SGLD-RM in sampling problems.

## Executive Summary
This paper investigates Random Reshuffling (RR) as a stochastic gradient strategy for sampling problems, specifically applied to Stochastic Gradient Langevin Dynamics (SGLD). While RR is well-known to improve optimization performance compared to Robbins-Monro (RM), its benefits for sampling have not been systematically studied. The authors provide theoretical convergence guarantees showing that SGLD-RR has improved bias bounds compared to SGLD-RM in Wasserstein distance. Under standard assumptions on the target distribution, their results show O(1/ϵ) steps to reach accuracy ϵ for SGLD-RR versus O(1/ϵ²) for SGLD-RM. They also prove reduced bias for a Gaussian model problem and demonstrate empirically on logistic regression problems that SGLD-RR achieves lower bias than SGLD-RM.

## Method Summary
The paper compares two stochastic gradient strategies for SGLD: Robbins-Monro (RM) and Random Reshuffling (RR). In RM, mini-batches are sampled randomly with replacement from the dataset at each iteration. In RR, the dataset is shuffled at the start of each epoch and then cycled through deterministically in R batches. The SGLD update is x_{k+1} ← x_k - h∇̂F(x_k) + √(2h)ξ_k where ∇̂F is the stochastic gradient estimate. The authors analyze convergence in Wasserstein distance and prove that SGLD-RR achieves improved bias bounds compared to SGLD-RM under standard assumptions on the target distribution.

## Key Results
- SGLD-RR has improved bias bounds in Wasserstein distance compared to SGLD-RM
- Under standard assumptions, SGLD-RR requires O(1/ϵ) steps to reach accuracy ϵ versus O(1/ϵ²) for SGLD-RM
- SGLD-RR achieves reduced bias for a Gaussian model problem
- Empirically demonstrates lower bias than SGLD-RM on logistic regression problems
- Computational efficiency gains from better memory access patterns compound statistical benefits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Random Reshuffling reduces stochastic gradient bias by ensuring each data point contributes exactly once per epoch, improving asymptotic sample quality.
- **Mechanism**: In RR, the dataset is partitioned into R batches and cycled through deterministically within each epoch. This creates correlation structure in the gradient noise that cancels over complete epochs, unlike RM's independent sampling which accumulates variance. The covariance between stochastic gradients at different iterations within an epoch has negative dependence: cov(b̂ⱼ, b̂ⱼ') = -V[b̂ⱼ]/(R-1), which reduces the cumulative variance when summed over R steps.
- **Core assumption**: The potential F is µ-strongly convex and L-smooth; the stochastic gradient variance σ²* is bounded (Assumption 1 and 2b).
- **Evidence anchors**:
  - [abstract]: "proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials"
  - [section 4, page 11]: cov(b̂ⱼ, b̂ⱼ') = -V[b̂ⱼ]/(R-1), demonstrating the negative correlation structure
  - [corpus]: Related work "Error dynamics of mini-batch gradient descent with random reshuffling" shows similar benefits in optimization, but lacks direct evidence for sampling contexts.
- **Break condition**: If the potential is non-convex or if data ordering creates pathological correlations (e.g., sorted by label), the theoretical guarantees may not hold.

### Mechanism 2
- **Claim**: SGLD-RR achieves O(h + Rh) asymptotic bias versus O(h½ + (Rh)½) for SGLD-RM under smooth Hessian assumptions.
- **Mechanism**: The improved order comes from the local discretization analysis (Theorem A.2 in appendix) showing that RR's structured noise integrates to O(h²) over each epoch rather than O(h3/2) per step accumulation. The interpolation argument (proof of Theorem 3.9) combines this with contraction of the continuous Langevin dynamics, where blocks of k̃ steps with shared noise enable tighter bounds.
- **Core assumption**: Assumption 3 (L₁-smooth Hessian) is required for O(1/ϵ) step complexity; without it, bounds degrade but RR still outperforms RM.
- **Evidence anchors**:
  - [abstract]: "SGLD-RR has improved bias bounds... O(1/ϵ) steps to reach accuracy ϵ for SGLD-RR versus O(1/ϵ²) for SGLD-RM"
  - [section 3, page 9, Remark 3.11]: "for targets which fulfill Assumption 3, the number of steps K to reach an error tolerance ϵ > 0 scales as O(1/ϵ)"
  - [corpus]: "Fisher information dissipation for time inhomogeneous stochastic differential equations" provides related Lyapunov convergence analysis but does not address randomization strategies.
- **Break condition**: Theoretical guarantees require step size h < 1/L; larger steps may violate the contraction argument.

### Mechanism 3
- **Claim**: Computational efficiency gains from cache-friendly memory access patterns compound the statistical benefits.
- **Mechanism**: RR naturally processes data sequentially within each shuffled epoch, enabling better cache utilization. Unlike RM which randomly accesses the full dataset each iteration (causing cache misses), RR's predictable access pattern within epochs allows pre-fetching. This hardware-level efficiency means "RR actually runs faster for a given number of epochs" as stated on page 15.
- **Core assumption**: Data can be stored in contiguous memory; batch size n divides dataset size N evenly.
- **Evidence anchors**:
  - [abstract]: "Random Reshuffling is typically more efficient due to memory access and cache reasons"
  - [section 1, page 2]: "RR is not only more computationally efficient (due to caching and memory access) but incurs a reduced stochastic gradient bias"
  - [corpus]: No corpus papers directly validate cache efficiency claims; this appears to reference external optimization literature [4, 37].
- **Break condition**: On systems with unusual memory hierarchies or when data cannot fit in available cache, benefits may diminish.

## Foundational Learning

- **Concept: Wasserstein distance**
  - **Why needed here**: The paper's main theoretical contribution is bounding W₂(eπₖ, π*)—the 2-Wasserstein distance between the empirical and target distributions. Without understanding this metric, the O(1/ϵ) vs O(1/ϵ²) claims are opaque.
  - **Quick check question**: Can you explain why Wasserstein distance captures both mean and variance differences between distributions, unlike KL divergence?

- **Concept: Langevin dynamics and discretization bias**
  - **Why needed here**: SGLD discretizes the continuous Langevin SDE (dXₜ = -∇F(Xₜ)dt + √2dWₜ) using Euler-Maruyama. The h-dependent bias terms in Theorems 3.8/3.9 arise from this discretization, separate from stochastic gradient bias.
  - **Quick check question**: Why does the continuous Langevin dynamics have π* as its invariant distribution, and why does discretization break this exact invariance?

- **Concept: Stochastic gradient variance decomposition**
  - **Why needed here**: Lemma 2.1 shows σ²* ∝ R = N/n, meaning larger batches reduce variance. The interplay between batch size, step size, and epoch structure determines the (Rh) terms in convergence bounds.
  - **Quick check question**: If you double the batch size n (halving R), what happens to the stochastic gradient variance and how does this affect the asymptotic bias for SGLD-RM vs SGLD-RR?

## Architecture Onboarding

- **Component map**:
  Data loader -> Gradient estimator -> SGLD integrator -> Epoch controller

- **Critical path**:
  1. At epoch start: shuffle indices, partition into R batches
  2. For r = 0 to R-1: compute stochastic gradient for batch r, apply SGLD update
  3. Repeat for nₑ epochs
  4. Collect samples (accounting for periodic variance structure in RR)

- **Design tradeoffs**:
  - Smaller batch size n → larger R → higher variance σ²* but more frequent reshuffling
  - Larger step size h → faster mixing but larger discretization bias (h term) and gradient noise bias (Rh term)
  - RM has well-defined invariant distribution; RR does not (variance oscillates periodically)

- **Failure signatures**:
  - If variance grows unbounded: check Assumption 1 (bounded gradient variance) may be violated
  - If oscillations don't decrease with smaller h: discretization bias may be dominated by gradient noise; reduce R
  - If RR performs worse than RM: potential data ordering issue or non-convexity breaking assumptions

- **First 3 experiments**:
  1. **Gaussian toy problem (Section 4)**: Implement 1D sampling from N(ȳ, σ²/N). Measure relative variance error N(V̂ - σ²/N)/σ² for RR vs RM across step sizes h ∈ {2⁻⁸, ..., 2⁻¹}. Verify periodic oscillations in RR and O(h) vs O(√h) scaling.
  2. **Logistic regression posterior**: Use StatLog or CTG dataset. Track ||Δμ||/||μ|| (relative error in posterior mean vs long HMC reference). Confirm RR achieves h¹ vs h1/2 convergence.
  3. **Ablation on batch size**: Fix total compute budget, vary batch size n ∈ {N/16, N/8, N/4, N/2}. Plot final Wasserstein distance (estimated via samples) vs wall-clock time to isolate cache efficiency gains from statistical gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the Wasserstein convergence bounds in Theorem 3.9 tight, or can they be improved to match the O(h + (Rh)²) convergence observed analytically for the Gaussian model problem?
- Basis in paper: [explicit] "For SGLD-RM, whether the Gaussian is simply special or actually suggests the bound in Theorem 3.8 is loose is a well-known open problem in the field; we can now add the analogous question for SGLD-RR."
- Why unresolved: The theoretical bounds predict O(h + Rh) convergence, but the Gaussian analysis shows O(h + (Rh)²), leaving a gap between theory and observed behavior.
- What evidence would resolve it: Either tightening the Wasserstein bounds to O(h + (Rh)²) or constructing counterexamples where the current bounds are achieved.

### Open Question 2
- Question: Can Random Reshuffling be combined with higher-order discretisation schemes (e.g., Leimkuhler-Matthews) to achieve overall second-order convergence with error O(h² + (Rh)²)?
- Basis in paper: [explicit] "Given that the Gaussian analysis suggests that RR gives O(h²) error in the gradient noise, it would be interesting to combine it with the Leimkuhler-Matthews discretisation [27], which is of weak order 2."
- Why unresolved: The RR gradient bias appears to be O(h²) from the Gaussian analysis, suggesting compatibility with second-order schemes, but no analysis or experiments have been conducted.
- What evidence would resolve it: Theoretical analysis and numerical experiments combining SGLD-RR with second-order integrators, measuring convergence rates.

### Open Question 3
- Question: Do the theoretical guarantees for SGLD-RR extend to non-convex potentials using the local discretisation analysis combined with alternative Wasserstein convergence results?
- Basis in paper: [explicit] "The most novel aspect of this work is the local discretisation analysis in Theorem A.2, which can be directly combined with alternative Wasserstein convergence results in the non-convex setting... by instead assuming each fω is convex outside of a ball."
- Why unresolved: Current theorems require strong convexity (Assumption 2b), but many practical problems are non-convex.
- What evidence would resolve it: Extending Theorem 3.9 to non-convex settings with local convexity assumptions, supported by experiments on non-convex targets.

## Limitations

- Theoretical guarantees require restrictive assumptions (µ-strong convexity, L-smoothness, bounded gradient variance) that may not hold in practical applications
- Lack of direct empirical validation of computational efficiency claims (cache utilization)
- No ablation studies isolating statistical vs computational benefits
- Does not address non-convex potentials or analyze trade-offs when data ordering creates pathological correlations

## Confidence

- **High confidence**: Theoretical convergence rate improvements (O(1/ϵ) vs O(1/ϵ²)) under stated assumptions, and empirical demonstration of lower bias on logistic regression.
- **Medium confidence**: The mechanism by which RR reduces bias (negative gradient covariance within epochs), as this relies on specific assumptions about the potential landscape.
- **Low confidence**: Computational efficiency gains from cache-friendly memory access, as these are stated but not empirically validated in the paper.

## Next Checks

1. **Verify cache efficiency claims**: Profile memory access patterns for SGLD-RR vs SGLD-RM on identical hardware, measuring cache hit rates and wall-clock time per effective sample.

2. **Test robustness to data ordering**: Run experiments on datasets with known structure (e.g., sorted by label) to determine if RR's theoretical advantages degrade under non-random data ordering.

3. **Compare with exact sampling**: For the Gaussian toy problem, compute exact Wasserstein distance between empirical and target distributions (analytically tractable) to validate the convergence rate claims beyond variance error.