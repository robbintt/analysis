---
ver: rpa2
title: 'Geometric Stability: The Missing Axis of Representations'
arxiv_id: '2601.09173'
source_url: https://arxiv.org/abs/2601.09173
tags:
- stability
- shesha
- geometric
- across
- drift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces geometric stability as a complementary axis
  to similarity for analyzing learned representations. While similarity metrics measure
  alignment with external references, stability quantifies the internal consistency
  of representational geometry under perturbation.
---

# Geometric Stability: The Missing Axis of Representations

## Quick Facts
- arXiv ID: 2601.09173
- Source URL: https://arxiv.org/abs/2601.09173
- Authors: Prashant C. Raju
- Reference count: 40
- Primary result: Geometric stability provides a complementary axis to similarity for analyzing representations, capturing internal consistency while similarity measures external alignment

## Executive Summary
This paper introduces geometric stability as a novel axis for analyzing learned representations, complementing existing similarity metrics. While similarity measures how well representations align with external references, stability quantifies internal consistency of representational geometry under perturbation. The authors present Shesha, a framework measuring this stability through self-consistency of Representational Dissimilarity Matrices (RDMs). The framework reveals that stability and similarity capture mechanistically distinct information, with stability being more sensitive to fine-grained manifold structure and providing complementary predictive power for controllability and transfer learning.

## Method Summary
The geometric stability framework measures internal consistency of representational geometry through self-consimilarity of RDMs under perturbation. The core approach involves computing RDMs from representations, applying controlled perturbations, and measuring how the geometric structure changes. Stability is quantified as the correlation between original and perturbed RDMs, capturing the robustness of representational geometry. The framework distinguishes between global geometric consistency (captured by stability) and alignment with external references (captured by similarity metrics like CKA), revealing that these axes are nearly orthogonal across diverse configurations.

## Key Results
- Stability and similarity show near-zero correlation (ρ≈0.01) across 2,463 configurations, capturing distinct information
- Stability uniquely predicts linear controllability (ρ=0.89-0.96) while similarity metrics collapse after removing top principal components
- Stability detects post-training alignment drift nearly 2× more sensitively than CKA while avoiding false alarms from rigid distance metrics
- Reveals "geometric tax" in transfer optimization, dissociating stability from transferability

## Why This Works (Mechanism)
Geometric stability works by quantifying the self-consistency of representational geometry under perturbation, which captures the intrinsic structure of the representation manifold. Unlike similarity metrics that measure alignment with external references, stability focuses on how well the internal geometric relationships between representations are preserved when the input is slightly modified. This makes stability sensitive to fine-grained manifold structure that similarity metrics miss, particularly after dimensionality reduction removes dominant components.

## Foundational Learning
**Representational Dissimilarity Matrices (RDMs)**: Distance-based summaries of representational geometry that capture pairwise dissimilarities between representations. Needed to quantify geometric structure in a way that's invariant to rotation and scaling. Quick check: Verify RDM construction produces symmetric matrices with zero diagonals.

**Principal Component Analysis (PCA)**: Dimensionality reduction technique that identifies orthogonal directions of maximum variance. Needed to demonstrate how similarity metrics collapse after removing top components while stability retains sensitivity. Quick check: Confirm top components capture >90% variance before ablation.

**Self-similarity correlation**: Measures correlation between RDMs computed from original and perturbed representations. Needed as the core stability metric that captures geometric consistency. Quick check: Verify correlation values fall in expected range [-1, 1].

## Architecture Onboarding
**Component Map**: Input representations → RDM construction → Perturbation application → RDM comparison → Stability score. The critical path is RDM → perturbation → self-similarity correlation.

**Critical Path**: The most important computation is the self-similarity correlation between original and perturbed RDMs, as this directly measures geometric stability.

**Design Tradeoffs**: Using RDMs provides rotation and scaling invariance but increases computational complexity from O(n) to O(n²). Perturbation magnitude must balance between being large enough to test robustness while small enough to stay within the same manifold region.

**Failure Signatures**: If stability scores are uniformly high across all configurations, this suggests either insufficient perturbation strength or overly smooth representations. If stability doesn't predict controllability despite high correlation values, this indicates the geometric measure isn't capturing the relevant structural features.

**First Experiments**: 1) Compare stability vs similarity on a simple linear model with known controllability properties. 2) Apply increasing levels of perturbation to observe stability degradation curves. 3) Remove principal components systematically to observe differential effects on stability vs similarity metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- Near-zero correlation between stability and similarity needs replication across more diverse model architectures and tasks
- Claims about stability capturing "fine-grained manifold structure" require more systematic ablation studies across different dimensionality reduction techniques
- Biological applications (CRISPR and neural-behavioral coupling) need independent validation in different biological systems

## Confidence
**High**: Stability's prediction of linear controllability (ρ=0.89-0.96) and its ability to detect post-training alignment drift 2× more sensitively than CKA
**Medium**: The "geometric tax" finding in transfer optimization, which requires more systematic hyperparameter sweeps and additional transfer scenarios
**Medium**: The biological applications that need independent validation in different biological systems

## Next Checks
1. Test stability-sensitivity across different RDM construction methods (different distance metrics, kernel choices) and verify that results hold under various perturbation regimes
2. Conduct systematic ablation studies removing different numbers of principal components to quantify exactly how similarity and stability metrics diverge
3. Replicate the controllability prediction across additional model families (RNNs, Transformers with different attention mechanisms) and non-image domains to establish generalizability