---
ver: rpa2
title: 'Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images'
arxiv_id: '2505.07704'
source_url: https://arxiv.org/abs/2505.07704
tags:
- facts
- weird
- whoops
- image
- strange
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of measuring image realism by
  evaluating whether images conform to common sense expectations. The proposed method,
  Through the Looking Glass (TLG), leverages Large Vision-Language Models (LVLMs)
  to extract atomic facts from images and uses a Transformer-based classifier to detect
  inconsistencies among these facts.
---

# Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images

## Quick Facts
- arXiv ID: 2505.07704
- Source URL: https://arxiv.org/abs/2505.07704
- Reference count: 18
- Primary result: Proposes TLG method achieving 73.54% accuracy on WHOOPS! and 87.57% on WEIRD datasets for detecting commonsense-violating images

## Executive Summary
This paper introduces a novel approach to evaluating image realism by detecting commonsense violations through inconsistent atomic facts generated by Large Vision-Language Models (LVLMs). The Through the Looking Glass (TLG) method leverages LVLM hallucinations as a signal - when confronted with "weird" images that defy common sense, LVLMs generate contradictory or nonsensical atomic facts. By fine-tuning a Transformer-based classifier to detect inconsistencies among these facts, TLG achieves state-of-the-art performance on both WHOOPS! and WEIRD datasets. The method demonstrates that hallucinations can be productively used rather than simply filtered out, with the attention-pooling classifier learning to identify weirdness by focusing on semantically inconsistent elements.

## Method Summary
TLG extracts atomic facts from images using LVLMs with diverse beam search, then trains an attention-pooling classifier to detect inconsistencies among these facts. The pipeline works as follows: LLaVA-1.6-Mistral-7B generates N atomic facts per image using diverse beam search (num_beams=5, diversity_penalty=1.0), each fact is encoded by a frozen DeBERTa-v3-large-tasksource-nli text encoder, and the resulting embeddings are aggregated via attention-pooling. The classifier learns attention weights to focus on facts that violate common sense, with the final probability computed from the weighted average of fact embeddings. The method achieves 73.54% accuracy on WHOOPS! and 87.57% on WEIRD through 5-fold cross-validation.

## Key Results
- TLG achieves 73.54% accuracy on WHOOPS! and 87.57% on WEIRD datasets
- Outperforms fine-tuned LVLMs (up to 16.45% higher on WEIRD) and CLIP-based methods
- NLI-fine-tuned text encoders (tasksource) significantly outperform standard encoders
- Attention-pooling classifier learns to focus on semantically inconsistent facts
- TLG shows strong robustness to small training sets and performs well in cross-dataset transfer

## Why This Works (Mechanism)

### Mechanism 1: LVLM Hallucinations as Signal
- **Claim:** LVLMs produce semantically inconsistent or contradictory "atomic facts" when describing images that violate common sense
- **Core assumption:** Hallucinations in LVLMs are correlated with semantic violations in the input image
- **Evidence:** LVLMs may generate contradictory facts when confronted with images defying common sense, creating a noisy signal that distinguishes strange images via internal contradiction

### Mechanism 2: Attention Pooling for Inconsistency Detection
- **Claim:** An attention-pooling classifier can learn to identify common-sense violations by assigning higher weights to semantically inconsistent facts
- **Core assumption:** The latent space clusters "strange" concepts in a way that is linearly separable or attention-accessible from "normal" concepts
- **Evidence:** TLG assigns higher attention weights to facts that violate common sense, such as "The man is using a vacuum cleaner on the beach" versus generic descriptions

### Mechanism 3: NLI-Fine-Tuned Encoders for Common Sense
- **Claim:** Text encoders fine-tuned on Natural Language Inference tasks provide stronger common sense foundations for classifying facts
- **Core assumption:** Textual encoders capture sufficient world knowledge to evaluate visual common sense when translated to text
- **Evidence:** DeBERTa models fine-tuned on tasksource collection outperform methods relying on alternative text encoders due to enhanced encoding capabilities

## Foundational Learning

- **Concept: Atomic Facts**
  - **Why needed here:** The system must break down scenes into individual propositions to find local inconsistencies rather than analyzing entire descriptions as single blocks
  - **Quick check:** Can you decompose "A dog driving a car on the moon" into three distinct atomic facts?

- **Concept: Diverse Beam Search**
  - **Why needed here:** Standard beam search produces generic, repetitive captions; diverse search increases probability of generating both normal context and hallucinated weird details
  - **Quick check:** Why would greedy decoding fail to surface contradictions in weird images compared to diverse beam search?

- **Concept: Attention Pooling**
  - **Why needed here:** Compresses variable number of facts into fixed-size vector while learning which facts matter for final "weird" classification
  - **Quick check:** How does attention pooling differ from average pooling in sensitivity to outlier facts?

## Architecture Onboarding

- **Component map:** LVLM (LLaVA-1.6-Mistral-7B) -> Text Encoder (DeBERTa-v3-tasksource) -> Pooling Layer -> Classifier
- **Critical path:** Generation hyperparameters for LVLM (num_beams, diversity_penalty) - if diversity penalty is too low, facts will be repetitive and lack necessary contradictions
- **Design tradeoffs:** Larger models (13B) didn't always outperform smaller ones (7B), suggesting better image understanding might reduce useful hallucination signal; NLI-fine-tuned encoders significantly outperform standard ones
- **Failure signatures:** High lexical overlap between facts for weird vs. normal images prevents discrimination; LVLM refusing to describe weird elements outputs generic descriptions
- **First 3 experiments:**
  1. Validate the signal by generating facts for weird/normal images and manually verifying contradictions
  2. Encoder ablation comparing standard DeBERTa vs. deberta-v3-large-tasksource-nli
  3. Attention visualization on known weird image to confirm model attends to "strange" fact

## Open Questions the Paper Calls Out

- **Open Question 1:** How does TLG performance fluctuate when using more recent or larger-scale LVLMs like Qwen2.5-VL that were excluded from this study?
- **Open Question 2:** Does reliance on GPT-4o and DALL-E 3 for WEIRD dataset introduce synthetic artifacts that allow models to detect "weirdness" via texture/style shortcuts rather than semantic violations?
- **Open Question 3:** Why does Linear Probing outperform TLG when transferring from WEIRD to WHOOPS! while TLG dominates in reverse direction?
- **Open Question 4:** To what extent does the "atomicity" and semantic independence of generated facts act as a bottleneck for the TLG classifier?

## Limitations

- Core mechanism depends on LVLM hallucinations, which may degrade as LVLMs become more robust to visual anomalies
- Small WHOOPS! dataset (204 samples) means results may have high variance
- WEIRD dataset represents curated "weird" images that may not generalize to all commonsense violations
- Reliance on textual representations assumes text encoder's training data adequately covers visual domain

## Confidence

- **High Confidence:** TLG architecture and superior performance on WEIRD are well-supported by experimental evidence
- **Medium Confidence:** Claim that LVLM hallucinations correlate with commonsense violations is plausible but needs more systematic investigation
- **Low Confidence:** Generalizability to real-world images beyond curated WEIRD dataset is uncertain

## Next Checks

1. Evaluate TLG on real-world images from social media or stock photo sites labeled for commonsense violations to assess generalizability
2. Categorize generated facts into hallucination types (semantic inconsistencies, visual absurdities, logical errors) and measure classifier sensitivity to each type
3. Retrain TLG using a more recent, potentially more robust LVLM (e.g., GPT-4V) to test if hallucination signal persists