---
ver: rpa2
title: 'VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent
  Multimodal Learning'
arxiv_id: '2511.20422'
source_url: https://arxiv.org/abs/2511.20422
tags:
- sound
- dataset
- geometry
- material
- modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VibraVerse is a large-scale dataset explicitly linking 3D geometry,
  material properties, and impact sound through physically grounded modal analysis.
  It includes 46,000+ high-quality objects with watertight meshes, physical attributes
  (density, Young's modulus, Poisson's ratio), modal eigenvalues/eigenvectors, and
  synthesized audio.
---

# VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning

## Quick Facts
- arXiv ID: 2511.20422
- Source URL: https://arxiv.org/abs/2511.20422
- Authors: Bo Pang; Chenxi Xu; Jierui Ren; Guoping Wang; Sheng Li
- Reference count: 40
- Key outcome: VibraVerse is a large-scale dataset explicitly linking 3D geometry, material properties, and impact sound through physically grounded modal analysis.

## Executive Summary
VibraVerse is a large-scale dataset explicitly linking 3D geometry, material properties, and impact sound through physically grounded modal analysis. It includes 46,000+ high-quality objects with watertight meshes, physical attributes (density, Young's modulus, Poisson's ratio), modal eigenvalues/eigenvectors, and synthesized audio. To align these modalities, we introduce CLASP, a contrastive learning framework preserving the causal correspondence between an object's physical structure and its acoustic response. This framework ensures that every sample is coherent, traceable to governing equations, and embedded in a unified representation space. Built upon VibraVerse, we establish benchmark tasks for geometry-to-sound synthesis, sound-guided shape reconstruction, and cross-modal retrieval. Extensive validations show that models trained on VibraVerse achieve high accuracy, interpretability, and generalization across modalities, establishing it as a benchmark for physically consistent multimodal learning. The dataset will be open-sourced.

## Method Summary
VibraVerse is built through a pipeline that starts with filtering and processing 3D meshes to ensure watertight, manifold geometries. Finite Element Method (FEM) is then applied to compute modal eigenvalues and eigenvectors from geometry and material properties, which are used to synthesize physically-grounded impact sounds via additive synthesis of damped sinusoids. The dataset is explicitly designed for multimodal learning, and to align the different modalities (geometry, image, audio), the CLASP framework employs three modality-specific encoders (SIREN for audio, OCNN for 3D geometry, VGG for images) trained with InfoNCE contrastive loss to preserve the causal correspondence between physical structure and acoustic response. This enables benchmark tasks in geometry-to-sound synthesis, sound-guided shape reconstruction, and cross-modal retrieval.

## Key Results
- CLASP framework achieves high accuracy in cross-modal retrieval with R@1 scores exceeding 0.7 on VibraVerse.
- Geometry-to-sound synthesis models trained on VibraVerse produce modal frequencies within 5% error of ground truth FEM results.
- Sound-guided shape reconstruction achieves IoU scores above 0.6, demonstrating effective cross-modal information transfer.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Finite Element Method modal analysis creates a deterministic, physically-grounded mapping from geometry + material properties to eigenfrequencies and mode shapes.
- **Mechanism:** Given a volumetric mesh with material properties (ρ, E, ν), FEM constructs mass (M) and stiffness (K) matrices. The generalized eigenvalue problem KU = MUΛ yields eigenvalues (λ_j = ω²_j) and eigenvectors (mode shapes u_j) that uniquely characterize vibrational behavior.
- **Core assumption:** Small deformations and linear elastic material behavior hold; objects are single connected components with watertight meshes.
- **Evidence anchors:**
  - [abstract]: "modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations"
  - [section 3.1]: "transforms into a generalized eigenvalue problem as: KU = MUΛ"
  - [corpus]: Weak corpus support; related multimodal datasets (ObjectFolder, SoundSpaces) use empirical correlations rather than explicit eigenvalue decomposition.
- **Break condition:** Negative eigenvalues, non-manifold geometries, or disconnected components violate eigenvalue problem assumptions.

### Mechanism 2
- **Claim:** Contrastive learning with InfoNCE loss creates a unified representation space where embeddings reflect underlying physical relationships rather than spurious correlations.
- **Mechanism:** Three modality encoders extract features; InfoNCE loss maximizes similarity between physically-matched pairs while pushing apart non-matching pairs: L = -log[exp(sim(z_i, z_j)/τ) / Σexp(sim(z_i, z_k)/τ)]. Temperature τ = 0.07 controls distribution sharpness.
- **Core assumption:** Physically consistent samples (same geometry + material → same sound) provide valid positive pairs; simulation-based generation ensures this.
- **Evidence anchors:**
  - [abstract]: "CLASP... preserves the causal correspondence between an object's physical structure and its acoustic response"
  - [section 4.3]: "train the model using a contrastive loss to align the embeddings of matching pairs"
  - [corpus]: LINA paper supports contrastive alignment for physical consistency but focuses on diffusion models, not multimodal retrieval.
- **Break condition:** Physically inconsistent pairs in training data (misassigned materials) cause contrastive objective to learn incorrect mappings.

### Mechanism 3
- **Claim:** Superposition of damped sinusoids derived from modal parameters produces physically plausible impact sounds.
- **Mechanism:** Each mode's impulse response: S_i(t) = A_i × e^(-d_i×t) × sin(2πω_i×t). Summing 64 modes yields final audio. Rayleigh damping (C = αM + βK) determines decay rates.
- **Core assumption:** Linear superposition of modes captures acoustic behavior; 64 modes are sufficient for perceptual validity.
- **Evidence anchors:**
  - [section 3.1]: "The resulting audio signal is their superposition: S(t) = Σ A_i e^(-σ_i t) sin(ω_d,i t)"
  - [section 3.5]: "sample a 1-second signal at 32,000 Hz for each of the 64 modes... summed to produce the impact sound"
  - [corpus]: Limited corpus evidence; audio-visual datasets (AudioSet, VGGSound) use real recordings, not modal synthesis.
- **Break condition:** Complex geometries may require >64 modes; real-world sounds include non-modal components (noise, environmental reverberation) not captured.

## Foundational Learning

- **Concept: Generalized Eigenvalue Problems**
  - Why needed here: The entire physics pipeline relies on solving KU = MUΛ. Without understanding how mass and stiffness matrices encode geometry and material, you cannot debug eigenvalue failures.
  - Quick check question: Why must the smallest eigenvalues be extracted rather than the largest for acoustic applications?

- **Concept: Contrastive Learning and Temperature Scaling**
  - Why needed here: CLASP's cross-modal alignment uses InfoNCE loss; temperature τ directly affects gradient behavior and embedding quality.
  - Quick check question: If τ is too high (e.g., 1.0), what happens to the soft max distribution over negative samples?

- **Concept: Rayleigh Damping Model**
  - Why needed here: Sound decay behavior is determined by C = αM + βK. Understanding this model is necessary to assess sim-to-real transfer limitations.
  - Quick check question: Why might a real struck metal bowl exhibit different decay characteristics than the Rayleigh model predicts?

## Architecture Onboarding

- **Component map:**
  - Geometry Pipeline: Raw mesh → voxel remesh → watertight manifold → fTetWild tetrahedralization
  - Physics Engine: Volumetric mesh + material (ρ, E, ν) → FEM matrices → ARPACK (64 smallest eigenvalues)
  - Audio Synthesis: Eigenvalues + Rayleigh damping → 64 damped sinusoids → sum to 1s @ 32kHz
  - CLASP: SIREN encoder (audio) + OCNN encoder (3D geometry) + VGG encoder (image) → shared latent → InfoNCE loss
  - Downstream Tasks: Geometry→Sound prediction, Sound→Geometry reconstruction (VAE), Cross-modal retrieval

- **Critical path:**
  1. Topological filtering (connectivity, manifold check, thickness constraint)
  2. Tetrahedralization (fTetWild) — must succeed for valid FEM
  3. Eigenvalue computation (ARPACK, 64 smallest) — must yield positive eigenvalues
  4. Audio synthesis (64-mode additive synthesis)
  5. CLASP triplet training (geometry, image, audio per sample)

- **Design tradeoffs:**
  - 64 modes vs. compute cost: More modes improve accuracy but slow eigenvalue decomposition
  - Synthetic data vs. sim-to-real gap: Ensures physical consistency but lacks real-world noise
  - VLM-based material assignment vs. measurement: Scalable but introduces labeling noise
  - Controlled excitation vs. real variability: Clean causal signal but unrealistic conditions

- **Failure signatures:**
  - Negative eigenvalues → invalid mesh or numerical instability
  - fTetWild failure → non-manifold geometry or degenerate triangles
  - High retrieval error (R@1 < 0.2) → check material-label consistency
  - Audio artifacts (clicking, aliasing) → verify 32kHz sample rate alignment
  - Geometry→Sound MSE spike → outlier meshes with high genus or thin shells

- **First 3 experiments:**
  1. **Eigenvalue validation on primitives:** Test cube, sphere, cylinder against analytical predictions to verify FEM correctness before scaling.
  2. **CLASP embedding probe:** Train on 1,000 samples, visualize t-SNE of audio embeddings colored by material; verify clustering before full training.
  3. **Reconstruction baseline:** Run Sound→Geometry task with fixed initial voxels, measure IoU vs. DiffSound on 100 test meshes to establish performance floor.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can models trained on the synthetic VibraVerse dataset effectively generalize to real-world acoustic environments where noise, environmental variability, and complex recording conditions exist?
- **Basis in paper:** [explicit] The Conclusion explicitly states that the data is generated under "idealized simulation conditions" and "has not yet been validated against real recordings," leaving "the degree of sim-to-real generalization to be explored in our future work."
- **Why unresolved:** The dataset is entirely simulation-based (FEM and additive synthesis), and the paper provides no experimental validation against physical real-world measurements or noisy "in-the-wild" audio data.
- **What evidence would resolve it:** Successful benchmarking of VibraVerse-trained models (like CLASP) on datasets of real-world object recordings (e.g., RealImpact) showing robust retrieval or reconstruction accuracy without domain adaptation fine-tuning.

### Open Question 2
- **Question:** How effectively can VibraVerse support the training of Physics-Informed Neural Networks (PINNs) or neural-based physical simulators to replace traditional solvers while respecting governing physical laws?
- **Basis in paper:** [explicit] The Conclusion identifies "potential for advancing physics-informed neural networks (PINNs) and neural-based physical simulation," but explicitly notes that "These need further validation."
- **Why unresolved:** While the paper shows data-driven approximations (e.g., SIREN predicting frequencies), it does not validate if these models satisfy the underlying partial differential equations (PDEs) or constraints required for reliable use as general-purpose physics engines.
- **What evidence would resolve it:** Demonstration of a neural surrogate model trained on VibraVerse that solves forward or inverse problems with strict adherence to conservation laws or boundary conditions, or improved sample efficiency in solving PDEs compared to standard supervised learning.

### Open Question 3
- **Question:** Does the reliance on linear modal analysis and single impulse excitations limit the dataset's applicability for modeling non-linear vibrations or continuous contact sounds found in complex interactions?
- **Basis in paper:** [inferred] Section 3.1 explicitly assumes "small deformations and linear elastic material" and uses a "unit impulse excitation," while the Conclusion notes the simulation conditions are "idealized."
- **Why unresolved:** Real-world acoustic events often involve non-linear material behavior (e.g., clattering, large deformations) or continuous excitations (scraping, rolling) which cannot be represented by the linear superposition of damped sinusoids used in the dataset's synthesis pipeline.
- **What evidence would resolve it:** A study comparing the spectral features of VibraVerse sounds against real recordings of the same objects under high-amplitude impacts or continuous contact, quantifying the point of divergence where linear assumptions fail.

## Limitations
- The dataset relies on linear elastic material assumptions and single impulse excitations, which may not capture non-linear vibrations or continuous contact sounds found in real-world interactions.
- Material properties are assigned via VLM-based labeling rather than physical measurements, introducing potential systematic errors and limiting accuracy.
- The dataset contains no real-world acoustic recordings, limiting direct applicability to systems trained on natural audio and leaving the sim-to-real transfer capability unproven.

## Confidence
- **High Confidence:** The deterministic relationship between geometry, material properties, and modal eigenvalues via FEM analysis is well-established physics. The CLASP framework's contrastive learning approach is a proven method for multimodal alignment when ground truth correspondences exist.
- **Medium Confidence:** The choice of 64 modes for perceptual sufficiency is reasonable but not rigorously validated against human perception studies. The VLM-based material assignment, while scalable, introduces uncertainty in material property accuracy.
- **Low Confidence:** The sim-to-real transfer capability of models trained on this synthetic dataset remains unproven without extensive real-world validation. The generalization to unseen material types beyond the 10 categories is untested.

## Next Checks
1. **Real-world validation:** Record impact sounds from 50+ physical objects spanning the 10 material categories, compute their modal parameters via FEM, and evaluate cross-modal retrieval performance using VibraVerse-trained models.
2. **Mode sufficiency analysis:** Systematically vary the number of modes (16, 32, 64, 128) in audio synthesis and measure impact on both human perceptual quality ratings and downstream task performance to determine the optimal tradeoff.
3. **Material property sensitivity:** Create perturbed versions of the dataset with ±10% variations in material properties and measure degradation in retrieval accuracy to quantify robustness to material labeling errors.