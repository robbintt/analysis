---
ver: rpa2
title: Intra-neuronal attention within language models Relationships between activation
  and semantics
arxiv_id: '2503.12992'
source_url: https://arxiv.org/abs/2503.12992
tags:
- categorical
- activation
- attention
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether artificial neurons in language
  models can perform intra-neuronal attention by identifying specific categorical
  segments within their encoded thought categories based on activation patterns. Using
  OpenAI's GPT-2XL, researchers analyzed 12,800 neurons across the first two layers,
  examining 100 core-tokens with highest mean activations per neuron.
---

# Intra-neuronal attention within language models Relationships between activation and semantics

## Quick Facts
- arXiv ID: 2503.12992
- Source URL: https://arxiv.org/abs/2503.12992
- Reference count: 0
- Primary result: Investigates whether artificial neurons can perform intra-neuronal attention by identifying categorical segments within their encoded thought categories based on activation patterns

## Executive Summary
This study examines whether artificial neurons in language models exhibit intra-neuronal attention by analyzing the relationship between activation patterns and categorical semantics. Using GPT-2XL's first two layers, researchers investigated 12,800 neurons and their top-100 activating tokens. The analysis revealed a systematic but weak relationship between activation levels and categorical segmentation, primarily observable at very high activation values where tokens showed greater categorical homogeneity. The findings suggest that activation functions as an attentional vector, enabling neurons to focus on semantically coherent token subsets that facilitate more effective processing in subsequent layers.

## Method Summary
The study analyzed GPT-2XL's first two perceptron layers (12,800 neurons total), extracting activations for all tokens in a large corpus to identify each neuron's 100 "core-tokens" with highest mean activations. Two approaches were employed: (1) top-down analysis using hierarchical clustering to group tokens into 5 categorical clusters per neuron, then comparing mean activations across clusters with Kruskal-Wallis tests and Cohen's d effect sizes; (2) bottom-up analysis segmenting tokens by activation quartiles and measuring categorical homogeneity (mean pairwise cosine similarity) within each segment, indexed against Q3 of neuron's overall token similarities. The study tested whether activation levels systematically correlate with categorical structure and whether high-activation tokens exhibit greater semantic coherence.

## Key Results
- A systematic but weak relationship exists between activation and categorical segmentation, primarily at very high activation levels
- High-activation tokens show greater categorical homogeneity (mean cosine similarity increasing from G1=.3485 to G4=.3933) compared to lower activation levels
- Activation segments interleave across categorical clusters, with no strict one-to-one correspondence, though categorical homogeneity improves as activation levels increase
- Only 18-31% of neurons showed statistically significant activation differences across categorical clusters (Kruskal-Wallis, α=0.05)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Activation levels function as an attentional vector that enables neurons to selectively identify tokens with very high activation values.
- **Mechanism:** Mathematical-cognitive factors (categorical priming, inter-categorical attention, categorical phasing) jointly determine activation values. When these factors strongly co-activate for a token, it achieves very high activation and becomes the target of selective attentional focus.
- **Core assumption:** Activation magnitude carries functional significance beyond computational intermediacy—it gates access to downstream processing.
- **Evidence anchors:**
  - [abstract] "activation serves as an attentional vector, enabling neurons to identify and focus on tokens with exceptionally high activation values"
  - [section 4.3] Mean cosine similarities increase systematically from G1 (.3485) to G4 (.3933), showing higher activation correlates with greater categorical homogeneity
  - [corpus] Related work on rare-token neurons shows distributed specialization mechanisms in LLMs for handling specific token categories
- **Break condition:** If tokens with very high activation showed equal or lower categorical homogeneity than low-activation tokens, the attentional vector hypothesis would fail.

### Mechanism 2
- **Claim:** Neurons establish an attentional dichotomization threshold—converting quantitative activation continua into qualitative "very high" vs. "other" distinctions.
- **Core assumption:** There exists a functional activation threshold (not necessarily explicit in the architecture) beyond which selective attention mechanisms engage.
- **Evidence anchors:**
  - [section 4.1] Effect size between K1 and K5 clusters reaches 0.82-0.99 Cohen's d, while adjacent cluster comparisons show much weaker effects (0.16-0.34)
  - [section 5.1] "This intra-neuronal attention mechanism, by facilitating attentional focus on only the highest activations, would thus operate an attentional dichotomization"
  - [corpus] Sparse activation patterns in transformers (lazy neuron phenomenon) suggest binary-like activation patterns are emergent
- **Break condition:** If activation-categorical relationships were uniformly distributed across all activation levels without threshold effects, dichotomization would not occur.

### Mechanism 3
- **Claim:** Intra-neuronal attention enables categorical clipping—extracting semantically coherent sub-dimensions that feed hierarchical abstraction in subsequent layers.
- **Core assumption:** Categorical restructuring across layers depends on intra-neuronal attention mechanisms identifying which token subsets merit propagation.
- **Evidence anchors:**
  - [abstract] "These tokens exhibit greater categorical homogeneity and become the basis for more effective categorical abstractions in subsequent layers"
  - [section 2.2] Categorical clipping involves "categorical reduction" (greater semantic homogeneity) and "categorical selectivity" (extraction of smaller subgroups)
  - [corpus] Weak corpus evidence for this specific mechanism; related work focuses on attention heads rather than intra-neuronal processes
- **Break condition:** If layer n+1 categorical structures showed no relationship to highly-activated token subsets from layer n, the hierarchical abstraction mechanism would be unsupported.

## Foundational Learning

- **Concept: Categorical homogeneity (semantic coherence within token groups)**
  - Why needed here: The entire methodology depends on measuring whether tokens in activation-based groups are semantically similar. Without understanding cosine similarity as a proxy for semantic relatedness, the results are uninterpretable.
  - Quick check question: Given two token groups with mean pairwise cosine similarities of 0.25 and 0.45, which shows greater categorical homogeneity?

- **Concept: Activation space segmentation**
  - Why needed here: The paper tests whether activation ranges map to categorical segments. You must understand that neurons produce scalar activation values per token, creating a distribution that can be partitioned.
  - Quick check question: If a neuron activates to values [0.3, 1.2, 2.1, 3.8] for four tokens, what are two ways to segment this activation space?

- **Concept: Polysemantic neurons**
  - Why needed here: Individual neurons respond to multiple seemingly unrelated token types. This explains why most tokens show activation indifferentiation—only the highest-activation subset achieves categorical convergence.
  - Quick check question: Why might a single neuron activate strongly for both "bank" (financial) and "bank" (river edge), and how does this relate to the weak overall activation-categorical correlation?

## Architecture Onboarding

- **Component map:** GPT-2XL FFN layers 0-1 (12,800 perceptron neurons) → 100 core-tokens per neuron (highest mean activations) → 5 categorical clusters via hierarchical clustering or GPT-4o prompting → 4 activation quartiles for bottom-up analysis → cosine similarity computed in GPT-2XL embedding space.

- **Critical path:** (1) Extract neuron activations across token corpus → (2) Identify top-100 activating tokens per neuron → (3) Cluster tokens categorically (embeddings or LLM) → (4) Test activation differences across categorical clusters (Kruskal-Wallis) → (5) Test categorical homogeneity across activation segments (cosine similarity analysis).

- **Design tradeoffs:** Using GPT-2XL embeddings for semantic similarity is self-referential but enables human-interpretable categories. Alternative: layer-specific embeddings would capture "alien concepts" but reduce interpretability. Quartile segmentation ensures equal group sizes; hierarchical clustering better separates activation ranges but creates uneven groups.

- **Failure signatures:** High activation interleaving (4-5x risk ratios in Tables 5-6) indicates categorical clusters don't occupy distinct activation ranges. Low Kruskal-Wallis significance (18-31% of neurons) shows most neurons lack strong activation-categorical relationships. If cosine similarity doesn't increase from G1→G4, the homogeneity hypothesis fails.

- **First 3 experiments:**
  1. **Replication check:** For a single neuron, plot activation values vs. pairwise cosine similarities of its 100 core-tokens. Verify the positive correlation exists primarily at high activation values.
  2. **Threshold sensitivity:** Vary the "very high activation" threshold (top 10%, 5%, 1% of tokens) and measure whether categorical homogeneity continues increasing. This tests whether the relationship is truly threshold-based or gradual.
  3. **Cross-layer validation:** Compare categorical homogeneity of highly-activated tokens in layer 0 vs. layer 1. If the mechanism enables hierarchical abstraction, layer 1's high-activation tokens should show even greater homogeneity (trend suggested by Tables 7-10).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does using layer-specific input embeddings (rather than the model's initial input embeddings) reveal a broader homomorphic relationship between activation and categorical segmentation?
- **Basis in paper:** [explicit] The authors state in the conclusion that "it could be worthwhile to explore the relationship... using each respective layer’s own input embeddings," suggesting the current method might be "biased, self-referential, and anthropomorphic."
- **Why unresolved:** The current study operationalized categorical homogeneity using only the GPT-2XL input embedding space, which may obscure layer-specific semantic structures ("alien concepts") intrinsic to deeper layers.
- **What evidence would resolve it:** A replication of the study's methodology where categorical homogeneity is calculated using the activation vectors specific to each neuron's layer, checking if the activation-categorical correlation extends beyond just the "very high" activation zones.

### Open Question 2
- **Question:** Does the intra-neuronal attention mechanism strengthen or evolve in deeper layers of the transformer architecture (layers 2–48)?
- **Basis in paper:** [explicit] The study notes that the slightly more pronounced trends in Layer 1 compared to Layer 0 "could support a hypothesis suggesting that the synthetic phenomena we highlight tend to increase in deeper layers."
- **Why unresolved:** The study was restricted to the first two perceptron layers (12,800 neurons total) to reduce analytical scope, leaving the dynamics of the remaining 46 layers unexplored.
- **What evidence would resolve it:** Extending the "bottom-up" analysis of categorical homogeneity in high-activation clusters to the deeper perceptron layers of GPT-2XL to observe if the correlation effect sizes (e.g., Cohen's d) increase.

### Open Question 3
- **Question:** Do the observed relationships between activation segmentation and categorical homogeneity generalize to larger, state-of-the-art language models?
- **Basis in paper:** [inferred] The authors intentionally selected GPT-2XL to "avoid... the interpretative challenges posed by GPT-4 or GPT-4o" for this initial exploration.
- **Why unresolved:** Larger models possess greater complexity and potentially different representational geometries; it is unknown if the "tenuous" relationship found in GPT-2XL is a universal feature of synthetic cognition or an artifact of this specific architecture size.
- **What evidence would resolve it:** Applying the same statistical protocols (top-down cluster analysis and bottom-up cosine similarity checks) to the perceptron layers of more advanced models like GPT-4 or Llama-3 to verify the presence of intra-neuronal attention.

## Limitations
- Analysis restricted to first two layers of a single model architecture, limiting generalizability to deeper layers and other model types
- Self-referential use of GPT-2XL embeddings for semantic similarity may introduce circularity and bias the observed relationships
- Focus on only high-activation tokens (top 100 per neuron) may miss important categorical information encoded in lower-activation contexts

## Confidence

- **High Confidence:** The systematic but weak relationship between activation and categorical segmentation is well-supported by the Kruskal-Wallis tests (18-31% significance) and effect sizes (0.82-0.99 Cohen's d for K1-K5). The observation that high-activation tokens show greater categorical homogeneity than lower-activation tokens is consistently observed across both top-down and bottom-up analyses.

- **Medium Confidence:** The mechanism by which activation serves as an attentional vector is plausible but requires additional validation. While the data shows higher activation correlates with greater categorical homogeneity, the causal relationship between this pattern and actual attention mechanisms in subsequent layers remains inferential.

- **Low Confidence:** The hierarchical abstraction mechanism (Mechanism 3) has the weakest support, as the paper provides minimal evidence for how categorical clipping actually propagates through layers to enable more effective abstractions. The related work cited focuses on attention heads rather than intra-neuronal processes.

## Next Checks

1. **Cross-Model Validation:** Apply the same analysis framework to GPT-3 or GPT-4 architectures to determine whether intra-neuronal attention mechanisms generalize beyond GPT-2XL. This would test whether the observed activation-categorical relationships are architecture-specific or represent a broader phenomenon in transformer-based language models.

2. **Layer-to-Layer Propagation Analysis:** Track how categorical homogeneity of highly-activated token subsets evolves from layer 0 to layer 1, then to higher layers. Specifically, measure whether the semantic coherence of tokens with highest activations in layer n increases monotonically through subsequent layers, providing direct evidence for the hierarchical abstraction hypothesis.

3. **Alternative Semantic Embeddings Test:** Repeat the analysis using external semantic embeddings (e.g., BERT, sentence-BERT) rather than GPT-2XL's own embeddings to control for potential circularity. If the activation-categorical relationships persist with independent embeddings, this would strengthen confidence that the observed patterns reflect genuine semantic structure rather than self-referential embedding properties.