---
ver: rpa2
title: Informative Text-Image Alignment for Visual Affordance Learning with Foundation
  Models
arxiv_id: '2509.17074'
source_url: https://arxiv.org/abs/2509.17074
tags:
- affordance
- learning
- visual
- features
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an informative framework for text-guided affordance
  learning by incorporating mutual information-based constraints to achieve text-image
  alignment at feature level. The method introduces affordance mutual information
  constraint to align visual affordance areas with textual descriptions by maximizing
  mutual information between features, and object-level information constraint to
  maintain text-image alignment properties of foundation models.
---

# Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models

## Quick Facts
- **arXiv ID**: 2509.17074
- **Source URL**: https://arxiv.org/abs/2509.17074
- **Reference count**: 21
- **Primary result**: Achieves SOTA one-shot affordance learning with KLD of 0.719, SIM of 0.581, and NSS of 1.787 on AGD20K Seen split

## Executive Summary
This paper introduces an informative framework for text-guided affordance learning that leverages mutual information-based constraints to achieve feature-level text-image alignment. The method combines an affordance mutual information constraint that aligns visual affordance areas with textual descriptions through feature maximization, and an object-level information constraint that preserves the text-image alignment properties of foundation models. Experiments on the AGD20K dataset demonstrate state-of-the-art performance in one-shot affordance learning scenarios.

## Method Summary
The proposed framework integrates two key constraints to enhance text-image alignment for visual affordance learning. The affordance mutual information constraint maximizes the mutual information between visual features of affordance regions and their corresponding textual descriptions, ensuring semantic consistency. The object-level information constraint maintains the alignment properties of foundation models like CLIP by preserving object-level correspondence between text and image features. This dual-constraint approach enables effective one-shot learning of visual affordances without requiring extensive labeled data.

## Key Results
- Achieves state-of-the-art performance on AGD20K dataset for one-shot affordance learning
- Reports KLD of 0.719, SIM of 0.581, and NSS of 1.787 on Seen split
- Outperforms existing approaches in visual affordance detection and localization

## Why This Works (Mechanism)
The framework's effectiveness stems from its dual-constraint approach that addresses both affordance-specific and object-level alignment. By maximizing mutual information between visual affordance regions and textual descriptions, the method ensures that learned representations capture the semantic relationship between what objects can do and how they appear. The object-level constraint preserves the foundational text-image alignment capabilities of pre-trained models, preventing degradation of general visual understanding while specializing for affordance detection.

## Foundational Learning
- **Mutual Information Maximization**: Measures statistical dependence between visual and textual features to ensure semantic alignment - needed to quantify the relationship between affordance regions and their descriptions
- **Feature-Level Alignment**: Operates at the embedding level rather than pixel-level for more abstract semantic matching - needed to capture higher-level relationships beyond surface appearance
- **Foundation Model Integration**: Leverages pre-trained models like CLIP as backbone for text-image correspondence - needed to benefit from large-scale pre-training without extensive re-training

## Architecture Onboarding

**Component Map**: Foundation Model -> Feature Extractor -> Mutual Information Module -> Object-Level Constraint -> Output

**Critical Path**: Input Image & Text → Foundation Model Feature Extraction → Mutual Information Constraint Application → Object-Level Constraint Integration → Affordance Prediction

**Design Tradeoffs**: The approach balances between leveraging pre-trained foundation models (requiring less training data) and introducing task-specific constraints (requiring careful hyperparameter tuning). This trade-off enables one-shot learning but may limit flexibility for domain-specific adaptations.

**Failure Signatures**: Poor performance on ambiguous affordances, failure to generalize beyond AGD20K dataset, sensitivity to textual description quality, and potential degradation of object-level alignment when mutual information constraints dominate.

**First Experiments**:
1. Validate mutual information maximization on synthetic affordance-text pairs
2. Test object-level constraint preservation on standard image retrieval tasks
3. Evaluate one-shot learning performance with varying amounts of training data

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the methodology raises several implicit questions about generalizability, handling of ambiguous affordances, and real-world robotic application scenarios.

## Limitations
- Limited evaluation to AGD20K dataset without testing on other benchmarks or real-world scenarios
- No assessment of performance on ambiguous affordances or context-dependent object usage
- Reliance on foundation models introduces dependency and potential coverage limitations for specialized domains

## Confidence

**High confidence**: The experimental methodology and results on AGD20K dataset are well-documented and reproducible. The reported metrics (KLD of 0.719, SIM of 0.581, NSS of 1.787) appear technically sound.

**Medium confidence**: The claim of state-of-the-art performance is supported by comparison with existing methods, but the absence of ablation studies makes it difficult to isolate the contribution of each proposed constraint.

**Low confidence**: Claims about real-world applicability and robustness to ambiguous affordances are not empirically validated in the paper.

## Next Checks
1. Evaluate the method on additional visual affordance datasets or real-world robotic manipulation scenarios to assess generalizability
2. Conduct ablation studies to quantify the individual contributions of the affordance mutual information constraint and object-level information constraint
3. Test the method's performance on ambiguous affordance scenarios where objects have multiple valid affordances or where textual descriptions are underspecified