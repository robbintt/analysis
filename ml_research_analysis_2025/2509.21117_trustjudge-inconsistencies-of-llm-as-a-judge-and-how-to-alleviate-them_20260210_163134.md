---
ver: rpa2
title: 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them'
arxiv_id: '2509.21117'
source_url: https://arxiv.org/abs/2509.21117
tags:
- evaluation
- trustjudge
- pairwise
- scoring
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TrustJudge addresses two fundamental inconsistencies in LLM-as-a-judge
  evaluation frameworks: Score-Comparison Inconsistency (lower-rated responses outperforming
  higher-scored ones in pairwise comparisons) and Pairwise Transitivity Inconsistency
  (non-transitive preference cycles). The framework introduces distribution-sensitive
  scoring that computes continuous expectations from discrete rating probabilities,
  preserving information entropy for more precise scoring, and likelihood-aware aggregation
  that resolves transitivity violations using bidirectional preference probabilities
  or perplexity.'
---

# TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them

## Quick Facts
- arXiv ID: 2509.21117
- Source URL: https://arxiv.org/abs/2509.21117
- Reference count: 30
- Reduces Score-Comparison inconsistency by 8.43% and Pairwise Transitivity inconsistency by 10.82%

## Executive Summary
TrustJudge addresses two fundamental inconsistencies in LLM-as-a-judge evaluation frameworks: Score-Comparison Inconsistency (where lower-rated responses outperform higher-scored ones in pairwise comparisons) and Pairwise Transitivity Inconsistency (non-transitive preference cycles). The framework introduces distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. When evaluated with Llama-3.1-70B-Instruct as judge, TrustJudge significantly reduces both types of inconsistencies while maintaining higher evaluation accuracy across various model architectures and scales.

## Method Summary
TrustJudge introduces two key innovations to address inconsistencies in LLM-as-a-judge frameworks. First, it employs distribution-sensitive scoring that treats discrete ratings (1-5 stars) as probability distributions rather than fixed values, computing continuous expectations to preserve information entropy and enable more precise scoring. Second, it implements likelihood-aware aggregation that resolves pairwise transitivity violations by incorporating bidirectional preference probabilities or perplexity measures, ensuring consistent preference relationships across multiple comparisons. These methods work together to create a more reliable evaluation framework that reduces both Score-Comparison and Pairwise Transitivity inconsistencies.

## Key Results
- Reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%)
- Reduces Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%)
- Maintains higher evaluation accuracy across various model architectures and scales

## Why This Works (Mechanism)
TrustJudge works by addressing the fundamental mismatch between absolute scoring and relative pairwise comparisons in LLM evaluation. Traditional LLM-as-a-judge frameworks suffer from inconsistencies because discrete ratings don't capture the full uncertainty and preference relationships between responses. By treating ratings as probability distributions and computing continuous expectations, TrustJudge preserves more information about the judge's uncertainty. The likelihood-aware aggregation then ensures that pairwise comparisons respect transitivity by incorporating bidirectional preference probabilities, preventing cycles where A is preferred to B, B to C, but C to A. This dual approach creates a more coherent evaluation framework that aligns absolute scores with relative preferences.

## Foundational Learning
- Probability distributions in ratings: Why needed - to capture uncertainty in discrete ratings; Quick check - verify that computed expectations vary smoothly with input probabilities
- Information entropy preservation: Why needed - to maintain precision in scoring; Quick check - confirm that entropy measures increase with more uncertain ratings
- Transitivity in preference relationships: Why needed - to ensure logical consistency in pairwise comparisons; Quick check - test that no preference cycles exist in simple three-item comparisons
- Bidirectional preference probabilities: Why needed - to resolve conflicts between competing preferences; Quick check - verify that bidirectional probabilities sum to reasonable values
- Perplexity measures: Why needed - as an alternative metric for preference strength; Quick check - confirm perplexity decreases with increasing preference confidence

## Architecture Onboarding

Component map: Input ratings -> Distribution-sensitive scoring -> Pairwise comparisons -> Likelihood-aware aggregation -> Consistent evaluation output

Critical path: The distribution-sensitive scoring must complete before pairwise comparisons can be made, and the likelihood-aware aggregation depends on both the scores and comparison results to produce the final consistent evaluation.

Design tradeoffs: The framework trades computational complexity for improved consistency - distribution-sensitive scoring requires more computation than simple averaging, and likelihood-aware aggregation adds overhead to resolve transitivity issues. However, these costs are justified by the significant improvements in evaluation reliability.

Failure signatures: Score-Comparison inconsistency manifests as cases where a response with a higher star rating loses to a lower-rated response in pairwise comparison. Pairwise Transitivity inconsistency appears as preference cycles (A>B, B>C, C>A) in multi-way comparisons.

Three first experiments:
1. Verify that distribution-sensitive scoring produces continuous rather than discrete output values
2. Test that pairwise comparisons respect transitivity in simple three-response scenarios
3. Measure the reduction in Score-Comparison inconsistency on a held-out test set

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across different LLM judge models remains untested beyond Llama-3.1-70B-Instruct
- Performance on diverse evaluation scenarios and domains needs further validation
- Real-world applicability in varied evaluation contexts is not fully established

## Confidence
- High confidence in the identification and mathematical formulation of Score-Comparison and Pairwise Transitivity inconsistencies
- Medium confidence in the effectiveness of distribution-sensitive scoring and likelihood-aware aggregation methods based on results from a single judge model
- Medium confidence in the claimed improvements in evaluation accuracy, as this depends on the specific evaluation datasets and tasks used

## Next Checks
1. Evaluate TrustJudge's performance across a broader range of LLM judge models (e.g., GPT-4, Claude, other Llama variants) to assess generalizability
2. Test the framework on diverse evaluation scenarios beyond the specific tasks used in the study, including different domains and types of responses
3. Conduct a longitudinal study to assess the stability of TrustJudge's improvements over time and across multiple evaluation cycles