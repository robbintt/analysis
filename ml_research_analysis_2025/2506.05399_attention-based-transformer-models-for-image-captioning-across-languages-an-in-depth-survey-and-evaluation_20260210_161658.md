---
ver: rpa2
title: 'Attention-based transformer models for image captioning across languages:
  An in-depth survey and evaluation'
arxiv_id: '2506.05399'
source_url: https://arxiv.org/abs/2506.05399
tags:
- image
- captioning
- attention
- transformer
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews attention-based transformer
  models for image captioning across multiple languages. It categorizes models into
  transformer-based, deep learning-based, and hybrid approaches, and discusses evaluation
  metrics such as BLEU, METEOR, CIDEr, and ROUGE.
---

# Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation

## Quick Facts
- arXiv ID: 2506.05399
- Source URL: https://arxiv.org/abs/2506.05399
- Reference count: 40
- Primary result: Comprehensive survey of attention-based transformer models for multilingual image captioning, identifying performance gaps and future research directions

## Executive Summary
This survey comprehensively reviews attention-based transformer models for image captioning across multiple languages, categorizing approaches into transformer-based, deep learning-based, and hybrid methods. The study evaluates performance using standard metrics like BLEU-4, METEOR, CIDEr, and ROUGE, showing strong results on English datasets (up to 0.414 BLEU-4 on MS COCO) but highlighting significant challenges in multilingual adaptation. Key challenges identified include semantic inconsistencies, data scarcity in non-English languages, and limitations in reasoning ability for complex scenes.

## Method Summary
The survey synthesizes research on encoder-decoder transformer architectures for image captioning, where CNN encoders (ResNet, Inception, Faster R-CNN) extract visual features that transformer decoders process through multi-head self-attention and cross-attention mechanisms. Models are trained using cross-entropy loss with beam search inference, and performance is evaluated across standard benchmark datasets including MS COCO, Flickr8k, and Flickr30k. The survey categorizes approaches by architecture type and discusses multilingual extensions through translation methodologies and culturally nuanced adaptations.

## Key Results
- BLEU-4 scores reach 0.414 on MS COCO for top English models
- Standard evaluation metrics (BLEU, METEOR, CIDEr, ROUGE) show poor correlation with human judgments
- Performance drops significantly for non-English languages due to data scarcity and translation quality issues
- Attention mechanisms enable better scene understanding compared to traditional RNN/LSTM approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-attention enables transformers to process complete sequences simultaneously rather than word-by-word, resolving long-distance dependency issues inherent in RNN/LSTM architectures.
- **Mechanism:** Self-attention computes scaled dot-product relationships between all query-key-value triplets, generating attention weights that determine how each element relates to every other element. Multi-head attention (h=8 heads in original design) runs parallel attention operations across different representation subspaces, capturing both short-term and long-term dependencies.
- **Core assumption:** The mathematical similarity of word vectors (dot product) correlates with semantic relatedness, and weighted sums of value vectors can meaningfully represent contextual information.
- **Evidence anchors:** [abstract] "leveraging attention mechanisms for better scene understanding"; [section 3.2.2-3.2.3] "Self-attention, or intra-attention, focuses on the relationships between different positions in a single sequence... Multi-head attention allows for simultaneous self-attention across different sections of the input sequence"; [corpus] "Beyond RNNs: Benchmarking Attention-Based Image Captioning Models" validates attention mechanisms outperform traditional RNN approaches.
- **Break condition:** When input sequences exceed memory capacity (quadratic complexity O(n²) for self-attention), or when relative spatial relationships in 2D/3D images don't align with 1D sequence assumptions.

### Mechanism 2
- **Claim:** The encoder-decoder architecture with cross-attention bridges visual features and language generation by learning which image regions to attend to at each word generation step.
- **Mechanism:** A CNN encoder (e.g., ResNet, Inception, Faster R-CNN) extracts visual features into fixed-size vectors or region proposals. The transformer decoder uses cross-attention where generated words act as queries and encoder outputs act as keys/values, dynamically focusing on relevant image regions during each generation step.
- **Core assumption:** Visual features extracted by CNNs contain sufficient semantic information to generate natural language descriptions, and the mapping between visual regions and words can be learned through attention weights.
- **Evidence anchors:** [abstract] "transformer-based models have significantly improved caption generation by leveraging attention mechanisms"; [section 3.1, Fig.9] "The encoder comprises a CNN for extracting image representations, while the decoder incorporates an LSTM for generating image captions"; [corpus] "Good Representation, Better Explanation: Role of Convolutional Neural Networks in Transformer-Based Remote Sensing Image Captioning" supports CNN encoder importance.
- **Break condition:** When object detection fails to identify relevant regions, or when non-visual words (e.g., "to," "itself") require linguistic rather than visual attention.

### Mechanism 3
- **Claim:** Bottom-up and top-down attention approaches provide complementary pathways—object-level saliency detection combined with task-driven context—that improve caption quality over single-direction attention.
- **Mechanism:** Bottom-up attention uses object detectors (e.g., Faster R-CNN) to identify salient image regions at the object level. Top-down attention applies task-specific context to weight these regions, computing attended feature vectors as weighted averages. The combination allows models to both detect what's visually prominent and determine what's linguistically relevant.
- **Core assumption:** Human visual processing combines stimulus-driven (bottom-up) and goal-directed (top-down) attention, and AI systems should mimic this dual pathway.
- **Evidence anchors:** [abstract] "key challenges including semantic inconsistencies, data scarcity in non-English languages, and limitations in reasoning ability"; [section 4.4.2] "All methods that utilize bottom-up attention perform better because bottom-up attention focuses on visual attention at the object level".
- **Break condition:** When images contain natural elements (mountains, sky, trees) without recognizable objects—bottom-up object detectors may fail, requiring domain-specific knowledge not captured by standard detectors trained on COCO/ImageNet.

## Foundational Learning

- **Concept: Softmax and probability distributions**
  - **Why needed here:** The attention mechanism outputs softmax-normalized weights that determine how much each image region contributes to each generated word. Understanding how softmax converts raw scores to probability distributions is essential for debugging attention patterns.
  - **Quick check question:** Given attention scores [2.0, 1.0, 0.1] for three image regions, what would the softmax weights be? (Should understand: exp normalization, sum to 1.0)

- **Concept: Encoder-decoder sequence-to-sequence architecture**
  - **Why needed here:** Image captioning fundamentally treats the task as sequence-to-sequence: an encoder compresses visual information into a latent representation, and a decoder generates a text sequence autoregressively. This pattern appears in all surveyed approaches.
  - **Quick check question:** In a transformer decoder generating captions, why is masking applied to prevent positions from attending to future positions? (Should connect to autoregressive generation constraints)

- **Concept: Transfer learning and pre-trained features**
  - **Why needed here:** All surveyed approaches use pre-trained CNNs (ResNet, Inception, VGG, Faster R-CNN) for feature extraction. Understanding why pre-trained features transfer and how to fine-tune them is critical for practical implementation.
  - **Quick check question:** Why might features learned on ImageNet (1000 object categories) be insufficient for medical image captioning or remote sensing? (Should understand domain gap, specialized features)

## Architecture Onboarding

- **Component map:** Input Image → Preprocessing → CNN Encoder → Visual Features → Positional Encoding → Transformer Encoder → Encoded Features → Transformer Decoder → Linear + Softmax → Caption

- **Critical path:** Feature extraction quality (CNN backbone choice, region proposals vs. grid features) → Attention alignment between visual regions and generated words → Language model capacity (vocabulary size, handling out-of-vocabulary words) → For multilingual: Dataset quality and translation methodology

- **Design tradeoffs:** Grid features vs. region features (spatial info vs. object semantics); Soft vs. hard attention (differentiable vs. non-differentiable); Model size vs. multilingual coverage (parameter sharing vs. language-specific nuances); Assumption: Trade-offs may shift with newer architectures

- **Failure signatures:** Hallucination (describes objects not present); Repetition (repeats phrases); Poor grammar in translated captions (machine-translated training data); Low BLEU but high CIDEr or vice versa (different metrics capture different aspects)

- **First 3 experiments:** 1) Baseline replication: Implement CNN+LSTM encoder-decoder on Flickr8k, establish BLEU-1/4 and CIDEr baselines; 2) Attention visualization: Extract and visualize attention maps for generated captions; 3) Language transfer test: Train on English MS COCO, evaluate on translated Arabic/Bengali test sets

## Open Questions the Paper Calls Out

**Open Question 1:** How can transformer-based image captioning models be effectively adapted to low-resource languages with limited training data?
- Basis in paper: [explicit] The paper identifies "data scarcity in non-English languages" as a key limitation and notes that models "often struggle with non-English languages and culturally nuanced images, leading to less accurate or contextually relevant captions."
- Why unresolved: Non-English datasets remain scarce, and direct translation approaches yield grammatical errors and poor sentence structure.
- What evidence would resolve it: Demonstrated improvements in BLEU/METEOR/CIDEr scores for low-resource languages through transfer learning or few-shot approaches.

**Open Question 2:** How can automatic evaluation metrics be improved to better correlate with human judgments of caption quality?
- Basis in paper: [explicit] The paper states: "These automated measures mainly focus on lexical or semantic data and do not fully capture the complex relationships between words and objects... it is essential to improve automatic evaluation methods to more accurately reflect human judgments."
- Why unresolved: Existing metrics show poor correlation with human evaluations and cannot assess factual accuracy or cultural appropriateness.
- What evidence would resolve it: New metrics demonstrating statistically significant higher correlation with human judgments across diverse image types.

**Open Question 3:** How can attention-based transformer models be optimized for real-time captioning applications in critical domains like healthcare and security?
- Basis in paper: [explicit] The paper highlights "ongoing difficulties in achieving real-time captioning in critical fields like healthcare and security" and identifies "real-time and interactive captioning" as a future research direction.
- Why unresolved: Transformer self-attention mechanisms are computationally expensive, and attention-based approaches often require longer training times than RNN-based methods.
- What evidence would resolve it: Models achieving reduced inference latency while maintaining competitive BLEU/CIDEr scores.

**Open Question 4:** How can image captioning models be enhanced to reason about complex, abstract, or ambiguous visual scenes beyond object recognition?
- Basis in paper: [explicit] The paper identifies "limitations in reasoning ability" as a key challenge and notes "a growing need to develop models that can understand and generate captions for complex, abstract, or ambiguous images, where traditional methods may fall short."
- Why unresolved: Current models excel at object detection and spatial relationships but struggle with implicit meanings, causal reasoning, and scenarios requiring world knowledge.
- What evidence would resolve it: Improved performance on benchmarks testing reasoning abilities, validated through human evaluations.

## Limitations

- Data scarcity in non-English languages remains a critical bottleneck, with performance gaps between native-annotated and machine-translated datasets not rigorously quantified
- Standard evaluation metrics (BLEU, CIDEr, METEOR, ROUGE) show poor correlation with human judgments, yet remain the primary performance benchmarks
- Computational complexity analysis focuses on traditional self-attention without addressing modern efficient transformer variants

## Confidence

**Transformer Attention Mechanisms (High):** Core claims about self-attention and multi-head attention mechanisms are well-supported by extensive citations and align with established transformer literature.

**Multilingual Performance Claims (Medium):** Performance degradation claims are reasonable but not rigorously quantified through controlled experiments isolating specific factors.

**Evaluation Metric Reliability (High):** Critical assessment of automatic metrics is well-founded and supported by citations showing poor human-metric correlation.

## Next Checks

**Check 1:** Replicate the performance gap analysis by training the same transformer model on three versions of a multilingual dataset: native-annotated captions, machine-translated English captions, and human-post-edited machine translations. Measure BLEU-4, METEOR, and CIDEr differences to quantify the exact impact of translation quality.

**Check 2:** Implement attention visualization for generated captions and conduct a human evaluation study where annotators rate caption quality vs. attention alignment quality. Test the hypothesis that better attention grounding correlates with human-perceived caption quality.

**Check 3:** Implement both standard self-attention and an efficient alternative (e.g., Performer or Longformer) on the same image captioning task. Measure not just accuracy metrics but also training/inference time and memory usage to provide concrete evidence about computational complexity claims.