---
ver: rpa2
title: 'DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs'
arxiv_id: '2504.20754'
source_url: https://arxiv.org/abs/2504.20754
tags:
- paths
- diffusion
- graph
- guidance
- discrete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating paths in layered
  graphs using discrete diffusion models while ensuring structural validity. The authors
  propose a novel "padded adjacency-list matrix" (PALM) representation that guarantees
  generated samples are valid paths by construction, unlike previous approaches that
  rely on post-processing.
---

# DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs

## Quick Facts
- arXiv ID: 2504.20754
- Source URL: https://arxiv.org/abs/2504.20754
- Reference count: 20
- Primary result: Proposed method achieves 100% valid path generation in layered graphs compared to baselines

## Executive Summary
This paper addresses the problem of generating valid paths in layered graphs using discrete diffusion models while ensuring structural validity. The authors propose a novel "padded adjacency-list matrix" (PALM) representation that guarantees generated samples are valid paths by construction, unlike previous approaches that rely on post-processing. The method uses discrete denoising diffusion probabilistic models (D3PM) and introduces a classifier guidance technique called "discrete diffusion posterior sampling" (DDPS) to steer sampling toward paths containing preferred edges. Experiments on synthetic and real-world graph datasets show that their method achieves 100% valid path generation compared to baselines, and effectively balances reward optimization with maintaining the learned distribution.

## Method Summary
The proposed method uses discrete denoising diffusion probabilistic models (D3PM) with a custom "padded adjacency-list matrix" (PALM) representation to guarantee valid path generation. The model is trained on path datasets where each path is encoded as a stack of one-hot vectors (PALM) indicating specific outgoing edges for every vertex. During inference, the DDPS technique applies classifier guidance by approximating the log-likelihood ratio with gradients of expected reward, steering the sampling process toward preferred paths. The approach uses per-vertex transition matrices based on out-degrees and includes a reward model to compute expected rewards from logits. The entire pipeline ensures 100% valid rate while optimizing for reward objectives.

## Key Results
- Achieved 100% valid rate (VR) on all datasets compared to baselines with VR < 100%
- Successfully balanced reward optimization with distribution fidelity across different guidance scales
- Demonstrated effectiveness on synthetic (Toy) and real-world (Heights, Ridge) layered graph datasets
- Showed superior performance compared to SwinGNN and EDP-GNN baselines

## Why This Works (Mechanism)

### Mechanism 1: Structural Guarantees via Constrained Representation Space
- Claim: PALM representation ensures all generated samples are valid paths by construction.
- Mechanism: PALM encodes paths as a collection of one-hot vectors, one per vertex, where each vector selects exactly one outgoing edge. The many-to-one mapping from PALM to paths (with uniform assignment for off-path vertices) guarantees that any valid PALM decodes deterministically to a valid path by following edges layer-by-layer from v₁.
- Core assumption: The layered graph structure (edges only between adjacent layers, singleton first layer, no isolated vertices) ensures any sequence of valid edge selections traces a complete path.
- Evidence anchors: [abstract], [section 3], [corpus]
- Break condition: If vertices have zero out-degree but non-zero in-degree (violating Definition 1 condition 4), or if graph is not properly layered, path tracing may terminate prematurely.

### Mechanism 2: Discrete Guidance via Gradient-Based Log-Likelihood Approximation
- Claim: Classifier guidance can be adapted to discrete diffusion by approximating the log-likelihood ratio with gradients of expected reward.
- Mechanism: DDPS applies Bayes' theorem to discrete diffusion (Eq. 5), where the posterior log p(x_{t-1}|x_t, y) combines the learned prior p_θ(x_{t-1}|x_t) with a log-likelihood ratio. Since discrete Stein scores are undefined, the method approximates this ratio as ∇_z R(z) where z are the predicted PALM logits and R(z) is the expected total reward. The gradient propagates through the categorical distribution to increase selection probabilities of edges leading to preferred paths.
- Core assumption: The gradient of expected reward with respect to logits approximates the true posterior adjustment well enough to steer sampling without catastrophic deviation from the learned distribution.
- Evidence anchors: [abstract], [section 4.2], [corpus]
- Break condition: If guidance scale λ is too large, the gradient term dominates and samples drift far from the learned prior distribution (observed in Section 5.3 as rising distribution distances after λ ≈ 100).

### Mechanism 3: Per-Vertex Transition Matrices for Variable-Dimensional Categorical Spaces
- Claim: Custom transition matrices per vertex enable diffusion over PALM's non-uniform edge dimensions.
- Mechanism: Each vertex v has out-degree D_v, requiring a D_v-dimensional categorical distribution. The transition matrix Q_t^v (Eq. 9) is constructed as a D_v × D_v doubly stochastic matrix with uniform off-diagonal transitions, ensuring the absorption state respects each vertex's local structure. Vertices with zero out-degree use identity matrices.
- Core assumption: The uniform transition structure within each vertex's edge space adequately models the corruption process across heterogeneous degree distributions.
- Evidence anchors: [section 3], [appendix B.2], [corpus]
- Break condition: If vertices have vastly different out-degrees (e.g., 2 vs. 100), uniform transition rates may corrupt sparse edge selections faster than dense ones, potentially causing uneven noise schedules.

## Foundational Learning

- Concept: **Discrete Denoising Diffusion Probabilistic Models (D3PM)**
  - Why needed here: DDPS builds directly on D3PM's forward/backward Markov chains over categorical distributions (Eqs. 1-3). Without understanding transition matrices Q_t and the variational bound loss L_vb, the training and inference modifications are opaque.
  - Quick check question: Can you explain why the backward process requires marginalizing over all possible x_0 (Eq. 2) rather than predicting x_{t-1} directly?

- Concept: **Bayes' Theorem for Posterior Sampling**
  - Why needed here: The core guidance mechanism derives from the logarithmic Bayes' rule (Eq. 5), decomposing the posterior into prior and likelihood ratio. This is the theoretical justification for why gradient-based guidance approximates conditional generation.
  - Quick check question: Why does the absence of a well-defined Stein score in discrete spaces force a return to Bayes' theorem rather than score-based guidance?

- Concept: **Layered Graph Theory (DAGs with Layer Constraints)**
  - Why needed here: The PALM representation exploits the specific structure of layered graphs—edges only between adjacent layers, singleton source, no dead-end intermediate vertices. This structure enables the guaranteed path decoding.
  - Quick check question: What goes wrong in the PALM-to-path conversion if a vertex in layer l < L has positive in-degree but zero out-degree?

## Architecture Onboarding

- Component map: Input: Layered graph G = (V, E) -> [PALM Encoder] -> [D3PM Backbone] -> [Reward Model] -> [Guidance Module] -> [Transition Matrices {Q_t^v}] -> [PALM Decoder] -> Output: Valid path (v_1, ..., v_L)

- Critical path:
  1. **PALM construction correctness**: Off-path vertices must receive valid (though arbitrary) one-hot assignments during training—losses only computed on-path.
  2. **Transition matrix alignment**: Q_t^v must match each vertex's D_v dimension; mismatched dimensions cause sampling errors.
  3. **Reward gradient flow**: Algorithm 1 must correctly aggregate transition probabilities through the graph structure for gradient computation.

- Design tradeoffs:
  - **Guidance scale λ vs. distribution fidelity**: High λ improves reward but increases KL divergence from learned prior (Figure 5 shows "sweet spot" around λ ≈ 10-100 before degradation).
  - **PALM redundancy vs. simplicity**: Many-to-one PALM-to-path mapping introduces representational inefficiency (off-path vertices carry meaningless choices) but guarantees validity by construction.
  - **Uniform vs. learned transitions**: Uniform Q_t^v simplifies implementation but may not optimally model structured noise for sparse graphs.

- Failure signatures:
  - **VR = 0% with high guidance**: Likely bug in PALM decoder or transition matrix dimensions don't match D_v values.
  - **Rewards plateau below maximum**: Guidance scale too low, or reward model has incorrect edge-to-reward mapping.
  - **Training loss doesn't converge**: Check that losses are only computed for on-path vertices (Section 3).
  - **Generated paths violate graph edges**: Transition matrices incorrectly constructed; verify doubly stochastic property.

- First 3 experiments:
  1. **Validity sanity check**: Train on Toy dataset (11 layers, 41 vertices), generate 1000 samples with λ=0, manually verify all decode to valid paths (target: 100% VR per Table 1).
  2. **Guidance scale sweep**: On Toy with max-reward=7 configuration, sweep λ ∈ {1, 10, 100, 1000}, plot reward vs. λ (should show S-curve as in Figure 4a).
  3. **Distribution tradeoff analysis**: For λ values from experiment 2, compute IS-L-L1 and FLGD metrics against target distribution (λ=0 conditioned on max reward). Identify optimal λ where metrics minimize (expect λ ≈ 10-100 per Figure 5).

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the DDPS framework be extended to enforce more elaborate structural constraints beyond simple path validity in layered graphs? (Basis: [explicit] Conclusion mentions future work on "more elaborate constraints or rewards")
- **Open Question 2**: Does the many-to-one nature of the PALM representation introduce training inefficiencies or gradient noise? (Basis: [inferred] Section 3 notes the redundancy but doesn't analyze impact)
- **Open Question 3**: Is there a theoretical basis for the empirically observed "sweet spot" in the guidance scale λ that balances reward optimization and distribution adherence? (Basis: [inferred] Section 5.3 observes non-monotonic trade-off without explaining mechanism)

## Limitations
- Empirical evaluation relies heavily on synthetic "Toy" dataset with controlled reward structures, limiting generalizability to real-world scenarios with complex reward landscapes.
- The DDPS approximation (using gradients of expected reward as proxy for log-likelihood ratio) lacks theoretical grounding in how well it preserves the true posterior distribution.
- The method's computational overhead from calculating expected rewards through Algorithm 1 is not characterized, making scalability assessment incomplete.

## Confidence
- **High confidence**: The PALM representation guarantees structural validity (100% VR claim) - this follows directly from the many-to-one mapping construction and layered graph assumptions.
- **Medium confidence**: The DDPS guidance mechanism effectively balances reward optimization with distribution fidelity - supported by empirical results but approximation quality is unproven.
- **Low confidence**: The method's performance on complex, noisy real-world reward functions - evaluation is limited to synthetic binary reward cases.

## Next Checks
1. **Robustness to noisy rewards**: Evaluate DDPS on synthetic datasets where edge rewards contain Gaussian noise (σ ∈ {0.1, 0.5, 1.0}) to test sensitivity to reward model inaccuracies.
2. **Scalability analysis**: Measure training and inference time for DDPS versus baselines on Ridge dataset (4293 vertices) and plot time vs. reward tradeoff curves.
3. **Transfer learning capability**: Train on Toy dataset then fine-tune on Heights dataset with minimal epochs to assess how well learned diffusion priors transfer across graph structures.