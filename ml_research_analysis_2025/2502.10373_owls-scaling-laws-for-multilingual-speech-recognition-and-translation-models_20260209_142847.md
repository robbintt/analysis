---
ver: rpa2
title: 'OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models'
arxiv_id: '2502.10373'
source_url: https://arxiv.org/abs/2502.10373
tags:
- speech
- scaling
- owls
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OWLS, a suite of 13 large-scale multilingual
  speech recognition and translation models ranging from 0.25B to 18B parameters,
  trained on up to 360K hours of public speech data across 150 languages. The study
  systematically investigates how model size, training data size, and compute budget
  influence performance in multilingual speech tasks, deriving neural scaling laws
  that reliably predict downstream performance.
---

# OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models

## Quick Facts
- arXiv ID: 2502.10373
- Source URL: https://arxiv.org/abs/2502.10373
- Reference count: 40
- Primary result: Scaling laws derived for multilingual ASR/ST; 18B OWLS model is largest known public speech model

## Executive Summary
This paper introduces OWLS, a suite of 13 large-scale multilingual speech recognition and translation models ranging from 0.25B to 18B parameters, trained on up to 360K hours of public speech data across 150 languages. The study systematically investigates how model size, training data size, and compute budget influence performance in multilingual speech tasks, deriving neural scaling laws that reliably predict downstream performance. Key findings include consistent improvements in WER/CER across languages with model scaling, significant gains for low-resource languages, and enhanced capabilities such as code-switching and orthographic understanding in larger models.

## Method Summary
The study trains Transformer encoder-decoder models with hybrid CTC/attention loss (CTC weight 0.3) on the OWSM v3.2 dataset (180K hours) plus YODAS (180K additional hours for 360K total). Models range from 0.25B to 18B parameters with varying encoder/decoder layers, hidden sizes, FFN sizes, and attention heads. Training uses Adam optimizer with piecewise learning rate schedule (warmup 5e-5 over 30K steps, hold, then exponential decay), batch size 256, 675K steps, Bfloat16, FlashAttention 2, and DeepSpeed ZeRO-2. Evaluation uses FLEURS for ASR WER/CER across 102 languages and CoVoST2/MuST-C for ST BLEU scores.

## Key Results
- Consistent WER/CER improvements across all 150 languages with model scaling, following power-law relationships
- Disproportionate gains for low-resource languages (50 lowest-resource languages: WER decreases from 59 to 45 when scaling from 1B to 9B parameters)
- Emergent abilities in larger models: code-switching capability, orthographic understanding, and mondegreen generation
- 18B model outperforms Whisper Large v3 on FLEURS benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific WER/CER scales predictably as a power law function of model parameters ($N$).
- Mechanism: Under fixed data/compute, increasing model capacity ($N$) expands the representational space, enabling better fitting of complex multilingual acoustic-phonetic and orthographic mappings per language, yielding WER($N$) = $\beta N^\alpha$.
- Core assumption: Model parameters are equally distributed across encoder and decoder; training has converged sufficiently for valid comparison.
- Evidence anchors:
  - [abstract] "We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling."
  - [section 2.1] "Equation (2): WER(x) = $\beta_x x^{\alpha_x}$. We empirically show that this power law can also generalize to the multi-modal task of ASR."
  - [corpus] Corpus signals indicate related work on scaling laws for multilingual tasks (e.g., "Scaling Laws for Conditional Emergence..."), but do not provide direct mechanistic confirmation.
- Break condition: If test data distribution diverges significantly from training distribution, the power law may not hold; see limitations noted in Section 4.2 on data diversity.

### Mechanism 2
- Claim: Low-resource languages show disproportionate WER/CER improvements from model scaling relative to high-resource languages.
- Mechanism: Larger models reduce sample complexity through improved feature sharing and cross-lingual transfer, benefiting languages with sparse training data more than those already data-saturated.
- Core assumption: Shared representations exist across languages in the training set that can be exploited.
- Evidence anchors:
  - [abstract] "One of our key findings is that scaling enhances performance on low-resource languages/dialects..."
  - [section 4.1] "The average WER on the 50 lowest-resource languages ... decreases from 59 to 45 when model size increases from ... 1B to 9B."
  - [corpus] Related works (e.g., "Speech-to-Text Translation with Phoneme-Augmented CoT") suggest cross-lingual transfer benefits low-resource settings, aligning with this mechanism.
- Break condition: Transfer may be minimal if linguistic similarity between low-resource and high-resource languages in the corpus is low.

### Mechanism 3
- Claim: Orthographic and code-switching capabilities emerge in larger models (≥4B-9B parameters) but not smaller ones.
- Mechanism: Larger models possess sufficient capacity to learn complex, non-phonetic symbol mappings and intra-utterance language switches; these abilities manifest as qualitative changes in task behavior ("emergent abilities").
- Core assumption: Model scaling allows acquisition of higher-order linguistic abstractions not required for simple phonetic transcription.
- Evidence anchors:
  - [abstract] "...demonstrating the potential of large-scale speech models for advancing multilingual speech technologies... emergent abilities like in-context learning..."
  - [section 5.2] "Orthographic Understanding... The N-CER curve shows that scaling does not have a large impact on learning phonetics... the steeper CER curve... indicate that larger models exhibit significantly stronger orthographic capabilities." Also, code-switching gains.
  - [corpus] "Scaling Laws for Conditional Emergence of Multilingual Image Captioning..." explicitly addresses emergent abilities in multilingual contexts, supporting the plausibility of this mechanism.
- Break condition: Emergent abilities may not appear if the training data lacks the necessary diversity or quality (e.g., code-switching examples, orthographic variety).

## Foundational Learning

- Concept: Power Law Scaling
  - Why needed here: The paper's core claim is that WER/CER scales predictably as a power law of model size ($N$), data ($T$), or compute ($B$). Understanding this is essential for predicting performance and designing experiments.
  - Quick check question: Given a model with parameters $N_1$ and WER $L_1$, how would you approximate the WER $L_2$ for a model with parameters $N_2 > N_1$ assuming the same data and compute? (Assume you know $\alpha$).

- Concept: Cross-Lingual Transfer
  - Why needed here: The disproportionate gains in low-resource languages are attributed to this mechanism. It explains why a single multilingual model can outperform language-specific models with less data per language.
  - Quick check question: Why might a multilingual model trained on English and Spanish data potentially improve performance on Italian ASR, even with zero Italian training data?

- Concept: Emergent Abilities
  - Why needed here: The paper claims that abilities like orthographic understanding and code-switching "emerge" in large models. Distinguishing these from smooth performance improvements is key to understanding the value of scale.
  - Quick check question: Is the improvement in code-switching WER from a 1B to 9B model best described as a continuous scaling benefit or an emergent ability? What evidence from the text supports your choice?

## Architecture Onboarding

- Component map: Audio Input -> 80-dim log-Mel filterbanks (10ms frame shift) -> Conv Subsampling (4x) -> Transformer Encoder -> Transformer Decoder (with prompt) -> Text Output
- Critical path: Audio Input -> Mel Spectrogram -> Conv Subsampling -> Transformer Encoder -> Transformer Decoder (with prompt) -> Text Output. The scaling experiments primarily vary the *Transformer Encoder/Decoder* dimensions (Layers, Hidden/FFN Size).
- Design tradeoffs:
  - Model Scale vs. Data Diversity: Section 4.2 shows data scaling saturates without diversity (YODAS helps). Investing in larger models ($N$) vs. more diverse data ($T$) involves cost/benefit analysis.
  - Training Stability vs. SOTA Performance: Authors used identical hyperparameters across scales (Section 3.2) prioritizing stability over optimal per-model tuning. This may understate peak performance for some sizes.
  - Compute at Train vs. Test Time: Section 5.1 shows larger models outperform even with equivalent test-time compute (using beam search on smaller models).
- Failure signatures:
  - Catastrophic Task Failure: Very small models (e.g., 0.25B) may produce blank predictions or fail completely on complex tasks like ST (Section 4.1) or contextual biasing (Appendix F).
  - Zero-Shot Failure on Unseen Languages: Performance may be random or poor without in-context examples, especially for smaller models (Section 5.3).
  - Orthographic Hallucination: Mondegreen experiments (Section 5.2) show larger models may produce semantically coherent but incorrect transcriptions.
- First 3 experiments:
  1. Replicate the Power Law on a Subset: Train a 0.25B, 1B, and 4B model on a fixed 45K hour subset of the data. Plot WER vs. $N$ for 5 diverse languages to validate the scaling law slope ($\alpha$) reported in the paper.
  2. Ablate Data Diversity: Train two 1B models: one on 90K hours from the same distribution, and one on 90K hours combining the original set with diverse external data (e.g., YODAS). Compare per-language WER changes to verify the diversity benefit seen in Figure 5.
  3. Probe Emergent Code-Switching: Evaluate the 0.5B, 4B, and 18B models on a held-out code-switching test set. Analyze if performance jumps qualitatively at a specific scale, and characterize error types (e.g., monolingual fallback vs. language ID errors).

## Open Questions the Paper Calls Out

- Question: How do the derived neural scaling laws for pre-training transfer to adaptation phases, such as parameter-efficient fine-tuning (PEFT) or full fine-tuning?
- Question: What is the optimal trade-off between model size and training data volume (Chinchilla-style optimality) for multilingual speech recognition, and does this trade-off differ significantly between low-resource and high-resource languages?
- Question: Does increasing model capacity resolve the performance degradation observed in speech translation tasks (e.g., English-to-German) when scaling training data, or is this phenomenon driven by data distribution interference?

## Limitations

- Scaling law analysis uses fixed, uniform hyperparameters prioritizing stability over optimal per-model tuning, potentially understating peak performance for very large models.
- Emergent abilities analysis relies on qualitative interpretation without rigorous threshold definitions for what constitutes "emergent" versus continuous improvement.
- YODAS 180K-hour subset is "to be made publicly available," creating potential access barriers for exact reproduction.

## Confidence

**High Confidence**: Power-law scaling relationship between model parameters and WER/CER across languages (Equations 2, 3; Figure 2); disproportionate benefits for low-resource languages (Section 4.1).

**Medium Confidence**: Emergent abilities claims (code-switching, orthographic understanding, mondegreen generation) lack rigorous threshold definitions; 18B model SOTA performance uses different evaluation protocols from baseline comparisons.

**Low Confidence**: In-context learning capabilities demonstrated on limited examples without systematic evaluation; zero-shot ST performance based on few-shot evaluations that may not generalize.

## Next Checks

1. **Scaling Law Reproducibility Test**: Train 0.25B, 1B, and 4B models on a fixed 45K-hour subset of the data. Plot WER vs. model parameters for 5 diverse languages to verify the power-law slopes (α values) reported in the paper.

2. **Data Diversity Impact Validation**: Train two 1B-parameter models—one on 90K hours from the base distribution and another combining 90K hours with diverse external data (e.g., YODAS). Compare per-language WER changes to empirically verify the diversity benefits shown in Figure 5.

3. **Emergent Ability Threshold Analysis**: Evaluate 0.5B, 4B, and 18B models on a held-out code-switching test set. Systematically analyze whether performance jumps occur at specific scales and characterize error types (monolingual fallback vs. language ID errors) to rigorously define what constitutes "emergent" versus continuous improvement.