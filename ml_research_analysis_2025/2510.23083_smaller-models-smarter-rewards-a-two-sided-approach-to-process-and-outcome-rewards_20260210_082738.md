---
ver: rpa2
title: 'Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome
  Rewards'
arxiv_id: '2510.23083'
source_url: https://arxiv.org/abs/2510.23083
tags:
- reward
- code
- dataset
- problem
- rollouts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether small-scale decoder-only transformer
  models can serve as effective reward models for code generation, blending process
  and outcome reward signals. The authors fine-tune Phi-4 family models by replacing
  the classifier layer with a regression head to estimate success probabilities for
  intermediate code generation steps.
---

# Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards

## Quick Facts
- **arXiv ID:** 2510.23083
- **Source URL:** https://arxiv.org/abs/2510.23083
- **Reference count:** 22
- **Primary result:** Small decoder-only transformer models can effectively serve as reward models for code generation, with 14B Phi-4 outperforming 3.8B variant by 20%+ in correct rollout selection

## Executive Summary
This paper demonstrates that small-scale decoder-only transformer models, specifically the Phi-4 family, can function as effective reward models for code generation tasks. The authors fine-tune these models by replacing the classifier layer with a regression head to estimate success probabilities for both complete code rollouts (outcome rewards) and intermediate generation steps (process rewards). Using the APPS coding challenge dataset, they show that the 14B model significantly outperforms the 3.8B variant, achieving over 20% improvement in selecting correct rollouts compared to baseline accuracy.

The research reveals a critical insight: while small models can effectively serve as lightweight code evaluation critics, their performance is heavily dependent on model size and the extent of code generation completion. Process reward modeling shows meaningful results only when at least 50% of tokens are generated, suggesting inherent limitations for early-stage code evaluation. The binary pass/fail labeling approach may miss nuanced quality distinctions, though the overall framework provides a promising foundation for efficient reward modeling in code generation.

## Method Summary
The authors fine-tune Phi-4 decoder-only transformer models by replacing the original classifier layer with a regression head to predict success probabilities. They use the APPS coding challenge dataset, generating multiple code rollouts per problem and labeling them with correctness outcomes. The models are trained to estimate the likelihood of success at each token position during generation. The approach creates two types of reward models: outcome reward models that judge complete rollouts and process reward models that evaluate intermediate steps. The 14B model is compared against the 3.8B variant to assess the impact of model size on reward modeling effectiveness.

## Key Results
- 14B Phi-4 model outperforms 3.8B variant by over 20% in selecting correct rollouts
- Process reward models show meaningful performance only when >50% of tokens are generated
- Small models can effectively serve as lightweight code evaluation critics
- Model size is the primary performance limitation for reward modeling

## Why This Works (Mechanism)
The effectiveness stems from leveraging the decoder-only architecture's inherent understanding of code generation context. By fine-tuning the classification layer for regression tasks, the models learn to estimate success probabilities based on the generated code prefix. The approach benefits from the model's exposure to correct/incorrect code patterns during training, allowing it to distinguish between successful and failed generation trajectories. The process reward capability emerges from the model's ability to evaluate intermediate states based on learned code quality patterns.

## Foundational Learning
- **Decoder-only transformer architecture**: Essential for understanding how these models process sequential code generation tasks
- **Reward modeling fundamentals**: Why needed - to evaluate code generation quality; Quick check - can the model distinguish correct from incorrect code
- **Regression head adaptation**: Why needed - to convert classification models to probability estimators; Quick check - does the regression output correlate with actual success rates
- **Process vs outcome rewards**: Why needed - to evaluate both complete and intermediate generation steps; Quick check - can the model judge partial code sequences
- **Code generation evaluation metrics**: Why needed - to measure reward model effectiveness; Quick check - does improvement in reward accuracy translate to better code selection

## Architecture Onboarding
- **Component map**: Code generation model -> Multiple rollouts -> Reward model prediction -> Success probability estimation
- **Critical path**: Training data (APPS) -> Rollout generation -> Labeling -> Fine-tuning -> Reward model deployment
- **Design tradeoffs**: Model size vs performance (14B outperforms 3.8B) vs computational efficiency
- **Failure signatures**: Binary labeling misses nuanced quality distinctions; process rewards ineffective below 50% completion
- **3 first experiments**: 1) Test reward accuracy across different completion percentages, 2) Evaluate multi-class labeling sensitivity, 3) Apply approach to non-code domains

## Open Questions the Paper Calls Out
None

## Limitations
- Binary pass/fail labeling may miss nuanced code quality distinctions
- Process reward modeling only effective for sequences with >50% completion
- Performance heavily dependent on model size, limiting applicability to smaller models
- May not generalize beyond code generation to other domains

## Confidence
- **High**: 14B model's 20%+ improvement in correct rollout selection over baseline
- **Medium**: Process reward model effectiveness requiring >50% token completion
- **Low**: Generalizability to other domains or smaller models beyond code generation

## Next Checks
1. Test reward models on code sequences with varying completion rates below 50% to establish precise thresholds where process reward modeling becomes unreliable.

2. Evaluate model performance using multi-class correctness labels (partial credit, compilation errors, runtime errors) rather than binary pass/fail.

3. Apply the same reward modeling approach to non-code domains (mathematical reasoning or creative writing) to determine if size-dependent performance patterns generalize.