---
ver: rpa2
title: Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings
arxiv_id: '2504.14766'
source_url: https://arxiv.org/abs/2504.14766
tags:
- figure
- test
- accuracy
- dimensions
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for identifying embedding dimensions
  that encode distinct linguistic properties (LPs) in models like BERT, GPT-2, and
  MPNet. The authors create the LDSP-10 dataset of 1000 minimal sentence pairs per
  LP (negation, polarity, tense, etc.) and apply statistical tests, mutual information,
  and recursive feature elimination to rank embedding dimensions by their relevance
  to each LP.
---

# Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings

## Quick Facts
- **arXiv ID**: 2504.14766
- **Source URL**: https://arxiv.org/abs/2504.14766
- **Reference count**: 40
- **Primary result**: Introduces EDI framework to identify embedding dimensions encoding specific linguistic properties, showing some properties (negation, polarity) are robustly localized in few dimensions while others (synonymy) are diffuse.

## Executive Summary
This paper presents a framework for identifying which dimensions in transformer model embeddings encode specific linguistic properties. The authors create the LDSP-10 dataset of 1000 minimal sentence pairs per property (negation, polarity, tense, etc.) and apply statistical tests, mutual information, and recursive feature elimination to rank embedding dimensions by their relevance to each property. They propose the Embedding Dimension Importance (EDI) score, aggregating these analyses to quantify dimension importance. Results show that certain properties like negation and polarity are robustly encoded in specific dimensions, while others like synonymy exhibit more diffuse patterns. The EDI framework enables accurate classification of linguistic properties using only a small subset of dimensions, offering a path toward more interpretable and debuggable language models.

## Method Summary
The method extracts final-layer embeddings from BERT, GPT-2, and MPNet using mean-pooling. For each linguistic property in the LDSP-10 dataset, three analysis methods are applied per dimension: Wilcoxon signed-rank test (detecting distributional differences between paired sentence embeddings), mutual information (quantifying information-theoretic relevance), and recursive feature elimination (identifying predictive utility). These scores are aggregated into the EDI score using weighted combination (Wilcoxon: 0.6, MI: 0.2, RFE: 0.2) after min-max scaling. The framework is evaluated by comparing classification accuracy using high-EDI vs low-EDI dimension subsets and cross-property evaluation to test dimension specificity.

## Key Results
- Negation and polarity show EDI scores >0.99 on specific dimensions (e.g., dimension 544 for negation), enabling classification with just 4-25 dimensions
- Synonymy and control properties show uniformly low EDI scores across all dimensions, requiring hundreds of dimensions for accurate classification
- Cross-property evaluation shows related properties (negation ↔ polarity) share dimensional patterns with ~0.90 accuracy
- The EDI framework achieves 0.863 accuracy for classifying linguistic properties using only top-ranking dimensions

## Why This Works (Mechanism)

### Mechanism 1: Minimal Pair Contrast Detection
The Wilcoxon signed-rank test detects distributional differences between paired sentence embeddings across each dimension. When a linguistic property consistently alters a dimension's values, the test yields low p-values, indicating the dimension encodes that property. This works because systematic linguistic variations produce measurable, non-random shifts in specific embedding dimensions that survive mean-pooling and are detectable via non-parametric statistics.

### Mechanism 2: Multi-Method Agreement via EDI Aggregation
The EDI score combines three complementary methods - Wilcoxon p-values (0.6 weight), mutual information (0.2), and RFE coefficients (0.2) - after min-max scaling. This aggregation captures statistical significance, information-theoretic relevance, and predictive utility in a single metric. Dimensions scoring highly across all three methods genuinely encode the linguistic property rather than artifacts of any single analytical approach.

### Mechanism 3: Property-Specific Dimension Localization
Different linguistic properties concentrate in characteristic dimension subsets with varying degrees of focal vs. diffuse encoding. Binary or clearly structured linguistic distinctions (negation, polarity) produce high EDI scores in few dimensions, enabling accurate classification with 4-25 dimensions. Semantic similarity relationships (synonymy) show low, diffuse EDI patterns requiring hundreds of dimensions, reflecting distributed representations.

## Foundational Learning

- **Concept: Wilcoxon Signed-Rank Test**
  - **Why needed here:** The primary statistical tool for detecting dimension-wise differences between paired sentence embeddings without assuming Gaussian distributions
  - **Quick check question:** If all dimensions show p-values near 0.5, what does this suggest about the linguistic property being tested?

- **Concept: Mutual Information for Discrete Variables**
  - **Why needed here:** Quantifies how much knowing a dimension's value reduces uncertainty about which sentence in a pair it came from, capturing non-linear relationships
  - **Quick check question:** Why use quantile-based binning with 10 bins rather than treating continuous embedding values directly?

- **Concept: Recursive Feature Elimination (RFE)**
  - **Why needed here:** Identifies dimensions with highest predictive utility by iteratively removing low-importance features, complementing statistical tests with task-relevant selection
  - **Quick check question:** Why might RFE select different dimensions than the Wilcoxon test for the same linguistic property?

## Architecture Onboarding

- **Component map:** LDSP-10 Dataset Generator (Gemini API with few-shot prompts) → Embedding Extractor (BERT/GPT-2/MPNet final layer + mean-pooling) → Three-Method Analyzer (Wilcoxon, MI, RFE) → EDI Calculator (weighted aggregation) → Evaluation Suite (baseline, high-EDI, low-EDI, cross-property)

- **Critical path:** 1) Validate LDSP quality manually (99%+ adherence rate), 2) Extract embeddings and compute all three analysis scores, 3) Aggregate into EDI scores and verify control/synonym show near-random patterns, 4) Run Evaluation 1 (high-EDI classification) to confirm dimension sufficiency

- **Design tradeoffs:** Wilcoxon weighted at 0.6 because it proved most predictive in testing, but this may not generalize to all LPs; LDSP generation via LLM introduces potential syntactic biases; mean-pooling loses token-level information but enables sentence-level analysis

- **Failure signatures:** Control/synonym showing high EDI scores → dataset ordering invariants violated; cross-property accuracy exceeding target-property accuracy → dimensions encode general linguistic features; low-EDI classifier outperforming high-EDI → EDI weighting scheme inappropriate

- **First 3 experiments:** 1) Replicate negation analysis on BERT to verify Figure 3 distributional shift and EDI >0.99 on dimension 544, 2) Test control LDSPs to confirm all dimensions show uniform Wilcoxon p-values and maximum EDI <0.4, 3) Run cross-property evaluation (negation dimensions on polarity classification) to verify semantic relatedness yields ~0.90 accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How do EDI scores and the localization of linguistic properties evolve across different layers of transformer models? The study currently relies exclusively on the final hidden layer outputs, but future work may analyze EDI scores across representations at different layers to understand how information about specific LPs propagate through the network.

### Open Question 2
Can intervention methods like mean-ablation on high-EDI dimensions effectively remove specific social biases (e.g., gender) without degrading downstream task performance? While the authors hypothesize that their method can isolate dimensions responsible for encoding gender, more experiments and analysis are needed to validate this causal utility.

### Open Question 3
Do larger, state-of-the-art language models exhibit similar dimension-wise localization of linguistic properties as observed in BERT and GPT-2? The authors acknowledge they limit experiments to small open-source models and suggest that analysis on larger, newer, and more widely-used models could solidify generalizability claims.

## Limitations

- The framework's reliance on mean-pooling discards token-level information that may be crucial for understanding how linguistic properties are encoded
- The EDI aggregation assumes equal validity across all linguistic properties, but varying performance patterns suggest property-specific tuning may be needed
- LDSP-10 dataset generation via LLM introduces potential syntactic and semantic biases that manual validation cannot fully eliminate at scale

## Confidence

- **High Confidence**: EDI framework accurately identifies dimensions encoding binary linguistic properties (negation, polarity) with >0.99 scores on specific dimensions and enables classification with few dimensions
- **Medium Confidence**: Multi-method aggregation approach works for structured properties but requires property-specific tuning for semantic relationships
- **Low Confidence**: Generalizability to properties beyond LDSP-10 and to models outside the three tested architectures

## Next Checks

1. **Cross-Model Consistency Test**: Apply the framework to RoBERTa and DeBERTa to verify that negation/polarity dimensions identified in BERT/GPT-2/MPNet show similar localization patterns across architectures

2. **Token-Level Validation**: Repeat the analysis using token-level embeddings (without mean-pooling) to determine if dimension importance patterns change significantly for properties showing diffuse encoding

3. **Property Generalization**: Create minimal pairs for additional linguistic properties (e.g., passive voice, aspect, evidentiality) to test whether the framework generalizes beyond the initial 10 properties and identify properties that consistently resist dimension-wise localization