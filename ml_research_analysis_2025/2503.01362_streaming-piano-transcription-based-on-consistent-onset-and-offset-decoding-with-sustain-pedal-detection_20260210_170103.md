---
ver: rpa2
title: Streaming Piano Transcription Based on Consistent Onset and Offset Decoding
  with Sustain Pedal Detection
arxiv_id: '2503.01362'
source_url: https://arxiv.org/abs/2503.01362
tags:
- transcription
- onset
- offset
- decoder
- piano
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a streaming piano transcription method that
  aims to sequentially convert a music signal into a sequence of note onset and offset
  events. The method addresses the performance limitations of existing sequence-to-sequence
  models, which often struggle to capture the correspondence between onset and offset
  events of the same note.
---

# Streaming Piano Transcription Based on Consistent Onset and Offset Decoding with Sustain Pedal Detection

## Quick Facts
- **arXiv ID:** 2503.01362
- **Source URL:** https://arxiv.org/abs/2503.01362
- **Authors:** Weixing Wei, Jiahao Zhao, Yulun Wu, Kazuyoshi Yoshii
- **Reference count:** 0
- **Primary result:** Streaming method achieves 89.44% note-level F1 with duration, competitive with offline methods while reducing computational cost

## Executive Summary
This paper presents a streaming piano transcription method that sequentially converts music signals into note onset and offset events while detecting sustain pedal state. The approach addresses limitations of single-decoder sequence-to-sequence models by employing separate autoregressive Transformer decoders for onset and offset detection, enforcing consistency between corresponding events. Experiments on the MAESTRO dataset demonstrate that this streaming method achieves comparable or superior performance to state-of-the-art offline methods with significantly reduced computational cost and 380ms system latency.

## Method Summary
The method uses a harmonic dilated CNN encoder to aggregate local acoustic features from CQT spectrograms within a 39-frame receptive field. Two separate T5-style Transformer decoders process the encoded features: one autoregressively predicts variable-length onset sequences, while the other predicts offset events only for currently active pitches. The offset decoder also performs multitask learning to detect sustain pedal state. An active onset set maintains state across frames, ensuring onset-offset correspondence. The model is trained on MAESTRO V3.0.0 with cross-entropy loss and evaluated using frame-level and note-level F1 metrics.

## Key Results
- Achieves 91.75% frame-level F1 and 89.44% note-level F1 with duration on MAESTRO test set
- Competitive with state-of-the-art offline methods (hFT-Transformer: 90.53% note F1 with duration) while streaming
- Single-decoder ablation drops note F1 with duration from 89.44% to 86.51% (2.93pp degradation)
- Sustain pedal detection ablation reduces note F1 with duration from 89.44% to 86.51%

## Why This Works (Mechanism)

### Mechanism 1: Separate Decoders for Onset and Offset Detection
- **Claim:** Specialized decoders improve accuracy by focusing on distinct acoustic features
- **Mechanism:** Onset decoder attends to transient attack characteristics while offset decoder focuses on decay and sustain patterns
- **Core assumption:** Time-frequency features optimal for onset detection differ meaningfully from those for offset detection
- **Evidence:** Single-decoder model degrades from 89.44% to 86.51% note F1 with duration
- **Break condition:** If onset and offset features share similar representations, separation becomes unnecessary

### Mechanism 2: Active Onset Set Constraint
- **Claim:** Constraining offset decoder to active notes enforces onset-offset correspondence
- **Mechanism:** Maintains explicit state about active pitches to prevent orphaned offsets and missing offsets
- **Core assumption:** Explicit state provides stronger inductive bias than implicit learning
- **Evidence:** Proposed method achieves 89.44% F1 vs. single-decoder's 86.51%
- **Break condition:** Dense polyphony or timing jitter may propagate errors through the constraint

### Mechanism 3: Sustain Pedal Detection as Multitask Learning
- **Claim:** Pedal detection improves offset timing accuracy
- **Mechanism:** Joint training of pedal state with note offsets provides indirect supervision for acoustic note endings
- **Core assumption:** Pedal timing is causally related to acoustic note offset times
- **Evidence:** Pedal ablation drops note F1 with duration from 89.44% to 86.51%
- **Break condition:** Variable pedal usage patterns may limit generalization

## Foundational Learning

- **Concept: Autoregressive Transformer Decoders (T5-style)**
  - **Why needed:** Generates variable-length onset sequences per frame
  - **Quick check:** Why is causal masking necessary for autoregressive generation but not bidirectional encoders?

- **Concept: Convolutional Encoders with Local Receptive Fields**
  - **Why needed:** Aggregates features within fixed window for streaming capability
  - **Quick check:** How does fixed receptive field differ from full self-attention in computational complexity?

- **Concept: Cross-Attention in Encoder-Decoder Architectures**
  - **Why needed:** Enables predictions to condition on acoustic features while maintaining causality
  - **Quick check:** What information does cross-attention provide that self-attention alone cannot?

## Architecture Onboarding

- **Component map:** CQT spectrogram (352 bins, 20ms hop) -> CNN encoder (receptive field M=39 frames) -> Onset decoder (6-layer Transformer) + Offset decoder (6-layer Transformer) -> Onset/offset sequences + pedal state

- **Critical path:**
  1. At frame t, encoder processes window X[t-M/2 : t+M/2] → H_t
  2. Offset decoder predicts offsets for all pitches in active set A, removes ended notes
  3. Onset decoder autoregressively generates new onsets until <EOS>
  4. New onsets added to A
  5. Latency = 19 future frames (380ms) + computation time

- **Design tradeoffs:**
  - Receptive field (M=39) vs. latency: Larger M provides more context but increases latency
  - Separate decoders vs. parameter efficiency: Two decoders outperform single-decoder but use more memory
  - Streaming vs. offline accuracy: 89.44% note F1 vs. hFT-Transformer's 90.53% (1.09pp gap)

- **Failure signatures:**
  - Orphaned offsets: Check state initialization and update logic if offset decoder has no valid targets
  - Dense polyphony dropouts: Max decoder sequence length (64) may truncate outputs in complex passages
  - Pedal false positives: Over-reliance on pedal may extend notes incorrectly if detection is noisy

- **First 3 experiments:**
  1. Reproduce baseline comparison on MAESTRO V3.0.0 split to validate 91.75% frame F1 and 89.44% note F1 with duration
  2. Ablate decoder separation to verify ~3pp drop in note F1 with duration
  3. Vary receptive field M ∈ {19, 39, 59} to measure latency-accuracy tradeoff

## Open Questions the Paper Calls Out

1. **Latency vs. Performance Trade-off:** The authors plan to investigate how reducing latency below 380ms impacts transcription performance, as current architecture uses fixed receptive field and hop length without evaluating this relationship.

2. **Adaptive Time-Step Decoding:** The paper questions whether frame-by-frame decoding is necessary for all real-time scenarios, suggesting potential computational efficiency improvements through adaptive or increased time steps.

3. **Generalization to "In-the-Wild" Audio:** Since evaluation relies exclusively on high-quality MAESTRO recordings, the model's performance on noisy or amateur recordings with different piano timbres remains unexplored.

## Limitations

- Fixed receptive field of 39 frames may create coverage gaps for fast passages or temporal blurring in dense textures
- Greedy decoding approach may not find globally optimal onset-offset pairings in polyphonic passages
- Lack of ablation studies examining the interaction between sustain pedal detection and offset accuracy
- Computational cost comparison with offline methods is not quantified despite claims of "significantly reduced" cost

## Confidence

**High Confidence:** Separate decoders improve note-level transcription accuracy (89.44% vs. 86.51% single-decoder), supported by direct ablation and logical architectural constraints.

**Medium Confidence:** Streaming method achieves "comparable or superior performance" to offline methods with reduced computational cost, though the 1.09pp gap to hFT-Transformer may be meaningful and computational cost is not quantified.

**Low Confidence:** Sustain pedal detection's causal benefit for offset timing accuracy, as performance drop when removed may reflect correlation rather than true causal learning.

## Next Checks

1. Reproduce baseline comparison by training on MAESTRO V3.0.0 split to verify 91.75% frame F1 and 89.44% note F1 with duration

2. Vary receptive field M ∈ {19, 39, 59} to quantify latency-accuracy tradeoff and verify linear scaling

3. Cross-validate on non-MAESTRO datasets (MAPS, MusicNet) to assess generalization beyond training domain