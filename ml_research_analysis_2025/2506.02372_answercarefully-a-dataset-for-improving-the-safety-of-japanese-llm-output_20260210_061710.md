---
ver: rpa2
title: 'AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output'
arxiv_id: '2506.02372'
source_url: https://arxiv.org/abs/2506.02372
tags:
- safety
- evaluation
- japanese
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnswerCarefully is a dataset of 1,800 Japanese question-answer
  pairs designed to improve the safety of LLM outputs. It covers a wide range of harm
  categories, including discrimination, adult content, mental health, privacy, and
  misinformation, based on the English Do-Not-Answer taxonomy but manually created
  to reflect Japanese socio-cultural context.
---

# AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output

## Quick Facts
- arXiv ID: 2506.02372
- Source URL: https://arxiv.org/abs/2506.02372
- Authors: Hisami Suzuki; Satoru Katsumata; Takashi Kodama; Tetsuro Takahashi; Kouta Nakayama; Satoshi Sekine
- Reference count: 26
- Key outcome: AnswerCarefully is a dataset of 1,800 Japanese question-answer pairs designed to improve the safety of LLM outputs. It covers a wide range of harm categories, including discrimination, adult content, mental health, privacy, and misinformation, based on the English Do-Not-Answer taxonomy but manually created to reflect Japanese socio-cultural context. The dataset includes reference answers to support fine-tuning and safety evaluation. Experiments show that fine-tuning a Japanese LLM with AnswerCarefully significantly reduces harmful outputs without compromising general response quality. The dataset also serves as a benchmark for evaluating 12 Japanese LLMs, revealing large performance differences. Recent updates include English translations and annotations to support cross-lingual dataset creation. The dataset addresses the need for culturally relevant safety data in Japanese and provides a resource for both instruction tuning and automatic evaluation of LLM safety.

## Executive Summary
AnswerCarefully is a manually curated dataset of 1,800 Japanese question-answer pairs designed to improve the safety of LLM outputs. It adapts the English Do-Not-Answer taxonomy to the Japanese socio-cultural context, covering harm categories such as discrimination, adult content, mental health, privacy, and misinformation. The dataset includes reference answers to support fine-tuning and evaluation, and is designed to be used with Supervised Fine-Tuning (SFT) to reduce harmful outputs without compromising general response quality. Experiments show significant safety improvements while maintaining utility, validated through LLM-as-a-judge evaluation with high correlation to human judgments.

## Method Summary
The paper introduces AnswerCarefully, a manually curated dataset of 1,800 Japanese question-answer pairs covering various harm categories. The dataset is used to fine-tune Japanese LLMs via Supervised Fine-Tuning (SFT), with the safety data oversampled (16x) and mixed with general utility data. Evaluation uses a dual metric system (Violation Rate and Acceptable Response Rate) assessed by GPT-4 as an LLM-as-a-judge, with reference answers provided to improve reliability. The method aims to reduce harmful outputs without sacrificing general response quality.

## Key Results
- Fine-tuning a Japanese LLM with AnswerCarefully significantly reduces harmful outputs without compromising general response quality.
- The dataset serves as a benchmark for evaluating 12 Japanese LLMs, revealing large performance differences.
- Recent updates include English translations and annotations to support cross-lingual dataset creation.
- LLM-as-a-judge evaluation with reference answers achieves near-human inter-annotator agreement (~0.67 vs ~0.68 human-human).

## Why This Works (Mechanism)

### Mechanism 1: Cultural Grounding via Manual Curation
- **Claim:** Manually created datasets capture implicit cultural risks that automated or translated datasets miss.
- **Mechanism:** Human annotators, operating within a specific socio-cultural context (Japan), naturally produce prompts and reference answers that reflect local taboos, biases, and nuances (e.g., specific social hierarchies or regional misunderstandings) that generic models might overlook. The paper notes that 27% of collected data contained culture-specific content despite no explicit instruction to include it.
- **Core assumption:** Annotators possess sufficient cultural intuition to identify context-specific risks without rigid templates.
- **Evidence anchors:**
  - [Section 2] "27% of the collected data ended up including some culture-specific content, suggesting that simply translating English safety datasets will not suffice."
  - [Abstract] "Data samples are original in that they are manually created to reflect the socio-cultural context."
  - [Corpus] Related work "CultureGuard" supports the general trend toward culturally-aware safety data, though specific evidence for this dataset is internal to the paper.
- **Break condition:** If annotators share a narrow worldview or lack diversity, the dataset may fail to cover the full spectrum of cultural risks or introduce new biases.

### Mechanism 2: Safety Alignment via Reference-Guided SFT
- **Claim:** Supervised Fine-Tuning (SFT) with high-quality reference answers reduces harmful output rates while maintaining general utility.
- **Mechanism:** By providing explicit "safe" examples (reference answers) for sensitive prompts, the model learns to generate "careful" responses rather than refusing outright or hallucinating. The paper suggests that oversampling this safety data (up to 16x) reinforces these patterns without catastrophic forgetting of general tasks.
- **Core assumption:** The base model has sufficient capacity to incorporate safety constraints without significant degradation in performance on unrelated tasks (e.g., MT-Bench).
- **Evidence anchors:**
  - [Section 4, Table 3] Violation rate dropped significantly (e.g., from 0.445 to 0.153 with ACv2 x16) while MT-Bench scores (3.64 to 3.81) improved or remained stable.
  - [Abstract] "Using this dataset for instruction fine-tuning significantly reduces harmful output without compromising general response quality."
- **Break condition:** If the mixing ratio of safety data to utility data is too high, overfitting to safety responses may occur; if too low, safety learning may be negligible.

### Mechanism 3: Evaluation Stabilization via Reference Grounding
- **Claim:** Providing reference answers to an LLM acting as a judge improves evaluation reliability to near-human inter-annotator levels.
- **Mechanism:** "Careful" answers exist in a gray area between refusal and harmfulness. A judge LLM without a reference may oscillate in its scoring. Providing a reference answer anchors the judge, reducing variance and aligning it closer to human consensus.
- **Core assumption:** The judge LLM (e.g., GPT-4) is capable of semantic similarity and safety comparison tasks.
- **Evidence anchors:**
  - [Section 3.2, Table 2] Correlation with human evaluators rose from ~0.50 (no reference) to ~0.67 (with reference), comparable to human-human correlation (~0.68).
- **Break condition:** If the reference answer itself is of low quality or controversial, it may misguide the judge, leading to systematic evaluation errors.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) Mixing Ratios**
  - **Why needed here:** The paper relies on mixing safety data (AnswerCarefully) with general utility data (Dolly, OpenAssistant) and oversampling (16x) to achieve results. Understanding how to balance these datasets is critical for reproduction.
  - **Quick check question:** How does increasing the duplication count (e.g., 1x vs 16x) of safety data affect the violation rate and general MT-Bench score?

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The paper validates its results using GPT-4 as a judge, citing correlation improvements. Understanding the limitations of this evaluation method is necessary for interpreting the claims.
  - **Quick check question:** Why does providing a reference answer to the judge LLM improve correlation with human evaluators?

- **Concept: Safety vs. Utility Trade-off**
  - **Why needed here:** The core value proposition is reducing harm *without* compromising utility (refusal fatigue).
  - **Quick check question:** What are the two metrics used to measure this trade-off in the paper? (Hint: One measures harm, the other measures helpfulness).

## Architecture Onboarding

- **Component map:** Data Source (Manually curated questions + Reference answers) -> Training (SFT loop combining AC with general datasets) -> Evaluation (Dual metric system using LLM-as-a-judge with reference answers).

- **Critical path:** Data Curation (cultural nuance) -> Reference Answer Creation (alignment target) -> SFT Training (mixing/oversampling) -> Evaluation (Judge + Reference).

- **Design tradeoffs:**
  - **Manual vs. Synthetic:** Manual creation is slower/costlier but yields higher naturalness and cultural specificity (vs. DNA's GPT-4 generation).
  - **Refusal vs. Careful Answer:** The dataset focuses on "answering carefully" rather than refusing, which preserves utility but requires more nuanced reference answers.

- **Failure signatures:**
  - **High Refusal Rate:** Model becomes overly cautious, refusing safe questions (not observed in paper but a risk).
  - **Catastrophic Forgetting:** General utility scores (MT-Bench) drop significantly (monitored in Table 3).
  - **Low Correlation:** Judge evaluation does not match human intuition (solved by adding references).

- **First 3 experiments:**
  1. **Baseline Safety:** Evaluate a base Japanese LLM (e.g., LLM-jp-13B-v2.0) on the AC test set to establish a violation rate baseline without safety tuning.
  2. **SFT Ablation:** Fine-tune the base model with AC data mixed 1x vs 16x against general utility data to measure the impact of safety data density on violation rates and MT-Bench scores.
  3. **Judge Correlation:** Compare GPT-4 evaluations of safety outputs with and without access to reference answers to validate the "LLM-as-a-judge" improvement claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the addition of "borderline data" (questions similar to unsafe ones but answerable) impact the trade-off between safety violation rates and over-refusal in model alignment?
- Basis in paper: [explicit] The conclusion states plans for future updates including "the addition of borderline data" to address evolving model capabilities.
- Why unresolved: Current dataset versions focus on clear-cut safety scenarios; adding ambiguous borderline cases may affect the "Acceptable Response Rate" metric by triggering unnecessary refusals.
- What evidence would resolve it: A comparative evaluation of models fine-tuned on the current dataset versus the updated version, specifically measuring over-refusal rates on benign but sensitive queries.

### Open Question 2
- Question: Can the "multi-language-multi-culture" (mlmc) annotation schema effectively facilitate the creation of culturally adapted safety datasets in regions outside Japan?
- Basis in paper: [explicit] The paper describes adding mlmc annotations (translations and adaptation-tags) aimed at "facilitating the derivation of similar datasets in different languages and regions."
- Why unresolved: While the authors provide the scaffolding (tags and translations), they do not demonstrate successful external adoption or validate that simple entity swapping suffices for cultural adaptation.
- What evidence would resolve it: Successful construction and validation of a new safety dataset for a different culture (e.g., French or Indonesian) utilizing the provided mlmc annotations as the primary data source.

### Open Question 3
- Question: To what extent does safety fine-tuning on AnswerCarefully generalize to protecting against adversarial "Jailbreak-type" prompts?
- Basis in paper: [inferred] Section 2 notes the dataset currently focuses on "straightforward" questions and explicitly excludes "Jailbreak-type questions that are deliberately devised to circumvent the safety measures."
- Why unresolved: The paper shows improved safety on the AC benchmark, but the exclusion of adversarial examples means robustness against malicious attacks remains an untested aspect of the methodology.
- What evidence would resolve it: Evaluation of AC-fine-tuned models against established adversarial attack benchmarks to measure violation rates under malicious prompting conditions.

## Limitations
- **Generalization Gap:** The dataset is manually curated for Japanese cultural context, but safety improvements may not generalize beyond the 27% of culture-specific examples.
- **Evaluation Reliability:** Reliance on GPT-4 as an LLM-as-a-judge may not be consistently available or aligned across deployments.
- **Reproducibility Constraints:** Key SFT hyperparameters (learning rate, batch size, epochs) and the exact judge prompt are not specified, limiting faithful reproduction.

## Confidence
- **High Confidence:** The dataset creation methodology (manual curation, cultural grounding) and the general trend of safety improvement via SFT are well-supported by internal results and related work.
- **Medium Confidence:** The specific claim that "no compromise to general response quality" is supported by MT-Bench scores but lacks statistical significance testing or ablation on refusal rates.
- **Low Confidence:** The generalizability of safety improvements to unseen cultural contexts or prompts outside the dataset is not empirically validated.

## Next Checks
1. **Ablation on Cultural Specificity:** Re-evaluate the tuned model on a held-out subset of prompts identified as *not* culture-specific to test if safety improvements are universal or context-bound.
2. **Judge Model Robustness:** Replace GPT-4 with an open-source judge (e.g., Llama 3 70B) to assess if the reference-answer improvement in correlation holds across judge models.
3. **Refusal vs. Careful Response Analysis:** Analyze the modelâ€™s response distribution (refuse vs. careful answer vs. harmful) on safety prompts to ensure utility is preserved without defaulting to refusal.