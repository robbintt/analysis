---
ver: rpa2
title: Efficient Personalization of Quantized Diffusion Model without Backpropagation
arxiv_id: '2503.14868'
source_url: https://arxiv.org/abs/2503.14868
tags:
- token
- zoodip
- diffusion
- memory
- unique
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ZOODiP, a method for efficient personalization
  of quantized diffusion models without backpropagation. The key idea is to use zeroth-order
  optimization on a quantized model with a subspace gradient projection and partial
  uniform timestep sampling.
---

# Efficient Personalization of Quantized Diffusion Model without Backpropagation

## Quick Facts
- arXiv ID: 2503.14868
- Source URL: https://arxiv.org/abs/2503.14868
- Reference count: 40
- Key outcome: Proposes ZOODiP, achieving up to 8.2× memory reduction compared to existing methods while maintaining comparable personalization performance.

## Executive Summary
This paper introduces ZOODiP, a method for efficient personalization of quantized diffusion models without backpropagation. The approach uses zeroth-order optimization on a quantized model with subspace gradient projection and partial uniform timestep sampling. This eliminates the need for gradient and activation storage, enabling personalization on memory-constrained devices. The method achieves comparable performance to gradient-based approaches while significantly reducing memory usage.

## Method Summary
ZOODiP personalizes quantized diffusion models using zeroth-order optimization on textual inversion tokens. It employs Random Gradient Estimation (RGE) to estimate gradients without backpropagation, projects noisy gradients onto a low-dimensional subspace to remove noise, and uses Partial Uniform Timestep Sampling (PUTS) to focus training on timesteps where text conditioning has the most influence. The method operates directly on INT8 quantized models, achieving memory efficiency while maintaining personalization quality.

## Key Results
- Achieves up to 8.2× memory reduction compared to existing personalization methods
- Maintains comparable CLIP-I and DINO scores to gradient-based approaches
- Successfully personalizes quantized Stable Diffusion v1.5 models with 2.37GB peak VRAM usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Eliminating backpropagation via zeroth-order optimization on quantized models drastically reduces memory usage for personalization.
- Mechanism: Uses random perturbations to estimate gradients directly from the objective function's output, avoiding storage of activations and gradients for backpropagation.
- Core assumption: The pseudo-token embedding being optimized has a significantly lower effective dimension than its full dimensionality.
- Evidence anchors: [abstract] "...leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation..."; [section 3.2] "...With the ZO approach, we were able to estimate the gradient without backpropagation while effectively reducing memory usage."; [corpus] QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models confirms ZO optimization on quantized models is an emerging, related strategy.

### Mechanism 2
- Claim: Projecting noisy gradient estimates onto a low-dimensional subspace accelerates convergence and improves personalization quality.
- Mechanism: Maintains a buffer of recent token updates and uses PCA to identify dimensions of low variance (assumed to contain noise), then projects the estimated gradient to remove components along these noisy dimensions.
- Core assumption: Meaningful changes to the token embedding during optimization occur within a low-dimensional subspace, while remaining dimensions are dominated by noise from ZO estimation.
- Evidence anchors: [abstract] "...gradient estimation using zeroth-order optimization is quite noisy... propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens..."; [section 3.3, Figure 3] "...personalized concept is preserved after back-projection, indicating that key changes from TI lie within this subspace."

### Mechanism 3
- Claim: Selectively sampling from a specific range of diffusion timesteps makes training more efficient and effective for text-conditional personalization.
- Mechanism: Proposes Partial Uniform Timestep Sampling (PUTS), which samples timesteps from a specific range (e.g., U(500, 1000)) rather than the full U(0, 1000) range.
- Core assumption: Text embeddings have varying influence across the diffusion process, and personalization benefits most from optimizing timesteps closer to pure noise (higher timesteps).
- Evidence anchors: [abstract] "...text embeddings predominantly influence image generation at specific diffusion timesteps."; [section 3.4, Figure 4] "...sampling timestep t from U(0, 500) failed to effectively learn the reference image concept, whereas sampling from U(500, 1000) led to successful learning."; [corpus] Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies discusses selective optimization, aligning with the idea of targeting specific training aspects for efficiency.

## Foundational Learning

- **Zeroth-Order (ZO) Optimization**: Understand how to estimate gradients using only forward passes, which is the core memory-saving technique.
  - Quick check question: How does the convergence rate of ZO optimization relate to the dimensionality of the parameters being optimized?

- **Textual Inversion**: Understand that ZOODiP is built on top of this personalization technique, optimizing a single token embedding in the text encoder's space.
  - Quick check question: What does Textual Inversion optimize to personalize a diffusion model?

- **Model Quantization (e.g., INT8)**: Understand how quantization makes models non-differentiable and saves memory.
  - Quick check question: How does post-training quantization affect a model's differentiability and memory footprint?

## Architecture Onboarding

- **Component map**: Quantized Backbone (INT8 Stable Diffusion) -> Pseudo-Token (trainable embedding) -> ZOOptimizer (ZOAdam with RGE) -> Subspace Gradient Module (PCA projection) -> PUTS Sampler (timestep selection) -> Loss Calculation -> Token Update

- **Critical path**: Reference Image -> VAE Encode -> Sample Timestep (PUTS) -> Add Noise -> U-Net Forward Pass with Perturbed Token -> Calculate Loss -> RGE Gradient Estimate -> Project Gradient (SG) -> Update Pseudo-Token

- **Design tradeoffs**:
  - Memory vs. Speed: Low memory but requires more iterations than gradient-based methods due to ZO optimization
  - Precision vs. Memory: INT4 quantization possible but degrades performance compared to INT8
  - PUTS Range: Hyperparameter that may require tuning for different model architectures

- **Failure signatures**:
  - Poor/Slow Convergence: Misconfigured tau or nu in SG, or incorrect PUTS range
  - Concept Not Learned: PUTS range too low, failing to capture reference image's key features
  - Token Collapse: Extreme cases of ZO optimization divergence

- **First 3 experiments**:
  1. Baseline Validation: Run naive ZO Textual Inversion (no SG, no PUTS) on a single subject from DreamBooth dataset
  2. Ablation on PUTS: Train with PUTS (fixed range 500-900) vs. uniform timestep sampling, measure CLIP-I and DINO scores
  3. Integrate Subspace Gradient (SG): Add SG module, compare convergence speed and final alignment scores to naive ZO baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the theoretical relationship between token effective dimensionality and ZO optimization convergence rate be formally characterized for diffusion model personalization?
  - Basis in paper: [inferred] Paper conjectures success arises from low dimensionality of optimized tokens, noting that over 80% of dimensions are projected out during SG, but provides only empirical validation rather than theoretical bounds
  - Why unresolved: Discussion mentions ZO convergence depends on effective dimension, yet no formal analysis links token subspace structure to convergence guarantees in the personalization setting
  - What evidence would resolve it: Theoretical analysis bounding convergence rate as a function of token intrinsic dimension, or empirical studies correlating token effective rank with optimization speed across diverse concepts

- **Open Question 2**: How can the sample-efficiency gap between ZO optimization and gradient-based methods be closed while maintaining memory efficiency?
  - Basis in paper: [explicit] Limitation section states: "it still requires a considerably larger number of iterations compared to gradient-based approaches. This increased iteration count stems from the fundamental nature of zeroth-order optimization."
  - Why unresolved: While SG and PUTS mitigate slow learning, fundamental limitation of function-evaluation-based optimization versus backpropagation remains unaddressed
  - What evidence would resolve it: Novel ZO estimators with improved variance reduction, hybrid methods combining limited backpropagation with ZO, or adaptive perturbation strategies achieving comparable iteration counts to first-order methods

- **Open Question 3**: What determines the optimal PUTS timestep range (TL, TU) across different diffusion architectures, model sizes, and concept types?
  - Basis in paper: [inferred] PUTS parameters were empirically selected (TL=500, TU=900 for SD1.5; TL=700 for SDXL), with paper noting "the value of TL was selected empirically" and acknowledging information loss varies with dimensionality
  - Why unresolved: No principled framework guides timestep range selection; paper relies on exhaustive grid search without theoretical justification for why certain timesteps are more effective for personalization
  - What evidence would resolve it: Analysis of cross-attention activation patterns across timesteps during personalization, or a predictive model mapping architecture/token properties to optimal timestep ranges

## Limitations

- **Memory vs. Iteration Trade-off**: ZOODiP requires significantly more iterations than gradient-based methods due to the fundamental nature of zeroth-order optimization
- **Hyperparameter Sensitivity**: PUTS range and SG parameters require careful tuning and may not generalize across different diffusion architectures
- **Limited Theoretical Analysis**: The relationship between token subspace structure and optimization convergence lacks formal theoretical characterization

## Confidence

- **High Confidence**: Memory efficiency claim (8.2× reduction) is well-supported by zeroth-order optimization mechanism that eliminates backpropagation gradient storage
- **Medium Confidence**: Convergence acceleration from subspace gradient projection is plausible given low-rank assumption, but quantitative impact not fully validated
- **Low Confidence**: Claim that meaningful token changes occur within low-dimensional subspace lacks quantitative validation beyond qualitative PCA visualizations

## Next Checks

1. **ZOAdam Implementation Verification**: Implement ZOAdam optimizer by adapting MeZO algorithm to track moving averages of estimated gradients and squared gradients. Compare convergence speed and final loss values against standard Adam optimizer to ensure ZO update rule is correctly implemented.

2. **Subspace Projection Ablation Study**: Run ablation experiment comparing ZOODiP with and without Subspace Gradient (SG) module on simple textual inversion task. Measure both convergence speed (iterations to reach target CLIP-I score) and final alignment scores to quantify SG module's contribution.

3. **PUTS Range Sensitivity Analysis**: Train ZOODiP using different PUTS ranges (e.g., U(400, 800), U(600, 1000), and full U(0, 1000)) on same dataset. Compare final CLIP-I, CLIP-T, and DINO scores to determine if optimal range is model-specific and identify sensitivity to this hyperparameter.