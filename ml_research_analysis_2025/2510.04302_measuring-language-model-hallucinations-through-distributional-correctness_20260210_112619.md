---
ver: rpa2
title: Measuring Language Model Hallucinations Through Distributional Correctness
arxiv_id: '2510.04302'
source_url: https://arxiv.org/abs/2510.04302
tags:
- answer
- correct
- score
- language
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DCS addresses the problem of traditional evaluation metrics that
  reward language models for guessing over abstention, leading to harmful hallucinations.
  It evaluates a model's full probability distribution over answer choices rather
  than just the top prediction, distinguishing between different types of uncertainty
  (hedging toward incorrect answers vs.
---

# Measuring Language Model Hallucinations Through Distributional Correctness

## Quick Facts
- arXiv ID: 2510.04302
- Source URL: https://arxiv.org/abs/2510.04302
- Authors: Thomas F Burns
- Reference count: 40
- Key outcome: Half of 12 benchmarks showed universally negative DCS scores across all models, with the best model achieving only 0.19 DCS (vs. 0.678 accuracy) on its strongest benchmark, revealing significant epistemic overconfidence in language models.

## Executive Summary
Traditional language model evaluation metrics reward guessing over abstention, leading to harmful hallucinations. This paper introduces the Distributional Correctness Score (DCS), which evaluates a model's full probability distribution over answer choices rather than just the top prediction. DCS distinguishes between different types of uncertainty (hedging toward incorrect answers vs. hedging toward abstention) and naturally incorporates abstention as a neutral anchor at zero, providing interpretable scores in [-1,1].

## Method Summary
DCS computes the length-normalized log-likelihoods for correct answers (p_c), incorrect answers (P_W), and an explicit IDK response (p_IDK), then applies the formula: DCS = (l_c·p_c - l_w·P_W)·(1 - p_IDK). The method uses 12 multiple-choice benchmarks and 6 language models, with prompts including specific instructions warning of penalties for mistakes. IDK options are carefully formatted to match the length and style of valid answer choices for each benchmark.

## Key Results
- 6/12 benchmarks showed universally negative DCS scores across all tested models
- Best model achieved only 0.19 DCS on its strongest benchmark (vs. 0.678 accuracy)
- TruthfulQA and Winogender showed particularly concerning performance with large gaps between ternary scores and DCS values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Evaluating full probability distributions over answer choices reveals epistemic patterns that single-answer metrics systematically obscure.
- **Mechanism:** Traditional metrics discard how remaining probability mass is distributed, making two models with identical accuracy appear equivalent despite qualitatively different belief states.
- **Core assumption:** A model's full probability distribution over answers reflects meaningful internal epistemic states.
- **Evidence anchors:** [abstract] "A novel evaluation metric... to solve this problem, i.e., of not considering a model's entire probability distribution over answer choices."
- **Break condition:** If model probability distributions are miscalibrated or do not reflect meaningful uncertainty.

### Mechanism 2
- **Claim:** The IDK damping factor (1 - p_IDK) creates a neutral anchor at zero, making abstention a structurally distinct response from incorrect answers.
- **Mechanism:** DCS formula structure pulls scores toward zero proportionally to expressed uncertainty, rather than penalizing all non-correct answers equally.
- **Core assumption:** Models can meaningfully express uncertainty through IDK responses.
- **Evidence anchors:** [abstract] "DCS naturally incorporates abstention as a neutral anchor at zero."
- **Break condition:** If models cannot reliably express uncertainty via IDK due to instruction-following failures.

### Mechanism 3
- **Claim:** The asymmetric (p_c - P_W) term penalizes confident incorrectness more heavily than uncertainty, creating an incentive ordering: Confident Correct > Honest Abstention > Confident Incorrect.
- **Mechanism:** Theorem 1 proves DCS(π_CI) < DCS(π_HA) < DCS(π_CC), with optimal guessing threshold p_c > l_w/(l_c + l_w).
- **Core assumption:** Models will respond to this incentive structure by shifting probability mass from incorrect options toward IDK when uncertain.
- **Evidence anchors:** [section 5] "The score structure incentivises a clear preference ordering over epistemic states."
- **Break condition:** If training/evaluation does not propagate DCS incentives back to model behavior.

## Foundational Learning

- **Concept: Proper scoring rules vs. utility functions**
  - Why needed here: DCS is explicitly NOT a proper scoring rule (§7); confusing the two leads to misinterpretation.
  - Quick check question: Does Brier score distinguish between a model hedging toward IDK vs. hedging toward wrong answers when both assign p_c = 0.4?

- **Concept: Conditional mutual information I(A;D|Q)**
  - Why needed here: Proposition 2 bounds maximum achievable DCS by information-theoretic limits in training data.
  - Quick check question: If training data contains no signal about answers to a query class, can any model achieve positive expected DCS on that class?

- **Concept: Length-normalized log-likelihoods**
  - Why needed here: Implementation requires comparing probabilities across answer choices of varying token lengths.
  - Quick check question: Why would raw log-likelihoods favor shorter answers like "A" over longer answers like " I don't know"?

## Architecture Onboarding

- **Component map:** Input: Query + answer set A → Forward pass: Compute log-likelihoods → Length normalization → Probability extraction: Softmax over answer set → Score computation: DCS = (l_c·p_c - l_w·P_W)·(1 - p_IDK) → Aggregation: Mean DCS across benchmark examples

- **Critical path:** Design IDK response format matching other answers in length/style → Verify models can actually follow the instruction format → Extract unconditional token probabilities, not conditional generation

- **Design tradeoffs:**
  - Loading parameters (l_c, l_w): Default l_c = l_w = 1 gives symmetric [-1, 1] range and 0.5 guessing threshold
  - IDK format: Must balance naturalness with controlled length; current work uses benchmark-specific formats
  - Single vs. multi-token answers: Paper focuses on discrete choices; extending to open generation requires defining answer regions

- **Failure signatures:**
  - 0% accuracy on a benchmark suggests instruction-following failure, not knowledge gap
  - Large gap between ternary score and DCS indicates error-hedging rather than abstention-hedging
  - Universally negative DCS across all models suggests either benchmark difficulty or systematic overconfidence patterns

- **First 3 experiments:**
  1. Implement DCS on a small benchmark (e.g., COPA with 1000 examples) comparing against reported accuracy
  2. Test same model/benchmark with different IDK phrasings ("I don't know" vs. "(?)" vs. "unsure") to measure prompt sensitivity
  3. For one model on one benchmark, plot DCS across varying l_w values (Table 3 range: 1/9 to 9) to observe score distribution shift

## Open Questions the Paper Calls Out
None

## Limitations
- Prompt format dependency: IDK response format is benchmark-specific and must be carefully matched in length and style to other answer options
- Assumption of meaningful uncertainty expression: DCS assumes models can and do express genuine epistemic uncertainty through IDK responses rather than prompt artifacts
- Instruction-following failures: Table 4 shows 0% accuracy cases indicating that some models fail to follow instruction formats, making the IDK damping factor uninterpretable

## Confidence
- Major Limitation: Prompt Format Dependency (High)
- Major Limitation: Assumption of Meaningful Uncertainty Expression (Medium)
- Major Limitation: Instruction-following failures (High)

## Next Checks
1. Verify DCS implementation produces non-trivial p_IDK values on a small benchmark
2. Test DCS sensitivity to different IDK phrasings across multiple models
3. Plot DCS score distributions across varying l_w parameters to confirm incentive structure behavior