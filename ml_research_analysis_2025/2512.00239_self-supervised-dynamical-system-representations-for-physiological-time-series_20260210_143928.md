---
ver: rpa2
title: Self-Supervised Dynamical System Representations for Physiological Time-Series
arxiv_id: '2512.00239'
source_url: https://arxiv.org/abs/2512.00239
tags:
- system
- information
- time
- pulse
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PULSE, a self-supervised learning method
  for physiological time-series that uses dynamical systems theory to guide representation
  learning. Unlike heuristic approaches that rely on augmentations or masking, PULSE
  explicitly separates system information (shared across similar samples) from sample-specific
  noise by using a cross-reconstruction task on pseudo-pairs from the same time series.
---

# Self-Supervised Dynamical System Representations for Physiological Time-Series

## Quick Facts
- arXiv ID: 2512.00239
- Source URL: https://arxiv.org/abs/2512.00239
- Reference count: 36
- Primary result: PULSE outperforms contrastive learning, masked autoencoding, and sequential VAE baselines in linear evaluation, data efficiency, and transfer learning across diverse physiological time-series tasks

## Executive Summary
This paper introduces PULSE, a self-supervised learning method for physiological time-series that uses dynamical systems theory to guide representation learning. Unlike heuristic approaches that rely on augmentations or masking, PULSE explicitly separates system information (shared across similar samples) from sample-specific noise by using a cross-reconstruction task on pseudo-pairs from the same time series. The method leverages a state-space model to identify which variables should be preserved and which should be discarded. Theoretical analysis shows that full-sample masking in cross-reconstruction provably recovers system parameters, and empirical validation on synthetic and real-world physiological datasets demonstrates strong performance.

## Method Summary
PULSE learns representations by reconstructing one time-series sample using system information extracted from another sample. It uses a state-space model framework where physiological signals are generated from latent dynamics with system parameters Θ and initial conditions X. The method creates pseudo-pairs by sampling random initial conditions within windows and requiring the system encoder to extract information that supports reconstruction across varying initial conditions. A dilated convolutional encoder extracts time-invariant system parameters, while a CNN extracts initial conditions. A GRU decoder then reconstructs the target sequence using these latent variables.

## Key Results
- PULSE outperforms contrastive learning, masked autoencoding, and sequential VAE baselines by 2-16% on linear evaluation across HAR, PPG, ECG, and EEG datasets
- Oracle experiments confirm full-sample masking is crucial for isolating system parameters (6-16% performance gap vs partial masking)
- PULSE achieves state-of-the-art results in semi-supervised learning and transfer learning scenarios
- The method maintains strong performance even with limited labeled data

## Why This Works (Mechanism)

### Mechanism 1: Cross-Reconstruction Forces Information Factorization
Reconstructing one time-series sample using system information extracted from another forces the encoder to discard sample-specific factors while retaining shared system parameters. Given pseudo-pairs from the same window with different initial conditions, the system encoder must extract Θ that supports reconstruction across randomly sampled starting points. Since sample-specific factors (initial conditions, noise) differ between pairs, gradient pressure eliminates them.

### Mechanism 2: Full-Sample Masking Theoretically Isolates System Parameters
Masking an entire sample while reconstructing it from another sample's representation uniquely recovers system parameters Θ as the minimal shared latent set. Theorem 1 proves that when one sample is fully masked, the only shared parent between masked and unmasked regions is Θ(s). Partial masking includes latent state variables at mask boundaries, confounding system and sample-specific information.

### Mechanism 3: Pseudo-Pairs Approximate Independent Sample Distribution
Randomly sampling initial conditions within windows creates pseudo-pairs that approximate the cross-reconstruction objective with independent samples. Since Θ is invariant to initial conditions, requiring reconstruction from random t₀ forces the system representation to generalize across latent trajectories, emulating independent sample diversity without requiring labeled pair identification.

## Foundational Learning

- **State-Space Models (SSMs)**: The entire framework models physiological signals as y_t = g_y(x_t) with latent dynamics x_{t+1} = g_x(x_t, Θ). Quick check: Given an SSM with transition function g_x, what information would be shared across two trajectories with different x_0 but the same Θ?

- **Variational Autoencoder ELBO Decomposition**: Understanding why SVAEs fail requires grasping that E_q(z|y)[log p(y|z)] reconstructs all observed variability—including noise—without distinction. Quick check: In a standard VAE, does the reconstruction term distinguish between "meaningful signal" and "noise"? How does PULSE change this?

- **Masked Autoencoding Theory (Minimal Shared Latents)**: Theorem 1 builds on Kong & Zhang (2023), which proves MAE implicitly recovers minimal shared latent variables C between masked/unmasked regions. Quick check: If you mask a random 50% of timesteps within a single sample, what latent variables would connect masked and unmasked regions according to PULSE's generative model?

## Architecture Onboarding

- **Component map:** Input window Y_i → f_sys (10-layer dilated conv) → Θ_i → f_init (2-layer CNN) → X_{i,t₀} → GRU decoder → reconstruction Ŷ

- **Critical path:** 1) Sample t₀ uniformly from [1, W/2] 2) Extract Θ_i from full window via f_sys 3) Extract X_{i,t₀} from local region around t₀ via f_init 4) Roll forward dynamics from X_{i,t₀} using Θ_i via GRU 5) Compute reconstruction loss on Ỹ_i = Y_{i,t₀:W} 6) Repeat with multiple t₀ samples per batch

- **Design tradeoffs:** Time-varying Θ̃ dimensionality constrained to 1D to prevent trivial copying of signal (ablation: -7.62% avg without); t₀ sampling range restricted to first half (W/2) prevents overfitting to short subsequences; separate encoders reduce factorization pressure by -1.22% avg compared to shared encoders

- **Failure signatures:** Representations cluster by amplitude/start time indicates f_init not being used correctly; check that X_{i,t₀} is extracted from correct receptive field; high reconstruction loss with random pairs but not pseudo-pairs indicates system encoder overfitting to sample-specific features; increase regularization on Θ̃ dimensionality; linear probe accuracy near chance suggests t₀ may be fixed or not properly randomized

- **First 3 experiments:** 1) Synthetic validation: Train on Lorenz/Thomas/Hindmarsh-Rose data with known parameter clusters; verify representation space clusters by Θ (not by initial condition or noise level) 2) Ablation sweep: Remove each component (time-varying params, separate encoders, random t₀) individually; expect -1% to -10% drops 3) Transfer probe: Pretrain on EEG, linear probe on Epilepsy dataset; target >94% AUROC to confirm system information transfers

## Open Questions the Paper Calls Out
- How can independent samples be identified in unlabeled physiological datasets to better align PULSE's practical implementation with its theoretical cross-reconstruction framework?
- Can more flexible or data-driven generative models improve the fidelity of learned system representations for complex physiological signals?
- Under what conditions does the pseudo-pair approximation fail relative to true independent sample cross-reconstruction?

## Limitations
- Architecture implementation details like batch size and exact regularization are unspecified, making exact reproduction difficult
- Theoretical guarantees rely on specific DAG generative model assumptions that may not hold for real physiological signals
- Empirical scope focuses on PULSE components rather than direct comparison to alternative SSL objectives on the same datasets

## Confidence
- **High Confidence:** The core mechanism of cross-reconstruction on pseudo-pairs to separate system parameters from sample-specific noise is well-supported by both theoretical analysis and empirical ablation studies
- **Medium Confidence:** The claim that full-sample masking is crucial for isolating system parameters is supported by oracle experiments, but the theoretical analysis assumes a specific generative model
- **Low Confidence:** The assertion that PULSE's performance gain is solely due to its SSL objective, rather than architectural choices or hyperparameter tuning, requires further validation

## Next Checks
1. **Oracle Ablation:** Compare PULSE's oracle performance (using true pairs with full-sample masking) against partial masking to quantify the impact of masking strategy on system parameter recovery
2. **Time-Varying Parameter Analysis:** Investigate the learned θ_i,tk dimensionality and its effect on factorization pressure by varying the dimensionality and observing reconstruction loss and downstream performance
3. **Nonstationary Dynamics Test:** Evaluate PULSE on datasets with known nonstationarities (e.g., varying heart rate in ECG) to assess robustness to the stationarity assumption in pseudo-pair construction