---
ver: rpa2
title: Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation
arxiv_id: '2507.11789'
source_url: https://arxiv.org/abs/2507.11789
tags:
- latent
- manifold
- space
- data
- flatvi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem that standard variational autoencoders
  (VAEs) do not naturally ensure that straight paths in the latent space correspond
  to geodesic paths on the data manifold, which limits their use in trajectory inference
  and interpolation tasks. The authors introduce FlatVI, a novel training framework
  that regularizes the latent manifold of discrete-likelihood VAEs toward Euclidean
  geometry by encouraging the pullback metric from the decoder to approximate the
  identity matrix.
---

# Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation

## Quick Facts
- arXiv ID: 2507.11789
- Source URL: https://arxiv.org/abs/2507.11789
- Reference count: 40
- Key outcome: Introduces FlatVI, a VAE framework that regularizes latent manifold geometry toward Euclidean, improving trajectory inference and interpolation on single-cell data.

## Executive Summary
This paper addresses the fundamental limitation that standard VAEs do not ensure straight paths in latent space correspond to geodesic paths on the data manifold, which limits their use in trajectory inference and interpolation tasks. The authors introduce FlatVI, a novel training framework that regularizes the latent manifold of discrete-likelihood VAEs toward Euclidean geometry by encouraging the pullback metric from the decoder to approximate the identity matrix. This approach is specifically tailored for modeling single-cell count data using negative binomial distributions. Experiments on synthetic data demonstrate that FlatVI successfully induces approximately Euclidean latent geometry while maintaining parameter reconstruction accuracy. On real single-cell RNA sequencing datasets, FlatVI improves trajectory reconstruction and vector field consistency when combined with optimal transport methods, and produces more interpretable interpolated gene expression patterns compared to existing methods.

## Method Summary
FlatVI trains a VAE with a Negative Binomial likelihood for single-cell count data, adding a regularization term that encourages the decoder's pullback metric in latent space to approximate a scaled identity matrix. The model computes the Fisher Information Metric for NB distributions and uses its pullback through the decoder Jacobian to measure local geometry. A flattening loss minimizes the Frobenius norm between this metric and αI_d, with λ controlling the regularization strength. The framework is validated on synthetic data and applied to real scRNA-seq datasets, showing improved trajectory reconstruction when paired with optimal transport methods.

## Key Results
- On synthetic data, FlatVI achieves 3-NN overlap between Euclidean and geodesic neighborhoods of 0.80 (λ=10) compared to 0.66 for unregularized VAE
- On real datasets, FlatVI improves trajectory reconstruction with 2-Wasserstein distances of 1.54 (EB) versus 2.07 for NB-VAE
- Velocity consistency metrics show FlatVI produces more interpretable interpolated gene expression patterns with improved vector field consistency across latent dimensions

## Why This Works (Mechanism)

### Mechanism 1: Pullback Metric Regularization Induces Euclidean Latent Geometry
Minimizing the Frobenius norm distance between the pullback metric M(z) and a scaled identity matrix αI_d encourages straight latent paths to approximate geodesics on the decoded statistical manifold. The decoder maps latent codes to parameters of a Negative Binomial distribution, inducing a Riemannian metric in latent space via the pullback of the Fisher Information Metric (FIM). When M(z) = J_h(z)^T M(φ) J_h(z) ≈ αI_d, the geodesic distance reduces to Euclidean distance, making straight lines optimal paths. The flattening loss L_flat = E[||M(z) - αI_d||²_F] directly penalizes deviations from this condition.

### Mechanism 2: Negative Binomial Likelihood Captures Single-Cell Count Statistics
Modeling gene expression counts with Negative Binomial distributions preserves discreteness and overdispersion properties essential for scRNA-seq data. Each gene count x_g ~ NB(μ_g, θ_g) where μ_g = l·softmax(ρ_φ(z)) depends on both the decoded proportions and observed size factor l. The FIM for NB distributions weights gradient contributions by inverse dispersion, naturally balancing high/low expression genes.

### Mechanism 3: Euclidean Latent Space Enables Compatibility with Optimal Transport Methods
A flattened latent representation improves trajectory reconstruction when paired with OT-CFM, which assumes straight-line interpolation (Euclidean geometry) is optimal. OT-CFM learns velocity fields by matching source/target distributions via linear interpolants: u_t(x|x_0, x_1) = x_1 - x_0. This is optimal only if straight paths are geodesics. FlatVI ensures this condition, reducing the mismatch between OT assumptions and actual manifold geometry.

## Foundational Learning

- **Concept: Pullback Metric**
  - **Why needed here:** The core mechanism transfers geometric structure from the decoded space to latent space. Without understanding how J^T M J propagates metric information, the flattening loss is unintelligible.
  - **Quick check question:** If the decoder Jacobian has rank < d at some point z, what happens to the pullback metric's invertibility?

- **Concept: Fisher Information Metric**
  - **Why needed here:** Defines the Riemannian geometry of statistical manifolds (probability distributions). The FIM provides the natural metric M(φ) that measures information distance between nearby distributions.
  - **Quick check question:** For a univariate Gaussian with fixed variance σ², what is the Fisher information with respect to the mean μ?

- **Concept: Geodesic Curves**
  - **Why needed here:** Geodesics are shortest paths on curved manifolds. The paper's goal is making Euclidean lines approximate geodesics—you need to understand what geodesics are to evaluate success.
  - **Quick check question:** On a sphere, is the straight line (through the interior) between two points a geodesic? What about the great circle arc?

## Architecture Onboarding

- **Component map:** Input: X ∈ N^{N×G} (raw counts) + size factors l → Encoder f_ψ: log(1+X) → z ∈ R^d → Decoder h_φ: (z, l) → μ = l·softmax(ρ_φ(z)) → Likelihood: X | z ~ NB(μ, θ) → Loss: L_ELBO + λ·L_flat

- **Critical path:** The flattening loss computation (Eq. 8-11) is the novel contribution. Implementation requires: 1) Computing ∇_z h_g(z) for each gene g using Jacobian-vector products, 2) Assembling M(z) = Σ_g [θ_g/(h_g(h_g+θ_g))] ∇_z h_g ⊗ ∇_z h_g, 3) Frobenius norm against αI_d where α is trainable

- **Design tradeoffs:**
  - λ selection: Higher λ flattens more but risks reconstruction degradation. Paper uses λ=1 for EB/Pancreas, λ=0.1 for MEF
  - Latent dimension d: Paper uses d=10. Lower dimensions may over-constrain; higher reduce compression benefits
  - Network depth: Shallow 2-layer [256, 10] works; deeper networks may require tuning λ to avoid instability

- **Failure signatures:**
  - High VoR/CN at class boundaries: indicates insufficient flattening in transition regions
  - Reconstruction MSE increasing with λ: over-regularization
  - Non-straight geodesics visualized in latent space: regularization not converged

- **First 3 experiments:**
  1. Train on 10-dim NB synthetic data with 3 classes, varying λ ∈ {0, 1, 3, 5, 7, 10}. Verify: (a) MSE(μ), MSE(θ) stable; (b) 3-NN overlap increases; (c) VoR/CN decrease
  2. On EB dataset, sweep λ and plot trajectory reconstruction (2-Wasserstein) vs. reconstruction loss. Identify Pareto frontier
  3. Sample point pairs from high-VoR regions of unregularized VAE. Plot geodesic paths in both latent spaces. Confirm FlatVI produces straighter paths

## Open Questions the Paper Calls Out

### Open Question 1
Can the trade-off between flattening regularization strength (λ) and reconstruction likelihood be mitigated through adaptive or learned regularization schemes? The current formulation uses a fixed λ hyperparameter that must be tuned per dataset, and increasing λ can harm data fidelity and lower model likelihood.

### Open Question 2
What alternative latent geometries (beyond Euclidean) would better capture biological processes dominated by cyclic dynamics, such as the cell cycle? FlatVI's flattening loss explicitly enforces M(z) → αI, which assumes geodesic convexity. Cyclic processes violate this assumption, but no alternative geometry formulations are proposed.

### Open Question 3
Why does FlatVI exhibit sub-optimal flattening at class boundaries, and can the regularization be improved to handle these transition regions? Appendix L.1.4 shows "high V oR and CN are concentrated in regions of class heterogeneity" and "FlatVI fails to unfold some fast-changing manifold areas at the intersection between classes."

### Open Question 4
Can FlatVI's flattening regularization be extended to other statistical manifolds (e.g., Poisson for chromatin accessibility) while maintaining theoretical guarantees? The flattening loss derivation is specific to the negative binomial Fisher information metric. Different distributions have different FIMs, potentially affecting tractability and optimization behavior.

## Limitations
- The Euclidean assumption may not hold for all single-cell trajectories, particularly branched or cyclic biological processes where inherent manifold curvature may be essential
- The computational burden of evaluating the full Jacobian across thousands of genes remains a practical challenge not fully addressed
- The assumption of gene-specific dispersion θ_g being independent of cell state could fail for technical artifacts correlated with biological state

## Confidence

- **High confidence**: Synthetic data validation showing improved 3-NN overlap and reduced VoR/CN with increasing λ (Table 1)
- **Medium confidence**: Real dataset results showing improved trajectory reconstruction and velocity consistency, though performance varies across datasets
- **Medium confidence**: The theoretical framework linking pullback metric regularization to Euclidean geometry, supported by the Fisher Information Metric derivation

## Next Checks

1. **Branching trajectory validation**: Apply FlatVI to a dataset with known branching (e.g., hematopoiesis) and evaluate whether the flattening preserves biologically meaningful branch points versus creating artificial straight paths
2. **Dispersion state dependence test**: Implement a variant where θ_g varies with latent position z and compare reconstruction quality and geometric regularization to the fixed-dispersion baseline
3. **Computational efficiency benchmark**: Profile memory usage and runtime for computing M(z) across varying gene counts (100-10,000 genes) and evaluate alternative JVP implementations (forward-mode autodiff vs. finite differences)