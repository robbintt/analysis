---
ver: rpa2
title: 'When Algorithms Play Favorites: Lookism in the Generation and Perception of
  Faces'
arxiv_id: '2506.11025'
source_url: https://arxiv.org/abs/2506.11025
tags:
- faces
- gender
- attractiveness
- classification
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates algorithmic lookism in text-to-image (T2I)
  systems, examining how facial attractiveness relates to other perceived traits.
  Using 13,200 synthetic faces generated with Stable Diffusion 2.1 across different
  genders, races, and trait descriptors, we find that T2I models associate attractiveness
  with positive traits like intelligence and trustworthiness, particularly for Asian
  and Black women.
---

# When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces

## Quick Facts
- **arXiv ID:** 2506.11025
- **Source URL:** https://arxiv.org/abs/2506.11025
- **Reference count:** 40
- **Primary result:** T2I systems associate facial attractiveness with positive traits like intelligence and trustworthiness, particularly for Asian and Black women, while gender classifiers show higher error rates on less-attractive faces.

## Executive Summary
This study investigates algorithmic lookism in text-to-image systems, revealing that Stable Diffusion 2.1 generates faces where attractiveness correlates with positive traits like intelligence and trustworthiness. The research shows these associations are particularly strong for Asian and Black women, while gender classification models exhibit significantly higher error rates on less-attractive female faces, especially among non-White women. These findings highlight fairness concerns in AI systems and underscore the need for bias mitigation strategies in digital identity applications.

## Method Summary
The study generated 13,200 synthetic faces using Stable Diffusion 2.1 across six demographic groups (woman/man × Asian/Black/White) and eleven attribute conditions (attractive/unattractive, intelligent/unintelligent, trustworthy/untrustworthy, sociable/unsociable, happy/unhappy, plus neutral baseline). CLIP embeddings were extracted and centroids computed for each group, with Euclidean distances measuring trait associations. Three gender classification models (InsightFace, DeepFace, FairFace) were evaluated on the generated dataset to assess accuracy differences between attractive and unattractive faces.

## Key Results
- T2I systems strongly associate attractiveness with positive traits like intelligence and trustworthiness, particularly for Asian and Black women
- Gender classification models show significantly higher error rates on less-attractive female faces (InsightFace: 94.6% vs 73.0% accuracy)
- Non-White women experience the highest error rates, demonstrating intersectional bias amplification
- Negative-trait faces often appear older or lack makeup, confounding the analysis of pure lookism

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Co-location of Social Traits (Halo Effect)
Text-to-Image models conflate distinct social concepts (e.g., intelligence, trustworthiness) with visual attractiveness markers within the embedding space. Due to training data correlations, vectors for positive traits occupy similar regions to the vector for "attractive," resulting in generated images that visually converge when prompted with semantically distinct but socially correlated terms. This relies on CLIP embedding space accurately capturing human-perceived visual similarities.

### Mechanism 2: Classifier Reliance on Proxy Features (Cosmetics/Age)
Gender classification models fail on "unattractive" or "negative-trait" female faces because these faces lack specific proxy features (e.g., makeup, youth) the models learned to associate with "female." Classifiers trained on standard datasets learn superficial correlations rather than biological structure, so when encountering faces without expected cosmetic signals, accuracy degrades significantly.

### Mechanism 3: Intersectional Error Amplification
Algorithmic lookism compounds with racial and gender biases, resulting in highest error rates for non-White women. The T2I model produces lower-fidelity or more stereotypical variations for minority groups, and when these already-distorted images are fed into biased classifiers, the deviation from expected "ideal" female face is maximized for non-White, "unattractive" inputs.

## Foundational Learning

- **Concept: CLIP Embedding Space & Centroids**
  - **Why needed here:** The paper quantifies "lookism" by measuring Euclidean distances between image group centroids in CLIP space. Understanding that vector proximity implies semantic similarity is essential to interpreting the heatmaps.
  - **Quick check question:** If the centroid distance between "Trustworthy" and "Attractive" is small (d → 0), what does that imply about the model's internal representation of these traits?

- **Concept: The "Attractiveness Halo Effect"**
  - **Why needed here:** This social psychology theory posits that humans irrationally associate beauty with goodness. The study investigates if AI models have absorbed this specific cultural bias from training data.
  - **Quick check question:** Does the paper claim AI creates this bias, or that it reflects an existing human bias present in the training data?

- **Concept: Spurious Correlations in Classification**
  - **Why needed here:** To understand why gender classifiers fail, one must grasp that deep learning models often learn "shortcuts" (e.g., presence of lipstick = female) rather than the actual concept of gender.
  - **Quick check question:** Why does the absence of makeup in the "unattractive" synthetic faces trigger a failure in the gender classifier, according to the Discussion section?

## Architecture Onboarding

- **Component map:** Stable Diffusion 2.1 -> CLIP ViT-B/32 -> Euclidean Distance -> InsightFace/DeepFace/FairFace
- **Critical path:**
  1. Generate 13,200 images across 6 demographic groups × 11 attributes (5 pairs + neutral)
  2. Extract embeddings and compute centroids for each group
  3. Calculate distances: d(Positive Trait, Attractive) vs d(Negative Trait, Unattractive)
  4. Run classification inference to establish accuracy deltas (Acc_attractive - Acc_unattractive)

- **Design tradeoffs:**
  - **Synthetic vs. Real Data:** Using Stable Diffusion allows controlled experimentation, isolating "trustworthy" vs "unattractive." Tradeoff: Generated faces may not reflect full diversity of real human features.
  - **Model Choice:** Using Stable Diffusion 2.1 provides a baseline. Tradeoff: Findings may not generalize to newer models without re-testing.

- **Failure signatures:**
  - **The "Grumpy Old Man" Artifact:** Negative traits often produce faces that look older or have neutral/downward expressions, confounding trait isolation.
  - **Classifier Collapse:** DeepFace accuracy dropping to ~11% on "unhappy" female faces indicates the classifier's decision boundary is heavily skewed by expression/makeup.

- **First 3 experiments:**
  1. **Disentanglement Probe:** Generate "Intelligent + Unattractive" or "Happy + Old" prompts to see if the T2I model can break the latent correlation.
  2. **Classifier Feature Importance (XAI):** Run Grad-CAM on misclassified "unattractive" female faces to confirm if the model focuses on lack of eye/lip makeup.
  3. **Cross-Model Generalization:** Duplicate the pipeline with SDXL or Midjourney to determine if "lookism" is architecture-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do observed associations between attractiveness and positive traits stem from biases inherent in the CLIP embeddings used for measurement?
- **Basis in paper:** The authors explicitly identify the need for "Disentangling the findings from biases that could potentially originate from the CLIP embeddings."
- **Why unresolved:** The study relies on CLIP embeddings to calculate similarity scores between traits. If the embedding space itself encodes societal biases, the measured "lookism" may be an artifact of the analysis tool.
- **What evidence would resolve it:** A comparative analysis using alternative, non-CLIP based feature extractors or conducting human perception studies to validate whether machine-measured distances correspond to human-judged associations.

### Open Question 2
- **Question:** How does training classification models on synthetic data exhibiting algorithmic lookism affect their performance and fairness?
- **Basis in paper:** The authors list "assessing the impact of algorithmic lookism on downstream AI applications, particularly classification models trained on synthetic data" as a key area for future research.
- **Why unresolved:** The current study evaluates pre-trained classifiers on generated images but does not simulate the feedback loop where these biased images are used to train new systems.
- **What evidence would resolve it:** An experiment where separate gender classification models are trained on "attractive" vs. "unattractive" synthetic datasets, followed by comparison of their accuracy and bias metrics on real-world test sets.

### Open Question 3
- **Question:** To what extent do specific visual features—apparent age, facial expression, and makeup—contribute to the high misclassification rates of "less attractive" female faces?
- **Basis in paper:** The authors call for "conducting a targeted XAI study to confirm the relative contributions of apparent age, facial expression, and makeup absence to the observed gender-classification bias."
- **Why unresolved:** The paper identifies correlation between negative traits and misclassification and hypothesizes that lack of cosmetics or older appearance causes error, but does not isolate these variables to prove causation.
- **What evidence would resolve it:** A saliency map analysis or ablation study using generated faces where only one variable (e.g., makeup) is altered while others are held constant.

## Limitations
- Study relies on synthetic faces generated by a single T2I model (Stable Diffusion 2.1), which may not generalize to newer models or real-world scenarios
- Generated faces for "negative" traits often exhibit confounding visual features like older age or lack of makeup, making it difficult to isolate pure "lookism" from other demographic biases
- CLIP embedding space is treated as a gold standard for semantic similarity, but geometric proximity doesn't always perfectly map to human-perceived conceptual similarity

## Confidence
- **High Confidence:** Gender classification models show significantly higher error rates on less-attractive female faces (InsightFace: 94.6% vs 73.0% accuracy)
- **Medium Confidence:** Association between attractiveness and positive traits in T2I generation is statistically significant but may be partially explained by confounding visual features
- **Medium Confidence:** Intersectional amplification of lookism for non-White women is well-documented but generation artifacts vs. true bias amplification remains uncertain

## Next Checks
1. **Cross-Model Replication:** Test the same methodology with newer T2I models (SDXL, DALL-E 3) to determine if lookism is architecture-specific or a universal property of large-scale models
2. **Disentanglement Experiment:** Generate prompts that explicitly separate traits (e.g., "intelligent but unattractive") to test whether the model can break the latent correlation
3. **Classifier Feature Analysis:** Apply Grad-CAM or similar XAI techniques to misclassified "unattractive" faces to verify whether classifiers rely on proxy features like makeup absence as predicted by the paper