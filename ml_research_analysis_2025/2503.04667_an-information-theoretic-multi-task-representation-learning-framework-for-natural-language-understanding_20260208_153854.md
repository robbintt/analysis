---
ver: rpa2
title: An Information-theoretic Multi-task Representation Learning Framework for Natural
  Language Understanding
arxiv_id: '2503.04667'
source_url: https://arxiv.org/abs/2503.04667
tags:
- information
- task
- representations
- multi-task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new principled multi-task representation
  learning framework (InfoMTL) to extract noise-invariant sufficient representations
  for all tasks. It ensures sufficiency of shared representations for all tasks and
  mitigates the negative effect of redundant features, which can enhance language
  understanding of pre-trained language models (PLMs) under the multi-task paradigm.
---

# An Information-theoretic Multi-task Representation Learning Framework for Natural Language Understanding

## Quick Facts
- arXiv ID: 2503.04667
- Source URL: https://arxiv.org/abs/2503.04667
- Reference count: 16
- Primary result: InfoMTL outperforms 12 comparative methods on 6 classification benchmarks, especially in data-constrained and noisy scenarios.

## Executive Summary
This paper introduces InfoMTL, a principled multi-task representation learning framework that leverages information theory to extract noise-invariant sufficient representations. The framework addresses key limitations in existing multi-task learning approaches by separately optimizing for shared representation sufficiency and task-specific compression. Through maximizing shared information and minimizing task-specific redundancy, InfoMTL learns representations that are more robust, data-efficient, and capable of handling noisy inputs.

## Method Summary
InfoMTL employs a dual optimization strategy using hard parameter sharing with a PLM backbone. The framework maximizes mutual information between the shared representation and input (SIMax) using InfoNCE contrastive loss, while simultaneously minimizing task-specific information through variational inference and KL divergence (TIMin). This decoupled approach ensures the shared representation contains sufficient information for all tasks while task-specific heads compress irrelevant features. The total loss combines cross-entropy prediction loss with the SIMax and TIMin terms, controlled by hyperparameters α and β.

## Key Results
- InfoMTL achieves significant improvements over 12 comparative multi-task methods on six classification benchmarks
- The framework demonstrates superior performance in data-constrained scenarios (e.g., 20% training data)
- InfoMTL shows enhanced robustness to noisy inputs and adversarial perturbations

## Why This Works (Mechanism)

### Mechanism 1
Maximizing mutual information between shared representation Z and input X preserves task-agnostic features that might otherwise be lost during compression, mitigating the "insufficiency issue" in multi-task learning (MTL). The framework implements Shared Information Maximization (SIMax) using InfoNCE loss, forcing the shared encoder to retain maximal information about X in Z, counteracting the tendency of Information Bottleneck methods to discard features relevant to some tasks while compressing for others. Core assumption: Retaining maximal input information in the shared layer is beneficial provided that subsequent task-specific heads can filter out the noise relevant to their specific goal.

### Mechanism 2
Minimizing mutual information between input X and task-specific output representation Z_t removes task-irrelevant redundant features (noise) without affecting the sufficiency of the shared representation for other tasks. The framework implements Task-specific Information Minimization (TIMin) using variational inference to map shared features to Gaussian distributions in the task-specific output space. A KL-divergence term regularizes these distributions toward a standard Gaussian prior, effectively compressing the information flow from X into Z_t. Core assumption: The redundancy in the input varies across tasks; thus, compression should happen at the task-specific head rather than the shared backbone.

### Mechanism 3
Decoupling the learning objective into shared sufficiency term (SIMax) and task-specific compression term (TIMin) resolves the conflict where information is "redundant" for one task but "necessary" for another. By establishing the Markov chain Y_t → X → Z → Z_t → Ŷ_t, the system ensures Z is sufficient for all tasks (via I(X; Z)) while Z_t is minimal for task t (via I(X; Z_t) minimization). Core assumption: Standard multi-task optimization fails because it attempts to balance gradients for a single representation, whereas information-theoretic disentanglement allows for separate optimization of sufficiency and compression.

## Foundational Learning

- **Concept: Information Bottleneck (IB) Principle**
  - Why needed here: The paper positions itself explicitly against the standard IB application in MTL. You must understand IB (compressing X to Z to predict Y) to grasp why the authors argue for maximizing input information in the shared layer.
  - Quick check question: If a standard IB method compresses X to be just sufficient for Task A, why might it fail for Task B?

- **Concept: Noise-Contrastive Estimation (InfoNCE)**
  - Why needed here: This is the implementation backbone for the SIMax principle. It estimates mutual information by contrasting positive pairs (input X and its representation Z) against negative pairs.
  - Quick check question: In the context of SIMax, what constitutes a "positive key" z^+ relative to the input x?

- **Concept: Variational Inference & Reparameterization**
  - Why needed here: This is required to implement the TIMin principle. Since mutual information is intractable, the paper uses a variational posterior q(z_t|z) and the reparameterization trick to make the sampling process differentiable.
  - Quick check question: Why is the reparameterization trick necessary when optimizing the Gaussian distributions in the task-specific heads?

## Architecture Onboarding

- **Component map**: Input → Backbone (PLM) → [Fork] → (Path A: SIMax Contrastive Projector) AND (Path B: TIMin Gaussian MLPs → Sampling → Prediction)

- **Critical path**: Input → Backbone → [Fork] → (Path A: SIMax Contrastive Projector) AND (Path B: TIMin Gaussian MLPs → Sampling → Prediction)

- **Design tradeoffs**:
  - α (Trade-off for SIMax): Controls the strength of shared information preservation. High α ensures no loss of input data but may retain noise; low α behaves more like a standard PLM fine-tune.
  - β (Trade-off for TIMin): Controls task-specific compression. High β creates robust but potentially underfit representations; low β retains more input info in the task head, potentially overfitting to noise.

- **Failure signatures**:
  - High Bias / Underfitting: If validation loss plateaus early and task accuracy is low, check if β is too high (over-compression in task heads).
  - Noise Sensitivity: If performance drops significantly on the noisy evaluation set, check if α is too high (forcing the shared layer to keep noise) or β is too low (failing to filter noise in the task head).
  - Training Instability: If the KL divergence term collapses or explodes, check the variance initialization in the TIMin heads.

- **First 3 experiments**:
  1. Baseline Reproduction: Implement the Equal Weighting (EW) baseline with the target PLM backbone on the provided benchmarks to establish the relative improvement metric (Δp).
  2. SIMax Ablation: Run the model with β=0 (removing TIMin) to isolate the contribution of the contrastive shared information maximization. Verify that I(X; Z) increases as shown in Figure 2.
  3. Noisy Robustness Test: Train the full InfoMTL model on a subset (e.g., 20% training data) and evaluate on the adversarial perturbation set to confirm the "noise-invariant" claim made in the abstract.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can InfoMTL be effectively adapted for sequence-to-sequence or generation tasks?
- Basis in paper: [explicit] The paper states it "mainly focuses on MTL in natural language understanding" and limits experiments to "six text classification benchmarks" (Page 4).
- Why unresolved: The current TIMin implementation optimizes probabilistic embeddings via classification metrics (F1), which may not directly translate to the continuous output spaces required for text generation.
- What evidence would resolve it: Application of InfoMTL to generative benchmarks (e.g., translation, summarization) using adapted mutual information constraints for sequential outputs.

### Open Question 2
- Question: Does the InfoMTL framework generalize to soft parameter-sharing architectures?
- Basis in paper: [explicit] The authors define their scope as "hard parameter-sharing" and describe soft parameter-sharing as an "orthogonal line of MTL research" (Page 2).
- Why unresolved: The mathematical formulation relies on a single shared variable Z (Markov chain X → Z → Z_t), whereas soft-sharing typically maintains separate encoders with constrained distances.
- What evidence would resolve it: Integrating SIMax and TIMin principles into soft-sharing architectures (e.g., AdapterFusion or Cross-stitch networks) and evaluating performance.

### Open Question 3
- Question: What is the computational overhead and training latency introduced by the dual optimization process?
- Basis in paper: [inferred] The methodology combines contrastive estimation (SIMax) and variational inference with reparameterization (TIMin) (Page 3), which are computationally more intensive than the simple aggregation used in baselines like EW.
- Why unresolved: The paper reports performance improvements (Δp) but does not provide analysis regarding training speed or memory efficiency.
- What evidence would resolve it: A comparative analysis of training time per epoch and GPU memory consumption between InfoMTL and standard baselines.

## Limitations

- The exact architecture of task-specific MLP heads (hidden layer size, activation function) is not fully specified in the main text, requiring assumptions for reproduction.
- The optimal hyperparameter settings for different PLM backbones or task combinations are not thoroughly explored, potentially limiting generalizability.
- The paper does not provide computational overhead analysis or training latency measurements compared to baseline methods.

## Confidence

- **High Confidence**: The core information-theoretic principles (SIMax and TIMin) are well-defined and theoretically sound. The overall architecture and loss formulation are clearly specified.
- **Medium Confidence**: The experimental results and their implications for sufficiency, data-efficiency, and robustness are compelling, but the exact implementation details for some components introduce minor uncertainty in reproducibility.
- **Low Confidence**: The paper's claim of being the first to propose this specific decoupled SIMax/TIMin framework is not independently verified against all prior work, and the effectiveness of the approach relative to all possible baselines is difficult to assess.

## Next Checks

1. **Implementation Verification**: Re-implement the InfoMTL framework using the specified hyperparameters and conduct an ablation study (e.g., β=0) to isolate the contribution of the SIMax principle and verify the reported increase in I(X; Z).

2. **Robustness Testing**: Evaluate the trained model on the provided adversarial perturbation sets and data-constrained subsets (e.g., 20% training data) to confirm the "noise-invariant" and "data-efficient" claims.

3. **Architectural Sensitivity**: Experiment with varying the hidden layer size and activation function in the task-specific MLP heads to assess the impact on model performance and determine if the default assumptions are optimal.