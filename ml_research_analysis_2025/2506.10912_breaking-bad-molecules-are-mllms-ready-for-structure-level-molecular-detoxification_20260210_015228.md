---
ver: rpa2
title: 'Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?'
arxiv_id: '2506.10912'
source_url: https://arxiv.org/abs/2506.10912
tags:
- toxicity
- molecular
- repair
- mllms
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToxiMol, the first benchmark for molecular
  toxicity repair using general-purpose multimodal large language models (MLLMs).
  It constructs a standardized dataset of 660 toxic molecules across 11 primary tasks
  and 660 representative toxic molecules spanning diverse mechanisms and granularities.
---

# Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?

## Quick Facts
- arXiv ID: 2506.10912
- Source URL: https://arxiv.org/abs/2506.10912
- Reference count: 40
- This paper introduces ToxiMol, the first benchmark for molecular toxicity repair using general-purpose multimodal large language models (MLLMs).

## Executive Summary
This paper introduces ToxiMol, the first benchmark for molecular toxicity repair using general-purpose multimodal large language models (MLLMs). It constructs a standardized dataset of 660 toxic molecules across 11 primary tasks and 660 representative toxic molecules spanning diverse mechanisms and granularities. A prompt annotation pipeline with mechanism-aware and task-adaptive capabilities is designed, informed by expert toxicological knowledge. An automated evaluation framework, ToxiEval, integrates toxicity endpoint prediction, synthetic accessibility, drug-likeness, and structural similarity into a high-throughput assessment chain. The study evaluates 43 mainstream MLLMs and conducts multiple ablation studies to analyze key issues. Results show that while current MLLMs face significant challenges, they begin to demonstrate promising capabilities in toxicity understanding, semantic constraint adherence, and structure-aware editing.

## Method Summary
The paper introduces ToxiMol, a benchmark for molecular toxicity repair using MLLMs. The method involves: (1) constructing a standardized dataset of 660 toxic molecules across 11 tasks using structure-aware representative sampling; (2) designing a prompt annotation pipeline that injects task-level and subtask-specific instructions into base templates; (3) implementing ToxiEval, an automated evaluation framework that integrates safety scoring, drug-likeness, synthetic accessibility, and structural similarity into a conjunctive decision protocol; and (4) evaluating 43 mainstream MLLMs on this benchmark with temperature=0.7, considering a sample successful if at least one of three generated candidates passes all criteria.

## Key Results
- Current MLLMs achieve low success rates (<10%) on most toxicity repair tasks, with 0% success on LD50 and DILI tasks
- Multimodal inputs (SMILES + 2D image) provide marginal benefits over text-only inputs
- Type-T failures (valid structures that remain toxic) dominate for hard endpoints like LD50 and DILI
- Increasing candidate count or model scale does not resolve the 0% success rate for the most challenging tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A strict conjunctive evaluation protocol is necessary to differentiate valid molecular "repairs" from structural hallucinations or toxicity label flips that compromise drug-likeness.
- **Mechanism:** The ToxiEval framework functions as a multi-stage filter. First, it parses generated SMILES strings for structural validity using RDKit. Second, it applies a hard threshold logic (AND-gate) across five dimensions: Safety ($S_{safe}$), Drug-likeness ($Q$), Synthetic Accessibility ($S_{sas}$), Rule-of-Five ($V_{ro5}$), and Structural Similarity ($S_{sim}$). A candidate is considered a "Success" only if it satisfies all constraints simultaneously.
- **Core assumption:** The paper assumes that minimizing toxicity while preserving scaffold similarity ($S_{sim} \geq 0.4$) and synthetic feasibility ($S_{sas} \leq 6$) is the correct operational definition of "repair" for drug discovery, rather than just binary toxicity classification.
- **Evidence anchors:**
  - [section] Section 3.3 defines the decision protocol: "a generated molecule is deemed successfully repaired only if all criteria are satisfied simultaneously."
  - [abstract] Mentions ToxiEval integrates "toxicity endpoint prediction, synthetic accessibility, drug-likeness, and structural similarity."
  - [corpus] Weak direct support; neighboring papers focus on prediction or token detoxification, not the multi-objective repair constraints defined here.
- **Break condition:** If the generated SMILES string is syntactically invalid (RDKit parse failure), the mechanism breaks immediately, filtering the candidate before property calculation.

### Mechanism 2
- **Claim:** Standard generic prompts are insufficient for complex toxicity mechanisms; mechanism-aware annotations are required to align the MLLM with specific biological repair objectives.
- **Mechanism:** The Prompt Annotation Pipeline ($A_{prompt}$) dynamically assembles inputs by injecting task-level ($P_{task}$) and subtask-specific ($P_{subtask}$) instructions into a base template ($P_{base}$). This explicitly informs the model about the specific toxicity mechanism (e.g., "hERG blockade" vs. "mutagenicity") and structural constraints.
- **Core assumption:** MLLMs possess latent knowledge of toxicophores but require explicit semantic steering via text and visual inputs to activate this knowledge for structure editing.
- **Evidence anchors:**
  - [section] Section 3.2 describes the annotator $A_{prompt}: (T, S_{raw}) \to P$ and the assembly of $P_i$.
  - [appendix] Appendix D provides examples of the prompt template structure (Role, Task Overview, Output Format).
  - [corpus] No direct corpus support for this specific prompt engineering strategy in molecular repair.
- **Break condition:** If the prompt fails to enforce the output format (e.g., model generates explanation text instead of semicolon-separated SMILES), the parsing of candidates fails.

### Mechanism 3
- **Claim:** Evaluating MLLMs on a uniform sample of clustered molecules prevents bias toward large datasets and ensures representative coverage of chemical space.
- **Mechanism:** The dataset construction uses Butina clustering on ECFP4 fingerprints to identify structural clusters. It then performs structure-aware representative sampling to select 60 molecules per task, ensuring the 660-sample benchmark is balanced and diverse.
- **Core assumption:** Toxicity repair capability generalizes across a chemical space represented by these centroids; performance on this subset is indicative of broader capability.
- **Evidence anchors:**
  - [section] Section 3.1 details the "structure-aware representative sampling strategy" and the choice of $n=60$.
  - [figure] Figure 2 visualizes the sampling statistics and logarithmic scale of clusters.
  - [corpus] Neighbor papers discuss substructure awareness, supporting the validity of fingerprint-based clustering.
- **Break condition:** If the model memorizes specific clusters in training data, the "representative" nature of the benchmark is compromised (data leakage), though the paper acknowledges this as a structural limitation.

## Foundational Learning

- **Concept: SMILES (Simplified Molecular Input Line Entry System)**
  - **Why needed here:** This is the primary language interface. The MLLM must generate syntactically valid SMILES (e.g., matching parentheses, valid ring closures) that can be parsed into a molecular graph.
  - **Quick check question:** Can you identify the error in a SMILES string like `C1CCN` (unclosed ring)?

- **Concept: Toxicity Mechanisms (hERG vs. AMES)**
  - **Why needed here:** The benchmark distinguishes 11 tasks. Understanding that hERG relates to cardiotoxicity (channel blockade) while AMES relates to mutagenicity (DNA damage) is essential to interpret why a repair strategy might work for one and fail for another.
  - **Quick check question:** Why would modifying a planar aromatic ring reduce mutagenicity (AMES) but might not affect a specific receptor binding (hERG)?

- **Concept: Drug-Likeness (QED & Lipinski's Rule of Five)**
  - **Why needed here:** Repairing toxicity is useless if the resulting molecule cannot function as a drug. The ToxiEval framework explicitly penalizes repairs that violate Lipinski's rules or reduce the Quantitative Estimate of Drug-likeness (QED).
  - **Quick check question:** If a model adds a large hydrophobic chain to hide a toxicophore, which drug-likeness metric (LogP or Molecular Weight) is most likely to break?

## Architecture Onboarding

- **Component map:**
  Input (Toxic Molecule) -> Prompt Annotator -> Generator (MLLM) -> ToxiEval (Validator + Predictor + Metrics) -> Decision Logic (AND-gate)

- **Critical path:** The primary bottleneck is the **Safety Score ($S_{safe}$)**. The paper identifies "Type-T" (Toxicity Bottleneck) failures as the dominant issue for hard endpoints (LD50, DILI), meaning models often generate valid, drug-like molecules that are still toxic.

- **Design tradeoffs:**
  * **Oracle Choice:** Using TxGemma (an LLM) as the toxicity oracle allows unified evaluation across 11 tasks but introduces "model-as-judge" bias. The paper argues this ensures reproducibility over using heterogeneous wet-lab data or disparate QSAR models.
  * **Strictness:** The "any-candidate-succeeds" rule (1 of 3 passes) makes the benchmark easier than requiring all candidates to pass, but still yields low success rates (<10% for hard tasks).

- **Failure signatures:**
  * **Type-T:** Valid SMILES, passes QED/SAS, but *fails* toxicity prediction (still toxic).
  * **Type-O:** Valid SMILES, passes toxicity, but *fails* drug-likeness (e.g., too complex to synthesize or poor solubility).
  * **Hallucination:** Invalid SMILES syntax (e.g., `Can't kekulize mol` errors in RDKit).

- **First 3 experiments:**
  1. **Baseline Validation:** Run the ToxiEval pipeline on the ground-truth "toxic" inputs to confirm the TxGemma oracle correctly flags them as toxic (verify $S_{safe}=0$).
  2. **Modality Ablation:** Compare text-only (SMILES only) vs. multimodal (SMILES + Image) inputs on a mid-tier model (e.g., GPT-4.1) to quantify the visual contribution (referencing Appendix C.4).
  3. **Failure Mode Analysis:** Generate repairs for the "hERG" task using a top model (e.g., Claude Opus 4.5) and sort failures into Type-T vs. Type-O to determine if the model lacks chemical knowledge (Type-T) or structural constraint adherence (Type-O).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do MLLM-generated repairs that pass the ToxiEval computational pipeline correlate with positive outcomes in wet-lab toxicity assays?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "We do not equate the outputs of this surrogate predictor with definitive real-world toxicological correctness. Reliable toxicity conclusions still require wet-lab validation and multi-source evidence."
- **Why unresolved:** The ToxiMol benchmark relies entirely on *in silico* evaluation (TxGemma-Predict) due to the high cost and complexity of experimental validation. It is currently unknown if structurally valid molecules generated by MLLMs to satisfy a computational oracle actually reduce toxicity in biological systems.
- **What evidence would resolve it:** A study validating a sample of MLLM-generated "detoxified" molecules through *in vitro* or *in vivo* experiments to measure the correlation between ToxiEval scores and actual biological toxicity reduction.

### Open Question 2
- **Question:** How can MLLMs be improved to handle the specific reasoning required for continuous dose-response relationships (LD50) and complex metabolic mechanisms (DILI)?
- **Basis in paper:** [inferred] The paper notes that "the success rates on the LD50 and DILI tasks are consistently very low." It attributes this to LD50 requiring explicit accounting for "continuous doseâ€“response relationships" and DILI involving complex "hepatic metabolism and systemic toxicity," suggesting current models lack the specific representational capacity or training data for these mechanisms.
- **Why unresolved:** The ablation study shows increasing candidate counts or model scale does not resolve the 0% success rate for LD50, indicating an intrinsic capability gap in the models rather than a simple resource issue.
- **What evidence would resolve it:** Demonstrating a model architecture or fine-tuning approach that achieves non-zero success rates on the LD50 and DILI tasks by explicitly integrating dose-response data or metabolic pathway knowledge into the generation process.

### Open Question 3
- **Question:** Can the ToxiMol benchmark and MLLM repair capabilities be effectively extended to macromolecular therapeutics and dose/metabolism-aware contexts?
- **Basis in paper:** [explicit] The authors explicitly state in the Conclusion and Limitations: "Future extensions may incorporate dose- and metabolism-aware constraints. The method may also move beyond small molecules to macromolecular therapeutics."
- **Why unresolved:** The current benchmark is strictly limited to small molecules and static endpoint predictions. Macromolecules (e.g., proteins, antibodies) require different structural representations and toxicity mechanisms that current general-purpose MLLMs are not designed to handle.
- **What evidence would resolve it:** The development of a benchmark extension including macromolecule structures and the demonstration of an MLLM successfully generating structural modifications for larger bio-therapeutics that satisfy multi-criteria evaluations.

## Limitations
- The evaluation framework relies on a single toxicity oracle (TxGemma-27B), which introduces potential model bias without validation against wet-lab data or multiple independent toxicity models.
- The benchmark's 660-molecule subset, while carefully constructed through clustering, may not fully capture the chemical space complexity of real-world drug discovery.
- The strict conjunctive evaluation protocol may be overly restrictive for practical applications where partial success (e.g., reduced toxicity without full compliance) could still be valuable.

## Confidence
- **High Confidence:** The multi-stage evaluation protocol (ToxiEval) is clearly specified and reproducible. The structure-aware sampling methodology and prompt engineering pipeline are well-documented with transparent thresholds and decision logic.
- **Medium Confidence:** The claim that current MLLMs demonstrate "promising capabilities" in toxicity understanding is supported by the structured analysis but limited by the single-oracle approach and the benchmark's relatively small size compared to the full TDC dataset.
- **Low Confidence:** The assertion that MLLMs begin to show structure-aware editing capabilities is difficult to verify without access to the specific model versions used (GPT-5.2, Claude Opus 4.5) and their exact configurations at the time of the experiment.

## Next Checks
1. **Oracle Validation:** Run the ToxiEval pipeline on a small subset of molecules with known wet-lab toxicity data to quantify the accuracy of TxGemma-27B predictions against ground truth, measuring false positive/negative rates for different toxicity endpoints.
2. **Model Version Sensitivity:** Reproduce the top-performing model results (GPT-5.2, Claude Opus 4.5) using the current API versions to assess whether performance degradation occurs due to model updates, and if so, identify which toxicity mechanisms are most sensitive to model version changes.
3. **Partial Success Analysis:** Modify the conjunctive evaluation protocol to allow partial credit (e.g., success if either safety improves OR drug-likeness improves) and re-run experiments on 2-3 challenging tasks (LD50, DILI) to determine if this relaxation reveals more nuanced model capabilities that the strict AND-gate obscures.