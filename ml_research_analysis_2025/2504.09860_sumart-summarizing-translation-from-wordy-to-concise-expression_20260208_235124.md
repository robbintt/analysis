---
ver: rpa2
title: 'SUMART: SUMmARizing Translation from Wordy to Concise Expression'
arxiv_id: '2504.09860'
source_url: https://arxiv.org/abs/2504.09860
tags:
- sumart
- translation
- speaker
- subtitle
- translated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SUMART addresses the problem of high cognitive load and slow comprehension
  in real-time subtitle translation by compressing verbose translations. It uses a
  human-in-the-loop approach where an LLM compresses subtitles on-site, storing paired
  data for fine-tuning translation models to generate more concise outputs.
---

# SUMART: SUMmARizing Translation from Wordy to Concise Expression

## Quick Facts
- arXiv ID: 2504.09860
- Source URL: https://arxiv.org/abs/2504.09860
- Authors: Naoto Nishida; Jun Rekimoto
- Reference count: 11
- Primary result: Reduces cognitive load and comprehension time in real-time subtitle translation through compression

## Executive Summary
SUMART addresses the problem of high cognitive load and slow comprehension in real-time subtitle translation by compressing verbose translations. It uses a human-in-the-loop approach where an LLM compresses subtitles on-site, storing paired data for fine-tuning translation models to generate more concise outputs. The system aims to reduce information transmission time, theoretically saving up to 5 seconds per utterance by increasing reading speed through summarization. A prototype was built using AR glasses and evaluated qualitatively, showing user demand for summarization in fast/long speech scenarios like lectures and podcasts. The system employs ChatGPT for summarization due to its low latency, and future work includes integrating context and paralinguistic features for improved translation accuracy.

## Method Summary
SUMART implements a two-phase approach: (1) During training, audio input is processed through ASR, translation, and LLM summarization (ChatGPT with "Summarize this sentence: {input}" prompt), storing (ASR text, summarized translation) pairs in a database; (2) For practical use, a translation model fine-tuned on these pairs directly outputs concise translations without runtime summarization. The prototype uses Nreal Light AR glasses, Unity/C# client, Python server, Microsoft Azure APIs for ASR/translation, and ZMQ sockets for communication.

## Key Results
- Theoretically saves up to 5 seconds per utterance by increasing reading speed through summarization
- User study shows demand for summarization in fast/long speech scenarios like lectures and podcasts
- ChatGPT selected for summarization due to acceptable latency (~2.45s) despite slower alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Summarization reduces total information transmission time by decreasing the reading burden.
- Mechanism: A compression rate (σ) reduces the word count a user must read. Since total time = reading time + processing time + translation time + summarization time, reducing word count directly reduces the reading component (ε × wc).
- Core assumption: Users can comprehend summarized content with equivalent accuracy to full translations for "big-picture" understanding.
- Evidence anchors:
  - [abstract] "theoretically saving up to 5 seconds per utterance by increasing reading speed through summarization"
  - [section 3.1] Formula shows ε range and max reduction: "20 ∗ (ε_max − ε_min) ≃ 5.04... seconds"
- Break condition: If users require full verbatim detail (e.g., legal/medical contexts), compression degrades comprehension rather than improving efficiency.

### Mechanism 2
- Claim: Fine-tuning translation models on (verbose input, compressed output) pairs produces inherently concise translations without runtime summarization overhead.
- Mechanism: During training, the system stores ASR results paired with LLM-summarized translations. Fine-tuning on these pairs teaches the model to output compressed translations directly, eliminating t_sum in practical deployment.
- Core assumption: The mapping from verbose source to compressed target is learnable and generalizes across speakers/topics.
- Evidence anchors:
  - [abstract] "storing paired data for fine-tuning translation models to generate more concise outputs"
  - [section 3.2.1] "SUMART uses data pairs from those non-compressed ASR results and compressed translated results for fine-tuning"
- Break condition: If fine-tuning data is sparse or domain-mismatched, the model may over-compress (losing meaning) or revert to verbose outputs.

### Mechanism 3
- Claim: LLM-based summarization is viable for real-time subtitle compression due to acceptable latency.
- Mechanism: ChatGPT API summarizes translated subtitles on-site during training. Its latency (~2.45s average per Table 1) was selected as the best tradeoff among tested models for the summarization task.
- Core assumption: 2-3 second summarization latency is acceptable during training-phase data collection (not deployment).
- Evidence anchors:
  - [section 3.2.2] "ChatGPT API was selected due to its minimal latency in summarization tasks"
  - [Table 1] Shows ChatGPT execution time 2.45s (SD 0.350) with 30.4 output tokens
- Break condition: If latency exceeds user tolerance or if summarization quality degrades for technical/domain content, the training data quality suffers.

## Foundational Learning

- Concept: **Automatic Speech Recognition (ASR) Pipelines**
  - Why needed here: SUMART begins with ASR to transcribe speaker audio before translation and summarization.
  - Quick check question: Can you trace audio input → text output in a typical ASR API call?

- Concept: **Fine-tuning vs. Few-shot Prompting**
  - Why needed here: SUMART uses stored paired data to fine-tune translation models, distinct from prompting LLMs at inference.
  - Quick check question: What is the difference between updating model weights (fine-tuning) and providing examples in a prompt?

- Concept: **Reading Rate Constraints in HCI**
  - Why needed here: The theoretical time savings derive from reading speed (~238 wpm) versus speaking rate (~150 wpm).
  - Quick check question: Why does compressing word count help if speaking rate is the bottleneck in conversation turn-taking?

## Architecture Onboarding

- Component map: Audio input → Server (ASR → Translation → LLM Summarization) → Summarized subtitle output to listener + (ASR text, summarized translation) pair stored in database → Fine-tuned Translation Model → Concise subtitle output
- Critical path:
  1. Collect verbose ASR + compressed translation pairs during training
  2. Fine-tune translation model on these pairs
  3. Deploy fine-tuned model for inference (no runtime summarization needed)
- Design tradeoffs:
  - ChatGPT summarizer vs. smaller models (PEGASUS/T5): ChatGPT slower but produces higher-quality summaries for training data
  - Compression rate σ: Higher compression saves more time but risks information loss
  - Simple prompts vs. context-aware prompts: Current system uses basic prompts; future work suggests incorporating conversational context
- Failure signatures:
  - Over-compression: Summaries lose critical details (users misunderstand "big-picture")
  - Domain mismatch: Fine-tuned model fails on technical/specialized vocabulary not in training pairs
  - Latency bottleneck in training phase: If LLM summarization is too slow, data collection stalls
  - User rejection in casual conversation: Paper notes users felt summarization unnecessary for simple/euphemistic speech
- First 3 experiments:
  1. Replicate the summarization model comparison (Table 1) on your target domain corpus to validate ChatGPT selection.
  2. Collect 50-100 (ASR, summarized translation) pairs in a controlled setting; fine-tune a small translation model and measure compression rate vs. semantic preservation.
  3. Deploy the fine-tuned model in a Wizard-of-Oz AR subtitle setup; measure perceived comprehension and cognitive load against a non-compressed baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can paralinguistic features (e.g., facial expressions, voice tone) be effectively integrated to improve the accuracy of summarized translations?
- Basis in paper: [explicit] Authors note participant feedback suggesting that reflecting emotions and "way of saying" could lead to more precise results.
- Why unresolved: The current prototype relies on simple text prompts and does not process multimodal emotional cues.

### Open Question 2
- Question: Does incorporating conversational context or user expertise into the summarization prompt significantly enhance the user's "big-picture" understanding?
- Basis in paper: [explicit] The paper lists "incorporat[ing] more of the conversational context or consider the expertise of the conversation partner" as a suggestion derived from the Prompt Principle.
- Why unresolved: The prototype utilized a simple prompt structure ("Summarize this sentence") without these adaptations.

### Open Question 3
- Question: Does the theoretical reduction in transmission time translate to a measurable decrease in cognitive load and conversation duration in real-world settings?
- Basis in paper: [inferred] The paper relies on a theoretical formula for time savings ($\approx$ 5 seconds) and a qualitative pilot study; it lacks quantitative validation of efficiency gains.
- Why unresolved: Theoretical calculations assume fixed reading speeds and do not capture the complexity of human cognitive processing during summarized reading.

## Limitations
- Lack of specified dataset size, language pairs, and compression rate thresholds used during data collection
- Missing technical details on translation model architecture and fine-tuning hyperparameters
- Evaluation is qualitative and based on a prototype with AR glasses, limiting quantitative validation

## Confidence
- High confidence in the core mechanism: reducing reading burden through summarization to improve comprehension speed
- Medium confidence in the feasibility of LLM-based summarization for real-time training data collection, supported by latency measurements
- Low confidence in the generalizability of fine-tuning results due to missing technical details and limited evaluation scope

## Next Checks
1. Replicate the summarization model comparison (Table 1) on your target domain corpus to validate ChatGPT selection.
2. Collect 50-100 (ASR, summarized translation) pairs in a controlled setting; fine-tune a small translation model and measure compression rate vs. semantic preservation.
3. Deploy the fine-tuned model in a Wizard-of-Oz AR subtitle setup; measure perceived comprehension and cognitive load against a non-compressed baseline.