---
ver: rpa2
title: Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces
arxiv_id: '2601.16907'
source_url: https://arxiv.org/abs/2601.16907
tags:
- similarity
- calibration
- human
- isotonic
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of miscalibration in cosine similarity
  scores from pretrained embedding spaces, where anisotropy causes systematic concentration
  of absolute values despite good rank correlation with human judgments. The proposed
  method applies isotonic regression to learn a monotonic transformation from raw
  cosine similarity to human-aligned scores, calibrated using human similarity judgments
  from the STS benchmark.
---

# Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces

## Quick Facts
- arXiv ID: 2601.16907
- Source URL: https://arxiv.org/abs/2601.16907
- Authors: Nicolas Tacheny
- Reference count: 14
- One-line primary result: Isotonic calibration achieves ECE ≈ 0 and MBE = 0 while preserving rank correlation and geometric properties in embedding similarity analysis.

## Executive Summary
This paper addresses the problem of miscalibration in cosine similarity scores from pretrained embedding spaces, where anisotropy causes systematic concentration of absolute values despite good rank correlation with human judgments. The proposed method applies isotonic regression to learn a monotonic transformation from raw cosine similarity to human-aligned scores, calibrated using human similarity judgments from the STS benchmark. The calibrated similarity achieves near-perfect alignment with human judgments (ECE ≈ 0, MBE = 0) while preserving rank correlation (Spearman ρ = 0.856) and local stability (98% across seven perturbation types). The paper proves that isotonic calibration preserves all order-based geometric constructions including angular ordering, nearest neighbors, threshold graphs and quantile-based decisions. This provides interpretable absolute similarity values without modifying the embedding space or compromising geometric analysis.

## Method Summary
The method computes raw cosine similarity for sentence pairs using pretrained embeddings, then fits isotonic regression to map these raw scores to human similarity scores from the STS benchmark. The calibration function is learned on STS-train data and evaluated on STS-test data. The calibrated similarity is obtained by applying the isotonic transformation to raw cosine values and clamping outputs to [0, 1]. A high-confidence similarity threshold is derived from human judgment distributions, providing statistically grounded decision boundaries that preserve probabilistic guarantees under calibration.

## Key Results
- Isotonic calibration achieves ECE ≈ 0 and MBE = 0 on STS-test while maintaining Spearman ρ ≈ 0.856
- Local stability remains at 98% across seven perturbation types including synonym substitution and sentence permutation
- Formal proof that isotonic calibration preserves all order-based geometric constructions (angular ordering, nearest neighbors, threshold graphs, quantile-based decisions)
- High-confidence similarity threshold τ̃_HCS ≈ 0.65 provides interpretable decision boundaries with preserved probabilistic guarantees

## Why This Works (Mechanism)

### Mechanism 1
Isotonic calibration preserves all order-based geometric constructions while restoring interpretability of absolute similarity values. Isotonic regression learns a monotonic (non-decreasing) piecewise-constant function that maps raw cosine scores to human-aligned scores. Since the transformation is order-preserving, if s(x,y) ≥ s(x,z), then f(s(x,y)) ≥ f(s(x,z)). This guarantees invariance of nearest neighbors, angular ordering, threshold graphs, and quantile-based decisions. The core assumption is that rank correlation between model and human judgments is already strong (ρ ≈ 0.84); only absolute values need correction.

### Mechanism 2
Anisotropy causes systematic concentration of similarity scores in a narrow high-value band, regardless of semantic relatedness. Pretrained embeddings cluster around a dominant mean direction in ℝ^d rather than distributing uniformly over the hypersphere. This violates the uniformity property from alignment-uniformity analysis. Theoretical isotropic baseline has similarity centered at 0; empirically, the model's distribution centers around 0.8. The assumption is that the concentration pattern is consistent enough across pairs that a single global calibration function suffices.

### Mechanism 3
High-confidence similarity thresholds derived from human judgment distributions provide statistically grounded decision boundaries with preserved probabilistic guarantees under calibration. Define τ_HCS(s) = Q₀.₀₅(s | s^(h) > 0.9)—the 5th quantile of similarity scores for pairs humans rated as highly similar. This yields P(s ≥ τ_HCS | s^(h) > 0.9) ≥ 0.95. Under isotonic calibration, τ̃_HCS = f_isotonic(τ_HCS), preserving the guarantee. The assumption is that human similarity judgments from STS Benchmark generalize to target application domains.

## Foundational Learning

- **Anisotropy in embedding spaces**: Understanding why raw cosine values cluster is essential for motivating calibration over ranking metrics. Quick check: On a unit hypersphere in high dimensions, where would isotropic vectors concentrate their pairwise cosine values? (Answer: near 0)

- **Isotonic regression**: This is the core algorithm; understanding its piecewise-constant nature explains both its calibration power and its discontinuity limitations. Quick check: What constraint does isotonic regression impose that polynomial regression does not? (Answer: monotonicity—outputs must be non-decreasing with inputs)

- **Expected Calibration Error (ECE)**: Primary metric for evaluating whether absolute values align with ground truth; distinguishes calibration from correlation. Quick check: If ECE = 0 but RMSE > 0, what does this imply about the model? (Answer: Perfect calibration—predicted values match empirical frequencies—but residual variance remains)

## Architecture Onboarding

- **Component map**: Text pairs (a₁, a₂) → Normalized embeddings via ψ: A → S^(d-1) → Raw similarity: s^(m)(e₁, e₂) = ⟨e₁, e₂⟩ → Calibration function: f_isotonic learned via sklearn.isotonic_regression on (s^(m), s^(h)) pairs from STS-train → Calibrated similarity: s̃ = f_isotonic ∘ s^(m), clamped to [0, 1] → Threshold: τ̃_HCS ≈ 0.65 for high-confidence similarity decisions

- **Critical path**: 1. Load pretrained sentence encoder (e.g., paraphrase-mpnet-base-v2) 2. Compute normalized embeddings for calibration dataset 3. Fit isotonic regression mapping raw cosine → human scores 4. Store fitted f_isotonic for inference 5. Apply to new pairs: raw cosine → calibrated score → compare to τ̃_HCS

- **Design tradeoffs**: Isotonic vs. polynomial: Isotonic achieves ECE ≈ 0 but is piecewise-constant (discontinuous); polynomial-3/4 provides smooth gradients but ECE ≈ 0.006. Choose isotonic for threshold decisions; polynomial if smooth derivatives required. STS Benchmark vs. domain-specific calibration: STS provides human-aligned ground truth but may not generalize. Consider domain-specific judgments if available.

- **Failure signatures**: Stability degradation near discontinuities: SYNONYM_SUBSTITUTION stability dropped from 0.95 → 0.87 due to step-function boundaries. Monitor perturbation pairs near threshold boundaries. Domain shift: If calibrated scores systematically misalign with domain expert judgments, recalibrate on domain-specific data. Negative similarity handling: Raw cosine ranges [-1, 1]; calibrated outputs clamped to [0, 1]. Rare negative values collapse to 0.

- **First 3 experiments**: 1. Replicate calibration metrics: Load STS-train, compute raw cosine similarities, fit isotonic regression, verify ECE ≈ 0 and MBE = 0 on held-out STS-test split. 2. Stability stress test: Generate 100 synonym-substitution pairs in your domain, compare stability rates before/after calibration; if degradation > 5%, investigate discontinuity regions. 3. Threshold validation: For a labeled binary similarity task (similar/dissimilar), compare decision accuracy using raw τ_HCS ≈ 0.72 vs. calibrated τ̃_HCS ≈ 0.65. Report precision/recall tradeoffs.

## Open Questions the Paper Calls Out

- **Domain-specific calibration**: Does isotonic calibration trained on general-domain STS data transfer effectively to specialized domains such as legal, medical, or scientific text? The paper identifies domain-specific calibration as a future direction, noting calibration may degrade for domains significantly different from the training distribution. Train calibration on domain-specific similarity judgments and evaluate whether ECE and MBE remain near-zero compared to general-purpose calibration.

- **Smooth calibration alternatives**: Can smooth monotonic calibration methods (splines, monotonic neural networks) achieve comparable calibration performance to isotonic regression while maintaining continuous gradients? The paper notes that for applications requiring smooth gradients, polynomial calibration may be preferable despite lower performance, and calls for investigating smooth alternatives.

- **Cross-model calibration**: Does calibration trained on one embedding model transfer effectively to other pretrained models, or is model-specific recalibration necessary? Different models may have distinct anisotropy patterns and similarity distributions, potentially requiring separate calibration functions. Train isotonic calibration on one model's STS predictions and evaluate ECE, MBE, and stability when applied to embeddings from other models.

## Limitations

- **In-sample evaluation**: Results are reported on STS-train data, making them in-sample. Out-of-distribution validation would strengthen claims about calibration generalization.

- **Domain dependency**: The calibration relies entirely on STS Benchmark human judgments, which may not represent specialized domains. No evidence demonstrates threshold generalization across domains.

- **Discontinuity limitations**: Isotonic regression produces piecewise-constant mappings with discontinuities, problematic for optimization. While stability is high (98%), the 2% failure rate concentrates near boundaries.

## Confidence

**High confidence**: Isotonic regression preserves order-based geometric constructions (angular ordering, nearest neighbors, threshold graphs, quantiles). The mathematical proof is rigorous and the corollaries directly follow.

**Medium confidence**: The calibration achieves near-perfect alignment (ECE ≈ 0, MBE = 0) while preserving rank correlation (Spearman ρ = 0.856) and stability (98%). These metrics are reported on the training set, and the ChatGPT-generated perturbation dataset is not publicly available for independent verification.

**Low confidence**: The high-confidence similarity threshold τ̃_HCS = 0.65 generalizes across domains. No cross-domain validation is provided, and threshold-based decisions are application-specific.

## Next Checks

1. **Domain transfer experiment**: Calibrate on STS data, then evaluate on a domain-specific similarity judgment dataset (e.g., medical or legal text pairs). Compare ECE and decision accuracy against domain-specific recalibration.

2. **Polynomial vs. isotonic trade-off**: For an application requiring gradient-based optimization, compare calibration methods on both ECE (calibration quality) and smoothness (gradient continuity). Identify break-even points where polynomial calibration's smoothness outweighs isotonic's calibration advantage.

3. **Threshold robustness analysis**: Systematically vary the high-confidence threshold across applications (e.g., information retrieval, duplicate detection, clustering). Report precision-recall curves and identify application-specific optimal thresholds beyond the generic τ̃_HCS = 0.65.