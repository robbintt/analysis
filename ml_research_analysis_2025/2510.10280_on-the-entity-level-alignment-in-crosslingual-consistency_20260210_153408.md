---
ver: rpa2
title: On the Entity-Level Alignment in Crosslingual Consistency
arxiv_id: '2510.10280'
source_url: https://arxiv.org/abs/2510.10280
tags:
- latn
- language
- rank
- alignment
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates why multilingual language models often\
  \ produce inconsistent factual answers across languages. It hypothesizes that such\
  \ inconsistencies stem from failures in entity alignment\u2014the mapping of subject\
  \ and object entities into a shared conceptual space across languages."
---

# On the Entity-Level Alignment in Crosslingual Consistency

## Quick Facts
- **arXiv ID:** 2510.10280
- **Source URL:** https://arxiv.org/abs/2510.10280
- **Reference count:** 40
- **Primary result:** Simple prompting methods (SUBSUB, SUBINJ) significantly improve both factual recall accuracy and crosslingual consistency in multilingual LLMs by enhancing entity alignment

## Executive Summary
This paper investigates why multilingual language models produce inconsistent factual answers across languages, hypothesizing that entity misalignment between languages causes these inconsistencies. The authors introduce an entity-level translation task and find strong correlation between entity alignment and crosslingual consistency. They propose two prompting methods—SUBSUB (replacing subjects with English translations) and SUBINJ (appending English subjects)—that substantially improve factual recall accuracy and consistency, especially in English-centric models. Mechanistic analysis via Logit Lens reveals these interventions enhance entity representation alignment in conceptual space, leading to more consistent multilingual factual predictions.

## Method Summary
The paper evaluates entity-level alignment and crosslingual factual consistency across 17 languages using the KLAR dataset (2,619 facts, 20 relation types). The entity translation task measures alignment accuracy (ACCsub, ACCobj, ACCboth) by checking if translated entities match ground truth. Factual recall accuracy is measured per language using 3-shot prompting, while crosslingual consistency is evaluated using Jaccard index across all 136 language pairs. The authors propose SUBSUB (replacing subjects with English translations) and SUBINJ (appending English subjects) as prompting interventions, analyzing their effects through correlation studies and mechanistic Logit Lens analysis.

## Key Results
- Strong correlation between entity alignment accuracy and crosslingual consistency (Pearson coefficient)
- SUBSUB and SUBINJ prompting methods substantially improve factual recall accuracy and crosslingual consistency
- Entity misalignment frequently causes crosslingual inconsistencies
- Logit Lens analysis shows enhanced entity representation alignment in conceptual space after interventions

## Why This Works (Mechanism)
Entity misalignment across languages creates conceptual space mismatches that lead to inconsistent factual predictions. When models fail to properly align subject and object entities between languages, they generate factually inconsistent answers despite having correct information in individual languages. The SUBSUB and SUBINJ interventions work by anchoring entities to their English representations, which serves as a common reference point that improves alignment in the conceptual space where the model processes information.

## Foundational Learning
- **Entity alignment**: Mapping entities across languages into shared conceptual space - needed to understand crosslingual consistency failures; quick check: can the model translate entities between languages?
- **Crosslingual consistency**: Measuring agreement between factual answers across language pairs - needed to quantify the problem being addressed; quick check: do different language queries about the same fact yield the same answer?
- **3-shot prompting**: Using three demonstration examples to guide model generation - needed for both evaluation tasks; quick check: are demonstrations representative of the task distribution?
- **Jaccard consistency index**: Measuring overlap between correct answers across languages - needed to quantify consistency; quick check: is intersection over union correctly computed?
- **Logit Lens analysis**: Examining model internal representations at different layers - needed to understand intervention mechanisms; quick check: do representations converge after prompting interventions?

## Architecture Onboarding

**Component map:** KLAR dataset -> Entity translation task -> Factual recall task -> Consistency measurement -> Prompting interventions (SUBSUB, SUBINJ) -> Logit Lens analysis

**Critical path:** Input fact → 3-shot prompt → Model generation → String matching evaluation → Correlation analysis

**Design tradeoffs:** Simple prompting interventions vs. complex alignment training; inference-only approach vs. fine-tuning; string-level matching vs. semantic similarity

**Failure signatures:** Low correlation values indicate poor entity alignment; minimal improvement from SUBSUB/SUBINJ suggests model isn't English-centric; inconsistent Jaccard scores across language pairs reveal alignment problems

**First experiments:** 1) Verify entity translation task implementation with sample facts; 2) Test baseline factual recall accuracy per language; 3) Apply SUBSUB intervention to measure immediate impact

## Open Questions the Paper Calls Out
None

## Limitations
- KLAR dataset modifications from BMLAMA17 are not detailed, creating potential reproducibility gaps
- Only 3-shot prompting is evaluated without ablation studies on shot count
- Focuses exclusively on autoregressive models without examining encoder-decoder or retrieval-augmented approaches

## Confidence
- **High confidence:** Entity alignment task methodology and correlation with consistency metrics are sound
- **Medium confidence:** SUBSUB and SUBINJ prompting mechanisms are effective but generalizability beyond tested models is unclear
- **Medium confidence:** Logit Lens analysis shows meaningful changes but deeper probing could strengthen causal claims

## Next Checks
1. Verify exact prompt templates for all 20 relation types to ensure faithful reproduction
2. Test prompting interventions on non-English-centric multilingual models (BLOOMZ, XGLM) to assess generalizability
3. Conduct ablation studies varying demonstration shot counts to understand sensitivity