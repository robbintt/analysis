---
ver: rpa2
title: Prompt Repetition Improves Non-Reasoning LLMs
arxiv_id: '2512.14982'
source_url: https://arxiv.org/abs/2512.14982
tags:
- prompt
- repetition
- query
- answer
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Prompt repetition improves performance of non-reasoning LLMs by
  repeating the input prompt, enabling each token to attend to every other token.
  Testing on popular models (Gemini, GPT, Claude, Deepseek) across 7 benchmarks showed
  that prompt repetition wins 47 out of 70 benchmark-model combinations with 0 losses,
  significantly improving accuracy without increasing generated token length or latency.
---

# Prompt Repetition Improves Non-Reasoning LLMs

## Quick Facts
- arXiv ID: 2512.14982
- Source URL: https://arxiv.org/abs/2512.14982
- Authors: Yaniv Leviathan; Matan Kalman; Yossi Matias
- Reference count: 15
- Primary result: Prompt repetition improves accuracy on 47/70 benchmark-model combinations without increasing latency or output length

## Executive Summary
This paper introduces prompt repetition, a simple yet effective technique that improves the performance of non-reasoning large language models by repeating the input prompt. The method works by enabling bidirectional attention within prompt content in causal language models, where tokens in the second copy can attend to all tokens in the first copy. Tested across 7 popular models (Gemini, GPT, Claude, Deepseek) and 7 benchmarks, prompt repetition won 47 out of 70 benchmark-model combinations with zero losses, significantly improving accuracy while maintaining efficiency by only affecting the parallelizable prefill stage.

## Method Summary
The method transforms input from "<QUERY>" to "<QUERY><QUERY>" by duplicating the entire prompt content. This approach was tested with variants including Verbose (adding "Let me repeat that:" between copies) and triple repetition (×3). The technique was evaluated on 7 models including Gemini 2.0 Flash/Lite, GPT-4o/4o-mini, Claude 3 Haiku/3.7 Sonnet, and Deepseek V3 across 7 benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU-Pro, MATH, NameIndex, MiddleMatch) using APIs in Feb-Mar 2025. Performance was measured using accuracy with McNemar tests (p<0.1 for significance), generated token length, and latency.

## Key Results
- Prompt repetition wins 47 out of 70 benchmark-model combinations with 0 losses
- No increase in generated token length or measured latency for most models
- Improvements are larger for options-first prompt formats compared to question-first formats
- When reasoning is enabled, results are neutral to slightly positive (5 wins, 1 loss, 22 ties)
- Outperforms padding methods and variants like verbose repetition and triple repetition on custom tasks

## Why This Works (Mechanism)

### Mechanism 1
Repeating the prompt enables bidirectional attention within prompt content in causal language models. When the prompt is repeated as "<QUERY><QUERY>", tokens in the second copy can attend to all tokens in the first copy, effectively giving each prompt token visibility to every other prompt token regardless of original ordering. This works because LLMs are trained as causal language models where past tokens cannot attend to future tokens, so order affects prediction performance.

### Mechanism 2
Prompt repetition mitigates order-dependent performance degradation without changing output format. By duplicating the query, information that appeared late (and thus had limited context) in the original ordering gets an earlier position in the second copy, reducing the disadvantage of "options-first" vs "question-first" prompt structures. This addresses the known issue that queries like "<CONTEXT> <QUESTION>" often perform differently from "<QUESTION> <CONTEXT>".

### Mechanism 3
Efficiency gains come from confining changes to the parallelizable prefill stage. Prompt repetition doubles input tokens but does not increase output tokens. Since prefill is fully parallelizable on modern hardware, added compute does not translate to proportional latency increases. Only the parallelizable prefill stage is affected, while the sequential decode stage remains unchanged.

## Foundational Learning

- Concept: Causal vs bidirectional attention in Transformers
  - Why needed here: Understanding why token order matters in LLMs and why repetition changes attention dynamics
  - Quick check question: In a causal LM, can token 5 attend to token 10? What changes if the sequence is repeated?

- Concept: Prefill vs decode stages in LLM inference
  - Why needed here: Explains why doubling input tokens doesn't double latency—the prefill stage is parallelized, decode is sequential
  - Quick check question: Which stage involves generating tokens one at a time, and which processes all prompt tokens simultaneously?

- Concept: Position bias in LLM prompting
  - Why needed here: Motivates why "question-first" vs "options-first" yields different accuracy, and why repetition helps
  - Quick check question: If you present answer choices before the question, why might an LLM perform worse?

## Architecture Onboarding

- Component map:
  Input prompt → Prompt duplication module → Tokenizer (processes doubled input) → Prefill stage (parallel attention over all tokens) → KV-cache (stores key/value pairs for both copies) → Decode stage (generates response, unchanged)

- Critical path:
  1. Identify prompt boundaries (what to repeat)
  2. Duplicate prompt content before tokenization
  3. Ensure KV-cache retains both copies (don't deduplicate)
  4. Monitor for context-length overflow on long prompts

- Design tradeoffs:
  - Doubling prompt length may approach context limits for long inputs
  - Minor latency increase possible for very long prompts (observed in Claude models)
  - Verbose repetition (adding "Let me repeat that:") may perform differently than raw duplication—paper suggests similar performance, but more research needed
  - Triple repetition shows stronger gains on some custom tasks but increases length further

- Failure signatures:
  - No accuracy gain on tasks that are already order-insensitive
  - Latency spikes on very long prompts (especially for models with less efficient prefill)
  - Context-length errors if original prompt is already near the limit
  - Neutral or negative results when reasoning/CoT is enabled (paper shows 1 loss in this setting)

- First 3 experiments:
  1. Baseline comparison: Run a standard multiple-choice benchmark (e.g., MMLU-Pro subset) with and without prompt repetition, measuring accuracy and latency. Expect accuracy gains, stable latency.
  2. Order sensitivity test: Compare "question-first" vs "options-first" prompts with and without repetition. Expect repetition to close the gap.
  3. Long-prompt stress test: Test on prompts approaching 50% of context window to identify latency or context-overflow edge cases before production deployment.

## Open Questions the Paper Calls Out

### Open Question 1
How do token representations vary between repetitions, and what attention mechanisms explain when repetition is helpful versus neutral? The paper demonstrates empirical gains but does not analyze the underlying attention patterns or representation dynamics. What evidence would resolve it: Attention head visualization showing cross-repetition attention weights; probing tasks measuring representation quality across positions.

### Open Question 2
Does prompt repetition transfer effectively to non-text modalities such as images or multimodal inputs? All experiments were text-only; vision-language models may process repeated visual tokens differently. What evidence would resolve it: Experiments with vision-language models (e.g., GPT-4V, Gemini Vision) using repeated image embeddings.

### Open Question 3
At what prompt lengths does repetition become latency-prohibitive, and can selective partial repetition mitigate this? The paper notes latency increases for Anthropic models on long requests and states repetition "might be impossible for very long ones," but thresholds are uncharacterized. What evidence would resolve it: Systematic latency profiling across prompt lengths; ablations repeating different prompt subsets.

### Open Question 4
How should prompt repetition be adapted for multi-turn conversational settings? All experiments are single-turn; context accumulation across turns introduces different constraints. What evidence would resolve it: Multi-turn dialogue benchmarks testing strategies for repeating full versus partial conversation history.

## Limitations

- Prompt template specification: The paper demonstrates the method on multiple-choice benchmarks with fixed templates, but does not provide complete prompt templates for all 7 benchmarks, creating ambiguity about exact implementation details.
- API parameter standardization: Temperature, max tokens, and other decoding parameters are not explicitly documented across all model/benchmark combinations, which could affect reproducibility.
- Generalizability to non-causal models: The method is designed for causal LMs but may not apply to models with bidirectional attention or different attention mechanisms.

## Confidence

- High Confidence: The empirical observation that prompt repetition improves accuracy on non-reasoning LLMs (47/70 benchmark-model combinations showing improvement with 0 losses)
- Medium Confidence: The causal mechanism explanation (that repetition enables bidirectional attention within prompt content)
- Medium Confidence: The efficiency claim that doubling input tokens does not increase latency

## Next Checks

1. Run attention visualization on the same prompts with and without repetition to empirically confirm that tokens in the second copy attend to all tokens in the first copy, and that this differs from the baseline attention patterns.

2. Systematically test prompts with reversed token orders (question-first vs options-first) across multiple tasks to measure the magnitude of order effects and how repetition mitigates them, comparing results to the paper's observations.

3. Conduct experiments with prompts approaching 50-70% of the context window to identify at what point latency increases become significant or context-overflow errors occur, particularly for Claude models which showed exceptions in the paper.