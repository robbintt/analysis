---
ver: rpa2
title: Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching
arxiv_id: '2506.04579'
source_url: https://arxiv.org/abs/2506.04579
tags:
- demonstrations
- learning
- random
- shot
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of demonstration selection for
  many-shot in-context learning (ICL), where existing methods rely on random selection
  despite suboptimal performance. The authors propose a gradient matching approach
  that aligns fine-tuning gradients between the entire training set and selected examples,
  leveraging the hypothesis that data requirements for ICL and fine-tuning are analogous.
---

# Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching

## Quick Facts
- arXiv ID: 2506.04579
- Source URL: https://arxiv.org/abs/2506.04579
- Reference count: 25
- One-line primary result: Gradient matching approach improves 128-shot ICL performance by 4% on open models and 2% on closed-source LLMs by selecting demonstrations that mimic full training set learning dynamics.

## Executive Summary
This paper addresses the challenge of demonstration selection for many-shot in-context learning (ICL), where existing methods rely on random selection despite suboptimal performance. The authors propose a gradient matching approach that aligns fine-tuning gradients between the entire training set and selected examples, leveraging the hypothesis that data requirements for ICL and fine-tuning are analogous. By computing curriculum latent gradients on small models (e.g., Qwen2.5-3B or Llama3-8B), the method selects n-shot demonstrations that mimic the learning effect of the full training set. Across 9 datasets and 128-shot scenarios, the approach consistently outperforms random selection by 4% on Qwen2.5-72B and Llama3-70B, and by 2% on 5 closed-source LLMs. The method also shows scalability to 1024-shot settings and transferability across model sizes, with further improvements achieved through hybrid task-level and instance-level demonstration strategies. Ablation studies confirm the effectiveness of gradient matching, and analysis reveals the method naturally maintains label diversity and coverage. Computational costs are practical, requiring only 3-4 times the time of prefix-tuning while enabling significant ICL performance gains.

## Method Summary
The authors propose Curriculum Latent Gradient Matching (CLG), a method that selects demonstrations for many-shot ICL by matching gradients computed during prefix tuning on small models. The approach involves three key steps: () Compute curriculum latent gradients by training a small LM with learnable prefix embeddings on the full training set for multiple epochs, capturing gradients at each checkpoint; 2) Select demonstrations through greedy search that minimize L2 distance between the average gradient of the full dataset and the selected subset; 3) Apply local optimization to refine the selected demonstrations. The selected examples are then used as fixed demonstrations for the target large language model, enabling prefix caching and efficient inference. The method assumes that examples effective for fine-tuning will also be effective for ICL, and that gradient signals transfer across model scales.

## Key Results
- CLG improves 128-shot ICL performance by 4% absolute on Qwen2.5-72B and Llama3-70B compared to random selection across 9 datasets
- Achieves 2% improvement on 5 closed-source LLMs including GPT-4o-mini and DeepSeek-V3, demonstrating cross-model transferability
- Scales effectively to 1024-shot settings, with open-ended tasks like Break and SMCalFlow continuing to improve while classification tasks show degradation
- Implicitly maintains label diversity, achieving KL divergence of 0.002-0.008 between demonstration and test set distributions vs. 0.012-0.020 for random selection
- Hybrid task-level + instance-level demonstration strategy further improves performance, particularly for tasks like SNLI and ANLI

## Why This Works (Mechanism)

### Mechanism 1: Gradient Matching as Learning Dynamics Proxy
- Claim: Demonstrations that produce similar gradient updates to the full training set will induce similar learning effects during ICL.
- Mechanism: By computing curriculum latent gradients (gradients with respect to learnable prefix embeddings across training epochs) and minimizing L2 distance between selected subset and full dataset, the method identifies examples that collectively approximate the "learning signal" of the entire task.
- Core assumption: The data requirements for ICL and fine-tuning are analogous—examples that would guide fine-tuning well also guide ICL well.
- Evidence anchors: "we hypothesize that the data requirements for in-context learning and fine-tuning are analogous" [abstract]; ablation shows reversing to gradient mismatching causes significant performance degradation below random selection [Section 4.7, Fig. 4]
- Break condition: If tasks require fundamentally different learning signals for ICL vs. FT (e.g., tasks where ICL relies on pattern matching rather than gradient-like updates), or if the small model's gradients don't correlate with large model behavior.

### Mechanism 2: Cross-Model Transfer via Shared Learning Dynamics
- Claim: Gradient signatures computed on small open-source models transfer to larger models and closed-source LLMs.
- Mechanism: Since gradients capture task-relevant learning signals rather than model-specific artifacts, demonstrations selected on Qwen2.5-3B or Llama3-8B remain effective on Qwen2.5-72B, Llama3-70B, and closed-source models.
- Core assumption: Learning dynamics are sufficiently similar across model scales and architectures for gradient-based selection to transfer.
- Evidence anchors: "through gradient matching on relatively small models... our method consistently outperforms random selection on larger LLMs" [abstract]; CLG improves over random by ~2% on 5 closed-source LLMs including GPT-4o-mini and DeepSeek-V3 [Table 4]
- Break condition: If target model has fundamentally different architecture, tokenizer, or pretraining distribution that alters which examples are informative.

### Mechanism 3: Implicit Diversity Maintenance Through Gradient Coverage
- Claim: Gradient matching naturally selects diverse demonstrations without explicit diversity constraints.
- Mechanism: Since the full training set's average gradient reflects contributions from all label classes and example types, matching this gradient requires selecting a representative mix—greedy selection automatically avoids redundant examples that would skew the gradient.
- Core assumption: Diversity in gradient space correlates with label diversity and coverage.
- Evidence anchors: CLG achieves lowest KL divergence (0.002-0.008) between demonstration and test set label distributions vs. random (0.012-0.020) [Section 4.10, Table 6]; "gradient matching can inherently preserve data coverage in terms of label" [Section 4.10]
- Break condition: If task has gradient-similar but semantically distinct examples, or if gradient space doesn't align with semantic diversity needs.

## Foundational Learning

- Concept: **Latent Concept Learning / Prefix Tuning**
  - Why needed here: The method uses learnable prefix embeddings as continuous proxies for discrete demonstrations to compute gradients.
  - Quick check question: Can you explain why optimizing continuous embeddings is easier than selecting discrete demonstrations?

- Concept: **Gradient-Based Data Selection / Dataset Condensation**
  - Why needed here: Core technique borrowed from dataset condensation—selecting examples that match full-dataset gradients.
  - Quick check question: How does matching gradients differ from matching example embeddings or loss values?

- Concept: **Many-Shot ICL vs. Instance-Level Retrieval**
  - Why needed here: Understanding why instance-level retrieval fails at scale (O(n²) complexity, no prefix caching) motivates task-level selection.
  - Quick check question: Why does a fixed demonstration set enable prefix caching while per-query retrieval doesn't?

## Architecture Onboarding

- Component map: Latent concept learner (small LM + learnable prefix zθ) -> Curriculum gradient computer -> Demonstration selector (greedy search + local optimization) -> Inference deployer (fixed demonstrations + prefix caching)
- Critical path: Prefix tuning → gradient computation → greedy selection → local optimization. Errors in gradient computation (wrong epoch checkpoints, incorrect gradient aggregation) cascade to poor selection.
- Design tradeoffs:
  - Small model size (3B vs 8B): Smaller = faster but potentially weaker gradient signal
  - Number of epochs E: More epochs = richer curriculum but higher compute
  - Local optimization iterations (l=32): More iterations = better matching but diminishing returns
  - Prefix length k: Longer = more expressive gradients but more memory
- Failure signatures:
  - Selected demonstrations all from one label class → check gradient computation or learning rate (may have collapsed)
  - Performance worse than random → likely gradient sign error or mismatching instead of matching
  - No improvement over random on target model → small model may be too different from target
- First 3 experiments:
  1. **Sanity check**: Run CLG on a small dataset (e.g., SST-5) with Qwen2.5-3B for both selection and evaluation—verify improvement over random before attempting transfer
  2. **Ablation gradient matching**: Compare L2 distance minimization vs. random selection vs. gradient *maximization* (mismatching) on 2-3 datasets to validate the core mechanism
  3. **Cross-model validation**: Select demonstrations on Llama3-8B, evaluate on Llama3-70B and Qwen2.5-72B separately to assess transfer bounds before trying closed-source models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can further performance gains be achieved by optimizing the *ordering* of the selected demonstrations, particularly by aligning them with the learning dynamics observed during different stages of fine-tuning?
- Basis in paper: The authors explicitly state in the Limitations section that their method does not address the sensitivity of ICL to demonstration order and suggest that investigating this could lead to further improvements.
- Why unresolved: The current implementation focuses solely on selecting the subset of examples via gradient matching, ignoring the sequential arrangement of those examples within the context window.
- What evidence would resolve it: Experiments applying curriculum learning strategies to order the CLG-selected demonstrations, demonstrating significant performance lifts over random ordering.

### Open Question 2
- Question: Why do specific tasks like MNLI and HellaSwag exhibit performance degradation when scaling from 128 to 1024 shots, while open-ended tasks like Break and SMCalFlow continue to improve?
- Basis in paper: In Section 4.6, the authors observe this phenomenon and state, "To explain this phenomenon, we analyse the scaling trends," but the underlying cause for the degradation in classification/reasoning tasks at high shot counts remains an open investigation.
- Why unresolved: The paper empirically identifies the divergent scaling trends but does not provide a theoretical or mechanistic explanation for why excess demonstrations harm specific task types.
- What evidence would resolve it: An analysis of attention patterns or noise accumulation in classification tasks versus generation tasks at extreme context lengths (1024+ shots).

### Open Question 3
- Question: Does the gradient matching approach inadvertently amplify or effectively mitigate social biases (e.g., gender, race) present in the training data compared to random selection?
- Basis in paper: The Limitations section notes that the method selects examples based on training data which "may suffer from biases... This could lead to discriminatory content in practice."
- Why unresolved: While the paper proves the method selects high-performing examples, it does not evaluate the fairness or bias profile of the selected demonstration sets.
- What evidence would resolve it: A comparative audit of bias metrics (e.g., demographic representation in selected shots) between demonstrations chosen by CLG versus those chosen randomly.

## Limitations
- The fundamental analogy between ICL and fine-tuning data requirements remains empirically untested beyond this work's scope
- Cross-model transfer assumption is vulnerable—small model gradients may not capture task-relevant signals for substantially larger or architecturally different models
- Scalability to extremely long contexts (>8K tokens) and behavior on highly imbalanced or few-class datasets haven't been explored

## Confidence

- **High Confidence**: The core gradient matching mechanism (selecting examples that minimize L2 distance to full dataset gradients) demonstrably improves ICL performance across multiple datasets and model scales. The implicit diversity maintenance through gradient coverage is empirically validated via KL divergence measurements.
- **Medium Confidence**: The cross-model transferability claim is supported by results on closed-source LLMs, but the underlying mechanism (whether gradients capture transferable learning signals or simply effective diversity) remains speculative without ablation studies isolating model-specific effects.
- **Low Confidence**: The foundational hypothesis that ICL and fine-tuning have analogous data requirements is asserted but not directly tested. No experiments compare gradient-based selection against alternatives that preserve diversity without gradient matching.

## Next Checks
1. **Ablation of Transfer Mechanism**: Run CLG on small models but evaluate with a diversity-only selection method (e.g., maximizing label coverage without gradient matching) to isolate whether gradient matching adds value beyond implicit diversity.
2. **Architecture-Specific Validation**: Select demonstrations on a transformer with different architecture (e.g., OPT instead of Llama) and evaluate on both the original and new architecture families to test whether transfer is model-family dependent.
3. **Extreme Scale Test**: Evaluate CLG at 4K+ shot settings on datasets with high class imbalance to test robustness beyond the reported 1024-shot regime and assess whether gradient coverage maintains effectiveness when label distributions are skewed.