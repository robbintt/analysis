---
ver: rpa2
title: Hey, That's My Data! Label-Only Dataset Inference in Large Language Models
arxiv_id: '2506.06057'
source_url: https://arxiv.org/abs/2506.06057
tags:
- dataset
- data
- training
- fine-tuning
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CatShift, a label-only dataset-inference
  framework for detecting unauthorized usage of proprietary datasets in large language
  models (LLMs). The method leverages catastrophic forgetting: if a suspicious dataset
  was part of the model''s training, fine-tuning on it causes a pronounced shift in
  the model''s outputs; if novel, the shift is smaller.'
---

# Hey, That's My Data! Label-Only Dataset Inference in Large Language Models

## Quick Facts
- arXiv ID: 2506.06057
- Source URL: https://arxiv.org/abs/2506.06057
- Reference count: 40
- Key outcome: Introduces CatShift, a label-only dataset-inference method leveraging catastrophic forgetting to detect unauthorized usage of proprietary datasets in LLMs, achieving AUC 0.979 and F1 0.863 on Pythia-410M.

## Executive Summary
CatShift is a novel framework for detecting unauthorized usage of proprietary datasets in large language models without requiring access to model internals. It leverages the phenomenon of catastrophic forgetting: when fine-tuning on a dataset already present in the model's training, the model's outputs shift significantly; for novel datasets, the shift is smaller. By statistically comparing these output shifts against a known non-member validation set, CatShift determines dataset membership using only input-output pairs. Experiments demonstrate strong performance on both open-source and API-based LLMs, providing a practical solution for data owners to protect their intellectual property.

## Method Summary
CatShift operates by fine-tuning a target LLM on a suspicious dataset while monitoring the statistical shift in its output distributions. The core hypothesis is that catastrophic forgetting will cause a larger output shift for datasets already present in the model's training, as the model must "recover" previously learned parameters, compared to minimal shifts for truly novel datasets. The method compares the output distribution shift (measured via the Kolmogorov-Smirnov statistic) for the suspicious dataset against a non-member validation set, using statistical significance to determine membership. Critically, this is done using only label outputs, without requiring access to internal model states like logits or gradients.

## Key Results
- On Pythia-410M, CatShift achieves an AUC score of 0.979 and an F1 score of 0.863, significantly outperforming baseline methods.
- Applied to GPT-3.5 with the BookMIA dataset, CatShift achieves a p-value of 6.44 × 10^-5 for member books versus 0.711 for non-member books, demonstrating effective discrimination.
- The method shows reduced performance on mathematical datasets (e.g., the PILE "Math" subset), indicating potential limitations for formal languages.

## Why This Works (Mechanism)
CatShift exploits catastrophic forgetting as a signal for dataset membership. When a model is fine-tuned on data it has already seen during pre-training, the optimization process must overcome the "pull" of previously learned parameters, causing a larger shift in output distributions. For truly novel data, the shift is smaller because the model is learning new patterns rather than recovering old ones. By quantifying this difference statistically, CatShift infers whether the suspicious dataset was part of the original training corpus.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to rapidly lose previously learned information when trained on new data. Why needed: Forms the core signal for detecting dataset membership.
  - Quick check: Observe model performance drop on old tasks after fine-tuning on new data.
- **Kolmogorov-Smirnov (KS) statistic**: A non-parametric test comparing two distributions. Why needed: Quantifies the statistical difference between output distributions before and after fine-tuning.
  - Quick check: Apply KS test to two synthetic distributions with known differences.
- **Fine-tuning dynamics**: How models update parameters during continued training. Why needed: Understanding how parameter updates differ for seen vs. unseen data.
  - Quick check: Track loss and accuracy curves during fine-tuning on different datasets.
- **Label-only inference**: Performing analysis using only model inputs and outputs, without internal states. Why needed: Enables application to API-based models where gradients/logits are inaccessible.
  - Quick check: Compare inference results using only outputs vs. full model access.
- **Statistical hypothesis testing**: Framework for making decisions based on p-values and significance thresholds. Why needed: Determines whether observed output shifts are statistically meaningful.
  - Quick check: Verify Type I and Type II error rates under controlled conditions.

## Architecture Onboarding

**Component Map:**
Suspicious Dataset → Fine-tuning Module → Output Distribution Shift → KS Statistic → Statistical Comparison → Membership Inference

**Critical Path:**
Suspicious Dataset → Fine-tuning → Output Collection → KS Calculation → p-value Comparison → Decision

**Design Tradeoffs:**
- Fine-tuning provides strong signal but is computationally expensive; no fine-tuning alternatives would be faster but less accurate.
- Label-only approach enables API use but loses granularity compared to gradient-based methods.
- KS statistic is non-parametric and robust but may be less sensitive than parametric alternatives.

**Failure Signatures:**
- Poor performance on formal languages (e.g., math) suggests catastrophic forgetting signal is domain-dependent.
- High computational cost for large models or datasets may limit practical applicability.
- False positives if validation set overlaps with suspicious dataset or is not representative.

**3 First Experiments:**
1. Replicate AUC and F1 scores on Pythia-410M using the provided datasets and fine-tuning setup.
2. Test performance on a simple synthetic dataset where membership is known (e.g., split CIFAR-10 training data).
3. Evaluate sensitivity to fine-tuning hyperparameters (learning rate, epochs) on a held-out validation set.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does partial dataset overlap affect the detectability of membership, and can CatShift be adapted for fine-grained partial membership inference?
- Basis in paper: The authors state in the "Future Work" section that "understanding how partial overlap influences catastrophic forgetting and reactivation could reveal more nuanced detection capabilities."
- Why unresolved: The current methodology evaluates datasets as mostly "in" or "out," whereas real-world copyright violations often involve only portions of a corpus being scraped.
- What evidence would resolve it: Experiments measuring the statistical power of the test as the ratio of member to non-member samples in the suspicious dataset varies from 0% to 100%.

### Open Question 2
- Question: What are the optimal fine-tuning hyperparameters (e.g., learning rate, epochs, LoRA rank) for maximizing detection accuracy while minimizing computational cost?
- Basis in paper: The "Limitations" section notes that "the fine-tuning procedure itself introduces hyperparameters... that can significantly affect the observed output shifts," and determining optimal configurations "remains an open research question."
- Why unresolved: The paper uses a specific LoRA configuration (rank 8, alpha 32) but does not explore how sensitive the "output shift" signal is to these choices or how to optimize them on a budget.
- What evidence would resolve it: A comprehensive ablation study showing the relationship between hyperparameter settings, the magnitude of the KS statistic, and the financial/computational cost of the fine-tuning run.

### Open Question 3
- Question: Why does CatShift show reduced performance on mathematical datasets (e.g., the PILE "Math" subset), and does this indicate a limitation in the catastrophic forgetting mechanism for formal languages?
- Basis in paper: The evaluation section notes that "CatShift performs relatively worse on the 'Math' subset," achieving poor separation compared to other domains.
- Why unresolved: The authors observe the performance drop but do not determine if it stems from the specific structure of math data, lower redundancy in the pre-training corpus, or distinct forgetting dynamics for logical reasoning.
- What evidence would resolve it: An analysis of the output shift distributions specifically for formal languages compared to natural language, potentially correlating performance with the density of the data in the pre-training set.

### Open Question 4
- Question: Can theoretical guarantees be established that formally link the magnitude of output shifts during fine-tuning to the statistical likelihood of dataset membership?
- Basis in paper: The "Future Work" section suggests "developing theoretical guarantees that more tightly connect the catastrophic forgetting process with detectable output shifts."
- Why unresolved: The paper provides a "concise theoretical perspective" involving gradient decomposition ($\Delta_{recover}$ vs. $\Delta_{new}$), but acknowledges this link "may not be perfectly tight" with practical label-only outputs.
- What evidence would resolve it: Deriving optimization bounds that mathematically define how parameter recovery translates to changes in the arg max of the output distribution, validated against empirical divergence metrics.

## Limitations
- Effectiveness may vary across model architectures and training procedures, with uncertain generalizability to larger or more diverse model families.
- Reliance on statistical comparison with a validation set assumes the validation distribution is representative and non-overlapping with the suspicious dataset, which may not hold in practice.
- Computational expense of fine-tuning on the suspicious dataset may be prohibitive for very large models or datasets.

## Confidence
- **High confidence** in the core methodology and experimental setup, as results are clearly presented and reproducible.
- **Medium confidence** in the generalizability of results to other model architectures and scales, given limited diversity in tested models.
- **Medium confidence** in the robustness of the catastrophic forgetting signal, as its reliability across different training regimes and dataset properties is not fully explored.

## Next Checks
1. Test CatShift on a broader range of LLM architectures (e.g., LLaMA, BLOOM) and scales (1B, 7B, 13B+ parameters) to assess generalizability.
2. Evaluate the impact of dataset similarity and overlap between the suspicious dataset and the validation set on detection accuracy.
3. Investigate the computational overhead and feasibility of fine-tuning for very large models or datasets, and explore potential approximations or alternatives.