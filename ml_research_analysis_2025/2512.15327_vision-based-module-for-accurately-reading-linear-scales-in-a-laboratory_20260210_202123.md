---
ver: rpa2
title: Vision-based module for accurately reading linear scales in a laboratory
arxiv_id: '2512.15327'
source_url: https://arxiv.org/abs/2512.15327
tags:
- image
- level
- which
- contours
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a vision-based system for accurately reading
  linear scales in laboratory environments. The method uses YOLO for object detection,
  followed by orientation correction using PCA, and feature extraction to read measurement
  levels from syringes and measuring cylinders.
---

# Vision-based module for accurately reading linear scales in a laboratory

## Quick Facts
- arXiv ID: 2512.15327
- Source URL: https://arxiv.org/abs/2512.15327
- Authors: Parvesh Saini; Soumyadipta Maiti; Beena Rai
- Reference count: 27
- Primary result: Vision-based system achieves MAE = 0.070-0.094 ml, RMSE = 0.089-0.109 ml, R² = 0.994-0.996 for reading syringe levels

## Executive Summary
This paper presents a vision-based system for accurately reading linear scales on laboratory apparatus such as syringes and measuring cylinders. The method employs YOLO for object detection, followed by orientation correction using PCA, and feature extraction to read measurement levels. The system corrects image orientation, extracts scale markings and digits using OCR, and estimates liquid levels based on detected indicators. When tested on a moving syringe, the system demonstrated high accuracy and reliability with low mean absolute error and high R² scores.

## Method Summary
The proposed pipeline uses YOLO to detect laboratory apparatus, then applies color segmentation and contour analysis to extract scale markers. PCA is used to compute the orientation angle from marker contours, allowing rotation to align the scale vertically. A second crop narrows to just the scale region. Major markers are classified using a 15% relative length jump threshold, and PyTesseract OCR reads adjacent digits. An auto-correction module uses slope consistency to fix OCR errors, and level indicator detection enables linear interpolation for final readings.

## Key Results
- Mean Absolute Error: 0.070-0.094 ml
- Root Mean Squared Error: 0.089-0.109 ml
- R² Score: 0.994-0.996
- Successfully handles arbitrary orientations and distances

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical image reduction creates a robust processing pipeline for arbitrarily oriented laboratory apparatus. YOLO detects and bounds the apparatus, PCA on segmented marker contours computes the orientation angle, rotation aligns the scale vertically, and a second crop narrows to just the scale region. This sequential narrowing reduces irrelevant pixels before feature extraction. The approach assumes scale markers are visually distinct and their collective spatial distribution reliably indicates apparatus orientation.

### Mechanism 2
Relative contour classification enables scale-agnostic marker separation without fixed size thresholds. Linear contours are sorted by length, and consecutive jumps >15% of the prior contour length trigger a new group. The first group is assumed to be major markers, enabling adaptation across distances and apparatus sizes. This method assumes major markers are consistently longer than minor markers within a given scale, regardless of imaging distance.

### Mechanism 3
OCR error auto-correction via slope consistency improves robustness of digit reading. For each pair of detected markers, the slope (value/position) is computed, and the mode slope is assumed correct. Using this slope and any reliable point, offset is computed, and all marker values are re-evaluated. This corrects misreads and fills missing values, assuming valid marker position-value pairs follow a consistent linear relationship.

## Foundational Learning

- **Principal Component Analysis (PCA) for orientation estimation**: PCA on binary contour points extracts the major axis of elongated structures (scale markings), providing a rotation angle for alignment. Quick check: Given a set of (x, y) points from segmented markers, which eigenvector corresponds to the primary orientation axis?

- **Linear interpolation from calibrated markers**: Once marker positions and numeric labels establish a y = mx + c mapping, the level indicator position yields the reading without per-pixel classification. Quick check: If markers at pixel positions 100 and 200 correspond to values 1 and 3 respectively, what value corresponds to pixel 150?

- **Color-based segmentation in HSV space**: Marker and indicator extraction relies on separating pixels by color ranges; HSV often provides more stable segmentation under varying illumination than RGB. Quick check: Why might hue be preferable to raw RGB for segmenting colored scale markings under non-uniform lighting?

## Architecture Onboarding

- **Component map**: Input → YOLO Object Detection → Bounding Box Crop → Color Segmentation → Contour Extraction → PCA Orientation → Rotation + Scale Crop → Marker Segmentation → Relative Contour Classification → PyTesseract OCR → Auto-Correction → Linear Position-Value Mapping → Level Indicator Segmentation → Measurement Point Extraction → Linear Interpolation → Output Reading

- **Critical path**: Orientation correction → major marker extraction → OCR → auto-correction → level indicator detection → interpolation. Errors in orientation or OCR propagate directly; auto-correction mitigates OCR but cannot fix orientation failures.

- **Design tradeoffs**: Fixed HSV color ranges vs adaptive thresholding (fixed ranges are simpler but brittle to lighting); 15% relative jump threshold for marker classification (tuned empirically, may not generalize); local ROI for OCR (improves accuracy but assumes consistent marker-digit spatial layout).

- **Failure signatures**: Consistently offset readings (likely miscalibrated slope/offset due to OCR errors); no reading produced (orientation correction failed or level indicator not segmented); high variance across frames (unstable marker detection); hysteresis between aspirating/dispensing (observable bias difference).

- **First 3 experiments**: 1) Validate orientation correction robustness across varied orientations and distances; 2) Test auto-correction under induced OCR noise; 3) Characterize indicator segmentation sensitivity under varying illumination and background conditions.

## Open Questions the Paper Calls Out

### Open Question 1
Can the current pipeline generalize to other linear scale apparatus, such as pipettes or burettes, without retraining the object detection model or manually retuning the contour filtering parameters? The authors state the aim for capability independence from apparatus type, but validation is restricted to syringes and measuring cylinders, and contour filtering relies on specific heuristics tuned to tested syringes.

### Open Question 2
How robust is the HSV color-based segmentation strategy to variations in ambient lighting and specular reflections common in unstructured laboratory environments? The methodology relies on fixed HSV color ranges, which are typically sensitive to illumination changes, yet performance under variable lighting conditions is not quantified.

### Open Question 3
Can the level indicator extraction method accurately detect the meniscus of clear or colorless liquids where high contrast is absent? The system extracts level indicators based on contrasting appearance using color segmentation, but examples use dark plungers or distinctly colored liquids, leaving performance for transparent fluids unclear.

## Limitations
- Reliance on specific HSV color ranges may not generalize across different lighting conditions or apparatus designs
- 15% relative jump threshold for marker classification is empirically tuned and may fail for non-standard scales
- Performance depends on YOLO detection quality and contour extraction stability, affected by occlusions or low-contrast markers

## Confidence

- **High Confidence**: Hierarchical processing pipeline is well-documented and demonstrably effective for tested configurations; reported metrics are internally consistent and supported by experimental setup
- **Medium Confidence**: Relative contour classification method is plausible but lacks direct validation against diverse scale designs; 15% threshold is justified empirically but not tested across varied apparatus
- **Low Confidence**: Robustness to extreme lighting variations, novel apparatus colors, or highly reflective surfaces is not demonstrated; reliance on fixed HSV ranges without adaptive methods limits real-world applicability

## Next Checks
1. **Orientation Correction Robustness**: Test system on syringes and measuring cylinders rotated at arbitrary angles (0-180°) and distances (10-100 cm) to quantify PCA angle accuracy and final reading consistency.

2. **Auto-Correction Under Induced OCR Noise**: Systematically corrupt OCR outputs for select markers (20-50% error rate) and evaluate slope-mode algorithm's ability to recover correct values, identifying failure cases with two or more coherent wrong slopes.

3. **Indicator Segmentation Sensitivity**: Vary illumination conditions (natural, fluorescent, LED) and background materials (white, black, patterned) to measure level indicator detection rate and positional accuracy, comparing HSV-based segmentation with adaptive thresholding or alternative color spaces.