---
ver: rpa2
title: Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in
  Acoustic Wavefield Simulations
arxiv_id: '2504.16553'
source_url: https://arxiv.org/abs/2504.16553
tags:
- layer
- pinn
- wavefield
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses slow convergence and instability in training
  Physics-Informed Neural Networks (PINNs) for high-frequency acoustic wavefield simulations
  based on the Helmholtz equation. The core contribution is a hybrid optimization
  framework that embeds a least-squares (LS) solver directly into the gradient descent
  (GD) loss function, enabling optimal updates for the linear output layer.
---

# Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations

## Quick Facts
- arXiv ID: 2504.16553
- Source URL: https://arxiv.org/abs/2504.16553
- Reference count: 3
- The method achieves faster convergence and higher accuracy than standard PINN training for high-frequency acoustic wavefield simulations, particularly where GD-based training fails.

## Executive Summary
This paper introduces a hybrid optimization framework for Physics-Informed Neural Networks (PINNs) that embeds a least-squares (LS) solver into the gradient descent loss function. The approach analytically computes optimal weights for the linear output layer at each training step, eliminating iterative gradient descent for that portion and ensuring consistency between optimization steps. Numerical experiments demonstrate the method achieves faster convergence, higher accuracy, and improved stability compared to conventional PINN training, particularly for high-frequency wavefields where standard approaches struggle.

## Method Summary
The method embeds a least-squares solver directly into PINN training to optimize the linear output layer weights analytically at each step. The network uses sinusoidal positional encoding and four hidden layers with sine activation, producing a penultimate output H. Spatial derivatives of H are computed via forward-mode differentiation and assembled into a matrix D. The LS solver computes optimal output weights W* = (D^T D + εI)^(-1) D^T R using Cholesky decomposition, where R contains source and background terms. The loss becomes ||DW* - R||², which is backpropagated only through the hidden layers. Tikhonov regularization ε starts at 0.1 and decays to 10^-4 to ensure stability during training.

## Key Results
- Converges in ~5k epochs for 10 Hz Simple Model versus 20k+ for standard PINN
- Maintains reasonable accuracy with as few as 500 collocation points versus 2,600+ required by standard methods
- Successfully solves high-frequency problems where standard GD-based training fails entirely

## Why This Works (Mechanism)

### Mechanism 1: Analytical Output Layer Optimization
The LS solver analytically computes optimal output-layer weights at each training step, eliminating iterative gradient descent for the linear portion. This converts what would require many GD iterations into a single matrix solve, effectively incorporating second-order optimization while maintaining first-order computational complexity for the hidden layers.

### Mechanism 2: Efficient Derivative Computation
Forward-mode differentiation of penultimate layer outputs enables efficient derivative computation that scales with input dimension rather than output dimension. For 2D problems with 2 inputs, this is O(2) per element rather than O(P) where P is penultimate layer size, providing significant computational savings when P >> 2.

### Mechanism 3: Simplified Loss Landscape
Decoupling linear from nonlinear optimization reduces the effective parameter space that gradient descent must traverse. By solving output weights analytically at each step, GD only navigates the hidden-layer parameter space, effectively "flattening" the loss landscape in the output-weight dimensions and reducing the risk of poor local minima.

## Foundational Learning

- **Physics-Informed Neural Networks (PINNs)**: Neural networks trained to satisfy physical laws encoded as PDEs, where PDE constraints enter the loss function. Understanding low-frequency bias and constraint enforcement is essential.
- **Least-Squares Regression and Normal Equations**: Mathematical tool for solving overdetermined systems with quadratic loss, where closed-form solutions exist via normal equations. Understanding regularization (Tikhonov damping) is critical for numerical stability.
- **Helmholtz Equation and Scattered Wavefields**: The target PDE governing acoustic wave propagation, where high-frequency solutions are oscillatory. Understanding the decomposition into background and scattered fields explains why standard PINNs struggle with high-frequency solutions.

## Architecture Onboarding

- **Component map**: Input Encoder -> Hidden Layers (4-5 layers, sine activation) -> Penultimate Output H -> LS Solver Module -> Output Weights W* -> Loss Assembler
- **Critical path**: Encode spatial coordinates → positional embeddings → forward pass through hidden layers → penultimate output H → compute derivatives ∂H/∂x, ∂H/∂z → assemble D matrix → solve W* = (D^T D + εI)^(-1) D^T R → compute GD loss ||DW* - R||² → backpropagate through hidden layers only
- **Design tradeoffs**:
  - Penultimate layer size P: Larger P increases representational capacity but raises LS solver cost to O(P³); paper uses P=64-160
  - Collocation points N: More points improve accuracy but increase D^T D construction cost O(NP²); method works with few points
  - Regularization ε: Large ε (0.1) stabilizes early training; small ε (10⁻⁴) improves final accuracy; schedule: decay ε during training
  - PML presence: PML nearly doubles D matrix size but is necessary for open-boundary problems
- **Failure signatures**:
  - Early-training divergence: Matrix D^T D ill-conditioned when H is random; fix: Start with large ε, decay gradually
  - Slow convergence despite LS: Penultimate layer too small or positional encoding insufficient; fix: Increase P or encoding bandwidth K
  - High-frequency artifacts near source: Soft constraint weight β too small; fix: Increase β or constraint radius
  - Boundary reflections: PML parameters incorrect; fix: Verify stretching factors and damping coefficient
- **First 3 experiments**:
  1. Simple model validation: Reproduce Figure 2 results on the simple velocity model at 10 Hz without PML; verify LS-GD converges in ~5k epochs vs. 20k+ for standard PINN; use P=64, N=2600, K=3
  2. Collocation point ablation: Test convergence with N ∈ {500, 2600, 10000} to verify method works with limited data; expect LS-GD to maintain reasonable accuracy even at N=500
  3. Penultimate layer sweep: Test P ∈ {8, 64, 128} to characterize tradeoff between LS solver cost and accuracy; monitor both final error and per-epoch runtime

## Open Questions the Paper Calls Out
- How can the size of the penultimate layer (P) be adaptively selected based on problem-specific characteristics like frequency or model complexity?
- Can the Least-Squares embedded optimization framework be efficiently extended to three-dimensional (3D) acoustic wavefield simulations?
- What is the optimal scheduling strategy for the Tikhonov regularization parameter (ε) to ensure stability without sacrificing accuracy?

## Limitations
- The study lacks systematic investigation of how varying the number of hidden layers or sinusoidal positional encoding bandwidth impacts convergence speed and accuracy.
- LS solver scalability for extremely large-scale problems (e.g., 3D simulations with millions of collocation points) is not addressed, leaving questions about practical applicability beyond 2D benchmark models.
- Forward-mode differentiation implementation and its computational advantages lack direct empirical benchmarking against reverse-mode implementations.

## Confidence
- **High Confidence**: Mathematical formulation of LS solver for linear output layer and its derivation from normal equations
- **Medium Confidence**: Claim that forward-mode differentiation significantly improves efficiency (theoretically sound but lacks empirical evidence)
- **Medium Confidence**: Assertion that method converges where standard GD fails (supported by experiments but limited ablation studies)

## Next Checks
1. **Forward-Mode Differentiation Benchmark**: Implement and compare computational efficiency of forward-mode versus reverse-mode differentiation for computing derivatives of H in LS solver; measure wall-clock time and memory usage for varying P and input dimensions
2. **Hyperparameter Sensitivity Analysis**: Conduct systematic sweep of Tikhonov damping ε schedule, penultimate layer size P, and soft constraint weight β; quantify impact on convergence speed and final accuracy across multiple velocity models
3. **Scalability Test**: Extend method to 3D Helmholtz problem with larger velocity model (e.g., SEG/EAGE salt model); evaluate LS solver performance with millions of collocation points and assess memory/computational bottlenecks