---
ver: rpa2
title: Evaluating Large Language Models for Real-World Engineering Tasks
arxiv_id: '2505.13484'
source_url: https://arxiv.org/abs/2505.13484
tags:
- llms
- engineering
- system
- reasoning
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates four state-of-art LLMs (Qwen2.5, DeepSeek-R1,
  LLaMA 3.3, and GPT-4o) on real-world engineering tasks using a curated dataset of
  over 100 questions derived from actual industrial systems. The study investigates
  five research questions related to world model generation, spatial reasoning, design
  intention inference, causal reasoning, and forecasting accuracy.
---

# Evaluating Large Language Models for Real-World Engineering Tasks

## Quick Facts
- **arXiv ID:** 2505.13484
- **Source URL:** https://arxiv.org/abs/2505.13484
- **Reference count:** 40
- **Primary result:** GPT-4o outperforms local LLMs on engineering tasks, but all models struggle with abstract reasoning and complex non-local problems

## Executive Summary
This study evaluates four state-of-the-art LLMs (Qwen2.5, DeepSeek-R1, LLaMA 3.3, and GPT-4o) on real-world engineering tasks using a curated dataset of over 100 questions from actual industrial systems. The research investigates five key capabilities: world model generation, spatial reasoning, design intention inference, causal reasoning, and forecasting accuracy. Results demonstrate that while LLMs show proficiency in basic temporal and structural reasoning, they face significant limitations in abstract reasoning, formal modeling, and context-sensitive engineering logic. GPT-4o consistently outperformed local models, but all exhibited weaknesses in handling complex, non-local engineering problems. The findings suggest LLMs are best positioned as assistive tools rather than autonomous agents in engineering contexts.

## Method Summary
The study employed a mixed-methods approach using both reasoning tasks (RQ1-4) and time-series forecasting (RQ5). For reasoning tasks, over 100 questions were derived from a glass foaming plant and discrete manufacturing simulation, with responses scored on a 3-point scale by expert evaluators using minimum score across multiple runs as the final metric. Forecasting tasks used 200 samples each from Three-Tank and ETTh1 datasets, comparing LLM predictions against LSTM and DLinear baselines using Mean Squared Error. Local models (Qwen2.5-72B, DeepSeek-R1-Distill-Llama-70B, LLaMA 3.3-70B) were hosted via vLLM/Ollama on 3x NVIDIA L40 or 2x A100 GPUs, while GPT-4o served as the cloud baseline.

## Key Results
- GPT-4o significantly outperformed local models across all five engineering capabilities
- All models demonstrated strong performance on local, sequential reasoning tasks but struggled with non-local dependencies
- Type-level abstraction proved particularly challenging for local models, with GPT-4o achieving 0.85 vs 0.4-0.7 for local alternatives
- In forecasting, specialized models (LSTM/DLinear) consistently outperformed LLMs, with some LLMs performing better with incorrect dataset descriptions
- No model successfully generated valid propositional models for strong-fault logical diagnosis

## Why This Works (Mechanism)

### Mechanism 1: Local Transitional Reasoning via Pattern Completion
- **Claim:** LLMs appear to handle engineering state transitions effectively because they process sequential text patterns that map to known procedural logic, rather than reasoning from a ground-truth physical model
- **Core assumption:** The engineering process is described in the prompt with sufficient explicit sequential cues
- **Evidence:** State Transition Comprehension scores reached 1.0 for three of four models, indicating high proficiency in tracking discrete system states
- **Break condition:** Performance degrades significantly when reasoning requires "Non-Locality" or transitional dependencies that are not explicitly sequential in the text

### Mechanism 2: Scale-Dependent Abstract Consistency
- **Claim:** The ability to maintain consistent "world models" and generate "type-level" abstractions scales non-linearly with model size
- **Core assumption:** Access to cloud-based resources is permissible for non-sensitive data
- **Evidence:** GPT-4o outperformed local models significantly in "Type-Level" concept generation (0.85 vs ~0.4-0.7)
- **Break condition:** When "Concept Generation on Type-Level" is required for reusable, system-independent knowledge, smaller local models fail to abstract from instance-level inputs

### Mechanism 3: Semantic Forecasting vs. Dynamical Simulation
- **Claim:** General-purpose LLMs forecast time-series data by leveraging semantic priors about cyclic patterns, not by solving underlying physical dynamics
- **Core assumption:** The user prompts the model with textual context; the model maps this to learned time-series archetypes
- **Evidence:** Providing a "wrong dataset description" sometimes improves forecasting, suggesting the model relies on generic pattern recognition triggered by text
- **Break condition:** Complex, non-linear, or non-cyclic dynamics where semantic priors are insufficient or misleading

## Foundational Learning

- **Concept: Closed-World Assumption (CWA)**
  - **Why needed here:** To understand why LLMs succeed in bounded engineering problems but fail when required to infer constraints outside explicitly modeled elements
  - **Quick check question:** If a system description does not list a failure mode, does the LLM correctly assume it cannot happen, or does it hallucinate one based on general training data?

- **Concept: Type-Level vs. Instance-Level Abstraction**
  - **Why needed here:** Essential for distinguishing between solving a specific machine's problem versus deriving a general design rule
  - **Quick check question:** Can the system derive a general principle about "conveyors" from a specific description of "Conveyor C-101"?

- **Concept: Strong-Fault vs. Weak-Fault Modeling**
  - **Why needed here:** To diagnose the limits of LLM-based logical diagnosis
  - **Quick check question:** Does the model know how a component fails (strong), or only its nominal behavior (weak)?

## Architecture Onboarding

- **Component map:** Engineering Context -> LLM Instance -> Task Router -> Output
- **Critical path:** 1) Define engineering task (Diagnosis vs. Forecasting) 2) Route to LLM ensuring context is "Local" and "Sequential" 3) For Forecasting, route to LLM to generate simulation code or specialized model
- **Design tradeoffs:** Cloud vs. Local (High performance/low privacy vs. Low performance/high privacy), Symbolic vs. Quantitative (LLMs viable for symbolic but not direct quantitative analysis)
- **Failure signatures:** Overgeneralization (speculative solutions), Semantic Hallucination (ignoring physical laws), Non-local Disconnect (inability to propagate effects across modular structures)
- **First 3 experiments:**
  1. Provide a description of a single module (e.g., a valve) and a failure symptom. Ask for the cause. Verify if reasoning stays within the "Closed World" of the description
  2. Ask the model to propose an optimization for a generic "Type" (e.g., "electric motor") based on a specific "Instance" description. Check if the advice is reusable or overfitted to the instance
  3. Instead of asking "What is the next value?", ask "Write Python code to predict the next value given this history." Compare the execution result to the LLM's direct text guess

## Open Questions the Paper Calls Out

- **Open Question 1:** Can domain-specialized or fine-tuned LLMs significantly outperform general-purpose models on complex engineering tasks requiring abstraction and non-local reasoning?
  - **Basis:** The conclusion states authors "plan to extend the scope of analysis by incorporating domain-specialized models and hybrid approaches"
  - **Why unresolved:** Study only evaluated four general-purpose LLMs; none were specifically trained for engineering domains
  - **What evidence would resolve it:** Fine-tune LLMs on engineering corpora and compare performance on the same benchmark

- **Open Question 2:** What evaluation metrics can reliably capture structural, causal, and domain-specific correctness in engineering contexts beyond surface-level accuracy?
  - **Basis:** Authors intend to develop "evaluation metrics that more explicitly capture structural, causal, and domain-specific correctness"
  - **Why unresolved:** Current three-point scoring scale may not adequately distinguish between syntactic correctness and semantic understanding
  - **What evidence would resolve it:** Propose formal metrics validated against expert engineer evaluations across multiple engineering domains

- **Open Question 3:** Why do LLMs perform comparably or better with incorrect dataset descriptions than with correct ones in forecasting tasks?
  - **Basis:** RQ5 results show "adding no or even the wrong dataset description can actually improve model performance"
  - **Why unresolved:** This counterintuitive finding suggests LLMs rely on pattern matching rather than causal understanding, but the mechanism remains unexplained
  - **What evidence would resolve it:** Systematic probing of model attention and internal representations during forecasting with varied prompting strategies

## Limitations
- Prompt specificity: Exact system prompts and few-shot examples used for reasoning tasks were not disclosed
- Human grading subjectivity: The 3-point rubric relies on expert judgment that may vary between evaluators
- Local model hardware specifications: Exact memory configurations and quantization methods are not specified

## Confidence
- **High confidence:** Relative performance rankings between models (GPT-4o > local models), and general competency gaps (abstract reasoning < spatial reasoning)
- **Medium confidence:** Absolute score values and MSE numbers, due to potential grading variability and unreported inference parameters
- **Low confidence:** Claims about why models fail (semantic vs. dynamical reasoning) without direct mechanistic evidence

## Next Checks
1. **Prompt reproducibility test:** Re-run reasoning tasks with multiple prompt variations to determine sensitivity to phrasing and formatting
2. **Grading reliability assessment:** Have two independent domain experts grade 20% of responses to establish inter-rater reliability coefficients
3. **Hardware fidelity check:** Reproduce one reasoning task on both full-precision and 4-bit quantized local models to quantify performance tradeoffs