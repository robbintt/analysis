---
ver: rpa2
title: Outcome-based Exploration for LLM Reasoning
arxiv_id: '2509.06941'
source_url: https://arxiv.org/abs/2509.06941
tags:
- training
- pass
- exploration
- arxiv
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the issue of diversity collapse during reinforcement\
  \ learning (RL) fine-tuning of large language models (LLMs) for reasoning tasks.\
  \ It introduces outcome-based exploration methods\u2014historical exploration via\
  \ UCB-style bonuses on final answers and batch exploration that penalizes within-batch\
  \ repetition\u2014to counteract this collapse."
---

# Outcome-based Exploration for LLM Reasoning

## Quick Facts
- arXiv ID: 2509.06941
- Source URL: https://arxiv.org/abs/2509.06941
- Reference count: 40
- Primary result: Outcome-based exploration methods (UCB-Con and Batch) mitigate diversity collapse during RL fine-tuning, improving pass@k performance and maintaining answer diversity.

## Executive Summary
This paper addresses diversity collapse during reinforcement learning fine-tuning of LLMs for reasoning tasks, where standard GRPO improves pass@1 accuracy but degrades pass@k diversity. The authors introduce outcome-based exploration methods that add bonuses for rarely visited answers (historical exploration via UCB-Con) or penalize within-batch answer repetition (batch exploration). Experiments on Llama and Qwen models show both methods improve test-time accuracy and diversity compared to vanilla GRPO, with batch exploration particularly effective for maintaining high pass@k at large k. Theoretical analysis confirms sample complexity benefits when exploring over the smaller outcome space rather than full trace space.

## Method Summary
The method builds on GRPO by adding exploration bonuses to the advantage estimate. Historical exploration (UCB-Con) adds bonuses proportional to 1/√N(x,a) minus a baseline, encouraging visits to underexplored answers while avoiding over-visiting incorrect ones. Batch exploration adds a penalty term that directly rewards diversity within each training batch. Both methods track answer visitation counts to compute bonuses, with UCB-Con using global counts and batch exploration using counts within each batch. The approach is tested on math reasoning tasks with models ranging from 8B to 32B parameters.

## Key Results
- Both UCB-Con and batch exploration consistently improve test performance across different models and datasets compared to vanilla GRPO
- Batch exploration achieves better diversity at the end of training, measured by pass@k performance for large k
- The methods maintain stable pass@k curves during training while vanilla GRPO shows diversity collapse
- Theoretical analysis confirms sample complexity benefits of outcome-based exploration over trace-based exploration

## Why This Works (Mechanism)

### Mechanism 1: Transfer of Diversity Degradation
Diversity collapse during RL fine-tuning propagates from solved to unsolved problems through shared model parameters. When RL concentrates probability mass on correct answers for solvable questions, the resulting mode collapse affects generations across all questions—not just those receiving positive gradients. This happens because the policy's behavior on unsolved questions is not fully independent of its updates on solved questions due to shared representations.

### Mechanism 2: Outcome-Based UCB Exploration with Baselines
Standard UCB exploration adds bonuses proportional to 1/√N for rarely visited answer-outcome pairs. The paper's innovation is applying this to outcomes (final answers) rather than sequences, and adding baselines to control positive/negative signal balance. UCB-Con subtracts a constant baseline, allowing both positive signals for underexplored answers and negative signals for over-visited ones, which avoids redundant visits to incorrect answers.

### Mechanism 3: Batch Exploration for Test-Time Diversity
Batch exploration penalizes within-batch answer repetition directly, optimizing the pass@k objective for large k. This aligns the training objective with test-time sampling diversity by adding a penalty term that rewards answer diversity within each training batch. Unlike historical exploration which optimizes for discovering correct answers during training, batch exploration explicitly shapes the final policy to remain stochastic.

## Foundational Learning

- Concept: **Pass@k Metric**
  - Why needed here: This is the primary evaluation framework—the probability of getting at least one correct answer among k independent samples. Pass@1 measures accuracy; pass@k for large k measures diversity.
  - Quick check question: If a model has pass@1=0.4 and pass@32=0.6, what does this tell you about its answer diversity compared to a model with pass@1=0.4 and pass@32=0.8?

- Concept: **KL-Regularized RL Objective**
  - Why needed here: The GRPO baseline optimizes a tradeoff between reward maximization and staying close to the base model via KL divergence. Understanding this helps see why exploration bonuses need careful calibration—they compete with both the correctness reward and the KL penalty.
  - Quick check question: In the objective J(π) = E[r(x,a)] - β·KL(π||π_base), what happens to exploration if β is very large vs. very small?

- Concept: **Upper Confidence Bound (UCB) Exploration**
  - Why needed here: UCB is the classical bandit exploration strategy adapted here for outcome-level exploration. The core idea is adding a bonus inversely proportional to visitation counts: bonus ∝ 1/√N.
  - Quick check question: If an answer has been visited 4 times, what's the UCB bonus (with bonus = min(1, 1/√N))? What happens after 100 visits?

## Architecture Onboarding

- Component map:
```
Training Loop (GRPO-based)
├── Rollout: Generate n samples per question (n=8 default)
├── Reward: Binary correctness of final answer
├── Advantage: Group-normalized rewards Â = (r - μ) / σ
├── Exploration Bonus (NEW):
│   ├── Historical (UCB-Con): buc(x,a) - b₀, counts over all time
│   └── Batch: -1/n Σ 1{aᵢ=aⱼ}, counts within batch only
├── Total Signal: Â + c·bonus - β·KL_penalty
└── Policy Update: Gradient step on combined objective
```

- Critical path:
  1. Track outcome visitation counts N(x,a) across training (for UCB variants)
  2. Compute exploration bonus per sample based on chosen method
  3. Combine with GRPO advantage estimate and KL term
  4. Apply gradient update with masked answer tokens (to prevent bonus hacking)
  5. Monitor both pass@1 and pass@k during training to catch diversity collapse early

- Design tradeoffs:
  - **Historical vs. Batch exploration**: Historical (UCB-Con) solves more questions during training; Batch maintains higher pass@k at end of training. Choice depends on whether deployment prioritizes peak accuracy or test-time scaling.
  - **Baseline value (b₀)**: Higher b₀ makes more answers receive negative signals (only very rare answers get bonuses). Paper uses b₀=1.0 for easy data, 0.5 for medium.
  - **Bonus coefficient (c)**: Too high and exploration dominates, hurting accuracy; too low and diversity collapse persists. Paper uses c=0.1 (Llama) to 0.2 (Qwen).

- Failure signatures:
  - Pass@k declining during training while pass@1 improves → vanilla GRPO collapse
  - Pass@1 much worse than baseline → exploration bonus too strong (c too high)
  - Training metrics improve but test metrics don't → naive UCB without baseline (redundant incorrect answer visits)
  - All batch answers identical → batch exploration not being applied or penalty too weak

- First 3 experiments:
  1. **Reproduce the diversity collapse baseline**: Train vanilla GRPO on a math dataset (e.g., MATH train split) with Llama-3.1-8B-Instruct. Monitor pass@1, pass@8, pass@32 every 100 steps. Confirm that pass@32 peaks early then degrades while pass@1 continues improving.
  2. **Ablate UCB-Con vs. Batch on same checkpoint**: Starting from step 300 of the vanilla run (where collapse is visible), fine-tune two branches: one with UCB-Con (c=0.1, b₀=0.5) and one with Batch exploration. Compare final pass@k curves to identify which better matches your deployment needs.
  3. **Sweep the baseline value b₀**: With UCB-Con fixed, test b₀ ∈ {0.0, 0.25, 0.5, 0.75, 1.0} on a validation set. Plot pass@1 vs. pass@32 to find the Pareto frontier. Expect higher b₀ to favor pass@1, lower b₀ to favor diversity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can outcome-based exploration be adapted for domains without verifiable rewards or a discrete outcome space, such as open-ended generation or creative writing?
- Basis in paper: [Explicit] Section 6 states, "our current algorithms only apply to the verifiable domain, and problems with a tractable outcome space, extending them to more general settings is an interesting future direction."
- Why unresolved: The current algorithms rely on counting unique, verifiable answers (outcomes) to calculate exploration bonuses. In open-ended tasks, the "outcome space" is ill-defined or continuous, making count-based UCB or batch penalties intractable.
- What evidence would resolve it: A successful modification of the bonus mechanism (e.g., using semantic embeddings or divergence metrics instead of exact match counts) that demonstrates improved diversity and quality in non-verifiable benchmarks.

### Open Question 2
- Question: How do historical and batch exploration methods interact in multi-turn reasoning tasks where intermediate actions influence future states?
- Basis in paper: [Explicit] Section 6 notes, "currently we only evaluate our methods on the single-turn benchmarks, and we believe exploration plays an even more significant role under the multi-turn settings."
- Why unresolved: The theoretical analysis (Section 4.2) simplifies the problem to an outcome-based bandit setting. Multi-turn settings introduce partial observability and state dynamics not captured by the current bandit model, potentially complicating the "tractability of the outcome space."
- What evidence would resolve it: Experiments applying UCB-Con and Batch exploration to multi-turn environments (e.g., code debugging or agentic workflows) showing whether the diversity gains hold when the trajectory length increases significantly.

### Open Question 3
- Question: Can historical exploration (UCB-Con) and batch exploration be unified into a single objective to optimize the trade-off between peak accuracy (pass@1) and test-time diversity (pass@k) throughout training?
- Basis in paper: [Inferred] Section 4.1 notes the methods are "not mutually exclusive" but presents them as distinct algorithms with different strengths (UCB-Con for training coverage, Batch for test-time diversity).
- Why unresolved: The paper compares the methods side-by-side but does not propose or test a hybrid algorithm. It remains unclear if combining them creates conflicts in the gradient signal or if they synergize to maintain diversity during training while explicitly enforcing it at inference.
- What evidence would resolve it: A hybrid algorithm integrating both visitation-count bonuses and within-batch penalties that outperforms either method in isolation across all k values in pass@k metrics.

## Limitations
- The theoretical analysis assumes policy updates on one reasoning trace generalize to others yielding the same outcome, which may not hold for complex reasoning problems where solution paths matter
- The empirical results show architecture-specific effects, with batch exploration benefits appearing more pronounced on Qwen models than Llama models
- The study focuses on relatively small models (8B parameters) and controlled datasets, leaving questions about scalability to frontier models and more diverse problem distributions

## Confidence
- **High Confidence**: Diversity collapse during RL fine-tuning is real and measurable (supported by clear pass@k degradation patterns across multiple datasets and models)
- **Medium Confidence**: Outcome-based UCB exploration with baselines improves test generalization (strong empirical results but theoretical assumptions about outcome generalization need more scrutiny)
- **Medium Confidence**: Batch exploration optimizes pass@k at large k (well-supported by final performance but training dynamics warrant deeper investigation)

## Next Checks
1. **Generalization Across Outcomes Test**: Systematically verify whether policy updates on one reasoning trace truly generalize to other traces producing the same answer by measuring performance deltas when updating on different paths to identical outcomes.
2. **Architecture-Agnostic Validation**: Test both exploration methods on significantly larger models (70B+ parameters) and non-reasoning tasks to determine if the benefits extend beyond the current experimental scope.
3. **Long-Horizon Reasoning Analysis**: Evaluate performance on problems requiring multiple-step reasoning chains where intermediate steps matter, not just final answers, to assess whether outcome-based exploration misses critical diversity in solution approaches.