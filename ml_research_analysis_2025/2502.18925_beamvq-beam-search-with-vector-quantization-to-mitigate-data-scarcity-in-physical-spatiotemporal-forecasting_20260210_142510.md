---
ver: rpa2
title: 'BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in
  Physical Spatiotemporal Forecasting'
arxiv_id: '2502.18925'
source_url: https://arxiv.org/abs/2502.18925
tags:
- beamvq
- data
- physical
- forecasting
- beam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data scarcity in physical
  spatiotemporal forecasting, particularly for extreme events. The authors propose
  BeamVQ, a novel probabilistic framework that combines beam search with vector quantization
  to improve physical consistency and generalization.
---

# BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting

## Quick Facts
- arXiv ID: 2502.18925
- Source URL: https://arxiv.org/abs/2502.18925
- Reference count: 24
- Primary result: Reduces forecasting MSE by up to 39% while improving extreme event detection

## Executive Summary
BeamVQ addresses data scarcity in physical spatiotemporal forecasting by combining beam search with vector quantization. The method encodes deterministic model outputs into a latent codebook, retrieves multiple codebook entries to generate diverse probabilistic predictions, and uses domain-specific metrics to guide beam search through the continuous prediction space. A self-ensemble strategy iteratively augments training data with high-quality candidates, creating a closed-loop system that improves both inference quality and robustness.

## Method Summary
BeamVQ operates through a three-stage training process: (1) a base predictor is trained with standard MSE loss, (2) a Top-K VQ-VAE is pretrained to encode predictions into a discrete codebook, and (3) joint optimization combines frozen VQ-VAE guidance with ensemble distillation. During inference, beam search maintains multiple candidate trajectories, expanding each through codebook variations and selecting top candidates using domain-specific metrics like CSI for extreme events.

## Key Results
- Reduces MSE by up to 39% compared to deterministic baselines
- Improves Critical Success Index (CSI) for extreme event detection
- Demonstrates effectiveness across five benchmarks: SEVIR, NSE, SWE, RBC, and Prometheus
- Shows consistent performance gains across different physical domains (weather, fluid dynamics, PDEs)

## Why This Works (Mechanism)

### Mechanism 1: Discretization of Continuous Prediction Space via Top-K VQ-VAE
Converting continuous physical outputs into discrete latent codebook entries enables probabilistic exploration of plausible future states. The base model produces a deterministic prediction, which the VQ-VAE encoder maps to latent code. Top-K retrieval selects K nearest codebook vectors, each decoded into distinct prediction variants, creating diverse but physically plausible outputs from a single input.

### Mechanism 2: Metric-Guided Beam Search Over Trajectory Space
Extending beam search to continuous spatiotemporal domains enables exploration of high-value trajectories that single-pass inference misses. The algorithm maintains B candidate trajectories, expanding each into B×K possibilities via codebook variations. Domain-specific metrics score candidates, and only top-B trajectories survive, with exponential discounting prioritizing recent accuracy for non-stationary dynamics.

### Mechanism 3: Self-Ensemble Distillation for Iterative Data Augmentation
Aggregating top candidates as pseudo-labels and feeding them back into training mitigates data scarcity by generating physically consistent synthetic samples. During joint optimization, top-K' candidates are averaged into an ensemble that serves dual purposes: as a distillation target and as augmented training data for iterative self-training cycles.

## Foundational Learning

- **Vector Quantization Variational Autoencoder (VQ-VAE)**: Core to discretizing continuous predictions; understand encoder-decoder architecture, codebook learning via commitment loss, and stop-gradient operators. Quick check: Given a latent vector z and codebook {c_i}, can you compute the Top-K nearest entries and explain why commitment loss prevents encoder drift?

- **Beam Search in Sequence Generation**: BeamVQ adapts this NLP technique; understand beam width, candidate expansion, and score accumulation. Quick check: In standard beam search with width B=3, if step 1 produces scores [0.9, 0.8, 0.7] and step 2 expands each into 2 candidates with scores [0.1, 0.2], which 3 trajectories survive?

- **Ensemble Methods and Self-Training**: The self-ensemble strategy combines predictions and uses them as pseudo-labels; understand EMA, distillation, and iterative training loops. Quick check: If you ensemble 5 predictions with MSEs [0.1, 0.1, 0.1, 0.5, 0.5], does averaging always reduce MSE compared to the best single predictor? Why or why not?

## Architecture Onboarding

- **Component map**: Base Predictor f_Θ -> VQ-VAE Encoder e_Φ -> Codebook C -> VQ-VAE Decoder d_Φ -> Metric Evaluator M -> Beam Controller

- **Critical path**: Stage 1 (base model convergence) → Stage 2 (VQ-VAE codebook stability, check reconstruction loss plateau) → Stage 3 (joint optimization with frozen VQ-VAE, monitor guide loss vs. distillation loss balance) → Inference (beam search with metric-guided pruning)

- **Design tradeoffs**: 
  - Beam width B vs. K: Larger B explores more trajectories but scales inference as O(B·K) per step. Paper uses B=5 or 10.
  - Codebook size N×d_z: Paper finds N=1024, d_z=64 optimal for NSE; larger N helps accuracy but increases memory.
  - λ in joint loss: Balances direct supervision vs. ensemble smoothing. High λ may over-regularize; low λ ignores distillation.
  - Metric choice M: CSI for extreme events; MSE for general accuracy. Non-differentiable metrics require selection, not direct optimization.

- **Failure signatures**:
  - Codebook collapse: All codebook entries converge to similar vectors; Top-K diversity vanishes. Monitor pairwise codebook distances.
  - Beam score explosion: Accumulated scores diverge over long horizons. Check discount factor α and score normalization.
  - Self-training drift: Iterative augmentation degrades rather than improves performance. Compare validation MSE across self-training iterations.
  - Metric misalignment: High CSI but low SSIM indicates overfitting to extreme-event detection at expense of overall quality.

- **First 3 experiments**:
  1. Ablate K on validation set: Train VQ-VAE with K ∈ {1, 3, 5, 10}, measure reconstruction MSE and candidate diversity (pairwise distance in output space). Identify inflection point where increasing K yields diminishing returns.
  2. Single-step metric correlation: For frozen base model + VQ-VAE, compute correlation between metric M scores and ground-truth MSE across 1000 samples. Verify metric alignment before enabling beam search.
  3. Beam search horizon sensitivity: Run inference with B=5 on SWE benchmark for horizons N ∈ {10, 20, 50}. Plot cumulative error growth with and without beam search. Identify horizon where beam advantage degrades.

## Open Questions the Paper Calls Out

- How does the computational overhead of BeamVQ's beam search scale with spatial resolution compared to standard deterministic inference, and can pruning strategies be improved? The paper mentions "Dynamic Beam Pruning" but does not quantify inference latency trade-offs on high-resolution datasets.

- Is the optimization problem for selecting K (Theorem 1) tight and generalizable across different physical domains without extensive hyperparameter tuning? While a theoretical bound is provided, the paper does not demonstrate that a single heuristic for K works universally across all benchmarks.

- How robust is the self-ensemble strategy against error accumulation (exposure bias) during long-horizon rollouts without ground truth correction? The method augments training data using ensemble averages, but the stability of the latent manifold when trained recursively on its own generated outputs is not verified.

## Limitations

- Missing architectural details for VQ-VAE encoder/decoder, particularly layer configurations and attention mechanisms
- Critical hyperparameters (λ, β, K', α) not provided, requiring extensive grid search for reproduction
- Domain-specific metric M implementations, especially CSI threshold definitions across datasets, require clarification
- No information on training epochs, data preprocessing, or split configurations

## Confidence

- **High confidence**: The core mechanism of combining beam search with vector quantization is technically sound and well-grounded in established methods (VQ-VAE, beam search)
- **Medium confidence**: The self-ensemble training approach is plausible but lacks direct validation in the physical forecasting literature; effectiveness depends heavily on metric alignment
- **Medium confidence**: The 39% MSE improvement claim is based on reported results, but reproduction is hindered by missing architectural and hyperparameter details

## Next Checks

1. **Codebook diversity validation**: After VQ-VAE training, compute pairwise cosine distances between all codebook entries. If average distance < 0.1, increase commitment weight β or reinitialize codebook to prevent collapse

2. **Metric alignment verification**: Before beam search, run frozen base model + VQ-VAE on validation set and compute Pearson correlation between metric M scores and actual MSE. Target correlation > 0.7 to ensure metric is meaningful for candidate selection

3. **Self-training stability test**: During iterative self-training, track validation MSE across cycles. If MSE increases after 3+ iterations, implement candidate diversity constraints or reduce λ to prevent bias amplification