---
ver: rpa2
title: 'V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms'
arxiv_id: '2503.17422'
source_url: https://arxiv.org/abs/2503.17422
tags:
- risc-v
- llama
- token
- inference
- deepseek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper optimizes LLM inference on the Sophon SG2042, a many-core
  RISC-V CPU, by enhancing llama.cpp with custom kernels, compiler tuning, and NUMA-aware
  threading. The authors develop a vector-optimized GEMV kernel that quantizes fp32
  inputs to int8, exploits the SG2042's vector units, and improves data locality,
  achieving up to 56.3% higher GOPS over baselines.
---

# V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms

## Quick Facts
- arXiv ID: 2503.17422
- Source URL: https://arxiv.org/abs/2503.17422
- Reference count: 8
- This paper optimizes LLM inference on the Sophon SG2042, a many-core RISC-V CPU, by enhancing llama.cpp with custom kernels, compiler tuning, and NUMA-aware threading.

## Executive Summary
This paper presents V-Seek, a comprehensive optimization framework for accelerating LLM inference on server-class RISC-V platforms. The authors focus on the Sophon SG2042 many-core CPU, implementing custom quantized kernels, optimizing compiler choices, and tuning NUMA policies. Their approach achieves up to 56.3% higher GOPS over baselines and demonstrates that RISC-V can match x86 energy efficiency for LLM inference while maintaining competitive throughput.

## Method Summary
The authors optimize LLM inference on the Sophon SG2042 by developing a custom quantized GEMV kernel that performs runtime fp32→int8 quantization with fused dequantization, compiling with Clang 19 (which outperforms GCC 13.2 by 25-34%), and applying NUMA policy tuning (disabling balancing with memory interleaving on). They integrate the kernel into llama.cpp and validate on DeepSeek R1 Distill Llama 8B and QWEN 14B models, achieving 4.32 tok/s generation and 6.54 tok/s prefill throughput on the MILK-V Pioneer board.

## Key Results
- Achieved 56.3% higher GOPS over baselines with custom quantized GEMV kernel
- Clang 19 outperforms GCC 13.2 by 25-34% for token generation and prefill
- 4.32/6.54 tok/s (generation/prefill) for DeepSeek R1 Distill Llama 8B with optimal NUMA configuration
- 2.9×/3.0× speedup over baseline on 8B model
- 5.5× (7B) and 3× (14B) throughput improvements across three models
- 1.65× improvement over prior SG2042 results on Llama 7B, matching x86 energy efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Runtime quantization with fused dequantization improves GEMV throughput on RISC-V vector units.
- Mechanism: The kernel quantizes fp32 inputs to int8 on-the-fly, performs GEMV in nested loops (outer over rows, inner over columns), then fuses scale-factor combination for dequantization per output element. This reduces memory bandwidth pressure and keeps data in vector registers longer.
- Core assumption: The SG2042's vector units execute int8 operations substantially faster than fp32, and the quantization overhead is amortized by the GEMV computation.
- Evidence anchors:
  - [abstract] "optimized quantized kernels for key LLM layers"
  - [section] "our proposed kernel: first, the fp32 input (vector or thin matrix) is quantized to int8; then, two nested loops are executed to perform a GEMV operation"
  - [corpus] Weak direct corpus support; related work (MEADOW) discusses memory-efficient dataflow for edge LLMs but not RISC-V-specific quantization.
- Break condition: If model accuracy degrades unacceptably at Q4_0 or if vector unit utilization saturates before memory bandwidth, gains will diminish.

### Mechanism 2
- Claim: Clang 19 with aggressive optimization passes yields higher LLM inference throughput than GCC on RISC-V.
- Mechanism: Clang 19 applies more aggressive inlining and loop unrolling, combined with ISA extension support, reducing function call overhead and improving instruction-level parallelism in the inference loop.
- Core assumption: The SG2042's pipeline benefits from unrolled loops and inlined functions without causing instruction cache thrashing.
- Evidence anchors:
  - [section] "Clang 19 constantly outperforms GCC 13.2, reaching average performance gains of 34% and 25% for token generation and prefill, respectively"
  - [section] "crucial reason is the combination of ISA extension support, and more advanced compilation passes (e.g., more aggressive in-lining and loop unrolling)"
  - [corpus] No direct corpus comparison of Clang vs GCC for RISC-V LLM inference.
- Break condition: If code size expansion from unrolling causes I-cache misses exceeding gains, or if future GCC versions add equivalent passes, the advantage disappears.

### Mechanism 3
- Claim: Disabling NUMA balancing with memory interleaving maximizes throughput for predictable LLM workloads.
- Mechanism: LLM inference has deterministic memory access patterns. Default NUMA balancing triggers unnecessary page migrations across sockets, incurring overhead. Disabling balancing and interleaving memory reduces migration while maintaining bandwidth across NUMA nodes.
- Core assumption: The workload's memory access pattern is sufficiently predictable that static interleaving outperforms dynamic balancing.
- Evidence anchors:
  - [section] "default NUMA balancing policy, which is suboptimal for LLM inference due to the predictable nature of the workload, leading to a high number of thread and memory page migrations"
  - [section] "with the NUMA balancing off and memory interleaving on... we achieve the best results for both token generation (4.32 tokens/s) and for prefill (6.54 tokens/s)"
  - [corpus] Related work on ARM many-core optimization (ICPP '22) characterizes transformer inference but does not isolate NUMA effects.
- Break condition: If prompt lengths or batch sizes vary dramatically at runtime, static interleaving may underperform adaptive policies.

## Foundational Learning

- Concept: NUMA (Non-Uniform Memory Access) architecture
  - Why needed here: The SG2042 has complex memory hierarchy; understanding NUMA domains is essential to interpret why memory interleaving improves throughput.
  - Quick check question: Can you explain why accessing memory on a remote NUMA node is slower than local, and how memory interleaving mitigates this?

- Concept: GEMV (General Matrix-Vector Multiplication)
  - Why needed here: LLM inference is dominated by GEMV operations; the kernel optimization targets this specifically.
  - Quick check question: What is the arithmetic intensity of GEMV, and why is it typically memory-bandwidth-bound?

- Concept: Quantization (fp32 → int8/Q4_0)
  - Why needed here: The kernel performs runtime quantization; understanding precision-accuracy tradeoffs is critical.
  - Quick check question: How does block-wise quantization (e.g., Q4_0) differ from per-tensor quantization, and what accuracy impact might you expect?

## Architecture Onboarding

- Component map: MILK-V Pioneer (Sophon SG2042, 64 RISC-V cores with vector extensions, 128GB DRAM) -> llama.cpp framework -> custom GEMV kernel -> Xuantie GCC 10.4 (kernel) / Clang 19 (framework) -> numactl policies (balancing, interleaving, core binding)

- Critical path:
  1. Profile baseline llama.cpp on SG2042
  2. Integrate custom quantized GEMV kernel compiled with Xuantie GCC
  3. Rebuild llama.cpp with Clang 19
  4. Apply NUMA policy: `numactl --interleave=all --balancing=off`
  5. Benchmark with target models (Llama 7B, DeepSeek 8B/14B)

- Design tradeoffs:
  - Thread count: >32 threads hurt performance under default NUMA; 64 threads optimal only with NUMA balancing disabled
  - Compiler choice: Xuantie GCC required for vector unit support in kernels, but incompatible with full llama.cpp; Clang 19 better for framework
  - Quantization precision: Q4_0 used; lower precision may further speed up but risks accuracy loss

- Failure signatures:
  - Throughput degrades beyond 32 threads → NUMA balancing likely still enabled
  - Vector instructions not executed → kernel compiled with wrong toolchain (standard GCC instead of Xuantie)
  - Sub-2 tok/s on 8B model → missing kernel integration or incorrect NUMA configuration

- First 3 experiments:
  1. Single-thread GEMV microbenchmark comparing GGML baseline vs proposed kernel across matrix sizes (verify 38.3% GOPS improvement per Figure 2).
  2. Compiler sweep: build llama.cpp with GCC 13.2 vs Clang 19, measure token generation on DeepSeek 8B at 16/32/64 threads (reproduce Figure 3).
  3. NUMA policy sweep: test all four policies on DeepSeek 8B at 64 threads, confirm memory interleaving + balancing off yields ~4.3 tok/s (reproduce Figure 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the gap between vendor-specific compiler forks (required for vector support) and modern framework compatibility be bridged for RISC-V LLM deployment?
- Basis in paper: [inferred] The authors note that "Xuantie fork of GCC 10.4... is the only one supporting the HW vector units" but "is not compatible with the latest llama.cpp release," forcing a reliance on Clang 19.
- Why unresolved: There is a fragmentation between the compiler version needed for the specific SG2042 vector extension support and the version needed to compile modern software frameworks like llama.cpp.
- What evidence would resolve it: The release of a unified compiler toolchain or upstream patches that support both the vendor-specific vector extensions and the language features required by the latest inference frameworks.

### Open Question 2
- Question: Can operating system kernel improvements automate the optimization of NUMA balancing for LLM workloads on many-core RISC-V CPUs?
- Basis in paper: [inferred] The paper identifies that "using > 32 threads leads to a performance loss" due to the default NUMA balancing policy, requiring manual intervention (disabling balancing, enabling interleaving) to restore scaling.
- Why unresolved: The default OS policies for thread and memory page migration are suboptimal for the specific memory access patterns of LLM inference on this architecture.
- What evidence would resolve it: A study demonstrating automated, workload-aware NUMA scheduling that matches or exceeds the performance of the authors' manual `numactl` configurations across various LLM architectures.

### Open Question 3
- Question: Do the proposed vector-optimized quantization kernels maintain their efficiency advantage when applied to sparse Mixture-of-Experts (MoE) architectures?
- Basis in paper: [inferred] While the title mentions "Reasoning," the evaluation is restricted to dense "Distill" models (Llama 8B/Qwen 14B), leaving the performance on the sparser, larger MoE models (like the full DeepSeek R1) unexplored.
- Why unresolved: MoE models have distinct computational profiles (sparse activation, distinct memory access patterns) that may interact differently with the proposed blocking and vectorization strategies compared to the tested dense models.
- What evidence would resolve it: Benchmark results showing the throughput and latency of the V-Seek kernel on a standard MoE architecture (e.g., Mixtral 8x7B or DeepSeek R1 671B) compared to the dense baselines.

## Limitations
- The exact implementation details of the custom GEMV kernel are not publicly available, making reproduction difficult
- The specific Clang 19 compilation flags and optimization passes that yielded the 25-34% performance advantage over GCC are not detailed
- The Xuantie GCC 10.4 toolchain installation and integration process with llama.cpp's build system is unclear

## Confidence

**High Confidence** in the overall approach: The combination of NUMA-aware threading, compiler tuning, and custom kernels is well-established for many-core performance optimization, and the reported speedups (up to 5.5× for 7B models) are consistent with what similar optimizations achieve on other architectures.

**Medium Confidence** in the specific mechanisms: The claimed 38.3% GEMV GOPS improvement and 56.3% overall throughput gain are supported by the presented methodology, but without access to the exact kernel implementation and compilation flags, reproducing these specific numbers would require significant engineering effort and optimization tuning.

**Low Confidence** in the absolute performance numbers: The paper reports 4.32 tok/s generation and 6.54 tok/s prefill for DeepSeek 8B, which would represent a substantial advance over prior SG2042 results. However, without independent verification or open-source code, these absolute numbers cannot be independently validated.

## Next Checks

1. **GEMV Microbenchmark Verification**: Implement the proposed quantized GEMV kernel (outer loop over rows, inner loop over columns with int8 quantization and fused dequantization) and run a single-threaded microbenchmark across different matrix dimensions. Verify that the kernel achieves at least 38.3% higher GOPS compared to llama.cpp's baseline GGML kernel on the SG2042 vector units.

2. **Compiler Sweep Validation**: Build llama.cpp with both GCC 13.2 and Clang 19 using identical optimization flags (-O3 plus any aggressive inlining/loop unrolling flags). Run token generation on DeepSeek 8B at 16, 32, and 64 threads. Confirm that Clang 19 consistently outperforms GCC by 25-34% for both token generation and prefill phases.

3. **NUMA Policy Impact Test**: Test all four NUMA policies (balancing on/off × interleave on/off) on DeepSeek 8B at 64 threads. Measure token generation and prefill throughput to confirm that memory interleaving with NUMA balancing disabled yields the highest performance (~4.3 tok/s generation), while other policies show degraded performance beyond 32 threads.