---
ver: rpa2
title: 'Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise
  Mixture-of-Experts'
arxiv_id: '2505.22582'
source_url: https://arxiv.org/abs/2505.22582
tags:
- languages
- language
- experts
- moe-lpr
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LayerMoE, a method for efficiently expanding
  the multilingual capabilities of large language models (LLMs) through layer-wise
  allocation of experts in a Mixture-of-Experts (MoE) architecture. The key challenge
  addressed is catastrophic forgetting of old languages while learning new ones.
---

# Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts

## Quick Facts
- arXiv ID: 2505.22582
- Source URL: https://arxiv.org/abs/2505.22582
- Reference count: 16
- Primary result: LayerMoE achieves better multilingual expansion performance with 33.3-60% fewer experts than baseline MoE-LPR method

## Executive Summary
This paper addresses the challenge of efficiently expanding large language models to support new languages without catastrophic forgetting of existing languages. The authors propose LayerMoE, a layer-wise Mixture-of-Experts (MoE) approach that allocates experts based on the similarity of hidden representations between old and new languages across different layers. By placing fewer experts in high-similarity layers (language-agnostic features) and more experts in low-similarity layers (language-specific features), LayerMoE achieves superior performance with significantly fewer parameters than previous methods. The approach includes a classifier mechanism to protect old-language tokens in high-similarity layers and uses a two-stage training process to preserve existing capabilities while learning new ones.

## Method Summary
LayerMoE extends MoE-LPR by introducing layer-wise expert allocation based on representation similarity. For each layer, the method computes cosine similarity between hidden states of old and new language tokens. Layers with higher similarity receive fewer new experts (language-agnostic features), while layers with lower similarity receive more experts (language-specific features). The method adds a binary classifier before the router in top-K high-similarity layers to guide old-language token routing. Training occurs in two stages: (1) freeze original model, train new experts and routers on new-language data; (2) add classifiers to high-similarity layers and train routers and classifiers on mixed old/new data with specialized losses. The approach was evaluated on Qwen1.5-1.8B and Llama-3.2-3B models across multiple language expansion scenarios.

## Key Results
- LayerMoE outperforms MoE-LPR with 60% fewer experts in single-language expansion (3.6 vs 9 experts per layer)
- In lifelong language expansion, LayerMoE uses 33.3% fewer experts (2.4 vs 3.6 per layer) while maintaining superior performance
- The method shows consistent improvements across both old-language preservation (Old-avg +0.47 to +0.67) and new-language learning (New-avg +0.60 to +0.71)
- Ablation studies confirm both the similarity-based allocation and classifier mechanisms are essential for performance gains

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Expert Allocation via Representation Similarity
- **Claim:** Layers with higher cross-lingual similarity require fewer new experts; lower-similarity layers require more.
- **Mechanism:** The method computes cosine similarity between hidden states (post-attention) of old and new language tokens per layer. Similarity is averaged across new↔old and new↔new pairs (Eq. 9). Expert count per layer is allocated inversely to similarity (Eq. 10): \(N^i = \lceil \frac{(S^i)^{-1}}{\sum_{j=1}^m (S^j)^{-1}} \times \delta \rceil\).
- **Core assumption:** Higher similarity indicates language-agnostic features; lower similarity indicates language-specific features requiring dedicated capacity.
- **Evidence anchors:**
  - [abstract] "...the higher similarity, the fewer experts."
  - [Section 3.1] "We hypothesize that a layer with higher similarity needs fewer new experts when expanding new languages, as the model can extract language-agnostic representations in this layer."
  - [corpus] Related work "Multilingual Routing in Mixture-of-Experts" (arXiv:2510.04694) observes interpretable layer-wise routing phenomena in multilingual MoE, supporting layer-wise heterogeneity in language processing.

### Mechanism 2: Routing Classification Network for Old-Language Token Protection
- **Claim:** Adding a classifier before the router in high-similarity layers reduces forgetting of old languages.
- **Mechanism:** A linear classifier \(W_c \in \mathbb{R}^{h \times 2}\) predicts whether a token belongs to old or new languages. For tokens classified as old, routing bypasses the learned router and goes directly to the old expert (Eq. 13). This is applied only to top-K high-similarity layers where the router is most likely confused.
- **Core assumption:** High similarity layers are where routers conflate old and new language tokens, causing misrouting and forgetting.
- **Evidence anchors:**
  - [abstract] "...add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens."
  - [Section 3.3] Eq. 11-13 define the classifier loss and inference routing logic.

### Mechanism 3: Two-Stage Training with Frozen Original Parameters
- **Claim:** Freezing the original dense model while training new experts (Stage 1), then refining routers with mixed data (Stage 2), preserves old-language ability.
- **Mechanism:** Stage 1 freezes the original model and trains new experts + routers on new-language data with NTP + load balance loss. Stage 2 trains only routers + classifiers on mixed old/new data with LPR loss (encouraging old tokens → old expert) and classification loss.
- **Core assumption:** Old experts retain original capabilities if parameters are frozen; routers can learn to selectively route without modifying representations.
- **Evidence anchors:**
  - [Section 2] Describes MoE-LPR two-stage process adopted by this work.
  - [Section 4.3] Training setup follows MoE-LPR with frozen original parameters in Stage 1.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) Routing**
  - **Why needed here:** Understanding top-K expert selection, router scores, and weighted expert output aggregation is essential to follow the architecture.
  - **Quick check question:** Given router scores [0.1, 0.3, 0.6] and K=2, which experts are selected and what are their normalized weights?

- **Concept: Catastrophic Forgetting in Continual Learning**
  - **Why needed here:** The paper's core motivation is expanding to new languages without degrading old ones; forgetting is the failure mode being addressed.
  - **Quick check question:** Why does fine-tuning on new-language data alone hurt old-language performance?

- **Concept: Representation Similarity (Cosine Similarity)**
  - **Why needed here:** The allocation algorithm depends on computing similarity between hidden state sets.
  - **Quick check question:** If two language hidden-state sets have cosine similarity of 0.95 vs. 0.3, which would receive more experts under LayerMoE and why?

## Architecture Onboarding

- **Component map:**
  - Dense base model -> New experts (layer-wise allocated) -> Router network (per layer) -> Classification network (top-K high-similarity layers) -> Old expert (expert 0 per layer)

- **Critical path:**
  1. Compute similarity \(S^i\) for each layer using sampled tokens (Eq. 8-9)
  2. Allocate experts per layer using Eq. 10
  3. Stage 1: Train new experts + routers on new-language data (original frozen)
  4. Stage 2: Add classifiers to top-K high-similarity layers; train routers + classifiers on mixed data with LPR + classification loss

- **Design tradeoffs:**
  - More experts vs. efficiency: Fewer experts reduce parameter cost but may underfit language-specific features
  - Classifier coverage (K): More classifiers improve old-language preservation but may constrain new-language routing; paper finds top-7 optimal for single-expansion
  - Total expert budget (δ): Paper fixes δ=72 for fair comparison; optimal budget unexplored

- **Failure signatures:**
  - Random allocation (ablation): "w/random" reduces New-avg by 0.60–0.71 (Table 3)
  - No classifier (ablation): "w/o classifier" reduces Old-avg by 0.47–0.67 (Table 3)
  - Wrong layer selection for classifiers: Adding classifiers to low-similarity layers ("last-7") underperforms high-similarity selection (Table 4)

- **First 3 experiments:**
  1. **Similarity profile computation:** On your base model, compute layer-wise similarity between old (proficient) and new (target) languages using 100K sampled tokens; visualize to identify high/low-similarity layers
  2. **Baseline MoE-LPR reproduction:** Implement uniform expert allocation (e.g., 3 experts/layer) with two-stage training; measure Old-avg and New-avg on your language pair
  3. **LayerMoE with K sweep:** With fixed expert budget, apply similarity-based allocation; sweep K (1–10) for classifier placement and identify peak avg performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the optimal total number of new experts ($\delta$) be determined automatically for a given model or language group, rather than setting it arbitrarily to match baseline parameters?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they "set the total number of new experts according to the baseline 'MoE-LPR (3*24)' for a fair comparison rather than exploring the optimal total number."
- **Why unresolved:** The current study prioritized head-to-head parameter efficiency comparisons over finding the theoretical limit or "sweet spot" for the absolute number of experts required for LayerMoE.
- **What evidence would resolve it:** An ablation study analyzing performance curves across a wide range of total expert counts ($\delta$) to identify saturation points independent of baseline constraints.

### Open Question 2
- **Question:** What are the underlying linguistic or structural factors that cause LayerMoE's performance improvements to be inconsistent across different benchmarks and target languages?
- **Basis in paper:** [explicit] The Limitations section notes that while average metrics improved, "the improvements in different benchmarks and languages are inconsistent. The inherent reasons for the inconsistency are not yet clear and need further investigation."
- **Why unresolved:** The paper observes that performance gains vary (e.g., G1 vs. G2 expansion) but does not conduct a deep analysis into whether this is caused by tokenization efficiency, script overlap, or morphological complexity.
- **What evidence would resolve it:** A fine-grained analysis correlating performance variance with specific metrics such as token fertility (tokens per word) or syntactic similarity between the old ($L_{old}$) and new ($L_{new}$) language groups.

### Open Question 3
- **Question:** Why does the order of language expansion in lifelong learning settings cause asymmetric performance outcomes (e.g., $G1 \to G2$ helps $G2$, but $G2 \to G1$ does not help $G1$)?
- **Basis in paper:** [inferred] The Results section observes that "G0→G1→G2 brings better expansion performance for G2... But G0→G2→G1 does not show similar improvement for G1," admitting this difference indicates "non-trivial influences" that are not explained.
- **Why unresolved:** The paper quantifies the asymmetry but does not propose a mechanism for why specific learning orders benefit the second language group while others do not.
- **What evidence would resolve it:** Experiments isolating the "interference" or "positive transfer" between specific language pairs (e.g., Latin-script vs. non-Latin-script) during the sequential routing training stage to see if initial exposure to diverse scripts primes the model better.

## Limitations

- **Limited linguistic diversity:** Evaluation is restricted to Indo-European and a few Asian languages, leaving generalization to non-Latin scripts and distant language families untested.
- **Inherited training procedure:** The two-stage training approach is adopted from MoE-LPR without rigorous ablation of whether freezing only experts (vs. entire model) would yield better performance.
- **Efficiency measurement gaps:** While parameter reduction is quantified, the paper does not report wall-clock training time, inference latency, or memory usage during routing, leaving the practical efficiency gains unclear.

## Confidence

**High Confidence:** The core empirical finding that LayerMoE outperforms MoE-LPR with fewer experts (Old-avg: 0.47-0.67 point improvement in single-expansion; New-avg: 0.60-0.71 point improvement) is well-supported by ablation studies. The layer-wise allocation mechanism is clearly specified and reproducible.

**Medium Confidence:** The interpretation that similarity-based allocation works because high-similarity layers contain language-agnostic features is plausible but not definitively proven. Alternative explanations (e.g., high-similarity layers simply have more capacity, or the classifier component dominates performance) cannot be ruled out without additional control experiments.

**Low Confidence:** The claim that LayerMoE provides a general solution for lifelong language expansion is overstated. The paper only demonstrates two expansion steps (G0→G1→G2) without testing whether the method scales to 5+ languages or whether similarity-based allocation remains effective when expanding to languages from distant families.

## Next Checks

1. **Cross-Linguistic Generalization Test:** Apply LayerMoE to a language pair with minimal typological similarity (e.g., Japanese and Swahili) and measure whether the similarity-based allocation still produces performance gains over uniform allocation. This would validate whether the mechanism depends on shared linguistic features or genuinely identifies language-agnostic layers.

2. **Dynamic Expert Budget Ablation:** Systematically vary the total expert budget δ (e.g., 36, 72, 108) while maintaining similarity-based allocation to determine whether the 60% efficiency gain is robust or dependent on the specific budget chosen for comparison with MoE-LPR.

3. **Classifier Necessity Analysis:** Remove the classifier component entirely while maintaining similarity-based expert allocation, and measure the degradation in Old-avg performance. This would quantify the actual contribution of the classifier versus the routing algorithm itself.