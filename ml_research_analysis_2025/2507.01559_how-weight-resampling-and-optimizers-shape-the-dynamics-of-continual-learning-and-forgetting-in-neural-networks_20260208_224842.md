---
ver: rpa2
title: How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning
  and Forgetting in Neural Networks
arxiv_id: '2507.01559'
source_url: https://arxiv.org/abs/2507.01559
tags:
- learning
- training
- loss
- transfer
- zapping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effects of weight resampling ("zapping")
  during neural network training on continual learning and transfer performance. The
  authors examine how zapping in the final layer influences model adaptation and recovery
  when transferred to new domains.
---

# How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks

## Quick Facts
- arXiv ID: 2507.01559
- Source URL: https://arxiv.org/abs/2507.01559
- Authors: Lapo Frati; Neil Traft; Jeff Clune; Nick Cheney
- Reference count: 40
- Primary result: Models pre-trained with weight resampling ("zapping") in the final layer recover more quickly from transfer shocks, and Adam optimizer shows sustained improvement on previous tasks unlike SGD.

## Executive Summary
This paper investigates how weight resampling (zapping) during pre-training and optimizer choice affect continual learning and transfer performance. Through experiments on Omniglot and Omni-image datasets, the authors show that models pre-trained with zapping recover more quickly from transfer shocks, with lower layers being less affected by perturbations. The study also reveals that optimizer choice significantly impacts learning dynamics, with Adam showing sustained improvement on previous tasks even after training concludes, unlike SGD. These findings highlight the importance of both architectural interventions and optimizer selection in designing robust continual learning systems.

## Method Summary
The paper employs a 3-block ConvNet with InstanceNorm, ReLU, and MaxPool layers (256 channels), plus a single fully connected classifier. Pre-training involves standard training or zapping (resampling the final FC layer weights) during training. Transfer experiments resample the FC layer before fine-tuning on new classes. The study compares SGD with momentum and Adam optimizers across sequential learning tasks, tracking per-task loss dynamics rather than just aggregate accuracy to diagnose interference and backward transfer patterns.

## Key Results
- Models pre-trained with zapping recover more quickly from transfer shocks and show reduced downstream effects on convolutional layers
- Adam optimizer exhibits sustained improvement on previous tasks after training ends, unlike SGD
- Adam achieves higher final accuracy (85.8%) compared to SGD (76.7%) after multiple epochs in sequential learning settings
- Optimizer choice creates complex patterns of synergy and interference between tasks during sequential learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zapping during pre-training induces rapid realignment of the final layer after perturbation and reduces downstream effects on convolutional layers.
- Mechanism: Repeated resampling of the last fully connected (FC) layer forces the network to learn features that are robust to sudden output-layer changes. When later transfer shocks occur, the FC layer returns toward its pre-zap trajectory faster, and gradient feedback disturbs lower layers less.
- Core assumption: The network settles into weight configurations where FC-layer changes do not propagate large gradients into convolutional layers during subsequent updates.
- Evidence anchors:
  - [abstract] "models that have undergone zapping during training more quickly recover from the shock of transferring to a new domain"
  - [section 3.2, Fig. 2b] "if a model was pretrained with zapping, its FC layer more easily re-aligns with its unperturbed control... lower layers are less affected"
  - [corpus] Weak direct corpus support for this specific zapping mechanism; no closely matching external anchor.
- Break condition: If learning rate is too high, the FC layer may diverge after resampling rather than realign (see Fig. 3b); if too low, recovery is slow and transfer benefits diminish.

### Mechanism 2
- Claim: Adam's second-moment tracking supports backward transfer, improving loss on prior tasks after training on them has ended.
- Mechanism: Adam maintains exponential moving averages of both first and second moments. The second-moment information may provide a local geometry resembling diagonal empirical Fisher preconditioning, allowing updates that reduce loss on previously seen tasks without explicit replay.
- Core assumption: The sustained improvement on prior tasks derives from stored optimizer statistics rather than momentum alone.
- Evidence anchors:
  - [abstract] "Adam showing sustained improvement on previous tasks even after training concludes, unlike SGD"
  - [section 3.3, Fig. 4c] "loss can continue decreasing even after training on a specific task has ended... momentum alone does not explain this phenomenon"
  - [corpus] Weak direct evidence; corpus does not provide a clear external validation of Adam-specific backward transfer in this sequential few-shot setting.
- Break condition: If learning rate is too high for the sequential few-shot regime, Adam can show large interference and fail to realize backward transfer benefits (Fig. 11a).

### Mechanism 3
- Claim: Optimizer choice shapes interference and synergy patterns across tasks during sequential learning.
- Mechanism: SGD exhibits sharp task-specific loss drops followed by gradual forgetting spikes; Adam produces higher initial interference on untrained tasks but stronger and more persistent per-task improvement, especially at lower learning rates.
- Core assumption: These dynamics emerge from differences in how optimizers accumulate and use gradient statistics over time.
- Evidence anchors:
  - [abstract] "optimizer choice can also deeply affect the dynamics of learning and forgetting, causing complex patterns of synergy/interference between tasks"
  - [section 3.3, Figs. 4–6] detailed per-task loss patterns for SGD vs. Adam in linear probing and full-model tuning
  - [corpus] "ZeroFlow" addresses catastrophic forgetting but does not directly explain optimizer-specific synergy/interference patterns described here.
- Break condition: In full-model sequential training, Adam can suffer strong interference early (Fig. 5.C), which decreases in later epochs; if training is too short, Adam may underperform SGD.

## Foundational Learning

### Concept: Transfer shock and last-layer resampling
- Why needed here: The paper attributes improved recovery to pre-training with repeated resampling, then examines how networks respond when the FC layer is resampled at transfer time.
- Quick check question: What happens to lower-layer features when only the final layer is resampled before fine-tuning?

### Concept: Plasticity–stability dilemma
- Why needed here: The paper frames the trade-off between rapid learning and preserving prior knowledge as central to continual learning.
- Quick check question: Why can aggressive learning on new tasks degrade performance on previously learned tasks?

### Concept: Per-task loss decomposition in sequential learning
- Why needed here: The paper diagnoses learning/forgetting dynamics by tracking individual task losses over time rather than aggregate accuracy.
- Quick check question: How can aggregate accuracy obscure forgetting in a multi-task continual learning setting?

## Architecture Onboarding

### Component map:
3-block ConvNet: Conv->InstanceNorm->ReLU->MaxPool (no pool in final block); 256 channels per conv layer; single FC classifier

### Critical path:
1. Choose pre-training mode (zapped-IID is simpler and strong; Meta-ASB is stronger but costlier)
2. Set zapping frequency/scope per mode (epoch-level for IID; per-cycle for ASB)
3. At transfer, resample the FC layer and train sequentially or IID on novel classes
4. Use per-task loss tracking to monitor interference/backward transfer

### Design tradeoffs:
- Zapped-IID vs. Meta-ASB: IID is computationally cheaper; Meta-ASB yields higher peak accuracy (Table 1)
- SGD vs. Adam: Faster initial learning with SGD (epoch 1), higher final accuracy with Adam at lower learning rates after more epochs (Table 3)
- Linear probing vs. full-model tuning: Probing restricts interference; full tuning enables more synergy but shows complex interference patterns (Figs. 4–6)

### Failure signatures:
- FC divergence after zapping: very high learning rate causes rapid initial realignment then divergence (Fig. 3b)
- Excessive interference in full-model Adam: large loss spikes at epoch boundaries early in training (Fig. 5.C)
- Catastrophic forgetting spikes in SGD: chaotic loss spikes in later epochs (Fig. 6)

### First 3 experiments:
1. Reproduce zapped-IID vs. non-zapped IID on Omniglot/Omni-image transfer with Adam; report per-task loss and accuracy (cf. Fig. 1, Table 1)
2. Run zap-divergence protocol (section 2.2, Fig. 2a) with cosine similarity tracking for conv and FC layers across learning rates
3. Compare SGD vs. Adam in sequential full-model tuning on Omniglot over 3 epochs; log per-task losses and final accuracy (cf. Fig. 5–6, Table 3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms drive the complex patterns of synergy and interference observed during full-model sequential training with Adam, such as the reversion of interference at the start of the second epoch?
- Basis: [explicit] Section 3.3 states regarding the dynamics in Figure 5: "Overall, it’s unclear why these surprising dynamics occur, and studying them is an interesting direction for future work."
- Why unresolved: The paper documents the phenomenon—where interference at the beginning of the second epoch reverts before tasks are retrained—but lacks a theoretical explanation for why Adam's optimization path creates this specific temporal pattern.
- What evidence would resolve it: A detailed analysis of the optimizer's moment estimates and gradient variance across epoch boundaries, or ablation studies isolating the learning rate dynamics relative to the model's convergence state.

### Open Question 2
- Question: Does the connection between Adam's second-moment estimation and the diagonal Empirical Fisher Information Matrix (FIM) definitively explain the optimizer's ability to sustain improvement on previous tasks?
- Basis: [inferred] Section 4.3 hypothesizes that "the connection with the empirical FIM could contribute to the surprising capability of Adam," distinguishing it from SGD, but frames it as a potential link rather than a proven cause.
- Why unresolved: While the authors note Adam's behavior resembles Natural Gradient Descent (which uses the FIM), they do not provide experimental verification that the squared-gradient tracking is the causal factor for the observed "backward transfer."
- What evidence would resolve it: A comparative study where Adam is modified to disable the square-root transformation (as discussed in the text) or compared directly against a Natural Gradient Descent optimizer to see if the learning dynamics diverge.

### Open Question 3
- Question: Under what specific conditions do local gradient updates tracked by adaptive optimizers align with longer-term training trajectories to facilitate continual learning?
- Basis: [explicit] The Conclusion states: "Future work should focus on studying the different gradient directions tracked by adaptive optimizers, to better understand under which conditions local gradient updates can align with longer term training trajectories."
- Why unresolved: The paper demonstrates that Adam achieves better final accuracy than SGD despite slower initial learning, but it does not define the dataset or architectural constraints required for this alignment to occur.
- What evidence would resolve it: Experiments varying the sequence length and distribution shift of tasks to identify the tipping point where Adam's "memory" of past gradients ceases to align with the global loss landscape.

## Limitations

- Specific weight initialization distribution for zapping is not explicitly stated, assumed to be standard PyTorch defaults
- Batch size details are missing for both pre-training and transfer phases
- Exact pre-training epoch counts are unspecified for transfer learning results

## Confidence

- **High Confidence:** Core experimental findings showing zapping improves transfer recovery and Adam exhibits sustained backward transfer in sequential learning
- **Medium Confidence:** The mechanistic explanations for why zapping promotes robust feature learning and why Adam's second-moment tracking enables backward transfer, as these rely on limited direct evidence
- **Low Confidence:** Generalizability of optimizer-specific interference/synergy patterns beyond the specific Omniglot/Omni-image datasets and 3-layer ConvNet architecture used

## Next Checks

1. Reproduce the zap-divergence protocol with cosine similarity tracking across multiple learning rates to verify the proposed mechanism of reduced gradient propagation to lower layers
2. Systematically vary batch size and pre-training epochs to establish sensitivity and determine if the reported benefits of zapping and optimizer differences are robust
3. Extend experiments to alternative architectures (e.g., ResNet variants) and datasets to test the generalizability of the observed learning dynamics and forgetting patterns