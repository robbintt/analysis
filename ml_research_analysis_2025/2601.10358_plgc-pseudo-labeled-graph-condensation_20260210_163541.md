---
ver: rpa2
title: 'PLGC: Pseudo-Labeled Graph Condensation'
arxiv_id: '2601.10358'
source_url: https://arxiv.org/abs/2601.10358
tags:
- graph
- condensation
- plgc
- labels
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PLGC addresses the challenge of graph condensation in scenarios
  where labels are noisy, scarce, or unavailable by proposing a self-supervised framework
  that constructs latent pseudo-labels from node embeddings. The method jointly learns
  pseudo-labels and condensed graph structures through an alternating optimization
  process, eliminating reliance on ground-truth labels.
---

# PLGC: Pseudo-Labeled Graph Condensation

## Quick Facts
- **arXiv ID:** 2601.10358
- **Source URL:** https://arxiv.org/abs/2601.10358
- **Reference count:** 38
- **Key outcome:** Achieves competitive performance with supervised methods on clean data and significantly outperforms them under label noise, often improving accuracy by a large margin across five benchmark datasets for node classification and link prediction tasks.

## Executive Summary
PLGC introduces a self-supervised graph condensation framework that operates without ground-truth labels. By constructing latent pseudo-labels from node embeddings and jointly optimizing them with condensed graph structures, PLGC addresses the challenge of graph condensation in scenarios where labels are noisy, scarce, or unavailable. The method uses an alternating optimization process that eliminates reliance on ground-truth labels while maintaining competitive performance.

## Method Summary
PLGC works by first generating pseudo-labels through a self-supervised learning process that uses augmented views of the graph and Sinkhorn-Knopp optimization for balanced assignments. It then condenses the graph by optimizing synthetic node features to match these pseudo-labels through mean-square error minimization. The framework operates in two alternating stages: pseudo-label learning (using swapped-assignment view prediction) and condensation (minimizing MSE to pseudo-labels). The condensed graph is created with K synthetic nodes (one per pseudo-label) and can be used for downstream tasks without requiring original labels.

## Key Results
- PLGC achieves competitive performance with supervised graph condensation methods on clean datasets
- Under label noise, PLGC significantly outperforms supervised baselines, often improving accuracy by a large margin
- The method maintains strong performance across five benchmark datasets for both node classification and link prediction tasks
- Theoretical guarantees show pseudo-labels preserve latent structural statistics and ensure accurate embedding alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Swapped-assignment view prediction learns stable pseudo-labels that capture latent cluster structure
- **Mechanism:** Two augmentations of the graph produce embeddings Z_i and Z_j. The assignment Q computed from view j predicts pseudo-labels for embeddings from view i via cross-entropy, forcing pseudo-labels to be consistent across perturbations
- **Core assumption:** Augmentations preserve semantic content while providing sufficient diversity
- **Evidence anchors:** Abstract mentions encoding statistical relationships in node features and neighborhood structures; Section 3.1.1 defines the swapped prediction loss equation

### Mechanism 2
- **Claim:** Balanced assignment via entropy-regularized Sinkhorn prevents trivial collapse and ensures uniform pseudo-label utilization
- **Mechanism:** Assignment Q_i is solved under constraints Q_i^⊤1_B = (1/K)1_K and Q_i1_K = (1/B)1_B, encouraging equal batch allocation across K pseudo-labels
- **Core assumption:** Latent clusters are roughly balanced or tolerably imbalanced
- **Evidence anchors:** Section 3.1.1 describes the Sinkhorn-Knopp normalization solution with specific constraints

### Mechanism 3
- **Claim:** Condensed graph optimization via MSE to pseudo-labels preserves embedding geometry with provable concentration
- **Mechanism:** With Q_S = I_K, MMD reduces to minimizing ||ỹ_k - z^S_ỹ_k||². Theorem 1 provides concentration bounds under sub-Gaussian structure and separability
- **Core assumption:** Node embeddings within each latent cluster satisfy sub-Gaussian concentration; clusters are separated by ∆ > 0
- **Evidence anchors:** Section 3.1.2 explains the MMD reduction to mean-square loss; Theorem 1 provides concentration bounds

## Foundational Learning

- **Sinkhorn-Knopp algorithm for entropy-regularized optimal transport**
  - Why needed: Solves balanced assignment efficiently; required to understand Eq. 5 and its constraints
  - Quick check: Given a similarity matrix, can you explain why the row/column normalization constraints enforce balanced assignments?

- **Sub-Gaussian random variables and concentration inequalities**
  - Why needed: Theoretical guarantees (Theorem 1) rely on sub-Gaussian tail bounds
  - Quick check: Can you state why sub-Gaussianity allows Chernoff-style concentration for cluster centroids?

- **Graph augmentations for self-supervised learning**
  - Why needed: Pseudo-label stability depends on augmentation quality
  - Quick check: What properties must augmentations satisfy to preserve latent cluster structure?

## Architecture Onboarding

- **Component map:** Input graph T = (X, A) -> Augmentation -> GNN encoder -> Embeddings -> Sinkhorn assignment -> Swapped prediction loss -> Update pseudo-labels -> Initialize synthetic features X' -> GNN encoder -> MSE match to pseudo-labels -> Update X' -> Output condensed graph S = (X', I_K), pseudo-labels Ÿ

- **Critical path:** Augmentation strategy (governs pseudo-label quality) -> Choice of K (governs compression ratio and cluster granularity) -> Encoder training stability (shared encoder for both stages)

- **Design tradeoffs:** Larger K → finer granularity but less compression; stronger augmentations → more robust pseudo-labels but risk destroying structure; freezing vs. joint encoder training during condensation

- **Failure signatures:** Assignment collapse (all nodes assigned to few pseudo-labels - check entropy of Q); No separation (||ỹ_k - ỹ_ℓ|| ≈ 0 - check inter-centroid distances); Poor downstream transfer (condensed graph fails to generalize - validate on held-out tasks)

- **First 3 experiments:**
  1. Sanity check: Run PLGC on a small synthetic graph with known Gaussian clusters; verify pseudo-labels recover true centroids (measure ||ỹ_k - μ_k||)
  2. Ablation on K: Sweep K ∈ {#classes, 2×#classes, 4×#classes} and measure downstream accuracy vs. compression ratio
  3. Noise robustness test: Inject label noise at rates {0.0, 0.3, 0.5, 0.7} during condensation; compare PLGC vs. supervised baseline to reproduce Figure 4 trends

## Open Questions the Paper Calls Out

- **Extending to dynamic or heterogeneous graphs:** Future work includes applying the framework to dynamic graphs where structures evolve over time or heterogeneous graphs with diverse node and edge types. The current framework assumes static, homogeneous structures.

- **Tighter theoretical bounds under weaker assumptions:** The authors seek to explore concentration bounds that don't rely on strict assumptions of spherical sub-Gaussian latent structure or strict cluster separability.

- **Privacy-preserving and continual learning applications:** The method could be applied to privacy-preserving graph learning and continual learning settings where data streams are sequential or restricted, though current experiments assume full graph availability.

## Limitations

- The method's reliance on balanced cluster assumptions may limit performance on datasets with highly imbalanced true labels
- Specific augmentation strategies are not detailed, making it difficult to assess sensitivity to augmentation choices
- Theoretical guarantees assume sub-Gaussian structure and separability, which may not hold in real-world graphs

## Confidence

- **High Confidence:** The core alternating optimization framework is sound and well-grounded in existing SSL techniques
- **Medium Confidence:** Empirical improvements under label noise are demonstrated, but the magnitude of gains across all datasets needs verification
- **Low Confidence:** Theoretical concentration bounds are mathematically correct but may not translate to practical performance guarantees

## Next Checks

1. **Synthetic Cluster Recovery:** Test PLGC on synthetic graphs with known Gaussian clusters to verify pseudo-label quality by measuring distance between recovered and true centroids
2. **K-Ablation Study:** Systematically vary the number of pseudo-labels (K) to understand the tradeoff between compression ratio and downstream accuracy
3. **Extreme Noise Robustness:** Evaluate PLGC under severe label noise (70-80%) to validate whether performance consistently improves relative to supervised baselines