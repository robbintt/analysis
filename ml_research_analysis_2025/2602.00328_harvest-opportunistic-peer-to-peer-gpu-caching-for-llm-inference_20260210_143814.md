---
ver: rpa2
title: 'Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference'
arxiv_id: '2602.00328'
source_url: https://arxiv.org/abs/2602.00328
tags:
- memory
- harvest
- cache
- wang
- peer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Harvest addresses GPU memory bottlenecks in LLM inference by exploiting
  unused peer GPU memory via high-bandwidth NVLink interconnects. It treats remote
  HBM as a transient cache tier, moving model weights and KV cache entries without
  requiring architectural changes.
---

# Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference

## Quick Facts
- arXiv ID: 2602.00328
- Source URL: https://arxiv.org/abs/2602.00328
- Authors: Nikhil Gopal; Kostis Kaffes
- Reference count: 23
- Primary result: Up to 9.5× latency reduction and 1.5–2.0× throughput improvement for LLM inference using peer GPU memory caching

## Executive Summary
Harvest addresses GPU memory bottlenecks in LLM inference by exploiting unused peer GPU memory via high-bandwidth NVLink interconnects. It treats remote HBM as a transient cache tier, moving model weights and KV cache entries without requiring architectural changes. Harvest dynamically places objects in peer memory and handles revocation via callbacks, ensuring correctness. Evaluated on MoE and KV cache offloading, Harvest achieved up to 9.5× latency reduction for expert transfers and 1.5–2.0× throughput improvement for models like Qwen2-MoE and Phi-3.5-MoE. For KV cache, peer offloading reduced transfer latency by up to 5.65× compared to PCIe host offloading.

## Method Summary
Harvest provides a best-effort peer memory caching layer for GPU-based LLM inference, exposing `harvest_alloc`, `harvest_free`, and `harvest_register_cb` APIs for opportunistic placement of objects in remote GPU memory. The runtime controller selects peer GPUs for allocation and handles revocation through callbacks, ensuring correctness by maintaining authoritative copies in host DRAM. The system integrates with MoE-Lightning's CGOPipe for expert weight offloading and vLLM's KV manager for KV cache blocks. CUDA P2P copies via `cudaMemcpyPeerAsync` enable data movement between peer GPUs, leveraging NVLink's higher bandwidth compared to PCIe host transfers.

## Key Results
- Up to 9.5× latency reduction for expert-weight-sized transfers compared to PCIe host offloading
- 1.5–2.0× throughput improvement for models like Qwen2-MoE and Phi-3.5-MoE
- Up to 5.65× reduction in KV cache transfer latency compared to PCIe host offloading
- Expert access patterns exhibit high temporal locality, enabling effective caching in peer HBM

## Why This Works (Mechanism)

### Mechanism 1: NVLink Bandwidth Differential
Peer GPU memory access over NVLink provides significantly lower transfer latency than host DRAM access over PCIe, creating a profitable intermediate cache tier. NVLink provides direct GPU-to-GPU transfers with higher bandwidth and lower latency than PCIe. The paper demonstrates 7.5× to 9.5× latency reduction for expert-sized transfers compared to PCIe host transfers. Core assumption: The NVLink interconnect is available, uncongested, and provides consistent bandwidth during inference workloads. Break condition: NVLink congestion from concurrent model-parallel collectives, or absence of NVLink fabric (e.g., PCIe-only multi-GPU systems).

### Mechanism 2: Best-Effort Cache Tier with Correctness Decoupling
Treating peer GPU memory as a non-authoritative cache tier improves throughput without risking correctness violations. Applications maintain authoritative copies in host DRAM or make objects reconstructible. Peer allocations are opportunistic; revocation triggers fallback without data loss. This allows aggressive placement when capacity exists. Core assumption: The application can tolerate the latency of falling back to host DRAM or recomputation when peer memory is revoked. Break condition: Workloads where revocation frequency is high enough that fallback latency dominates any caching benefit.

### Mechanism 3: Temporal Locality in Expert and KV Access
MoE expert weights and KV cache blocks exhibit skewed, temporally-local access patterns that justify caching in a faster tier. Expert routing is sparse (only k≪N experts activated per token), and KV cache access is sequential during autoregressive decoding. Frequently-accessed state placed in peer HBM sees repeated reuse, amortizing transfer costs. Core assumption: Expert/KV access patterns remain stable enough across micro-batches for caching to provide benefit. Break condition: Workloads with random expert routing or KV access patterns with low temporal locality.

## Foundational Learning

- Concept: **NVLink vs PCIe interconnect characteristics**
  - Why needed here: Understanding why peer-GPU caching outperforms host offloading requires knowing NVLink provides ~5–10× higher bandwidth than PCIe for same-node transfers.
  - Quick check question: On a system with 2 GPUs connected by NVLink and a PCIe 5.0 link to host DRAM, which path has lower latency for a 350MB tensor transfer?

- Concept: **Mixture-of-Experts (MoE) routing sparsity**
  - Why needed here: Harvest's effectiveness depends on understanding that only a subset of experts (top-k) are activated per token, enabling selective caching of "hot" experts.
  - Quick check question: If an MoE model has 64 experts with top-2 routing, what fraction of expert weights are accessed per token?

- Concept: **KV cache growth during autoregressive decoding**
  - Why needed here: KV cache memory grows linearly with sequence length, creating memory pressure that motivates offloading strategies.
  - Quick check question: For a 70B model with 80 layers, 4096 hidden dimension, FP16 precision, and 16K context length, approximately how much memory does the KV cache require?

## Architecture Onboarding

- Component map:
  Harvest API (harvest_alloc, harvest_free, harvest_register_cb) -> Runtime controller (selects peer GPU, performs allocation) -> OffloadingHandler (executes CUDA P2P copies) -> Revocation callback path (drains DMA/kernels, invalidates placement, invokes application callback)

- Critical path:
  1. Application calls `harvest_alloc` for object (expert weights or KV block)
  2. Runtime selects peer GPU with available capacity (best-fit policy in prototype)
  3. Data moved to peer HBM via CUDA P2P copy
  4. On subsequent access, application checks placement map, fetches from peer if present
  5. If peer allocation revoked, callback triggers fallback to host DRAM or recomputation

- Design tradeoffs:
  - Consistency model: Harvest explicitly does NOT maintain coherence; application must manage consistency if mutating cached data
  - Isolation vs availability: MIG provides strong isolation but may restrict P2P access on some driver configurations
  - Policy flexibility: Current prototype uses best-fit; alternative policies (locality-aware, fairness-aware) not evaluated

- Failure signatures:
  - Revocation storm: Rapid peer memory churn causes repeated fallbacks, degrading throughput to baseline or worse
  - NVLink congestion: Concurrent collective operations (e.g., model-parallel all-reduce) saturate NVLink, increasing transfer latency
  - Placement map inconsistency: Incorrect residency tracking leads to stale pointer access

- First 3 experiments:
  1. **Bandwidth baseline**: Measure GPU↔GPU vs GPU↔CPU transfer latency for tensor sizes matching your model's expert weights and KV block sizes to confirm NVLink advantage.
  2. **Revocation sensitivity**: Gradually reduce peer memory availability and measure throughput degradation to identify the operating regime where caching remains beneficial.
  3. **Expert reuse profiling**: Log expert activation frequency across representative queries to quantify temporal locality before deploying MoE offloading.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Harvest perform under NVLink congestion and routing overheads in larger NVSwitch fabrics compared to the evaluated two-GPU setup? The evaluation was limited to a single two-GPU system without significant concurrent traffic or contention from model-parallel collectives.

- **Open Question 2**: Can profiling-driven dynamic page replacement policies significantly outperform static approaches for managing the KV cache in opportunistic peer memory? The current design employs standard eviction hooks without exploring workload-specific replacement algorithms that might adapt better to shifting expert activation patterns.

- **Open Question 3**: Do Just-In-Time (JIT) compilation frameworks like JAX offer performance gains for Harvest's dynamic memory management compared to PyTorch? Dynamic offloading disables static optimizations like CUDA graphs used in PyTorch, potentially limiting performance relative to frameworks that can re-compile kernels on the fly.

## Limitations
- Evaluation limited to two-GPU systems without NVSwitch fabrics or concurrent workload contention
- Does not characterize performance under high revocation frequency or fallback latency scenarios
- No comparison of alternative cache replacement policies for KV cache management

## Confidence
- **Medium**: Core performance claims supported by controlled microbenchmarks but limited system diversity
- **Medium-High**: Correctness guarantees clearly articulated with explicit API contracts
- **Medium**: Temporal locality assumptions validated for specific MoE architectures but generalization uncertain

## Next Checks
1. **Interconnect Configuration Sensitivity**: Test Harvest across different GPU configurations (PCIe-only vs NVLink-enabled, varying link counts) to quantify the minimum interconnect bandwidth required for meaningful performance gains. Measure the inflection point where NVLink advantage diminishes.

2. **Revocation Frequency Characterization**: Instrument Harvest to log revocation events and fallback latency under sustained inference workloads. Determine the maximum revocation rate that still yields net performance benefit compared to host DRAM offloading alone.

3. **MoE Architecture Generalization**: Evaluate Harvest with MoE models using different routing strategies (varying top-k values, expert counts, and load balancing schemes) to assess the breadth of temporal locality patterns where peer caching remains beneficial.