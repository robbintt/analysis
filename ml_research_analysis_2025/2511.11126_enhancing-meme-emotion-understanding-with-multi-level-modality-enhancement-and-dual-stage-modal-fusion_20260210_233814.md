---
ver: rpa2
title: Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement
  and Dual-Stage Modal Fusion
arxiv_id: '2511.11126'
source_url: https://arxiv.org/abs/2511.11126
tags:
- meme
- fusion
- text
- memes
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MemoDetector is a novel framework for Meme Emotion Understanding
  (MEU) that leverages Multimodal Large Language Models (MLLMs) for multi-level meme
  interpretation and employs a dual-stage fusion strategy to integrate enhanced visual
  and textual features. The method addresses the challenges of fine-grained multimodal
  fusion and insufficient mining of implicit meanings and background knowledge in
  memes.
---

# Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion

## Quick Facts
- arXiv ID: 2511.11126
- Source URL: https://arxiv.org/abs/2511.11126
- Authors: Yi Shi; Wenlong Meng; Zhenyuan Guo; Chengkun Wei; Wenzhi Chen
- Reference count: 9
- Key outcome: MemoDetector achieves F1 score improvements of 4.3% on MET-MEME and 3.4% on MOOD datasets compared to state-of-the-art baselines.

## Executive Summary
MemoDetector introduces a novel framework for Meme Emotion Understanding (MEU) that addresses the challenges of fine-grained multimodal fusion and insufficient mining of implicit meanings and background knowledge in memes. The method leverages Multimodal Large Language Models (MLLMs) to generate enhanced textual representations at four hierarchical levels (visual perception, text semantics, cross-modal synthesis, and cultural context). These enriched features are then integrated with visual information using a dual-stage fusion strategy, achieving significant improvements over existing approaches on two benchmark datasets.

## Method Summary
The framework operates in two main phases: first, an MLLM (Qwen2.5-VL-32B) processes each meme through a four-step prompt engineering pipeline to extract hierarchical semantic understanding (ID, TM, CIM, CA); second, a small multimodal model with dual-stage fusion integrates the enhanced textual representations with visual features. The fusion consists of a shallow concatenation stage followed by a deep bidirectional cross-attention stage, allowing the model to attend to both raw and semantically-enriched features before classification.

## Key Results
- Achieves 4.3% improvement in F1 score on MET-MEME dataset
- Achieves 3.4% improvement in F1 score on MOOD dataset
- Outperforms state-of-the-art baselines across both datasets
- Ablation studies confirm the effectiveness of both the four-step text enhancement and dual-stage fusion strategy

## Why This Works (Mechanism)

### Mechanism 1: MLLM-Driven Semantic Scaffolding
- Extracting implicit meaning and cultural context via an MLLM provides necessary semantic "scaffolding" that smaller models cannot generate independently due to limited parametric knowledge.
- A large model acts as a reasoning engine, decomposing meme understanding into four hierarchical levels and injecting external knowledge as enhanced text tokens into the small model's fusion layer.
- Core assumption: The MLLM possesses accurate world knowledge and does not hallucinate false contexts that would mislead the classifier.

### Mechanism 2: Hierarchical Feature Fusion (Dual-Stage)
- Decoupling fusion into shallow alignment and deep semantic integration stages prevents the loss of fine-grained emotional cues that often occurs in single-stage fusion.
- Stage 1 forces raw text tokens to attend to visual patches via concatenation, creating a coarse multimodal visual representation. Stage 2 applies bidirectional cross-attention between this "visual-rich" representation and the "semantically-rich" MLLM text.
- Core assumption: Shallow concatenation is sufficient for initial alignment, and subsequent cross-attention can effectively resolve resulting noise.

### Mechanism 3: Progressive Chain-of-Thought Prompting
- Structuring the textual enhancement as a cognitive hierarchy (perception → interpretation → association) yields higher quality auxiliary signals than direct inference.
- The model is prompted sequentially to describe the image, analyze text, combine them, and finally infer context, forcing the MLLM to ground its analysis in specific visual and textual evidence.
- Core assumption: The MLLM follows the prompt structure strictly and the quality of final context output depends on intermediate steps.

## Foundational Learning

- **Cross-Modal Attention Mechanisms**: Essential for the second stage of fusion where the model must learn to attend to specific words in enhanced text when looking at visual regions (and vice versa). Quick check: Can you explain how Query, Key, and Value matrices differ when attending from text-to-image versus image-to-text?

- **Knowledge Distillation (Data Augmentation view)**: The framework uses an MLLM to "teach" a smaller model by generating explanatory text. Understanding how to process and encode this auxiliary text without overfitting is critical. Quick check: How does the model handle the token length discrepancy between short raw meme text and the potentially long 4-step enhanced text sequence?

- **Vision Transformers (ViT) and XLM-R**: The architecture relies on these specific encoders to generate base feature vectors before fusion. Quick check: Why is XLM-R chosen over BERT for this specific task (hint: check the datasets used)?

## Architecture Onboarding

- **Component map**: Inference Engine (Offline) -> Encoders (ViT + XLM-R) -> Fusion Module (Stage 1 Concatenation -> Stage 2 Bi-Cross Attention) -> Classifier (Linear Layer + Softmax)

- **Critical path**: The MLLM Text Generation phase is the primary bottleneck for inference latency. The model is not end-to-end differentiable through the MLLM; it relies on frozen text generations.

- **Design tradeoffs**: The paper opts for complexity in feature preparation (generating 4 specific text sequences per image) to simplify the final classifier. This trades off data storage/processing cost for a lighter, trainable downstream classifier that outperforms zero-shot MLLMs.

- **Failure signatures**:
  - High Hallucination: If the MLLM describes objects not in the image or invents false cultural contexts, the classifier learns spurious correlations.
  - Modality Collapse: If Cross-Attention weights uniform out, the model ignores the expensive enhanced text, relying only on raw features.

- **First 3 experiments**:
  1. Ablation of Prompts: Remove the "Context Analysis" (CA) and "Combined Implicit Meaning" (CIM) steps to measure the specific contribution of deep reasoning versus surface description.
  2. Fusion Strategy Swap: Replace the Bi-Cross Attention in Stage 2 with a simple "Add" or "Concat" operation to quantify the gain from bidirectional attention.
  3. Encoder Robustness: Swap ViT for ResNet to ensure performance gains come from the fusion/enhancement strategy and not just a stronger vision backbone.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved regarding the framework's limitations and potential biases.

## Limitations
- Heavy reliance on MLLM quality and computational cost for text generation creates a significant inference bottleneck and data storage burden.
- Performance is contingent on the MLLM's ability to accurately capture cultural context and implicit meanings, with no external validation of generated content.
- Limited evaluation on diverse meme formats and domains raises questions about generalization to emerging meme templates and emotional contexts.

## Confidence

- **High Confidence**: The reported F1 score improvements (4.3% on MET-MEME, 3.4% on MOOD) over state-of-the-art baselines are statistically significant and well-supported by ablation studies.

- **Medium Confidence**: The dual-stage fusion strategy's specific contribution to performance gains is less certain without direct comparison to alternative fusion methods.

- **Low Confidence**: The framework's ability to reliably handle implicit meanings and background knowledge is highly dependent on MLLM accuracy, which lacks external validation.

## Next Checks

1. **Ablation of Fusion Strategy**: Replace the dual-stage fusion with a single-stage fusion and simple concatenation baseline to quantify the specific contribution of the staged approach.

2. **MLLM Output Validation**: Conduct human evaluation of a random sample of MLLM-generated "Context Analysis" texts to assess accuracy, relevance, and freedom from hallucination.

3. **Robustness Testing**: Evaluate the model on memes with low-resolution images, minimal text, or highly niche cultural references to measure performance degradation and identify failure modes.