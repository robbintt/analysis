---
ver: rpa2
title: Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter
  Space
arxiv_id: '2509.25876'
source_url: https://arxiv.org/abs/2509.25876
tags:
- policy
- gradient
- learning
- on-policy
- explorler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ExploRLer, a pluggable pipeline that integrates
  empty-space search (ESA) into on-policy reinforcement learning (RL) to improve exploration
  and convergence. ExploRLer leverages the observation that policy checkpoints within
  an iteration often leave unexplored, high-performing regions in parameter space.
---

# Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space

## Quick Facts
- **arXiv ID:** 2509.25876
- **Source URL:** https://arxiv.org/abs/2509.25876
- **Reference count:** 40
- **Primary result:** ExploRLer consistently improves PPO/TRPO performance on MuJoCo, Box2D, and classic control tasks by exploring sparse parameter spaces at the iteration level.

## Executive Summary
This paper introduces ExploRLer, a pluggable pipeline that enhances on-policy reinforcement learning by integrating empty-space search (ESA) to improve exploration and convergence. The key insight is that policy checkpoints within an iteration often leave unexplored, high-performing regions in parameter space. By periodically collecting these checkpoints as anchors, applying ESA to generate candidate policies, and evaluating them online, ExploRLer efficiently corrects gradient drift without increasing the number of gradient updates. The method operates at the iteration level, making it computationally efficient compared to batch-level methods.

## Method Summary
ExploRLer integrates ESA into on-policy RL algorithms like PPO and TRPO. During training, it collects policy checkpoints (anchors) at the end of each epoch within an iteration. Every 10 iterations, it uses ESA to generate candidate policies by exploring the "empty" regions in parameter space defined by these anchors. Candidates are evaluated online via short rollouts, and the best-performing policy replaces the current one. The method uses a Lennard-Jones potential to guide particles toward sparse regions, making it a zero-order (gradient-free) search method. Experiments show consistent improvements over PPO and TRPO baselines.

## Key Results
- ExploRLer improves PPO/TRPO performance on MuJoCo, Box2D, and classic control tasks.
- The method operates at the iteration level, reducing computational overhead compared to batch-level approaches.
- Ablation studies confirm the importance of ESA and pretrained policies for effectiveness.

## Why This Works (Mechanism)

### Mechanism 1
Standard on-policy gradient estimates often suffer from high variance and poor alignment with the true reward landscape; searching the parameter space "around" these updates may recover higher-performing policies missed by the gradient direction. ExploRLer collects intermediate checkpoints (anchors) during a training iteration. Instead of relying solely on the stochastic gradient, it uses a geometry-based search (ESA) to identify "empty" regions in the parameter space defined by these anchors. By sampling and evaluating candidates from these sparse regions, the system can "jump" to better weight configurations, effectively correcting gradient drift. Core assumption: The trajectory of gradient updates within an iteration traces a manifold where superior policies exist in nearby, unvisited ("empty") parameter regions, rather than strictly along the gradient vector. Evidence anchors: [abstract] "...reveal that higher performing solutions often lie in nearby unexplored regions..." [section 4.1] "Visualization of the reward landscape shows that PPO's and TRPO's surrogate objectives deviate increasingly from the real reward surface..." Break condition: If the reward landscape is convex or the gradient noise is negligible, the "empty" regions surrounding the trajectory likely offer no improvement over the gradient path, rendering the search overhead wasteful.

### Mechanism 2
Intervening at the iteration level rather than the batch level decouples exploration from gradient computation, reducing computational overhead while maintaining the ability to correct course. The pipeline applies the Empty-Space Search (ESA) only after a fixed number of RL iterations (e.g., every 10 iterations). It aggregates recent checkpoints to define a local search manifold. By skipping batch-level perturbations, it avoids the heavy computational cost of repeated gradient re-estimation or massive parallel rollouts per update. Core assumption: Policy drift occurs over the medium-term (iterations), not instantaneously (batches), such that a periodic correction is sufficient to stabilize convergence. Evidence anchors: [abstract] "It operates at the iteration level, making it computationally efficient compared to batch-level methods." [section 4.2] "Iteration-level gradient correction... reduces computational overhead in single updates..." Break condition: If the environment changes rapidly or requires extremely fine-grained adaptation, a 10-iteration delay in exploration may be too slow to prevent catastrophic forgetting or policy collapse.

### Mechanism 3
A zero-order (gradient-free) search operator can effectively approximate high-value regions in the parameter space defined by recent policy history without requiring differentiable rewards. The Empty-Space Search (ESA) uses a Lennard-Jones potential between "particles" (candidate parameters) and "anchors" (checkpoints). Particles are repelled from anchors to find sparse regions and then evaluated online. This physics-based heuristic assumes that high-performing policies lie in the "voids" between observed checkpoints. Core assumption: High-performing policies are distributed such that they occupy distinct clusters or manifolds in parameter space, and the "voids" between current low-performing or suboptimal checkpoints are predictive of better performance. Evidence anchors: [section 3.2] "Empty spaces are contiguous regions that contain no observed samples but may hold novel and potentially high-value data..." [section 4.3] "As a zero-order method, ESA can escape the surrogate-gradient direction..." Break condition: If the parameter space is extremely high-dimensional and the "empty" regions are vast and mostly low-performing, the heuristic may sample mostly noise, failing to find better policies.

## Foundational Learning

- **Concept:** **On-Policy Training Loop Granularity**
  - **Why needed here:** The paper specifically intervenes at the "iteration" level (fresh data collection) vs. "epoch" (multiple passes) or "batch" (single update). Understanding this hierarchy is required to place the ESA module correctly.
  - **Quick check question:** Can you distinguish between when a rollout buffer is filled (iteration) vs. when it is sampled for a weight update (batch)?

- **Concept:** **Surrogate Objective & Gradient Drift**
  - **Why needed here:** The core motivation is that the PPO/TRPO surrogate objective is a noisy proxy for true reward. You must understand why "trusting" the gradient blindly leads to suboptimal convergence.
  - **Quick check question:** Does PPO optimize the true expected return directly, or a clipped approximation of it?

- **Concept:** **Zero-Order Optimization**
  - **Why needed here:** ESA is a gradient-free method. Understanding that it relies solely on function evaluation (rollouts) and not backpropagation is key to analyzing its computational cost and "pluggable" nature.
  - **Quick check question:** Does a zero-order method require access to the gradient of the loss function, or only the scalar output (reward) of the function?

## Architecture Onboarding

- **Component map:** Base Learner (PPO/TRPO) -> Anchor Buffer -> ESA Engine -> Online Evaluator -> Policy Updater
- **Critical path:**
  1. **Pretraining:** Train standard PPO/TRPO for 1M steps (recommended).
  2. **Trigger:** Every $I$ iterations (default 10), pause gradient updates.
  3. **Search:** ESA generates candidates from the last $I$ checkpoints.
  4. **Evaluate:** Run short rollouts (e.g., 3 episodes) for each candidate to rank them.
  5. **Resume:** Load best candidate weights into the Base Learner and continue training.

- **Design tradeoffs:**
  - **Evaluation Overhead vs. Correctness:** The method requires 3 extra rollouts per candidate per iteration. While cheaper than batch-level correction, this adds ~6 episodes per iteration.
  - **Pretraining:** The paper recommends starting ESA after 1M steps. Early application may fail as initial random weights do not form a meaningful geometric manifold for ESA.
  - **Evaluation Noise:** Relying on only 3 episodes for ranking is high-variance. The paper argues it is sufficient for *relative* ranking, but this is a risk.

- **Failure signatures:**
  - **Divergence after Resume:** If the best candidate is actually worse than the current policy due to evaluation noise, training may destabilize.
  - **Stagnation:** If ESA parameters (e.g., $\sigma$ or step size $\alpha$) are too small, candidates will be identical to anchors; if too large, they may diverge into incoherent policy regions.
  - **High Variance in Hopper/Humanoid:** Environments with high return variance (like Hopper) may mislead the short evaluator.

- **First 3 experiments:**
  1.  **Validation of "Empty Space" Visualization:** Replicate Figure 1. Train PPO for 1000 steps, save 10 checkpoints, project to 2D (PCA), and plot reward contours to confirm high-reward regions exist near but not *at* the checkpoints.
  2.  **Pipeline Integration Test:** Implement the "Anchor -> ESA -> Evaluate -> Resume" loop on a simple environment (Pendulum) without pretraining to verify the mechanics of weight loading and iteration counting.
  3.  **Ablation on Pretraining:** Compare ExploRLer started at iteration 0 vs. iteration 1000 vs. the recommended 1M steps to validate the paper's claim that pretraining is necessary for geometric stability.

## Open Questions the Paper Calls Out

- **Question:** How can parameter-space exploration be effectively adapted for off-policy algorithms where policy and value networks are tightly coupled?
  - **Basis in paper:** [explicit] The authors state in Section 6 that applying gradient correction to off-policy learning is a "promising avenue for future work" after preliminary tests on SAC showed limited effectiveness.
  - **Why unresolved:** The ExploRLer pipeline relies on the decoupled nature of on-policy architectures to safely replace policy parameters; off-policy coupling introduces approximation errors that destabilize this insertion.
  - **What evidence would resolve it:** A successful integration of ESA into an off-policy algorithm (e.g., SAC) that demonstrates stable convergence and performance gains comparable to the on-policy results.

- **Question:** Can off-policy evaluation (OPE) or world models fully replace online rollouts for candidate selection without introducing selection bias?
  - **Basis in paper:** [explicit] Section 6 notes that while hybrid evaluation helps, "standalone OPE under non-stationary data remains an open challenge."
  - **Why unresolved:** The non-stationary nature of the training data distribution causes current OPE methods to fail at reliably ranking candidate policies without online verification.
  - **What evidence would resolve it:** A study showing that a purely offline evaluation metric can select candidates that result in identical or superior final returns compared to the current online evaluation protocol.

- **Question:** What is the optimal initialization strategy or "start time" for ExploRLer, given the mixed results between pretrained and scratch training?
  - **Basis in paper:** [inferred] Section 5.3 shows ablation results where neither training from scratch nor starting from a pretrained policy consistently dominates across all environments.
  - **Why unresolved:** While pretraining is recommended for computational efficiency, the ablation indicates the method's sensitivity to initial conditions is not fully understood or optimized.
  - **What evidence would resolve it:** An adaptive mechanism that dynamically triggers the ESA pipeline based on gradient drift metrics rather than a fixed pretraining schedule.

## Limitations

- The ESA mechanism is presented as novel, but lacks rigorous comparison against established derivative-free optimizers like CMA-ES or Bayesian optimization baselines.
- The method relies on short evaluation episodes (3 per candidate) for selection, which is high-variance and could lead to noisy selections in environments with high return variance.
- The Lennard-Jones potential formulation for ESA is novel but lacks detailed justification for the chosen parameters or sensitivity analysis.

## Confidence

- **High confidence:** The core observation—that policy checkpoints within an iteration often leave unexplored, high-performing regions in parameter space—is well-supported by the visualization in Figure 1.
- **Medium confidence:** The reported performance improvements on MuJoCo, Box2D, and classic control tasks are consistent across experiments, but the lack of direct comparison to other exploration methods limits the strength of the claim that ESA is the key driver of improvement.
- **Low confidence:** The claim that ExploRLer is "pluggable" and works seamlessly with both on-policy and off-policy algorithms is based on only preliminary results.

## Next Checks

1. **Ablation on ESA vs. Other Zero-Order Methods:** Replace ESA with a simpler random search or a standard derivative-free optimizer (e.g., CMA-ES) to isolate whether the specific ESA heuristic is necessary for the gains.
2. **Sensitivity Analysis of Evaluation Budget:** Systematically vary the number of evaluation episodes per candidate (e.g., 1, 3, 5, 10) to quantify the trade-off between selection accuracy and computational cost.
3. **Direct Comparison to Parameter-Space Noise:** Implement a version of ExploRLer that replaces ESA with parameter-space noise (e.g., from Plappert et al., 2017) to test whether the "empty space" search is more effective than traditional parameter perturbation for exploration.