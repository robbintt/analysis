---
ver: rpa2
title: Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric
  Depth Estimation for Low-Cost Autonomous UAVs
arxiv_id: '2510.16624'
source_url: https://arxiv.org/abs/2510.16624
tags:
- depth
- drone
- segmentation
- autonomous
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a vision-only autonomous flight system for
  small UAVs operating in controlled indoor environments. The system combines semantic
  segmentation with monocular depth estimation to enable obstacle avoidance, scene
  exploration, and autonomous safe landing operations without requiring GPS or expensive
  sensors such as LiDAR.
---

# Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs

## Quick Facts
- arXiv ID: 2510.16624
- Source URL: https://arxiv.org/abs/2510.16624
- Authors: Sebastian Mocanu; Emil Slusanschi; Marius Leordeanu
- Reference count: 40
- Primary result: Achieves 14.4 cm mean depth error and 100% mission success rate using vision-only navigation for low-cost UAVs

## Executive Summary
This paper presents a vision-only autonomous flight system for small UAVs operating in controlled indoor environments. The system combines semantic segmentation with monocular depth estimation to enable obstacle avoidance, scene exploration, and autonomous safe landing operations without requiring GPS or expensive sensors such as LiDAR. A key innovation is an adaptive scale factor algorithm that converts non-metric monocular depth predictions into accurate metric distance measurements by leveraging semantic ground plane detection and camera intrinsic parameters. The approach uses a knowledge distillation framework where a color-based Support Vector Machine (SVM) teacher generates training data for a lightweight U-Net student network capable of real-time semantic segmentation.

## Method Summary
The method employs a knowledge distillation framework where an HSV-based SVM teacher generates segmentation masks from manually selected patches, which train a lightweight U-Net student (1.6M parameters) for real-time inference. Monocular depth is estimated using SC-Depth variants and converted to metric depth through an adaptive scale factor algorithm that uses ground plane detection. The system operates in controlled 5×4 meter environments with cardboard obstacles, using Parrot ANAFI drone with 4K camera. Training includes hue rotation ±5°, saturation ±2%, and value ±6% augmentations. The modular system achieves 100% success rate, while an end-to-end student network trained via imitation learning achieves 87.5% success.

## Key Results
- Achieves 14.4 cm mean depth error using adaptive scale factor algorithm
- Modular system achieves 100% mission success rate in 30 real-world flights
- End-to-end student network achieves 87.5% success rate, reducing inference latency
- Real-time semantic segmentation (1.6M parameters) outperforms 6-7 second SVM inference

## Why This Works (Mechanism)

### Mechanism 1
Monocular relative depth predictions can be converted to metric depth if a known geometric reference (ground plane) is visible and camera parameters are calibrated. The system uses semantic segmentation to identify a ground plane (carpet), projects image pixels onto this plane using camera intrinsics and extrinsics to calculate geometric distances, and computes an adaptive scale factor by comparing these distances to predicted relative depth.

### Mechanism 2
A computationally expensive classical teacher can train a lightweight neural student for real-time semantic segmentation in controlled environments. An SVM trained on manually selected HSV color patches generates initial segmentation masks, which serve as "ground truth" to train a U-Net student network. The student learns to approximate the teacher's classification logic with significantly lower inference latency.

### Mechanism 3
A single compact neural policy can approximate a complex modular navigation stack via imitation learning. The best-performing modular system (Seg + Metric Depth + Heuristic Control) is used to fly the drone while recording frames and control commands. A compact student network (1.6M params) is then trained on this dataset to output velocity commands directly from images.

## Foundational Learning

- **Pinhole Camera Model & Intrinsic Matrix ($K$)**: Essential for the Adaptive Scale Factor. You cannot project 2D pixels onto a 3D ground plane without understanding focal length and principal point. Quick check: If you know a pixel $(u,v)$ and the camera matrix $K$, how do you find the ray direction vector in camera coordinates?

- **Knowledge Distillation (Teacher-Student)**: Used to compress a slow, classical SVM into a real-time U-Net. Understanding that the student learns from "pseudo-labels" (SVM outputs) rather than human ground truth is critical. Quick check: Why might a student network fail if the teacher model is overconfident on hard examples?

- **Virtual Safety Corridor**: The control logic isn't just "don't hit obstacles"; it's "stay inside this geometric prism." Understanding the 5-plane construct (Left, Right, Front, Up, Down) explains how depth maps convert to steering decisions. Quick check: How does the system decide which direction to turn when an obstacle is detected?

## Architecture Onboarding

- **Component map**: Input RGB Frame -> Branch A (Semantic): SVM (Offline) → U-Net (Online) → Segmentation Mask; Branch B (Geometric): SC-Depth (Monocular) → Relative Depth Map; Fusion: Segmentation Mask + Camera Intrinsics + Relative Depth → Metric Depth Estimation; Control: Metric Depth → 3D Safety Corridor Logic → Velocity Commands (ν, ω); Optimization: Teacher Logs → End-to-End Student Net (Deployment)

- **Critical path**: The Segmentation Branch is the linchpin. Without a successful ground plane segmentation, the Adaptive Scale Factor cannot be computed, and the system reverts to non-metric depth (or fails). If segmentation breaks, the entire metric perception pipeline breaks.

- **Design tradeoffs**: Modular vs. E2E - the modular system (Seg+Depth) achieves 100% success but requires a laptop (GPU/CPU heavy). The End-to-End student runs on embedded hardware but drops to 87.5% success (12.5% crash rate). Controlled vs. General - relying on HSV/SVM for segmentation restricts the system to environments with known textures (indoor/cardboard), trading generalization for ease of training.

- **Failure signatures**: Scale Drift - if the ground plane is misidentified, metric depth will be garbage, causing aggressive crashes or "freezing." Oscillation - in segmentation-only mode, the drone moves in small increments (30cm). If depth scaling is noisy, the drone might jitter. Student Hallucination - the student network crashes into boxes it "saw" in training but fails to generalize on slightly shifted configurations.

- **First 3 experiments**:
  1. Static Scale Validation - Before flying, take images of known objects at measured distances. Verify that the Adaptive Scale Factor computes the correct depth error against a tape measure.
  2. Segmentation Robustness Test - Vary lighting in the lab (shadows, dim lights) and run the SVM/U-Net. If the carpet mask breaks up, the depth scaling will fail.
  3. Digital Twin Simulation - Run the full pipeline in the Unreal Engine digital twin provided. Validate that the "Time to Find Helipad" metric decreases with metric depth enabled vs. segmentation-only.

## Open Questions the Paper Calls Out

### Open Question 1
How can the end-to-end student control network be improved to eliminate the 12.5% collision rate and match the reliability of the modular teacher system? The authors state that future work will focus on improving the student control network to achieve comparable reliability of the two-model approach. This is unresolved because the current compact student network crashes in 12.5% of trials, primarily due to collisions with obstacles. Evidence that would resolve this includes demonstrating a success rate comparable to the modular system (>95%) during autonomous flight tests using only the distilled student network.

### Open Question 2
Can the vision-based framework be extended to handle dynamic obstacles without compromising the real-time performance required for low-cost hardware? The conclusion identifies extending system capabilities to handle dynamic obstacles as a specific goal for future work. This is unresolved because the current validation was restricted to a controlled environment with static cardboard obstacles, whereas real-world scenarios often involve moving objects. Evidence that would resolve this includes successful autonomous navigation metrics in environments containing moving obstacles.

### Open Question 3
Does the adaptive scale factor algorithm function accurately in unstructured environments where a distinct ground plane is unavailable or poorly textured? The method relies heavily on a "segmented carpet" for metric depth calculation, and results show increased errors (0.37m) in regions with "poor texture." This is unresolved because the system assumes a semantic ground class is detectable to compute the scale factor, which is a constraint not present in all outdoor or complex indoor settings. Evidence that would resolve this includes validation of metric depth accuracy in environments without uniform, detectable floor textures.

## Limitations
- Environmental dependency on flat, texture-distinct floor for ground plane scaling mechanism
- Dataset scope limited to single 5×4m controlled environment with 8 cardboard obstacles
- End-to-end student network has 12.5% crash rate without characterization of failure modes

## Confidence

- **High confidence**: Technical derivations for adaptive scale factor algorithm, knowledge distillation framework, and modular system's 100% success rate in tested conditions
- **Medium confidence**: HSV-based SVM provides sufficient supervision for student training in controlled environment
- **Low confidence**: End-to-end student network's safety for real-world deployment given 12.5% crash rate and unknown failure modes

## Next Checks

1. **Generalization test**: Deploy the system in a different indoor environment (different lighting, obstacle materials, floor textures) and measure depth error and mission success rate.

2. **Failure mode analysis**: Instrument the student network flights to log the specific cause of each failure (collision, localization loss, control failure) to understand the 12.5% crash rate.

3. **Real-time performance verification**: Measure actual inference latency of the U-Net student on embedded hardware (not just claimed "real-time") under varying computational loads.