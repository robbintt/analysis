---
ver: rpa2
title: Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via
  an Automated Online Design Framework
arxiv_id: '2505.21559'
source_url: https://arxiv.org/abs/2505.21559
tags:
- agents
- karma
- cluster
- kubernetes
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of maintaining operational resilience
  in Kubernetes clusters, particularly under adversarial conditions like DDoS attacks.
  The core method, KARMA, introduces an automated framework for designing and implementing
  a Multi-Agent System (MAS) for Horizontal Pod Autoscaling (HPA).
---

# Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework

## Quick Facts
- **arXiv ID:** 2505.21559
- **Source URL:** https://arxiv.org/abs/2505.21559
- **Reference count:** 33
- **Primary result:** KARMA outperforms three state-of-the-art HPA systems (AWARE, Gym-HPA, Rlad-core) in sustaining operational resilience under various adversarial conditions in a complex chained service cluster.

## Executive Summary
This paper introduces KARMA, an automated framework for designing and implementing a Multi-Agent System (MAS) to achieve resilient Horizontal Pod Autoscaling (HPA) in Kubernetes clusters under adversarial conditions like DDoS attacks. The framework decomposes operational resilience into failure-specific sub-goals handled by specialized agents, each with defined roles and missions. KARMA operates through four phases: modeling a digital twin from cluster traces, training agents in simulation using MARL, analyzing agent behaviors for explainability, and transferring learned policies to the real cluster. Experimental results demonstrate KARMA's superiority over existing HPA systems in maintaining service availability, reducing latency, and achieving faster recovery times during attacks.

## Method Summary
KARMA addresses Kubernetes HPA resilience through a four-phase automated framework: (1) Digital twin modeling using MLP-based transition function approximation from collected cluster traces; (2) MARL training with MAPPO and MOISE+ organizational constraints (hard/soft) for four specialized agent roles (Bottleneck, DDoS, Failure, Resource Managers); (3) Behavior analysis via trajectory clustering and inter-agent relation graphs; (4) Policy transfer to real clusters with safety mechanisms. The method uses PettingZoo environment with cluster state vectors and defender/attacker action spaces, training until convergence criteria are met, then validating performance through operational resilience metrics across adversarial scenarios.

## Key Results
- KARMA achieves 90.9% success rate, 85.7% latency compliance, and 5.9% pending requests ratio under adversarial conditions
- Outperforms AWARE, Gym-HPA, and Rlad-core in maintaining operational resilience during DDoS attacks
- Faster recovery time (33.0s) compared to baseline HPA systems
- High adaptability with low reward standard deviation (5.3%) across episodes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing operational resilience into failure-specific sub-goals with organizational constraints accelerates policy convergence and improves coordination under adversarial conditions.
- **Mechanism:** MOISE+ organizational model assigns each agent a Role Action Guide (RAG) constraining actions and Goal Reward Guide (GRG) shaping rewards toward intermediate goals. Hard constraints (ch=1) strictly limit action spaces; soft constraints (ch=0) apply reward bonuses/penalties, narrowing policy search space and reducing conflicting scaling actions.
- **Core assumption:** Failure modes (bottlenecks, DDoS, pod crashes, resource contention) can be cleanly decomposed into semi-independent sub-goals without excessive interdependency.
- **Evidence anchors:** [abstract] decomposing operational resilience into failure-specific sub-goals; [section III.C] convergence criteria based on reward standard deviation; [corpus] weak direct corpus support for MOISE+ specifically.
- **Break condition:** If failure modes become highly correlated (e.g., DDoS triggering cascading pod failures + bottlenecks simultaneously), role boundaries may blur and agent conflicts re-emerge.

### Mechanism 2
- **Claim:** An MLP-based transition model improves digital twin fidelity, enabling safer and more effective simulation-to-real transfer.
- **Mechanism:** Collected traces form partial transition function T̂t for observed (s, a, s') tuples. MLP approximator with 3 hidden layers (128 neurons each) learns to predict next states for unobserved transitions. Complete transition function T̂ combines both, achieving 94.9% accuracy versus 83.5% without MLP.
- **Core assumption:** Kubernetes cluster state transitions are approximately Markovian—next state depends primarily on current state and actions, not long history.
- **Evidence anchors:** [section III.B] MLP choice motivated by universal approximation capabilities; [section V.C] KARMA achieves 94.9% accuracy, outperforming non-MLP model (83.5%); [corpus] neighbor paper uses similar approaches but doesn't validate transition accuracy.
- **Break condition:** If cluster exhibits highly non-Markovian behavior (e.g., long-term memory effects from accumulated resource fragmentation), MLP will systematically mispredict.

### Mechanism 3
- **Claim:** Centralized training with decentralized execution (MAPPO) enables emergent coordination while preserving deployment practicality.
- **Mechanism:** MAPPO uses centralized critic with global state information during training, allowing agents to learn cooperative strategies through shared value function. At execution, decentralized actors make independent decisions using only local observations, balancing coordination benefits with real-world deployment constraints.
- **Core assumption:** Agents can implicitly coordinate through shared training without explicit communication protocols at runtime.
- **Evidence anchors:** [section III.C] MAPPO centralizes critic for global feedback while decentralizing actors for independent decision-making; [section V.A] Multi-Agent w/ Hard Org. Spec. achieves 90.9% success rate vs Single-Agent w/ Hard Org. Spec. at 80.8%; [corpus] "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning" supports MARL for adversarial resilience in cyber-physical systems.
- **Break condition:** If runtime coordination requires explicit communication (e.g., negotiating resource handoffs during simultaneous failures), decentralized execution may produce suboptimal or conflicting actions.

## Foundational Learning

- **Concept:** **Stochastic Games (SG)**
  - **Why needed here:** The environment is formalized as a zero-sum SG between defenders and attacker. Understanding SGs (states, joint actions, transitions, rewards) is prerequisite to grasping why MARL is appropriate.
  - **Quick check question:** Can you explain why a zero-sum SG formulation makes sense for defender vs. attacker scenarios, and what limitations it might have for non-adversarial failures?

- **Concept:** **Actor-Critic Methods and PPO**
  - **Why needed here:** KARMA uses MAPPO (Multi-Agent PPO). Understanding PPO's clipping objective, the actor-critic split, and why centralized critic/decentralized actors matter is essential.
  - **Quick check question:** What problem does PPO's clipped surrogate objective solve compared to vanilla policy gradient, and why does a centralized critic help multi-agent coordination?

- **Concept:** **Organizational Models for MAS (MOISE+)**
  - **Why needed here:** KARMA inherits from MOISE+ for roles, missions, and inter-agent relations (acquaintance, authority, communication). Without this, the RAG/GRG concepts are opaque.
  - **Quick check question:** How do roles (constraints on actions) differ from missions (reward shaping), and why might hard constraints outperform soft constraints in adversarial settings?

## Architecture Onboarding

- **Component map:** Traces → Digital Twin (T̂t + MLP T̂a) → MARL Training (roles/missions) → Behavior Analysis → Policy Transfer → New traces (loop)
- **Critical path:** The framework flows from trace collection through digital twin construction, MARL training with organizational constraints, behavior analysis via clustering, and finally policy transfer to the real cluster, with new traces enabling continuous improvement.
- **Design tradeoffs:**
  - Hard vs. soft organizational constraints: Hard (96.2% alignment, faster convergence) vs. soft (more exploration, 85.3% alignment)
  - Multi-agent vs. single-agent: Multi-agent (90.9% success) requires more compute vs. single-agent (80.8%) simpler but less resilient
  - MLP transition model: Higher accuracy (94.9%) vs. lookup-only (83.5%) but adds training overhead and potential overfitting risk
- **Failure signatures:**
  - High reward variance across episodes → convergence not achieved, may need tighter organizational constraints or more training
  - Low clustering purity (<70%) → agents not specializing, role definitions may be misaligned with actual failure modes
  - Large sim-to-real performance gap → digital twin accuracy insufficient, MLP may need retraining with more diverse traces
- **First 3 experiments:**
  1. **Baseline comparison in DDoS-only scenario:** Run KARMA vs. KHPA vs. AWARE vs. Gym-HPA on a single service under sustained traffic spike. Measure recovery time and service availability. Expect KARMA < 35s recovery, KHPA > 60s.
  2. **Ablation: hard vs. soft constraints in mixed scenario:** Train two KARMA variants (hard RAG vs. soft RAG) on combined failures. Compare convergence time and final alignment score. Expect hard constraints to converge ~50% faster with higher alignment.
  3. **Digital twin accuracy validation:** Collect held-out transition tuples from real cluster. Compare MLP predictions vs. actual next states. Target >90% accuracy; if below 85%, expand training traces or increase MLP capacity.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the digital twin modeling phase be enhanced to simulate unaccounted, unexpected system failures?
  - **Basis in paper:** [explicit] The conclusion explicitly states the need to "better simulate unaccounted unexpected system failures" despite using trace-based modeling.
  - **Why unresolved:** The current MLP-based transition model learns from historical traces, making it difficult to generalize to failure modes not present in the collected data.
  - **What evidence would resolve it:** Integration of synthetic anomaly generation techniques or robust prediction mechanisms for out-of-distribution states within the simulation.

- **Open Question 2:** Can the definition of roles, missions, and reward structures be automated to reduce dependence on domain-specific expertise?
  - **Basis in paper:** [explicit] The conclusion identifies "Dependence on Domain Expertise" as a factor limiting the framework's generalizability.
  - **Why unresolved:** The current implementation relies on the MOISE+ organizational model, requiring manual definition of agent constraints based on expert knowledge.
  - **What evidence would resolve it:** A fully autonomous pipeline that infers optimal roles and missions from raw cluster data without human labeling.

- **Open Question 3:** How does the framework scale computationally when applied to multi-node clusters compared to the tested single-node environment?
  - **Basis in paper:** [explicit] The conclusion notes that current experiments focus on single-node clusters, with preliminary evaluations ongoing for larger deployments.
  - **Why unresolved:** Multi-agent coordination overhead and training convergence time may increase non-linearly with the complexity of distributed topologies.
  - **What evidence would resolve it:** Performance metrics regarding convergence time and operational resilience specifically evaluated on distributed, multi-node clusters.

- **Open Question 4:** How can formal safety guarantees be implemented to prevent catastrophic scaling decisions during deployment?
  - **Basis in paper:** [inferred] Table I lists "Safety Guarantees" as "No" for KARMA, and Section III-E relies on heuristic runtime safeguards like action capping.
  - **Why unresolved:** Reactive safeguards may fail against sophisticated adversarial exploits or unforeseen agent behaviors, lacking formal proofs of stability.
  - **What evidence would resolve it:** Integration of safe reinforcement learning techniques that provide mathematical certificates for action stability and bounded resource usage.

## Limitations

- **Digital twin fidelity dependency:** Framework performance is fundamentally bounded by the MLP transition model's accuracy (94.9%), which may not generalize to failure modes outside training traces.
- **MOISE+ decomposition assumptions:** The organizational model assumes failure modes can be cleanly separated, but adversarial conditions often trigger cascading, correlated failures that violate these boundaries.
- **Resource overhead constraints:** The 4-agent setup requires 8 vCPUs and 32GB RAM just for simulation training, limiting applicability to smaller clusters.

## Confidence

- **High confidence:** Digital twin accuracy claims (94.9% vs 83.5%) are directly measured with specified metrics
- **Medium confidence:** Operational resilience improvements (90.9% success rate) are benchmarked against specific competitors under controlled adversarial scenarios
- **Medium confidence:** Convergence benefits from organizational constraints, though hard constraints outperforming soft ones by 10.9% needs validation across more failure mode combinations
- **Low confidence:** Framework's generalizability to production clusters with complex topologies, as experiments were limited to 4 chained services on a single worker node

## Next Checks

1. **Cross-cluster generalization test:** Deploy KARMA on a different Kubernetes cluster topology (e.g., mesh instead of chain, 8 services instead of 4) and measure performance degradation from the original 90.9% success rate baseline.

2. **Cascade failure stress test:** Design an experiment where DDoS attack triggers pod failures which then cause resource contention bottlenecks.