---
ver: rpa2
title: 'Scaling Open-Weight Large Language Models for Hydropower Regulatory Information
  Extraction: A Systematic Analysis'
arxiv_id: '2511.11821'
source_url: https://arxiv.org/abs/2511.11821
tags:
- extraction
- information
- performance
- regulatory
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated open-weight LLMs (0.6B-70B
  parameters) for extracting information from hydropower regulatory documents. It
  identified a critical 14B parameter threshold where validation methods transition
  from ineffective to viable, enabling consumer-deployable models to achieve 64% F1
  through appropriate validation while smaller models plateau at 51%.
---

# Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis

## Quick Facts
- **arXiv ID**: 2511.11821
- **Source URL**: https://arxiv.org/abs/2511.11821
- **Reference count**: 8
- **Primary result**: Open-weight LLMs achieve 64% F1 at 14B parameters with validation; smaller models plateau at 51%, larger models approach 77% but need enterprise infrastructure.

## Executive Summary
This systematic analysis evaluates open-weight LLMs (0.6B-70B parameters) for extracting information from hydropower regulatory documents, identifying a critical 14B parameter threshold where validation methods transition from ineffective to viable. Below this threshold, models systematically hallucinate absent information, achieving perfect recall but near-zero precision. The study establishes the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, demonstrating that consumer-deployable models can achieve 64% F1 through appropriate validation while smaller models plateau at 51% and large-scale models approach 77% but require enterprise infrastructure.

## Method Summary
The study evaluated 7 open-weight LLMs (Qwen 0.6B-32B, Llama 1B-70B, GPT-OSS 20B) on FERC hydropower licensing documents from 2014-2017, preprocessed into 209 chunks with 1,000-word text and 200-word overlap. Four extraction methods (single-step, two-step, categorical, chain-of-thought) and three reflective validation strategies (lenient, moderate, stringent) were tested. Performance was measured using field-level precision, recall, and F1 scores computed against a bronze standard (GPT-4o mini as evaluator) using semantic comparison rather than exact string matching, with deterministic generation (temperature=0).

## Key Results
- A critical 14B parameter threshold enables viable reflective validation (F1=0.64) versus ineffective validation below threshold (F1<0.15)
- Small models (0.6B-3B) exhibit systematic hallucination with perfect recall (0.93-1.00) but near-zero precision (0.03-0.27)
- Category-specific performance varies dramatically: Capacity Information achieves F1=0.509 average while Environmental Information remains intractable at F1=0.075

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A 14B parameter threshold enables meta-cognitive validation capabilities that smaller models lack.
- **Mechanism**: Models below 14B parameters fail at self-evaluation tasks because they lack emergent reasoning abilities. Above this threshold, models can assess their own extraction confidence and meaningfully reject low-quality outputs. The validation process shifts from near-random acceptance to discriminating between supported and unsupported extractions.
- **Core assumption**: Emergent reasoning capabilities follow discrete thresholds rather than continuous improvement curves (drawing on Wei et al. 2022, cited in paper).
- **Evidence anchors**: Validation F1 remains below 0.15 regardless of validation stringency; rejection rates converge to near-zero or near-100% across all strategies.

### Mechanism 2
- **Claim**: Perfect or near-perfect recall in small models indicates systematic hallucination, not extraction success.
- **Mechanism**: Small models (0.6B-3B) exhibit a "positive generation bias"—they default to generating plausible values rather than acknowledging absent information. This produces high recall (0.93-1.00) coupled with very low precision (0.03-0.27), meaning they extract values even when information doesn't exist in source text.
- **Core assumption**: Information absence lacks distinctive features that small models can detect; baseline noise and genuine absence appear identical.
- **Evidence anchors**: Recall ≥0.95 with precision <0.30, particularly for fields where ground truth indicates information is absent.

### Mechanism 3
- **Claim**: Category-specific extraction difficulty depends on semantic complexity and standardization, not simply text vs. numerical distinction.
- **Mechanism**: Well-structured fields with standardized units (Capacity: F1=0.509 average) outperform context-dependent regulatory requirements (Environmental: F1=0.075 average). Models succeed when distinctive features enable positive signal identification but hallucinate when confronting consistent information absence or domain-specific contextual interpretation.
- **Core assumption**: Technical specifications with standardized formatting create recognizable patterns; regulatory compliance language requires contextual reasoning that scales non-linearly.
- **Evidence anchors**: Large models (70B) show dramatic improvement in some categories but remain near-zero for Environmental Information, suggesting architectural limits beyond parameter scale.

## Foundational Learning

- **Concept: Signal Detection Theory Applied to LLM Hallucination**
  - **Why needed here**: The paper reframes hallucination as a signal detection problem—models can identify positive signals (patterns present) but struggle with negative signals (absence lacks features). This explains why recall metrics mislead.
  - **Quick check question**: If a model achieves 95% recall on regulatory extraction, what two possible explanations exist, and how would you distinguish them?

- **Concept: Emergent Capability Thresholds vs. Continuous Scaling**
  - **Why needed here**: Performance improvements aren't linear with parameter count. Certain capabilities (meta-cognitive validation, self-evaluation) emerge discontinuously around 14B parameters, contradicting assumptions of gradual improvement.
  - **Quick check question**: Why might doubling parameters from 7B to 14B produce larger performance gains than doubling from 32B to 64B for validation tasks?

- **Concept: Aggregate Metrics Masking Category-Specific Failures**
  - **Why needed here**: A model achieving acceptable overall F1 (e.g., 0.42) might simultaneously fail catastrophically on mission-critical regulatory categories. Basic Information fields (41% of targets) dominate aggregate scores while hiding systematic failures.
  - **Quick check question**: How could a model with 75% overall precision still be dangerous for regulatory compliance deployment?

## Architecture Onboarding

- **Component map**: Document preprocessing (1,000-word chunks with 200-word overlap) -> Extraction layer (single-step, two-step, categorical, or chain-of-thought) -> Validation layer (reflective reasoning with lenient/moderate/stringent rejection) -> Evaluation layer (bronze standard with semantic comparison)

- **Critical path**: 1. Preprocess regulatory documents into overlapping chunks 2. Run single-step extraction with deterministic generation (temperature=0) 3. Apply reflective reasoning validation (only viable if model ≥14B parameters) 4. Evaluate category-specific performance, not just aggregate F1

- **Design tradeoffs**: Two-step extraction reduces false positives but causes near-complete failure (F1=0.00-0.05) across all model sizes—avoid; categorical extraction works best for small models; single-step provides most consistent baseline; stringent validation maximizes precision but requires ≥14B models

- **Failure signatures**: Recall ≥0.95 with precision <0.30: Systematic hallucination in small models; Validation rejection rates near 0% or 100% across all stringency levels: Meta-cognitive failure; Complete F1=0.000 on specific categories (e.g., Environmental): Architectural limitation, not training issue

- **First 3 experiments**: 1. Benchmark Qwen 14B with single-step extraction and moderate validation; verify rejection rates fall in 40-60% range 2. Compare Llama 3B recall vs. precision across all six information categories to identify category-specific hallucination patterns 3. Test whether two-step extraction failure replicates with different prompt formulations before abandoning this approach

## Open Questions the Paper Calls Out

- Would human-annotated gold standard evaluation reveal different performance patterns or hallucination rates than the bronze standard approach using GPT-4o mini?
- What specific architectural mechanisms could enable reliable information absence detection in open-weight LLMs?
- Why does Environmental Information extraction remain largely intractable (F1 = 0.075) even at 70B parameters, while other technical categories show substantial improvement?
- Does the 14B parameter threshold for effective reflective reasoning generalize to other regulatory domains and document structures?

## Limitations

- **Corpus-specificity concerns**: Findings rely on FERC hydropower regulatory documents from a specific 4-year period (2014-2017); unclear whether 14B threshold generalizes to other regulatory domains
- **Prompt engineering dependence**: Performance varies dramatically based on prompting strategy; optimal configurations may depend heavily on prompt engineering quality rather than fundamental capability
- **Bronze standard reliability**: Evaluation uses GPT-4o mini as judge, introducing potential circularity and bias that could make the 14B threshold finding partially an artifact of the evaluation method

## Confidence

- **High confidence**: Systematic hallucination patterns in small models showing perfect recall with low precision; category-specific performance differences (Capacity vs. Environmental Information)
- **Medium confidence**: 14B parameter threshold for viable validation—most significant finding but requires additional validation across domains
- **Low confidence**: Practical deployment guidance assuming validation strategies transfer well to production environments with different document distributions

## Next Checks

1. **Cross-domain threshold validation**: Test the 14B parameter hypothesis on a completely different regulatory corpus (e.g., SEC filings or FDA drug approval documents) to determine if the same critical threshold emerges.

2. **Independent bronze standard verification**: Generate an alternative bronze standard using human annotation of a subset of documents to assess whether the 14B threshold remains significant when evaluated against ground truth.

3. **Prompt robustness testing**: Systematically vary the prompt templates for single-step extraction and reflective validation across 14B threshold models to determine sensitivity of performance gains to specific prompt formulations.