---
ver: rpa2
title: Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning
arxiv_id: '2509.19631'
source_url: https://arxiv.org/abs/2509.19631
tags:
- speech
- summarization
- phi-4mm
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the performance gap between open-source multi-modal
  large language models (MLLMs) and state-of-the-art text-based LLMs in speech summarization,
  a task of generating textual summaries directly from spoken input. The authors propose
  a three-stage training framework: (1) supervised fine-tuning on large-scale synthetic
  summarization data, (2) on-policy knowledge distillation to transfer summarization
  ability from a strong text-based LLM to the audio-conditioned MLLM, and (3) Direct
  Preference Optimization to reduce hallucinations and improve output quality.'
---

# Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.19631
- Source URL: https://arxiv.org/abs/2509.19631
- Reference count: 0
- One-line primary result: Achieves up to 28% relative improvement over baselines in speech summarization using a three-stage RL training pipeline

## Executive Summary
This paper addresses the performance gap between open-source multi-modal large language models (MLLMs) and state-of-the-art text-based LLMs in speech summarization. The authors propose a three-stage training framework: supervised fine-tuning on synthetic summarization data, on-policy knowledge distillation to transfer summarization ability from a strong text-based LLM, and Direct Preference Optimization to reduce hallucinations and improve output quality. The approach enables the student MLLM to benefit from the teacher's linguistic competence while grounding it in audio, thereby narrowing the modality gap.

## Method Summary
The proposed approach employs a three-stage training framework: (1) supervised fine-tuning on large-scale synthetic summarization data, (2) on-policy knowledge distillation to transfer summarization ability from a strong text-based LLM to the audio-conditioned MLLM, and (3) Direct Preference Optimization to reduce hallucinations and improve output quality. This training pipeline enables the student MLLM to benefit from the teacher's linguistic competence while grounding it in audio, thereby narrowing the modality gap.

## Key Results
- Up to 28% relative improvement over strong baselines in speech summarization
- Surpasses much larger MLLMs such as GPT-4o-audio in performance
- Demonstrates strong zero-shot cross-lingual generalization capabilities

## Why This Works (Mechanism)
The three-stage training pipeline works by first establishing a strong baseline through supervised fine-tuning on synthetic data, then transferring knowledge from a superior text-based LLM through distillation, and finally refining outputs to reduce hallucinations and improve quality through preference optimization.

## Foundational Learning
- **Multi-modal LLM architecture**: Understanding how audio and text modalities are processed together is essential for grasping the model's capabilities and limitations. Quick check: Review the input/output handling of typical MLLMs.
- **Knowledge distillation**: This technique transfers knowledge from a larger, more capable model to a smaller one, enabling performance gains. Quick check: Understand the basic teacher-student paradigm in ML.
- **Direct Preference Optimization**: DPO fine-tunes models based on human preference data, improving output quality and reducing undesirable behaviors. Quick check: Review the mechanics of DPO and its application in LLM fine-tuning.

## Architecture Onboarding
**Component Map**: Audio encoder -> MLLM backbone -> Summarization decoder -> DPO layer
**Critical Path**: Audio input → Encoding → Multi-modal fusion → Text generation → Preference optimization
**Design Tradeoffs**: Balancing model size with performance, choosing between synthetic vs. real data for training, and determining the optimal mix of distillation and preference optimization
**Failure Signatures**: Hallucinations in summaries, poor cross-lingual generalization, and degradation in audio-text alignment
**First Experiments**:
1. Ablation study to isolate contributions of each training stage
2. Cross-lingual evaluation across diverse language families
3. Hallucination detection using established metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data quality and diversity are unclear, raising concerns about potential overfitting
- Evaluation metrics for hallucination detection are not fully specified
- Cross-lingual generalization results lack detail on tested languages and their linguistic distances

## Confidence
- **Performance Gains**: High - supported by experimental results, though ablation studies would strengthen confidence
- **Zero-Shot Cross-Lingual Generalization**: Medium - promising but lacks detail on languages and evaluation methodology
- **Reduction in Hallucinations**: Low - DPO stage described but evaluation metrics and evidence are insufficient

## Next Checks
1. Conduct ablation studies to isolate the contributions of each training stage (SFT, knowledge distillation, DPO) to the reported performance gains
2. Expand cross-lingual evaluation to include a broader range of languages with varying linguistic distances from the training data, and report detailed results for each language
3. Perform human evaluation or use established hallucination detection metrics to rigorously assess the effectiveness of the Direct Preference Optimization stage in reducing hallucinations