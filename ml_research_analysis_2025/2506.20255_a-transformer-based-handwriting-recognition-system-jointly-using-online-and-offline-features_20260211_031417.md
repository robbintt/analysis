---
ver: rpa2
title: A Transformer Based Handwriting Recognition System Jointly Using Online and
  Offline Features
arxiv_id: '2506.20255'
source_url: https://arxiv.org/abs/2506.20255
tags:
- recognition
- stroke
- handwriting
- image
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a handwriting recognition system that fuses
  online stroke data and offline image data early in the model pipeline using cross-modal
  attention. By encoding visual patches and pen trajectories into a shared latent
  space, the approach enables temporal and visual cues to reinforce each other during
  learning.
---

# A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features

## Quick Facts
- arXiv ID: 2506.20255
- Source URL: https://arxiv.org/abs/2506.20255
- Reference count: 40
- Primary result: Early cross-modal fusion of online stroke and offline image data improves handwriting recognition accuracy by up to 1.5% over prior methods

## Executive Summary
This paper introduces a handwriting recognition system that fuses online stroke data and offline image data early in the model pipeline using cross-modal attention. By encoding visual patches and pen trajectories into a shared latent space, the approach enables temporal and visual cues to reinforce each other during learning. Experiments on IAMOn-DB, VNOn-DB, and ISI-Air show state-of-the-art accuracy, with improvements up to 1.5% over prior methods. The method remains robust even when only one modality is available, demonstrating effective multimodal integration.

## Method Summary
The system uses a Swin-B backbone to encode images into patch tokens, while stroke sequences are embedded with coordinate and pen-state information, passed through rotary positional encoding, and processed by a transformer encoder. Learnable latent queries attend to image tokens, then stroke queries attend to these latents, creating a shared representation. Attention pooling aggregates the cross-modal embeddings into a single vector for classification. The model supports image-only, stroke-only, and dual-input modes, with convergence achieved in 3-4 epochs using AdamW optimization.

## Key Results
- Achieves state-of-the-art accuracy on IAMOn-DB, VNOn-DB, and ISI-Air datasets
- Early fusion improves recognition by up to 1.5% over prior methods
- Maintains robust performance even with single-modality input
- Converges in 3-4 epochs compared to 150+ epochs for late-fusion baselines

## Why This Works (Mechanism)

### Mechanism 1: Early Cross-Modal Fusion via Learnable Latent Queries
- Fusing image and stroke modalities early enables joint representation learning that captures complementary cues through Perceiver-IO-style architecture with learnable latents attending to both token streams.

### Mechanism 2: Cross-Modal Querying Allows Visual Context to Refine Temporal Representations
- Stroke embeddings refined by querying visual latents improve recognition over stroke-only approaches by disambiguating characters with similar trajectories but distinct visual forms.

### Mechanism 3: Attention Pooling Produces Robust Single-Vector Classification
- Learned attention weights over tokens produce a pooled representation that improves classification stability by emphasizing informative tokens rather than treating all equally.

## Foundational Learning

- **Concept: Cross-attention mechanisms**
  - Why needed: Core to early fusion—stroke and image tokens exchange information via learnable queries
  - Quick check: Given query Q ∈ R^(T×d) and key/value K, V ∈ R^(L×d), what is the output shape of MHA(Q, K, V)?

- **Concept: Rotary positional embeddings (RoPE)**
  - Why needed: Encodes temporal order in stroke sequences without learned absolute positions
  - Quick check: How does RoPE differ from sinusoidal positional encoding in handling relative positions?

- **Concept: Attention pooling for classification**
  - Why needed: Aggregates variable-length token sequences into a fixed-size vector for character classification
  - Quick check: Why might attention pooling outperform mean pooling for characters with varying stroke lengths?

## Architecture Onboarding

- **Component map:** Raw image/stroke → token embeddings → latent attention → cross-modal query → attention pool → logits
- **Critical path:** Image branch (Swin-B → patch tokens) and stroke branch ((x,y,pen) → projection → rotary encoding → transformer) converge via latent cross-attention, then cross-modal querying, attention pooling, and classification
- **Design tradeoffs:** Frozen Swin-B backbone (88.6M params) vs. trainable (10% accuracy drop); early vs. late fusion (3 epochs vs. 150+); latent count L=64 (fewer loses detail, more increases compute)
- **Failure signatures:** Confusion between visually similar characters (O/0, f/y); stroke-only mode drops 4-7% on IAMOn-DB; frozen backbone underperforms by ~10%
- **First 3 experiments:**
  1. Validate data pipeline: Visualize patches overlaid with stroke coordinates; check pen-state encoding
  2. Ablate fusion timing: Compare early vs. late fusion on validation split to replicate Table 5 trends
  3. Stress-test modality dropout: Train dual-mode, evaluate image-only and stroke-only to replicate Table 6 degradation curves

## Open Questions the Paper Calls Out

- **Can the early fusion approach scale to word-level and line-level handwriting recognition?** Authors note future work lies in curating large multilingual word and line level datasets to extend framework usage to real applications.

- **How robust is the model across diverse writer demographics and individuals with motor impairments?** Authors acknowledge performance has not been audited across writer demographics or fine-grained stroke disorders.

- **Can the model's parameter count and computational cost be reduced while maintaining early fusion benefits?** The system depends on a 90M parameter visual backbone and has heavier FLOPs than other dual-input methods.

## Limitations
- Heavy parameter count (94.4M) and computational cost limit deployment on resource-constrained devices
- Requires pre-segmented character inputs, limiting real-world applicability for continuous handwriting
- Performance not validated across writer demographics or clinical populations with motor impairments

## Confidence
- **High Confidence:** Overall system architecture and performance improvements over baselines are well-documented
- **Medium Confidence:** Early fusion benefits lack direct ablation studies isolating fusion timing from other design choices
- **Low Confidence:** Assumption that visual and stroke features are complementary rather than redundant is not directly tested

## Next Checks
1. Implement and train both early fusion and late fusion versions on validation data to quantify specific contribution of early cross-attention
2. Train separate image-only and stroke-only models, measure feature correlation between modalities in learned latent space to assess complementarity
3. Test different stroke coordinate normalization schemes to determine impact on stroke-only performance and cross-modal alignment quality