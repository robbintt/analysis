---
ver: rpa2
title: On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization
arxiv_id: '2505.18830'
source_url: https://arxiv.org/abs/2505.18830
tags:
- grpo
- responses
- nthr
- correct
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the effect of negative gradients in group-based
  reinforcement learning (GRPO), identifying a phenomenon called Lazy Likelihood Displacement
  (LLD) where correct answers' likelihoods marginally increase or even decrease during
  training. The source is traced to penalizing shared reasoning tokens in incorrect
  responses.
---

# On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization

## Quick Facts
- arXiv ID: 2505.18830
- Source URL: https://arxiv.org/abs/2505.18830
- Reference count: 40
- Primary result: Introduces Negative Token Hidden Reward (NTHR) to mitigate Lazy Likelihood Displacement (LLD) in GRPO, achieving 0.8–1.5% average performance gains across math benchmarks

## Executive Summary
This paper identifies a phenomenon called Lazy Likelihood Displacement (LLD) in Group Relative Policy Optimization (GRPO), where correct answers' likelihoods stagnate or decrease during training due to penalizing shared reasoning tokens in incorrect responses. The authors propose Negative Token Hidden Reward (NTHR), a selective token penalization method that reduces penalties on tokens that harm correct response likelihood. Experiments on mathematical reasoning benchmarks show NTHR consistently improves performance across models from 0.5B to 3B parameters, reducing LLD and yielding average performance gains of 0.8–1.5%.

## Method Summary
The authors introduce NTHR to address LLD in GRPO by selectively reducing penalties on tokens in wrong answers that harm correct response likelihood. The method computes NTHR scores using hidden embeddings and prediction errors, then applies scaled advantages to tokens exceeding a threshold. The approach is evaluated on math reasoning benchmarks using models ranging from 0.5B to 3B parameters, with training procedures specified for different model sizes and datasets.

## Key Results
- NTHR consistently improves performance across models from 0.5B to 3B parameters
- Average performance gains of 0.8–1.5% on math benchmarks (AIME24, AMC, MATH500, Minerva, OlympiadBench)
- Reduces Lazy Likelihood Displacement by selectively penalizing tokens that harm correct response likelihood
- Maintains computational overhead below 20% compared to baseline GRPO

## Why This Works (Mechanism)
The method works by identifying and selectively penalizing tokens in incorrect responses that have negative influence on correct answer likelihood. By computing influence scores using hidden embeddings and prediction errors, NTHR can distinguish between tokens that should be penalized versus those that harm correct responses. This targeted approach prevents the stagnation of correct answer likelihoods that occurs in standard GRPO when shared reasoning tokens are uniformly penalized across all incorrect responses.

## Foundational Learning

**Reinforcement Learning with Binary Rewards**: Understanding how binary rewards (correct/incorrect) create sparse gradients that can lead to plateauing in model performance. Why needed: The paper's core contribution addresses a specific failure mode in binary reward RL.

**Influence Functions in Neural Networks**: Computing how individual tokens influence the likelihood of correct responses using hidden state embeddings. Why needed: NTHR's core mechanism relies on influence-based token selection for selective penalization.

**GRPO Algorithm Mechanics**: Group Relative Policy Optimization's use of multiple rollouts and relative advantages for optimization. Why needed: The paper identifies a failure mode specific to GRPO's group-relative advantage computation.

**Token-Level Optimization in LLMs**: How individual token predictions contribute to overall sequence likelihood and how gradient updates propagate through the model. Why needed: NTHR operates at the token level to selectively modify gradients.

**Quick check**: Verify understanding by implementing a simple GRPO baseline and observing likelihood distributions for correct vs incorrect responses during training.

## Architecture Onboarding

**Component Map**: Data → GRPO Base (8 rollouts) → NTHR Scoring (hidden embeddings + prediction errors) → Selective Penalization (threshold τ) → Updated Advantage → Model Update

**Critical Path**: Forward pass with hidden embeddings extraction → NTHR score computation → Token selection via threshold → Advantage scaling → Backward pass with modified gradients

**Design Tradeoffs**: Selective penalization vs. uniform penalization (precision vs. computational cost), threshold sensitivity (β parameter) vs. robustness, token-level vs. sequence-level optimization (granularity vs. stability)

**Failure Signatures**: 
- LLD manifests as plateauing or decreasing likelihood of correct answers despite training
- Context window overflow with smaller models (4K limit)
- NTHR overhead exceeding 20% of total training time
- No improvement over GRPO baseline indicating LLD may not be present

**First Experiments**:
1. Implement baseline GRPO and visualize Δ(x) distributions to confirm LLD presence
2. Apply NTHR to a small model on MATH dataset and compare accuracy gains
3. Profile NTHR computation time relative to total training to verify <20% overhead claim

## Open Questions the Paper Calls Out

**Open Question 1**: Does NTHR's effectiveness scale to LLMs beyond 3B parameters (e.g., 7B, 70B, or frontier-scale models)? The authors explicitly limit their experiments to "models ranging from 0.5B to 3B parameters" and state this as a scope constraint.

**Open Question 2**: How does NTHR perform on non-mathematical reasoning tasks such as code generation, medical reasoning, or open-domain QA? The paper notes that GRPO has been applied to "code generation, mathematical problem solving, medical reasoning, and retrieval-augmented generation" but all experiments here use only math benchmarks.

**Open Question 3**: Can NTHR mitigate the response length collapse observed under constrained context windows (e.g., 4K tokens)? In the Limitations section, the authors report that DeepSeek-1.5B trained with a 4K context window showed "response length to progressively decrease" and results were "very unstable."

## Limitations

- Computational intensity of NTHR remains a concern, though claimed to be <20% overhead
- Effectiveness varies across model architectures, with limited gains on DeepScaler for certain models
- Selective penalization relies on precise thresholding with limited sensitivity analysis
- Generalization to non-mathematical reasoning tasks remains unverified
- Study focuses on smaller models (≤3B parameters), leaving scalability uncertainty

## Confidence

- **High confidence**: Identification of Lazy Likelihood Displacement as distinct phenomenon; computational overhead estimates
- **Medium confidence**: Effectiveness of NTHR in consistently improving accuracy by 0.8-1.5%; model-specific limitations not fully explained
- **Low confidence**: Scalability claims to larger models and non-mathematical domains; no evidence beyond 3B parameters or outside math reasoning

## Next Checks

1. **Runtime profiling**: Measure NTHR computation time relative to total training time across different batch sizes and model scales to verify the <20% overhead claim and identify potential bottlenecks.

2. **Cross-task generalization**: Apply NTHR to a non-mathematical reasoning benchmark (e.g., MMLU or DROP) with the same model sizes to test domain transferability and identify task-specific failure modes.

3. **Sensitivity analysis**: Systematically vary β (threshold scaling) and η (scaling factor) parameters across multiple runs to determine optimal ranges and assess robustness to hyperparameter choices.