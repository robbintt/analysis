---
ver: rpa2
title: 'ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems'
arxiv_id: '2505.11831'
source_url: https://arxiv.org/abs/2505.11831
tags:
- tasks
- arc-agi-2
- task
- human
- prize
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARC-AGI-2 is an upgraded version of the Abstraction and Reasoning
  Corpus (ARC) benchmark, designed to provide a more granular and challenging evaluation
  of abstract reasoning and problem-solving abilities in AI systems. It retains the
  input-output pair task format of ARC-AGI-1 but introduces a newly curated set of
  tasks with greater complexity and uniqueness, specifically designed to resist brute-force
  methods and better assess compositional generalization.
---

# ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems

## Quick Facts
- arXiv ID: 2505.11831
- Source URL: https://arxiv.org/abs/2505.11831
- Authors: Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, Henry Pinkard
- Reference count: 14
- Top models achieve near-zero scores on this benchmark

## Executive Summary
ARC-AGI-2 is an upgraded version of the Abstraction and Reasoning Corpus benchmark designed to provide a more granular and challenging evaluation of abstract reasoning and problem-solving abilities in AI systems. It retains the input-output pair task format of ARC-AGI-1 but introduces a newly curated set of tasks with greater complexity and uniqueness, specifically designed to resist brute-force methods and better assess compositional generalization. Extensive human testing established robust baseline performance metrics, demonstrating that all tasks are human-solvable but require deliberate thinking, with an average completion time of 2.7 minutes. Current AI models, including top-performing systems like o3 and o3-mini, achieve near-zero scores on ARC-AGI-2, highlighting its effectiveness as a next-generation tool for measuring progress toward more general and human-like AI capabilities.

## Method Summary
ARC-AGI-2 evaluates grid-to-grid transformation reasoning using 2-5 demonstration input-output pairs to infer transformation rules, which are then applied to unseen test inputs. The benchmark includes Public Training, Public Evaluation, Semi-Private Evaluation (120 tasks), and Private Evaluation (120 tasks) sets with grid sizes from 1×1 to 30×30. Evaluation uses the Model Baseline repository on 4× NVIDIA L4 GPUs with 12-hour wall-clock limit and no internet access. Human testing with 407 participants established baseline solvability (66% success rate, 2.7 min median time), and all tasks were confirmed solvable by at least two independent non-expert testers.

## Key Results
- Top AI models (o3, o3-mini, Claude 3.7, Gemini 2.0) achieve near-zero scores on ARC-AGI-2
- Human solvers achieve 66% success rate with 2.7 minute median completion time
- Tasks require ~14x more bits per task than ARC-AGI-1, making brute-force search computationally intractable
- All 240 evaluation tasks are human-solvable but require deliberate thinking

## Why This Works (Mechanism)

### Mechanism 1: Compositional Generalization Pressure
ARC-AGI-2 evaluates whether systems can combine known concepts in novel ways rather than retrieving memorized patterns. Tasks require multi-rule composition, multi-step composition, contextual rule application, and in-context symbol definition. This tests compositional generalization as a necessary condition for fluid intelligence that cannot be solved by pattern matching alone.

### Mechanism 2: Brute-Force Resistance via Information Density
Tasks are designed with sufficient complexity that exhaustive program search becomes computationally intractable. Increased grid sizes, more objects per grid, and more concepts per task raise the bits-per-task required for compression, reducing susceptibility to naive enumeration. This assumes brute-force search scales poorly with task information content.

### Mechanism 3: Human-Calibrated Difficulty Distribution
Reliable evaluation requires that performance on public subsets predicts private subset performance, with all tasks confirmed human-solvable. First-party human testing establishes baseline solvability and calibrates subsets so mean human accuracy differs by ≤1 percentage point across partitions. This assumes human solvability ensures tasks test reasoning rather than specialized knowledge or impossibility.

## Foundational Learning

- **Core Knowledge priors** (object persistence, goal-directedness, number sense, topology, symmetry): Required as the only priors; systems must either encode or acquire them. Quick check: Can your system recognize that two separated grid regions represent "the same object" after movement?

- **Few-shot program induction**: Tasks provide only 2-5 demonstration pairs; systems must infer transformation rules from sparse examples. Quick check: Given 3 input-output grid pairs, can your system generate a program that produces the correct 4th output?

- **Test-time adaptation**: ARC Prize 2024 showed TTA (search or gradient descent at inference) was necessary for competitive performance. Quick check: Does your approach modify behavior based on the specific task instance, or use fixed weights?

## Architecture Onboarding

- **Component map**: Task encoder → Rule hypothesis generator → Program synthesis / neural executor → Output validator → Submission
- **Critical path**: Correctly parsing grid structure → generating candidate transformations → filtering via demonstration pairs → applying to test input
- **Design tradeoffs**: Program synthesis offers interpretability but sparse search; neural approaches offer density but poor generalization; hybrid TTA balances both at compute cost
- **Failure signatures**: Near-zero scores with high confidence indicates system not adapting to task-specific rules; solving training tasks but failing evaluation suggests overfitting; high variance across runs indicates unstable or underconstrained search process

- **First 3 experiments**:
  1. Establish baseline: Run o3-mini or Claude 3.7 on Public Evaluation set using Model Baseline repository; document per-task failure modes (perception vs. reasoning vs. execution)
  2. Ablate TTA: Compare fixed-weights inference vs. test-time search on a 20-task Public Eval subset; measure accuracy vs. compute tradeoff
  3. Diagnose composition gaps: Manually classify failures by type (multi-rule, multi-step, contextual, symbolic); identify which compositional pattern your system handles worst

## Open Questions the Paper Calls Out

### Open Question 1
Can test-time adaptation approaches, which proved effective on ARC-AGI-1, scale to achieve meaningful performance (>5%) on ARC-AGI-2 without prohibitive computational costs? This remains unresolved because current TTA methods may not transfer to tasks requiring multi-step compositional reasoning and contextual rule application. Evidence would come from a TTA-based system exceeding 5% on Semi-Private Evaluation with documented compute costs.

### Open Question 2
What specific cognitive mechanisms enable humans to solve ARC-AGI-2 tasks that remain inaccessible to frontier AI systems? This remains unresolved because the paper documents the performance gap but does not identify which human capabilities most account for this difference. Evidence would come from ablation studies comparing human and AI performance across individual compositional generalization categories.

### Open Question 3
How resistant is ARC-AGI-2 to brute-force program search over time as computational resources and search algorithms improve? This remains unresolved because ARC-AGI-1's vulnerability emerged only through multi-year competition analysis; equivalent longitudinal data for ARC-AGI-2 does not yet exist. Evidence would come from a meta-analysis of competition submissions measuring the fraction of tasks solved by brute-force vs. reasoning-based approaches over multiple competition cycles.

## Limitations
- Unknown robustness of brute-force resistance under future compute scaling, as the 14x bits-per-task increase assumes current algorithmic constraints
- Human-calibration methodology depends on self-reported time and success rates from non-expert participants, which may not perfectly align with optimal reasoning strategies
- Withheld Semi-Private and Private Evaluation sets prevent full validation of cross-subset performance consistency

## Confidence

- **High Confidence**: Human solvability (100% of tasks solved by at least two humans) and basic task format preservation from ARC-AGI-1
- **Medium Confidence**: Claims about compositional generalization pressure and the specific 14x bits-per-task increase
- **Medium Confidence**: Current AI model performance (near-zero scores), though this may reflect evaluation methodology
- **Low Confidence**: Long-term robustness against brute-force methods as compute scales

## Next Checks

1. **Brute-force resistance validation**: Implement simple program search baseline on 20-task sample from ARC-AGI-2. Measure search space size, time-to-solution, and success rate. Compare against same methodology on ARC-AGI-1 tasks to quantify claimed 14x increase in information density.

2. **Human-AI performance gap analysis**: Run multiple independent evaluations of current frontier models (o3, Claude 3.7, Gemini 2.0) on public ARC-AGI-2 tasks under identical conditions. Document per-task accuracy, compute usage, and failure modes. Compare against established human baseline (66% success rate, 2.7 min median time) to verify near-zero model performance claim.

3. **Compositional pattern isolation**: Manually classify ARC-AGI-2 tasks into four compositional categories. Select 5 tasks from each category and create minimal variants that isolate single compositional elements. Test whether systems that fail full tasks can succeed on isolated components, validating whether compositional generalization is truly the bottleneck.