---
ver: rpa2
title: Machine Learning-Based Model for Postoperative Stroke Prediction in Coronary
  Artery Disease
arxiv_id: '2503.11973'
source_url: https://arxiv.org/abs/2503.11973
tags:
- stroke
- risk
- patients
- postoperative
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a machine learning model to predict postoperative
  stroke risk in coronary artery disease patients undergoing revascularization. Using
  MIMIC-IV data (7,023 patients), an SVM model was built after data preprocessing,
  feature selection with LASSO, and SMOTE oversampling.
---

# Machine Learning-Based Model for Postoperative Stroke Prediction in Coronary Artery Disease

## Quick Facts
- arXiv ID: 2503.11973
- Source URL: https://arxiv.org/abs/2503.11973
- Reference count: 32
- Primary result: SVM model achieved AUC 0.855 (95% CI: 0.829-0.878) for predicting postoperative stroke in CAD patients

## Executive Summary
This study developed a machine learning model to predict postoperative stroke risk in coronary artery disease patients undergoing revascularization. Using MIMIC-IV data (7,023 patients), an SVM model was built after data preprocessing, feature selection with LASSO, and SMOTE oversampling. The SVM model achieved an AUC of 0.855 (95% CI: 0.829-0.878), outperforming logistic regression and CatBoost. SHAP analysis showed the Charlson Comorbidity Index, diabetes, chronic kidney disease, and heart failure were the most important predictors. The model improved predictive accuracy by incorporating specific comorbidities alongside CCI, offering a more comprehensive preoperative risk assessment.

## Method Summary
The study used MIMIC-IV data to develop an SVM model for postoperative stroke prediction in CAD patients. After preprocessing (removing features with >30% missing values, correlation filtering >90%), 12 features were selected using LASSO with coefficient thresholding (|β| > 0.01). SMOTE oversampling addressed class imbalance (8% stroke vs 92% non-stroke) by generating synthetic minority samples only on the training set. The SVM model with RBF kernel (C=1, gamma=0.01) was trained on a 70/30 train-test split and achieved an AUC of 0.855. SHAP analysis revealed that while CCI absorbed much of the risk attribution, individual comorbidities (diabetes, CKD, heart failure) contributed independent predictive value confirmed through ablation analysis.

## Key Results
- SVM model achieved AUC 0.855 (95% CI: 0.829-0.878), outperforming logistic regression and CatBoost
- SHAP analysis identified CCI, diabetes, chronic kidney disease, and heart failure as top predictors
- Ablation analysis showed removing CKD, diabetes, and heart failure reduced AUC to 0.790
- Multicollinearity between CCI and its components caused negative SHAP values despite improved predictive performance

## Why This Works (Mechanism)

### Mechanism 1
SMOTE applied to training data only addresses class imbalance (8% stroke vs 92% non-stroke) and improves minority class detection without data leakage. Synthetic Minority Over-sampling Technique generates synthetic minority samples by interpolating between existing minority instances, forcing the classifier to learn decision boundaries that don't default to the majority class. Core assumption: The minority class samples follow a distribution that can be meaningfully interpolated; stroke cases in the training set are representative of true stroke patterns. Evidence anchors: [abstract] "SMOTE oversampling" mentioned as key preprocessing step; [Methods] "SMOTE (Synthetic Minority Class Oversampling) on the training set to generate additional minority class samples... SMOTE was applied only to the training set to prevent potential data leakage"; [corpus] Neighbor paper "Predicting Postoperative Stroke in Elderly SICU Patients" also uses MIMIC data with similar class imbalance challenges. Break condition: If minority class contains mislabeled cases or extreme outliers, SMOTE may propagate noise rather than signal.

### Mechanism 2
LASSO with coefficient threshold (|β| > 0.01) reduces noise features more aggressively than standard non-zero coefficient retention. LASSO regression applies L1 penalty that shrinks weak feature coefficients toward zero. By raising the retention threshold above zero, the model filters features with non-trivial predictive contribution, reducing overfitting from low-signal variables. Core assumption: Features with |β| < 0.01 contribute noise rather than signal; the threshold appropriately balances dimensionality reduction against information loss. Evidence anchors: [Methods] "Parameter tuning yielded an optimal regularization parameter (λ = 0.0011908). Features with absolute coefficients exceeding 0.01 were then retained, resulting in 12 selected variables"; [Introduction] Notes Lin et al. (2024) "may inadvertently preserve numerous features having low absolute coefficients, leading to excessive noise"; [corpus] Limited direct corpus evidence on this specific thresholding approach. Break condition: If predictive features have naturally small coefficients due to scale or effect size, the threshold may exclude useful predictors.

### Mechanism 3
Including individual comorbidities (diabetes, CKD, heart failure) alongside CCI improves prediction over CCI alone, despite apparent negative SHAP values from multicollinearity. CCI is a composite score containing multiple conditions. When both CCI and its component diseases are included, SHAP attribution redistributes risk explanation primarily to CCI, masking individual contributions. Ablation confirms individual comorbidities add independent predictive value. Core assumption: CCI does not fully capture the stroke-specific risk contributions of its component conditions; individual conditions encode additional signal. Evidence anchors: [Results] "Ablation analysis by removing CKD, diabetes, and heart failure led to a decline in AUC to 0.790"; [Discussion] "Following the removal of CCI from the model... the significance of age increased, whereas the contributions of CKD, diabetes, and heart failure diminished, though their SHAP values adjusted positively"; [corpus] Neighbor paper on prognostic factors in CAD also emphasizes individual risk factor identification. Break condition: If multicollinearity destabilizes coefficient estimates severely, model predictions may become unreliable on out-of-distribution samples.

## Foundational Learning

- Concept: SMOTE (Synthetic Minority Over-sampling Technique)
  - Why needed here: Medical datasets often have rare adverse events; naive classifiers achieve high accuracy by always predicting the majority class.
  - Quick check question: If your dataset has 5% positive cases and 95% negative cases, what accuracy would a classifier achieve by always predicting "negative"?

- Concept: SHAP (SHapley Additive exPlanations) and multicollinearity effects
  - Why needed here: Negative SHAP values for known risk factors can appear counterintuitive; understanding feature attribution redistribution is essential for clinical interpretation.
  - Quick check question: If feature A is mathematically derivable from feature B (e.g., BMI from weight and height), what happens to SHAP values when all are included in a model?

- Concept: LASSO regularization (L1 penalty)
  - Why needed here: High-dimensional clinical data with correlated features require automatic feature selection to prevent overfitting.
  - Quick check question: Why does L1 regularization tend to produce sparse coefficient vectors (many zeros) compared to L2 regularization?

## Architecture Onboarding

- Component map: MIMIC-IV extraction (7,023 patients, 103 initial features) -> Preprocessing (missing value filter 30%, Random Forest imputation, correlation filter >90%, one-hot encoding, standard scaling) -> Feature Selection (LASSO with 10-fold CV, coefficient thresholding |β| > 0.01) -> Class Balancing (SMOTE training set only) -> Model Training (SVM RBF kernel, C=1, gamma=0.01 with Grid Search) -> Evaluation (70/30 train-test split, metrics: AUC, sensitivity, specificity, accuracy) -> Interpretation (SHAP analysis bar plot and beeswarm plot)

- Critical path: Feature selection threshold -> SMOTE application -> SVM hyperparameters. The 12-feature selection determines what signal reaches the model; improper SMOTE application causes leakage; SVM kernel and regularization parameters control bias-variance tradeoff.

- Design tradeoffs: CCI + individual comorbidities improves AUC but introduces multicollinearity that complicates SHAP interpretation; aggressive LASSO threshold reduces overfitting but risks excluding weak but real predictors; SMOTE improves sensitivity but synthetic samples may not reflect true clinical heterogeneity.

- Failure signatures: AUC drops significantly on external validation -> overfitting to MIMIC-IV cohort characteristics; SHAP values remain negative for all individual comorbidities even after CCI removal -> insufficient training data or severe multicollinearity; sensitivity much lower than specificity -> residual class imbalance or inappropriate decision threshold.

- First 3 experiments: 1) Reproduce the baseline using the same 70/30 split, confirm SVM AUC ≈ 0.855, then test without SMOTE to quantify its contribution. 2) Run ablation study: train separate models with CCI only, individual comorbidities only, and both combined; compare AUC and SHAP patterns. 3) Apply the trained model to a temporally held-out subset of MIMIC-IV (if available) or simulate external validation by stratifying on a confounder like age to assess generalization within the same database.

## Open Questions the Paper Calls Out

- Question: Does re-weighting the Charlson Comorbidity Index components specifically for stroke-risk estimation improve model performance compared to the standard CCI?
  - Basis in paper: [explicit] "Future endeavors should prioritize refining the CCI by re-weighting its components for more accurate stroke-risk estimation"
  - Why unresolved: Current CCI weighting was developed for mortality prediction generally, not postoperative stroke specifically; the study demonstrated that CCI "absorbs" risk information from individual comorbidities, potentially obscuring their independent contributions.
  - What evidence would resolve it: A comparative study testing multiple CCI weighting schemes optimized for stroke prediction, evaluated against the standard CCI using discriminative metrics (AUC, net reclassification improvement) and calibration assessments.

- Question: How well does the SVM model generalize to patient populations outside the single tertiary academic medical center represented in MIMIC-IV?
  - Basis in paper: [explicit] "This study is deficient in external validation, as the evaluation of model performance was conducted exclusively within the confines of the MIMIC-IV database, without the corroboration of an external dataset"
  - Why unresolved: MIMIC-IV represents a single institution in Boston with specific patient demographics, practice patterns, and coding practices that may not reflect broader clinical settings.
  - What evidence would resolve it: Prospective validation on independent multicenter datasets from geographically and demographically diverse healthcare systems, with reporting of performance metrics across different subgroups.

- Question: Can explicit modeling of interrelationships among CCI component diseases mitigate multicollinearity while maintaining or improving predictive accuracy?
  - Basis in paper: [explicit] "Future research may explore potential solutions such as modifying the weighting scheme of CCI calculations or explicitly modeling the interrelationships among its component diseases"
  - Why unresolved: Including comorbidities already embedded in CCI creates multicollinearity, yet removing them reduces model performance (AUC dropped to 0.790 in ablation analysis).
  - What evidence would resolve it: Development and comparison of alternative model architectures (e.g., hierarchical models, interaction terms, or regularized approaches) that explicitly model disease interrelationships, with evaluation of both predictive performance and multicollinearity metrics.

## Limitations

- The model's performance relies heavily on MIMIC-IV data, limiting generalizability to other healthcare systems.
- Key feature definitions and stroke outcome algorithms are not explicitly specified, making exact reproduction challenging.
- The SHAP interpretation becomes complicated due to multicollinearity between CCI and its component comorbidities, making clinical interpretation less straightforward despite improved predictive performance.

## Confidence

- High Confidence: The core methodological approach (SVM with SMOTE and LASSO feature selection achieving AUC ~0.855) is well-documented and technically sound.
- Medium Confidence: The claim that individual comorbidities improve prediction beyond CCI alone is supported by ablation studies, but the negative SHAP values and multicollinearity effects introduce interpretive complexity.
- Medium Confidence: The clinical significance of the model is reasonable given the strong predictors identified, but external validation is needed to confirm real-world utility.

## Next Checks

1. Apply the trained model to a temporally held-out subset of MIMIC-IV (if available) to assess internal temporal stability and generalization within the same database.
2. Test the model on a completely independent external dataset from another healthcare system to evaluate true generalizability beyond MIMIC-IV.
3. Conduct SHAP sensitivity analysis by systematically varying feature inclusion (CCI only, individual comorbidities only, combinations) to better understand multicollinearity effects on feature attribution.