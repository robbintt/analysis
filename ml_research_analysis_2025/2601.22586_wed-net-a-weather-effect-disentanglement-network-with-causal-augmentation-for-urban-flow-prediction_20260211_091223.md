---
ver: rpa2
title: 'WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation
  for Urban Flow Prediction'
arxiv_id: '2601.22586'
source_url: https://arxiv.org/abs/2601.22586
tags:
- weather
- causal
- traffic
- spatio-temporal
- conditions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses urban flow prediction under extreme weather
  conditions, where existing models fail to generalize due to data scarcity and coarse
  weather modeling. WED-Net introduces a dual-branch Transformer architecture that
  disentangles intrinsic traffic dynamics from weather-induced effects using self-
  and cross-attention, memory banks, and adaptive fusion.
---

# WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction

## Quick Facts
- arXiv ID: 2601.22586
- Source URL: https://arxiv.org/abs/2601.22586
- Authors: Qian Hong; Siyuan Chang; Xiao Zhou
- Reference count: 28
- Primary result: MAE of 0.1172 and 0.0863 under extreme and normal weather in NYC, outperforming baselines by up to 14% in extreme conditions

## Executive Summary
This paper introduces WED-Net, a dual-branch Transformer architecture designed to improve urban flow prediction under extreme weather conditions. The key innovation is the disentanglement of intrinsic traffic dynamics from weather-induced effects using self- and cross-attention mechanisms, combined with a causality-driven augmentation strategy. Experiments on NYC, Chicago, and DC datasets demonstrate significant improvements in prediction accuracy, particularly under extreme weather conditions where traditional models struggle to generalize due to data scarcity.

## Method Summary
WED-Net employs a dual-branch Transformer architecture where one branch captures intrinsic spatio-temporal patterns through self-attention while the other branch models weather effects through cross-attention. The model incorporates memory banks for pattern retrieval, adaptive fusion via gating, and a discriminator with gradient reversal to enforce weather-invariant representations in the intrinsic branch. A key innovation is the causal augmentation strategy that perturbs non-causal regions while preserving causal structures, improving out-of-distribution robustness. The model is trained on taxi trip data from three major US cities with corresponding weather data, using MAE and RMSE as evaluation metrics on both normal and extreme weather subsets.

## Key Results
- Achieves MAE of 0.1172 under extreme weather and 0.0863 under normal weather in NYC test set
- Outperforms baselines by up to 14% on extreme weather conditions
- Demonstrates robust generalization under rare scenarios through causal augmentation
- Shows consistent performance improvements across NYC, Chicago, and DC datasets

## Why This Works (Mechanism)

### Mechanism 1
The dual-branch attention architecture successfully separates intrinsic traffic patterns from weather-induced effects. The I-STEnc branch uses self-attention to capture stable spatio-temporal dependencies like commuting patterns, while W-STEnc uses cross-attention with traffic as queries and weather as keys/values to isolate fine-grained weather modulations. This explicit disentanglement enables distinct representations for intrinsic and environmental dependencies.

### Mechanism 2
Adversarial weather discrimination through a Gradient Reversal Layer enforces weather-invariant representations in the intrinsic branch. By training a discriminator to predict weather conditions from I-STEnc outputs while inverting gradients, the model learns to suppress weather-discriminative information while maintaining predictive power for traffic flow.

### Mechanism 3
Causal augmentation via attention-based identification and perturbation of non-causal regions improves out-of-distribution robustness. Attention maps identify causal neighbors and time steps for each sample, allowing non-causal regions to be replaced with values from matched normal-weather samples while preserving periodic patterns and stable causal structures.

## Foundational Learning

- **Concept: Self-attention and Cross-attention in Transformers**
  - Why needed here: I-STEnc relies on self-attention for intrinsic dependencies; W-STEnc requires cross-attention to query traffic against weather embeddings
  - Quick check question: Can you explain how cross-attention differs from self-attention in terms of Q, K, V sources?

- **Concept: Adversarial Training with Gradient Reversal**
  - Why needed here: The weather discriminator uses GRL to create adversarial pressure for weather-invariant representations
  - Quick check question: What happens to the gradient sign when passing through a GRL during backpropagation?

- **Concept: Causal Intervention and Invariance Principle**
  - Why needed here: The augmentation strategy applies do-calculus intuition—perturbing non-causal parts while preserving causal structure
  - Quick check question: Why does intervening on non-causal variables help generalization under distribution shift?

## Architecture Onboarding

- **Component map:** Embedding Layer → (I-STEnc [self-attention + ST Memory] || W-STEnc [cross-attention + ST Memory]) → [Adaptive Fusion via gating] → MLP Predictor → Parallel: Weather Discriminator (GRL + classifier) for I-STEnc output

- **Critical path:**
  1. Embed traffic and weather with temporal/spatial adaptive projections and periodic features
  2. Route to dual branches: I-STEnc (self-attention), W-STEnc (cross-attention)
  3. Retrieve patterns from ST Memory banks per branch
  4. Fuse via learned gating (α-weighted sum)
  5. Predict via MLP; simultaneously enforce weather-invariance via discriminator
  6. Apply causal augmentation during training to improve robustness

- **Design tradeoffs:**
  - Memory bank size vs. retrieval cost: Larger banks improve pattern coverage but increase compute
  - Disentanglement strength (η): Higher values enforce stronger weather-invariance but risk over-regularization
  - Augmentation ratio (r): More augmented samples improve OOD robustness but may distort rare-pattern learning if references are mismatched

- **Failure signatures:**
  - Intrinsic features cluster poorly → discriminator may be too weak or weather labels too noisy
  - Performance drops under normal weather → over-reliance on weather branch; check gating weights
  - Causal augmentation harms performance → attention-based identification may be mislabeling causal regions

- **First 3 experiments:**
  1. Run WED-Net without weather branch and without augmentation on normal-weather test split
  2. Visualize PCA of I-STEnc vs. W-STEnc hidden states for normal vs. extreme samples
  3. Train with and without augmentation across multiple r and r_A settings

## Open Questions the Paper Calls Out

### Open Question 1: Generalization to Non-Precipitation Extreme Events
The paper exclusively addresses rainfall effects but doesn't examine performance on snowstorms, heat waves, hurricanes, or non-weather disruptions like large events or infrastructure failures. Different disruptions may have fundamentally different spatio-temporal signatures and causal mechanisms.

### Open Question 2: Ground-Truth Validation of Attention-Based Causal Identification
The paper uses attention maps to identify causal parcels and time steps without independent validation that attention reflects genuine causality rather than learned correlation. Attention is an importance proxy, not a causality guarantee.

### Open Question 3: Sensitivity to Weather Data Spatial Resolution
Weather data is interpolated from stations via inverse-distance weighting, but the impact of interpolation quality on fine-grained weather-effect modeling is unexamined. Micro-climate variations or localized precipitation may be lost in interpolation, potentially undermining the cross-attention mechanism's effectiveness.

### Open Question 4: Long-Term and Seasonal Causal Structure Stability
The paper uses 8-month datasets and assumes stable causal structures, but seasonal variations or long-term behavioral shifts may alter causal dependencies. The invariance assumption underlying the causal augmentation may not hold when underlying urban dynamics change.

## Limitations

- Memory bank dimensions (L_m, d_m) are unspecified, making exact reproduction impossible
- Weather condition label taxonomy for the discriminator is unclear (binary vs. multi-class)
- Causal augmentation reference selection procedure lacks detail on candidate pool size and matching criteria
- Extreme-weather sample scarcity may cause unstable augmentation if reference matching fails

## Confidence

- **High:** Dual-branch architecture design and its role in disentangling intrinsic vs. weather-induced dynamics; causal augmentation mechanism using attention maps for non-causal region identification
- **Medium:** Effectiveness of adversarial weather discrimination in enforcing weather-invariant representations; generalization claims under extreme conditions
- **Low:** Exact implementation details of memory banks, reference sample selection in augmentation, and discriminator head architecture

## Next Checks

1. **Sample distribution audit:** Report the number of extreme-weather samples in each dataset split; if extreme samples <5% of training data, verify augmentation stabilizes training via loss curves

2. **Attention map inspection:** Visualize top-r_A causal vs. non-causal parcels for a few extreme-weather samples; confirm they align with domain knowledge (e.g., flooded vs. unaffected areas)

3. **Ablation under data scarcity:** Train WED-Net and variants with artificially reduced extreme-weather samples (e.g., 10%, 5%); measure performance drop to quantify robustness dependence on sample volume