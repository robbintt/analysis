---
ver: rpa2
title: Advancing Medical Artificial Intelligence Using a Century of Cases
arxiv_id: '2509.12194'
source_url: https://arxiv.org/abs/2509.12194
tags:
- diagnosis
- cases
- cpcs
- differential
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces CPC-Bench, a comprehensive benchmark for evaluating
  AI medical reasoning, and Dr. CaBot, an AI expert discussant trained on over a century
  of clinical cases from the New England Journal of Medicine.
---

# Advancing Medical Artificial Intelligence Using a Century of Cases

## Quick Facts
- arXiv ID: 2509.12194
- Source URL: https://arxiv.org/abs/2509.12194
- Reference count: 40
- o3 achieved 60% top-1 diagnostic accuracy on 377 CPC cases, outperforming physician baselines

## Executive Summary
This study introduces CPC-Bench, a comprehensive benchmark for evaluating AI medical reasoning across ten tasks using over a century of NEJM Clinical Problem Solving cases. The researchers developed Dr. CaBot, an AI expert discussant that generates differential diagnoses and video presentations indistinguishable from human experts in blinded comparisons. Evaluation across 377 cases showed o3 achieving 60% top-1 diagnostic accuracy, with performance improving incrementally as clinical information accumulates. The work highlights both the impressive progress and remaining limitations of AI in medical reasoning, particularly in literature retrieval and image interpretation.

## Method Summary
The researchers compiled 7,102 NEJM Clinical Problem Solving cases (1923-2025) and 1,021 Image Challenges (2006-2025) into CPC-Bench, a benchmark with ten tasks assessing differential diagnosis, testing plans, literature search, and multimodal interpretation. Dr. CaBot was developed using o3 as a base model, retrieving two similar historical cases via embedding similarity to mimic expert presentation style. The system generates written and video differential diagnoses using iterative literature queries from a clinical database. Performance was evaluated using physician-validated LLM judges and blinded physician comparisons for CaBot quality assessment.

## Key Results
- o3 achieved 60% top-1 diagnostic accuracy and 84% top-10 accuracy on CPC cases
- Diagnostic accuracy improved incrementally across sequential clinical touchpoints (29 percentage point increase)
- Physicians misclassified the source of CaBot's differential diagnoses in 74% of blinded trials
- Image-only diagnosis performance remained weak at 67% accuracy for top models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagnostic accuracy improves incrementally as clinical information accumulates across sequential touchpoints.
- Mechanism: Models update probability distributions over diagnoses with each new clinical event (e.g., lab result, imaging), leveraging pattern matching against training data where similar sequences led to specific outcomes.
- Core assumption: The model's internal representations encode meaningful associations between clinical trajectories and diagnoses rather than surface-level keyword matching.
- Evidence anchors:
  - [results] "From Event 1 to Event 5, accuracy increased by 29 percentage points for o3 (95% CI, 22% to 36%)"
  - [methods] "Physicians also recorded word-level event type (e.g., disease events, investigations, management)"
  - [corpus] Related work "Sequential Diagnosis with Language Models" (arXiv:2506.22405) explores similar stepwise diagnostic processes
- Break condition: If cases require reasoning about negated findings explicitly (e.g., "no lymphadenopathy"), performance may degrade—observed 4-5% drop when normal findings omitted.

### Mechanism 2
- Claim: Few-shot style emulation via embedding-based case retrieval enables expert-like presentation generation.
- Mechanism: CaBot retrieves the two most similar historical CPC cases via embedding similarity, then conditions generation on their "Differential Diagnosis" sections to reproduce rhetorical patterns, structure, and citation practices.
- Core assumption: Stylistic features of expert reasoning transfer across cases with similar clinical presentations.
- Evidence anchors:
  - [methods] "CaBot identifies the two CPC 'Presentation of Case' sections most similar to the index case by searching embeddings of over 6,000 cases"
  - [results] "physicians misclassified the source of the differential in 46 of 62 (74%) of trials, and scored CaBot more favorably across quality dimensions"
  - [corpus] "Human-Centered AI in Multidisciplinary Medical Discussions" (arXiv:2503.16464) shows chat-based AI can support specialist case assessment
- Break condition: Retrieved exemplars may propagate stylistic quirks or outdated clinical knowledge from earlier decades if historical mode is used uncritically.

### Mechanism 3
- Claim: Scale and inference-time reasoning, not domain-specific fine-tuning, drive performance gains on clinical tasks.
- Mechanism: Larger parameter counts and architectures optimized for extended chain-of-thought reasoning enable emergent clinical capabilities from general pretraining alone.
- Core assumption: Medical reasoning is sufficiently represented in general pretraining corpora that specialist adaptation is unnecessary.
- Evidence anchors:
  - [discussion] "the early GPT-3.5 model solved 44% of CPCs, now up to 84% with o3 without any additional clinical fine-tuning"
  - [discussion] "sheer parameter count, pretraining scale, and architectures optimised for reasoning have eclipsed specialist fine-tuning"
  - [corpus] Corpus papers show mixed findings—some emphasize fine-tuning (MEDITRON), others generalist approaches; evidence is not uniform
- Break condition: Tasks requiring up-to-date literature retrieval or specialized image interpretation remain weaker (67% on image challenges, ~50% on citation retrieval), suggesting scale alone may not close all gaps.

## Foundational Learning

- Concept: Differential Diagnosis (DDx)
  - Why needed here: Central task in CPC-Bench; requires understanding how clinicians generate ranked hypotheses from clinical presentations
  - Quick check question: Given a patient with fever, cough, and chest pain, what are the top three diagnostic considerations and what findings would increase vs. decrease each?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: CaBot uses RAG for literature search; benchmark evaluates citation accuracy with and without retrieval
  - Quick check question: When does retrieving external documents improve LLM outputs versus relying on parametric knowledge?

- Concept: Embedding Similarity Search
  - Why needed here: CaBot retrieves similar cases via vector embeddings; evaluation uses LLM-as-judge with embedding-based validation
  - Quick check question: If two clinical cases have cosine similarity of 0.85 in embedding space, does that guarantee diagnostic similarity?

## Architecture Onboarding

- Component map: CPC ingestion -> structured extraction -> physician annotation -> validated ground truth -> LLM annotation scaling -> full corpus coverage -> task definition -> benchmark creation -> model evaluation -> leaderboard population -> CaBot assembly -> agentic system with retrieval + generation

- Critical path:
  1. CPC ingestion → structured extraction
  2. Physician annotation → validated ground truth
  3. LLM annotation scaling → full corpus coverage
  4. Task definition → benchmark creation
  5. Model evaluation → leaderboard population
  6. CaBot assembly → agentic system with retrieval + generation

- Design tradeoffs:
  - LLM judge vs. human evaluation: Faster, scalable, but validated on only 1,467 physician-annotated differentials (86% accuracy)
  - Historical vs. contemporary cases: Historical cases enable temporal analysis but may reflect outdated clinical knowledge
  - Closed vs. open retrieval: o4-mini with RAG achieved 47% on citation task vs. Gemini 2.5 Pro's 49% without retrieval—retrieval helps but index completeness matters

- Failure signatures:
  - Image-only diagnosis: o3 achieves only 19% top-1 accuracy (vs. 60% with text)—multimodal integration is brittle
  - Literature retrieval for pre-1990 or post-2020 citations: Accuracy declines significantly
  - Radiology images: 55% accuracy vs. 76% for dermatology—specialty-specific gaps

- First 3 experiments:
  1. Replicate the Diagnostic Touchpoints task: Feed incremental case snippets to a model and measure accuracy trajectory per event to validate the sequential reasoning claim
  2. Ablate the exemplar retrieval in CaBot: Generate differentials with zero-shot vs. 2-shot case retrieval and compare physician preference ratings
  3. Test image-text integration: Run Visual DDx on the 356 cases with images/tables only, then add case text, and quantify the information gain per modality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can improvements in image interpretation be decoupled from improvements in text-based reasoning, or are multimodal integration capabilities fundamentally bottlenecked by language model scale?
- Basis in paper: [explicit] The authors state that "clinical image interpretation and multimodal integration are key remaining challenges for generalist clinical AI" and note o3 achieved only 67% accuracy on image-only diagnosis tasks versus 84% top-10 on text-based differential diagnosis.
- Why unresolved: The paper evaluates multimodal performance but does not isolate whether image interpretation lags due to vision encoder limitations, cross-modal alignment, or insufficient medical image pretraining data.
- What evidence would resolve it: Ablation studies comparing models with matched language backbones but different vision encoders, or evaluating whether scaling vision components independently improves multimodal medical reasoning.

### Open Question 2
- Question: How well does CPC-Bench performance generalize to real-world clinical settings with less curated, messier patient data?
- Basis in paper: [explicit] The authors acknowledge that "CPCs are highly curated, with a bias towards rare diseases or unusual presentations of common diseases" and that "incautious uses of the CPCs can lead to overstating the capabilities of AI systems in real-life diagnosis."
- Why unresolved: The benchmark uses structured, information-dense case presentations designed for education rather than realistic clinical workflows.
- What evidence would resolve it: Prospective evaluation of models on de novo hospital cases with EHR-extracted presentations, or comparison between CPC performance and performance on unstructured clinical notes.

### Open Question 3
- Question: What is the extent of training data contamination from NEJM CPCs in frontier model pretraining corpora?
- Basis in paper: [explicit] The authors state that "the training data used for the LLMs evaluated here are not publicly known and may include some of the CPCs along with their answers" and attempt mitigation by analyzing newer cases, though this reduces but does not eliminate concern.
- Why unresolved: Proprietary model training data opacity makes direct contamination assessment impossible; temporal cutoff analysis is an imperfect proxy.
- What evidence would resolve it: Collaboration with model developers to audit pretraining corpora for CPC content, or evaluation on synthetic cases with novel disease presentations unavailable in any public corpus.

## Limitations

- Data contamination risk: Potential overlap between training corpora and CPC content remains uncertain despite authors' mitigation attempts
- LLM judge reliability: While validated at 86% accuracy, performance on edge cases and rare diagnoses was not characterized
- Historical case relevance: Using pre-1945 CPCs introduces potential knowledge gaps as medical practice has evolved significantly

## Confidence

- High confidence: Sequential diagnostic accuracy improvements (29 percentage point increase, 95% CI 22-36%) and o3's 60% top-1 accuracy are well-supported by direct measurements with physician-validated ground truth
- Medium confidence: CaBot's stylistic quality claims rely on blinded physician comparisons (74% misclassification rate) but lack detailed breakdown by case complexity or presentation style
- Medium confidence: Scale-driven performance gains (from 44% to 84% without fine-tuning) are demonstrated, but the specific architectural contributions versus pretraining scale remain inferential

## Next Checks

1. Conduct systematic data contamination analysis: Stratify performance by case publication date relative to model training cutoffs to quantify leakage effects
2. Perform adversarial evaluation of LLM judge: Create challenging cases with rare or complex presentations to stress-test the 86% accuracy validation
3. Analyze multimodal failure modes: Systematically compare image-only, text-only, and combined performance across specialties to identify architectural bottlenecks in visual reasoning