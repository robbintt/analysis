---
ver: rpa2
title: SHAP-based Explanations are Sensitive to Feature Representation
arxiv_id: '2505.08345'
source_url: https://arxiv.org/abs/2505.08345
tags:
- feature
- shap
- data
- race
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that SHAP-based explanations are highly
  sensitive to feature representation choices, such as bucketization and encoding
  methods. Through systematic experiments on ACS Income and ACS Public Coverage datasets,
  the authors show that simple data engineering techniques can manipulate feature
  importance rankings by as much as 20 positions.
---

# SHAP-based Explanations are Sensitive to Feature Representation

## Quick Facts
- arXiv ID: 2505.08345
- Source URL: https://arxiv.org/abs/2505.08345
- Reference count: 40
- Primary result: SHAP explanations can be manipulated by feature engineering, obscuring protected attributes like age and race while maintaining explanation fidelity

## Executive Summary
This paper reveals a critical vulnerability in SHAP-based explanations: they are highly sensitive to how features are represented in the data. Through systematic experiments on ACS Income and ACS Public Coverage datasets, the authors demonstrate that common data engineering techniques like bucketization and categorical encoding can manipulate feature importance rankings by as much as 20 positions. They design a feature engineering attack using Bayesian Optimization to deliberately reduce the importance of protected features while maintaining explanation fidelity, outperforming standard bucketization methods.

## Method Summary
The authors use ACS Income (Virginia 2018, 46,144 samples, 8 features) and ACS Public Coverage (Virginia 2018, 25,524 samples, 16 features) datasets with XGBoost classifiers. They systematically vary feature representations through bucketization and categorical encoding, then measure SHAP importance changes. For the attack, they formulate an optimization problem using Bayesian Optimization (300 iterations, 4 bucket boundaries) to minimize protected feature SHAP rank while maintaining fidelity. 5-fold cross-validation is reported throughout.

## Key Results
- Bucketizing age into 10 bins can shift its SHAP rank by up to 20 positions among false negative individuals
- Merging race categories (e.g., "White+Black, Asian+Other") reduces race's global SHAP importance by 3.6× compared to one-hot encoding
- The Bayesian Optimization attack achieves 5.4× better suppression of protected feature importance than standard equi-width bucketization while maintaining explanation fidelity

## Why This Works (Mechanism)

### Mechanism 1
Bucketization alters the marginal contribution of a feature by changing the intervention distribution used in Shapley value estimation. When continuous features like age are bucketized, the intervention shifts from changing specific values to changing representative bucket values, obscuring the distinctiveness of original values and reducing their apparent contribution.

### Mechanism 2
Merging categorical values dilutes feature importance by increasing within-group variance and reducing predictive power. When distinct categories with opposing correlations to the target are merged, the signal-to-noise ratio decreases and the average marginal contribution approaches zero, lowering the feature's rank.

### Mechanism 3
Adversarial optimization can identify bucket boundaries that explicitly minimize SHAP rank of protected features while maintaining predictive fidelity. The Bayesian Optimization framework iteratively proposes boundary configurations to minimize the objective function (negative SHAP rank) subject to fidelity constraints.

## Foundational Learning

- **Shapley Values and Marginal Contribution**: Needed to understand how SHAP attributes credit through marginal contributions; Quick check: If a feature is split into two perfectly correlated features, how does the sum of their Shapley values compare to the original?

- **Feature Discretization (Bucketization/Binning)**: Critical to understanding the attack vector; Quick check: Why might equi-depth (quantile) binning affect feature importance differently than equi-width binning on a skewed distribution?

- **Explanation Fidelity vs. Faithfulness**: Distinguishes between explanation accuracy to the model versus accuracy to the real-world features; Quick check: If an explanation has 100% fidelity to a model, does that guarantee the model is fair or that the explanation is truthful about real-world features?

## Architecture Onboarding

- **Component map**: Raw Data -> Preprocessing Layer (Bucketizer, Encoder) -> Model (XGBoost) -> Explainer (SHAP) -> Attack Module (Bayesian Optimization)
- **Critical path**: The Preprocessing Layer is the attack surface; trace the mapping from Raw Feature -> Preprocessed Feature
- **Design tradeoffs**: Stability vs. Granularity (granular features resist attacks but may be noisier) and Auditability vs. Performance (standard transformations reduce attack surface)
- **Failure signatures**: Rank volatility (15+ position drops), high fidelity with low protected feature importance, unnatural bucket boundaries
- **First 3 experiments**:
  1. Train model on raw data, measure SHAP rank of age; then train on 10-bin equi-width bucketized age, compare rank shift for false negative individuals
  2. Implement BO attack on age feature targeting >10 position rank reduction while maintaining >90% fidelity
  3. Apply "White+Black, Asian+Other" race merge strategy and measure drop in global SHAP importance vs. one-hot baseline

## Open Questions the Paper Calls Out

- **Detection and prevention of feature engineering attacks**: While the paper demonstrates vulnerability, developing technical defenses to detect when bucketization/encoding choices suppress protected feature importance remains an open challenge.

- **Multi-feature suppression**: The current framework targets single protected features; developing holistic methods to jointly consider multiple sensitive features (e.g., age and race) is needed for real-world applications.

- **Generalization to other explanation methods**: The vulnerability may not be unique to SHAP; testing whether LIME, QII, and other post-hoc methods are equally susceptible requires investigation.

- **Scalable exploration of categorical groupings**: For high-cardinality features (>10 categories), semi-automated exploration of combinatorial grouping space needs efficient algorithms beyond exhaustive enumeration.

## Limitations

- The paper focuses exclusively on SHAP, not testing whether other explanation methods share the same vulnerability
- Only predefined categorical groupings are tested; principled search through combinatorial space remains unexplored
- The relative advantage of BO attacks over standard methods needs verification with exact implementation details

## Confidence

- **High confidence**: SHAP sensitivity to bucketization encoding is real and reproducible with well-grounded mechanism
- **Medium confidence**: Bayesian Optimization attack effectiveness is plausible given methodology but requires implementation details
- **Low confidence**: The 5.4× advantage of BO attack over equi-width needs verification with exact implementation

## Next Checks

1. **Hyperparameter stability test**: Fix XGBoost hyperparameters and run sensitivity experiments across 5 random seeds, documenting rank variance
2. **Background data ablation**: Compare SHAP outputs using 50, 100, 500 background samples on same bucketed features to assess sensitivity
3. **Per-instance fidelity audit**: Compute fidelity breakdown by prediction outcome (FN, FP, TP, TN) for BO attack configuration to identify systematic explanation failures