---
ver: rpa2
title: Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline
  Reinforcement Learning in Care Coordination and Population Health Management
arxiv_id: '2509.16291'
source_url: https://arxiv.org/abs/2509.16291
tags:
- cost
- calibration
- value
- local
- deliberation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a lightweight offline reinforcement learning
  approach, TTL+ITD, for optimizing care coordination and population health management
  decisions in large Medicaid populations. The method augments trained policies with
  local neighborhood calibration (TTL) and inference-time deliberation (ITD) via a
  Q-ensemble that incorporates predictive uncertainty and time/effort cost.
---

# Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management

## Quick Facts
- arXiv ID: 2509.16291
- Source URL: https://arxiv.org/abs/2509.16291
- Authors: Sanjay Basu, Sadiq Y. Patel, Parth Sheth, Bhairavi Muralidharan, Namrata Elamaran, Aakriti Kinra, Rajaie Batniji
- Reference count: 25
- One-line primary result: TTL+ITD achieves near-zero harm with substantially lower expected effort costs (17.1 vs 0.05 normalized cost units) compared to baselines in large Medicaid care coordination.

## Executive Summary
This study introduces TTL+ITD, a lightweight offline reinforcement learning framework for optimizing care coordination and population health management in large Medicaid populations. The approach combines local neighborhood calibration (TTL) with inference-time deliberation (ITD) via a Q-ensemble that incorporates predictive uncertainty and time/effort cost. By deferring efficiency-safety trade-offs to inference time through adjustable governance dials, TTL+ITD achieves stable value estimates while enabling predictable efficiency trade-offs without requiring retraining. The method supports auditable and equitable deployment in strictly offline settings.

## Method Summary
TTL+ITD augments trained policies with two inference-time components: local neighborhood calibration (TTL) and inference-time deliberation (ITD). TTL builds a kNN index over z-scored calibration states to derive local action-conditional thresholds from (1-α) quantiles, blending base policy with neighborhood action priors. ITD uses a bootstrap ensemble of linear FQE models to estimate Q_mean and Q_std per action, creating a deliberation score that trades off expected value, uncertainty, risk, and cost while masking unsafe actions. The framework provides transparent governance dials for neighborhood size and uncertainty/cost penalties, enabling predictable efficiency-safety trade-offs without retraining.

## Key Results
- TTL+ITD achieves near-zero harm (V̂₀≈0) at 0.05 normalized cost units versus 17.1 cost units for behavior cloning
- Efficiency-safety trade-off controlled via inference-time dials (K, β, λ, λ_cost) without retraining
- Monotone efficiency frontier maintained through monotonic masking of actions based on safety thresholds
- Local calibration via kNN reduces expected harm compared to global thresholds while maintaining policy performance

## Why This Works (Mechanism)

### Mechanism 1: Local Conformal Calibration via kNN (TTL)
- Claim: Personalizing safety thresholds to local neighborhoods reduces expected harm compared to global thresholds.
- Mechanism: A kNN index over z-scored calibration states retrieves similar historical cases; the (1−α) quantile of neighbor-specific risk scores defines action-conditional local thresholds τ_s(a). The base policy's action probabilities are then blended with neighborhood action priors (η=0.3 default).
- Core assumption: Similar patients (in feature space) have similar risk profiles; calibration data is representative of deployment distribution.
- Evidence anchors: [abstract] "local neighborhood calibration...incorporates predictive uncertainty and time/effort cost"; [§3 Methods] "TTL builds a kNN index over the z-scored calibration states and derives local thresholds τ_s(a) from the (1−α) quantile"
- Break condition: If calibration neighborhoods are sparse or features are poorly discriminative, local quantiles become noisy; reverts toward global threshold behavior.

### Mechanism 2: Uncertainty-Weighted Deliberation via Q-Ensemble (ITD)
- Claim: Penalizing actions with high predictive uncertainty shifts selection toward better-supported decisions without retraining.
- Mechanism: A bootstrap ensemble of linear FQE models estimates Q_mean and Q_std per action. The deliberation score Q_mean − β·Q_std − λ·p_harm − λ_cost·c(a) trades off expected value, uncertainty, risk, and cost; unsafe actions (p_harm ≥ τ_s(a)) are masked before selection.
- Core assumption: Ensemble variance correlates with epistemic uncertainty; the feature map captures state-action relevance.
- Evidence anchors: [§3 Methods, Eq.1] explicit score formula with uncertainty penalty β; [§4 Results] "larger β suppresses uncertain actions" shown in sensitivity heatmaps
- Break condition: If ensemble diversity collapses, Q_std no longer reflects meaningful uncertainty; penalty becomes arbitrary noise.

### Mechanism 3: Monotone Efficiency–Safety Frontier via Governance Dials
- Claim: Inference-time dials (K, β, λ, λ_cost) produce predictable, auditable trade-offs without model retraining.
- Mechanism: Increasing any penalty (β, λ, λ_cost) or decreasing α only removes candidate actions (monotonic masking). This induces a smooth efficiency frontier where operators navigate value vs. effort cost post-hoc.
- Core assumption: Actions are substitutable and harm is already gated by conformal thresholds.
- Evidence anchors: [§4 Results, Table 2] TTL+ITD achieves V̂₀≈0 at 0.05 normalized cost vs. BC at 17.1 cost units; [Appendix C] "combination of global and local quantiles induces a monotone safety–efficiency frontier"
- Break condition: If cost penalties are set too high, policy collapses to cheapest modality regardless of value.

## Foundational Learning

- **Offline Reinforcement Learning**
  - Why needed here: Policy learning from logged data without environment interaction; all optimization must occur within the support of historical behavior.
  - Quick check question: Can you explain why off-policy evaluation (FQE, DR) is necessary when you cannot run the policy online?

- **Conformal Prediction**
  - Why needed here: Provides finite-sample coverage guarantees for risk thresholds; enables calibration of harm-probability cutoffs with interpretable error rates.
  - Quick check question: What does marginal coverage at level (1−α) guarantee about the probability of exceeding the threshold?

- **Ensemble Uncertainty Estimation**
  - Why needed here: Bootstrap aggregating across FQE models yields Q_std as a proxy for epistemic uncertainty; enables penalization of poorly supported actions.
  - Quick check question: Why does ensemble variance approximate uncertainty, and when might it fail (e.g., mode collapse)?

## Architecture Onboarding

- **Component map:** State (JSON) → Feature Parser (64 keys + temporal) → z-scored features → Risk Model (multinomial logistic) → p_harm(s,a) per action → Preference Model (safe subset) → Base policy π_base(a|s) → Blending (η=0.3) ← Local thresholds τ_s(a) ← kNN Calibrator (K neighbors) ← Q-Ensemble (bootstrap linear FQE) → Q_mean, Q_std → Deliberation Score: Q_mean - β·Q_std - λ·p_harm - λ_cost·c(a) → Action Selection (mask unsafe, greedy/softmax)

- **Critical path:** Feature parsing → kNN query → ensemble scoring → mask + selection. Latency dominated by kNN lookup (O(N_cal·d)) and ensemble forward passes (small constant for linear models).

- **Design tradeoffs:**
  - K (neighborhood size): Larger K improves stability but dilutes locality; smaller K increases personalization risk if sparse.
  - β (uncertainty penalty): Higher β increases conservatism; may reject beneficial actions with noisy estimates.
  - λ_cost: Primary lever for efficiency; setting too high collapses to cheapest action.
  - Linear vs. nonlinear models: Linear FQE is auditable but may underfit complex value functions; gradient-boosted variants available as drop-in.

- **Failure signatures:**
  - All actions masked: Check if τ_s(a) is too strict (α too small) or p_harm estimates systematically high.
  - Cost insensitive to λ_cost: Verify cost dictionary c(a) is loaded and normalized correctly.
  - Ensemble variance near zero: Inspect bootstrap sampling (episode-level resampling required).
  - Value estimates unstable across seeds: Increase calibration set size or reduce K.

- **First 3 experiments:**
  1. **Baseline sweep:** Run TTL+ITD with (K=200, β=0.5, λ=1.0, λ_cost=0) against BC, CQL, and global-τ; reproduce Table 2 using provided FQE harness.
  2. **Dial sensitivity:** Vary λ_cost ∈ {0, 0.25, 0.5, 0.75, 1.0, 2.0} and plot efficiency frontier (expected value vs. cost); identify knee point.
  3. **Ablation:** Disable TTL (use global τ only) and separately disable ITD (use base policy alone); quantify contribution of each component to harm reduction and cost savings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TTL+ITD demonstrate safety and efficiency in prospective clinical deployments compared to standard care coordination?
- Basis in paper: [explicit] "Prospective evaluation remains the decisive next step: we plan to run matched-control pilots that monitor visit rates, harm events, and staff satisfaction."
- Why unresolved: The current study relies on retrospective offline policy evaluation (FQE) and historical data, which cannot capture real-time operational dynamics or staff behavioral changes.
- What evidence would resolve it: Results from prospective matched-control pilots showing maintained or improved outcomes in live settings.

### Open Question 2
- Question: Do local kNN calibration methods provide formal conditional coverage guarantees, or are they limited to empirical marginal coverage?
- Basis in paper: [explicit] "Local (kNN) calibration yields empirical benefits but does not provide formal conditional coverage guarantees... formal guarantees... are left for future work."
- Why unresolved: Theoretical analysis in the current work is restricted to marginal coverage (global conformal gating), while local calibration remains an approximation.
- What evidence would resolve it: Theoretical proofs establishing conditional coverage bounds under specific smoothness or overlap assumptions.

### Open Question 3
- Question: Can the inference-time deliberation framework be extended to high-risk clinical interventions using episodic model-based lookahead?
- Basis in paper: [explicit] The paper suggests "local conformal calibration can be combined with episodic model-based lookahead... extending TTL+ITD beyond the low-risk coordination setting explored here."
- Why unresolved: The current validation is restricted to low-risk outreach modalities (text, phone, etc.) rather than high-stakes medical decisions.
- What evidence would resolve it: Successful integration of lookahead mechanisms and safety validation on clinical actions (e.g., medication changes).

## Limitations
- Calibration set representativeness: Local conformal thresholds rely on calibration neighborhoods being drawn from the same distribution as deployment; drift may cause miscalibration.
- Ensemble uncertainty quality: Bootstrap linear FQE may underfit complex value surfaces; Q_std may not reliably signal epistemic uncertainty in sparse reward regimes.
- Cost dictionary sensitivity: Exact cost mapping c(a) and normalization scale are not fully specified in public release; incorrect scaling could distort efficiency frontier.

## Confidence
- **High**: Empirical efficiency–safety trade-off results (Table 2, Figure 1); monotone frontier claim supported by ablation curves; reproducibility via provided codebase.
- **Medium**: Theoretical grounding of kNN conformal calibration for offline RL; conditional coverage guarantees not formally proven for non-i.i.d. action selection.
- **Low**: Assumptions about substitutability of actions (that lower-cost modalities can replace higher-cost ones without catastrophic value loss) in real care settings.

## Next Checks
1. **Out-of-distribution calibration test**: Simulate deployment drift by restricting calibration set to early time periods and measuring threshold coverage on later periods; quantify calibration degradation.
2. **Ensemble variance ablation**: Compare TTL+ITD performance with (a) full ensemble, (b) single FQE, (c) dropout-based uncertainty; measure impact on harm reduction and efficiency frontier width.
3. **Cost sensitivity robustness**: Systematically perturb c(a) by ±20% and trace resulting efficiency frontiers; verify that key knee points remain stable and no catastrophic policy collapse occurs.