---
ver: rpa2
title: 'Revisiting the Relationship between Adversarial and Clean Training: Why Clean
  Training Can Make Adversarial Training Better'
arxiv_id: '2504.00038'
source_url: https://arxiv.org/abs/2504.00038
tags:
- training
- adversarial
- clean
- learning
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adversarial training (AT)
  in deep learning, where models become robust to adversarial attacks but suffer from
  decreased generalization on clean data. The authors propose that this decline stems
  from AT's difficulty in learning certain features of single-view samples.
---

# Revisiting the Relationship between Adversarial and Clean Training: Why Clean Training Can Make Adversarial Training Better

## Quick Facts
- arXiv ID: 2504.00038
- Source URL: https://arxiv.org/abs/2504.00038
- Reference count: 22
- Primary result: CKTAT achieves 56.80% robust accuracy and 83.45% natural accuracy on CIFAR-10 with WideResNet34-10

## Executive Summary
This paper addresses the fundamental challenge in adversarial training where models become robust to attacks but suffer from decreased generalization on clean data. The authors propose that this degradation stems from adversarial training's difficulty in learning certain sample features, particularly from single-view samples. They introduce CKTAT (Clean Knowledge Transfer for Adversarial Training), a novel label design method that leverages clean-trained models to guide adversarial training. By using the predictions of a clean model as targets for adversarial samples and reducing learning difficulty through initialization and temperature scaling, CKTAT significantly improves both natural and robust accuracy compared to existing methods.

## Method Summary
CKTAT employs a two-phase approach: first training a clean model using standard cross-entropy, then initializing an adversarial model with the clean model's weights. The adversarial model is trained using a combined loss function that incorporates both the clean model's temperature-scaled predictions for adversarial samples and consistency regularization. The temperature scaling softens the clean model's output distribution to reduce learning difficulty, while the initialization allows the adversarial model to start with features already learned by the clean model. This framework aims to help adversarial training learn features it would otherwise struggle with while maintaining the robustness benefits of adversarial training.

## Key Results
- CKTAT (β = 6) achieves 56.80% robust accuracy and 83.45% natural accuracy on CIFAR-10
- The method outperforms state-of-the-art AT methods including TRADES and PGDAT on both natural and robust accuracy
- Ablation studies show that both clean model initialization and temperature scaling are necessary for optimal performance
- The approach works particularly well with larger models like WideResNet34-10, with smaller models like ResNet18 requiring additional difficulty reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training fails to learn features from single-view samples at higher rates than clean training, contributing to generalization degradation.
- Mechanism: When a model begins learning a new feature $v_{y,3-j}$, the associated dense mixture weights $m_{v_{y,3-j}}$ have not yet been purified through adversarial training. This causes high sensitivity to perturbations (large $k_2$), which can increase global robust error $R_{robust}^2 - R_{robust}^1 > 0$ under the condition derived in Section 3.2. Multi-view data containing already-learned features further suppresses learning of new features. Clean training avoids this because it optimizes only for clean accuracy, where learning new features monotonically reduces error ($R_{clean}^2 - R_{clean}^1 = -0.5\mu/N < 0$).
- Core assumption: Data follows a multi-view distribution where $\mu$-fraction are single-view (lacking some label-relevant features) and $(1-\mu)$-fraction are multi-view; features are approximately orthogonal; adversarial perturbations align with dense mixture directions.
- Evidence anchors:
  - [abstract] "We reveal that the problem of generalization degradation faced by AT partly stems from the difficulty of adversarial training in learning certain sample features."
  - [section 3.2] Equations (7)–(12) and the inequality condition showing when $R_{robust}^2 - R_{robust}^1 \geq 0$; concrete example with $\mu=0.4, k_2=0.8, k_1=0.3$ yielding non-negative error change.
  - [corpus] Narrowing Class-Wise Robustness Gaps (FMR=0.726) discusses class-wise robustness disparities in AT, consistent with feature-learning heterogeneity, but does not directly test the single-view mechanism.
- Break condition: If dense mixture purification occurs rapidly early in training, or if single-view samples dominate ($\mu$ large), the incentive to learn new features may recover. The mechanism also breaks if perturbations do not align with dense mixture directions.

### Mechanism 2
- Claim: Clean model predictions provide correct guidance for adversarial training by encoding more complete feature knowledge (including features missed by AT), which can be transferred to the adversarial model.
- Mechanism: Per Equations (13)–(15), clean-trained weights $\omega_v^{clean-training} = \omega_v^{pure} + \omega_v^{mixture}$; adversarially-trained weights $\omega_v^{adv-training} = \omega_v^{pure} + k\omega_v^{mixture}$ with $0 < k < 1$. An ideal robust model would have $\omega_v^{great-training} = \omega_v^{pure}$. Section 4.1 argues $O_{clean-training}^{clean-example} \approx O_{great-training}^{clean-example} = O_{great-training}^{adv-example}$, so clean predictions approximate what an ideally robust model would output on both clean and adversarial inputs. Using $T^\tau(x)$ as the target for adversarial samples $S(x')$ helps (a) eliminate dense mixtures and (b) transfer features the adversarial model would otherwise miss.
- Core assumption: The clean model has learned both label-relevant features for most classes; the difference between clean and adversarial outputs is primarily due to dense mixtures rather than fundamentally different representations.
- Evidence anchors:
  - [abstract] "We propose a new idea of leveraging clean training to further improve the performance of advanced AT methods."
  - [section 4.1] Equations (13)–(16) and Definition 4.1 linking clean/adversarial feature composition to output alignment; visualization (Table 5) showing CKTAT learns more discriminative features than TRADES/PGDAT.
  - [corpus] Weak direct evidence; related papers focus on data augmentation or robust distillation, not clean-to-robust knowledge transfer via labels.
- Break condition: If clean model predictions encode spurious correlations not present in adversarial samples, or if the clean model is severely overfitted/miscalibrated, guidance may be incorrect. The assumption that $O_{clean-training}^{clean-example} \approx O_{great-training}^{adv-example}$ requires validation on specific datasets.

### Mechanism 3
- Claim: Reducing learning difficulty through clean-model initialization and temperature scaling is necessary for the adversarial model to successfully absorb guidance from the clean teacher.
- Mechanism: Table 1 shows that directly using a clean teacher improves both clean and robust accuracy on WideResNet34-10 but degrades both on ResNet18. This suggests smaller-capacity models cannot directly mimic clean predictions. Two interventions reduce difficulty: (1) initializing $S$ with $T$'s weights so it need not re-learn features; (2) applying temperature $\tau$ to soften $T$'s output distribution, lowering KL divergence targets per prior work on knowledge distillation difficulty. The final loss (Equation 17) combines $KL(T^\tau(x), S(x'))$ (guidance) with $\beta \cdot KL(S(x), S(x'))$ (consistency regularization).
- Core assumption: The transfer gap between clean and adversarial models is primarily a capacity/difficulty issue, not a fundamental incompatibility; temperature scaling appropriately modulates difficulty.
- Evidence anchors:
  - [abstract] "...while also reducing the learning difficulty through model initialization and temperature scaling."
  - [section 4.2, Table 1] WideResNet34-10 vs. ResNet18 comparison; Table 3(b) showing accuracy improves with $\tau \in [1, 6]$ and peaks at $\tau=5$; Table 3(a) showing removal of initialization or $KL(T^\tau(x), S(x'))$ degrades performance.
  - [corpus] No direct corpus validation of the difficulty-reduction mechanism; related work on distillation difficulty exists but is not cited in provided neighbors.
- Break condition: If the clean model and adversarial model have incompatible architectures (preventing weight initialization), or if $\tau$ is set too high (over-smoothing labels) or too low (retaining hard targets), benefits diminish. The $\beta$ parameter also trades off consistency vs. guidance strength.

## Foundational Learning

- Concept: Multi-view hypothesis for data distribution
  - Why needed here: The paper's theoretical argument that AT struggles with single-view samples relies on understanding that a fraction $\mu$ of data lacks some label-relevant features, while multi-view data has redundant features. Without this, the error analysis in Section 3.2 is opaque.
  - Quick check question: Given a binary classification problem with features $\{v_1, v_2\}$ for class 1 and $\{v_3, v_4\}$ for class 2, if 60% of class-1 samples have both $v_1$ and $v_2$ while 40% have only one, which portion is "single-view"?

- Concept: Dense mixture weights and adversarial vulnerability
  - Why needed here: The paper's core explanation for why AT initially struggles to learn new features is that newly-learned feature weights contain unpurified dense mixtures that are highly responsive to adversarial perturbations (Equation 6). Understanding this is essential for grasping why learning new features can temporarily increase robust error.
  - Quick check question: Per Equation (6), why does the inner product $\langle m, \delta \rangle$ grow as $\Omega(k\sqrt{d}\|\delta\|_2)$ when $\delta$ aligns with dense mixture directions, compared to $\langle m, x \rangle = O(k\|\delta\|_2/d)$ for clean inputs?

- Concept: Knowledge distillation with temperature scaling
  - Why needed here: The CKTAT loss uses $KL(T^\tau(x), S(x'))$ where $\tau$ softens the teacher's output distribution. Without understanding how temperature affects the hardness of the distillation target, the ablation results in Table 3(b) are difficult to interpret.
  - Quick check question: If a teacher model outputs logits $[4.0, 1.0, 1.0]$ for a 3-class problem, what happens to the softmax probabilities when temperature $\tau$ increases from 1 to 4? How does this affect the KL divergence a student must minimize?

## Architecture Onboarding

- Component map:
  - Clean teacher model $T$ -> Temperature scaling $\tau$ -> Softened predictions $T^\tau(x)$
  - Clean teacher model $T$ -> Weight initialization for adversarial student $S$
  - Adversarial student model $S$ -> PGD attack generation $x' = \text{attack}(x, y)$
  - Adversarial student model $S$ -> Combined loss: $KL(T^\tau(x), S(x')) + \beta \cdot KL(S(x), S(x'))$

- Critical path:
  1. Train clean model $T$ on dataset $D$ for $N_{clean}$ iterations using standard cross-entropy.
  2. Initialize adversarial model $S$ with $T$'s weights (warm-start).
  3. For $N_1$ iterations: sample batch, generate adversarial examples $x'$, compute loss via Equation (17), update $S$.

- Design tradeoffs:
  - **$\beta$ (consistency weight)**: Higher $\beta$ (e.g., 6–7) prioritizes robust accuracy; lower $\beta$ (e.g., 0–2) favors natural accuracy (Table 3(c)). Default recommendation: $\beta=6$ for robustness-focused deployments.
  - **$\tau$ (temperature)**: Sweeping $\tau \in [1, 6]$ shows peak at $\tau=5$ on CIFAR-10 (Table 3(b)). Higher $\tau$ softens labels, reducing difficulty but potentially losing fine-grained information.
  - **Model capacity**: Clean-to-robust transfer works better with higher capacity (Table 1). On smaller models (ResNet18), may need additional adjustments or reduced $\tau$.

- Failure signatures:
  - **Removing initialization**: Table 3(a) shows ~1% drop in both natural and robust accuracy; model must relearn features from scratch.
  - **Removing $KL(T^\tau(x), S(x'))$**: Similar degradation; student lacks guidance on what features to acquire.
  - **Removing $KL(S(x), S(x'))$**: Natural accuracy increases but robust accuracy drops sharply (~2.5%); consistency term is critical for robustness.
  - **Small model + clean teacher without difficulty reduction**: Table 1(b) shows dual degradation on ResNet18.

- First 3 experiments:
  1. **Reproduction check**: Train CKTAT on CIFAR-10 with WideResNet34-10, $\tau=5$, $\beta=6$, 10-step PGD attacks ($\epsilon=8/255$). Verify robust accuracy ~56–57% and natural accuracy ~83% against Table 4(a) baselines.
  2. **Ablation sweep**: Systematically remove each component (initialization, $KL(T^\tau(x), S(x'))$, $KL(S(x), S(x'))$) to confirm contribution magnitudes match Table 3(a). Log training curves to observe if/when robust error increases during feature learning.
  3. **Capacity sensitivity**: Replicate Table 1 by running clean-teacher-to-adversarial-student on both ResNet18 and WideResNet34-10. Plot natural vs. robust accuracy as a function of model width/depth to characterize the capacity threshold where clean guidance becomes beneficial without difficulty reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CKTAT framework effectively scale to high-resolution datasets like ImageNet, or does the requirement for a pre-trained clean teacher model introduce prohibitive computational overhead?
- Basis in paper: [inferred] The experimental evaluation is restricted to CIFAR-10 and CIFAR-100. While the introduction claims label design has "lower computational cost" than data augmentation methods, the proposed CKTAT requires training a full clean model first.
- Why unresolved: The computational cost of training a clean model on ImageNet followed by adversarial training was not benchmarked, and the interactions between clean/robust features may differ in high-dimensional spaces.
- What evidence would resolve it: Benchmarking CKTAT performance and training time on ImageNet compared to standard AT methods.

### Open Question 2
- Question: Does the theoretical explanation regarding "hard-to-learn single-view samples" hold empirically for Vision Transformers (ViTs), or is it specific to the convolutional architectures tested?
- Basis in paper: [inferred] The theoretical analysis (Section 3) relies on the "dense mixtures of network's weight" theory derived for CNNs, and all experiments use ResNet and WideResNet architectures.
- Why unresolved: ViTs exhibit different feature learning dynamics (e.g., through attention mechanisms) compared to CNNs; it is unclear if the "single-view" vs. "multi-view" dynamic applies to attention weights.
- What evidence would resolve it: Applying CKTAT to ViT models and analyzing if the improvements in natural accuracy correlate with the learning of single-view sample features.

### Open Question 3
- Question: Can the optimal balance between "correct guidance" and "learning difficulty reduction" be determined adaptively, rather than relying on manual hyperparameter tuning?
- Basis in paper: [inferred] Section 5.1 analyzes sensitivity to temperature $\tau$ and regularization $\beta$, showing that performance varies significantly (e.g., natural accuracy vs. robust accuracy trade-offs change with $\beta$).
- Why unresolved: The paper sets these parameters via grid search (e.g., $\beta=6$ for robust accuracy), but provides no mechanism to automatically select the optimal $\tau$ or $\beta$ for a new dataset.
- What evidence would resolve it: A dynamic scheduling method for $\tau$ or $\beta$ that matches or exceeds the performance of the current static best-fit hyperparameters.

### Open Question 4
- Question: Does the "correct guidance" from the clean model inadvertently transfer non-robust features (dense mixtures) that compromise the robustness of the adversarial model under stronger attacks?
- Basis in paper: [explicit] Section 4.1 states: "This kind of label design can help adversarial training eliminate the dense mixtures... but various ways are needed to reduce the learning difficulty."
- Why unresolved: While CKTAT improves AutoAttack (AA) results, the transfer of "correct guidance" inherently involves transferring the clean model's weights ($\omega_{mixture}$), which contain non-robust dense mixtures, creating a tension between guidance and purification.
- What evidence would resolve it: A layer-wise analysis of the student model's weights to verify if the dense mixtures from the clean teacher are strictly suppressed or if traces remain that degrade robustness.

## Limitations

- The theoretical framework relies on specific assumptions about feature orthogonality and perturbation alignment that require empirical validation across diverse datasets and architectures.
- The effectiveness of clean-to-robust knowledge transfer assumes the clean model's predictions approximate ideal robust outputs, which may not hold for all datasets or model architectures.
- The difficulty-reduction mechanisms (initialization, temperature) show empirical success but the exact thresholds where they become necessary are not fully characterized across different model capacities.

## Confidence

- **High confidence**: The empirical results demonstrating CKTAT's performance gains over baselines on CIFAR-10 and CIFAR-100; the ablation studies showing individual components contribute positively to the final performance.
- **Medium confidence**: The theoretical explanation of why adversarial training struggles with single-view samples and the mechanism by which clean model guidance helps; the characterization of when clean-model initialization becomes necessary based on model capacity.
- **Low confidence**: The generalizability of the multi-view hypothesis across different dataset types and architectures; the precise relationship between temperature scaling and difficulty reduction in the distillation context.

## Next Checks

1. Test CKTAT on non-CIFAR datasets (e.g., ImageNet-1K, SVHN) to verify if the multi-view hypothesis holds and if clean guidance remains beneficial.
2. Conduct controlled experiments varying the fraction of single-view samples to empirically validate the theoretical error analysis in Section 3.2.
3. Analyze the learned representations (e.g., using probing classifiers or feature visualization) to verify that CKTAT indeed learns more discriminative features than standard adversarial training, as claimed.