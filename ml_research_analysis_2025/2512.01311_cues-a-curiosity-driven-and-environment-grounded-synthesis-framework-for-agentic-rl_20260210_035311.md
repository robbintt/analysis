---
ver: rpa2
title: 'CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for
  Agentic RL'
arxiv_id: '2512.01311'
source_url: https://arxiv.org/abs/2512.01311
tags:
- task
- arxiv
- cues
- tasks
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CuES addresses task scarcity in agentic RL by generating diverse,
  executable training tasks from environment structure without predefined seeds. It
  combines curiosity-driven bottom-up exploration with top-down guidance via concept
  pools and environment memory to synthesize tasks that align with learning needs.
---

# CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL

## Quick Facts
- arXiv ID: 2512.01311
- Source URL: https://arxiv.org/abs/2512.01311
- Reference count: 5
- Matches or surpasses manually curated datasets in diversity and executability, achieving up to 64.10% greedy success and 50.70% avg@8

## Executive Summary
CuES addresses task scarcity in agentic RL by generating diverse, executable training tasks from environment structure without predefined seeds. It combines curiosity-driven bottom-up exploration with top-down guidance via concept pools and environment memory to synthesize tasks that align with learning needs. Across AppWorld, WebShop, and BFCLv3, CuES matches or surpasses manually curated datasets in diversity and executability, achieving up to 64.10% greedy success and 50.70% avg@8, outperforming strong baselines and larger models.

## Method Summary
CuES is a task synthesis framework that addresses the challenge of limited training data in agentic RL by generating executable tasks directly from environment observations. The framework employs a dual-exploration strategy: bottom-up curiosity-driven exploration discovers novel states and actions, while top-down concept pool guidance provides structured task templates. An environment memory component stores successful task instances for future synthesis. The system generates tasks without requiring predefined seeds, making it adaptable to various environments. CuES was evaluated across three benchmarks (AppWorld, WebShop, BFCLv3) and demonstrated strong performance in task diversity and executability.

## Key Results
- Achieved up to 64.10% greedy success rate and 50.70% avg@8 across benchmarks
- Outperformed strong baselines and larger language models
- Matched or surpassed manually curated datasets in task diversity and executability

## Why This Works (Mechanism)
CuES leverages the synergy between curiosity-driven exploration and structured concept guidance to generate diverse, executable tasks. The bottom-up exploration discovers novel states and actions through intrinsic motivation, while the top-down concept pool provides templates that ensure task coherence and relevance. The environment memory component enables learning from successful experiences, creating a feedback loop that improves task quality over time. This dual approach allows the framework to generate tasks that are both diverse and aligned with the agent's learning needs, without requiring predefined task seeds.

## Foundational Learning
- Curiosity-driven exploration: Enables discovery of novel states and actions through intrinsic motivation. Why needed: To generate diverse tasks that cover the environment's possibilities. Quick check: Measure entropy of visited states over time.
- Concept pool guidance: Provides structured templates for task generation. Why needed: To ensure generated tasks are coherent and relevant to learning objectives. Quick check: Task executability rate across different concept categories.
- Environment memory: Stores successful task instances for future synthesis. Why needed: To learn from past experiences and improve task quality iteratively. Quick check: Success rate of generated tasks over training epochs.
- Dual exploration strategy: Combines bottom-up discovery with top-down guidance. Why needed: To balance task diversity with learning relevance. Quick check: Correlation between task diversity metrics and learning progress.

## Architecture Onboarding

Component Map:
Curiosity Exploration -> Task Synthesis -> Environment Memory -> Concept Pool -> Task Validation

Critical Path:
Environment observations -> Curiosity-driven state discovery -> Task template selection from concept pool -> Task generation -> Execution validation -> Environment memory update

Design Tradeoffs:
- Balance between task diversity and executability
- Computational cost of curiosity-driven exploration vs. task quality
- Size of environment memory vs. retrieval efficiency
- Granularity of concept pool templates vs. flexibility

Failure Signatures:
- Low task diversity despite high exploration
- High task generation rate but low executability
- Environment memory becoming stale or biased
- Concept pool templates not covering relevant scenarios

First Experiments:
1. Measure baseline task diversity and executability without CuES components
2. Test curiosity exploration alone to quantify its contribution to task discovery
3. Evaluate environment memory effectiveness by comparing task success rates with and without memory

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of ablation studies isolating individual component contributions
- Potential brittleness due to reliance on language models for task synthesis
- Limited discussion of how performance scales with environment complexity
- Vague comparison claims regarding "larger models" without specifying exact architectures

## Confidence
- Core claims: Medium
- Benchmark results: Medium
- Generalization to new environments: Low
- Scalability assessment: Low

## Next Checks
1. Conduct systematic ablation studies removing each component (curiosity-driven exploration, concept pool guidance, environment memory) to quantify their individual contributions
2. Test the framework's performance across different language model sizes and prompting strategies to establish robustness to model variations
3. Evaluate task transfer by measuring downstream performance on held-out tasks not seen during training to verify genuine skill acquisition versus metric optimization