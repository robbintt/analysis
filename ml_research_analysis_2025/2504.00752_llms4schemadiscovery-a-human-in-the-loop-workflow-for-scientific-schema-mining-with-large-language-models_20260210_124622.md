---
ver: rpa2
title: 'LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining
  with Large Language Models'
arxiv_id: '2504.00752'
source_url: https://arxiv.org/abs/2504.00752
tags:
- schema
- schemas
- data
- stage
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents schema-miner, a human-in-the-loop tool for
  scientific schema mining that combines large language models with expert feedback.
  The approach uses a three-stage workflow (initial schema generation, refinement
  with curated papers, and finalization with larger corpora) to extract structured
  schemas from unstructured scientific text.
---

# LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models

## Quick Facts
- **arXiv ID**: 2504.00752
- **Source URL**: https://arxiv.org/abs/2504.00752
- **Reference count**: 40
- **Primary result**: schema-miner achieves strong semantic alignment in scientific schema mining through human-in-the-loop LLM refinement, validated on atomic layer deposition in materials science.

## Executive Summary
This paper presents schema-miner, a human-in-the-loop tool for scientific schema mining that combines large language models with expert feedback. The approach uses a three-stage workflow (initial schema generation, refinement with curated papers, and finalization with larger corpora) to extract structured schemas from unstructured scientific text. Applied to atomic layer deposition in materials science, schema-miner achieved strong semantic alignment across models, with GPT-4o and Llama 3.1 (8B) showing consistent performance in capturing domain relationships. The tool successfully generated semantically rich schemas suitable for knowledge graph construction, with experts confirming its effectiveness in handling complex scientific processes while maintaining generalizability.

## Method Summary
The schema-miner workflow operates through three iterative stages using LLM prompts: (1) initial schema generation from a domain specification document, (2) refinement using a small set of curated scientific papers with expert feedback, and (3) finalization using a larger corpus to expand and generalize the schema. The tool employs ontology lookup services to ground schema properties in standardized vocabularies. Human experts provide feedback in two forms - descriptive suggestions and direct schema edits - which are incorporated into subsequent iterations. The workflow uses different LLM models (GPT-4o, GPT-4-turbo, Llama 3.1) accessed via Ollama or HuggingFace, with evaluation based on schema quality, semantic alignment, and expert assessment.

## Key Results
- GPT-4o and Llama 3.1 (8B) showed consistent performance in generating semantically aligned schemas for atomic layer deposition
- Human feedback was crucial in correcting repeated properties, unnecessary boolean fields, and overly complex nesting structures
- The tool successfully generated schemas suitable for knowledge graph construction with strong domain relationship capture
- GPT-4-turbo demonstrated "schema drift" by adding overly specific properties when processing larger, non-curated corpora

## Why This Works (Mechanism)
The approach leverages LLMs' pattern recognition capabilities to extract structured schemas from unstructured scientific text while using human experts to correct model limitations and ensure domain accuracy. The three-stage workflow progressively grounds the schema in real scientific literature, starting with expert-curated papers for quality and moving to larger corpora for generalizability. The iterative feedback loop allows the model to learn from corrections, improving schema relevance and structure. Ontology grounding provides semantic interoperability by linking schema properties to standardized vocabularies, enabling integration with existing knowledge bases.

## Foundational Learning
- **Concept: Schema (in the context of data/KGs)**
  - Why needed here: The entire paper is about "schema discovery"â€”extracting this structure S = (P, T, C, R) from text. Without understanding what a schema is, the paper's goal is unintelligible.
  - Quick check question: For a dataset of scientific experiments, what would be the 'P', 'T', 'C', and 'R' in its schema?

- **Concept: Human-in-the-Loop (HITL) Machine Learning**
  - Why needed here: The paper's core contribution is a HITL workflow. It assumes that LLM output alone is insufficient and requires expert correction at specific stages to achieve high-quality results.
  - Quick check question: At what two points in the three-stage schema-miner workflow does a human expert intervene, and what are the two forms their feedback can take?

- **Concept: Ontology and Semantic Grounding**
  - Why needed here: The final stage of the tool involves "ontology grounding." This requires understanding that an ontology is a formal representation of knowledge and that grounding means linking a model's output to this formal representation for interoperability.
  - Quick check question: Why does the paper argue that linking a schema property like "growth per cycle" to an ontology concept is valuable?

## Architecture Onboarding

- **Component map:**
  Input Processor -> LLM Core -> Iterative Workflow Manager -> Ontology Lookup Service (OLS) Interface

- **Critical path:**
  1. **Stage 1 (Initial Generation):** User provides a domain specification. System prompts LLM to generate `S1`. **Critical:** The quality of `S1` heavily depends on the clarity of the domain specification document.
  2. **Stage 2 (Refinement):** System iterates over a small set of curated papers (1-10). For each paper, it prompts the LLM with `S_prev`, paper content, and any expert feedback to produce `S_next`. **Critical:** This stage grounds the schema in high-quality literature; poor paper selection will bias the schema.
  3. **Stage 3 (Finalization):** System iterates over a larger, non-curated corpus. LLM is prompted to expand and generalize `S2` while avoiding irrelevant additions. **Critical:** Without proper prompting and expert oversight, models like GPT-4-turbo can "derail" and add overly specific properties.
  4. **Stage 4 (Grounding):** Final schema properties are sent to OLS to be mapped to ontology concepts. **Critical:** Success depends on the coverage of the connected ontology service.

- **Design tradeoffs:**
  - **Automated vs. Manual:** The system automates the heavy lifting of reading papers, but intentionally trades full automation for quality by requiring manual expert curation of the initial paper set and providing feedback. This reduces scalability but improves accuracy.
  - **Model Stability vs. Complexity:** The paper found GPT-4o and Llama 3.1 (8B) to be more stable than GPT-4-turbo, which often created overly complex or irrelevant properties. A tradeoff exists between using a more capable model that might be less predictable vs. a smaller, more stable one.
  - **Curated vs. Uncurated Data:** Stage 2 uses high-quality curated papers for grounding, while Stage 3 uses a larger, noisier corpus for generalizability. The tradeoff is between depth of domain grounding and breadth of coverage.

- **Failure signatures:**
  - **Schema Drift:** In Stage 3, the schema becomes bloated with properties specific to individual papers, losing generalizability. This was observed with GPT-4-turbo in the simulation use case.
  - **Excessive Complexity:** Generated schemas become overly nested and contain redundant properties, making them difficult for domain experts to use.
  - **Feedback Incorporation Failure:** The LLM ignores expert feedback in subsequent iterations, leading to a schema that does not reflect expert corrections.
  - **Ontology Grounding Failure:** The OLS returns no matches or irrelevant matches for schema properties, failing to provide semantic enrichment.

- **First 3 experiments:**
  1. **Stage 1 Baseline Test:** Provide a simple, well-defined domain specification (e.g., for a "book" or "event") and run Stage 1 only. Evaluate if the initial schema `S1` contains the expected properties, types, and constraints.
  2. **Feedback Ablation Study:** For a given domain, run the full workflow (Stages 1-3) in three modes: (a) no expert feedback, (b) descriptive feedback only, (c) direct schema edits only. Compare the quality and accuracy of the final schemas to quantify the impact of each feedback type.
  3. **Stage 3 Scalability Test:** Start with a well-refined `S2` from Stage 2. Run Stage 3 with progressively larger corpora (e.g., 20, 50, 100 papers) for a stable model (e.g., GPT-4o). Monitor for "schema drift" (addition of irrelevant or overly specific properties) as the corpus size increases.

## Open Questions the Paper Calls Out
- Can ensemble approaches combining multiple LLMs generate more robust and generalizable schemas than single-model workflows?
- To what extent can the human-in-the-loop feedback be automated or reduced without compromising schema quality?
- How does the workflow perform in scientific domains with heterogeneous reporting styles, such as social sciences or environmental science?

## Limitations
- The tool requires manual expert curation of initial paper sets, limiting scalability
- Performance depends heavily on the quality and coverage of connected ontology services
- The workflow has only been validated on one scientific domain (atomic layer deposition)
- GPT-4-turbo demonstrated "schema drift" when processing larger, non-curated corpora

## Confidence
- **Methodological soundness**: High - The paper presents a clear three-stage workflow with well-defined evaluation criteria and expert validation
- **Experimental design**: Medium - Limited to one domain with three LLM models, though results are detailed and reproducible
- **Generalizability claims**: Low - Claims of domain-agnostic applicability are not empirically validated beyond materials science

## Next Checks
- Run Stage 1 only with a simple domain specification to verify basic schema generation capability
- Implement the feedback ablation study to quantify the impact of human expert intervention
- Test Stage 3 scalability by progressively increasing corpus size and monitoring for schema drift
- Validate ontology grounding success rate across different schema property types
- Compare schema quality between GPT-4o and Llama 3.1 (8B) using the same input corpus