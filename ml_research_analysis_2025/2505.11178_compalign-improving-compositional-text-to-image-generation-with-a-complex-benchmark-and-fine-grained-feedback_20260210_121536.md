---
ver: rpa2
title: 'CompAlign: Improving Compositional Text-to-Image Generation with a Complex
  Benchmark and Fine-Grained Feedback'
arxiv_id: '2505.11178'
source_url: https://arxiv.org/abs/2505.11178
tags:
- compositional
- generation
- object
- image
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of compositional text-to-image
  (T2I) generation, where models struggle to accurately depict scenes involving multiple
  objects, attributes, and spatial relationships. They propose CompAlign, a benchmark
  with 900 complex prompts combining numerical and 3D-spatial relationships, and CompQuest,
  a fine-grained evaluation framework using a multimodal LLM to decompose prompts
  into atomic questions and judge each aspect of the generated image.
---

# CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback

## Quick Facts
- **arXiv ID**: 2505.11178
- **Source URL**: https://arxiv.org/abs/2505.11178
- **Reference count**: 37
- **Primary result**: Proposes CompAlign benchmark and CompQuest framework to evaluate and improve compositional accuracy in text-to-image generation using fine-grained MLLM feedback and diffusion model alignment.

## Executive Summary
The paper addresses the challenge of compositional text-to-image generation, where models struggle to accurately depict scenes involving multiple objects, attributes, and spatial relationships. The authors introduce CompAlign, a benchmark with 900 complex prompts combining numerical and 3D-spatial relationships, and CompQuest, a fine-grained evaluation framework using a multimodal LLM to decompose prompts into atomic questions and judge each aspect of the generated image. Their alignment framework uses this feedback as per-image preferences to improve diffusion models' compositional accuracy. Evaluations show CompAlign's difficulty, with performance gaps between open- and closed-source models. Fine-tuning diffusion models with CompQuest's feedback yields significant improvements in compositional accuracy, especially on complex generation tasks.

## Method Summary
CompAlign consists of 900 hierarchical prompts across 5 spatial configurations (1row×2sub through 2rows×3sub) and 6 categories (people_only, object_only, object_color, object_texture, object_color_bathroom, object_color_kitchen). The CompQuest framework decomposes each complex prompt into n atomic yes/no questions (one per entity), queries gpt-4o-mini-2024-07-18 with batched questions per image, and computes Aggregated Compositional Accuracy (ACA) as the proportion of positive judgments. For alignment, the framework generates augmented images using SD3.5 and DALL-E 3, computes ACA for each, and uses binary win/loss preferences (win if ACA ≥ τ=0.5) to fine-tune diffusion models via a Li et al. [16] objective with β=1000 KL penalty. Training uses 2× NVIDIA A6000, batch size 4 per GPU, Adam optimizer with lr=1e-7 for 10000 iterations, and γ=0.8.

## Key Results
- CompAlign demonstrates significant difficulty for state-of-the-art models, with clear performance gaps between open- and closed-source systems.
- Fine-tuning diffusion models with CompQuest feedback yields measurable improvements in compositional accuracy, particularly for complex multi-object scenes.
- The hierarchical prompt structure effectively captures 3D-spatial relationships and attribute bindings that challenge current T2I systems.
- Models can "hack" the accuracy score by generating collaged scenes with high compositional accuracy but low naturalness.

## Why This Works (Mechanism)
The approach works by providing precise, atomic-level feedback on compositional accuracy rather than holistic image quality judgments. By decomposing complex prompts into specific yes/no questions about each entity's presence, attributes, and spatial relationships, the MLLM can identify exactly which compositional elements succeed or fail. This fine-grained feedback enables targeted improvements during alignment training, as the model learns to satisfy specific compositional requirements rather than optimizing for general image quality.

## Foundational Learning
- **Compositional accuracy metrics**: Needed to measure multi-object scene generation fidelity; quick check: verify ACA correlates with human judgments on compositional correctness.
- **Hierarchical prompt templates**: Enable systematic generation of complex compositional scenes; quick check: ensure all spatial configurations and attribute combinations are represented.
- **Multimodal LLM evaluation**: Provides automated, consistent feedback on image-text alignment; quick check: validate MLLM consistency across repeated evaluations.
- **Diffusion model fine-tuning with preferences**: Allows targeted improvements on specific failure modes; quick check: monitor KL divergence to prevent over-fitting during alignment.
- **Binary preference learning**: Simplifies complex feedback into actionable training signals; quick check: verify threshold τ selection doesn't create class imbalance.

## Architecture Onboarding

**Component Map**: Hierarchical prompt templates → CompQuest decomposition → MLLM evaluation → ACA computation → Binary preference assignment → Diffusion model fine-tuning

**Critical Path**: Prompt generation → MLLM evaluation → ACA calculation → Preference labeling → Model alignment → Performance validation

**Design Tradeoffs**: Binary feedback simplifies training but may miss nuanced compositional errors; augmentation relies on strong models but introduces potential bias; hierarchical templates ensure coverage but may not capture all real-world complexity.

**Failure Signatures**: Low ACA gains after alignment suggest threshold τ sensitivity or insufficient "win" samples; MLLM inconsistency on complex images indicates decomposition issues or need for stronger judge model; overfitting during fine-tuning shows insufficient regularization or data diversity.

**First Experiments**: 1) Validate MLLM consistency on held-out prompts; 2) Test threshold sensitivity (τ=0.3 to 0.7); 3) Assess augmentation quality through human evaluation.

## Open Questions the Paper Calls Out
- How can evaluation benchmarks effectively penalize models that achieve high compositional accuracy by generating unnatural, disjointed "collaged" scenes rather than coherent, harmonic images?
- Can the CompAlign fine-tuning framework effectively scale to improve state-of-the-art transformer-based diffusion models (e.g., Flux.1, SD3.5), or is it limited to improving weaker U-Net architectures?
- To what extent do pre-training biases or hallucinations in the judge MLLM (GPT-4o-mini) compromise the reliability of the CompQuest evaluation scores?

## Limitations
- The binary win/loss labeling at threshold τ=0.5 may be overly simplistic for nuanced compositional errors.
- The evaluation framework relies heavily on MLLM consistency, which may not generalize across model versions.
- The hierarchical prompt templates may not capture the full diversity of real-world compositional complexity.

## Confidence

**High Confidence**: CompAlign benchmark construction methodology and its inherent difficulty relative to existing benchmarks; observation that open-source models underperform closed-source models on compositional tasks.

**Medium Confidence**: Effectiveness of CompQuest's fine-grained decomposition and feedback mechanism; alignment framework's improvements demonstrated, but dependent on single MLLM version and binary protocol.

**Low Confidence**: Claim that framework can "improve" compositional accuracy assumes augmented data from SD3.5 and DALL-E 3 is sufficiently high-quality for training.

## Next Checks
1. **MLLM Consistency Validation**: Evaluate inter-annotator agreement and temporal consistency of gpt-4o-mini-2024-07-18 on held-out CompAlign prompts across multiple runs.
2. **Threshold Sensitivity Analysis**: Systematically vary τ from 0.3 to 0.7 and measure impact on alignment performance and training stability.
3. **Augmentation Quality Assessment**: Generate validation set using augmentation pipeline and have human annotators rate compositional accuracy versus original outputs.