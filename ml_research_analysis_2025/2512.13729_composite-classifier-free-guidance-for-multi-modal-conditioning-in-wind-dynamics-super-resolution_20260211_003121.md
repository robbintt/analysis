---
ver: rpa2
title: Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics
  Super-Resolution
arxiv_id: '2512.13729'
source_url: https://arxiv.org/abs/2512.13729
tags:
- wind
- data
- speed
- low-res
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a diffusion model, WindDM, for super-resolving
  wind dynamics data. Wind data is high-dimensional, with over 10 input variables,
  unlike natural images.
---

# Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution

## Quick Facts
- arXiv ID: 2512.13729
- Source URL: https://arxiv.org/abs/2512.13729
- Reference count: 40
- This paper introduces a diffusion model, WindDM, for super-resolving wind dynamics data with Composite Classifier-Free Guidance (CCFG) that achieves state-of-the-art reconstruction quality among deep learning models.

## Executive Summary
This paper introduces WindDM, a diffusion model for super-resolving wind dynamics data from coarse (24km) to fine (3km) resolution. Wind data presents unique challenges compared to natural images, with over 10 input variables versus 3 in typical image models. To address this, the authors propose Composite Classifier-Free Guidance (CCFG), a generalization of CFG for multiple conditioning variables that decomposes the full conditioning likelihood into a product of subset likelihoods. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to 1000× less than classical methods.

## Method Summary
WindDM is a 100M parameter diffusion U-Net trained on 265,390 paired low- and high-resolution wind data samples from seven European domains. The model uses independent dropout on each of the eight input variables during training, exposing it to partial conditioning configurations. At inference, CCFG combines multiple model evaluations using different conditioning subsets, with subset weights optimized via gradient descent on validation data. The approach leverages a composite likelihood factorization that produces smoother guidance signals than standard CFG. WindDM is trained with auxiliary losses (wavelet, divergence, Sobel) and evaluated using metrics including Mean Map RMSE, Timestamp RMSE, and CRPS.

## Key Results
- WindDM with CCFG achieves state-of-the-art reconstruction quality among deep learning models for wind dynamics super-resolution
- The model costs up to 1000× less than classical numerical weather prediction methods
- CCFG outputs are higher-fidelity than those from standard CFG, with the number of subsets serving as a budget parameter for trading compute for quality

## Why This Works (Mechanism)

### Mechanism 1: Composite Likelihood Factorization
Standard CFG estimates p(C|x) as a single ratio p_θ(x|C)/p_θ(x). CCFG instead constructs a composite likelihood p*_θ(C|x) = ∏ᵢ p_θ(Kᵢ|x)^wᵢ where Kᵢ ⊂ C. This is implemented via the modified score function: ε̃*_θ(x|C,t,w) = ε_θ(x|C,t) + Σᵢ wᵢ(ε_θ(x|Kᵢ,t) - ε_θ(x|t)). The sum over partial conditionings replaces the single all-or-nothing guidance term, creating a gradient that better respects the structure of multi-modal inputs. Assumes partial conditionings provide meaningful and complementary guidance signals.

### Mechanism 2: Inference-Time Compute-Quality Tradeoff via Budget Parameter
CCFG introduces a controllable budget parameter (number of subsets m and total weight W) that allows practitioners to trade additional neural function evaluations (NFEs) for improved sample quality, without retraining. During inference, each subset Kᵢ requires a separate model evaluation ε_θ(x|Kᵢ,t). The number of subsets m directly controls NFEs per denoising step: Direct=1, CFG=2, CCFG with m=2=4 NFEs per step. Assumes the relationship between NFEs and quality does not saturate prematurely.

### Mechanism 3: Multi-Modal Conditioning via Independent Dropout
Training with independent dropout on each conditioning variable (rather than all-or-nothing dropout) enables the model to learn robust representations for all partial conditioning configurations required at inference time. During training, each conditioning variable C₁, ..., Cₖ is independently dropped out with probability p=0.1, replaced with zeros. This exposes the model to 2^k possible conditioning states. Assumes the model generalizes to arbitrary conditioning subsets it may not have seen during training.

## Foundational Learning

- **Concept: Classifier-Free Guidance (CFG)**
  - Why needed here: CCFG is a direct generalization of CFG to multiple conditioning variables; understanding the base mechanism (mixing conditioned and unconditional score estimates via w) is prerequisite.
  - Quick check question: Can you write the CFG-modified score function and explain what the guidance weight w controls?

- **Concept: Diffusion Score Functions and ODE Solvers**
  - Why needed here: CCFG operates by modifying the score function ε_θ during the reverse process; understanding how score functions relate to denoising and how ODE samplers (DPM++, DDPM) use them is essential.
  - Quick check question: What is the relationship between the denoising network output and the score function ∇ₓ log p(x|C,t)?

- **Concept: Composite Likelihood Methods**
  - Why needed here: The theoretical motivation for CCFG draws from composite likelihood literature; understanding why factorized likelihoods can be smoother and unbiased helps justify the approach.
  - Quick check question: Why might a composite likelihood p(x|A)p(x|B) be easier to optimize than the joint p(x|A,B)?

## Architecture Onboarding

- **Component map:** Diffusion U-Net (100M params, 4-layer, 2 self-attention layers) -> Conditioning Concatenation -> CCFG Inference Module -> Weight Optimization (Algorithm 1) -> Auxiliary Losses
- **Critical path:** 1) Pre-train diffusion model with independent per-variable dropout (p=0.1) and auxiliary losses. 2) Run Algorithm 1 on held-out domain to select subsets Kᵢ and weights wᵢ given budget m. 3) At inference, for each denoising step, evaluate all ε_θ(x|Kᵢ,t) and combine via CCFG score function. 4) Ensemble multiple samples if further quality boost needed.
- **Design tradeoffs:** Subset count m: Higher m = more NFEs = better quality but slower. Total weight W: Controls overall guidance strength; W=1.5 chosen via hyperparameter search. Exclusion count p: Limits search space to subsets missing ≤ p variables; reduces 2^k complexity to O(k^p). Conditioning encoding: Concatenation outperforms cross-attention/AdaNorm in Table 5. Architecture: U-Net chosen over DiT for 3x faster training despite marginal DiT quality edge.
- **Failure signatures:** High RMSE with CCFG vs. CFG: Check if subsets Kᵢ are poorly chosen (e.g., redundant); verify Algorithm 1 convergence. NaN/artifacts in outputs: Check for NaN in input data; verify dropout is zero-filling, not NaN. Slow inference: Profile NFE count; if m is large and ensemble > 1, NFEs can escalate quickly. Poor generalization to held-out domain: Verify training covered diverse topographies; check if land-use categories differ significantly.
- **First 3 experiments:** 1) Reproduce CFG vs. CCFG comparison on UK domain (Table 3) with basic variables, m=2, W=1.5. Confirm CCFG improves T-RMSE. 2) Sweep budget m ∈ {2, 4, 6, 8} with fixed W=1.5. Plot T-RMSE vs. NFEs (Figure 5 replication). 3) Ablate independent dropout: Train a model with standard all-or-nothing CFG dropout, then attempt CCFG inference. Compare to independently dropped model.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section, several remain unresolved regarding scaling to hundreds of variables, temporal consistency, and optimization without ground-truth data.

## Limitations
- **Composite Likelihood Benefit**: The theoretical claim that composite likelihood factorization produces "smoother surfaces" lacks direct empirical validation in this or related work.
- **Training-Inference Generalization Gap**: The independent dropout strategy assumes the model generalizes to arbitrary conditioning subsets at inference, but with k=10 variables, 2^10=1024 possible subsets exist while training only samples a tiny fraction.
- **Compute-Quality Saturation**: The claim that practitioners can "trade additional neural function evaluations for improved sample quality" assumes monotonic improvement, but no saturation analysis is provided.

## Confidence
- **High Confidence**: Claims about achieving SOTA reconstruction quality among deep learning models are well-supported by quantitative metrics (T-RMSE, CRPS) on held-out domains.
- **Medium Confidence**: Claims about up to 1000× cost reduction versus classical methods are supported but rely on specific classical method assumptions not fully detailed.
- **Low Confidence**: Theoretical claims about composite likelihood factorization producing "smoother surfaces" lack direct empirical validation in this or related work.

## Next Checks
1. **Ablation Study on Dropout Strategy**: Train two models - one with standard all-or-nothing CFG dropout and one with independent dropout (p=0.1). Compare their performance under identical CCFG inference to isolate the contribution of the independent dropout strategy.
2. **Subset Redundancy Analysis**: For the UK domain with 10 variables, enumerate all 2^10-1 possible non-empty subsets. Compute pairwise correlations between subset scores and identify redundant subsets that may cancel each other in CCFG.
3. **Compute-Quality Saturation Curve**: Extend the budget sweep (m ∈ {2,4,6,8}) to m=16 and measure T-RMSE, NFEs, and wall-clock time. Plot the Pareto frontier to identify the inflection point where additional subsets cease providing meaningful quality gains.