---
ver: rpa2
title: Improving LLMs for Machine Translation Using Synthetic Preference Data
arxiv_id: '2508.14951'
source_url: https://arxiv.org/abs/2508.14951
tags:
- translation
- training
- language
- preference
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to improve an open-source LLM for English-to-Slovene
  translation using synthetic preference data. The core idea is to generate translation
  pairs using two independent models, then filter and rank them using heuristics and
  COMET scores to create a high-quality preference dataset.
---

# Improving LLMs for Machine Translation Using Synthetic Preference Data

## Quick Facts
- **arXiv ID:** 2508.14951
- **Source URL:** https://arxiv.org/abs/2508.14951
- **Authors:** Dario Vajda; Domen Vreš; Marko Robnik-Šikonja
- **Reference count:** 34
- **One-line primary result:** GaMS-9B-Instruct LLM improves English-to-Slovene translation COMET score by ~0.04 using synthetic preference data.

## Executive Summary
This paper presents a method to improve an open-source LLM for English-to-Slovene translation using synthetic preference data. The core idea is to generate translation pairs using two independent models, then filter and rank them using heuristics and COMET scores to create a high-quality preference dataset. The GaMS-9B-Instruct model is fine-tuned using Direct Preference Optimization (DPO) on this dataset. Results show significant improvements over the baseline, with COMET score gains of around 0.04 on Wikipedia articles and a reduction in translation errors from 12% to 0.8%. The method is language-agnostic and can be applied to other low-resource languages.

## Method Summary
The authors generate synthetic preference data by translating English text with two independent models (GaMS-9B-Instruct and EuroLLM-9B-Instruct). The resulting translations are filtered using heuristics to remove truncations and wrong-language outputs, then ranked using CometKiwi scores. Pairs are only formed if the score difference exceeds 0.05. The GaMS-9B-Instruct model is fine-tuned using Direct Preference Optimization (DPO) on this dataset, with LoRA enabled for efficient training.

## Key Results
- COMET score improvement of 0.0381-0.0405 on Wikipedia articles
- Reduction in translation errors from 12% to 0.8%
- Method is language-agnostic and can be applied to other low-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Synthetic Preference Generation
Generating translation candidates with two independent models allows for the automated extraction of preference pairs without human annotation, provided one output is objectively superior. The pipeline uses GaMS-9B-Instruct and EuroLLM-9B-Instruct to translate the same English text. If one model produces a clean translation and the other exhibits a distinct failure mode (e.g., language switching), the former becomes the "chosen" response and the latter the "rejected" response for DPO. The core assumption is that the two source models have uncorrelated error distributions, ensuring a sufficient volume of contrasting pairs where one output is clearly superior.

### Mechanism 2: Metric-Guided Quality Ranking
Automated evaluation metrics (COMET) with a margin threshold can proxy human judgment to distinguish subtle quality differences for fine-tuning. Translations that pass heuristic checks are scored using CometKiwi. Pairs are only formed if the score difference exceeds a threshold (0.05), ensuring the preference signal reflects meaningful quality gaps rather than metric noise. The core assumption is that the reference-less COMET metric correlates strongly with human perception of translation quality for Slovene.

### Mechanism 3: DPO for Failure Mode Suppression
Direct Preference Optimization can reliably suppress specific structural failure modes (truncation, wrong language) by explicitly optimizing against them. By explicitly pairing clean translations with synthetically corrupted versions (e.g., adding prefixes) or naturally failed outputs (truncation), DPO maximizes the likelihood of the clean output while minimizing the likelihood of the error, acting as a targeted behavioral correction. The core assumption is that the failure modes are consistent enough to be learned as a distinct "rejected" class by the optimization process.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** This is the core training algorithm replacing Reinforcement Learning from Human Feedback (RLHF). You must understand that it optimizes a policy using preference pairs directly, removing the need for a separate reward model.
  - **Quick check question:** How does the $\beta$ parameter in DPO affect the balance between the reference model and the preference data?

- **Concept: Reference-less Quality Estimation (COMET/CometKiwi)**
  - **Why needed here:** The pipeline relies entirely on this metric to assign "chosen" vs. "rejected" labels. You need to distinguish this from reference-based metrics like BLEU.
  - **Quick check question:** Why is a reference-less metric essential for this specific pipeline where ground-truth translations are not available for Wikipedia articles?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The paper trains a 9B parameter model efficiently. Understanding LoRA is critical to reproducing the hardware requirements and training speed.
  - **Quick check question:** Does LoRA freeze the base model weights and train only low-rank decomposition matrices?

## Architecture Onboarding

- **Component map:** English text -> GaMS-9B-Instruct + EuroLLM-9B-Instruct (dual translations) -> FastText (language ID) + Length heuristics (truncation) + COMET (quality scoring) -> GaMS-9B-Instruct + DPO Loss (LoRA enabled) -> SloBench + Custom Wikipedia benchmark

- **Critical path:** The data generation and filtering stage. The quality of the DPO model is entirely dependent on the reliability of the COMET scores and heuristic filters to correctly identify "chosen" vs. "rejected" pairs.

- **Design tradeoffs:**
  - **Synthetic vs. Human Data:** Chose synthetic to scale to 35k pairs cheaply and quickly; tradeoff is potential label noise from metric errors.
  - **DPO vs. RLHF:** Chose DPO for stability and simplicity; tradeoff is potentially lower ceiling on reward maximization compared to complex RL methods.

- **Failure signatures:**
  - **Reward Hacking:** Model learns to generate outputs that artificially inflate COMET scores (e.g., excessive length, safe phrasing) without improving semantic accuracy.
  - **Language Collapse:** Despite training, model defaults to English if the "wrong language" rejection pairs were insufficient or inconsistent.

- **First 3 experiments:**
  1. **Pipeline Validation:** Run the generation pipeline on a small sample (e.g., 100 sentences) to verify that the heuristic filters correctly identify truncation and language errors.
  2. **Metric Correlation Check:** Manually inspect a sample of translation pairs ranked by COMET to confirm the "chosen" translation is actually better, validating the metric assumption.
  3. **Hyperparameter Sweep:** Replicate the grid search for DPO $\beta$ and learning rate on a small subset of data to observe loss convergence before full training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would Curriculum DPO improve translation quality compared to vanilla DPO by training on heuristically-identified errors before COMET-scored pairs?
- Basis in paper: [explicit] "A potential improvement to consider in the future is Curriculum DPO instead of vanilla DPO... The datasets could be divided into two major groups. The first group would contain the training examples from our heuristic-based analysis... and the second group would have the training examples ranked by COMET score."
- Why unresolved: The authors propose this approach but do not implement or test it; the benefit of ordering training by difficulty remains unverified.
- What evidence would resolve it: A controlled experiment comparing vanilla DPO vs. Curriculum DPO on the same dataset with translation quality metrics.

### Open Question 2
- Question: How well does the synthetic preference data approach generalize to domains beyond Wikipedia-style articles (e.g., conversational, legal, technical texts)?
- Basis in paper: [inferred] The authors note SloBench scores could improve "if our preference dataset included a broader range of data, not just Wikipedia documents" and acknowledge evaluation data has "similar distribution to our training data."
- Why unresolved: Training and evaluation both rely heavily on Wikipedia/CC-News; cross-domain robustness is not systematically tested.
- What evidence would resolve it: Evaluation on out-of-domain benchmarks with diverse text types, comparing performance gaps against baseline models.

### Open Question 3
- Question: Can combining DPO with GRPO yield better translation quality than either method alone?
- Basis in paper: [explicit] "Another possibility would be combining both methods by first focusing on language and truncation errors using DPO and then performing GRPO based on COMET scores."
- Why unresolved: This hybrid approach is proposed but not implemented or evaluated.
- What evidence would resolve it: An ablation study comparing DPO-only, GRPO-only, and sequential DPO+GRPO training pipelines on identical data.

### Open Question 4
- Question: Does the approach scale effectively to larger models (27B parameters) with similar or greater relative improvements?
- Basis in paper: [explicit] "We plan to use the insight gained during this project to fine-tune the 27B parameter model with the same training pipeline. Since the systems are already in place, the remaining challenges are to scale the training process."
- Why unresolved: Only the 9B model was tested; scaling behavior to 27B remains unknown.
- What evidence would resolve it: Training and evaluating a 27B model using the same pipeline, reporting COMET gains and error rate reductions.

## Limitations

- The synthetic preference generation pipeline depends heavily on the assumption that the two source models have uncorrelated error distributions, which is not empirically validated.
- The reliance on CometKiwi as a reference-less metric for Slovene translation quality introduces uncertainty, as the correlation between this metric and human judgment is assumed but not tested.
- The study's results are constrained by the small scale of the final test set (132 sentences), which limits the statistical significance of the error reduction claims.

## Confidence

- **High Confidence:** The improvement in COMET scores (0.0381-0.0405) on Wikipedia articles and the reduction in translation errors (12% to 0.8%) are well-supported by the experimental data. The method's language-agnostic design is also clearly articulated.
- **Medium Confidence:** The claim that the dual-model synthetic generation approach is more scalable than human annotation is reasonable but relies on untested assumptions about model error independence. The assertion that the method can be generalized to other low-resource languages is plausible but not demonstrated.
- **Low Confidence:** The paper does not validate the quality of the synthetic preference pairs through human evaluation or test the robustness of the method to metric noise. The potential for reward hacking or overfitting to synthetic patterns is acknowledged but not quantified.

## Next Checks

1. **Error Distribution Analysis:** Conduct a statistical analysis of the error patterns in the two source models to confirm their independence. If the models share significant overlap in failure modes, the preference signal may be insufficient.
2. **Human Evaluation of Preference Pairs:** Manually review a random sample of 100 synthetic preference pairs to verify that the "chosen" translations are indeed superior according to human judgment. This will validate the reliability of the CometKiwi metric for Slovene.
3. **Generalization Test:** Apply the synthetic preference generation pipeline to a different low-resource language (e.g., Croatian) and evaluate whether the same improvements in COMET scores and error reduction are observed. This will test the method's language-agnostic claims.