---
ver: rpa2
title: 'TOM: An Open-Source Tongue Segmentation Method with Multi-Teacher Distillation
  and Task-Specific Data Augmentation'
arxiv_id: '2508.14932'
source_url: https://arxiv.org/abs/2508.14932
tags:
- tongue
- segmentation
- image
- images
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TOM, a novel tongue segmentation model based
  on multi-teacher knowledge distillation and diffusion-based data augmentation. It
  addresses the challenge of accurately segmenting tongue images, which are affected
  by color similarity to surrounding areas and complex backgrounds.
---

# TOM: An Open-Source Tongue Segmentation Method with Multi-Teacher Distillation and Task-Specific Data Augmentation

## Quick Facts
- arXiv ID: 2508.14932
- Source URL: https://arxiv.org/abs/2508.14932
- Authors: Jiacheng Xie; Ziyang Zhang; Biplab Poudel; Congyu Guo; Yang Yu; Guanghui An; Xiaoting Tang; Lening Zhao; Chunhui Xu; Dong Xu
- Reference count: 13
- Key outcome: TOM improves tongue segmentation mIoU to 95.22% while reducing parameters by 96.6% through multi-teacher distillation and diffusion-based data augmentation

## Executive Summary
TOM addresses the challenge of accurate tongue segmentation in Traditional Chinese Medicine by introducing a novel approach combining multi-teacher knowledge distillation and diffusion-based data augmentation. The method leverages synthetic tongue images generated through diffusion models to enhance generalization while reducing reliance on extensive real-world annotations. TOM achieves state-of-the-art segmentation performance (up to 95.22% mIoU) while dramatically reducing model size, making it suitable for both clinical and resource-constrained deployment. The approach is validated across three real datasets and shows improvements in downstream classification tasks.

## Method Summary
TOM employs a multi-stage approach: first, a high-capacity SAM-based model (TOM_L) is fine-tuned using LoRA layers with prompt guidance from YOLOv9 for precise boundary detection. Simultaneously, UNet and DeepLabV3 teachers are trained independently. Synthetic tongue images are generated using diffusion models (image-to-image, mask-to-image, text-to-image) and filtered by TCM experts. A lightweight TinyViT student model (TOM_S) is then trained via multi-teacher distillation, learning from the three heterogeneous teachers through combined KL divergence, MSE, and BCE losses. This architecture achieves high accuracy while reducing parameters by 96.6%.

## Key Results
- Achieved 95.22% mIoU on tongue segmentation while reducing parameters by 96.6% compared to teacher models
- TOM_L (SAM-based) achieved highest accuracy at 98.78% mIoU with lowest Hausdorff Distance (1.933-2.186)
- Segmented tongue patches improved downstream classification accuracy from 93.17% to 93.88% and enhanced interpretability
- Paired t-test showed statistically significant improvement (p=0.0042) over classic augmentation methods

## Why This Works (Mechanism)

### Mechanism 1: Multi-Teacher Knowledge Distillation
Distilling from three heterogeneous segmentation teachers (SAM-based, UNet, DeepLabV3) into a TinyViT student preserves accuracy while reducing parameters by 96.6%. The student learns from both soft probability distributions (via KL divergence on logits) and hard spatial predictions (via MSE on masks) from each teacher, weighted by confidence scores. This aggregates complementary strengths: SAM contributes boundary precision, UNet contributes local texture consistency, and DeepLabV3 contributes multi-scale context. Core assumption: The teachers' prediction disagreements are informative rather than contradictory; their errors are uncorrelated enough that fusion reduces variance.

### Mechanism 2: Diffusion-Based Data Augmentation for Domain Generalization
Synthetic tongue images generated via diffusion models (image-to-image, mask-to-image, text-to-image) improve segmentation generalization over classic augmentations. Diffusion models learn the manifold of realistic tongue appearances, enabling generation of semantically coherent variations in background, lighting, coating thickness, and texture. Manual screening by TCM experts filters anatomically implausible outputs. Core assumption: Generated images capture sufficient domain variability to reduce overfitting without introducing systematic distribution shift.

### Mechanism 3: Prompt-Guided SAM Adaptation via LoRA
Fine-tuning SAM with LoRA layers and domain-specific prompts (bounding boxes + foreground/background points) achieves high boundary precision on low-contrast tongue boundaries. YOLOv9 generates coarse bounding boxes; sampled center points provide fine-grained spatial priors. LoRA enables efficient adaptation of the frozen ViT-H encoder by training only low-rank decomposition matrices on attention layers. Core assumption: SAM's pre-trained representations are sufficiently general that minimal fine-tuning can specialize for tongue morphology.

## Foundational Learning

- Concept: Knowledge Distillation (logits-based and feature-based)
  - Why needed here: Understanding how soft labels transfer richer information than hard labels; critical for implementing the multi-teacher loss combination.
  - Quick check question: Can you explain why temperature scaling in softmax is used before computing KL divergence?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Required to understand how TOM_L adapts SAM without full fine-tuning; enables parameter-efficient domain transfer.
  - Quick check question: What are the trainable components when LoRA is applied to attention layers in a frozen transformer?

- Concept: Diffusion Model Sampling (image-to-image, in-painting, text-to-image)
  - Why needed here: Essential for understanding the three-path augmentation pipeline and why manual screening is necessary.
  - Quick check question: What is the difference between forward diffusion (adding noise) and reverse diffusion (denoising) in generating synthetic images?

## Architecture Onboarding

- Component map: Prompt Generator (YOLOv9) → SAM-based TOM_L (ViT-H encoder + LoRA + decoder) → Teacher ensemble (UNet-ResNet34, DeepLabV3-ResNet50) → Student TOM_S (TinyViT + lightweight decoder) → Data Pipeline: Real datasets (3 sources) + Synthetic dataset (diffusion-generated, expert-screened) → Combined training set with binary masks

- Critical path:
  1. Train diffusion models and generate synthetic images → Expert screening
  2. Fine-tune TOM_L using LoRA on combined dataset with prompt generation
  3. Train UNet and DeepLabV3 teachers independently
  4. Distill knowledge to TOM_S using multi-teacher loss (KL + MSE + BCE)

- Design tradeoffs:
  - TOM_L (ViT-H, 639M params): Higher accuracy, requires GPU, offline-only → Use for clinical precision tasks
  - TOM_S (TinyViT, 22M params): ~3% lower mIoU but 96.6% smaller → Use for web deployment and resource-constrained environments
  - Manual screening of synthetic data adds labor cost but prevents anatomically invalid images from corrupting training

- Failure signatures:
  - Low mIoU on Dataset 2 (mobile-captured images) suggests lighting/pose generalization issues → Increase synthetic background diversity
  - HD spikes indicate boundary drift → Check YOLOv9 prompt quality or teacher prediction conflicts
  - Synthetic images resembling animal tongues (noted in Discussion) indicate prompt pool contamination → Strengthen expert filtering

- First 3 experiments:
  1. Ablation: Train TOM_S with single-teacher distillation (SAM only, UNet only, DeepLabV3 only) to quantify each teacher's contribution to student performance.
  2. Data augmentation comparison: Train TOM_L on real data only vs. real + classic augmentation vs. real + diffusion augmentation to isolate synthetic data impact.
  3. Prompt sensitivity analysis: Evaluate TOM_L with box-only, point-only, and hybrid prompts to determine minimum prompt requirements for target mIoU threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can conditional diffusion models be utilized to guide synthetic data generation to effectively support downstream tasks like tongue image classification and diagnosis better than current augmentation methods?
- Basis in paper: The authors state in the Discussion: "For synthetic data augmentation, we will explore the use of conditional diffusion models to guide the generation of synthetic data... supporting a variety of downstream tasks, including tongue image classification and tongue diagnosis."
- Why unresolved: The current study utilized standard diffusion models (e.g., Stable Diffusion, DALL-E 3) which suffer from "subtle inconsistencies" and generate artifacts (e.g., animal-like tongues) requiring manual screening.
- What evidence would resolve it: Experiments comparing classification and diagnostic accuracy when training models on datasets augmented by conditional diffusion models versus standard diffusion models, specifically measuring the reduction in manual screening requirements.

### Open Question 2
- Question: How can prediction conflicts among the multiple teacher models (SAM, UNet, DeepLabV3) be resolved to prevent performance saturation and reduced label sensitivity in the student model?
- Basis in paper: The Discussion notes that while multi-teacher distillation is effective, it faces limitations including "prediction conflicts among teachers, and performance saturation due to redundancy—potentially causing overfitting and reduced label sensitivity."
- Why unresolved: The paper acknowledges that the simple aggregation of teachers can lead to conflicting supervisory signals that degrade the student model's sensitivity to specific labels.
- What evidence would resolve it: A study analyzing the correlation between teacher predictions and the introduction of a conflict-resolution mechanism (e.g., dynamic weighting or attention-based fusion) that results in higher student mIoU and Dice scores compared to the baseline fusion method.

### Open Question 3
- Question: Can the computational overhead of the diffusion-based data augmentation pipeline be reduced to facilitate faster training cycles without compromising the generalization ability of the segmentation model?
- Basis in paper: The Discussion highlights that "the iterative denoising process inherent to diffusion models incurs significant computational costs, particularly for high-resolution image generation."
- Why unresolved: While effective, the current method is resource-intensive, potentially limiting its accessibility or scalability for rapid prototyping in resource-constrained environments.
- What evidence would resolve it: A comparative benchmark of generation time and computational resources (FLOPs/GPU hours) against the achieved segmentation performance (mIoU) using optimized or one-step diffusion models.

## Limitations
- Manual screening of synthetic data introduces labor costs and potential subjectivity in filtering anatomically implausible images
- Incomplete methodological detail for reproducible distillation (LoRA rank, loss weights, distillation temperature not specified)
- Claims about systematic error reduction through multi-teacher fusion assume uncorrelated teacher errors, which is not empirically verified

## Confidence
- **High Confidence**: mIoU improvements (95.22% student, 98.78% TOM_L) and parameter reduction (96.6%) are well-supported by experimental results and ablation studies across three datasets
- **Medium Confidence**: Generalization claims rely on diffusion augmentation effectiveness, but corpus evidence for diffusion-based medical image augmentation is limited
- **Low Confidence**: Claims about systematic error reduction through multi-teacher fusion assume uncorrelated teacher errors, but this correlation structure is not empirically verified in the paper

## Next Checks
1. Conduct ablation studies varying teacher combinations (SAM-only, UNet-only, DeepLabV3-only) to quantify each teacher's unique contribution to student performance
2. Compare diffusion-based augmentation against alternative synthetic data generation methods (GANs, CutMix) to isolate the specific benefit of diffusion models
3. Analyze teacher prediction correlation matrices on test images to verify the assumption that teacher errors are uncorrelated, which underpins the multi-teacher distillation effectiveness