---
ver: rpa2
title: 'DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework'
arxiv_id: '2602.00996'
source_url: https://arxiv.org/abs/2602.00996
tags:
- table
- agent
- answer
- reasoning
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeALOG is a decentralized multi-agent reasoning framework for complex
  question answering across tables, text, and images. It replaces centralized planning
  with log-mediated collaboration among specialized agents (Table, Context, Visual,
  Summarizing, Verification) that communicate via a shared natural-language log.
---

# DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework

## Quick Facts
- arXiv ID: 2602.00996
- Source URL: https://arxiv.org/abs/2602.00996
- Reference count: 20
- Primary result: Achieves state-of-the-art or competitive accuracy (e.g., 80% on FinQA) for multi-modal question answering using decentralized log-mediated agent collaboration

## Executive Summary
DeALOG is a decentralized multi-agent reasoning framework for complex question answering across tables, text, and images. It replaces centralized planning with log-mediated collaboration among specialized agents (Table, Context, Visual, Summarizing, Verification) that communicate via a shared natural-language log. This enables distributed error detection and correction without central control. Experiments on six benchmarks (FeTaQA, FinQA, TAT-QA, WikiTableQuestions, MMQA, CRT-QA) show DeALOG achieves state-of-the-art or competitive accuracy under matched model capacity. The shared log, agent specialization, and verification are critical for accuracy and robustness, particularly for long-horizon and noisy reasoning tasks.

## Method Summary
DeALOG uses five specialized agents (Table, Context, Visual, Summarizing, Verification) that communicate through a shared append-only natural-language log. Each agent autonomously decides when to contribute, and the log serves as persistent memory for peer verification and re-engagement. The framework operates in rounds where agents append findings to the log, a SummarizingAgent proposes answers, and a VerificationAgent cross-checks arithmetic, units, and factual support. If verification flags issues, a single re-engagement round allows retrieval agents to target flagged gaps. The system uses zero-shot prompting without task-specific fine-tuning and achieves competitive accuracy across six benchmarks while providing transparent, interpretable reasoning chains.

## Key Results
- Achieves 80% accuracy on FinQA benchmark, demonstrating strong performance on complex financial reasoning
- VerificationAgent contributes ~4 EM points improvement (78.5 vs 74.2) by detecting arithmetic and omission errors at 88% rate
- Maintains competitive accuracy on TAT-QA, WikiTableQuestions, and FeTaQA while providing interpretable reasoning chains
- Shows robustness to noisy inputs and long-horizon reasoning tasks compared to centralized planning approaches

## Why This Works (Mechanism)

### Mechanism 1
A shared natural-language log enables distributed error detection and collaborative reasoning without a central planner, reducing single-point-of-failure error cascades. Five specialized agents read from and append to an append-only log, each autonomously deciding when to contribute while the log persists intermediate state for peer verification and re-engagement. This works under the assumption that agents can reliably identify relevant contributions and interpret prior log entries, with natural-language log providing sufficient coordination structure.

### Mechanism 2
A dedicated VerificationAgent cross-checks arithmetic, units, and factual support, triggering a single re-engagement round when issues are detected. This improves answer faithfulness and reduces certain error types, with verification contributing ~4 EM points improvement. The mechanism assumes VerificationAgent can parse unstructured log entries reliably and that errors are primarily arithmetic/unit/retrieval-based rather than conceptual.

### Mechanism 3
Specialized agents with distinct roles and global log visibility outperform monolithic single-LLM CoT for multi-hop, multi-modal tasks, especially under noise and long reasoning chains. Each agent focuses on one modality/task while maintaining full log visibility, reducing cognitive load and enabling targeted re-engagement. This modularization works under the zero-shot prompting assumption and requires global visibility despite context costs.

## Foundational Learning

- **Multi-hop reasoning**: DeALOG targets compositional questions requiring integration across tables, text, and images across multiple reasoning steps. Quick check: Can you decompose "What is the birth year of the oldest American author older than the author of Eat Pray Love?" into at least three sub-questions with dependencies?

- **Shared memory / blackboard systems**: The log is a modern LLM-adapted blackboard enabling decentralized coordination and provenance tracking. Quick check: How does a shared memory approach differ from a central planner in terms of error propagation and fault isolation?

- **Zero-shot prompting and agent design**: DeALOG relies on zero-/few-shot prompting for each specialized agent without task-specific fine-tuning. Quick check: What are the tradeoffs between zero-shot prompt engineering versus fine-tuning for domain-specific tasks like financial QA?

## Architecture Onboarding

- **Component map**: User question -> Shared Log (append-only, typed entries with metadata) <- TableAgent (tables) / ContextAgent (text) / VisualAgent (images) / SummarizingAgent (answer synthesis) / VerificationAgent (cross-checks) <- Controller (turn-taking, guardrails)

- **Critical path**: User question initializes log → Round-robin specialist agents append findings → SummarizingAgent proposes answer → VerificationAgent verifies (on ANSWER) → If FLAG, one re-engagement round → On OK or no updates, return answer

- **Design tradeoffs**: Decentralization removes central planner error concentration but increases sequential LLM calls (~3.1 per query vs 1 for CoT); global log visibility improves coordination but requires truncation/summarization policies; verification strictness catches arithmetic errors (88%) but misses visual/OCR errors (17.1%)

- **Failure signatures**: Infinite loops (mitigated by max rounds and no-progress detection); log clutter with near-duplicates (deduplication filters); domain-specific underperformance (e.g., TAT-QA financial reasoning); visual/OCR noise misreads not caught by verification

- **First 3 experiments**:
  1. Run DeALOG on FeTaQA and FinQA with VerificationAgent enabled vs disabled; compare EM scores to quantify verification contribution (~4 EM points expected)
  2. Ablate global log visibility (restrict to partial log) and measure EM drop (paper reports 68.3 EM vs higher with full visibility)
  3. Stress-test under corruption (10-30%) on a held-out subset of WikiTableQuestions; compare EM degradation curves vs a planner-based baseline

## Open Questions the Paper Calls Out

- **Visual/OCR error detection**: How can the VerificationAgent's detection capability be improved for visual/OCR errors, which currently stand at only 17.1%? The authors explicitly identify this as future work, noting the current mechanism lacks robust cross-modal grounding to identify perceptual errors in charts or OCR.

- **Parallel agent execution**: To what extent does true parallel agent execution reduce latency without introducing inconsistencies in the shared log? The paper identifies sequential execution latency as a limitation and suggests parallel execution as future work, noting that true concurrency would require thread-safe logging.

- **Adaptive log compression**: What adaptive log compression strategies effectively preserve critical intermediate steps when context windows are exceeded? The authors highlight sensitivity to long or cluttered logs as a limitation and suggest adaptive compression for future research, noting accuracy drops sharply when logs exceed 8 entries.

## Limitations
- Visual/OCR error detection remains weak (17.1% detection rate), limiting robustness for image-heavy datasets like MMQA
- Agent prompts and learned gating policy details are only partially specified, introducing uncertainty in reproduction
- Domain-specific tasks (e.g., TAT-QA financial reasoning) may underperform due to zero-shot prompting constraints

## Confidence
- **High Confidence**: Log-mediated decentralized coordination effectively reduces error cascades and improves robustness for long-horizon reasoning
- **Medium Confidence**: VerificationAgent's ~4 EM point contribution is well-supported on main benchmark but may not generalize uniformly across all error types
- **Low Confidence**: Learned gating policy benefits are briefly mentioned without full experimental validation or feature details

## Next Checks
1. Run DeALOG on FeTaQA and FinQA with VerificationAgent enabled vs disabled; compare EM scores to verify the ~4 EM point improvement
2. Ablate global log visibility (restrict to partial log) and measure EM drop; confirm full visibility is critical
3. Stress-test DeALOG under 10-30% input corruption on WikiTableQuestions; compare EM degradation curves vs planner-based baseline to validate robustness claims