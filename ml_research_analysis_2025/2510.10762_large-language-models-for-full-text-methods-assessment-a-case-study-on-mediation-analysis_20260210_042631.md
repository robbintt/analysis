---
ver: rpa2
title: 'Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation
  Analysis'
arxiv_id: '2510.10762'
source_url: https://arxiv.org/abs/2510.10762
tags:
- methodological
- human
- mediator
- performance
- mediation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs show near-human accuracy on explicit methodological criteria
  but lag significantly on inference-heavy tasks in full-text systematic reviews.
  Performance correlates strongly with human reviewers (accuracy r=0.71, F1 r=0.97),
  excelling at identifying clearly stated features like randomization and covariate
  inclusion.
---

# Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis

## Quick Facts
- arXiv ID: 2510.10762
- Source URL: https://arxiv.org/abs/2510.10762
- Reference count: 0
- LLMs achieve near-human accuracy on explicit methodological criteria but lag on inference-heavy tasks

## Executive Summary
This study evaluates large language models on full-text systematic review tasks, specifically assessing 14 methodological criteria for mediation analysis across 180 articles. LLMs achieve near-human accuracy on explicit criteria like randomization and covariate inclusion but show significant degradation on inference-heavy tasks requiring nuanced interpretation. Performance correlates strongly with human reviewers (accuracy r=0.71, F1 r=0.97), but accuracy drops up to 15% on complex assessments. The findings highlight the need for human oversight on complex tasks and suggest integrating automated extraction with targeted expert review to enhance efficiency and rigor in evidence synthesis.

## Method Summary
The study evaluates GPT-4o, GPT-4o-mini, GPT-o3, and GPT-5 on binary classification of 14 methodological criteria across 180 full-text articles from psychiatry/psychology journals (2013-2018). Two prompting strategies were used: BASIC (binary only) and DETAILED (with examples). Models were benchmarked against expert consensus labels from Stuart et al. [7], with metrics including accuracy, F1, precision, recall, AUC, and PR-AUC. Chain-of-thought reasoning was enabled by default for GPT-o3 and GPT-5. Performance was analyzed across document length, publication year, and methodological complexity.

## Key Results
- LLMs achieve 90%+ accuracy on explicit criteria (randomization, covariate inclusion) but drop to 15-30% F1 on complex tasks (sensitivity analysis, linearity tests)
- Longer documents (>3,000 words) reduce accuracy by up to 15% due to context attention degradation
- Detailed prompting improves F1 by 0.30-0.35 on causal mediation identification but minimal gains on assumption discussion
- Model performance correlates strongly with human reviewers (accuracy r=0.71, F1 r=0.97)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve near-human performance on explicitly stated methodological features but degrade on inference-heavy tasks.
- Mechanism: Pattern matching on surface-level linguistic cues (keywords like "randomized," "longitudinal," "sensitivity") substitutes for genuine reasoning about study design. Models conflate mention of a concept with methodological implementation.
- Core assumption: Explicit statements in text correlate reliably with actual methodological practice.
- Evidence anchors:
  - Errors commonly resulted from superficial linguistic cuesâ€”for instance, models frequently misinterpreted keywords like "longitudinal" or "sensitivity" as automatic evidence of rigorous methodological approaches
  - Language models incorrectly interpret statements such as "both emotion regulation and dissociation were assessed at the same timepoint (T1), and thus the temporal sequencing of mediators remains to be ascertained through a longitudinal design" as explicit confirmation of temporal sequencing
- Break condition: When methodological rigor requires synthesizing information across distant sections or inferring unstated assumptions from study design descriptions.

### Mechanism 2
- Claim: Document length inversely correlates with extraction accuracy due to context attention degradation.
- Mechanism: Longer texts strain context windows and attention mechanisms, causing models to lose track of relevant methodological details dispersed across sections.
- Core assumption: Information relevant to each criterion appears in proximate contexts within the document.
- Evidence anchors:
  - Longer documents yielded lower model accuracy
  - Articles in the top accuracy quintile averaged only 623 words, whereas those in the bottom accuracy quintile averaged significantly longer at 3,089 words (p<0.001)
- Break condition: When key methodological details are concentrated in structured sections rather than dispersed; explicit Methods sections showed no significant effect (p=0.8).

### Mechanism 3
- Claim: Detailed prompting with domain-specific examples improves performance on complex identification tasks.
- Mechanism: Explicit guidelines and illustrative examples anchor model attention to relevant conceptual features rather than surface keywords, reducing false positives from keyword over-matching.
- Core assumption: The criterion can be operationalized in natural language instructions with representative examples.
- Evidence anchors:
  - Detailed prompting notably improves performance on causal mediation identification
  - Detailed prompting significantly improved performance in identifying studies that explicitly used causal mediation methods (GPT-5: F1 +0.30; GPT-o3: F1 +0.35)
- Break condition: When the target concept requires judgment that cannot be captured by procedural instructions (e.g., what constitutes adequate "sensitivity analysis" for causal assumptions).

## Foundational Learning

- Concept: **Mediation analysis assumptions framework** (temporal ordering, confounder control, sequential ignorability)
  - Why needed here: The 14 evaluation criteria derive from formal requirements for valid causal inference in mediation; without this conceptual foundation, you cannot interpret what LLMs are failing to detect or why false positives occur.
  - Quick check question: Can you explain why controlling for post-exposure variables can *introduce* bias rather than reduce it?

- Concept: **Class imbalance in binary classification** (accuracy vs. F1 vs. PR-AUC)
  - Why needed here: Many methodological criteria have sparse positives (e.g., linearity tests, sensitivity analyses); accuracy alone produces deceptively high scores that mask poor detection of rare but critical features.
  - Quick check question: If only 5% of studies perform sensitivity analysis and a model always predicts "No," what accuracy does it achieve? What F1 score?

- Concept: **Chain-of-thought reasoning in LLMs**
  - Why needed here: Advanced reasoning models (GPT-o3, GPT-5) enabled chain-of-thought by default and showed superior performance on intermediate-complexity tasks; understanding when this helps vs. when it fails guides model selection.
  - Quick check question: Does chain-of-thought help more on explicit extraction tasks or on inference-heavy assessments? (Answer per paper: intermediate gains; still lags on complex reasoning.)

## Architecture Onboarding

- Component map: PDF extraction pipeline -> text preprocessing (remove references) -> LLM inference module (basic vs. detailed prompts) -> confidence-scored binary outputs -> evaluation layer (accuracy, F1, AUC, PR-AUC)

- Critical path:
  1. PDF text extraction quality directly bounds all downstream performance
  2. Prompt design (basic vs. detailed) determines whether models attend to conceptual features or surface keywords
  3. Confidence score calibration affects threshold selection for production deployment

- Design tradeoffs:
  - Basic prompts: Faster, cheaper, but ~15% lower F1 on complex criteria
  - Detailed prompts: +0.30-0.35 F1 on specific tasks (causal mediation), but minimal gains on assumption discussion/sensitivity analysis
  - Reasoning models (o3, GPT-5): Higher performance on structured tasks but still over-rely on linguistic cues
  - Context length vs. accuracy: Must truncate or segment long documents, but this risks losing dispersed methodological details

- Failure signatures:
  - False positives triggered by keyword presence in limitations/discussion sections (e.g., "longitudinal" mentioned as recommendation, not implemented design)
  - Conflating demographic adjustments with post-exposure controls
  - Over-crediting routine sensitivity checks as rigorous causal stress-testing
  - Near-zero recall on sparse-positive criteria despite high accuracy

- First 3 experiments:
  1. Replicate the basic vs. detailed prompt comparison on a held-out sample of 20 papers; measure per-criterion F1 deltas to identify which criteria gain most from prompt engineering.
  2. Test retrieval-augmented approach: Extract only Methods/Discussion sections, then compare accuracy against full-text baseline to isolate where context dilution occurs.
  3. Calibrate confidence thresholds: Plot precision-recall curves per criterion using model confidence scores; identify criteria where threshold tuning could reduce false positives without sacrificing recall.

## Open Questions the Paper Calls Out

- Do reasoning-based LLMs inherently outperform standard models on inference-heavy methodological assessments?
  - Basis in paper: The authors state that "future studies could systematically investigate whether reasoning models inherently outperform others on such tasks."
  - Why unresolved: While reasoning models (GPT-o3, GPT-5) outperformed standard models (GPT-4o) in this study, it is unclear if this advantage is robust across diverse causal inference domains.
  - What evidence would resolve it: A comparative benchmark of reasoning vs. non-reasoning architectures across multiple distinct methodological domains (e.g., RCTs, difference-in-differences).

- Can fine-tuning open-weight models match or exceed the performance of proprietary models for evidence synthesis?
  - Basis in paper: The discussion suggests "exploring fine-tuning of open-weight models" to improve rigor in evidence synthesis.
  - Why unresolved: This study evaluated only proprietary state-of-the-art models; the feasibility and accuracy of specialized, open-weight models for full-text extraction remain unknown.
  - What evidence would resolve it: Benchmarking fine-tuned open-source models against the GPT-family using the provided mediation analysis dataset.

- Do LLM performance patterns generalize to methodological reviews of other causal inference techniques?
  - Basis in paper: The authors note in the limitations that the study focused on mediation analysis and "findings might not translate directly to other methodological domains."
  - Why unresolved: Mediation analysis involves specific linguistic markers (e.g., "mediator," "indirect effect") that may trigger distinct error patterns compared to other methods.
  - What evidence would resolve it: Replicating the evaluation pipeline on a corpus of studies using different statistical methods, such as instrumental variable analysis.

## Limitations

- Access to ground truth: The study relies on gold-standard labels from a prior expert consensus (Stuart et al., 2021), creating a reproducibility bottleneck without access to the original 180 annotated articles.
- Generalizability to other domains: The 14 criteria are specific to mediation analysis in psychiatry/psychology and may not transfer to other methodological frameworks or scientific fields.
- Class imbalance effects: High accuracy scores mask poor performance on rare criteria (F1 <0.37 for linearity tests), and the study lacks precision-recall curves for comprehensive assessment.

## Confidence

- High Confidence: Near-human accuracy on explicit methodological criteria (randomization, covariate inclusion) and strong correlation with human reviewers (accuracy r=0.71, F1 r=0.97).
- Medium Confidence: Performance degradation on inference-heavy tasks and the role of superficial linguistic cues.
- Low Confidence: The impact of prompt engineering (detailed vs. basic) and the necessity of human oversight for complex tasks.

## Next Checks

1. Replicate the basic vs. detailed prompt comparison on a held-out sample of 20 papers. Measure per-criterion F1 deltas to identify which criteria gain most from prompt engineering.
2. Test a retrieval-augmented approach: extract only Methods/Discussion sections, then compare accuracy against full-text baseline to isolate where context dilution occurs.
3. Calibrate confidence thresholds: plot precision-recall curves per criterion using model confidence scores; identify criteria where threshold tuning could reduce false positives without sacrificing recall.