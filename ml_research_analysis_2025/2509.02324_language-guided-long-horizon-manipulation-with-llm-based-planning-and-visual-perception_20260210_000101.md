---
ver: rpa2
title: Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual
  Perception
arxiv_id: '2509.02324'
source_url: https://arxiv.org/abs/2509.02324
tags:
- cloth
- manipulation
- language
- task
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of language-guided long-horizon
  manipulation of deformable objects, specifically cloth folding, which involves high
  degrees of freedom, complex dynamics, and the need for accurate vision-language
  grounding. The authors propose a unified framework that integrates an LLM-based
  planner to decompose high-level language instructions into low-level action primitives,
  a VLM-based perception module with bidirectional cross-attention fusion and DoRA
  fine-tuning for language-conditioned fine-grained visual grounding, and a task execution
  module.
---

# Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception

## Quick Facts
- arXiv ID: 2509.02324
- Source URL: https://arxiv.org/abs/2509.02324
- Reference count: 40
- Key outcome: This paper addresses the challenge of language-guided long-horizon manipulation of deformable objects, specifically cloth folding, which involves high degrees of freedom, complex dynamics, and the need for accurate vision-language grounding. The authors propose a unified framework that integrates an LLM-based planner to decompose high-level language instructions into low-level action primitives, a VLM-based perception module with bidirectional cross-attention fusion and DoRA fine-tuning for language-conditioned fine-grained visual grounding, and a task execution module. In simulation, the method outperforms state-of-the-art baselines by 2.23%, 1.87%, and 33.3% on seen instructions, unseen instructions, and unseen tasks, respectively. On a real robot, it robustly executes multi-step folding sequences from language instructions across diverse cloth materials and configurations, demonstrating strong generalization in practical scenarios.

## Executive Summary
This work tackles the challenge of executing long-horizon, language-guided manipulation of deformable objects (cloth folding) through a unified framework combining LLM-based planning and vision-language grounding. The key innovation lies in bridging the semantic-execution gap: GPT-4o decomposes high-level instructions into actionable sub-tasks, while a VLM with bidirectional cross-attention and DoRA fine-tuning generates precise pick-and-place heatmaps from RGB-D images. The system achieves strong performance in both simulation and real-world settings, demonstrating generalization to unseen instructions and tasks.

## Method Summary
The framework consists of three main modules: an LLM-based planner that uses GPT-4o with in-context instruction learning (ICIL) and chain-of-thought (CoT) reasoning to decompose language instructions into pick-and-place sub-tasks; a VLM-based perception module that uses a frozen SigLIP2 encoder with DoRA fine-tuning and bidirectional cross-attention fusion to generate language-conditioned visual heatmaps for grasping and placement; and a task execution module that converts heatmaps to 3D robot coordinates and executes actions with a UR5 arm and Robotiq gripper. The perception module is trained on 15,750 expert demonstrations from the SoftGym simulator, using BCE loss on Gaussian-centered heatmaps for 100 epochs.

## Key Results
- Outperforms state-of-the-art baselines by 2.23%, 1.87%, and 33.3% on seen instructions, unseen instructions, and unseen tasks in simulation, respectively.
- Achieves 68% success rate on real-world unseen tasks, demonstrating strong sim-to-real transfer.
- DoRA fine-tuning with bidirectional cross-attention achieves 86.40% average success rate on visual grounding, outperforming LoRA, IA3, and transformer fusion baselines.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based task decomposition enables generalization to unseen tasks by bridging semantic intent and executable primitives.
- **Mechanism:** GPT-4o, prompted with in-context instruction learning (ICIL) and chain-of-thought (CoT) reasoning, parses high-level commands (e.g., "Fold the T-Shirt in thirds") into structured sub-tasks aligned with pick-and-place primitives. Each sub-task specifies a grasped region and placement target, creating interpretable intermediate goals.
- **Core assumption:** The LLM's symbolic reasoning transfers to deformable-object domains where state spaces are high-dimensional and dynamics are complex; the decomposition preserves task semantics while remaining groundable.
- **Evidence anchors:**
  - [abstract]: "The LLM-based planner decomposes high-level language instructions into low-level action primitives, bridging the semantic-execution gap, aligning perception with action, and enhancing generalization."
  - [section III-B]: "The decomposition process is facilitated through carefully crafted prompts incorporating in-context instruction learning (ICIL) and chain-of-thought (CoT) reasoning."
  - [corpus]: Related work (Gondola, ODYSSEY, RoboDexVLM) supports LLM planning for long-horizon manipulation, though primarily in rigid-object domains.
- **Break condition:** When high-level instructions require spatial reasoning that cannot be expressed as sequential pick-and-place operations (e.g., continuous smoothing, multi-contact manipulation), or when sub-task language cannot be parsed into "grasp X and place to Y" format.

### Mechanism 2
- **Claim:** Bidirectional cross-attention fusion improves visual grounding by enabling mutual conditioning between language and visual modalities.
- **Mechanism:** The module computes both image-to-language attention (visual features query language) and language-to-image attention (language queries visual features), concatenating outputs before projection. Language is split at "and" into pick/place segments, with separate decoder branches for each. This allows pick-relevant visual regions to attend to pick-specific language tokens, and similarly for place.
- **Core assumption:** Fine-grained grounding benefits from bidirectional information flow rather than unidirectional conditioning; the frozen SigLIP2 backbone provides transferable features that can be effectively adapted via attention-layer tuning.
- **Evidence anchors:**
  - [abstract]: "a VLM-based perception module with bidirectional cross-attention fusion mechanism and DoRA fine-tuning for language-conditioned fine-grained visual grounding."
  - [section III-C]: "we propose a bidirectional cross-attention mechanism to fuse information across modalities... The two attention outputs are concatenated and projected back."
  - [Table II]: Cross-attention fusion achieves 86.40% avg SR vs 84.39% for transformer fusion (2.01% improvement).
- **Break condition:** When visual resolution is insufficient to distinguish fine cloth features, or when language segments are ambiguous and don't clearly separate pick/place semantics.

### Mechanism 3
- **Claim:** DoRA (Weight-Decomposed Low-Rank Adaptation) provides more effective domain adaptation for cloth perception than LoRA or IA3.
- **Mechanism:** DoRA decomposes pretrained weights into magnitude and direction components, applying low-rank updates only to the direction while learning magnitude scaling separately. Applied to query and value matrices in SigLIP2 attention layers, this preserves pretrained knowledge while adapting to fabric-specific features.
- **Core assumption:** Cloth-specific visual features require different adaptation dynamics than general vision-language tasks; magnitude and direction should be updated independently for optimal fine-tuning.
- **Evidence anchors:**
  - [abstract]: "Weight-Decomposed Low-Rank Adaptation (DoRA)-based fine-tuning to achieve language-conditioned fine-grained visual grounding."
  - [section III-C]: "To tailor it to cloth manipulation while preserving pretrained knowledge, we apply DoRA to the query and value matrices of the attention layers."
  - [Table III]: DoRA achieves 86.40% avg SR vs 84.97% (LoRA), 82.79% (IA3), 75.38% (no adaptation).
- **Break condition:** When sim-to-real domain gap is too large for low-rank adaptation alone, or when cloth materials exhibit visual properties (transparency, extreme texture) outside the training distribution.

## Foundational Learning

- **Concept: Cross-Attention for Multimodal Fusion**
  - **Why needed here:** The bidirectional cross-attention mechanism is central to visual grounding. Without understanding how query/key/value projections enable modality interaction, the fusion design will be opaque.
  - **Quick check question:** Given visual features F_o ∈ R^(B×P×D) and language features F_ℓ ∈ R^(B×T×D), can you sketch how image-to-language attention would be computed?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** DoRA is the adaptation strategy for the frozen SigLIP2 backbone. Understanding LoRA baseline helps contextualize why DoRA's magnitude-direction decomposition matters.
  - **Quick check question:** Why might updating magnitude and direction separately improve fine-tuning compared to standard low-rank updates?

- **Concept: Language-Grounded Robotic Manipulation**
  - **Why needed here:** This work bridges language instructions to continuous action spaces via heatmaps. Understanding how spatial probability maps translate to robot end-effector poses is essential.
  - **Quick check question:** Given a pick heatmap Q_pick ∈ R^(H×W), how would you convert the predicted pixel location to a 3D robot base coordinate?

## Architecture Onboarding

- **Component map:**
  - Microphone -> ASR (Baidu Qianfan) -> Text instruction
  - GPT-4o with ICIL + CoT prompts -> Sub-task sequence L = (ℓ_1, ..., ℓ_n)
  - RGB-D camera -> Grounding DINO + SAM (segmentation) -> SigLIP2 encoder (frozen + DoRA) -> Bidirectional cross-attention -> Dual CUN decoders -> Pick/place heatmaps
  - Pixel->3D projection -> UR5 + Robotiq gripper -> grasp/move/place primitives

- **Critical path:** Visual grounding accuracy (heatmaps -> pick/place locations) determines execution success. If heatmaps are inaccurate, downstream robot actions fail regardless of planning quality.

- **Design tradeoffs:**
  - **Frozen vs full fine-tuning:** Frozen SigLIP2 + DoRA preserves general features but may limit cloth-specific adaptation; full fine-tuning risks catastrophic forgetting.
  - **LLM choice:** GPT-4o achieves 100% planning SR (Table IV) but requires API dependency; open alternatives (DeepSeek-V3: 92% avg) trade performance for autonomy.
  - **Single vs dual-arm:** Current single-arm design limits folding complexity; dual-arm could enable more sophisticated manipulations but increases coordination complexity.

- **Failure signatures:**
  - **Planning failures:** LLM outputs sub-tasks that don't map to valid pick/place pairs (check Table IV for LLM comparison).
  - **Grounding failures:** Heatmaps concentrate on wrong regions (high MPD, low MIoU in Table I); often due to visual ambiguity or instruction mismatch.
  - **Execution failures:** Grasp misses cloth, place location inaccurate (sim-to-real gap); check real-world SR drops in Table V (88%->68% from seen to unseen tasks).
  - **Segmentation failures:** Grounding DINO + SAM fails to isolate garment; cropping includes background noise.

- **First 3 experiments:**
  1. **Validate visual perception in isolation:** Feed held-out simulation images with known ground-truth pick/place locations to the VLM; verify heatmap accuracy (pixel-level IoU with ground-truth heatmaps) before integrating with robot.
  2. **Ablate bidirectional vs unidirectional attention:** Replace bidirectional cross-attention with image-to-language only; compare SR on unseen instructions to quantify contribution of language-to-image pathway.
  3. **Test planning robustness with adversarial instructions:** Provide instructions with ambiguous spatial references ("fold this side over"); measure planning SR and identify failure modes for prompt engineering iteration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to dual-arm robotic setups to handle coordination tasks that are infeasible for a single arm?
- Basis in paper: [explicit] The conclusion states, "Future work will focus on expanding this approach to dual-arm setups and other cloth categories."
- Why unresolved: The current system is designed for a single 6-DoF UR5 arm, limiting the complexity of folding strategies to sequential pick-and-place actions.
- What evidence would resolve it: Successful implementation and evaluation of the planning and perception modules on a bimanual platform performing tasks requiring simultaneous manipulation.

### Open Question 2
- Question: How robust is the LLM-based planner when handling instructions that require dynamic re-planning due to execution failures?
- Basis in paper: [inferred] Algorithm 1 generates a fixed sequence of sub-tasks ($L$) at the start, but lacks an explicit feedback loop for the LLM to revise the plan if an action fails.
- Why unresolved: Cloth manipulation is prone to physical uncertainties (e.g., slipping); without closed-loop re-planning, the robot may blindly execute the remaining steps on an invalid state.
- What evidence would resolve it: Experiments analyzing success rates when intermediate perturbations are introduced, necessitating on-the-fly plan revision by the LLM.

### Open Question 3
- Question: Does the visual perception module generalize to highly diverse cloth categories with significantly different geometric properties?
- Basis in paper: [explicit] The conclusion explicitly lists "other cloth categories" as a target for future work.
- Why unresolved: While experiments included T-shirts, trousers, and towels, the authors acknowledge the need to verify generalization across a wider variety of garments.
- What evidence would resolve it: Quantitative performance metrics (Success Rate, MIoU) on complex items (e.g., dresses, button-down shirts) not present in the training data.

## Limitations

- The framework's performance on highly deformable cloth states (e.g., crumpled configurations) and tasks requiring continuous contact (e.g., smoothing wrinkles) remains untested, as the current design focuses on discrete pick-and-place operations.
- Real-world generalization to novel fabric materials (e.g., silk, leather) and complex garment types (e.g., dresses, jackets) may be limited by the training distribution in simulation.
- The dependency on GPT-4o introduces potential reliability and deployment constraints, though ablation with DeepSeek-V3 (92% avg SR) suggests reasonable open alternatives exist.

## Confidence

- **High confidence**: Visual grounding module with bidirectional cross-attention and DoRA fine-tuning outperforms baselines on cloth-specific perception tasks (Table II, III).
- **Medium confidence**: LLM-based planning generalizes to unseen instructions and tasks in simulation, but real-world execution introduces additional complexity not fully characterized.
- **Medium confidence**: Sim-to-real transfer works for the tested materials and configurations, but robustness to extreme domain shifts (lighting, clutter) requires further validation.

## Next Checks

1. Evaluate grounding accuracy on cloth configurations with high occlusions and ambiguous visual features (e.g., layered garments, complex folds).
2. Test LLM planning with instructions requiring continuous manipulation beyond pick-and-place (e.g., "smooth the shirt", "tuck in the sleeves").
3. Assess performance across a broader material spectrum including non-rigid, high-sheen, and patterned fabrics not present in training data.