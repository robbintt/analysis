---
ver: rpa2
title: 'When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage
  Fine-Tuning'
arxiv_id: '2509.13079'
source_url: https://arxiv.org/abs/2509.13079
tags:
- data
- reasoning
- reverse
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how bidirectional reasoning data impacts multi-stage
  fine-tuning in large language models. It introduces a reverse reasoning dataset
  derived from existing high-quality examples and shows that fine-tuning on reverse
  data improves reasoning accuracy by 1.6%-6.8% over forward-only training.
---

# When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning

## Quick Facts
- **arXiv ID:** 2509.13079
- **Source URL:** https://arxiv.org/abs/2509.13079
- **Reference count:** 17
- **Primary result:** Fine-tuning exclusively on reverse reasoning data improves reasoning accuracy by 1.6%-6.8% over forward-only training, but mixing forward and reverse examples degrades performance by weakening the model's ability to distinguish reasoning directions.

## Executive Summary
This paper investigates how bidirectional reasoning data impacts multi-stage fine-tuning in large language models. The authors introduce a reverse reasoning dataset derived from existing high-quality examples and demonstrate that fine-tuning on reverse data improves reasoning accuracy by 1.6%-6.8% over forward-only training. However, mixing forward and reverse examples during supervised fine-tuning degrades performance as it weakens the model's ability to distinguish reasoning directions. Direct Preference Optimization can partially restore directional distinction but introduces issues by shifting probability mass toward irrelevant outputs, highlighting the need for robust, direction-aware strategies in reasoning model training.

## Method Summary
The study uses a two-stage pipeline: first applying Supervised Fine-Tuning (SFT) on Qwen2.5-Instruct models with LoRA adapters, followed by Direct Preference Optimization (DPO). The key innovation is creating a reverse reasoning dataset (r1k) by generating inverse questions and solutions from a forward dataset (s1k) using DeepSeek-R1. The models are trained on three variants: forward-only (s1k), reverse-only (r1k), and mixed data. Evaluation uses accuracy metrics on AIME, Math 500, and GPQA benchmarks, along with Average Log Probability (ALP) analysis to measure directional distinction.

## Key Results
- SFT on reverse reasoning data (r1k) yields 1.6%-6.8% accuracy improvement over forward-only (s1k) across all evaluated benchmarks and model scales
- Mixing forward and reverse examples during SFT degrades performance by narrowing the ALP margin between preferred and rejected responses
- DPO can partially restore directional distinction but shifts probability mass toward irrelevant outputs, degrading overall quality
- Reverse-only training outperforms mixed data across 3B, 7B, and 14B model scales

## Why This Works (Mechanism)

### Mechanism 1: Directional Confusion from Gradient Interference
Naively mixing forward and reverse reasoning examples during Supervised Fine-Tuning degrades performance by collapsing the model's ability to distinguish reasoning directionality. When trained on a mixture, gradient updates from forward and reverse examples likely conflict in the model's latent space, settling on a compromise that weakens the distinct logical pathways required for forward vs. backward inference.

### Mechanism 2: Inverse Reasoning as Robust Logical Regularization
Fine-tuning exclusively on reverse reasoning data improves generalization accuracy (1.6%-6.8%) over forward-only data. Inverse problems often require identifying underlying first principles rather than applying rote procedures, enforcing a more robust, bidirectional representation of logical relationships.

### Mechanism 3: DPO Probability Mass Shift to Irrelevant Outputs
Direct Preference Optimization can restore directional distinction but degrades output quality by shifting probability mass toward irrelevant responses. Since preferred and rejected responses share significant semantic overlap, aggressively penalizing the rejected response inadvertently suppresses features shared with the preferred response, forcing the model to shift probability to semantically distinct tokens.

## Foundational Learning
- **Neural Tangent Kernel (NTK) & Gradient Interference**: The paper uses NTK theory to explain why mixed data fails—gradient updates from one example can destructively interfere with the confidence of another. Quick check: If you train on example A, does the loss on example B stay the same?
- **Bidirectional vs. Unidirectional Reasoning**: The core intervention is reversing the (Question, Answer) pair. Understanding that logical entailment works differently in forward vs. reverse directions is essential. Quick check: Is solving "x + 5 = 10" (forward) computationally identical to "Find x such that x + 5 = 10" (reverse)?
- **Average Log Probability (ALP) as a Proxy for Certainty**: The paper uses ALP to quantify "directional distinction." Higher ALP means higher model confidence. Quick check: If ALP(y+) is 2.0 and ALP(y-) is 1.9, is the model confidently distinguishing the correct direction?

## Architecture Onboarding
- **Component map:** s1k dataset (Forward) -> DeepSeek-R1 Generator -> r1k dataset (Reverse) -> Qwen2.5-Instruct with LoRA -> SFT -> Checkpointing -> DPO -> Evaluation
- **Critical path:** The construction of the r1k dataset is most critical. If the "Reverse Question" does not naturally elicit the "Reverse Reasoning," the entire premise of "directional distinction" fails.
- **Design tradeoffs:** Data Consistency vs. Diversity (adding reverse data improves diversity but destroys consistency if mixed naively); SFT vs. DPO (SFT better for reasoning capability, DPO better for alignment but risks irrelevant outputs)
- **Failure signatures:** Narrow Margin (Δ ≈ 0.1) indicates directional confusion; Performance Collapse on Mixed Data (accuracy drops when combining forward and reverse)
- **First 3 experiments:** 1) Replicate the Interference: Train on D_s1k only vs D_mixed and verify mixed model has lower ALP margin; 2) Verify Reverse Advantage: Train on D_r1k and compare accuracy to D_s1k baseline; 3) Stress Test DPO: Apply DPO to mixed model and analyze generated text for "irrelevant outputs"

## Open Questions the Paper Calls Out
- Can alternative alignment strategies beyond the standard SFT+DPO pipeline (e.g., multi-task learning or curriculum learning) resolve conflicting supervision signals observed in mixed-direction data?
- How can the DPO formulation be modified to accommodate cases where forward and reverse reasoning paths provide complementary insights rather than acting as strict negatives?
- Does human validation of the automatically generated reverse reasoning chains significantly reduce conflicting supervision signals?

## Limitations
- The quality and consistency of the r1k dataset construction remain unverified, creating potential confounding factors
- The DPO-induced probability shift to irrelevant outputs is observed but not deeply analyzed or validated with concrete examples
- The assumption that reverse reasoning inherently enforces better logical generalization is asserted but not empirically separated from teacher model quality effects

## Confidence
- **High Confidence:** Performance degradation from mixing data is well-supported; accuracy improvement from reverse-only training is directly demonstrated
- **Medium Confidence:** DPO-induced probability shift is reported but not extensively validated; NTK-based mechanism is plausible but not rigorously proven
- **Low Confidence:** Assumption about reverse reasoning enforcing better generalization is asserted but not empirically verified

## Next Checks
1. Manually inspect a random sample of 50 forward/reverse pairs to verify that reverse questions genuinely require different reasoning paths and that generated reverse answers are logically consistent
2. Replace DPO with PPO or unlikelihood training on the mixed dataset to determine if the "irrelevant output" phenomenon is specific to DPO
3. Generate a control dataset where forward and reverse examples are created independently to determine if performance differences stem from data structure or teacher model quality