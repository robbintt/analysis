---
ver: rpa2
title: Knowledge Distillation for Speech Denoising by Latent Representation Alignment
  with Cosine Distance
arxiv_id: '2505.03442'
source_url: https://arxiv.org/abs/2505.03442
tags:
- speech
- teacher
- student
- denoising
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying speech denoising
  models on low-resource devices by proposing a knowledge distillation method that
  aligns latent representations using cosine distance. Unlike existing approaches
  that rely on averaging or attention-based mechanisms, the method employs a linear
  bottleneck to match feature dimensionality and re-order information without being
  constrained by the teacher's learned distribution.
---

# Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance

## Quick Facts
- arXiv ID: 2505.03442
- Source URL: https://arxiv.org/abs/2505.03442
- Reference count: 38
- Primary result: A knowledge distillation method using cosine distance for latent representation alignment achieves consistent improvements over state-of-the-art baselines in speech denoising tasks with significant dimensionality mismatches.

## Executive Summary
This paper addresses the challenge of deploying speech denoising models on low-resource devices by proposing a knowledge distillation method that aligns latent representations using cosine distance. Unlike existing approaches that rely on averaging or attention-based mechanisms, the method employs a linear bottleneck to match feature dimensionality and re-order information without being constrained by the teacher's learned distribution. The cosine distance metric aligns the angle of latent features rather than their scale, providing more flexibility in the knowledge transfer process. Experiments conducted on the DNS-Challenge dataset using DAE-based teacher and student models demonstrate that the proposed method outperforms state-of-the-art baselines, particularly in challenging scenarios with significant dimensionality mismatches.

## Method Summary
The method employs a pre-trained teacher network to guide a smaller student network in speech denoising tasks. A linear bottleneck composed of 1×1 convolutions projects teacher latent representations to match student dimensionality while enabling learned re-ordering of information across channels. The cosine distance metric is used to align the angular orientation of latent features between teacher and student, rather than their scale or distribution. The training objective combines this knowledge distillation loss with a supervised SI-SNR loss, optimized jointly using Adam. The approach is motivated by the denoising autoencoder framework, where the teacher's latent representations already encode directional information toward the clean speech manifold.

## Key Results
- The proposed method consistently outperforms state-of-the-art baselines across various teacher-student pairs and dimensionality mismatches
- Most notable improvements occur in scenarios with channel and time dimension mismatches (up to 1.04 dB SI-SDR improvement)
- The method shows robustness to λ_kd and λ_out values, with equal weighting (λ_kd = λ_out = 1) performing well across settings
- Performance gains are observed even when the teacher and student have significantly different architectures and feature distributions

## Why This Works (Mechanism)

### Mechanism 1: Scale-Invariant Latent Alignment via Cosine Distance
The cosine distance metric (Eq. 13-14) measures angular similarity between teacher and student latent representations. Unlike Lp-based metrics or KL divergence, it normalizes vectors before comparison, making it invariant to magnitude differences. This allows the student to learn feature orientations that point toward the clean speech manifold while retaining freedom in its learned distribution.

### Mechanism 2: Linear Bottleneck for Dimensionality and Ordering Reconciliation
The bottleneck B_t (Eq. 12) applies sequential 1×1 convolutions that operate as affine transforms. Each CNN can match one dimension (C, H, or W). The linear constraint prevents information distortion while allowing the network to learn which teacher channels map to which student channels—addressing the "information ordering" problem where channel indices don't correspond between architecturally different models.

### Mechanism 3: DAE Framework Manifold Guidance
A DAE learns to project corrupted inputs onto the manifold of clean data. Since the pre-trained teacher M_t has already learned this mapping, its latent representations H_E,t contain directional information toward clean speech. The KD process transfers this directional knowledge without requiring the student to replicate the teacher's exact representation distribution.

## Foundational Learning

- **Concept: Knowledge Distillation (KD)**
  - Why needed: Understanding the teacher-student paradigm is prerequisite to grasping how latent representation alignment differs from output-based distillation
  - Quick check: Can you explain why feature-based KD requires dimensionality matching while response-based KD does not?

- **Concept: Cosine Similarity/Distance**
  - Why needed: The core innovation uses cosine distance; understanding its scale-invariance property is essential
  - Quick check: Given two vectors [3, 4] and [6, 8], what is their cosine similarity? What property does this illustrate?

- **Concept: Denoising Autoencoder (DAE) Architecture**
  - Why needed: Both teacher and student use encoder-decoder structure with skip connections (U-Net variant)
  - Quick check: In a DAE, what does the latent representation encode, and why might different architectures encode it differently?

## Architecture Onboarding

- **Component map:**
  Noisy Input → STFT → Magnitude → Student Encoder (E_s) → H_E,s
                                              ↓
  Teacher (frozen): Input → E_t → H_E,t → Linear Bottleneck B_t → H_B,t
                                              ↓
                              Cosine Distance Loss: cosdist(H_B,t, H_E,s)
                                              +
                              Supervised Loss: SI-SNR(output, clean)
                                              ↓
                              Joint Loss: L_tot = λ_kd·L_kd + λ_out·L_out

- **Critical path:**
  1. Pre-train teacher on DNS-Challenge with SI-SNR loss
  2. Freeze teacher weights
  3. Initialize student and bottleneck randomly
  4. For each training batch: forward pass through both networks, compute L_kd on latent representations, compute L_out on outputs, backprop through student and bottleneck jointly
  5. Early stopping on validation loss

- **Design tradeoffs:**
  - Bottleneck complexity: 1 vs 2 vs 3 CNNs depending on which dimensions mismatch (C only, C+H, or C+H+W)
  - λ values: Paper uses λ_kd = λ_out = 1 for robustness testing; tuning may improve results
  - Student architecture: Smaller kernels (3×3 vs 5×5) and fewer channels reduce MOps but limit capacity

- **Failure signatures:**
  - High variance across repeated runs → indicates sensitivity to initialization
  - ATKL baseline failing completely → dimensionality mismatch exceeds method's capacity
  - Marginal improvement over student-only training → teacher may not provide substantially better knowledge

- **First 3 experiments:**
  1. Reproduce baseline comparison: Implement T1→S1 (C only mismatch) with both ATKL and proposed method; verify ~0.1 dB SI-SDR improvement
  2. Ablate bottleneck: Remove linear bottleneck and pad/interpolate to match dimensions instead; expect performance degradation showing bottleneck's learned ordering contribution
  3. Stress test dimensionality mismatch: Attempt T2→S2 (C, H, W mismatch) with varying student capacities; identify the point where cosine alignment becomes insufficient

## Open Questions the Paper Calls Out

- **Open Question 1:** Does decoupling the training of the linear bottleneck from the student network improve knowledge transfer? The paper notes that concurrent training creates "conflicting objectives," potentially leading to suboptimal information extraction.

- **Open Question 2:** Why are the performance improvements from knowledge distillation marginal compared to the baseline? The paper acknowledges that observed increases are small and states, "The explanation for the observation requires further investigation."

- **Open Question 3:** Do correlated feature dimensions reduce the reliability of the cosine distance metric in this context? The paper acknowledges that cosine similarity may not accurately measure similarity if feature dimensions are correlated, suggesting the need for alternative metrics.

## Limitations
- The method's performance on mismatched time dimensions (T) remains unclear, as experiments only demonstrate results with matching T values
- Simultaneous training of the bottleneck and student creates conflicting objectives that may prevent optimal learning
- When feature dimensions are highly correlated, cosine similarity may not accurately measure feature similarity

## Confidence

- **High confidence:** The technical implementation of cosine distance for latent alignment, the linear bottleneck architecture for dimensionality matching, and the core experimental methodology are well-specified and reproducible
- **Medium confidence:** The claim that cosine distance provides superior flexibility over Lp metrics or KL divergence is supported by experimental results but lacks ablation studies isolating the contribution of scale-invariance
- **Low confidence:** The assertion that the DAE framework provides unique theoretical grounding for this approach is weakly supported, as the paper doesn't demonstrate that alternative architectures would fail under the same conditions

## Next Checks

1. **Ablation on bottleneck design:** Compare learned 1×1 convolutions versus fixed linear transformations (identity, random projection) to quantify the contribution of learned re-ordering
2. **Stress test on T-mismatch:** Implement experiments with teacher and student having different time dimensions to verify method robustness beyond the C+H+W mismatches shown
3. **Baseline sensitivity analysis:** Vary λ_kd and λ_out values systematically to identify optimal weighting and test whether the claimed "robustness" to equal weighting holds across different SNR ranges and network architectures