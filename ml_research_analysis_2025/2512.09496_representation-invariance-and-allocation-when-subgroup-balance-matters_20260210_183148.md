---
ver: rpa2
title: 'Representation Invariance and Allocation: When Subgroup Balance Matters'
arxiv_id: '2512.09496'
source_url: https://arxiv.org/abs/2512.09496
tags:
- subgroup
- allocation
- across
- fine-tuning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how subgroup representation in training
  data affects model performance, challenging the assumption that balanced data always
  leads to better fairness. Through systematic experiments across four datasets and
  vision/language models, the authors find that subgroup performance sensitivity to
  allocation varies dramatically.
---

# Representation Invariance and Allocation: When Subgroup Balance Matters

## Quick Facts
- arXiv ID: 2512.09496
- Source URL: https://arxiv.org/abs/2512.09496
- Reference count: 40
- Primary result: Subgroup performance sensitivity to data allocation is determined by latent representation separation in pre-trained models, not conventional factors like under-representation or class imbalance.

## Executive Summary
This paper challenges the conventional wisdom that balanced data always leads to better fairness in machine learning models. Through systematic experiments across four datasets and vision/language models, the authors find that subgroup performance sensitivity to allocation varies dramatically and is determined by the degree of separation between subgroups in the pre-trained model's latent space. They propose the "latent separation hypothesis": a model's dependence on subgroup representation is determined by the degree of separation between subgroups in the pre-trained model's latent space. The findings suggest that analyzing latent subgroup separation can guide data collection and balancing decisions, with practical applications in foundation model fine-tuning where balancing by imaging-related attributes significantly improved subgroup accuracy by over 0.02.

## Method Summary
The paper uses a systematic experimental framework to test how subgroup allocation in fine-tuning data affects subgroup-specific test performance. Pre-trained models are fine-tuned on datasets with varying subgroup allocation ratios (α ∈ {0, 0.1, ..., 1.0}) while keeping total sample size fixed. The authors extract penultimate-layer embeddings, apply PCA to retain ≥70% variance, and compute class-conditional Total Variation (TV) distance between subgroup distributions. They then fit linear regression to subgroup performance vs. allocation and correlate these slopes with TV distances. The approach is validated across four datasets (MNIST, MIMIC-CXR, HAM10000, Civil_comments) with various subgroup attributes.

## Key Results
- Strong empirical correlation (r=0.60-0.95) between latent TV distance and allocation sensitivity across MIMIC, HAM10000, and Civil_comments datasets
- In MIMIC, balancing by imaging-related attributes (view position, hardware manufacturer) improved subgroup accuracy by over 0.02
- TV regularization during pre-training reduces allocation sensitivity but at a substantial accuracy cost (>0.10)
- Conventional explanations for allocation sensitivity (under-representation, poor baseline performance, class imbalance) fail to predict when balancing helps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Subgroup performance sensitivity to data allocation is determined by the degree of latent representation separation between subgroups in the pre-trained model.
- **Mechanism:** When class-conditional representations P(Z|Y,A=a) are similar across subgroups (low total variation distance), a classifier trained on one subgroup's data generalizes to others because the learned decision boundary applies universally. When representations are separated, subgroup-specific data becomes necessary for good performance on that subgroup.
- **Core assumption:** Fine-tuning datasets differ in subgroup allocation but maintain stable label distributions P(Y) and conditional distributions P(Y|A).
- **Evidence anchors:**
  - [abstract] "a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model"
  - [section 5] Theorem 5.2 shows that if TV(D) ≤ ε for both datasets, then |Acc'(A=a) - Acc''(A=a)| ≤ 4ε + |Acc' - Acc''|
  - [section 6.3] Strong correlations (r=0.60-0.95) between TV distance and allocation sensitivity across MIMIC, HAM10000, and Civil_comments
- **Break condition:** When fine-tuning substantially modifies representations (full network fine-tuning), the correlation persists but assumptions about fixed representations are violated.

### Mechanism 2
- **Claim:** Conventional explanations for allocation sensitivity (under-representation in pre-training, poor baseline performance, class imbalance) fail to predict when balancing helps.
- **Mechanism:** These factors don't capture whether subgroups require different features or decision boundaries. A subgroup can be severely under-represented but still achieve good performance if its representations overlap with the majority group for the task-relevant features.
- **Core assumption:** Task-relevant features may or may not correlate with subgroup attributes.
- **Evidence anchors:**
  - [section 4.3] "we see certain subgroups which are extremely under-represented in the pre-training set (e.g., less than 20% of the pre-training data) which show no reduction in loss as fine-tuning data allocation increases"
  - [section 6.2] In MNIST, both color and digit groups have equal P(Y), equal base accuracy, and equal base allocation, yet only digit groups show allocation sensitivity
  - [corpus] Weak direct support; corpus neighbors focus on shortcut learning and representation disentanglement rather than allocation sensitivity specifically
- **Break condition:** When subgroup labels are confounded with task difficulty (different intrinsic hardness), class imbalance may still have independent effects.

### Mechanism 3
- **Claim:** Explicit regularization to reduce latent separation decreases sensitivity to subgroup allocation.
- **Mechanism:** Adding a differentiable TV surrogate penalty during pre-training encourages the model to learn subgroup-invariant representations, reducing the need for balanced allocation during fine-tuning.
- **Core assumption:** The regularization proxy (Mahalanobis distance between group means) adequately approximates TV distance without requiring density estimation.
- **Evidence anchors:**
  - [section 6.4] TV regularization reduces accuracy slope from 0.016 to -0.007 for MIMIC view position groups
  - [section 6.4] Regularization reduces latent TV distance between frontal/lateral images from ~0.17 to ~0.12
  - [section J] Loss formulation: L = L_task + λ * d_TV(Z|A,Y) with λ=0.01
- **Break condition:** Regularization comes at a cost—overall accuracy decreases by >0.10 in the intervention experiment, suggesting potential accuracy-fairness tradeoffs.

## Foundational Learning

- **Concept: Total Variation (TV) Distance**
  - **Why needed here:** The paper's central theoretical contribution uses TV distance to quantify representation separation and bound subgroup accuracy differences.
  - **Quick check question:** Given two discrete probability distributions p and q over the same bins, can you compute TV(p,q) = (1/2)Σ|p(b) - q(b)|?

- **Concept: Class-Conditional Representations**
  - **Why needed here:** The mechanism depends on P(Z|Y,A) rather than marginal P(Z|A); controlling for the label is essential to isolate subgroup-specific variation.
  - **Quick check question:** Why does conditioning on Y matter when measuring whether subgroup representations are similar?

- **Concept: Last-Layer vs. Full Fine-Tuning**
  - **Why needed here:** The theoretical bound assumes fixed representations (last-layer fine-tuning); the paper tests whether findings extend when representations can change.
  - **Quick check question:** If you freeze the backbone and only train the classifier head, what can and cannot change about the model's representations?

## Architecture Onboarding

- **Component map:**
  - Pre-trained encoder (h_η): Produces penultimate-layer embeddings Z
  - Classification head (g_θ): Linear layer mapping Z to predictions
  - TV computation module: Extracts embeddings, applies PCA, estimates class-conditional histograms, computes TV per dimension
  - Fine-tuning loop: Varies subgroup allocation α ∈ {0, 0.1, ..., 1.0} while holding total budget K constant

- **Critical path:**
  1. Pre-train model on natural dataset proportions
  2. Extract embeddings and compute TV distances for each attribute
  3. Fine-tune last layer across 11 allocation levels per attribute
  4. Fit linear regression to subgroup performance vs. allocation
  5. Correlate slopes with TV distances

- **Design tradeoffs:**
  - Linear regression slopes vs. power-law fits: Authors chose linear for stability; power-law parameters had high variance with small samples
  - PCA dimensionality: Retain components explaining ≥70% variance; trades off noise reduction vs. information loss
  - Last-layer vs. full fine-tuning: Theory requires fixed representations; full fine-tuning shows stronger effects but violates assumptions

- **Failure signatures:**
  - TV estimates become unreliable with small sample sizes (HAM10000 had only 1000 test images, causing TV > 0.25 even for random groups)
  - Upper bound becomes vacuous (>1) when TV is very high
  - Correlation may appear spurious if P(Y|A) differs significantly across subgroups (violates theorem assumption)

- **First 3 experiments:**
  1. **Replicate MNIST sanity check:** Train parity classifier on colored MNIST; verify that digit groups (over/under 5) show high TV and allocation sensitivity while color groups show low TV and insensitivity. This validates the hypothesis in a controlled setting.
  2. **Measure TV distances on your pre-trained model:** For your dataset and attributes of interest, extract penultimate-layer embeddings, apply PCA, and compute class-conditional TV. Prioritize attributes with TV > 0.15 for balancing efforts.
  3. **Sensitivity analysis with two allocation extremes:** Compare subgroup performance when fine-tuning on 100% vs. 0% allocation for high-TV and low-TV attributes. Confirm that only high-TV attributes show meaningful performance gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the latent separation hypothesis generalize to multi-class classification tasks and settings with multi-valued or continuous subgroup attributes?
- **Basis in paper:** [explicit] "future work should explore how our findings apply to more complex settings, including tasks other than binary classification, with multi-valued or even continuous subgroups"
- **Why unresolved:** All experiments used binary classification with binary subgroup attributes; the theory (Theorem 5.2) extends to K discrete groups but remains untested empirically beyond binary partitions.
- **What evidence would resolve it:** Experiments on multi-class datasets with continuous attributes (e.g., age as continuous rather than binary), measuring correlation between latent separation and allocation sensitivity.

### Open Question 2
- **Question:** Can enforcing representation invariance during training be used as an effective bias mitigation strategy, and what are the trade-offs with overall performance?
- **Basis in paper:** [explicit] "Future work should also explore if our findings can be leveraged for bias mitigation (for instance by enforcing representation invariance where appropriate)"
- **Why unresolved:** The TV regularization experiment (§6.4) showed reduced allocation sensitivity but also a substantial accuracy drop (>0.10), leaving open whether better trade-offs exist.
- **What evidence would resolve it:** Development of alternative regularization methods that reduce latent separation for spurious attributes while preserving task-relevant features.

### Open Question 3
- **Question:** How can optimal allocation strategies be determined when balancing across multiple subgroups simultaneously with competing objectives?
- **Basis in paper:** [explicit] "We also do not provide a method for determining the optimal allocation strategy to maximise accuracy across multiple subgroups"
- **Why unresolved:** The paper characterizes when allocation matters but does not prescribe how to allocate; practitioners face competing subgroup priorities.
- **What evidence would resolve it:** A framework or algorithm that takes latent separation measurements for multiple attributes and outputs optimal allocation proportions under constrained budgets.

### Open Question 4
- **Question:** Do the findings transfer to settings where training data is expanded (new data collection) rather than re-allocated from a fixed pool?
- **Basis in paper:** [explicit] Future work should explore "settings where the training data is expanded instead of simply re-allocated"
- **Why unresolved:** All experiments held total fine-tuning budget K constant; practical applications often involve collecting additional data.
- **What evidence would resolve it:** Experiments varying total training set size while measuring subgroup-specific scaling, testing whether latent separation predicts when additional subgroup data provides diminishing returns.

## Limitations
- The theoretical bound requires P(Y) to remain constant across fine-tuning datasets, which may not hold in practice
- TV estimation is sensitive to sample size and binning choices, potentially introducing noise in the correlation analysis
- The regularization experiment shows a >0.10 accuracy drop, suggesting potential accuracy-fairness tradeoffs not fully characterized

## Confidence
- **High confidence:** The empirical correlation between latent TV distance and allocation sensitivity (r=0.60-0.95) across multiple datasets and attributes
- **Medium confidence:** The causal mechanism linking representation separation to allocation needs, as the correlation could be confounded by other factors
- **Medium confidence:** The regularization intervention reduces TV distance and allocation sensitivity, though at substantial accuracy cost

## Next Checks
1. Replicate the MNIST parity classifier experiment to verify that digit groups (high TV) show allocation sensitivity while color groups (low TV) remain insensitive under controlled conditions
2. Compute class-conditional TV distances for your specific pre-trained model and dataset to identify which attributes warrant balancing investments
3. Test the sensitivity of TV estimates to sample size by comparing full vs. PCA-reduced space TV calculations, ensuring ≥70% variance retention