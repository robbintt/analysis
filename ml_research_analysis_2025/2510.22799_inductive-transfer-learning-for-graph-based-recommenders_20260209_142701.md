---
ver: rpa2
title: Inductive Transfer Learning for Graph-Based Recommenders
arxiv_id: '2510.22799'
source_url: https://arxiv.org/abs/2510.22799
tags:
- nbf-rec
- transfer
- recommendation
- learning
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generalizing graph-based
  recommender systems to new users, items, or datasets by proposing NBF-Rec, an inductive
  transfer learning framework. Unlike traditional embedding-based methods that require
  retraining for each domain, NBF-Rec dynamically computes node embeddings at inference
  time using interaction-level message passing.
---

# Inductive Transfer Learning for Graph-Based Recommenders

## Quick Facts
- arXiv ID: 2510.22799
- Source URL: https://arxiv.org/abs/2510.22799
- Reference count: 35
- One-line primary result: NBF-Rec achieves competitive zero-shot transfer performance across seven real-world datasets using dynamic message passing with edge features

## Executive Summary
This paper introduces NBF-Rec, an inductive transfer learning framework for graph-based recommendation systems that addresses the fundamental limitation of traditional embedding-based methods requiring retraining for new users, items, or datasets. The key innovation is dynamic node embedding generation at inference time through interaction-level message passing, combined with edge feature integration that captures rich contextual information like ratings, timestamps, and interaction counts. Experiments across seven datasets spanning movies, music, e-commerce, and location check-ins demonstrate that NBF-Rec achieves competitive performance in zero-shot settings and that fine-tuning on limited target interactions further improves results, often matching or exceeding end-to-end trained baselines.

## Method Summary
NBF-Rec is built on NBFNet and extends it with edge feature integration for inductive transfer learning. The model computes node embeddings dynamically at inference time by performing message passing starting from a query user, where initialization h_v^(0) = 1(u=v) anchors all propagation to the query context. Edge features (ratings, timestamps, counts) are transformed through dataset-specific MLPs into weights for message passing using a DistMult operation. The architecture removes "batch edges" during training to force learning from non-trivial paths rather than direct links, compelling the model to learn general link prediction logic. The approach is evaluated across three settings: end-to-end training on single domains, zero-shot transfer where the model is pretrained on source domains and tested on disjoint target domains, and fine-tuning on limited target interactions.

## Key Results
- NBF-Rec achieves competitive zero-shot transfer performance across seven real-world datasets (ML-1M, LastFM, Amazon Beauty, Gowalla, Epinions, BookX, Yelp18)
- Fine-tuning on limited target interactions (1-10% of data) significantly improves zero-shot performance, often matching or exceeding end-to-end trained baselines
- Edge feature integration provides consistent improvements over structure-only methods, though performance varies by dataset pair
- Transferability is asymmetric: LastFM-to-MovieLens transfer performs better than MovieLens-to-LastFM

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic, query-dependent embedding generation enables generalization to unseen nodes
- **Mechanism:** Instead of learning fixed embedding vectors for specific User/Item IDs, the model learns a message-passing function that computes node representations dynamically by aggregating neighborhood information relative to a specific query user u
- **Core assumption:** The structural role of a node (defined by its connectivity patterns) is more transferable across domains than its specific identity
- **Evidence anchors:** Abstract states "NBF-Rec computes node embeddings dynamically at inference time"; section 3.2 describes query-dependent initialization; related work supports moving away from static ID-based embeddings
- **Break condition:** If target domain's graph density is significantly lower than source, structural signals required for effective message passing may vanish

### Mechanism 2
- **Claim:** Edge feature integration allows the model to learn transferable "interaction semantics" rather than just topology
- **Mechanism:** Edge embedding function g(r) processes raw features (ratings, timestamps, counts) into weights w_q for message passing, allowing the model to distinguish between interaction types when aggregating neighbor signals
- **Core assumption:** Interaction behaviors share statistical similarities across different domains that can be captured by a shared MLP
- **Evidence anchors:** Abstract mentions edge features enable capturing rich contextual information; section 3.2 describes edge embedding transformation; related work focuses on structural transfer
- **Break condition:** If edge features in source domain are semantically distinct from target (e.g., "durations" vs. "binary clicks") without normalization, learned projection g(r) may output nonsensical weights

### Mechanism 3
- **Claim:** Interaction-level path reasoning supports zero-shot transfer by abstracting user-item relations
- **Mechanism:** By removing "easy edges" during training, the model is forced to learn from non-trivial paths rather than memorizing direct links, compelling it to learn general "link prediction logic" based on path patterns
- **Core assumption:** High-order connectivity patterns are a domain-agnostic signal for relevance
- **Evidence anchors:** Abstract states interaction-level message passing supports generalization across datasets; section 3.3 describes removing batch edges; related work emphasizes necessity of moving beyond single-domain interaction modeling
- **Break condition:** If target domain is strictly cold-start with no interaction history to form paths, message passing has no substrate to traverse

## Foundational Learning

- **Concept: Transductive vs. Inductive Learning**
  - **Why needed here:** The paper frames its primary contribution as solving the "transductive limitation" of models like LightGCN
  - **Quick check question:** If I add a brand new user with 3 interactions to the system, does the model need to retrain the entire weights matrix to represent them?

- **Concept: Message Passing (GNNs)**
  - **Why needed here:** This is the core operation of the architecture
  - **Quick check question:** In a 2-layer message passing setup, which nodes influence the final embedding of the target user? (Answer: The target user, their direct neighbors, and the neighbors of those neighbors)

- **Concept: Link Prediction**
  - **Why needed here:** The paper formulates recommendation not as classification or regression, but as "link prediction" on a bipartite graph
  - **Quick check question:** Is the goal to classify the type of item, or to predict the likelihood of a connection (interaction) between u and i?

## Architecture Onboarding

- **Component map:** Bipartite Graph -> Edge Encoder (MLP_proj + MLP_emb) -> Message Passing Loop (T layers) -> Score Function
- **Critical path:** The Edge Encoder -> Message Weight Calculation. The paper highlights that NBF-Rec outperforms the base NBFNet specifically because of edge feature integration
- **Design tradeoffs:** Inference Compute vs. Transferability (higher inference costs O(T|E|) vs. embedding-lookup models), Feature Specificity (dataset-specific projection MLP may need adjustment for different feature schemas)
- **Failure signatures:** Asymmetric Transfer (transfer is not symmetric across domains), Edge Feature Noise (uninformative edge features degrade performance)
- **First 3 experiments:**
  1. End-to-End Baseline: Train and test NBF-Rec on a single domain (e.g., MovieLens) to establish upper bound performance
  2. Ablation on Edge Features: Run zero-shot transfer (e.g., Amazon â†’ Yelp) with edge features disabled to quantify contribution of interaction-level features vs. graph topology
  3. Fine-Tuning Efficiency: Fine-tune pretrained model on small subset (e.g., 1% or 10%) of target dataset to verify lightweight adaptation closes gap to end-to-end training

## Open Questions the Paper Calls Out
- **Question:** How does NBF-Rec scale to industrial-sized graphs containing millions of nodes, given the computational overhead of dynamic message passing during inference?
- **Question:** What specific characteristics of edge features determine successful transfer, given that rich features do not consistently correlate with improved performance?
- **Question:** Can heuristics be developed to predict the optimal source domain for pre-training to avoid the observed asymmetry in transfer performance?

## Limitations
- The paper lacks critical architectural hyperparameters (T, hidden dimensions, learning rate, negative sampling rate), making exact reproduction impossible
- Asymmetric transfer performance suggests the model's generalization is sensitive to source-target domain alignment, particularly edge feature compatibility
- Reliance on rich edge features in source domains may limit practical applicability to datasets with sparse or dissimilar interaction metadata

## Confidence
- **High Confidence:** The core mechanism of dynamic embedding generation through message passing is technically sound and well-supported
- **Medium Confidence:** Edge feature integration provides measurable improvements, though specific contribution varies by dataset
- **Low Confidence:** The claim that NBF-Rec "achieves competitive performance in zero-shot settings" is overstated given performance degradation when edge features are absent or semantically mismatched

## Next Checks
1. **Architectural Sensitivity:** Systematically vary the number of message-passing layers (T) and hidden dimensions to establish sensitivity and optimal configurations
2. **Feature Transferability:** Conduct controlled experiments ablating edge features in both source and target domains to quantify their specific contribution to cross-domain generalization
3. **Dataset Alignment Analysis:** Quantify structural and feature similarity between source-target domain pairs to explain asymmetric transfer performance and establish domain compatibility guidelines