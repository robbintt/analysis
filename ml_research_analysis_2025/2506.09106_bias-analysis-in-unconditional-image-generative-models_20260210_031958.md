---
ver: rpa2
title: Bias Analysis in Unconditional Image Generative Models
arxiv_id: '2506.09106'
source_url: https://arxiv.org/abs/2506.09106
tags:
- prob
- bias
- each
- train
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes bias shifts in unconditional image generative
  models by comparing attribute frequencies between training and generated data. The
  authors define bias shift as the difference in attribute presence probability between
  these distributions and evaluate it using a classifier-based framework on CelebA
  and DeepFashion datasets.
---

# Bias Analysis in Unconditional Image Generative Models

## Quick Facts
- arXiv ID: 2506.09106
- Source URL: https://arxiv.org/abs/2506.09106
- Reference count: 40
- Primary result: Bias shifts in unconditional image generative models are generally small, with spectrum-based attributes showing larger shifts than non-spectrum-based ones, and classifier boundary density strongly predicting shift sensitivity.

## Executive Summary
This study analyzes bias shifts in unconditional image generative models by comparing attribute frequencies between training and generated data. The authors define bias shift as the difference in attribute presence probability between these distributions and evaluate it using a classifier-based framework on CelebA and DeepFashion datasets. Experiments with diffusion models and BigGAN reveal that bias shifts are generally small, with spectrum-based attributes (e.g., smiling, young) showing larger shifts than non-spectrum-based ones (e.g., bangs, eyeglasses). BigGAN and smaller diffusion models exhibit greater bias shifts despite similar image generation quality. Classifier decision boundary density strongly predicts shift sensitivity, with boundaries in low-density regions showing smaller shifts.

## Method Summary
The authors define bias shift as the difference in attribute presence probability between training and generated distributions. They use a classifier-based framework where a ResNeXt-50 or Swin Transformer classifier is fine-tuned on ground truth attributes from the training set. The classifier's pre-sigmoid logits are used to compute attribute probabilities for both validation and generated images. Bias shift is calculated as the absolute difference between these probabilities, averaged across attributes (ABS). The framework measures how unconditional generative models shift attribute distributions compared to the training data.

## Key Results
- Average bias shifts are small (0.1-1%) for most attributes, with spectrum-based attributes showing larger shifts (3-5%) than non-spectrum-based ones (0.7%)
- BigGAN and smaller diffusion models show greater bias shifts than larger diffusion models, despite similar image quality
- Classifier decision boundary density strongly predicts bias shift sensitivity, with high-density boundaries showing larger shifts
- EMD distance correlates with bias shift magnitude, suggesting global distribution shifts influence attribute shifts

## Why This Works (Mechanism)
The framework works by measuring how the generative model's output distribution shifts relative to the training distribution through the lens of an attribute classifier. The classifier's decision boundary density determines how sensitive the measured attribute probability is to small distribution shifts. When the boundary falls in a high-density region, small shifts cause large changes in measured probability; when in low-density regions, the boundary acts as a buffer, minimizing measured shifts. This explains why binary attributes (with boundaries at distribution tails) show smaller shifts than spectrum attributes (with boundaries in high-density regions).

## Foundational Learning

**Concept: Bias Shift vs. Absolute Bias**
- Why needed here: The paper's core definition distinguishes between bias (difference from an ideal reference) and bias shift (change in bias from training to generation). Understanding this is critical to interpret the results as measuring relative change, not inherent model fairness.
- Quick check question: If a training set is 90% male and a model generates 85% male, what is the bias shift for the attribute "male"? (Answer: 5 percentage points, or 0.05.)

**Concept: Classifier as a Measurement Tool**
- Why needed here: The entire evaluation framework relies on an external classifier to assign labels to both real and generated images. Its behavior, especially the shape of its decision boundaries, directly influences measured bias shifts. This is not a neutral observer.
- Quick check question: Why is it crucial to use the same classifier to label both the validation set and the generated set in this framework? (Answer: To control for the classifier's own biases and ensure the comparison is fair, focusing on the shift between distributions.)

**Concept: Logit Distribution Density at the Decision Boundary**
- Why needed here: This is the paper's primary explanatory variable. It explains why some attributes show large shifts (high density) while others are stable (low density). It connects model distribution shifts to measured bias outcomes.
- Quick check question: Draw a bimodal distribution with a decision boundary at x=0. If you shift the entire distribution by +1, what determines the magnitude of the change in the probability of being classified as positive? (Answer: The density (height) of the distribution between x=0 and x=1.)

## Architecture Onboarding

**Component Map:**
Training Data (CelebA/DeepFashion) -> Unconditional Generative Model -> Attribute Classifier -> Evaluation Pipeline (Generate → Label → Compute Bias Shift)

**Critical Path:**
The classifier's logit distribution → Decision boundary density → Sensitivity to distribution shift → Measured Bias Shift. This is the core causal chain. The generative model's distribution shift is the input, but the classifier's structure determines the output (measured shift).

**Design Tradeoffs:**
- Using a single classifier vs. multiple: This paper uses one main classifier (ResNeXt) for consistency. The tradeoff is that findings might be specific to that classifier's learned representations, though they test with a Swin Transformer to mitigate this.
- Binary labels for spectrum attributes: The framework relies on binary ground truth. The paper acknowledges this is a limitation for "spectrum-based" attributes like "smiling," where the binary label is a poor proxy for a continuous reality.
- Sampling 10K images: Ensures reliable statistical estimation of probabilities while keeping computational cost manageable. The paper validates that this sample size is sufficient.

**Failure Signatures:**
- Inconsistent Classifier: Using different classifiers or pre-trained models with unknown biases for labeling training vs. generated sets will conflate classifier bias with model bias shift.
- Overfitting/Memorization: If the generative model memorizes the training set, P_gen ≈ P_train, bias shift would be near zero. The paper checks for this using generalization gap and FLD.
- Large EMD Distribution Shifts: If a model fundamentally changes the data distribution (e.g., always generates smiling faces regardless of input), the low-density boundary protection mechanism fails.

**First 3 Experiments:**
1. Baseline Bias Shift Measurement: Train a standard diffusion model (ADM) on CelebA. Fine-tune a ResNeXt classifier on the same training set. Generate images, label all sets, and compute ABS. Plot ABS over training steps to see if it converges.
2. Attribute Categorization & Analysis: For each attribute, visualize the classifier's pre-sigmoid logit distribution for training and generated data. Categorize attributes as "spectrum-based" or "non-spectrum-based" based on the density at the decision boundary (threshold > 0.01). Compare ABS between the two categories.
3. Model Architecture Comparison: Train models of different capacities (e.g., small vs. large diffusion) and architectures (diffusion vs. BigGAN). Compare their FID/FLD (image quality) against their ABS (bias shift). Confirm that similar image quality does not guarantee similar bias shift.

## Open Questions the Paper Calls Out
None

## Limitations
- The classifier-based bias measurement framework introduces inherent measurement bias dependent on the specific classifier's learned representations and decision boundaries
- Binary labeling of spectrum attributes (e.g., "smiling," "young") is acknowledged as a poor proxy for continuous attributes, potentially inflating measured bias shifts
- The analysis is limited to two datasets (CelebA, DeepFashion) and two model architectures (diffusion, BigGAN), which may not generalize to other domains or model types

## Confidence
- High Confidence: The core finding that classifier decision boundary density predicts bias shift sensitivity is well-supported by the theoretical framework and consistent across experiments
- Medium Confidence: The categorization of attributes into "spectrum" and "non-spectrum" types, while intuitive and supported by density analysis, depends on the chosen threshold and could vary with different estimation methods
- Low Confidence: The claim that "non-spectrum" attributes are "inherently stable" may be overstated, as they can still exhibit shifts in extreme distribution changes (e.g., EMD > 0.02)

## Next Checks
1. Replicate the "spectrum" vs. "non-spectrum" classification using multiple density estimation methods (e.g., histogram vs. KDE with varying bandwidths) to confirm the stability of the categorization and the >0.01 threshold
2. Evaluate bias shifts using multiple independent classifiers (e.g., 3 different ResNeXt models trained from different seeds, plus Swin Transformer) to quantify the variance introduced by classifier choice
3. Intentionally induce a large data distribution shift (e.g., train a classifier on CelebA, then generate images from a model trained on a significantly altered subset) to test if the low-density boundary protection mechanism fails as hypothesized when EMD > 0.02