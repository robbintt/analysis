---
ver: rpa2
title: 'ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning'
arxiv_id: '2509.21070'
source_url: https://arxiv.org/abs/2509.21070
tags:
- problems
- arxiv
- difficult
- reasoning
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ScaleDiff, a pipeline to scale the creation
  of difficult mathematical problems for training advanced reasoning models. The core
  method identifies difficult problems from existing datasets using an adaptive thinking
  model that automatically switches between "Thinking" and "NoThinking" modes, then
  trains a specialized problem generator (DiffGen-8B) on this filtered difficult data
  to produce new difficult problems at scale.
---

# ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning

## Quick Facts
- **arXiv ID:** 2509.21070
- **Source URL:** https://arxiv.org/abs/2509.21070
- **Reference count:** 40
- **Primary result:** 65.9% average accuracy on AIME'24/25, HMMT-Feb'25, BRUMO'25, MATH500 benchmarks, outperforming OpenThinker3

## Executive Summary
ScaleDiff addresses the critical bottleneck in training advanced mathematical reasoning models: the scarcity of difficult problems. The authors propose a pipeline that automatically identifies difficult problems from existing datasets using an adaptive thinking model, then trains a specialized generator (DiffGen-8B) to create new difficult problems at scale. This approach eliminates the need for complex per-instance prompting and high API costs associated with traditional methods. The resulting ScaleDiff-Math dataset yields substantial performance improvements when used to fine-tune existing models.

The key innovation is the two-phase process: first using AdaptThink-7B-delta0.05 to classify problems as difficult (first generated token ≠ `</think/>`) or simple, then training a generator specifically on the difficult subset to produce new challenging problems. This creates a virtuous cycle where difficult problems beget more difficult problems, ultimately producing a dataset that significantly enhances mathematical reasoning capabilities.

## Method Summary
ScaleDiff implements a pipeline that identifies difficult problems from existing datasets using an adaptive thinking model, trains a specialized problem generator on this filtered difficult data, and then uses this generator to create new difficult problems at scale. The core innovation is the use of AdaptThink-7B-delta0.05 to automatically classify problems based on their first generated token—if it's `</think/>`, the problem is simple; otherwise, it's difficult. This filtered difficult data (192K problems) is then used to train DiffGen-8B, which generates new problems that are distilled using Qwen3-8B in "Thinking" mode and filtered to remove easy problems and low-quality outputs. The final ScaleDiff-Math dataset combines original data with the generated difficult problems to create a significantly enhanced training corpus.

## Key Results
- **Performance gain:** 11.3% improvement over original dataset when fine-tuning Qwen2.5-Math-7B-Instruct
- **Benchmark achievement:** 65.9% average accuracy across AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500
- **Cost efficiency:** Achieves strong results using Qwen3-8B (8B) as teacher rather than larger, more expensive models

## Why This Works (Mechanism)
The pipeline works by creating a positive feedback loop: difficult problems generate more difficult problems. The AdaptThink model effectively identifies challenging problems by analyzing the reasoning depth required (measured through token generation patterns). Training DiffGen-8B specifically on these difficult problems biases the generator toward producing similarly challenging content. The solution distillation step ensures that generated problems have verifiable solutions, while the filtration process maintains quality by removing problems that the base model can already solve.

## Foundational Learning
**Mathematical reasoning depth classification**
- Why needed: To identify which problems require advanced reasoning versus simple computation
- Quick check: Verify AdaptThink correctly classifies problems by testing on benchmark datasets with known difficulty ratings

**Problem generation with solution distillation**
- Why needed: To create new challenging problems with verifiable solutions at scale
- Quick check: Generate 100 problems and manually verify solution correctness and difficulty level

**Difficulty-based dataset augmentation**
- Why needed: To overcome the scarcity of difficult problems in existing datasets
- Quick check: Compare performance curves when training on original vs. augmented datasets

## Architecture Onboarding

**Component map:**
AM-Qwen3-Distilled (558K) → AdaptThink-7B → Difficulty filter → DiffGen-8B → Problem generator → Qwen3-8B (Thinking mode) → Solution distillation → Rule + model filtration → ScaleDiff-Math (1.7M)

**Critical path:**
AdaptThink classification → DiffGen training → Solution distillation → Final filtration

**Design tradeoffs:**
- Uses 8B teacher model instead of larger models for cost efficiency
- Single epoch training for DiffGen to prevent overfitting
- Extended context (32K) for final fine-tuning to handle complex problems

**Failure signatures:**
- Low difficulty classification rate (<70%) indicates AdaptThink misclassification
- High filtration rate (>60%) suggests generation parameters too permissive
- Performance plateau indicates model capacity limits rather than data scarcity

**First experiments:**
1. Run AdaptThink on 1000 held-out problems to validate classification accuracy
2. Generate 100 problems with current parameters and manually assess difficulty
3. Train DiffGen-8B for 1 epoch and evaluate perplexity on held-out difficult problems

## Open Questions the Paper Calls Out
**Open Question 1:** Can an automated verification mechanism be integrated into the pipeline to ensure the mathematical correctness and solvability of the generated problems? The paper leaves this as future work, noting that assessing correctness remains highly non-trivial.

**Open Question 2:** What are the saturation limits of the performance scaling when the volume of generated difficult problems is increased significantly beyond the 2x augmentation ratio? The paper observes performance gains remain unsaturated at 2x but doesn't explore higher ratios.

**Open Question 3:** Can the ScaleDiff methodology be effectively generalized to non-mathematical reasoning domains, such as code generation or logical deduction? While claimed to be generalizable, all experiments are strictly confined to mathematical reasoning.

## Limitations
- Relies heavily on the quality and reliability of AdaptThink-7B-delta0.05 for difficulty classification
- The 88% difficult rate for generated problems may indicate overly lenient generation parameters
- Filtration criteria lack precise thresholds, making reproducibility challenging
- Assumes strong capability transfer from Qwen3-8B-Base to Qwen2.5-Math-7B-Instruct

## Confidence
**High confidence:** Basic pipeline architecture is sound and well-documented; reported performance improvements are specific and verifiable
**Medium confidence:** AdaptThink-based difficulty classification effectiveness; 88% difficult generation rate; 43% filtration rate
**Low confidence:** Generalizability to other mathematical domains; robustness of Qwen3-8B "Thinking" mode invocation

## Next Checks
1. **Difficulty classification validation:** Run AdaptThink on held-out validation set to verify classification accuracy and first-token detection reliability
2. **Generation parameter sensitivity:** Systematically vary temperature, top-p, and top-k to determine relationship between sampling aggressiveness and difficulty rate
3. **Model transferability test:** Apply full pipeline to different base model (e.g., Llama-3-8B) to verify approach generalizes beyond Qwen model family