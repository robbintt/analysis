---
ver: rpa2
title: 'Memorization vs. Reasoning: Updating LLMs with New Knowledge'
arxiv_id: '2504.12523'
source_url: https://arxiv.org/abs/2504.12523
tags:
- knowledge
- update
- entity
- training
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark and training method for updating
  large language models (LLMs) with evolving real-world knowledge. The Knowledge Update
  Playground (KUP) framework automatically generates realistic training corpora and
  evaluation sets, moving beyond the limited entity-substitution paradigm used in
  prior work.
---

# Memorization vs. Reasoning: Updating LLMs with New Knowledge

## Quick Facts
- arXiv ID: 2504.12523
- Source URL: https://arxiv.org/abs/2504.12523
- Reference count: 40
- Primary result: Introduces KUP benchmark and MCT training method showing 25.4% improvement on direct probing over CPT baselines, but all methods fail on indirect reasoning (<2% accuracy).

## Executive Summary
This paper introduces KUP (Knowledge Update Playground), a framework for generating realistic training corpora and evaluation sets to update LLMs with evolving real-world knowledge. Unlike prior work that uses entity substitution, KUP tests both memorization of updated facts and reasoning over their implications through direct and indirect probing questions. The authors propose MCT (Memory Conditioned Training), which conditions training tokens on self-generated "memory" tokens to better surface and reason over new knowledge at inference. Experiments on Llama-3.1-8B and Mistral-7B show KUP is highly challenging: the best CPT models achieve <2% on indirect probing, while MCT improves direct probing by up to 25.4% over baselines. However, all methods struggle with indirect reasoning, highlighting KUP as a valuable testbed for future research.

## Method Summary
The Knowledge Update Playground (KUP) framework automatically generates training corpora and evaluation sets for updating LLMs with new knowledge. It creates fictitious but realistic updates for entities (e.g., "X is no longer the capital of Y") and generates evidence corpora supporting both old and new facts. Memory Conditioned Training (MCT) then trains models by prepending self-generated "memory" tokens (Wikipedia-style completions about entities) to training data, with the LM loss computed only on the knowledge update portion. At inference, MCT leverages chain-of-thought prompting that mirrors the training-time memory conditioning.

## Key Results
- MCT improves direct probing (memorization) results by up to 25.4% over CPT baselines
- All methods achieve <2% accuracy on indirect probing tasks requiring reasoning over implications
- Even with oracle passages, RAG models still entail old knowledge 25% of the time on indirect probing
- Standard perplexity on update corpus does not correlate with downstream knowledge update performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prepending self-generated memory tokens to training data improves knowledge update retrieval during inference
- Mechanism: By conditioning training tokens on model-generated "memory" tokens (Wikipedia-style completions about entities), the model learns to associate updated knowledge with its parametric representations, creating stronger retrieval paths between old and new knowledge contexts
- Core assumption: The model's own completions serve as reasonable proxies for pre-training distribution contexts where old knowledge was originally learned
- Evidence anchors:
  - [abstract]: "MCT training significantly outperforms prior continued pre-training (CPT) baselines, improving direct probing (memorization) results by up to 25.4%."
  - [section 3]: "During training, MCT prepends 'memory' tokens, i.e. completions sampled from the model itself and conditioned on a specific entity, to training data about that entity."
  - [corpus]: Weak direct evidence; related work "Train Once, Answer All" explores memorization/reasoning tradeoffs but does not validate MCT specifically
- Break condition: If pre-training data distribution differs radically from model-generated completions, memory tokens may fail to bridge old/new knowledge effectively

### Mechanism 2
- Claim: Excluding memory tokens from the loss function prioritizes learning new knowledge over reinforcing existing knowledge
- Mechanism: By computing loss only on the knowledge update portion (not prepended memory tokens), training signals focus on integrating new facts rather than reinforcing parametric knowledge that may conflict with updates, avoiding contradictory gradient signals within the same training block
- Core assumption: Memory tokens already have high probability under M_T, so excluding them from loss does not hurt convergence
- Evidence anchors:
  - [section 3]: "We also modify the language modeling objective... by excluding the loss of mi tokens from the input sequence."
  - [section 3]: "This loss design is to prioritize the training signals from knowledge update rather than to reinforce already acquired knowledge."
  - [corpus]: No corpus evidence directly addresses this loss masking approach
- Break condition: If memory tokens contain noisy or hallucinated content, excluding them from loss misses correction opportunities

### Mechanism 3
- Claim: Memory-conditioned training creates alignment between training-time memory tokens and inference-time chain-of-thought prompting
- Mechanism: Since MCT trains with prepended context tokens, at inference the model is better prepared to leverage CoT-style reasoning traces that retrieve and reason over newly learned knowledge, as the training procedure has already practiced conditioning on self-generated context
- Core assumption: CoT traces at test time functionally parallel memory tokens during training
- Evidence anchors:
  - [section 2]: "MCT can better leverage chain-of-thought (CoT) at inference, likely because of the parallels between CoT traces at test time and 'memory' tokens during training."
  - [section 5.1]: "The biggest improvement... is observed for our MCT training method" when using CoT at inference
  - [corpus]: "Mirage of Mastery" discusses memorization vs. reasoning but does not validate CoT-training alignment
- Break condition: If inference questions require memory recall qualitatively different from training-time entity-conditioned completions, alignment may not transfer

## Foundational Learning

- **Concept: Continued Pre-Training (CPT)**
  - Why needed here: MCT is a modification of standard CPT; understanding the baseline helps contextualize improvements and failure modes
  - Quick check question: What loss function does standard CPT use, and on which tokens is it computed?

- **Concept: Knowledge Conflict in LLMs**
  - Why needed here: The paper addresses how models handle conflicts between parametric (old) knowledge and new corpus knowledge—a core tension MCT attempts to resolve
  - Quick check question: When an LLM's training corpus contains contradictory facts, which tends to surface at inference and why?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: MCT's inference procedure explicitly uses memory recall which functions as CoT; understanding this is necessary to replicate the method correctly
  - Quick check question: How does prepending reasoning steps before an answer affect model behavior on knowledge-intensive tasks?

## Architecture Onboarding

- **Component map**: Entity selection -> (f_old, f_new) generation -> Evidence corpus (D_evd) + Auxiliary corpus -> Training data D>T -> MCT training (memory generation -> chunking -> prepending -> masked loss) -> KUPEval (direct + indirect probing)

- **Critical path**:
  1. Generate memory tokens for each entity using base model M_T before training begins
  2. During CPT, for each batch about entity e, prepend a random memory chunk m_i
  3. Compute LM loss only on update corpus tokens, not memory tokens
  4. At inference, prompt model to "recall memory" before answering (CoT-style)

- **Design tradeoffs**:
  - Fictitious vs. real-world updates: Fictitious data enables stable benchmark for future models but may miss some real-world complexity
  - Memory token chunking: Smaller chunks provide varied context but may lose coherence
  - CoT at inference: Adds computational cost but significantly improves MCT performance (28.4% → 71.0% on update vs. distractors)

- **Failure signatures**:
  - Direct probing passes but indirect probing fails (<2% accuracy): Model memorized update but cannot reason over implications
  - High preference for old knowledge in MCQ (e.g., 83.6% choosing f_old): Training did not override pre-training associations
  - Perplexity on update corpus drops but accuracy does not improve: Standard LM loss does not correlate with knowledge update success (Section 6 analysis)

- **First 3 experiments**:
  1. Reproduce baseline CPT vs. MCT comparison on Llama-3.1-8B for direct probing MCQs, confirming the 25.4% improvement claim
  2. Ablate memory token chunking strategy: compare single full memory prepending vs. random chunk selection
  3. Test indirect probing with oracle retrieval: determine whether the <2% ceiling is from retrieval failure or reasoning failure (Table 7 shows 69.6% with oracle passages, suggesting reasoning is possible but rare)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What learning methods can enable LLMs to reason over updated knowledge in indirect probing settings, where current CPT approaches achieve <2% accuracy?
- Basis in paper: [explicit] "all methods primarily learn to memorize updates and fail to reason over their implications"; "indirect probing tasks remain particularly challenging for all existing training methods, and we encourage future research to continue to work on this problem."
- Why unresolved: The fundamental gap between memorization (improved by MCT) and reasoning (unimproved) suggests architectural or training objective limitations not addressed by conditioning on memory tokens
- What evidence would resolve it: Novel training methods achieving >20% on indirect probing benchmarks while maintaining direct probing gains

### Open Question 2
- Question: How does model scale affect knowledge update capabilities, particularly the memorization-to-reasoning gap observed in 7B-8B models?
- Basis in paper: [explicit] "Continue pretraining behaviors may differ for larger models with more memorization capacity. We leave this exploration to future work."
- Why unresolved: Experiments were limited to Llama-3.1-8B and Mistral-7B-v0.3; scaling effects on both update memorization and downstream reasoning remain unknown
- What evidence would resolve it: Systematic evaluation of MCT and baselines across model scales (e.g., 1B to 70B parameters) showing scaling curves for direct and indirect probing

### Open Question 3
- Question: Can alternative training objectives beyond auto-regressive loss better align optimization with downstream knowledge update performance?
- Basis in paper: [inferred] "perplexity does not suggest how well M>T memorizes updated knowledge" - minimizing standard loss doesn't correlate with preferring new knowledge over old at test time
- Why unresolved: The disconnect between training perplexity and evaluation metrics suggests the LM objective may be fundamentally misaligned with knowledge update goals
- What evidence would resolve it: Correlation analysis between perplexity and update performance; new objectives that explicitly target knowledge conflict resolution

### Open Question 4
- Question: Why does RAG with oracle passages still fail on indirect probing 25% of the time, and what does this reveal about reasoning over conflicting knowledge?
- Basis in paper: [inferred] "Even with oracle passages in the context, LLMs still entail old knowledge 25% of the times" - the failure persists even with perfect retrieval
- Why unresolved: This suggests the bottleneck may not be knowledge access but rather reasoning about conflicts between parametric and contextual knowledge
- What evidence would resolve it: Error analysis of RAG failures; interventions that reduce oracle RAG error rates on indirect probing

## Limitations

- KUP uses fictitious updates which may not capture all complexities of real-world knowledge conflicts
- The 25.4% MCT improvement is measured only on MCQ accuracy, a narrow metric that doesn't capture reasoning depth
- Section 6's perplexity analysis shows standard LM loss poorly predicts update success, but this observation isn't leveraged to refine MCT or propose alternative training objectives

## Confidence

- **High confidence**: MCT improves direct probing memorization (25.4% gain empirically demonstrated)
- **Medium confidence**: KUP provides a more challenging and realistic knowledge update benchmark than prior entity-substitution methods
- **Low confidence**: MCT's mechanisms (memory conditioning, loss masking) are the primary drivers of improvement; the paper doesn't rigorously ablate these components separately

## Next Checks

1. **Ablation study on MCT components**: Test MCT variants that (a) include memory tokens in loss, (b) use static vs. dynamic memory generation, and (c) vary chunk size—this would isolate whether memory conditioning or loss masking drives the 25.4% gain

2. **Real-world update validation**: Apply MCT to a small set of actual breaking news events (e.g., updated sports records, award winners) and measure memorization/reasoning accuracy—this tests whether fictitious updates generalize to real knowledge conflicts

3. **Perplexity vs. accuracy correlation**: Systematically vary training objectives (standard LM, masked LM, contrastive loss) while keeping update corpus constant, then measure both perplexity and downstream accuracy—this would validate or refute the paper's claim that perplexity poorly predicts update success