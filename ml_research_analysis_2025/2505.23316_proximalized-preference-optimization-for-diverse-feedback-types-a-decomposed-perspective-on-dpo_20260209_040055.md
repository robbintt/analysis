---
ver: rpa2
title: 'Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed
  Perspective on DPO'
arxiv_id: '2505.23316'
source_url: https://arxiv.org/abs/2505.23316
tags:
- feedback
- loss
- responses
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies likelihood underdetermination as a core issue
  in direct preference optimization (DPO), where both preferred and dispreferred response
  likelihoods decrease during alignment, leading to reward-hacking effects. The authors
  propose a decomposed reformulation of DPO that separates an optimizer term (which
  reorganizes pairwise feedback into pointwise signals) and a regularizer term (which
  independently regularizes response likelihoods).
---

# Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO

## Quick Facts
- arXiv ID: 2505.23316
- Source URL: https://arxiv.org/abs/2505.23316
- Reference count: 40
- DPO suffers from likelihood underdetermination, causing both preferred and dispreferred response likelihoods to decrease during training.

## Executive Summary
This paper identifies likelihood underdetermination as a core issue in direct preference optimization (DPO), where both preferred and dispreferred response likelihoods decrease during alignment, leading to reward-hacking effects. The authors propose a decomposed reformulation of DPO that separates an optimizer term (which reorganizes pairwise feedback into pointwise signals) and a regularizer term (which independently regularizes response likelihoods). This reformulation reveals that standard DPO implicitly oversimplifies the regularizer, causing underdetermination; restoring the full regularizer resolves this issue.

Building on these insights, they introduce PRoximalized PReference Optimization (PRO), which approximates the full regularizer efficiently using a "hyper-response" mechanism that aggregates unobserved responses. PRO supports diverse feedback types (pairwise, binary, scalar) while guaranteeing mitigation of likelihood underdetermination. Empirical evaluations show that PRO outperforms or matches DPO, KTO, and NCA across multiple benchmarks (AlpacaEval 2, MT-Bench, ARC, IFEval, TruthfulQA, GPQA). Notably, PRO effectively prevents length exploitation and performance degradation during training, and maintains stable response quality even under extremely imbalanced binary feedback conditions (1:100 desired:undesired ratio).

## Method Summary
The authors reformulate DPO into a decomposed form separating an optimizer term that reorganizes pairwise feedback into pointwise signals and a regularizer term that independently regularizes response likelihoods. They identify that standard DPO implicitly simplifies the regularizer, causing likelihood underdetermination where both preferred and dispreferred response probabilities decrease. PRO addresses this by efficiently approximating the full regularizer using a hyper-response mechanism that aggregates unobserved responses. The method supports diverse feedback types while theoretically guaranteeing mitigation of likelihood underdetermination.

## Key Results
- PRO effectively prevents length exploitation and performance degradation during training
- PRO maintains stable response quality under extreme imbalance (1:100 desired:undesired ratio)
- PRO outperforms or matches DPO, KTO, and NCA across AlpacaEval 2, MT-Bench, ARC, IFEval, TruthfulQA, and GPQA benchmarks

## Why This Works (Mechanism)
PRO works by decomposing DPO into optimizer and regularizer components, revealing that standard DPO's simplified regularizer causes likelihood underdetermination. By restoring the full regularizer through efficient approximation with a hyper-response mechanism, PRO ensures that response likelihoods are properly regularized while still incorporating preference feedback. This prevents the reward-hacking effects where both preferred and dispreferred responses decrease in probability.

## Foundational Learning
- **Likelihood underdetermination**: The phenomenon where both preferred and dispreferred response likelihoods decrease during standard DPO training. Understanding this is crucial for recognizing why reward-hacking occurs and how it degrades alignment quality.
- **Hyper-response mechanism**: A technique that aggregates unobserved responses to efficiently approximate the full regularizer. This enables PRO to maintain theoretical guarantees while being computationally tractable.
- **Decomposed DPO perspective**: Separating DPO into optimizer and regularizer terms provides insights into the algorithm's behavior and reveals implicit simplifications that cause problems.

## Architecture Onboarding
Component map: Preference feedback -> Optimizer term (pointwise signals) + Regularizer term (response likelihood regularization) -> PRO model

Critical path: Preference data → Hyper-response aggregation → Regularized optimization → Aligned model

Design tradeoffs: PRO trades some computational complexity for theoretical guarantees and improved robustness to imbalance, versus standard DPO's simpler but underdetermined formulation.

Failure signatures: If the hyper-response approximation is poor, PRO may not fully mitigate likelihood underdetermination. Extreme imbalance in feedback could also challenge the approximation quality.

First experiments: 1) Verify likelihood underdetermination occurs in standard DPO on a toy dataset, 2) Test PRO's performance on a simple binary feedback task, 3) Compare response likelihood distributions between DPO and PRO during training.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can existing DPO improvement strategies (e.g., SimPO, R-DPO, IPO) be integrated into the PRO framework to further enhance alignment performance?
- Basis in paper: [explicit] The Discussion states: "Prior studies have proposed various improvements to DPO... Since PRO is derived as a reformulation of DPO, it is interesting to explore how these strategies can further improve PRO's performance."
- Why unresolved: PRO provides a decomposed perspective on DPO, but whether improvements designed for standard DPO transfer to this reformulation remains untested.
- What evidence would resolve it: Empirical comparisons of PRO with and without DPO-style enhancements across alignment benchmarks.

### Open Question 2
- Question: How does PRO compare to RLHF when applied in on-policy or online training scenarios?
- Basis in paper: [explicit] Appendix C states: "it is worthwhile to investigate how PRO performs compared to RLHF when applied in the on-policy setting."
- Why unresolved: PRO was evaluated only in offline alignment; its behavior with on-policy sampling—where RLHF traditionally excels—remains unexplored.
- What evidence would resolve it: Head-to-head comparison of PRO and PPO-based RLHF using on-policy reward feedback on reasoning or instruction-following tasks.

### Open Question 3
- Question: Does PRO's mass-covering regularizer better preserve response diversity during reinforcement learning for reasoning tasks?
- Basis in paper: [explicit] Appendix C notes: "Since the regularizer in PRO exhibits a mass-covering behavior, it is likely more effective at preserving diversity during post-training. We consider examining its practical effects as another future research direction."
- Why unresolved: The theoretical property is established, but practical impact on exploration quality and downstream reasoning capability is unknown.
- What evidence would resolve it: Diversity metrics (entropy, n-gram overlap) and task performance comparisons between PRO and PPO/GRPO on reasoning benchmarks.

### Open Question 4
- Question: Can PRO's bounded-gradient regularizer mitigate over-regularization issues reported in PPO/GRPO for reasoning-focused post-training?
- Basis in paper: [inferred] Appendix C discusses how PPO's KL regularizer can be "excessively strong" and notes PRO's gradient uses bounded weights, suggesting it may offer gentler optimization.
- Why unresolved: The hypothesis that PRO provides better regularization balance is suggested but not empirically validated in online RL settings.
- What evidence would resolve it: Training dynamics analysis comparing gradient norms, policy divergence, and final performance between PRO and standard RL methods on mathematical or logical reasoning tasks.

## Limitations
- Theoretical guarantees assume specific conditions that may not hold with real-world preference data
- Ablation studies could be more comprehensive to isolate contributions of optimizer vs. regularizer effects
- Computational overhead relative to standard DPO implementations is not thoroughly characterized

## Confidence
High confidence in:
- The identification of likelihood underdetermination as a real issue in DPO
- The empirical improvements of PRO over standard DPO on tested benchmarks
- The mathematical decomposition framework as a valid analytical tool

Medium confidence in:
- The general applicability of PRO across diverse feedback types
- The robustness claims under extreme imbalance conditions
- The practical significance of the improvements relative to computational costs

Low confidence in:
- Long-term stability of PRO-trained models beyond the evaluated training periods
- Transferability of results to significantly larger model scales
- Generalization to preference datasets with different characteristics than those tested

## Next Checks
1. Conduct ablation studies isolating the effects of the optimizer term versus the regularizer term on final performance, to quantify which component contributes most to the improvements.

2. Measure and compare wall-clock training time and memory overhead of PRO versus standard DPO across different model sizes to validate the "efficient approximation" claim.

3. Test PRO on preference datasets with different characteristics (e.g., different task domains, different quality distributions) to assess robustness beyond the current benchmarks.