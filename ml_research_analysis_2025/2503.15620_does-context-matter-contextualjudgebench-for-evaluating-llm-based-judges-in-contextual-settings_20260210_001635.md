---
ver: rpa2
title: Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in
  Contextual Settings
arxiv_id: '2503.15620'
source_url: https://arxiv.org/abs/2503.15620
tags:
- response
- judge
- context
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ContextualJudgeBench, a benchmark designed
  to evaluate large language model judges in contextual settings where external information
  is used as context to generate outputs. The benchmark addresses the gap in existing
  evaluation frameworks, which focus on non-contextual scenarios like instruction
  following while ignoring contextual settings such as retrieval-augmented generation
  (RAG) and summarization.
---

# Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings

## Quick Facts
- **arXiv ID**: 2503.15620
- **Source URL**: https://arxiv.org/abs/2503.15620
- **Reference count**: 40
- **Primary result**: Introduces ContextualJudgeBench, a benchmark for evaluating LLM judges on contextual settings, revealing that contextual assessment remains challenging with the best model (o1) achieving only 55.3% consistent accuracy.

## Executive Summary
This paper introduces ContextualJudgeBench, a benchmark designed to evaluate large language model judges in contextual settings where external information is used as context to generate outputs. The benchmark addresses a gap in existing evaluation frameworks, which focus on non-contextual scenarios while ignoring contextual settings like retrieval-augmented generation and summarization. The benchmark consists of 2,000 response pairs across eight splits testing refusal, faithfulness, completeness, and conciseness criteria. Evaluation of 11 judge models and 9 general-purpose models reveals that contextual assessment remains an open challenge, with inference-time scaling techniques like self-consistency failing to improve judge performance despite the reasoning-intensive nature of the task.

## Method Summary
The authors created ContextualJudgeBench with 2,000 response pairs across 8 splits testing different evaluation criteria. Data was constructed using human annotations and model-based perturbations. Evaluation uses a consistency setup where each sample is evaluated twice with response order swapped to measure positional bias. Judges are prompted using a conditional evaluation hierarchy prioritizing refusal accuracy and response faithfulness. The primary metric is consistent accuracy (correct in both runs), along with run-specific accuracy, optimistic accuracy, and consistency. The benchmark is available at Salesforce/ContextualJudgeBench.

## Key Results
- The best-performing model (OpenAI's o1) achieves only 55.3% consistent accuracy on contextual evaluation
- Inference-time scaling techniques like self-consistency and LLM-as-jury do not improve judge performance and may degrade it
- Judge accuracy decreases as context and response lengths increase, with compounding effects
- Finetuned judges show strong performance on some splits but notably struggle with refusal detection due to concreteness bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reasoning ability is a primary driver of contextual evaluation performance
- **Mechanism**: Contextual evaluation requires comparing responses against provided context, verifying factual attribution, and navigating a hierarchical decision process. Models with stronger reasoning capabilities can better perform this multi-step verification and comparison task.
- **Evidence**: DeepSeek-R1-Llama-70B (reasoning model) outperforms Llama-3.3-70B-Instruct by 4.4 points, with the only difference being reasoning-specific training.

### Mechanism 2
- **Claim**: A principled conditional evaluation hierarchy improves evaluation consistency and utility
- **Mechanism**: By defining a clear order of operations—prioritizing refusal accuracy and faithfulness over completeness and conciseness—the benchmark reduces ambiguity in pairwise comparisons and reflects practitioner priorities.
- **Evidence**: On average, verified accuracies tend to be 20 absolute percent lower than outcome-based accuracy, revealing that judges are using incorrect reasoning to reach correct outcomes.

### Mechanism 3
- **Claim**: Contextual evaluation difficulty scales with input context and response length
- **Mechanism**: Longer contexts and responses increase the cognitive load on the judge model, requiring it to process more information and compare more content, leading to decreased accuracy and consistency.
- **Evidence**: Judge performance decreases as context length increases and is inversely correlated with response length, with compounding effects when both increase.

## Foundational Learning

- **LLM-as-Judge Paradigm**: Understanding this paradigm is essential as the entire paper uses LLMs as automated evaluators. *Quick check*: What is the primary advantage of using an LLM-as-judge over human evaluation? (Answer: Scalability, speed, and cost).

- **Contextual Generation (RAG, Summarization)**: The benchmark targets these specific use cases where responses must be grounded in provided context. *Quick check*: In a RAG system, what should a model do if the retrieved context doesn't contain the answer? (Answer: Refuse to answer or state that information is insufficient).

- **Positional and Length Bias in LLMs**: The paper explicitly analyzes these biases. *Quick check*: Why does the paper run each evaluation twice with swapped response order? (Answer: To measure and control for positional bias via consistency).

## Architecture Onboarding

- **Component map**: Benchmark Splits (8 total) -> Data Construction Pipeline -> Evaluation Protocol -> Judge Models (11) + General/Reasoning Models (9)

- **Critical path**:
  1. Select judge model and prompt template
  2. For each sample, run evaluation twice with swapped orders
  3. Parse model output to extract judgment
  4. Calculate metrics including verification if used

- **Design tradeoffs**:
  - Pairwise vs. Pointwise: Pairwise better aligns with human preference but requires creating response pairs
  - Consistent Accuracy vs. Run 1/2 Accuracy: Consistent accuracy is stricter but may be overly punitive
  - Finetuned Judges vs. Prompted General Models: Finetuned judges can be smaller/cheaper but may inherit biases

- **Failure signatures**:
  - High Run 1/2 Accuracy, Low Consistent Accuracy: Strong positional bias
  - High Outcome Accuracy, Low Verified Accuracy: Correct judgment for wrong reasons
  - Poor performance on Conciseness: Length bias

- **First 3 experiments**:
  1. Baseline Assessment: Run consistency evaluation on target judge model across all 8 splits
  2. Ablation on Hierarchy: Compare performance using standard prompt vs. criteria-specific prompts
  3. Length and Bias Analysis: Bin results by context length and response length to identify degradation patterns

## Open Questions the Paper Calls Out

1. **Classifier-based Reward Models**: Can classifier-based RMs be effectively developed for criteria-specific evaluation in contextual settings? The paper notes this as a fruitful direction since current RMs cannot easily derive criteria-specific rewards.

2. **Inference-time Scaling Failure**: Why does inference-time scaling, such as self-consistency or juries, fail to improve judge performance in contextual evaluation despite the task being reasoning-intensive? The paper notes judges lack structured agreement.

3. **Judge Fine-tuning Modifications**: How can judge fine-tuning be modified to eliminate degradation in refusal detection? The paper identifies that standard fine-tuning notably hurts performance for identifying accurate refusals.

## Limitations

- The benchmark relies on consistency-based evaluation which may not reflect real-world single-pass usage
- The evaluation hierarchy (refusal → faithfulness → completeness → conciseness) is empirically grounded but not definitively proven as optimal
- The finding that inference-time scaling techniques don't improve performance is surprising and may warrant further investigation

## Confidence

- **Confidence: Medium** - The paper provides a comprehensive benchmark but relies on consistency-based evaluation which may not reflect real-world single-pass usage
- **Confidence: Low** - The benchmark's evaluation hierarchy is empirically grounded but not definitively proven as optimal, and the observed performance degradation with longer contexts may be model-specific

## Next Checks

1. **Replication of Length Effects**: Validate whether the observed inverse correlation between judge accuracy and context/response length holds across different model families and sizes

2. **Hierarchy Ablation Study**: Test judge performance using only the specific criterion for each split versus the full conditional hierarchy to determine if the hierarchy improves or confuses the evaluation process

3. **Inference-Time Scaling Parameter Sweep**: Conduct a systematic investigation of different inference-time scaling parameters to determine if the null finding on o1 and DeepSeek-R1 generalizes across settings