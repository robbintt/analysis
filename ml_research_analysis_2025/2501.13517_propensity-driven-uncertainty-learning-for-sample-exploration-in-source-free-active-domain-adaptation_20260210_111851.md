---
ver: rpa2
title: Propensity-driven Uncertainty Learning for Sample Exploration in Source-Free
  Active Domain Adaptation
arxiv_id: '2501.13517'
source_url: https://arxiv.org/abs/2501.13517
tags:
- domain
- samples
- adaptation
- sample
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ProULearn, a Propensity-driven Uncertainty
  Learning framework for source-free active domain adaptation. The method addresses
  the challenge of adapting pre-trained models to new domains without source data
  while minimizing labeling costs through informative sample selection.
---

# Propensity-driven Uncertainty Learning for Sample Exploration in Source-Free Active Domain Adaptation

## Quick Facts
- arXiv ID: 2501.13517
- Source URL: https://arxiv.org/abs/2501.13517
- Authors: Zicheng Pan; Xiaohan Yu; Weichuan Zhang; Yongsheng Gao
- Reference count: 40
- Primary result: 5% labeling budget achieves 95.6% (Office-31), 79.0% (Office-Home), 94.0% (VisDA-2017), 78.6% (DomainNet-126) accuracy

## Executive Summary
This paper introduces ProULearn, a framework for source-free active domain adaptation that combines homogeneity propensity estimation with correlation-based uncertainty learning. The method addresses the challenge of adapting pre-trained models to new domains without source data while minimizing labeling costs through informative sample selection. ProULearn achieves state-of-the-art performance across four benchmark datasets using only 5% of target data labels, outperforming existing methods by 1.5-1.6% on average.

## Method Summary
ProULearn operates in two phases: pre-adaptation sample selection and target adaptation. The selection phase uses homogeneity propensity estimation (HPE) to identify representative yet uncertain samples by analyzing feature space density through random separation trees. The adaptation phase employs weighted cross-entropy, information maximization, and central correlation losses to refine pseudo-labels and create compact class distributions. Sample selection occurs once before training, eliminating progressive labeling complexity. The framework achieves superior performance across four benchmark datasets using only 5% labeling budget.

## Key Results
- Achieves 95.6% accuracy on Office-31, 79.0% on Office-Home, 94.0% on VisDA-2017, and 78.6% on DomainNet-126 with 5% labeling
- Outperforms state-of-the-art methods by 1.5-1.6% average accuracy across benchmarks
- Sample selection before training eliminates annotation bottleneck while maintaining performance
- HPE-based selection consistently outperforms K-means and random selection by 1-2% accuracy

## Why This Works (Mechanism)

### Mechanism 1: Homogeneity Propensity Estimation for Outlier-Aware Sample Selection
Selecting samples that are both uncertain AND representative yields more informative annotations than uncertainty alone. HPE uses random separation trees where path length inversely correlates with anomaly - samples requiring more splits to isolate are more representative of dense regions. By combining h(x) with neighborhood entropy, the selection score prioritizes samples in high-density regions where the model is uncertain, avoiding noisy outliers.

### Mechanism 2: Pre-Training Sample Selection Eliminates Annotation Bottleneck
One-time sample selection before adaptation is practically superior to progressive labeling. Selection scores are computed once using the pre-trained source model's feature extractor, then fixed. This avoids logistical complexity of requesting annotations during training iterations, making deployment feasible in real-world scenarios.

### Mechanism 3: Central Correlation Loss Creates Compact Class Distributions
Explicitly penalizing distance between sample features and their assigned class centroids improves intra-class compactness and inter-class separability. The correlation index captures distributional similarity better than Euclidean distance, pulling samples toward centroids rather than just encouraging confident predictions.

## Foundational Learning

- **Concept: Isolation Forest / Anomaly Detection**
  - Why needed here: HPE adapts isolation forest principles; understanding that anomalies require fewer splits to isolate is core to why longer paths = more representative samples
  - Quick check question: Given a 2D dataset with a dense cluster and scattered outliers, which samples would have the shortest average path length in a random separation tree?

- **Concept: Correlation vs. Distance Metrics**
  - Why needed here: The paper uses Pearson-like correlation index instead of Euclidean/cosine; understanding why distributional correlation captures different relationships than spatial distance is essential
  - Quick check question: Two feature vectors [1,2,3] and [2,4,6] have zero Euclidean distance after normalization but what correlation value? What does this imply for measuring distributional similarity?

- **Concept: Source-Free Domain Adaptation Constraints**
  - Why needed here: Unlike standard UDA, source data is unavailable during adaptation. This forces reliance on pseudo-labels and self-supervised objectives, creating different failure modes
  - Quick check question: Why can't we use MMD-based domain alignment between source and target in SFADA? What must we use instead?

## Architecture Onboarding

- **Component map:** Source model (encoder + classifier) → HPE module (g trees, depth log₂(n)) → Correlation calculator → Sample selector (top B% by U_i) → Weighted CE + Info Max + Central Correlation losses → SGD updates → Periodic centroid refinement
- **Critical path:** 1) Load source model checkpoint, 2) Forward pass on all target data to extract features, 3) Build HPE trees (g=200), 4) Compute correlation matrix, 5) Select samples → request labels → freeze, 6) Training loop with loss weighting
- **Design tradeoffs:** Higher g (trees) provides more stable HPE scores but slower selection; higher K (neighbors) smooths entropy estimates but may dilute local uncertainty signals; budget B% shows diminishing returns above 10%
- **Failure signatures:** All samples get low h(x) scores indicates feature collapse or extreme domain shift; pseudo-label accuracy doesn't improve suggests centroid drift; L_cc dominates indicates scale imbalance
- **First 3 experiments:** 1) Sanity check on Office-31 A→W with B=5%, g=200, K=8 (expected ~96%), 2) Ablation - HPE vs. K-means on same task (expected 1-2% drop), 3) Budget sensitivity on DomainNet-126 R→C (expected monotonic improvement)

## Open Questions the Paper Calls Out

### Open Question 1
Can the ProULearn sample selection strategy be effectively adapted for dense prediction tasks like semantic segmentation or object detection? The conclusion suggests the principles could benefit active learning strategies in various deep learning tasks, yet experiments are restricted to image classification benchmarks. Defining "informative sample" is more complex in segmentation/detection (pixels, regions, bounding boxes) than classification.

### Open Question 2
Can the optimal number of neighbors (K) be determined adaptively based on local data density rather than through dataset-specific manual tuning? Implementation details note K is "tuned based on dataset characteristics," varying from 8 to 84 across datasets, suggesting lack of universal rule. Fixed or manually tuned K may fail to capture true local structure in datasets with variable class densities.

### Open Question 3
Does reliance on the pre-trained model's initial features for Homogeneity Propensity Estimation limit effectiveness in extreme domain shift scenarios? HPE calculates path lengths using features extracted by the source model; if the source model fails to extract meaningful features from the target domain, the "homogeneity" scores may be derived from noisy or meaningless distributions.

## Limitations
- HPE method's effectiveness depends critically on the pre-trained source model's feature extractor retaining meaningful structure under domain shift
- Correlation-based losses assume pseudo-labels are reasonably accurate early in training; systematic label noise could propagate through centroid updates
- Memory complexity for computing full correlation matrices scales quadratically with target dataset size, creating practical constraints for large-scale deployment

## Confidence

- **High Confidence:** Pre-training sample selection methodology (eliminates annotation bottleneck); experimental methodology and dataset configurations
- **Medium Confidence:** Homogeneity propensity estimation mechanism (LCC-based outlier detection in high-dimensional spaces has known limitations); central correlation loss effectiveness (correlation vs. distance metric tradeoffs)
- **Low Confidence:** Claims about HPE being "novel" given related isolation forest applications; specific numerical improvements over SOTA (1.5-1.6% average gain may depend on implementation details)

## Next Checks
1. **Feature Space Validation:** Visualize selected samples using t-SNE/UMAP to verify HPE selects dense cluster centers rather than outliers
2. **Extreme Domain Gap Test:** Evaluate ProULearn on synthetic→real adaptation tasks to identify break points in pre-selection reliability
3. **Progressive vs. Pre-Labeling Comparison:** Implement a progressive labeling baseline to empirically validate the claimed practical advantages of pre-selection