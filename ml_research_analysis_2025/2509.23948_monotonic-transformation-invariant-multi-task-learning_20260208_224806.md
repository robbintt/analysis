---
ver: rpa2
title: Monotonic Transformation Invariant Multi-task Learning
arxiv_id: '2509.23948'
source_url: https://arxiv.org/abs/2509.23948
tags:
- learning
- task
- dibs-mtl
- multi-task
- monotonic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DiBS-MTL, a multi-task learning method that
  is invariant to monotonic non-affine transformations of task losses. This addresses
  the problem of task domination when losses are arbitrarily scaled.
---

# Monotonic Transformation Invariant Multi-task Learning

## Quick Facts
- arXiv ID: 2509.23948
- Source URL: https://arxiv.org/abs/2509.23948
- Reference count: 40
- The paper introduces DiBS-MTL, a multi-task learning method that is invariant to monotonic non-affine transformations of task losses, addressing task domination when losses are arbitrarily scaled.

## Executive Summary
This paper introduces DiBS-MTL, a multi-task learning method designed to be invariant to monotonic non-affine transformations of task losses. The key insight is that by using normalized gradients in the Direction-based Bargaining Solution (DiBS) framework, the method preserves invariance while remaining computationally efficient. The theoretical analysis proves convergence to Pareto stationary points in nonconvex settings, and experiments demonstrate superior performance on transformed reward RL tasks while maintaining competitive results on standard benchmarks.

## Method Summary
DiBS-MTL adapts the Direction-based Bargaining Solution to multi-task learning by computing normalized task gradients and using them to determine update directions. The 1-step variant simply sums normalized gradients for the update direction, while the T-step variant performs bargaining updates within an ε-radius ball around current parameters. The method treats MTL as a cooperative bargaining game where each task is an agent with a preferred state, and the update direction pulls parameters toward a compromise that balances all tasks. This approach avoids the task domination problem that occurs when losses are arbitrarily scaled.

## Key Results
- DiBS-MTL converges to Pareto stationary points in nonconvex settings under standard assumptions
- On transformed reward RL tasks (Meta-World MT10), DiBS-MTL outperforms baselines like Nash-MTL and FAMO while remaining stable
- On standard benchmarks (NYU-v2, QM9), DiBS-MTL achieves competitive performance with state-of-the-art methods
- The method demonstrates strong invariance to monotonic non-affine transformations, unlike previous approaches

## Why This Works (Mechanism)

### Mechanism 1
Using normalized task gradients produces update directions invariant to monotonic non-affine transformations of task losses. Normalization removes magnitude information, leaving only directional information that remains unchanged under transformations like $h(\ell) = \text{sign}(\ell) \cdot \ell^4$. Core assumption: Task losses are differentiable with non-zero gradients. Evidence anchors: [abstract], [Section 2.2, Remark 1]. Break condition: Near-zero gradients cause numerical instability.

### Mechanism 2
Treating MTL as a cooperative bargaining game with tasks as agents yields balanced Pareto stationary solutions. Each task has a preferred state $\theta^*_i$ representing its local minimum, and the DiBS update direction is a distance-weighted combination: $d = \sum_i \|\theta - \theta^*_i\|_2 \cdot \bar{g}_i$. Core assumption: Pareto stationary points lie in the interior of the feasible set (Assumption 1); iterates remain bounded (Assumption 2). Evidence anchors: [Section 3, Theorem 1], [Section 2.2, Definition 1]. Break condition: Aligned task gradients provide no meaningful balance.

### Mechanism 3
First-order local approximation with ball constraint enables efficient single-step updates without sacrificing invariance properties. The bargaining game is constrained to an $\epsilon$-ball, and linear objectives yield closed-form individual optima. The 1-step variant simplifies to $\Delta\theta = -\epsilon \sum_i \bar{g}_i$, absorbing $\epsilon$ into learning rate. Core assumption: First-order approximation remains valid within the $\epsilon$-ball. Evidence anchors: [Section 4], [Section 5.1, 5.3]. Break condition: High curvature within the $\epsilon$-ball degrades approximation.

## Foundational Learning

- **Pareto Optimality and Stationarity**: Understanding the distinction between Pareto optimality (no feasible improvement for all objectives) and Pareto stationarity (first-order necessary condition) is essential for grasping the theoretical guarantees. Quick check: Given two loss functions $\ell_1(\theta), \ell_2(\theta)$, can you identify whether a point $\theta^\dagger$ is Pareto stationary by examining $\nabla\ell_1(\theta^\dagger)$ and $\nabla\ell_2(\theta^\dagger)$?

- **Cooperative Bargaining Theory**: Understanding Nash bargaining vs. DiBS clarifies why prior methods (Nash-MTL) fail under non-affine transformations. Quick check: If agent utilities are transformed by $h(u) = u^2$, does the Nash bargaining solution change? Does DiBS change?

- **Robbins-Monro Stochastic Approximation**: The convergence proof requires step sizes satisfying $\sum_k \alpha_k = \infty$ and $\sum_k \alpha_k^2 < \infty$. Quick check: Why do these conditions ensure convergence in stochastic optimization?

## Architecture Onboarding

- **Component map**: Gradient Normalization Module -> Bargaining Update Module -> Parameter Update

- **Critical path**: Forward pass → compute task losses $\ell_1, \ldots, \ell_N$ → Backward pass → compute raw gradients $\nabla_\theta \ell_i$ → Normalize each gradient → Sum normalized gradients (1-step) or iterate T bargaining steps (T-step) → Update $\theta \leftarrow \theta - \eta \cdot d$

- **Design tradeoffs**:
  - 1-step vs. T-step: 1-step is faster ($\mathcal{O}(1)$ per epoch) but may be less precise; T-step adds $\mathcal{O}(T)$ overhead
  - Gradient normalization vs. raw gradients: Normalization provides invariance but discards magnitude information
  - Computational cost: DiBS-MTL avoids solving convex optimization subproblems (unlike Nash-MTL), reducing runtime

- **Failure signatures**:
  - Near-zero gradients: Normalization becomes unstable; add small epsilon to denominator
  - No gradient conflict: Method still works but may not improve over linear scalarization
  - Highly imbalanced task difficulties: Normalization equalizes influence, which may underweight harder tasks

- **First 3 experiments**:
  1. Two-task synthetic problem with known Pareto front; verify DiBS-MTL reaches balanced solutions and is invariant to transforming one loss by $h(\ell) = \ell^4$
  2. MT10 RL experiment with one task reward transformed; compare DiBS-MTL vs. Nash-MTL and FAMO on success rate and training stability
  3. Run 1-step DiBS-MTL on NYU-v2 or QM9; compare $\Delta_m\%$ and mean rank against baselines (MGDA, Nash-MTL, FAMO)

## Open Questions the Paper Calls Out

### Open Question 1
What are the convergence rates for DiBS-MTL in nonconvex settings? The paper establishes asymptotic subsequence convergence to a Pareto stationary point but does not provide bounds on convergence speed or iteration complexity. What evidence would resolve it: Theoretical derivation of iteration complexity bounds or empirical scaling analysis across varying model sizes.

### Open Question 2
Is the "radially attractive" modification detailed in Appendix B strictly necessary to prevent divergence? The proof relies on Assumption 2 (bounded iterates), which the authors relax in Appendix B via a specific dynamical system modification, but it is unstated if this modification was used in the main benchmarks. What evidence would resolve it: Ablation studies comparing standard vs. modified dynamics on volatile loss landscapes or a proof that standard DiBS dynamics are naturally bounded for neural networks.

### Open Question 3
Under what theoretical conditions does the efficient 1-step DiBS-MTL approximation fail to match the performance of the multi-step T-step variant? Section 4 notes that the 1-step variant effectively discards the distance-to-optima weighting used in the general DiBS formulation. What evidence would resolve it: Theoretical bounds on the approximation error or identification of specific MTL problems where multi-step variants significantly outperform the 1-step version.

## Limitations

- Theoretical analysis assumes bounded gradients and iterates (Assumptions 1-2), which may not hold in practice for deep networks
- Convergence proof relies on monotonic step sizes satisfying stochastic approximation conditions, but empirical validation uses constant learning rates
- Method does not extensively explore computational overhead of T-step variant beyond noting similar performance to 1-step

## Confidence

- **High confidence**: The mechanism of gradient normalization for transformation invariance (Mechanism 1) is well-supported by both theory and empirical evidence. The claim that DiBS-MTL converges to Pareto stationary points (Theorem 1) is rigorously proven under stated assumptions.
- **Medium confidence**: The bargaining-theoretic framing (Mechanism 2) is mathematically sound, but practical benefits over simpler gradient balancing methods require more extensive ablation studies. The first-order approximation validity (Mechanism 3) is reasonable but not extensively validated for highly curved loss landscapes.
- **Medium confidence**: The empirical superiority on transformed RL tasks is demonstrated, but the number of seeds (10) and specific transformation choices may not capture all failure modes of baseline methods.

## Next Checks

1. **Convergence analysis**: Track gradient norms and iterate distances during training to verify Assumptions 1-2 empirically. Test whether violating these assumptions (e.g., unbounded gradients) breaks convergence.

2. **Robustness to zero gradients**: Create synthetic tasks where gradients vanish at certain points. Test whether DiBS-MTL's normalization remains stable and whether the method degrades gracefully compared to baselines.

3. **Ablation on T-step depth**: Systematically vary the number of inner bargaining steps (T) in DiBS-MTL and measure both convergence speed and final performance on standard benchmarks to quantify the tradeoff between computational cost and solution quality.