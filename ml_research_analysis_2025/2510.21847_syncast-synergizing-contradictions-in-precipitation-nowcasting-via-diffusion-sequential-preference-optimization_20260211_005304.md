---
ver: rpa2
title: 'SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion
  Sequential Preference Optimization'
arxiv_id: '2510.21847'
source_url: https://arxiv.org/abs/2510.21847
tags:
- preference
- precipitation
- syncast
- diffusion
- nowcasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynCast, a novel approach for precipitation
  nowcasting that addresses the challenges of over-smoothing in deterministic models
  and performance instability in probabilistic generative models. SynCast employs
  a two-stage post-training framework called Diffusion Sequential Preference Optimization
  (Diffusion-SPO) to optimize conflicting metrics, specifically the Critical Success
  Index (CSI) and False Alarm Ratio (FAR).
---

# SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion Sequential Preference Optimization

## Quick Facts
- arXiv ID: 2510.21847
- Source URL: https://arxiv.org/abs/2510.21847
- Reference count: 40
- Key outcome: SynCast achieves state-of-the-art performance on precipitation nowcasting by jointly optimizing conflicting CSI and FAR metrics through a two-stage Diffusion-SPO framework, outperforming deterministic models and stabilizing probabilistic diffusion models.

## Executive Summary
SynCast addresses the fundamental trade-off between Critical Success Index (CSI) and False Alarm Ratio (FAR) in precipitation nowcasting. While deterministic models over-smooth predictions and probabilistic diffusion models suffer from performance instability, SynCast employs a novel two-stage post-training framework called Diffusion Sequential Preference Optimization (Diffusion-SPO). The method first reduces false alarms through FAR alignment, then optimizes CSI while preserving FAR gains through constrained optimization. Experiments across three radar precipitation datasets demonstrate that SynCast effectively balances these conflicting metrics, achieving performance comparable to state-of-the-art methods while providing more stable predictions.

## Method Summary
SynCast operates through a two-stage post-training process built on a pixel-space UNet diffusion model. First, it generates N predictions per input using different noise initializations, creating frame-level preference pairs ranked by CSI and FAR metrics. Stage 1 employs Diffusion-DPO to align the model with FAR preferences, producing policy π_α that suppresses false alarms. Stage 2 then optimizes CSI while constraining KL-divergence from π_α and maintaining FAR reward above threshold H_1 through a Lagrangian formulation. The method uses frame-wise rather than sequence-wise preference construction to preserve fine-grained intra-sequence variations, and its effectiveness scales with base model stochasticity.

## Key Results
- SynCast reduces FAR while improving CSI on SEVIR, MeteoNet, and HKO-7 datasets, achieving state-of-the-art performance
- Frame-level preference construction outperforms whole-sequence ranking strategies across all datasets
- Models with higher stochasticity (DDPM > DDIM > deterministic hybrids) show larger preference gaps and stronger Diffusion-SPO gains
- Two-stage sequential optimization enables joint improvement of conflicting metrics where single-stage methods fail

## Why This Works (Mechanism)

### Mechanism 1
Sequential two-stage optimization enables joint improvement of conflicting metrics where single-stage methods fail. Stage 1 aligns the model to FAR preferences using Diffusion-DPO, producing policy π_α that suppresses false alarms. Stage 2 then optimizes CSI while constraining KL-divergence from π_α and maintaining FAR reward above threshold H_1. The constraint term α·r_FAR in the Lagrangian dual (Eq. 14-16) prevents CSI optimization from degrading FAR gains. Core assumption: the constraint coefficient α and KL penalty β can be tuned such that the FAR alignment from Stage 1 remains stable during Stage 2 CSI optimization.

### Mechanism 2
Frame-level preference pair construction provides finer-grained supervision than whole-sequence ranking. For each input, N independent noise sequences generate N predictions plus one ensemble (N+1 total). Rather than ranking entire sequences, each lead-time frame is ranked separately by CSI and FAR. Best/worst frames per metric are reassembled into win-lose pairs, yielding sharper gradient signals. Core assumption: frame-level metric variations within a sequence are meaningful and reflect learnable model behaviors rather than noise.

### Mechanism 3
Higher base model stochasticity yields larger preference gaps and stronger Diffusion-SPO gains. Models with greater sampling stochasticity (DDPM > DDIM > deterministic hybrids) produce more diverse candidate samples, increasing the win-lose metric gap. Larger gaps provide clearer preference signals for optimization. PreDiff (DDPM) shows 10.75% CSI improvement vs. DiffCast's 1.97% after Diffusion-SPO. Core assumption: the preference gap (win - lose metric difference) correlates with optimization signal strength.

## Foundational Learning

- **Concept: Diffusion Models (DDPM/DDIM)**
  - Why needed here: SynCast's base model is a pixel-space diffusion model; understanding forward noising and reverse denoising (Eq. 1-5) is prerequisite for grasping preference pair generation and Diffusion-DPO loss formulation.
  - Quick check question: Can you explain why DDPM sampling introduces more stochasticity than DDIM, and how this affects the diversity of generated precipitation predictions?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Diffusion-SPO builds on Diffusion-DPO (Eq. 6-11), extending it with sequential constraints for multi-metric alignment. Without DPO foundations, the KL-regularized reward formulation is opaque.
  - Quick check question: How does DPO avoid explicit reward model training, and what role does the reference model p_ref play in the loss function?

- **Concept: CSI and FAR Trade-off in Nowcasting**
  - Why needed here: The entire method is motivated by the CSI-FAR conflict (Fig. 2). CSI rewards hits while penalizing misses and false alarms; FAR penalizes false alarms specifically. Understanding why these conflict is essential.
  - Quick check question: Why does increasing prediction sensitivity to catch more precipitation events (improving CSI) typically increase false alarms (worsening FAR)?

## Architecture Onboarding

- **Component map:** Base Model (π₀) → Preference Pair Constructor → Stage 1 (FAR Alignment) → Stage 2 (CSI + FAR Constraint)
- **Critical path:** Base model training → Preference pair generation → Stage 1 FAR-DPO → Stage 2 CSI-SPO with FAR constraint. Errors in base model stochasticity or pair construction propagate to both stages.
- **Design tradeoffs:** Pixel-space vs. latent diffusion: Pixel-space preserves resolution and sampling diversity but is computationally heavier. N (number of candidates): Higher N increases preference pair diversity but raises inference cost during pair construction. α and β tuning: Higher α preserves FAR more strongly but may limit CSI gains; higher β enforces tighter KL constraint around π_α.
- **Failure signatures:** FAR degrades after Stage 2: α is too low or β is too small; FAR constraint is insufficient. CSI plateaus despite Stage 2: Preference pairs lack diversity; base model stochasticity is too low (check win-lose gaps in Fig. 6 style analysis). Flickering predictions: Frame-level preference construction may be introducing temporal inconsistencies.
- **First 3 experiments:**
  1. Verify base model stochasticity: Generate N=8 samples per input, compute per-frame CSI/FAR variance. If win-lose gaps are small (<0.05 CSI difference), increase sampling steps or switch from DDIM to DDPM before proceeding to preference optimization.
  2. Ablate sequential vs. joint optimization: Compare Diffusion-SPO (two-stage) against single-stage Diffusion-DPO optimizing CSI+FAR jointly. Expect Diffusion-SPO to achieve lower FAR at comparable CSI (Table V pattern).
  3. Sensitivity analysis on α and β: Run Stage 2 with α ∈ {0.1, 0.5, 1.0} and β ∈ {0.01, 0.1, 1.0}. Plot Pareto frontier of CSI vs. FAR to identify stable operating region where FAR constraint is active but CSI gains are not fully suppressed.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Diffusion-SPO framework be effectively extended to high-resolution precipitation nowcasting without destabilizing the training process? Current experiments downscaled all datasets to 128×128 due to computational resource limitations, leaving the framework's stability and efficiency at native resolutions unverified.

### Open Question 2
Can the sequential preference optimization strategy be generalized to jointly optimize more than two conflicting metrics? The current Diffusion-SPO formulation is a two-stage process specifically designed to resolve the dyadic trade-off between CSI and FAR; extending this to multi-dimensional conflicts involves complex constraint interactions that remain unexplored.

### Open Question 3
Is the effectiveness of Diffusion-SPO fundamentally limited by the stochasticity of the base probabilistic model? While the paper verifies the method works on different diffusion models, it does not determine if the framework fails for models with inherently low stochasticity or if there is a lower bound of diversity required for effective preference optimization.

## Limitations

- Limited ablation on sequential vs. joint optimization: Direct comparison with single-stage Diffusion-DPO optimizing CSI+FAR jointly is not provided.
- Lack of frame-level temporal consistency validation: Frame-wise preference construction may introduce flickering artifacts, but this is not explicitly tested.
- Sensitivity to hyperparameters: α and β tuning in Diffusion-SPO is critical, yet systematic sensitivity analysis is absent.
- Base model stochasticity dependence: Gains are tied to sampling diversity, but the relationship between stochasticity and preference signal quality is not rigorously quantified.

## Confidence

- **High Confidence**: CSI and FAR are conflicting metrics in precipitation nowcasting; Diffusion-SPO achieves the stated trade-off improvements on the tested datasets.
- **Medium Confidence**: Frame-level preference construction provides finer supervision than sequence-level ranking; sequential two-stage optimization outperforms joint optimization for conflicting metrics.
- **Low Confidence**: Diffusion-SPO generalizes robustly across all diffusion models for precipitation nowcasting; the method is robust to hyperparameter variations without extensive tuning.

## Next Checks

1. **Ablate sequential vs. joint optimization**: Train a single-stage Diffusion-DPO model optimizing CSI+FAR jointly. Compare FAR and CSI trade-offs against Diffusion-SPO to isolate the benefit of the two-stage approach.
2. **Test frame-level temporal consistency**: Generate predictions using frame-level optimized models and compute temporal coherence metrics (e.g., frame-to-frame SSIM or optical flow consistency). Check for flickering artifacts.
3. **Analyze stochasticity-signal coupling**: For a range of base models (deterministic, DDIM, DDPM), quantify win-lose metric gaps and preference pair diversity. Correlate gap size with Diffusion-SPO performance gains to validate the stochasticity-signal hypothesis.