---
ver: rpa2
title: 'The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason'
arxiv_id: '2506.12286'
source_url: https://arxiv.org/abs/2506.12286
tags:
- swe-bench
- performance
- memorization
- tasks
- issue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a critical problem in evaluating large language\
  \ models for software engineering: SWE-Bench Verified, a widely-used benchmark,\
  \ may overstate models' genuine coding abilities due to data contamination and memorization.\
  \ The authors introduce three diagnostic tasks\u2014file path identification, function\
  \ reproduction, and prefix completion\u2014designed to isolate memorization effects\
  \ from problem-solving skills."
---

# The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason

## Quick Facts
- arXiv ID: 2506.12286
- Source URL: https://arxiv.org/abs/2506.12286
- Authors: Shanchao Liang; Spandan Garg; Roshanak Zilouchian Moghaddam
- Reference count: 23
- Primary result: State-of-the-art models achieve suspiciously high performance on SWE-Bench Verified due to memorization, with substantial drops on external benchmarks.

## Executive Summary
This paper identifies a critical problem in evaluating large language models for software engineering: SWE-Bench Verified may overstate models' genuine coding abilities due to data contamination and memorization. The authors introduce three diagnostic tasks—file path identification, function reproduction, and prefix completion—designed to isolate memorization effects from problem-solving skills. They demonstrate that state-of-the-art models achieve suspiciously high performance (up to 76% accuracy for file path identification) on SWE-Bench Verified, with substantial performance drops (up to 47 percentage points) on external benchmarks and repositories not included in SWE-Bench.

## Method Summary
The authors evaluate six benchmarks using three diagnostic tasks to detect memorization in LLMs. File path identification requires predicting buggy files from issue descriptions alone. Function reproduction generates complete functions without signatures or specifications. Prefix completion completes code given only prefixes. They compare performance across SWE-Bench Verified, Full-SWE-Bench, SWE-Bench Extra, Outside-Repo Tasks, RefactorBench, and SWE-Bench C#, using accuracy, 5-gram overlap, and instance-level verbatim match metrics. Ten models were evaluated via API with various sampling strategies.

## Key Results
- State-of-the-art models achieve up to 76% accuracy in identifying buggy file paths using only issue descriptions
- Performance drops up to 47 percentage points when tested on repositories not included in SWE-Bench
- 5-gram overlap reaches 35% on SWE-Bench Verified but only 18% on other benchmarks
- Two types of memorization identified: instance-specific (within SWE-Bench) and repository-bias (SWE-Bench repositories vs external repos)

## Why This Works (Mechanism)

### Mechanism 1
Performance disparities across comparable benchmarks indicate memorization rather than generalizable coding ability. Cross-benchmark comparison uses performance differences on structurally similar tasks as a proxy for detecting data contamination. Genuine problem-solving should yield consistent performance across comparable tasks; memorization produces anomalously high scores on specific benchmarks likely present in training data. Core assumption: Tasks from different benchmarks with similar complexity require comparable skill levels, so performance differences reflect differential exposure rather than difficulty.

### Mechanism 2
High accuracy on file-path identification without repository context reveals memorized issue-file associations. By withholding all repository structure and code, the diagnostic task forces models to rely solely on issue descriptions. Correct file identification without contextual cues indicates prior exposure to specific issue-solution pairs stored during training. Core assumption: Genuine bug localization requires understanding repository structure and code relationships—capabilities unavailable when only issue text is provided.

### Mechanism 3
Elevated n-gram overlap between model outputs and ground truth patches—absent necessary specifications—indicates verbatim sequence memorization. The function reproduction task provides only function names and issue descriptions, deliberately omitting signatures, docstrings, and cross-file context. Models generating high 5-gram overlap under these constraints must be reproducing memorized training sequences rather than deriving solutions. Core assumption: Correct implementations require specification information; reproducing solutions without specifications implies prior exposure.

## Foundational Learning

- **Data contamination in benchmark evaluation**: Understanding that training data overlap with evaluation benchmarks inflates performance metrics is essential for interpreting the paper's core claim. Quick check: If a model was trained on GitHub data including the SWE-Bench repositories, what two alternative explanations could account for high benchmark performance?

- **N-gram overlap as a memorization proxy**: The 5-gram overlap metric quantifies verbatim reproduction; understanding its interpretation is critical for evaluating the function reproduction results. Quick check: Why does the paper use 5-gram overlap rather than exact match or token-level accuracy?

- **Instance-specific vs. repository-bias memorization**: The paper distinguishes memorization of specific benchmark instances from broader overfitting to repository patterns—each has different implications for benchmark validity. Quick check: If a model performs well on SWE-Bench Extra (new issues from SWE-Bench repos) but poorly on Outside-Repo Tasks, which type of memorization does this suggest?

## Architecture Onboarding

- **Component map**: Diagnostic tasks (file-path identification, function reproduction, prefix completion) → Benchmarks (SWE-Bench-Verified, Full-SWE-Bench, SWE-Bench-Extra, Outside-Repo-Tasks, RefactorBench, SWE-Bench-C#) → Metrics (accuracy, filtered accuracy, 5-gram overlap, instance-level verbatim match)

- **Critical path**: File-path identification results → establish instance-specific memorization via Verified > Full > Extra hierarchy → repository-bias memorization via SWE-Bench > external repos comparison → function reproduction confirms via n-gram patterns

- **Design tradeoffs**: Filtered accuracy reduces false positives from explicit path mentions but reduces sample size (73% of Verified instances retained vs. 97% of RefactorBench); cross-benchmark approach avoids requiring training data access but introduces task heterogeneity confounds; single-round generation in function reproduction prevents agent-style tool use but may understate model capabilities

- **Failure signatures**: Inconsistent model rankings across benchmarks (observed in OpenAI chat models) suggests noise in measurement; RefactorBench's short issue descriptions (14.6 tokens) create a confound: poor performance may reflect insufficient context rather than lack of memorization; SWE-Bench-C# language switch introduces ecosystem exposure as an additional variable

- **First 3 experiments**: 1) Replicate file-path identification on a held-out temporal split (post-cutoff issues from SWE-Bench repos) to validate the instance-specific memorization claim; 2) Run function reproduction with gradually increasing context (signature only → signature + docstring → signature + related functions) to establish the minimum context required for legitimate generation; 3) Apply the prefix completion task to external benchmarks to establish baseline verbatim rates for non-contaminated data

## Open Questions the Paper Calls Out
None

## Limitations
- Cannot definitively distinguish memorization from legitimate transfer learning where models develop general problem-solving patterns that generalize to SWE-Bench tasks
- Task heterogeneity confounds: RefactorBench's minimal issue descriptions (14.6 tokens) and SWE-Bench-C#'s language switch introduce uncontrolled variables
- Key procedural elements remain unspecified, including function reproduction prompt template, filtered accuracy heuristics, and ground truth parsing procedures

## Confidence
- **High confidence**: Identification of suspiciously high performance on file-path tasks without repository context, and consistent pattern of performance drops on external benchmarks
- **Medium confidence**: N-gram overlap analysis provides reasonable evidence for verbatim memorization, though absence of explicit specification context may underestimate legitimate reasoning capabilities
- **Low confidence**: Distinction between instance-specific and repository-bias memorization remains speculative, as available evidence patterns could reflect other systematic differences between benchmarks

## Next Checks
1. Apply file-path identification to post-cutoff issues from SWE-Bench repositories to determine if performance remains high on chronologically novel data
2. Systematically vary the context provided in function reproduction tasks (signature only, signature + docstring, signature + related functions) to establish minimum information required for legitimate code generation versus memorization
3. Apply the prefix completion task to uncontaminated code repositories to establish baseline verbatim reproduction rates, enabling proper calibration of the memorization threshold