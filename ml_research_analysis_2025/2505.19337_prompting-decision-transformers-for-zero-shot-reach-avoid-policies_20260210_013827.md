---
ver: rpa2
title: Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies
arxiv_id: '2505.19337'
source_url: https://arxiv.org/abs/2505.19337
tags:
- avoid
- state
- radt
- learning
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RADT introduces a decision transformer that learns reach-avoid
  policies from random offline data without rewards or cost functions. It encodes
  goals and avoid regions as prompt tokens, enabling zero-shot generalization to new
  avoid-region sizes and counts.
---

# Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies

## Quick Facts
- **arXiv ID:** 2505.19337
- **Source URL:** https://arxiv.org/abs/2505.19337
- **Reference count:** 40
- **One-line primary result:** RADT achieves up to 35.7% improvement in normalized cost in zero-shot settings while maintaining high goal success rates across 11 tasks.

## Executive Summary
RADT introduces a decision transformer that learns reach-avoid policies from random offline data without rewards or cost functions. It encodes goals and avoid regions as prompt tokens, enabling zero-shot generalization to new avoid-region sizes and counts. Using goal and avoid-region hindsight relabeling, RADT trains entirely from suboptimal trajectories. Across 11 tasks, RADT matches or exceeds baselines retrained on new avoid-region configurations, achieving up to 35.7% improvement in normalized cost in zero-shot settings while maintaining high goal success rates. In cell reprogramming, RADT reduces visits to unsafe gene expression states, adapting flexibly even under stochastic transitions.

## Method Summary
RADT uses a GPT-2 style causal transformer that receives a structured prompt encoding goals and avoid regions before the trajectory sequence. The model attends to these tokens during autoregressive action prediction, conditioning behavior on arbitrary avoid specifications without changing state-space dimensionality. A two-pass hindsight relabeling algorithm creates paired trajectories with opposite avoid success indicators, isolating the concept of "avoid success" from trajectory-specific variations. Training uses MSE loss on actions plus BCE loss on avoid violation prediction, with attention boosting to ensure prompt tokens are attended to.

## Key Results
- RADT achieves up to 35.7% improvement in normalized cost in zero-shot settings while maintaining high goal success rates
- Outperforms baseline models that require retraining when avoid-region sizes/counts change
- Reduces visits to unsafe gene expression states in cell reprogramming tasks
- Matches or exceeds baselines even when trained only on random-policy trajectories

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Decoupling of Behavioral Constraints
Encoding goals and avoid regions as discrete prompt tokens (rather than augmented state vectors) enables zero-shot generalization to unseen avoid-region counts and sizes at evaluation time. A causal transformer receives a structured prompt before the trajectory sequence and attends to these tokens during autoregressive action prediction, conditioning behavior on arbitrary avoid specifications without changing the state-space dimensionality.

### Mechanism 2: Hindsight Avoid-Region Relabeling
Random-policy trajectories that pass through avoid regions can be relabeled as meaningful demonstrations of "what to avoid," enabling purely offline learning without reward engineering. A two-pass algorithm creates paired trajectories: for each original trajectory, generate one version with avoid regions it violates and one with regions it circumvents. This pairs successful and unsuccessful demonstrations of avoidance, isolating the concept of "avoid success" from trajectory-specific variations.

### Mechanism 3: Binary Success Token as Behavioral Switch
The avoid success indicator token serves as an explicit behavioral switch, allowing the model to generate both successful and unsuccessful avoidance behaviors, then conditioning on success at test time. During training, the model learns to predict actions conditioned on both the avoid specification and the success token. At evaluation, setting z=1 steers generation toward avoidance-compliant trajectories.

## Foundational Learning

- **Concept: Offline Reinforcement Learning**
  - Why needed here: RADT operates entirely from pre-collected datasets without online exploration, a core design constraint.
  - Quick check question: Can you explain why online exploration is prohibited in safety-critical settings?

- **Concept: Hindsight Experience Replay (HER)**
  - Why needed here: RADT extends HER from goal relabeling to avoid-region relabeling.
  - Quick check question: How does relabeling a failed trajectory's final state as the "goal" convert it into a successful demonstration?

- **Concept: Decision Transformers and Causal Language Modeling**
  - Why needed here: RADT frames policy learning as sequence modeling with autoregressive action prediction.
  - Quick check question: How does a causal transformer differ from a bidirectional transformer in handling sequential decision-making?

- **Concept: Goal-Conditioned Reinforcement Learning**
  - Why needed here: RADT is goal-conditioned and avoid-region-conditioned, generalizing beyond standard GCRL.
  - Quick check question: Why can't standard GCRL methods handle dynamically specified avoid regions at test time?

## Architecture Onboarding

- **Component map:** Prompt encoder -> Trajectory encoder -> Causal transformer backbone -> Action prediction head -> Auxiliary box-awareness head
- **Critical path:** Prepare prompt with goal g, avoid boxes {bj}, and z=1 (at test time) -> Feed prompt tokens followed by trajectory prefix -> Autoregressively predict next action ât from transformer output -> Execute ât in environment, observe st+1, repeat
- **Design tradeoffs:** Box coordinates vs. centroids for size preservation, attention boosting strength vs. overfitting, paired relabeling vs. single-pass for dataset size, transformer size vs. training time
- **Failure signatures:** High MNC with high SR (rushing through avoid regions), low MNC with low SR (staying in place), attention collapse (ignored prompt tokens), overfitting to training avoid-region distribution
- **First 3 experiments:** 1) Train RADT on FetchReachObstacle with fixed avoid-box size, verify MNC < 0.1 and SR > 0.95, compare against random baseline. 2) Evaluate trained model on avoid-box widths [0.18, 0.20, 0.22, 0.24] without retraining, plot MNC and SR vs. box width. 3) Train variant with only single-pass relabeling, compare MNC/SR on same OOD settings.

## Open Questions the Paper Calls Out

### Open Question 1
Can RADT be extended to handle non-rectangular avoid regions with tight boundaries, such as convex polygons or arbitrary irregular shapes? The current iteration of RADT can only handle avoid boxes, and while we have been using conservative box sizes for avoid regions that are not inherently rectangular, applications that require tight boundaries for avoid regions of more complex shapes will be difficult for our model.

### Open Question 2
Can RADT achieve competitive performance with significantly reduced model size and training time? The flexibility and zero-shot capabilities of RADT are balanced out by the fact that it takes a lot longer and a lot more computational resources to train compared to the baselines models, having many more parameters (as it is based on a GPT-2 architecture).

### Open Question 3
Does RADT maintain strong performance when trained on mixed-quality offline data containing a blend of expert, near-optimal, and random trajectories? The paper explicitly restricts training data to "purely random policy" trajectories to satisfy Property (2), but real-world offline datasets often contain heterogeneous quality.

## Limitations

- RADT's zero-shot generalization relies on attention mechanisms flexibly binding meaning to prompt tokens, which is plausible but not formally proven
- Paired relabeling algorithm creates dataset twice as large, increasing computational cost
- Biological domain results are based on Boolean network models that may not capture real-world stochasticity
- Requires conservative over-approximation for non-rectangular avoid regions, potentially excluding valid safe trajectories

## Confidence

- **High confidence:** Offline learning from random data without rewards, paired relabeling methodology, quantitative performance improvements in robotic control tasks
- **Medium confidence:** Prompt-based generalization mechanism, biological application results, attention boosting effectiveness
- **Low confidence:** Specific mechanisms of attention-based prompt binding, robustness to extreme avoid-region configurations, scalability to high-dimensional state spaces

## Next Checks

1. **Attention weight analysis:** Verify that prompt tokens receive meaningful attention weights during both training and evaluation. Check for attention collapse by comparing attention distributions with and without the boosting term `a_delta`.

2. **Paired vs. single-pass ablation:** Train RADT variants with single-pass relabeling (one trajectory per avoid region, no pairs) and compare zero-shot generalization performance. Measure the impact on MNC and SR across OOD avoid-region configurations.

3. **Stochasticity robustness test:** Evaluate RADT on variants of FetchReachObstacle and MazeObstacle with increased transition noise (e.g., 10-20% action perturbation). Assess whether prompt-based conditioning degrades gracefully compared to baselines.