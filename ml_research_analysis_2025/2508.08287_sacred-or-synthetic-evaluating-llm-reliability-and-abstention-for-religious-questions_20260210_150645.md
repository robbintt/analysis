---
ver: rpa2
title: Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious
  Questions
arxiv_id: '2508.08287'
source_url: https://arxiv.org/abs/2508.08287
tags:
- answer
- abstention
- arabic
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces FiqhQA, a benchmark for evaluating LLM performance
  in answering Islamic religious questions according to the four major Sunni schools
  of thought. Unlike prior work, it assesses both accuracy and abstention ability,
  emphasizing when models should refrain from answering.
---

# Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions

## Quick Facts
- **arXiv ID:** 2508.08287
- **Source URL:** https://arxiv.org/abs/2508.08287
- **Reference count:** 6
- **Primary result:** Introduces FiqhQA benchmark to evaluate LLM accuracy and abstention in answering Islamic religious questions; finds GPT-4o most accurate, Gemini/Fanar best at abstaining, and Arabic performance lags English.

## Executive Summary
This paper introduces FiqhQA, a novel benchmark for evaluating large language models (LLMs) on Islamic religious questions according to the four major Sunni schools of thought. Unlike prior work, it assesses both the correctness of answers and the model's ability to abstain when uncertainâ€”a critical capability for sensitive domains like religion. Experiments with six LLMs in Arabic and English reveal that while GPT-4o leads in accuracy, Gemini and Fanar models are superior at abstaining from answering, especially in Arabic. Performance is consistently lower in Arabic than English, and accuracy varies across schools of thought. The study highlights the need for careful, domain-specific evaluation and transparent, expert-supervised deployment of LLMs in religious contexts.

## Method Summary
The authors developed FiqhQA, a benchmark dataset of Islamic religious questions paired with correct answers from each of the four major Sunni schools of thought. Six LLMs were evaluated in both Arabic and English, measuring accuracy (correctness of answers) and abstention (ability to refrain from answering when uncertain). Human annotators labeled answers for correctness and abstention decisions. The benchmark design emphasizes both factual accuracy and responsible abstention, addressing gaps in prior religious QA evaluations. Performance was compared across languages and schools of thought, with models' abstention behavior scrutinized alongside accuracy.

- **dataset:** FiqhQA, Islamic religious questions with four Sunni school answers
- **models:** Six LLMs tested in Arabic and English
- **metrics:** Accuracy (correctness) and abstention (refraining from answering)
- **annotators:** Human experts labeled answer correctness and abstention

## Key Results
- GPT-4o achieved the highest accuracy among all models tested.
- Gemini and Fanar models exhibited superior abstention, particularly in Arabic.
- All models performed worse in Arabic than in English.
- Accuracy varied significantly across the four Sunni schools of thought.

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its dual focus on accuracy and abstention, addressing a critical gap in religious QA evaluation. By incorporating expert-labeled answers from all four Sunni schools, it ensures comprehensive coverage of Islamic jurisprudence. The cross-lingual design reveals language-specific performance differences, while human annotation of both correctness and abstention provides nuanced evaluation beyond simple accuracy metrics. This approach better reflects real-world requirements for responsible LLM deployment in sensitive domains.

## Foundational Learning
- **Fiqh (Islamic jurisprudence):** The body of Islamic law derived from the Quran and Hadith; needed to understand the benchmark's religious context and answer labeling. Quick check: Review definitions of the four Sunni schools and their core principles.
- **LLM evaluation metrics:** Accuracy and abstention as complementary measures; needed to assess both correctness and model's judgment in withholding answers. Quick check: Compare abstention rates across models and correlate with accuracy.
- **Cross-lingual model performance:** Arabic vs. English performance differences; needed to identify language-specific biases and training gaps. Quick check: Analyze question complexity and translation quality effects on performance.
- **Human annotation reliability:** Expert-labeled correctness and abstention; needed to ensure benchmark validity and reproducibility. Quick check: Conduct inter-annotator agreement studies.
- **Religious domain sensitivity:** Importance of abstention in high-stakes domains; needed to justify abstention as a key evaluation criterion. Quick check: Survey religious scholars on abstention expectations.
- **Benchmark construction:** Curation of FiqhQA with balanced school representation; needed to ensure fair and comprehensive evaluation. Quick check: Validate dataset diversity across schools and question types.

## Architecture Onboarding
- **component map:** LLMs (GPT-4o, Gemini, Fanar, etc.) -> FiqhQA benchmark -> Human annotators -> Accuracy/abstention metrics
- **critical path:** Input question -> LLM generation -> Answer evaluation (correctness/abstention) -> Performance comparison across models/languages/schools
- **design tradeoffs:** Balancing accuracy and abstention; prioritizing responsible abstention over always answering; focusing on Sunni Islam may limit generalizability.
- **failure signatures:** Low abstention in sensitive domains; worse Arabic than English performance; variable accuracy across schools of thought.
- **first experiments:** 1) Replicate abstention labeling with additional annotators for consistency. 2) Test models on Arabic religious corpora outside FiqhQA. 3) Expand benchmark to include minority Islamic schools and other religions.

## Open Questions the Paper Calls Out
The paper does not explicitly list open questions but implicitly raises several: How can abstention strategies be improved for religious domains? What factors contribute to Arabic performance gaps? How might benchmark results generalize to other Islamic schools or religions? What training approaches could better equip models for religious question answering?

## Limitations
- FiqhQA may not fully represent the diversity of Islamic jurisprudence or lay questions.
- Benchmark focuses on Sunni Islam, limiting generalizability to other Islamic or religious domains.
- Language-specific performance gaps suggest potential undertraining on Arabic religious texts.
- Abstention metric relies on subjective human judgment, raising reproducibility concerns.
- Limited number of models tested may not capture full landscape of LLM capabilities.
- Benchmark construction details (question selection, annotation process) are not fully specified.

## Confidence
- Claims about GPT-4o's accuracy and Gemini/Fanar's abstention: **High** for within-study comparisons, **Medium** for broader real-world reliability claims.
- Cross-language and cross-school performance comparisons: **Medium** due to confounding factors like question complexity and translation quality.
- Generalizability of results to other religious domains: **Low** given benchmark's focus on Sunni Islam.

## Next Checks
1. Replicate the benchmark with a larger, more diverse set of religious questions, including minority Islamic schools and other religions, to test generalizability.
2. Conduct a multi-expert validation of the abstention labels to ensure consistency and reduce subjective bias.
3. Evaluate the same models on Arabic religious corpora outside of FiqhQA to assess whether performance gaps persist in naturalistic settings.
4. Analyze the specific types of questions where models perform poorly to identify knowledge gaps or training deficiencies.
5. Investigate whether fine-tuning on religious texts improves model performance in Arabic and religious domains.