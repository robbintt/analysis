---
ver: rpa2
title: Shapley Value-driven Data Pruning for Recommender Systems
arxiv_id: '2505.22057'
source_url: https://arxiv.org/abs/2505.22057
tags:
- uni00000013
- interactions
- data
- shapley
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of denoising recommender systems
  by shifting focus from subjective intent-based filtering to objective, model-driven
  evaluation of interaction quality. Existing methods filter interactions based on
  assumed user intent, but this ignores that some "noisy" interactions may still aid
  model training while some "clean" interactions may be uninformative.
---

# Shapley Value-driven Data Pruning for Recommender Systems

## Quick Facts
- arXiv ID: 2505.22057
- Source URL: https://arxiv.org/abs/2505.22057
- Reference count: 40
- Primary result: Outperforms existing denoising methods with up to 7.22% improvement in Recall@5 and 6.34% in NDCG@5

## Executive Summary
This paper addresses the problem of denoising recommender systems by shifting focus from subjective intent-based filtering to objective, model-driven evaluation of interaction quality. Existing methods filter interactions based on assumed user intent, but this ignores that some "noisy" interactions may still aid model training while some "clean" interactions may be uninformative. The proposed Shapley Value-driven Valuation (SVV) framework quantifies each interaction's value based on its marginal contribution to reducing training loss using real-time Shapley value estimation. A simulated noise injection protocol provides a verifiable ground truth for evaluating denoising effectiveness. Experiments on four real-world datasets show SVV outperforms existing methods with up to 7.22% improvement in Recall@5 and 6.34% in NDCG@5, while also offering interpretable noise assessment by identifying interactions critical to training versus those that merely reflect user intent.

## Method Summary
The SVV framework trains a Denoising Autoencoder (DAE) as the base recommendation model, then uses FastSHAP to estimate Shapley values for each interaction based on its contribution to reconstruction loss reduction. The value function measures normalized decrease in squared error, and the FastSHAP explainer learns to predict these values without expensive retraining. Interactions are ranked by their Shapley values and the bottom k% are pruned before retraining the DAE on the cleaned dataset. The method uses binary implicit feedback converted from ratings, with confidence-weighted loss where positive interactions are scaled by Ï=5-25. The framework employs simulated noise injection (randomly flipping 0s to 1s) to create verifiable ground truth for evaluating denoising effectiveness.

## Key Results
- SVV outperforms existing denoising methods with up to 7.22% improvement in Recall@5 and 6.34% in NDCG@5
- Pruned datasets consistently achieve better performance than training on full noisy data or heuristic clean data
- The method provides interpretable noise assessment by identifying interactions that are critical to training versus those that merely reflect user intent
- Simulated noise injection protocol validates that low Shapley value interactions align with artificially injected noise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating "training utility" from "user intent" allows the model to retain noisy interactions that are beneficial for learning.
- **Mechanism:** The framework posits that interactions often labeled as noise (e.g., low ratings/accidental clicks) can reduce training loss by providing latent signals or gradient diversity. By evaluating the marginal contribution to loss reduction rather than adherence to a heuristic intent profile, the system retains data that aids generalization.
- **Core assumption:** The marginal contribution to training loss is a valid proxy for data quality, even if the interaction originates from a "noisy" user behavior.
- **Evidence anchors:**
  - [abstract] ("interactions deemed noisy could still aid model training... objective impact on model training")
  - [Page 2] (Figure 1 shows training on "noisy" full data outperformed training on heuristically "clean" data, with Clean3 resulting in a 5.32% decline in Recall).
  - [corpus] Corpus evidence for this specific mechanism is weak; neighbors focus on LLM-based denoising rather than game-theoretic valuation.
- **Break condition:** If the base model is too simple to extract latent signals from noisy data, "noisy" interactions will show negative Shapley values and be pruned, reverting to standard filtering.

### Mechanism 2
- **Claim:** Amortized Shapley value estimation (FastSHAP) enables scalable, context-aware valuation of individual interactions.
- **Mechanism:** Instead of retraining the model for every possible subset of interactions (computationally impossible), a parametric function (the explainer) is trained to predict Shapley values in a single forward pass. This explainer learns to estimate the impact of removing an interaction by observing the model's behavior on various masked subsets.
- **Core assumption:** The explainer network is sufficiently expressive to approximate the true Shapley values, and the efficiency constraint (Eq. 12) holds.
- **Evidence anchors:**
  - [Page 4] ("FastSHAP trains an amortized parametric function... approximates the Shapley values by minimizing a weighted least squares loss")
  - [Page 4] ("global optimizer... outputs exact Shapley values... almost surely")
  - [corpus] No direct evidence in the provided corpus regarding FastSHAP efficiency; focus is on multi-modal/LLM denoising.
- **Break condition:** If the explainer underfits or the distribution of interactions shifts significantly from the training set, the estimated values become random noise, rendering pruning ineffective.

### Mechanism 3
- **Claim:** A custom value function focusing on reconstruction accuracy aligns valuation with recommendation performance.
- **Mechanism:** The "value" of a subset of interactions is defined by the normalized decrease in squared error (reconstruction loss) of the autoencoder. Interactions that help the model reconstruct the user's preference vector more accurately are assigned higher values.
- **Core assumption:** Minimizing the reconstruction loss of the autoencoder correlates directly with improved ranking metrics (Recall/NDCG).
- **Evidence anchors:**
  - [Page 5] ("value function is designed to reflect the training loss... measures the normalized decrease in the squared error")
  - [Page 4] ("contribution... to reducing training loss")
  - [corpus] Not applicable; this is a method-specific definition.
- **Break condition:** If the base model (DAE) learns to reconstruct noise (overfitting) rather than preference, the value function will erroneously assign high scores to noisy interactions that help "memorize" the input.

## Foundational Learning

- **Concept: Shapley Values (Game Theory)**
  - **Why needed here:** This is the mathematical core of SVV. You must understand that it calculates a player's (interaction's) contribution by averaging its marginal impact across all possible coalitions (subsets of data).
  - **Quick check question:** If removing interaction A *always* increases the loss regardless of what other data is present, is its Shapley value positive or negative?

- **Concept: Denoising Autoencoders (DAE)**
  - **Why needed here:** DAE is the chosen base model because it naturally handles "missing features" (masked interactions), which is required to simulate subsets during Shapley estimation without expensive retraining.
  - **Quick check question:** Why is the corruption step (masking input) essential for the DAE to learn robust user representations?

- **Concept: Implicit Feedback & Noise**
  - **Why needed here:** The problem domain. You need to distinguish between explicit noise (wrong labels) and implicit noise (misclicks/bias) to understand why "intent-based" filtering fails.
  - **Quick check question:** Why might a "noisy" click (e.g., accidental) still provide useful information about user proximity in the latent space?

## Architecture Onboarding

- **Component map:** Base Model (DAE) -> Value Function -> FastSHAP Explainer -> Pruning Logic
- **Critical path:** The training loop of the FastSHAP explainer. It requires sampling subsets $S$, computing the ground truth value $v(S)$ using the *frozen* base model, and updating the explainer weights to match this value.
- **Design tradeoffs:**
  - **DAE vs. Standard CF (MF/LightGCN):** The paper notes that MF/LightGCN would require retraining for every subset evaluation. **Tradeoff:** DAE is computationally cheaper for valuation but may have lower theoretical peak accuracy than GNNs.
  - **Exact vs. Amortized Shapley:** Using FastSHAP trades theoretical exactness for speed. **Tradeoff:** Requires training a second neural network (the explainer), adding complexity.
- **Failure signatures:**
  - **Uniform Valuations:** If the explainer outputs near-identical values for all interactions, it has failed to converge.
  - **Negative Performance Delta:** If retraining on pruned data yields lower accuracy than the baseline, the pruning threshold $k$ is too aggressive or the value function is misaligned with ranking goals.
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Replicate the "Clean vs. Clean3" experiment (Fig 1) to verify that your DAE base model actually learns from "noisy" low-rating data.
  2. **Simulated Noise Injection:** Inject random interactions (flip 0s to 1s) and verify that SVV assigns them significantly lower Shapley values than genuine interactions.
  3. **Explainer Convergence:** Monitor the loss of the FastSHAP explainer network (Eq. 10) to ensure it is actually learning to predict the value function output.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the SVV framework be adapted for dynamic or streaming recommendation environments where user preferences evolve and full model retraining is infeasible?
- **Basis in paper:** [explicit] The authors state in the Conclusion: "Future work will extend SVV to dynamic settings... to further enhance its applicability in evolving recommendation environments."
- **Why unresolved:** The current methodology relies on a static interaction matrix $D^+$ and a specific "Retraining with Pruned Data" phase, which assumes a fixed dataset rather than a continuous influx of interactions.
- **What evidence would resolve it:** An extension of SVV that supports incremental Shapley value updates or sliding-window valuation without requiring a complete recomputation of values for all historical interactions.

### Open Question 2
- **Question:** Can the interaction valuation approach be generalized to standard collaborative filtering architectures (e.g., LightGCN, MF) that cannot natively handle missing features without prohibitive computational costs?
- **Basis in paper:** [inferred] Section 4.1 explicitly justifies the choice of the Denoising Autoencoder (DAE) because "models such as matrix factorization (MF) or LightGCN must be retrained for every modification in the feature set," implying the current method is architecture-dependent.
- **Why unresolved:** The "missing feature" requirement is fundamental to the Shapley value calculation (Eq. 3), and the paper does not propose a method to approximate this for models that require dense inputs or full graph structures.
- **What evidence would resolve it:** A modified valuation mechanism or surrogate model that effectively estimates interaction contributions for non-autoencoder architectures without requiring iterative model retraining for every subset evaluation.

### Open Question 3
- **Question:** To what extent does the removal of low-value interactions identified by SVV align with the removal of actual real-world noise (e.g., accidental clicks) compared to the simulated random noise used in the paper's validation?
- **Basis in paper:** [inferred] In Section 5.3, the authors note that "low-quality data we artificially injected may not perfectly represent the true noise labels," and the simulated protocol (Section 4.3) relies on uniform random flipping rather than behavioral modeling.
- **Why unresolved:** The paper validates performance primarily on self-injected random noise (Table 3); it remains unclear if "low Shapley value" consistently maps to specific real-world error types like popularity bias or accidental clicks, or if it merely identifies statistically redundant interactions.
- **What evidence would resolve it:** Experiments on datasets containing verified negative feedback or dwell-time-based noise labels (rather than simulated random flips) to validate the correlation between SVV scores and genuine user dissatisfaction.

## Limitations
- The reliance on FastSHAP explainer's approximation quality may introduce errors if the amortized estimator fails to capture true marginal contributions
- The simulated noise injection protocol may not capture all real-world noise patterns, limiting validation of practical denoising capability
- The value function based on reconstruction loss from a single-layer DAE may not correlate well with ranking metrics for more complex models like GNNs

## Confidence

- **High Confidence**: The theoretical foundation of Shapley value computation and its application to interaction valuation is well-established. The reported performance improvements over baselines (up to 7.22% Recall@5) are supported by the experimental methodology.
- **Medium Confidence**: The efficiency of FastSHAP amortization is claimed but not fully validated against exact Shapley computation. The choice of a single-layer DAE as the base model may limit generalizability to more complex architectures.
- **Low Confidence**: The simulated noise injection protocol may not fully represent real-world noise distributions. The paper does not explore the sensitivity of pruning thresholds across different datasets or model architectures.

## Next Checks
1. **Approximation Error Analysis**: Compare FastSHAP Shapley values against exact Shapley values computed for a small subset of users to quantify the approximation error and its impact on pruning decisions.
2. **Cross-Model Generalization**: Test SVV with a more complex base model (e.g., LightGCN) to assess whether the value function and Shapley estimation remain effective beyond the single-layer DAE.
3. **Real-World Noise Patterns**: Evaluate SVV on datasets with known real-world noise (e.g., clickstreams with misclicks) rather than simulated noise to validate its practical denoising capability.