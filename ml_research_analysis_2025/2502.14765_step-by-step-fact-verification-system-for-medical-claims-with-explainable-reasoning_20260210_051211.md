---
ver: rpa2
title: Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning
arxiv_id: '2502.14765'
source_url: https://arxiv.org/abs/2502.14765
tags:
- claims
- evidence
- linguistics
- claim
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops and evaluates an iterative fact verification
  system that leverages large language models to verify medical claims through step-by-step
  question generation and evidence collection. The system generates follow-up questions
  to gather additional context, retrieves evidence from web search or internal knowledge,
  and produces verdicts with explainable reasoning.
---

# Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning

## Quick Facts
- **arXiv ID**: 2502.14765
- **Source URL**: https://arxiv.org/abs/2502.14765
- **Reference count**: 19
- **Primary result**: Iterative LLM-based verification improves F1 performance by 3.4-4.9 points over traditional three-part pipelines

## Executive Summary
This study develops an iterative fact verification system for medical claims that leverages large language models to generate follow-up questions, gather additional context, and produce explainable verdicts. The system addresses limitations of traditional three-part pipelines by using iterative question generation to collect evidence from web search or internal knowledge sources. Tested across three medical fact-checking datasets, the approach demonstrates significant performance improvements and provides transparent reasoning for each verification decision.

## Method Summary
The system employs an iterative approach where an LLM generates follow-up questions to gather additional context about medical claims. These questions are answered using either web search or internal knowledge sources, with the combined evidence used to produce final verdicts. The architecture incorporates predicate logic to enhance reasoning precision for well-formed claims. The system was evaluated on three medical datasets (MFact, MediFact, and SciFact) using GPT-4o-mini, comparing performance across different knowledge sources and claim types.

## Key Results
- Iterative question generation improves F1 performance by 3.4-4.9 points compared to traditional three-part pipelines
- GPT-4o-mini with web search performs best for scientific claims, while internal knowledge excels for common health claims
- Predicate logic enhancement increases precision for well-formed claims but reduces recall for informal language

## Why This Works (Mechanism)
The iterative approach works by breaking down complex medical verification tasks into smaller, manageable sub-questions that can be answered with targeted evidence retrieval. This decomposition allows the system to handle the complexity of medical concepts more effectively than attempting to verify claims in a single step. The use of different knowledge sources (web search vs. internal knowledge) enables adaptation to claim types, with web search providing broader coverage for scientific claims while internal knowledge offers more precise answers for common health queries.

## Foundational Learning
- **Iterative Question Generation**: Breaking verification into sequential sub-questions allows for more thorough evidence collection and reasoning
  - Why needed: Medical claims often require complex reasoning across multiple concepts that cannot be addressed in a single query
  - Quick check: Verify that generated follow-up questions are relevant and advance toward verification

- **Knowledge Source Selection**: Choosing between web search and internal knowledge based on claim type optimizes retrieval effectiveness
  - Why needed: Different claim types have different evidence requirements and availability patterns
  - Quick check: Compare retrieval precision across knowledge sources for different claim categories

- **Predicate Logic Integration**: Adding logical reasoning enhances precision for structured claims
  - Why needed: Medical claims often involve logical relationships that benefit from formal reasoning
  - Quick check: Test performance on claims with clear logical structures versus informal language

## Architecture Onboarding

**Component Map**: Claim Input -> Question Generator -> Evidence Retriever (Web Search/Internal Knowledge) -> Evidence Aggregator -> Verdict Generator -> Explainable Reasoning Output

**Critical Path**: The system's critical path follows the iterative loop where question generation leads to evidence retrieval, which informs subsequent questions until sufficient evidence is gathered for a final verdict.

**Design Tradeoffs**: The system balances precision and recall by choosing between web search (broader coverage, potentially noisier) and internal knowledge (more precise, potentially limited scope). Predicate logic improves precision but may hurt recall for informal language.

**Failure Signatures**: The system may struggle with claims containing informal language, produce false positives in high-stakes medical contexts, or fail when appropriate knowledge sources are unavailable for specific claim types.

**First Experiments**:
1. Test iterative question generation on a sample of medical claims to verify relevance and coverage of follow-up questions
2. Compare evidence retrieval precision between web search and internal knowledge for different claim categories
3. Evaluate the impact of predicate logic on both precision and recall across well-formed and informal claims

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three specific medical fact-checking datasets, constraining generalizability
- High false positive rates in medical contexts could have serious consequences despite performance improvements
- Lack of clear operational definitions for distinguishing between common health claims and scientific claims

## Confidence
- **High Confidence**: Iterative LLM-based approach improves performance over traditional three-part pipelines (3.4-4.9 F1 improvement)
- **Medium Confidence**: Distinction between optimal knowledge sources for common vs. scientific claims (limited operational definitions)
- **Medium Confidence**: Effectiveness of predicate logic enhancement (precision-recall trade-off for informal language)

## Next Checks
1. **External Dataset Validation**: Test the system on additional medical fact-checking datasets beyond MFact, MediFact, and SciFact to assess generalizability across different medical claim types and source materials.
2. **False Positive Analysis**: Conduct detailed error analysis focusing on false positive rates and cases where the system incorrectly verifies false medical claims, given the high-stakes nature of medical misinformation.
3. **Knowledge Source Generalization**: Evaluate the system's performance when using alternative knowledge sources (e.g., different search engines, domain-specific medical databases) to verify the robustness of the observed differences between web search and internal knowledge approaches.