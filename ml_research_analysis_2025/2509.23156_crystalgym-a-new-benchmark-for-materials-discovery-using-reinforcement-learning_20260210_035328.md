---
ver: rpa2
title: 'CrystalGym: A New Benchmark for Materials Discovery Using Reinforcement Learning'
arxiv_id: '2509.23156'
source_url: https://arxiv.org/abs/2509.23156
tags:
- learning
- crystal
- crystals
- density
- band
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CrystalGym, an open-source RL environment
  for crystalline material discovery, enabling direct optimization of DFT-calculated
  properties like band gap, bulk modulus, and density through reinforcement learning.
  The environment uses a graph-based MDP formulation where agents sequentially fill
  atomic positions in crystal lattices, with rewards derived from DFT calculations.
---

# CrystalGym: A New Benchmark for Materials Discovery Using Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.23156
- Source URL: https://arxiv.org/abs/2509.23156
- Reference count: 40
- Primary result: Introduces CrystalGym, an open-source RL environment for crystalline material discovery using DFT-based rewards

## Executive Summary
CrystalGym is an open-source RL environment for discovering crystalline materials by optimizing DFT-calculated properties like band gap, bulk modulus, and density. The environment uses a graph-based MDP where agents sequentially fill atomic positions in crystal lattices, with rewards derived from Quantum Espresso DFT calculations. Experiments across multiple crystal structures and action spaces demonstrate that value-based methods (DQN, Rainbow) outperform policy gradient methods (PPO) on density and bulk modulus tasks, while all methods struggle with band gap due to DFT's systematic underestimation and high failure rates.

## Method Summary
CrystalGym frames crystal composition as sequential decision-making using graph-based MDPs where states are partially-filled crystal graphs with nodes (atomic positions) and edges (neighbor connections). The MEGNet GNN encodes structural information, and agents select atoms from discrete action spaces to fill lattice positions. After episode completion, DFT simulations via Quantum Espresso compute target properties, generating rewards based on property-specific distance functions. Four RL algorithms (PPO, DQN, Rainbow, SAC) are evaluated across single and mixed crystal structures with varying action spaces and target property values.

## Key Results
- DQN and Rainbow achieve strongest performance on density and bulk modulus optimization, showing steady convergence to optimal solutions
- PPO converges quickly but often to suboptimal policies, particularly struggling with band gap tasks
- SAC fails to learn across all experiments, attributed to discrete action space limitations
- Larger action spaces (30 vs 18 elements) increase task difficulty and DFT failure rates, especially with transition metals

## Why This Works (Mechanism)

### Mechanism 1: Graph-Based State Representation Enables Spatial Reasoning
Representing crystals as graphs with nodes (atomic positions) and edges (neighbor connections) allows RL agents to learn compositional patterns that respect crystal geometry. MEGNet encodes node features (atom type or empty), edge features (Gaussian distances between neighbors), and graph-level features (lattice parameters, space group, target property). The GNN performs message passing across K layers, aggregating local neighborhood information into graph-level representations used by policy/value networks. Core assumption: Crystal properties emerge from local atomic interactions that can be captured through neighborhood aggregation.

### Mechanism 2: DFT-Derived Rewards Provide Ground-Truth Property Feedback
Using DFT-calculated properties as rewards grounds learning in physical reality rather than ML surrogate predictions. At episode termination, the completed crystal undergoes DFT simulation via Quantum Espresso. The computed property (band gap, bulk modulus, density) is compared to target value using property-specific distance functions, yielding a scalar reward. Failed DFT calculations receive fixed negative penalties. Core assumption: DFT calculations provide sufficiently accurate property estimates for policy optimization, despite known systematic errors (e.g., band gap underestimation).

### Mechanism 3: Value-Based Methods Outperform Policy Gradients for Noisy, Sparse Rewards
DQN and Rainbow's experience replay and ε-greedy exploration provide more stable learning than PPO's on-policy updates when rewards are noisy and delayed. Value-based methods store transitions in replay buffers, enabling batch processing of past experiences and decorrelation of updates. ε-greedy ensures continued exploration even as value estimates mature. PPO converges rapidly but may prematurely settle on suboptimal policies when high-reward states are rare. Core assumption: The MDP is sufficiently deterministic that past experience remains relevant despite stochastic DFT outcomes.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - Why needed here: CrystalGym frames crystal composition as sequential decision-making; understanding states, actions, transitions, rewards is essential.
  - Quick check question: Can you explain why filling atomic positions sequentially satisfies the Markov property?

- **Concept: Density Functional Theory (DFT) Basics**
  - Why needed here: DFT is the reward signal source; its limitations (band gap underestimation, convergence failures) directly affect learning.
  - Quick check question: Why might DFT systematically underestimate band gaps but accurately predict density?

- **Concept: Graph Neural Networks for Materials**
  - Why needed here: MEGNet is the default policy architecture; understanding message passing and crystal graph construction is required for customization.
  - Quick check question: How does MEGNet handle periodic boundary conditions in crystal representations?

## Architecture Onboarding

- **Component map:** CrystalGym Environment (Gymnasium API) -> Graph State Construction (nodes, edges, global features) -> MEGNet/CHGNet Policy Network (message passing → graph embedding) -> Action Selection (discrete: choose atom from action space) -> Environment Transition (fill next position) -> Episode Termination → DFT Calculation (Quantum Espresso) -> Reward Computation (property-specific distance function) -> RL Algorithm Update (PPO/DQN/Rainbow/SAC)

- **Critical path:** DFT calculation time (~20s for density, ~130s for bulk modulus) dominates wall-clock training. Start with density tasks for rapid iteration.

- **Design tradeoffs:**
  - Action space size vs. DFT failure rate: Larger action spaces (30-50 elements) increase exploration but also DFT convergence failures, especially with transition metals
  - Single vs. mixed structures: Single-structure training is faster but yields specialized policies; mixed-structure training generalizes but requires more samples
  - Relaxation vs. fixed lattice: Skipping structure relaxation saves computation but may generate thermodynamically unstable crystals

- **Failure signatures:**
  - SAC fails to learn: entropy collapse or temperature misconfiguration with discrete actions
  - Band gap always near zero: DFT systematic underestimation + sparse high-reward states
  - High negative reward density: action space contains elements causing DFT convergence failures

- **First 3 experiments:**
  1. Sanity check: Train PPO on single crystal (C1) with density target (3 g/cm³), small action space. Should reach high reward within 10K steps.
  2. Algorithm comparison: Compare DQN vs. Rainbow vs. PPO on bulk modulus (300 GPa) with same settings. Expect Rainbow/DQN to show steadier convergence.
  3. Failure mode analysis: Attempt band gap optimization (1.12 eV) with medium action space. Track DFT failure rate and reward distribution to diagnose sparse reward issues.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptations of Soft Actor-Critic (SAC) for discrete action spaces overcome the learning failures observed in the CrystalGym environment? Basis: SAC failed to show positive learning curves in any experiment. Authors explicitly state further investigation is needed to determine the cause of learning issues with SAC.

- **Open Question 2:** What is the optimal procedure for aligning pre-trained Large Language Models (LLMs) with noisy DFT-based reward signals? Basis: The REINFORCE fine-tuning approach resulted in monotonic policy degradation and entropy collapse, failing to improve upon the supervised fine-tuning baseline.

- **Open Question 3:** Can machine learning potentials be integrated into the reinforcement learning loop to enable efficient on-the-fly structure relaxation? Basis: Current experiments use fixed, unrelaxed lattices to manage computational costs, resulting in generated crystals having positive formation energies and limited thermodynamic stability.

## Limitations
- DFT systematic underestimation of band gap creates sparse, misleading reward landscapes that no algorithm can overcome
- High DFT failure rates (>20% for band gap) with larger action spaces limit exploration and learning efficiency
- Fixed lattice structures without relaxation may generate thermodynamically unstable crystals with positive formation energies

## Confidence
- **High confidence:** Graph-based MDP formulation, DFT as reward signal mechanism, general performance rankings (DQN/Rainbow > PPO > SAC), action space and structure complexity effects
- **Medium confidence:** MEGNet architecture specifics, exact hyperparameter choices, scalability to larger crystals beyond the 4-8 atom range tested
- **Low confidence:** LLM fine-tuning success rates, transferability of learned policies to unseen structures, long-term stability of optimized crystals

## Next Checks
1. Reproduce the density optimization baseline (single crystal, small action space, PPO) to verify environment setup and basic learning functionality
2. Test algorithm sensitivity by comparing DQN vs Rainbow performance on bulk modulus with identical hyperparameters to confirm the steady-convergence advantage
3. Systematically vary action space size while tracking DFT failure rates to quantify the trade-off between exploration capability and computational feasibility