---
ver: rpa2
title: 'Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning
  and Phonetic-Semantic Embeddings'
arxiv_id: '2507.06506'
source_url: https://arxiv.org/abs/2507.06506
tags:
- puns
- translation
- word
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research introduces a novel three-stage approach for translating
  English puns into French, combining large language models (LLMs) with wordplay generation
  techniques. The method employs contrastive learning, phonetic-semantic embeddings,
  and a multi-agent evaluation framework to preserve humor and linguistic creativity
  rather than literal translation.
---

# Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings

## Quick Facts
- arXiv ID: 2507.06506
- Source URL: https://arxiv.org/abs/2507.06506
- Authors: Russell Taylor; Benjamin Herbert; Michael Sana
- Reference count: 30
- Primary result: First and second place in human evaluation (9.27% and 7.85% pun preservation) at CLEF JOKER 2025 Task 2

## Executive Summary
This research introduces a novel three-stage approach for translating English puns into French, combining large language models (LLMs) with wordplay generation techniques. The method employs contrastive learning, phonetic-semantic embeddings, and a multi-agent evaluation framework to preserve humor and linguistic creativity rather than literal translation. Evaluated through the CLEF JOKER 2025 Task 2, the approach achieved first and second place in human evaluation, with human scores of 9.27% and 7.85% for pun preservation, respectivelyâ€”outperforming all other submissions. Automated metrics like BLEU and BERTScore remained low, reflecting the method's emphasis on non-literal, humor-preserving translation.

## Method Summary
The approach consists of three stages: (1) baseline LLM generation with contrastive learning discriminator (up to 10 retries); (2) guided chain-of-thought with combined phonetic-semantic embeddings (600-dim, BiLSTM encoder for phonetics, cosine threshold >0.75); (3) multi-agent evaluator loop (4 agents, 5 iterations, select highest average score). The system constructs a 600-dimensional embedding space by concatenating 300D FastText semantic vectors with 300D BiLSTM phonetic vectors to find French words that bridge semantic meaning from one sense and phonetic similarity from another. The multi-agent framework decomposes evaluation into specialized roles (Equivalence, Mistranslation, Emotion, Authenticity) to provide granular feedback that preserves humor better than single-pass generation.

## Key Results
- Achieved 9.27% and 7.85% human evaluation scores for pun preservation (1st and 2nd place out of 51 submissions)
- BLEU and BERTScore remained low (41-49/51), reflecting emphasis on non-literal translation
- Multi-agent approach outperformed guided chain-of-thought pipeline in human evaluation
- Contrastive learning discriminator achieved ~100% accuracy in distinguishing puns from non-puns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving target-language words that bridge semantic meaning from one sense and phonetic similarity from another creates viable pun candidates that literal translation misses.
- Core assumption: A continuous vector space can effectively capture the discrete articulatory features (via PanPhon) and semantic nuances required for cross-lingual wordplay.
- Evidence: The "Guided" run achieved 7.85% human evaluation score (2nd place) vs the baseline's 1.07%, attributing the jump to these constraints.

### Mechanism 2
- Claim: Decomposing evaluation into specialized roles (Equivalence, Mistranslation, Emotion, Authenticity) allows for granular feedback that preserves humor better than single-pass generation.
- Core assumption: LLMs can reliably act as "expert native speakers" to evaluate nuanced humor and authenticity when assigned specific personas.
- Evidence: The Multi-Agent run achieved the top human score (9.27%), outperforming the guided chain-of-thought run.

### Mechanism 3
- Claim: Contrastive learning enables a discriminator to distinguish puns from non-puns with high accuracy, acting as a gatekeeper to filter failed generations.
- Core assumption: The definition of a "destroyed pun" (literal sentence) provides a robust decision boundary for what constitutes successful wordplay.
- Evidence: The discriminator achieved ~100% accuracy on the test set, though without the phonetic-semantic constraints, outputs scored poorly on human relevance.

## Foundational Learning

- Concept: **Contrastive Learning**
  - Why needed here: Essential for training the discriminator to tell the difference between a sentence with wordplay and a literal sentence, which standard loss functions do not optimize for.
  - Quick check question: Can you explain how creating "negative samples" by removing the ambiguity from a pun sentence helps a model learn to recognize humor?

- Concept: **Articulatory Feature Vectors (PanPhon)**
  - Why needed here: Used to create the phonetic embeddings. Unlike simple spelling similarity, this represents how sounds are physically produced, which is crucial for finding phonetic lookalikes across languages.
  - Quick check question: Why would mapping IPA symbols to articulatory features (like 'bilabial' or 'voiced') be better for finding puns than comparing string spellings?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Used in the second stage to guide the LLM to first identify the pun, then the meanings, and *then* generate the translation, rather than attempting it all at once.
  - Quick check question: How does forcing a model to explain the "two meanings" of a pun before translating it change the likelihood of preserving the joke?

## Architecture Onboarding

- Component map:
  Input (English pun) -> Stage 1 (Generator + Discriminator) -> Stage 2 (Retriever) -> Stage 3 (Refiner) -> Output (French pun)

- Critical path:
  1. Identify pun word/type in English
  2. Translate meanings and search the Phonetic-Semantic Vector Space for French bridge words
  3. Generate with constraints -> Discriminator Check -> Multi-Agent Refinement

- Design tradeoffs:
  - **Literalness vs. Humor**: The system explicitly sacrifices BLEU/BERTScore (ranking 41-49/51) to maximize human-rated humor (ranking 1-2/51)
  - **Complexity vs. Control**: The baseline (Stage 1 only) was fast but produced unrelated puns. The full pipeline (Stages 2+3) is slower/computationally heavier but ensures relevance

- Failure signatures:
  - **The "Hallucinated Pun"**: The model generates a French sentence that is funny but has no relation to the English source meaning
  - **The "Safe Translation"**: The output is a perfect literal translation but contains no wordplay
  - **Empty Candidate List**: The phonetic-semantic search returns no words with >0.75 similarity, causing the pipeline to fall back to generic generation

- First 3 experiments:
  1. **Discriminator Ablation**: Run the pipeline without the contrastive learning feedback loop to see if the model still produces puns
  2. **Embedding Threshold Sensitivity**: Lower the cosine similarity threshold in the phonetic-semantic search (e.g., from 0.75 to 0.60) and measure if human evaluators find the resulting puns more or less creative/relevant
  3. **Agent Ablation**: Remove the "Emotion" or "Authenticity" evaluator from the multi-agent loop to determine which specific agent contributes most to the high human scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would combining the guided chain-of-thought pipeline (Part 2) with the multi-agent evaluation framework (Part 3) outperform either approach alone?
- Basis: The authors state: "We believe that combining our approaches from Parts 2 and 3 would produce results that would outperform both of them."
- Why unresolved: The experiments used baseline outputs as inputs to Part 3, while Part 2 was evaluated only with the simple contrastive learning discriminator. The two improved approaches were never combined.
- What evidence would resolve it: Running the guided chain-of-thought pipeline outputs through the multi-agent evaluation loop and comparing human evaluation scores against the current best submissions.

### Open Question 2
- Question: Would implementing additional levels of Low's polygonal algorithm (hexagon and beyond) significantly improve homonym candidate discovery?
- Basis: The authors implemented only the pentagon level and state: "Our results in Part 2 demonstrated the need to implement additional levels of Low's polygonal algorithm... we are confident that better homonyms could be found with additional levels."
- Why unresolved: In most cases, no candidate meeting the phonetic-semantic similarity thresholds was found. The algorithm was truncated at pentagon level due to time/constraint tradeoffs.
- What evidence would resolve it: Implementing hexagon and higher levels of the polygonal search and measuring the increase in viable homonym candidates found.

### Open Question 3
- Question: Can the methodology generalize to language pairs beyond English-to-French, particularly for languages with different phonological structures?
- Basis: The paper exclusively addresses English-to-French translation. The phonetic embeddings relied on French-specific resources (Lexique database, PanPhon IPA mappings).
- Why unresolved: No experiments were conducted on other language pairs. Different phonological systems may affect the phonetic-semantic embedding approach effectiveness.
- What evidence would resolve it: Applying the same three-stage methodology to other language pairs (e.g., English-German, English-Japanese) and comparing human evaluation scores.

### Open Question 4
- Question: How should the approach handle sentences containing multiple puns, which the current implementation explicitly ignores?
- Basis: "Our implementation did not account for the presence of multiple puns in a given pun sentence... limited our approach to only one pun word per input."
- Why unresolved: The authors observed multiple puns in the dataset but constrained processing to single puns, potentially losing wordplay complexity.
- What evidence would resolve it: Extending the pipeline to detect and translate multiple puns per sentence, then evaluating whether humor preservation improves.

## Limitations
- Reliance on black-box LLM components (o4-mini, Gemini Flash/Pro) whose internal reasoning processes are not fully interpretable
- Poor BLEU/BERTScore rankings (41-49/51) reflect trade-off between literal translation quality and humor preservation
- 600-dimensional phonetic-semantic embedding space assumes continuous vectors can capture discrete articulatory features and semantic nuances
- Contrastive learning "destruction" method may not fully represent pun/non-pun boundaries in all contexts

## Confidence

**High Confidence** in the reported human evaluation results and their relative ranking among submissions, as these are objective competition outcomes.

**Medium Confidence** in the mechanism explanations, particularly regarding the effectiveness of the 600D embedding concatenation and the contrastive learning approach, as these rely on specific implementation details not fully detailed in the paper.

**Low Confidence** in the generalizability of results beyond the CLEF JOKER 2025 dataset, as the method was optimized specifically for English-to-French pun translation.

## Next Checks

1. **Discriminator Ablation Study**: Run the full pipeline without the contrastive learning discriminator to quantify how much the "gatekeeper" mechanism contributes to pun preservation versus creating unrelated humorous outputs.

2. **Phonetic-Semantic Threshold Sensitivity**: Systematically vary the cosine similarity threshold (0.60 to 0.90) in the embedding search and measure the trade-off between pun relevance and creativity in human evaluations.

3. **Cross-Lingual Generalization Test**: Apply the same three-stage approach to translate puns between different language pairs (e.g., Spanish to Italian) to assess whether the phonetic-semantic embedding concatenation strategy generalizes beyond English-French.