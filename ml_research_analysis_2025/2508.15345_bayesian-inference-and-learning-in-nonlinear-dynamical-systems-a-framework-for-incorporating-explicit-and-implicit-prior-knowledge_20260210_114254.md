---
ver: rpa2
title: 'Bayesian Inference and Learning in Nonlinear Dynamical Systems: A Framework
  for Incorporating Explicit and Implicit Prior Knowledge'
arxiv_id: '2508.15345'
source_url: https://arxiv.org/abs/2508.15345
tags:
- learning
- system
- prior
- knowledge
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning unknown nonlinear
  dynamics within partially known system models using limited input-output data. The
  proposed framework integrates explicit system equations with flexible, learning-based
  approximations of unknown components via an interface variable.
---

# Bayesian Inference and Learning in Nonlinear Dynamical Systems: A Framework for Incorporating Explicit and Implicit Prior Knowledge

## Quick Facts
- **arXiv ID:** 2508.15345
- **Source URL:** https://arxiv.org/abs/2508.15345
- **Reference count:** 40
- **Primary result:** A framework for learning unknown nonlinear dynamics within partially known models using limited data, validated on three case studies including an experimental electro-mechanical system.

## Executive Summary
This paper addresses the challenge of learning unknown nonlinear dynamics within partially known system models using limited input-output data. The proposed framework integrates explicit system equations with flexible, learning-based approximations of unknown components via an interface variable. Prior knowledge, including smoothness assumptions, is incorporated through a basis function expansion coupled with conjugate Bayesian priors. Efficient online and offline inference and learning are enabled by deriving closed-form parameter densities and leveraging marginalized Sequential Monte Carlo methods. The approach avoids complex model inversions and is validated across three case studies, including an experimental electro-mechanical positioning system, demonstrating accurate state estimation and function learning with improved performance over existing methods.

## Method Summary
The method learns unknown dynamics nested within known physics by parameterizing the unknown function as a basis function expansion (Aφ(x) + ε) that feeds into the known state transition via an interface variable. A Matrix-Normal Inverse-Wishart (MN-IW) prior enables closed-form recursive updates of parameter posteriors through sufficient statistics, avoiding MCMC sampling of parameters. Marginalized Particle Filtering integrates out parameters analytically, reducing variance compared to standard particle filtering. The framework supports both online learning with a forgetting factor and offline learning via Particle Gibbs with Ancestor Sampling.

## Key Results
- The framework successfully learns unknown friction dynamics in an experimental electro-mechanical positioning system, achieving accurate state estimation and function learning.
- Online learning with the proposed method shows improved performance over standard particle filtering approaches in terms of state RMSE and learned function accuracy.
- The method handles systems where the unknown dynamics are nested inside known state equations without requiring analytical inversion of the known model components.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework enables learning of unknown dynamics nested within known physics without requiring analytical inversion of the known model components.
- **Mechanism:** An interface variable ξₜ decouples the unknown function Ξ(xₜ) from the known state transition f(xₜ, ξₜ). Instead of learning Ξ directly inside the nonlinearity, the method learns a linear basis expansion ξₜ = Aφ(xₜ) + εₜ that feeds into the known dynamics. This transforms a difficult inverse problem into a tractable forward filtering problem.
- **Core assumption:** The unknown dynamics can be sufficiently approximated by a linear combination of selected basis functions, and the resulting interface variable interacts additively or predictably with the known state transition function.
- **Evidence anchors:**
  - [abstract]: "...learning functions that are nested inside known system equations... avoids complex model inversions."
  - [section 3.1]: "We parameterize the approximation... as a basis function expansion... explicit model knowledge f and h is merged with the approximation... using the interface variable."
  - [corpus]: Related work in Universal Learning of Nonlinear Dynamics supports spectral/basis approaches for stability but does not verify the specific interface decoupling strategy used here.
- **Break condition:** If the unknown dynamics cannot be represented as a separate input signal to f (e.g., if the unknown parameters multiply state variables directly inside f in a non-separable way), this specific interface mechanism fails.

### Mechanism 2
- **Claim:** Analytical tractability and data efficiency are achieved by exploiting conjugate priors to maintain a closed-form parameter density.
- **Mechanism:** By placing a Matrix-Normal Inverse-Wishart (MN-IW) prior on the weights A and covariance Σε of the basis expansion, the parameter posterior updates become additive. Sufficient statistics (sums of φ(x)φ(x)ᵀ and φ(x)ξᵀ) are updated recursively, avoiding the need for MCMC sampling of the parameters themselves.
- **Core assumption:** The noise εₜ is Gaussian and the basis functions φ(xₜ) are fixed (non-adaptive) during inference.
- **Evidence anchors:**
  - [abstract]: "...incorporated through a basis function expansion coupled with conjugate Bayesian priors. Efficient... inference... enabled by deriving closed-form parameter densities."
  - [section 4.1]: "Due to the choice of a conjugate prior, the parameter posterior can be calculated recursively in closed form by summation..."
  - [corpus]: Probabilistic Digital Twin... discusses latent force modeling with GPs, which conceptually aligns with handling uncertainty, but doesn't confirm the MN-IW efficiency.
- **Break condition:** If the likelihood of the interface variable is non-Gaussian (e.g., heavy-tailed noise or discrete events), the conjugacy breaks, necessitating approximations.

### Mechanism 3
- **Claim:** The framework reduces the variance of the state estimates compared to standard particle filtering by marginalizing out the parameters.
- **Mechanism:** The method utilizes Rao-Blackwellization (Marginalized Particle Filter). Instead of sampling both states x and parameters θ (which increases dimensionality and variance), particles only sample the state trajectory. The parameters θ are integrated out analytically. The predictive density for ξₜ becomes a multivariate Student-t distribution (Theorem 2) derived from the MN-IW posterior.
- **Core assumption:** The system is conditionally linear in the parameters given the state trajectory.
- **Evidence anchors:**
  - [abstract]: "...enabled by deriving closed-form parameter densities and leveraging marginalized Sequential Monte Carlo methods."
  - [section 4.2]: "For marginalization of the parameters... expand the likelihood... and integrate out the parameters particle-based... predictive density p(ξₜ|xₜ, ηₜ) is a matrixvariate Student-t."
  - [corpus]: Flow-based Bayesian filtering suggests alternative methods for high-dimensional non-Gaussian cases, implying this method's efficiency relies heavily on the validity of the linear/Gaussian assumptions.
- **Break condition:** If the dimensionality of the basis functions (nφ) becomes too large relative to the data, numerical instability in the sufficient statistics inversion may degrade performance.

## Foundational Learning

- **Concept:** Conjugate Priors (Exponential Families)
  - **Why needed here:** The core efficiency claim rests on updating beliefs about A and Σε without sampling. You must understand that a Gaussian likelihood + MN-IW Prior = MN-IW Posterior.
  - **Quick check question:** If I change the basis function output noise to a Laplace distribution, do I still get a closed-form update? (Answer: No).

- **Concept:** Basis Function Expansion (Hilbert Space Approximation)
  - **Why needed here:** The method approximates unknown nonlinear functions Ξ(x) using a finite set of fixed features (e.g., sines/cosines from a GP approximation). You need to grasp how these features span the function space.
  - **Quick check question:** Can the model learn a function that oscillates faster than the highest frequency basis function provided? (Answer: No, it will suffer from spectral bias).

- **Concept:** Sequential Monte Carlo (Particle Filtering)
  - **Why needed here:** The state xₜ is non-linear and non-Gaussian. The method relies on particles (weighted samples) to propagate belief over time.
  - **Quick check question:** What happens to the particle weights if the process noise Σω is underestimated significantly? (Answer: Weight degeneracy/impoverishment; all weights go to zero except one).

## Architecture Onboarding

- **Component map:** System Model (f, h) -> Interface Layer (A, φ(x), ξ) -> Inference Core (Marginalized PF) -> Statistics Accumulator (χ)
- **Critical path:**
  1. **Initialization:** Define physics f, select basis functions φ, set prior statistics η₀.
  2. **Propagation (Time Update):** Advance particles xₜ₋₁ → xₜ using physics f and the mean of the interface variable ξ.
  3. **Auxiliary Weighting:** Check consistency of predicted state with measurement yₜ.
  4. **Interface Sampling:** Draw ξₜ from the Student-t predictive distribution (Theorem 2).
  5. **Update:** Weight particles using measurement likelihood and update sufficient statistics χ.
- **Design tradeoffs:**
  - **Expressiveness vs. Observability:** Increasing basis functions (nφ) allows modeling complex unknown functions but risks unidentifiability (learning distinct parameters becomes impossible) and increases computational cost.
  - **Forgetting Factor (γ):** A lower γ allows faster adaptation to changing dynamics (online learning) but increases variance/noise in the parameter estimates. Setting γ=1 assumes time-invariant dynamics.
- **Failure signatures:**
  - **Particle Collapse:** RMSE stops decreasing; uncertainty bounds widen excessively. Likely caused by too few particles or mis-specified process noise Σω.
  - **Parameter Drift:** The learned function Ξ looks like white noise. Likely caused by an overly small forgetting factor or poor initialization of the prior covariance.
  - **Numerical Underflow:** Statistics matrix χ₁ becomes singular. Caused by insufficient exploration of the state space.
- **First 3 experiments:**
  1. **Linear Sanity Check:** Apply to a system where Ξ(x) is known to be linear (e.g., kx). Use linear basis functions. Verify the posterior mean of A converges to the true k.
  2. **Noise Robustness:** Inject varying levels of measurement noise eₜ. Plot the wRMSE of the learned function vs. noise level to determine the data efficiency limit.
  3. **Drift Detection:** Introduce a sudden change in the unknown function (e.g., change friction coefficient at t=T/2). Test different forgetting factors γ to see how quickly the online algorithm detects the shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed learning-based representation be extended to enforce or incorporate more general implicit physical knowledge, such as energy conservation laws or Hamiltonian structures?
- Basis in paper: [explicit] The conclusion states that "a potentially fruitful research direction is to incorporate more general implicit knowledge, e.g., energy conservation laws, in the learning-based representation."
- Why unresolved: The current framework incorporates implicit knowledge primarily through smoothness assumptions (via basis function kernels) or symmetry, but it does not yet support enforcing hard constraints like energy conservation which require specific structural formulations.
- What evidence would resolve it: A demonstration of the framework learning dynamics where the posterior distribution strictly adheres to conservation laws (e.g., constant Hamiltonian) without these properties being explicitly encoded in the known state equations f.

### Open Question 2
- Question: Can the framework be extended to learn or adapt the basis functions φ(xₜ) and kernel hyperparameters (e.g., length-scales, variances) online, rather than requiring manual tuning or offline preconditioning?
- Basis in paper: [inferred] Section 6.1 and 6.3 state that basis functions can be defined by expert knowledge or offline data-driven approaches and that "covariance kernel hyperparameters are tuned manually" for the experiments.
- Why unresolved: The current implementation assumes a fixed set of basis functions and hyperparameters defined prior to inference. The authors note that a poor choice requires expert intervention or offline conditioning, limiting the "general-purpose" utility in entirely unknown environments.
- What evidence would resolve it: An extension of the marginalized particle filter that includes a recursive update mechanism for the hyperparameters θhyp (e.g., via gradient ascent or an additional Gibbs sampling step) that shows convergence comparable to manual tuning.

### Open Question 3
- Question: How can the framework handle the identification of noise covariances (Σω, Σe) which are currently assumed to be known?
- Basis in paper: [inferred] The problem formulation in Section 2 explicitly lists the noise terms with "known covariance matrices," and the initialization in Section 6.3 assumes "zero-mean Gaussian process and measurement noise" without detailing a learning mechanism for these parameters.
- Why unresolved: The derivations of the closed-form parameter densities (Theorems 1 and 3) rely on the conjugacy of the weights A given the noise covariances. Treating the covariances as unknown variables would break the current conjugate structure, requiring a different inference approach (e.g., Particle MCMC for covariances).
- What evidence would resolve it: An algorithmic modification that successfully marginalizes or estimates Σω and Σe alongside the system weights A in the online setting, validated by accurate recovery of true noise levels in a simulation.

## Limitations

- The framework relies heavily on the basis function expansion's ability to accurately represent unknown dynamics, inheriting spectral bias limitations.
- The linear-in-parameters assumption is critical for analytical tractability; significant nonlinearity in parameters would break closed-form updates.
- The method assumes Gaussian noise and known noise covariances, which may not hold in real-world systems.

## Confidence

- **High Confidence:** The core mechanism of decoupling known physics from unknown dynamics via the interface variable is sound and well-supported by the mathematical derivations in sections 3.1 and 4.2.
- **Medium Confidence:** The efficiency claims regarding the closed-form updates and marginalized particle filtering are credible given the conjugate prior structure, but full validation would require extensive testing across diverse system classes beyond the three case studies presented.
- **Low Confidence:** The robustness of the method to significant model misspecification (e.g., if the unknown dynamics have a structure incompatible with the chosen basis) is not thoroughly explored. The sensitivity to hyperparameters like the forgetting factor and prior covariances also requires more systematic investigation.

## Next Checks

1. **Generalization Test:** Apply the framework to a system where the unknown function has a significantly different structure (e.g., a discontinuous function or one with sharp transitions) compared to the training data used in the paper. Measure the degradation in state estimation and function learning accuracy.

2. **Noise Sensitivity Analysis:** Systematically vary the process and measurement noise levels (including non-Gaussian noise) to determine the upper bounds of the framework's robustness and identify the breaking point where the closed-form updates or marginalized filter assumptions fail.

3. **Basis Function Stress Test:** Intentionally use an insufficient number of basis functions or a poorly chosen basis (e.g., using only low-frequency sines for a high-frequency signal) to quantify the impact of the basis function approximation error on the overall system identification performance.