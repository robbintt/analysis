---
ver: rpa2
title: 'Composer: A Search Framework for Hybrid Neural Architecture Design'
arxiv_id: '2510.00379'
source_url: https://arxiv.org/abs/2510.00379
tags:
- search
- scale
- hybrid
- size
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Composer, an automatic framework for discovering
  hybrid neural architectures that combine computational primitives like Attention
  and MLP. Composer uses small-scale Bayesian optimization to explore architectural
  design spaces and extrapolates promising candidates to large scale.
---

# Composer: A Search Framework for Hybrid Neural Architecture Design

## Quick Facts
- **arXiv ID:** 2510.00379
- **Source URL:** https://arxiv.org/abs/2510.00379
- **Reference count:** 40
- **Primary result:** Discovered Composite architectures reduce validation loss by 0.05-0.08 and improve downstream task accuracy by 1.1-3.1% compared to Llama 3.2 while improving efficiency

## Executive Summary
This paper introduces Composer, an automatic framework for discovering hybrid neural architectures that combine computational primitives like Attention and MLP. Composer uses small-scale Bayesian optimization to efficiently explore architectural design spaces, then extrapolates promising candidates to large scale. The framework addresses the challenge of scaling down hybrid architectures for search, demonstrating that proportional width-to-depth scaling preserves architectural inductive biases better than depth-only scaling. Composite architectures discovered by Composer achieve significant quality and efficiency improvements over standard Transformers across model sizes from 350M to 3B parameters.

## Method Summary
Composer is a framework for automated hybrid neural architecture design that combines small-scale Bayesian optimization with extrapolation techniques. The search engine generates candidates using Bayesian optimization over layer arrangements, while the evaluator trains small models on a synthetic MAD dataset to score candidates efficiently. The aggregator uses N0 clustering to select the dominant layer type for each position across top candidates. Finally, the extrapolator scales the small architecture to target size using either stacking (for shallow searches) or stretching (for deeper searches) while maintaining relative performance gains. The framework addresses the key challenge of scaling down hybrid architectures by simultaneously reducing width and depth, which preserves architectural inductive biases and enables efficient search.

## Key Results
- Composite architectures reduce validation loss by 0.05-0.08 compared to Llama 3.2
- Improve downstream task accuracy by 1.1-3.1% on average across six benchmarks
- Increase training throughput by 1.25× while reducing KV cache size by 1.69×
- Decrease inference latency by 1.33× through reduced attention layer count

## Why This Works (Mechanism)

### Mechanism 1: Width-to-Depth Scaling for Search
Scaling down model width and depth simultaneously preserves architectural inductive biases better than scaling depth alone, enabling efficient small-scale search. The framework reduces the search space by training small models with proportional width-to-depth ratios. Without width scaling, the model becomes too wide and shallow, causing optimal architectures at small scale to diverge from large-scale optima. The relative performance ranking of hybrid architectures is invariant to scale when the width-to-depth ratio is roughly preserved.

### Mechanism 2: Extrapolation via Stretching
Architecture interleaving patterns discovered at small scales can be extrapolated to large scales via "Stretching" or "Stacking" without losing relative performance gains. Stretching proportionally expands contiguous layer groups (e.g., 2A+5M → 4A+10M), preserving transition points between primitives and allowing complex interleavings. This is robust for shallow searches but enables more creative patterns in deeper searches. The optimal ratio and transition points of computational primitives are continuous functions of model depth.

### Mechanism 3: Synthetic Proxy Datasets
Synthetic proxy datasets (MAD) are more effective for guiding small-scale search than sampled web-scale data. Web-scale datasets scaled down via Chinchilla laws yield noisy performance signals for architecture search. Synthetic datasets with token-manipulation tasks are learnable by small models and probe specific capabilities that correlate with large-scale pre-training success. Token-manipulation capabilities in small models serve as a proxy for general language modeling capabilities in large models.

## Foundational Learning

**Concept: Bayesian Optimization (BO)**
- **Why needed here:** Used in the Search Engine to navigate the discrete, exponential design space (2^N) of layer arrangements efficiently.
- **Quick check question:** Why does the paper prefer BO over Reinforcement Learning (RL) for this specific search? (Answer: Sample efficiency and uncertainty modeling).

**Concept: Hybrid Architectures (Attention & MLP)**
- **Why needed here:** The primitives being arranged. Understanding the role of Attention (context mixing) vs. MLP (feature processing) helps interpret the resulting 1:2 ratio.
- **Quick check question:** What specific computational primitives does Composer search over in this paper? (Answer: Grouped Query Attention and SwiGLU MLP).

**Concept: Chinchilla Scaling Laws**
- **Why needed here:** To understand the baseline methodology for scaling down models, which Composer argues is insufficient for architecture search.
- **Quick check question:** Why does the paper reject the strict application of Chinchilla laws for the search phase? (Answer: Small models trained on small data do not reflect large-scale hybrid performance).

## Architecture Onboarding

**Component map:** Search Engine -> Evaluator -> Aggregator -> Extrapolator

**Critical path:** Defining the search space (number of layers n) → Selecting the dataset (MAD) → Running BO → Aggregating results → Stretching/Stacking to 1B+

**Design tradeoffs:**
- *Search Depth vs. Creativity:* Searching 6 layers is cheap and robust to stack; searching 16 layers is costly but allows stretching, which finds superior interleavings (1:2 ratio).
- *Aggregation Strategy:* N0 (independent per layer) smooths noise better than N_{i-1} (prefix-based), which can overfit to specific small-scale artifacts.

**Failure signatures:**
- **High Search Cost, Low Quality:** Using DCLM (web-scale) data instead of MAD for the evaluator.
- **Suboptimal Ratios:** Searching without width scaling, leading to 1:1 ratios instead of the optimal 1:2 Attention-to-MLP ratio.
- **Homogeneous Blocks:** Stretching a shallow search (e.g., 4 layers), resulting in monolithic Attention or MLP segments.

**First 3 experiments:**
1. **Reproduce Small-Scale Search:** Run a 6-layer One-Shot Search on the MAD dataset with width scaling to verify the 2A + 4M pattern emergence.
2. **Validate Extrapolation:** Take the 2A+4M pattern and "Stack" it to 1B parameters. Pre-train on a small slice of DCLM and compare loss against a standard Llama 3.2 1B baseline.
3. **Ablate Aggregation:** Compare the validation loss of the N0 aggregated architecture vs. the "p100" (single best) architecture at 1B scale to verify the noise-smoothing hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Do Composer's extrapolation techniques (stacking and stretching) maintain performance advantages when scaling to tens or hundreds of billions of parameters?
- **Basis:** Appendix E.1 states, "further investigation is required to validate that Composer's extrapolation techniques scale to larger model sizes (e.g., tens to hundreds of billions of parameters)."
- **Why unresolved:** The paper validates the framework only up to 3B parameters; it is unconfirmed if the discovered architectures hold their efficiency and quality advantages at frontier model scales.
- **Evidence:** Applying the Composer framework to train models at 70B+ scale and comparing validation loss and downstream accuracy against baselines like Llama 3.1 70B.

### Open Question 2
**Question:** How do Composite LLMs perform on tasks requiring extended context or complex reasoning compared to standard Transformers?
- **Basis:** Appendix A notes, "Further work remains to understand Composer's efficacy for settings that require extended contexts or complex reasoning."
- **Why unresolved:** The current evaluation focuses on standard downstream tasks (PIQA, HellaSwag) and validation loss, leaving specific performance on long-context or reasoning benchmarks unknown.
- **Evidence:** Benchmarking the discovered Composite architectures against Llama baselines on long-context benchmarks (e.g., LongBench) and reasoning benchmarks (e.g., GSM8K).

### Open Question 3
**Question:** What specific proxy dataset designs are most effective for probing long-context and reasoning capabilities during small-scale search?
- **Basis:** Appendix A asks, "Determining the best approach for dataset design and model evaluation in these domains remains an open question," noting that sampled web-scale datasets are ineffective.
- **Why unresolved:** Existing synthetic datasets (MAD) are designed for general token manipulation, but the authors suspect different proxies are needed to predict reasoning or long-context performance at scale.
- **Evidence:** Creating new small-scale synthetic datasets targeting reasoning primitives and measuring the correlation between search results on these datasets and large-scale reasoning task performance.

### Open Question 4
**Question:** Can Composer successfully integrate heterogeneous computational primitives such as Mamba, Mixture-of-Experts (MoE), or Sliding Window Attention?
- **Basis:** Section 5 states, "Composer's efficient and extensible framework opens up possibilities to incorporate different computational primitives – such as Mamba, MoE, Sliding Window Attention."
- **Why unresolved:** The presented framework and experiments restrict the search space to only Attention and MLP layers.
- **Evidence:** Expanding the search space definition P to include these primitives and evaluating the quality and efficiency of the resulting large-scale hybrid architectures.

## Limitations
- Framework validated only up to 3B parameters; scalability to tens or hundreds of billions of parameters remains unconfirmed
- Performance on extended context and complex reasoning tasks is not yet evaluated
- Specific proxy dataset designs for reasoning capabilities remain an open question

## Confidence
- **High Confidence:** Claims about improved validation loss (0.05-0.08 reduction) and downstream task accuracy (1.1-3.1% improvement) at large scale, as these are directly measured and reported with specific architectures.
- **Medium Confidence:** Claims about training throughput (1.25×), KV cache reduction (1.69×), and inference latency improvements (1.33×), as these depend on specific hardware configurations and implementation details not fully disclosed.
- **Medium Confidence:** Claims about the 1:2 Attention-to-MLP ratio being optimal, as this emerges from specific search conditions (16-layer depth with width scaling) that may not generalize.

## Next Checks
1. **Transferability Test:** Run the small-scale search (6-layer, MAD dataset) without width scaling to verify that it converges to the suboptimal 1:1 ratio, confirming the paper's observation about the width-scaling trap.

2. **Extrapolation Robustness:** Take a 16-layer architecture discovered with stretching and systematically test both stacking and stretching extrapolation methods at 1B scale to verify that stretching consistently outperforms stacking as claimed.

3. **Proxy Task Correlation:** Train a small model (400K params) on both MAD and DCLM datasets, then evaluate their architectural rankings on a small slice of actual web data to measure the correlation between synthetic and real performance signals.