---
ver: rpa2
title: 'First Attentions Last: Better Exploiting First Attentions for Efficient Transformer
  Training'
arxiv_id: '2510.14614'
source_url: https://arxiv.org/abs/2510.14614
tags:
- first
- attention
- training
- output
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces FAL (First Attentions Last), a transformer\
  \ architecture that improves training efficiency by eliminating per-block MHA-MLP\
  \ connections and redirecting the first MHA output to subsequent layers. This reconfiguration\
  \ removes all-reduce communication in tensor parallelism and enables parallel execution\
  \ of MHA and MLP, achieving up to 44% faster multi-GPU training and 1.18\xD7 faster\
  \ single-GPU throughput."
---

# First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training

## Quick Facts
- **arXiv ID**: 2510.14614
- **Source URL**: https://arxiv.org/abs/2510.14614
- **Authors**: Gyudong Kim; Hyukju Na; Jin Hyeon Kim; Hyunsung Jang; Jaemin Park; Jaegi Hwang; Namkoo Ha; Seungryong Kim; Young Geun Kim
- **Reference count**: 40
- **Primary result**: Introduces FAL, a transformer architecture improving training efficiency by eliminating per-block MHA-MLP connections and redirecting the first MHA output, achieving up to 44% faster multi-GPU training and 1.18× single-GPU throughput.

## Executive Summary
The paper introduces FAL (First Attentions Last), a transformer architecture that improves training efficiency by eliminating per-block MHA-MLP connections and redirecting the first MHA output to subsequent layers. This reconfiguration removes all-reduce communication in tensor parallelism and enables parallel execution of MHA and MLP, achieving up to 44% faster multi-GPU training and 1.18× faster single-GPU throughput. FAL+ further improves model quality by augmenting MHA-MLP connections with the first attention output, achieving lower perplexity without increasing training time. The approach is validated across multiple model scales, tasks, and transformer variants, demonstrating robust efficiency gains and improved model quality through effective reuse of the first attention signal.

## Method Summary
FAL restructures the transformer by removing all-reduce communication between each MHA-MLP block and instead propagates the first attention output through all subsequent layers. This eliminates per-block synchronization overhead in tensor parallelism and allows MHA and MLP to execute in parallel. The key insight is that the first attention layer's output can serve as a global signal for all following layers, reducing redundant communication. FAL+ further enhances this by adding a connection from the first attention output to each MHA-MLP block, improving model quality (perplexity) without sacrificing the training speed benefits.

## Key Results
- FAL achieves up to 44% faster multi-GPU training and 1.18× single-GPU throughput by removing per-block MHA-MLP connections and enabling parallel MHA-MLP execution.
- FAL+ improves model quality, achieving lower perplexity on enwik8 and WikiText-103 without increasing training time by augmenting MHA-MLP connections with the first attention output.
- The approach demonstrates robust efficiency gains across multiple model scales, tasks, and transformer variants, with consistent speedups and quality improvements.

## Why This Works (Mechanism)
FAL works by eliminating the all-reduce communication that typically occurs after each MHA-MLP block in tensor-parallel transformer training. By propagating the first attention output through all subsequent layers, FAL removes the need for per-block synchronization, allowing MHA and MLP to execute in parallel. This reduces communication overhead and improves hardware utilization. FAL+ further enhances model quality by providing each block with access to the global attention signal from the first layer, effectively enriching the representation without additional computational cost.

## Foundational Learning

**Attention Mechanism**: Self-attention allows each token to attend to all others in the sequence, capturing long-range dependencies. Why needed: Core to transformer's ability to model complex relationships. Quick check: Verify attention weights capture expected dependencies.

**Tensor Parallelism**: Distributes transformer layers across multiple GPUs to handle large models. Why needed: Enables scaling beyond single GPU memory limits. Quick check: Confirm all-reduce communication patterns are eliminated.

**Layer-wise Communication**: Standard transformers communicate after each MHA-MLP block, creating synchronization bottlenecks. Why needed: Understanding the communication overhead FAL aims to eliminate. Quick check: Measure reduction in all-reduce operations.

## Architecture Onboarding

**Component Map**: Input -> First MHA -> (Parallel MHA/MLP blocks) -> Output, with FAL+ adding connections from First MHA to each block.

**Critical Path**: The first attention output is the critical path, as it's reused by all subsequent layers. Optimizing its computation and propagation is key to FAL's efficiency.

**Design Tradeoffs**: FAL trades the fine-grained communication of standard transformers for global signal reuse. This reduces synchronization overhead but may affect gradient flow or representation diversity. FAL+ mitigates quality loss by reconnecting the first attention output to each block.

**Failure Signatures**: If FAL degrades model quality, it may indicate insufficient signal propagation or gradient issues due to the altered communication pattern. Monitoring perplexity and convergence is essential.

**3 First Experiments**:
1. Compare training throughput (samples/second) between standard transformer and FAL on 2-8 GPUs.
2. Measure all-reduce communication volume and frequency with and without FAL.
3. Evaluate perplexity on a held-out validation set for FAL vs. standard transformer on a small-scale language model.

## Open Questions the Paper Calls Out
None explicitly called out.

## Limitations

- The architectural changes may affect model quality or generalization in ways not fully explored, particularly on tasks beyond autoregressive language modeling.
- The experimental scope is narrow, focusing on specific benchmarks and model scales, with limited coverage of extreme scaling scenarios or diverse downstream tasks.
- The impact of removing per-block MHA-MLP connections versus reusing the first attention output is not fully isolated, making it difficult to attribute gains.

## Confidence

- **High confidence**: Claims about training speedup (up to 44%) and reduced all-reduce communication are well-supported by experimental results across multiple benchmarks and model sizes.
- **Medium confidence**: The assertion that FAL+ improves model quality (lower perplexity) is supported by presented results, but the effect may be task-dependent and requires further validation.
- **Low confidence**: The claim that FAL generalizes robustly across all transformer variants and scales is not fully substantiated, as the evaluation does not cover all possible architectures or extreme scaling scenarios.

## Next Checks

1. Evaluate FAL and FAL+ on a broader suite of NLP and vision tasks, including summarization, question answering, object detection, and fine-tuning scenarios, to assess whether quality improvements generalize beyond perplexity-based metrics.

2. Conduct experiments with FAL on larger GPU clusters (e.g., 32 or more GPUs) to determine if the communication and efficiency benefits scale linearly, and analyze the impact on training stability and convergence at scale.

3. Perform controlled ablations to isolate the impact of removing per-block MHA-MLP connections versus the benefits of reusing the first attention output, clarifying which component drives the primary efficiency and quality gains.