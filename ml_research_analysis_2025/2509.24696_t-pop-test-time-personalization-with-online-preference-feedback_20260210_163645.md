---
ver: rpa2
title: 'T-POP: Test-Time Personalization with Online Preference Feedback'
arxiv_id: '2509.24696'
source_url: https://arxiv.org/abs/2509.24696
tags:
- user
- reward
- preference
- personalization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of personalizing large language
  models (LLMs) for individual users without requiring parameter fine-tuning or pre-collected
  user data. The authors introduce T-POP (Test-Time Personalization with Online Preference
  Feedback), a novel algorithm that combines test-time alignment with dueling bandits.
---

# T-POP: Test-Time Personalization with Online Preference Feedback

## Quick Facts
- **arXiv ID:** 2509.24696
- **Source URL:** https://arxiv.org/abs/2509.24696
- **Reference count:** 13
- **Primary result:** Achieves rapid personalization without fine-tuning using online pairwise feedback

## Executive Summary
This paper introduces T-POP, a novel approach for test-time personalization of large language models that operates without parameter fine-tuning or pre-collected user data. The method combines test-time alignment with dueling bandits to steer LLM decoding using a reward function learned online from pairwise user preference feedback. T-POP intelligently queries users to balance exploration of their preferences with exploitation of learned knowledge. Extensive experiments demonstrate that T-POP achieves rapid, data-efficient personalization, significantly outperforming existing baselines across multiple datasets and preference attributes.

## Method Summary
T-POP personalizes a frozen LLM through online learning from pairwise preference feedback. The method generates two response candidates token-by-token: one that exploits current reward knowledge and another that explores uncertain regions using a gradient-based uncertainty bonus. A lightweight MLP reward model is trained online on preference pairs using Bradley-Terry-Luce loss. The approach requires no fine-tuning of the base model and operates efficiently with minimal user feedback.

## Key Results
- Achieves significant improvements over existing baselines across multiple datasets and preference attributes
- Demonstrates rapid convergence, with reward scores stabilizing within 20 interactions
- Maintains high alignment with user preferences while showing strong improvement with more user interactions
- Effective in cold-start personalization scenarios without requiring pre-collected user data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting a learned reward signal into decoding steers a frozen model toward specific user preferences without weight updates
- **Mechanism:** At each generation step $p$, T-POP modifies token selection probability by adding weighted reward score to base model's log-probabilities ($\pi_{base}(v|y_{<p}) + \omega \cdot r([y_{<p}, v]; \theta)$)
- **Core assumption:** Base LLM's prior is sufficiently broad that linear combination with lightweight reward function can steer output without collapsing coherence
- **Evidence anchors:** Section 2 defines scoring function combining base likelihood with reward term; Section 3.3 describes greedy maximization based on this score
- **Break condition:** If $\omega$ is too high (e.g., 5.0), reward signal dominates causing "reward hacking" and incoherent text

### Mechanism 2
- **Claim:** Constructing specific "exploration" sequence using uncertainty bonus accelerates reward model learning compared to random sampling
- **Mechanism:** T-POP generates second sequence by maximizing score plus "Uncertainty Bonus" ($\|\nabla r(\cdot) - \nabla r(\cdot)\|_{V_{t-1}^{-1}}$), forcing generation in high-uncertainty regions
- **Core assumption:** Gradient of reward network serves as valid feature map for estimating epistemic uncertainty in dueling bandit setting
- **Evidence anchors:** Section 3.3 Equation (5) defines Uncertainty Bonus derived from neural dueling bandit theory
- **Break condition:** If exploration parameter $\nu$ is misaligned with reward scale, exploration sequence may become too distinct from exploitation sequence to yield comparable preference data

### Mechanism 3
- **Claim:** Lightweight neural network can rapidly capture user preferences from pairwise comparisons using online gradient descent
- **Mechanism:** Upon receiving preference $l_t$, system updates reward model $r(\cdot; \theta)$ by minimizing binary cross-entropy loss (Bradley-Terry-Luce model)
- **Core assumption:** User preferences are consistent enough to be modeled by static reward function during interaction window
- **Evidence anchors:** Section 3.2 details loss function $L_t(\theta)$ in Equation (2) used for online update; Figure 2 shows rapid convergence within first 20 iterations
- **Break condition:** If user preferences are inconsistent or contradictory, online gradient descent may fail to converge or oscillate

## Foundational Learning

- **Concept: Bradley-Terry-Luce (BTL) Model**
  - **Why needed here:** T-POP relies on BTL model to translate binary user feedback into scalar reward signal for training neural network
  - **Quick check question:** If user prefers Response A over Response B, how does BTL model mathematically relate rewards $r(A)$ and $r(B)$ to probability of this choice?

- **Concept: Upper Confidence Bound (UCB) / Exploration-Exploitation**
  - **Why needed here:** Algorithm uses "Dueling Bandit" strategy, specifically UCB-style bonus, to decide which tokens to generate to learn fastest
  - **Quick check question:** Why does adding "uncertainty bonus" to score of token encourage model to explore unknown areas of preference space?

- **Concept: Reward Hacking**
  - **Why needed here:** Paper explicitly identifies this risk where model generates high-reward gibberish if steering weight is too high
  - **Quick check question:** In T-POP, what prevents learned reward model from assigning high scores to incoherent text sequences?

## Architecture Onboarding

- **Component map:**
  - User Query -> Dual Decoding (Exploit + Explore) -> Preference Feedback -> Reward Model Update -> Next Query

- **Critical path:**
  1. User Query $q_t$ enters
  2. **Dual Decoding:** Generate two responses token-by-token
     - Path A (Exploit): Maximize $\pi_{base} + \omega \cdot r$
     - Path B (Explore): Maximize $\pi_{base} + \omega \cdot r + \text{Uncertainty}$
  3. **Feedback:** User selects preferred response
  4. **Update:** Train Reward Model (MLP) on this new pair using AdamW
  5. **Next Query:** Use updated Reward Model

- **Design tradeoffs:**
  - **Reward Weight ($\omega$):** Hyperparameter sensitive spot. Too low = generic responses; Too high = reward hacking. Paper suggests $\omega=1.0$
  - **Candidate Set ($k$):** Restricting token selection to top-$k$ from base model reduces search space but may limit personalization "creativity"

- **Failure signatures:**
  - **Oscillating Preferences:** Win rates fluctuate wildly (potential noisy user feedback or high learning rate)
  - **Generic Output:** Exploration sequence looks identical to exploitation (Uncertainty bonus vanishes or $\nu$ too low)
  - **Semantic Drift:** Responses become grammatical but topically irrelevant (Reward model overfitting to short sequences)

- **First 3 experiments:**
  1. **Sanity Check (Overfit):** Run T-POP on single user with simple preference (e.g., "always output 'cat'"). Verify if model can force this output rapidly
  2. **Ablation on $\omega$:** Plot Reward Model Score vs. Human Evaluation Score for $\omega \in \{0.0, 0.5, 1.0, 2.0\}$ to verify "reward hacking" cliff
  3. **Exploration Efficacy:** Compare T-POP against version that generates Path B randomly (Random Search) to validate necessity of gradient-based Uncertainty Bonus

## Open Questions the Paper Calls Out
- Extending framework to handle long-term shifts in user preferences
- Extending framework to handle more complex feedback structures beyond binary pairwise comparisons
- Computational latency overhead of online neural network training loop during interactive decoding phase

## Limitations
- Assumes consistent user preferences, which may struggle with noisy or evolving preferences
- Scalability to longer conversations or more complex preference spaces remains untested
- Computational efficiency of uncertainty bonus calculation at every token step is uncertain

## Confidence
- **High Confidence:** Core mechanism of test-time alignment with dueling bandits is well-grounded in bandit theory; empirical results across multiple datasets show consistent improvements
- **Medium Confidence:** Data efficiency claims are compelling but primarily validated on English datasets; performance on nuanced preference attributes needs further exploration
- **Low Confidence:** Computational efficiency claims are uncertain; while reward model is lightweight, uncertainty bonus calculation could create significant latency

## Next Checks
1. **Robustness to Noisy Feedback:** Implement version with 20-30% random noise in user preferences and measure performance degradation
2. **Cross-Domain Transfer:** Train T-POP on preference data from one domain (e.g., creative writing) and test personalization in different domain (e.g., technical Q&A)
3. **Real-Time Performance Benchmark:** Measure end-to-end latency of T-POP on single GPU compared to standard decoding to validate practical usability