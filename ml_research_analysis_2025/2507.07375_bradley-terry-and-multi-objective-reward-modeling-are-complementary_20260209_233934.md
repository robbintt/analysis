---
ver: rpa2
title: Bradley-Terry and Multi-Objective Reward Modeling Are Complementary
arxiv_id: '2507.07375'
source_url: https://arxiv.org/abs/2507.07375
tags:
- reward
- multi-objective
- baseline
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking in RLHF, where the policy exploits
  imperfections in the reward model rather than learning the intended behavior. Existing
  methods primarily focus on in-distribution settings and often fail in more challenging
  out-of-distribution (OOD) scenarios.
---

# Bradley-Terry and Multi-Objective Reward Modeling Are Complementary

## Quick Facts
- **arXiv ID**: 2507.07375
- **Source URL**: https://arxiv.org/abs/2507.07375
- **Reference count**: 40
- **Primary result**: SMORM enables a 7B model to outperform a 70B baseline in multi-objective reward modeling

## Executive Summary
This paper addresses reward hacking in reinforcement learning from human feedback (RLHF), where policies exploit imperfections in reward models rather than learning intended behaviors. The authors propose SMORM, a unified framework that jointly trains Bradley-Terry single-objective and multi-objective regression-based reward functions using a shared embedding space. The approach leverages complementary benefits: regression tasks refine the embedding space to enhance robustness against reward hacking in out-of-distribution settings, while Bradley-Terry training improves the multi-objective head's scoring capability. The framework demonstrates significant improvements in both robustness and scoring performance, with the notable achievement of a 7B model outperforming a 70B baseline.

## Method Summary
SMORM introduces a unified reward modeling framework that jointly optimizes Bradley-Terry (BT) single-objective and multi-objective regression objectives within a shared embedding space. The architecture consists of a shared encoder that processes input data, followed by two distinct heads: one for BT pairwise comparisons and another for multi-objective regression. During training, the regression task refines the embedding space representation, which in turn enhances the BT head's robustness against reward hacking, particularly in out-of-distribution scenarios. Conversely, BT-based training improves the multi-objective head's ability to score different objectives effectively. This complementary training approach allows the model to capture both preference ranking capabilities and multi-dimensional reward estimation within a single framework.

## Key Results
- SMORM significantly improves both robustness against reward hacking and scoring performance in multi-objective reward modeling
- The framework enables a 7B model to outperform a 70B baseline model
- Theoretical analysis shows BT and regression objectives optimize different components, providing complementary benefits
- Extensive experiments demonstrate improved performance on LLM-as-judge benchmarks and human preference data

## Why This Works (Mechanism)
The complementary nature of Bradley-Terry and regression objectives creates a mutually reinforcing training dynamic. The regression task acts as a regularizer that refines the embedding space to be more discriminative and robust, which directly benefits the BT head's ability to handle out-of-distribution inputs where reward hacking typically occurs. Simultaneously, BT training provides structured pairwise comparison data that improves the multi-objective head's ability to rank and score different objectives. The shared embedding space ensures that improvements in representation quality benefit both objectives simultaneously, creating a virtuous cycle where each component strengthens the other. This architectural design addresses the fundamental challenge in reward modeling where single-objective approaches struggle with OOD robustness while multi-objective approaches may lack precise scoring capabilities.

## Foundational Learning

**Bradley-Terry Model**: A probabilistic model for pairwise comparisons that estimates the probability of one item being preferred over another based on latent strengths. *Why needed*: Provides the theoretical foundation for modeling preference rankings in reward modeling. *Quick check*: Verify that the BT head correctly computes pairwise comparison probabilities using the softmax of latent strength differences.

**Multi-Objective Regression**: Predicting multiple continuous reward dimensions simultaneously rather than a single scalar value. *Why needed*: Captures the complexity of real-world reward functions that often involve multiple competing objectives. *Quick check*: Confirm that the regression head outputs values in the appropriate ranges for each objective dimension.

**Reward Hacking**: The phenomenon where policies exploit loopholes or imperfections in the reward model to maximize reward without achieving the intended behavior. *Why needed*: The primary problem SMORM addresses, particularly in out-of-distribution scenarios. *Quick check*: Test whether the model shows unusual behavior when exposed to slightly modified or adversarial inputs.

**Shared Embedding Space**: A common representation layer that multiple task-specific heads can access and benefit from. *Why needed*: Enables knowledge transfer between the BT and regression tasks, creating the complementary training effect. *Quick check*: Analyze embedding similarity between in-distribution and OOD samples to verify improved robustness.

## Architecture Onboarding

**Component Map**: Input -> Shared Encoder -> BT Head (Pairwise Comparison) + Regression Head (Multi-Objective) -> Output

**Critical Path**: The most performance-critical components are the shared encoder (as it directly impacts both heads) and the BT head (which must handle OOD robustness). The regression head serves primarily as a regularizer to improve the embedding space quality.

**Design Tradeoffs**: The shared embedding space creates tight coupling between tasks, which provides complementary benefits but may limit individual head optimization. Alternative designs with separate embeddings or intermediate fusion layers could offer more flexibility but might lose the synergistic effects. The choice of balancing BT and regression loss weights during training is also critical for achieving optimal performance.

**Failure Signatures**: Poor OOD robustness indicates insufficient embedding refinement from the regression task. Weak multi-objective scoring suggests inadequate BT training or imbalance in the shared representation. Catastrophic forgetting of either task during joint training points to optimization instability or inappropriate loss weighting.

**First Experiments**:
1. Ablation study comparing SMORM against separate BT-only and regression-only models on the same data
2. OOD stress test with adversarial preference orderings designed to trigger reward hacking
3. Sensitivity analysis of loss weight hyperparameters to identify optimal training configuration

## Open Questions the Paper Calls Out

None

## Limitations

The theoretical analysis is limited to proving that BT and regression objectives optimize different components without establishing convergence guarantees or performance bounds. The empirical evaluation focuses primarily on LLM-as-judge benchmarks and human preference data but lacks validation on truly challenging OOD scenarios where reward hacking would be most severe. The claim that SMORM enables a 7B model to outperform a 70B baseline needs more rigorous statistical analysis to rule out random variation or benchmark-specific effects.

## Confidence

**High confidence**: The experimental results showing improved preference ranking and human preference satisfaction are convincing, given the controlled comparisons and established benchmark usage.

**Medium confidence**: The claim about robustness against reward hacking requires more scrutiny, as the evaluation metrics may not fully capture sophisticated hacking attempts, and the OOD generalization claims need more diverse test distributions.

**Low confidence**: The assertion that SMORM enables a 7B model to consistently outperform a 70B baseline across multiple tasks and settings, given the lack of statistical significance testing and potential confounding factors in the experimental design.

## Next Checks

1. Conduct a systematic ablation study on the shared embedding space design, testing alternative architectures (separate embeddings, intermediate fusion, etc.) to isolate whether the complementarity claims hold specifically for the proposed architecture or are more general.

2. Design and implement more challenging OOD test scenarios specifically engineered to expose reward hacking vulnerabilities, including adversarial preference orderings and out-of-distribution preference patterns that existing methods fail on.

3. Perform statistical power analysis on the 7B vs 70B comparison results, including multiple random seeds, confidence intervals, and effect size measurements to validate that the performance improvement is robust and not due to chance or benchmark-specific artifacts.