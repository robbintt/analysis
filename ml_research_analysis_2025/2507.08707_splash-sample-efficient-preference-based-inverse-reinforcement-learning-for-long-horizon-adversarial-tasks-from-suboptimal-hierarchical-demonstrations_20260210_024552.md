---
ver: rpa2
title: SPLASH! Sample-efficient Preference-based inverse reinforcement learning for
  Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations
arxiv_id: '2507.08707'
source_url: https://arxiv.org/abs/2507.08707
tags:
- learning
- reward
- demonstrations
- splash
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPLASH introduces a preference-based inverse reinforcement learning
  algorithm that enables learning from suboptimal demonstrations in long-horizon and
  adversarial tasks. The method addresses limitations of existing IRL approaches by
  incorporating options-level demonstrations, downsampled full trajectory comparisons,
  success and progress-based learning constraints, and temporal consistency regularization.
---

# SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations

## Quick Facts
- arXiv ID: 2507.08707
- Source URL: https://arxiv.org/abs/2507.08707
- Reference count: 38
- Primary result: Novel IRL algorithm for long-horizon tasks from suboptimal demonstrations with successful real-world transfer

## Executive Summary
SPLASH introduces a preference-based inverse reinforcement learning algorithm that enables learning from suboptimal demonstrations in long-horizon and adversarial tasks. The method addresses limitations of existing IRL approaches by incorporating options-level demonstrations, downsampled full trajectory comparisons, success and progress-based learning constraints, and temporal consistency regularization. The algorithm automatically generates ranked demonstrations through behavioral cloning with noise injection and leverages both task success criteria and progress tracking to improve reward learning.

Experimental results on a maritime capture-the-flag task show that SPLASH significantly outperforms the state-of-the-art in reward learning from suboptimal demonstrations, with demonstrated ability to extrapolate beyond demonstrator performance. The learned policies successfully transfer from simulation to real-world autonomous unmanned surface vehicles, validating the approach's practical applicability for complex robotic tasks.

## Method Summary
SPLASH is a novel preference-based inverse reinforcement learning algorithm designed to learn from suboptimal demonstrations in long-horizon, adversarial tasks. The method incorporates hierarchical demonstrations at the options level, enabling learning from incomplete trajectories while maintaining computational efficiency through downsampling of full trajectory comparisons. Key innovations include automatic generation of ranked demonstrations via behavioral cloning with noise injection, success-based learning constraints that capture task completion requirements, progress-based constraints that track intermediate goal achievement, and temporal consistency regularization that ensures reward predictions remain stable across time.

The algorithm processes demonstrations hierarchically, first learning options-level behaviors before integrating them into complete trajectories. By combining success and progress tracking with preference-based learning, SPLASH can extract meaningful reward signals even from imperfect demonstrations. The noise injection approach automatically creates a ranked set of demonstrations without requiring explicit human ranking, making the method more scalable and practical for real-world applications.

## Key Results
- SPLASH significantly outperforms state-of-the-art IRL methods in learning from suboptimal demonstrations on maritime capture-the-flag tasks
- The algorithm successfully transfers learned policies from simulation to real-world autonomous unmanned surface vehicles
- Demonstrated ability to extrapolate beyond demonstrator performance, achieving higher success rates than the original demonstrations
- Effective handling of long-horizon tasks (15-60 minute missions) with computational efficiency maintained through downsampling strategies

## Why This Works (Mechanism)
SPLASH works by combining multiple complementary learning constraints that address the fundamental challenges of IRL from suboptimal demonstrations. The hierarchical approach breaks down complex tasks into manageable options, while preference-based learning allows the algorithm to focus on relative quality rather than absolute reward values. The success and progress constraints provide structured guidance that helps the algorithm distinguish between truly suboptimal behaviors and those that are merely incomplete. Temporal consistency regularization ensures that learned rewards maintain coherence across time, preventing the model from overfitting to transient demonstration artifacts.

The automatic generation of ranked demonstrations through noise injection is particularly effective because it creates a natural curriculum of increasing difficulty without requiring manual intervention. By combining these elements with efficient downsampling strategies, SPLASH can handle the computational demands of long-horizon tasks while maintaining learning quality from imperfect demonstrations.

## Foundational Learning
**Inverse Reinforcement Learning**: Learning reward functions from expert demonstrations; needed because explicit reward specification is often difficult for complex tasks; quick check: can recover known rewards from synthetic demonstrations

**Hierarchical Reinforcement Learning**: Decomposing tasks into sub-policies (options); needed to handle long-horizon tasks efficiently; quick check: options terminate appropriately and solve subtasks

**Preference-based Learning**: Learning from pairwise comparisons rather than absolute labels; needed when absolute reward values are ambiguous; quick check: consistent preferences emerge from similar demonstrations

**Behavioral Cloning with Noise**: Generating diverse demonstrations by adding noise to expert policies; needed to create ranked demonstration sets automatically; quick check: noise level correlates with performance degradation

**Temporal Consistency Regularization**: Ensuring reward predictions remain stable across time steps; needed to prevent overfitting to demonstration artifacts; quick check: reward predictions smooth across similar states

## Architecture Onboarding

Component Map:
Noisy Behavioral Cloning -> Ranked Demonstrations -> Preference Comparator -> Reward Network -> Policy Optimizer -> Real-World USV

Critical Path:
The critical learning loop follows: (1) Generate noisy demonstrations from expert policy, (2) Create preference pairs through comparison, (3) Train reward network with success/progress/temporal constraints, (4) Optimize policy using learned rewards, (5) Transfer to real-world platform.

Design Tradeoffs:
- Hierarchical vs flat demonstration processing: Hierarchy enables handling long-horizon tasks but adds complexity
- Full vs downsampled trajectory comparison: Downsampling improves efficiency but may miss fine-grained preferences
- Automatic vs manual ranking: Automatic ranking scales better but may introduce noise in the ranking

Failure Signatures:
- Poor performance on success-critical subtasks indicates insufficient success constraint weight
- Inconsistent policy behavior suggests inadequate temporal regularization
- Failure to improve beyond demonstrator performance points to insufficient preference diversity

First Experiments:
1. Verify noise injection creates meaningful performance gradients across demonstrations
2. Test preference comparator accuracy on synthetic ranked demonstrations
3. Validate success and progress constraints correctly identify task completion states

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the limitations discussed in the main text.

## Limitations
- Performance on non-maritime tasks remains untested, limiting generalizability claims
- Noise injection approach may not scale well to extremely complex or high-dimensional state spaces
- Computational overhead of maintaining and comparing multiple demonstration options is not thoroughly analyzed

## Confidence
High confidence: Core methodology effectiveness on tested maritime tasks
Medium confidence: Generalizability to other domains and task types
Medium confidence: Scalability to more complex scenarios

## Next Checks
1. Test SPLASH on non-maritime tasks (e.g., aerial robotics or industrial manipulation) to evaluate domain transferability
2. Conduct comprehensive computational complexity analysis comparing SPLASH to baseline methods
3. Perform ablation studies to quantify the contribution of each learning constraint to overall performance