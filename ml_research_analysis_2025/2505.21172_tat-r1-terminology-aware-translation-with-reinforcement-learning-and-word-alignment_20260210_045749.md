---
ver: rpa2
title: 'TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word
  Alignment'
arxiv_id: '2505.21172'
source_url: https://arxiv.org/abs/2505.21172
tags:
- translation
- alignment
- reward
- word
- terminology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving terminology translation
  accuracy in machine translation using reinforcement learning and word alignment.
  The authors propose TAT-R1, a terminology-aware translation model that leverages
  word alignment to design three types of rule-based rewards for reinforcement learning
  training.
---

# TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment

## Quick Facts
- arXiv ID: 2505.21172
- Source URL: https://arxiv.org/abs/2505.21172
- Reference count: 17
- Primary result: Terminology-aware translation model using RL and word alignment achieves 2.58% BLEU, 3.56% COMETKiwi, 1.05% XCOMET improvements on terminology test set

## Executive Summary
This paper addresses the challenge of improving terminology translation accuracy in machine translation by introducing TAT-R1, a terminology-aware translation model that leverages word alignment to design rule-based rewards for reinforcement learning. The approach extracts keyword translation pairs using SimAlign and incorporates format rewards, COMET rewards, and word alignment rewards into the training process. Experimental results show significant improvements in terminology translation accuracy while maintaining comparable performance on general translation tasks. The model demonstrates strong generalization to out-of-distribution language pairs compared to supervised fine-tuning approaches.

## Method Summary
TAT-R1 uses Qwen2.5-7B-Instruct as backbone and employs GRPO reinforcement learning with custom reward functions. The key innovation is using word alignment (via SimAlign) to extract noun-level terminology correspondences and compute three alignment-based rewards: Raaw (keyword overlap ratio), Raao (order preservation), and Rtaw (reasoning about terminology). These are combined with format rewards and COMET-22 semantic scores to train the model. The approach requires no terminology detection during inference and demonstrates superior performance on both terminology-specific and general translation tasks.

## Key Results
- Achieved 2.58% BLEU, 3.56% COMETKiwi, and 1.05% XCOMET improvements on terminology test set compared to RL without alignment rewards
- Showed 2% improvement in Terminology Accuracy (TA) metric
- Demonstrated strong generalization to out-of-distribution language pairs (EN→DE) where supervised fine-tuning catastrophically failed
- Ablation studies confirmed the effectiveness of alignment rewards over BLEU-based rewards

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Word alignment rewards provide targeted signal for terminology accuracy without constraining overall translation fluency.
- **Mechanism:** The system extracts noun-level alignment pairs between source-reference and source-prediction using SimAlign. Three rewards are computed: (1) Raaw measures keyword overlap ratio, normalized by output length to prevent gaming; (2) Raao captures order preservation of key terms; (3) Rtaw rewards reasoning about key terminology in the `` section. These signals guide RL to prioritize term accuracy while allowing syntactic flexibility.
- **Core assumption:** Nouns are the critical terminology carriers; alignment models can reliably extract cross-lingual correspondences.
- **Evidence anchors:**
  - [abstract]: "extract keyword translation pairs using a word alignment model and incorporate format rewards, COMET rewards, and word alignment rewards"
  - [section 2.1]: "we leverage word alignment to design three distinct reward mechanisms... retaining nouns as key elements requiring alignment"
  - [corpus]: Weak direct evidence—related work (DuTerm, DeepTrans) addresses terminology but uses different approaches (post-editing, literary translation focus).

### Mechanism 2
- **Claim:** Combining semantic (COMET) and lexical (word alignment) rewards yields better terminology accuracy than BLEU-based rewards while preserving fluency.
- **Mechanism:** The overall reward R_all = R_comet + α·Raaw + β·Raao + γ·Rtaw (when format is valid). COMET captures semantic adequacy; alignment rewards add terminology-specific pressure. BLEU rewards, by contrast, enforce strict n-gram matching that the authors show degrades fluency (Table 3: BLEU reward improves BLEU score but drops COMETKiwi from 77.51 to 75.83 on ZH→EN).
- **Core assumption:** Translation quality factorizes into semantic adequacy (COMET) plus localized term accuracy (alignment); these objectives do not conflict significantly.
- **Evidence anchors:**
  - [section 3.2.2]: "models trained with BLEU as the reward exhibit an apparent degradation in translation fluency"
  - [table 3]: Shows RL-Rcomet+RBLEU achieves higher BLEU (25.08) but lower COMETKiwi (75.83) vs. TAT-R1 (BLEU 24.40, COMETKiwi 77.20)
  - [corpus]: No direct corpus comparison of BLEU vs. alignment rewards for terminology tasks.

### Mechanism 3
- **Claim:** RL training with GRPO exhibits better generalization to out-of-distribution language pairs than supervised fine-tuning.
- **Mechanism:** GRPO samples G outputs per input, computes advantages by normalizing rewards within each group, and optimizes policy with clipping and KL penalty. The paper shows SFT on WMT data catastrophically fails on the RTT (EN→DE) test set (near-zero metrics), while RL-trained TAT-R1 maintains performance—suggesting RL learns more robust translation policies rather than overfitting to training distribution.
- **Core assumption:** The reward function captures task-relevant behavior better than reference imputation; group-relative advantage estimation provides stable gradients.
- **Evidence anchors:**
  - [section 3.2.2]: "On the terminology test set RTT, the fine-tuned model almost entirely mistranslates... the RL-trained TAT-R1 model improved across all metrics, demonstrating strong performance on the out-of-distribution (OOD) En->De task"
  - [figure 2]: Visual comparison showing SFT collapse vs. RL stability
  - [corpus]: Assumption: This generalization claim is not directly validated in neighboring corpus papers; generalization evidence is paper-specific.

## Foundational Learning

- **Concept: Word Alignment**
  - **Why needed here:** Core to extracting terminology correspondences from parallel corpora; enables reward computation without requiring explicit terminology annotations.
  - **Quick check question:** Given parallel sentences "The engine failed" / "Le moteur a tombé en panne", which word pairs would an alignment model likely link?

- **Concept: Proximal Policy Optimization (PPO) / GRPO**
  - **Why needed here:** Underlying RL algorithm that enables stable policy updates; understanding clipping and KL penalties is essential for debugging training dynamics.
  - **Quick check question:** Why does GRPO normalize rewards within a group rather than across the entire batch?

- **Concept: Neural MT Evaluation Metrics (BLEU vs. COMET)**
  - **Why needed here:** Different metrics capture different quality aspects; BLEU is n-gram overlap, COMET is learned semantic similarity. Understanding their failure modes explains why BLEU rewards hurt fluency.
  - **Quick check question:** Would a translation with perfect terminology but reordered syntax score higher on BLEU or COMET?

## Architecture Onboarding

- **Component map:** Input -> Word Alignment Module (SimAlign) -> Reward Computation (R_format + R_comet + R_aaw + R_ao + R_taw) -> RL Engine (GRPO) -> Output Model (Qwen2.5-7B-Instruct)

- **Critical path:**
  1. Prompt formatting ensures structured output (breaks → R_format=0 → zero reward)
  2. Word alignment quality determines terminology signal fidelity
  3. Reward weighting (α=1, β=0.1, γ=0.1) balances terminology vs. fluency
  4. GRPO advantage computation requires diverse rollouts for meaningful normalization

- **Design tradeoffs:**
  - Higher α (alignment weight) improves terminology but may constrain valid paraphrases
  - Lower rollout count G speeds training but reduces advantage estimate quality
  - SFT warmstart vs. pure RL: paper uses pure RL (like DeepSeek-R1-Zero), but R1-T1 shows SFT+RL hybrid approach exists

- **Failure signatures:**
  - All metrics near zero on test set → check for format mismatch or language detection failure (SFT collapse pattern)
  - High BLEU, low COMET → likely BLEU reward leakage or excessive n-gram pressure
  - Empty/generic reasoning in `` → missing Rtaw component; model not incentivized to reason about terms

- **First 3 experiments:**
  1. **Reward ablation:** Train RL-Rcomet only vs. RL-Rcomet+Raaw vs. full TAT-R1 to isolate alignment reward contribution on RTT terminology test
  2. **Generalization probe:** Evaluate SFT vs. RL checkpoint on held-out language pair (e.g., train on ZH↔EN, test on EN→DE) to replicate OOD finding
  3. **Reward hacking check:** Inspect model outputs for repetitive terminology insertion or length inflation that artificially inflates Raaw without improving actual accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating diverse evaluation metrics like BLEURT, MetricX, or GEMBA as reward signals further enhance terminology translation accuracy when combined with word-alignment rewards?
- **Basis in paper:** [explicit] The Limitations section states the authors "have not systematically explored multiple translation evaluation metrics as potential rewards" and identify this as a promising research direction.
- **Why unresolved:** The current experiments are restricted to COMET and BLEU baselines.
- **What evidence would resolve it:** Training TAT-R1 variants with these specific metrics as rewards and comparing performance on the RTT test set.

### Open Question 2
- **Question:** Does the absence of complex reasoning behaviors (e.g., self-correction, verification) in TAT-R1 stem from the inherent nature of machine translation or a lack of specialized RL training design?
- **Basis in paper:** [explicit] The authors observe "relatively simple" reasoning and question if this reflects "differences between the machine translation task and the mathematical task or indicate the need for specialized design."
- **Why unresolved:** The paper documents the lack of complex reasoning but does not isolate the underlying cause.
- **What evidence would resolve it:** Analyzing reasoning patterns across different tasks with identical RL parameters, or introducing specific constraints to force self-correction steps.

### Open Question 3
- **Question:** To what extent does the accuracy of the word alignment model (SimAlign) bottleneck the effectiveness of the proposed alignment rewards?
- **Basis in paper:** [inferred] The method relies entirely on SimAlign to extract keyword pairs for reward calculation, yet the paper does not analyze how alignment errors might propagate noise into the RL signal.
- **Why unresolved:** No ablation study is conducted regarding the choice or accuracy of the alignment tool itself.
- **What evidence would resolve it:** Comparing TAT-R1 performance when using gold-standard alignments versus different unsupervised alignment models.

## Limitations

- The approach depends heavily on SimAlign's ability to reliably extract cross-lingual terminology correspondences, with unknown performance for low-resource language pairs or domain-specific technical terminology.
- The reward function design introduces potential instability, with alignment rewards' interaction effects and optimal weighting remaining empirical rather than theoretically grounded.
- The generalization claim about RL outperforming SFT on out-of-distribution language pairs is demonstrated only for one language pair (EN→DE) and lacks broader validation across multiple language families and domains.

## Confidence

- **High confidence:** Terminology translation accuracy improvements on the RTT test set (measured by BLEU, COMETKiwi, XCOMET, and Terminology Accuracy metrics). The experimental setup and results are clearly specified with measurable improvements over baselines.
- **Medium confidence:** The mechanism by which word alignment rewards improve terminology accuracy without harming fluency. While the paper provides theoretical justification and some empirical evidence, the relationship between reward components and translation quality could benefit from deeper analysis.
- **Low confidence:** The claim that RL exhibits superior generalization to out-of-distribution language pairs compared to SFT. This is demonstrated only for one language pair (EN→DE) and requires validation across multiple language combinations.

## Next Checks

1. **Alignment quality validation:** Measure word alignment accuracy on domain-specific terminology pairs (e.g., medical or legal texts) and compare performance when using different alignment models (SimAlign vs. supervised aligners) to assess the robustness of terminology reward signals.

2. **Reward component ablation across domains:** Systematically vary α, β, γ parameters across different domains (technical, literary, conversational) to identify optimal weighting schemes and detect potential reward conflicts or instabilities.

3. **Multi-language generalization study:** Evaluate the RL vs. SFT generalization claim on a diverse set of language pairs (e.g., ZH→FR, AR→RU, HI→PT) to determine whether the observed EN→DE pattern holds across language families and typological distances.