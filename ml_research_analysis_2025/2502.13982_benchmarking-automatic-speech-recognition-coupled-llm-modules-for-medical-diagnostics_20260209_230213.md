---
ver: rpa2
title: Benchmarking Automatic Speech Recognition coupled LLM Modules for Medical Diagnostics
arxiv_id: '2502.13982'
source_url: https://arxiv.org/abs/2502.13982
tags:
- speech
- audio
- medical
- noise
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study develops a two-stage medical diagnostics system combining
  Automatic Speech Recognition (ASR) and a Large Language Model (LLM) to transcribe
  and classify patient audio. ASR modules (Whisper and wav2vec2) were fine-tuned on
  medical call recordings, with Whisper achieving a 21.3% word error rate (WER), outperforming
  wav2vec2.
---

# Benchmarking Automatic Speech Recognition coupled LLM Modules for Medical Diagnostics

## Quick Facts
- arXiv ID: 2502.13982
- Source URL: https://arxiv.org/abs/2502.13982
- Reference count: 0
- Primary result: Two-stage medical diagnostics system achieves 21.3% WER with fine-tuned Whisper ASR and 20.0% classification accuracy with fine-tuned LLM on medical call dataset

## Executive Summary
This study develops a two-stage medical diagnostics system combining Automatic Speech Recognition (ASR) and Large Language Model (LLM) modules for transcribing and classifying patient audio recordings. The system uses domain-specific fine-tuning of both components on medical call recordings, with Whisper achieving a 21.3% word error rate (WER) compared to wav2vec2's 48.9%. Audio preprocessing with equalization filters improves robustness to noise and clipping artifacts. The Qwen2 LLM, fine-tuned using LoRA for medical label classification, reaches 20.0% validation accuracy. Results demonstrate that effective ASR fine-tuning and preprocessing are critical for accurate transcription, while LLM fine-tuning enables domain-specific classification in medical contexts.

## Method Summary
The system employs a two-stage pipeline: ASR for speech transcription followed by LLM for medical label classification. Audio preprocessing applies denoising and equalization with high-pass filter at 250 Hz, low-pass at 11 kHz, and high-shelf at 4 kHz. Whisper-small is fine-tuned on 381 medical call samples using learning rate 5e-5, weight decay 0.005, warmup 200 steps, and ~1000-2000 steps. The Qwen2-7B LLM is fine-tuned with LoRA via unsloth library using 20 epochs, 5 warmup steps, linear scheduler, and bf16 precision on converted Alpaca format data. Evaluation uses WER for ASR and classification accuracy for the LLM.

## Key Results
- Whisper-small achieves 21.3% WER after fine-tuning, outperforming wav2vec2-base (48.9%) on medical call recordings
- Audio preprocessing with equalization improves robustness to noise, clipping, and ambient conditions
- Qwen2 LLM fine-tuned with LoRA reaches 20.0% validation accuracy for medical label classification
- Domain-specific fine-tuning is essential: base models fail completely on medical terminology (128% WER for Whisper)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning is required for both ASR and LLM components to function in medical contexts.
- Mechanism: Pre-trained models carry general-purpose representations that fail on specialized medical terminology and classification tasks without adaptation. Fine-tuning on medical call recordings aligns model weights to domain-specific acoustic patterns, vocabulary, and output formats.
- Core assumption: The training distribution of medical call recordings is representative of deployment conditions.
- Evidence anchors:
  - [abstract]: "ASR modules (Whisper and wav2vec2) were fine-tuned on medical call recordings"
  - [section 4.3.1]: "without finetuning, no model, either ASR or LLM is able to perform on the dataset" — base Whisper showed 128% WER, fine-tuned achieved 21.3%
  - [corpus]: Related work on German medical ASR (arXiv:2601.19945) similarly finds domain-specific evaluation and adaptation essential for clinical contexts
- Break condition: If medical terminology or accent distribution in deployment differs significantly from the fine-tuning dataset, WER and classification accuracy will degrade.

### Mechanism 2
- Claim: Equalization-based audio preprocessing reduces transcription errors caused by recording artifacts.
- Mechanism: High-pass filtering (250 Hz) removes low-frequency rumble and clipping artifacts from consumer-grade microphones; low-pass filtering (11 kHz) eliminates high-frequency static noise; high-shelf boost (4 kHz) enhances vocal clarity. This reduces input variance before the ASR stage.
- Core assumption: Noise is primarily static (microphone/transmission artifacts) rather than dynamic (ambient speech, variable background).
- Evidence anchors:
  - [abstract]: "Audio preprocessing with equalization filters improved robustness to noise, clipping, and ambient conditions"
  - [section 3.1.3]: "Equalization offers an effective solution for addressing static noise and clipping in call recordings"
  - [corpus]: Weak direct corpus validation — neighboring papers focus on LLM error correction rather than DSP preprocessing pipelines
- Break condition: If dynamic noise (crowd chatter, variable environmental sounds) dominates, equalization alone is insufficient; AI denoising models would be required but add latency.

### Mechanism 3
- Claim: Two-stage decoupling (ASR → LLM) enables independent optimization but compounds error propagation.
- Mechanism: ASR transcribes audio to text; LLM classifies transcribed text to medical labels. Each stage can be fine-tuned separately, but LLM accuracy is bounded by ASR transcription quality — errors in transcription cascade to classification.
- Core assumption: Transcription errors that remain after ASR are recoverable by the LLM's semantic understanding capabilities.
- Evidence anchors:
  - [abstract]: "two-stage system: Automatic Speech Recognition (ASR) for speech transcription and a Large Language Model (LLM) for context-aware, professional responses"
  - [section 4.3.2]: LLM misclassifications often "logically seem to fit those labels to some extent" — e.g., "hard to breath" classified as "feeling dizzy" rather than ground truth
  - [corpus]: "Confidence-Guided Error Correction" (arXiv:2509.25048) explores LLMs as ASR post-processors, suggesting error propagation is a recognized challenge
- Break condition: If ASR WER is too high, LLM receives semantically corrupted input and classification accuracy collapses; the paper's 21.3% WER leaves substantial room for error propagation.

## Foundational Learning

- Concept: **Word Error Rate (WER)**
  - Why needed here: Primary metric for ASR evaluation; measures insertions, deletions, and substitutions required to match predictions to ground truth.
  - Quick check question: If an ASR outputs "patent has chest pain" instead of "patient has chest pain," what type of error contributes to WER?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Parameter-efficient fine-tuning method used for Qwen2; enables adaptation with limited compute resources.
  - Quick check question: Why might LoRA be preferred over full fine-tuning when computational budget is constrained?

- Concept: **Digital Signal Processing Filters (High-pass, Low-pass, Shelf)**
  - Why needed here: Core preprocessing technique for cleaning audio before ASR; understanding frequency cutoffs is essential for tuning.
  - Quick check question: A high-pass filter at 250 Hz removes which type of artifact common in phone recordings?

## Architecture Onboarding

- Component map:
  - Audio Preprocessing: denoising → equalization (high-pass 250 Hz, low-pass 11 kHz, high-shelf 4 kHz)
  - ASR Module: Whisper-small (3B) fine-tuned on medical recordings
  - LLM Module: Qwen2-7B fine-tuned with LoRA for label classification
  - Data Flow: S → S′ (denoised) → S′′ (equalized) → S₀ (cleaned) → {Tᵢ} (transcription) → L (label)

- Critical path:
  1. Audio preprocessing quality directly impacts ASR WER
  2. ASR WER bounds maximum achievable LLM classification accuracy
  3. LLM fine-tuning determines whether transcribed text maps correctly to medical labels

- Design tradeoffs:
  - Whisper vs. wav2vec2: Whisper offers lower WER (21.3% vs. 48.9%) but is more compute-intensive
  - Equalization vs. AI denoising: Equalization adds minimal latency; AI denoising handles dynamic noise but slows pipeline
  - Qwen2 vs. Llama3: Qwen2 selected for speed (7-24% faster) and multilingual support

- Failure signatures:
  - High WER (>40%) without fine-tuning — models fail on medical terminology
  - LLM generating conversational responses instead of labels — fine-tuning data not in correct format
  - Oscillating loss during LLM training — reduce epochs or learning rate (observed at 200 steps)

- First 3 experiments:
  1. Baseline ASR evaluation: Run Whisper-small and wav2vec2-base on held-out medical audio without fine-tuning to establish WER baselines.
  2. Preprocessing ablation: Compare WER with and without equalization filters on noisy/clipped samples to quantify preprocessing contribution.
  3. LLM classification sanity check: Fine-tune Qwen2 on a small subset (100 samples) in Alpaca format to verify label generation before scaling to full dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between AI-based denoising models and traditional equalization preprocessing when accounting for both dynamic noise environments and real-time latency constraints?
- Basis in paper: [explicit] Section 3.1.4 states that AI-driven denoising models handle dynamic noise better but "add latency to the pipeline which makes it slow," while the current system assumes static noise only.
- Why unresolved: The paper explicitly assumes static noise to avoid latency, leaving the tradeoff between handling dynamic ambient noise (e.g., crowd chatter) and maintaining low latency unexplored.
- What evidence would resolve it: A systematic comparison of ASR+LLM pipeline performance and latency under AI denoising versus equalization across dynamic noise conditions.

### Open Question 2
- Question: How does increasing training dataset size affect LLM classification accuracy in the medical diagnosis task, and what is the minimum viable dataset for clinical reliability?
- Basis in paper: [explicit] The author attributes the 20.0% validation accuracy to "lack of data, as LLMs are data hungry models," with only 381 training samples for the LLM.
- Why unresolved: The paper acknowledges data insufficiency as a limitation but does not experiment with larger datasets or establish data requirements for acceptable accuracy.
- What evidence would resolve it: Experiments training Qwen2 on progressively larger medical call datasets with accuracy benchmarks at each scale.

### Open Question 3
- Question: To what extent do ASR transcription errors propagate and compound through the LLM classification stage in the two-stage pipeline?
- Basis in paper: [inferred] The paper evaluates ASR (21.3% WER) and LLM (20.0% accuracy) separately but does not analyze how specific transcription errors affect downstream classification.
- Why unresolved: Without end-to-end error analysis, it remains unclear whether improving ASR WER or improving LLM robustness to noisy transcripts would yield greater overall gains.
- What evidence would resolve it: Ablation studies injecting controlled transcription errors into LLM inputs and measuring classification degradation.

## Limitations

- Dataset size constraints limit model generalization, with only 381 training samples for ASR and 385 for LLM fine-tuning
- Static equalization preprocessing may be insufficient for dynamic noise environments common in real medical settings
- The paper does not provide complete reproduction details, including LoRA hyperparameters (rank, alpha) and batch sizes

## Confidence

- **High Confidence**: The necessity of domain-specific fine-tuning for both ASR and LLM components is strongly supported by the dramatic WER reduction from 128% to 21.3% and the complete failure of base models.
- **Medium Confidence**: The effectiveness of equalization preprocessing is supported by the claim of improved robustness, though direct quantitative evidence is limited and neighboring research focuses on different preprocessing approaches.
- **Medium Confidence**: The error propagation mechanism in the two-stage pipeline is logically sound and partially supported by observed misclassification patterns, but the quantitative relationship between WER and classification accuracy is not explicitly established.

## Next Checks

1. **Generalization Stress Test**: Evaluate the fine-tuned system on an external medical audio dataset with different patient demographics, accents, or recording conditions to assess real-world robustness beyond the original dataset distribution.

2. **Error Propagation Analysis**: Systematically vary the ASR WER (e.g., by adjusting preprocessing or fine-tuning parameters) and measure corresponding changes in LLM classification accuracy to quantify the exact relationship between transcription quality and downstream medical label prediction.

3. **Dynamic Noise Handling**: Replace static equalization with AI-based denoising on a subset of samples containing variable ambient noise (crowd chatter, variable background) and compare WER improvements against the current preprocessing pipeline to validate its limitations.