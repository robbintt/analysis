---
ver: rpa2
title: Bias Fitting to Mitigate Length Bias of Reward Model in RLHF
arxiv_id: '2505.12843'
source_url: https://arxiv.org/abs/2505.12843
tags:
- reward
- length
- bias
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses length bias in Reinforcement Learning from
  Human Feedback (RLHF), where reward models favor longer responses regardless of
  quality. Existing methods either don't characterize bias form or assume linear length-reward
  relationships.
---

# Bias Fitting to Mitigate Length Bias of Reward Model in RLHF

## Quick Facts
- arXiv ID: 2505.12843
- Source URL: https://arxiv.org/abs/2505.12843
- Reference count: 40
- Primary result: FiMi-RM framework autonomously learns and corrects non-linear length bias patterns, improving length-controlled win rate while reducing verbosity in RLHF systems.

## Executive Summary
This paper addresses length bias in Reinforcement Learning from Human Feedback (RLHF), where reward models favor longer responses regardless of quality. The authors propose FiMi-RM, a three-stage framework that autonomously learns and corrects non-linear length bias patterns. The approach trains a standard reward model, fits the non-linear length-reward relationship using a lightweight model with length encoding and ResNet architecture, then debiases the reward model. Experimental results show FiMi-RM achieves a more balanced length-reward distribution and improves length-controlled win rate while reducing verbosity without compromising performance.

## Method Summary
FiMi-RM is a three-stage framework: (1) Warm-up stage trains a standard reward model using Bradley-Terry loss, deliberately preserving length bias; (2) Fitting stage trains a lightweight model to predict reward scores from response length alone using combined Pearson correlation and MSE loss; (3) Debiasing stage alternately trains the reward model to minimize correlation with predicted bias while maintaining preference discrimination, and refines the fitting model. The method uses length encoding (d=32) and a 2-layer ResNet architecture to capture the multi-phase bias pattern: strongly linear for short responses, sublinear for medium-length responses, and stochastic variability for extended outputs.

## Key Results
- FiMi-RM achieves more balanced length-reward distribution compared to vanilla reward models
- When applied to alignment algorithms like Direct Preference Optimization and Best-of-N, the debiased model improves length-controlled win rate
- The framework reduces verbosity without compromising overall performance on preference tasks

## Why This Works (Mechanism)

### Mechanism 1
Length bias in reward models follows a non-linear, multi-phase pattern that linear assumptions fail to capture accurately. The fitting model (Length Encoding + 2-layer ResNet) learns to predict reward scores from response length alone. Training with combined Pearson correlation and MSE loss forces the model to match both the trend and magnitude of the biased reward distribution.

### Mechanism 2
Alternating training with gradient blocking enables stable debiasing while preserving preference modeling capability. The framework uses detach() strategically: when training the fitting model, the reward model's output is detached so gradients don't corrupt the reward model. When training the reward model for debiasing, the fitting model's output is detached while maximizing L'pearson (minimizing correlation with predicted bias) and minimizing LBT (maintaining preference discrimination).

### Mechanism 3
Subtracting the learned bias function from reward scores decouples length from quality assessment without requiring manual penalty coefficients. Rather than applying a fixed length penalty, FiMi-RM learns the functional form f(len(y)) from data. The debiased reward implicitly removes this learned component by minimizing correlation between r(x,y) and f(len(y)) via L'pearson.

## Foundational Learning

- **Bradley-Terry Preference Model**: The reward model's base training uses LBT to learn preference discrimination. Understanding that r(x,yw) - r(x,yl) should be positive for preferred responses is essential to see why debiasing must preserve this while removing length correlation. *Quick check*: Given two responses with equal quality but different lengths, what should r(x,yw) - r(x,yl) equal before and after debiasing?

- **Pearson Correlation Coefficient**: Both fitting (maximizing ρ) and debiasing (minimizing |ρ|) stages rely on correlation as the objective. The Pearson coefficient measures linear correlation, but here it guides a non-linear fitting process by optimizing alignment between predicted and actual reward distributions. *Quick check*: Why would minimizing |ρ(r, r̂)| between reward and fitted bias effectively remove length dependence, even though Pearson measures linear correlation?

- **Gradient Detachment (detach() in PyTorch)**: The asymmetric gradient blocking is critical—fitting model learns from frozen reward outputs, then reward model learns to be uncorrelated with frozen fitting predictions. Without detachment, gradients would corrupt the wrong model's weights. *Quick check*: If you forgot to detach rdetach during fitting model training, what would happen to the reward model's weights?

## Architecture Onboarding

- **Component map**: Reward Model (model_r) -> Fitting Model (model_f) -> Loss functions (LBT, Lpearson, Lmse) -> Training orchestrator
- **Critical path**: 1. Stage 1 (Warm-Up): Train model_r with LBT only; 2. Stage 2 (Fitting): Train model_f with Lfit using frozen model_r outputs; 3. Stage 3 (Debiasing): Alternately train model_r with Ldebiased and refine model_f with Lfit
- **Design tradeoffs**: Fitting model capacity (2-layer ResNet may underfit complex bias patterns); alternation frequency (a=8) controls stability vs. adaptation speed; MSE loss exclusion in Ldebiased prevents reward explosion but limits explicit magnitude debiasing; multi-GPU aggregation (8 devices) required for stable Pearson computation
- **Failure signatures**: Fitting model achieves near-zero Lpearson (length bias too weak); reward model accuracy drops dramatically on both C-longer and R-longer subsets (over-debiasing); loss divergence during Stage 3 (learning rate too high or alternation too frequent); selected responses remain overly long in BoN (fitting model failed to capture bias)
- **First 3 experiments**: 1. Diagnostic: Plot length vs. reward scatter for warm-up model; compute Pearson correlation; 2. Ablation: Train fitting model with only Lmse (no Lpearson); 3. Validation: Run BoN with N=8 using debiased model on held-out prompts

## Open Questions the Paper Calls Out
- Is complete decorrelation of length and reward desirable, or does a genuine positive correlation exist between length and human preference? The paper notes this remains a question worthy of further investigation.
- How does the non-linear debiasing strategy interact with the stability and convergence of Proximal Policy Optimization (PPO)? The authors explicitly exclude PPO from experiments due to computational demands and hyperparameter sensitivity.
- Is the observed multi-phase bias pattern (linear to sublinear) universal across different model architectures and tokenizers? The paper doesn't analyze whether this pattern depends on specific model architectures.

## Limitations
- The lightweight ResNet architecture may be insufficient for capturing complex bias patterns in diverse datasets, potentially leading to overfitting
- The alternation frequency of 8 steps is empirically chosen without systematic exploration of optimal values
- Generalizability of the learned bias function to completely unseen prompt types or domains remains unproven

## Confidence
- **High Confidence**: The multi-phase bias pattern characterization and alternating training mechanism are well-validated and technically sound
- **Medium Confidence**: Effectiveness in improving length-controlled win rates while reducing verbosity is demonstrated, but ablation studies on key parameters are limited
- **Low Confidence**: Generalizability to unseen domains and the impact on PPO training are not explored

## Next Checks
1. **Generalization Test**: Evaluate FiMi-RM on a held-out dataset with substantially different prompt distributions to assess whether the learned bias function transfers across domains
2. **Alternation Frequency Sweep**: Systematically vary the alternation frequency parameter from 1 to 32 steps and measure its impact on debiasing effectiveness, reward model stability, and convergence speed
3. **Fitting Model Capacity Study**: Replace the 2-layer ResNet with deeper architectures (4-6 layers) and compare fitting accuracy, debiasing performance, and overfitting behavior on both training and validation sets