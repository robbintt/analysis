---
ver: rpa2
title: 'CARPE: Context-Aware Image Representation Prioritization via Ensemble for
  Large Vision-Language Models'
arxiv_id: '2601.13622'
source_url: https://arxiv.org/abs/2601.13622
tags:
- vision
- visual
- classification
- image
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2601.13622
- Source URL: https://arxiv.org/abs/2601.13622
- Reference count: 40
- Primary result: CARPE improves LVLM performance by 3.5% average across benchmarks via context-aware vision-language ensemble

## Executive Summary
CARPE addresses the suboptimally aligned vision features in large vision-language models (LVLMs) by introducing a context-aware ensemble strategy that dynamically balances raw visual information with LLM-aligned representations. The framework employs a vision-integrator module using cross-attention fusion and a learnable context prompt token that conditions ensemble weights based on task instructions. CARPE demonstrates consistent performance improvements across 16 benchmark datasets while maintaining efficiency through parameter reuse and minimal additional computation.

## Method Summary
CARPE integrates with existing LVLMs through a vision-integrator module that fuses pre-alignment CLIP features with post-alignment LLM representations via multi-head cross-attention. A single learnable context prompt token appended to text embeddings produces hidden states that condition a context encoder to generate ensemble weights (α, β) for combining vision and language logits. The framework optionally extends to CARPE-MoE with a vision router selecting among four expert encoders (CLIP, SigLIP, DINOv2, CLIP-MoE) based on contextual information. The entire system is trained end-to-end with a 1:7 ImageNet to instruction-tuning data mixing ratio.

## Key Results
- CARPE achieves 3.5% average improvement across 16 benchmarks compared to LLaVA-1.5-7B baseline
- Context-aware ensemble weights show task-specific patterns: 0.26 vision weight for classification vs 0.13 for VQA tasks
- CARPE-MoE variant with multi-expert vision encoders provides additional gains for classification tasks
- Improved alignment between visual and textual features enables better zero-shot transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing pre-alignment and post-alignment vision features recovers fine-grained visual information that becomes suboptimally aligned during LLM adaptation.
- Mechanism: Vision-integrator applies multi-head cross-attention with LLM queries and raw CLIP features as keys/values, followed by self-attention and MLP.
- Core assumption: Vision encoders retain discriminative details attenuated during language-space alignment.
- Evidence anchors: [abstract], [section 3.1], weak corpus support
- Break condition: If raw vision features and LLM-aligned features encode nearly identical information, fusion gains are minimal.

### Mechanism 2
- Claim: Context-aware ensemble weighting enables task-adaptive prioritization of visual versus language-based reasoning pathways.
- Mechanism: Learnable context prompt token processed by LLM, with final hidden state projected to produce ensemble weights α and β for logit fusion.
- Core assumption: Task context in text instructions determines optimal balance between visual evidence and LLM reasoning.
- Evidence anchors: [abstract], [section 5.3 Table 4], indirect corpus support
- Break condition: If context prompts collapse to near-identical representations across diverse instructions, router cannot differentiate task types.

### Mechanism 3
- Claim: Ensembling diverse vision backbone representations yields more robust visual features than single encoder.
- Mechanism: Vision router (conditioned on context prompt) performs top-1 gating to select one of four expert encoders with dedicated adapters.
- Core assumption: Different vision encoders capture complementary aspects of visual content.
- Evidence anchors: [section 5.1 Table 2], [section 3.1], no direct validation
- Break condition: If expert outputs are highly correlated or router overfits to spurious patterns, MoE adds complexity without diversity.

## Foundational Learning

- Concept: Cross-attention fusion of multimodal features
  - Why needed here: Vision-integrator must merge pre-alignment and post-alignment visual representations
  - Quick check question: Given query vectors from modality A and key/value vectors from modality B, how does cross-attention compute the output?

- Concept: Soft prompt tuning
  - Why needed here: Context-aware ensemble weights derive from learnable context prompt token
  - Quick check question: How do soft prompts differ from discrete text prompts, and what inductive bias do they introduce?

- Concept: Gating/router mechanisms in Mixture-of-Experts
  - Why needed here: CARPE-MoE uses vision router to select among multiple encoders
  - Quick check question: In top-k gating, what happens when router output distribution collapses?

## Architecture Onboarding

- Component map: Image → Vision encoder → MLP adapter → Vision-integrator (cross-attention) → Z_vision logits; Text + context prompt → LLM → Z_llm logits + context prompt hidden state → Context encoder → (α, β) → Weighted logit fusion → Final output

- Critical path:
  1. Image → vision encoder (and optionally MoE experts)
  2. Raw vision features → MLP adapter → vision-integrator (cross-attention with LLM hidden states) → Z_vision logits
  3. Text + context prompt → LLM → Z_llm logits + context prompt hidden state
  4. Context encoder → (α, β) weights → weighted logit fusion → final output

- Design tradeoffs:
  - Ensemble vs. single pathway: Adds parameters and inference cost but enables adaptive balancing
  - Single-token context prompt vs. longer prompt: More efficient but may limit expressivity
  - MoE routing vs. fixed ensemble: Routing reduces compute but introduces training complexity

- Failure signatures:
  - Collapsed ensemble weights (α ≈ 0 or β ≈ 0 for all inputs) → context encoder not learning
  - No improvement over baseline on vision-centric benchmarks → vision-integrator ineffective
  - Performance drops on language-heavy tasks → vision pathway over-weighted

- First 3 experiments:
  1. Replicate linear probing vs. zero-shot gap on held-out classification dataset
  2. Ablate context encoder by fixing α = β = 0.5 and compare against full CARPE
  3. Visualize learned α/β distributions across task types to verify context-dependent separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does context encoder assign relatively low vision weights (0.26 for classification, 0.13 for VL benchmarks) and would enforcing higher vision weights improve performance?
- Basis in paper: [explicit] Table 4 reports average vision weights significantly below 0.5 for both task types
- Why unresolved: Paper reports weights but does not analyze optimality or training dynamics
- What evidence would resolve it: Ablation experiments with fixed or constrained vision weights

### Open Question 2
- Question: Does CARPE generalize to LVLM architectures beyond LLaVA1.5-7B with different adapter types?
- Basis in paper: [inferred] Authors claim model-agnostic design but only validate on LLaVA1.5-7B
- Why unresolved: No empirical validation on architectures with Q-former adapters or different vision encoders
- What evidence would resolve it: Experiments on 2-3 additional LVLM architectures

### Open Question 3
- Question: Is conditioning ensemble weights solely on text context sufficient, or would visual context improve adaptive weighting?
- Basis in paper: [explicit] Authors ensure context prompt influenced solely by text inputs
- Why unresolved: Design assumes instructions unambiguously signal task demands, but complex images may require different weighting
- What evidence would resolve it: Ablation comparing text-only vs. multimodal context conditioning

### Open Question 4
- Question: What is optimal mixing ratio between classification data and general instruction-tuning data?
- Basis in paper: [inferred] Fixed ratio at 1:7 without systematic exploration
- Why unresolved: Chosen ratio may not be optimal, sensitivity analysis could reveal trade-offs
- What evidence would resolve it: Grid search over mixing ratios with full benchmark evaluation

## Limitations
- Core mechanism relies on assumed recoverable fine-grained information without direct feature-space analysis
- Single-token context prompt may lack sufficient discriminative capacity across diverse instructions
- MoE gains potentially from spurious correlations rather than genuine feature complementarity
- Limited validation beyond LLaVA1.5-7B architecture

## Confidence

- High Confidence: Architectural implementation is clearly specified and reproducible; empirical results showing performance improvements are verifiable
- Medium Confidence: Claim about recovering "suboptimally aligned" visual information is plausible but lacks direct analysis; task-specific weight patterns observed but not deeply analyzed
- Low Confidence: Assertion that MoE gains come from "complementary aspects" is unsupported; routing mechanism could exploit spurious correlations without expert correlation analysis

## Next Checks

1. Compute pairwise correlation coefficients between raw CLIP features, LLM-aligned features, and vision-integrator outputs across diverse image classes to validate fusion information gain

2. Systematically vary context prompt token embedding across small perturbations while measuring resulting α/β distributions to test routing discriminative capacity

3. Measure pairwise cosine similarity between expert adapter outputs for identical inputs in CARPE-MoE to determine if MoE provides genuine diversity or parameter bloat