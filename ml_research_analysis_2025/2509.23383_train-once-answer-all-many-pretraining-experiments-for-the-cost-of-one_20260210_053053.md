---
ver: rpa2
title: 'Train Once, Answer All: Many Pretraining Experiments for the Cost of One'
arxiv_id: '2509.23383'
source_url: https://arxiv.org/abs/2509.23383
tags:
- training
- data
- experiments
- experiment
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to reduce the computational
  cost of pretraining experiments by conducting multiple experiments simultaneously
  within a single training run. The authors demonstrate this approach by training
  a 1.5B parameter model on 210B tokens while simultaneously conducting ten different
  experiments, including replications of previous work on memorization, contamination,
  poisoning, and forgetting, as well as novel experiments on knowledge acquisition,
  mathematical reasoning, and watermarking.
---

# Train Once, Answer All: Many Pretraining Experiments for the Cost of One

## Quick Facts
- **arXiv ID:** 2509.23383
- **Source URL:** https://arxiv.org/abs/2509.23383
- **Reference count:** 40
- **Primary result:** Ten diverse pretraining experiments can be run simultaneously within a single training run with negligible interference and minimal performance impact.

## Executive Summary
This paper introduces a novel approach to significantly reduce the computational cost of pretraining experiments by conducting multiple experiments simultaneously within a single training run. The authors demonstrate this by training a 1.5B parameter model on 210B tokens while simultaneously conducting ten different experiments. They show that all experiments can be successfully replicated and introduce Continual Pretraining Dependence Testing (CPDT) to test for dependencies between experiments before pretraining, finding negligible interactions in their setup. The experiments have minimal impact on the model's training dynamics and overall performance, validating the practical feasibility of conducting multiple pretraining experiments in a single training run.

## Method Summary
The method involves training OLMo-2-1B-Exp for 100,000 gradient steps on 8xH100 GPUs for approximately 15 days. The training uses 210B tokens from OLMo-mix-1124, with 3.7B tokens (1.8% of total) replaced by experimental data. Ten experiments are conducted simultaneously, including data poisoning, reasoning acquisition, watermarking, and replications of prior work. CPDT is used to test for dependencies between experiments by injecting interventions into a frozen checkpoint and measuring cross-experiment outcomes. The learning rate schedule follows OLMo-2-1B for the first 90k steps, then linearly decays to zero over the final 10k steps.

## Key Results
- Ten diverse pretraining experiments can be conducted simultaneously within a single training run without significant interference
- CPDT effectively identifies negligible dependencies between experiments in the tested setup
- The 1.8% experimental data modification has minimal impact on overall training dynamics and final model performance
- All five replicated prior experimental results (memorization, contamination, poisoning, forgetting) are successfully reproduced
- Novel experiments on knowledge acquisition, mathematical reasoning, and watermarking are successfully conducted

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent pretraining experiments can be run simultaneously within a single training run without significantly altering outcomes.
- Mechanism: Experiments target distinct data subsets or embedding perturbations that do not interfere with each other's signal propagation.
- Core assumption: Model capacity is sufficient to learn multiple independent signals without catastrophic interference.
- Evidence anchors: [abstract] "we demonstrate feasibility by conducting ten experiments... finding negligible dependencies in our setup." [section 5.2] CPDT experiments show no evidence of dependencies between experiments (Figure 4b).

### Mechanism 2
- Claim: Continual Pretraining Dependence Testing (CPDT) approximates experiment dependencies prior to full pretraining.
- Mechanism: By intervening on a frozen checkpoint with small, intense data injections and measuring cross-experiment outcomes, CPDT estimates interaction effects.
- Core assumption: Dependencies observed in short continual pretraining reflect those in full pretraining (scale-invariant interaction).
- Evidence anchors: [abstract] "We introduce Continual Pretraining Dependence Testing (CPDT)... finding negligible dependencies." [section 5.2] CPDT is applied to OLMo-2-1B checkpoints; Figure 4b shows near-zero off-diagonal entries.

### Mechanism 3
- Claim: Limited experiment volume (1.8% of tokens) preserves overall training dynamics and final performance.
- Mechanism: Small data perturbations act as low-noise interventions, leaving loss curves and weight norms largely unchanged.
- Core assumption: General-purpose pretraining is robust to minor data distribution shifts; the model can absorb interventions without destabilizing optimization.
- Evidence anchors: [abstract] "Remarkably, the influence of the ten experiments on the model's training dynamics and overall performance is minimal." [section 6] Figure 5 shows nearly identical train/validation loss and output layer norm between OLMo-2-1B-Exp and OLMo-2-1B.

## Foundational Learning

- **Concept: Causal Independence (SUTVA)**
  - Why needed here: Ensures that the outcome of one experiment is not confounded by another; foundational for multi-experiment validity.
  - Quick check question: If I remove experiment A, does the outcome of experiment B change?

- **Concept: Continual Pretraining**
  - Why needed here: CPDT relies on checkpoint-based interventions as a proxy for full pretraining.
  - Quick check question: Can a short, intense fine-tune on a frozen checkpoint reveal interactions that would appear in full training?

- **Concept: Multitask Learning / Capacity**
  - Why needed here: Simultaneous experiments leverage shared representations; capacity constraints may limit independence.
  - Quick check question: Is the model large enough relative to task complexity to avoid negative transfer?

## Architecture Onboarding

- **Component map:** OLMo-2-1B (1.5B parameters) -> 10 experimental interventions -> CPDT checkpoint injector -> Benchmark suites (OLMES), privacy metrics, reasoning tasks

- **Critical path:** 1. Design experiments with isolated data/embedding modifications. 2. Run CPDT on intermediate checkpoint to validate independence. 3. If passed, integrate experiments into training data mix (1.8% total tokens). 4. Train OLMo-2-1B-Exp for 100k steps with cooldown LR schedule. 5. Evaluate each experiment independently against baselines.

- **Design tradeoffs:** Token budget: More experiments â†’ higher risk of interference; keep total modifications <2% of tokens. CPDT intensity vs fidelity: Stronger intervention increases signal but may not reflect long-term dynamics. Replication vs novelty: Prioritize experiments with measurable, independent outcomes for initial runs.

- **Failure signatures:** Significant divergence in validation loss or gradient norms vs baseline. Off-diagonal entries in CPDT matrix >5% of diagonal. Experiment outcomes that fail to replicate prior results without explanation.

- **First 3 experiments:** 1. Benchmark Contamination (BC): Insert known benchmark answers, measure overfitting and forgetting. 2. Memorization Patterns (MemP): Canary insertion to test privacy leakage under different token strategies. 3. Knowledge Acquisition (KA): Dynamic data control to assess required exposure for factual acquisition.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the maximum number of simultaneous experiments or volume of modified data that can be introduced before significant dependencies or training instabilities emerge?
  - Basis in paper: [explicit] The Discussion section states, "Future work could study the number of experiments that can be conducted in a single training run in more detail."
  - Why unresolved: The authors only validated the approach with ten experiments modifying 1.8% of the data; the "breaking point" where the model's overall behavior degrades or interactions become confounding was not identified.

- **Open Question 2:** How can the independence of experiments requiring the strict absence of specific data (e.g., verbatim memorization) be validated without full pretraining?
  - Basis in paper: [explicit] Supplement F asks, "Which pretraining phenomena can and cannot be approximated with continual fine-tuning experiments?" and notes CPDT fails for the verbatim memorization experiment.
  - Why unresolved: Continual Pretraining Dependence Testing (CPDT) adds data to checkpoints to test influence, but it cannot simulate the "never-seen" condition required to validate experiments that rely on data exclusion.

- **Open Question 3:** Do experiments targeting similar model behaviors (e.g., two distinct reasoning tasks) introduce confounding dependencies even if CPDT suggests independence?
  - Basis in paper: [inferred] The Discussion warns, "one needs to be very careful when two different experiments target similar behaviors of the model," as this might lead to dependencies.
  - Why unresolved: While CPDT showed negligible dependencies for the specific ten experiments chosen, the authors did not test the "worst-case" scenario of semantically overlapping interventions.

## Limitations
- CPDT methodology needs broader validation across diverse experimental conditions and model sizes
- The 1.8% token budget may not be sufficient to contain interference from more aggressive interventions
- Experiments tested are largely benign; adversarial data poisoning might reveal hidden dependencies

## Confidence
- **High confidence:** The core demonstration that multiple pretraining experiments can be conducted simultaneously with minimal impact on overall training dynamics
- **Medium confidence:** The effectiveness of CPDT in detecting dependencies between experiments
- **Medium confidence:** The replication of prior experimental results within the multi-experiment framework

## Next Checks
1. **CPDT Robustness Test:** Apply CPDT to a wider range of experimental pairs, including those with known dependencies (e.g., two experiments both targeting reasoning capabilities). Verify that CPDT correctly identifies strong interactions and that these predictions hold in full pretraining runs.
2. **Adversarial Intervention Stress Test:** Introduce a more aggressive experiment (e.g., significant data poisoning with trigger phrases) and measure its impact on both the target experiment and unrelated experiments. Check if the 1.8% token budget or the CPDT pre-screening is sufficient to contain interference.
3. **Long-Training Phase Transition Analysis:** Monitor for phase transitions or emergent behaviors that only manifest after 50% or 75% of training steps. Use checkpoint-based evaluations to see if short-term CPDT predictions remain valid over the entire training horizon.