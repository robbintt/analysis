---
ver: rpa2
title: Towards Atoms of Large Language Models
arxiv_id: '2509.20784'
source_url: https://arxiv.org/abs/2509.20784
tags:
- layer
- atoms
- language
- representation
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Atom Theory to formally define, evaluate,
  and identify fundamental representational units (FRUs) of large language models
  (LLMs), termed atoms. The theory is built on the atomic inner product (AIP), a non-Euclidean
  metric that captures the underlying geometry of LLM representations.
---

# Towards Atoms of Large Language Models

## Quick Facts
- arXiv ID: 2509.20784
- Source URL: https://arxiv.org/abs/2509.20784
- Reference count: 40
- This paper introduces Atom Theory to formally define, evaluate, and identify fundamental representational units (FRUs) of large language models (LLMs), termed atoms.

## Executive Summary
This paper introduces Atom Theory as a principled framework for identifying and evaluating fundamental representational units (FRUs) in large language models. The theory addresses a critical gap in mechanistic interpretability by defining atoms through three key properties: representability (faithfulness), sparsity, and separability (stability). The authors develop the Atomic Inner Product (AIP) to correct a pervasive representation shift caused by Softmax training, and prove that threshold-activated sparse autoencoders (TSAEs) can reliably identify atoms under specific coherence and sparsity conditions. Empirically, they demonstrate that TSAEs achieve near-perfect faithfulness (R²=99.9%) and stability (q*=99.8%) across multiple models, outperforming traditional neurons and features.

## Method Summary
The method centers on Threshold-activated Sparse Autoencoders (TSAEs) trained to identify atoms in LLM activations. The process involves: (1) extracting activations from target models (Gemma2-2B, Llama3.1-8B) at specific positions (final token of subject mentions in CounterFact/WikiData), (2) applying the Atomic Inner Product (AIP) correction to address representation shift by estimating and applying a metric tensor from activation covariance, (3) training TSAEs with JumpReLU activation and learned thresholds to enforce support separation, and (4) evaluating identified atoms against ideal criteria using faithfulness (R²), stability (q* via coherence-sparsity bound), and monosemanticity via LLM-as-Judge. The approach requires careful capacity scaling relative to data size, with 4× expansion typically sufficient.

## Key Results
- TSAEs achieve near-perfect faithfulness (R²=99.9%) and stability (q*=99.8%) across multiple models, outperforming neurons (0.5% stable) and features (~68% stable)
- AIP correction successfully restores the angular centroid from ~60° to 90°, validating the representation shift theory
- Identified atoms exhibit substantially higher monosemanticity compared to traditional units, confirmed through GPT-5.2 LLM-as-Judge evaluation

## Why This Works (Mechanism)

### Mechanism 1: AIP Corrects Representation Shift
The standard Euclidean inner product introduces geometric distortion in LLM representations. During Softmax training, embeddings drift toward a dominant direction (anisotropy), moving the centroid of pairwise angles away from 90°. AIP applies a metric tensor $\tilde{S} = (DD^\top)^{-1}$ (estimated via activation covariance) to "re-normalize" the space, removing global bias and restoring the centroid to 90°. This ensures angular separability reflects semantic distance rather than architectural artifact.

### Mechanism 2: TSAEs Enable Unique Atom Identification
TSAEs uniquely identify atoms by enforcing support-separation through threshold activation. When activating a specific atom, non-target dimensions experience interference noise. JumpReLU's threshold $\tau$ filters out this interference ("noise floor") while retaining true signal ("amplitude gap"), guaranteeing the sparse code uniquely identifies active atoms and satisfies monorepresentationality.

### Mechanism 3: Stability via Coherence-Sparsity Bound
Atom stability requires high sparsity ($K$) and low coherence ($\mu$), unified by $\mu < \frac{1}{2K-1}$. This generalizes RIP from compressed sensing, ensuring the mapping from representation to sparse code is injective. This prevents feature splitting or merging, guaranteeing structural stability across varying decomposition settings.

## Foundational Learning

- **Sparse Coding & Superposition**: LLMs store more features than dimensions via "superposition." Understanding sparse coding is necessary to grasp why sparsity ($L_0$) and orthogonality are competing constraints that AIP helps resolve. *Quick check*: Can you explain why a sparse vector allows for an overcomplete dictionary (more atoms than dimensions) without interference?

- **Riemannian/Mahalanobis Geometry**: The core methodological shift is moving from Euclidean Inner Product to Atomic Inner Product (AIP). Understanding that inner products can be defined by a metric tensor ($x^T S y$) is essential to understand how the paper "corrects" the geometry. *Quick check*: How does changing the metric tensor $S$ change the definition of the "angle" between two vectors?

- **Restricted Isometry Property (RIP)**: The stability proof hinges on RIP from compressed sensing. This explains why sparsity guarantees uniqueness in reconstruction. *Quick check*: Why does RIP ensure that a sparse vector can be recovered from fewer measurements than its dimension?

## Architecture Onboarding

**Component map**: Input activations $M$ → AIP pre-processor (estimates $\tilde{S}$) → TSAE encoder ($W_{enc} m$ with JumpReLU) → sparse code $\delta$ → TSAE decoder ($W_{dec} \delta$) → Atoms (columns of $W_{dec}$)

**Critical path**: The relationship between Data Scale and TSAE Capacity. Reliable identification only occurs when capacity exceeds a threshold relative to data scale. A standard 4x expansion is used in final experiments, but smaller capacities fail.

**Design tradeoffs**: Faithfulness vs. Stability: Neurons are 100% faithful but 0.5% stable; Features are ~50% faithful and ~68% stable. The architecture must balance reconstruction loss ($R^2$) with stability constraint ($\mu < \frac{1}{2K-1}$). Threshold $\tau$ must be high enough to filter noise but low enough to catch small atom activations.

**Failure signatures**: Representation Shift: If using standard Euclidean metrics, you will see the angular centroid stuck at ~60-70 degrees. Low Stability ($q^*$): If using standard ReLU SAEs or insufficient capacity, $q^*$ will remain low, leading to polysemantic units. Hyperparameter Sensitivity: Training is robust to $\lambda$ but sensitive to data/capacity matching.

**First 3 experiments**:
1. **Validate Geometry Shift**: Extract activations from GPT-2, compute pairwise angles using Euclidean metric. Confirm the centroid is not 90 degrees.
2. **AIP Correction**: Estimate $\tilde{S}$ from 100k samples and re-compute angles. Confirm the centroid returns to 90 degrees.
3. **Capacity Scaling**: Train TSAE on WikiData with increasing hidden widths (1x, 2x, 4x, 8x). Plot $R^2$ to observe the "cliff" where faithfulness suddenly improves.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the identified atoms be used for precise causal interventions to control model behavior? While the paper establishes that atoms are faithful and monorepresentational, it does not demonstrate that modifying these atoms results in predictable changes to the model's output logic or knowledge.

**Open Question 2**: What is the theoretical relationship between the scale of training data and the required capacity of the TSAE? Section 4.3 states that while Theorem 3.11 proves atoms are identifiable, "it does not specify how to choose the model capacity given the data scale in practice."

**Open Question 3**: Do atoms derived from reasoning tasks exhibit high monosemanticity, or is the atomic structure specific to entity knowledge? The paper demonstrates that atoms can faithfully reconstruct reasoning data but restricts monosemanticity evaluation to entity-based knowledge.

## Limitations

- The core claim that 90° centroid is the ideal assumes the underlying representational geometry should be orthogonal, but the theoretical justification for this remains implicit
- Evaluation focuses on a specific dataset (CounterFact/WikiData subject entities) and activation extraction at final token position, limiting generalizability to other linguistic phenomena
- The stability proof relies on specific conditions for threshold activation that are theoretically derived but not extensively validated across different noise regimes

## Confidence

**High Confidence**: The empirical demonstration of representation shift (centroid moving from ~60° to 90° with AIP) across multiple models is well-supported by figures and methodology.

**Medium Confidence**: The theoretical framework for atom identification via TSAEs is rigorous with proofs establishing conditions for uniqueness, but practical sufficiency in real-world noisy representations requires more validation.

**Medium Confidence**: The claim that TSAEs outperform neurons and features is well-supported empirically, but the comparison methodology and generalizability could be more thoroughly explored.

## Next Checks

**Validation Check 1**: Test AIP correction across diverse datasets beyond CounterFact/WikiData, including different linguistic tasks (sentiment analysis, question answering) and domains (code, mathematical reasoning). Verify that the 90° centroid restoration is consistent and beneficial across these contexts.

**Validation Check 2**: Systematically vary the noise floor and amplitude gap conditions by introducing controlled perturbations to the activation space. Quantify how these variations affect atom identification success rates and determine the practical bounds of theoretical guarantees.

**Validation Check 3**: Compare TSAE-identified atoms against alternative sparse coding approaches (different activation functions, regularization schemes) on the same evaluation metrics. Establish whether the specific threshold mechanism is critical or if similar results can be achieved through other means.