---
ver: rpa2
title: 'RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and
  Vector Quantization'
arxiv_id: '2505.03803'
source_url: https://arxiv.org/abs/2505.03803
tags:
- rwkv
- quantization
- arxiv
- weights
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RWKVQuant, a post-training quantization framework
  designed for RWKV models. RWKV, a modern RNN architecture, is efficient for deployment
  but faces challenges when applying standard quantization techniques.
---

# RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization

## Quick Facts
- **arXiv ID:** 2505.03803
- **Source URL:** https://arxiv.org/abs/2505.03803
- **Reference count:** 40
- **Primary result:** Quantizes RWKV-6-14B to approximately 3-bit with less than 1% accuracy loss and 2.14× speedup

## Executive Summary
RWKVQuant introduces a post-training quantization framework designed to address unique challenges in quantizing RWKV models, a modern RNN architecture efficient for deployment but difficult to quantize using standard methods. The framework introduces a coarse-to-fine proxy mechanism that adaptively selects between scalar and vector quantization based on weight distribution properties, and optimizes codebook generation for RWKV's element-wise multiplication operations. Experiments demonstrate successful quantization of RWKV models from 0.1B to 14B parameters with minimal accuracy loss and significant speedup improvements.

## Method Summary
RWKVQuant employs a hybrid quantization approach that uses a coarse-to-fine proxy to determine whether scalar or vector quantization is more appropriate for each weight tensor. The coarse proxy uses Information Entropy on transformed weight intervals to assess uniformity, while the fine proxy uses weighted higher-order central moments to detect local outliers. Layers passing both thresholds use scalar quantization (GPTQ at 3.25 bpw), while others use vector quantization (GPTVQ at 3.5 bpw). For vector quantization layers containing element-wise multiplication, codebook generation is optimized using weighted KMeans where errors are weighted by squared calibration activations. The framework targets approximately 3.275 bits-per-weight and requires 128 calibration samples.

## Key Results
- Successfully quantizes RWKV-6-14B to approximately 3-bit with less than 1% accuracy loss
- Achieves 2.14× speedup across various model sizes
- Outperforms existing quantization methods on both language and vision tasks
- Maintains low perplexity (PPL < 20) and high zero-shot accuracy (>60%) across different model scales

## Why This Works (Mechanism)

### Mechanism 1
A coarse-to-fine proxy enables adaptive selection between scalar and vector quantization by assessing weight distribution properties. The coarse-grained proxy uses Information Entropy (IE) on transformed weight intervals to measure overall uniformity. Weights are flattened, sorted, and converted to intervals G, then normalized to a probability distribution G' where IE is computed. For weights passing the uniformity threshold, a fine-grained proxy using weighted higher-order central moments (variance, skewness, kurtosis) detects local outliers. Only weights that are both uniform AND outlier-free use SQ; others use VQ.

### Mechanism 2
Codebook optimization weighted by activation squared improves VQ accuracy for element-wise multiplication layers unique to RWKV. For element-wise multiplication X ⊙ μ, the quantization loss L = Σ(X²_ij × Δμ²_ij) shows that larger activations amplify weight errors. The codebook generation uses weighted KMeans where X² serves as the weight, prioritizing accurate quantization of positions with high activation magnitude. Percentile-based clipping handles batch integration by removing outlier activations before averaging.

### Mechanism 3
RWKV's architectural properties (non-linear operators, uniform weights, low compute-to-memory ratio) cause standard Transformer quantization methods to fail or underperform. Non-linear operators (token-shift, Sigmoid, exp) along the fusion path prevent absorption of smoothing vectors and rotation matrices into adjacent layers, adding runtime overhead. Uniform weight distributions (60% SQ-suitable vs 10% for LLaMA) increase K-Means clustering loss. Low FLOPs/bytes ratio (0.97 vs 4.88) makes RWKV memory-bound, so weight compression directly speeds inference.

## Foundational Learning

- **Concept:** Scalar Quantization (SQ) vs Vector Quantization (VQ)
  - **Why needed here:** RWKVQuant's core innovation is hybrid SQ/VQ selection; understanding both is prerequisite.
  - **Quick check question:** Given a weight tensor with uniform distribution and no outliers, which method would you expect to perform better at 3-bit, and why?

- **Concept:** RWKV Time Mixing and Channel Mixing modules
  - **Why needed here:** The element-wise multiplication optimization targets these specific architectural components.
  - **Quick check question:** In Equations 20-27, identify all element-wise multiplication (⊙) operations vs matrix multiplications (·). Which use the μ parameters?

- **Concept:** Information Entropy and Central Moments
  - **Why needed here:** The proxy mechanism relies on IE for uniformity and moments for outlier detection.
  - **Quick check question:** If a weight distribution has high variance but low kurtosis, what does this tell you about its outlier structure?

## Architecture Onboarding

- **Component map:**
  Calibration Data (128 samples) → Weight Analysis → Coarse Proxy (IE) → Fine Proxy (moments) → Layer Classification ← Thresholds (τc, τf) → SQ (GPTQ) / VQ (GPTVQ + weighted codebook) → Quantized Model (~3.275 bpw average)

- **Critical path:**
  1. Extract calibration activations for each layer
  2. Compute Pc and Pf for every weight tensor
  3. Apply thresholds to determine SQ vs VQ per layer (target: ~90% SQ at 3.25 bpw, ~10% VQ at 3.5 bpw)
  4. For VQ layers, generate weighted codebook using clipped activation statistics

- **Design tradeoffs:**
  - Higher τc → more layers use VQ → higher accuracy but larger model size
  - Lower τf → more uniform layers flagged for VQ due to outliers → conservative but larger
  - Calibration sample count vs quantization quality (paper uses 128)
  - Codebook clipping percentile vs representative feature preservation

- **Failure signatures:**
  - PPL spike on small models (>10 point increase): thresholds too aggressive, forcing inappropriate SQ on non-uniform weights
  - Accuracy collapse on specific tasks: codebook optimization may have overfit to calibration distribution
  - No speedup despite quantization: check that dequantization is fused, not materialized

- **First 3 experiments:**
  1. **Baseline validation:** Replicate Table 2 results on RWKV6-7B with τc=1.5, τf=30; verify PPL < 3.3 and zero-shot accuracy > 60%
  2. **Ablation on thresholds:** Run Table 12 sweep on RWKV7-0.5B; plot accuracy vs (τc, τf) to find optimal region
  3. **Codebook optimization isolation:** Compare weighted vs unweighted KMeans on a single VQ layer; measure LAMBADA PPL delta to quantify component contribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-tuning-based initialization be used to automatically determine the optimal proxy thresholds (τc and τf) for different RWKV model architectures?
- **Basis in paper:** Appendix A.5 states the authors plan to "utilize fine-tuning to achieve the optimal configuration" for these thresholds, moving away from empirical settings.
- **Why unresolved:** The current study sets thresholds manually based on specific model configurations, which may not reflect the most balanced or effective proportion for all models.
- **What evidence would resolve it:** A framework where thresholds are learned dynamically, resulting in superior accuracy or efficiency compared to the empirically set baselines.

### Open Question 2
- **Question:** Does applying the coarse-to-fine proxy at a channel-level or block-level granularity yield better quantization results than the current weight-level approach?
- **Basis in paper:** Appendix A.5 outlines the intention to use the proxy to determine if "specific features at a finer granularity" (channel or block) are better suited for SQ or VQ.
- **Why unresolved:** The current method assesses uniformity and outliers at the layer/weight level, potentially overlooking local data characteristics that could inform a more precise hybrid strategy.
- **What evidence would resolve it:** Ablation studies showing accuracy retention or memory savings improvements when the proxy logic is applied to sub-weight partitions.

### Open Question 3
- **Question:** How does removing the fixed 3.275 bits-per-weight (bpw) constraint affect the trade-off between compression rate and accuracy?
- **Basis in paper:** Appendix A.5 notes future work will "remove the fixed constraint of bpw being 3.275" to better manage the trade-off between compression and performance.
- **Why unresolved:** The current experiments fix the bpw target (averaging 3.25 and 3.5 methods), limiting the exploration of the optimal efficiency-accuracy Pareto frontier.
- **What evidence would resolve it:** Results demonstrating variable compression rates that maximize accuracy for given memory budgets without adhering to a specific average bpw.

## Limitations
- The coarse-to-fine proxy mechanism lacks external validation across different architectures or weight distributions
- Codebook optimization for element-wise multiplication is entirely novel with no corpus validation
- Proxy thresholds appear tuned for RWKV-7 models, with RWKV-6 threshold values unspecified
- 2.14× speedup claim assumes successful fusion of dequantization operations, but runtime overhead analysis is incomplete

## Confidence
- **High Confidence (4/5):** The fundamental architectural challenges (non-linear operators blocking parameter fusion, uniform weight distributions complicating clustering) are well-documented and empirically validated
- **Medium Confidence (3/5):** The coarse-to-fine proxy mechanism demonstrates superior performance to baseline metrics within RWKV context, but lacks cross-architecture validation
- **Low Confidence (2/5):** The codebook optimization for element-wise multiplication is entirely novel with no corpus validation, making it difficult to assess whether this represents the optimal solution

## Next Checks
1. **Threshold Generalization Study:** Systematically sweep τc and τf values across RWKV-6 and RWKV-7 models of varying sizes (0.1B-14B), measuring accuracy and PPL to identify whether the same thresholds generalize or require architecture-specific tuning

2. **Cross-Architecture Proxy Validation:** Apply the coarse-to-fine proxy mechanism to non-RWKV architectures with similar challenges (Mamba, Hyena) to test whether the information entropy and moment-based selection criteria remain effective when weight distributions and non-linear operator patterns differ

3. **Codebook Optimization Ablation:** Implement and compare the weighted KMeans codebook generation against standard unweighted KMeans and alternative weighting schemes (e.g., activation magnitude without squaring, batch normalization statistics) to isolate the contribution of the X² weighting factor to observed performance gains