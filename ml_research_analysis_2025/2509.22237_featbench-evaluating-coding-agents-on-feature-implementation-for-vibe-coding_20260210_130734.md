---
ver: rpa2
title: 'FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding'
arxiv_id: '2509.22237'
source_url: https://arxiv.org/abs/2509.22237
tags:
- test
- coding
- vibe
- agent
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FeatBench, a new benchmark for evaluating
  Large Language Model (LLM)-based coding agents on feature implementation within
  the "vibe coding" paradigm, where users interact through high-level natural language.
  Unlike existing benchmarks focused on bug fixing, FeatBench uses pure natural language
  prompts and includes rigorous test cases to validate new features while preventing
  regressions.
---

# FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding

## Quick Facts
- arXiv ID: 2509.22237
- Source URL: https://arxiv.org/abs/2509.22237
- Reference count: 34
- One-line primary result: Agents achieve only 29.94% success rate on feature implementation, with most failures due to regression bugs from "aggressive implementation" strategies.

## Executive Summary
This paper introduces FeatBench, a benchmark for evaluating LLM-based coding agents on feature implementation within the "vibe coding" paradigm, where users interact through high-level natural language. Unlike existing benchmarks focused on bug fixing, FeatBench uses pure natural language prompts and includes rigorous test cases to validate new features while preventing regressions. Built from 157 tasks across 27 real-world open-source repositories, it features an automated pipeline for evolving the benchmark and mitigating data contamination. Experiments with two state-of-the-art agent frameworks and four leading LLMs show that feature implementation remains highly challenging, with the best performance reaching only a 29.94% success rate. The analysis reveals that agents often employ "aggressive implementation" strategies—leading both to superior software design and to critical failures through scope creep and regressions—highlighting the need for better control mechanisms in future coding agents.

## Method Summary
FeatBench evaluates LLM-based coding agents on feature implementation using natural language prompts without code-level specifications. The benchmark comprises 157 tasks from 27 GitHub repositories, each with natural language feature descriptions, repository at base commit, Docker environment, and F2P/P2P test cases. Two agent frameworks are evaluated: Trae-agent (autonomous planning-based with max 150 steps and tool access) and Agentless (two-stage pipeline with single rollout). Experiments use four leading LLMs with temperature=0.0 and top-p=1.0. Primary metric is Resolved Rate (%), measuring tasks where feature passes tests and no regressions occur. Auxiliary metrics include Patch Apply Rate, File-level Localization Success Rate, Feature Validation Pass Rate, Regression Tests Pass Rate, and Token Cost.

## Key Results
- Best resolved rate is 29.94% (Trae-agent + GPT-5), demonstrating feature implementation remains highly challenging
- Agents exhibit "aggressive implementation" strategies that produce both superior architectural solutions and critical regression failures
- Performance degrades systematically with repository complexity (files, LOC) and patch complexity (files modified, LOC changed)
- Planning-based agents (Trae-agent) significantly outperform pipeline-based agents (Agentless) due to superior file localization and reflective debugging capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pure natural language prompts without code-level specifications enable more authentic evaluation of "vibe coding" capabilities
- Mechanism: By stripping task inputs of function signatures and structural hints, the benchmark forces agents to infer implementation context from repository exploration and natural language understanding alone, mirroring real user requests
- Core assumption: Users in vibe coding workflows genuinely lack code-level knowledge to provide
- Evidence anchors:
  - [abstract] "Task inputs consist solely of abstract natural language descriptions, devoid of any code or structural hints"
  - [section 1] "Traditional code generation benchmarks... their inputs demand code-level specifics like function signatures, a methodology fundamentally misaligned with the vibe coding paradigm"
  - [corpus] Related work on vibe coding safety and novice programmer engagement supports the natural language focus but does not directly validate benchmark design choices
- Break condition: If future work shows that providing minimal structural hints improves agent reliability without undermining realism, the pure-NL constraint may need reconsideration

### Mechanism 2
- Claim: Aggressive implementation by agents produces both superior architectural solutions and regression failures through scope creep
- Mechanism: Agents autonomously extend beyond explicit requirements, creating reusable abstractions (positive) or declaring unimplemented configurations (negative), with the divergence determined by whether extensions are fully implemented
- Core assumption: The same underlying tendency drives both outcomes; control mechanisms could selectively preserve benefits
- Evidence anchors:
  - [abstract] "agents often employ 'aggressive implementation' strategies—leading both to superior software design and to critical failures through scope creep and regressions"
  - [section 4.1] C++26 case shows agent adding Intel compiler settings without corresponding implementation logic, causing regressions
  - [section 4.2] Arrow streaming case shows agent creating reusable helper function superior to human's localized fix
  - [corpus] No direct corpus evidence on this specific mechanism; related work on agentic vs vibe coding discusses autonomy differences but not this failure mode
- Break condition: If aggressive implementation is not a unitary trait but separable behaviors, interventions must target specific sub-behaviors

### Mechanism 3
- Claim: Autonomous planning-based agents outperform rigid pipeline-based agents on feature implementation due to superior file localization and reflective debugging
- Mechanism: Planning agents dynamically explore repositories, invoke tools like code graph analysis, and design verification tests; pipeline agents follow fixed locate-then-patch sequences without reflection
- Core assumption: Feature implementation requires more adaptive problem-solving than bug fixing
- Evidence anchors:
  - [section 3.3] "Trae-agent generally outperforms Agentless... This advantage likely stems from its ability to autonomously plan execution paths, invoke external tools... and design its own test cases"
  - [section 3.3] File-level localization success: Trae-agent 76.42% vs Agentless 48.90%
  - [corpus] "Vibe Coding vs. Agentic Coding" discusses autonomy differences but does not provide comparative performance data
- Break condition: If pipeline agents are enhanced with localization and reflection stages, the performance gap may narrow

## Foundational Learning

- Concept: Pass-to-Pass (P2P) vs Fail-to-Pass (F2P) test cases
  - Why needed here: Understanding regression detection (P2P) versus feature validation (F2P) is essential for interpreting why agents succeed on features but fail overall
  - Quick check question: If an agent implements a new feature correctly but breaks existing functionality, which test category fails?

- Concept: Repository complexity scaling (files, LOC) and patch complexity (files modified, LOC changed)
  - Why needed here: Performance degrades systematically with both dimensions; understanding this helps set realistic expectations for agent deployment
  - Quick check question: Why might an agent succeed on a 20-LOC single-file change but fail on a 60-LOC multi-file change?

- Concept: Data contamination and benchmark evolution
  - Why needed here: FeatBench's automated pipeline evolves the benchmark to prevent memorization; researchers must understand why temporal stability of resolved rates (Figure 6) matters
  - Quick check question: What would an upward trend in resolved rates over time suggest about benchmark integrity?

## Architecture Onboarding

- Component map: Data Curation -> Environment Configuration -> Test Case Validation -> Agent Evaluation
- Critical path: PR selection → environment setup (most fragile; requires correct Python version, dependency resolution) → test extraction → agent evaluation → resolved rate calculation
- Design tradeoffs: Restricting feature patches to modifying existing functions (no additions/deletions) ensures testability but may exclude valid implementation patterns; Python-only initial release limits generality but ensures representative coverage
- Failure signatures: 73.6% of failures are regressive implementations (feature works but breaks existing tests); 17.8% incomplete; 8.5% misunderstood intent. Low RT% (regression test pass rate) across all configurations signals systemic issue
- First 3 experiments:
  1. Reproduce baseline results with Trae-agent + GPT-5 on the 157-task dataset, verifying resolved rate approximates 29.94%
  2. Ablate Trae-agent's tool access (remove code graph analysis) to isolate contribution of autonomous exploration
  3. Manually annotate 20 failed cases for regression patterns to validate the 73.6% figure and identify actionable control targets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can future coding agents control "aggressive implementation" strategies to prevent scope creep while retaining the benefits of superior software design?
- Basis in paper: [explicit] The authors state there is a "critical need for mechanisms that can control the agent’s level of implementation aggressiveness" to balance robust architectural improvements against defect-inducing scope creep.
- Why unresolved: Current agents exhibit a double-edged behavior where the same autonomy that produces clean, abstracted code also causes critical failures by implementing unrequested features.
- What evidence would resolve it: An agent framework that dynamically adjusts its implementation aggressiveness, demonstrating a reduced regression rate without a corresponding drop in code quality metrics.

### Open Question 2
- Question: How can the efficiency of token utilization be improved to achieve high resolved rates without linear cost increases?
- Basis in paper: [explicit] The analysis highlights a dichotomy where high performance (Trae-agent) requires millions of tokens, while efficient frameworks (Agentless) yield poor results, emphasizing the "need to find methods that enhance the efficiency of token utilization."
- Why unresolved: There is currently a strict trade-off where solving complex, repository-level tasks requires expensive, iterative planning that consumes significantly more tokens than rigid pipeline approaches.
- What evidence would resolve it: A hybrid agent architecture that achieves a resolved rate comparable to Trae-agent (>25%) but with a token consumption profile closer to the Agentless framework (<0.5M tokens).

### Open Question 3
- Question: What techniques can overcome the "fundamental barrier" of repository and patch complexity that causes agent performance to degrade to near zero?
- Basis in paper: [explicit] The authors identify that "ability of the LLM agent systematically declines as project complexity increases," noting that success rates fall to nearly zero for patches exceeding 50 lines of code or repositories with more than 800 files.
- Why unresolved: Current context windows and localization capabilities struggle to maintain coherence and precision over large codebases or widespread changes.
- What evidence would resolve it: Methods (such as hierarchical planning or advanced retrieval) that maintain a stable resolved rate (e.g., >20%) even as the number of modified files or lines of code in the gold patch increases significantly.

## Limitations
- Benchmark currently restricted to Python repositories and feature patches that modify existing functions without additions/deletions
- Performance evaluation depends on availability of F2P and P2P test cases, which may not exist for all repository features
- Agent evaluation requires significant computational resources (millions of tokens per run) and proprietary model access

## Confidence
High confidence in methodology and results based on:
- Clear experimental design with multiple agent frameworks and LLMs
- Comprehensive evaluation metrics beyond simple pass/fail
- Rigorous test case validation including regression detection
- Systematic analysis of failure modes and performance degradation patterns

## Next Checks
1. Verify resolved rate reproducibility by running Trae-agent + GPT-5 on the full 157-task dataset
2. Test the impact of tool access by comparing Trae-agent with and without code graph analysis
3. Analyze 20 randomly selected failed cases to validate the 73.6% regression failure rate and identify specific patterns