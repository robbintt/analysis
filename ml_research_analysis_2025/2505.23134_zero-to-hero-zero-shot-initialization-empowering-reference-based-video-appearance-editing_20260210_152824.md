---
ver: rpa2
title: 'Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance
  Editing'
arxiv_id: '2505.23134'
source_url: https://arxiv.org/abs/2505.23134
tags:
- video
- target
- editing
- frame
- appearance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Zero-to-Hero addresses reference-based video appearance editing,
  enabling users to modify object appearance by providing reference images while preserving
  target video structure. The method disentangles editing into two steps: editing
  an anchor frame to create a reference and propagating its appearance consistently
  to other frames.'
---

# Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing

## Quick Facts
- **arXiv ID**: 2505.23134
- **Source URL**: https://arxiv.org/abs/2505.23134
- **Reference count**: 40
- **Primary result**: Achieves 2.6 dB PSNR improvement over baselines on video appearance editing

## Executive Summary
Zero-to-Hero introduces a two-stage framework for reference-based video appearance editing that enables users to modify object appearance using reference images while preserving the target video's structure. The method first creates a degraded intermediate result through a correspondence-guided attention mechanism that transfers appearance from an edited anchor frame to other video frames. This zero-shot initialization is then enhanced by a Hero-Stage that learns a conditional generative model to restore the specific degradation artifacts (blur and oversaturation) introduced during the initial transfer. Evaluation on synthetic Blender videos shows the method outperforms existing approaches with significant quality improvements.

## Method Summary
Zero-to-Hero addresses reference-based video appearance editing through a two-stage process. First, the Zero-Stage uses diffusion features from the original video to guide cross-image attention, transferring the reference appearance to target frames with high accuracy but introducing blur and oversaturation artifacts. Second, the Hero-Stage fine-tunes a DiT-based diffusion model with LoRA to restore these specific degradation patterns, using the degraded intermediate result as input condition along with the original target frame. The method employs dual conditioning (Mode 2) to preserve background structure while preventing appearance leakage from the original target frame.

## Key Results
- Achieves 2.6 dB PSNR improvement over baseline methods on constructed Blender videos
- Demonstrates superior performance in handling large object motions compared to optical flow-based approaches
- Shows effective generalization across different editing tasks including colorization, Blender-Color-Edit, and General-Edit

## Why This Works (Mechanism)

### Mechanism 1: Correspondence-Guided Attention Injection (Zero-Stage)
- **Claim:** Utilizing diffusion features (DIFT) from the original video to mask cross-image attention creates a more robust appearance transfer than standard attention or optical flow, particularly under large motion.
- **Mechanism:**
    1.  **Feature Extraction:** Extract intermediate diffusion features ($F^{anc}, F^{tgt}$) from the anchor and target frames of the *original* video.
    2.  **Correlation Masking:** Compute a correlation matrix ($Corr = F^{tgt} \cdot F^{ancT}$). Select the top-$k$ highest similarity indices to create a binary mask $M$.
    3.  **Attention Override:** During the denoising process of the target frame, inject the Key/Value from the *reference* (edited) frame, but apply the mask $M$ to the attention scores ($A \oplus M$) before the softmax. This forces the target pixels to attend only to semantically aligned reference pixels.
- **Core assumption:** The semantic correspondence extracted from diffusion features at specific timesteps is geometrically consistent enough to guide pixel-level appearance transfer.

### Mechanism 2: Degradation-Aware Restoration (Hero-Stage)
- **Claim:** The specific artifacts (blur, over-saturation) introduced by aggressive attention masking can be treated as a fixed degradation pattern and reversed by a conditional generative model.
- **Mechanism:**
    1.  **Degraded Initialization:** Generate the "Zero-Stage" output using strict masking (e.g., $k=1$). This output has correct color layout but poor quality.
    2.  **Conditional Modeling:** Fine-tune a DiT-based diffusion model (using LoRA) to take this degraded image as an input condition and output the high-fidelity reference image.
    3.  **Generalization:** Because the degradation pattern is consistent across frames, the model learns to restore the artifacts on the anchor frame and applies this restoration capability to subsequent frames.
- **Core assumption:** The "compound degradation" (blur + saturation) caused by attention intervention is spatially consistent and learnable, similar to how ControlNet learns de-blurring or tile upscaling.

### Mechanism 3: Dual-Condition Shortcut (Mode 2)
- **Claim:** Providing the *original* target frame as a secondary explicit condition allows the model to reconstruct non-edited regions (background) without "leaking" the old appearance of the edited object.
- **Mechanism:**
    1.  **Dual Input:** Feed two conditions to the DiT: The Zero-Stage output (provides new appearance) and the Original Target frame (provides structural details/background).
    2.  **Role Differentiation:** Use two independent LoRAs (Low-Rank Adaptations) to process these tokens. The model learns to use the Zero-Stage input for "where the object is" and the Original Target for "everything else" or "texture details".
    3.  **Leakage Prevention:** By avoiding training pairs that map original $\to$ original (Table 1d), the model is forced to synthesize the new appearance rather than copying the old one.
- **Core assumption:** The model can successfully disentangle the "appearance" code from the "structure" code when presented with conflicting inputs.

## Foundational Learning

- **Concept: Cross-Image Attention (CiA) vs. Self-Attention (SA)**
    -   **Why needed here:** The core "Zero-Stage" relies on modifying the attention mechanism. You must understand that SA builds structure from the image itself, while CiA borrows appearance from a reference image by swapping $K, V$ matrices.
    -   **Quick check question:** If you swap $Q$ (Query) instead of $K$ and $V$ in the attention layer, whose structure and whose appearance would the resulting image possess?
- **Concept: Diffusion Feature Correspondence (DIFT)**
    -   **Why needed here:** The paper uses features from the U-Net decoder rather than pixel RGB or optical flow to find matching points.
    -   **Quick check question:** Why are features extracted from *intermediate* timesteps (noisy latents) often better for finding semantic correspondence than features from the clean image?
- **Concept: DiT (Diffusion Transformer) Conditioning**
    -   **Why needed here:** The Hero-Stage uses a DiT architecture with LoRA, not a U-Net with ControlNet.
    -   **Quick check question:** In a DiT, how are conditional image tokens typically integrated with the noisy latent tokens? (Hint: Concatenation vs. Cross-attention).

## Architecture Onboarding

- **Component map:**
    -   **Zero-Stage (Inference):** DIFT Extractor $\to$ Correlation Matrix $\to$ Top-$k$ Masking $\to$ Modified Cross-Image Attention Denoiser.
    -   **Hero-Stage (Training & Inference):** DiT Backbone + 2x LoRA Adapters.
        -   *Condition 1:* Zero-Stage Output (Degraded).
        -   *Condition 2:* Original Target Frame.
        -   *Objective:* Reconstruct Reference Frame (for anchor) / Consistent Video (for others).
- **Critical path:**
    1.  Select Anchor Frame ($I_{anc}$) and Edit Reference ($I_{ref}$).
    2.  **Zero-Stage:** For every target frame $I_{tgt}$, compute Corr between $I_{anc}$ and $I_{tgt}$. Generate degraded intermediate $I_{M(k)}^{tgt}$.
    3.  **Hero-Stage Training:** Train DiT-LoRA on the triplet ($I_{M(k)}^{anc}$, $I_{anc}$) $\to I_{ref}$.
    4.  **Hero-Stage Inference:** Apply trained LoRA to all frames using ($I_{M(k)}^{tgt}$, $I_{tgt}$) as input conditions.
- **Design tradeoffs:**
    -   **Mask size $k$:** The paper sets $k=1$ (most aggressive).
        -   *Pros:* Most accurate color transfer; fixed structure.
        -   *Cons:* Severe blur/oversaturation (requires robust Hero-Stage).
        -   *Alternative:* Higher $k$ reduces blur but causes "color leakage" (mixing colors from wrong spatial areas).
    -   **Training Pairs (Table 1):**
        -   *Mode 1 (Implicit):* Harder to restore missing background regions.
        -   *Mode 2 (Explicit):* Restores background perfectly but requires careful balancing to prevent appearance leakage.
- **Failure signatures:**
    -   **Color Leakage:** The output looks like a mix of the reference and original colors $\to$ Increase strictness of mask (decrease $k$) or check LoRA independence.
    -   **Over-saturation/Blur:** Output looks like a low-quality JPEG $\to$ Hero-Stage training failed or data pairs (d) caused a shortcut.
    -   **Missing Parts:** Background disappears in later frames $\to$ You are likely using Mode 1; switch to Mode 2 (Explicit condition).
- **First 3 experiments:**
    1.  **Calibrate $k$:** Run Zero-Stage alone on a static scene. Visualize the blur/saturation tradeoff as you sweep $k$ from $1$ to $h \times w$ to confirm the degradation hypothesis.
    2.  **Isolate Hero-Stage:** Train the LoRA *only* on the anchor frame pair ($I_{M(k)}^{anc} \to I_{ref}$). Verify it learns to deblur/desaturate before running it on the whole video.
    3.  **Ablate Mode 2:** Run Hero-Stage with only Condition 1 vs. Condition 1 + Condition 2 on a video with a complex, moving background. Check if the background is preserved.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the trade-off between correspondence accuracy and imaging degradation in masked attention be resolved theoretically without a restoration stage?
- Basis in paper: [explicit] The authors state that as the top-$k$ mask constraint tightens to improve accuracy, "intervention in the attention mechanism results in compounded imaging degradation," and conclude that "there exists an upper limit to what can be achieved with zero-shot methods."
- Why unresolved: The paper bypasses this limit by introducing the Hero-Stage (a conditional generative model) to restore the degraded outputs, rather than solving the intrinsic instability or "collapsed structure" within the zero-shot attention mechanism itself.
- What evidence would resolve it: A theoretical derivation or modified attention mechanism that maintains structural integrity (preventing blur) and color fidelity (preventing oversaturation) even when $k$ is small, rendering the restoration stage unnecessary.

### Open Question 2
- Question: Can a single framework adaptively handle both simple colorization and complex texture/style transfer without manual mode switching?
- Basis in paper: [explicit] The paper notes that Mode 1 (implicit usage) fails for style transfer ("forces the model to overfit"), whereas Mode 2 (explicit usage) succeeds. The current method requires selecting specific training data pairs and LoRA configurations (single vs. dual branch) based on the editing task.
- Why unresolved: The reliance on distinct "Modes" for different editing types suggests the model cannot currently distinguish or self-regulate the balance between preserving target structure and transferring reference texture/style autonomously.
- What evidence would resolve it: A unified training objective where the model dynamically attends to the reference for texture/style information while relying on the target for structure, without requiring explicit separation of training pairs or manual architectural toggles.

### Open Question 3
- Question: How does the method perform on videos with severe occlusion or non-rigid deformation where correspondence estimation fails entirely?
- Basis in paper: [inferred] The method relies on DIFT-based correspondence between the anchor and target frames. The authors mention "unsuccessful matching" causes color-missing issues in backgrounds, which Mode 2 mitigates by copying from the target.
- Why unresolved: It is unclear if the Hero-Stage can faithfully reconstruct reference details on the object if the correspondence is lost due to occlusion or topological changes (e.g., a person turning around), rather than just background changes where copying the target is acceptable.
- What evidence would resolve it: Evaluation on datasets specifically designed to test "correspondence failure" scenarios, such as objects undergoing 360-degree turns or substantial non-rigid deformations, comparing the hallucination capabilities of the Hero-Stage against ground truth.

## Limitations
- Evaluation limited to synthetic Blender videos with controlled conditions rather than real-world footage
- Performance may degrade significantly with extreme object motion or view-angle changes where correspondence estimation fails
- The two-stage approach requires careful balancing between transfer accuracy and quality restoration

## Confidence
- **High**: The dual-stage architecture effectively separates appearance transfer (Zero-Stage) from quality restoration (Hero-Stage)
- **Medium**: The claim that diffusion feature correspondence is more robust than optical flow for large motions - supported by qualitative results but limited quantitative comparison
- **Medium**: The assertion that Mode 2 prevents appearance leakage better than Mode 1 - theoretical justification provided but not extensively validated

## Next Checks
1. Test correspondence stability on videos with large view-angle changes or object disappearances to identify breaking conditions
2. Conduct ablation study on masking threshold k to find optimal tradeoff between transfer accuracy and degradation severity
3. Evaluate performance on real-world videos (not just synthetic Blender content) to assess practical applicability limits