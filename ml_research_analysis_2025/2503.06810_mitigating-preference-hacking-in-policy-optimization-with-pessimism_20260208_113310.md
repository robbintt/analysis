---
ver: rpa2
title: Mitigating Preference Hacking in Policy Optimization with Pessimism
arxiv_id: '2503.06810'
source_url: https://arxiv.org/abs/2503.06810
tags:
- preference
- policy
- data
- learning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces P3O and PRPO, two novel pessimistic RLHF algorithms
  designed to mitigate reward/preference hacking by incorporating uncertainty in preference
  models. P3O handles general pairwise preferences through a restricted pessimistic
  Nash formulation, while PRPO simplifies this for reward-based BTL preferences.
---

# Mitigating Preference Hacking in Policy Optimization with Pessimism

## Quick Facts
- arXiv ID: 2503.06810
- Source URL: https://arxiv.org/abs/2503.06810
- Reference count: 40
- Primary result: Novel pessimistic RLHF algorithms P3O and PRPO demonstrate superior resilience to overoptimization compared to standard baselines, achieving higher win-rates over reference policies without degrading performance or exhibiting hacking behaviors.

## Executive Summary
This work addresses the problem of reward/preference hacking in RLHF, where policies overfit to learned preference models by producing undesirable outputs (excessive length, repetitive lists, etc.). The authors introduce P3O and PRPO, two pessimistic RLHF algorithms that incorporate uncertainty in preference models through a restricted pessimistic Nash formulation. P3O handles general pairwise preferences while PRPO simplifies this for reward-based Bradley-Terry-Luce preferences. Empirically, both methods demonstrate improved resilience to overoptimization on summarization and helpfulness tasks while maintaining or improving performance relative to reference policies.

## Method Summary
The paper proposes two pessimistic RLHF algorithms: P3O for general pairwise preferences and PRPO for reward-based BTL preferences. Both methods solve a minimax game between a policy (maximizing preference) and an adversarial preference function (minimizing preference), with KL regularization ensuring the adversarial preference stays close to MLE estimates on in-distribution pairs. The algorithms use an EMA of past policies as the opponent distribution and incorporate uncertainty through KL regularization of the preference model. The key innovation is restricting the opponent policy to actions covered by the preference data, preventing pathological hedging on out-of-support outputs.

## Key Results
- P3O and PRPO achieve higher win-rates over reference policies compared to standard RLHF baselines (DPO, REINFORCE, Nash-EMA) on both TL;DR summarization and Anthropic HH helpfulness tasks
- Methods maintain KL divergence close to reference policies while baselines show increasing divergence indicating overoptimization
- P3O/PRPO avoid common hacking behaviors like excessive length and list-format artifacts that baselines exhibit
- P3O shows significant improvement over PRPO on TL;DR task, suggesting general preference formulation provides benefits

## Why This Works (Mechanism)

### Mechanism 1: Restricted Pessimistic Nash Formulation
Restricting the opponent policy in the pessimistic Nash game to actions covered by preference data prevents pathological hedging on out-of-support outputs. Standard pessimistic Nash allows the min player to choose any action, including unobserved ones, forcing the policy to hedge against completely uncertain preferences. By constraining to policies with bounded likelihood ratios with πdata, the algorithm competes only against actions where preference estimates are reliable.

### Mechanism 2: Adversarial Preference Model with KL-Regularized Uncertainty Set
Maintaining an adversarially-optimized preference model within a KL-regularized uncertainty set provides robustness to overoptimization. Instead of optimizing against a fixed preference model, the algorithm jointly optimizes a policy and an adversarial preference function, with KL regularization ensuring the adversarial preference stays close to MLE estimates on in-distribution pairs while allowing deviation on out-of-support comparisons.

### Mechanism 3: EMA Policy as Variational Approximation to Optimal Opponent
Using an exponential moving average (EMA) of policy iterates approximates the variational bound on the intractable log-partition function. The EMA policy serves as the opponent distribution, creating pessimistic self-play where the policy competes against a smoothed version of itself. This approximation is tight when the EMA policy matches the optimal variational distribution.

## Foundational Learning

- **Concept: Preference-based RL as two-player zero-sum games**
  - Why needed: Understanding JP(π, π', p) as a game where π maximizes and π' minimizes preference is foundational for the pessimistic Nash formulation
  - Quick check: Why does the Nash equilibrium of a symmetric preference game have identical policies for both players?

- **Concept: Pessimism in offline RL (epistemic uncertainty)**
  - Why needed: The core problem is that learned preference models are unreliable outside training data support; standard KL-regularization doesn't address this
  - Quick check: Why does KL(π∥πref) fail to address preference model uncertainty, and how does pessimism differ?

- **Concept: Variational inference for log-partition functions**
  - Why needed: The theoretical objective involves −log E[exp(−p/β)], which is intractable; Jensen's inequality provides a tractable upper bound
  - Quick check: Sketch how Jensen's inequality transforms the log-partition function into a form amenable to gradient-based optimization

## Architecture Onboarding

- **Component map:**
  - Policy network πθ -> generates responses -> updated via gradient ascent on JP3O
  - Preference model pφ (or reward model rφ for PRPO) -> scores response pairs -> updated via gradient descent (adversarial direction)
  - EMA policy π̄ -> maintained as γπt + (1−γ)π̄t -> used to define opponent distribution πα_mix
  - Pre-trained MLE models (pmle/rmle) -> frozen regularization targets -> pre-trained on preference dataset D

- **Critical path:**
  1. Pre-train pmle and rmle on preference dataset D via MLE
  2. Initialize π1 = πref, p1 = pmle, π̄1 = πref
  3. Each step: (a) sample from opponent πα_mix(π̄t, πdata); (b) compute preference between πt outputs and opponent samples; (c) update π (ascent) and p (descent); (d) update EMA π̄

- **Design tradeoffs:**
  - α (mixing coefficient): Higher α → stronger restriction to πdata support, may limit beneficial exploration
  - λ (preference regularization): Higher λ → tighter uncertainty set → less pessimism → more overoptimization risk
  - β (KL to πref): Paper uses β=10⁻⁵ vs. 0.1–1.0 for baselines; pessimism replaces KL as primary regularizer
  - γ (EMA decay): Too low → unstable opponent; too high → opponent lags current policy

- **Failure signatures:**
  - Length hacking persists → λ too low or preference model underfitting
  - Policy collapses to single response → α too restrictive or preference model collapsed
  - No improvement over baseline → verify p is being updated adversarially (gradients flowing), not frozen

- **First 3 experiments:**
  1. Verify adversarial dynamics: Log preference scores on held-out pairs from D; confirm they decrease during training
  2. Replicate tabular example: Run EP3O(0.1) vs. EP3O(0) on Figure 3 setup to verify restricted Nash prevents pathological y3 allocation
  3. Monitor hacking proxies: Track response length and list-format fraction during training; confirm P3O/PRPO converge to πdata statistics while baselines diverge

## Open Questions the Paper Calls Out

### Open Question 1
Does the KL-regularization heuristic used to approximate the restricted policy set Π(πdata, C) preserve the theoretical guarantees of the restricted pessimistic Nash formulation, or does it introduce approximation errors that could lead to performance degradation? The authors acknowledge this is a "lossy" heuristic because the exact definition of Π(πdata, C) is not amenable to easy implementation.

### Open Question 2
How does the looseness of the variational upper bound in Lemma 4.2 affect the convergence and optimality of the learned policy compared to optimizing the ideal theoretical objective? The authors note that maximizing the variational bound is not equivalent to maximizing the true objective due to the direction of the inequality.

### Open Question 3
How does the algorithm's performance and sensitivity to the mixing coefficient α change when the reference policy πref differs significantly from the data sampling policy πdata? The authors set α = 0 in their experiments because πref and πdata are similar, leaving this case untested.

### Open Question 4
Does the minimax training dynamic of P3O suffer from rotational instability or convergence issues common to adversarial games, and do modifications like optimistic mirror descent improve optimization? The algorithm is formulated as a "minimax game between a policy and a preference player" solved via gradient ascent-descent.

## Limitations
- Theoretical analysis relies on idealized assumptions (exact symmetry of p*, perfect data coverage) that may not hold in practice
- Practical benefits of restricted pessimistic Nash depend heavily on data coverage quality, which varies across tasks
- Variational approximation using EMA policies lacks direct empirical validation of approximation quality
- KL-regularized uncertainty set assumption may fail when MLE preference models are misspecified

## Confidence
- **High Confidence:** Empirical results showing P3O/PRPO achieve higher win-rates over reference policies without degradation in KL divergence or increased length hacking
- **Medium Confidence:** Theoretical justification for restricted pessimistic Nash provides sound mathematical framework, but practical benefits depend on data coverage
- **Low Confidence:** Variational approximation using EMA policies lacks direct empirical validation, and KL-regularized uncertainty set assumption may fail with misspecified models

## Next Checks
1. Test P3O/PRPO on a preference dataset with artificially introduced gaps (missing entire response categories) to measure degradation in performance and determine failure thresholds for the data coverage assumption
2. Compare against a simpler baseline: pessimistic RLHF using only KL regularization to πref without the adversarial preference model component, to isolate the contribution of each mechanism
3. Implement an ablation study varying the KL regularization coefficient λ across multiple orders of magnitude to identify the sensitivity of overoptimization resistance to this hyperparameter