---
ver: rpa2
title: Using Large Language Models for education managements in Vietnamese with low
  resources
arxiv_id: '2501.15022'
source_url: https://arxiv.org/abs/2501.15022
tags:
- data
- language
- training
- question
- educational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes VietEduFrame, a framework for applying large
  language models (LLMs) to educational management tasks in Vietnamese institutions.
  The authors developed a tailored dataset from student education documents at Hanoi
  VNU, addressing resource constraints.
---

# Using Large Language Models for education managements in Vietnamese with low resources

## Quick Facts
- arXiv ID: 2501.15022
- Source URL: https://arxiv.org/abs/2501.15022
- Reference count: 18
- The Vistral model with LoRA fine-tuning achieved an exact match score of 43.23% and F1-score of 81.24% on Vietnamese educational management QA tasks.

## Executive Summary
This paper introduces VietEduFrame, a framework for applying large language models to educational management tasks in Vietnamese institutions with limited resources. The authors developed a custom dataset from student education documents at Hanoi VNU and fine-tuned both Bloom and Vistral models with and without LoRA (Low-Rank Adaptation). The Vistral model with LoRA achieved the best performance, demonstrating that LLMs can be effectively applied to Vietnamese educational management despite resource constraints. The framework addresses the challenge of scarce Vietnamese educational data through synthetic data generation and parameter-efficient fine-tuning techniques.

## Method Summary
The authors developed a custom Vietnamese QA dataset of 985 samples by extracting regulations from VNU student documents and generating synthetic QA pairs using GPT-3.5-turbo with Chain-of-Thought, Self-Consistency, and Tree of Thought prompting techniques. Human evaluators reviewed the generated data, classifying 54.92% as "Very good" quality. They fine-tuned Vistral-7B and Bloom models using LoRA with rank=128 and dropout=0.1, along with full fine-tuning as baseline. Training used AdamW optimizer with warmup ratio=0.05, weight decay=0.01, batch size=4-8, and max_length=1024. The Vistral model, pre-trained specifically for Vietnamese, outperformed the multilingual Bloom model, particularly when combined with LoRA fine-tuning.

## Key Results
- Vistral + LoRA achieved Exact Match 43.23 and F1 81.24, outperforming Bloom + LoRA (EM 33.89, F1 72.36) by approximately 9 points in EM
- LoRA reduced Vistral's GPU memory usage from 61.2GB to 32GB and training time from 14 to 6 hours per epoch
- The framework successfully applied LLMs to Vietnamese educational management with limited resources, though with identified limitations in reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient fine-tuning via LoRA enables deployment of 7B-parameter models on constrained GPU infrastructure with marginal performance trade-offs. LoRA decomposes weight updates into low-rank matrices (Wup ∈ R^(d×r), Wdown ∈ R^(r×k) where r ≪ min(d,k)), freezing the pretrained backbone and training only the adapter weights. This reduces trainable parameters by approximately (d×k)/(d+k)/r times. Core assumption: Task-specific knowledge for Vietnamese educational QA can be captured in low-dimensional subspaces without modifying foundational representations. Evidence: LoRA reduces Vistral GPU usage from 61.2GB to 32GB while maintaining F1-score at 81.24 vs 81.57 (full fine-tuning).

### Mechanism 2
Synthetic data generation using advanced prompting techniques (CoT, Self-Consistency CoT, Tree of Thought) can produce viable training data when authentic annotated corpora are unavailable. Raw regulatory documents are segmented into context passages, then GPT-3.5-turbo generates (context, question, answer) triplets guided by engineered prompts. Quality is assessed via ROUGE/BLEU metrics and human evaluation. Core assumption: Synthetic Q&A pairs derived from authoritative source documents preserve sufficient semantic fidelity for downstream fine-tuning. Evidence: Human evaluators classified 54.92% as "Very good" and 28.28% as "Good" quality.

### Mechanism 3
Language-specialized pre-training provides measurable advantages for domain-specific QA in low-resource settings, outperforming multilingual alternatives. Vistral-7B, pre-trained specifically for Vietnamese with sliding-window attention and rolling buffer cache, captures language-specific patterns that Bloom (46 languages) dilutes across multilingual capacity. Core assumption: Vietnamese-specific pre-training corpora and architectural optimizations transfer to specialized educational QA. Evidence: Vistral + LoRA achieves Exact Match 43.23 and F1 81.24 vs Bloom + LoRA at 33.89 and 72.36 respectively—approximately 9-point EM improvement.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables training 7B models on 16-32GB GPUs instead of requiring 60GB+. Critical for resource-constrained academic deployments.
  - Quick check question: Given a weight matrix W₀ ∈ R^(4096×4096) and rank r=128, how many parameters does LoRA add versus full fine-tuning?

- **Extractive QA Evaluation Metrics (Exact Match, F1)**
  - Why needed here: The paper reports EM=43.23 and F1=81.24—understanding why F1 substantially exceeds EM reveals token-level partial credit versus all-or-nothing evaluation.
  - Quick check question: If the gold answer is "giảng viên" and the prediction is "giảng viên và sinh viên," what are the precision, recall, and F1?

- **Vietnamese Tokenization Challenges**
  - Why needed here: Vietnamese is an isolating language without word boundaries marked by spaces. Tokenization quality directly impacts both training efficiency and metric computation.
  - Quick check question: How does the absence of explicit word boundaries in Vietnamese text affect F1-score calculation compared to English?

## Architecture Onboarding

- **Component map:**
  Raw regulations -> Text cleaning/segmentation -> GPT-3.5-turbo prompt-based generation -> Human quality review -> (context, question, answer) triplets (985 samples) -> Vistral-7B backbone + LoRA adapters -> AdamW training -> Exact Match + Token-level F1 evaluation

- **Critical path:**
  1. Regulatory document preprocessing (segmentation, formula conversion to KATEX)
  2. Prompt engineering for synthetic Q&A generation (CoT/Self-Consistency/ToT techniques)
  3. Human annotation quality gating (target: >80% "Very good" + "Good")
  4. LoRA adapter initialization (Wdown random Gaussian, Wup zero-initialized)
  5. Training with early stopping on validation loss
  6. Inference with merged LoRA weights (no latency overhead)

- **Design tradeoffs:**
  - Vistral vs Bloom: Vistral offers +9 EM points but requires 32GB vs 16GB GPU RAM with LoRA
  - Full fine-tuning vs LoRA: Full fine-tuning yields marginally higher F1 (81.57 vs 81.24) but doubles training time and nearly doubles memory
  - Synthetic vs real data: Synthetic enables rapid dataset creation but 6.78% "Bad" + 1.05% "Very Bad" samples may introduce noise

- **Failure signatures:**
  - Low EM with high F1: Model generates correct semantic content but fails exact string matching (common with verbose answers—see Table 8 example with redundant "Hãy suy nghĩ từng bước")
  - Validation loss diverges from training loss: Overfitting on synthetic data patterns; reduce epochs or increase dropout
  - Context length errors: Regulatory contexts exceed 1024 tokens (observed max: 4446); requires chunking or longer context window

- **First 3 experiments:**
  1. **Baseline replication:** Train Vistral-7B + LoRA on provided dataset with hyperparameters from Table 5. Verify EM≈43, F1≈81 on held-out validation. Purpose: Confirm reproducibility.
  2. **Ablation on LoRA rank:** Test r=16, 32, 64, 128, 256. Plot EM/F1 vs GPU memory. Purpose: Identify optimal efficiency-performance frontier for target hardware.
  3. **Data quality filter:** Retrain after removing "Bad" and "Very Bad" samples (7.83% of data). Measure impact on metrics. Purpose: Quantify noise tolerance of synthetic data pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
How effectively does the VietEduFrame framework generalize to educational management regulations from institutions other than Vietnam National University (VNU) Hanoi? The current study relies exclusively on a custom dataset derived from student education documents and regulations specific to VNU Hanoi. Testing the fine-tuned Vistral and Bloom models on an external dataset from a different Vietnamese university (e.g., VNU Ho Chi Minh City) and reporting the subsequent Exact Match and F1 scores would resolve this question.

### Open Question 2
Can specific prompting strategies during inference, such as Chain-of-Thought (CoT), mitigate the logical reasoning limitations identified in the fine-tuned models? While CoT was used for synthetic data generation, the paper does not evaluate whether applying such reasoning techniques during inference improves performance on complex, multi-step queries. A comparative analysis of standard inference versus CoT-prompted inference on a subset of complex "Medium" or "Bad" quality questions would measure improvements in logical consistency.

### Open Question 3
Do the "Contextual Errors" identified in the limitations arise primarily from the 1024-token context window limit truncating vital regulatory information? The paper does not analyze performance degradation relative to document length or context window saturation, despite the max length setting potentially clipping longer regulatory sections. An ablation study evaluating model performance on regulatory contexts of varying lengths (e.g., 512 vs 2048 tokens) using a model variant with a larger effective context window would resolve this question.

## Limitations
- The framework's generalizability is limited to the specific Vietnamese university (VNU Hanoi) whose regulations were used for training
- Reliance on synthetic data generation introduces potential quality issues, with 6.78% of samples classified as "Bad" quality
- The 1024-token context window may truncate lengthy regulatory documents, leading to contextual errors in answers

## Confidence
**High Confidence Claims:**
- LoRA effectively reduces GPU memory requirements while maintaining reasonable performance
- Vistral-7B outperforms Bloom on Vietnamese educational QA tasks
- Synthetic data generation with human review can produce viable training datasets

**Medium Confidence Claims:**
- The framework's generalizability to other Vietnamese educational institutions beyond VNU
- The sufficiency of 985 samples for robust model performance across diverse educational management scenarios
- The long-term stability of LoRA adapters without catastrophic forgetting

**Low Confidence Claims:**
- The scalability of this approach to real-time production environments with high query volumes
- The model's ability to handle complex reasoning tasks beyond extractive QA
- The robustness of synthetic data quality assessment methodology

## Next Checks
1. **Dataset Quality Validation**: Conduct blind human evaluation comparing model predictions on synthetic vs. real student queries from multiple Vietnamese institutions. Measure performance degradation when models trained on synthetic data encounter authentic user queries, with target: <5% absolute F1 drop.

2. **Cross-Institutional Generalization**: Deploy the fine-tuned Vistral+LoRA model on educational management datasets from at least two other Vietnamese universities (not used in training). Evaluate EM and F1 scores, specifically testing domain transfer for regulatory compliance questions versus general knowledge questions.

3. **Long-Term Adaptation Testing**: Implement continuous learning protocol where the model encounters new Vietnamese educational regulations monthly. Measure catastrophic forgetting by tracking performance on original validation set while adapting to new content, with target: maintain >85% of original F1 after 6 months of updates.