---
ver: rpa2
title: 'FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA
  Hardware Co-design'
arxiv_id: '2505.16335'
source_url: https://arxiv.org/abs/2505.16335
tags:
- quantization
- layer
- fpqv
- arxiv
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FPQVAR, an efficient post-training floating-point
  (FP) quantization framework for visual autoregressive (VAR) models with FPGA hardware
  co-design. The authors address the challenges of quantizing VAR models, which include
  low-bit INT format incurring large quantization error, highly imbalanced activation
  distribution of the FC2 layer, and time-varying outlier channels.
---

# FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design

## Quick Facts
- arXiv ID: 2505.16335
- Source URL: https://arxiv.org/abs/2505.16335
- Reference count: 40
- 4-bit quantization improves FID from 10.83 to 3.58 and IS from 175.9 to 241.5

## Executive Summary
FPQVAR presents a post-training quantization framework for visual autoregressive models that addresses the unique challenges of low-bit quantization in VAR architectures. The method combines Dual Format Quantization (DFQ) for highly imbalanced activations, Group-wise Hadamard Transformation (GHT) for outlier reduction, and GALT for learnable smoothing. At the hardware level, it implements the first low-bit floating-point quantizer and multiplier on FPGA, achieving 3.1× higher throughput and 3.6× better energy efficiency than integer-based accelerators.

## Method Summary
FPQVAR uses post-training quantization to compress the VAR model through three key techniques: DFQ separates positive and negative activations in the FC2 layer using different FP formats to match their imbalanced distributions; GHT applies block-diagonal Hadamard transformations to reduce outliers while enabling pipelined hardware execution; GALT learns a smoothing factor via optimization to minimize quantization error across all time steps. The FPGA accelerator implements low-bit FP computation using lookup tables and features a two-level pipeline for overlapping GHT, quantization, and matrix multiplication operations.

## Key Results
- 4-bit quantization achieves FID of 3.58 and IS of 241.5, significantly outperforming state-of-the-art methods
- FPGA accelerator achieves 1.1 image/s throughput, 3.1× higher than integer-based accelerators
- Demonstrates 3.6× and 2.8× higher energy efficiency compared to integer-based accelerator and GPU baseline

## Why This Works (Mechanism)

### Mechanism 1: Dual Format Quantization (DFQ)
The FC2 layer input shows 97.6% of values concentrated between -0.17 and 0, with only 2.4% in the positive tail. Standard FP quantization wastes half its levels on the sparse negative region. DFQ assigns different FP grids (FP4-E1M2 for negative, FP4-E2M1 for positive) with independent scales, aligning quantization density to actual distribution.

### Mechanism 2: Group-wise Hadamard Transformation (GHT)
Full Hadamard Transformation requires multiplying the entire activation tensor by H∈R^(C×C), which must execute online in VAR because the scale factor α is generated at runtime. GHT replaces H with block-diagonal H_B comprising 128×128 submatrices, reducing FLOPs by 15× when C=1920 and enabling rotation-quantization-MM pipelining.

### Mechanism 3: GALT
GALT optimizes λ to minimize quantization error across all time steps, eliminating the need for step-specific smoothing. The smoothing factor is absorbed offline into weights and the MLP layer generating α/β, adding zero runtime overhead while improving over GHT alone.

## Foundational Learning

- **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**: PTQ avoids full retraining but requires careful calibration. Quick check: Given a pretrained model, would you expect PTQ or QAT to achieve better accuracy at 4-bit precision, and what is the tradeoff?

- **Hadamard Matrices and Outlier Amortization**: HT rotates activations to distribute outlier magnitude across channels. Quick check: If you apply HT to activations but not to weights, does the matrix multiplication result change? Why or why not?

- **FPGA LUT-based Arithmetic**: FP4 multipliers use lookup tables rather than DSP slices, trading DSP utilization for LUT consumption. Quick check: For a 4-bit × 4-bit multiplication with FP8 output, how many LUT entries are required? What is the address width?

## Architecture Onboarding

- Component map:
  ```
  VAR Transformer (97% of compute/storage)
  ├── Condition MLP → generates α, β per step (fused with GALT λ)
  ├── Transformer Blocks (×N)
  │   ├── LayerNorm → Scale/Shift (α, β)
  │   ├── MHA
  │   │   ├── QKV Projection [GHT + GALT applied] → FP4 GeMM
  │   │   └── Out Projection → FP4 GeMM
  │   └── FFN
  │       ├── FC1 [GHT + GALT applied] → FP4 GeMM
  │       └── FC2 [DFQ applied] → DFQ GeMM (separate pos/neg paths)
  └── VQVAE Decoder (3% of compute, not quantized)

  FPGA Accelerator (VCK190)
  ├── FMU (Floating-point Matrix Unit)
  │   ├── FP4-E2M1 GeMM (standard layers)
  │   └── DFQ GeMM (FC2 layer)
  ├── GHTU (128-point Fast Hadamard Transform)
  └── SFU (LayerNorm, Softmax, GeLU)
  ```

- Critical path: The two-level pipeline is the key latency reducer. Level 1: GHT→Quant→MM pipelining within each block (enabled by 128-group alignment). Level 2: Overlap Condition MLP of block i+1 with FC2 of block i. Combined: 2.3× latency reduction.

- Design tradeoffs:
  - FP4 vs INT4: FP4-E2M1 has 15 levels (vs 16 for INT4) but 1.5× lower quantization error for Gaussian distributions. Tradeoff: Slightly reduced dynamic range, better aligned to weight/activation distributions.
  - GHT vs full HT: GHT is 3.9× faster but less effective (IS 219.7 vs 232.3). GALT recovers performance without throughput cost.
  - DFQ vs standard FP: DFQ requires two quantization paths and separate LUTs, increasing area (two LUT4 sets vs one LUT5 set) but critical for FC2 layer.

- Failure signatures:
  - FC2 quantization failure: If IS drops significantly and generated images show artifacts, verify DFQ is applied to FC2 input specifically. Check that negative grid is FP4-E1M2 and positive is FP4-E2M1.
  - GALT convergence failure: If optimization loss plateaus early, check calibration dataset diversity. Ensure 100 samples cover representative conditions and all K steps are included.
  - Pipeline stall: If throughput is below 1.0 img/s, verify GHT group size matches quantization group size (128). Misalignment forces serial execution.

- First 3 experiments:
  1. **Reproduce FP4 baseline**: Quantize VAR-d30 with FP4-E2M1 for all weights and activations, no DFQ/GHT/GALT. Target: IS ~167, FID ~10.72.
  2. **Ablate DFQ on FC2 only**: Apply DFQ to FC2 input, keep other layers at standard FP4. Target: IS ~189.5, FID ~8.25.
  3. **Measure GHT latency on FPGA**: Implement 128-point FHT unit and measure cycles for single group transformation. Compare against theoretical 128×log₂(128) ≈ 896 operations. Target: <1.5× overhead vs theoretical.

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- The paper relies on calibration datasets (100 samples) to optimize GALT and DFQ parameters offline, which may not generalize to all inference conditions.
- GHT's block-diagonal approximation may not fully eliminate outliers in highly correlated channel groups, potentially leaving quantization noise.
- Hardware evaluation is limited to a single FPGA platform (AMD-Xilinx VCK190), limiting generalizability to other accelerators.

## Confidence
- **High Confidence**: DFQ's effectiveness for the FC2 layer (97.6% activation concentration is directly observable in the paper's analysis).
- **Medium Confidence**: GHT + GALT performance claims, as ablation studies show incremental gains but the combined optimization is complex.
- **Medium Confidence**: Hardware acceleration claims (1.1 img/s, 3.1× speedup) due to lack of comparison to alternative FPGA designs or ASIC implementations.

## Next Checks
1. Test DFQ sensitivity by perturbing the calibration dataset size (e.g., 10 vs. 100 samples) and measuring FID degradation.
2. Evaluate GHT's outlier reduction on a synthetic activation tensor with known outlier patterns to verify the 15× FLOPs reduction claim.
3. Implement the FPGA DFQ multiplier unit and measure LUT utilization vs. INT4 multiplier to confirm the area tradeoff.