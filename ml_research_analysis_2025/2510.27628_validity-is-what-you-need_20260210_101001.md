---
ver: rpa2
title: Validity Is What You Need
arxiv_id: '2510.27628'
source_url: https://arxiv.org/abs/2510.27628
tags:
- agentic
- arxiv
- systems
- which
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a realist definition of Agentic AI as a software
  delivery mechanism that puts multi-step applications to work autonomously in complex
  enterprise settings. The authors argue that while foundation models like LLMs enable
  agentic systems, their success depends on validation by end users and stakeholders.
---

# Validity Is What You Need

## Quick Facts
- arXiv ID: 2510.27628
- Source URL: https://arxiv.org/abs/2510.27628
- Reference count: 40
- Primary result: Strong validation enables simpler, interpretable models to replace LLMs in enterprise Agentic AI.

## Executive Summary
This paper proposes a realist definition of Agentic AI as a software delivery mechanism for deploying multi-step autonomous applications in complex enterprise environments. The authors argue that while foundation models like LLMs enable agentic systems, their success depends critically on validation by end users and stakeholders. They outline a multi-stage design process emphasizing mechanism design, objective definition, feedback analysis, and validation. The key irony is that with strong validation measures in place, simpler and more interpretable models can often replace LLMs, as they better handle core logic. Ultimately, validity—not the foundation model itself—is what matters for effective Agentic AI.

## Method Summary
The authors present a 5-stage design process for Agentic AI: (1) model enterprise context as a multi-agent system, (2) define operational objectives, (3) check for feedback loops and leaks via guardrails, (4) build the system using appropriate engines (LLM, SLM, or expert systems), and (5) implement continuous validation through unit tests, drift monitoring, and stress testing. The approach emphasizes that foundation models are components in a software supply chain, with validation and guardrails operationalizing enterprise-specific logic that pretraining data lacks.

## Key Results
- Agentic AI success depends on validation measures that bridge the information gap between general pretraining and specific enterprise context
- With robust validation, simpler and more interpretable models can replace general-purpose foundation models
- Multi-step agentic workflows face compounding error rates that require localized validation steps to arrest degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Validity measures may enable the replacement of general-purpose foundation models with simpler, more interpretable systems.
- Mechanism: Effective validation and guardrails operationalize the specific logic and constraints of an enterprise use case. Once this logic is formally captured, the "reasoning" load on the underlying model decreases, potentially allowing smaller models (SLMs) or deterministic expert systems to execute the task without the overhead or risk of a Large Language Model (LLM).
- Core assumption: The complexity of the enterprise logic can be effectively captured by the validation layer (tests, guardrails), and the remaining cognitive load is low enough for a simpler model to handle.
- Evidence anchors:
  - [abstract] "Ironically, with good validation measures in place, in many cases the foundation models can be replaced with much simpler, faster, and more interpretable models that handle core logic."
  - [section 5] "Natural language processing functions may be performed by smaller, more dedicated language models... Multi-step reasoning can be performed by more precisely designed expert systems..."
  - [corpus] "Recurrence-Complete Frame-based Action Models" and "State and Memory is All You Need..." suggest architectural alternatives to pure attention-based LLMs, supporting the feasibility of diverse inference engines.
- Break condition: If the validation logic itself requires deep semantic interpretation that only a general-purpose LLM can provide, the substitution fails.

### Mechanism 2
- Claim: Agentic AI success depends on bridging the "information gap" between general pretraining and specific enterprise context via application-level validation.
- Mechanism: Foundation models are trained on general data, creating an information deficit regarding specific enterprise constraints and goals. Validation by end-users (principals) and context-specific guardrails injects this missing information, aligning the system's outputs with situated needs.
- Core assumption: General pretraining data does not contain (or sufficiently weight) the specific rules, norms, or objectives of the target enterprise environment.
- Evidence anchors:
  - [section 3] "A specific Agentic AI deployment... will be deployed into a specific sociotechnical environment which we will presume is a proprietary context not available to the pretraining regime."
  - [section 1] "Agentic AI... faces complexity because of how it is situated in real enterprise contexts... And it manages that complexity by taking multiple, connected actions in context."
  - [corpus] "TRAIL: Trace Reasoning and Agentic Issue Localization" supports the difficulty of evaluating complex, situated traces, implicitly validating the need for specialized assessment over general benchmarks.
- Break condition: If a foundation model can infer specific enterprise constraints without explicit validation (zero-shot alignment), this mechanism is redundant.

### Mechanism 3
- Claim: Reliability in multi-step Agentic systems degrades non-linearly (compounding error) unless arrested by localized validation steps.
- Mechanism: Agentic workflows chain multiple distinct tasks. If the probability of success for a single step is $p$, the probability of success for a chain of $n$ steps is $p^n$. Without intervention, this exponential decay ensures failure in complex workflows.
- Core assumption: Steps in the agentic chain are dependent, and errors in early steps propagate or invalidate later steps.
- Evidence anchors:
  - [section 3] "...these errors compound as the number of tasks in a multi-step process increases... As an intuition pump, if an agent has .9 accuracy on a single task... that can compound to .9 x .9 x .9 x .9 = .66 accuracy."
  - [section 3] "For a multi-step agentic program... this significantly expands the surface area to manage... Non-standardized univariate model monitoring... needs to be performed at both an end to end as well as a discrete step level."
  - [corpus] Weak direct corpus evidence for the math; corpus papers focus more on trace evaluation ("TRAIL") than the probability of error propagation.
- Break condition: If the system possesses robust error correction or "backtracking" capabilities that prevent error propagation, the compounding effect is mitigated.

## Foundational Learning

- Concept: **The Principal-Agent Problem (Economics)**
  - Why needed here: The paper defines Agentic AI through the lens of a "principal" (stakeholder) employing an "agent" (AI) to act on their behalf. Understanding this relationship is critical to grasping why "alignment" and "validation" are the primary challenges.
  - Quick check question: How does the incentives of a chatbot (maximizing user engagement) differ from that of a fiduciary agent acting for a client?

- Concept: **The Bellman Equation**
  - Why needed here: The paper cites this as the formal basis for rational agency and reinforcement learning. It frames the AI's task as maximizing a value function over time, which helps explain why specifying the correct "reward" or objective is difficult but necessary.
  - Quick check question: In a multi-step process, how does the "discount factor" ($\beta$) affect the agent's preference for immediate vs. future rewards?

- Concept: **Software Supply Chain Security**
  - Why needed here: The paper treats the foundation model as a third-party component in a software supply chain. Understanding dependencies, versioning, and vendor lock-in is necessary to appreciate the risks of model drift and obsolescence.
  - Quick check question: Why might a software update to a "general" foundation model cause failures in a "specific" downstream application?

## Architecture Onboarding

- Component map:
  - **Principal/Stakeholder** -> **Application Designer** -> **The Mechanism (Agentic App)**: *Core Logic/Engine* (LLM, SLM, or Expert System) -> *Guardrails/Controls* (Input filters, output verifiers, state checks) -> *Context* (Proprietary data/finetuning) -> **Environment** (The enterprise system, multi-agent, sociotechnical)

- Critical path: **Mechanism Design** → **Objective Specification** → **Validation/Guardrails** → **Engine Selection**. The paper argues that engine selection (LLM vs. SLM) should be a final step, determined by the requirements defined in the earlier stages.

- Design tradeoffs:
  - **LLM vs. Expert System**: Trading off flexibility/generality for interpretabilty/guaranteed performance.
  - **General vs. Specific Data**: Relying on massive pretraining vs. investing in high-quality, specific finetuning data.
  - **End-to-End vs. Step-wise Monitoring**: Monitoring the final output vs. adding validation overhead to every intermediate step.

- Failure signatures:
  - **Context Drift**: The agent acts on outdated assumptions as the enterprise environment changes.
  - **Objective Leakage**: The agent optimizes for the explicit metric but violates the implicit intent (e.g., "approve all loans" to maximize volume, ignoring risk).
  - **Compounding Hallucination**: Small factual errors in early reasoning steps lead to completely irrelevant actions later.

- First 3 experiments:
  1. **Trace Step Degradation Analysis**: Run a multi-step agent task 100 times. Measure success rates at step 1, step 3, and step 5 to confirm if compounding error is present in your specific workflow.
  2. **Guardrail Stress Testing**: Intentionally feed "poisoned" context or adversarial inputs to the agent to verify if the designated guardrails catch the issue before it reaches the core engine or the output.
  3. **Model Substitution Test**: Once a validation suite is green, swap the core LLM for a smaller, quantized model (SLM) and measure the delta in pass-rates to test the hypothesis that "validity is what you need" (i.e., the logic is in the validation, not the model).

## Open Questions the Paper Calls Out

- Question: What specific frameworks are required for application-level evaluations that verify situated validity, distinct from current foundation model benchmarking?
  - Basis in paper: [explicit] The conclusion identifies "Application evaluations which verify and validate" as an "open research area," distinguishing them from model evaluations.
  - Why unresolved: Current benchmarks focus on general model capability rather than the specific, complex validity requirements of enterprise end-users.
  - What evidence would resolve it: A standardized suite of metrics and auditing protocols that successfully predict Agentic AI performance in specific sociotechnical contexts.

- Question: To what extent can AI systems automate the "mechanism design" process of translating dynamic stakeholder goals into operational metrics and guardrails?
  - Basis in paper: [explicit] The introduction explicitly invites research to "question how AI can improve on this design and governance process."
  - Why unresolved: The translation of social and organizational goals into technical constraints is currently a manual, complex specification task.
  - What evidence would resolve it: Demonstrations of AI systems autonomously generating valid objective functions and constraints that satisfy principal stakeholders.

- Question: Does the implementation of strong validation measures effectively allow smaller, interpretable models to replace Large Language Models (LLMs) for core agentic logic?
  - Basis in paper: [inferred] The paper hypothesizes that validity reduces the need for foundation models, while Section 5 notes it "remains to be seen" if LLMs are viable in business use cases.
  - Why unresolved: It is unclear if the efficiency gains of simpler models (SLMs, expert systems) are sufficient for complex tasks once the validation layer is established.
  - What evidence would resolve it: Comparative performance data showing equal or superior business outcomes when LLMs are swapped for simpler models within validated architectures.

## Limitations

- The central claim that robust validation enables simpler models to replace LLMs rests on an untested assumption that enterprise logic can be fully operationalized in guardrails
- The concept of "validity" as a metric remains operationalized only at a high level without specific quantitative thresholds or statistical tests
- The framework assumes stakeholders can accurately specify their true objectives and constraints, without addressing misaligned incentives or conflicting priorities

## Confidence

- **High Confidence**: The identification of compounding error in multi-step agentic workflows and the general importance of validation in enterprise settings
- **Medium Confidence**: The hypothesis that robust validation enables simpler models to replace LLMs
- **Low Confidence**: The assertion that the proposed 5-stage design process is sufficient for all enterprise agentic AI deployments

## Next Checks

1. **Cross-Validation of the Substitution Hypothesis**: Select three diverse enterprise workflows (e.g., financial document processing, customer service triage, and inventory management). For each, implement the full validation infrastructure as described, then A/B test an LLM-based agent against a simpler alternative (SLM or expert system). Measure not just end-to-end success rates but also resource consumption, latency, and failure modes to determine if the "validity-first" hypothesis holds across domains.

2. **Validation Infrastructure Stress Testing**: Design adversarial test suites that probe the boundaries of the validation logic. Include cases with subtly conflicting objectives, out-of-distribution inputs, and gradual context drift. Measure the false negative rate (valid inputs rejected) versus false positive rate (invalid inputs accepted) to assess whether the guardrails introduce their own forms of bias or brittleness.

3. **Longitudinal Validity Decay Study**: Deploy a validated agentic system in a real enterprise environment for 90 days. Continuously monitor the drift in input distributions, objective alignment scores, and validation pass rates. Correlate these metrics with business outcomes to determine if "validity" as operationalized in the paper predicts real-world performance and whether the proposed monitoring mechanisms can detect and prevent performance degradation before it impacts stakeholders.