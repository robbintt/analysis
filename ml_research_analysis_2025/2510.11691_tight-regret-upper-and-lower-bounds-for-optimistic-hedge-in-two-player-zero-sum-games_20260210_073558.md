---
ver: rpa2
title: Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player Zero-Sum
  Games
arxiv_id: '2510.11691'
source_url: https://arxiv.org/abs/2510.11691
tags:
- regret
- learning
- bounds
- lower
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the optimality of regret bounds for optimistic\
  \ Hedge in two-player zero-sum games. The key contributions are: Refined Regret\
  \ Analysis: The authors refine the regret analysis of optimistic Hedge, showing\
  \ that social and individual regret bounds can be improved to O(\u221Alog m log\
  \ n) when each player knows the opponent's number of actions (cardinality-aware\
  \ setting)."
---

# Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player Zero-Sum Games

## Quick Facts
- arXiv ID: 2510.11691
- Source URL: https://arxiv.org/abs/2510.11691
- Reference count: 40
- Primary result: Optimistic Hedge achieves tight regret bounds in two-player zero-sum games, with social regret being exactly optimal

## Executive Summary
This paper investigates the optimality of regret bounds for optimistic Hedge in two-player zero-sum games. The authors refine regret analysis, showing that social and individual regret bounds can be improved to O(√log m log n) when each player knows the opponent's number of actions. They derive algorithm-dependent lower bounds that prove the social regret bound is optimal including the leading constant. The analysis extends to dynamic regret with improved upper bounds and matching lower bounds.

## Method Summary
The paper analyzes optimistic Hedge in two-player zero-sum games, deriving upper and lower bounds on social and individual regret. For cardinality-aware settings where players know opponent's action count, the authors optimize learning rates to achieve O(√log m log n) regret bounds. They express regret upper bounds as optimization problems with respect to learning rates and negative term coefficients. The analysis provides closed-form solutions for social regret and requires numerical optimization for individual regret. The paper also extends results to dynamic regret scenarios.

## Key Results
- Refined social and individual regret bounds to O(√log m log n) in cardinality-aware settings
- Derived algorithm-dependent lower bounds proving social regret optimality including leading constants
- Extended analysis to dynamic regret with improved upper and lower bounds
- Showed the factor of 2 gap between individual regret upper bound (2√log m log n) and lower bound (√log m log n - o(1))

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cardinality-aware learning rates improve social regret from O(log(mn)) to O(√log m log n) when action spaces are imbalanced.
- Mechanism: Players knowing opponent's action count n allows setting learning rate η = √(log m · (log n + 1/2)) / D where D = √((M+1/2)(N+1/2)) + √(MN). This asymmetric tuning optimizes the regret bound's leading constants via AM-GM inequality, particularly effective when log m/log n is large.
- Core assumption: Players are in "cardinality-aware strongly-uncoupled" setting—they observe their own rewards and know opponent's action cardinality, but not opponent's strategies or observations.
- Evidence anchors:
  - [abstract] "in the strongly-uncoupled setting where the opponent's number of actions is known, both the social and individual regret bounds can be improved to O(√log m log n)"
  - [Section 3.2] Lemma 4 provides optimal parameters achieving min Ω = 2√(M·(N+1/2)) + 2√(N·(M+1/2))
  - [corpus] Weak corpus support; related work "Optimism Without Regularization" shows constant regret achievable but doesn't address cardinality-dependent bounds
- Break condition: If players cannot know opponent's action count, must use η = η' = 1/2, reverting to O(log(mn)) bound.

### Mechanism 2
- Claim: Negative term decomposition with coefficient c enables tight individual regret bounds.
- Mechanism: The regret bound contains negative terms (1-c)/(2η)Σ||xt - xt-1||²₁. By introducing parameter c ∈ (0,1), the analysis balances two roles: (1) offsetting the positive term η/(2c)Σ||gt - gt-1||²∞, and (2) bounding strategy variation to transfer regret between players.
- Core assumption: The payoff matrix is bounded in [-1, 1] ensuring ||gt - gt-1||∞ ≤ ||yt - yt-1||₁.
- Evidence anchors:
  - [abstract] "we express the regret upper bound as an optimization problem with respect to the learning rates and the coefficients of certain negative terms"
  - [Section 3.1] Lemma 2 shows Reg^x_T ≤ log m/η + η/(2c)Σ||gt - gt-1||²∞ - (1-c)/(2η)Σ||xt - xt-1||²₁
  - [corpus] No direct corpus evidence for this specific negative term mechanism
- Break condition: If c approaches 1 (minimizing one player's regret), the constraint ηη' ≤ c'(1-c) forces η' → 0, effectively making opponent play uniform strategy.

### Mechanism 3
- Claim: Algorithm-dependent lower bounds prove social regret optimality including leading constants.
- Mechanism: Construct payoff matrix A with A(1,1)=0, A(1,j)=Δ for j≠1, A(i,1)=-Δ for i≠1. Against this structure, optimistic Hedge's weight ratio follows wt(k)/wt(1) = exp(-ηΔt), yielding closed-form regret lower bound log m/η - o(1).
- Core assumption: The adversary can select worst-case payoff matrices; optimistic Hedge uses fixed learning rates independent of specific game instance.
- Evidence anchors:
  - [Section 4] Theorem 10: individual regret lower bounded by log m/η - log((m-1)(T+1)) + 1/(η(T+1))
  - [Section 4.1] Eq. (11) derives wt(k)/wt(1) = exp(-ηΔt) from update rule
  - [corpus] "A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice" provides related tight lower bound methodology
- Break condition: If algorithm can adapt learning rate to payoff structure (violating strongly-uncoupled assumption), bounds may not hold.

## Foundational Learning

- Concept: **Online Convex Optimization / Regret Minimization**
  - Why needed here: The paper builds on the fundamental setup where a learner selects actions, observes losses, and aims to minimize cumulative regret vs. best fixed action in hindsight.
  - Quick check question: Can you explain why regret of O(√T log m) is minimax optimal for adversarial losses with m actions?

- Concept: **Exponential Weights / Hedge Algorithm**
  - Why needed here: Optimistic Hedge is the core algorithm—the update xt(i) ∝ exp(η(Σs=1 to t-1 gs(i) + gt-1(i))) extends standard Hedge by incorporating the most recent observed gradient.
  - Quick check question: How does the "optimistic" modification (adding gt-1) differ from standard Hedge, and why might it help in game settings?

- Concept: **Zero-Sum Games and Minimax Duality**
  - Why needed here: The social regret SocialReg_T = Reg^x_T + Reg^y_T equals the duality gap, and bounded social regret implies convergence to ε-approximate Nash equilibrium at rate O(SocialReg_T/T).
  - Quick check question: Why does bounded regret for both players imply the time-averaged strategies form an approximate Nash equilibrium?

## Architecture Onboarding

- Component map:
  - Optimistic Hedge core (Algorithm 1): Updates weights via Eq. (1) with learning rates η, η'
  - Parameter optimizer: Solves min_{(η,η',c,c')∈Λ} Ω(η,η',c,c') for social regret or min max{f(λ),g(λ)} for individual regret
  - Gradient observer: Receives gt = Ayt for x-player, ℓt = A^T xt for y-player
  - Time-averaged strategy variant (Algorithm 2): For dynamic regret, maintains bxt via optimistic Hedge, outputs xt = (1/t)Σs bxs

- Critical path:
  1. Determine setting: cardinality-aware (can use m, n) vs. cardinality-unaware (use constants)
  2. Select objective: social regret (use Lemma 4 closed-form) vs. individual regret (solve convex optimization via log-transformed variables p, p', q, q')
  3. Set learning rates η, η' from optimization result
  4. Run optimistic Hedge update; for dynamic regret, use Algorithm 2 with reconstruction Eq. (16)

- Design tradeoffs:
  - **Social vs. individual regret**: Parameters minimizing social regret (c=c'=√(M'N')/D) lie on constraint boundary where f=g=∞—individual regret cannot be bounded
  - **Cardinality-awareness**: Requires knowing opponent's action count; violates pure strongly-uncoupled assumption but enables √(MN) vs. M+N improvement
  - **Closed-form vs. numerical**: Social regret admits closed-form; individual regret max requires solving convex program (Section B.4.4)

- Failure signatures:
  - If ηη' > c'(1-c) or ηη' > c(1-c'), social regret bound Ω does not apply—sum of positive terms can't be cancelled
  - If m, n highly imbalanced and using cardinality-unaware rates, social regret dominated by larger player's log factor
  - If using social-optimal rates but querying individual regret, the analysis provides no guarantee (f, g undefined)

- First 3 experiments:
  1. **Verify lower bound construction**: Implement payoff matrix from Eq. (9) with Δ = 1, run optimistic Hedge with various η, confirm Reg^x_T approaches log m/η as T grows
  2. **Compare cardinality-aware vs. unaware**: Set (m,n) = (2, 10^4) as in Figure 2, measure social regret gap between η=1/2 and η from Eq. (4)
  3. **Test individual regret tradeoff curve**: For γ ∈ (0,1), solve min J_γ(λ), plot resulting f*(γ) vs. g*(γ) to visualize x-player vs. y-player regret tradeoff (reproduce Figure 1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the factor-of-two gap between the individual regret upper bound ($2\sqrt{\log m \log n}$) and lower bound ($\sqrt{\log m \log n} - o(1)$) for optimistic Hedge be closed?
- Basis in paper: [explicit] "For the individual regret, the upper and lower bounds match in many cases up to constant factors... this leaves an additive gap of $\log n$ compared with $\log m - o(1)$... Closing the gaps between the upper and lower bounds for individual regret remains an important direction for future work."
- Why unresolved: The current analysis framework treats negative terms with a parameter $c$ that creates a tradeoff between players' regrets, and the optimization approach does not yield matching bounds for individual regret.
- What evidence would resolve it: Either a refined analysis showing individual regret upper bounds of $\sqrt{\log m \log n} + O(1)$ for optimistic Hedge, or a lower bound construction achieving $2\sqrt{\log m \log n} - o(1)$.

### Open Question 2
- Question: Do the cardinality-aware regret improvements extend to external regret minimization and swap regret minimization in multiplayer general-sum games?
- Basis in paper: [explicit] "It is an interesting question whether such improvements also extend to external regret minimization (Anagnostides et al., 2022a) and swap regret minimization (Anagnostides et al., 2022b; Tsuchiya et al., 2025) in multiplayer general-sum games."
- Why unresolved: The current analysis is specific to two-player zero-sum games and the optimistic Hedge algorithm; the interaction between cardinality-awareness and more complex regret notions is unexplored.
- What evidence would resolve it: Deriving $O(\sqrt{\prod_i \log n_i})$ regret bounds for cardinality-aware learning dynamics in general-sum games with swap regret, or proving that no such improvement is possible.

### Open Question 3
- Question: Can algorithm-independent lower bounds on the dependence on $m$ and $n$ be derived for general strongly-uncoupled learning dynamics in two-player zero-sum games?
- Basis in paper: [explicit] "A limitation of this paper is that the derived regret lower bounds are specific to the learning dynamics based on optimistic Hedge... A more important direction for future work is to investigate the dependence on the numbers of actions $m$ and $n$ for general strongly-uncoupled learning dynamics."
- Why unresolved: The lower bound construction exploits specific properties of optimistic Hedge; follow-the-leader achieves $O(1)$ regret on that construction but suffers linear regret on other matrices, suggesting a need to distinguish stable from unstable algorithms.
- What evidence would resolve it: A unified lower bound for all algorithms satisfying a stability property, or a characterization separating classes of algorithms by their optimal regret dependencies on $m$ and $n$.

## Limitations
- Lower bounds are specific to optimistic Hedge and don't apply to all strongly-uncoupled learning dynamics
- Analysis requires knowing opponent's action count for cardinality-aware improvements, which may not always be available
- Factor-of-two gap remains between individual regret upper and lower bounds
- Results are limited to two-player zero-sum games, with open questions about multiplayer general-sum games

## Confidence

| Claim | Confidence |
|-------|------------|
| Social regret bounds are tight including leading constants | High |
| Individual regret bounds have factor-of-two gap | High |
| Cardinality-aware setting requires knowing opponent's action count | High |
| Learning rate optimization follows from AM-GM inequality | Medium |
| Lower bound construction is worst-case for optimistic Hedge | Medium |

## Next Checks
1. Implement the payoff matrix construction from Eq. (9) and verify that optimistic Hedge achieves regret approaching log m/η as T grows
2. Reproduce the numerical experiments comparing cardinality-aware vs. cardinality-unaware learning rates with (m,n) = (2, 10^4)
3. Implement the convex optimization for individual regret (Section B.4.4) and verify the tradeoff curve between players' regrets