---
ver: rpa2
title: 'When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data
  Generation'
arxiv_id: '2512.08875'
source_url: https://arxiv.org/abs/2512.08875
tags:
- data
- synthetic
- tabular
- privacy
- levatt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LevAtt, a membership inference attack that
  exploits string-level memorization in LLM-based tabular data generation. LevAtt
  operates by encoding tabular rows into strings and measuring Levenshtein distance
  between synthetic outputs and potential training records, bypassing traditional
  feature-space MIAs.
---

# When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation

## Quick Facts
- arXiv ID: 2512.08875
- Source URL: https://arxiv.org/abs/2512.08875
- Reference count: 40
- Primary result: LLM-based tabular generators vulnerable to string-level memorization attacks; TLP defense reduces privacy leakage while maintaining fidelity

## Executive Summary
This paper introduces LevAtt, a membership inference attack that exploits string-level memorization in LLM-based tabular data generation. By encoding tabular rows into canonical strings and measuring Levenshtein distance between synthetic outputs and potential training records, LevAtt bypasses traditional feature-space MIAs to reveal substantial privacy leakage across ICL and SFT regimes. The attack demonstrates that LLM-based tabular generators are uniquely vulnerable to string-level memorization, with some models achieving perfect classification of training membership. To defend against this, the authors propose TLP (Tendency-based Logit Processor), which strategically perturbs logits during generation, reducing LevAtt's AUC from as high as 0.79 to 0.55 while maintaining synthetic data fidelity.

## Method Summary
The paper presents LevAtt, a no-box membership inference attack that encodes tabular rows into canonical string representations and computes negative minimum Levenshtein edit distance to synthetic outputs. The attack exploits the rigid, constrained nature of numeric strings in tabular data where even single-character matches are highly informative. For defense, TLP applies a concave curving function to min-max scaled logits, boosting lower logits proportionally more than higher ones to increase sampling randomness while preserving relative token ordering. The method is evaluated across ICL (using LLaMA 3.3-70B and TabPFN-v2) and SFT (using RealTabFormer) regimes on multiple tabular datasets, measuring privacy via AUC-ROC and TPR@FPR=0.1, fidelity via Wasserstein Distance and MMD, and utility via downstream XGBoost performance.

## Key Results
- LevAtt achieves AUC-ROC as high as 0.79 and TPR@FPR=0.1 up to 0.48 on some models
- Privacy leakage scales with model size, with LLaMA 3.3-70B showing the largest leakage
- TLP reduces LevAtt's AUC from 0.79 to 0.55 and TPR@FPR=0.1 from 0.48 to below 0.125
- TLP maintains synthetic data fidelity with virtually no penalty in Maximum Mean Discrepancy
- LevAtt shows orthogonal vulnerability compared to traditional feature-space MIAs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-based tabular generators are uniquely vulnerable to string-level memorization attacks that bypass traditional feature-space MIAs.
- **Mechanism**: LevAtt encodes tabular rows into canonical string representations (preserving numeric precision, categorical tokens, delimiters) and computes the negative minimum Levenshtein edit distance between a test record and all synthetic outputs. Lower edit distances indicate memorization—synthetic records that nearly match training records digit-for-digit.
- **Core assumption**: Numeric strings in tabular data are rigid and constrained (fixed precision, small vocabularies), making even single-character matches highly informative for membership inference, unlike ambiguous paraphrases in natural language.
- **Evidence anchors**: [abstract] "LevAtt operates by encoding tabular rows into strings and measuring Levenshtein distance between synthetic outputs and potential training records, bypassing traditional feature-space MIAs." [section 3.2] "the string representations of numeric values in tabular data are highly constrained... even a single-character modification (e.g., '17.5' → '17.6') corresponds to a meaningful change" [corpus] Related work (Tab-MIA, Risk In Context) corroborates privacy leakage in LLM-based tabular generation, though corpus lacks direct TLP-specific evidence.
- **Break condition**: Attack fails if synthetic data contains no near-duplicate string patterns of training records, or if training data has high entropy/variance across digit sequences such that exact matches are improbable by chance.

### Mechanism 2
- **Claim**: Autoregressive token-level generation creates an attack surface absent in conventional deep-learning synthesizers.
- **Mechanism**: LLMs decompose tabular generation into sequential token prediction where each digit is conditioned on all previously generated tokens. This creates token-level dependencies where memorized digit sequences can be reproduced verbatim. Traditional generators (GANs, VAEs, diffusion) model the joint distribution atomically, treating each sample as a complete feature vector rather than exploiting sequential structure.
- **Core assumption**: Models have sufficient capacity to memorize training patterns, and training data contains repeated patterns, long numeric strings, or low-variance columns.
- **Evidence anchors**: [abstract] "The attack reveals substantial privacy leakage across ICL and SFT regimes, with some models achieving perfect classification of training membership." [section 4.2] "Privacy leakage scales with model size... LLaMA 3.3-70B shows the largest privacy leakage across our benchmark." [section 6.3] "LLMs decompose generation into sequential token prediction, where each digit is conditioned on previously generated values... This autoregressive process creates opportunities for the model to reproduce memorized digit sequences."
- **Break condition**: Mechanism is disrupted if generation uses non-autoregressive methods, or if tokenization breaks digit sequences into non-recoverable units.

### Mechanism 3
- **Claim**: Strategic logit perturbation at inference time can reduce membership inference success while maintaining statistical fidelity.
- **Mechanism**: TLP applies a monotone increasing, concave curving function f_t(x) = x^(1/t) to min-max scaled logits. Concavity ensures lower logits receive proportionally larger boosts than higher logits, increasing sampling randomness for high-confidence digit predictions while preserving relative token ordering. The tendency parameter t controls perturbation strength.
- **Core assumption**: The model has learned meaningful feature correlations that persist under controlled perturbation, and t can be tuned to meet a privacy threshold (AUC ≤ 0.55 or TPR@FPR=0.1 ≤ 0.125) with acceptable fidelity loss.
- **Evidence anchors**: [abstract] "TLP... strategically perturbs logits during generation, reducing LevAtt's AUC from as high as 0.79 to 0.55 while maintaining synthetic data fidelity." [section 5.3] "TLP reduces LevAtt's AUC from as high as 0.79 to 0.55 and drives the TPR@FPR=0.1 from 0.48 to below 0.125, all while incurring virtually no penalty in the Maximum Mean Discrepancy." [corpus] Corpus lacks direct evidence for TLP-specific defense mechanisms; related work focuses on attack methodologies.
- **Break condition**: Defense fails if t is set too high (destroying distributional fidelity) or too low (insufficient privacy), or if downstream tasks require exact digit precision (e.g., identifiers, financial amounts).

## Foundational Learning

- **Concept: Levenshtein edit distance**
  - **Why needed here**: Core metric for LevAtt attack; measures character-level overlap between string-encoded records.
  - **Quick check question**: Given two strings "123.45" and "123.46", what is their Levenshtein distance? (Answer: 1)

- **Concept: Membership Inference Attacks (MIAs)**
  - **Why needed here**: Frames privacy risk assessment; determines if a record was in the training set based on synthetic output.
  - **Quick check question**: If a synthetic record exactly matches a training record, what does this suggest about membership? (Answer: Strong evidence the record was memorized and likely a member)

- **Concept: Autoregressive generation**
  - **Why needed here**: Explains why LLMs have unique token-level vulnerabilities compared to joint-distribution models.
  - **Quick check question**: In autoregressive generation, what does each token prediction condition on? (Answer: All previously generated tokens)

## Architecture Onboarding

- **Component map**:
  String encoder -> LLM generator -> LevAtt attack module -> TLP module (optional) -> Evaluation suite

- **Critical path**:
  1. Encode training data → Train/prompt LLM → Generate synthetic data
  2. Encode test records → Compute LevAtt scores against synthetic data → Classify membership
  3. If leakage detected: Apply TLP with tuned t → Regenerate → Re-evaluate

- **Design tradeoffs**:
  - **Privacy vs. fidelity**: Stronger TLP perturbation reduces leakage but may distort learned distributions
  - **Model size vs. leakage risk**: Larger models memorize more but may offer better base fidelity
  - **Synthetic volume vs. attack success**: More samples increase chance of reproducing memorized records
  - **Post-hoc (DM) vs. inference-time (TLP)**: DM is model-agnostic but ignores learned correlations; TLP preserves structure but requires model access

- **Failure signatures**:
  - LevAtt AUC-ROC ≥ 0.70 or TPR@FPR=0.1 ≥ 0.25 indicates significant leakage
  - Perfect membership classification (AUC = 1.0) suggests verbatim memorization
  - High Wasserstein distance after TLP indicates over-perturbation
  - Correlation analysis: If LevAtt correlates weakly with feature-space MIAs, string-level vulnerability is orthogonal

- **First 3 experiments**:
  1. **Baseline LevAtt evaluation**: Generate synthetic data from RealTabFormer on CASP dataset; compute LevAtt AUC and TPR@FPR against known training/holdout split. Compare to feature-space MIAs (DCR, MC, Density).
  2. **TLP parameter sweep**: Apply TLP with t ∈ {2, 5, 10, 20} to RealTabFormer generation; measure privacy (LevAtt AUC), fidelity (MMD), and utility (XGBoost RMSE on holdout). Identify smallest t meeting privacy threshold.
  3. **Scaling analysis**: Generate synthetic datasets at 1×, 5×, 10× training size; plot LevAtt AUC vs. volume to quantify amplification of privacy risk with larger releases.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can theoretical privacy guarantees analogous to Differential Privacy be established for LLM-based tabular generation? The authors state that future research should aim to establish theoretical foundations analogous to differential privacy frameworks. This remains unresolved because proposed defenses like TLP are empirically validated but lack provable bounds against arbitrary adversaries.

- **Open Question 2**: How does the efficacy of string-based membership inference change under less restrictive threat models? Section 6.5 suggests investigating richer threat models beyond the No-box setting, such as those leveraging model internals or surrogate data. This is unresolved because the current study assumes a conservative No-box threat model that may underestimate risks.

- **Open Question 3**: Are defenses like TLP robust against adaptive adversaries aware of the protection mechanism? Section 6.4 highlights the limitation that the study does not address adaptive adversaries who are aware that protective mechanisms are in place. This remains unresolved because an adaptive adversary might exploit statistical artifacts introduced by logit perturbation.

- **Open Question 4**: Do LLM-based tabular generators learn the underlying joint distribution or merely perform approximate memorization? The authors ask what these models exactly learn and suggest they may behave as perturbation mechanisms rather than learning general patterns. This is unresolved because the rigidity of tabular data may force models toward memorization, but this is not mechanistically proven.

## Limitations

- The defense TLP requires access to open-source models and model internals, limiting applicability to black-box scenarios.
- The paper doesn't address adaptive adversaries who might learn to circumvent the defense by exploiting its statistical artifacts.
- Limited validation against non-autoregressive tabular synthesizers makes it unclear if the vulnerability is unique to LLM-based generators.
- String encoding format and exact TLP tuning parameters are not fully specified, potentially affecting reproducibility.

## Confidence

- **Privacy risk in LLM-based tabular generation**: High confidence - attack achieves perfect classification in some cases, and scaling analysis shows larger models leak more.
- **Effectiveness of TLP defense**: High confidence for tested scenarios (open-source models, inference-time application). Medium confidence for generalization to all LLM-based tabular generators.
- **Orthogonality of string-level vs. feature-space attacks**: Medium confidence - correlation analysis supports the claim, but more comprehensive feature-space baselines would strengthen the argument.

## Next Checks

1. **Ablation with non-autoregressive generators**: Test LevAtt against tabular synthesizers that don't use autoregressive generation (e.g., diffusion models, joint-distribution GANs) to confirm whether the attack surface is unique to LLMs.

2. **Black-box defense evaluation**: Implement a version of TLP that works without direct model access (e.g., through API rate limiting or output filtering) and measure its effectiveness against adaptive attacks.

3. **Adaptive attack scenario**: Design an attack that learns to detect TLP perturbation patterns and adjusts LevAtt scoring accordingly to test whether TLP provides defense in depth or merely raises the attack bar.