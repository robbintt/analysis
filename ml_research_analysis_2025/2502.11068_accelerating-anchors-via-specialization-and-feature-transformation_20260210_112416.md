---
ver: rpa2
title: Accelerating Anchors via Specialization and Feature Transformation
arxiv_id: '2502.11068'
source_url: https://arxiv.org/abs/2502.11068
tags:
- anchors
- input
- rule
- explanation
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of Anchors,
  a popular model-agnostic explanation technique. The authors propose MAnchors, a
  memorization-based approach that accelerates Anchors by reusing intermediate results
  from previous explanations.
---

# Accelerating Anchors via Specialization and Feature Transformation

## Quick Facts
- **arXiv ID**: 2502.11068
- **Source URL**: https://arxiv.org/abs/2502.11068
- **Authors**: Haonan Yu; Junhao Liu; Xin Zhang
- **Reference count**: 17
- **Primary result**: Memorization-based approach achieves up to 8.74× speedup and 87% sampling reduction for Llama 2 explanations

## Executive Summary
This paper addresses the computational inefficiency of Anchors, a popular model-agnostic explanation technique, by introducing MAnchors—a memorization-based approach that accelerates explanation generation through caching and reusing intermediate results. The method employs horizontal transformation to adapt cached rules to new inputs and vertical transformation to refine general rules into specific ones. Experiments across tabular, text, and image datasets demonstrate significant efficiency gains while maintaining explanation fidelity and interpretability.

## Method Summary
MAnchors accelerates Anchors explanations by caching intermediate rules at precision threshold τ_pmid (default 0.8) during the iterative refinement process. When explaining a new input, the system first searches for similar cached rules using k-d trees on perturbation embeddings. If a similar rule is found (similarity ≥ τ_sim = 0.6), horizontal transformation adapts the cached rule to the new input by substituting features with semantically similar alternatives, followed by vertical transformation to refine the rule to the target precision τ_p = 0.95. If no suitable cached rule exists, the system falls back to standard Anchors execution.

## Key Results
- Up to 8.74× speedup and 87% sampling reduction for Llama 2 on RT-Polarity dataset
- 2.73× speedup and 50% sampling reduction for random forest on Adult dataset
- Maintains explanation fidelity: precision remains at 0.95, coverage degrades by ≤4.5%
- Reduces explanation length by up to 37% (from 6.89 to 4.34 predicates for Llama 2)

## Why This Works (Mechanism)

### Mechanism 1: Memorization of Low-Precision Intermediate Rules
Anchors iteratively refines rules from general (high-coverage, low-precision) to specific (low-coverage, high-precision). By caching intermediate rules at τ_pmid instead of final explanations, MAnchors can skip early iterations for similar inputs. The k-d tree enables O(log m) retrieval from memory, and the core assumption is that similar inputs have similar explanation structures.

### Mechanism 2: Horizontal Transformation for Feature Space Adaptation
Cached rules can be adapted to new inputs by substituting predicates with semantically similar features. Given a cached rule and new input, HT maps each predicate to the most similar feature using distance metrics appropriate to the data type (BERT embeddings for text, superpixel embeddings for images, absolute difference for tabular). This produces an initial rule covering the new input without sampling.

### Mechanism 3: Vertical Transformation for Precision Refinement
Starting from HT's output, VT iteratively adds predicates to refine the rule to meet precision thresholds. This mirrors Anchors' original refinement loop but starts closer to the target, reducing required iterations. The precision-coverage gradient is assumed to be smooth enough that good initialization reduces total sampling proportionally.

## Foundational Learning

- **Concept: Anchors algorithm (precision-coverage tradeoff, KL-LUCB)**
  - Why needed here: MAnchors directly manipulates Anchors' iterative refinement; understanding how precision increases as coverage decreases is essential for setting τ_pmid appropriately.
  - Quick check question: If a rule has 3 predicates and precision 0.75, what happens when you add a 4th predicate?

- **Concept: Perturbation-based explanation and perturbation space**
  - Why needed here: HT's distance metric Dist operates in perturbation space, not raw feature space; similarity for rule adaptation depends on how perturbations affect model predictions.
  - Quick check question: For text classification, why might "good" and "excellent" be closer in perturbation space than "good" and "bad"?

- **Concept: Multi-dimensional similarity search (k-d trees, embeddings)**
  - Why needed here: Memory retrieval efficiency determines overall speedup; k-d tree provides O(log m) lookup vs. O(m) linear scan.
  - Quick check question: Why does k-d tree performance degrade in high-dimensional spaces, and how does this affect MAnchors for image data?

## Architecture Onboarding

- **Component map**: Input x → Embedding → k-d tree search → similarity check → [similarity < τ_sim] → ProcessMemoryMiss → Anchors run → Cache (x, r_mid) OR [similarity ≥ τ_sim] → ProcessMemoryHit → Horizontal Transform → Vertical Transform → Output r

- **Critical path**: Memory hit rate determines speedup. For Llama 2 experiments, cold start completes within ~100 inputs per category; prioritize populating memory with representative samples before production deployment.

- **Design tradeoffs**:
  - τ_pmid (0.8 default): Lower = more general rules, higher cache hit rate but more VT work; higher = less VT but lower hit rate
  - τ_sim (0.6 default): Higher = better fidelity preservation, lower = more aggressive reuse with potential quality degradation
  - Memory size m: Larger = better hit rate but O(m) worst-case search; k-d tree mitigates but doesn't eliminate

- **Failure signatures**:
  - Coverage drops significantly: τ_sim too low, retrieving dissimilar cached inputs
  - Explanation length increases (e.g., 4.34 → 5.39 predicates): HT initialization poor, VT adding compensatory predicates
  - No speedup initially: Cold start phase; monitor hit rate after 100+ queries per category

- **First 3 experiments**:
  1. **Baseline validation**: Run standard Anchors vs. MAnchors on 50 held-out samples; verify precision matches (τ_p = 0.95) and measure speedup
  2. **Ablation on τ_pmid**: Sweep τ_pmid ∈ {0.6, 0.7, 0.8, 0.9}; plot speedup vs. coverage degradation to find task-optimal value
  3. **Memory hit rate analysis**: Log similarity scores and hit/miss decisions; identify failure cases where HT produces low-precision initializations

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Domain shift sensitivity: Cached rules may become ineffective when input distributions change significantly
- Feature interaction complexity: Horizontal transformation assumes features are functionally interchangeable, which may not hold for complex interactions
- Memory management: No mechanism for pruning or updating memory set, risking unbounded growth in production systems

## Confidence
- **High confidence**: The fundamental concept of caching intermediate rules to accelerate Anchors is sound and well-validated through experiments showing consistent speedup across multiple model types and datasets
- **Medium confidence**: The horizontal transformation mechanism works as described for the tested datasets, but generalization to other feature types or more complex feature interactions remains uncertain
- **Medium confidence**: The efficiency gains (up to 8.74× speedup, 87% sampling reduction) are demonstrated but may be dataset-dependent; the Adult dataset results show more modest improvements (2.73× speedup)

## Next Checks
1. **Domain shift robustness test**: Evaluate MAnchors on a dataset with known temporal or distributional drift to quantify performance degradation when cached rules become stale, measuring hit rate and explanation quality degradation over time

2. **Feature interaction complexity analysis**: Test MAnchors on datasets with known complex feature interactions (e.g., XOR patterns, feature conjunctions) to determine whether horizontal transformation can correctly identify functionally equivalent features when interactions matter

3. **Cold start optimization study**: Systematically measure the number of queries needed to populate memory before consistent speedup is achieved, and test whether a weighted sampling strategy targeting diverse feature space regions accelerates this process