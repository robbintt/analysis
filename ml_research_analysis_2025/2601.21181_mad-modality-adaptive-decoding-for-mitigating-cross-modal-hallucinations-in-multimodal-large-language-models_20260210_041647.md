---
ver: rpa2
title: 'MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations
  in Multimodal Large Language Models'
arxiv_id: '2601.21181'
source_url: https://arxiv.org/abs/2601.21181
tags:
- modality
- audio
- visual
- video
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modality-Adaptive Decoding (MAD) addresses cross-modal hallucinations
  in multimodal large language models by adaptively weighting modality-specific decoding
  branches based on task requirements. The method leverages the model's self-assessment
  capability to determine which modalities are relevant for each task, then uses these
  modality probabilities to guide contrastive decoding.
---

# MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2601.21181
- Source URL: https://arxiv.org/abs/2601.21181
- Reference count: 40
- Primary result: MAD reduces cross-modal hallucinations in multimodal LLMs using self-assessed modality relevance to guide adaptive contrastive decoding

## Executive Summary
MAD addresses cross-modal hallucinations in multimodal large language models by adaptively weighting modality-specific decoding branches based on task requirements. The method leverages the model's self-assessment capability to determine which modalities are relevant for each task, then uses these modality probabilities to guide contrastive decoding. MAD significantly reduces cross-modal hallucinations across multiple audio-visual language models, achieving 7.8% and 2.0% improvements for VideoLLaMA2-AV and 8.7% and 4.7% improvements for Qwen2.5-Omni on CMM and AVHBench benchmarks respectively.

## Method Summary
MAD is a training-free inference method that mitigates cross-modal hallucinations by combining self-assessment for modality relevance extraction with adaptive contrastive decoding. The method first appends a modality query prompt to the input, extracts logits for "audio", "video", and "both" tokens, and applies softmax to obtain normalized weights. These weights are then used to adaptively fuse logits from four contrastive decoding branches: visual CD with/without audio, and audio CD with/without video. The fused logits guide token selection, with higher weights amplifying contrastive suppression for relevant modalities.

## Key Results
- MAD achieves 7.8% improvement on CMM benchmark for VideoLLaMA2-AV
- MAD achieves 8.7% improvement on AVHBench benchmark for Qwen2.5-Omni
- Adaptive weighting outperforms uniform weighting (79.4% vs 81.3% overall accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Self-Assessment for Modality Relevance Extraction
MLLMs can estimate which modalities a task requires by querying themselves with a structured prompt. Append a modality query prompt ("To answer this question, which modality is needed?") to the input, extract logits for tokens "audio", "video", and "both", then apply softmax to obtain normalized weights (w_a, w_v, w_av). These weights reflect the model's internal assessment of modality relevance. Core assumption: The model has internalized modality-task relationships during pretraining sufficient to self-assess relevance. Evidence: [section 3.2.2], [Figure 3], [corpus].

### Mechanism 2: Adaptive Contrastive Decoding via Modality-Weighted Fusion
Contrastive decoding effectiveness improves when contrastive strength scales with task-specific modality relevance. Replace fixed contrastive strength α_m with adaptive term γ·w_m, where w_m is the self-assessed weight. Higher w_m for a modality amplifies contrastive suppression of hallucinations arising from that modality; lower w_m reduces unnecessary penalization. Core assumption: Hallucinated tokens exhibit smaller logit gaps between clean and perturbed inputs than grounded tokens. Evidence: [section 3.2.1], [Table 2], [corpus].

### Mechanism 3: Four-Branch Logit Fusion for Joint Audio-Visual Reasoning
Decomposing contrastive decoding into four branches (visual CD with/without audio, audio CD with/without video) enables fine-grained hallucination suppression across modality configurations. Compute logits from four input configurations: (v, a, q), (ṽ, a, q), (v, ã, q), (ṽ, ã, q). Fuse them using modality-adaptive weights (Eq. 9), allowing each branch to contribute proportionally to its assessed relevance. Core assumption: Cross-modal hallucinations are bidirectional—visual dominance can induce audio hallucinations and vice versa. Evidence: [section 3.2.3], [Table 3], [corpus].

## Foundational Learning

- **Contrastive Decoding (CD)**: Why needed here: MAD extends CD from single-modality to multi-modality settings. Understanding the base intuition—contrasting clean vs. perturbed inputs to suppress hallucinations—is essential. Quick check question: Given logits from clean image (L_clean) and perturbed image (L_perturbed), how would you compute a contrastive logit that amplifies visually grounded tokens?

- **Softmax Normalization for Weight Extraction**: Why needed here: Converting raw logits from self-assessment into normalized weights (summing to 1) enables interpretable, stable fusion across branches. Quick check question: If logits for "audio", "video", "both" are [2.0, 0.5, 1.0], what are the normalized weights after softmax?

- **Autoregressive Token Prediction**: Why needed here: MAD operates at each decoding step, requiring understanding of how next-token distributions are sampled and how logits are modified before sampling. Quick check question: At generation step t, what inputs does the model condition on when computing logits for the next token?

## Architecture Onboarding

- Component map: Modality Query Module -> Weight Computation -> Four-Branch Logit Computation -> Adaptive Fusion -> Token Selection

- Critical path: 1. Self-assessment query → weight extraction (once per sample, not per token) 2. Four forward passes per token generation step 3. Adaptive logit fusion → token selection

- Design tradeoffs:
  - Accuracy vs. Latency: Four forward passes per token significantly increase inference time (Table 6 shows 3571-6700 ms/token). Consider caching perturbed KV states or reducing branches for latency-sensitive applications.
  - Perturbation Strategy: Paper uses noise/masking; alternative strategies (e.g., modality dropout, adversarial perturbation) may yield different contrastive signals.
  - Hyperparameter γ: Default γ=2.5 works across benchmarks (Section 6), but optimal value may vary by model/dataset.

- Failure signatures:
  - Degenerate weights: If self-assessment yields uniform weights (~0.33 each), MAD reduces to modality-agnostic baseline.
  - Over-suppression: Excessive γ or w_m can suppress valid tokens, reducing fluency and accuracy on standard tasks.
  - Misaligned query prompts: Alternative prompts show robustness (Section 9), but poorly designed prompts may yield unreliable weights.

- First 3 experiments:
  1. Validate self-assessment alignment: Sample 50 examples per modality category (visual-only, audio-only, audio-visual), compute weight distributions, confirm alignment as in Figure 3.
  2. Ablate single branches: Disable each weight (w_a, w_v, w_av) individually and measure degradation to confirm branch contributions (replicate Table 3).
  3. Latency profiling: Measure per-token latency for MAD vs. baseline decoding, identify bottlenecks (forward pass count vs. fusion overhead).

## Open Questions the Paper Calls Out

### Open Question 1
Can the modality-adaptive framework be effectively generalized to modalities beyond audio and video, such as thermal or RGB-D sensors? Basis in paper: [explicit] The conclusion explicitly states the intent to "extend the modality-adaptive framework to richer modality combinations beyond audio–video, including thermal–RGB or other multi-sensor settings." Why unresolved: The current experimental scope is strictly limited to audio-visual benchmarks (CMM, AVHBench) and models (VideoLLaMA2, Qwen2.5-Omni). What evidence would resolve it: Application of MAD to thermal/RGB-D datasets, demonstrating consistent cross-modal hallucination reduction.

### Open Question 2
Can a lightweight predictor replace the inference-time self-assessment prompt to reduce computational overhead without sacrificing accuracy? Basis in paper: [explicit] The conclusion proposes "learning a lightweight, parameter-efficient predictor that estimates modality weights more quickly and accurately." Why unresolved: The current method relies on querying the LLM at inference time, which Table 6 shows adds significant latency compared to simpler baselines. What evidence would resolve it: Training a separate, small network to predict $w_m$ and comparing its speed/accuracy trade-off against the prompt-based method.

### Open Question 3
How does the method perform on models with weaker instruction-following capabilities that may fail the initial self-assessment step? Basis in paper: [inferred] The method assumes the model can accurately answer the query "To answer this question, which modality is needed?" (Eq. 6). Why unresolved: The study uses high-performing models (7B parameters), but does not analyze failure cases where the model misidentifies the relevant modality itself. What evidence would resolve it: Evaluating MAD on smaller models (e.g., <3B parameters) and analyzing the correlation between instruction-following accuracy and hallucination mitigation success.

## Limitations
- Self-assessment reliability is only validated on specific benchmarks, not generalized to other domains
- Four forward passes per token create substantial computational overhead (3.5-6.7 seconds per token)
- Distortion method for creating perturbed inputs is not fully specified, leaving implementation details unclear

## Confidence
**High Confidence Claims:**
- MAD reduces cross-modal hallucinations on CMM and AVHBench benchmarks
- Modality-adaptive weighting outperforms uniform weighting
- Self-assessment weights correlate with question modality requirements

**Medium Confidence Claims:**
- The four-branch formulation provides better hallucination suppression than fewer branches
- γ=2.5 is optimal across different datasets and models
- The mechanism of adaptive contrastive strength improves CD effectiveness

**Low Confidence Claims:**
- Self-assessment weights reliably indicate true modality relevance in all contexts
- Four forward passes are necessary for optimal performance

## Next Checks
1. External Dataset Validation: Test MAD on at least two datasets outside the CMM/AVHBench benchmarks to verify that self-assessment weights generalize beyond the original benchmarks. Compare MAD's modality weight distributions against human-annotated modality relevance labels.

2. Alternative Distortion Strategy Comparison: Implement and compare at least two different perturbation methods (e.g., Gaussian noise vs. modality dropout vs. adversarial perturbation) to determine whether the specific distortion strategy significantly impacts MAD's effectiveness. Measure both hallucination reduction and any degradation in grounded generation.

3. Efficiency Optimization Study: Profile MAD's computational overhead and implement at least one optimization (e.g., KV caching for perturbed inputs, reduced precision inference, or branch pruning based on weight thresholds). Quantify the tradeoff between accuracy degradation and latency improvement to identify practical deployment configurations.