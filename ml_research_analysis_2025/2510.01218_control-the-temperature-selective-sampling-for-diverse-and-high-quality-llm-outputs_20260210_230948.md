---
ver: rpa2
title: 'Control the Temperature: Selective Sampling for Diverse and High-Quality LLM
  Outputs'
arxiv_id: '2510.01218'
source_url: https://arxiv.org/abs/2510.01218
tags:
- sampling
- diversity
- classifier
- min-p
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of balancing output diversity
  and quality in language models, particularly for tasks requiring high precision
  like mathematical reasoning. It identifies that uncontrolled high-temperature sampling
  degrades reasoning accuracy by introducing errors at sensitive decoding positions.
---

# Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs

## Quick Facts
- arXiv ID: 2510.01218
- Source URL: https://arxiv.org/abs/2510.01218
- Reference count: 40
- Key outcome: Selective sampling improves the quality-diversity trade-off for mathematical reasoning tasks by dynamically switching between greedy and high-temperature sampling based on predicted sampling risk

## Executive Summary
This paper addresses the fundamental tension between output diversity and quality in language models, particularly for precision-critical tasks like mathematical reasoning. The authors identify that uncontrolled high-temperature sampling degrades reasoning accuracy by introducing errors at sensitive decoding positions where only narrow token sets lead to correct answers. They propose selective sampling, a method that uses a lightweight classifier to predict sampling risk at each decoding step and dynamically switches between greedy and temperature sampling. The classifier is trained on a small subset of verifiable problems and adds minimal latency overhead. Experiments show selective sampling achieves better diversity-quality trade-offs than existing methods while maintaining lower perplexity scores, indicating more coherent outputs.

## Method Summary
Selective sampling dynamically controls the sampling strategy during decoding by predicting sampling risk positions where errors are likely to occur. The method trains a lightweight linear classifier on frozen hidden states to predict whether sampling at each position will lead to incorrect outputs. When high risk is predicted, the model uses greedy decoding to ensure accuracy; when low risk is predicted, it samples with high temperature (combined with min-p truncation) to maximize diversity. The sampling risk is calculated as the difference between greedy reward and expected sampling reward, approximated by force-decoding candidate tokens. The classifier is trained on a small subset of problems where greedy decoding yields correct answers, and the approach integrates seamlessly with existing models with minimal computational overhead.

## Key Results
- Selective sampling achieves higher diversity-quality AUC (0.42-0.47) compared to min-p truncation (0.38-0.40) on mathematical reasoning benchmarks
- The method maintains lower perplexity scores than baseline approaches, indicating more coherent outputs
- Classifier transfers across tasks, maintaining performance when trained on Minerva and evaluated on GSM-Symbolic
- The approach adds negligible latency overhead while providing significant quality-diversity improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sensitive decoding positions exist where sampling from the model distribution produces incorrect continuations even when greedy decoding would succeed.
- Mechanism: The method identifies "sampling risk" positions—timesteps where the expected reward from sampling falls below the greedy reward. At these positions (often intermediate calculations in math reasoning), only a narrow set of tokens lead to correct final answers.
- Core assumption: Reward (accuracy) can be approximated by force-decoding candidate tokens and completing with greedy—a low-cost proxy for true expected quality.
- Evidence anchors:
  - [abstract] "loss of accuracy is caused by sampling incorrect continuations in sensitive decoding positions"
  - [Section 3.1] s-risk(x) := R(x) − E[v∼p][R([x,v])], motivated by RL regret
  - [Figure 1] Greedy continuation "20" yields correct answer 115; non-greedy "30" yields incorrect answer 105
  - [corpus] Weak direct corpus support; related work on truncation sampling (min-p, top-p) addresses distribution shape but not position-specific risk
- Break condition: If no verifiable reward signal is available, s-risk labels cannot be generated automatically.

### Mechanism 2
- Claim: A lightweight linear classifier on frozen hidden states can predict sampling risk with sufficient accuracy to guide decoding.
- Mechanism: Extract last-token hidden states from all layers (residual stream), average them, and apply a learned linear projection with sigmoid. The classifier is trained via binary cross-entropy on auto-generated s-risk labels.
- Core assumption: The model's internal representations encode task-relevant uncertainty that distinguishes safe sampling positions from risky ones.
- Evidence anchors:
  - [Section 4.2] Classifier s_θ(f(x)) = σ(1/L Σ w_i^T h_i) applied on frozen hidden states
  - [Section 6.3] Hidden-states classifier achieves 85% accuracy, 0.78 ROC AUC on Minerva validation
  - [Figure 3] Classifier predictions align well with ground-truth s-risk labels
  - [corpus] Mahaut et al. (2024) and Duan et al. (2024) show hidden states support reliability estimation—consistent with this approach
- Break condition: If hidden states do not generalize across domains, classifier predictions become unreliable.

### Mechanism 3
- Claim: Selective application of greedy decoding at high-risk positions preserves quality while allowing temperature sampling elsewhere for diversity.
- Mechanism: At each decoding step, the classifier outputs a risk score. If risk exceeds threshold, decode greedily; otherwise sample with high temperature (combined with min-p truncation). Higher temperatures increase greedy usage (Table 2: 17–44% greedy at τ=3.0).
- Core assumption: Diversity gains from safe positions compensate for reduced sampling at risky positions.
- Evidence anchors:
  - [Section 6.1, Figure 4] Selective sampling achieves higher diversity-quality AUC (0.42–0.47) vs min-p (0.38–0.40) on math benchmarks
  - [Section 6.2, Figure 5] Lower perplexity and higher accuracy at τ=2.0–3.0 compared to min-p baseline
  - [Section 6.5, Figure 7] Classifier transfers across tasks (trained on Minerva, evaluated on GSM-Symbolic)
  - [corpus] No direct corpus comparison; EDT (Zhang et al., 2024b) dynamically adjusts temperature via entropy but underperforms this classifier-based approach
- Break condition: If the task lacks discrete reward signal or requires open-ended creativity without clear correctness criteria, s-risk labeling becomes infeasible.

## Foundational Learning

- Concept: **Regret in sequential decision-making**
  - Why needed here: Sampling risk is directly inspired by RL regret—the difference between optimal (greedy) and expected (sampling) cumulative reward.
  - Quick check question: Can you explain why s-risk(x) measures the penalty for sampling instead of acting greedily at position x?

- Concept: **Hidden-state probing**
  - Why needed here: The classifier extracts information from frozen representations; understanding probing helps diagnose whether the model encodes risk signals.
  - Quick check question: What does it imply if a linear probe achieves high accuracy on s-risk prediction?

- Concept: **Temperature scaling and truncation sampling**
  - Why needed here: Selective sampling builds on top of existing truncation methods (min-p); understanding how temperature flattens distributions is prerequisite.
  - Quick check question: How does min-p differ from top-p in defining the candidate token set?

## Architecture Onboarding

- Component map:
  Base LLM -> Risk Classifier -> Sampling Controller -> Truncation Sampler

- Critical path:
  1. Forward pass through base LLM → logits + hidden states
  2. Classifier reads hidden states → risk score s_θ(x)
  3. If s_θ(x) < threshold (safe): sample from truncated distribution
  4. If s_θ(x) ≥ threshold (risky): take argmax token
  5. Append token, repeat until EOS

- Design tradeoffs:
  - Classifier complexity vs. latency: Linear classifier adds minimal overhead; deeper probes may improve accuracy but increase inference cost
  - Threshold setting: Lower threshold → more greedy, higher quality, lower diversity; higher threshold → more sampling, inverse trade-off
  - Training data size: Experiments show convergence at ~500 prompts (Section F.2), but labels require correct greedy outputs

- Failure signatures:
  - Classifier overfitting: High training accuracy but poor diversity-quality AUC on held-out tasks
  - Reward misspecification: Using accuracy-only reward ignores CoT correctness, may mask subtle errors
  - Domain shift: Classifier trained on math may fail on open-ended tasks where s-risk is less discrete

- First 3 experiments:
  1. **Classifier validation**: Train on GSM8K training split, evaluate s-risk prediction accuracy and ROC AUC on held-out validation set. Compare hidden-states classifier vs. n-gram ablation.
  2. **Diversity-quality sweep**: Run selective sampling across temperatures τ ∈ {0.5, 1.0, 2.0, 3.0}, plot accuracy vs. diversity (distinct n-grams over correct samples), compute AUC. Compare against min-p, top-p, EDT baselines.
  3. **Transfer test**: Train classifier on Minerva, evaluate on GSM-Symbolic without retraining. Measure AUC gap vs. in-domain classifier to assess generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on tasks with discrete, verifiable rewards limits applicability to domains like mathematical reasoning where correctness can be automatically verified
- Effectiveness relies on the assumption that hidden states encode sufficient information to distinguish sampling risk positions, which may not transfer well across domains or model architectures
- Claims about computational efficiency are vague without specific timing measurements or comparisons to alternative approaches

## Confidence
- **High confidence**: The core mechanism of using a classifier to predict sampling risk and selectively apply greedy decoding is technically sound and well-supported by experimental results
- **Medium confidence**: The claim that selective sampling transfers across different mathematical reasoning tasks is supported by evidence but requires more extensive validation across diverse domains
- **Low confidence**: The paper's claims about computational efficiency are somewhat vague, stating that the classifier adds "negligible latency overhead" without providing specific timing measurements

## Next Checks
1. **Cross-domain robustness evaluation**: Train the s-risk classifier on mathematical reasoning tasks and evaluate its performance on open-ended generation tasks like story completion or code synthesis. Measure whether the classifier can identify sampling risk positions in domains without clear correctness criteria, and assess the quality of generated outputs using human evaluation rather than automated metrics.

2. **Classifier generalization study**: Train multiple classifiers on different subsets of mathematical reasoning data and test their transferability across problem types (arithmetic, algebra, geometry). Analyze the feature importance of the linear classifier to determine which hidden state dimensions are most predictive of sampling risk, and test whether these features generalize to other reasoning domains or model architectures.

3. **Threshold sensitivity analysis**: Conduct an ablation study systematically varying the risk threshold parameter across different temperatures and tasks. Plot the resulting quality-diversity curves to identify optimal thresholds for different use cases, and develop heuristics or automated methods for threshold selection based on task characteristics or model properties.