---
ver: rpa2
title: Interpreting the Repeated Token Phenomenon in Large Language Models
arxiv_id: '2503.08908'
source_url: https://arxiv.org/abs/2503.08908
tags:
- poem
- company
- token
- attention
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explains why large language models (LLMs) fail to repeat
  tokens when prompted to do so, a vulnerability that allows models to diverge from
  intended behavior. The authors link this "repeated token divergence" phenomenon
  to "attention sinks," an emergent behavior where the initial token receives disproportionately
  high attention, crucial for fluency.
---

# Interpreting the Repeated Token Phenomenon in Large Language Models

## Quick Facts
- **arXiv ID:** 2503.08908
- **Source URL:** https://arxiv.org/abs/2503.08908
- **Reference count:** 40
- **Primary result:** This paper explains why large language models (LLMs) fail to repeat tokens when prompted to do so, a vulnerability that allows models to diverge from intended behavior. The authors link this "repeated token divergence" phenomenon to "attention sinks," an emergent behavior where the initial token receives disproportionately high attention, crucial for fluency. Through mechanistic analysis, they identify a two-stage neural circuit responsible for attention sinks: the first attention layer marks the initial token, and later neurons amplify its hidden state. This mechanism misidentifies repeated tokens, causing high attention and output divergence. They extend this finding to a broader "cluster attack" that induces attention sinks without exact repetition. A targeted patch effectively mitigates this issue without harming model performance. This study provides the first mechanistic explanation for this LLM vulnerability, demonstrating how interpretability can diagnose and address such issues.

## Executive Summary
This paper provides the first mechanistic explanation for why large language models diverge from intended behavior when prompted to repeat tokens. The authors identify a two-stage neural circuit that creates "attention sinks" - disproportionately high attention on the first token - which is essential for fluent generation but creates a vulnerability. When identical tokens are repeated, the circuit misidentifies them as the beginning-of-sequence token, causing the model to diverge from the repetition task. The researchers demonstrate this through ablation studies, convergence analysis, and propose a targeted patch that mitigates the issue without harming model performance. This work demonstrates how mechanistic interpretability can diagnose and fix specific LLM vulnerabilities.

## Method Summary
The authors use inference-time analysis to study repeated token divergence across multiple LLaMa and Mistral models. They identify "sink neurons" in early MLP layers that amplify the first token's hidden state, then validate their role through zero-ablation experiments. The theoretical foundation includes a convergence proof showing that repeated token representations converge to singleton representations as sequence length increases. They develop a targeted patch that clamps sink neuron values for non-BoS tokens, preventing the amplification mechanism while preserving normal generation. The approach is validated across multiple benchmarks to ensure performance isn't degraded.

## Key Results
- Identified a two-stage neural circuit (first attention layer marking + later sink neuron amplification) responsible for attention sinks
- Demonstrated that repeated tokens trigger the same attention sink mechanism as the first token, causing divergence
- Developed and validated a targeted patch that mitigates the vulnerability without harming benchmark performance
- Extended the vulnerability from exact repetition to "cluster attacks" using semantically similar tokens
- Provided the first mechanistic explanation linking attention sinks to the repeated token divergence phenomenon

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The "repeated token divergence" is caused by a specific neural circuit that misidentifies repeated tokens as a Beginning-of-Sequence (BoS) token.
- **Mechanism:** A two-stage circuit is responsible. First, the initial attention layer identifies and "marks" the first token. Second, specific "sink neurons" in later layers detect this mark and amplify the token's hidden state magnitude. When a sequence of identical tokens is presented, the first attention layer cannot distinguish them from a single-token sequence, causing the circuit to mark and amplify *all* repeated tokens, thereby destabilizing the attention distribution.
- **Core assumption:** The model relies on causal masking and relative attention signals (like RoPE) to distinguish tokens, rather than absolute positional signals, leading to ambiguity in identical sequences.
- **Evidence anchors:**
  - [abstract]: "The first attention layer marks the initial token, and later neurons amplify its hidden state."
  - [section 4.3, Page 7]: Theorem 4.1 proves that as $n \to \infty$, the representation of a repeated token sequence converges to that of a singleton sequence.
  - [corpus]: Related work "Why do LLMs attend to the first token?" supports the general existence of attention sinks but does not describe this specific neuron-driven failure mode.
- **Break condition:** If an architecture uses strong absolute positional embeddings that break the symmetry of repeated tokens, the first attention layer may successfully distinguish them, breaking the causal chain.

### Mechanism 2
- **Claim:** Disproportionately high attention on the first token (the "attention sink") is functionally created by a sparse set of MLP neurons in early layers.
- **Mechanism:** Specific neurons (e.g., neuron 7890 in LLaMa-2 layer 1) write large values to the residual stream of the token identified as "first." This drastically increases the L2 norm of that token's hidden state. Subsequent layers attend to this token heavily due to the large Key/Query magnitudes derived from this inflated hidden state.
- **Core assumption:** High hidden state norms in early layers are causally linked to high attention scores in subsequent layers.
- **Evidence anchors:**
  - [section 4.2, Page 5]: "We search for a sparse set of neurons... we name 'sink neurons'... Ablation of sink neurons... reduces the high norms."
  - [section 4.3, Page 8]: "Repeated tokens exhibit high norms mediated by the same neurons identified..."
  - [corpus]: The paper "A Unified View of Attention and Residual Sinks" corroborates the role of outlier dimensions/residual sinks in this behavior.
- **Break condition:** If the MLP weights in early layers are initialized or regularized to prevent massive activation scaling, the sink mechanism (and thus the vulnerability) may not emerge.

### Mechanism 3
- **Claim:** The vulnerability extends beyond exact repetition to "cluster attacks" where semantically or functionally similar tokens trigger the same attention head behavior.
- **Mechanism:** The first attention layer clusters tokens into subspaces. If a sequence consists of tokens from the same cluster (activating the same "attend to others" head), they can induce the same "first-token" misidentification effect without exact repetition.
- **Core assumption:** The "attend to others" mechanism relies on distinct subspaces for first/non-first tokens; confusing these subspaces is sufficient to trigger the sink circuit.
- **Evidence anchors:**
  - [section 5.2, Page 10]: "This understanding allows us to extend the repeating tokens attack... to an attack that repeats tokens from the same set."
  - [figure 4]: Shows linear separability of first vs. subsequent tokens in the first attention layer.
  - [corpus]: Evidence for "cluster attacks" specifically is weak or missing in the provided corpus; this appears to be a novel contribution of this paper.
- **Break condition:** Defenses that rely solely on detecting exact token repetition (n-gram blocking) will fail; mitigation requires addressing the underlying attention mechanism.

## Foundational Learning

- **Concept:** **Attention Sinks & Streaming LLMs**
  - **Why needed here:** The paper links the failure mode to "attention sinks," a mechanism where models rely on the first token as a "bias" or anchor to maintain fluency. Understanding this functional role explains why the model has a circuit dedicated to amplifying the first token.
  - **Quick check question:** Why does removing the first token (the sink) often degrade generation quality even if the token has no semantic meaning?

- **Concept:** **Softmax Leakage / Convergence**
  - **Why needed here:** The theoretical explanation relies on how the Softmax function distributes probability mass. As the prefix remains constant while the sequence grows ($n \to \infty$), the influence of the prefix fades (leaks), causing the representation of repeated tokens to converge.
  - **Quick check question:** In a causal attention mask, how does increasing the sequence length of identical tokens affect the attention weight assigned to the original prefix?

- **Concept:** **Rotary Position Embeddings (RoPE)**
  - **Why needed here:** The paper notes that RoPE encodes position via rotation but acts bounded. The mechanism fails partly because RoPE allows representations of repeated tokens to converge or look similar in specific subspace projections, lacking a strong absolute position signal to break the symmetry.
  - **Quick check question:** Does RoPE provide an absolute position signal or a relative one, and how does that affect the distinguishability of two identical tokens at different positions?

## Architecture Onboarding

- **Component map:** Input -> Layer 0 (Attention First Detector) -> Layer 1 (MLP Sink Neurons) -> Deep Layers (Attention Saturation)

- **Critical path:** The vulnerability hinges on the connection between **Layer 0's attention head clustering** and **Layer 1's sink neuron activation**. If the Layer 0 head projects repeated tokens into the "First" subspace, the Layer 1 neuron executes the amplification.

- **Design tradeoffs:**
  - **Fluency vs. Robustness:** The attention sink circuit (via sink neurons) aids fluency but creates an attack surface for repetition divergence.
  - **Efficiency vs. Control:** Shared attention mechanisms across token clusters reduce parameter count but enable "cluster attacks."

- **Failure signatures:**
  - **Norm Explosion:** Hidden state norms in early layers (e.g., Layer 1) jump from ~2.4 (typical) to >6.9 (BoS levels) or higher during repetition attacks.
  - **Attention Saturation:** Attention maps in intermediate layers show uniform or misplaced high attention on repeated tokens rather than semantic context.

- **First 3 experiments:**
  1. **Neuron Ablation Test:** Locate "sink neurons" in an early MLP layer (via high activation on BoS). Zero-ablate them and verify if the hidden state norms of repeated tokens drop to typical levels.
  2. **Convergence Verification:** Input a sequence of $N$ repeated tokens (where $N$ varies). Measure the L2 distance between the output of the first attention layer for the last token vs. a single token. Confirm convergence as $N$ increases.
  3. **Patching Validation:** Implement the "patch" (Listing 1) to clamp the sink neuron value for non-BoS tokens. Run standard benchmarks (MMLU, HellaSwag) to confirm performance retention, then test repetition prompts to confirm mitigation.

## Open Questions the Paper Calls Out

- **Question:** What is the precise mechanism linking attention sink activation to the extraction of memorized training data?
- **Basis in paper:** [explicit] Section 6 (Limitations) states, "the mechanism by which training data leakage occurs through attention sinks remains unclear, requiring further investigation."
- **Why unresolved:** The paper identifies the circuit causing divergence but does not explain why this specific failure mode results in the model emitting verbatim training data rather than random gibberish.
- **What evidence would resolve it:** Causal tracing analysis connecting the activation of "sink neurons" directly to the retrieval of specific memorized sequences in the model's weights.

- **Question:** Why do certain tokens fail to induce attention sinks while others successfully trigger the divergence phenomenon?
- **Basis in paper:** [explicit] Section 6 notes that "Not all tokens in LLaMa2 induce sinks," and Figure 6 shows examples (e.g., "Another", "bit") where repetition does not lead to high norms, suggesting "other factors are at play."
- **Why unresolved:** The authors identify the circuit but do not determine the specific properties (e.g., semantic, frequency, embedding norm) that make a token "sink-compatible."
- **What evidence would resolve it:** A systematic analysis correlating token embedding features or training data frequency with the magnitude of the hidden state norm increase during repetition.

- **Question:** To what extent does the "first-token marking" circuit identified in LLaMa-2 exist in other Transformer architectures?
- **Basis in paper:** [explicit] Section 6 highlights that "the first attention layerâ€™s behavior was unique to LLaMa2," implying the mechanism may not be universal.
- **Why unresolved:** While motifs were found in LLaMa-1/3 and Mistral, the specific implementation of "first token detection" via the first attention layer differed, leaving the universality of the circuit ambiguous.
- **What evidence would resolve it:** A comparative circuit analysis across non-LLaMa architectures (e.g., GPT variants) to determine if similar "first-detector" neurons or attention head patterns exist.

## Limitations

- **Theoretical Generalization Gap:** The convergence theorem proves behavior as $n \to \infty$, but practical divergence occurs at much smaller repetition lengths (500-1000 tokens), and the connection between asymptotic behavior and finite-sequence effects isn't rigorously established.
- **Neuron Specificity Uncertainty:** While specific sink neurons are identified for each tested model, the selection criterion uses TopK without specifying K, and different initialization or training runs might yield different neuron IDs.
- **Cluster Attack Validation:** The extension to cluster attacks is novel but less validated than the base repetition attack, with limited systematic testing across different token sets and model architectures.

## Confidence

**High Confidence:** The existence of attention sinks and their role in fluency is well-established both in this work and related literature. The two-stage circuit mechanism (marking + amplification) is directly observed through neuron ablation and norm analysis. The patch effectively mitigates the specific vulnerability on tested models.

**Medium Confidence:** The extension from exact repetition to cluster attacks follows logically from the mechanistic understanding but has limited empirical validation. The theoretical convergence proof connects to practical behavior but the finite-sequence dynamics aren't fully characterized. The claim that this represents the first mechanistic explanation for this vulnerability is plausible but difficult to exhaustively verify.

**Low Confidence:** The precise conditions under which the vulnerability manifests (repetition length thresholds, model architecture dependencies) are not systematically mapped. The generalization of sink neuron identification across model families is assumed but not demonstrated. The relationship between the attention sink phenomenon and other known LLM vulnerabilities remains unexplored.

## Next Checks

**Check 1: Finite-Sequence Convergence Analysis:** Systematically measure the L2 distance between repeated token representations and singleton representations across varying repetition lengths (100, 200, 500, 1000, 2000 tokens). Plot this convergence curve to identify the practical threshold where divergence typically occurs, and compare against the theoretical asymptotic prediction.

**Check 2: Cross-Architecture Sink Detection:** Apply the sink neuron identification procedure to a diverse set of architectures beyond LLaMa/Mistral (including decoder-only models with different positional encoding schemes like ALiBi or learned positional embeddings). Verify whether the two-stage circuit pattern holds and whether sink neurons consistently appear in early MLP layers.

**Check 3: Cluster Attack Robustness Testing:** Design systematic experiments to test cluster attacks with varying degrees of semantic similarity (synonyms, hypernyms, functionally related tokens). Measure the attention distribution changes and divergence probability as a function of cluster cohesion, and test whether defenses designed for exact repetition (like the proposed patch) also mitigate cluster-based divergence.