---
ver: rpa2
title: Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model
  Generation from Text
arxiv_id: '2507.08362'
source_url: https://arxiv.org/abs/2507.08362
tags:
- dataset
- process
- bpmn
- extraction
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a machine learning pipeline for generating
  BPMN models from text. The key contribution is a new annotated dataset (LESCHNEIDER)
  that extends the PET dataset with 15 documents containing 32 additional parallel
  gateways, addressing a critical gap in existing data.
---

# Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text

## Quick Facts
- arXiv ID: 2507.08362
- Source URL: https://arxiv.org/abs/2507.08362
- Reference count: 28
- Key outcome: 23% F1 for AND Gateways (improved from 0%) via dataset augmentation

## Executive Summary
This paper presents a machine learning pipeline for generating BPMN models from text, with a key contribution being the LESCHNEIDER dataset that extends the PET dataset with 15 documents containing 32 additional parallel gateways. The pipeline uses BERT and RoBERTa for Named Entity Recognition and CatBoost for relation extraction. Results show the enhanced dataset improves parallelism detection, with F1 scores for AND Gateways rising from 0% to 23%. The BERT-base-cased model performed best overall. The pipeline achieved 89.2% F1 for elements and 73.7% F1 for relations across six test documents. The work addresses limitations in dataset diversity and parallelism representation, offering a promising foundation for automated BPMN generation, though challenges remain with complex structures and implicit gateway closures.

## Method Summary
The method employs a two-stage machine learning approach: BERT-based Named Entity Recognition using IOB2 tagging format for identifying BPMN elements, followed by CatBoost-based Relation Extraction using engineered features (POS tags, dependency tags, token distances) with Random Over-Sampling to address class imbalance. The pipeline was trained on a combined dataset of PET and LESCHNEIDER documents, with hyperparameter tuning including batch size 8 and learning rate sweep [2e-5, 3e-5, 4e-5, 5e-5]. The approach specifically targets parallelism detection by augmenting existing datasets with annotated AND Gateways.

## Key Results
- AND Gateway F1 improved from 0% to 23% with dataset augmentation
- BERT-base-cased outperformed larger models (RoBERTa-large, BERT-large) due to overfitting on small dataset
- Pipeline achieved 89.2% F1 for elements and 73.7% F1 for relations on six test documents
- CatBoost with Random Over-Sampling showed best performance for flow relations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted dataset augmentation may enable the detection of previously ignored semantic structures, specifically parallel gateways.
- **Mechanism:** The authors augmented the existing PET dataset with the LESCHNEIDER dataset, increasing the count of AND Gateways from 8 to 40. This increase in training examples appears to have shifted the model's capacity to recognize "simultaneously" markers, improving F1 scores for AND Gateways from 0% to 23%.
- **Core assumption:** The lexical patterns found in the augmented data (e.g., "and simultaneously") generalize sufficiently to unseen process descriptions to trigger gateway classification.
- **Evidence anchors:**
  - [abstract]: "Specifically, we augment the PET dataset with 15 newly annotated documents... F1 scores for AND Gateways rising from 0% to 23%."
  - [Section 5.2]: Shows performance gain in "CRF Model 3" (combined dataset) over baseline.
  - [corpus]: Corpus evidence is weak for this specific mechanism; neighbor papers focus on computing parallelism (e.g., GSplit, RoboPARA) rather than BPMN semantic parallelism.
- **Break condition:** Mechanism fails if the textual descriptions rely on implicit parallelism (lack of explicit markers like "while" or "simultaneously") or if the vocabulary drifts significantly from the 15 added documents.

### Mechanism 2
- **Claim:** A hybrid architecture leveraging Transformers for sequential labeling and Gradient Boosting for relation classification provides a balanced trade-off between context understanding and structured classification.
- **Mechanism:** BERT/RoBERTa handles Named Entity Recognition (NER) by capturing bidirectional context for token classification (IOB format). Separately, CatBoost is employed for Relation Extraction (RE) using engineered features (POS tags, distance) and Random Over-Sampling (ROS) to handle class imbalance.
- **Core assumption:** Relation extraction depends more heavily on structured features and sampling strategies to define graph edges than on deep semantic context alone.
- **Evidence anchors:**
  - [Section 3.1]: "The proposed approach... leverages the use of machine learning and large language models."
  - [Section 5.3]: "ROS showed the best performance for the flow class... BERT-base-cased model outperforms others."
  - [corpus]: "Assessing the Business Process Modeling Competences of Large Language Models" suggests LLMs are viable for this domain, supporting the use of Transformer-based components.
- **Break condition:** Fails if error propagation from the NER stage (e.g., mislabeled entities) cascades into the Relation Extraction phase, or if the CatBoost model encounters relation types not covered by the sampling strategy.

### Mechanism 3
- **Claim:** Smaller, case-sensitive pre-trained models may outperform larger variants in low-data regimes due to generalization limits.
- **Mechanism:** The study observes that `bert-base-cased` outperforms `roberta-large` and `bert-large`. The authors suggest larger models tend to overfit on the relatively small combined dataset (PET + LESCHNEIDER), whereas the base model offers sufficient capacity for the specific tagset.
- **Core assumption:** The complexity of the BPMN extraction task is constrained enough that the representation capacity of "large" models becomes a liability rather than an asset.
- **Evidence anchors:**
  - [Section 5.5]: "Larger models such as BERT-large and RoBERTa-large tend to overfit when trained on the relatively small dataset..."
  - [Section 4.3]: Mentions batch size and learning rate tuning was specific to avoid "catastrophic forgetting."
  - [corpus]: Not explicitly supported or refuted by corpus neighbors.
- **Break condition:** If the dataset were to scale up significantly (e.g., thousands of documents), the "large" models would likely cease to overfit and might reclaim superior performance.

## Foundational Learning

- **Concept: IOB (Inside, Outside, Beginning) Tagging**
  - **Why needed here:** The NER module relies on this format to distinguish between the start of an entity (e.g., `B-Activity`) and its continuation (e.g., `I-Activity`), which is critical for reconstructing multi-word process steps.
  - **Quick check question:** Given the sequence "sends the request," would "sends" be tagged as `B-Activity` or `I-Activity`?

- **Concept: Class Imbalance & Sampling**
  - **Why needed here:** The paper highlights that "flow" relations and AND Gateways are significantly rarer than Activities. Understanding ROS (Random Over-Sampling) is necessary to grasp why the model can detect minority classes at all.
  - **Quick check question:** Why might oversampling the "flow" class lead to higher recall but potentially lower precision in the final graph?

- **Concept: BPMN Gateways (XOR vs. AND)**
  - **Why needed here:** The core value proposition rests on distinguishing XOR (exclusive choice) from AND (parallel execution). Without this distinction, the generated diagram misrepresents the business logic.
  - **Quick check question:** In text, does the phrase "either... or" typically map to an AND Gateway or an XOR Gateway?

## Architecture Onboarding

- **Component map:** Pre-processor -> NER Module (BertForTokenClassification) -> Feature Engineer -> RE Module (CatBoost) -> Generator (NetworkX DiGraph)
- **Critical path:** The **NER Module** is the primary bottleneck. If `B-AND Gateway` is missed here, the RE module cannot form the parallel relation, causing a total failure in parallelism detection for that instance.
- **Design tradeoffs:**
  - **Generalization vs. Overfitting:** Choosing `bert-base` over `roberta-large` sacrifices potential semantic depth to prevent overfitting on a small corpus.
  - **Sampling Strategy:** The use of ROS prioritizes the detection of rare "flow" relations but risks introducing noise compared to SMOTE or negative sampling.
- **Failure signatures:**
  - **Silent Parallelism:** AND Gateways missed (F1 score 23% implies 77% are still missed or imprecise).
  - **Implicit Closure Failures:** The model struggles to close gateways when text does not explicitly state the end of a parallel branch.
  - **Spurious Edges:** Low precision in RE (seen in variable Doc 1/4 results) leading to disconnected or incorrect graph structures.
- **First 3 experiments:**
  1. **Sanity Check (NER):** Train the CRF baseline on PET only and test on LESCHNEIDER to confirm the domain shift and poor parallelism detection (reproducing Table 3b).
  2. **Optimization (RE):** Implement the CatBoost trainer with ROS vs. SMOTE on the combined dataset to validate the claim that ROS yields higher F1 for the "flow" class.
  3. **Pipeline Validation:** Run the full `bert-base-cased` + `CatBoost` pipeline on a "noisy" document containing non-essential information to verify if the NER correctly assigns "O" (Outside) labels to irrelevant text.

## Open Questions the Paper Calls Out
- Can Large Language Models (LLMs) effectively replace or augment the current CatBoost approach for relation extraction to improve the structural accuracy of generated BPMN models?
- How can automated pipelines be improved to accurately detect and resolve implicit gateway closures in textual process descriptions?
- To what extent do hybrid approaches combining rule-based methods with machine learning improve generalization across diverse writing styles compared to pure ML models?
- What further dataset expansions or model refinements are necessary to achieve robust performance in detecting AND Gateways?

## Limitations
- Modest dataset size (51 documents) constrains model generalization and statistical robustness
- Reliance on specific lexical markers limits ability to detect implicit parallelism
- Error propagation from NER to RE creates cascading failure points
- Pipeline does not address complex BPMN structures like nested gateways or loops

## Confidence
- **High Confidence:** Baseline PET dataset's insufficiency for detecting AND Gateways (F1 = 0%); improvement with LESCHNEIDER augmentation (F1 rising to 23%)
- **Medium Confidence:** `bert-base-cased` outperforms larger models due to overfitting on small dataset; Random Over-Sampling improves flow relation detection
- **Low Confidence:** Pipeline can generate BPMN models with 89.2% F1 for elements and 73.7% F1 for relations, as this masks poor AND Gateway performance (23% F1)

## Next Checks
1. Acquire and annotate a significantly larger dataset (200+ documents) and retrain the pipeline to determine if larger models cease to overfit and improve AND Gateway detection beyond 23% F1
2. Construct a test set of process descriptions using implicit parallelism and evaluate whether the model can detect AND Gateways without relying on specific lexical cues
3. Implement detailed error analysis tracking misclassifications from NER through to final BPMN diagram, quantifying percentage of AND Gateway misses attributable to NER versus RE failures