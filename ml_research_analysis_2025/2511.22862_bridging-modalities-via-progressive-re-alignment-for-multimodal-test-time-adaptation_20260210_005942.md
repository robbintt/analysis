---
ver: rpa2
title: Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation
arxiv_id: '2511.22862'
source_url: https://arxiv.org/abs/2511.22862
tags:
- adaptation
- brimpr
- source
- unimodal
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multimodal test-time adaptation
  (MMTTA), where different modalities experience varying degrees of domain shift,
  leading to a complex coupling effect of unimodal shallow feature shift and cross-modal
  high-level semantic misalignment. The authors propose BriMPR, a framework that progressively
  re-aligns modalities by first decomposing MMTTA into multiple unimodal feature alignment
  sub-problems, using prompt tuning to calibrate unimodal global feature distributions,
  and then refining alignment through inter-modal instance-wise contrastive learning
  and a novel cross-modal masked embedding recombination strategy.
---

# Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation

## Quick Facts
- **arXiv ID:** 2511.22862
- **Source URL:** https://arxiv.org/abs/2511.22862
- **Reference count:** 40
- **Primary result:** BriMPR achieves state-of-the-art performance on MMTTA benchmarks, with accuracy improvements of up to 11.5% on corrupted domain datasets.

## Executive Summary
This paper addresses the challenge of multimodal test-time adaptation (MMTTA), where different modalities experience varying degrees of domain shift, leading to complex coupling effects of unimodal shallow feature shifts and cross-modal high-level semantic misalignment. The authors propose BriMPR, a framework that progressively re-aligns modalities by first decomposing MMTTA into multiple unimodal feature alignment sub-problems, using prompt tuning to calibrate unimodal global feature distributions, and then refining alignment through inter-modal instance-wise contrastive learning and a novel cross-modal masked embedding recombination strategy. Experiments on Kinetics50-C, VGGSound-C, CMU-MOSI, and CH-SIMS demonstrate that BriMPR consistently outperforms state-of-the-art methods, achieving accuracy improvements such as 60.5%→65.9% on Kinetics50-C (video corruption) and 25.0%→36.5% on VGGSound-C (audio corruption), while maintaining robustness under limited data and continual domain shift scenarios.

## Method Summary
The BriMPR framework addresses MMTTA through a two-stage progressive re-alignment approach. First, it uses unimodal prompt tuning to align each modality's feature distributions to their source counterparts through moment-matching, decomposing the complex MMTTA problem into simpler unimodal alignment sub-problems. Second, it employs cross-modal masked embedding recombination and instance-wise contrastive learning to capture multimodal correlations while being robust to modality-specific corruptions. The method introduces a novel cross-modal masked embedding recombination strategy that creates robust multimodal representations by selectively masking and recombining embeddings from different modalities, allowing the model to learn invariant representations even when individual modalities are corrupted.

## Key Results
- BriMPR achieves state-of-the-art performance across four multimodal benchmark datasets (Kinetics50-C, VGGSound-C, CMU-MOSI, CH-SIMS).
- On Kinetics50-C (video corruption), accuracy improves from 60.5% to 65.9%.
- On VGGSound-C (audio corruption), accuracy improves from 25.0% to 36.5%.
- The method maintains effectiveness under limited data scenarios and exhibits robustness to continual domain shifts.

## Why This Works (Mechanism)
The progressive re-alignment strategy works by first simplifying the complex MMTTA problem into manageable unimodal alignment tasks through prompt tuning, which calibrates feature distributions without extensive retraining. This unimodal calibration creates a stable foundation for the subsequent cross-modal alignment stage, where masked embedding recombination and contrastive learning capture multimodal correlations while being robust to modality-specific corruptions. The two-stage approach addresses the coupling effect where unimodal shifts interfere with cross-modal alignment, ensuring that each modality's features are properly aligned before attempting to capture their interactions.

## Foundational Learning
- **Multimodal Test-Time Adaptation (MMTTA)**: The process of adapting multimodal models to new domains at test time without access to target domain labels, addressing the challenge of different modalities experiencing varying domain shifts.
  - *Why needed:* Standard single-modality adaptation methods fail when modalities shift differently, requiring specialized approaches that handle multimodal complexity.
  - *Quick check:* Can the model adapt to new domains without retraining while maintaining performance across all modalities?

- **Visual Prompt Tuning (VPT)**: A parameter-efficient fine-tuning method that inserts learnable prompts into Transformer layers to adapt feature distributions without modifying original model weights.
  - *Why needed:* Enables efficient unimodal alignment by calibrating feature distributions through moment-matching without expensive full model retraining.
  - *Quick check:* Does the prompt tuning effectively align unimodal feature distributions to source statistics?

- **Cross-modal Masked Embedding Recombination**: A strategy that creates robust multimodal representations by selectively masking and recombining embeddings from different modalities, similar to masked language modeling but applied to multimodal features.
  - *Why needed:* Provides robustness to modality-specific corruptions by learning to reconstruct complete representations from incomplete modality inputs.
  - *Quick check:* Can the model maintain performance when individual modalities are corrupted or missing?

- **Instance-wise Contrastive Learning**: A method that pulls together features from the same multimodal instance while pushing apart features from different instances, capturing fine-grained multimodal correlations.
  - *Why needed:* Refines cross-modal alignment by learning precise instance-level relationships between modalities after unimodal calibration.
  - *Quick check:* Does the contrastive loss improve the model's ability to distinguish between different multimodal instances?

## Architecture Onboarding
- **Component Map:** Unimodal Prompt Tuning → Cross-modal Masked Recombination → Instance-wise Contrastive Learning → MMTTA Output
- **Critical Path:** The progressive pipeline where unimodal prompt tuning (LayerNorm + prompts) must complete before cross-modal recombination and contrastive learning can effectively capture multimodal correlations.
- **Design Tradeoffs:** The method trades computational overhead from prompt tuning and masked reconstruction against improved adaptation performance and robustness to modality corruption.
- **Failure Signatures:** Poor unimodal alignment leading to degraded cross-modal performance, or excessive masking ratios causing information loss and reduced accuracy.
- **First Experiments:**
  1. Test unimodal prompt tuning effectiveness on Kinetics50-C video-only corruption scenario.
  2. Evaluate cross-modal masked embedding recombination on VGGSound-C audio-only corruption.
  3. Assess overall BriMPR performance on CMU-MOSI under multimodal corruption.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can an adaptive masking ratio strategy further improve BriMPR's robustness compared to the fixed 0.5 ratio?
- **Basis in paper:** [explicit] Figure 8 analysis notes that while a 0.5 mask ratio is a balanced default, "under the unimodal shift setting, increasing the mask ratio tends to improve performance... whereas under the multimodal shift setting, an excessively high mask ratio degrades performance."
- **Why unresolved:** The paper identifies this dependency but implements a static hyperparameter rather than a dynamic adjustment mechanism that could respond to the specific type of shift encountered during testing.
- **What evidence would resolve it:** A study implementing a feedback loop where the mask ratio adjusts based on real-time metrics (e.g., distribution discrepancy $Disc_u$ or entropy) showing superior accuracy over fixed ratios.

### Open Question 2
- **Question:** Is BriMPR applicable to multimodal architectures utilizing Convolutional Neural Networks (CNNs) rather than Transformers?
- **Basis in paper:** [inferred] The methodology relies on Visual Prompt Tuning (VPT), and the experiments exclusively utilize Transformer-based backbones (CA V-MAE, Stacked Transformers) with LayerNorm.
- **Why unresolved:** The paper does not evaluate if the "strong function approximation ability" of prompts holds effectively in CNN feature spaces or if the moment-matching alignment works with Batch Normalization statistics common in CNNs.
- **What evidence would resolve it:** Experimental results applying BriMPR to CNN-based multimodal fusion networks (e.g., ResNet-based audio-visual models) to verify if the prompt-based alignment translates to convolutional feature maps.

### Open Question 3
- **Question:** Can BriMPR be adapted for "completely" source-free scenarios where pre-computed source statistics are unavailable?
- **Basis in paper:** [inferred] The method requires pre-computing source statistics $\{\hat{\mu}_{i}^{s,u}, \hat{\sigma}_{i}^{s,u}\}$ offline (Eq. 4), assuming access to source data or statistics prior to deployment.
- **Why unresolved:** Practical deployment constraints might prevent the transfer of even aggregated source statistics due to privacy or storage limits, leaving the method's efficacy in a strictly source-free environment unknown.
- **What evidence would resolve it:** A modified version of the loss function that estimates source distributions using synthesized prototypes or model-weight priors, showing competitive performance without accessing source statistics.

## Limitations
- The framework requires pre-computed source statistics, limiting its applicability in truly source-free scenarios where even aggregated statistics cannot be accessed.
- The progressive re-alignment strategy assumes that unimodal prompt tuning adequately prepares features for cross-modal alignment, which may not hold for highly asymmetric domain shifts between modalities.
- Computational overhead from prompt tuning and masked reconstruction may limit scalability to larger datasets or more complex multimodal architectures.

## Confidence
- **High Confidence:** The reported performance improvements over baseline methods (e.g., 5.4% accuracy gain on Kinetics50-C, 11.5% on VGGSound-C) are well-supported by experimental results and appear reproducible given the detailed methodology description.
- **Medium Confidence:** The effectiveness of the progressive two-stage approach (unimodal prompt tuning followed by cross-modal alignment) is demonstrated empirically, but the generalizability to other multimodal tasks beyond the tested domains (action recognition, audio classification, sentiment analysis) requires further validation.
- **Medium Confidence:** The framework's robustness under limited data and continual domain shift scenarios is shown, but the sensitivity to hyperparameters and the number of training samples for prompt tuning is not extensively explored.

## Next Checks
1. **Ablation Study Extension**: Conduct a more comprehensive ablation study isolating the contribution of each component (prompt tuning, contrastive learning, masked embedding recombination) across different domain shift intensities and noise levels to better understand their individual impacts.

2. **Theoretical Analysis**: Develop a theoretical framework analyzing the convergence properties and stability of the progressive re-alignment strategy, particularly focusing on the interaction between unimodal and cross-modal alignment stages.

3. **Scalability Assessment**: Evaluate the framework's performance and computational efficiency on larger-scale multimodal datasets (e.g., Charades, AudioSet) and more complex architectures (e.g., Vision Transformers with multimodal encoders) to assess real-world applicability and scalability constraints.