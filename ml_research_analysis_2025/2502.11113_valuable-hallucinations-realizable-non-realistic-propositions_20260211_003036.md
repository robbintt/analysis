---
ver: rpa2
title: 'Valuable Hallucinations: Realizable Non-realistic Propositions'
arxiv_id: '2502.11113'
source_url: https://arxiv.org/abs/2502.11113
tags:
- hallucinations
- arxiv
- valuable
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "valuable hallucinations"
  in large language models, defining them as realizable but non-realistic propositions
  that can inspire innovative ideas. Through experiments with Qwen2.5 using HalluQA
  dataset and ReAct prompting, the authors found that ReAct prompts reduced overall
  hallucinations by 5.12% while increasing the proportion of valuable hallucinations
  from 6.45% to 7.92%.
---

# Valuable Hallucinations: Realizable Non-realistic Propositions

## Quick Facts
- arXiv ID: 2502.11113
- Source URL: https://arxiv.org/abs/2502.11113
- Reference count: 15
- Key outcome: ReAct prompting reduces hallucinations by 5.12% while increasing valuable hallucination proportion from 6.45% to 7.92%

## Executive Summary
This paper introduces the concept of "valuable hallucinations" in large language models—realizable but non-realistic propositions that can inspire innovation. Through experiments with Qwen2.5 using HalluQA dataset and ReAct prompting, the authors demonstrate that explicit reasoning prompts can reduce harmful hallucinations while preserving or increasing the proportion of potentially useful creative outputs. The study shows that controlling hallucinations can enhance their usefulness without compromising factual reliability, suggesting that certain types of hallucinations have constructive value in specific contexts.

## Method Summary
The study uses Qwen2.5 model with two prompting conditions: standard prompt and ReAct prompt requiring explicit reasoning steps, confidence levels, and citation grounding. Responses are manually annotated into three categories: non-hallucination, valuable hallucination, and non-valuable hallucination. The valuable hallucination definition follows (¬p ∩ q) logic—propositions that are not currently true but could theoretically be realized. The experimental design compares 450 HalluQA question responses under both prompting conditions, measuring changes in hallucination proportions and correlations with model confidence.

## Key Results
- ReAct prompting reduces overall hallucinations by 5.12% while increasing valuable hallucination proportion from 6.45% to 7.92%
- Model confidence shows near-zero correlation with hallucination value (r = 0.009), indicating confidence calibration alone cannot identify valuable hallucinations
- Factual fabrication emerges as the primary source of valuable hallucinations, representing novel content that synthesizes real concepts coherently

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReAct-style prompting with explicit reasoning steps reduces total hallucinations while preserving or increasing the proportion of valuable hallucinations.
- Mechanism: The prompt requires the model to expose intermediate reasoning ("thought process"), state confidence levels, and cite relevant information before answering. This forces self-verification loops that catch non-valuable fabrications (e.g., invented numbers, citations) while still allowing creative synthesis that may be non-realistic but internally coherent.
- Core assumption: Models can distinguish between verifiable facts and creative synthesis when forced to externalize reasoning; valuable hallucinations arise from coherent recombination of real concepts rather than random fabrication.
- Evidence anchors:
  - [abstract] "ReAct prompting results in a 5.12% reduction in overall hallucinations and an increase in the proportion of valuable hallucinations from 6.45% to 7.92%."
  - [section 3.1.2] "The experimental group was prompted using the ReAct framework... encourages the model to show its reasoning steps, thereby reducing the likelihood of generating hallucinations."
  - [corpus] Related paper "A comprehensive taxonomy of hallucinations in Large Language Models" (arXiv:2508.01781) provides formal definitions but does not validate this specific mechanism.
- Break condition: If the domain requires strict factual accuracy (e.g., medical dosages, legal citations), the mechanism may suppress valuable creative outputs alongside harmful ones; the tradeoff curve may not hold across all task types.

### Mechanism 2
- Claim: Valuable hallucinations predominantly emerge from the "factual fabrication" category—content that cannot be verified against existing knowledge but is internally consistent and synthesizes real concepts in novel ways.
- Mechanism: The model draws on learned patterns from valid scientific/engineering frameworks (e.g., NRTL model, activity coefficients) and recombines them into new configurations. The "CCCC" example shows this: the acronym is fabricated, but each component maps to real thermodynamic concepts, making the output realizable even though the specific formulation doesn't exist.
- Core assumption: Internal coherence + grounding in real concepts → realizability; realizability + non-existence → valuable hallucination.
- Evidence anchors:
  - [section 2.2.2] "Among these, factual fabrication is the primary source of valuable hallucinations, as it involves generating novel content that, while not currently verifiable, may offer innovative insights."
  - [appendix A] "The model's response provides a technically coherent explanation for the invented 'CCCC' formula... draws upon existing theoretical knowledge and frameworks."
  - [corpus] Weak corpus validation—no direct papers test this specific fabrication-to-value pathway experimentally.
- Break condition: If fabricated content appears coherent but violates domain constraints the model wasn't trained on (e.g., physical impossibilities in engineering contexts), the output may appear valuable but be unrealizable in practice.

### Mechanism 3
- Claim: The correlation between model confidence and hallucination value is near zero (r = 0.009), meaning confidence calibration alone cannot identify valuable hallucinations.
- Mechanism: Models do not internally distinguish between "confidently wrong about a fact" and "confidently proposing a creative synthesis." The confidence signal is not diagnostic of the valuable/non-valuable distinction, which requires external judgment (human feedback in RL frameworks).
- Core assumption: Valuable hallucinations are not a confidence-calibration problem but a content-classification problem requiring external validation.
- Evidence anchors:
  - [section 3.1.2] "r = 0.009, which is close to 0, indicating that there is almost no linear correlation between the degree of hallucination of the output content of the large model and its trust in the answers it gives."
  - [section 2.2.1] "The 'value' of these hallucinations can be assessed through feedback, particularly human feedback, in reinforcement learning frameworks."
  - [corpus] "Three Types of Calibration with Properties" (arXiv:2504.18395) discusses calibration but doesn't address this specific dissociation.
- Break condition: If future models develop introspective access to their own uncertainty about creative vs. factual claims, confidence might become more predictive; current architecture does not support this.

## Foundational Learning

- Concept: **Extrinsic vs. Intrinsic Hallucinations**
  - Why needed here: The paper's classification scheme depends on this distinction. Extrinsic hallucinations (output not verifiable from source) are more likely to be valuable because they represent creative speculation rather than contradiction.
  - Quick check question: Does the output contradict the input (intrinsic), or does it introduce new unverifiable content (extrinsic)?

- Concept: **Realizability as distinct from Truth**
  - Why needed here: The core definition (¬p ∩ q) requires evaluating whether a proposition could theoretically be implemented, not whether it currently exists. This is a modal logic concept.
  - Quick check question: Given unlimited resources and correct physics, could this proposition be made true?

- Concept: **ReAct Prompting Pattern**
  - Why needed here: The experimental intervention uses this specific prompt structure. Understanding the "Reason + Act" cycle is prerequisite to replicating or extending the work.
  - Quick check question: Can you write a prompt that requires: (1) explicit reasoning trace, (2) confidence statement, (3) citation/evidence grounding, before the final answer?

## Architecture Onboarding

- Component map:
  Input Layer (HalluQA dataset) -> Prompting Layer (Standard vs. ReAct) -> Model Layer (Qwen2.5) -> Evaluation Layer (Human annotation) -> Feedback Loop (Proposed RLHF)

- Critical path:
  1. Dataset preparation with class/category labels
  2. Prompt template construction (ReAct format)
  3. Model inference with both prompt types
  4. Output annotation using the formal definition (¬p ∩ q)
  5. Statistical comparison of proportions

- Design tradeoffs:
  - Dataset scope vs. generalization: HalluQA is Chinese-focused QA; results may not transfer to English or to non-QA tasks (summarization, dialogue, code generation)
  - Human annotation cost vs. automated metrics: Paper acknowledges no automated metric exists for "value" assessment; human judgment is bottleneck
  - Prompt complexity vs. latency: ReAct prompts increase response time and token costs; the 1.47% gain in valuable hallucinations may not justify overhead in production

- Failure signatures:
  - High pseudoscience/myth domains: Paper notes 17.5% of improved responses were in pseudoscience—model may appear more accurate while still generating scientifically invalid content
  - Misleading questions: 47.5% of problematic cases were "Misleading" category; prompting helps here but doesn't eliminate vulnerability to adversarial framing
  - Single-model results: If you replicate with GPT-4, Claude, or LLaMA and get different effect sizes, the mechanism may be model-specific rather than architectural

- First 3 experiments:
  1. Replication with English dataset: Run the same ReAct vs. baseline comparison on TruthfulQA or HaluEval to test cross-linguistic generalization; document whether the 5.12% reduction and 1.47% valuable-hallucination gain replicates.
  2. Domain transfer test: Apply the prompting framework to a creative writing task (where valuable hallucinations are explicitly desired) vs. a medical QA task (where they're harmful); measure whether the same prompt has opposite effects by domain.
  3. Annotation reliability check: Have multiple annotators classify 100 outputs into the three categories; compute inter-annotator agreement (Cohen's κ) to establish whether the formal definition (¬p ∩ q) is operationally usable or too subjective for automation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the finding that ReAct prompting increases valuable hallucinations while reducing overall hallucinations be replicated across different LLM architectures (e.g., GPT-4, LLaMA, Claude) and diverse datasets beyond HalluQA?
- Basis in paper: [explicit] Section 5.1 states: "Our findings are specific to Qwen2.5, and the results may not generalize to other LLMs like GPT-4, LLaMA-2, or Claude. Future work should expand the scope to include multiple datasets... and models... to validate the robustness and generalizability of our findings."
- Why unresolved: The experiment used only one model and one dataset, limiting external validity.
- What evidence would resolve it: Replication experiments across multiple LLM families and datasets showing consistent effect sizes.

### Open Question 2
- Question: How can the value of hallucinations be measured through scalable, objective automated metrics rather than relying on subjective human annotation?
- Basis in paper: [explicit] Section 5.2 states: "there is no universally accepted automated metric to measure the usefulness of hallucinations. Future work could explore more robust computational frameworks for evaluation."
- Why unresolved: Current evaluation depends on human judgment (scoring: 0, 1, 2), which introduces annotation bias and limits scalability.
- What evidence would resolve it: Development and validation of computational metrics that correlate strongly with human assessments of hallucination value.

### Open Question 3
- Question: How can retrieval-augmented generation (RAG) be specifically designed to distinguish between valuable and non-valuable hallucinations during the generation process?
- Basis in paper: [explicit] Section 3.2.1 states: "While we do not propose specific implementations in this paper, RAG offers a promising direction for future research in controlling hallucinations and increasing the proportion of valuable hallucinations."
- Why unresolved: The paper conceptualizes RAG's potential but provides no implementation or empirical test.
- What evidence would resolve it: A working RAG system that dynamically classifies hallucination types and differentially validates or filters them.

### Open Question 4
- Question: What mechanisms explain the near-zero correlation (r = 0.009) between model confidence and hallucination value, and can confidence signals be redesigned to better indicate output quality?
- Basis in paper: [inferred] Section 3.1.2 reports r = 0.009 correlation between hallucination scores and trust levels, indicating model confidence does not predict whether outputs are valuable hallucinations, non-valuable hallucinations, or non-hallucinatory.
- Why unresolved: The paper reports this finding but does not investigate why model self-assessment fails to discriminate value.
- What evidence would resolve it: Analysis of model internal states or alternative calibration methods that produce confidence scores correlated with hallucination value categories.

## Limitations
- The study's central claim relies on subjective human annotation rather than automated validation, limiting scalability and objectivity
- Exclusive use of Qwen2.5 and HalluQA dataset restricts generalizability to other models, languages, and task types
- The 1.47% absolute increase in valuable hallucination proportion represents a marginal improvement that may not justify additional computational overhead

## Confidence
- **High confidence**: The empirical finding that ReAct prompting reduces overall hallucinations (5.12% reduction) is well-supported by the experimental design and reproducible in principle.
- **Medium confidence**: The mechanism linking explicit reasoning steps to selective preservation of valuable hallucinations is plausible but not definitively proven—the study shows correlation, not causation.
- **Low confidence**: The claim that valuable hallucinations predominantly emerge from factual fabrication category relies on weak theoretical support rather than experimental validation.

## Next Checks
1. **Cross-linguistic replication**: Test whether the 5.12% hallucination reduction and 1.47% valuable hallucination increase replicate using an English dataset (TruthfulQA or HaluEval) with the same ReAct prompting structure.
2. **Domain-specific validation**: Apply the prompting framework to both creative writing (where valuable hallucinations are desirable) and medical QA (where they're harmful) to determine if the same prompt has opposite effects across domains.
3. **Annotation reliability assessment**: Conduct inter-annotator agreement studies (Cohen's κ) on 100+ responses to establish whether the formal definition of valuable hallucinations (¬p ∩ q) can be operationalized consistently across human raters.