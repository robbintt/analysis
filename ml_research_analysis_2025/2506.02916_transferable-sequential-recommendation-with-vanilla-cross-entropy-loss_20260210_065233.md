---
ver: rpa2
title: Transferable Sequential Recommendation with Vanilla Cross-Entropy Loss
arxiv_id: '2506.02916'
source_url: https://arxiv.org/abs/2506.02916
tags:
- multi-modal
- learning
- recommendation
- information
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMM4Rec addresses the problem of efficient transfer learning in
  multi-modal sequential recommendation, where existing methods suffer from complex
  optimization requirements and slow convergence when adapting to new domains. The
  core method idea combines State Space Duality (SSD)'s temporal decay properties
  with time-aware modeling to dynamically prioritize key modality information.
---

# Transferable Sequential Recommendation with Vanilla Cross-Entropy Loss

## Quick Facts
- arXiv ID: 2506.02916
- Source URL: https://arxiv.org/abs/2506.02916
- Reference count: 40
- Primary result: Achieves 31.78% NDCG@10 improvement and 10× faster convergence in multi-modal sequential recommendation transfer learning

## Executive Summary
MMM4Rec addresses the problem of efficient transfer learning in multi-modal sequential recommendation, where existing methods suffer from complex optimization requirements and slow convergence when adapting to new domains. The core method idea combines State Space Duality (SSD)'s temporal decay properties with time-aware modeling to dynamically prioritize key modality information. The framework implements a constrained two-stage process: sequence-level cross-modal alignment via shared projection matrices, followed by temporal fusion using a newly designed Cross-SSD module and dual-channel Fourier adaptive filtering. This architecture maintains semantic consistency while suppressing noise propagation. MMM4Rec achieves rapid fine-tuning convergence with simple cross-entropy loss, significantly improving multi-modal recommendation accuracy while maintaining strong transferability.

## Method Summary
MMM4Rec uses a two-stage architecture for multi-modal sequential recommendation. First, both image and text modalities pass through weight-shared TiSSD modules to achieve sequence-level cross-modal alignment. Second, the aligned representations are fused using a TiCoSSD module with dual-channel Fourier adaptive filtering that processes time-difference signals in the frequency domain. The model is pre-trained on 5 Amazon domains using cross-entropy loss with in-batch negative sampling, then fine-tuned on 5 downstream domains with full corpus ranking. The architecture maintains simple training objectives while achieving state-of-the-art transfer performance.

## Key Results
- Achieves maximum 31.78% NDCG@10 improvement over existing models
- Exhibits 10× faster average convergence speed when transferring to large-scale downstream datasets
- Maintains strong transferability across diverse Amazon Review domains
- Simple cross-entropy loss achieves rapid fine-tuning convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSD's structured temporal decay naturally prioritizes recent interactions in user sequences, improving sequential recommendation without explicit recency engineering.
- Mechanism: State Space Duality (SSD) implements a semi-separable mask matrix L with state decay coefficient A·Δ, where the algebraic structure causes later sequence positions to receive exponentially higher effective attention weights. This matches SR patterns where short-term preferences dominate.
- Core assumption: Recent interactions better predict next-item behavior than distant history.
- Evidence anchors:
  - [abstract] "combining State Space Duality (SSD)'s temporal decay properties with time-aware modeling design, our model dynamically prioritizes key modality information"
  - [section II-C] Truncation experiment shows retaining last 5 items achieves 97% of full 20-item performance, validating recency bias in SR
  - [corpus] Related work on efficient SR training (arXiv:2509.09682) notes transformer retraining costs but doesn't address SSD's inherent recency properties
- Break condition: If target domain exhibits long-term preference patterns (e.g., seasonal purchases), pure recency bias may underperform.

### Mechanism 2
- Claim: Sequence-level cross-modal weight sharing aligns visual and text modalities into unified recommendation semantics without contrastive learning.
- Mechanism: Both modalities pass through identical TiSSD weights (shared W1, same convolution kernel ω), forcing the projection to learn modality-agnostic sequential patterns. The shared parameters create an implicit alignment constraint.
- Core assumption: Optimal sequential representations should be modality-invariant at the sequence level.
- Evidence anchors:
  - [abstract] "sequence-level cross-modal alignment via shared projection matrices"
  - [section III-C2] "TiSSD modules for both modalities in eq. (10) are weight-shared... compels the feature sequence extraction results of image and text modalities to be projected into a convergent recommendation semantic space"
  - [corpus] BBQRec (arXiv:2504.06636) uses quantization for alignment; MISSRec uses candidate-side fusion—both differ from weight-sharing approach
- Break condition: If modalities carry fundamentally different semantic structures (e.g., video vs. text), weight sharing may cause interference rather than alignment.

### Mechanism 3
- Claim: Dual-channel Fourier filtering adaptively fuses temporal patterns by filtering frequency-domain representations of inter-item time differences.
- Mechanism: Time-difference signals from both modalities undergo FFT → learnable complex-valued filter → inverse FFT. The filter K = δ(D̂) learns to emphasize frequency bands corresponding to relevant temporal patterns while suppressing noise.
- Core assumption: Temporal patterns in user behavior have distinct frequency signatures; filtering in frequency domain is more efficient than time-domain gating.
- Evidence anchors:
  - [abstract] "temporal fusion using our newly designed Cross-SSD module and dual-channel Fourier adaptive filtering"
  - [section III-D1] Eq. 12-15 formalize the FFT → complex linear filter → IFFT pipeline with explicit PyTorch implementation detail
  - [corpus] BSARec and related frequency-domain SR work exist, but corpus papers don't specifically validate dual-channel Fourier for multi-modal fusion
- Break condition: If time differences are uniformly distributed or lack clear frequency structure, learned filters may overfit to noise.

## Foundational Learning

- **State Space Models (Mamba/SSD)**:
  - Why needed here: MMM4Rec's core TiSSD and TiCoSSD modules are built on SSD, which differs fundamentally from attention-based architectures. Understanding discretization (Δ), state transitions (A, B, C), and the linear attention interpretation is essential.
  - Quick check question: Can you explain why SSD achieves O(T·N²) complexity vs. transformer's O(T²·N), and what the mask matrix L represents?

- **Sequential Recommendation Semantics**:
  - Why needed here: The paper's algebraic constraints are designed to match SR-specific properties (recency bias, cross-domain transfer). Without understanding why recency matters, the design rationale is opaque.
  - Quick check question: Why do ID-based models struggle with cold-start and cross-domain transfer compared to multi-modal approaches?

- **Multi-Modal Representation Learning**:
  - Why needed here: The paper explicitly avoids contrastive learning, claiming weight-sharing achieves alignment. Understanding the alignment problem helps evaluate this claim critically.
  - Quick check question: What are the tradeoffs between contrastive alignment (MMSRec) vs. shared-projection alignment (MMM4Rec) for multi-modal fusion?

## Architecture Onboarding

- **Component map**:
  Raw Item (image, text) → Frozen Pretrained Encoders (ViT, BERT) → Modality Adapters (Ψv, Ψt) [trainable] → Optional Modality Bias (Ev, Et) [trainable, domain-specific] → TiSSD (weight-shared across modalities) [alignment] → TiCoSSD with Fourier Filtering [fusion] → Final user representation yL → Score: ⟨yL, candidate⟩ via dot product

- **Critical path**: TiSSD weight-sharing → TiCoSSD fusion → yL extraction. If alignment fails (TiSSD weights diverge by modality), fusion receives misaligned inputs. If time-aware Δ computation is incorrect, the structured mask L won't capture temporal dynamics.

- **Design tradeoffs**:
  - Single-layer TiSSD/TiCoSSD (default) vs. 2-layer: Ablation shows 2-layer helps large domains (Office: N@10 +2.0%) but hurts small domains (Scientific: N@10 -2.6%)
  - ID bias inclusion: Improves personalization but reduces transferability to domains with different ID spaces
  - Pre-training scale: Paper uses 5 merged Amazon domains; smaller pre-training may not learn sufficient priors

- **Failure signatures**:
  - Slow convergence (>20 epochs fine-tuning): Check if pre-trained weights loaded correctly; weight-sharing may have been accidentally disabled
  - Performance degradation vs. ID-only baselines: Likely modality alignment failure; inspect TiSSD output distributions per modality
  - NaN losses: FFT can produce numerical instability with very large time differences; check D normalization in Eq. 5
  - Large domain underperformance vs. small domain: Model capacity may be insufficient; try 2-layer variant

- **First 3 experiments**:
  1. **Sanity check**: Train MMM4Rec from scratch (no pre-training) on a single downstream domain. Compare to ablation "w/o Pretrained" in Table VI to verify implementation.
  2. **Ablation path**: Remove time-aware enhancement (variant w/o Time) and weight-sharing (variant w/o Shared) separately. Results should approximate Table VI degradation patterns.
  3. **Transfer efficiency test**: Measure epochs to convergence on Pantry (small) vs. Office (large). Expect ~5 epochs for Office (per Table V); if >15, investigate candidate scoring implementation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MMM4Rec's architecture effectively extend to three or more modalities (e.g., adding audio or video) without requiring architectural redesign?
- Basis in paper: [inferred] The paper only evaluates image and text modalities. The TiCoSSD module fuses exactly two modalities through cross-attention inspired operations, but the framework does not demonstrate or discuss scaling to additional modalities.
- Why unresolved: The dual-channel Fourier filtering and Cross-SSD fusion are designed for pairwise modality interaction. Adding more modalities may require exponential fusion paths or fundamentally different fusion strategies.
- What evidence would resolve it: Experiments extending MMM4Rec to datasets with three modalities (e.g., video with audio, text, and frames), comparing against both pairwise fusion cascades and redesigned multi-modal fusion blocks.

### Open Question 2
- Question: What are optimal strategies for handling items with missing modalities, and at what missing-rate threshold does multi-modal transfer become detrimental?
- Basis in paper: [explicit] The authors note "many items lack image modalities due to expired URLs" (Table I shows Kindle has 0% image coverage). They "retain modality-missing items following Wang et al.'s experimental settings" but do not systematically analyze the impact of missing rates on transfer performance.
- Why unresolved: The ablation study removes components but does not vary modality missing rates. The Kindle dataset's 0% image coverage means transfer relies entirely on text, yet this extreme case is not isolated for analysis.
- What evidence would resolve it: Controlled experiments synthetically varying modality missing rates (0%, 25%, 50%, 75%, 100%) across pre-training and fine-tuning domains, measuring performance degradation and identifying recovery strategies.

### Open Question 3
- Question: Under what user interaction pattern characteristics does time-aware enhancement provide significant benefits versus negligible gains?
- Basis in paper: [explicit] The ablation study (Table VI, Variant 2 "w/o Time") shows time-aware components provide meaningful gains on Scientific but limited improvements on Office. The authors attribute this to "inherently weak temporal patterns in user interactions within this domain" without quantifying what constitutes weak vs. strong temporal patterns.
- Why unresolved: No quantitative metrics (e.g., inter-action time variance, session regularity) are provided to predict when time-awareness will be beneficial.
- What evidence would resolve it: Analysis correlating temporal interaction statistics (time gap entropy, periodicity measures) with time-aware module ablation gains across multiple domains, yielding predictive guidelines for deployment.

## Limitations
- Assumes consistent temporal patterns across domains, which may not hold for all recommendation scenarios
- Fourier filtering approach lacks ablation studies isolating its specific contribution
- Performance on domains with fundamentally different modality distributions (e.g., video-heavy domains) remains untested

## Confidence
- Transfer efficiency claims (10× faster convergence): High confidence, supported by direct ablation comparisons in Table V
- State-of-the-art performance improvements: High confidence, with statistically significant results across 10 datasets
- Mechanism claims (recency bias, alignment via weight-sharing): Medium confidence, supported by theoretical justification but limited ablation studies

## Next Checks
1. Ablation study isolating the contribution of dual-channel Fourier filtering versus simple temporal gating
2. Transfer learning experiment to a domain with long-term preference patterns (e.g., travel or furniture) to test recency bias limitations
3. Performance evaluation on a video-heavy recommendation dataset to test modality alignment assumptions