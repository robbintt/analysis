---
ver: rpa2
title: 'MergeVQ: A Unified Framework for Visual Generation and Representation with
  Disentangled Token Merging and Quantization'
arxiv_id: '2504.00999'
source_url: https://arxiv.org/abs/2504.00999
tags:
- mergevq
- generation
- tokens
- token
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MergeVQ bridges the gap between visual representation learning
  and image generation by decoupling coarse semantics from fine details via token
  merging, allowing each task to optimize independently while minimizing information
  loss. It introduces a unified architecture where top-k semantic tokens are extracted
  and globally aligned during pre-training, with fine-grained details recoverable
  through source matrix-guided reconstruction.
---

# MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization

## Quick Facts
- **arXiv ID**: 2504.00999
- **Source URL**: https://arxiv.org/abs/2504.00999
- **Reference count**: 40
- **Primary result**: Achieves 79.8% linear probing and 84.2% fine-tuning accuracy for representation learning while using fewer tokens than prior methods

## Executive Summary
MergeVQ introduces a two-stage unified framework that bridges visual representation learning and image generation through disentangled token merging and quantization. The approach decouples coarse semantics from fine details by extracting top-k semantic tokens and globally aligning them during pre-training, while preserving fine-grained information through source matrix-guided reconstruction. The unified architecture supports three variants (G, G+R, R) with different token counts (256, 144, 36 respectively), enabling competitive performance across both representation learning (79.8% linear probing, 84.2% fine-tuning) and high-quality generation (0.54 rFID, gFID 2.24, IS 320.4) on ImageNet-1K.

## Method Summary
MergeVQ operates in two stages: first training a hybrid CNN+Transformer tokenizer with token merging and LFQ quantization, then training an AR generator using the learned codebook. The tokenizer extracts top-k semantic tokens while preserving fine details via source matrices, with losses combining reconstruction (L2), GAN, perceptual, commitment, source recovery, and DINOv2 alignment objectives. For generation, MergeVQ offers MergeAR with KV cache compression and a Source Recovery model compatible with randomized AR generators. The framework uses adaptive merge ratio sampling (exponential for G/R, Gaussian for G+R) and supports class-conditional generation through LlamaGen-L architecture with class conditioning.

## Key Results
- **Representation learning**: 79.8% linear probing top-1 accuracy and 84.2% fine-tuning accuracy on ImageNet-1K
- **Generation quality**: 0.54 rFID for reconstruction and gFID of 2.24 with IS of 320.4 for class-conditional generation
- **Token efficiency**: Achieves competitive results using fewer tokens than prior methods (256/144/36 tokens for G/G+R/R variants)

## Why This Works (Mechanism)
The framework succeeds by disentangling semantic understanding from fine-grained detail preservation through token merging. Coarse semantic tokens are globally aligned during pre-training for better representation learning, while fine details are recoverable through source matrix-guided reconstruction. This separation allows each task to optimize independently without interference, minimizing information loss during quantization.

## Foundational Learning
- **Token Merging and Quantization (TMQ)**: Reduces token sequence length while preserving semantic information through selective merging of similar tokens. Why needed: enables efficient processing and better global alignment of semantic content.
- **LFQ (Learnable Feature Quantization)**: Sign-based channel-wise quantization that learns quantization parameters during training. Quick check: Verify quantization accuracy by comparing reconstructed vs original features.
- **Source Matrix Recovery**: Cross-attention mechanism that maps merged tokens back to original positions for fine-grained detail preservation. Why needed: prevents information loss during token reduction.
- **DINOv2 Alignment**: Self-distillation-based alignment between [CLS] token and DINOv2 features for semantic understanding. Quick check: Measure correlation between aligned features and downstream task performance.

## Architecture Onboarding
**Component map**: Image → CNN Encoder → Transformer Encoder (ToMe blocks) → Quantizer → Decoder (MixFFN) → Reconstructed Image
**Critical path**: Token merging and quantization during pre-training directly impacts both representation accuracy and generation quality
**Design tradeoffs**: Fewer tokens improve global alignment but risk losing fine details; adaptive merge ratios balance efficiency vs fidelity
**Failure signatures**: Poor reconstruction indicates incorrect source matrix implementation; low representation accuracy suggests insufficient alignment or improper token selection
**3 first experiments**: 1) Train tokenizer with fixed merge ratio to verify basic functionality, 2) Implement and visualize source matrix outputs, 3) Test DINOv2 alignment with different feature selection strategies

## Open Questions the Paper Calls Out
None

## Limitations
- DINOv2 alignment implementation is underspecified, particularly regarding which features to align and how to handle the hybrid encoder architecture
- The claim of "unified" framework is somewhat misleading as different variants use different token counts and training objectives
- Direct comparisons with specific baselines for token efficiency are limited, making efficiency claims difficult to validate

## Confidence
- **Representation learning performance**: Medium confidence - methodology is sound but depends heavily on correct DINOv2 alignment
- **Generation quality**: Medium confidence - LlamaGen-L architecture is established but merging mechanism's contribution needs validation
- **Token efficiency claims**: Low confidence - limited direct comparisons with baselines
- **Unified framework benefits**: Low confidence - does not conclusively prove advantages over specialized models

## Next Checks
1. **Validate DINOv2 alignment implementation**: Test different alignment strategies (CLS token vs pooled features, different DINOv2 layers) and measure impact on representation accuracy
2. **Verify source matrix correctness**: Implement visualization of source matrix outputs to confirm merged token positions correctly map to original sequences
3. **Ablation study on token merging**: Train identical architectures with and without token merging to isolate contribution to representation accuracy and generation quality