---
ver: rpa2
title: 'MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting
  Models'
arxiv_id: '2507.06502'
source_url: https://arxiv.org/abs/2507.06502
tags:
- time
- series
- mofe-time
- frequency
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoFE-Time, a novel time series forecasting
  model that leverages a Mixture of Experts (MoE) architecture to integrate both time
  and frequency domain features. By introducing Frequency-Time Cells (FTCs) as specialized
  expert modules, the model enhances its ability to capture intrinsic properties of
  time series signals, addressing the limitations of existing methods that primarily
  focus on time-domain features.
---

# MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models

## Quick Facts
- arXiv ID: 2507.06502
- Source URL: https://arxiv.org/abs/2507.06502
- Authors: Yiwen Liu, Chenyu Zhang, Junjie Song, Siqi Chen, Sun Yin, Zihan Wang, Lingming Zeng, Yuji Cao, Junming Jiao
- Reference count: 38
- One-line primary result: MoFE-Time achieves state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared to Time-MoE

## Executive Summary
MoFE-Time introduces a novel time series forecasting model that combines Mixture of Experts (MoE) architecture with frequency domain analysis. The model addresses limitations of existing methods that primarily focus on time-domain features by introducing Frequency-Time Cells (FTCs) as specialized expert modules. By leveraging both frequency and time domain features through MoE routing, the model captures intrinsic properties of time series signals more effectively. The approach employs a pretraining-finetuning paradigm to transfer prior knowledge across datasets with varying periodicity distributions, demonstrating significant improvements on six public benchmarks and a proprietary NEV-sales dataset.

## Method Summary
MoFE-Time is a decoder-only Transformer-based architecture that integrates Frequency-Time Cells (FTCs) into a Mixture of Experts framework. The model uses RevIN normalization and point-wise embeddings with gated dilated convolutions as input preprocessing. After attention layers, inputs are routed through multiple FTC experts that learn harmonic representations of frequency components. The MoE router adaptively selects relevant experts per input, combining their frequency-domain outputs with time-domain features. The model employs a two-stage training approach: large-scale pretraining on Time-300B (300B time points across multiple domains) followed by fine-tuning on target datasets. FTC experts learn distinct critical harmonic frequencies during pretraining, which are then combined with routing weights as amplitudes during inference.

## Key Results
- Achieves state-of-the-art performance on six public benchmarks, reducing MSE by 6.95% and MAE by 6.02% compared to Time-MoE
- Pretraining stage provides the largest contribution to performance improvements across all datasets
- Demonstrates effectiveness on both public benchmarks and proprietary NEV-sales dataset
- Ablation studies confirm the importance of frequency domain analysis and pretraining in improving predictive accuracy

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Time Cells Learn Harmonic Representations
The FTC module enables the model to learn which frequency components are critical for prediction rather than applying fixed frequency transforms. Each expert in the MoE layer learns h distinct critical harmonic frequencies during pretraining. During inference, the MoE router selects top-k experts, and their learned frequencies are combined with routing weights as amplitudes. The output is a harmonic combination representing the signal's periodicity, concatenated with time-domain features. This approach assumes time series signals can be decomposed into a small set of dominant harmonic frequencies that generalize across domains.

### Mechanism 2: MoE Routing Enables Adaptive Frequency Selection
The sparse routing mechanism allows different experts to specialize in different frequency bands, with the router adaptively selecting relevant experts per input. After the attention layer, input is routed through multiple FTC experts. The router produces weights α_i for each expert. The final output combines time-domain features with a frequency-domain representation built from the weighted sum of expert harmonics, creating input-adaptive frequency modeling. This assumes different input signals benefit from different frequency representations, and a sparse subset of experts suffices for each signal.

### Mechanism 3: Pretraining Transfers Frequency Priors Across Domains
Large-scale pretraining on diverse time series teaches the model common periodicity patterns that improve downstream forecasting. The model is pretrained on Time-300B (300B time points across energy, retail, weather, finance). During this phase, experts learn frequency components common across domains. Fine-tuning adapts these priors to target datasets. Ablation shows pretraining provides the largest performance gain among all components. This assumes time series from different domains share some underlying periodicity structures (daily cycles, seasonal patterns) that transfer.

## Foundational Learning

- **Fourier Analysis and Harmonic Decomposition**
  - Why needed here: The FTC module assumes signals can be represented as sums of sinusoids. Understanding Equation 9 (x_n = Σ a_i e^{-jω_i n}) as signal reconstruction from frequency components is essential.
  - Quick check question: Can you explain why a non-periodic discrete-time signal has a continuous spectrum?

- **Mixture of Experts (MoE) Routing**
  - Why needed here: The model uses sparse top-k routing to select which FTC experts process each token. You must understand how router weights control expert selection and how auxiliary loss (Eq. 15) prevents expert collapse.
  - Quick check question: What happens to model capacity if the router always selects the same expert?

- **Instance Normalization for Non-Stationary Data**
  - Why needed here: RevIN (Eq. 2-4) normalizes each input sequence to consistent mean/variance, critical for handling distribution shift in real-world time series.
  - Quick check question: Why must denormalization be applied at output time, and what information must be preserved from input?

## Architecture Onboarding

- Component map:
```
Input → RevIN → Point-wise Embedding → Dilated Conv1D → [N × Transformer Blocks] → Multi-Resolution Head → RevIN Denorm → Output
                                        |
                                        ↓
                                   [Attention → RMSNorm → MoE Layer with FTCs → RMSNorm]
                                                       |
                                                       ↓
                                                  Router → Top-k FTC Experts
                                                       |
                                                       ↓
                                              Each FTC: Linear → Harmonic Functions → Combine with Time Features
```

- Critical path:
  1. RevIN normalization (removes non-stationarity, preserves affine params)
  2. Embedding with dilated conv (captures local context efficiently)
  3. Attention layers (capture long-range dependencies)
  4. **MoE with FTC** (core: learn and select frequency representations)
  5. Multi-resolution head (predicts at multiple horizons)
  6. RevIN denormalization (restores original scale)

- Design tradeoffs:
  - **Number of experts (k)**: More experts = more frequency specialization, but higher memory and routing complexity. Paper doesn't specify exact k.
  - **Frequencies per expert (h)**: More frequencies = richer representation but may overfit. Learned during pretraining.
  - **Pretraining dataset composition**: Time-300B covers many domains; if your domain is absent, pretraining may not transfer well.

- Failure signatures:
  - **Expert collapse**: If auxiliary loss weight α is too low, router always selects same experts. Monitor expert utilization.
  - **Frequency aliasing**: If input has higher frequency content than learned harmonics, model misses those patterns. Check spectral analysis of residuals.
  - **RevIN artifacts**: If denorm parameters aren't preserved correctly, predictions have wrong scale/offset.

- First 3 experiments:
  1. **Replicate ablation without pretraining**: Train from scratch on a single dataset (e.g., ETTh1) to isolate FTC contribution. Compare to Time-MoE with standard feedforward experts.
  2. **Visualize expert frequency specialization**: Extract learned frequencies Ω_i from each expert after training. Plot which frequencies each expert specializes in and check alignment with known domain periodicities.
  3. **Test on synthetic signals with known periodicity**: Create signals with specific frequencies (as in Figure 5). Verify the model correctly uses experts with matching learned frequencies—sanity check for the frequency learning mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of architectural specifications (hidden dimensions, number of layers, experts, routing parameters) makes faithful reproduction challenging
- Extremely aggressive training setup (batch size=2, 1 epoch pretraining) may not generalize across different hardware configurations
- Proprietary NEV-sales dataset results cannot be independently verified, limiting external validation
- No analysis of computational efficiency or inference cost comparisons against baselines

## Confidence
- **High Confidence**: Core architectural contribution (FTC module with learned frequency representations) and pretraining-finetuning paradigm are well-explained and grounded in established techniques. Ablation study results showing pretraining's importance are convincing.
- **Medium Confidence**: Empirical results on public benchmarks appear strong, but unusual training setup (1 epoch total training) raises questions about robustness without hyperparameter sensitivity analysis.
- **Low Confidence**: Proprietary dataset results and exact implementation details of FTC module (particularly complex number handling in harmonic functions) lack sufficient detail for independent verification.

## Next Checks
1. **Replicate ablation without pretraining**: Train MoFE-Time from scratch on ETTh1 (no pretraining) and compare against Time-MoE with standard feedforward experts. This isolates the FTC contribution and tests whether learned frequency representations provide value independent of pretraining.

2. **Expert frequency specialization analysis**: After training, extract and visualize the learned frequencies from each expert. Plot the frequency distribution per expert and verify they specialize in different bands rather than collapsing to similar frequencies. Check alignment with known domain periodicities (daily, weekly patterns in energy data).

3. **Synthetic signal test**: Create synthetic time series with known periodic components (as shown in Figure 5). Test whether MoFE-Time correctly selects experts whose learned frequencies match the signal's dominant frequencies. This provides a controlled environment to validate the frequency learning mechanism.