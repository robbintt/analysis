---
ver: rpa2
title: 'Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually
  Rich Documents Understanding'
arxiv_id: '2601.12260'
source_url: https://arxiv.org/abs/2601.12260
tags:
- document
- retriever
- synthetic
- inference
- docs2synth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Docs2Synth introduces a synthetic-data-driven retriever framework
  for scanned visually rich document understanding. The system automatically generates
  and verifies diverse QA pairs from raw documents, fine-tunes a lightweight visual
  retriever on synthetic data, and uses iterative retrieval-generation loops to enhance
  MLLM inference with improved grounding and reduced hallucination.
---

# Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding

## Quick Facts
- arXiv ID: 2601.12260
- Source URL: https://arxiv.org/abs/2601.12260
- Reference count: 9
- Primary result: Synthetic-data-driven retriever framework for scanned visually rich document understanding

## Executive Summary
Docs2Synth presents a novel approach to understanding scanned visually rich documents (VRDs) by leveraging synthetic data generation and a trained retriever framework. The system automatically generates and verifies diverse question-answer pairs from raw documents, fine-tunes a lightweight visual retriever on this synthetic data, and employs iterative retrieval-generation loops to enhance large multimodal language model (MLLM) inference. This approach improves grounding and reduces hallucination in document understanding tasks. Experiments demonstrate consistent performance improvements across Form-NLU, CORD, and Ephoie datasets compared to strong MLLMs, all without requiring human annotations.

## Method Summary
The Docs2Synth framework operates through a three-stage pipeline: first, it automatically generates synthetic QA pairs from raw scanned documents using template-based and context-aware generation strategies; second, it fine-tunes a lightweight visual retriever model on this synthetic dataset to learn document-specific retrieval patterns; and third, it implements an iterative retrieval-generation loop where the retriever provides grounded evidence to the MLLM during inference, with the MLLM's outputs feeding back into the retrieval process for refinement. The framework is packaged as a modular Python library designed for production deployment across various document types and domains.

## Key Results
- Consistently outperforms strong MLLMs on key information extraction tasks across Form-NLU, CORD, and Ephoie datasets
- Achieves improved grounding and reduced hallucination in document understanding through iterative retrieval-generation loops
- Delivers a modular, production-ready Python package supporting flexible deployment without human annotations

## Why This Works (Mechanism)
The framework's effectiveness stems from its synthetic data-driven approach that addresses the scarcity of annotated VRDs while maintaining high quality through automatic verification. By fine-tuning a lightweight retriever specifically on document-grounded synthetic data, the system learns to identify relevant evidence patterns that general MLLMs might miss. The iterative retrieval-generation loop creates a feedback mechanism where retrieved evidence continuously improves the MLLM's responses, while the MLLM's outputs help refine subsequent retrieval queries, creating a self-improving cycle that enhances both precision and recall in information extraction tasks.

## Foundational Learning
- **Synthetic Data Generation**: Automatically creating QA pairs from raw documents - needed to overcome annotation scarcity; quick check: verify generation templates cover diverse document structures
- **Visual Retriever Training**: Fine-tuning models on synthetic data - needed for domain-specific retrieval patterns; quick check: measure retrieval accuracy on held-out synthetic samples
- **Iterative Retrieval-Generation**: Looping between retrieval and generation - needed to reduce hallucination; quick check: compare performance with single-pass generation baseline
- **MLLM Grounding**: Using retrieved evidence to constrain generation - needed for factual consistency; quick check: measure hallucination rate reduction
- **Cross-Document Generalization**: Applying learned patterns to new document types - needed for production flexibility; quick check: test on documents from unseen domains
- **Automated Verification**: Quality control for synthetic data - needed to maintain training data reliability; quick check: measure verification precision and recall

## Architecture Onboarding
**Component Map**: Raw Document -> Synthetic QA Generator -> Data Verifier -> Visual Retriever Trainer -> Iterative Loop Controller -> MLLM Generator -> Output
**Critical Path**: Document Ingestion → Synthetic QA Generation → Retriever Training → Inference Pipeline → Iterative Refinement
**Design Tradeoffs**: Lightweight retriever versus computational overhead of iteration; synthetic data volume versus quality; modular flexibility versus integration complexity
**Failure Signatures**: Poor retrieval quality manifests as irrelevant evidence to MLLM; hallucination persists if verification threshold is too low; performance degradation on unseen document types suggests overfitting to training synthetic patterns
**Three First Experiments**: 1) Test synthetic QA generation coverage across document types, 2) Benchmark retriever performance on synthetic versus real data, 3) Measure hallucination reduction with varying iteration counts

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance claims lack detailed baseline comparisons, statistical significance measures, and specific metric values
- The complexity of synthetic data generation and verification processes is not fully described, raising questions about potential hidden human involvement
- Production readiness claims are not supported with computational requirements, deployment complexity analysis, or scalability validation

## Confidence
- High confidence: Framework architecture description (synthetic data generation → retriever training → iterative retrieval-generation) is internally consistent and technically plausible
- Medium confidence: Claims about improved grounding and reduced hallucination are reasonable given the iterative retrieval approach, but specific effectiveness measures are not provided
- Low confidence: Claims about production readiness and ease of deployment across domains lack supporting implementation details

## Next Checks
1. Request and analyze the full experimental results table showing baseline MLLM performance versus Docs2Synth performance across all metrics, including confidence intervals and statistical significance tests
2. Examine the synthetic data generation pipeline code to identify any human-in-the-loop steps or domain-specific knowledge requirements that might limit the "annotation-free" claim
3. Request benchmark results on additional document types beyond the three mentioned datasets to verify the claimed "flexible deployment across domains" capability