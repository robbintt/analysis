---
ver: rpa2
title: 'Extracting Patient History from Clinical Text: A Comparative Study of Clinical
  Large Language Models'
arxiv_id: '2503.23281'
source_url: https://arxiv.org/abs/2503.23281
tags:
- sample
- history
- clinical
- error
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Clinical large language models (cLLMs) were fine-tuned to extract
  patient history entities (CC, HPI, PFSH) from 61 outpatient clinical notes. GatorTronS+CLAMP
  achieved the lowest error rate at 61.8%, saving over 20% of manual extraction time.
---

# Extracting Patient History from Clinical Text: A Comparative Study of Clinical Large Language Models

## Quick Facts
- arXiv ID: 2503.23281
- Source URL: https://arxiv.org/abs/2503.23281
- Reference count: 40
- GatorTronS+CLAMP achieved the lowest error rate at 61.8%, saving over 20% of manual extraction time.

## Executive Summary
This study compares clinical large language models (cLLMs) with GPT-4o for extracting patient history entities from outpatient clinical notes. Seven cLLMs were fine-tuned on 61 annotated clinical notes to extract Chief Complaint (CC), History of Present Illness (HPI), and Past/Family/Social History (PFSH) entities. GatorTronS+CLAMP achieved the best performance with a 61.8% error rate, significantly outperforming GPT-4o's 164.1% error rate in a zero-shot setting. The study identifies key challenges including polysemous medical concepts and long-entity extraction difficulties.

## Method Summary
The study fine-tuned seven clinical LLMs (BioBERT variants, PubMedBERT, BioMegatron, GatorTron, GatorTronS) using BIO tagging to extract medical history entities from 61 outpatient clinical notes. Two approaches were compared: basic fine-tuning with linear output layers, and enhanced fine-tuning incorporating Basic Medical Entities (BMEs) pre-identified by CLAMP. The models were evaluated using five-fold cross-validation on exact match, relaxed match, and mismatch criteria, with error rates calculated as the sum of mismatches and under/over-detections divided by total annotations.

## Key Results
- GatorTronS+CLAMP achieved the lowest error rate at 61.8% for patient history extraction
- Fine-tuned cLLMs significantly outperformed GPT-4o zero-shot (61.8% vs 164.1% error rate)
- Well-structured sections with headings improved CC and PFSH detection accuracy
- Entities longer than 3 tokens had significantly higher error rates due to relaxed matches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning clinical LLMs on domain-specific corpora yields lower error rates than zero-shot general-purpose models.
- Mechanism: Specialized pre-training on clinical data encodes medical semantics and polysemous relationships, enabling better distinction between entity types.
- Core assumption: The training corpus shares sufficient linguistic distribution with pre-training data for effective transfer learning.
- Evidence anchors: GatorTron+CLAMP achieved 61.4% error rate vs GPT-4o's 164.1%; both abstracts and table 2 support this comparison.

### Mechanism 2
- Claim: Explicit structural cues improve extraction accuracy for high-level concepts while detection fidelity decreases as entity text length increases.
- Mechanism: Headers act as strong priors narrowing search space for specific entity classes; longer spans increase token-level misalignments.
- Core assumption: Models process tokens sequentially without robust global span memory.
- Evidence anchors: Abstract mentions benefits of well-organized segments; table 4 shows average token lengths of ~1.9 for exact vs ~3.0-4.6 for relaxed matches; figure 5 shows p<0.001 significance for CC and Social History in dedicated sections.

### Mechanism 3
- Claim: Integrating pre-identified BMEs as auxiliary features helps resolve ambiguity for complex history entities.
- Mechanism: Concatenating one-hot encoded BME tags with cLLM embeddings provides explicit semantic clues.
- Core assumption: CLAMP's precision is sufficient and errors don't cascade into final MHE classification.
- Evidence anchors: Figure 3 illustrates architecture; page 11 notes CLAMP integration improved accuracy for CC, location, duration, and modifying factors.

## Foundational Learning

- Concept: **Named Entity Recognition (NER) & BIO Tagging**
  - Why needed here: The paper models extraction as token classification where every word is tagged as Beginning (B), Inside (I), or Outside (O) an entity.
  - Quick check question: If a model tags "chest pain" as B-Problem and O, what is the resulting entity span?

- Concept: **Fine-tuning vs. Zero-Shot Learning**
  - Why needed here: The study's central comparison between cLLMs (fine-tuned) and GPT-4o (zero-shot).
  - Quick check question: Which approach requires labeled training data but offers better performance on specialized tasks?

- Concept: **Exact vs. Relaxed Match Evaluation**
  - Why needed here: The paper reports error rates based on strict exact matches but notes that "relaxed matches" might still be usable.
  - Quick check question: Does a "Relaxed Match" require the predicted text span to perfectly align with the annotated span?

## Architecture Onboarding

- Component map: Raw clinical text -> CLAMP Toolkit (optional) -> BMEs -> One-hot encoding -> cLLM embeddings -> Concatenation layer -> Linear output layer -> HTML/JSON entities

- Critical path: The Fusion Layer and Output Head are where learning happens. The model relies on the fusion layer to map explicit BME tags to target MHE tags effectively.

- Design tradeoffs:
  - GatorTron (Fine-tuned) vs. GPT-4o (Zero-shot): GatorTron offers lower error rates and data privacy but requires annotation/training overhead.
  - With vs. Without CLAMP: CLAMP integration improves detection but adds complexity and dependency on external NER tool.

- Failure signatures:
  - High Over-Detection: GPT-4o showed >100% over-detection, cluttering output.
  - Long-Entity Fragmentation: Models frequently produced "Relaxed Matches" for entities >4 tokens.
  - Polysemy Confusion: Entities like "Modifying Factors" suffer high error rates due to semantic ambiguity.

- First 3 experiments:
  1. Run GPT-4o in zero-shot setting with HTML extraction prompt to establish baseline.
  2. Train GatorTron using basic fine-tuning vs CLAMP-enhanced architecture to quantify BME gain.
  3. Evaluate model performance specifically on "Long Entity" subset (>4 tokens) to confirm span-improvement needs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative LLMs improve extraction of semantically complex entities (e.g., HPI-Context, Quality, Timing) compared to fine-tuned discriminative cLLMs?
- Basis in paper: The Discussion section notes that "Generative LLMs that excel at discovering semantic relationships... are potentially well-suited" for these difficult entity types.
- Why unresolved: The study evaluated discriminative cLLMs and zero-shot GPT-4o but not fine-tuned or few-shot generative approaches for these semantic tasks.
- What evidence would resolve it: Comparative study evaluating fine-tuned generative LLMs against discriminative models specifically on HPI-Context and HPI-Timing entities.

### Open Question 2
- Question: Do specialized NER architectures tailored for long textual spans significantly reduce relaxed match rates for Medical History Entities?
- Basis in paper: The Discussion states that "NER techniques tailored for long textual span, though currently underdeveloped, are desired" because long entities are prone to fragmented extraction.
- Why unresolved: Current methodology relied on standard fine-tuning with fully-connected output layers resulting in higher relaxed match rates.
- What evidence would resolve it: Experiments applying span-based or hierarchical NER models to measure reduction in relaxed matches versus exact matches.

### Open Question 3
- Question: How does performance of fine-tuned cLLMs generalize to non-outpatient clinical notes like inpatient or discharge summaries?
- Basis in paper: The Limitations section states the study focused on outpatient notes and "Future work should expand the evaluation to a broader corpus."
- Why unresolved: Training and evaluation corpus consisted solely of 61 outpatient-related notes.
- What evidence would resolve it: Validation of top-performing models on benchmark datasets comprising diverse clinical settings.

## Limitations

- Small sample size (61 clinical notes) limits generalizability to real-world clinical documentation diversity
- Error rate calculation method (MMUD + over-detections) is unconventional and may understate true performance gaps
- Massive over-detection with GPT-4o may be due to prompt design rather than fundamental model limitations

## Confidence

- Fine-tuning cLLMs outperforms zero-shot approaches: Medium
- Structural cues (headers) improve extraction accuracy: Medium-High
- BME feature concatenation provides meaningful improvement: Medium
- Long entities are inherently harder to extract: Medium-High
- Clinical notes' polysemous concepts remain a significant challenge: High

## Next Checks

1. **Dataset Diversity Validation**: Test GatorTronS+CLAMP on larger, more diverse corpus from multiple institutions and note types to verify 61.8% error rate holds across different clinical contexts.

2. **Prompt Engineering Benchmark**: Systematically compare GPT-4o with optimized prompts versus fine-tuned cLLMs to determine if performance gap is due to model architecture or prompt design.

3. **Long-Entity Extraction Benchmark**: Create evaluation set containing only entities longer than 4 tokens and test whether span-based NER techniques outperform token classification approach for these cases.