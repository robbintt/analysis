---
ver: rpa2
title: 'Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification'
arxiv_id: '2508.12418'
source_url: https://arxiv.org/abs/2508.12418
tags:
- data
- time
- which
- sensor
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Bi-Axial Transformer (BAT), a model designed
  to handle the increasing complexity of Electronic Health Records (EHRs) by attending
  to both the clinical variable and time point axes of EHR data. BAT addresses the
  limitations of existing transformer models by preserving informative missingness
  and learning complex, multi-axis relationships from EHR data.
---

# Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification

## Quick Facts
- arXiv ID: 2508.12418
- Source URL: https://arxiv.org/abs/2508.12418
- Reference count: 40
- Primary result: State-of-the-art sepsis prediction performance with increased robustness to missing data

## Executive Summary
This paper introduces the Bi-Axial Transformer (BAT), a novel transformer architecture designed specifically for Electronic Health Records (EHRs) that addresses the increasing complexity of modern clinical data. BAT employs bi-axial attention to capture relationships across both temporal and sensor dimensions simultaneously, preserving informative missingness patterns and learning unique sensor embeddings for transfer learning. The model achieves state-of-the-art results on sepsis prediction and competitive performance on mortality classification while demonstrating superior robustness to data sparsity compared to existing transformer approaches.

## Method Summary
BAT processes EHR data using two parallel attention tracks that operate on different axis orders (sensor→time vs. time→sensor) with shared parameters within each track. The model concatenates sensor readings with binary missingness indicators, embeds sensor identities separately for transfer learning potential, and includes sinusoidal positional encodings. A parallel demographics branch is merged with pooled attention outputs for final classification. The architecture is trained with class balancing (3× positive resampling per epoch) using AdamW optimizer.

## Key Results
- State-of-the-art AUROC/AUPRC on P19 sepsis prediction (62.68/50.34)
- Competitive mortality prediction on P12 (84.35/52.07) and MIMIC-III (85.25/52.67)
- Superior robustness to data sparsity, maintaining performance at 75% missingness where other transformers degrade
- Shared sensor embeddings reduce variance in joint training without sacrificing accuracy

## Why This Works (Mechanism)

### Mechanism 1: Bi-Axial Attention for Cross-Dimensional Relationship Learning
BAT captures complex relationships across both temporal and sensor dimensions that single-axis transformers miss. Two parallel attention tracks process the data in different axis orders—one attends across sensors first then time, the other time first then sensors. Parameters are shared across attention passes within each track, enabling information flow between axes without flattening to a prohibitive sequence length.

### Mechanism 2: Informative Missingness Preservation via Indicator Masks
Binary indicator masks concatenated to sensor readings allow BAT to learn from patterns of missing data, which carries predictive signal in clinical contexts. Each observation embedding includes both the sensor reading and a binary mask indicating whether the reading was observed (1) or missing (0).

### Mechanism 3: Explicit Sensor Embeddings for Cross-Dataset Transfer
Learning separate linear embeddings for each sensor identity (rather than implicit position-based representation) enables transfer learning and dataset harmonization. Sensor identities are embedded with a dedicated linear layer applied to one-hot encodings, producing embeddings that persist across all time series.

## Foundational Learning

- **Self-Attention and Multi-Head Attention**: BAT builds on standard transformer attention (Q, K, V projections with scaled dot-product). Understanding how attention weights represent token importance is essential before grasping axial decomposition.
- **Informative Missingness in Clinical Data**: The paper's core design choice is preserving rather than imputing missing values. You need to understand why missingness patterns correlate with outcomes (e.g., sicker patients get more frequent labs).
- **Axial Attention**: BAT adapts axial attention (originally for images) to EHR by treating time and sensors as two axes. Understanding the decomposition into row/column attention is prerequisite to implementing the dual-track architecture.

## Architecture Onboarding

- **Component map**: Input Embedding Layer -> Two Attention Tracks (sensor→time and time→sensor) -> Pooling Layer -> Demographics Branch -> Merge + Classification
- **Critical path**: Data must be shaped as Ti×D×E (time × sensors × embedding dim) after embedding stage. Each attention track clones input, applies axial attention in sequence, produces weighted embeddings. Both tracks must complete before pooling—no cross-track communication until merge stage.
- **Design tradeoffs**: Shared vs. separate track parameters (paper shares encoder params within track but not across tracks); pooling function (max vs. mean); embedding size vs. sensors (larger embeddings capture more sensor semantics but increase memory).
- **Failure signatures**: Attention attending uniformly across time/sensors (may indicate embeddings lack discriminative signal or learning rate too high); performance degradation at high sparsity (>95%) (paper shows all transformers degrade to random at 99% sparsity); high variance across splits (may indicate overfitting to specific patient subgroups).
- **First 3 experiments**: Ablation on your data (train BAT with and without indicator mask); sparsity robustness test (artificially mask increasing percentages of data and compare BAT vs. standard Transformer); sensor embedding transfer (train jointly with shared vs. separate sensor embeddings and measure variance reduction).

## Open Questions the Paper Calls Out

- How do knowledge graph-based or LLM-derived sensor embeddings compare to the simple linear layer approach used in BAT regarding classification accuracy and transferability?
- To what extent does large-scale pretraining of the Bi-Axial Transformer on heterogeneous EHR datasets improve downstream performance compared to training from scratch?
- Why does BAT outperform GRU-D on sepsis prediction but underperform on mortality classification, and is this variance attributable to dataset-specific sparsity or label definitions?

## Limitations
- Performance advantage of bi-axial attention over single-axis transformers not isolated in ablation studies
- Shared sensor embeddings show modest improvements in variance but limited gains in accuracy
- Model complexity may be unnecessary for datasets with weak sensor-time interdependencies

## Confidence

- **High confidence**: Informative missingness preservation improves performance over imputation/dropping
- **Medium confidence**: Bi-axial attention provides meaningful improvements over single-axis transformers
- **Low confidence**: Shared sensor embeddings provide robust transfer learning benefits

## Next Checks

1. **Mechanism isolation**: Run ablation comparing BAT's bi-axial attention to a standard transformer with double the layers but single-axis attention
2. **Cross-dataset transfer test**: Train BAT on MIMIC-III with shared embeddings, then evaluate zero-shot on a new ICU dataset with overlapping sensors
3. **Sparsity sensitivity analysis**: Systematically vary missingness patterns (random vs. clinical) from 0-99% and plot performance curves for BAT, standard Transformer, and RNN baselines