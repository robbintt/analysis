---
ver: rpa2
title: 'The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance
  in Multimodal AI'
arxiv_id: '2505.03020'
source_url: https://arxiv.org/abs/2505.03020
tags:
- multimodal
- performance
- modalities
- fairness
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how the addition and absence of modalities
  impact the performance and fairness of multimodal AI models, using healthcare datasets
  MIMIC-Eye and MIMIC-IV-Ext-MDS-ED. The authors evaluate two research questions:
  (RQ1) whether adding modalities consistently improves performance and fairness,
  and (RQ2) how missing modalities at inference affect robustness.'
---

# The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI

## Quick Facts
- **arXiv ID:** 2505.03020
- **Source URL:** https://arxiv.org/abs/2505.03020
- **Reference count:** 18
- **Primary result:** Adding modalities improves performance (F1 up to 0.92, AUC-ROC up to 0.91) but fairness outcomes fluctuate, while missing modalities degrade both performance and fairness even at low rates.

## Executive Summary
This paper investigates how the addition and absence of modalities impact the performance and fairness of multimodal AI models, using healthcare datasets MIMIC-Eye and MIMIC-IV-Ext-MDS-ED. The authors evaluate two research questions: (RQ1) whether adding modalities consistently improves performance and fairness, and (RQ2) how missing modalities at inference affect robustness. Their method incrementally adds modalities during training and randomly masks modalities during inference to assess these impacts. Results show that adding modalities improves overall performance (F1 scores up to 0.92 and AUC-ROC up to 0.91), but fairness disparities fluctuate, particularly for race and gender. Missing modalities, even in small percentages, consistently degrade both performance and fairness. The study highlights the need for careful modality selection and robust model design to ensure fair and reliable multimodal AI in real-world healthcare applications.

## Method Summary
The method converts all modalities (structured data, ECG, chest X-rays) to text representations using MedBERT embeddings. X-rays are transcribed to text using a fine-tuned Qwen 2.5 VL model, ECGs are interpreted using PULSE-7B, and structured data is serialized to clinical pseudo-notes. These text embeddings are concatenated and processed through a self-attention module for cross-modal fusion, followed by a feed-forward classifier. Research Question 1 evaluates incremental modality addition during training on MIMIC-EYE and MIMIC-IV-Ext-MDS-ED datasets. Research Question 2 tests robustness by randomly masking modalities at inference using Bernoulli sampling. Performance is measured via F1, AUC-ROC, AUPRC, and balanced accuracy, while fairness is assessed using Demographic Parity and True Positive Rate stratified by race and gender.

## Key Results
- Adding modalities consistently improves performance (F1 up to 0.92, AUC-ROC up to 0.91) across datasets and tasks.
- Fairness trends exhibit variability across different evaluation measures and datasets, with DP and TPR disparities fluctuating by race and gender.
- Missing modalities at inference, even at low missingness rates, consistently degrade both performance and fairness, raising concerns about real-world robustness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding modalities during training yields monotonic performance improvements but non-monotonic fairness outcomes.
- Mechanism: Each modality is embedded into a unified text-based feature space via MedBERT, concatenated, and processed through self-attention to capture cross-modal interactions. Additional modalities provide complementary signal for classification but may also introduce modality-specific biases that differentially affect demographic subgroups.
- Core assumption: Modality monotonicity holds for predictive performance but not necessarily for fairness metrics; the unified embedding space preserves both predictive signal and bias-correlated patterns.
- Evidence anchors:
  - [abstract]: "incorporating new modalities during training consistently enhances the performance of multimodal models, while fairness trends exhibit variability across different evaluation measures and datasets"
  - [section 5.2.1]: "fairness disparities fluctuate among the datasets. For MIMIC-Eye, disparity emerges, particularly between White and Non-White groups, with Non-White individuals consistently experiencing lower fairness"
  - [corpus]: Related work (FAME, impuTMAE) confirms that multimodal embeddings can propagate or amplify bias if not explicitly regularized; corpus evidence for monotonic fairness improvement is absent.
- Break condition: If a modality is corrupted, misaligned, or highly correlated with protected attributes, adding it may degrade both performance and fairness simultaneously.

### Mechanism 2
- Claim: Missing modalities at inference cause compounding degradation in both performance and fairness, even at low missingness rates.
- Mechanism: The self-attention fusion layer learns cross-modal dependencies during training. When modalities are masked at inference (via Bernoulli sampling), the attention weights receive zeroed inputs, disrupting the learned fusion patterns and producing out-of-distribution representations that disproportionately harm underrepresented groups.
- Core assumption: The model does not have robust imputation or graceful degradation mechanisms; masked inputs propagate through the fusion layer as information gaps rather than neutral placeholders.
- Evidence anchors:
  - [abstract]: "the absence of modalities at inference degrades performance and fairness, raising concerns about its robustness in real-world deployment"
  - [section 3 + Algorithm 1]: Random modality masking uses independent Bernoulli sampling per modality; predictions are generated from masked inputs without imputation.
  - [corpus]: Related papers (impuTMAE, Distilled Prompt Learning) propose masked pre-training and imputation to address this specific failure mode, confirming the brittleness of standard fusion.
- Break condition: If the architecture includes modality-agnostic fallback paths, learned imputation, or robust attention masking, degradation may be mitigated.

### Mechanism 3
- Claim: Modality-specific contributions to fairness vary, with some modalities (e.g., structured clinical notes like "Arrival") more predictive and equitable than others (e.g., imaging).
- Mechanism: Different modalities capture different underlying distributions. Structured data (Arrival, Codes) may encode standardized clinical workflows with less demographic skew, while imaging (X-ray, ECG) interpretation via VLMs can inherit biases from training data or generate descriptions that reflect systemic healthcare disparities.
- Core assumption: The text-based unification approach (MedBERT + VLM-generated reports) preserves bias patterns present in the original modality or introduced during the transcription process.
- Evidence anchors:
  - [section 5.2.1, Table 1]: "Arrival achieves the highest F1 Score (0.920), while X-ray struggles with the lowest AUC-ROC (0.513)"; DP and TPR disparities by race and gender vary non-uniformly across modalities.
  - [section 4]: Chest X-rays are transcribed using a fine-tuned Qwen 2.5 VL model; ECGs use PULSE-7B—both introduce VLM-specific biases.
  - [corpus]: Related work on text-guided fairness (Leveraging Text Guidance for Demographic Fairness) suggests textual representations can either mitigate or propagate bias depending on training.
- Break condition: If VLM-generated text descriptions are explicitly debiased or if structured modalities are missing for certain subpopulations, the fairness distribution across modalities will shift unpredictably.

## Foundational Learning

- Concept: **Demographic Parity (DP) and True Positive Rate (TPR) as fairness metrics**
  - Why needed here: The paper evaluates fairness using DP (equal prediction rates across groups) and TPR (equal true positive rates). Understanding these metrics is essential to interpret why adding modalities may widen DP gaps while TPR trends differ.
  - Quick check question: If a model admits 70% of White patients and 60% of Non-White patients to the hospital, does this violate DP, TPR, or both?

- Concept: **Modality fusion via concatenation and self-attention**
  - Why needed here: The architecture unifies heterogeneous modalities (images, time series, structured data) by converting all to text embeddings, then fusing via self-attention. This fusion mechanism is directly responsible for both performance gains and brittleness under missing modalities.
  - Quick check question: If one modality is masked to zero at inference, how does the self-attention layer behave differently compared to training?

- Concept: **Bernoulli masking for robustness testing**
  - Why needed here: The paper uses independent Bernoulli sampling to simulate missing modalities. This is a standard but simplistic approach—understanding its limitations helps interpret the RQ2 results.
  - Quick check question: What is the expected probability of all modalities being present if p=0.9 and there are 5 modalities?

## Architecture Onboarding

- Component map: Raw modalities → text conversion → MedBERT embedding → concatenation → self-attention fusion → classification
- Critical path: Raw modalities → text conversion → MedBERT embedding → concatenation → self-attention fusion → classification. Missing modalities break this path at the concatenation stage.
- Design tradeoffs:
  - Text-based unification enables heterogeneous modalities but introduces VLM transcription bias.
  - Self-attention fusion captures cross-modal interactions but is brittle to missing inputs.
  - Incremental modality addition improves performance but fairness outcomes are dataset-dependent.
- Failure signatures:
  - Performance plateaus or drops when adding a modality: suspect modality misalignment or noise.
  - Fairness disparities widen with added modalities: check for spurious correlations in the new modality.
  - Sharp performance/fairness drop at low missingness rates: fusion layer lacks robustness; consider imputation or modality dropout during training.
- First 3 experiments:
  1. **Baseline modality-wise evaluation**: Train and evaluate on each modality independently to establish individual contributions (replicate Tables 1 and 2).
  2. **Incremental fusion test**: Add modalities one at a time during training, measuring F1, AUC-ROC, DP, and TPR after each addition to replicate Figure 2.
  3. **Missing modality robustness test**: Apply Bernoulli masking at inference with p ∈ {0.1, 0.2, 0.5, 0.8} and plot degradation curves for performance and fairness (replicate Figure 3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training mechanisms can ensure multimodal models maintain fairness when modalities are missing at inference time?
- Basis in paper: [explicit] The authors state: "We call upon the research community to focus on developing methods to address these challenges, ensuring that future multimodal models are not only performant but also robust and fair in real-world applications, where modality availability can fluctuate."
- Why unresolved: The paper demonstrates that missing modalities degrade fairness but does not propose mitigation strategies; Algorithm 1 only evaluates degradation via masking, not recovery.
- What evidence would resolve it: A training procedure or architecture that, when evaluated under the same masking protocol, shows stable fairness metrics (DP and TPR disparities) even as missing modality rates increase.

### Open Question 2
- Question: Why does adding modalities consistently improve performance while producing inconsistent effects on fairness across datasets and metrics?
- Basis in paper: [explicit] The authors report: "incorporating new modalities during training consistently enhances the performance of multimodal models, while fairness trends exhibit variability across different evaluation measures and datasets."
- Why unresolved: The paper characterizes the phenomenon but does not isolate whether variability stems from modality-specific spurious correlations, group representation shifts, or metric sensitivity.
- What evidence would resolve it: Controlled ablation identifying which modalities introduce disparities and analysis of feature attribution or subgroup representation changes per added modality.

### Open Question 3
- Question: What principles or frameworks should guide modality selection to balance performance gains against potential fairness costs?
- Basis in paper: [explicit] The conclusion emphasizes: "the need for careful modality selection and robust multimodal models that maintain fairness under real-world data variability."
- Why unresolved: The study evaluates incremental addition but does not provide a decision framework for practitioners to determine whether a given modality justifies its fairness trade-offs.
- What evidence would resolve it: A modality selection protocol with quantitative criteria that predicts fairness impact before full training, validated across multiple multimodal datasets.

## Limitations
- The text-based modality unification introduces potential transcription biases from VLMs used for imaging.
- The self-attention fusion layer lacks robust imputation mechanisms, potentially overestimating fragility under missing modalities.
- Lack of specified classifier architecture details (hidden sizes, attention heads, epochs) limits precise reproducibility.

## Confidence

- **High confidence**: Performance improvements with added modalities (F1 up to 0.92, AUC-ROC up to 0.91) are well-supported by the results and consistent with multimodal fusion benefits.
- **Medium confidence**: The claim that missing modalities degrade both performance and fairness is plausible but depends on the brittleness of the self-attention fusion layer without robust imputation.
- **Low confidence**: The assertion that fairness disparities fluctuate unpredictably across datasets and demographic groups requires further validation, as the underlying causes (modality-specific bias, data distribution differences) are not fully isolated.

## Next Checks

1. **Isolate modality-specific bias**: Train classifiers on individual modalities and analyze DP/TPR disparities by race and gender to identify which modalities contribute most to fairness gaps.
2. **Test imputation robustness**: Implement learned modality imputation or modality dropout during training and compare performance/fairness degradation under missing modality scenarios.
3. **Validate VLMs for imaging**: Compare fairness and performance when using raw imaging features versus text-generated reports from VLMs to quantify transcription bias impact.