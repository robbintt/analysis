---
ver: rpa2
title: Options-Aware Dense Retrieval for Multiple-Choice query Answering
arxiv_id: '2501.16111'
source_url: https://arxiv.org/abs/2501.16111
tags:
- query
- retrieval
- arxiv
- oadr
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-context multiple-choice
  question answering (MCQA) by proposing Options Aware Dense Retrieval (OADR), a method
  that fine-tunes dense retrieval using contrastive learning with query-options embeddings
  to better mimic oracle query representations. The core innovation lies in training
  the model to make options-aware query embeddings closely resemble those of the oracle
  query (query paired with correct answer), thereby improving evidence identification.
---

# Options-Aware Dense Retrieval for Multiple-Choice query Answering

## Quick Facts
- arXiv ID: 2501.16111
- Source URL: https://arxiv.org/abs/2501.16111
- Reference count: 0
- Achieves 49.2% accuracy on QuALITY dataset with DeBERTaV3-large

## Executive Summary
This paper introduces Options-Aware Dense Retrieval (OADR), a method that fine-tunes dense retrieval using contrastive learning with query-options embeddings to better mimic oracle query representations. The approach addresses the challenge of long-context multiple-choice question answering by training the model to make options-aware query embeddings closely resemble those of the oracle query (query paired with correct answer). Experiments on the QuALITY benchmark show OADR achieves state-of-the-art performance, with the DeBERTaV3-large variant reaching 49.2% accuracy on the full dataset and 42.4% on the hard subset.

## Method Summary
OADR employs a two-stage pipeline: first, a sentence transformer is fine-tuned using contrastive triplet learning where the anchor is the oracle query (query + correct answer), the positive is query + all options, and the negative is query + only incorrect options. The model is trained to minimize distance between anchor-positive pairs while maximizing anchor-negative distance. In the second stage, the fine-tuned model retrieves top-K evidence sentences (concatenated to max 300 tokens), which are then processed by standard encoder models like RoBERTa or DeBERTaV3 to select the answer.

## Key Results
- OADR achieves 49.2% accuracy on QuALITY with DeBERTaV3-large vs 33.7% for Longformer baseline
- OADR fine-tuned on RACE (OAQR) achieves 61.4% overlap with oracle-retrieved sentences vs 53.6% for vanilla query embeddings
- Transfer learning from RACE to QuALITY improves performance (59.3% vs 49.2%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive triplet training aligns options-aware query embeddings with oracle query representations
- Mechanism: The method constructs triplets where the anchor is the oracle query (Q + correct answer), the positive is query + all options, and the negative is query + only incorrect options. Triplet loss minimizes anchor-positive distance while maximizing anchor-negative distance, forcing the model to amplify signals from the correct option even when all options are present.
- Core assumption: The oracle query embedding (query + known correct answer) retrieves better evidence than query alone, and this property can be transferred to an options-aware embedding that doesn't require knowing the answer at inference time.
- Evidence anchors:
  - [abstract] "fine-tuning retrieval by leveraging query-options embeddings, which aim to mimic the embeddings of the oracle query"
  - [section 3.1] "Triplet Loss works to minimize the distance between the Anchor and Positive sentences while maximizing the distance between the Anchor and Negative sentences"
  - [corpus] Weak direct evidence; neighbor papers focus on RAG architectures rather than contrastive alignment for MCQA
- Break condition: If oracle queries do not substantially outperform vanilla queries in evidence retrieval, the alignment target is invalid.

### Mechanism 2
- Claim: Options-aware embeddings prioritize evidence relevant to the correct answer over generally relevant evidence
- Mechanism: By training the model to distinguish between "query + all options" (positive) and "query + wrong options only" (negative), the learned representation emphasizes semantic features that point toward the correct answer even when the model doesn't know which option is correct at inference.
- Core assumption: Correct-answer-relevant evidence is distinguishable from generally query-relevant evidence in the embedding space.
- Evidence anchors:
  - [abstract] "Analysis confirms that OADR effectively learns to prioritize evidence relevant to the correct answer, as evidenced by higher overlap with oracle-retrieved sentences"
  - [section 5.2, Table 2] OADR fine-tuned on RACE (OAQR) achieves 61.4% overlap with oracle-retrieved sentences vs. 53.6% for vanilla query embeddings
  - [corpus] No direct corroboration; this is a domain-specific claim
- Break condition: If all options are semantically similar (low distractor quality), the positive-negative distinction collapses and the signal degrades.

### Mechanism 3
- Claim: Two-stage retrieval-to-reader pipeline enables short-sequence models to handle long-context MCQA
- Mechanism: OADR extracts K evidence sentences (concatenated to max 300 tokens), which are then processed by standard encoder models like RoBERTa or DeBERTaV3. This bypasses the context window limitation while preserving task-relevant information.
- Core assumption: The correct answer can be determined from a subset of evidence sentences (evidence sufficiency hypothesis).
- Evidence anchors:
  - [section 3.2] "we leverage the fine-tuned OADR to extract the most relevant sentences from the context... we can employ high-performing short-sequence encoder models"
  - [section 5.1, Table 1] OADR -> DeBERTaV3-large achieves 49.2% accuracy vs. Longformer-base at 33.7%
  - [corpus] Neighbor papers on dense retrieval (arXiv:2503.17507) support retrieval-then-read architectures broadly
- Break condition: If questions require synthesis across many dispersed evidence spans, retrieval truncation loses necessary context.

## Foundational Learning

- Concept: Triplet Loss and Contrastive Learning
  - Why needed here: Core training objective for OADR; understanding how anchor-positive-negative construction shapes the embedding space is essential for debugging retrieval quality.
  - Quick check question: Given a triplet (A, P, N), what happens to the embedding space if P and N are too similar?

- Concept: Dense Retrieval and Nearest Neighbor Search
  - Why needed here: OADR operates by encoding queries and sentences into dense vectors and retrieving via L2 distance; understanding this pipeline is prerequisite for modifying the retrieval stage.
  - Quick check question: Why might L2 distance in embedding space correlate with semantic relevance?

- Concept: Transfer Learning from Resource-Rich to Resource-Scarce Domains
  - Why needed here: The paper shows RACE → QuALITY training improves results (59.3% vs. 49.2%), leveraging a larger dataset to compensate for limited supervision.
  - Quick check question: What risks arise when transferring from short-context (RACE) to long-context (QuALITY) datasets?

## Architecture Onboarding

- Component map: Triplet Generator -> Sentence Transformer Fine-Tuner -> Retrieval Module -> MCQA Reader
- Critical path: Triplet quality → embedding alignment → retrieval overlap with oracle → reader accuracy. Errors in triplet construction (e.g., mislabeled answers) propagate through the entire pipeline.
- Design tradeoffs:
  - Max passage length (300 tokens) balances evidence sufficiency vs. reader capacity; too short may miss required context
  - Using all options vs. single option in query embedding trades off information richness against embedding noise
  - Training on RACE+QuALITY improves performance but introduces distribution shift from short to long contexts
- Failure signatures:
  - Low overlap with oracle-retrieved sentences (<55%): indicates alignment training failed or triplet quality poor
  - Large gap between full and HARD subset accuracy: model over-relies on surface cues, under-reasons on difficult examples
  - Performance degradation vs. pre-trained ST baseline: overfitting to training distribution or negative sampling issues
- First 3 experiments:
  1. Baseline verification: Reproduce pre-trained Sentence Transformer → DeBERTaV3 pipeline on QuALITY dev set to confirm baseline numbers (target: ~40-44% accuracy per Table 1)
  2. Triplet sanity check: Visualize t-SNE of Q, OQ, OAQ, OAQR embeddings on a sample; verify OAQR clusters closer to OQ than OAQ (replicate Figure 2 pattern)
  3. Overlap analysis: Compute % overlap with oracle-retrieved sentences for pre-trained ST vs. OADR on dev set; target improvement of 5-10 percentage points per Table 2

## Open Questions the Paper Calls Out
None

## Limitations
- The oracle-mimicking assumption is untested - the paper doesn't verify that oracle-retrieved evidence is qualitatively superior to baseline-retrieved evidence
- The 300-token passage truncation could systematically exclude critical evidence for complex questions requiring broader context synthesis
- The contrastive learning mechanism may be learning spurious correlations rather than genuine answer-relevant patterns

## Confidence

**High Confidence**: The two-stage retrieval-to-reader architecture effectively addresses the long-context limitation, as evidenced by the substantial accuracy gap (49.2% vs 33.7%) between OADR and Longformer baselines on QuALITY.

**Medium Confidence**: The triplet loss training objective genuinely improves retrieval quality. While quantitative overlap metrics (61.4% vs 53.6%) support this, the methodology doesn't establish that the retrieved evidence is actually more useful for answering, only that it's more similar to oracle-retrieved evidence.

**Low Confidence**: The oracle-mimicking claim itself. The paper demonstrates that OADR embeddings are closer to oracle embeddings than baseline embeddings, but doesn't conclusively prove this proximity translates to better downstream reasoning.

## Next Checks
1. **Oracle Quality Verification**: Manually annotate 50 random oracle-retrieved sentences from QuALITY dev set to verify they contain substantive evidence for the correct answer. Compare against baseline-retrieved sentences to quantify the actual information gain beyond similarity metrics.

2. **Negative Sampling Robustness**: Ablate the triplet training with different negative sampling strategies (e.g., random negatives, hard negatives) on RACE dev set to determine if the 61.4% oracle overlap is robust to negative construction choices or an artifact of the specific "wrong options only" approach.

3. **Evidence Sufficiency Test**: For 100 dev questions, compare OADR performance when using top-5 sentences vs. top-10 sentences. If accuracy plateaus or drops with more evidence, this supports the evidence sufficiency hypothesis; if it improves substantially, the 300-token truncation may be limiting performance.