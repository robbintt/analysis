---
ver: rpa2
title: Tubular Riemannian Laplace Approximations for Bayesian Neural Networks
arxiv_id: '2512.24381'
source_url: https://arxiv.org/abs/2512.24381
tags:
- laplace
- neural
- gaussian
- approximation
- tubular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tubular Riemannian Laplace (TRL), a geometric
  Bayesian neural network inference method that models the posterior as a tube following
  low-loss valleys induced by functional symmetries. TRL uses a Fisher/Gauss-Newton
  metric to separate prior-dominated tangential uncertainty along the valley from
  data-dominated transverse uncertainty, addressing the geometric mismatch of standard
  Gaussian approximations.
---

# Tubular Riemannian Laplace Approximations for Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2512.24381
- Source URL: https://arxiv.org/abs/2512.24381
- Authors: Rodrigo Pereira David
- Reference count: 40
- Primary result: TRL achieves ResNet-18 CIFAR calibration matching Deep Ensembles with 1/5 the training cost

## Executive Summary
This paper introduces Tubular Riemannian Laplace (TRL), a geometric Bayesian neural network inference method that models the posterior as a tube following low-loss valleys induced by functional symmetries. TRL uses a Fisher/Gauss-Newton metric to separate prior-dominated tangential uncertainty along the valley from data-dominated transverse uncertainty, addressing the geometric mismatch of standard Gaussian approximations. The method is implemented via a discretized spine construction with implicit curvature estimation using Lanczos iteration and Hessian-vector products, enabling scalability to high-dimensional networks. TRL is evaluated on ResNet-18 (CIFAR-10 and CIFAR-100), demonstrating excellent calibration that matches or exceeds Deep Ensembles in Expected Calibration Error while requiring only 1/5 of the training cost.

## Method Summary
TRL constructs a probabilistic tube in parameter space by first identifying a low-loss valley via a predictor-corrector spine algorithm. The method uses a Fisher/Gauss-Newton metric to decompose parameter space into tangential (valley-aligned) and transverse (data-sensitive) directions. A discretized spine is built using Lanczos iteration with implicit Hessian-vector products to estimate local curvature. At inference, samples are drawn from an isotropic latent Gaussian and pushed forward through the tubular map, with BatchNorm statistics recalibrated per sample. The approach amortizes geometric computation by pre-computing the spine offline, making inference efficient while maintaining uncertainty calibration.

## Key Results
- TRL matches or exceeds Deep Ensembles' Expected Calibration Error on CIFAR-10/100 with 1/5 the training cost
- Maintains competitive NLL and Brier Score while achieving superior OOD detection (SVHN AUROC)
- Demonstrates the effectiveness of tubular geometry in capturing posterior uncertainty along functional symmetry directions
- Achieves these results with a single model architecture rather than ensemble methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing parameter space into valley-aligned (tangential) and data-sensitive (transverse) directions enables appropriate variance scaling for each.
- **Mechanism:** The Fisher/Gauss-Newton metric G₀ = F₀ + λI identifies directions where v^T F₀ v ≈ 0 (symmetry/valley directions) versus high-curvature data-constrained directions. Tangential variance ≈ 1/λ (prior-dominated), while transverse variance ≈ 1/λᵢ(F₀) (data-dominated).
- **Core assumption:** The Hessian spectrum exhibits extreme anisotropy with a clear separation between near-zero eigenvalues (bulk) and large eigenvalues (stiff directions).
- **Evidence anchors:** [abstract] Fisher/Gauss-Newton metric separation; [Section 3.1] Formal tangent/normal decomposition; [corpus] Related work validates subspace-focused Laplace.
- **Break condition:** If eigenvalues decay gradually without clear separation, the tangent/normal split becomes arbitrary and variance scaling may not correspond to actual uncertainty structure.

### Mechanism 2
- **Claim:** A tubular pushforward of an isotropic latent Gaussian captures curved posterior geometry better than a single local ellipsoid.
- **Mechanism:** Sample z = (z∥, z⊥) ~ N(0, I), then apply T(z∥, z⊥) = γ(αz∥) + N(αz∥)L⊥(αz∥)z⊥. The curve γ traces the valley; N provides transverse basis; L⊥ encodes local curvature.
- **Core assumption:** The low-loss region forms a connected one-dimensional valley (or tractable approximation thereof) rather than isolated modes.
- **Evidence anchors:** [abstract] TRL explicitly models posterior as a probabilistic tube; [Section 4.4] Full tubular map definition; [corpus] "Walking on the Fiber" explores related geometric fiber structure.
- **Break condition:** If the loss landscape has multiple disconnected basins or high-dimensional flat regions, a 1D spine cannot capture the full posterior structure.

### Mechanism 3
- **Claim:** Pre-computing a discretized spine amortizes geometric cost, making inference O(S·K·k⊥) instead of O(S·T_int·C_curv) per sample.
- **Mechanism:** Offline spine construction uses Lanczos iteration with implicit Hessian-vector products (no full Hessian materialization). At inference, sampling is a linear map θ(s) = γ_t + N_t(L⊥,t z⊥) requiring only matrix-vector products.
- **Core assumption:** The posterior mass concentrates sufficiently along the pre-computed spine that online geometric updates are unnecessary.
- **Evidence anchors:** [Section 4.5] Inference scheme summary; [Appendix D] Complexity comparison; [corpus] Standard Laplace packages lack this amortized structure.
- **Break condition:** If the true posterior geometry shifts significantly with new data or during deployment, the pre-computed spine becomes stale.

## Foundational Learning

- **Concept: Fisher Information Matrix**
  - Why needed here: Core to distinguishing symmetry directions (low Fisher curvature) from data-constrained directions (high Fisher curvature). Without this, you cannot implement the tangent/normal decomposition.
  - Quick check question: Given a direction v in parameter space, how would you determine if it's a symmetry direction using F₀?

- **Concept: Parallel Transport on Riemannian Manifolds**
  - Why needed here: The transverse basis N_t must be transported along the spine without artificial rotation. Understanding why simple translation fails and why the QR correction (Eq. 27-28) is necessary.
  - Quick check question: Why can't you just copy N_t to γ_{t+1} and use it directly?

- **Concept: Lanczos Algorithm for Implicit Eigendecomposition**
  - Why needed here: Full Hessian is O(K²) storage—impossible for 11M parameters. Lanczos extracts top-k⊥ eigenvectors using only Hessian-vector products.
  - Quick check question: How many Hessian-vector products are required to extract k⊥ eigenvectors, and what determines convergence?

## Architecture Onboarding

- **Component map:** MAP training -> Spine constructor (predictor-corrector loop) -> Lanczos engine (implicit HVP) -> Parallel transport module (QR orthogonalization) -> Sampling module (latent z -> parameter θ mapping) -> Prior handler (layer-wise precision)

- **Critical path:**
  1. Train MAP model → obtain θ_MAP
  2. Initialize spine at θ_MAP, compute initial N₀ via Lanczos
  3. Iterate: valley direction → spine step → transport basis → compute L⊥,t → store
  4. At inference: sample z → map to θ(s) → FixBN → forward pass

- **Design tradeoffs:**
  - Spine length T vs. storage: Longer spines capture more geometry but cost O(T·K) storage (T=40 vs M=5 for ensembles)
  - Transverse rank k⊥ vs. accuracy: k⊥=20-30 captures dominant curvature; diminishing returns beyond
  - Tube scale β⊥: Controls exploration; too small → underfitting, too large → destabilizes BN

- **Failure signatures:**
  - Exploding spine trajectory: Indicates insufficient prior strength on conv layers (increase γ_boost)
  - Poor calibration despite good NLL: Check if FixBN is properly applied per sample
  - Lanczos convergence failure: Increase batch buffer size m to reduce HVP variance

- **First 3 experiments:**
  1. Toy regression (sine wave): Validate tubular geometry captures valley structure against Full-Network Laplace baseline. Confirm z-variance ≈ 0.3-1.0 (not ≪ 1 or ≫ 1).
  2. Two-moons classification: Verify decision boundaries and uncertainty concentration along class boundaries. Compare entropy maps against LLA.
  3. CIFAR-10 ResNet-18 ablation: Sweep β⊥ ∈ {0.5, 1.0, 1.2} on validation set; confirm ECE < 0.02 achievable. Check that FixBN (not frozen MAP stats) is essential for sampled weights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TRL scale to billion-parameter architectures like Transformers?
- Basis in paper: [explicit] The Conclusion identifies applying curvature-based methods to billion-scale transformers as a "significant computational challenge for future research."
- Why unresolved: Implicit Hessian-vector products and Lanczos iterations, while scalable to ResN-18, may become prohibitively expensive in billion-dimensional parameter spaces.
- What evidence would resolve it: Successful application of TRL to a large language model with feasible computational overhead.

### Open Question 2
- Question: Can TRL be integrated with hybrid function-space inference?
- Basis in paper: [explicit] The Conclusion suggests leveraging TRL's valley traversal to propose particles for "hybrid function-space inference" to bridge parameter-space geometry with asymptotic safety.
- Why unresolved: TRL currently prioritizes geometric fidelity over function-space constraints, potentially lacking safety guarantees far from training data.
- What evidence would resolve it: A modified TRL framework that enforces function-space priors without losing the efficiency of the tubular parameterization.

### Open Question 3
- Question: Is the 1D spine approximation sufficient for complex invariance manifolds?
- Basis in paper: [inferred] Section 4.5 notes that while the invariance manifold may be high-dimensional, TRL explicitly models a 1D spine to maintain tractability.
- Why unresolved: Restricting the tube to a single dimension risks failing to capture the full variance of posteriors with complex, multi-dimensional symmetry groups.
- What evidence would resolve it: A comparative study analyzing performance degradation when high-dimensional invariance manifolds are forced into a 1D tube structure.

## Limitations
- The 1D spine approximation may not capture complex posterior structures with multiple basins or high-dimensional flat regions
- Pre-computed spines could become stale if posterior geometry shifts significantly with new data or during deployment
- The method relies on strong assumptions about the loss landscape structure that may not hold for all architectures

## Confidence

- Mechanism 1 (variance decomposition): High confidence - The Fisher/Gauss-Newton metric interpretation is well-established and the decomposition is mathematically rigorous
- Mechanism 2 (tubular geometry): Medium confidence - While the geometric intuition is sound, the 1D spine approximation may be overly restrictive for complex landscapes
- Mechanism 3 (computational efficiency): High confidence - The amortization argument is straightforward and complexity analysis is provided

## Next Checks

1. **Valley structure validation:** Apply TRL to a synthetic landscape with known multi-basin structure to verify that the 1D spine approximation fails gracefully or that the method can be extended to capture multi-dimensional valleys.

2. **Out-of-distribution robustness:** Test TRL on extreme OOD datasets (e.g., uniform noise, completely different modalities) to verify that the uncertainty quantification remains calibrated when the model encounters truly unfamiliar inputs.

3. **Ablation on spine length:** Systematically vary the spine length T (e.g., T=10, 20, 40, 60) and measure the trade-off between calibration quality and computational cost to determine if the current choice of T=40 is optimal or conservative.