---
ver: rpa2
title: Patient-Centred Explainability in IVF Outcome Prediction
arxiv_id: '2506.18760'
source_url: https://arxiv.org/abs/2506.18760
tags:
- user
- prediction
- tool
- data
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates user understanding and trust in an IVF
  outcome prediction tool, focusing on explainability needs. Analysis of four years
  of user feedback revealed that patients often question why certain personal factors
  (e.g., endometriosis, PCOS) are not considered in predictions, even when excluded
  due to lack of statistical significance.
---

# Patient-Centred Explainability in IVF Outcome Prediction

## Quick Facts
- arXiv ID: 2506.18760
- Source URL: https://arxiv.org/abs/2506.18760
- Authors: Adarsa Sivaprasad; Ehud Reiter; David McLernon; Nava Tintarev; Siladitya Bhattacharya; Nir Oren
- Reference count: 33
- One-line primary result: Users often question why personal factors are not considered in IVF predictions, even when excluded due to lack of statistical significance, highlighting the need for dialogue-based interfaces to address personalized explanation needs.

## Executive Summary
This paper investigates user understanding and trust in an IVF outcome prediction tool, focusing on explainability needs. Analysis of four years of user feedback revealed that patients often question why certain personal factors (e.g., endometriosis, PCOS) are not considered in predictions, even when excluded due to lack of statistical significance. A user survey (n=11) and interviews found that while model reasoning and data questions were selected from a predefined list, users struggled to articulate technical concerns. The study highlights the need for explainability beyond the model’s feature space, particularly in healthcare contexts where users develop complex mental models. It proposes dialogue-based interfaces to address personalized explanation needs.

## Method Summary
The study analyzed 62 user feedback responses collected between May 2020 and October 2024, conducted an online survey with 11 participants (after excluding 2 with IVF experience), and performed semi-structured interviews with 4 participants. User feedback was coded using an adapted XAI question bank with deductive/inductive thematic analysis. The survey assessed perceived understandability, trust, and task-based comprehension using a Rasch numeracy scale. Spearman correlation was used to analyze relationships between understandability, task scores, and trust.

## Key Results
- 22% of users reported difficulty understanding their results
- Negative correlation (-0.4) between user-perceived understandability and task-based score
- Strong positive correlation (0.75) between perceived understandability and trust
- Users frequently asked "why not considered" questions about excluded personal factors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A dialogue-based interface may bridge the gap between a user's mental model and the system's statistical model by addressing "why not considered" queries that static interfaces ignore.
- **Mechanism:** Users often possess detailed knowledge of their specific conditions (e.g., endometriosis, PCOS) and expect these to be predictive. When the model excludes these features (due to lack of statistical significance in the training data), users perceive an omission. A dialogue interface allows the system to classify these excluded features and generate a rationale (e.g., "known but not predictive in population data" vs. "unknown to the model"), aligning the user's expectations with the model's scope.
- **Core assumption:** Users will trust the system more if their specific "out-of-model" concerns are acknowledged and explained, rather than simply being ignored because they fall outside the input vector.
- **Evidence anchors:**
  - [abstract] Highlights the need for "explainability beyond the model feature space" and proposes dialogue-based interfaces.
  - [section 2.1] Introduces the code "Feature why not considered" (FEATWN), noting users ask: "I have specifically factor X which is not considered by the model. Can I trust this model prediction?"
  - [corpus] Paper 27101 ("Is Trust Correlated With Explainability in AI?") suggests the link between explainability and trust is complex and context-dependent; thus, this mechanism relies on *personalized* explanation rather than generic XAI.
- **Break condition:** If users reject the distinction between "statistically insignificant" and "medically irrelevant," the dialogue mechanism fails to resolve the trust gap.

### Mechanism 2
- **Claim:** Categorizing user intents regarding missing features into specific epistemic categories (e.g., "excluded by domain knowledge" vs. "unknown feature") allows for more precise trust calibration than standard feature-importance explanations.
- **Mechanism:** Standard XAI methods (like SHAP or LIME) explain the contribution of *present* features. This paper proposes a framework (Table 3) to handle *absent* features. By classifying a user's query about a missing factor (e.g., "Why no question on smoking?") into "Unknown feature" or "Feature has no impact," the system can provide a causal justification for the exclusion, preventing users from assuming the model is simply "broken" or "incomplete."
- **Core assumption:** Lay users can distinguish between "the model doesn't know this factor" and "this factor was tested and found to have no impact" if explained clearly.
- **Evidence anchors:**
  - [section 2.2] Discusses the need to distinguish between features excluded by developer choices vs. domain expertise vs. unknown features.
  - [table 3] Defines categories like "Feature has no impact on model" and "Feature excluded by domain knowledge."
  - [corpus] Paper 25736 ("Going beyond explainability...") supports the need for adaptation in multi-modal/complex models, though specific evidence for "missing feature categorization" in the corpus is weak.
- **Break condition:** If the explanation for exclusion relies on statistical jargon (e.g., "p-value > 0.05") rather than natural language narratives, user comprehension drops.

### Mechanism 3
- **Claim:** Personalizing the explanation modality (e.g., visual vs. textual vs. dialogue) is critical because user numeracy levels and emotional states vary significantly, directly impacting perceived understandability.
- **Mechanism:** The study found a negative correlation between actual task-based understanding and self-perceived understanding. Users with high self-perceived understandability trusted the tool more. A system that adapts its output (e.g., simplifying probability graphs for low-numeracy users) reduces the cognitive load and prevents the "illusion" of understanding that can lead to misplaced trust or anxiety.
- **Core assumption:** High-numeracy users and healthcare professionals interpret probability graphs differently than lay users, and a single "one-size-fits-all" UI causes the 22% misunderstanding rate observed in patients.
- **Evidence anchors:**
  - [section 2.2] Notes that interpreting cumulative probability is challenging and user-friendliness issues stem from UI rendering and terminology comprehension.
  - [section 3.2] Reports a negative correlation (-0.4) between user-perceived understandability and task-based score, and a strong positive correlation (0.75) between perceived understandability and trust.
  - [corpus] Paper 62257 ("Evaluating Node-tree Interfaces") and Paper 96752 ("Assessing AI Explainability") provide context on usability frameworks but do not validate the specific correlation in IVF contexts.
- **Break condition:** If the dialogue system introduces additional complexity (e.g., hallucinations or verbose answers) that lowers the task-based score, trust may degrade despite higher perceived engagement.

## Foundational Learning

- **Concept: Closed-World Assumption vs. Open-World Mental Models**
  - **Why needed here:** The OPIS model assumes all relevant factors are in its 1999-2008 dataset. Patients operate in an "open world" where they research current factors (e.g., PGT-A testing, modern lifestyle factors). Understanding this mismatch is the core problem statement of the paper.
  - **Quick check question:** Why does a user query about a "missing" feature (like endometriosis) break standard XAI explanation paradigms?

- **Concept: Cumulative Probability Distribution**
  - **Why needed here:** The paper identifies this as a major usability bottleneck. Users struggle to interpret the graph showing success rates over multiple cycles. To build the proposed dialogue system, you must understand *what* is being explained (the probability of live birth over 6 cycles).
  - **Quick check question:** Why might a user misinterpret a cumulative probability graph as the probability of a single event?

- **Concept: Retrieval Augmented Generation (RAG)**
  - **Why needed here:** The paper proposes RAG as a potential technical solution to ground the dialogue system in curated, factual data rather than allowing an LLM to hallucinate medical advice.
  - **Quick check question:** How does RAG help mitigate the risk of an LLM providing incorrect medical explanations for "unknown features"?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Standard OPIS form inputs (Age, Infertility duration, etc.)
  - Static Output Layer: Probability Graph + Textual Summary (Current OPIS)
  - Proposed Dialogue Layer:
    - Intent Classifier: Maps user text to codes (FEATWN, XAI-data, XAI-performance)
    - Feature Knowledge Base: A lookup for Table 3 categories (e.g., maps "Endometriosis" -> "Feature has no impact on model")
    - RAG Module: Retrieves medically vetted explanations for feature exclusions
    - Response Generator: Natural Language Generation (NLG) to formulate the answer

- **Critical path:**
  1. User enters data -> Receives prediction -> Feels distrust due to missing condition X
  2. User opens Dialogue Interface -> Types "I have condition X, is it included?"
  3. **Intent Classifier** identifies this as `FEATWN` (Feature why not considered)
  4. **Feature KB** checks "Condition X": Status = "Excluded by domain knowledge (not significant)"
  5. **Response Generator** formulates: "Condition X was analyzed in our dataset of 113k women but was not found to be a significant predictor of live birth, so it is not included in the calculation"

- **Design tradeoffs:**
  - *Static FAQ vs. LLM Dialogue:* Static is safer but fails personalization; LLM is flexible but risks hallucination (Section 3.3)
  - *Clinical Accuracy vs. User Satisfaction:* Telling a user their factor "has no impact" might be statistically true but emotionally invalidating. The system must balance factual accuracy with empathetic phrasing

- **Failure signatures:**
  - **"Hallucinated Impact":** The LLM suggests a feature *does* matter when the model excludes it, breaking alignment
  - **"Trust Paradox":** Users rate the tool as "user-friendly" (high trust) but fail comprehension tasks (Section 3.2), suggesting the dialogue interface must verify understanding, not just soothe the user
  - **Mobile Rendering:** The current graph fails on some mobile devices (Section 2.2); any dialogue UI must be mobile-first

- **First 3 experiments:**
  1. **Intent Classification Accuracy:** Train/test a classifier on the 31 coded user feedback responses to see if it can accurately distinguish "Feature why not considered" from "Modeling assumption" queries
  2. **A/B Testing Explanation Styles:** Compare user trust scores (using the 3-point scale mentioned in 3.1) between users who receive a standard "feature importance" explanation vs. a "feature exclusion rationale" explanation
  3. **Wizard-of-Oz Dialogue Study:** Simulate the dialogue interface with a human operator to determine if users actually ask fewer "Why not considered" questions after a proactive explanation of model scope, before building the automated RAG system

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific user expectations and concerns must be addressed to design an effective dialogue-based interface for IVF risk communication?
- **Basis in paper:** [explicit] The authors explicitly state as a research question: "What are the user’s expectations and concerns regarding a dialogue system for risk communication?"
- **Why unresolved:** While preliminary interviews identified privacy and personalization as concerns, the paper notes that "further investigation is needed" and that work on the actual dialogue-based explainer is "currently underway."
- **What evidence would resolve it:** A user study evaluating a dialogue system prototype that specifically measures user satisfaction regarding the identified concerns (e.g., handling of acronyms, personalization of responses).

### Open Question 2
- **Question:** How can an explanation system technically generate accurate responses for features excluded from the model's feature space (e.g., features excluded by domain knowledge or unknown features)?
- **Basis in paper:** [inferred] The paper proposes a taxonomy for excluded features (Table 3) and suggests Retrieval Augmented Generation (RAG), but admits that addressing features outside the model's knowledge base remains "largely unexplored" and that general-purpose LLMs are unsuitable due to hallucination risks.
- **What evidence would resolve it:** A functional software prototype demonstrating a RAG architecture that successfully maps user queries about excluded features (e.g., "Why is smoking not included?") to curated medical facts or statistical justifications.

### Open Question 3
- **Question:** Does a dialogue-based explanation interface improve user understandability and trust compared to the current static user interface?
- **Basis in paper:** [explicit] The paper validates baseline understandability for the static UI but concludes that they "aim to explore concrete paradigms in future work" regarding the proposed dialogue interface to address current shortcomings.
- **Why unresolved:** The study quantifies the limitations of the current static tool (e.g., 22% difficulty understanding results) but provides no data on whether the proposed conversational alternative improves these metrics.
- **What evidence would resolve it:** Comparative metrics from an A/B test showing statistically significant improvements in the "task-based score" and "trust score" (defined in Section 3.1) for the dialogue interface versus the static graph interface.

## Limitations
- The proposed dialogue system architecture is largely theoretical with no empirical validation of its effectiveness
- Limited sample size (n=11 survey respondents, n=4 interviewees) constrains generalizability of findings
- The connection between model exclusions (statistical insignificance) and user perceptions of medical relevance remains unexplored empirically
- The specific RAG implementation details and evaluation framework are not fully specified

## Confidence
- **High confidence**: User feedback analysis identifying "feature not considered" questions as a significant concern (based on 62 responses)
- **Medium confidence**: The negative correlation (-0.4) between perceived and actual understandability (11 participants, preliminary finding)
- **Low confidence**: The proposed dialogue-based solution effectiveness and specific implementation approach

## Next Checks
1. Conduct Wizard-of-Oz studies to test whether users' trust improves when receiving personalized explanations for excluded features versus standard static explanations
2. Perform A/B testing comparing user comprehension and trust between a standard feature-importance interface and a dialogue interface that addresses "why not considered" questions
3. Validate the feature categorization framework (Table 3) by testing whether users can correctly distinguish between "unknown feature" and "excluded by domain knowledge" explanations in clinical scenarios