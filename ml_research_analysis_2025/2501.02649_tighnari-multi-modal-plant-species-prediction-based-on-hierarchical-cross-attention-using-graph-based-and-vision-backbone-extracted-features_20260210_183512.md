---
ver: rpa2
title: 'Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention
  Using Graph-Based and Vision Backbone-Extracted Features'
arxiv_id: '2501.02649'
source_url: https://arxiv.org/abs/2501.02649
tags:
- feature
- features
- training
- species
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-modal plant species prediction using
  survey data, satellite imagery, time series data, and environmental rasters. The
  authors propose a solution pipeline involving graph-based feature construction,
  temporal feature extraction using a Swin-Transformer backbone, hierarchical cross-attention
  fusion, and threshold Top-K post-processing.
---

# Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features

## Quick Facts
- **arXiv ID:** 2501.02649
- **Source URL:** https://arxiv.org/abs/2501.02649
- **Reference count:** 20
- **Primary result:** Third place (0.36242 private leaderboard score) in GeoLifeCLEF 2024 plant species prediction competition

## Executive Summary
This paper addresses the challenge of multi-label plant species prediction using heterogeneous data sources including satellite imagery, time series data, environmental rasters, and survey occurrence data. The authors propose Tighnari, a model that constructs graph-based label priors, extracts temporal features using a Swin-Transformer backbone, and fuses modalities through a hierarchical cross-attention mechanism. The model achieves third place in the GeoLifeCLEF 2024 competition with an F1 score of 0.36242 on the private leaderboard, demonstrating the effectiveness of combining graph-based priors with vision transformer architectures for ecological prediction tasks.

## Method Summary
Tighnari processes multi-modal inputs through specialized encoders: a custom Swin-Transformer for temporal data (reshaped into 2D "time cubes"), a vision backbone (Swin-Tiny or EfficientNet-B0) for satellite imagery, and MLPs for environmental rasters. The key innovation is the hierarchical cross-attention mechanism (HCAM) that fuses dense visual features with medium-density tabular features, then injects sparse graph-based label priors. The graph features are constructed by aggregating neighbor labels within 10km radius, same year, and same region, normalized by square root of node degree. The model is trained using 10-fold cross-fusion with early stopping and post-processed using threshold Top-K selection combined with PO data correction.

## Key Results
- Achieved third place with 0.36242 private leaderboard score in GeoLifeCLEF 2024 competition
- Model significantly outperforms baselines (0.328 private score) across all test sets
- HCAM fusion reduces overfitting compared to simple concatenation, with more stable training curves
- Graph features improve validation loss but require threshold-based post-processing to maintain leaderboard performance

## Why This Works (Mechanism)

### Mechanism 1: Graph-Based Label Propagation for Sparse Priors
Aggregating labels from spatially and temporally proximate survey nodes creates a high-value prior feature (Graph Feature Vector) that outperforms raw environmental data alone. The authors construct a graph where nodes are survey IDs, with edges created only if nodes are within 10km, share the same year, and share the same region. The "Graph Feature Vector" is the weighted sum of neighbor labels, normalized by the square root of node degree. This works because species distribution exhibits spatial clustering and temporal consistency within specific ecological niches. If distributions are highly random or driven by micro-climates not captured by the filters, the GFV will introduce noise.

### Mechanism 2: 2D Transformation of Temporal Data for Vision Backbones
Treating 1D time-series data as 2D "images" (Time Cubes) allows Vision Transformers to capture multi-scale periodic patterns more effectively than 1D CNNs. Temporal data is reshaped into a 2D matrix (e.g., seasons vs. years), and a Swin-Transformer processes this grid using shifted windows to capture cross-season and cross-year dependencies. This works because important temporal dependencies can be mapped to spatial locality in 2D space. If reshaping destroys sequential causality or if time series lack periodicity, the 2D assumption fails.

### Mechanism 3: Hierarchical Cross-Attention for Density Alignment
Fusing modalities with vastly different information densities requires a hierarchical attention mechanism rather than simple concatenation to prevent overfitting and loss instability. The HCAM fuses features in two stages: tri-modal fusion where image features serve as Keys/Values for Time and Tabular queries, followed by graph injection where the tri-modal output queries compressed Graph features. This works because dense environmental features should contextualize medium-density features, and the resulting representation should query the sparse label priors. If Graph features are too sparse or noisy, the second attention layer may fail to converge.

## Foundational Learning

- **Concept: Multi-Head Attention & Positional Encoding**
  - Why needed here: The Swin-Transformer backbone relies heavily on attention mechanisms with specific positional biases for processing time cubes. You cannot debug the temporal backbone without understanding how Q, K, V matrices interact.
  - Quick check question: Can you explain why the authors used 1D positional encoding for the temporal Swin-Transformer instead of the standard 2D encoding used in image processing?

- **Concept: Graph Convolution/Aggregation (Basic)**
  - Why needed here: The authors build a custom "Graph Feature Vector" by aggregating neighbor labels. Understanding that this is a form of message passing (weighted sum of neighbors) is critical to understanding the GFV.
  - Quick check question: Why did the authors choose to normalize the aggregated neighbor labels by the square root of the node degree rather than the raw degree?

- **Concept: Multi-Label Classification (Sigmoid vs. Softmax)**
  - Why needed here: This is a multi-label task where multiple species can co-exist. Standard classification metrics do not apply; you must understand BCE Loss and F1 score.
  - Quick check question: Why is the "Top-K" post-processing strategy potentially flawed for multi-label surveys where the number of present species varies wildly?

## Architecture Onboarding

- **Component map:** Input Modality Encoders (Temporal Swin-Transformer, Visual Swin/ EfficientNet, Tabular MLP, Graph GFV Generator) -> HCAM (Hierarchical Cross-Attention) -> Head (FC + Sigmoid) -> Post-Processor (Threshold Top-K + PO Correction)

- **Critical path:** The HCAM (Section 3.5) is the integration bottleneck. If dimensions don't match between the Tri-modal output and the Graph embedding, the model fails.

- **Design tradeoffs:**
  - *Efficiency vs. Accuracy:* Using EfficientNet-B0 for image features is 2x faster than Swin-Tiny but yields slightly lower raw accuracy (Table 1).
  - *Noise vs. Prior:* Adding Graph features improves validation loss but can hurt leaderboard score if post-processing isn't tuned to handle the shifted logit distributions.

- **Failure signatures:**
  - **Unstable Loss:** If you concatenate modalities directly without HCAM, the loss curve becomes unstable due to density mismatches.
  - **Overfitting on Validation/Low Test Score:** Using Graph features without Threshold-Top-K boosts minority class logits but hurts the final F1 score because low-confidence predictions are not filtered out.

- **First 3 experiments:**
  1. **Baseline Verification:** Replicate the "Baseline" row in Table 3 using ResNet18 for temporal and simple concatenation to establish a benchmark score (~0.328 Private).
  2. **Temporal Backbone A/B Test:** Swap ResNet18 for the custom Swin-Transformer on the time-cube data. Verify that the validation loss drops below 0.00379.
  3. **Fusion Strategy Ablation:** Implement the HCAM module. Compare "Concatenation" vs. "HCAM" on the validation set to verify if overfitting is reduced.

## Open Questions the Paper Calls Out

### Open Question 1
Can Graph Neural Networks (GNNs) replace the current manual edge-weighting method to better aggregate features from adjacent nodes and correct weakly supervised labels? The current method relies on heuristic edge weights and manual normalization; a GNN approach could learn these dynamics but has not been implemented. Evidence would be a comparative ablation study showing that a GNN-based aggregation layer outperforms the current manually weighted GFV method.

### Open Question 2
Can crowdsourced PO (Presence-Only) data be effectively integrated into the PA (Presence-Absence) training set via weakly supervised learning to improve accuracy? The paper currently treats PO data as auxiliary, using it only for post-processing correction. Evidence would be a training pipeline where filtered PO samples are added to the training set, resulting in a higher F1-score compared to using PA data alone.

### Open Question 3
Can Neural Architecture Search (NAS) optimize the hyperparameters of the temporal Swin-Transformer backbone and the Hierarchical Cross-Attention Mechanism (HCAM) to reduce sensitivity and improve performance? The authors admit that HCAM and MLP are sensitive to parameter changes and lacked time for sufficient analysis. Evidence would be identification of a specific architecture configuration via NAS that yields higher scores or more stable gradient descent.

### Open Question 4
Does extracting surrounding environmental rasters as image patches provide a significant performance increase over the current single-point extraction method? This data modality was excluded due to arithmetic and time constraints. Evidence would be experimental results showing a statistically significant improvement in prediction accuracy when multi-spectral raster patches are fed into the vision backbone alongside satellite imagery.

## Limitations

- **Graph Feature Vector Construction:** The exact definition of "region" for edge filtering remains unspecified, potentially affecting GFV quality and reproducibility. Additionally, the authors claim GFV improves results but do not provide ablation results comparing models with and without this feature.

- **Temporal Swin Transformer Architecture:** The implementation details of the custom temporal Swin Transformer (particularly relative positional bias indexing for 1D sequences) are not fully specified, which may affect faithful reproduction.

- **Post-Processing Threshold Optimization:** The optimal threshold (0.23) and Top-K (44) values appear tuned to the competition validation set, raising concerns about generalization to other datasets or distributions.

## Confidence

- **High Confidence:** The core methodology of using Swin Transformers for temporal feature extraction and the hierarchical cross-attention mechanism for multi-modal fusion. The validation loss improvements are well-documented.

- **Medium Confidence:** The graph-based feature construction approach and its normalization strategy. While the method is described, its effectiveness depends heavily on the undefined "region" parameter.

- **Low Confidence:** The exact architectural configurations of the HCAM module and the specific implementation of the temporal Swin Transformer for 1D sequences.

## Next Checks

1. **Graph Feature Ablation:** Implement the model with and without the Graph Feature Vector to empirically verify the claimed improvement. This should be tested on a held-out validation set separate from the competition data.

2. **Temporal Backbone Verification:** Replace the custom temporal Swin Transformer with a standard 1D CNN (e.g., ResNet18) to confirm that the 2D transformation and Swin architecture provide measurable benefits over conventional approaches.

3. **Post-Processing Robustness:** Test the Threshold Top-K (0.23, 44) strategy on datasets with different species distributions and survey characteristics to evaluate whether the thresholds generalize beyond the GeoLifeCLEF 2024 competition.