---
ver: rpa2
title: Has My System Prompt Been Used? Large Language Model Prompt Membership Inference
arxiv_id: '2502.09974'
source_url: https://arxiv.org/abs/2502.09974
tags:
- prompt
- prompts
- system
- task
- detective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces Prompt Detective, a training-free statistical\
  \ method for prompt membership inference that determines whether a proprietary system\
  \ prompt was reused in a third-party LLM-based service. The method compares response\
  \ distributions from two models\u2014one using the proprietary prompt and another\
  \ with an unknown prompt\u2014using BERT embeddings and a permutation test on mean\
  \ vector cosine similarity."
---

# Has My System Prompt Been Used? Large Language Model Prompt Membership Inference

## Quick Facts
- arXiv ID: 2502.09974
- Source URL: https://arxiv.org/abs/2502.09974
- Reference count: 23
- This work introduces a training-free statistical method for detecting prompt reuse in LLM-based services with near-zero false positive rates.

## Executive Summary
This paper presents Prompt Detective, a novel approach for prompt membership inference that determines whether a proprietary system prompt was reused in a third-party LLM-based service. The method uses statistical analysis of response distributions, comparing embeddings from a target model with an unknown prompt to a reference model with a known proprietary prompt. Through permutation tests on BERT embeddings, the approach achieves high accuracy in detecting prompt reuse across multiple model architectures and prompt types, even in black-box settings.

## Method Summary
Prompt Detective operates by sending identical task prompts to both a target model (with unknown system prompt) and a reference model (with known proprietary prompt), collecting multiple generations for each. The responses are converted to BERT embeddings, and the cosine similarity between mean embedding vectors is calculated. A permutation test then determines whether the two groups of embeddings come from the same distribution, with a low p-value indicating the prompts are distinct. The method requires no training and works directly on model outputs.

## Key Results
- Achieves near-zero false positive rates and false negative rates close to significance level (0.05) across multiple models and datasets
- Effective in black-box settings where only model outputs are available
- Performance improves with more generations per task prompt, with 50 generations enabling near-perfect separation for highly similar prompts
- Works across diverse model architectures including Llama2, Llama3, Mistral, Mixtral, Claude Haiku, and GPT-3.5

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** System prompt modifications create statistically distinguishable response distributions.
- **Mechanism:** A language model conditions its generation trajectory on the entire context, including the system prompt. Even semantically similar system prompts lead the model to occupy different regions in the semantic embedding space when generating responses to the same task prompt. Prompt Detective leverages this by comparing the mean embedding vectors of responses from two groups.
- **Core assumption:** The semantic embedding model meaningfully captures the differences in generation trajectories induced by different system prompts.
- **Evidence anchors:** The paper demonstrates that even minor changes in system prompts manifest in distinct response distributions, enabling verification with statistical significance.

### Mechanism 2
- **Claim:** A permutation test on embeddings provides a non-parametric, statistical test for distributional equality.
- **Mechanism:** The test's null hypothesis is that the two sets of embeddings come from the same distribution. By randomly shuffling the embeddings between the two groups and re-calculating the cosine similarity of their means many times, it builds a null distribution. If the observed similarity is an extreme outlier, the null hypothesis is rejected.
- **Core assumption:** Shuffling embeddings while preserving the task-prompt structure provides a valid null distribution for the test statistic.
- **Evidence anchors:** The permutation test is described as a non-parametric approach that does not make assumptions about the underlying distribution, making it suitable for high-dimensional embedding data.

### Mechanism 3
- **Claim:** Effectiveness depends on task prompts that act as "probes" to elicit system-prompt-dependent behavior.
- **Mechanism:** The method relies on using task prompts that force the model to draw upon the specific persona, constraints, or knowledge defined in the system prompt. A diverse set of task prompts can increase the robustness of Prompt Detective.
- **Core assumption:** An adversary can craft or select task prompts that are effective probes for the target system prompt.
- **Evidence anchors:** The paper emphasizes that a task prompt is a good probe if it elicits responses directly influenced by and related to the system prompt.

## Foundational Learning

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here:** Prompt Detective is framed as a form of membership inference applied to a prompt within a context window rather than a data point in a training set. Understanding traditional MIA helps contextualize the threat model.
  - **Quick check question:** In traditional MIA, what is being inferred? (Answer: Whether a specific data sample was in the model's training set). In this paper, what is being inferred? (Answer: Whether a specific system prompt was in the model's input context).

- **Concept: Permutation Tests (Non-parametric Hypothesis Testing)**
  - **Why needed here:** This is the core statistical engine of the method. Unlike a t-test which assumes a normal distribution, a permutation test builds the null distribution empirically by shuffling the data.
  - **Quick check question:** What is the null hypothesis being tested in this paper's permutation test? (Answer: That the two sets of embeddings come from the same distribution, meaning the system prompts are the same).

- **Concept: Semantic Embeddings (e.g., BERT, Sentence-BERT)**
  - **Why needed here:** To perform a statistical test on text, the text must be converted to numbers. The paper uses BERT embeddings to map model responses into a high-dimensional vector space where semantic similarity is represented by geometric proximity.
  - **Quick check question:** What is the primary distance metric used in the paper to compare the two groups of embeddings? (Answer: Cosine similarity between the mean embedding vectors).

## Architecture Onboarding

- **Component map:** Target Query Engine -> Reference Query Engine -> Embedding Encoder -> Statistical Core -> Decision Module
- **Critical path:** The loop between the Statistical Core and the Query Engines is critical. If the responses collected are not distinct enough (due to poor task prompts or low sample count), the statistical test will fail to separate the distributions.
- **Design tradeoffs:**
  - Cost vs. Confidence: Increasing task prompts and generations increases API costs but lowers p-values for negative cases
  - Power vs. Specificity: More aggressive task probes increase detection power but require knowledge about the proprietary prompt
  - Model Choice: More powerful embedding models might capture nuances better but increase computational cost
- **Failure signatures:**
  - High False Positive Rate: Occurs when task prompts don't elicit distinct behaviors, leading to similar response distributions
  - High False Negative Rate: Occurs with too few samples, causing noisy null distribution and unreliable p-values
  - Black-Box Model Mismatch: Different architectures may produce inherently different distributions, confounding the test
- **First 3 experiments:**
  1. Baseline Validation: Replicate the "Awesome-ChatGPT-Prompts" experiment with 5 distinct roles, n=5 task prompts, k=5 generations, verifying low p-values for different prompts
  2. Probe Sensitivity Analysis: Compare p-values and FPR/FNR when using generic questions vs. domain-specific probes for the same system prompt
  3. Hard Example Test: Attempt to distinguish minimally rephrased prompts (e.g., "You are a helpful assistant" vs. "You are an assistive helper") starting with low samples and increasing to observe separation improvement

## Open Questions the Paper Calls Out
- What is the theoretically optimal configuration of task prompts and generation lengths to maximize detection power for a fixed token budget?
- Can output filtering or adversarial perturbations effectively mask system prompt usage from detection?
- What specific linguistic or structural features drive the "distinct low-dimensional role trajectories" observed in embedding space?

## Limitations
- Effectiveness critically depends on the choice of task prompts, with generic prompts potentially leading to high false positive rates
- Performance may degrade when reference and target models have significantly different architectures
- The method requires a reference model with similar behavior to the target model, which may not always be available

## Confidence
- **High Confidence:** The core statistical mechanism (permutation test on BERT embeddings) is sound and well-validated with clear evidence of low error rates
- **Medium Confidence:** Real-world adversarial effectiveness is less certain, relying on assumptions about obtaining reference models with similar behavior
- **Low Confidence:** The paper does not address potential defenses an adversary could employ to evade detection through prompt obfuscation

## Next Checks
1. **Cross-Embedding Model Validation:** Replicate main experiments using different embedding models (Sentence-BERT or CLIP) to verify robustness to semantic encoder choice
2. **Task Prompt Ablation Study:** Systematically vary task prompt specificity from generic to highly specific, measuring how p-value and accuracy change to identify minimum threshold for effective probing
3. **Architecture Mismatch Stress Test:** Compare Llama2 vs. GPT-3.5 using identical system prompts to measure baseline p-values and false positive rates when architectures differ significantly