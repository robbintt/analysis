---
ver: rpa2
title: Collaborative Bayesian Optimization via Wasserstein Barycenters
arxiv_id: '2504.10770'
source_url: https://arxiv.org/abs/2504.10770
tags:
- function
- co-kg
- data
- agents
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses collaborative Bayesian optimization (BO) with
  data privacy constraints, where multiple agents must jointly optimize an unknown
  function without sharing their raw data. The core method constructs a central Gaussian
  process (GP) surrogate model via Wasserstein barycenters of local GP models, enabling
  collaboration without exposing data.
---

# Collaborative Bayesian Optimization via Wasserstein Barycenters

## Quick Facts
- **arXiv ID:** 2504.10770
- **Source URL:** https://arxiv.org/abs/2504.10770
- **Reference count:** 40
- **One-line primary result:** Novel Co-KG acquisition function with Wasserstein barycenters achieves collaborative BO performance comparable to centralized approaches while preserving data privacy

## Executive Summary
This paper addresses collaborative Bayesian optimization with data privacy constraints, where multiple agents must jointly optimize an unknown function without sharing raw data. The method constructs a central Gaussian process surrogate model via Wasserstein barycenters of local GP models, enabling collaboration without exposing sensitive information. A novel collaborative acquisition function, Co-KG, balances exploitation and exploration by combining central and local models with a time-varying hyperparameter. Theoretical analysis proves asymptotic consistency and numerical accuracy, while empirical results demonstrate superior performance compared to other collaborative methods across synthetic functions and real-world hyperparameter tuning tasks.

## Method Summary
The method operates through a server-client architecture where agents maintain local GP models and share only discretized posterior statistics. The server computes a central GP as the Wasserstein barycenter of local GPs, then optimizes a collaborative Knowledge Gradient acquisition function that weights central and local models via an increasing schedule βt = log(2t+1). This enables collaborative exploration while preserving privacy through discretization and warm-up sampling. The framework includes formal theoretical guarantees for asymptotic consistency and a Monte Carlo approximation scheme for the acquisition function optimization.

## Key Results
- Co-KG acquisition function outperforms Co-EI and Co-ES across all tested synthetic functions and real-world hyperparameter tuning tasks
- Increasing βt schedule (βt = log(2t+1)) consistently outperforms constant or decreasing alternatives
- Co-KG achieves performance comparable to non-private centralized BO while maintaining data privacy through discretization
- The framework successfully tunes neural network hyperparameters on Breast Cancer and California Housing datasets, achieving competitive validation loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Wasserstein barycenter enables aggregation of nonparametric GP models into a central GP without accessing raw data.
- Mechanism: Each agent computes posterior mean μ̃n(x) and kernel K̃n(x,x') from local data. The server computes the barycenter as the probability measure minimizing squared 2-Wasserstein distance to all local GPs. Critically, the barycenter of GPs is itself a GP with explicit mean and kernel: μc(x) = (1/N)Σn=1N μ̃n(x), while the kernel satisfies a fixed-point equation via kernel operators.
- Core assumption: Local GP posteriors adequately capture the objective function given each agent's data; kernel functions satisfy smoothness conditions.
- Evidence anchors: [abstract] "construct a central model to approximate the objective function by leveraging the concept of Wasserstein barycenters of GPs"; [section II, Proposition 1] Formal statement that barycenter exists uniquely and remains a GP
- Break condition: If local GP posteriors are severely miscalibrated (e.g., wrong kernel hyperparameters), the barycenter inherits and may amplify errors.

### Mechanism 2
- Claim: The Co-KG acquisition function balances collaborative exploration (central model) with local exploitation by time-varying weighting.
- Mechanism: Co-KG = q-KG(central) + βt · Σn KG(localn). Early iterations use small βt, favoring the central model's aggregated knowledge. As βt → ∞, local models dominate, reducing reliance on potentially biased barycenter approximations.
- Core assumption: The central model approximation improves with more data, while local models become individually reliable over time.
- Evidence anchors: [abstract] "balances exploration and exploitation, allowing for the optimization of decision variables collaboratively in each iteration"; [section IV-B, Figures 3-4] Empirical comparison showing increasing βt outperforms constant or decreasing schedules
- Break condition: If βt grows too fast, collaboration collapses prematurely; if too slow, local models may converge to suboptimal local optima.

### Mechanism 3
- Claim: Discretization maintains privacy by preventing reconstruction of raw data from shared GP statistics.
- Mechanism: Agents share only discretized posterior means μ̃n ∈ RD and kernels K̃n ∈ RD×D over mesh grid XD. Without knowing which decision variables were sampled (especially during warm-up), the server cannot invert equation (3) to recover observations (xn, yn).
- Core assumption: Adversary lacks auxiliary information about sampling locations during warm-up; discretization resolution D is finite.
- Evidence anchors: [section III] "discretization also helps maintain data privacy... this inference cannot be implemented"; [section II, Algorithm 1] Warm-up stage where agents sample independently before sharing models
- Break condition: If server learns sampling locations or if discretization is extremely fine, reconstruction attacks become feasible.

## Foundational Learning

- Concept: **Gaussian Process Posterior Inference**
  - Why needed here: Agents must compute and transmit GP posteriors; understanding equation (2)-(3) is essential for implementation.
  - Quick check question: Given 5 observations, can you write the posterior mean and covariance formulas for a new test point?

- Concept: **Knowledge Gradient Acquisition Function**
  - Why needed here: Co-KG builds directly on KG; the one-step lookahead formulation in equation (4) is the building block.
  - Quick check question: Explain why KG is non-negative and what it measures in terms of expected improvement.

- Concept: **Wasserstein Distance and Barycenters**
  - Why needed here: The central model is defined via Wasserstein barycenter; intuition about "optimal transport averaging" is needed.
  - Quick check question: How does Wasserstein barycenter differ from pointwise averaging of distributions?

## Architecture Onboarding

- Component map:
  - Local Agents -> Central Server -> Discretization Grid
  - Each agent maintains GP models, shares discretized statistics, evaluates server-directed points, and reports recommendations

- Critical path:
  1. Warm-up: Each agent samples 5 random points, fits local GP independently
  2. Per-iteration: Transmit discretized (μ̃n, K̃n) → Compute barycenter via fixed-point solver → Maximize MC-approximated Co-KG → Agents evaluate and update
  3. Termination: Each agent reports x̂*n = argmax μ̃n(x); server selects best

- Design tradeoffs:
  - Discretization granularity: Larger D improves accuracy but complexity scales near O(D2) per iteration (Table I shows 10×10=2s, 20×20=6.7s, 30×30=16.2s)
  - Number of agents: More agents help early but may introduce noise; N=4 worked well, N=8 showed diminishing returns
  - βt schedule: Increasing schedules outperform constant/decreasing; log(2t+1) recommended

- Failure signatures:
  - Co-KG underperforms early on complex functions: barycenter approximation too crude with limited data
  - Loss plateaus prematurely: discretization too coarse
  - Privacy breach: If sampling locations leak, discretization alone is insufficient protection

- First 3 experiments:
  1. **Sanity check**: Run Co-KG on synthetic f1(x) = x1² + x2² + sin(2πx1) + cos(2πx2) with N=4 agents, 30 iterations; verify Co-KG converges faster than Co-EI/Co-ES
  2. **Hyperparameter sweep**: Compare βt ∈ {e-t/2, 1, log(2t+1)} on Rosenbrock function; confirm increasing schedule achieves lower optimal value difference after iteration 15
  3. **Privacy stress test**: Attempt to reconstruct agent data from shared discretized GP statistics assuming known kernel and sampling locations; document failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the collaborative acquisition function be modified to assign adaptive, non-uniform weights to agents to improve robustness against suboptimal local observations?
- Basis in paper: [explicit] Page 14 states, "Future work could explore assigning different weights in the collaborative acquisition function to enhance algorithm robustness."
- Why unresolved: The current Co-KG function assigns equal weights to all agents, which causes performance degradation when some agents are initialized in less promising regions or collect suboptimal data.
- What evidence would resolve it: A weighted aggregation scheme (e.g., based on local model uncertainty or quality) that demonstrably outperforms the uniform averaging method in heterogeneous agent scenarios.

### Open Question 2
- Question: Can the discretization of the feasible set be implemented as a time-varying, adaptive strategy to reduce computational complexity?
- Basis in paper: [explicit] Page 14 notes, "the discretization could be time-varying regarding different iterations and adaptive to the collaborative optimization procedure, while the detailed discussions are left for future work."
- Why unresolved: The current fixed-grid approach incurs a computational complexity of nearly O(N2), which becomes burdensome as the granularity of the discretization increases.
- What evidence would resolve it: An algorithmic implementation that dynamically refines the mesh grid based on the central model's uncertainty or acquisition function values, achieving comparable accuracy with significantly lower computational cost.

### Open Question 3
- Question: Can formal differential privacy guarantees be established for the sharing of GP model parameters?
- Basis in paper: [inferred] The paper acknowledges that "given the full knowledge of... all the decision variables... the data can be inferred" (Page 5) and relies on discretization and warm-up masking rather than formal mathematical privacy guarantees.
- Why unresolved: While discretization mitigates reconstruction risks, the framework does not provide rigorous guarantees (e.g., (ε, δ)-differential privacy) against sophisticated reconstruction attacks on the shared mean and covariance functions.
- What evidence would resolve it: A theoretical analysis proving that the mechanism of sharing GP posteriors satisfies differential privacy definitions, or an integration of noise-injection techniques that balance privacy with the proven consistency of the optimization.

## Limitations

- The discretization-based privacy mechanism is heuristic rather than formally differentially private, leaving potential reconstruction attacks unaddressed if sampling locations leak
- Empirical validation focuses on low-dimensional synthetic functions and a single real-world hyperparameter tuning task, limiting generalizability to high-dimensional or non-smooth objective functions
- Theoretical guarantees hinge on well-specified kernel hyperparameters and sufficient data coverage; finite-sample performance may degrade if local GP models are poorly calibrated

## Confidence

- Mechanism 1 (Wasserstein barycenter GP aggregation): Medium-High - well-grounded in existing theory but practical implementation details are underspecified
- Mechanism 2 (Co-KG weighting schedule): Medium - empirical evidence supports the recommended schedule but theoretical justification is limited
- Mechanism 3 (Discretization privacy): Low-Medium - heuristic protection without formal differential privacy guarantees

## Next Checks

1. Test Co-KG on a high-dimensional synthetic function (e.g., 10D) to assess scalability and performance degradation
2. Implement formal differential privacy guarantees using the "Differentially Private Wasserstein Barycenters" framework and compare utility-privacy tradeoffs
3. Vary kernel hyperparameters across agents to test robustness when local models have systematic bias