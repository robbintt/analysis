---
ver: rpa2
title: Analyticup E-commerce Product Search Competition Technical Report from Team
  Tredence_AICOE
arxiv_id: '2510.20674'
source_url: https://arxiv.org/abs/2510.20674
tags:
- data
- query
- languages
- were
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The team addressed multilingual e-commerce product search relevance
  classification across two tasks: Query-Category (QC) and Query-Item (QI). They fine-tuned
  Gemma-3 12B and Qwen-2.5 14B models using multilingual data augmentation, translating
  queries into underrepresented languages and creating negative samples to handle
  class imbalance.'
---

# Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE

## Quick Facts
- arXiv ID: 2510.20674
- Source URL: https://arxiv.org/abs/2510.20674
- Reference count: 4
- 4th place on final leaderboard with average F1-score of 0.8857 on private test set

## Executive Summary
This technical report details Team Tredence_AICOE's approach to multilingual e-commerce product search relevance classification across two tasks: Query-Category and Query-Item. The team employed a two-stage strategy combining multilingual data augmentation through translation with controlled fine-tuning of Gemma-3 12B and Qwen-2.5 14B models. Their approach addressed class imbalance through careful negative sampling and achieved strong cross-lingual generalization by translating training data into underrepresented languages. The solution secured 4th place in the competition with robust performance across multiple languages and tasks.

## Method Summary
The team fine-tuned Gemma-3 12B (4-bit) and Qwen-2.5 14B models using LoRA adapters with rank 32, learning rate 2e-4, and 5 epochs with early stopping. They performed multilingual data augmentation by translating existing queries into languages missing from the development set using Gemini-2.5 Flash, creating approximately 42,000-50,000 translated samples per language. For Query-Category, they trained on original and translated data, while for Query-Item they added easy-negative samples selected by lowest cosine similarity. The approach focused on high-quality translation rather than aggressive data rebalancing, addressing the challenge of limited training data in underrepresented languages.

## Key Results
- Achieved 0.8930 F1-score on Query-Category private test set
- Achieved 0.8779 F1-score on Query-Item private test set
- Average F1-score of 0.8857 across both tasks (4th place on final leaderboard)
- 4-bit quantization outperformed full-precision fine-tuning for both tasks
- Easy-negative sampling improved precision and stability compared to hard negatives

## Why This Works (Mechanism)

### Mechanism 1
Translation-based multilingual augmentation improves cross-lingual generalization when source data has high semantic fidelity. By translating existing queries into missing languages while preserving label-consistent structure, the model learns language-invariant relevance patterns rather than language-specific spurious correlations. Core assumption: translation quality preserves query intent and labels remain valid post-translation. Evidence anchors: translation quality mattered more than quantity, high-fidelity multilingual data yielded stronger cross-lingual generalization. Break condition: low-quality translations introducing semantic drift.

### Mechanism 2
Easy-negative sampling based on embedding similarity improves discriminative capability more reliably than hard-negative sampling. Selecting negative samples with the lowest cosine similarity to positive items creates unambiguous contrastive signals, helping the model establish clearer decision boundaries without introducing edge-case confusion. Core assumption: multilingual embeddings reliably capture semantic distance. Evidence anchors: easy negatives achieved better dev and private test performance than hard negatives, improved precision and stability. Break condition: embedding space misalignment across languages making cosine similarity unreliable.

### Mechanism 3
4-bit quantized fine-tuning with LoRA can outperform full-precision training under specific data regimes and model constraints. Quantization acts as implicit regularization, reducing overfitting to training noise—particularly beneficial when augmented data contains subtle inconsistencies or when class imbalance creates overprediction bias. Core assumption: performance gain is not from quantization alone but from interaction with specific data distribution. Evidence anchors: 4-bit configuration outperformed full-precision for both tasks. Break condition: larger models without severe parameter constraints or cleaner training data.

## Foundational Learning

- **LoRA (Low-Rank Adaptation) fine-tuning**: Competition constrained models to <15B parameters; LoRA enables efficient fine-tuning by updating low-rank decomposition matrices rather than full weights, reducing memory and compute requirements. Quick check: Can you explain why LoRA rank=32 was chosen and what tradeoff exists between rank size and adaptation capacity?

- **Embedding-based semantic similarity**: Core to negative sampling strategy—requires understanding how multilingual embeddings represent cross-lingual semantic distance and how cosine similarity thresholds define "easy" vs "hard" negatives. Quick check: Why would a similarity threshold of 0.7 create "ambiguous near-matches" rather than helpful training signal?

- **Class imbalance in binary classification**: Training data had ~69% positive (QC) and ~62% positive (QI); the team's choice to use moderate augmentation rather than aggressive rebalancing reflects understanding of how LLMs handle imbalanced distributions. Quick check: Why might "overpredicting positives" be the default failure mode, and why didn't strict label balancing help?

## Architecture Onboarding

- **Component map**: Raw multilingual queries → Language tag correction (Gemini 2.5 Flash) → Translation augmentation → Deduplication → Negative sample generation (easy/hard via embedding similarity) → Model (Gemma-3 12B backbone → 4-bit quantization → LoRA adapters → Binary classification head) → Training (zero-shot prompting framework → 5 epochs with early stopping) → Inference (Fine-tuned model → Binary relevance output)

- **Critical path**: Data quality (translation fidelity + deduplication) → Model's ability to generalize to unseen languages in dev/test → Negative sampling strategy → Precision-recall tradeoff in minority class detection → LoRA configuration → Efficient training within competition constraints

- **Design tradeoffs**: Translation source diversity vs. consistency (QC used multiple sources; QI used only English); Easy vs. hard negatives (easy negatives improved stability); 4-bit vs. full precision (4-bit provided implicit regularization); Aggressive rebalancing vs. controlled augmentation (synthetic queries lacked linguistic naturalness)

- **Failure signatures**: Overprediction of positives when training on imbalanced data without calibration; Performance degradation when hard negatives contain ambiguous near-matches; Cross-lingual transfer failure when translation quality is low; Synthetic query generation producing linguistically unnatural samples

- **First 3 experiments**: Baseline fine-tuning with original + translated data; Easy-negative augmentation for QI task; Hard-negative stress test to identify threshold effects on misclassification rates

## Open Questions the Paper Calls Out

1. **Quantization mechanism**: What mechanisms cause 4-bit quantized fine-tuning to outperform full-precision training for multilingual relevance classification? The authors report the phenomenon but offer no explanation for this counterintuitive finding.

2. **Multi-source translation**: Would multi-source translation-based augmentation improve cross-lingual generalization? The paper identifies that QI performance gains were smaller due to using only English as translation source, but doesn't validate whether diverse source languages would yield additional gains.

3. **Hard negative design**: Can hard negative sampling strategies be designed to preserve semantic coherence while avoiding ambiguous near-match misclassifications? The authors found hard negatives increased misclassifications due to ambiguous near-matches, leaving the optimal tradeoff unexplored.

## Limitations

- Translation quality impact remains unverified - specific metrics or human evaluation of translation quality are absent
- Quantization vs. full-precision findings are team-specific and lack independent verification
- Cross-lingual transfer effectiveness varies by task (QC vs. QI), but underlying factors driving this difference are not fully characterized

## Confidence

- **High confidence**: Translation-based multilingual augmentation effectiveness
- **Medium confidence**: Easy-negative sampling superiority
- **Low confidence**: 4-bit quantization outperforming full-precision

## Next Checks

1. Conduct human evaluation of translated query quality to verify semantic preservation and label consistency
2. Compare 4-bit vs. full-precision fine-tuning on the exact competition datasets to independently verify quantization findings
3. Test negative sampling sensitivity by varying similarity thresholds (0.3, 0.5, 0.7) and measuring impact on precision-recall tradeoff across languages