---
ver: rpa2
title: 'Concept Generalization in Humans and Large Language Models: Insights from
  the Number Game'
arxiv_id: '2512.20162'
source_url: https://arxiv.org/abs/2512.20162
tags:
- concepts
- number
- bayesian
- generalization
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares human and LLM generalization in the number game,
  a concept inference task, using a Bayesian model as an analytical framework. Humans
  flexibly infer rule-based and similarity-based concepts, while LLMs rely more on
  mathematical rules.
---

# Concept Generalization in Humans and Large Language Models: Insights from the Number Game

## Quick Facts
- **arXiv ID:** 2512.20162
- **Source URL:** https://arxiv.org/abs/2512.20162
- **Reference count:** 32
- **Key outcome:** Humans flexibly infer rule-based and similarity-based concepts with few-shot generalization, while LLMs rely more on mathematical rules and require more samples.

## Executive Summary
This study investigates how humans and large language models (LLMs) generalize mathematical concepts in a number game task. Humans demonstrate remarkable sample efficiency, generalizing from single examples through flexible inference of both rule-based and similarity-based concepts. In contrast, LLMs like GPT-o1-mini show strong rule bias but limited sample efficiency, requiring more examples to generalize effectively. The authors introduce a Bayesian model with hypothesis averaging and the size principle as an analytical framework that better captures human behavior than LLMs, revealing fundamental differences in how these agents infer and generalize mathematical concepts.

## Method Summary
The study compares human and LLM performance on a number game task using 255 example sets derived from 79 full concepts. Humans and GPT-o1-mini respond to whether target numbers belong to the same concept as given examples. A Bayesian model serves as an analytical framework, implementing hypothesis averaging with both rule-based and interval-based concepts, using the size principle for likelihood estimation. The model's parameters (rule bias λ and noise α) are fitted via grid search to human data. Performance is evaluated using Jensen-Shannon Divergence between response distributions, with GPT-o1-mini responses averaged over 10 trials.

## Key Results
- Humans flexibly infer both rule-based and similarity-based concepts, while LLMs show stronger rule bias
- Humans demonstrate few-shot generalization, even from single examples, while LLMs require more samples
- The Bayesian model with hypothesis averaging better captures human behavior than LLMs
- GPT-o1-mini shows diverse response patterns but fails to generalize from a single example, unlike humans and the Bayesian model

## Why This Works (Mechanism)

### Mechanism 1: Hypothesis Averaging for Flexible Generalization
- Claim: Bayesian inference with hypothesis averaging supports flexible generalization from limited examples, a capability where current LLMs show limitations.
- Mechanism: The model maintains a posterior distribution over multiple hypotheses and generalizes to new targets by averaging predictions across all hypotheses weighted by their posterior probability.
- Core assumption: Concepts exist within a structured hypothesis space containing both rule-based and similarity-based options.
- Evidence anchors:
  - [abstract] "Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize."
  - [section] (Methods, 2.2) "The model generalizes to a new target y by averaging predictions P(y|h) across hypotheses weighted by their posterior probabilities P(h|X) (hypothesis averaging)."
  - [corpus] The paper "On the Failure of Latent State Persistence in Large Language Models" is relevant as it explores LLMs' difficulty in maintaining and manipulating complex internal states, which aligns with the hypothesis that they may not maintain a distribution over multiple hypotheses.

### Mechanism 2: The Size Principle as a Driver of Sample Efficiency
- Claim: A likelihood function implementing the size principle allows for more informative inference from fewer examples.
- Mechanism: The size principle assigns higher likelihood to more specific (smaller) hypotheses that still explain the observed data, amplifying evidence from each example.
- Core assumption: Examples are sampled uniformly at random from the underlying concept.
- Evidence anchors:
  - [abstract] "...while LLMs required more samples to generalize."
  - [section] (Methods, 2.2) "The likelihood incorporates the size principle, which favors more specific concepts with a smaller cardinality... The likelihood is defined as follows: P(X|h) = (1/|h|)^n".
  - [corpus] The corpus on "LLM-BI" is relevant, suggesting that LLMs can be guided to perform Bayesian inference, but may not do so natively.

### Mechanism 3: Inductive Bias (Rule vs. Similarity) Shapes Generalization
- Claim: The relative prior weighting of rule-based vs. similarity-based concepts determines whether an agent generalizes via abstract rules or proximity.
- Mechanism: The prior P(h) assigns initial probability to each hypothesis, with higher prior on rule-based concepts biasing inference toward mathematical rules.
- Core assumption: The hypothesis space includes both discrete, rule-based concepts and continuous, interval-based concepts.
- Evidence anchors:
  - [abstract] "humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules."
  - [section] (Results, 3.4) "The best-fit value of λ was 0.9 in humans and 1 in GPT. This higher rule bias in GPT is consistent with its limited reliance on intervals".
  - [corpus] Evidence from corpus neighbors is weak for this specific mechanism.

## Foundational Learning
- **Concept: Bayesian Inference**
  - Why needed here: The paper uses a Bayesian model as its core analytical framework to explain generalization differences.
  - Quick check question: Given a hypothesis H and evidence E, how do you update the probability of H?
- **Concept: The Size Principle**
  - Why needed here: It is identified as a key component for human-like sample efficiency, which the ablation study suggests LLMs lack.
  - Quick check question: For the example set {2, 8}, why does the hypothesis "powers of 2" get a higher likelihood than "even numbers"?
- **Concept: Inductive Bias**
  - Why needed here: The core difference between humans and LLMs is framed as a difference in inductive bias (rule bias λ).
  - Quick check question: If a model always predicts "even numbers" regardless of the input, what does this reveal about its inductive bias?

## Architecture Onboarding
- **Component map:** Hypothesis Space (H) -> Prior P(h) -> Likelihood P(X|h) -> Posterior P(h|X) -> Generalization P(y|X) via hypothesis averaging
- **Critical path:**
  1. Define hypothesis space with both rule-based and interval-based concepts
  2. Implement the size principle in the likelihood function
  3. Perform Bayesian update to compute the posterior
  4. Generalize via hypothesis averaging, not MAP selection
- **Design tradeoffs:**
  - **Rule Bias (λ):** High λ favors rules (GPT-like), low λ favors similarity. Human behavior is a flexible mixture
  - **Likelihood Type:** Size principle (human-like) vs. Binary Likelihood (simpler, less sample-efficient)
  - **Generalization:** Hypothesis averaging (flexible) vs. MAP/MaxL (simpler, less adaptive)
- **Failure signatures:**
  - **Single-example anchoring:** Failing to generalize beyond the given example. Suggests lack of hypothesis averaging or strong prior on trivial hypotheses
  - **Over-reliance on rules:** Failing to show similarity gradients. Suggests λ is too high
  - **Poor sample efficiency:** Requiring many examples. Suggests likelihood is not using the size principle
- **First 3 experiments:**
  1. **One-shot Generalization Test:** Give the model a single example (e.g., {16}) and query P(y) for y=1-100. Check for both similarity gradient and rule-based peaks
  2. **Likelihood Ablation:** Replace the size principle with a binary likelihood. Measure the drop in sample efficiency, especially for single-example sets
  3. **Parameter Fitting:** Fit the model's rule bias (λ) to its own response patterns. Compare to the paper's benchmarks (Human: 0.9, GPT: 1.0) to diagnose its inductive bias

## Open Questions the Paper Calls Out
- **Open Question 1:** Can compositionally structured rules better account for LLM behavior in the number game than the primitive mathematical rules currently used in the Bayesian prior?
  - Basis in paper: [explicit] The authors state it "remains to be tested whether other rule-based concepts may better account for GPT's behavior such as compositionally structured rules used in language."
  - Why unresolved: The current Bayesian model limits the hypothesis space to primitive mathematical rules, potentially failing to capture the complexity of concepts leveraged by LLMs.
  - What evidence would resolve it: Expanding the Bayesian model's hypothesis space to include compositional rules and testing if this improves the fit between the model and LLM responses.

- **Open Question 2:** Is the lack of one-shot generalization in LLMs attributable to limited memory capacity preventing the consideration of multiple hypotheses?
  - Basis in paper: [explicit] The authors propose that "GPT may not consider multiple hypotheses during an inference step due to limited memory capacity—a possibility that warrants further investigation."
  - Why unresolved: The study identifies that LLMs use a maximum likelihood strategy (selecting one hypothesis) rather than Bayesian averaging, but the specific computational constraint causing this is hypothesized but not proven.
  - What evidence would resolve it: Experiments varying the context window or internal memory availability for LLMs to see if increased capacity enables Bayesian-like hypothesis averaging and one-shot generalization.

- **Open Question 3:** Can training on mathematical and symbolic representations enrich LLMs' inductive biases to achieve human-like sample efficiency?
  - Basis in paper: [explicit] "Future work could explore different training data that incorporate mathematical and symbolic representations... to enrich LLMs' inductive biases."
  - Why unresolved: The paper demonstrates a gap in sample efficiency (humans generalize from one example, LLMs need more) but does not test whether specific data interventions can bridge this gap.
  - What evidence would resolve it: Fine-tuning LLMs on curated mathematical/symbolic datasets and re-evaluating their performance on the number game to measure changes in sample efficiency and rule bias.

## Limitations
- The hypothesis space for the Bayesian model is partially specified through external references, creating uncertainty about exact implementation details
- The study's findings are based on a specific mathematical concept learning task and may not generalize to broader domains
- GPT-o1-mini's response parsing and temperature settings are not fully specified, potentially affecting reproducibility

## Confidence
- **High confidence:** The fundamental observation that humans and LLMs show different generalization patterns in the number game task
- **Medium confidence:** The specific mechanism attribution (size principle and hypothesis averaging) explaining human advantage
- **Low confidence:** The generalizability of these findings to broader concept learning domains beyond mathematical rules

## Next Checks
1. Implement the Bayesian model with explicit enumeration of all rule-based and interval-based hypotheses to verify the exact rule bias parameter values (λ = 0.9 for humans, λ = 1.0 for GPT)
2. Conduct controlled experiments varying example set size systematically to quantify the sample efficiency gap between humans and LLMs across multiple trials
3. Test whether alternative likelihood functions (beyond the size principle) can recover human-like generalization from single examples in the LLM setting