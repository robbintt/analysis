---
ver: rpa2
title: 'FedDAPL: Toward Client-Private Generalization in Federated Learning'
arxiv_id: '2509.23688'
source_url: https://arxiv.org/abs/2509.23688
tags:
- federated
- domain
- data
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of domain shift in federated learning
  for medical imaging, where differences in acquisition protocols across sites can
  degrade model performance on unseen data. Most harmonization techniques require
  direct data sharing, which violates privacy constraints in FL.
---

# FedDAPL: Toward Client-Private Generalization in Federated Learning

## Quick Facts
- **arXiv ID:** 2509.23688
- **Source URL:** https://arxiv.org/abs/2509.23688
- **Reference count:** 37
- **Primary result:** FedDAPL achieves MAE of 5.62 years for cross-site brain-age prediction, outperforming FedAvg (6.25) and naive FedDANN (7.28) on OpenBHB.

## Executive Summary
FedDAPL addresses the challenge of domain shift in federated learning for medical imaging, where site-specific acquisition protocols degrade model performance on unseen data. Unlike most harmonization methods that require data sharing, FedDAPL integrates Domain-Adversarial Neural Networks (DANN) with proximal regularization to stabilize adversarial training across clients without violating privacy constraints. The framework anchors the discriminator's local updates to a global reference, preventing the trivial site-classification collapse seen in naive federated DANN. Experiments on the OpenBHB dataset demonstrate that FedDAPL achieves performance approaching centralized DANN while preserving client privacy.

## Method Summary
FedDAPL is a federated domain generalization framework that combines DANN with proximal regularization to address domain shift in medical imaging across distributed sites. The key innovation is applying the proximal penalty exclusively to the discriminator component, anchoring local updates to a global reference. This prevents the trivial site-classification problem that occurs when naive federated DANN training collapses to learning site-specific features. By avoiding raw data exchange while maintaining strong cross-site generalization, FedDAPL preserves privacy while delivering performance that approaches centralized training methods.

## Key Results
- FedDAPL achieves MAE of 5.62 years on unseen sites in brain-age prediction task
- Significantly outperforms FedAvg (6.25) and naive FedDANN (7.28) baselines
- Approaches centralized DANN performance (5.12 MAE) while preserving privacy

## Why This Works (Mechanism)
FedDAPL works by stabilizing adversarial training across distributed clients through proximal regularization. The proximal penalty is applied only to the discriminator, which anchors local updates to a global reference point. This prevents the discriminator from learning trivial site-specific features that would enable perfect site classification but fail to generalize. The approach maintains the domain-invariant feature learning of DANN while adapting it to the federated setting where clients cannot share raw data.

## Foundational Learning

**Domain Adversarial Neural Networks (DANN)**: A framework for learning domain-invariant features by introducing an adversarial discriminator that tries to distinguish between source and target domains. Needed to handle domain shift across medical imaging sites. Quick check: Does the discriminator successfully confuse domain labels during training?

**Proximal Regularization**: A technique that constrains model updates by penalizing deviation from a reference point. Needed to stabilize federated training when clients have different data distributions. Quick check: Are local updates bounded appropriately without sacrificing learning capacity?

**Federated Learning**: A distributed machine learning paradigm where multiple clients train a shared model without sharing raw data. Needed to preserve privacy in medical data settings. Quick check: Is communication efficient and does the model converge across all clients?

## Architecture Onboarding

**Component Map**: Input Data -> Feature Extractor -> Predictor + Discriminator -> Global Model Update -> All Clients

**Critical Path**: Feature extractor learns domain-invariant representations while discriminator attempts to distinguish sites; proximal penalty on discriminator anchors updates to global reference.

**Design Tradeoffs**: The proximal penalty on only the discriminator (not the feature extractor) balances stability with learning capacity, but may limit adaptation to extreme domain shifts.

**Failure Signatures**: Trivial site classification (discriminator accuracy near 1.0), model divergence across clients, or performance degradation on unseen sites indicate training instability.

**First Experiments**:
1. Train FedDAPL on a subset of OpenBHB sites with 2-3 clients to verify convergence
2. Compare discriminator loss and accuracy curves between FedDAPL and naive FedDANN
3. Evaluate cross-site performance on held-out sites after each communication round

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap remains between FedDAPL and centralized DANN, suggesting room for improvement in handling unseen site variability
- Limited validation on heterogeneous client architectures and data distributions
- No formal privacy analysis (differential privacy guarantees or membership inference resistance)

## Confidence

| Claim | Confidence |
|-------|------------|
| Performance gains over baselines | High |
| Privacy preservation via no raw data sharing | High |
| Generalization to other medical imaging tasks | Medium |

## Next Checks
1. Test FedDAPL on heterogeneous client models (different backbone architectures)
2. Evaluate robustness to non-IID label distributions and varying client participation rates
3. Conduct formal privacy analysis including membership inference and gradient leakage risk