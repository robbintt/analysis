---
ver: rpa2
title: 'M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation'
arxiv_id: '2512.22628'
source_url: https://arxiv.org/abs/2512.22628
tags:
- code
- generation
- language
- arxiv
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M2G-Eval, a multi-granularity, multilingual
  framework for evaluating code generation capabilities across four structural levels
  (Class, Function, Block, Line) in 18 programming languages. The authors construct
  a large-scale instruction dataset (17K+ tasks) from 150K repositories and train
  two M2G-Eval-Coder models (Qwen3-8B-based) using supervised fine-tuning and Group
  Relative Policy Optimization.
---

# M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation

## Quick Facts
- arXiv ID: 2512.22628
- Source URL: https://arxiv.org/abs/2512.22628
- Reference count: 23
- Primary result: Multi-granularity, multilingual code generation framework covering 18 languages at Class, Function, Block, and Line levels

## Executive Summary
M2G-Eval introduces a framework for evaluating and improving code generation across multiple programming languages and structural granularities. The system constructs a large-scale instruction dataset from 150K repositories, filters tasks by edit similarity to balance difficulty, and trains models using supervised fine-tuning followed by Group Relative Policy Optimization. The resulting M2G-Eval-Coder models demonstrate strong cross-language correlations and competitive performance against state-of-the-art models while revealing a clear difficulty hierarchy across granularity levels.

## Method Summary
The framework builds a training corpus by parsing code from The-Stack-v2 using Tree-Sitter, retrieving relevant functions via BM25, and generating natural language descriptions with an LLM. Tasks are filtered by edit similarity (0.1-0.45) to ensure appropriate difficulty. Two-stage training uses LlamaFactory for SFT (5 epochs) followed by GRPO optimization (15 epochs) with length-normalized edit similarity as the reward. Evaluation uses a human-annotated test set of 1,286 instances across all granularities and languages.

## Key Results
- Clear difficulty hierarchy: Line easiest, Class hardest across all languages
- Performance gaps widen between full- and partial-granularity languages at higher granularities
- Strong cross-language correlations (0.6-0.9) indicate transferable programming concepts
- M2G-Eval-Coder-RL achieves competitive performance against state-of-the-art models

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Calibrated Task Filtering
Filtering training tasks by edit similarity (0.1-0.45) yields higher-quality instruction data by balancing learnability with challenge. Tasks with S < 0.1 are trivially easy; S > 0.45 may be infeasible or noisy. Core assumption: edit similarity on LLM-generated drafts approximates task difficulty for learners.

### Mechanism 2: GRPO Reward Signal via Length-Normalized Edit Similarity
Using length-normalized edit similarity as the GRPO reward improves generation quality more stably than raw accuracy. S is bounded in [0, 1] and length-robust, reducing variance compared to raw edit distance. Core assumption: edit similarity metric correlates with human judgments of code correctness and usefulness.

### Mechanism 3: Cross-Language Transfer via Shared Programming Concepts
Strong cross-language correlations indicate models learn transferable programming logic rather than syntax-specific patterns. Multi-granularity, multilingual training exposes models to shared algorithmic structures across languages, enabling zero-shot transfer. Core assumption: programming concepts are more universal than language-specific syntax.

## Foundational Learning

- **Edit Distance and Normalization**
  - Why needed here: The core metric S depends on understanding Levenshtein distance and why normalization is necessary for fair cross-example comparison
  - Quick check question: Given two sequences [1,2,3,4] and [1,2,4] with edit distance 1, what is the length-normalized similarity S?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Stage 2 training uses GRPO; understanding how group-based relative rewards differ from standard PPO is essential
  - Quick check question: How does comparing outputs within a group rather than to an absolute baseline affect variance?

- **Tree-Sitter Parsing**
  - Why needed here: The pipeline uses Tree-Sitter to extract multi-granularity code units
  - Quick check question: What is the difference between an AST node for a "block" vs. a "function" in a grammar?

## Architecture Onboarding

- **Component map**: The-Stack-v2 -> Tree-Sitter parsing -> BM25 retrieval -> LLM description -> Edit-similarity filter -> Dt (17K tasks) -> SFT -> GRPO -> M2G-Eval-Coder

- **Critical path**: Build contamination-controlled test set De (post-Jan 2024 repos, human annotation) -> Construct training set Dt with balanced granularity/language coverage and difficulty filtering -> Two-stage training (SFT -> GRPO) with regular validation on De

- **Design tradeoffs**: Syntactic similarity vs. execution correctness (edit similarity is cheap but may miss semantic errors); Scale vs. quality (human annotation ensures quality but limits scalability); Full- vs. partial-granularity languages (resource allocation differs)

- **Failure signatures**: Low S on Class-level tasks across all languages (context-length or long-horizon planning issue); High S on Line/Block but near-zero on Function/Class (failing to compose structures); Large performance gap between full- and partial-granularity languages at higher granularities (insufficient cross-lingual transfer or sparse training data)

- **First 3 experiments**: Ablate difficulty filter (train with S ∈ [0, 0.1] ∪ [0.45, 1] vs. [0.1, 0.45] and compare average S on De); Cross-granularity transfer (train only on Line/Block tasks, evaluate on Function/Class to test compositional generalization); Language ablation (exclude Python from training, evaluate transfer to Python via correlation with remaining languages)

## Open Questions the Paper Calls Out

- How strongly does the length-normalized edit similarity metric correlate with execution-based correctness (e.g., pass@1) across different granularities? Creating executable environments for 18 languages is complex and was outside the scope of this work.

- Can the widening performance gap between full- and partial-granularity languages be mitigated simply by scaling the training data? It is unclear if the gap stems from data scarcity or intrinsic language constraints.

- To what degree does the human annotation process introduce subjective bias into the test set's difficulty calibration? The paper does not quantify inter-annotator agreement or bias.

## Limitations

- Reliance on syntactic edit similarity rather than execution-based correctness
- Relatively small human-annotated test set (1,286 instances)
- Difficulty calibration via edit similarity lacks direct empirical validation
- Claims about transferability rest on correlation matrices rather than controlled experiments

## Confidence

- **High confidence**: Multi-granularity evaluation framework construction, corpus filtering methodology, and overall performance trends across languages
- **Medium confidence**: Claims about difficulty hierarchy, performance gaps between full- and partial-granularity languages, and cross-language correlation findings
- **Low confidence**: Claims about transferability of programming concepts, superiority of the 0.1-0.45 difficulty band, and effectiveness of length-normalized edit similarity as a reward signal

## Next Checks

1. Execute generated code for a stratified sample of test instances to measure semantic correctness beyond syntactic similarity, establishing the correlation between edit similarity scores and actual program behavior.

2. Vary the difficulty filter bounds (e.g., test [0.05, 0.4] vs. [0.15, 0.5]) and retrain models to determine if the claimed 0.1-0.45 band is optimal or if performance is robust across a wider range.

3. Perform zero-shot cross-granularity transfer tests by training on only Line and Block tasks, then evaluating on Function and Class tasks to quantify the model's ability to compose lower-level structures into higher-level abstractions.