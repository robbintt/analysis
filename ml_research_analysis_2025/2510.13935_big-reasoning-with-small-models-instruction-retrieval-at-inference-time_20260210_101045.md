---
ver: rpa2
title: 'Big Reasoning with Small Models: Instruction Retrieval at Inference Time'
arxiv_id: '2510.13935'
source_url: https://arxiv.org/abs/2510.13935
tags:
- reasoning
- instruction
- instructions
- knowledge
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling small language models
  (SLMs) to perform complex reasoning tasks that typically require large-scale models.
  The proposed solution, instruction retrieval at inference time, involves augmenting
  SLMs with structured reasoning procedures rather than relying on the models to generate
  these procedures from scratch.
---

# Big Reasoning with Small Models: Instruction Retrieval at Inference Time

## Quick Facts
- **arXiv ID:** 2510.13935
- **Source URL:** https://arxiv.org/abs/2510.13935
- **Reference count:** 40
- **Primary result:** Small language models (3B-14B) achieve 9.4% accuracy gains on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA by retrieving and following structured instructions at inference time.

## Executive Summary
This paper addresses the challenge of enabling small language models to perform complex reasoning tasks by introducing instruction retrieval at inference time. Rather than relying on the models to generate reasoning procedures from scratch, the approach augments SLMs with structured instructions retrieved from a pre-constructed corpus. The method shows consistent improvements across three challenging benchmarks without requiring additional fine-tuning, with the most notable result being that 14B-parameter models with retrieved instructions surpass GPT-4o's zero-shot performance on knowledge-intensive tasks.

## Method Summary
The method involves constructing an Instruction Corpus through clustering similar training questions and generating step-by-step instructions via a larger model (GPT-5). At inference, the SLM retrieves the most relevant instructions based on cosine similarity and follows them to solve the problem. The approach uses agglomerative clustering on embedded training examples with domain-specific thresholds, generates instructions using standardized templates, and retrieves top-5 instructions to prepend to the prompt. The framework is evaluated across multiple model families (Llama 3, Gemma 2, Qwen 3, Mistral, DeepSeek R1 Distilled) without fine-tuning.

## Key Results
- 9.4% accuracy improvement on MedQA medical board exams
- 7.9% accuracy improvement on MMLU Professional Law
- 5.1% accuracy improvement on MathQA
- Concise instructions consistently outperform longer ones
- 14B-parameter models with retrieved instructions surpass GPT-4o's zero-shot performance on knowledge-intensive tasks

## Why This Works (Mechanism)
The method works by providing small language models with structured reasoning procedures that they can follow, rather than expecting them to generate these procedures independently. This approach leverages the fact that while small models may lack the capacity to discover complex reasoning strategies on their own, they can effectively execute well-defined procedures when provided. The retrieval mechanism ensures that instructions are contextually relevant to each specific problem, and the use of concise instructions prevents overwhelming the model's context window while still providing sufficient guidance.

## Foundational Learning

**Instruction Retrieval:** The process of finding and providing relevant procedural guidance to models at inference time. *Why needed:* Small models struggle to generate complex reasoning procedures independently. *Quick check:* Verify retrieval returns contextually relevant instructions for diverse query types.

**Agglomerative Clustering:** Hierarchical clustering method that builds clusters by merging similar items. *Why needed:* Groups similar training questions to generate representative instructions for each cluster. *Quick check:* Confirm cluster coherence by examining representative samples from each cluster.

**Cosine Similarity in Embedding Space:** Measures the cosine of the angle between two vectors to determine similarity. *Why needed:* Enables efficient retrieval of most relevant instructions based on semantic similarity. *Quick check:* Validate that similar questions have high cosine similarity scores.

## Architecture Onboarding

**Component Map:** Training Data -> Embedding + Clustering -> Instruction Generation -> Instruction Corpus; Inference Query -> Embedding -> Retrieval (Top-K) -> Instruction Augmentation -> SLM

**Critical Path:** The inference-time retrieval and augmentation of instructions is the critical path, as it directly impacts the model's ability to solve problems correctly. The quality of clustering and instruction generation affects the corpus quality, but the retrieval step determines whether the right guidance is applied to each query.

**Design Tradeoffs:** The paper trades off instruction length (concise vs. verbose) and retrieval breadth (top-5 vs. fewer) against model context limitations and accuracy. The choice of agglomerative clustering with fixed thresholds balances computational efficiency against retrieval quality. Using GPT-5 for instruction generation trades off cost and availability against instruction quality.

**Failure Signatures:** Models under 3B parameters may experience accuracy degradation with instructions. Verbose instructions can reduce accuracy compared to baseline. Context overflow can occur when concatenating multiple instructions. Retrieval relevance degrades as corpus size grows without re-ranking.

**3 First Experiments:**
1. Test instruction retrieval on a single model family (e.g., Gemma-2-9B) on MedQA to verify the basic methodology works
2. Compare concise vs. verbose instruction variants on the same model to validate the superiority of concise instructions
3. Evaluate retrieval with k=3 vs k=5 to find the optimal balance between guidance and context constraints

## Open Questions the Paper Calls Out

**Knowledge vs. Structure Contribution:** Do the performance gains primarily stem from injecting targeted domain knowledge that SLMs lack, or from constraining the generation search space via explicit procedural structure? The authors observe that improvements likely result from a combination of surfacing relevant information and imposing a reasoning policy, but they do not isolate the relative contribution of each mechanism.

**Competence-Aware Retrieval:** Can models perform competence-aware retrieval by estimating their own uncertainty to invoke instructions only when they are likely to benefit from external guidance? The current framework retrieves instructions for every query, which may be redundant or even detrimental if the model already possesses the requisite capability.

**Domain-Scale Maintenance:** How should instruction corpora be represented and maintained in open-ended, domain-scale environments outside of fixed benchmarks? Current experiments rely on static, fixed benchmarks rather than continuously evolving data found in real-world usage.

**Retrieval Strategy Improvement:** Does incorporating re-ranking or adaptive selection strategies improve retrieval relevance and coverage in large instruction corpora? The study relied on simple cosine similarity for top-k retrieval, which may degrade in effectiveness as the instruction corpus grows larger and noisier.

## Limitations
- Reliance on GPT-5 for instruction generation, which is not publicly available
- Inconsistent benefits across model sizes, with some models under 3B parameters experiencing slight degradation
- Clustering thresholds appear sensitive to domain-specific characteristics and may require adjustment for new tasks
- Approach specifically designed for multi-step reasoning tasks and may not generalize well to other reasoning paradigms

## Confidence

**High Confidence:** The core methodology of instruction retrieval at inference time is clearly defined and reproducible. The experimental results showing consistent improvements on the three benchmarks are well-documented and verifiable.

**Medium Confidence:** The claim that concise instructions outperform longer ones is supported by results but may depend on the specific instruction templates used. The assertion that the approach works better with models that can follow instructions rather than those with more parameters is plausible but requires more systematic analysis across diverse model families.

**Low Confidence:** The assertion that 14B models with retrieved instructions surpass GPT-4o's zero-shot performance should be interpreted cautiously, as it depends heavily on the specific zero-shot configuration of GPT-4o and may not generalize to all knowledge-intensive tasks.

## Next Checks
1. **Instruction Generation Substitution:** Replicate the instruction corpus using GPT-4o instead of GPT-5 and measure the impact on downstream task performance to validate the substitution claim.
2. **Cross-Domain Generalization:** Apply the method to a new multi-step reasoning benchmark (e.g., GSM8K or APPS) with adjusted clustering thresholds to test generalizability beyond the three evaluated domains.
3. **Model Size Sensitivity Analysis:** Systematically test the method across the full range of model sizes (1B-14B) on all three benchmarks to better characterize the minimum effective model size and identify failure modes more precisely.