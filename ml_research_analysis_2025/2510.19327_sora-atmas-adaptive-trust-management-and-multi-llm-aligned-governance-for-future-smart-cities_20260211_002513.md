---
ver: rpa2
title: 'SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for
  Future Smart Cities'
arxiv_id: '2510.19327'
source_url: https://arxiv.org/abs/2510.19327
tags:
- governance
- trust
- sora
- agent
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SORA-ATMAS introduces a multi-agent adaptive trust management framework
  for smart cities, integrating decentralized AI agents with centralized SORA governance
  and dual-chain blockchain anchoring. It enables real-time, context-aware risk and
  trust evaluation while preserving accountability and compliance.
---

# SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities

## Quick Facts
- arXiv ID: 2510.19327
- Source URL: https://arxiv.org/abs/2510.19327
- Reference count: 40
- Key outcome: SORA-ATMAS introduces a multi-agent adaptive trust management framework for smart cities, integrating decentralized AI agents with centralized SORA governance and dual-chain blockchain anchoring.

## Executive Summary
SORA-ATMAS addresses the challenge of coordinating heterogeneous AI agents in smart cities by introducing a centralized governance layer that validates and aligns agent decisions through multi-LLM consensus. The framework employs a dual-chain blockchain architecture to separate local agent logging from global governance anchoring, enabling both high-frequency edge operations and immutable audit trails. Through error-directed feedback loops and contextual trust degradation, the system achieves 35% average MAE reduction in LLM outputs while maintaining sub-100ms governance delays under high-risk conditions.

## Method Summary
The framework ingests sensor data from weather (Open-Meteo API), traffic (YOLOv8 vehicle detection), and safety (YOLO11 fire/smoke detection) domains, processing it through three LLM instances (GPT, Grok, DeepSeek) that output risk and trust scores. SORA selects the lowest-MAE model and provides iterative feedback to non-selected models using a 0.5 adjustment factor over three iterations. Agentic decisions are logged to a local blockchain stream while validated governance decisions are anchored to a global SORA blockchain. The system employs cross-domain policies to ensure safe interoperability between agents, with real-time trust calculations that emphasize contextual trust as environmental risk increases.

## Key Results
- 35% average MAE reduction in LLM outputs through error-directed feedback convergence
- Stable monitoring under high-risk conditions (traffic plateau R≈0.85, safety τt=0.65)
- Runtime performance: 13.8–17.2 requests per second throughput, execution times under 72 ms, governance delays below 100 ms

## Why This Works (Mechanism)

### Mechanism 1: Error-Directed Feedback Convergence
SORA computes reference trust-risk values and compares them against agent LLM outputs, selecting the lowest-MAE model and sending signed feedback to others. The adjustment factor (0.5) clips errors, pulling subsequent outputs closer to the reference. This assumes LLMs can interpret numerical feedback derived from MAE calculations.

### Mechanism 2: Hierarchical Dual-Chain Anchoring
Agents asynchronously log raw observations to the "Agentic Blockchain" (edge) while SORA anchors only validated, policy-compliant decisions to the "SORA Blockchain" (global). This separates high-frequency edge traffic from lower-frequency global audit records, assuming latency remains below operational deadlines.

### Mechanism 3: Contextual Trust Degradation
Overall Trust is calculated by weighting Historical Trust and Contextual Trust inversely to Risk. As risk rises, Contextual Trust (sensor health, integrity) is weighted higher, forcing recalculation that reflects current hazards. This assumes defined thresholds accurately separate safe from unsafe autonomy.

## Foundational Learning

- **Concept: Mean Absolute Error (MAE) as a Control Signal**
  - Why needed here: MAE acts as an operational feedback signal to steer LLMs, not just an evaluation metric
  - Quick check question: If an LLM reports Risk 0.4 but SORA calculates 0.6, with adjustment factor 0.5, what feedback signal is sent?

- **Concept: Dual-Chain / Multi-Ledger Architecture**
  - Why needed here: The system relies on distinct ledgers for different trust boundaries
  - Quick check question: Which chain would you query to audit a specific sensor's raw input, and which to verify if a city-wide evacuation was authorized?

- **Concept: Hysteresis in Control Systems**
  - Why needed here: Prevents "chatter" in volatile environments using hysteresis (h=0.05)
  - Quick check question: Why is a simple threshold ("If Risk > 0.8, Alert") insufficient for real-world urban systems?

## Architecture Onboarding

- **Component map:** PC-A (Edge/Agent: Agents + Local DB + LLM APIs) -> PC-B (Governance: SORA Node + Security/Cross-Domain Engines) -> Ledger Layer (Multichain: Agentic-Chain async, SORA-Chain sync)

- **Critical path:** 1. Sensor data ingestion (PC-A) 2. LLM Inference & Risk/Trust Calculation 3. Agentic-Chain Anchoring (Async) 4. Transmission to SORA (PC-B) 5. SORA Policy Validation (S1-S6 Gates) 6. SORA-Chain Anchoring (Sync) & Feedback dispatch

- **Design tradeoffs:** Throughput vs. Verification (anchoring every request ensures auditability but limits throughput to ~17 req/s); Latency vs. Multi-Model Consensus (querying 3 LLMs adds latency but reduces variance/MAE by 35%)

- **Failure signatures:** Trust Collapse (T_Overall drops below τt while R_Env stays low); Governance Bottleneck (D exceeds 100ms during request spikes >2000 reqs); Fallback Loop (system continuously activates "Fallback Policy")

- **First 3 experiments:** 1. Single Agent Latency Profile (measure end-to-end time for one Weather agent request) 2. Feedback Loop Verification (misconfigure one LLM and verify SORA feedback correction over 3 iterations) 3. Stress Test the Gate (simulate "High Risk" scenario and verify SORA triggers "Joint Actuation")

## Open Questions the Paper Calls Out

### Open Question 1
How does SORA-ATMAS governance latency and throughput behave under realistic city-scale deployments with 50+ heterogeneous agents? Basis: future work should incorporate large-scale stress tests beyond nine-agent simulations. Why unresolved: only 3 agents were empirically tested; 6- and 9-agent projections relied on analytical M/M/c queueing models without validation. What evidence would resolve it: empirical deployment with 50+ agents measuring throughput, governance delay, and MAE convergence.

### Open Question 2
What is the resilience of SORA-ATMAS to adversarial attacks including sensor spoofing, data poisoning, and LLM prompt injection? Basis: resilience against adversarial scenarios will require embedding uncertainty quantification. Why unresolved: evaluation assumed benign conditions; no adversarial robustness experiments were conducted. What evidence would resolve it: red-teaming experiments measuring false acceptance rates under spoofed sensor data and maliciously crafted LLM outputs.

### Open Question 3
Can privacy-preserving mechanisms (differential privacy, homomorphic encryption) be integrated while maintaining sub-100ms governance delays? Basis: privacy-preserving analytics should be integrated to safeguard inter-domain data exchanges. Why unresolved: current framework transmits raw sensor data without formal privacy guarantees. What evidence would resolve it: latency and MAE measurements comparing baseline against privacy-enhanced variants.

## Limitations

- The error-directed feedback mechanism lacks validation that LLMs can meaningfully interpret and adjust to numerical policy signals
- Sensor data quality assumptions are critical but failure modes (sensor degradation, spoofing, network partitioning) are not thoroughly stress-tested
- Cross-domain policy engine's ability to handle truly emergent, novel scenarios is asserted but not demonstrated beyond predefined thresholds

## Confidence

**High Confidence**: Runtime performance metrics (throughput 13.8-17.2 req/s, execution time <72 ms, governance delay <100 ms) are directly measurable and supported by profiling data.

**Medium Confidence**: The hierarchical dual-chain architecture's ability to prevent single points of failure is conceptually sound, but practical implementation details are glossed over.

**Low Confidence**: The error-directed feedback convergence mechanism's effectiveness across diverse LLM architectures and the framework's resilience to sophisticated attacks are asserted without rigorous testing.

## Next Checks

1. **Adversarial Sensor Test**: Inject corrupted sensor data into the Weather agent pipeline and measure whether the trust degradation mechanism correctly identifies and quarantines compromised inputs.

2. **Multi-Agent Conflict Resolution**: Create scenarios where agents disagree on critical thresholds and validate whether the cross-domain policy engine produces consistent, safety-preserving decisions.

3. **Feedback Loop Robustness**: Systematically vary the adjustment factor and iteration count parameters while monitoring for oscillation, divergence, or convergence speed.