---
ver: rpa2
title: 'HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models'
arxiv_id: '2509.22300'
source_url: https://arxiv.org/abs/2509.22300
tags:
- higs
- diffusion
- sampling
- ours
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating high-quality images
  with diffusion models under limited computational budgets, particularly when using
  fewer sampling steps or lower guidance scales. The authors propose History-Guided
  Sampling (HiGS), a momentum-based method that leverages the history of past model
  predictions to steer the sampling process toward higher-quality outputs.
---

# HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models

## Quick Facts
- arXiv ID: 2509.22300
- Source URL: https://arxiv.org/abs/2509.22300
- Authors: Seyedmorteza Sadat; Farnood Salehi; Romann M. Weber
- Reference count: 40
- Primary result: HiGS achieves SOTA FID of 1.61 for unguided ImageNet generation at 256×256 resolution with only 30 sampling steps

## Executive Summary
HiGS (History-Guided Sampling) addresses the challenge of generating high-quality images with diffusion models under limited computational budgets, particularly when using fewer sampling steps or lower guidance scales. The method leverages the history of past model predictions to steer the sampling process toward higher-quality outputs without requiring additional training or fine-tuning. By computing a weighted average of past predictions and using the difference between the current prediction and this average as a guidance term, HiGS improves image quality while maintaining computational efficiency.

The approach is designed as a universal plug-and-play enhancement that integrates seamlessly with existing diffusion frameworks. Extensive experiments demonstrate that HiGS consistently improves image quality across diverse models, architectures, and sampling configurations, achieving state-of-the-art results while reducing the computational resources required for high-quality image generation.

## Method Summary
HiGS introduces a momentum-based approach that leverages the history of past model predictions during the diffusion sampling process. The core mechanism involves computing a weighted average of past predictions and using the difference between the current prediction and this average as a guidance term during sampling. This history-guided guidance helps steer the sampling process toward more stable and higher-quality outputs. The method includes refinements such as scheduling the guidance weight over time, optional orthogonal projection to prevent oversaturation, and frequency-domain filtering to reduce color artifacts. HiGS adds no additional training requirements and integrates seamlessly with existing diffusion models, making it a practical enhancement for various applications.

## Key Results
- Achieves state-of-the-art FID of 1.61 for unguided ImageNet generation at 256×256 resolution with only 30 sampling steps (compared to standard 250 steps)
- Consistently improves image quality across diverse models, architectures, and sampling configurations
- Enhances quality under lower guidance scales and fewer NFEs, demonstrating universal applicability as a plug-and-play enhancement

## Why This Works (Mechanism)

## Foundational Learning
1. **Diffusion Sampling Process**: Understanding how diffusion models gradually denoise from pure noise to generate images
   - Why needed: Core mechanism that HiGS modifies
   - Quick check: Can explain forward and reverse processes in diffusion models

2. **Guidance Scaling (Classifier-Free Guidance)**: How conditioning signals are amplified during sampling
   - Why needed: HiGS works with guidance scales, particularly lower ones
   - Quick check: Can describe how CFG affects sample diversity vs quality

3. **Momentum-Based Methods**: Using historical information to guide current predictions
   - Why needed: Core principle behind HiGS's history-guided approach
   - Quick check: Can explain basic momentum concepts in optimization

## Architecture Onboarding

**Component Map**: Diffusion model -> HiGS guidance module -> Modified sampling step

**Critical Path**: Current prediction → History average → Difference term → Modified prediction

**Design Tradeoffs**: 
- Computational overhead vs quality improvement (minimal overhead claimed)
- Complexity of refinements (orthogonal projection, frequency filtering) vs performance gains
- Universality vs domain-specific optimization

**Failure Signatures**:
- Oversaturation artifacts when history guidance is too aggressive
- Color distortions from improper frequency filtering
- Instability at extreme parameter settings (very low CFG or few steps)

**First 3 Experiments to Run**:
1. Baseline comparison: Standard diffusion sampling vs HiGS with default parameters
2. Parameter sensitivity: Test different guidance weight schedules and history window sizes
3. Ablation study: Evaluate performance with/without orthogonal projection and frequency filtering

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness at extremely low guidance scales (CFG < 1.0) or very few sampling steps (< 10) is not thoroughly explored
- Orthogonal projection and frequency-domain filtering add implementation complexity that may not always be necessary
- Computational overhead is claimed to be minimal but not explicitly quantified in wall-clock time

## Confidence
- HiGS consistently improves image quality across diverse models: High
- Achieves state-of-the-art results with reduced computational budgets: High
- Truly "universal" applicability beyond tested domains: Medium
- Minimal computational overhead claim: Medium

## Next Checks
1. Test HiGS on specialized domains (medical imaging, scientific visualization) to verify universal applicability beyond natural images and art
2. Measure and report actual wall-clock time overhead compared to baseline sampling to quantify the "no additional computation" claim
3. Evaluate stability and performance at extreme parameter settings (very low guidance scales, minimal sampling steps) to identify practical limits