---
ver: rpa2
title: Repository-level Code Search with Neural Retrieval Methods
arxiv_id: '2502.07067'
source_url: https://arxiv.org/abs/2502.07067
tags:
- code
- commit
- files
- query
- file
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-stage retrieval system for repository-level
  code search that leverages commit histories to improve bug fixing. The approach
  combines BM25-based retrieval over commit messages with neural reranking using CodeBERT
  to identify relevant files for a user query.
---

# Repository-level Code Search with Neural Retrieval Methods

## Quick Facts
- arXiv ID: 2502.07067
- Source URL: https://arxiv.org/abs/2502.07067
- Reference count: 40
- Repository-level code search system combining BM25 and CodeBERT reranking achieves up to 80% improvement over BM25 baseline

## Executive Summary
This paper introduces a multi-stage retrieval system for repository-level code search that leverages commit histories to improve bug fixing. The approach combines BM25-based retrieval over commit messages with neural reranking using CodeBERT to identify relevant files for a user query. Experiments on a dataset created from 7 popular open-source repositories show substantial improvements of up to 80% in MAP, MRR and P@1 over the BM25 baseline. The system demonstrates that commit histories provide valuable context for code search and that neural reranking significantly improves retrieval quality.

## Method Summary
The proposed system employs a multi-stage retrieval pipeline that begins with BM25 retrieval over commit messages to identify potentially relevant files, followed by a CodeBERT-based neural reranker that learns to distinguish between relevant and irrelevant files. The approach uses pseudo-labeling from commit histories to create training data, where files modified in commits with matching messages are considered positive examples. The CodeBERT reranker is trained using hard negatives from the initial retrieval results and optionally supplemented with random negatives from lower ranks. The system is evaluated on a dataset of 7 popular open-source repositories with 1,133 queries focused on bug-fixing scenarios.

## Key Results
- CodeBERT reranking achieves up to 80% improvement in MAP, MRR, and P@1 over BM25 baseline
- Performance peaks at reranking depth of 100 documents before degradation occurs
- CodeReranker outperforms CommitReranker in both oracle and non-oracle settings
- Oracle MRR of 0.61 demonstrates CodeBERT's strong potential when given relevant candidates

## Why This Works (Mechanism)
The system leverages the semantic understanding of CodeBERT to overcome the limitations of keyword-based retrieval methods like BM25. By incorporating commit histories, the approach captures the contextual relationship between bug descriptions and code changes. The multi-stage pipeline first narrows down candidates using commit messages, then applies neural reranking to distinguish relevant files based on semantic similarity. This combination addresses both the recall challenge of finding relevant files and the precision challenge of ranking them appropriately.

## Foundational Learning
- **BM25 retrieval** - Why needed: Provides strong initial candidate selection based on keyword matching; Quick check: Verify baseline retrieval performance and document coverage
- **CodeBERT model architecture** - Why needed: Enables semantic understanding of code and natural language; Quick check: Confirm pretraining objectives and vocabulary coverage
- **Hard negative mining** - Why needed: Improves model discrimination by focusing on challenging false positives; Quick check: Analyze distribution of hard negatives in training data
- **Pseudo-labeling from commit histories** - Why needed: Provides scalable training data without manual annotation; Quick check: Validate quality of commit-based labels against ground truth
- **Multi-stage retrieval pipeline** - Why needed: Balances recall and precision across different retrieval phases; Quick check: Measure intermediate recall at each stage
- **Reranking depth optimization** - Why needed: Controls computational cost while maintaining retrieval quality; Quick check: Plot performance curves against reranking depth

## Architecture Onboarding

Component map: BM25 -> CommitReranker -> CodeReranker -> Final results

Critical path: User query → BM25 retrieval over commits → File candidate selection → CodeBERT reranking → Ranked file results

Design tradeoffs:
- Commit-based retrieval vs. direct file search: Commit messages provide semantic context but may introduce noise
- Hard negatives vs. random negatives: Hard negatives improve discrimination but may bias training
- Reranking depth: Deeper reranking improves recall but increases computational cost
- Pseudo-labeling quality: Leverages existing data but may propagate commit message inaccuracies

Failure signatures:
- Low R@100 indicates poor initial candidate selection
- Performance degradation at reranking depths >100 suggests insufficient negative diversity
- Gap between oracle and non-oracle performance indicates upstream retrieval issues
- Inconsistent improvements across repositories suggests dataset or model limitations

First experiments:
1. Baseline BM25 retrieval performance on commit messages
2. Oracle CodeBERT reranking with perfect recall candidates
3. Ablation study comparing hard vs. random negative mining strategies

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does incorporating random easy negatives from lower ranks resolve the performance degradation observed at reranking depths greater than 100?
- Basis: The authors observe a performance peak at depth 100 and explicitly "suggest including random easy negatives from lower ranks instead of solely relying on hard negatives."
- Why unresolved: The current training data curation limits the model's exposure to documents beyond the top 100, causing metrics to drop as depth increases.
- Evidence: Stable or increasing MAP and MRR scores when evaluating reranking depths of 250, 500, and 1000.

### Open Question 2
- Question: How can intermediate recall at depth 100 (R@100) be improved to fully leverage the CodeReranker's potential?
- Basis: The paper states, "Developing better intermediate rerankers and increasing R@100 is the most pertinent problem."
- Why unresolved: While the CodeReranker performs well in oracle settings, its performance in the full pipeline is bottlenecked by the recall of the preceding BM25 and CommitReranker stages.
- Evidence: A CodeReranker MRR score approaching the oracle performance (0.61) in the non-oracle setting (currently 0.43).

### Open Question 3
- Question: Can better labeling heuristics for the CommitReranker overcome the noise inherent in commit messages?
- Basis: The authors identify "better labelling heuristics for CommitReranker" as a future direction, noting that current heuristics may be the reason for its underperformance.
- Why unresolved: The CommitReranker lags behind the CodeReranker, and the authors suspect the heuristic used to assign labels (file intersection) is too noisy for commit messages.
- Evidence: A CommitReranker configuration that significantly outperforms the BM25 baseline by a margin closer to the CodeReranker's improvement.

## Limitations
- Limited dataset scope with only 7 repositories tested
- Performance on repositories with inconsistent commit message practices is unknown
- Generalizability to other programming languages not evaluated
- No analysis of computational overhead introduced by the multi-stage pipeline

## Confidence
- High confidence in the effectiveness of CodeBERT reranking for the tested repositories
- Medium confidence in the generalizability of results to other codebases
- Medium confidence in the practical utility for bug fixing scenarios

## Next Checks
1. Evaluate performance across a more diverse set of 50+ repositories with varying commit practices and programming languages
2. Conduct ablation studies to measure the individual contribution of commit-based retrieval versus neural reranking
3. Test the system's performance on repositories with poor commit message quality to assess robustness