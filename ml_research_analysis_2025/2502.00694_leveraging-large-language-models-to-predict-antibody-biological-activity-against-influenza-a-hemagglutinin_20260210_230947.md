---
ver: rpa2
title: Leveraging Large Language Models to Predict Antibody Biological Activity Against
  Influenza A Hemagglutinin
arxiv_id: '2502.00694'
source_url: https://arxiv.org/abs/2502.00694
tags:
- antibody
- https
- binding
- uenza
- antibodies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops AI models to predict antibody binding and receptor
  blocking activity against influenza A hemagglutinin using sequence information.
  The researchers fine-tuned a pretrained biomedical language model (MAMMAL) on laboratory-derived
  antibody-HA binding and hemagglutination inhibition assay data.
---

# Leveraging Large Language Models to Predict Antibody Biological Activity Against Influenza A Hemagglutinin

## Quick Facts
- arXiv ID: 2502.00694
- Source URL: https://arxiv.org/abs/2502.00694
- Reference count: 0
- This study develops AI models to predict antibody binding and receptor blocking activity against influenza A hemagglutinin using sequence information

## Executive Summary
This study develops AI models to predict antibody binding and receptor blocking activity against influenza A hemagglutinin using sequence information. The researchers fine-tuned a pretrained biomedical language model (MAMMAL) on laboratory-derived antibody-HA binding and hemagglutination inhibition assay data. Their models achieved strong performance in predicting existing antibody activity against known hemagglutinins (AUROC ≥ 0.91) and new hemagglutinin sequences (AUROC = 0.9). For novel antibody activity prediction, performance was moderate (AUROC = 0.73) but declined under stringent similarity constraints (AUROC = 0.63-0.66). These results demonstrate the potential of foundation models to streamline antibody research by reducing laboratory testing requirements and improving candidate prioritization, while highlighting the need for diverse datasets to enhance generalization for novel antibody development.

## Method Summary
The study employed a sequence-only approach using the MAMMAL foundation model fine-tuned on paired antibody-hemagglutinin sequences with binary labels indicating binding (ELISA) or hemagglutination inhibition (HAI) activity. The dataset comprised 188 monoclonal antibodies and 79 hemagglutinin sequences with 4,922 binding pairs and 5,035 HAI pairs. The model processed paired sequences (HC + LC variable regions for antibodies, full HA sequences) through a transformer-based architecture with binary classification output. Training used AdamW optimization with cosine learning rate decay across 1000 iterations, evaluated via 5-fold cross-validation on four split strategies: lenient (random pairs), HA-exclusive (unseen HAs), mAb-exclusive (unseen antibodies), and mAb-cluster exclusive (highly divergent antibodies).

## Key Results
- AUROC ≥ 0.91 for predicting activity of existing antibodies against seen hemagglutinins
- AUROC = 0.9 for predicting activity against unseen hemagglutinin sequences
- AUROC = 0.73 for novel antibody activity prediction, declining to 0.63-0.66 under stringent similarity constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pretrained biomedical language model on limited task-specific data yields superior performance over random initialization.
- Mechanism: The MAMMAL foundation model encodes generalizable patterns from large-scale protein sequences (UniProt), antibody sequences (OAS), and protein-protein interactions (STRING) via self-supervised tasks. Fine-tuning transfers these learned representations to the specific antibody-HA binding and HAI classification tasks.
- Core assumption: The pretrained representations capture transferable structural and functional motifs relevant to antibody-antigen interactions.
- Evidence anchors:
  - [abstract] "Our present model is developed with the MAMMAL framework for biologics discovery to predict antibody-antigen interactions using only sequence information."
  - [section: Methods, Page 7] "We used the model ibm/biomed.omics.bl.sm.ma-ted-458m, which was trained on extensive multi-domain data, including proteins (UniProt), antibodies (OAS), and protein-protein interactions (STRING) via various self-supervised tasks."
  - [corpus] Related work on antibody language models (AbLang, AntiBERTa) supports transfer learning for antibody property prediction, though specific MAMMAL architecture validation is limited in corpus.
- Break condition: Performance degrades significantly when task-specific data distribution diverges from pretraining data (e.g., highly divergent antibody sequences not represented in OAS).

### Mechanism 2
- Claim: The model generalizes better to unseen antigens (HA sequences) than to unseen antibodies.
- Mechanism: The model learns HA sequence patterns that determine antibody binding, enabling prediction for novel HA variants. However, generalization to novel antibodies is limited by the diversity of antibody sequences in the training data.
- Core assumption: HA sequence space is more constrained and predictable than antibody sequence space for this task.
- Evidence anchors:
  - [abstract] "AUROC of 0.9 for unseen HAs... For novel antibody activity prediction, the AUROC was 0.73, which further declined to 0.63-0.66 under stringent constraints."
  - [section: Results, Page 11] "These findings suggest that the model generalizes more effectively to unseen HA sequences than to unseen mAb sequences."
  - [corpus] Related papers on antibody-antigen affinity prediction (AbRank, Llama-Affinity) note similar generalization challenges with novel antibodies.
- Break condition: Novel antibody sequences with low similarity (<50% identity) to training antibodies may fall outside the model's effective prediction range.

### Mechanism 3
- Claim: Binary classification of antibody-HA pairs can predict both binding activity and functional HAI activity from sequence alone.
- Mechanism: The model takes paired antibody (HC + LC variable regions) and antigen (HA) amino acid sequences as input, processing them through transformer attention mechanisms to learn interaction patterns that correlate with experimental binding and HAI outcomes.
- Core assumption: Sequence information encodes sufficient signal for predicting biological activity without explicit structural data.
- Evidence anchors:
  - [abstract] "Our models achieved an AUROC ≥ 0.91 for predicting the activity of existing antibodies against seen HAs."
  - [section: Methods, Page 6] "We classified HAI values below 10 μg/mL as positive outcomes... We considered AUC-ELISA values greater than 1 as positive binding."
  - [corpus] HelixDesign-Antibody and dyAb papers demonstrate structure-informed approaches as alternatives, suggesting sequence-only methods may miss conformational information.
- Break condition: Predictions may fail for binding modes dependent on conformational epitopes not inferable from primary sequence.

## Foundational Learning

- Concept: **Transfer Learning in Biomedical Language Models**
  - Why needed here: The entire approach depends on leveraging pretrained representations from MAMMAL; without understanding this, you cannot diagnose why fine-tuning works or fails.
  - Quick check question: Can you explain why a pretrained model on general proteins might help predict specific antibody-HA interactions?

- Concept: **Antibody-Antigen Interaction Assays (ELISA and HAI)**
  - Why needed here: The model predicts outcomes of specific laboratory assays; understanding what these assays measure is essential for interpreting model outputs and potential failure modes.
  - Quick check question: What is the difference between binding activity (ELISA) and functional neutralization (HAI), and why might a model predict one better than the other?

- Concept: **Data Splitting Strategies for Real-World Evaluation**
  - Why needed here: The paper uses four different split types (lenient, HA-exclusive, mAb-exclusive, mAb-cluster exclusive) to simulate deployment scenarios; understanding these is critical for proper model evaluation.
  - Quick check question: If you want to predict whether existing antibodies will work against a newly emerged influenza strain, which split strategy should guide your evaluation?

## Architecture Onboarding

- Component map:
  Input Layer (paired antibody-HA sequences, max 900 tokens) -> MAMMAL Backbone (transformer-based biomedical foundation model) -> Binary Classification Head (AbAg Bind task)

- Critical path:
  1. Prepare paired antibody-HA sequence data with binary labels (binding: AUC-ELISA > 1; HAI: < 10 μg/mL)
  2. Load MAMMAL pretrained weights from Hugging Face
  3. Fine-tune with specified hyperparameters on training folds
  4. Evaluate on held-out folds using AUROC and AUPRC metrics

- Design tradeoffs:
  - Sequence-only vs. structure-informed: Faster inference but may miss conformational dependencies
  - MAMMAL-finetuned vs. random initialization: Higher performance but dependent on pretraining data quality
  - Dataset size (188 mAbs, 79 HAs): Enables focused evaluation but limits generalization to novel antibodies

- Failure signatures:
  - AUROC drops from ~0.9 to ~0.63-0.66 on mAb-cluster exclusive splits → model fails on highly divergent antibody sequences
  - Large standard deviations across folds (e.g., ±0.12 for mAb-cluster exclusive HAI) → unstable predictions on novel data
  - Performance gaps between human and mouse antibodies → host-specific biases in training data

- First 3 experiments:
  1. **Baseline validation**: Replicate the lenient split results (AUROC ≥ 0.91) to confirm MAMMAL fine-tuning pipeline is working correctly.
  2. **Generalization stress test**: Run HA-exclusive and mAb-exclusive splits to understand where the model fails; expect AUROC ~0.9 for HA-exclusive, ~0.73 for mAb-exclusive.
  3. **Ablation study**: Compare MAMMAL-finetuned vs. random-initialization models to quantify the contribution of pretrained representations; expect ~0.3 AUROC difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can performance for predicting novel antibody activity be improved through dataset expansion with automated curation from public repositories, and what harmonization strategies effectively address cross-study methodological inconsistencies?
- Basis in paper: [explicit] Authors state: "A key factor to improving model performance regards the expansion of training datasets through automated collection and curation of influenza neutralization assays from public repositories" and note "challenges tied to lack of harmonization in methodologies used to determine the biological activities of mAbs."
- Why unresolved: Current novel antibody prediction performance is moderate (AUROC = 0.73) and declines to 0.63-0.66 under stringent similarity constraints, with only 188 mAbs in the training set.
- What evidence would resolve it: Systematic aggregation of diverse antibody-HA datasets from SAbDab, PLAbDab, and IEDB with standardized assay protocols, demonstrating improved AUROC for novel antibody prediction.

### Open Question 2
- Question: How does model performance translate to computationally designed de novo antibodies not derived from natural immune responses?
- Basis in paper: [inferred] The "novel" antibodies in mAb-exclusive splits are still naturally derived sequences from laboratory sources. The conclusion mentions integration with "AI-based systems" for antibody design, but designed sequences may differ fundamentally from natural antibodies.
- Why unresolved: No evaluation was conducted on truly synthetic or computationally designed antibody sequences, which may have different sequence characteristics than naturally occurring antibodies.
- What evidence would resolve it: Prospective evaluation of model predictions on de novo designed antibodies subsequently validated through laboratory binding and HAI assays.

### Open Question 3
- Question: How sensitive are model predictions to the binary classification thresholds used for defining positive binding and HAI outcomes?
- Basis in paper: [inferred] The study applies fixed thresholds (AUC-ELISA > 1 for binding; HAI < 10 μg/mL for receptor blocking) without systematic analysis of threshold sensitivity or exploration of continuous affinity prediction.
- Why unresolved: Binary thresholds may oversimplify the biological activity spectrum and mask meaningful differences in binding strength or neutralization potency.
- What evidence would resolve it: Threshold sensitivity analysis showing prediction stability across alternative cutoff values, or extension to regression-based affinity prediction models.

### Open Question 4
- Question: Does incorporating structural information (e.g., AlphaFold-Multimer predicted complexes) improve prediction accuracy for novel antibody-antigen pairs beyond sequence-only approaches?
- Basis in paper: [explicit] Authors note AlphaFold "encounter[s] significant challenges with predicting antibody-antigen complex structures" but acknowledge these methods have "transformed efforts toward protein structure prediction." The current approach uses only sequence information.
- Why unresolved: Structure-based methods are computationally intensive and have known limitations for antibody-antigen complexes, but whether hybrid sequence-structure approaches could improve novel antibody prediction remains unexplored.
- What evidence would resolve it: Comparative evaluation of MAMMAL with structure-informed models on held-out novel antibody-HA pairs.

## Limitations
- Data scarcity and representativeness: Small dataset (188 mAbs, 79 HAs) may not capture full diversity of antibody-antigen interactions
- Foundation model dependency: Results contingent on MAMMAL pretraining quality and coverage
- Sequence-only approach limitations: May miss critical conformational and structural information for antibody-antigen interactions

## Confidence
- High confidence: Predictions for existing antibodies against known hemagglutinins (AUROC ≥ 0.91)
- Medium confidence: Generalization to unseen hemagglutinins (AUROC = 0.9)
- Low confidence: Novel antibody activity prediction (AUROC = 0.73, declining to 0.63-0.66)

## Next Checks
1. **Dataset transparency audit**: Request or reconstruct the exact antibody-HA dataset used in training to verify sequence distributions and potential overlap with MAMMAL pretraining data.
2. **Cross-species generalization test**: Evaluate model performance separately for human vs. mouse antibodies to quantify host-specific biases.
3. **Structure-informed comparison**: Run parallel experiments using the same dataset with structure-aware models to quantify the performance gap between sequence-only and structure-informed approaches.