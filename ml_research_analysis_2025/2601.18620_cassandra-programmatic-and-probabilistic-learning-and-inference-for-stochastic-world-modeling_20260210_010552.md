---
ver: rpa2
title: 'CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic
  World Modeling'
arxiv_id: '2601.18620'
source_url: https://arxiv.org/abs/2601.18620
tags:
- function
- action
- code
- state
- park
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CASSANDRA, a neurosymbolic world modeling
  approach that integrates LLM-generated code for deterministic dynamics with a probabilistic
  graphical model for stochastic variables. The key idea is to leverage an LLM as
  a knowledge prior to construct lightweight transition models for planning in complex
  environments.
---

# CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling

## Quick Facts
- arXiv ID: 2601.18620
- Source URL: https://arxiv.org/abs/2601.18620
- Reference count: 40
- One-line primary result: Neurosymbolic world model combining LLM-synthesized code for deterministic dynamics with probabilistic graphical models for stochastic variables, achieving 95% survival rate in complex theme park planning versus 8-29% for baselines

## Executive Summary
CASSANDRA is a neurosymbolic world modeling approach that integrates LLM-generated code for deterministic dynamics with a probabilistic graphical model for stochastic variables. The key idea is to leverage an LLM as a knowledge prior to construct lightweight transition models for planning in complex environments. CASSANDRA uses LLM-synthesized code optimized by evolutionary algorithms for deterministic features, and LLM-guided structure learning of a probabilistic graphical model for capturing causal relationships among stochastic variables. Evaluated in a coffee shop simulator and a complex theme park business simulator, CASSANDRA significantly improves transition prediction and planning over baselines, demonstrating its effectiveness in handling mixed deterministic-stochastic dynamics from limited data.

## Method Summary
CASSANDRA factors the state space into deterministic (s) and stochastic (x) components, with deterministic variables handled by LLM-generated Python functions and stochastic variables modeled using Bayesian Networks. The deterministic code is initialized via LLM and refined through evolutionary algorithms with validation gates to prevent overfitting. The stochastic component uses LLM-guided causal structure search via simulated annealing, where the LLM provides priors on edge plausibility. Conditional distributions are learned using quantile regression. The complete model is used with MCTS for planning. The approach is evaluated on CoffeeShopSimulator (90 training trajectories, 8 variables) and Mini Amusement Parks (84 training trajectories, 6 selected stochastic variables), showing significant improvements in both transition prediction accuracy and long-horizon planning performance.

## Key Results
- Achieved 95% survival rate over 50 days in Mini Amusement Park simulator versus 8-29% for baselines
- Action validity F1 score improved from 0.872 to 0.948 after code refinement
- Successfully handled mixed deterministic-stochastic dynamics with limited training data (90 trajectories for CoffeeShop, 84 for MAPs)

## Why This Works (Mechanism)

### Mechanism 1: Factorized State-Space Modeling
- Decomposing the state space into deterministic and stochastic components allows for specialized, sample-efficient learning algorithms that outperform monolithic models.
- The framework models the transition probability by separating deterministic state variables (handled by code execution) from stochastic variables (handled by a probabilistic graphical model).
- Core assumption: The environment can be accurately described by a hybrid system where a subset of variables changes according to fixed, logic-driven rules while others depend on probabilistic causal relationships.

### Mechanism 2: LLM-Guided Causal Structure Search
- Large Language Models (LLMs) provide effective priors for causal graph structure, significantly reducing the search space for learning probabilistic dependencies.
- Instead of learning the Bayesian Network structure from data alone, the system uses an LLM to assess the semantic plausibility of causal edges.
- Core assumption: The LLM has sufficient embedded world knowledge to distinguish plausible causal relationships from implausible ones in the specific domain.

### Mechanism 3: Evolutionary Code Refinement with Validation Gates
- Targeted, evolutionary refinement of code functions improves the accuracy of deterministic transition predictions better than monolithic code regeneration.
- The system identifies specific error types in the current code, proposes refinements, and validates proposed changes against a hold-out set to ensure generalization before updating the model.
- Core assumption: Errors in the deterministic code are local and can be fixed by modifying individual functions without requiring a full rewrite.

## Foundational Learning

- **Probabilistic Graphical Models (Bayesian Networks)**
  - Why needed here: Required to model the conditional dependencies among stochastic variables which cannot be captured by deterministic code alone.
  - Quick check question: Can you explain how conditional independence in a DAG reduces the number of parameters needed to model a joint distribution?

- **Simulated Annealing**
  - Why needed here: This is the search algorithm used to navigate the super-exponential space of possible graph structures.
  - Quick check question: How does the "temperature" parameter in Simulated Annealing prevent the algorithm from getting stuck in local optima during graph search?

- **Evolutionary Algorithms (Refinement)**
  - Why needed here: Used to iteratively improve the Python code representing the deterministic dynamics.
  - Quick check question: In the context of this paper, what specific metric determines the "fitness" of a code refinement proposal?

## Architecture Onboarding

- Component map: Natural Language Description (L) -> LLM -> Code Initialization -> Error Heuristic -> Refinement LLM -> Validated Python Functions (f_φ). Simultaneously: L -> LLM -> Topological Sort -> DAG Sampling -> Simulated Annealing (with LLM Prior & Data Fit) -> Neural Net Parameter Fitting (Quantile Regression). Both streams feed into MCTS planner.

- Critical path: Code Initialization (generating baseline Python functions), DAG Search (finding correct causal structure), Validation Gate (ensuring code patches improve generalization)

- Design tradeoffs: LLM Prior Strength (λ₂) - high reliance speeds up search but risks hallucinated causalities; Sparsity (λ₁) - encouraging simpler graphs prevents overfitting but might miss weak causal signals; Code vs. PGM - moving variables between deterministic and stochastic affects speed/precision

- Failure signatures: Spurious Edges (DAG search creates false causal links), Validation Drift (code refinement accepts patches that lower aggregate performance), LLM Latency (excessive calls during DAG sampling or code refinement)

- First 3 experiments: 1) Ablation on Structure Learning - run CASSANDRA with independent variables vs. full DAG search; 2) Code Refinement Validity - compare action validity prediction accuracy before/after refinement; 3) Planning Survival Rate - execute full MCTS planner in MAPs for 50 days to benchmark against baselines

## Open Questions the Paper Calls Out
- Can the deterministic–stochastic decomposition be automatically inferred from environment descriptions and observed data, rather than requiring manual specification?
- How does CASSANDRA perform in domains beyond business simulators, such as robotics, scientific discovery, or physical systems?
- Can the code refinement process avoid local optima when trajectory data contains contradictory or noisy transitions?
- How does the approach scale to environments with hundreds of stochastic variables?

## Limitations
- Relies heavily on LLM quality for both code generation and causal structure priors, with limited discussion of failure modes when LLM provides incorrect priors
- Evaluation focuses on planning performance rather than accuracy of learned world models themselves
- Computational costs are significant - MCTS planning takes ~1 hour per action, limiting practical applicability

## Confidence
- **High Confidence**: Planning survival rates in MAPs environment (95% vs. 8-29% baselines), the basic factorization approach of separating deterministic and stochastic variables
- **Medium Confidence**: The effectiveness of LLM-guided DAG structure learning, the evolutionary code refinement mechanism
- **Low Confidence**: Claims about sample efficiency and generalization to truly unseen environments, the robustness of the approach with different LLM models

## Next Checks
1. **LLM Prior Sensitivity Analysis**: Systematically vary the LLM prior strength (λ₂) and quality to quantify how sensitive the DAG structure learning is to prior quality, testing with both correct and intentionally misleading priors.

2. **Cross-Domain Transfer Evaluation**: Evaluate CASSANDRA on environments with different causal structures (e.g., manufacturing simulation, traffic management) to test whether the approach generalizes beyond coffee shop and theme park domains.

3. **Model-Based vs. Model-Free Planning Comparison**: Compare planning performance using the learned CASSANDRA model against direct model-free RL approaches on the same MAPs environment to determine whether the symbolic approach is actually learning useful models.