---
ver: rpa2
title: 'Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular
  Intervals with Vision Transformers'
arxiv_id: '2501.18237'
source_url: https://arxiv.org/abs/2501.18237
tags:
- data
- modalities
- measurements
- clinical
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ViTiMM, a method that simplifies multi-modal
  medical data integration by representing diverse modalities (clinical measurements,
  medications, ECG, and X-ray images) as images, allowing training with a single vision-text
  transformer. This approach eliminates the need for specialized architectures for
  each modality and reduces training complexity to visual prompt engineering.
---

# Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers

## Quick Facts
- arXiv ID: 2501.18237
- Source URL: https://arxiv.org/abs/2501.18237
- Reference count: 40
- ViTiMM outperforms state-of-the-art methods on MIMIC-IV tasks, achieving AUROC of 0.922 for mortality prediction and 0.784 for phenotyping

## Executive Summary
ViTiMM presents a novel approach to multi-modal medical data integration by representing diverse clinical data types as images, enabling training with a single vision-text transformer. This eliminates the need for specialized architectures for each modality and reduces training complexity to visual prompt engineering. The method demonstrates superior performance on in-hospital mortality prediction and phenotyping tasks using the MIMIC-IV dataset, with AUROC scores of 0.922 compared to 0.875 for the previous state-of-the-art MeTra method. The approach provides interpretable attention visualizations and shows that incorporating more modalities improves predictive performance.

## Method Summary
ViTiMM converts clinical measurements, medications, ECG data, and X-ray images into a unified image representation format, allowing a single vision-text transformer to process all modalities simultaneously. The method uses visual prompt engineering to guide the transformer's attention to relevant features across different data types. By eliminating the need for specialized architectures for each modality, ViTiMM simplifies the training pipeline while maintaining or improving predictive accuracy. The approach handles irregular temporal intervals by encoding temporal information within the image representations, enabling the model to learn patterns across varying time scales.

## Key Results
- AUROC of 0.922 for in-hospital mortality prediction (vs. 0.875 for MeTra)
- AUROC of 0.784 for phenotyping when using all four modalities
- Performance improves with incorporation of additional modalities
- Provides interpretable attention visualizations highlighting influential input features

## Why This Works (Mechanism)
ViTiMM leverages the powerful representation learning capabilities of vision transformers by converting all medical data into a common image format. This standardization allows the model to learn shared feature representations across modalities without requiring complex fusion mechanisms. The vision transformer's self-attention mechanism naturally handles the variable-length and irregular temporal nature of medical data by learning to attend to relevant time points and features. Visual prompt engineering guides the transformer to focus on clinically relevant patterns, while the unified architecture reduces optimization challenges associated with multi-modal training.

## Foundational Learning
- **Vision Transformers**: Why needed - to handle image-based representations of diverse data types; Quick check - verify understanding of self-attention mechanism and positional encoding
- **Multi-modal Fusion**: Why needed - to integrate clinical measurements, medications, ECG, and imaging data; Quick check - understand limitations of modality-specific architectures
- **Visual Prompt Engineering**: Why needed - to guide transformer attention without complex fusion mechanisms; Quick check - know how prompts influence attention patterns
- **Medical Time Series Processing**: Why needed - to handle irregular temporal intervals in clinical data; Quick check - understand challenges of missing data and varying sampling rates
- **Attention Visualization**: Why needed - to provide interpretability for clinical decision support; Quick check - recognize how attention weights highlight influential features

## Architecture Onboarding

Component Map:
Clinical Data Sources -> Image Conversion Pipeline -> Vision Transformer -> Prediction Head -> Clinical Outcomes

Critical Path:
Image conversion occurs first, followed by transformer encoding, then attention-based feature aggregation, and finally prediction through the output head.

Design Tradeoffs:
- Unified image representation simplifies training but may lose modality-specific information
- Vision transformer provides powerful feature learning but requires significant computational resources
- Visual prompt engineering reduces architectural complexity but requires domain expertise
- Single model approach eliminates fusion challenges but may limit flexibility for task-specific optimization

Failure Signatures:
- Poor performance on modalities that lose critical information during image conversion
- Attention visualizations that don't align with clinical expectations
- Computational bottlenecks during training with large numbers of modalities
- Degradation in performance when temporal patterns are crucial but poorly encoded

Three First Experiments:
1. Test image conversion quality by comparing modality-specific vs. converted representations on simple classification tasks
2. Evaluate attention visualization interpretability by comparing highlighted features with clinical expert annotations
3. Measure performance degradation when removing individual modalities to identify critical data types

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Performance based solely on MIMIC-IV dataset, limiting generalizability to other clinical settings
- Image conversion may introduce information loss for certain data types
- Visual prompt engineering requires domain expertise that may limit accessibility
- No extensive analysis of computational requirements compared to traditional approaches
- Interpretability through attention may not provide granular clinical insights needed for decision support

## Confidence
High confidence in:
- Core methodology of representing multi-modal data as images for vision transformer processing
- Claim that this approach simplifies training by eliminating specialized architectures
- General improvement over baseline methods on MIMIC-IV tasks

Medium confidence in:
- Absolute performance numbers (AUROC scores) due to sensitivity to implementation details
- Claim that more modalities always improve performance across different tasks

Low confidence in:
- Scalability to very large numbers of modalities or extremely high-dimensional data
- Robustness to missing data patterns beyond MIMIC-IV testing

## Next Checks
1. Reproduce results on an independent, multi-center clinical dataset to verify generalizability beyond MIMIC-IV
2. Conduct ablation studies comparing performance when converting different modalities to images versus using modality-specific processing pipelines
3. Perform computational complexity analysis measuring training time, inference latency, and memory requirements compared to traditional multi-modal fusion methods