---
ver: rpa2
title: 'CoDiCodec: Unifying Continuous and Discrete Compressed Representations of
  Audio'
arxiv_id: '2509.09836'
source_url: https://arxiv.org/abs/2509.09836
tags:
- audio
- continuous
- embeddings
- discrete
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoDiCodec unifies continuous and discrete audio compression by
  producing both compressed continuous embeddings (~11 Hz) and discrete tokens (2.38
  kbps) from a single trained model. It uses summary embeddings to efficiently capture
  global features, consistency training with a single loss, and Finite Scalar Quantization
  (FSQ) with FSQ-dropout to enable high-quality continuous decoding.
---

# CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio

## Quick Facts
- arXiv ID: 2509.09836
- Source URL: https://arxiv.org/abs/2509.09836
- Reference count: 0
- Achieves superior audio quality (FAD 0.0112, FAD_clap 0.344) compared to existing autoencoders at similar compression ratios

## Executive Summary
CoDiCodec is a unified audio compression model that produces both continuous embeddings (~11 Hz) and discrete tokens (2.38 kbps) from a single trained architecture. It leverages summary embeddings to efficiently capture global audio features, consistency training with a single loss, and Finite Scalar Quantization (FSQ) with FSQ-dropout to enable high-quality dual-mode compression. The model supports both autoregressive and novel parallel decoding strategies, achieving state-of-the-art reconstruction quality while maintaining faster decoding speeds than existing approaches.

## Method Summary
CoDiCodec unifies continuous and discrete audio compression through a transformer-based encoder-upsampler-decoder architecture. The encoder processes STFT spectrograms through a 5-level convolutional patchifier and 12 transformer blocks to produce K=128 summary embeddings of dimension 4. These embeddings are either passed through FSQ with N=5 levels or directly through tanh to produce discrete tokens or continuous embeddings respectively. FSQ-dropout (p=0.75) probabilistically bypasses quantization during training, allowing the model to learn representations useful for both modes. The upsampler and decoder use cross-connections and chunked causal masking to reconstruct the audio, with parallel decoding using shifting pair strategies for boundary artifact elimination.

## Key Results
- Achieves FAD score of 0.0112 and FAD_clap of 0.344, outperforming existing autoencoders at similar compression ratios
- Supports both autoregressive and parallel decoding, with parallel achieving superior audio quality and faster decoding
- Single model produces both ~11 Hz continuous embeddings and 2.38 kbps discrete tokens
- Training converges in ~2 weeks on a single A100 GPU with 2M iterations

## Why This Works (Mechanism)

### Mechanism 1: FSQ-dropout Enables Dual-Mode Quantization
Probabilistically bypassing quantization during training allows a single encoder to produce both informative discrete tokens and expressive continuous embeddings. During training, with probability p=0.75, the rounding operation in Finite Scalar Quantization is skipped, passing continuous tanh(z) values directly. This prevents pre-quantization values from clustering near discrete levels while training the decoder to handle both input types.

### Mechanism 2: Summary Embeddings Reduce Redundant Global Encoding
Learnable summary embeddings that aggregate global context achieve better fidelity at high compression than ordered sequence encodings. K=128 summary embeddings (not temporally ordered) are concatenated with audio embeddings and processed through transformer blocks. Only these K embeddings are retained, forcing efficient global feature capture rather than redundant per-timestep encoding.

### Mechanism 3: Parallel Decoding with Shifting Pairs Eliminates Boundary Artifacts
Decoding adjacent latent pairs in parallel while shifting pair assignments across denoising steps propagates information across chunk boundaries. Split T summary embeddings into pairs, decode with noise, then shift pairs by one position for the next denoising step. This iterative cross-boundary refinement prevents artifacts from independent chunk decoding.

## Foundational Learning

- **Concept: Consistency Models**
  - Why needed here: CoDiCodec uses consistency training to enable single-step decoding, replacing iterative diffusion sampling
  - Quick check question: Can you explain why consistency models map any point on a noise trajectory back to the origin, and how this differs from standard diffusion denoising?

- **Concept: Finite Scalar Quantization (FSQ)**
  - Why needed here: FSQ provides discrete tokens without VQ's codebook learning overhead, using bounded rounding on each dimension
  - Quick check question: Given N=5, what is the implicit codebook size for a 4-dimensional FSQ, and why does FSQ achieve near-perfect codebook utilization?

- **Concept: STFT with Amplitude Transformation**
  - Why needed here: Audio input uses complex STFT with amplitude compression (c̃ = β|c|^α e^(i∠c)) to handle skewed energy distributions across frequency bins
  - Quick check question: Why does compressing amplitude with α∈(0,1] help neural networks learn from spectrograms?

## Architecture Onboarding

**Component map:**
Encoder: STFT → Conv Patchifier → [Audio Embeddings + Summary Embeddings] → Transformer Blocks → K Summary Embeddings → FSQ/tanh
Upsampler: Summary Embeddings → Transformer Blocks + Mask Embeddings → Conv De-Patchifier → Cross-connections
Decoder: Noisy STFT → Conv Patchifier + Cross-connections → Transformer Blocks (chunked causal) → De-Patchifier → Clean STFT

**Critical path:**
1. Summary embedding quality determines both compression efficiency and reconstruction fidelity
2. FSQ-dropout probability (p=0.75) balances discrete/continuous quality tradeoff
3. Parallel decoding step count (s=3-4) controls speed/quality tradeoff

**Design tradeoffs:**
- **Latent shape:** 128×4 vs 8×64 (same total dims): Paper shows 128×4 superior (Table 1, +128 lat. row)
- **Transformer vs Conv ratio:** Scaled toward transformers for easier scaling, but more memory
- **Parallel vs Autoregressive:** Parallel faster for long sequences but memory scales linearly; AR has constant memory

**Failure signatures:**
- Metallic/muffled audio: FSQ-dropout p too low, embeddings clustered near quantization levels
- Clicking at chunk boundaries: Insufficient parallel decoding steps (use s≥3)
- Poor downstream generation: Latents too gaussian (pre-training) or too uniform (check atanh preprocessing for generative models)

**First 3 experiments:**
1. **Validate FSQ-dropout effect:** Encode→decode same audio with p∈{0, 0.5, 0.75, 1.0}, plot latent distributions (should match Fig 2) and measure FAD gap between continuous/discrete modes
2. **Parallel decoding scaling:** Measure FAD_clap vs number of steps (s=1-6) on 60s audio, identify elbow point (paper shows s=4)
3. **Summary embedding ablation:** Compare K∈{32, 64, 128, 256} with fixed total dimensionality (K×d_lat constant), measure reconstruction quality vs compression

## Open Questions the Paper Calls Out

### Open Question 1
Does FSQ-dropout provide implicit regularization that improves the robustness of downstream generative models compared to standard continuous embeddings? Initial experiments show lower FAD_clap scores with fewer denoising steps, but the specific mechanism and extent of this robustness are not yet characterized.

### Open Question 2
How does CoDiCodec performance scale with increased model size and data volume? The current study evaluates a ~150M parameter model; the behavior of the proposed transformer-heavy architecture at larger scales remains unknown.

### Open Question 3
Are CoDiCodec representations effective for a broader range of Music Information Retrieval (MIR) tasks beyond reconstruction? The paper focuses on reconstruction and generative modeling; it does not benchmark the latent space for discriminative tasks like classification or tagging.

### Open Question 4
Can the parallel decoding strategy be optimized to overcome linear memory scaling constraints for long sequences? The trade-off between the speed of parallel decoding and its memory inefficiency is a practical limitation not addressed by the current method.

## Limitations

- Summary embeddings are the critical bottleneck, with quality depending entirely on K=128 summary embeddings capturing all relevant global features
- FSQ-dropout probability tuning is delicate, requiring dataset-specific tuning to balance discrete and continuous quality
- Parallel decoding complexity vs gains unclear, with memory scaling linearly with sequence length potentially offsetting speed benefits
- No generative model integration, despite claims about generative applications, lacking any evaluation with diffusion models

## Confidence

**High confidence** in:
- The core mechanism of FSQ-dropout enabling dual-mode quantization (well-supported by Figure 2 and ablation showing continuous quality degradation when p=0)
- Summary embeddings reducing redundancy compared to ordered sequences (directly supported by Music2Latent2 [2501.17578] and CoDiCodec's Table 1 comparison)

**Medium confidence** in:
- Parallel decoding with shifting pairs eliminating boundary artifacts (mechanism described but no ablation showing boundary-specific metrics)
- Overall quality improvements (FAD 0.0112) - strong on standard metrics but no generative model validation

**Low confidence** in:
- Cross-dataset generalization claims (models trained on multiple datasets but only evaluated on test splits from same domains)
- Real-time processing claims (decoder speeds measured on A100, not commodity hardware)

## Next Checks

1. **FSQ-dropout sensitivity analysis:** Systematically vary p∈{0, 0.25, 0.5, 0.75, 1.0} and measure: (a) continuous embedding distribution uniformity, (b) FAD gap between continuous/discrete modes, (c) audio quality degradation patterns (metallic/muffled artifacts)

2. **Summary embedding scaling study:** Fix total latent dimensionality (K×d_lat constant) and vary K∈{32, 64, 128, 256}. Measure reconstruction quality (FAD, FAD_clap) and compression efficiency to identify the optimal tradeoff point

3. **Parallel decoding boundary artifact analysis:** Decode 60s audio sequences with parallel decoding (s=1-6 steps) and measure: (a) FAD_clap at chunk boundaries vs middle regions, (b) spectral differences at 1024-sample boundaries, (c) memory usage vs sequence length