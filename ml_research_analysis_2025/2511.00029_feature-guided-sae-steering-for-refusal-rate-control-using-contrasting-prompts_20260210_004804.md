---
ver: rpa2
title: Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts
arxiv_id: '2511.00029'
source_url: https://arxiv.org/abs/2511.00029
tags:
- feature
- steering
- features
- prompts
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel feature-guided Sparse Autoencoder (SAE)
  steering method for controlling refusal rates in Large Language Models (LLMs) using
  contrasting prompts. The authors developed a composite scoring function to systematically
  identify optimal SAE features from 65,536 candidates in Llama-3 8B, then applied
  targeted steering to these features to improve safety performance.
---

# Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts

## Quick Facts
- **arXiv ID**: 2511.00029
- **Source URL**: https://arxiv.org/abs/2511.00029
- **Authors**: Samaksh Bhargav; Zining Zhu
- **Reference count**: 5
- **Primary result**: 18.9% improvement in safety performance while increasing utility by 11.1% through systematic SAE feature selection

## Executive Summary
This paper presents a novel feature-guided Sparse Autoencoder (SAE) steering method for controlling refusal rates in Large Language Models (LLMs) using contrasting prompts. The authors developed a composite scoring function to systematically identify optimal SAE features from 65,536 candidates in Llama-3 8B, then applied targeted steering to these features to improve safety performance. Using their contrasting prompt methodology with AI-Generated Prompts Dataset and Air Bench EU-Dataset, they achieved 18.9% improvement in safety performance while simultaneously increasing utility by 11.1%, demonstrating that targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods. The approach addresses limitations in current SAE-based steering methods by providing systematic feature selection and principled evaluation of safety-utility tradeoffs.

## Method Summary
The authors developed a feature-guided SAE steering framework that systematically identifies and targets safety-relevant features in LLMs. The method uses a composite scoring function to evaluate 65,536 SAE features from Llama-3 8B, selecting those most relevant to safety behaviors. The contrasting prompt methodology creates paired prompts that highlight differences in model responses, enabling precise feature attribution. Targeted steering is then applied to the selected features to control refusal rates. The approach uses the AI-Generated Prompts Dataset and Air Bench EU-Dataset for evaluation, measuring both safety performance improvements and utility impacts. The key innovation is the systematic feature selection process that overcomes the traditional safety-utility tradeoff by identifying features that can be steered without degrading overall model performance.

## Key Results
- Achieved 18.9% improvement in safety performance metrics
- Simultaneously increased utility by 11.1%, demonstrating break from traditional safety-utility tradeoff
- Successfully identified and steered optimal SAE features from 65,536 candidates in Llama-3 8B
- Demonstrated effectiveness using contrasting prompt methodology with AI-Generated Prompts Dataset and Air Bench EU-Dataset

## Why This Works (Mechanism)
The method works by systematically identifying SAE features that capture safety-relevant behaviors in LLMs, then applying targeted steering to these specific features. The composite scoring function evaluates features based on their correlation with safety behaviors identified through contrasting prompts, ensuring that only truly relevant features are selected for steering. This targeted approach avoids the blunt interventions of traditional safety techniques that often degrade overall model performance. By focusing steering efforts on precisely identified safety-relevant features, the method can improve safety metrics while maintaining or even enhancing utility. The contrasting prompt methodology provides a principled way to attribute behaviors to specific features, enabling more precise control over model responses to potentially harmful content.

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural network components that decompose activations into sparse, interpretable features. Needed for feature-level interpretability and steering. Quick check: Verify that SAE features capture semantically meaningful concepts in the model's representations.
- **Contrasting Prompts**: Pairs of prompts designed to highlight differences in model behavior. Needed for precise feature attribution and identification of safety-relevant behaviors. Quick check: Ensure contrasting prompts produce reliably different responses that can be used for feature identification.
- **Composite Scoring Functions**: Mathematical frameworks that combine multiple evaluation criteria to rank features. Needed for systematic selection of optimal steering targets from large feature spaces. Quick check: Validate that scoring function weights appropriately balance different evaluation criteria.
- **Safety-Utility Tradeoffs**: The fundamental tension between improving model safety and maintaining performance. Needed as the baseline problem that the method aims to overcome. Quick check: Measure both safety and utility metrics to confirm improvement in both dimensions.
- **Feature Attribution**: Methods for determining which features contribute to specific model behaviors. Needed for precise control over model responses through targeted steering. Quick check: Verify that attributed features consistently predict behavior across different prompts.
- **Targeted Steering**: The application of interventions to specific identified features rather than model-wide adjustments. Needed for precise, minimally invasive control over model behavior. Quick check: Confirm that steering only affects intended behaviors without collateral impact.

## Architecture Onboarding

**Component Map**: Prompt Generation -> SAE Feature Extraction -> Composite Scoring -> Feature Selection -> Targeted Steering -> Evaluation

**Critical Path**: The most critical sequence is Prompt Generation -> SAE Feature Extraction -> Composite Scoring -> Feature Selection, as errors in any of these steps will propagate to the steering effectiveness. The quality of contrasting prompts directly determines the reliability of feature attribution, which in turn affects the selection of optimal steering targets.

**Design Tradeoffs**: The method trades computational complexity (evaluating 65,536 features) for precision in steering. While this requires significant upfront computation, it enables more effective interventions that don't degrade utility. The choice of composite scoring function represents another tradeoff between simplicity and the ability to capture nuanced feature importance.

**Failure Signatures**: If the contrasting prompts don't produce reliably different responses, feature attribution will be unreliable. Poor SAE feature quality will lead to selection of irrelevant features for steering. An imbalanced composite scoring function may overweight certain criteria, leading to suboptimal feature selection. Inadequate evaluation datasets may result in overestimation of safety improvements.

**First Experiments**: 1) Test contrasting prompt methodology on a small subset of features to validate attribution reliability. 2) Run ablation studies on composite scoring function weights to assess sensitivity. 3) Evaluate steering effectiveness on a held-out dataset to measure generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across different model architectures and safety domains remains uncertain
- Feature selection sensitivity to composite scoring function weighting scheme needs further investigation
- Assumption that identified optimal features truly capture safety-relevant behaviors rather than spurious correlations
- Computational overhead of running SAE-based steering at inference time and its impact on latency is not fully characterized

## Confidence
- Overcoming traditional safety-utility tradeoffs (Medium): Results show improvement in both metrics, but require validation across diverse scenarios
- Feature selection reliability (Medium): Composite scoring function effectiveness depends on appropriate weighting
- Cross-model generalizability (Low): Current results limited to Llama-3 8B architecture

## Next Checks
1. Test the contrasting prompt methodology on at least two additional model architectures (e.g., GPT-family and open-source alternatives) to assess cross-model generalizability
2. Conduct ablation studies on the composite scoring function weights to determine robustness of feature selection
3. Evaluate performance under adversarial prompting strategies to assess resistance to circumvention attempts