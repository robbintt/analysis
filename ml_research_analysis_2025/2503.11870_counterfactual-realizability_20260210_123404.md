---
ver: rpa2
title: Counterfactual Realizability
arxiv_id: '2503.11870'
source_url: https://arxiv.org/abs/2503.11870
tags:
- counterfactual
- causal
- each
- agent
- unit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of determining which counterfactual
  distributions can be directly sampled from in real-world environments, where fundamental
  physical constraints like the inability to go back in time and re-expose the same
  unit to different conditions exist. The core method idea involves introducing a
  formal definition of realizability for counterfactual distributions and developing
  a complete algorithm (CTF-REALIZE) to determine whether an arbitrary counterfactual
  distribution is realizable given physical constraints.
---

# Counterfactual Realizability

## Quick Facts
- arXiv ID: 2503.11870
- Source URL: https://arxiv.org/abs/2503.11870
- Reference count: 40
- Primary result: Introduces a complete algorithm to determine which counterfactual distributions can be directly sampled from in real-world environments given physical constraints.

## Executive Summary
This paper addresses the fundamental problem of determining which counterfactual distributions are physically realizable in real-world environments. While the Pearl Causal Hierarchy (PCH) defines L3 as counterfactual distributions, most such distributions are not practically accessible due to physical constraints like the inability to go back in time and re-expose units to different conditions. The authors introduce a formal definition of realizability and develop the CTF-REALIZE algorithm to determine whether an arbitrary counterfactual distribution is realizable. They show that realizability depends on the absence of "conflicts" where the same variable must be in different regimes (intervened vs. natural) simultaneously.

## Method Summary
The paper introduces counterfactual randomization as an extension of Fisherian randomization, allowing intervention on a variable's value as perceived by its causal children without globally overwriting the mechanism. The CTF-REALIZE algorithm checks for realizability by detecting conflicts in ancestor sets where variables appear under different regimes. The method is validated through applications to causal fairness and reinforcement learning, showing that counterfactual strategies can provably dominate observational and interventional approaches in bandit settings when realizable distributions are available.

## Key Results
- CTF-REALIZE is a complete algorithm providing a graphical criterion for realizability: an L3-distribution is realizable if its ancestor set doesn't contain the same variable under different regimes.
- The Fundamental Problem of Causal Inference is shown to be a specific consequence of physical constraints on counterfactual distributions.
- In a bandit setting, an optimal counterfactual strategy based on sampling from P(Yx, X, Dx'') is proven optimal among all realizable strategies.

## Why This Works (Mechanism)

### Mechanism 1
It is physically possible to sample from specific Layer 3 distributions without time travel by intervening on a variable's value as perceived by its causal children. The CTF-RAND procedure generalizes Fisherian randomization by selectively fixing inputs for specific children using counterfactual mediators (e.g., editing pixels to change perceived race while preserving the actual file). The core assumption is that the environment contains counterfactual mediators—variables that fully encode parent values and allow mechanisms to be tricked into perceiving different values. This mechanism fails if no counterfactual mediator exists for a specific path.

### Mechanism 2
An L3 distribution is physically realizable if its set of counterfactual ancestors does not contain the same variable under conflicting regimes. CTF-REALIZE formalizes the "Fundamental Constraint of Experimentation" by checking for edge-coloring conflicts where measuring different counterfactuals would require the same mechanism to produce different outputs simultaneously. The algorithm is complete in detecting these conflicts, returning FAIL when the query requires the same mechanism to produce two different outputs for the same unit simultaneously.

### Mechanism 3
In decision-making settings with confounding, strategies utilizing realizable L3 samples provably dominate interventional strategies. An L3 strategy samples the unit's natural choice and potential downstream states to conditionally optimize expectations, exploiting correlations between natural propensities and latent confounders that standard Fisherian randomization ignores. This mechanism requires the decision problem to fit specific structural conditions where realizable L3 distributions can be sampled.

## Foundational Learning

- **Concept: Pearl Causal Hierarchy (L1/L2/L3)**
  - **Why needed here:** The paper redefines the boundary between L2 (interventional) and L3 (counterfactual). Understanding that standard RCTs access only L2 while this paper claims L3 access under specific structural conditions is fundamental.
  - **Quick check question:** Can you explain why P(Yx | x') (Effect of Treatment on the Treated) is an L3 quantity, whereas P(Y | do(x)) is L2?

- **Concept: Structural Causal Models (SCMs) & Potential Outcomes**
  - **Why needed here:** The algorithm relies on analyzing graphical ancestors and mechanisms to detect conflicts. You need to map variables to mechanisms to understand the "conflicts" defined in the paper.
  - **Quick check question:** In an SCM Y ← fY(X, UY), what does the operator Yx(u) represent for a specific unit u?

- **Concept: Fisherian Randomization**
  - **Why needed here:** This paper positions itself as an extension of standard experimental randomization. Understanding the baseline (overwriting mechanism fX) is necessary to grasp the innovation of CTF-RAND (selective overwriting).
  - **Quick check question:** Standard randomization breaks the link between X and U. Does CTF-RAND preserve this link? (Answer: Yes, for the natural value).

## Architecture Onboarding

- **Component map:** Causal Graph G -> Realizability Engine (CTF-REALIZE) -> Execution Layer (CTF-RAND actions) -> Decision Policy
- **Critical path:**
  1. Define the Causal Graph G and verify existence of Counterfactual Mediators in the environment
  2. Run CTF-REALIZE to ensure the target L3 query doesn't violate the "same variable, different regime" rule
  3. Deploy CTF-RAND to collect samples
- **Design tradeoffs:** Experimental Feasibility vs. Optimality (L3 strategy is theoretically optimal but requires CTF-RAND ability), Rejection Sampling efficiency issues
- **Failure signatures:** FPCI Violation (P(Yx, Yx') will fail immediately), Missing Mediators (physical action fails to match symbolic claim)
- **First 3 experiments:**
  1. Implement CTF-REALIZE to verify non-realizability of P(Yx, Yx') (Fundamental Problem of Causal Inference)
  2. Implement system to sample P(Yx, X) using CTF-RAND in a simulated environment to confirm ETT measurement
  3. Run MAB simulation comparing standard Thompson Sampling vs. proposed L3 strategy to verify dominance claim in cumulative regret

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the optimality of the proposed counterfactual strategy be generalized to sequential decision-making settings with arbitrary causal graphs?
  - **Basis in paper:** [explicit] Authors state in Appendix A that "Generalizing this to sequential decision-making settings with arbitrary graphs is an important, non-trivial extension."
  - **Why unresolved:** Paper proves optimality only for specific Multi-Armed Bandit template, which is a single-step decision process.
  - **What evidence would resolve it:** Formal proof or algorithm demonstrating a provably optimal counterfactual strategy within an MDP or sequential environment with arbitrary causal structures.

- **Open Question 2:** How does CTF-REALIZE behave under conditions of partial causal knowledge or model misspecification?
  - **Basis in paper:** [explicit] Authors note in Appendix A that a limitation is the requirement for a known graph, suggesting "Subsequent work could accommodate partial knowledge or model misspecification."
  - **Why unresolved:** Completeness relies on input graph accurately reflecting the environment; does not account for uncertainty or errors in causal structure.
  - **What evidence would resolve it:** Robust version of algorithm or analysis of error rates when input graph is an approximation or incomplete representation.

- **Open Question 3:** How does the ability to sample from realizable L3-distributions affect partial identification bounds for non-identifiable quantities?
  - **Basis in paper:** [explicit] Appendix A poses research question: "Another fascinating research question involves 'partial identification'... how would the new L3-data further tighten the bounds for non-identifiable L3-quantities?"
  - **Why unresolved:** Paper provides method to determine realizability but doesn't explore how resulting samples mathematically interact with existing methods for bounding non-identifiable counterfactuals.
  - **What evidence would resolve it:** Theoretical framework showing that including realizable L3-samples strictly narrows partial identification bounds compared to using only L1/L2 data.

## Limitations

- The framework assumes counterfactual mediators exist, which is a strong assumption about environment structure that may not hold in many real-world settings.
- The algorithm's practical applicability to high-dimensional, complex causal structures remains untested, with rejection sampling potentially becoming computationally prohibitive.
- The claimed optimality of counterfactual strategies in bandits relies on specific structural conditions and may not extend to all decision-making problems.

## Confidence

**High Confidence:**
- The algorithm's correctness for detecting realizability conflicts (supported by formal proofs in Section 3)
- The Fundamental Problem of Causal Inference as a special case (Cor. 3.8)

**Medium Confidence:**
- The existence and utility of counterfactual mediators in real environments (relies on assumption rather than empirical validation)
- The bandit optimality result (proven but in a specific, limited setting)

**Low Confidence:**
- The scalability of the approach to complex, high-dimensional problems
- The practical feasibility of finding counterfactual mediators in real-world applications

## Next Checks

1. **Mediator Validation:** For a concrete application domain (e.g., medical imaging), empirically verify whether counterfactual mediators exist that would enable CTF-RAND operations.

2. **Algorithm Stress Testing:** Systematically test CTF-REALIZE on progressively more complex causal graphs to identify performance bottlenecks and failure modes.

3. **Generalization to Other Settings:** Extend the counterfactual strategy analysis beyond bandits to settings like dynamic treatment regimes or sequential decision-making problems with partial observability.