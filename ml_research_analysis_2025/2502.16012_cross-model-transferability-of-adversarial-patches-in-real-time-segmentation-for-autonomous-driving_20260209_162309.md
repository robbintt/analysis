---
ver: rpa2
title: Cross-Model Transferability of Adversarial Patches in Real-time Segmentation
  for Autonomous Driving
arxiv_id: '2502.16012'
source_url: https://arxiv.org/abs/2502.16012
tags:
- patch
- adversarial
- attacks
- segmentation
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates cross-model transferability of adversarial
  patches in real-time semantic segmentation for autonomous driving. A novel Expectation
  Over Transformation (EOT)-based adversarial patch attack is proposed, along with
  a simplified loss function for training.
---

# Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving

## Quick Facts
- arXiv ID: 2502.16012
- Source URL: https://arxiv.org/abs/2502.16012
- Reference count: 30
- Primary result: Adversarial patches show high transferability across image dimensions but minimal cross-model transferability regardless of architecture (CNN vs. ViT)

## Executive Summary
This paper investigates the cross-model transferability of adversarial patches in real-time semantic segmentation for autonomous driving. The authors propose an Expectation Over Transformation (EOT)-based attack with a simplified loss function that focuses only on correctly classified pixels. Evaluating CNN-based models (PIDNet variants, ICNet, BiSeNetV1/V2) and a ViT-based model (SegFormer) on Cityscapes, they find that patches trained on one model are minimally effective on others, regardless of architecture. However, these patches exhibit high transferability when applied to unseen images of any dimension. The study also reveals that CNN models suffer localized patch effects while ViTs show broader influence, and that simple classes like "sky" are less vulnerable to misclassification.

## Method Summary
The method involves generating adversarial patches using EOT with random scaling [0.5, 2.0], 1024×1024 cropping, and horizontal flipping. A 200×200 patch is optimized at the image center using a simplified loss that aggregates cross-entropy only over correctly classified pixels. Gradient ascent updates the patch with ε=0.005, using batch size 6 for CNNs and 1 for SegFormer. Training runs for 30 epochs for CNNs and 15 for SegFormer. The patches are then evaluated on all models using Cityscapes validation set, measuring mIoU drops and per-class performance.

## Key Results
- Patches trained on a specific model cause minimal mIoU degradation on other models (changes <0.01)
- High transferability exists for unseen images regardless of dimensions
- CNN models show localized patch effects while ViTs demonstrate broader influence
- Per-class analysis reveals simple classes like "sky" suffer less misclassification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EOT-based transformation sampling produces patches that generalize across unseen images and dimensions.
- Mechanism: During optimization, each training image undergoes random scaling [0.5, 2.0], cropping to 1024×1024, and horizontal flipping before patch placement. The patch gradient is aggregated over this distribution, forcing the optimizer to find perturbations robust to viewpoint and scale variation rather than overfitting to specific pixel locations.
- Core assumption: The transformation distribution T approximates the variability encountered at inference time.
- Evidence anchors:
  - [abstract]: "the transferability (effectiveness) of attacks on unseen images of any dimension is really high"
  - [section III-A]: "During patch training we work with Cityscapes-train dataset... we perform scaling of the images in the range [0.5,2], and then crop it to 1024×1024. Additionally, we also flip the image horizontally"
  - [corpus]: Related work on Vision Transformers and realistic adversarial patches (arXiv:2509.21084) confirms EOT improves real-world patch effectiveness.
- Break condition: If deployment transformations diverge significantly from training distribution T (e.g., extreme occlusion, non-uniform lighting), patch effectiveness may degrade.

### Mechanism 2
- Claim: Simplified loss restricted to correctly classified pixels is sufficient for patch optimization while reducing hyperparameter sensitivity.
- Mechanism: The loss L = (1/|Ωx|) Σ y_i·log(m(x_i)) aggregates cross-entropy only over pixels where the model initially predicts correctly (Ωx). As the patch degrades these predictions, mIoU drops even though the loss magnitude remains stable (mean over shrinking set). This avoids the γ weighting hyperparameter from prior work [13].
- Core assumption: Degrading correctly classified pixels is sufficient to reduce overall segmentation quality; incorrectly classified pixels need not be explicitly targeted.
- Evidence anchors:
  - [section II]: "we exclusively focus on the cross-entropy loss of all pixels that are correctly classified"
  - [section II]: "This is a simplification of the previous works such as [13] that considered both the losses of correctly and incorrectly classified pixels and weighted them according to a hyper-parameter γ"
  - [corpus]: No direct corpus comparison to prior loss formulations; effectiveness is claimed relative to this paper's experimental results only.
- Break condition: If a model already has low baseline accuracy, the correctly-classified set Ωx may be too small to provide stable gradients.

### Mechanism 3
- Claim: Architecture-specific feature extraction limits cross-model patch transferability even within the same family (CNN vs. CNN or CNN vs. ViT).
- Mechanism: Each model learns unique spatial feature hierarchies and decision boundaries during pretraining. Patches optimized against one model's gradients exploit its specific vulnerabilities (e.g., particular filter responses, attention patterns). These exploits do not align with other models' learned representations, causing minimal performance degradation when transferred.
- Core assumption: The gradient direction ∇δL is model-specific and does not correlate across architectures.
- Evidence anchors:
  - [abstract]: "attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models"
  - [Table I]: PIDNet-L patch reduces PIDNet-L mIoU from 0.8996→0.7914 but leaves other models within ±0.001 of baseline.
  - [corpus]: Related work (arXiv:2511.00411) discusses transferability enhancement via gradient diversification, suggesting native transferability is low without explicit intervention.
- Break condition: If models share identical architectures and pretrained weights, or if ensemble-based patch optimization is used, transferability may increase.

## Foundational Learning

- Concept: Expectation Over Transformation (EOT)
  - Why needed here: Core technique making patches robust to real-world variation; understanding this is required to modify the attack or design defenses.
  - Quick check question: If you remove the scaling transformation from T and only use identity, would you expect the patch to generalize to images captured at different distances?

- Concept: Semantic Segmentation mIoU
  - Why needed here: The primary metric for attack effectiveness; understanding class-wise IoU helps interpret per-class vulnerability analysis.
  - Quick check question: Why might "sky" class IoU remain stable under attack while "road" degrades significantly?

- Concept: Gradient-based Adversarial Optimization (FGSM-style)
  - Why needed here: The patch update rule δ ← δ + ε·sign(∇δL) is directly derived from this paradigm; needed to understand attack implementation and potential defenses.
  - Quick check question: What happens to patch convergence if ε is set too large versus too small?

## Architecture Onboarding

- Component map:
  Image batch -> EOT transforms -> Patch application -> Model inference -> Loss computation -> Gradient ∇δL -> Patch update -> mIoU logging

- Critical path: Image batch → EOT transforms → Patch application → Model inference → Loss computation → Gradient ∇δL → Patch update → mIoU logging

- Design tradeoffs:
  - Patch size (200×200): Larger patches increase attack surface but are less realistic for physical deployment
  - Batch size (6 for CNNs, 1 for SegFormer): Memory-constrained for ViTs; smaller batches increase gradient noise
  - Epochs (30 CNN / 15 ViT): Diminishing returns after IoU stabilizes (see Fig. 2)

- Failure signatures:
  - Patch has no effect on target model: Check that patch is being applied correctly (mask I non-zero at patch location); verify loss is computed only on Ωx
  - mIoU does not decay across epochs: Learning rate ε may be too small, or model may already have low baseline accuracy (small Ωx)
  - CUDA OOM on SegFormer: Reduce batch size to 1 or use gradient accumulation

- First 3 experiments:
  1. Reproduce intra-model attack on PIDNet-L: Train patch for 30 epochs on Cityscapes-train, verify mIoU drop matches Table I (~0.79 on val set).
  2. Cross-model transfer test: Apply PIDNet-L trained patch to BiSeNetV2 and SegFormer; confirm mIoU change is <0.01 (minimal transfer).
  3. Ablate loss formulation: Compare simplified loss (Ωx only) vs. full loss (Ωx ∪ Ωc with γ weighting) on PIDNet-M; quantify mIoU decay difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial patches be optimized to be simultaneously effective across multiple diverse architectures (CNNs and ViTs)?
- Basis in paper: [explicit] The conclusion states the authors' "next immediate goal is to make these patches more robust... so that they are simultaneously effective across multiple architectures."
- Why unresolved: The current study concludes that patches trained on a specific model are minimally effective on others, failing to transfer even within the same architecture family (e.g., PIDNet-S to PIDNet-L).
- What evidence would resolve it: A training methodology that generates a single patch causing significant mIoU drops across both CNN-based (e.g., PIDNet) and ViT-based (e.g., SegFormer) models without architecture-specific retraining.

### Open Question 2
- Question: What architectural mechanisms in PIDNet-L cause the accelerated performance decay observed around epoch 15 during patch optimization?
- Basis in paper: [explicit] Section III-B notes that the PIDNet-L mIoU decay curve shows an "accelerated decay in performance... interesting behavior to study further" around epoch 15.
- Why unresolved: The paper observes the non-linear decay curve but does not provide an ablation study or theoretical explanation linking this specific temporal dynamic to the model's structure.
- What evidence would resolve it: An analysis isolating PIDNet-L's specific components (e.g., PID controller-inspired branches) to determine which feature triggers the sudden vulnerability onset.

### Open Question 3
- Question: What defense mechanisms can effectively mitigate EOT-based patch attacks in real-time systems without violating latency constraints?
- Basis in paper: [inferred] The abstract and conclusion emphasize the "need for robust defenses" in safety-critical autonomous driving, but the study focuses exclusively on attack generation and transferability analysis.
- Why unresolved: While the paper successfully demonstrates the susceptibility of real-time models, it does not evaluate or propose countermeasures that preserve the required inference speed.
- What evidence would resolve it: Implementation of a defense strategy (e.g., adversarial training or input sanitization) that restores mIoU on attacked inputs while maintaining the high frame rates required for autonomous driving.

## Limitations

- Simplified loss formulation effectiveness is only validated against the authors' own experimental results, lacking external validation
- EOT transformation coverage assumes real-world variability matches the [0.5, 2.0] scaling range and horizontal flipping
- Cross-architecture transferability conclusions are limited to the specific models tested and may not generalize to other architectures

## Confidence

- High confidence: Intra-model attack effectiveness (mIoU degradation on target model), EOT-based transformation robustness across image dimensions
- Medium confidence: Cross-model transferability conclusions (limited to evaluated architectures), per-class vulnerability patterns
- Low confidence: Comparative effectiveness of simplified loss vs. γ-weighted formulations (lacks external validation)

## Next Checks

1. Reproduce cross-architecture transferability tests on additional segmentation architectures (e.g., DeepLabV3+, HRNet) to verify generalizability beyond the tested models.
2. Conduct ablation studies comparing simplified loss vs. γ-weighted formulations on multiple datasets to quantify performance differences.
3. Test patch effectiveness under realistic deployment conditions (variable lighting, occlusion, weather effects) to validate EOT transformation coverage assumptions.