---
ver: rpa2
title: 'Large Language Models for Pedestrian Safety: An Application to Predicting
  Driver Yielding Behavior at Unsignalized Intersections'
arxiv_id: '2509.19657'
source_url: https://arxiv.org/abs/2509.19657
tags:
- yielding
- pedestrian
- driver
- behavior
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that multimodal large language models (LLMs)
  can effectively predict driver yielding behavior at unsignalized intersections by
  integrating visual, textual, and structured data through domain-specific prompts.
  Among evaluated models, GPT-4o achieved the highest accuracy (91.06%) and recall
  (92.41%), while Deepseek-V3 excelled in precision (92.06%).
---

# Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections

## Quick Facts
- arXiv ID: 2509.19657
- Source URL: https://arxiv.org/abs/2509.19657
- Reference count: 11
- Primary result: Multimodal LLMs can predict driver yielding behavior at unsignalized intersections with high accuracy using visual, textual, and structured data.

## Executive Summary
This study explores the use of multimodal large language models (LLMs) to predict driver yielding behavior at unsignalized intersections, integrating visual, textual, and structured data through domain-specific prompts. The research demonstrates that LLMs can effectively analyze complex traffic scenes and reason about pedestrian safety, offering a novel approach to intersection safety assessment. By leveraging the reasoning capabilities of LLMs, the study aims to provide interpretable and context-aware predictions that could inform pedestrian safety interventions.

## Method Summary
The study employs a multimodal approach, utilizing large language models such as GPT-4o and Deepseek-V3 to predict driver yielding behavior. Visual inputs from images of unsignalized intersections are combined with structured data (e.g., road geometry, pedestrian characteristics) and textual descriptions to form prompts for the LLMs. The models process these multimodal inputs to generate predictions about whether drivers will yield to pedestrians. The study evaluates model performance using metrics such as accuracy, recall, and precision, comparing results across different LLM architectures.

## Key Results
- GPT-4o achieved the highest accuracy (91.06%) and recall (92.41%) in predicting driver yielding behavior.
- Deepseek-V3 excelled in precision (92.06%), demonstrating strong performance in correctly identifying yielding instances.
- The approach provides interpretable reasoning, allowing for context-aware predictions in complex traffic environments.

## Why This Works (Mechanism)
Multimodal LLMs can integrate diverse data types (visual, textual, structured) to reason about driver behavior, leveraging their advanced pattern recognition and contextual understanding. The domain-specific prompts guide the models to focus on relevant safety factors, enhancing prediction accuracy.

## Foundational Learning
- **Unsignalized intersections**: Areas where traffic control is absent, requiring drivers to yield to pedestrians; critical for understanding yielding behavior.
- **Driver yielding behavior**: The act of drivers stopping for pedestrians; key metric for pedestrian safety assessment.
- **Multimodal data integration**: Combining visual, textual, and structured data; necessary for comprehensive scene analysis.
- **Domain-specific prompts**: Tailored instructions for LLMs; ensure focus on relevant safety factors.
- **Prediction accuracy metrics**: Accuracy, recall, and precision; used to evaluate model performance.

## Architecture Onboarding
- **Component map**: Image data -> Preprocessing -> Multimodal LLM -> Prediction output
- **Critical path**: Image acquisition and preprocessing are bottlenecks; model inference depends on prompt quality.
- **Design tradeoffs**: Balancing model complexity with interpretability; prioritizing safety-critical predictions over exhaustive scene analysis.
- **Failure signatures**: Misclassification in low-visibility conditions or atypical intersection geometries.
- **First experiments**:
  1. Test model performance on a diverse dataset with varying intersection types and weather conditions.
  2. Evaluate bias in predictions across pedestrian demographics.
  3. Assess real-time inference capabilities under dynamic traffic scenarios.

## Open Questions the Paper Calls Out
The study does not explicitly call out open questions, but implicit areas for further research include the model's robustness in real-world, dynamic traffic scenarios and its ability to handle rare or complex situations not represented in the evaluation set.

## Limitations
- Limited external validity due to constrained dataset composition and scene variability.
- Potential biases in predictions related to pedestrian demographics or environmental conditions are not addressed.
- Interpretability of model reasoning is highlighted but not systematically validated.

## Confidence
- **High Confidence**: The reported performance metrics (accuracy, recall, precision) for GPT-4o and Deepseek-V3 within the tested dataset.
- **Medium Confidence**: The generalizability of the approach to broader, real-world pedestrian safety contexts and diverse intersection types.
- **Low Confidence**: Claims about the model's ability to handle rare or complex traffic scenarios not represented in the evaluation set.

## Next Checks
1. Conduct cross-validation using a geographically and environmentally diverse dataset of unsignalized intersections, including adverse weather and low-visibility conditions.
2. Perform bias and fairness audits to assess model performance across pedestrian demographics and intersection geometries.
3. Implement a real-time simulation or field test to evaluate model robustness under dynamic, multi-agent traffic scenarios with varying pedestrian and driver behaviors.