---
ver: rpa2
title: 'Large Language Models'' Varying Accuracy in Recognizing Risk-Promoting and
  Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination
  and Heated Tobacco Products'
arxiv_id: '2507.04364'
source_url: https://arxiv.org/abs/2507.04364
tags:
- health
- accuracy
- sentiments
- public
- messages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the accuracy of three large language models
  (GPT, Gemini, and LLAMA) in detecting risk-promoting versus health-supporting sentiments
  in public health discourse on Facebook and Twitter. Using 1,600 human-annotated
  social media messages about HPV vaccination and heated tobacco products, the models
  were tested for their ability to classify sentiment with high accuracy.
---

# Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products

## Quick Facts
- arXiv ID: 2507.04364
- Source URL: https://arxiv.org/abs/2507.04364
- Authors: Soojong Kim; Kwanho Kim; Hye Min Kim
- Reference count: 14
- Primary result: LLAMA model achieved up to 98% accuracy in classifying public health sentiment on social media, outperforming GPT and Gemini across multiple conditions

## Executive Summary
This study evaluated three large language models (GPT, Gemini, and LLAMA) for their ability to classify risk-promoting versus health-supporting sentiments in public health discourse on Facebook and Twitter. Using 1,600 human-annotated social media messages about HPV vaccination and heated tobacco products, the researchers found significant variation in model accuracy based on platform, health topic, and sentiment type. LLAMA consistently outperformed the other models, achieving accuracy as high as 98% for certain tasks. The findings highlight the importance of careful model selection and validation when using LLMs to analyze public health communication.

## Method Summary
The study used 1,600 human-annotated social media messages (400 per platform-topic pair) to evaluate LLM classification accuracy. Researchers generated 20 independent responses per message using three models (GPT-4 Turbo, Gemini-1.0 Pro, LLAMA-3) with probabilistic sampling enabled. A simulated majority vote approach was employed, where 1,000 iterations sampled 3 responses each time to determine the final classification against human gold standard labels. The analysis compared accuracy across platforms (Facebook vs Twitter), health topics (HPV vaccination vs heated tobacco products), and sentiment types (risk-promoting, health-supporting, neutral).

## Key Results
- LLAMA achieved accuracy up to 98% for HPV risk sentiment detection on Facebook, outperforming both GPT and Gemini
- Risk-promoting sentiment was more accurately detected on Facebook (longer messages), while health-supporting sentiment was better identified on Twitter (shorter messages)
- All models struggled with neutral sentiment classification, achieving accuracy below 0.5 across conditions
- HPV vaccination topics showed higher classification accuracy than heated tobacco products, suggesting topic familiarity affects performance

## Why This Works (Mechanism)

### Mechanism 1: Context Richness and Message Length
Longer-form Facebook content provides additional semantic cues and context windows that stabilize classification, while shorter Twitter messages reduce signal-to-noise ratio. This creates platform-dependent accuracy patterns where extended messages facilitate more reliable classification.

### Mechanism 2: Training Data Prevalence and Topic Familiarity
HPV vaccination has decade-long discourse history with extensive public discussion, while heated tobacco products are newer with niche terminology. Models encode topic-specific linguistic distributions during pretraining, creating accuracy gradients based on training corpus composition.

### Mechanism 3: Salience Bias in Risk-Promoting Language
Risk-promoting messages use more salient linguistic patterns and emotional appeals, improving detectability. Content with higher perceived societal risk receives disproportionate attention and documentation, creating more distinctive pattern embeddings that LLMs recognize with higher confidence.

## Foundational Learning

- **Multi-instance LLM classification with majority voting**: Why needed - The study generates 20 responses per message and uses simulated 3-response majority voting to reduce stochasticity. Quick check - Can you explain why 20 instances with 1,000 simulations provides a more stable accuracy estimate than single-pass classification?

- **Sentiment polarity vs. neutral detection asymmetry**: Why needed - Neutral sentiment accuracy dropped below 0.5 across all models, while polarized sentiments exceeded 0.8. Quick check - Why might transformer attention mechanisms struggle with ambivalent or context-dependent language lacking polar expressions?

- **Platform-specific discourse conventions**: Why needed - Facebook and Twitter have different message length norms, audience structures, and linguistic styles that affect model performance. Quick check - What platform features beyond length might contribute to classification accuracy differences?

## Architecture Onboarding

- **Component map**: Social media corpus → Keyword filtering → Human annotation (gold standard) → Prompt construction → LLM API calls (20 instances per message) → Majority vote simulation (1,000 iterations) → Accuracy calculation

- **Critical path**: 
  1. Define sentiment schema (risk-promoting, health-supporting, neutral, mixed, irrelevant)
  2. Build human-annotated validation set (balanced across sentiment types)
  3. Construct classification prompts with explicit category definitions
  4. Configure API calls with temperature > 0 for response diversity
  5. Aggregate responses via majority voting; report mean accuracy across simulations

- **Design tradeoffs**:
  - Model selection: LLAMA showed highest accuracy but requires local deployment; GPT/Gemini offer API convenience with slight accuracy penalty
  - Response multiplicity: 20 instances improve stability but multiply cost; fewer instances may suffice for high-confidence classifications
  - Neutral handling: Consider excluding neutral from primary analysis or implementing secondary review pipeline

- **Failure signatures**:
  - High misclassification of polarized messages as neutral (thick connections between risk/health and neutral nodes)
  - Platform-specific accuracy inversion (risk-accurate on Facebook, health-accurate on Twitter)
  - Low-confidence outputs on niche terminology (HTP-specific language)

- **First 3 experiments**:
  1. Replicate with platform-stratified prompt templates that include context about message length and audience norms
  2. Test few-shot prompting by including 2-3 labeled examples per category in the prompt
  3. Evaluate fine-tuned LLAMA checkpoint on HTP-specific data to assess whether domain adaptation closes the topic gap

## Open Questions the Paper Calls Out

None

## Limitations

- **Platform representation bias**: Only evaluated Facebook and Twitter, excluding newer platforms like TikTok or Reddit that may have different linguistic conventions and user demographics
- **Temporal stability**: Model accuracy may fluctuate as training corpora evolve and public discourse patterns shift; the study's snapshot evaluation cannot capture performance changes over time
- **Prompt sensitivity**: The study didn't systematically test how minor prompt variations affect accuracy; small wording changes could significantly impact classification reliability

## Confidence

- **High confidence**: LLAMA's superior accuracy compared to GPT and Gemini across all conditions
- **Medium confidence**: Platform-specific accuracy patterns (Facebook vs Twitter)
- **Medium confidence**: Topic familiarity effects (HPV vs HTPs)
- **Low confidence**: Salience bias mechanism for risk-promoting detection

## Next Checks

1. **Cross-platform generalization test**: Evaluate the same models on Reddit and TikTok posts about HPV vaccination to determine if platform-specific accuracy patterns hold across additional social media environments.

2. **Temporal stability monitoring**: Track model accuracy over 6-month intervals as new health information emerges, measuring performance drift on both established topics (HPV) and emerging ones (novel tobacco products).

3. **Prompt sensitivity analysis**: Systematically vary prompt wording while holding all other variables constant to quantify how classification accuracy responds to specific linguistic framing choices.