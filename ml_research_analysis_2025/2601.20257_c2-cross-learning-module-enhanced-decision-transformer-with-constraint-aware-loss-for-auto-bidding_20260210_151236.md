---
ver: rpa2
title: C2:Cross learning module enhanced decision transformer with Constraint-aware
  loss for auto-bidding
arxiv_id: '2601.20257'
source_url: https://arxiv.org/abs/2601.20257
tags:
- learning
- loss
- bidding
- budget
- auto-bidding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses limitations in Decision Transformer (DT) for
  auto-bidding: insufficient modeling of cross-correlations among state, action, and
  return-to-go sequences, and indiscriminate learning of both optimal and suboptimal
  behaviors. The proposed C2 framework introduces two core innovations: (1) a Cross
  Learning Block (CLB) using cross-attention to strengthen inter-sequence correlation
  modeling, and (2) a Constraint-aware Loss (CL) incorporating budget and CPA constraints
  for selective learning of optimal trajectories.'
---

# C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding

## Quick Facts
- arXiv ID: 2601.20257
- Source URL: https://arxiv.org/abs/2601.20257
- Authors: Jinren Ding; Xuejian Xu; Shen Jiang; Zhitong Hao; Jin Hui Yang; Peng Jiang
- Reference count: 34
- Key outcome: C2 framework achieves up to 3.23% performance improvement over state-of-the-art auto-bidding methods through Cross Learning Blocks and Constraint-aware Loss

## Executive Summary
This paper addresses two key limitations in Decision Transformer (DT) for auto-bidding: insufficient modeling of cross-correlations among state, action, and return-to-go sequences, and indiscriminate learning of both optimal and suboptimal behaviors. The authors propose C2, which introduces a Cross Learning Block (CLB) using cross-attention to strengthen inter-sequence correlation modeling, and a Constraint-aware Loss (CL) incorporating budget and CPA constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains of up to 3.23% over state-of-the-art methods across diverse budget settings.

## Method Summary
C2 enhances Decision Transformer with two core innovations: Cross Learning Blocks (CLB) and Constraint-aware Loss (CL). CLB uses cross-attention between state, action, and return-to-go sequences to capture interdependencies that standard self-attention misses. CL scales the loss function based on constraint violations (budget and CPA), forcing the model to prioritize optimal trajectories. The framework is trained on the AuctionNet dataset using AdamW optimizer with specific hyperparameters (lr=1e-5, batch_size=128, 10,000 iterations) and evaluated using a score metric that accounts for budget constraints.

## Key Results
- C2 achieves up to 3.23% improvement over state-of-the-art auto-bidding methods
- Performance gains are consistent across budget ratios: 50%, 75%, 100%, 125%, and 150%
- Ablation studies confirm the complementary synergy of CLB and CL components
- Cross-correlation analysis validates CLB's effectiveness in modeling sequence dependencies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Replacing standard self-attention with cross-attention between State, Action, and Return-to-Go (RTG) sequences improves the model's ability to capture interdependencies required for bidding.
- **Mechanism**: The Cross Learning Block (CLB) computes attention across distinct modalities (e.g., Query from Action, Key/Value from State) rather than within a single concatenated sequence.
- **Core assumption**: The optimal bid action at time $t$ is a function of the interaction between the current state $s_t$, previous actions $a_{t-M:t}$, and target return $r_t$.
- **Evidence anchors**: Abstract mentions insufficient cross-correlation modeling; section 3.2 describes CLB's parallel hidden state sequences and cross-attention interactions.

### Mechanism 2
- **Claim**: Scaling the loss function by a dynamic penalty term derived from constraint violations forces the model to prioritize learning from optimal trajectories while suppressing suboptimal ones.
- **Mechanism**: The Constraint-aware Loss (CL) calculates a penalty $P$ based on Cost-Per-Acquisition (CPA) and Budget Consumption rate, amplifying MSE for constraint-violating samples.
- **Core assumption**: Historical data contains a mix of "lucky" high-reward but constraint-violating behaviors and "safe" compliant behaviors.
- **Evidence anchors**: Abstract discusses indiscriminate learning; section 3.3 details penalty calculation when bidding strategies lead to constraint violations.

### Mechanism 3
- **Claim**: Jointly modeling constraints as a multiplicative penalty ($P = P_{CPA} \times P_{BC}$) provides a stronger gradient signal for "Safe RL" than additive constraints.
- **Mechanism**: Instead of adding separate loss terms, C2 multiplies the penalties, causing "double-failure" trajectories to receive exponentially higher punishment.
- **Core assumption**: The interaction between constraints is non-linear; a trajectory that fails both budget and CPA constraints is exponentially more "toxic" to policy learning.
- **Evidence anchors**: Section 3.3 specifies the total penalty term as the product of $P_{CPA}$ and $P_{BC}$.

## Foundational Learning

- **Concept: Decision Transformers (DT) & Return-to-Go (RTG)**
  - **Why needed here**: C2 is a modification of the DT architecture that generates actions conditioned on a target "Return-to-Go" (RTG) value.
  - **Quick check question**: If you feed a high RTG value into a trained DT during inference, what behavior do you expect? (Answer: The model should output actions associated with high-reward trajectories).

- **Concept: Cross-Attention vs. Self-Attention**
  - **Why needed here**: The core innovation (CLB) relies on cross-attention. Standard transformers use self-attention (words attending to words). C2 uses cross-attention (Actions attending to States).
  - **Quick check question**: In the CLB block, if the "Query" comes from the Action sequence, where do the "Key" and "Value" vectors come from? (Answer: From the State and RTG sequences).

- **Concept: Generalized Second-Price (GSP) Auctions & CPA**
  - **Why needed here**: The loss function is built on domain-specific metrics. CPA (Cost Per Action) = Cost / Conversions.
  - **Quick check question**: If a campaign has a CPA constraint of 10, and a trajectory yields 100 cost for 5 conversions (CPA=20), how should the loss penalty behave? (Answer: It should trigger a heavy penalty because 20 > 10).

## Architecture Onboarding

- **Component map**: State ($S$), Action ($A$), RTG ($R$) sequences $\rightarrow$ Embedding Layer $\rightarrow$ $B$ stacked Cross Learning Blocks (CLB) $\rightarrow$ Output Heads $\rightarrow$ Loss Module
- **Critical path**:
  1. Data Prep: Batch generation must include full trajectory context to calculate constraint-aware penalty terms at episode end
  2. Forward Pass: Focus on CLB block where $A$, $S$, and $R$ tensors interact via cross-attention
  3. Loss Calc: Applying penalty $P$ element-wise to MSE loss before backprop
- **Design tradeoffs**: Cross-attention on 3 sequences is more expensive than simple concatenation + self-attention; multiplying penalties can cause gradient instability if not normalized
- **Failure signatures**:
  - "Constraint Amnesia": Model ignores CL loss (check if penalty $P$ is actually $> 1.0$ for bad samples)
  - "Correlation Collapse": CLB outputs indistinguishable from vanilla DT (verify masking logic in cross-attention)
  - "Reward Hacking": Model lowers bid to $0$ to ensure CPA constraints (check loss balance $\lambda$ between Action Loss and RTG Loss)
- **First 3 experiments**:
  1. Overfit Sanity Check: Train C2 on 100 AuctionNet trajectories, verify loss drops to near zero and penalty mechanism activates
  2. Ablation Architecture: Run vanilla DT vs. CLB-only (No CL loss), compare embedding similarity metric
  3. Loss Scaling Analysis: Run C2 with CL vs. standard MSE, plot predicted bids vs. ground truth for high-violation samples

## Open Questions the Paper Calls Out
- The paper mentions future work will explore dynamic penalty adjustment for better adaptability to real-time market conditions, but doesn't specify implementation details or evaluation metrics.

## Limitations
- **Unknown hyperparameters**: Number of Cross Learning Blocks (B), hidden dimension (d_h), and sequence length (M) are not specified in the paper
- **Dataset specifics**: Exact train/validation/test splits and preprocessing pipeline for AuctionNet are not detailed
- **Constraint parameterization**: Penalty coefficients (α1, α2) and threshold (θ) values lack sensitivity analysis

## Confidence
- **High**: Core mechanisms of cross-attention in CLB and multiplicative constraint penalties are well-specified and theoretically sound
- **Medium**: Ablation study results showing C2's superiority over DT and GAVE are convincing, but specific hyperparameter choices could influence gains
- **Low**: Effectiveness of cross-correlation analysis depends on implementation details of embedding extraction and comparison

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary B, d_h, and M to identify their impact on performance
2. **Constraint Robustness**: Test C2 with varying penalty coefficients (α1, α2) and thresholds (θ) to determine stability
3. **Cross-Attention Verification**: Visualize attention weight distributions from CLB to confirm genuine cross-sequence correlation learning