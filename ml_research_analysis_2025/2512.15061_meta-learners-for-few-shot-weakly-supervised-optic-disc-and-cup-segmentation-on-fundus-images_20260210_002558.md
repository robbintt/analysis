---
ver: rpa2
title: Meta-learners for few-shot weakly-supervised optic disc and cup segmentation
  on fundus images
arxiv_id: '2512.15061'
source_url: https://arxiv.org/abs/2512.15061
tags:
- segmentation
- image
- weasel
- protoseg
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops meta-learners for few-shot weakly-supervised
  segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC)
  segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly
  improve existing meta-learners by introducing Omni meta-training which balances
  data usage and diversifies the number of shots.
---

# Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images

## Quick Facts
- **arXiv ID**: 2512.15061
- **Source URL**: https://arxiv.org/abs/2512.15061
- **Authors**: Pandega Abyan Zumarsyah; Igi Ardiyanto; Hanung Adi Nugroho
- **Reference count**: 40
- **Primary result**: Efficient Omni ProtoSeg achieves 88.15% IoU for OD and 71.17% for OC on REFUGE with just one sparsely labeled image

## Executive Summary
This study addresses the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis using few-shot weakly-supervised learning. The authors develop meta-learners that significantly improve upon existing approaches through Omni meta-training, which balances data usage and diversifies shot numbers. They also create efficient versions that reduce computational costs and develop sparsification techniques for generating customizable sparse labels.

## Method Summary
The research introduces meta-learners for few-shot weakly-supervised segmentation (FWS) specifically designed for OD and OC segmentation in fundus images. The key innovation is Omni meta-training, which improves data utilization by balancing data usage and diversifying the number of shots per task. The authors also develop efficient versions of these meta-learners to reduce computational costs. Additionally, they create sparsification techniques that generate more customizable and representative scribbles and other sparse labels. The framework is evaluated across multiple datasets including REFUGE, DRISHTIGS, and RIM-ONE r3.

## Key Results
- EO-ProtoSeg achieves 88.15% IoU for OD and 71.17% for OC on REFUGE with only one sparsely labeled image
- Outperforms both few-shot and semi-supervised methods that require more labeled images
- Achieves best performance of 86.80% IoU for OD and 71.78% for OC on DRISHTIGS
- Best performance reaches 88.21% IoU for OD and 73.70% for OC on REFUGE
- Best performance of 80.39% IoU for OD and 52.65% for OC on RIM-ONE r3
- Comparable to unsupervised domain adaptation methods while being much lighter (less than two million parameters)
- Does not require any retraining, unlike domain adaptation approaches

## Why This Works (Mechanism)
The Omni meta-training approach balances data usage across tasks while diversifying the number of shots, creating a more robust meta-learner that generalizes better to unseen data. The efficient versions reduce computational overhead without sacrificing segmentation accuracy. The sparsification techniques generate more representative and customizable sparse labels, which are crucial for weakly-supervised learning where labeled data is limited. By combining these elements, the framework achieves high performance with minimal labeled examples, making it practical for clinical applications where expert annotations are scarce.

## Foundational Learning
- **Few-shot learning**: Why needed - enables model training with minimal labeled examples; Quick check - evaluate performance with 1-5 labeled samples
- **Weakly-supervised segmentation**: Why needed - addresses scarcity of pixel-level annotations in medical imaging; Quick check - compare with fully supervised baselines
- **Meta-learning**: Why needed - learns to learn across multiple tasks for better generalization; Quick check - ablation study removing meta-learning component
- **Sparsification techniques**: Why needed - creates efficient labeling schemes from limited annotations; Quick check - test different sparsity patterns
- **Domain adaptation**: Why needed - handles variations across different fundus image datasets; Quick check - cross-dataset evaluation
- **Efficient neural architectures**: Why needed - reduces computational burden for clinical deployment; Quick check - measure inference time and memory usage

## Architecture Onboarding

**Component Map:**
Input Images -> Preprocessing -> Omni Meta-training Framework -> Efficient ProtoSeg Module -> Output Segmentation Masks

**Critical Path:**
Sparse Labels → Sparsification Module → Meta-learner Training → Efficient ProtoSeg → Segmentation Prediction

**Design Tradeoffs:**
- Model complexity vs. computational efficiency (efficient versions sacrifice some capacity for speed)
- Label sparsity vs. segmentation accuracy (more sparse labels reduce annotation burden but may impact quality)
- Shot diversity vs. training stability (more diverse shot numbers improve generalization but may complicate training)

**Failure Signatures:**
- Underfitting when shot diversity is too high relative to available data
- Poor generalization across datasets if domain shift is significant
- Reduced accuracy with excessive label sparsity

**First 3 Experiments to Run:**
1. Baseline comparison: Test EO-ProtoSeg against standard few-shot and semi-supervised methods on REFUGE dataset
2. Efficiency analysis: Measure inference time and memory usage of efficient versions vs. original meta-learners
3. Sparsity ablation: Evaluate performance degradation as label sparsity increases from dense to very sparse

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to three glaucoma-related datasets, restricting generalizability to other fundus pathologies
- Sparsity-based labeling may not fully capture pathological variations across diverse patient populations
- Computational efficiency claims based on relative comparisons rather than absolute clinical deployment constraints

## Confidence

**High Confidence:**
- Performance improvements over few-shot baselines on the three tested datasets
- Architectural efficiency gains in the Omni meta-training variant

**Medium Confidence:**
- Generalization potential to other fundus segmentation tasks
- Practical clinical utility given fixed parameter counts

**Low Confidence:**
- Applicability to non-glaucoma pathologies
- Robustness across diverse imaging equipment and protocols

## Next Checks
1. Evaluate EO-ProtoSeg performance on multi-disease fundus datasets (e.g., diabetic retinopathy, age-related macular degeneration) to assess cross-pathology generalization
2. Conduct ablation studies isolating the contribution of sparsity patterns versus meta-learning strategy to quantify each component's impact
3. Benchmark against real-time clinical deployment constraints, including GPU memory limits and inference latency requirements for point-of-care applications