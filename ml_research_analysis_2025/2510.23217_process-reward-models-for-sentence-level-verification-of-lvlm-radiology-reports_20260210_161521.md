---
ver: rpa2
title: Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports
arxiv_id: '2510.23217'
source_url: https://arxiv.org/abs/2510.23217
tags:
- arxiv
- reports
- clinical
- report
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a sentence-level Process Reward Model (PRM) for verifying
  LVLM-generated radiology reports. Our PRM predicts factual correctness of each sentence,
  conditioned on clinical context and prior text, functioning as a black-box verifier.
---

# Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports

## Quick Facts
- arXiv ID: 2510.23217
- Source URL: https://arxiv.org/abs/2510.23217
- Reference count: 35
- Primary result: A 0.5B-parameter PRM achieves 7.5% relative MCC improvement and 1.8% AUROC gain over white-box baselines on MAIRA-2 outputs

## Executive Summary
This paper presents a Process Reward Model (PRM) for sentence-level verification of LVLM-generated radiology reports. The PRM predicts factual correctness of each sentence, conditioned on clinical context and prior text, functioning as a black-box verifier. Fine-tuned on MIMIC-CXR with weakly-supervised labels from RadNLI, a lightweight 0.5B-parameter PRM achieves significant improvements over strong white-box baselines. The PRM generalizes well to unseen LVLMs and effectively filters low-quality reports, improving downstream clinical metrics by 4.5% in F1-CheXbert when discarding the worst 10%.

## Method Summary
The PRM operates as a sequential binary decoder, fine-tuned on weakly-supervised labels from RadNLI entailment judgments. It takes clinical context (Indication, Technique, Comparison) plus previously verified sentences as prefix, then predicts correctness probability for each generated sentence. The model uses Qwen2.5-base (0.5B or 3B) with TRL PRM-trainer, trained on ~200k balanced instances from MIMIC-CXR. Verification uses aggregation strategies (MinProb, AvgProb, ProdProb) for report-level scoring, enabling downstream applications like rejection sampling and weighted Best-of-N selection.

## Key Results
- 0.5B-PRM achieves 7.5% relative MCC improvement (0.571→0.614) and 1.8% AUROC gain over ReXTrust on MAIRA-2 outputs
- Generalizes to unseen CheXagent-8B: 3B-PRM maintains MCC=0.306, AUROC=0.754 vs. ReXTrust's MCC=0.000
- PRM-Avg rejection at 10% threshold improves F1-CheXbert by 4.5% (0.699→0.730)
- Weighted Best-of-N selection yields 7.4% F1-CheXbert improvement and 0.6% BERTScore gain

## Why This Works (Mechanism)

### Mechanism 1: Sequential Contextual Verification
Conditioning on both clinical context and previously verified sentences improves hallucination detection accuracy over sentence-isolated approaches. The PRM builds a prefix from [clinical prompt + sentence₁ + label₁ + ... + sentenceᵢ] before predicting the correctness probability for sentenceᵢ, allowing detection of contradictions with prior claims or clinical context.

### Mechanism 2: Weak Supervision via Domain-Specific Entailment
Noisy labels from RadNLI (80% accuracy) provide sufficient signal for training an effective verifier at scale. Generated sentences are labeled y=1 if RadNLI predicts entailment against any ground-truth sentence; otherwise y=0, enabling training without human annotation.

### Mechanism 3: Black-Box Architecture Enables Cross-Generator Transfer
Using only text inputs (no internal activations) allows the PRM to generalize to unseen LVLMs where white-box methods fail. White-box methods like ReXTrust depend on generator-specific hidden state structure; under distribution shift, this structure may not preserve class separability.

## Foundational Learning

- **Process Reward Models (PRMs)**: Originally developed for step-by-step verification of mathematical reasoning (Lightman et al., 2023). Quick check: Can you explain how a PRM differs from an Outcome Reward Model (ORM) that only scores final outputs?

- **Natural Language Inference (NLI) / Entailment**: The weak supervision pipeline uses RadNLI to determine if generated sentences are entailed by ground truth. Quick check: If a generated sentence states "no pleural effusion" but ground truth doesn't mention pleural effusion at all, should RadNLI predict entailment, contradiction, or neutral?

- **Best-of-N Selection and Rejection Sampling**: Standard techniques for improving output quality using a verifier. Quick check: In Best-of-N, why might weighted selection (grouping by CheXbert labels) outperform direct ranking by PRM score?

## Architecture Onboarding

- **Component map**: Clinical context H_k + Generated sentences s^gen_{k,1...m} -> PRM backbone -> Sequential prediction p(correct | prefix) -> Aggregation (MinProb, AvgProb, ProdProb) -> Report-level score -> Downstream (Rejection threshold OR Best-of-N selection)

- **Critical path**: 1) Generate reports from LVLM using clinical context 2) Segment into sentences 3) Feed context + sentences sequentially to PRM 4) Collect per-sentence correctness probabilities 5) Aggregate → apply rejection or selection logic

- **Design tradeoffs**: Model size: 0.5B better at threshold-dependent metrics (MCC), 3B better at ranking (AUROC)—choose based on application. Aggregation: MinProb is conservative (sensitive to single bad sentence), AvgProb is smoother, ProdProb penalizes compounding uncertainty. Context ablation: "Technique" and "Indication" are critical; "Comparison" can be noise, especially for larger models.

- **Failure signatures**: PRM flags correct novel findings as hallucinations (limitation of entailment-based labels—correct statements not in ground truth are marked wrong). Degenerate MCC on new generator suggests distribution shift beyond PRM's training distribution. High rejection rates may improve F1-CheXbert but reduce coverage—evaluate tradeoff for clinical deployment.

- **First 3 experiments**: 1) Reproduce sentence-level verification metrics (Table 1) on MIMIC-CXR test set with Qwen2.5-0.5B-PRM to validate setup. 2) Run ablation study removing "Comparison" context to confirm asymmetric effect on 0.5B vs. 3B models (Table 4). 3) Implement PRM-Avg rejection at 10% threshold and measure F1-CheXbert improvement (target: +4.5% relative).

## Open Questions the Paper Calls Out

- Can providing the PRM with direct visual context (the chest X-ray image) significantly improve sentence-level hallucination detection compared to text-only context?

- Can training with human-annotated or LLM-annotated labels mitigate the false-negative bias inherent in using RadNLI entailment as a proxy for factual correctness?

- Can the PRM function effectively as a reward model within a reinforcement learning framework to directly improve the factuality of the LVLM generator during training?

- Is the "Comparison" section context beneficial for verification, or does it introduce noise that degrades performance in larger models?

## Limitations

- Weak supervision via RadNLI introduces significant label noise (80% accuracy), creating an upper bound on PRM performance and penalizing novel but true findings
- Cross-model generalization demonstrated only on one additional model (CheXagent-8B), limiting confidence in transferability
- Absence of human-annotated validation sets prevents direct assessment of clinical alignment

## Confidence

- **High confidence**: Sequential verification mechanism and black-box architecture design are well-supported by experimental results
- **Medium confidence**: Downstream applications show promise but depend on unmeasured clinical tradeoffs
- **Low confidence**: Weak supervision effectiveness is inferred rather than directly validated

## Next Checks

1. Generate a small human-annotated validation set (100-200 sentences) and measure RadNLI's actual accuracy, particularly for novel findings

2. Test the PRM on at least two additional unseen LVLM architectures beyond CheXagent-8B to establish cross-model generalization

3. Measure the tradeoff between F1-CheXbert improvement and report coverage reduction at various rejection thresholds, and conduct small-scale clinician evaluation