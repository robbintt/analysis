---
ver: rpa2
title: 'ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback'
arxiv_id: '2503.19988'
source_url: https://arxiv.org/abs/2503.19988
tags:
- reasoning
- on-policy
- arxiv
- text-to-sql
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExCoT addresses the challenge of improving text-to-SQL reasoning
  in large language models by integrating Chain-of-Thought (CoT) reasoning with Direct
  Preference Optimization (DPO) using only execution accuracy as feedback. The method
  generates diverse CoT solutions, verifies correctness through database execution,
  and iteratively refines the model via off-policy and on-policy DPO.
---

# ExCoT: Optimizing Reasoning for Text-to-SQL with Execution Feedback

## Quick Facts
- arXiv ID: 2503.19988
- Source URL: https://arxiv.org/abs/2503.19988
- Reference count: 15
- Primary result: ExCoT achieves 68.53% execution accuracy on BIRD test set with LLaMA-3.1 70B, setting new state-of-the-art for single-model setting

## Executive Summary
ExCoT addresses the challenge of improving text-to-SQL reasoning in large language models by integrating Chain-of-Thought (CoT) reasoning with Direct Preference Optimization (DPO) using only execution accuracy as feedback. The method generates diverse CoT solutions, verifies correctness through database execution, and iteratively refines the model via off-policy and on-policy DPO. This approach eliminates the need for reward models or human annotations. Experimental results show significant performance gains: ExCoT improves execution accuracy on BIRD from 57.37% to 68.51% and on Spider from 78.81% to 86.59% for LLaMA-3.1 70B, achieving state-of-the-art performance in the single-model setting with 68.53% on the BIRD test set.

## Method Summary
ExCoT generates multiple Chain-of-Thought solutions for each text-to-SQL query using GPT-4o, executes them in a sandboxed database environment to identify correct solutions, then applies Direct Preference Optimization using execution accuracy as the sole feedback signal. The method employs a two-phase training strategy: off-policy DPO using teacher-generated data with maximum edit distance sampling for broad distinctions, followed by iterative on-policy DPO using the model's own generations with minimum edit distance sampling for fine-grained refinement. The approach includes supervised fine-tuning on verified correct solutions before applying DPO, eliminating the need for reward models or human annotations.

## Key Results
- ExCoT improves execution accuracy on BIRD from 57.37% to 68.51% and on Spider from 78.81% to 86.59% for LLaMA-3.1 70B
- Achieves state-of-the-art 68.53% execution accuracy on BIRD test set in single-model setting
- Outperforms baselines including zero-shot CoT (minimal gains) and DPO without CoT (marginal improvements)
- Significant improvements over previous best results: 64.54% on BIRD and 85.21% on Spider for comparable models

## Why This Works (Mechanism)

### Mechanism 1: CoT reasoning provides intermediate signal for DPO
Chain-of-Thought reasoning exposes the model's intermediate logic steps, allowing DPO to attribute errors to specific reasoning paths rather than just final SQL outputs. This makes the gradient signal more actionable for training.

### Mechanism 2: Binary execution accuracy as scalable preference proxy
Execution feedback serves as an objective, automated preference labeling system. Correct executions become "winners" and errors become "losers," eliminating the need for human annotations or separate reward models.

### Mechanism 3: Iterative on-policy DPO with dynamic sampling
The method uses furthest edit distance pairs for initial off-policy training to establish broad distinctions, then switches to nearest edit distance pairs for on-policy refinement to polish subtle logic errors.

## Foundational Learning

### Direct Preference Optimization (DPO)
**Why needed:** ExCoT replaces Reinforcement Learning with DPO, which directly compares winning and losing responses without learning an explicit reward function. **Quick check:** How does DPO differ from PPO in terms of requiring a reward model during optimization?

### Execution-Guided Training vs. Inference
**Why needed:** Prior work uses execution feedback only during inference to fix queries, while ExCoT uses it for training to change model weights. **Quick check:** Does this system execute SQL to fix output during inference or to generate gradients for training?

### Edit Distance Metrics
**Why needed:** The paper uses edit distance as a proxy for difficulty in distinguishing correct from incorrect SQL, with furthest vs. nearest selection strategies. **Quick check:** Why would you prefer high edit distance pairs for initial training and low edit distance pairs for final refinement?

## Architecture Onboarding

### Component map:
CoT Generator -> Execution Verifier -> Pair Constructor -> DPO Trainer

### Critical path:
1. Seed Data Generation: Generate up to 32 candidates per question using GPT-4o
2. Verification: Execute and label as correct/incorrect based on results
3. SFT: Warm-start model on positive examples only
4. Off-Policy DPO: Train on maximum edit-distance pairs from seed data
5. On-Policy Loop: Generate new candidates -> Verify -> Construct minimum edit-distance pairs -> Train

### Design tradeoffs:
- Off-policy (teacher data) is cheaper but suffers distribution mismatch
- On-policy is expensive but aligns model to its own error distribution
- "Furthest" pairs are easier to learn but may miss subtle bugs
- "Nearest" pairs are harder but necessary for pushing accuracy boundaries

### Failure signatures:
- Data Exhaustion: Accuracy plateaus while valid preference pair count drops
- CoT Faithfulness: Model generates correct SQL with nonsensical reasoning traces
- Format Drifting: SQL output outside expected code blocks causes verification failure

### First 3 experiments:
1. Baselines: Run base model with zero-shot CoT vs. No-CoT to confirm minimal gains
2. SFT Ablation: Fine-tune on correct execution examples only to establish baseline
3. Sampling Ablation: Compare Random vs. Furthest pair selection in DPO

## Open Questions the Paper Calls Out

### Open Question 1: Free-style CoT vs. Structured CoT
Would free-style CoT reasoning without structured divide-and-conquer templates further improve performance compared to ExCoT's constrained format?

### Open Question 2: Advanced RL Methods
Can more advanced reinforcement learning methods (Online-DPO, PPO, GRPO) surpass offline DPO's performance gains for text-to-SQL alignment?

### Open Question 3: Complex Schema Performance
How does performance degrade on highly intricate schemas with complex table relationships and domain-specific conventions?

### Open Question 4: CoT Quality Validation
Can CoT reasoning quality be validated beyond execution accuracy to ensure logical consistency and eliminate contradictory intermediate steps?

## Limitations
- Performance may struggle on highly intricate schemas with complicated table relationships and domain-specific conventions
- Computational cost is high due to generating multiple candidates per question and iterative on-policy refinement
- The method assumes execution equivalence implies semantic correctness, which may not hold for all database scenarios

## Confidence
- Generalizability to other benchmarks: Medium
- Faithfulness of CoT traces: Medium
- Data efficiency and scalability: Medium
- Edit distance implementation details: Low

## Next Checks
1. Test ExCoT performance on additional text-to-SQL benchmarks beyond BIRD and Spider to assess generalizability
2. Analyze whether CoT traces generated by the fine-tuned model accurately reflect the reasoning that led to correct SQL outputs
3. Evaluate how performance scales with different numbers of candidate generations per question and different iteration counts for on-policy DPO