---
ver: rpa2
title: 'Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild'
arxiv_id: '2501.02964'
source_url: https://arxiv.org/abs/2501.02964
tags:
- visual
- arxiv
- image
- questions
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Socratic Questioning (SQ), a multi-round
  training and reasoning framework for lightweight Multimodal Large Language Models
  (MLLMs) that integrates Chain of Thought reasoning with visual instruction tuning.
  The method uses heuristic self-questioning to guide MLLMs to focus on relevant visual
  clues, reducing hallucinations and improving fine-grained image detail description.
---

# Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild

## Quick Facts
- arXiv ID: 2501.02964
- Source URL: https://arxiv.org/abs/2501.02964
- Reference count: 40
- Primary result: Lightweight MLLM achieves 31.2% hallucination reduction on CapQA and state-of-the-art POPE F1 88.78 via multi-round self-questioning

## Executive Summary
Socratic Questioning (SQ) introduces a multi-round training and reasoning framework for lightweight Multimodal Large Language Models (MLLMs) that integrates Chain of Thought reasoning with visual instruction tuning. By structuring reasoning as a sequence of self-generated questions and answers grounded in image details, the method addresses the "visual ignorance" problem where MLLMs ignore visual features in favor of language priors. The approach achieves strong performance on multiple benchmarks while maintaining low computational costs, demonstrating particular effectiveness in fine-grained visual reasoning tasks.

## Method Summary
SQ trains MLLMs through a structured conversation format where the model first generates questions about fine-grained image details, then answers them, and finally produces a summarized description. The framework uses GPT-4V to generate training data in this multi-turn format from the CAP dataset, creating a compact dataset of ~1k images. The model (Vicuna-7B with ViT-L/14 encoder) is fine-tuned using LoRA on a mixture of standard LLaVA training data and the custom CapQA dataset, learning to perform visual reasoning through self-questioning rather than direct description.

## Key Results
- 31.2% improvement in hallucination score on CapQA dataset
- State-of-the-art POPE F1 score of 88.78
- Strong performance on LLaVA-QA90 (81.3) and MME benchmarks (1523.4)
- Effective zero-shot reasoning capabilities while maintaining low computational costs
- Demonstrates superiority of lightweight MLLMs over larger models for specific fine-grained reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Visual Grounding Through Self-Questioning
Structured self-questioning mitigates "visual ignorance" by forcing the model to ground reasoning in specific image details before generating final descriptions. The model generates questions about fine-grained details, forcing visual encoder features to be attended to during the answer phase, counteracting the tendency to rely on language priors over visual data.

### Mechanism 2: Multi-Round Conversation Formatting
The CapQA format (Question List → Q&A Pairs → Description → Summary) creates a dense reward signal for lightweight models, improving zero-shot generalization. This compact representation maximizes the utility of the small CapQA dataset by teaching the model a meta-policy for visual reasoning rather than simple input-output mapping.

### Mechanism 3: Condensation as Denoising
The summarization step acts as a denoising filter for intermediate reasoning steps, filtering out extraneous details and potential hallucinations from the verbose "Detailed Description" phase. This compression helps the model distinguish between salient visual evidence and irrelevant noise in its own generated chain of thought.

## Foundational Learning

**Visual Instruction Tuning (e.g., LLaVA):** Understanding how text-image pairs are converted to conversation tokens is essential since SQ builds directly upon LLaVA-1.5 architecture. Quick check: Do you understand how an image feature map is projected into the LLM's embedding space via an adapter (MLP)?

**Chain of Thought (CoT) Reasoning:** SQ is a specific instantiation of CoT, modifying it with visual grounding. Quick check: Can you explain why standard CoT (reasoning purely in latent space) might fail in vision tasks where visual evidence is transient?

**Hallucination in MLLMs:** Understanding why models hallucinate (object existence bias) is crucial since the primary metric here is hallucination reduction. Quick check: Why does a standard MLLM often say "Yes" when asked if an object exists in an image, even if it's not there?

## Architecture Onboarding

**Component map:** Visual Encoder (ViT-L/14) → Adapter (2-Layer MLP) → LLM (Vicuna-7B)

**Critical path:** Pretraining with LLaVA-CC3M-595K to align the Adapter only, then instruction tuning with LoRA on LLaVA-Mix-665K + CapQA mixture, followed by 1-turn or 3-turn inference selection.

**Design tradeoffs:** 1-turn inference is faster but less accurate on fine-grained tasks, while 3-turn adds latency but significantly lowers hallucination. Using only ~1k CapQA images is efficient but relies heavily on GPT-4v synthetic annotation quality.

**Failure signatures:** Visual ignorance where the model answers questions using language priors rather than image features, and question drift where self-asked questions become irrelevant to the main task.

**First 3 experiments:** 1) Sanity check baseline Vicuna-1.5 vs. SQ-7b on POPE benchmark, 2) Ablation comparing 1-turn vs. 3-turn inference on CapQA subset, 3) Qualitative visual grounding test with subtle activities to check if questions target specific interactions.

## Open Questions the Paper Calls Out

**Question 1:** How can a specific loss function be designed to constrain the model to generate more effective, utility-driven questions that directly benefit the final visual reasoning task? The current training objective uses standard auto-regressive loss which ensures grammatical correctness but doesn't optimize the utility or information gain of self-asked questions.

**Question 2:** Does integrating Visual Large Models (VLMs) with region alignment capabilities (such as GLIP or SAM) into the SQ framework improve fine-grained visual grounding compared to the current ViT-L/14 encoder? The current architecture may lack specific spatial or region-level features provided by detection-based or segmentation-based VLMs.

**Question 3:** Can a dynamic routing mechanism be developed to automatically select between 1-turn and 3-turn inference modes based on query complexity, optimizing the trade-off between computational cost and reasoning depth? The paper presents two distinct inference strategies but doesn't provide a method for the model to autonomously discern problem complexity.

## Limitations

- Dataset dependency on GPT-4V quality for the small CapQA dataset (~1k images) raises questions about generalizability to other domains
- Experiments limited to Vicuna-7B model size, leaving performance ceiling and benefits vs. larger models unclear
- 3-turn inference protocol adds computational latency without quantified overhead or cost-benefit analysis
- No ablation study on dataset size to establish method's data efficiency curve

## Confidence

- **High Confidence:** Quantitative improvements on established benchmarks (POPE F1 88.78, LLaVA-QA90 81.3) are reproducible given specified architecture and training procedure
- **Medium Confidence:** Mechanism of "visual ignorance" mitigation through self-questioning is plausible but lacks direct attention visualization or feature attribution studies
- **Low Confidence:** Generalization of Socratic Questioning to domains outside human activity description is speculative, with robustness to diverse image types and real-world noisy inputs untested

## Next Checks

1. **Attention Grounding Verification:** Run the fine-tuned model on images with known fine-grained details and visualize cross-attention weights during "Self-ask" and "Self-answer" phases to confirm attention focuses on regions corresponding to self-generated questions.

2. **Dataset Scaling Sensitivity:** Repeat CapQA data generation with varying dataset sizes (0.1k, 0.5k, 1k, 2k images) while keeping all other parameters constant to measure hallucination scores and establish method's data efficiency curve.

3. **Cross-Domain Generalization:** Evaluate SQ-7b model on non-human activity datasets such as RefCOCO+ or ScienceQA to test whether multi-round reasoning format transfers beyond training distribution.