---
ver: rpa2
title: Contextualizing Search Queries In-Context Learning for Conversational Rewriting
  with LLMs
arxiv_id: '2502.15009'
source_url: https://arxiv.org/abs/2502.15009
tags:
- query
- in-context
- conversational
- learning
- rewriting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Prompt-Guided In-Context Learning for conversational
  query rewriting. It addresses the challenge of transforming context-dependent conversational
  queries into standalone search queries, particularly in low-resource settings where
  labeled data is scarce.
---

# Contextualizing Search Queries In-Context Learning for Conversational Rewriting with LLMs

## Quick Facts
- arXiv ID: 2502.15009
- Source URL: https://arxiv.org/abs/2502.15009
- Reference count: 30
- The paper introduces Prompt-Guided In-Context Learning for conversational query rewriting, achieving significant improvements over baselines with BLEU-4 of 30.5 and Success Rate@10 of 0.57.

## Executive Summary
This paper addresses the challenge of transforming context-dependent conversational queries into standalone search queries using Large Language Models (LLMs) without fine-tuning. The method, Prompt-Guided In-Context Learning, leverages carefully designed prompts with task descriptions, format specifications, and illustrative examples to guide query rewriting. Extensive experiments on TREC and Taskmaster-1 datasets demonstrate significant improvements over supervised and contrastive co-training baselines across multiple metrics, particularly for elliptical queries where the method achieves a 37% relative improvement over baselines.

## Method Summary
The approach uses Prompt-Guided In-Context Learning with LLaMA-3.1 to rewrite conversational queries into standalone search queries. The method employs prompts containing task definitions, input/output format specifications, and 2-5 in-context examples showing History→Query→Rewrite mappings. No gradient updates or fine-tuning are used. The input is structured into ordered conversation turns and current query, and the LLM generates rewritten queries based on the implicit guidance embedded in the prompt examples. Evaluation uses BLEU-4, ROUGE-L, Success Rate@10, and MRR metrics on TREC Conversational Assistance Track and Taskmaster-1 datasets.

## Key Results
- BLEU-4 score of 30.5 compared to supervised baseline of 28.3 and contrastive co-training of 26.8
- Success Rate@10 of 0.57 versus supervised baseline of 0.44 and contrastive co-training of 0.38
- MRR of 0.63 compared to supervised baseline of 0.56 and contrastive co-training of 0.48
- 37% relative improvement on elliptical queries (Success Rate@10: 0.52 vs. 0.38 for supervised baseline)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context examples provide implicit task learning without gradient updates.
- Mechanism: The prompt includes 2-5 illustrative examples demonstrating History→Query→Rewrite mappings. The LLM conditions on these patterns and generalizes to new inputs, leveraging its pre-trained knowledge of language structure and reference resolution.
- Core assumption: The pre-trained LLM has sufficient linguistic knowledge to recognize and replicate the rewriting pattern from demonstrations alone.
- Evidence anchors: [abstract] "employs carefully designed prompts... and a small set of illustrative examples, to guide pre-trained LLMs... without explicit fine-tuning"; [section III.B] "The 'learning' process is not driven by gradient updates but rather by the implicit guidance embedded within the meticulously crafted prompt"

### Mechanism 2
- Claim: Explicit conversation history structuring enables reliable context extraction.
- Mechanism: Input is partitioned into ordered conversation turns (H = [Turn1, Turn2, ..., Turnn-1]) and a separate Current Query field. This explicit segmentation reduces ambiguity about what context to leverage.
- Core assumption: The LLM can correctly identify which prior turns contain relevant information for resolving the current query.
- Evidence anchors: [section III.A.2] "The input is structured to present the conversational context and the query requiring rewriting in a structured manner"; [section IV.F] On elliptical queries, the method achieves Success Rate@10 of 0.52 vs. 0.38 for supervised baseline—a 37% relative improvement

### Mechanism 3
- Claim: Task definition anchoring reduces output drift and format violations.
- Mechanism: The prompt begins with an explicit task statement ("rewrite the user's last query into a standalone search query") before examples and input. This orients the model toward the specific transformation objective.
- Core assumption: The LLM follows instructions in the order presented and maintains focus on the stated objective throughout generation.
- Evidence anchors: [section III.A.1] "We initiate the prompt with a clear and unambiguous statement that explicitly defines the task"; [section III.A.4] The structured output specification ("Rewritten Query") provides a clear generation target

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The entire method depends on understanding that LLMs can perform tasks from prompt demonstrations without weight updates. Without this, the approach seems like "just prompting."
  - Quick check question: Can you explain why adding examples to a prompt changes model output if no weights are updated?

- Concept: **Coreference and Ellipsis Resolution**
  - Why needed here: The target task requires recovering omitted information (ellipsis) and resolving pronoun references from conversation history. Understanding these linguistic phenomena clarifies why structured context matters.
  - Quick check question: Given "User: Find Italian restaurants. System: Here are options. User: Which ones deliver?", what information must be recovered to form a standalone query?

- Concept: **Retrieval Evaluation Metrics (MRR, Success Rate@k)**
  - Why needed here: BLEU/ROUGE measure surface similarity but not downstream search effectiveness. Understanding MRR and Success Rate connects rewriting quality to actual retrieval performance.
  - Quick check question: Why might a rewritten query with low BLEU score still achieve high Success Rate@10?

## Architecture Onboarding

- Component map:
  Input Processing: Raw conversation → Structured History + Current Query
  Prompt Assembly: Task Definition + Format Specs + In-Context Examples + Test Input
  LLM Inference: LLaMA-3.1 (or similar) → Rewritten Query
  Evaluation: BLEU-4, ROUGE-L, Success Rate@10, MRR

- Critical path:
  1. Example selection (quality and diversity of 2-5 demonstrations directly impacts performance)
  2. Prompt formatting (consistent structure across examples and test input)
  3. Output parsing (extract clean rewritten query from LLM response)

- Design tradeoffs:
  - More examples improve performance but consume context window; paper shows diminishing returns from 2→5 examples
  - Zero-shot avoids example curation but sacrifices ~14% BLEU improvement
  - Larger LLMs likely improve performance but increase latency and cost (not tested in paper)

- Failure signatures:
  - Model generates conversational responses instead of rewritten queries → check task definition clarity
  - Rewrites ignore conversation history → verify History field formatting in prompt
  - Output includes extra commentary → tighten format specification or add post-processing
  - Performance drops on domain-specific terminology → examples may lack coverage; expand demonstration set

- First 3 experiments:
  1. **Zero-shot vs. Few-shot ablation**: Replicate Table II on a held-out subset. Confirm monotonic improvement with 0, 2, 5 examples. Validates ICL mechanism.
  2. **Elliptical query analysis**: Manually annotate 50 queries as elliptical/non-elliptical. Compare Success Rate@10 between categories. Confirms where the method provides greatest value.
  3. **Cross-dataset generalization**: Train prompt examples on TREC, test on Taskmaster-1 (and reverse). Measures example portability across conversational domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced prompting strategies like Chain-of-Thought (CoT) further improve performance on complex elliptical queries compared to the standard illustrative examples used?
- Basis in paper: [explicit] The conclusion states a goal to "explore more sophisticated prompt engineering techniques."
- Why unresolved: The paper utilizes standard zero-shot and few-shot prompting with illustrative examples but does not evaluate complex reasoning prompts like CoT, despite discussing them in the related work section.
- What evidence would resolve it: An ablation study comparing standard few-shot performance against CoT-guided prompting on the same datasets, specifically analyzing error rates for complex ellipsis resolution.

### Open Question 2
- Question: How can the computational efficiency of prompt-guided rewriting be improved to compete with the lower latency of fine-tuned, smaller models?
- Basis in paper: [explicit] The conclusion explicitly lists "methods for improving the efficiency" as a direction for future work.
- Why unresolved: The paper compares quality metrics (BLEU, ROUGE) but does not analyze inference latency or computational cost, which are critical for real-time search applications.
- What evidence would resolve it: A comparison of inference time and FLOPs between the LLaMA-3.1 approach and the lighter Transformer-based baselines, potentially exploring distillation methods.

### Open Question 3
- Question: To what extent does this in-context learning approach transfer to other conversational tasks beyond query rewriting?
- Basis in paper: [explicit] The authors state they will "investigate the applicability of our approach to other conversational tasks."
- Why unresolved: The current evaluation is limited strictly to the query rewriting task using TREC and Taskmaster-1 datasets.
- What evidence would resolve it: Applying the same prompt-guided structure to related tasks like conversational response generation or intent detection and reporting comparative performance against task-specific baselines.

## Limitations
- The method's reliance on prompt engineering introduces significant uncertainty about reproducibility due to unspecified prompt templates and example selection strategies.
- Performance may degrade on conversations exceeding the LLM's context window or containing conflicting/redundant information.
- The approach may struggle with domain-specific terminology not covered by pre-training or demonstration examples.

## Confidence

- **High Confidence**: The core mechanism of in-context learning for query rewriting is well-established in the literature. The comparative performance improvements over supervised and contrastive baselines (BLEU-4: 30.5 vs. 26.8-28.3, Success Rate@10: 0.57 vs. 0.38-0.44) are statistically significant and reproducible.
- **Medium Confidence**: The claim that structured context input (explicit History + Current Query fields) enables reliable context extraction is supported by ablation results (37% relative improvement on elliptical queries) but lacks detailed analysis of long conversation handling or conflicting context scenarios.
- **Low Confidence**: The effectiveness of task definition ordering (placing instructions before examples) is assumed but not empirically validated. The paper does not test instruction-example conflicts or alternative prompt structures.

## Next Checks

1. **Prompt Template Fidelity Test**: Implement the exact prompt structure described and measure performance degradation when varying example selection strategies (random vs. curated). This isolates the impact of example quality on the claimed ICL mechanism.
2. **Cross-Domain Robustness**: Apply the method to a third conversational dataset (e.g., Amazon Alexa Prize) to test whether the in-context examples transfer across different conversational domains and query types.
3. **Long Conversation Analysis**: Systematically test performance degradation on conversations exceeding 7 turns to identify the context window limits and quantify the impact on coreference resolution accuracy.