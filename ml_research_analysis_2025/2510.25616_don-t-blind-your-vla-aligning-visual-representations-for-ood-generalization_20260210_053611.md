---
ver: rpa2
title: 'Don''t Blind Your VLA: Aligning Visual Representations for OOD Generalization'
arxiv_id: '2510.25616'
source_url: https://arxiv.org/abs/2510.25616
tags:
- arxiv
- visual
- alignment
- representations
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of visual representation degradation
  in Vision-Language-Action (VLA) models during fine-tuning for robotic tasks. It
  shows that standard supervised fine-tuning causes these models to lose the strong
  visual-language grounding inherited from their pretrained Vision-Language Model
  (VLM) base, leading to degraded attention maps, collapsed representations, and reduced
  generalization.
---

# Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization

## Quick Facts
- **arXiv ID**: 2510.25616
- **Source URL**: https://arxiv.org/abs/2510.25616
- **Reference count**: 40
- **Primary result**: Visual Representation Alignment preserves VLA visual-language grounding during fine-tuning, improving OOD generalization by up to 10% relative on Simpler benchmark and linear probing

## Executive Summary
This paper addresses the problem of visual representation degradation in Vision-Language-Action (VLA) models during robotic fine-tuning. Standard supervised fine-tuning causes these models to lose the strong visual-language grounding inherited from their pretrained Vision-Language Model (VLM) base, leading to collapsed representations and reduced generalization. The authors propose a lightweight Visual Representation Alignment method that anchors the VLA's visual features to those of a frozen, pretrained vision teacher during fine-tuning. Experiments on the Simpler benchmark and ImageNet-100 linear probing demonstrate that this approach consistently improves out-of-distribution (OOD) generalization while preserving better representational quality.

## Method Summary
The proposed method involves aligning the visual features of a VLA model to those of a frozen vision teacher during fine-tuning. Specifically, the VLA's transformer layer 16 features are projected to the teacher's embedding space using a frozen MLP projector, and a cosine similarity loss is applied between the aligned features. This encourages the VLA to preserve semantically consistent visual representations while adapting to action tasks. The method is applied during standard supervised fine-tuning with LoRA adapters, adding negligible computational overhead. Experiments use OpenVLA-7B as the backbone and C-RADIOv3 as the frozen vision teacher, with alignment loss weight Î»=0.2.

## Key Results
- Alignment method improves OOD generalization by up to 10% relative on Simpler benchmark's VL-Think suite
- Preserves better representational quality, shown through ImageNet-100 linear probing improvements
- Outperforms baselines like freezing the encoder and other alignment strategies (e.g., Enc2Enc)
- Adds negligible computational overhead to standard fine-tuning

## Why This Works (Mechanism)
The method works by preventing the VLA from losing its visual-language grounding during fine-tuning. When VLA models are adapted to robotic tasks, they tend to degrade their visual representations to focus on action prediction, losing the rich semantic structure learned during pretraining. By aligning the VLA's visual features to those of a frozen vision teacher, the model is encouraged to maintain semantically consistent representations while still learning task-specific action mappings. This preserves the model's ability to generalize to novel visual contexts and tasks.

## Foundational Learning
- **Vision-Language-Action (VLA) models**: Combine visual perception, language understanding, and action generation for robotics. Needed to understand the target architecture being improved.
- **Vision-Language Models (VLMs)**: Pretrained models that align visual and language representations. Required as the knowledge source for VLA models.
- **Representation alignment**: Technique to encourage models to produce features similar to a teacher model. Core mechanism of the proposed method.
- **Out-of-distribution (OOD) generalization**: Model's ability to perform well on data distributions different from training. Key evaluation metric for robotic generalization.
- **Linear probing**: Evaluating representation quality by training a linear classifier on frozen features. Used to assess representational quality independently of action prediction.
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method using low-rank matrices. Used to adapt VLA models efficiently.

## Architecture Onboarding
- **Component map**: OpenVLA-7B -> Frozen C-RADIOv3 teacher -> Alignment loss -> LoRA adapters -> Action prediction
- **Critical path**: Visual encoder -> Transformer layers -> Alignment layer (16) -> Frozen projector -> Teacher features -> Cosine similarity loss
- **Design tradeoffs**: Alignment layer selection (middle vs early vs late) vs representation preservation vs task adaptation
- **Failure signatures**: 
  - Low VL-Think scores despite high VLA task accuracy (overfitting to training distribution)
  - Poor ImageNet-100 linear probing results (degraded visual representations)
  - Projector loss drops but probing accuracy stays low (alignment shortcut)
- **First experiments**:
  1. Compare Backbone2Enc alignment vs Enc2Enc alignment on Simpler OOD splits
  2. Test different alignment layer choices (14, 16, 18) on VL-Think success rates
  3. Evaluate ImageNet-100 linear probing with and without alignment during fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Method is evaluated only on Simpler benchmark and ImageNet-100, not on real-world robotic deployments
- Computational overhead is claimed negligible but not quantified with runtime measurements
- Relies on a frozen vision teacher without addressing teacher selection or adaptation for different domains
- Alignment layer choice (layer 16) is heuristic without systematic exploration of optimal layers

## Confidence
- **High confidence**: VLA models lose visual-language grounding during fine-tuning is well-supported by empirical evidence
- **Medium confidence**: Alignment method consistently improves OOD generalization on Simpler and ImageNet-100
- **Medium confidence**: Method adds negligible computational overhead, though not verified with measurements

## Next Checks
1. Measure actual runtime overhead during fine-tuning compared to naive fine-tuning
2. Evaluate method on a real robotic manipulation task to verify OOD generalization gains translate to physical robot performance
3. Investigate whether domain-specific or updated vision teachers improve alignment effectiveness