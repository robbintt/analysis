---
ver: rpa2
title: 'BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation'
arxiv_id: '2506.00482'
source_url: https://arxiv.org/abs/2506.00482
tags:
- language
- zhang
- evaluation
- datasets
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BenchHub, a unified benchmark suite designed
  to address the challenges of fragmented and outdated LLM evaluation datasets. BenchHub
  aggregates and automatically classifies 303K questions from 38 benchmarks across
  64 subject categories, enabling customizable, domain-aware evaluations tailored
  to specific tasks.
---

# BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation

## Quick Facts
- arXiv ID: 2506.00482
- Source URL: https://arxiv.org/abs/2506.00482
- Authors: Eunsu Kim; Haneul Yoo; Guijin Son; Hitesh Patel; Amit Agarwal; Alice Oh
- Reference count: 40
- Primary result: Introduces BenchHub, a unified benchmark suite with 303K questions across 38 benchmarks and 64 subject categories, enabling customizable, domain-aware LLM evaluation.

## Executive Summary
BenchHub addresses the fragmentation and staleness of existing LLM benchmarks by aggregating 303K questions from 38 diverse datasets and automatically classifying them into a unified taxonomy of Skill, Subject, and Target categories. This enables users to create tailored evaluation sets that reflect their specific needs, revealing that model performance rankings vary significantly across subject categories and are heavily influenced by dataset distribution. The framework uses LLM-based synthetic rationale generation for scalable categorization and demonstrates that holistic benchmarks may mask critical domain-specific weaknesses.

## Method Summary
The system aggregates 303K questions from 38 benchmarks, automatically classifies them using a fine-tuned LLM (BenchHub-Cat-7B) based on synthetic rationales, and provides a web interface for filtering and downloading customized evaluation sets. The categorizer is trained on Qwen-2.5-7B-Instruct with synthetic data generated by prompting GPT-4o to create explanations for category assignments. Evaluation experiments test model rankings across different sampling strategies (Random, Stratified, MixEval) using statistical significance testing (Friedman test, p < 0.01).

## Key Results
- Model performance rankings vary significantly across subject categories, with STEM knowledge showing greater sensitivity to benchmark composition than general reasoning tasks.
- Benchmark composition directly affects model rankings; statistical testing confirms that different sampling strategies yield significantly different leaderboards.
- The automated categorization approach achieves 0.871 accuracy for Subject labels but only 0.494 for Target labels, highlighting challenges in region-specific classification.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained categorization enables evaluations that are more predictive of domain-specific performance than aggregate scores.
- **Mechanism:** The system aggregates 303K questions and tags them by Skill (reasoning, knowledge), Subject (64 categories), and Target (culture), allowing users to filter test sets matching specific deployment contexts.
- **Core assumption:** A single aggregate score masks critical weaknesses in specific sub-domains that would otherwise determine success or failure in vertical applications.
- **Evidence anchors:** Model performance varies significantly across subject categories; rankings change substantially when using customized vs. stratified benchmarks; related work supports domain-specific evaluation.

### Mechanism 2
- **Claim:** Variations in benchmark composition directly cause instability in model leaderboards.
- **Mechanism:** By running simulations with different sampling strategies, the framework isolates the distribution variable and confirms ranking changes through statistical testing.
- **Core assumption:** The mix of question types in a benchmark acts as a confounding variable in model comparison.
- **Evidence anchors:** Outcomes are heavily influenced by subject type distribution; Friedman test validates sampling strategies significantly affect rankings.

### Mechanism 3
- **Claim:** Synthetic rationale generation enables scalable alignment of diverse datasets to a unified schema.
- **Mechanism:** Instead of manual labeling, an LLM generates rationales explaining category fits, which train a smaller student model to perform classification automatically.
- **Core assumption:** Training on synthetic rationales transfers category definitions more effectively than simple input-output pairs.
- **Evidence anchors:** Synthetic rationales ensure reliability in classification; rationale-based training is described in methodology; limited direct evidence in corpus for this specific approach.

## Foundational Learning

- **Concept: Stratified vs. Random Sampling**
  - **Why needed here:** Section 4.2 relies on understanding how different sampling methods affect statistical outcomes.
  - **Quick check question:** If I sample 100 questions from a dataset containing 90% History and 10% Math using random sampling, will the resulting evaluation be math-heavy or history-heavy?

- **Concept: Multi-label Taxonomy**
  - **Why needed here:** BenchHub classifies questions across three dimensions where a single question can belong to multiple categories.
  - **Quick check question:** Can a question be labeled as both "Knowledge" and "Reasoning," or is it mutually exclusive in this schema?

- **Concept: Statistical Significance (p-value)**
  - **Why needed here:** The paper claims ranking changes are statistically significant (p < 0.01), necessary to accept the conclusion about distribution effects.
  - **Quick check question:** If the p-value were 0.50 instead of < 0.01, would the paper be able to claim that sampling strategies significantly change rankings?

## Architecture Onboarding

- **Component map:** Ingestion Layer (LLM reformatting agent) -> Classification Engine (BenchHub-Cat-7B) -> Storage (unified dataset) -> Interface (web UI)
- **Critical path:** The accuracy of BenchHub-Cat-7B classifier; mislabeling categories invalidates domain-specific results.
- **Design tradeoffs:** Automated vs. manual labeling prioritizes scalability over perfect precision; fixed taxonomy improves consistency but may miss emerging domains.
- **Failure signatures:** Classifier drift failing to map new question types to existing buckets; reformatting errors causing misaligned columns.
- **First 3 experiments:**
  1. Classifier Validation: Run BenchHub-Cat-7B on held-out samples and manually verify accuracy against ground truth.
  2. Ranking Reproduction: Replicate Section 4.2 using Random vs. Stratified sampling on a single model family.
  3. Ablation on Rationales: Train control classifier without synthetic rationales and compare accuracy.

## Open Questions the Paper Calls Out
- To what extent can human-in-the-loop methods or ensemble models mitigate categorization biases inherent in using a single LLM for taxonomy assignment?
- How can the framework be effectively adapted for low-resource languages where labeled data for retraining the categorizer is scarce?
- Is it possible to construct a "universally fair" sample distribution for general-purpose LLM evaluation, or do ranking fluctuations necessitate strictly domain-specific customization?

## Limitations
- Categorizer accuracy is asymmetric across dimensions, with Target labels showing notably low performance (0.494).
- Reliance on synthetic rationale generation introduces potential distribution shifts between training data and real-world queries.
- Framework's effectiveness depends on comprehensiveness of fixed 64-category taxonomy, potentially missing emerging domains.

## Confidence
- **High confidence:** Core mechanism that benchmark composition affects model rankings (supported by p < 0.01 statistical testing)
- **Medium confidence:** Automated categorization approach given performance gap between Subject (0.871) and Target (0.494) accuracy
- **Low confidence:** Transferability to non-English/Korean contexts without explicit validation

## Next Checks
1. Test BenchHub-Cat-7B on external dataset (e.g., MMLU) to assess generalization beyond included benchmarks.
2. Perform error analysis on Target classification dimension to identify source of misclassifications.
3. Evaluate categorizer performance on multilingual benchmark to determine taxonomy generalizability beyond Korean/English.