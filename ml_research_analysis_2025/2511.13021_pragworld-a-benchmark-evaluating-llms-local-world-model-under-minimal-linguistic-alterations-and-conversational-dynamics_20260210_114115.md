---
ver: rpa2
title: 'PragWorld: A Benchmark Evaluating LLMs'' Local World Model under Minimal Linguistic
  Alterations and Conversational Dynamics'
arxiv_id: '2511.13021'
source_url: https://arxiv.org/abs/2511.13021
tags:
- conversation
- accuracy
- synthetic
- layers
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates whether large language models (LLMs) can maintain\
  \ robust local world models during dyadic conversations, even when the context undergoes\
  \ minimal linguistic alterations. To this end, it constructs PRAGWORLD, a benchmark\
  \ that applies seven types of lexical changes\u2014such as negation, variable swap,\
  \ and quantifier change\u2014to seed dialogues from the GRICE and CICERO datasets,\
  \ producing yes-no questions."
---

# PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics

## Quick Facts
- **arXiv ID:** 2511.13021
- **Source URL:** https://arxiv.org/abs/2511.13021
- **Reference count:** 35
- **Key outcome:** PRAGWORLD benchmark reveals LLMs struggle to maintain robust local world models under minimal linguistic alterations, with proposed layer-regularization techniques improving robustness.

## Executive Summary
PRAGWORLD evaluates whether large language models can maintain robust local world models during dyadic conversations, even when context undergoes minimal linguistic alterations. The benchmark applies seven types of lexical changes (negation, variable swap, etc.) to seed dialogues, producing yes-no questions to test entity tracking and world state updates. Testing a wide range of open- and closed-source LLMs reveals most models struggle with robust accuracy under such alterations, indicating fragility in world model maintenance. A dual-perspective interpretability framework identifies specific transformer layers that are either useful or harmful in processing altered contexts, with harmful layers often encoding spurious signals. Inspired by these insights, the authors propose layer-regularization techniques that suppress harmful effects, improving robustness.

## Method Summary
The PRAGWORLD benchmark constructs conversational contexts by applying seven types of minimal linguistic alterations (negation, variable swap, quantifier change, etc.) to seed dialogues from GRICE and CICERO datasets. These altered conversations are paired with yes-no questions, creating both manual (500 examples) and synthetic (2,114 examples) splits. Models are evaluated using greedy decoding (temperature 0.0, top_p 1.0) with Prompt 1. The primary metric is robust accuracy—correctness on both original and all altered variants. Interpretability analysis uses MLP zero-out ablation and direct effect patching to identify harmful and useful transformer layers. Mitigation strategies include fine-tuning with LoRA (rank 8, learning rate 1e-4, 3 epochs) and layer-regularized fine-tuning via Harmful Layer Suppression (HLS) and Useful Layer Amplification (ULA).

## Key Results
- Most LLMs achieve low robust accuracy (e.g., Phi-3.5-mini-instruct: 48.98%) despite high original/altered accuracy, indicating fragility to linguistic alterations.
- Interpretability analysis identifies specific transformer layers as harmful (e.g., layers 5, 6, 7, 11, 13, 17, 31 in Phi-3.5) that encode spurious signals affecting altered context processing.
- Layer-regularization techniques (HLS and ULA) improve robust accuracy across model families, with notable gains for Llama-3 and Qwen-2.5 series.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimal linguistic alterations expose systematic weaknesses in how LLMs encode and update entity states within local world models.
- **Mechanism:** The seven alteration types preserve surface form similarity while changing underlying truth conditions. Models relying on heuristics rather than precise state tracking fail to adjust internal representations, causing answer inconsistencies.
- **Core assumption:** LLMs encode conversational world states sensitive to lexical details; alterations changing semantics should propagate through representations and affect downstream decisions.
- **Evidence anchors:** Table 1 shows low robust accuracy across models; Phi-3.5-mini-ins achieves only 48.98% robust accuracy on manual split despite high original/altered accuracy. Related benchmarks (TurBLiMP, ZhoBLiMP) focus on minimal pairs but not conversational dynamics.

### Mechanism 2
- **Claim:** Dual-perspective interpretability framework identifies transformer layers contributing positively ("useful") or negatively ("harmful") to processing altered contexts.
- **Mechanism:** Direct effect patching isolates individual layer contributions by swapping residual activations between original and altered runs. MLP zero-out ablation measures accuracy change when a layer's feedforward submodule is disabled.
- **Core assumption:** Layer-wise contributions to final output are approximately separable and can be attributed to individual MLP or residual components.
- **Evidence anchors:** Figure 5 shows MLP zero-out at layers 2, 9, and 16 decreases accuracy (useful), while ablating layers 5, 6, 7, 11, 13, 17, and 31 improves accuracy (harmful). Logical Connective most impacted by harmful layers; Variable Swap least affected.

### Mechanism 3
- **Claim:** Layer-regularized fine-tuning techniques (ULA and HLS) improve robust accuracy by selectively amplifying beneficial layer outputs and suppressing harmful ones.
- **Mechanism:** ULA adds auxiliary classification heads to useful layers, encouraging task-relevant signal encoding. HLS applies L2 penalty to harmful layer activations, reducing their influence on residual stream.
- **Core assumption:** Layers identified via ablation/patching are causally responsible for robustness failures, and modifying their behavior will generalize to unseen alterations.
- **Evidence anchors:** All tuned models show improvements in robust accuracy, with notable gains for Llama-3 and Qwen-2.5 series (Table 2). HLS reduces accuracy for both useful and harmful layers compared to ULA.

## Foundational Learning

- **Concept: Local world model (in conversation)**
  - Why needed here: The benchmark assumes LLMs must maintain an implicit, updateable representation of entities, states, and relations described across dialogue turns to answer yes/no questions correctly.
  - Quick check question: In a dialogue where Alice says "I put the apples in the kitchen" and Bob later says "Actually, I moved them to the garage," what must a model's world model track to answer "Are the apples in the kitchen?" correctly?

- **Concept: Linguistic alteration types (negation, quantifier change, etc.)**
  - Why needed here: The seven alterations are the core intervention used to probe world model malleability. Understanding each type is essential to interpreting performance differences across categories.
  - Quick check question: Which alteration type is most likely to introduce scalar implicature effects that a pragmatic reasoner must resolve?

- **Concept: Transformer layer interpretability (patching, ablation)**
  - Why needed here: The dual-perspective framework uses causal interventions to attribute model behavior to specific layers. Understanding residual streams, MLP submodules, and patching mechanics is prerequisite to following the analysis.
  - Quick check question: In direct effect patching, what does it mean if patching layer L's residual from an altered run into the original run increases confidence in the altered answer?

## Architecture Onboarding

- **Component map:** Dataset construction (77 seed conversations → 7 alterations → manual/synthetic splits) → Baseline evaluation (robust accuracy, flip/invariant accuracy) → Interpretability analysis (MLP zero-out ablation, direct effect patching) → Mitigation strategies (HLS/ULA regularization) → Fine-tuning on synthetic split

- **Critical path:** 1) Select seed conversations and apply alterations → create PRAGWORLD benchmark 2) Evaluate baseline models → identify low robust accuracy and Yes/No gaps 3) Run interpretability analysis → identify harmful/useful layers for each model 4) Apply regularization → measure robust accuracy improvement

- **Design tradeoffs:** Manual vs. synthetic splits (quality vs. scale); Interpretability vs. scalability (full ablation is computationally expensive); Regularization strength (too strong may harm fluency, too weak may not suppress shortcuts effectively)

- **Failure signatures:** Large Yes/No accuracy gap indicates label bias (e.g., Llama-3.2-1B: 10.83% Yes vs. 95.96% No); Low robust accuracy despite high original/altered accuracy indicates fragility to alterations; Harmful layers that, when ablated, improve accuracy suggest spurious signal reliance

- **First 3 experiments:**
  1. **Baseline replication:** Run GPT-3.5 and two open-source models (Phi-3.5-mini, Llama-3.1-8B) on PRAGWORLD manual split → reproduce Table 1 robust accuracy and Yes/No gaps
  2. **Targeted ablation:** For Phi-3.5, perform MLP zero-out ablation on all layers → identify useful vs. harmful layers and correlate with alteration types (cf. Figure 4)
  3. **Regularization ablation study:** Apply HLS and ULA to Phi-3.5 with varying harmful/useful weight (1e-4, 1e-3, 1e-2) → measure robust accuracy tradeoff and compare to standard fine-tuning on synthetic split

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the application of layer-regularization techniques (HLS and ULA) to suppress harmful layers result in a trade-off with general language capabilities or performance on non-conversational benchmarks?
- **Basis in paper:** The authors propose layer-regularization strategies that improve robust accuracy on PRAGWORLD, but do not evaluate whether zeroing out or penalizing specific MLP layers degrades the model's broader knowledge base or fluency.
- **Why unresolved:** The evaluation is restricted to the PRAGWORLD benchmark and the specific task of Yes/No question answering. Suppressing layers identified as "harmful" for this specific task might remove features necessary for other tasks.
- **What evidence would resolve it:** A comparative evaluation of the HLS/ULA fine-tuned models against the baseline on standard benchmarks (e.g., MMLU, GSM8K) to check for capability degradation.

### Open Question 2
- **Question:** Do the "useful" and "harmful" layers identified via MLP zero-out ablation generalize across different model architectures, or are they specific to the Phi and Llama families tested?
- **Basis in paper:** The interpretability framework identifies specific layer indices (e.g., layers 5, 7, 11 in Phi-3.5) as harmful, but the paper does not determine if this is a universal phenomenon in Transformers or an artifact of specific training dynamics.
- **Why unresolved:** The paper analyzes a limited set of models (mostly Llama, Phi, and Qwen). Whether the "middle layers" are universally responsible for spurious correlations in entity tracking remains unknown.
- **What evidence would resolve it:** Applying the MLP zero-out ablation framework to diverse architectures (e.g., Mistral, Gemma, or older models like GPT-2) to see if the same layers consistently encode "spurious signals."

### Open Question 3
- **Question:** Can the improvements from training on the synthetic split be attributed to a robust update of the local world model, or do models simply overfit to the deterministic templates used for minimal lexical alterations?
- **Basis in paper:** The authors note that fine-tuning on the synthetic split improves robustness, but the synthetic data is generated using deterministic algorithms and GPT-4, which might lack the ambiguity and noise of the manual split.
- **Why unresolved:** It is unclear if the model learns a generalized principle of state tracking or if it learns the specific "rules" of the synthetic alteration algorithms.
- **What evidence would resolve it:** Testing the synthetic-fine-tuned models on a held-out set of human-generated conversations with natural, unscripted pragmatic errors or state changes to verify generalization.

## Limitations

- The harmful/useful layer identification is highly model- and checkpoint-specific; reported layer indices may not transfer across model versions or training runs, raising reproducibility questions.
- The regularization techniques improve robust accuracy on held-out alterations but may still rely on spurious shortcuts; long-term generalization to unseen linguistic variations remains unproven.
- The synthetic split's template-based question generation may not fully capture the pragmatic complexity of manual dialogues, potentially inflating synthetic vs. manual performance differences.

## Confidence

- **High confidence:** The core observation that LLMs struggle with robust accuracy under minimal linguistic alterations is well-supported by consistent performance gaps across multiple models and alteration types (e.g., Table 1 shows Phi-3.5-mini at 48.98% robust accuracy despite high original/altered accuracy).
- **Medium confidence:** The interpretability framework's identification of harmful/useful layers is methodologically sound, but the specific layer indices are model-dependent and require replication per model. The causal attribution of robustness failures to these layers is plausible but not definitively proven.
- **Low confidence:** The claim that HLS/ULA regularization "significantly improves" robustness is supported by Table 2 improvements, but the magnitude of gains varies widely (e.g., Qwen-2.5-72B shows modest 1.5% improvement), and the regularization may introduce trade-offs in fluency or generalization.

## Next Checks

1. **Replication of harmful/useful layer identification:** Run MLP zero-out ablation on a held-out model (e.g., Llama-3.1-70B) to verify that the same alteration types (negation, quantifier change) consistently map to harmful/useful layers, or if the pattern is model-specific.

2. **Generalization of regularization:** Apply HLS/ULA to a model trained on the synthetic split and evaluate on a new, unseen alteration type (e.g., "scalar implicature flip") to test whether improvements transfer beyond the benchmark's defined alterations.

3. **Comparison to alternative mitigation:** Replace the ULA/HLS regularization with a simpler approach (e.g., adversarial training on altered examples) and measure if robust accuracy improves comparably, isolating whether the gains are due to regularization or increased exposure to altered contexts.