---
ver: rpa2
title: 'HILGEN: Hierarchically-Informed Data Generation for Biomedical NER Using Knowledgebases
  and Large Language Models'
arxiv_id: '2503.04930'
source_url: https://arxiv.org/abs/2503.04930
tags:
- data
- umls
- synthetic
- biomedical
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HILGEN improves biomedical named entity recognition (NER) performance
  in few-shot learning settings by combining domain knowledge from the Unified Medical
  Language System (UMLS) with synthetic data generated by large language models (LLMs),
  specifically GPT-3.5. The approach leverages UMLS's hierarchical structure to expand
  training data with related concepts, while incorporating contextual information
  from LLMs through targeted prompts.
---

# HILGEN: Hierarchically-Informed Data Generation for Biomedical NER Using Knowledgebases and Large Language Models

## Quick Facts
- arXiv ID: 2503.04930
- Source URL: https://arxiv.org/abs/2503.04930
- Reference count: 0
- HILGEN achieves up to 42.29% average F1 score improvement in few-shot biomedical NER

## Executive Summary
HILGEN addresses the challenge of few-shot biomedical named entity recognition by combining structured domain knowledge from UMLS with synthetic data generation from large language models. The approach leverages UMLS's hierarchical structure to expand training data with related concepts while incorporating contextual diversity through GPT-3.5-generated examples. When evaluated across four biomedical datasets using BERT-Large and DANN models, HILGEN achieved significant performance improvements over baseline models, with the Best-Ensemble approach showing the highest gains.

## Method Summary
HILGEN combines two data augmentation strategies for few-shot biomedical NER. First, it uses UMLS's hierarchical structure to expand training data through three layers: synonyms sharing the same Concept Unique Identifier (CUI), concepts with the same semantic type, and parent-child-sibling relationships from SNOMEDCT_US. Second, it employs GPT-3.5 to generate synthetic sentences through targeted two-step prompts, converting outputs to IOB format. The method creates an ensemble combining models trained on both UMLS-based and LLM-based synthetic data using weighted voting and intersection methods.

## Key Results
- BERT-Large with UMLS incorporation: 40.36% average F1 improvement
- BERT-Large with GPT-3.5: 40.52% average F1 improvement
- Best-Ensemble BERT-Large: 42.29% average F1 improvement
- DANN models showed 22.74% (UMLS), 21.53% (GPT-3.5), and 25.03% (Best-Ensemble) average F1 improvements

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical knowledge expansion from UMLS enriches sparse entity representations by injecting semantically related, clinically validated concepts. UMLS provides three augmentation layers: (1) synonyms sharing the same Concept Unique Identifier (CUI), (2) concepts with the same semantic type (e.g., pharmacological substances), and (3) parent-child-sibling relationships from SNOMEDCT_US. These are substituted or added into training examples to expand lexical coverage without manual annotation.

### Mechanism 2
LLM-generated synthetic sentences provide contextual diversity that complements structured knowledge expansion. GPT-3.5 receives few-shot examples via a two-step prompt: (1) generate semantically similar sentences with varied expressions for the same entities, (2) convert outputs to IOB format. The quantity of generated examples is controlled to match UMLS-based augmentation, preventing data imbalance.

### Mechanism 3
Ensemble combination of UMLS-based and LLM-based synthetic data captures complementary signal sources, improving robustness. Weighted voting assigns higher influence to better-performing models; intersection methods accept only multi-model agreements. UMLS provides precision through curated hierarchy; GPT provides recall through contextual diversity.

## Foundational Learning

- **Few-shot Learning**: Why needed here - HILGEN targets scenarios with only 5 labeled examples per entity type; understanding meta-learning, metric learning, and prompt-based adaptation is prerequisite. Quick check question: Can you explain why prototypical networks struggle with lexical variants in biomedical text?

- **IOB Tagging Format**: Why needed here - Synthetic sentences must be converted to IOB format for NER model fine-tuning; errors here propagate to training. Quick check question: Given "aspirin treats headache," what are the correct IOB tags if both entities are labeled?

- **UMLS Structure (CUIs, Semantic Types, Hierarchies)**: Why needed here - Three-layer augmentation depends on understanding how CUIs group synonyms, how semantic types cluster concepts, and how SNOMEDCT_US parent-child relationships work. Quick check question: If a CUI has no parent in SNOMEDCT_US, which augmentation layers remain available?

## Architecture Onboarding

- **Component map**: Input (5-shot examples) -> Pipeline A (UMLS: CUI lookup → 3-layer expansion → synthetic examples) -> Pipeline B (GPT: prompting → IOB conversion) -> Merge (concatenate synthetic data) -> Training (BERT-Large/DANN) -> Ensemble (weighted voting/intersection)

- **Critical path**: UMLS expansion quality → prompt design for GPT → IOB format correctness → ensemble weight calibration

- **Design tradeoffs**: More related concepts increases coverage but raises computational cost and noise risk; basic prompts vs. advanced prompt engineering (paper acknowledges prompts were "relatively basic"); ensemble intersection improves precision but may reduce recall on rare entities

- **Failure signatures**: Low F1 on NCBI-Disease with GPT-only (suggests generated sentences lack disease-specific linguistic patterns); ZEROGEN-style generic outputs (repetitive sentence structures, missing multi-entity context); sibling relationship augmentation shows mixed results (may introduce less relevant concepts)

- **First 3 experiments**: 1) Baseline replication: Run 5-shot BERT-Large on BC5CDR without augmentation; confirm baseline F1 matches paper (~40-50% range implied by improvement magnitudes). 2) UMLS-only ablation: Implement Layer 1 (synonym) augmentation only; measure F1 delta; then add Layer 2 (semantic type); then Layer 3 (hierarchy). 3) Prompt sensitivity test: Vary the GPT prompt (basic vs. UMLS-anchored as described); compare IOB conversion accuracy and downstream F1 on Med-Mentions (where HILGEN showed strongest gains vs. GPT-only).

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced prompt engineering techniques yield higher quality synthetic data and improved NER performance compared to the fundamental prompting strategy currently employed by HILGEN? The authors explicitly state that "The prompts we used were relatively basic" and suggest exploring advanced prompt engineering techniques. The current study focused on establishing feasibility using simple prompts rather than optimizing prompt design.

### Open Question 2
How does increasing the few-shot sample size (e.g., to 10 or 20 examples) and the number of related UMLS concepts affect the trade-off between NER performance and computational cost? The authors acknowledge they used only a 5-shot setting and top 10 related concepts, hypothesizing that utilizing more annotated examples and concepts could yield superior results but entails additional computational costs.

### Open Question 3
Why do sibling relationships in the UMLS hierarchy produce inconsistent performance improvements across different biomedical datasets compared to parent-child relationships? The Results section notes that while hierarchical information generally helped, "The performance when using sibling relationships was somewhat mixed, with improvements in certain datasets but not consistently outperforming the other methods."

## Limitations
- Reliance on UMLS coverage means effectiveness is bounded by knowledge base comprehensiveness; entities not well-represented in UMLS will not benefit from hierarchical expansion
- GPT-3.5's tendency toward hallucination poses risks to synthetic data reliability, though UMLS anchoring may reduce but not eliminate this risk
- Ensemble weighting and intersection methods are not fully detailed, making exact replication difficult

## Confidence
- **High Confidence**: Core finding that combining UMLS-based and LLM-based synthetic data improves few-shot biomedical NER performance is well-supported by presented results across multiple datasets and models
- **Medium Confidence**: Claim that hierarchical knowledge expansion from UMLS enriches sparse entity representations is plausible given results, but exact contribution of each augmentation layer is not fully isolated
- **Medium Confidence**: Assertion that LLM-generated synthetic sentences provide contextual diversity is supported by results, but quality and semantic coherence remains a concern given GPT-3.5's known hallucination risks

## Next Checks
1. **Ablation Study on UMLS Augmentation Layers**: Implement and evaluate the three layers of UMLS augmentation (synonyms, semantic types, hierarchy) independently and in combination to quantify contribution of each layer to overall performance improvement

2. **Hallucination Detection in LLM-Generated Data**: Develop and apply a hallucination detection mechanism to assess semantic coherence and biomedical plausibility of GPT-3.5-generated synthetic sentences, and correlate hallucination rates with downstream NER performance

3. **Detailed Ensemble Method Analysis**: Fully specify the ensemble weighting scheme and intersection thresholds, then conduct experiments to determine optimal ensemble configuration and validate impact on NER performance across the four biomedical datasets