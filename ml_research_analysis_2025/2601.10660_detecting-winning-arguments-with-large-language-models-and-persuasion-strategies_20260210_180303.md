---
ver: rpa2
title: Detecting Winning Arguments with Large Language Models and Persuasion Strategies
arxiv_id: '2601.10660'
source_url: https://arxiv.org/abs/2601.10660
tags:
- message
- persuasion
- strategy
- strategies
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel approach for detecting persuasive
  arguments by leveraging large language models (LLMs) to score six specific persuasion
  strategies: Attack on Reputation, Distraction, Manipulative Wording, Simplification,
  Justification, and Call. The proposed Multi-Strategy Persuasion Scoring (MS-PS)
  framework first prompts LLMs to analyze each strategy''s presence in a message,
  then assigns a 1-10 persuasiveness score per strategy.'
---

# Detecting Winning Arguments with Large Language Models and Persuasion Strategies

## Quick Facts
- arXiv ID: 2601.10660
- Source URL: https://arxiv.org/abs/2601.10660
- Reference count: 40
- Primary result: MS-PS framework achieves 64.53% accuracy on Winning Arguments test set using OpenAI-o3

## Executive Summary
This paper introduces a novel approach for detecting persuasive arguments by leveraging large language models (LLMs) to score six specific persuasion strategies: Attack on Reputation, Distraction, Manipulative Wording, Simplification, Justification, and Call. The proposed Multi-Strategy Persuasion Scoring (MS-PS) framework first prompts LLMs to analyze each strategy's presence in a message, then assigns a 1-10 persuasiveness score per strategy. These scores are aggregated either by averaging or fed into a multilayer perceptron to predict the more persuasive message. Evaluated on three datasets—Winning Arguments, Anthropic/Persuasion, and Persuasion for Good—MS-PS significantly outperforms baseline methods, achieving up to 64.53% accuracy on the Winning Arguments test set.

## Method Summary
The MS-PS framework employs a two-step prompting approach where LLMs independently analyze each of six persuasion strategies (Attack on Reputation, Distraction, Manipulative Wording, Simplification, Justification, Call) in a message. For each strategy, the model first generates textual analysis of its presence, then assigns a 1-10 score. These scores are aggregated using either simple averaging (MS-PS-AVG) or fed into a multilayer perceptron (MS-PS-MLP) with additional features (mean, variance, entropy) to predict persuasiveness. The framework is evaluated on three datasets including the novel topic-annotated TWA dataset.

## Key Results
- MS-PS-AVG achieves 60.59% accuracy on Winning Arguments test set, outperforming baselines
- MS-PS-MLP variant reaches 64.53% accuracy with OpenAI-o3, showing statistically significant improvement over averaging
- Strategy effectiveness varies by topic: Attack on Reputation more effective in Food & Culture than Economics & Politics
- MS-PS achieves 0.722 Micro F1 vs. 0.664 for single-prompt classification on human-annotated SemEval 2023 data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing persuasion assessment into strategy-specific reasoning improves detection accuracy over monolithic evaluation.
- Mechanism: The model first generates textual analysis for each of six persuasion strategies independently, then assigns a 1-10 score grounded in that analysis. This two-step process anchors numerical judgments in explicit reasoning rather than direct scoring.
- Core assumption: LLMs produce more reliable strategy detection when prompted with definitions and asked to reason before scoring.
- Evidence anchors: Results show that strategy-guided reasoning improves the prediction of persuasiveness; MS-PS achieves 0.722 Micro F1 vs. 0.664 for single-prompt classification on SemEval 2023 human-annotated data.

### Mechanism 2
- Claim: Independent per-strategy prompts reduce cross-strategy interference and yield more interpretable representations.
- Mechanism: Unlike joint prompting approaches, MS-PS processes each strategy in isolation with dedicated prompts. This prevents the model from conflating strategies (e.g., confusing Simplification with Distraction) and produces a clean 6-dimensional score vector.
- Core assumption: Strategies are sufficiently distinct that independent assessment does not miss important interactions during the analysis phase.
- Evidence anchors: Our approach fully disentangles strategies by using independent prompts for each one... reduce cross-strategy interference and yield a more interpretable and expressive representation.

### Mechanism 3
- Claim: Learned non-linear aggregation of strategy scores captures complex persuasion patterns better than averaging.
- Mechanism: The MS-PS-MLP variant constructs a 9-dimensional feature vector (6 scores + mean, variance, entropy) and trains a multilayer perceptron to predict persuasiveness. This allows the model to weight strategies differently and capture interactions.
- Core assumption: The relationship between strategy scores and persuasiveness is non-linear and context-dependent.
- Evidence anchors: MS-PS-MLP achieves 64.53% (o3) vs. 60.59% for MS-PS-AVG, with statistically significant improvement for o3 (p=0.03).

## Foundational Learning

- Concept: **Positional bias in LLM comparison tasks**
  - Why needed here: Direct pairwise comparison prompts (Message 1 vs. Message 2) exhibit strong preference for the second position regardless of content (30-85% accuracy swing based on ordering; Appendix H.2). Understanding this failure mode is essential for designing evaluation frameworks.
  - Quick check question: If you swap the order of two messages in a direct comparison prompt, does the model's preference change?

- Concept: **Feature engineering for strategy score aggregation**
  - Why needed here: Beyond raw strategy scores, the paper uses mean, variance, and entropy to capture distributional properties. Understanding why these features help (capturing intensity, imbalance, and evenness) enables principled extension to other tasks.
  - Quick check question: What does high variance in strategy scores indicate about a message's persuasive profile?

- Concept: **Zero-shot vs. supervised aggregation tradeoffs**
  - Why needed here: MS-PS-AVG is zero-shot but assumes equal strategy weights; MS-PS-MLP is supervised and learns weights but requires labeled data. Choosing between them depends on data availability and interpretability needs.
  - Quick check question: In a new domain with no labeled persuasion data, which variant should you start with?

## Architecture Onboarding

- Component map: Input preprocessing -> Strategy analyzer (6 parallel prompts) -> Strategy scorer (6 parallel prompts) -> Aggregator (AVG or MLP) -> Prediction
- Critical path: Input → Strategy analysis (6 parallel prompts) → Strategy scoring (6 parallel prompts) → Feature vector construction → MLP inference → Prediction
- Design tradeoffs:
  - 1-10 scale validated against 1-5 and 1-7 (Appendix J); 1-10 captures more nuance but may introduce noise
  - Independent prompts increase API costs (12 calls per message pair) but reduce cross-strategy interference
  - MLP adds complexity and requires labeled data; AVG is simpler but assumes equal strategy importance
- Failure signatures:
  - Low inter-model agreement on specific strategies (e.g., Justification κ=0.098 between LLaMA and o3) suggests unreliable signals
  - Scores clustering at 6-8 range (Appendix P) may indicate insufficient discrimination
  - Models refusing to score due to safety filters (Appendix E); requires rephrasing fallback
- First 3 experiments:
  1. Replicate baseline comparison: Run Independent Scoring + Context on WA validation set with your target LLM; expect ~58-62% accuracy
  2. Ablate single strategy: Remove one strategy (e.g., Call) from MS-PS and measure accuracy drop to assess contribution
  3. Cross-domain transfer: Train MLP on WA training split, evaluate on Anthropic/Persuasion test split to probe generalization without fine-tuning

## Open Questions the Paper Calls Out

- Can topic-conditioned or domain-adaptive weighting of persuasion strategies improve detection accuracy compared to uniform aggregation?
- Why does the MLP aggregation significantly improve performance for OpenAI-o3 but not for other models, despite consistent improvements across all models?
- To what extent does MS-PS generalize to non-Western, non-Reddit cultural contexts where persuasion norms and strategies may differ?

## Limitations
- The six persuasion strategies are not empirically validated as exhaustive or optimally discriminative
- The effectiveness of MLP aggregation depends on quality and representativeness of training labels
- All datasets are English-language and Western-centric, limiting cross-cultural generalization

## Confidence

- **High confidence**: Decomposition of persuasion assessment into strategy-specific reasoning improves detection accuracy over direct comparison
- **Medium confidence**: Effectiveness of proposed strategy taxonomy for capturing persuasion
- **Medium confidence**: Non-linear aggregation approach (MS-PS-MLP) improves accuracy over averaging

## Next Checks

1. Ablate each strategy from MS-PS and measure accuracy degradation to quantify individual contributions
2. Train the MLP on Winning Arguments, then evaluate on Anthropic/Persuasion test set without fine-tuning to assess transfer capability
3. Conduct human evaluation of strategy detection by having annotators label strategy presence and comparing agreement with LLM outputs