---
ver: rpa2
title: Lane Change Intention Prediction of two distinct Populations using a Transformer
arxiv_id: '2509.06529'
source_url: https://arxiv.org/abs/2509.06529
tags:
- dataset
- which
- datasets
- prediction
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the transferability of lane change intention\
  \ prediction models across different driving populations. The authors train and\
  \ test transformer networks on two naturalistic driving datasets\u2014one from Germany\
  \ (exiD) and one from Hong Kong\u2014and find that models trained on one population\
  \ perform significantly worse when tested on the other (accuracy as low as 39.43%)."
---

# Lane Change Intention Prediction of two distinct Populations using a Transformer

## Quick Facts
- arXiv ID: 2509.06529
- Source URL: https://arxiv.org/abs/2509.06529
- Authors: Francesco De Cristofaro; Cornelia Lex; Jia Hu; Arno Eichberger
- Reference count: 18
- Key outcome: Multi-population training yields robust lane change prediction models, while single-population models fail on cross-population data (accuracy as low as 39.43%).

## Executive Summary
This paper investigates the transferability of lane change intention prediction models across different driving populations using transformer networks. The authors train and test models on two naturalistic driving datasets—one from Germany (exiD) and one from Hong Kong—and find that models trained on one population perform significantly worse when tested on the other. Even after controlling for traffic speed by filtering samples, performance differences persist, suggesting that driving style differences, not just traffic conditions, affect prediction accuracy. However, a transformer trained on a combination of both datasets performs well on each individually (accuracy up to 86.71%), indicating that multi-population training can yield robust, transferable models.

## Method Summary
The method employs a transformer encoder to classify lane change maneuvers using 2-second trajectory windows. Inputs are Frenet coordinates and relative positions of surrounding vehicles, converted from Cartesian coordinates using an SVM-generated reference path. The model uses sinusoidal positional encoding and multi-head attention to capture temporal relationships. Three training regimes are compared: single-population training (exiD or Hong Kong), and combined training on both datasets. The approach controls for traffic speed differences by filtering samples to overlapping velocity ranges (20-30 m/s) to isolate driving style effects.

## Key Results
- Single-population models fail on cross-population data: exiD-trained model achieves only 44.56% accuracy on Hong Kong test set, while Hong Kong-trained model achieves 39.43% on exiD
- Multi-population training yields robust performance: Combined model achieves 86.71% accuracy on exiD and 77.95% on Hong Kong
- Traffic speed differences alone don't explain performance gaps: Speed-filtered models still show poor transfer (41.30% and 23.37% accuracy respectively)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-population training enables cross-regional generalization for lane change prediction.
- Mechanism: The transformer learns a richer feature representation when exposed to diverse driving styles from both German and Hong Kong datasets, capturing both shared and population-specific patterns.
- Core assumption: The underlying physics of lane changes are similar across populations, but behavioral timing and contextual cues differ.
- Evidence anchors:
  - [abstract] "when trained on both populations simultaneously it could achieve an accuracy as high as 86.71%"
  - [section IV, Table III] Combined training yields 86.71% accuracy on exiD and 77.95% on Hong Kong
  - [corpus] Limited corpus evidence; neighbor papers focus on single-population scenarios without cross-transfer validation
- Break condition: If populations have fundamentally different road geometries, traffic rules, or vehicle dynamics (not tested here), multi-population training may not help.

### Mechanism 2
- Claim: Single-population models fail to generalize due to learned population-specific patterns that do not transfer.
- Mechanism: The attention mechanism learns population-specific correlations (e.g., velocity profiles, lane change timing) that are not present or are inverted in other populations.
- Core assumption: Driving behaviors are culturally/regionally patterned beyond just traffic conditions.
- Evidence anchors:
  - [abstract] "accuracy plummeted when tested on a population different to the one it was trained on with accuracy values as low as 39.43%"
  - [section IV, Table I] Trained on exiD, tested on Hong Kong: 44.56% accuracy; trained on Hong Kong, tested on exiD: 39.43% accuracy
  - [corpus] Neighbors do not address cross-population transfer; assumption remains unvalidated externally
- Break condition: If traffic speed distributions were the only difference, controlling for speed would eliminate the gap (it does not, see Mechanism 3).

### Mechanism 3
- Claim: Traffic speed differences alone do not explain cross-population performance gaps; driving style differences persist.
- Mechanism: Even after filtering samples to overlapping average longitudinal velocity ranges (20-30 m/s), performance gaps remain, suggesting deeper behavioral differences in how drivers initiate and execute lane changes.
- Core assumption: The 20-30 m/s velocity range captures comparable traffic conditions across both populations.
- Evidence anchors:
  - [section IV, Table II] Speed-filtered models still show poor transfer: 41.30% (exiD→Hong Kong) and 23.37% (Hong Kong→exiD)
  - [section IV] Notes different lane-change preferences in traffic jams between populations
  - [corpus] No corpus papers test this specific confound; assumption is internal to this study
- Break condition: If other unobserved confounds (road curvature, vehicle types, data collection methods) differ systematically between datasets, speed filtering may not isolate driving style.

## Foundational Learning

- Concept: **Frenet Coordinates**
  - Why needed here: Converts Cartesian (x, y) positions to longitudinal (s) and lateral (l) coordinates relative to a road-aligned reference path, essential for curved highways.
  - Quick check question: Given a reference path point (xr, yr) with tangent angle θr, how would you compute the lateral coordinate l for a vehicle at (x, y)?

- Concept: **Transformer Self-Attention for Time Series**
  - Why needed here: The model uses multi-head attention to capture temporal relationships in 2-second trajectory windows without recurrence.
  - Quick check question: Why might attention-based models handle variable-length dependencies differently than LSTMs for lane change prediction?

- Concept: **Cross-Dataset Domain Shift**
  - Why needed here: Understanding why models fail when deployed on populations different from training data is the core problem addressed.
  - Quick check question: What are three possible sources of domain shift between German and Hong Kong highway driving data?

## Architecture Onboarding

- Component map: Cartesian→Frenet conversion using SVM-generated reference paths -> Sample cutting/labeling with 2s observation, 4s prediction horizon -> Normalization and mean-centering of position features -> Transformer inference with softmax classification

- Critical path:
  1. Cartesian→Frenet conversion using SVM-generated reference paths
  2. Sample cutting/labeling with 2s observation, 4s prediction horizon
  3. Normalization and mean-centering of position features
  4. Transformer inference with softmax classification

- Design tradeoffs:
  - Single encoder layer reduces computational cost but may limit temporal abstraction depth
  - 16 attention heads provide diverse feature interactions but increase parameters
  - Balanced class sampling (LLC:RLC:LK = 1:1:2) trades natural distribution fidelity for training stability

- Failure signatures:
  - Low cross-population accuracy (<50%) suggests domain mismatch, not model capacity issues
  - High within-population accuracy but low cross-population accuracy indicates overfitting to population-specific cues
  - Asymmetric transfer (exiD→HK better than HK→exiD) may indicate dataset quality or complexity differences

- First 3 experiments:
  1. Replicate cross-population transfer: Train on exiD alone, test on Hong Kong, and vice versa. Confirm accuracy drops below 50%.
  2. Ablate speed confound: Filter both datasets to 20-30 m/s average velocity, retrain and test. Verify performance gap persists.
  3. Multi-population baseline: Combine exiD and Hong Kong training data, test on held-out subsets from each. Target >75% accuracy on both.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the performance discrepancies in cross-population prediction caused primarily by differences in driving style or by differences in road scenarios?
- Basis in paper: [explicit] The authors state, "Further investigations are needed to highlight if differences in the driving style or if differences in the scenarios are the cause of the lack in performances."
- Why unresolved: The study controlled for traffic speed (average longitudinal velocity), finding that differences persisted, but did not isolate other specific scenario or behavioral variables.
- What evidence would resolve it: An ablation study analyzing feature importance across populations or a study matching specific scenario types (e.g., curvature, traffic density) to disentangle the effects.

### Open Question 2
- Question: Is the prediction accuracy gap between the German and Hong Kong datasets caused by Chinese driving maneuvers being inherently harder to predict, or by the transformer architecture being biased toward German driving patterns?
- Basis in paper: [explicit] The authors question "if the architecture of the transformer... needs to be optimized differently depending on the population" or if trajectories are simply harder to predict.
- Why unresolved: The specific transformer configuration used was previously optimized on the highD dataset (German), potentially introducing a structural bias that disadvantages the Hong Kong data.
- What evidence would resolve it: Performing a separate hyperparameter or architecture search specifically for the Hong Kong data to determine if performance parity with the German model can be achieved.

### Open Question 3
- Question: Can the finding that multi-population training yields robust models be generalized to a wider variety of geographical regions and driving cultures?
- Basis in paper: [explicit] The conclusion requests that "Future research should test these conclusions on a greater and more varied amount of data."
- Why unresolved: The study was limited to two distinct populations (German and Hong Kong) and specific highway datasets; it is unknown if adding a third or fourth distinct dataset would maintain the same performance levels.
- What evidence would resolve it: Training the model on a combined dataset including additional regions (e.g., US or other Asian countries) and evaluating if the "single model" performance remains high across all subsets.

## Limitations
- The study identifies domain shift between populations but doesn't isolate specific behavioral or environmental factors beyond traffic speed
- The transformer architecture is relatively shallow (one encoder layer), and its capacity to capture complex cross-population patterns remains unclear
- Speed filtering approach is coarse and may not fully isolate driving style differences from other confounds

## Confidence
- High confidence: The empirical finding that single-population models fail on cross-population data (accuracy drops to 39-44%) is well-supported by the experimental results
- Medium confidence: The claim that multi-population training yields robust models (86.71% accuracy) is supported but relies on balanced sampling that may not reflect real-world distributions
- Low confidence: The assertion that traffic speed differences alone don't explain performance gaps, while plausible given the filtered results, lacks comprehensive control for other potential confounds

## Next Checks
1. Conduct a feature importance analysis to identify which input dimensions contribute most to cross-population prediction errors
2. Test the model on a third, independent dataset from a different region to verify the generalizability of the multi-population training approach
3. Implement an ablation study varying transformer depth and attention heads to determine if the architecture is the limiting factor in cross-population transfer