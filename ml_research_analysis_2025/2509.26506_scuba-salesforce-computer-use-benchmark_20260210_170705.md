---
ver: rpa2
title: 'SCUBA: Salesforce Computer Use Benchmark'
arxiv_id: '2509.26506'
source_url: https://arxiv.org/abs/2509.26506
tags:
- agents
- task
- agent
- salesforce
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCUBA, a benchmark for evaluating computer-use
  agents on customer relationship management workflows within the Salesforce platform.
  SCUBA contains 300 task instances derived from real user interviews across three
  personas - platform administrators, sales representatives, and service agents.
---

# SCUBA: Salesforce Computer Use Benchmark

## Quick Facts
- arXiv ID: 2509.26506
- Source URL: https://arxiv.org/abs/2509.26506
- Reference count: 29
- Success rate: Open-source models <5% zero-shot vs. closed-source models up to 39% on enterprise CRM tasks

## Executive Summary
This paper introduces SCUBA, a benchmark for evaluating computer-use agents on customer relationship management workflows within the Salesforce platform. SCUBA contains 300 task instances derived from real user interviews across three personas - platform administrators, sales representatives, and service agents. Tasks test abilities like UI navigation, data manipulation, workflow automation, and troubleshooting in realistic Salesforce sandbox environments.

Experiments benchmarked nine agents under zero-shot and demonstration-augmented settings. Open-source model-powered agents achieved less than 5% success rate in zero-shot settings, while closed-source model-based methods reached up to 39%. Demonstration augmentation improved success rates to 50% while reducing time and costs by 13% and 16% respectively. The benchmark reveals significant performance gaps between agent design paradigms and open vs closed-source models, highlighting challenges in enterprise task automation and demonstrating the effectiveness of demonstration-augmented approaches.

## Method Summary
SCUBA evaluates computer-use agents on Salesforce CRM workflows using a POMDP formulation where agents interact with sandbox environments through either browser-use (SOM+DOM observations with Playwright actions) or computer-use (screenshot observations with PyAutoGUI actions) frameworks. The benchmark contains 300 task instances across 60 templates spanning admin, sales, and service personas, with tasks ranging from simple data entry to complex workflow automation. Agents operate under a 50-step/90-minute budget, and performance is evaluated through milestone scoring against ground truth state using Salesforce Metadata/Tooling APIs. The benchmark supports both zero-shot and demonstration-augmented settings where human demonstration examples guide agent execution.

## Key Results
- Open-source model-powered agents achieved less than 5% success rate in zero-shot settings on enterprise CRM tasks
- Closed-source model-based methods reached up to 39% task success rate, demonstrating significant performance gaps
- Demonstration augmentation improved success rates to 50% while simultaneously reducing time and costs by 13% and 16% respectively
- Browser-use agents (SOM+DOM observations) achieved 51.33% success rate versus 39% for computer-use agents in zero-shot settings

## Why This Works (Mechanism)

### Mechanism 1: Demonstration-Augmented Task Guidance
- Claim: Providing human demonstration examples alongside task queries improves agent success rates while reducing time and costs.
- Mechanism: Demonstrations distill successful trajectories into actionable plans that provide domain-specific navigation patterns, UI element references, and task decomposition strategies that agents lack in zero-shot settings.
- Core assumption: Agents can generalize from similar or exact demonstration examples to new task instances when the demonstration retrieval is accurate.
- Evidence anchors:
  - [abstract] "In the demonstration-augmented settings, task success rates can be improved to 50% while simultaneously reducing time and costs by 13% and 16%, respectively."
  - [Section 4.3] "Demonstration is a effective way to improve performance metrics... It can consistently improve task success rates, while lowering time consumption, number of steps, token consumptions, and costs."
  - [corpus] UI-Evol paper identifies a "knowledge-execution gap" where "90% correct knowledge yields only 41% execution success rate," suggesting demonstration quality matters significantly.
- Break condition: Demonstrations may increase latency and costs when agents follow suboptimal human paths instead of discovering shortcuts, or when open-source models cannot effectively leverage demonstration content.

### Mechanism 2: Observation Space Design Determines Grounding Accuracy
- Claim: Browser-use agents with Set-of-Mark (SOM) + DOM text observations achieve higher success rates than screenshot-only computer-use agents due to more precise element grounding.
- Mechanism: SOM provides bounding boxes with numeric tags on interactive elements, reducing the grounding problem from continuous coordinate prediction to discrete element selection. DOM text supplies element attributes that disambiguate similar-looking UI components.
- Core assumption: The DOM parser correctly identifies all interactive elements including iframe and cross-origin content.
- Evidence anchors:
  - [Section 4.3] "The high task success rates seen in browser-use agents can be attributed to the combination of... richer observation space, and the more efficient action space design."
  - [Section 4.3] "Without these changes, agents run with browser-use would perform significantly worse compared to the number we reported."
  - [Appendix F] "Grounding is still a persistent issue for the computer-use agents" with coordinate prediction failures illustrated in Figure 8.
  - [corpus] Weak/missing: No direct corpus comparison of SOM vs. screenshot-only approaches.
- Break condition: DOM parsers fail to detect elements in complex enterprise UIs; the paper notes the "out-of-box DOM parser from browser-use cannot detect the add button (index 121)" making tasks impossible.

### Mechanism 3: Planning-Foundation Model Alignment
- Claim: Closed-source foundation models (GPT-5, Claude-4-sonnet, Gemini-2.5-pro) achieve substantially higher success rates than open-source models (UI-TARS-1.5-7B, OpenCUA-7B) on enterprise tasks.
- Mechanism: Stronger foundation models provide better planning, reasoning, and instruction-following capabilities that compound with agentic frameworks. Enterprise tasks require multi-step navigation across deeply nested UI hierarchies where planning errors cascade.
- Core assumption: Performance gaps reflect model capability differences rather than implementation artifacts.
- Evidence anchors:
  - [abstract] "Open-source model powered computer-use agents that have strong performance on related benchmarks like OSWorld only have less than 5% success rate on SCUBA, while methods built on closed-source models can still have up to 39% task success rate."
  - [Section 4.3] Figure 6 shows 27.8%-97.6% performance drops when moving from OSWorld to SCUBA across computer-use agents.
  - [corpus] OpenCUA paper discusses "closed" nature of leading CUA systems limiting reproducibility.
- Break condition: Open-source models with strong OSWorld performance may have overfit to benchmark-specific patterns; enterprise UI complexity exposes generalization gaps.

## Foundational Learning

- Concept: **Set-of-Mark (SOM) Prompting**
  - Why needed here: Core observation space for browser-use agents; understanding how visual grounding is achieved through numeric element tagging.
  - Quick check question: Given a webpage screenshot with SOM annotations, can you identify which element index corresponds to a "Submit" button based on the visual tags?

- Concept: **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper formalizes tasks as POMDPs (S, A, T, R, Î©, O); understanding this frames agent evaluation as sequential decision-making under uncertainty.
  - Quick check question: In the SCUBA formulation, what comprises the observation space O for browser-use vs. computer-use agents?

- Concept: **Process Reward / Milestone Scoring**
  - Why needed here: SCUBA evaluates partial task completion through milestone scores, not just binary success; this enables diagnosing where agents fail.
  - Quick check question: If an agent creates a queue with the correct name but fails to add members, what would the milestone score reflect vs. task success?

## Architecture Onboarding

- Component map:
  Environment Layer -> Task Layer -> Agent Layer -> Evaluation Layer -> Infrastructure Layer

- Critical path:
  1. Task initialization: Run prerequisite scripts (data upload, settings configuration)
  2. Agent execution: 50-step/90-minute budget with screenshot+action history as context
  3. State evaluation: Compare org state against ground_truth_dict via API queries
  4. Milestone scoring: Apply rubric weights to compute partial credit

- Design tradeoffs:
  - **Browser-use vs. Computer-use**: Browser-use offers higher success (51.33% vs. 39% zero-shot) with higher latency (19.31 min vs. 8.02 min) due to API overhead and multi-agent framework design
  - **Snapshot vs. container reset**: Sandbox orgs cannot be quickly recreated; configuration diff-and-revert approach required task-level knowledge of expected changes
  - **Exact vs. similar demonstration**: Exact demonstrations test instruction-following; similar demonstrations test generalization

- Failure signatures:
  - **Grounding failures**: Computer-use agents predict coordinates on element text rather than clickable area (Figure 8: click(378,693) lands on "Add" text, not button)
  - **Progress tracking failures**: Agents append failed actions to history and incorrectly believe subtasks completed (Figure 10)
  - **Language mismatch**: UI-TARS-1.5-7B outputs Q&A answers in Chinese characters, degrading performance
  - **DOM parser gaps**: Standard parsers miss enterprise-specific element types; custom iframe/cross-origin handling required

- First 3 experiments:
  1. **Baseline establishment**: Run browser-use GPT-5 on 20 tasks across admin/sales/service splits to establish expected success rates and latency baselines for your infrastructure
  2. **Demonstration ablation**: Compare zero-shot vs. exact-demonstration vs. similar-demonstration on a single template's 5 instances to measure demonstration utility before full benchmark runs
  3. **Grounding diagnosis**: Run Claude-4-sonnet in both browser-use and computer-use modes on identical tasks; log coordinate predictions vs. element indices to quantify grounding error rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agents effectively leverage unstructured knowledge articles and tutorials to improve performance when structured human demonstrations are unavailable?
- Basis in paper: [explicit] The conclusion states: "how to leverage the unstructured documents and tutorial to more effectively since structured human demonstration is not always available."
- Why unresolved: Experiments only tested structured demonstrations; unstructured tutorial documents were provided but not evaluated as a stand-alone augmentation strategy.
- What evidence would resolve it: Ablation experiments comparing agent performance with (a) no augmentation, (b) unstructured tutorials only, (c) structured demonstrations only, and (d) both combined.

### Open Question 2
- Question: What causes the severe generalization gap where agents performing well on OSWorld achieve less than 5% success on SCUBA?
- Basis in paper: [explicit] Figure 6 shows UI-TARS-1.5-7B dropping from ~10% to 2.67%, OpenCUA-7B from ~5% to 0.67%, and similar drops for other agents. The paper states this reveals generalization issues but does not isolate the cause.
- Why unresolved: The paper identifies planning and grounding as failure modes but does not quantify their relative contributions or explain why OSWorld capabilities do not transfer.
- What evidence would resolve it: Fine-grained error analysis categorizing failures by type (navigation, grounding, reasoning) on both benchmarks, plus domain adaptation experiments.

### Open Question 3
- Question: Would majority-voting or multi-prediction strategies for coordinate grounding significantly improve computer-use agent success rates?
- Basis in paper: [explicit] Appendix F states: "if the agents can make multiple predictions on the coordinates and use majority-voting approach, the grounding accuracy might also be improved" and cites Yang et al. (2025c), but this was not tested.
- Why unresolved: The paper identifies grounding as a persistent issue causing task failures but did not experimentally evaluate the proposed voting solution.
- What evidence would resolve it: Experiments comparing single-prediction vs. multi-prediction voting strategies on the same agent backbones, measuring grounding accuracy and end-to-end task success.

### Open Question 4
- Question: How can demonstration curation balance correctness with efficiency when human demonstrations are suboptimal?
- Basis in paper: [explicit] Appendix F, Observation 3 notes "Human Demonstration is correct yet not necessarily optimal" and shows agents following demonstrations took longer paths than zero-shot agents in some cases.
- Why unresolved: The paper observes the problem but does not propose or evaluate methods for optimizing or filtering demonstrations.
- What evidence would resolve it: Studies comparing raw human demonstrations against post-processed or model-optimized demonstrations on task success, latency, and cost metrics.

## Limitations

- **Sandbox Configuration Variability**: SCUBA uses Salesforce sandbox environments that require manual data uploads and configuration changes between task instances, but does not quantify environmental variability or its impact on reproducibility.
- **Demonstration Retrieval Quality**: The benchmark uses semantic search + exact match for demonstration retrieval but provides limited analysis of retrieval accuracy or cases where poor matching may harm performance.
- **Closed-Source Model Opacity**: Performance gaps between closed-source (39% success) and open-source (<5% success) models may reflect framework design choices as much as model capability differences.

## Confidence

- **High Confidence**: The benchmark architecture and evaluation methodology are well-specified with clear POMDP formulation, milestone scoring rubrics, and reproducible agent implementations.
- **Medium Confidence**: Claims about observation space superiority (SOM+DOM vs screenshots) are supported by performance data but lack direct ablation studies isolating the observation mechanism.
- **Low Confidence**: The generalizability of results to other enterprise platforms beyond Salesforce is uncertain, as the benchmark's task templates and DOM structure are specifically designed for Salesforce UI patterns.

## Next Checks

1. **Cross-Sandbox Consistency Test**: Run identical task instances across three different Salesforce sandbox configurations to quantify environmental variability and establish confidence intervals for reported success rates.

2. **Open-Source Framework Parity**: Implement the closed-source model agents (GPT-5, Claude-4-sonnet) within the browser-use framework to isolate whether performance gaps stem from model capability versus framework design choices.

3. **Demonstration Retrieval Analysis**: Instrument the demonstration retrieval pipeline to log semantic search rankings, exact match hits, and agent performance per demonstration quality tier to identify whether retrieval failures contribute to performance variance.