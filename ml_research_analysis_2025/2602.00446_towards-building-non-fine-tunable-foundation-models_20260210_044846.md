---
ver: rpa2
title: Towards Building Non-Fine-Tunable Foundation Models
arxiv_id: '2602.00446'
source_url: https://arxiv.org/abs/2602.00446
tags:
- fine-tuning
- mask
- pre-training
- unauthorized
- ticket
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of building non-fine-tunable\
  \ foundation models\u2014models that maintain high performance in their base form\
  \ while limiting the effectiveness of unauthorized fine-tuning. The authors propose\
  \ Private Mask Pre-Training (PMP), a method that concentrates representation learning\
  \ into a sparse subnetwork identified early in training, keeping the mask private\
  \ while releasing only dense weights."
---

# Towards Building Non-Fine-Tunable Foundation Models

## Quick Facts
- arXiv ID: 2602.00446
- Source URL: https://arxiv.org/abs/2602.00446
- Reference count: 8
- This paper proposes Private Mask Pre-Training (PMP) to create foundation models that maintain base performance while limiting unauthorized fine-tuning effectiveness.

## Executive Summary
This paper addresses the challenge of building non-fine-tunable foundation models—models that maintain high performance in their base form while limiting the effectiveness of unauthorized fine-tuning. The authors propose Private Mask Pre-Training (PMP), a method that concentrates representation learning into a sparse subnetwork identified early in training, keeping the mask private while releasing only dense weights. Without access to the mask, unauthorized fine-tuning updates parameters misaligned with the pre-training geometry, destabilizing adaptation. Theoretical analysis shows this induces an optimization mismatch that limits fine-tuning gains. Experiments on GPT-2 and TinyLlama demonstrate that PMP preserves base model performance on GLUE tasks while consistently degrading unauthorized fine-tuning outcomes. Authorized fine-tuning with the private mask mitigates this degradation and often outperforms unauthorized fine-tuning. The non-fine-tunability is robust across learning rates, fine-tuning durations, and model architectures. Overall, PMP provides a pretraining-level mechanism for inducing non-fine-tunability without task-specific assumptions, supporting controlled open model deployment.

## Method Summary
Private Mask Pre-Training (PMP) is a three-stage method that creates non-fine-tunable foundation models. In Stage I, early-bird mask discovery identifies a sparse subnetwork during warm-up training using gradient magnitude statistics. In Stage II, pre-training proceeds with gradient updates restricted only to parameters in the discovered mask while freezing others. In Stage III, only dense weights are released without the mask information. The key insight is that unauthorized fine-tuning without the mask operates on an optimization objective that differs fundamentally from pre-training, causing instability. Theoretical analysis shows this creates an optimization mismatch that limits fine-tuning gains. The method preserves base model capability while degrading unauthorized adaptation, with authorized fine-tuning (using the private mask) recovering performance.

## Key Results
- PMP preserves base model performance on GLUE tasks while consistently degrading unauthorized fine-tuning outcomes
- Non-fine-tunability is robust across learning rates, fine-tuning durations, and model architectures (GPT-2, TinyLlama)
- Authorized fine-tuning with the private mask mitigates degradation and often outperforms unauthorized fine-tuning
- Early-bird ticket selection outperforms random masks for both base performance and non-fine-tunability

## Why This Works (Mechanism)

### Mechanism 1: Sparse Subnetwork Concentration via Early-Bird Tickets
Concentrating representation learning into a sparse subnetwork identified early in training preserves base model capability while creating structural asymmetry. During a short warm-up phase, gradient magnitudes are used to identify a stable sparse mask (top-ρ parameters globally). This mask defines a "lottery ticket" subnetwork. All subsequent pre-training updates are restricted to θ_M (masked parameters) while θ_M̄ (complement) remains frozen at initialization. The forward pass still uses full dense weights, but learning concentrates into the selected subspace. Core assumption: Early-bird lottery tickets exist and capture effective subnetworks before full training. Evidence anchors: Abstract states concentration of learning into sparse subnetworks; equations formalize mask discovery and restricted gradient updates. Break condition: If mask discovery fails to stabilize (IoU threshold not met), or if the selected subnetwork is too small to represent the pre-training task, base performance degrades.

### Mechanism 2: Optimization Objective Mismatch
Unauthorized fine-tuning without the mask optimizes a fundamentally mismatched objective, inducing destabilizing perturbations along unadapted parameter directions. Pre-training optimizes L_pre(θ_M, θ_M̄(0)) with θ_M̄ frozen. Unauthorized fine-tuning optimizes L_ft(θ_M, θ_M̄) jointly. At the conditional optimum (θ*_M, θ_M̄(0)), the Hessian block for θ_M̄ exhibits positive curvature (never adapted). When fine-tuning updates θ_M̄, the quadratic term in the Taylor expansion contributes positively, destabilizing the solution. Core assumption: Asymmetric local geometry—flat along θ_M directions, sharp along θ_M̄ directions. Evidence anchors: Proposition 1 shows expected loss increases when updating mismatched directions; loss landscape visualization shows steep barriers along unauthorized directions. Break condition: If the frozen complement θ_M̄ happens to be near-optimal for downstream tasks, or if curvature is flat in θ_M̄ directions, the mismatch effect weakens.

### Mechanism 3: Gradient Distribution Obfuscation
The private mask cannot be reliably reverse-engineered from gradient observations alone due to overlapping gradient magnitude distributions. Masked and unmasked parameters produce gradient magnitude distributions with substantial overlap. Without prior knowledge of mask ratio or distributional priors, gradient magnitudes lack semantic signal for parameter-wise mask inference. Core assumption: Adversary has no prior knowledge of mask distribution, ratio, or gradient magnitude statistics. Evidence anchors: Figure 6 shows overlapping gradient magnitude distributions; text states substantial overlap prevents reliable parameter-wise mask detection. Break condition: If an adversary obtains multiple gradient observations across diverse inputs, or has prior knowledge of the training distribution, statistical attacks may become feasible.

## Foundational Learning

- **Lottery Ticket Hypothesis**:
  - Why needed here: PMP relies on the existence of sparse subnetworks that can match full-model performance. Without this property, restricting training to a subset would degrade base capability.
  - Quick check question: Can you explain why early-bird tickets stabilize before full convergence, and how IoU stability is used as a criterion?

- **Hessian Geometry and Loss Curvature**:
  - Why needed here: The theoretical argument hinges on asymmetric curvature—flat in optimized directions, sharp in frozen directions. Understanding second-order optimization geometry is essential.
  - Quick check question: In a second-order Taylor expansion of a loss function, how does positive curvature along a direction affect the expected loss when updating that direction?

- **Gradient Masking and Sparsity**:
  - Why needed here: The method uses global top-k gradient-based masking. Understanding how gradient magnitude relates to parameter importance informs why this selects effective subnetworks.
  - Quick check question: Why might gradient magnitude be a reasonable proxy for parameter importance during early training?

## Architecture Onboarding

- **Component map**: Warm-up pre-training -> Early-bird mask discovery -> Private mask pre-training -> Release dense weights -> Downstream fine-tuning (authorized/unauthorized)

- **Critical path**:
  1. Run warm-up pre-training for t_EB iterations (paper uses 500)
  2. At each step, compute element-wise absolute gradients; construct candidate mask via global top-k
  3. Check IoU stability across consecutive masks (threshold 0.99 for 5 steps in paper)
  4. Fix mask and proceed with masked pre-training
  5. At release, strip mask metadata; distribute weights only

- **Design tradeoffs**:
  - Mask ratio ρ: Higher ρ (e.g., 0.9) preserves base performance but weakens non-fine-tunability; lower ρ (e.g., 0.5) strengthens suppression but risks base degradation. Paper finds ρ=0.7 as a balance.
  - Warm-up duration: Too short may yield unstable masks; too long wastes compute before mask selection.
  - Early-bird vs random mask: Early-bird improves both base performance and non-fine-tunability (Figure 2).

- **Failure signatures**:
  - Base performance drops significantly: Mask ratio too low, or warm-up insufficient for stable ticket discovery
  - Fine-tuning not suppressed: Mask ratio too high (near 1.0), or adversary somehow recovers mask information
  - Authorized fine-tuning doesn't improve over unauthorized: Mask not correctly applied during fine-tuning, or downstream task is too dissimilar from pre-training

- **First 3 experiments**:
  1. **Mask ratio sweep**: Pre-train with PMP at ρ ∈ {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. Report base GLUE scores and fine-tuned scores. Expect U-shaped tradeoff with optimal near 0.7.
  2. **Early-bird vs random mask ablation**: Compare PMP with early-bird mask vs. random mask at same sparsity. Verify that early-bird yields better base performance and stronger suppression (replicate Figure 2).
  3. **Loss landscape visualization**: Interpolate along authorized (masked) and unauthorized (full-space) directions from the converged point. Plot loss curves to confirm steep barriers along unauthorized directions (replicate Figure 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversaries with query access infer or approximate the private mask through systematic probing, multiple gradient observations, or model behavior analysis?
- Basis in paper: Section 5.2 states "without prior knowledge of the mask distribution, an adversary cannot effectively detect or exploit the private mask" but only considers single-observation gradient analysis.
- Why unresolved: The defense assumes adversaries lack distributional priors, but determined attackers could potentially accumulate information across multiple queries or compare model behaviors across inputs.
- What evidence would resolve it: Security evaluations against adaptive adversaries attempting mask inference via (1) statistical analysis across many gradient samples, (2) probing loss landscape geometry, or (3) membership inference-style attacks on parameter subsets.

### Open Question 2
- Question: Does PMP scale effectively to much larger foundation models (7B+ parameters) and multimodal architectures while maintaining the balance between base performance and fine-tuning suppression?
- Basis in paper: Experiments are limited to GPT-2 and TinyLlama-1.1B; scaling behavior of lottery ticket discovery and the optimization mismatch effect in larger models remains unexplored.
- Why unresolved: Early-bird ticket discovery may behave differently at scale, and the curvature effects that cause instability could weaken or intensify in overparameterized regimes.
- What evidence would resolve it: Experiments applying PMP to Llama-7B/13B or multimodal models, reporting base performance retention and fine-tuning suppression rates compared to smaller-scale results.

### Open Question 3
- Question: How sensitive is PMP to variations in the early-bird mask discovery phase, and can alternative subnetwork selection criteria improve the capability preservation vs. non-fine-tunability trade-off?
- Basis in paper: Figure 2 compares EarlyBird vs. random masks, showing EarlyBird improves both metrics, but other selection strategies (magnitude-based, gradient-free, iterative) remain unexplored.
- Why unresolved: The current method relies on gradient-magnitude-based early-bird tickets; whether this is optimal or merely sufficient is unknown.
- What evidence would resolve it: Systematic comparison of mask discovery algorithms (magnitude pruning, SNIP, GraSP, iterative pruning) measuring both base GLUE scores and fine-tuning degradation rates.

## Limitations
- Security claims around gradient distribution obfuscation lack empirical validation against active adversaries
- Theoretical analysis assumes quadratic loss approximations near the optimum, which may not hold for complex foundation model objectives
- Warm-up mask discovery mechanism depends critically on early-bird ticket existence, which is not independently verified for the specific model architectures used

## Confidence
- **High confidence**: Base model performance preservation, optimization objective mismatch theory, authorized fine-tuning effectiveness
- **Medium confidence**: Non-fine-tunability robustness across architectures, gradient obfuscation security claim
- **Low confidence**: Long-term effectiveness against adaptive adversaries, generalization to larger model scales beyond tested sizes

## Next Checks
1. Conduct gradient-based mask inference attacks using multiple gradient observations across diverse inputs to test the practical security of the obfuscation mechanism
2. Evaluate PMP on larger foundation models (7B+ parameters) to verify scalability of the non-fine-tunability effect
3. Test robustness against gradient-accumulation-based attacks where adversaries combine observations across multiple batches to recover mask information