---
ver: rpa2
title: 'StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold'
arxiv_id: '2510.01938'
source_url: https://arxiv.org/abs/2510.01938
tags:
- stella
- lora
- learning
- low-rank
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "StelLA introduces a geometry-aware extension of LoRA that uses\
  \ a three-factor decomposition USV\u22A4, where U and V are constrained to lie on\
  \ the Stiefel manifold to ensure orthonormality during training. This approach separates\
  \ input and output subspaces from the scaling factor S, allowing for principled\
  \ subspace learning."
---

# StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold

## Quick Facts
- **arXiv ID**: 2510.01938
- **Source URL**: https://arxiv.org/abs/2510.01938
- **Reference count**: 40
- **Primary result**: Achieves up to +1.3 accuracy points on commonsense reasoning, +2.33 on math and code generation, +0.25 on image classification, and 7.11 point FID reduction for text-to-image generation

## Executive Summary
StelLA introduces a geometry-aware extension of LoRA that uses a three-factor decomposition USV⊤, where U and V are constrained to lie on the Stiefel manifold to ensure orthonormality during training. This approach separates input and output subspaces from the scaling factor S, allowing for principled subspace learning. The method employs a flexible geometric optimization algorithm that converts Euclidean optimizers to Riemannian ones, maintaining compatibility with existing fine-tuning pipelines.

Empirical results across diverse tasks including commonsense reasoning, math and code generation, image classification, and text-to-image generation demonstrate consistent improvements over state-of-the-art LoRA variants. Notably, StelLA achieves up to +1.3 accuracy points on commonsense reasoning, +2.33 on math and code generation, +0.25 on image classification, and a 7.11 point reduction in FID for text-to-image generation. The approach shows particular effectiveness in generative tasks and maintains strong performance across various model architectures.

## Method Summary
StelLA extends LoRA through a three-factor decomposition (USV⊤) where U and V are constrained to the Stiefel manifold to preserve orthonormality during training. The method uses a flexible geometric optimization framework that converts standard Euclidean optimizers into Riemannian optimizers, enabling the use of existing optimization techniques while respecting the manifold structure. This architecture separates input and output subspaces from the scaling factor S, providing more principled subspace learning. The approach maintains compatibility with existing fine-tuning pipelines while achieving superior performance across diverse tasks including commonsense reasoning, math and code generation, image classification, and text-to-image generation.

## Key Results
- Achieves up to +1.3 accuracy points on commonsense reasoning tasks
- Demonstrates +2.33 improvement on math and code generation benchmarks
- Shows 7.11 point reduction in FID for text-to-image generation tasks
- Consistently outperforms state-of-the-art LoRA variants across all tested tasks

## Why This Works (Mechanism)
StelLA's effectiveness stems from the geometric constraints imposed by the Stiefel manifold, which ensure orthonormality during training. By separating the input and output subspaces (U and V) from the scaling factor (S), the method enables more principled subspace learning compared to traditional LoRA's single low-rank decomposition. The geometric optimization algorithm converts Euclidean optimizers to Riemannian ones, allowing the model to maintain compatibility with existing fine-tuning pipelines while respecting the manifold structure. This separation of concerns allows for more efficient parameter utilization and better generalization across diverse tasks, particularly in generative scenarios where maintaining orthogonality is crucial for preserving information flow.

## Foundational Learning

**Stiefel Manifold**: A set of matrices with orthonormal columns, essential for maintaining orthogonality constraints during training. Needed because standard LoRA can suffer from orthonormality issues during fine-tuning. Quick check: Verify that U and V matrices maintain orthonormal properties throughout training.

**Riemannian Optimization**: Optimization techniques that respect the geometric structure of manifolds rather than treating parameters as Euclidean vectors. Required to properly update parameters on the Stiefel manifold. Quick check: Confirm gradient updates preserve manifold constraints.

**Low-rank Adaptation**: Parameter-efficient fine-tuning approach that approximates weight updates using low-rank matrices. Fundamental baseline that StelLA extends. Quick check: Compare computational efficiency against standard LoRA.

**Geometric Optimization Conversion**: Algorithm that transforms Euclidean optimizers into their Riemannian counterparts. Critical for maintaining compatibility with existing optimization frameworks. Quick check: Validate that converted optimizers produce meaningful updates on the manifold.

## Architecture Onboarding

**Component Map**: Input embeddings -> U matrix (Stiefel manifold) -> S scaling matrix -> V⊤ matrix (Stiefel manifold) -> Output embeddings. The three-factor decomposition USV⊤ replaces LoRA's single low-rank matrix.

**Critical Path**: Forward pass through the three-factor decomposition, where maintaining orthonormality of U and V matrices is crucial for preserving information flow and achieving optimal performance.

**Design Tradeoffs**: The geometric constraints ensure orthonormality but introduce computational overhead compared to standard LoRA. The three-factor decomposition provides more flexibility but requires careful optimization on the Stiefel manifold.

**Failure Signatures**: Performance degradation if orthonormality constraints are violated, suboptimal results if the geometric optimizer fails to properly convert Euclidean updates, and potential incompatibility with certain model architectures if manifold constraints are too restrictive.

**First Experiments**:
1. Validate orthonormality preservation of U and V matrices during initial training steps
2. Compare training stability and convergence speed against standard LoRA on a simple classification task
3. Test compatibility with different optimizer configurations (Adam, SGD) after geometric conversion

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Computational overhead introduced by geometric optimization algorithm compared to standard LoRA implementations
- Scalability to extremely large models beyond those tested remains unverified
- Potential interactions with other parameter-efficient fine-tuning techniques are not explored

## Confidence

**High Confidence**: The empirical results showing consistent improvements across diverse tasks are well-supported by the presented data. The quantitative metrics (accuracy gains, FID reduction) appear robust and reproducible based on the methodology described.

**Medium Confidence**: The theoretical justification for using Stiefel manifold constraints to ensure orthonormality during training is sound, but the practical implications for different model architectures and task types warrant further exploration.

**Medium Confidence**: While the separation of input/output subspaces from scaling factors is an elegant design choice, its actual impact on downstream performance may vary depending on specific fine-tuning scenarios.

## Next Checks

1. **Computational Efficiency Analysis**: Conduct a detailed benchmarking study comparing training time and memory usage of StelLA against standard LoRA across various model sizes and batch configurations

2. **Architecture Transferability**: Test StelLA's performance on additional model architectures including encoder-only transformers and speech models to validate generalizability claims

3. **Cross-technique Compatibility**: Evaluate the method's performance when combined with other PEFT techniques like prefix tuning or adapter layers to assess integration capabilities