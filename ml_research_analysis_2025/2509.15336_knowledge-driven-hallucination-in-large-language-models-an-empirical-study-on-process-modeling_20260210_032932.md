---
ver: rpa2
title: 'Knowledge-Driven Hallucination in Large Language Models: An Empirical Study
  on Process Modeling'
arxiv_id: '2509.15336'
source_url: https://arxiv.org/abs/2509.15336
tags:
- process
- language
- standard
- llms
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical reliability issue in large language
  models (LLMs) called knowledge-driven hallucination, where models prioritize their
  internal knowledge over explicit evidence in prompts. The study investigates this
  phenomenon in business process modeling, where processes often follow standard patterns,
  making LLMs likely to have strong pre-trained schemas.
---

# Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling

## Quick Facts
- **arXiv ID**: 2509.15336
- **Source URL**: https://arxiv.org/abs/2509.15336
- **Reference count**: 30
- **Primary result**: LLMs consistently revert to standard process models even when given conflicting evidence, demonstrating knowledge-driven hallucination that prompt engineering can only partially mitigate.

## Executive Summary
This paper identifies a critical reliability issue in large language models called knowledge-driven hallucination, where models prioritize their internal knowledge over explicit evidence in prompts. The study investigates this phenomenon in business process modeling, where processes often follow standard patterns, making LLMs likely to have strong pre-trained schemas. A controlled experiment was conducted using standard, reversed, and shuffled process models to create deliberate conflicts between provided evidence and the model's background knowledge. The results show that LLMs consistently revert to standard process models even when given conflicting evidence, demonstrating a significant tendency for knowledge-driven hallucination. While prompt engineering can mitigate this issue to some extent, it does not eliminate it, highlighting the need for rigorous validation of AI-generated artifacts in evidence-based domains.

## Method Summary
The study uses zero-shot inference with 10 LLMs to generate formal business process models (Petri nets/BPMN) from textual descriptions or event logs. The experimental setup creates conflicts between input evidence and the model's internal knowledge by using standard, reversed, and shuffled versions of four business processes. Generated models are evaluated using behavioral-footprint similarity scores (0.0 to 1.0) computed via PM4Py, measuring similarity between generated models and ground truth variants. Two prompt conditions are tested: Standard Prompt versus Strict Adherence Prompt. The methodology relies on the ProMoAI framework for converting text to POWL/Petri nets and uses a controlled artifact generation pipeline to isolate knowledge-driven hallucination effects.

## Key Results
- LLMs generated models matching the standard process (M+) 72% of the time when given reversed evidence (D-), while only matching the reversed ground truth (M-) 16% of the time
- Strict adherence prompts reduced but did not eliminate the tendency to revert to standard models (M+ similarity decreased to 13%, but remained higher than M- at 10%)
- Structured inputs like event logs produced fewer hallucinations than unstructured text descriptions, suggesting stronger evidence constraints

## Why This Works (Mechanism)

### Mechanism 1: Prioritized Schema Activation
- Claim: LLMs prioritize generalized internal knowledge (schemas) over explicit prompt evidence when the two conflict, particularly in domains with standardized patterns like Business Process Management.
- Mechanism: The model retrieves strong pre-trained associations for common patterns during decoding. These high-confidence internal representations override the lower-confidence signal from the provided context, effectively "correcting" the input to match the expected norm.
- Core assumption: The model has acquired robust, generalized schemas during pre-training that are more strongly weighted than the attention mechanism's adherence to specific, atypical inputs.
- Evidence anchors:
  - [abstract] "...a phenomenon where the model’s output contradicts explicit source evidence because it is overridden by the model’s generalized internal knowledge."
  - [section 1] "...revert to standard process models even when given conflicting evidence..."
  - [corpus] "KaLM" (arXiv:2412.04948) highlights that autoregressive models often struggle with knowledge-driven tasks without alignment.

### Mechanism 2: Mitigation via Explicit Attention Re-weighting
- Claim: Strict adherence prompts can partially shift the model's attention weights to prioritize the input context, reducing but not eliminating hallucinations.
- Mechanism: Explicit instructions likely amplify the attention scores assigned to the input tokens relative to the positional embeddings or internal state biases derived from pre-training.
- Core assumption: The instruction-following capabilities (alignment) are flexible enough to modulate the influence of pre-trained knowledge representations.
- Evidence anchors:
  - [section 4.1] "...strict adherence prompt reduced these numbers to 13 and 10, respectively... inability to fully resolve the problem underscores how deeply ingrained the model’s background knowledge is."
  - [abstract] "While prompt engineering can mitigate this issue to some extent, it does not eliminate it..."

### Mechanism 3: Input Structure Fidelity
- Claim: Structured inputs (event logs) provide stronger "evidence" constraints than unstructured text, reducing the frequency of knowledge-driven hallucinations.
- Mechanism: Event logs provide a sequence of explicit, timestamped facts that are harder to reinterpret than ambiguous natural language.
- Core assumption: The model treats the structured log abstraction as a higher-fidelity constraint than semantic text.
- Evidence anchors:
  - [section 4.1] "We observed fewer hallucinations when models were generated from event logs compared to textual descriptions... structured and unambiguous format... serves as stronger evidence."

## Foundational Learning

- **Concept**: **Behavioral Footprint Similarity**
  - Why needed here: This is the evaluation metric used to determine if the model "hallucinated." It measures the similarity of the generated process model to the ground truth based on order relations rather than just graph structure.
  - Quick check question: If Model A has the same nodes as Model B but a reversed sequence, would their footprint similarity be high or low?

- **Concept**: **Process Model Schemas (M+, M-, M*)**
  - Why needed here: The paper relies on creating "conflicting" inputs. Understanding that M- is a causally inverted version of the standard (M+) is essential to grasp the experimental conflict.
  - Quick check question: Why is the "Shuffled" model (M*) used in addition to the "Reversed" model (M-)?

- **Concept**: **Knowledge-Driven vs. Factuality Hallucination**
  - Why needed here: Standard hallucination checks often look for "made up" facts. Here, hallucination is defined as ignoring instructions in favor of internal knowledge.
  - Quick check question: Can a model hallucinate even if every token it generates is factually true in a general sense?

## Architecture Onboarding

- **Component map**: Artifact Generator -> ProMoAI Framework -> PM4Py Evaluator
- **Critical path**: The most critical step for an engineer to reproduce is the **Artifact Generation**. Ensuring that the "Reversed" text (D-) logically contradicts the "Standard" (M+) without introducing ambiguity is the only way to isolate knowledge-driven hallucination from simple misunderstanding.
- **Design tradeoffs**: The authors chose **semantic similarity** over **formal conformance checking** (Page 7). This trades absolute correctness verification for a relative comparison that is easier to automate and sufficient for identifying which "version" (Standard vs. Reversed) the model favored.
- **Failure signatures**:
  - **False Positive Hallucination**: The model generates a perfect standard process (M+) when fed reversed evidence (D-), resulting in high similarity to M+ and low similarity to M-.
  - **Plausibility Trap**: The output is syntactically correct and logically sound, but it is semantically wrong relative to the input.
- **First 3 experiments**:
  1. **Baseline Reversal**: Run a standard process (e.g., Sales Order) through the LLM. Then run the Reversed text (D-) with a standard prompt. Verify the model incorrectly generates the standard process (M+).
  2. **Strict Adherence Test**: Rerun the Reversal experiment with the "Strict Adherence" prompt modification. Measure the delta in similarity scores (does it shift toward M-?).
  3. **Log vs. Text Comparison**: Feed the same process information as both a text description and an event log abstraction. Compare which input format yields higher fidelity to the reversed (M-) ground truth.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can architectural modifications or fine-tuning strategies mitigate knowledge-driven hallucination more effectively than prompt engineering?
  - Basis in paper: [explicit] The conclusion states the need to "develop more robust mitigation techniques beyond prompting that allow for better control over the influence of pre-trained knowledge."
  - Why unresolved: The study only evaluated "Standard" and "Strict Adherence" prompts, confirming that prompting alone is insufficient.
  - What evidence would resolve it: A comparative study evaluating techniques like constrained decoding, retrieval-augmented generation (RAG), or domain-specific fine-tuning using the same reversed/shuffled artifact methodology.

- **Open Question 2**: Does knowledge-driven hallucination occur in other structured generation tasks, such as code generation or data schema creation?
  - Basis in paper: [explicit] The authors suggest the experimental methodology "could be adapted to investigate knowledge-driven hallucination in other structured generation tasks, such as code generation."
  - Why unresolved: The empirical study was restricted to the Business Process Management (BPM) domain using Petri nets and event logs.
  - What evidence would resolve it: Successful replication of the conflict-based experimental setup (standard vs. reversed/shuffled artifacts) in domains like software code or database schema generation.

- **Open Question 3**: How does the magnitude of deviation between evidence and internal knowledge affect the rate of knowledge-driven hallucination?
  - Basis in paper: [inferred] The methodology utilized extreme conflicts (fully reversed or fully shuffled processes), but did not test minor or subtle deviations from standard schemas.
  - Why unresolved: It is unclear if the "knowledge override" is binary or scales with the degree of "atypicality" in the input evidence.
  - What evidence would resolve it: Experiments using input artifacts with incremental structural deviations (e.g., single skipped steps vs. full reversals) to measure the correlation between deviation severity and hallucination frequency.

## Limitations

- The study cannot conclusively isolate whether observed behavior stems from pre-trained knowledge dominance or alignment fine-tuning effects
- The behavioral-footprint similarity metric may not capture all aspects of process model correctness, potentially conflating structural similarity with semantic accuracy
- The experimental design creates extreme conflicts but does not test whether the knowledge override effect scales with the degree of deviation

## Confidence

- **High Confidence**: The observation that LLMs systematically revert to standard process patterns when presented with conflicting evidence
- **Medium Confidence**: The claim that prompt engineering can only partially mitigate knowledge-driven hallucination
- **Medium Confidence**: The assertion that structured inputs (event logs) provide stronger evidence constraints than unstructured text

## Next Checks

1. Conduct ablation studies varying the degree of conflict between evidence and standard patterns to determine the threshold at which models consistently prioritize internal knowledge over input
2. Implement formal conformance checking alongside behavioral-footprint similarity to validate whether high similarity scores truly indicate correct process modeling rather than coincidental structural matches
3. Test models with domain-specific knowledge injection (RAG-style) versus standard prompting to quantify whether externalized knowledge can effectively compete with pre-trained schemas