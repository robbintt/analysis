---
ver: rpa2
title: 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents'
arxiv_id: '2503.20201'
source_url: https://arxiv.org/abs/2503.20201
tags:
- action
- search
- thought
- reasoning
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Open Deep Search (ODS) closes the gap between proprietary search
  AI and open-source solutions by integrating powerful open-source LLMs with novel
  search and reasoning capabilities. ODS introduces Open Search Tool, which enhances
  web search retrieval by rephrasing queries, extracting relevant context, and implementing
  custom handling for major websites, outperforming proprietary counterparts.
---

# Open Deep Search: Democratizing Search with Open-source Reasoning Agents

## Quick Facts
- **arXiv ID:** 2503.20201
- **Source URL:** https://arxiv.org/abs/2503.20201
- **Reference count:** 40
- **Primary result:** ODS achieves 88.3% SimpleQA accuracy and 75.3% FRAMES accuracy with DeepSeek-R1, surpassing proprietary alternatives

## Executive Summary
Open Deep Search (ODS) is an open-source search AI framework that closes the gap between proprietary search tools and open alternatives. It combines powerful open-source LLMs with novel search and reasoning capabilities through two main components: Open Search Tool for enhanced web retrieval and Open Reasoning Agent for tool orchestration. When paired with DeepSeek-R1, ODS outperforms closed-source solutions like Perplexity Sonar Reasoning Pro and GPT-4o Search Preview on standard benchmarks, achieving state-of-the-art results while remaining freely accessible and customizable.

## Method Summary
ODS consists of two integrated components: Open Search Tool and Open Reasoning Agent. The Open Search Tool enhances web search retrieval through query rephrasing, SERP scraping, and content augmentation, implementing custom handling for major websites. The Open Reasoning Agent offers two versions - ODS-v1 (ReAct-based) and ODS-v2 (CodeAct-based) - which interpret queries, orchestrate tools (search, calculator, code interpreter), and leverage web search to answer questions effectively. The framework is designed as a flexible, plug-and-play system that can be adapted to various search and reasoning tasks.

## Key Results
- ODS-v2+DeepSeek-R1 achieves 88.3% accuracy on SimpleQA benchmark
- ODS-v2+DeepSeek-R1 achieves 75.3% accuracy on FRAMES benchmark
- Outperforms proprietary alternatives like Perplexity Sonar Reasoning Pro and GPT-4o Search Preview
- ODS-v2 uses adaptive multi-hop search (1.45 searches on SimpleQA vs 3.39 on FRAMES)

## Why This Works (Mechanism)

### Mechanism 1
Query rephrasing and multi-source augmentation improve retrieval coverage for ambiguous or underspecified queries. The Open Search Tool generates k rephrased queries, retrieves SERP results, then scrapes and chunks top-m links to extract relevant passages above a relevance threshold. This bridges gaps between user phrasing and indexable content. The core assumption is that relevant information exists somewhere on the web; the bottleneck is query-context matching, not information absence. Break condition: if base LLM lacks instruction-following capability to generate useful rephrasings, or if retrieval corpus lacks relevant documents entirely.

### Mechanism 2
Tool-orchestrating reasoning agents (ReAct/CodeAct) outperform single-pass retrieval by iteratively decomposing tasks and selecting appropriate tools. ODS-v1 uses ReAct's Thought→Action→Observation loop with access to search, calculation (Wolfram Alpha), and "continue thinking." ODS-v2 uses CodeAct to generate executable Python code for tool calls. Both can call Open Search Tool multiple times based on intermediate reasoning. The core assumption is that the base LLM has sufficient reasoning capability to plan multi-step tool sequences and interpret tool outputs. Break condition: if the task requires more than the available tools, or if the LLM fails to recognize when tool outputs are insufficient.

### Mechanism 3
Adaptive multi-hop search—deciding when to search again based on result quality—improves efficiency and accuracy over fixed-search regimes. The reasoning agent evaluates whether initial search results answer the query. If insufficient, it issues follow-up searches. ODS-v2 averages 1.45 searches on SimpleQA vs 3.39 on harder FRAMES questions. The core assumption is that the LLM can accurately judge answer sufficiency and identify what's missing from current context. Break condition: if the LLM cannot reliably assess information gaps, adaptive search degrades to wasted or harmful additional calls.

## Foundational Learning

- **Concept: ReAct Framework (Reasoning + Acting)**
  - **Why needed here:** ODS-v1's agent architecture is built on ReAct. Understanding the Thought/Action/Observation loop is prerequisite to debugging agent traces and designing new tools.
  - **Quick check question:** Given a trace showing "Thought: I need the birth year → Action: search → Observation: born 1946 → Thought: Done," can you identify where the agent decided the task was complete?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** CoT underlies the reasoning quality. ODS uses few-shot CoT examples (200 prompts from community campaign) to guide the base LLM's decomposition behavior.
  - **Quick check question:** If you remove the few-shot CoT examples from ODS-v1's prompt, would you expect performance to stay similar, degrade slightly, or collapse? Why?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** ODS extends RAG by making retrieval agentic. You need to understand baseline RAP (retrieve-then-generate) to appreciate what the reasoning agent adds.
  - **Quick check question:** In standard RAG, retrieval happens once before generation. How does ODS differ structurally?

## Architecture Onboarding

- **Component map:** Base LLM ↔ Open Reasoning Agent ↔ Tools: [Open Search Tool, Calculator (Wolfram), Code Interpreter]
- **Critical path:** 1) User query enters Open Reasoning Agent; 2) Agent generates Thought + selects Action (search/calculate/continue); 3) If search: Open Search Tool returns augmented context; 4) Agent evaluates sufficiency, loops or terminates; 5) Final answer generated
- **Design tradeoffs:**
  - ReAct (ODS-v1): More interpretable traces; structured prompts; easier debugging. Lower FRAMES performance (56.7% vs 75.3%).
  - CodeAct (ODS-v2): More flexible composition; code naturally expresses complex logic. Higher performance but requires code-execution environment.
  - Few-shot count: 200 prompts improve diversity but increase prompt token costs.
- **Failure signatures:**
  - Agent loops indefinitely without terminating (insufficient "Done" trigger in prompt)
  - Perplexity-style errors: retrieving conflicting values and selecting wrong one without verification
  - Search tool returns empty/noisy context (SERP API rate limits, blocked domains)
- **First 3 experiments:**
  1. **Ablate query rephrasing:** Run ODS-v1 with k=1 (no rephrasing) on a 100-sample subset of FRAMES. Expect accuracy drop if rephrasing mechanism is critical.
  2. **Swap base models:** Run ODS-v1+DeepSeek-R1 vs ODS-v1+Llama3.1-70B on identical queries. Isolate reasoning capability contribution vs search tool quality.
  3. **Trace analysis on failures:** Run ODS-v2 on 20 FRAMES questions where Perplexity fails but ODS succeeds (from paper examples). Identify which step (rephrasing, retrieval, or reasoning) drove the difference.

## Open Questions the Paper Calls Out

### Open Question 1
How does ODS perform on a broader range of evaluation benchmarks beyond SimpleQA and FRAMES, particularly on tasks requiring multi-modal reasoning or long-form answer generation? The paper evaluates ODS exclusively on SimpleQA (short-form factuality) and FRAMES (multi-hop retrieval), leaving open its generalization to diverse task types.

### Open Question 2
What are the latency and computational cost trade-offs of ODS compared to closed-source alternatives, particularly given the multiple search iterations ODS-v2 employs? The paper focuses on accuracy but does not analyze efficiency, which is critical for real-world deployment.

### Open Question 3
How robust is ODS to noisy or adversarial web content, given its reliance on scraped and re-ranked search results? The paper mentions prioritizing reliable sources but does not test performance when search results contain deliberate misinformation.

## Limitations

- Critical implementation details missing (k, m, n parameters; embedding models; re-rankers)
- Only 20 of 200 few-shot ReAct prompts provided; dynamic selection mechanism unclear
- CodeAct implementation (ODS-v2) minimally described, lacking SmolAgents configuration details
- Web content drift could cause benchmark score variations over time
- No analysis of scalability, cost-per-query, or performance under real-world constraints

## Confidence

**High confidence:** ODS achieves superior accuracy (88.3% SimpleQA, 75.3% FRAMES) compared to Perplexity and GPT-4o Search Preview when using DeepSeek-R1 as base model. The mechanism of query rephrasing plus iterative tool orchestration is plausible and aligns with established ReAct/CodeAct literature.

**Medium confidence:** The claimed gap closure between proprietary and open-source search AI depends heavily on the specific base LLM choice. ODS-v2+DeepSeek-R1 outperforms ODS-v1+Llama3.1-70B (75.3% vs 56.7% on FRAMES), suggesting base model reasoning capability is a dominant factor rather than the ODS framework itself.

**Low confidence:** The practical deployment feasibility without significant API costs or rate limiting. The paper doesn't address scalability, cost-per-query, or performance under real-world constraints like SERP API limits and blocked domains.

## Next Checks

1. **Parameter sensitivity analysis:** Systematically vary k (rephrased queries), m (links scraped), and n (passages returned) on a 50-question FRAMES subset to identify which parameters most affect accuracy and which can be minimized to reduce costs.

2. **Base model ablation study:** Run ODS-v1 with Llama3.1-70B, DeepSeek-R1, and GPT-4o (if accessible) on identical 100-question SimpleQA samples. Quantify the contribution of ODS's search tool versus base model reasoning capability to final accuracy.

3. **Trace-based failure analysis:** Collect and categorize 100 ODS-v2 FRAMES failures into three buckets: search tool failures (bad context), reasoning failures (wrong tool choice), or base model failures (incoherent answers). Compare distributions against Perplexity failures to identify ODS's specific weaknesses.