---
ver: rpa2
title: 'It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense
  Knowledge'
arxiv_id: '2509.16107'
source_url: https://arxiv.org/abs/2509.16107
tags:
- response
- language
- simple
- normal
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluates how Large Language Models (LLMs)
  handle referential ambiguity in multi-turn conversations, particularly when commonsense
  knowledge is required to resolve underspecified references. Using a multilingual
  evaluation dataset, the authors test several models, including GPT-4o, DeepSeek
  v3, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B, both with and without simplified language
  prompts.
---

# It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge

## Quick Facts
- arXiv ID: 2509.16107
- Source URL: https://arxiv.org/abs/2509.16107
- Reference count: 40
- LLMs struggle with referential ambiguity, defaulting to single interpretations or exhaustive enumeration rather than hedging or seeking clarification.

## Executive Summary
This study systematically evaluates how Large Language Models (LLMs) handle referential ambiguity in multi-turn conversations, especially when commonsense knowledge is required to resolve underspecified references. Using a multilingual evaluation dataset, the authors test several models, including GPT-4o, DeepSeek v3, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B, both with and without simplified language prompts. Results show that current LLMs struggle to resolve ambiguity effectively, typically committing to a single interpretation or enumerating all possible referents, rather than hedging or seeking clarification. This issue worsens when prompted for simplified language, as commonsense reasoning and response diversity decline. Fine-tuning Llama-3.1-8B with Direct Preference Optimization (DPO) substantially improves ambiguity resolution across all prompt types, increasing correct responses from 13.46% to 96.45% (Normal) and from 13.83% to 91.59% (Simple), with more use of hedging and clarification. The findings underscore the need for advanced fine-tuning to improve LLMs' handling of ambiguity and robustness across communication styles.

## Method Summary
The authors constructed a multilingual evaluation dataset for referential ambiguity resolution, covering English, French, German, Russian, Chinese, and Arabic. They used DeepL to translate English examples into the other languages, then manually corrected errors. The dataset included both ambiguous (SharedRef) and unambiguous (ClearRef) contexts. Five LLMs (GPT-4o, DeepSeek v3, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B) were evaluated using two prompt types (Normal and Simple). Response categories were manually annotated for correctness, hedging, and clarification. Additionally, the Llama-3.1-8B model was fine-tuned using DPO on ambiguous and unambiguous examples, and the fine-tuned model was evaluated under the same conditions.

## Key Results
- LLMs typically commit to a single interpretation or enumerate all possible referents, rather than hedging or seeking clarification in ambiguous contexts.
- Simplified language prompts reduce commonsense reasoning and response diversity, leading to more exhaustive enumeration and less hedging.
- Fine-tuning Llama-3.1-8B with DPO substantially improves ambiguity resolution, increasing correct responses from 13.46% to 96.45% (Normal) and from 13.83% to 91.59% (Simple), with more use of hedging and clarification.

## Why This Works (Mechanism)
The study demonstrates that LLMs lack robust mechanisms for resolving referential ambiguity, especially in simplified language contexts. The DPO fine-tuning approach successfully steers the model toward more appropriate ambiguity-handling strategies (hedging and clarification) by explicitly rewarding these behaviors during training.

## Foundational Learning
- **Referential Ambiguity**: Situations where pronouns or underspecified references can refer to multiple entities. Needed to understand the core problem; check by identifying ambiguous pronouns in a dialogue.
- **Direct Preference Optimization (DPO)**: A fine-tuning method that optimizes models to align with human preferences. Needed to explain the intervention; check by summarizing how DPO differs from standard supervised fine-tuning.
- **Response Categories**: Classification of model outputs (direct, hedging, clarification, exhaustive enumeration). Needed to interpret evaluation results; check by matching example responses to their categories.

## Architecture Onboarding

### Component Map
Dataset (translated + corrected) -> Evaluation Prompts (Normal, Simple) -> LLM (GPT-4o, DeepSeek v3, Qwen3-32B, GPT-4o-mini, Llama-3.1-8B) -> Response Annotation -> Analysis

### Critical Path
Translation -> Correction -> Prompting -> Response generation -> Manual annotation -> Performance analysis

### Design Tradeoffs
Translated dataset (broad language coverage) vs. native datasets (cultural nuance); Manual annotation (high quality) vs. automated (scalability).

### Failure Signatures
Defaulting to single interpretations; exhaustive enumeration; loss of commonsense reasoning in simplified prompts; positional bias in entity selection.

### First 3 Experiments
1. Compare model performance on ambiguous vs. unambiguous contexts within the same dataset.
2. Evaluate fine-tuned model on a held-out test set with diverse ambiguity types.
3. Conduct a prompt ablation study to determine sensitivity to prompt formulations.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How would LLM performance on referential ambiguity resolution differ when evaluated on native, culturally-grounded datasets versus translated datasets?
- Basis in paper: "Future work should create native datasets for each language to ensure more accurate and culturally appropriate evaluation."
- Why unresolved: The study relied on DeepL translations from English, which may introduce translation bias, cultural mismatches, or loss of nuance for Arabic, French, Russian, and Chinese.
- What evidence would resolve it: Comparison of model performance on native datasets constructed independently for each language versus translated versions, measuring correctness and response category distributions.

### Open Question 2
- Question: How can DPO training be optimized to encourage direct answers in unambiguous contexts (ClearRef) while maintaining clarification behavior in ambiguous contexts (SharedRef)?
- Basis in paper: "Future alignment efforts should incorporate more training examples from ClearRef to encourage direct answers where appropriate."
- Why unresolved: The DPO model showed undesirable category shift on ClearRef, preferring clarification/hedging over direct answers where commonsense should resolve ambiguity.
- What evidence would resolve it: Training with balanced ClearRef and SharedRef examples, then evaluating whether the model appropriately adapts response strategies to context type.

### Open Question 3
- Question: What mechanisms underlie the strong positional bias in entity selection, and can training or prompting interventions mitigate it?
- Basis in paper: The ablation study revealed entities at position 3 are selected ~43% of the time versus ~23% for position 2 in SharedRef, indicating strong positional bias.
- Why unresolved: The paper fixed entity order due to computational constraints and did not investigate whether this bias stems from attention patterns, training data, or other factors.
- What evidence would resolve it: Analysis of attention weights during entity selection; training with randomized entity positions; evaluation across all permutations for multiple models.

### Open Question 4
- Question: Why does Chain-of-Thought prompting degrade performance on referential ambiguity resolution compared to standard prompting?
- Basis in paper: CoT prompting dropped accuracy from 81.06% to 44.49% in SharedRef Normal, often resolving to one positive entity while ignoring the other.
- Why unresolved: The paper observed worse performance but did not investigate why explicit reasoning steps lead to incomplete disambiguation rather than improved coverage.
- What evidence would resolve it: Analysis of intermediate CoT steps to identify where reasoning diverges; comparison with different CoT prompt formulations.

## Limitations
- Evaluation relies on a manually curated multilingual dataset that may not fully represent real-world ambiguity patterns or domain-specific language use.
- Fine-tuning with DPO shows dramatic improvements, but sample size and diversity of training instances are not detailed, raising questions about generalization.
- The study does not explicitly compare performance on ambiguous versus unambiguous references within the same dataset, making it difficult to isolate the effect of ambiguity from other linguistic factors.

## Confidence
- High: The finding that LLMs default to single interpretations or exhaustive enumeration rather than hedging/clarification is well-supported by the experimental data and analysis.
- Medium: The claim that simplified language prompts reduce commonsense reasoning and diversity is plausible but may depend on prompt construction and dataset specifics.
- Medium: The reported improvements from fine-tuning are substantial, but generalization and robustness to prompt variations are uncertain.

## Next Checks
1. Evaluate the fine-tuned model on a held-out test set with diverse ambiguity types not seen during training to assess generalization.
2. Conduct a prompt ablation study to determine the sensitivity of results to specific prompt formulations, especially for simplified language.
3. Perform a qualitative analysis comparing ambiguous and unambiguous reference resolution within the same dataset to isolate the impact of ambiguity.