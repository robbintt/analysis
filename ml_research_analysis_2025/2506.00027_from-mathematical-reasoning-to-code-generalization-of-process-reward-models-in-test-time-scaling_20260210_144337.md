---
ver: rpa2
title: 'From Mathematical Reasoning to Code: Generalization of Process Reward Models
  in Test-Time Scaling'
arxiv_id: '2506.00027'
source_url: https://arxiv.org/abs/2506.00027
tags:
- training
- reasoning
- accuracy
- prms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization capabilities of Process
  Reward Models (PRMs) trained on mathematical reasoning datasets when applied to
  code generation tasks. Through controlled experiments, the study finds that PRMs
  trained on mathematical data perform comparably to those trained on code-specific
  datasets, demonstrating robust cross-domain generalization.
---

# From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling

## Quick Facts
- arXiv ID: 2506.00027
- Source URL: https://arxiv.org/abs/2506.00027
- Reference count: 40
- Primary result: PRMs trained on mathematical reasoning datasets perform comparably to code-specific PRMs on code generation tasks, demonstrating robust cross-domain generalization.

## Executive Summary
This paper investigates whether Process Reward Models (PRMs) trained on mathematical reasoning data can generalize to code generation tasks. Through controlled experiments across model scales (0.5B-72B parameters) and search strategies (MCTS, Best-of-N, Beam Search, Majority Voting), the authors find that PRMs trained on math datasets achieve comparable performance to code-specific PRMs on benchmarks like HumanEval+ and LiveCodeBench. The study reveals diminishing returns in PRM performance with increasing model size, identifies dataset diversity as a critical factor, and demonstrates that MCTS is the most effective test-time scaling strategy when computational resources are abundant.

## Method Summary
The authors train PRMs using a binary cross-entropy loss to classify intermediate reasoning steps as correct or incorrect. Models are initialized from Qwen2.5 backbones (0.5B-72B parameters) with language modeling heads replaced by scalar-value heads (two linear layers). Training data combines PRM800K and Math-Shepherd datasets, with ASLAF filtering using ensemble LLM annotations to reduce label noise. For inference, the PRM guides search strategies including Best-of-N sampling, Beam Search, Majority Voting, and Monte Carlo Tree Search (MCTS) with UCT policy. The framework evaluates generalization by comparing math-trained PRMs against code-specific PRMs on both mathematical and code generation benchmarks.

## Key Results
- PRMs trained on mathematical datasets achieve 91.5 accuracy on HumanEval+ vs 89.0 for code-specific PRMs
- MCTS outperforms all other search strategies when computational resources are abundant
- Dataset diversity significantly impacts PRM performance more than raw dataset size
- PRM performance shows diminishing returns beyond 32B parameters
- A gradient-based similarity metric reveals PRMs prefer responses with similar underlying reasoning patterns across domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PRMs trained on mathematical reasoning data can generalize to code generation tasks at comparable performance to code-specific PRMs.
- Mechanism: The PRM learns to evaluate intermediate reasoning steps for correctness rather than domain-specific syntax. Mathematical reasoning and code generation share underlying problem decomposition patterns (e.g., identifying edge cases, iterative refinement). A gradient-based similarity metric (Section 4.3, Eq. in paper) shows that PRMs prefer responses with similar internal activation patterns across domains, suggesting they capture transferable reasoning structure rather than surface form.
- Core assumption: Reasoning correctness is partially domain-invariant; step-level correctness in math correlates with step-level correctness in code for structurally similar problems.
- Evidence anchors:
  - [abstract] "PRMs trained on mathematical datasets exhibit performance comparable to those tailored for code generation, suggesting robust cross-domain generalization."
  - [section 4.3, Table 1] PRM-Math achieves 91.5 on HumanEval+ with Qwen2.5-Coder-32B vs 89.0 for PRM-Code; similar or better on LiveCodeBench (30.3 vs 29.7).
  - [section 4.3, Table 2] Gradient-based pattern similarity between MathPRM and CodePRM (30.95) exceeds Math-Code baseline (26.75).
  - [corpus] GenPRM (2504.00891) and FunPRM (2601.22249) confirm PRM scaling via test-time compute; corpus does not directly replicate math-to-code transfer claim.
- Break condition: If code tasks require reasoning patterns not represented in math training data (e.g., API knowledge, system design), transfer will degrade.

### Mechanism 2
- Claim: MCTS outperforms simpler search strategies (Best-of-N, Beam Search, Majority Voting) when test-time compute is abundant, but Best-of-N is superior under tight time constraints.
- Mechanism: MCTS balances exploration and exploitation via UCT policy, building a search tree where PRM scores guide node expansion and rollouts. This allows deeper reasoning path exploration. Best-of-N generates N complete solutions and selects by highest PRM score—simpler and faster but less adaptive. Under limited time, Best-of-N's parallel generation outpaces MCTS's sequential tree building; with more compute, MCTS's strategic exploration yields higher accuracy.
- Core assumption: PRM scores are sufficiently correlated with true solution correctness to guide search; compute budget is the primary constraint.
- Evidence anchors:
  - [abstract] "Monte Carlo Tree Search as the most effective method when computational resources are abundant, while Best-of-N Sampling serves as a practical alternative under resource-limited conditions."
  - [section 4.2, Figure 4-5] MCTS achieves highest accuracy as generation budget increases; Best-of-N initially leads under time constraints before MCTS overtakes.
  - [corpus] GenPRM (2504.00891) corroborates that scaling test-time compute with PRMs improves performance; specific MCTS vs Best-of-N comparison not replicated in corpus.
- Break condition: If PRM scores are miscalibrated (overconfident on wrong steps), MCTS will exploit incorrect paths; if inference latency dominates over compute, Best-of-N remains preferable.

### Mechanism 3
- Claim: Dataset quality and diversity impact PRM performance more than raw quantity; ensemble-based filtering reduces label noise.
- Mechanism: The ASLAF pipeline uses multiple LLMs to annotate step correctness via Monte Carlo estimation, then filters via ensemble consensus—only retaining steps where all annotators agree on error location. This reduces false positives/negatives from single-model annotation. Training on the filtered, combined dataset (PRM800k + Math-Shepherd) yields higher accuracy than either source alone, suggesting diverse reasoning patterns improve generalization.
- Core assumption: Ensemble agreement correlates with ground-truth step correctness; diverse training distributions improve model robustness.
- Evidence anchors:
  - [abstract] "diversity of training datasets significantly impacts PRM performance."
  - [section 2.1] "Only those steps where all LLMs agree on the error location are retained for training."
  - [section 4.1, Figure 3] ASLAF dataset outperforms PRM800k and Math-Shepherd across BON values on MATH500 and PRM800k benchmarks.
  - [corpus] No direct corpus evidence on ensemble filtering specifically; related work (Math-Shepherd, PRM800k) cited but not replicated.
- Break condition: If ensemble models share systematic biases, consensus filtering will not eliminate errors; if filtering is too aggressive, training data may become insufficient.

## Foundational Learning

- Concept: Process Reward Models vs Outcome Reward Models
  - Why needed here: The paper's entire framework depends on understanding that PRMs score intermediate reasoning steps, not just final answers. Without this, the mechanism of test-time guidance and step-level error detection is opaque.
  - Quick check question: Given a solution with 5 reasoning steps where step 3 is wrong but the final answer is correct, would an ORM or PRM be more likely to catch the error?

- Concept: Binary Cross-Entropy Loss for Step Classification
  - Why needed here: PRM training formulates step evaluation as binary classification (correct/incorrect). Understanding the loss function (Eq. 2) is necessary to interpret training dynamics and emergence behavior (Figure 2).
  - Quick check question: If a PRM outputs p_t=0.7 for a step with true label y_t=1, what is the contribution to the loss for that step?

- Concept: Monte Carlo Tree Search (UCT Policy)
  - Why needed here: MCTS is identified as the optimal test-time scaling strategy under sufficient compute. Implementing or debugging this requires understanding selection, expansion, simulation, and backpropagation phases.
  - Quick check question: In UCT, what happens to the exploration term c·√(ln N(s)/N(s,a)) as a node is visited more frequently?

## Architecture Onboarding

- Component map: Qwen2.5 backbone -> Scalar-value head replacement -> ASLAF dataset construction -> Binary cross-entropy training -> Inference-time search with PRM scores
- Critical path: ASLAF data construction → PRM training (cross-entropy on filtered steps) → Inference-time search with PRM scores. Errors in annotation/filtering propagate directly to PRM quality; PRM miscalibration cascades into poor search guidance.
- Design tradeoffs:
  - Model size vs compute: Larger PRMs show diminishing returns (Figure 1); 7B–14B may be optimal given compute constraints
  - Search strategy selection: MCTS for accuracy with ample time; Best-of-N for latency-constrained deployments
  - Training data diversity vs filtering aggressiveness: More diverse data improves generalization; stricter filtering reduces noise but may discard useful samples
- Failure signatures:
  - PRM overconfidence on incorrect steps → MCTS exploits bad paths, Best-of-N selects wrong solution
  - Training emergence not reached (Figure 2 plateau) → Insufficient training steps; PRM hasn't learned meaningful patterns
  - Poor cross-domain transfer → Training data lacks reasoning patterns relevant to target domain
  - Search strategy mismatch → Using MCTS under tight latency budget yields worse results than Best-of-N
- First 3 experiments:
  1. Replicate scaling curve: Train PRMs at 0.5B, 7B, 14B on ASLAF subset; evaluate on MATH500 to verify diminishing returns pattern and identify emergence threshold in training steps.
  2. Cross-domain transfer test: Train PRM on math-only (PRM800k), evaluate on HumanEval+ and LiveCodeBench; compare against code-trained PRM to validate generalization claim.
  3. Search strategy ablation: For fixed PRM, benchmark Best-of-N (N=8) vs MCTS under 10s and 60s inference budgets on held-out reasoning tasks; confirm time/compute tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the cross-domain generalization capability of Process Reward Models (PRMs) extend beyond mathematical reasoning and code generation to other distinct domains?
- Basis in paper: [explicit] The authors state in the Limitations section that "further research is needed to explore PRM performance in other domains to fully understand their cross-domain adaptability."
- Why unresolved: The current study is strictly confined to mathematical and coding tasks, leaving the universality of the observed transfer learning effects unverified.
- What evidence would resolve it: Evaluation of math-trained PRMs on benchmarks requiring logical, commonsense, or scientific reasoning (e.g., MMLU, LogiQA).

### Open Question 2
- Question: How does PRM performance correlate with metrics beyond accuracy, specifically regarding interpretability and robustness?
- Basis in paper: [explicit] The paper notes in Section 7 (Limitations) that "Future work could benefit from incorporating additional metrics, such as interpretability and robustness, to provide a more comprehensive evaluation of PRM capabilities."
- Why unresolved: The current analysis relies primarily on downstream task accuracy (Best-of-N performance) and FLOP efficiency, overlooking the reliability of the feedback mechanism itself.
- What evidence would resolve it: Quantitative results measuring error localization precision (interpretability) and stability under adversarial or out-of-distribution inputs (robustness).

### Open Question 3
- Question: Do the observed patterns of diminishing returns and sudden "emergence" during training persist or alter fundamentally when scaling PRMs beyond the 72B parameter limit?
- Basis in paper: [inferred] While the authors note diminishing returns at 72B, they acknowledge in Section 7 that "experiments were limited by available computational resources," restricting the exploration of "larger model architectures."
- Why unresolved: It remains unclear if the performance plateau is an inherent property of the PRM methodology or simply a characteristic of the specific model scales tested.
- What evidence would resolve it: Training convergence curves and accuracy scaling laws for PRMs initialized from models significantly larger than 72B.

## Limitations

- The study's generalization claims rest on controlled comparisons between math-trained and code-trained PRMs, but the assumption that step-level correctness patterns transfer across domains lacks rigorous validation beyond the specific datasets tested.
- The optimal model size conclusions (diminishing returns beyond 32B) are based on single scaling experiments without ablation of other factors like training duration or data diversity.
- The gradient-based similarity metric, while novel, may capture surface-level activation patterns rather than true reasoning structure, leaving the mechanism of cross-domain transfer unverified.

## Confidence

- **High confidence:** PRMs can be effectively trained on mathematical reasoning data and demonstrate reasonable performance on code tasks (validated by direct benchmark comparisons showing 91.5 vs 89.0 on HumanEval+).
- **Medium confidence:** MCTS outperforms simpler search strategies under abundant compute (supported by time-budget experiments but dependent on PRM calibration quality).
- **Low confidence:** Dataset diversity is more important than quantity for PRM generalization (based on qualitative observations from the ASLAF filtering process rather than systematic ablation studies).

## Next Checks

1. Conduct systematic ablation studies varying dataset diversity while holding quantity constant to isolate the impact of training data composition on cross-domain transfer.
2. Implement cross-validation on held-out reasoning patterns (e.g., recursion, edge case handling) to verify that PRMs actually learn transferable reasoning structures rather than memorizing domain-specific cues.
3. Test PRM generalization to additional domains (e.g., scientific reasoning, logical puzzles) to establish whether mathematical training provides broad reasoning transfer or is specific to code generation tasks.