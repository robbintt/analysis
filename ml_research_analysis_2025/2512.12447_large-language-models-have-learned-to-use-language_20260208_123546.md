---
ver: rpa2
title: Large language models have learned to use language
arxiv_id: '2512.12447'
source_url: https://arxiv.org/abs/2512.12447
tags:
- language
- llms
- have
- structure
- hips
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) have achieved human-level competence\
  \ in using language, including the ability to generate meaningless grammatical sentences\
  \ and explicitly judge grammaticality\u2014skills previously thought unique to humans.\
  \ In informal tests, both people and GPT-5 produced sentences rated as similarly\
  \ meaningless, and neither could reliably distinguish human from LLM-generated sentences."
---

# Large language models have learned to use language

## Quick Facts
- arXiv ID: 2512.12447
- Source URL: https://arxiv.org/abs/2512.12447
- Authors: Gary Lupyan
- Reference count: 0
- One-line primary result: LLMs now demonstrate human-level competence in language use, including the ability to generate meaningless grammatical sentences and judge grammaticality explicitly—capabilities previously thought uniquely human.

## Executive Summary
Large language models have achieved human-level competence in using language, including the ability to generate meaningless grammatical sentences and explicitly judge grammaticality—skills previously thought unique to humans. In informal tests, both people and GPT-5 produced sentences rated as similarly meaningless, and neither could reliably distinguish human from LLM-generated sentences. LLMs' ability to uncouple meaning from grammar emerges only after fine-tuning, despite their pre-existing latent linguistic knowledge. These findings suggest that explicit grammatical judgment is not necessary for competent language use and may instead be a consequence of it. LLMs now surpass the Turing test, demonstrating that language proficiency does not require conformity to any single linguistic theory.

## Method Summary
The paper presents informal comparisons between human and LLM capabilities in generating meaningless grammatical sentences and judging grammaticality. Human participants generated sentences rated for meaningfulness, while GPT-5 was prompted to produce similar sentences. Both sources were then rated by participants for meaningfulness and origin (human vs. LLM). The study also examines base models versus instruction-tuned models to investigate when metalinguistic capabilities emerge. The Databricks-dolly-15k dataset is mentioned as an example of fine-tuning data that enables explicit grammaticality judgments despite not containing grammaticality examples.

## Key Results
- GPT-5 and humans generated sentences rated as similarly meaningless (t<1)
- Neither humans nor GPT-5 could reliably distinguish human from LLM-generated sentences (d'<1)
- Base models struggle with explicit grammaticality judgments, which emerge only after fine-tuning
- LLMs now surpass the Turing test, demonstrating human-level language proficiency

## Why This Works (Mechanism)

### Mechanism 1
Prediction-based learning is sufficient for acquiring human-level language competence. A general-purpose neural network with a simple learning rule (minimize prediction error), architectural capacity to track long-distance associations, and a corpus of language use can learn language without explicit linguistic rules or innate grammatical structures. Core assumption: Language knowledge does not require conformity to generative linguistic theories (e.g., Minimalist syntax) to be functionally competent. Evidence anchors: Author states that acknowledging LLMs have learned to use language can open doors to breakthrough language science, and that a neural network with simple learning rules can learn language from data. Break condition: If language use requires symbolic grammatical representations that cannot emerge from statistical prediction alone, this mechanism fails.

### Mechanism 2
Fine-tuning unlocks explicit metalinguistic judgment capabilities that exist latently in base models. Base models acquire latent linguistic knowledge through self-supervision but cannot make explicit grammaticality judgments. Fine-tuning—even on small datasets without grammaticality examples (e.g., Databricks-dolly-15k)—enables the ability to uncouple meaning from grammar and make explicit judgments. Core assumption: Fine-tuning reorganizes or makes accessible latent representations rather than injecting entirely new capabilities. Evidence anchors: Base models appear incapable of making explicit grammaticality judgments, and the ability to explicitly uncouple meaningfulness from grammaticality appears to emerge with fine-tuning on even small datasets. Break condition: If fine-tuning merely provides output formatting rather than enabling genuinely new metalinguistic reasoning, the claimed emergence is superficial.

### Mechanism 3
Explicit grammatical judgment is a consequence of language competence, not a prerequisite. Proficient language use (passing Turing tests, generating meaningless grammatical sentences) develops without explicit grammatical knowledge. The ability to judge grammaticality independently of meaning emerges later, suggesting it builds upon—rather than enables—language use. Core assumption: The developmental trajectory in LLMs (use first, then judgment) parallels human development where children use language proficiently before acquiring metalinguistic awareness. Evidence anchors: Children engage in sophisticated language use while lacking the ability to make explicit judgments is prima facie evidence that being a competent language user does not require being able to dissociate conceptual content from structural configurations. Break condition: If metalinguistic judgment is shown to causally improve or stabilize language use (rather than being epiphenomenal), the claimed directionality reverses.

## Foundational Learning

- **Concept: Self-supervised prediction learning**
  - Why needed here: The paper's central claim rests on the idea that minimizing prediction error over large corpora is sufficient for language acquisition, challenging theories requiring innate grammatical knowledge.
  - Quick check question: Can you explain how next-token prediction might induce representations of hierarchical structure without explicit syntactic supervision?

- **Concept: Base model vs. instruction-tuned model distinction**
  - Why needed here: The paper emphasizes that latent knowledge exists in base models but explicit metalinguistic capabilities emerge only after fine-tuning.
  - Quick check question: What is the operational difference between GPT-3 (base) and ChatGPT (fine-tuned) in terms of conversational ability?

- **Concept: Grammaticality-meaningfulness uncoupling**
  - Why needed here: The ability to judge "Colorless green ideas sleep furiously" as grammatical but meaningless is a key metalinguistic skill the paper uses to evaluate LLM competence.
  - Quick check question: Why is generating a meaningless grammatical sentence harder than generating a plausible sentence?

## Architecture Onboarding

- **Component map:** Base model (self-supervised transformer) -> Fine-tuning layer (instruction-tuning) -> Evaluation harness (grammaticality judgment tasks, Turing test protocols)
- **Critical path:** Pretrain base model → Fine-tune on instruction data (even small, ~15k examples) → Test explicit grammaticality judgment and meaning-grammar uncoupling
- **Design tradeoffs:** Larger pretraining corpora increase latent knowledge but do not automatically yield explicit judgment capabilities. Fine-tuning dataset quality may matter less than expected for metalinguistic emergence (paper claims emergence from datasets without grammaticality examples).
- **Failure signatures:** Base model generates implausible sentences but struggles with deliberately ungrammatical or meaningless grammatical constructions. Users report "parrot-like" responses when using base models conversationally. Model cannot reliably distinguish its own outputs from human-generated text (human-level indistinguishability in Turing-style tests).
- **First 3 experiments:**
  1. Replicate the meaningless grammatical sentence generation task: prompt base vs. fine-tuned models to generate maximally meaningless grammatical sentences; compare human ratings.
  2. Ablate fine-tuning dataset: train separate models on datasets with/without explicit grammaticality examples to test whether emergence requires grammatical supervision.
  3. Probe latent grammatical knowledge: use surprisal-based methods to extract grammaticality judgments from base models and compare to explicit judgments from fine-tuned models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: When does the ability to distinguish meaningfulness from grammaticality emerge in LLMs, and does it have any causal bearing on proficient language use?
- Basis in paper: Author states: "An intriguing question is when the ability to distinguish meaningfulness from grammaticality emerges in LLMs and whether it has any causal bearing on proficient language use."
- Why unresolved: Base models can generate implausible sentences but struggle with ungrammatical or truly meaningless ones; the uncoupling ability emerges with fine-tuning, but the causal relationship to competent language use remains unclear.
- What evidence would resolve it: Controlled experiments tracking grammaticality judgment performance across training stages, correlated with independent measures of functional language proficiency.

### Open Question 2
- Question: Is the ability to make explicit grammatical judgments causally related to being able to use language competently?
- Basis in paper: Author states: "Understanding whether the ability to make explicit grammatical judgments is causally related to being able to use language would be a fascinating development."
- Why unresolved: Children use language proficiently without explicit metalinguistic judgment abilities, yet LLMs only demonstrate this uncoupling after fine-tuning—suggesting the relationship is not straightforward.
- What evidence would resolve it: Intervention studies that selectively enhance or impair grammaticality judgment abilities in models, measuring effects on conversational competence.

### Open Question 3
- Question: Which specific aspects of linguistic structure are critical for language use versus incidental byproducts of learning?
- Basis in paper: Paper argues that LLMs "provide an unprecedented opportunity to understand [which] aspects of linguistic structure are critical for language use, even when learned by a system with an architecture radically different from ours."
- Why unresolved: LLMs achieve proficiency without conforming to specific linguistic theories (e.g., Minimalist syntax), but systematic identification of which structural representations are necessary remains incomplete.
- What evidence would resolve it: Ablation studies targeting specific structural representations in LLMs, combined with behavioral assessments of functional communication outcomes.

## Limitations

- The comparison to GPT-5 lacks specific technical details about model versions, prompting strategies, and evaluation protocols, making direct replication challenging.
- The claim about fine-tuning enabling metalinguistic capabilities from datasets without explicit grammaticality examples needs verification, as the mechanism remains underspecified.
- The parallel to human developmental trajectories is asserted but not empirically tested in the current study.

## Confidence

- **High Confidence**: LLMs can generate and judge meaningless grammatical sentences at human-comparable levels; base models possess latent linguistic knowledge; instruction-tuning enables explicit grammaticality judgment.
- **Medium Confidence**: The emergence of metalinguistic judgment is a consequence rather than prerequisite of language competence; prediction-based learning is sufficient for human-level language acquisition.
- **Low Confidence**: Specific mechanisms of how fine-tuning reorganizes latent representations; direct comparability of GPT-5 performance to human participants.

## Next Checks

1. Replicate the meaningless grammatical sentence generation task with multiple model families (base vs. fine-tuned) and systematic ablation of fine-tuning data types.
2. Conduct controlled experiments testing whether instruction-tuning on datasets containing grammaticality examples produces different metalinguistic capabilities than those without.
3. Perform developmental trajectory analysis comparing human children's language acquisition stages with LLM training progression from base to fine-tuned models.