---
ver: rpa2
title: 'A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging
  Continuous Perception and Discrete Symbolic Planning'
arxiv_id: '2511.14533'
source_url: https://arxiv.org/abs/2511.14533
tags:
- uncertainty
- planning
- symbolic
- information
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a neuro-symbolic framework that bridges continuous
  perception and discrete symbolic planning by explicitly modeling and propagating
  uncertainty from visual observations to planning decisions. The key idea is to use
  a transformer-based perceptual front-end coupled with graph neural network relational
  reasoning to extract probabilistic symbolic states with calibrated confidences,
  and an uncertainty-aware symbolic planner that triggers information gathering when
  confidence is low.
---

# A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning

## Quick Facts
- arXiv ID: 2511.14533
- Source URL: https://arxiv.org/abs/2511.14533
- Reference count: 27
- Primary result: Achieves 90.7% average success rate on tabletop manipulation tasks, exceeding POMDP baselines by 10-14 percentage points

## Executive Summary
This paper presents a neuro-symbolic framework that bridges continuous perception and discrete symbolic planning by explicitly modeling and propagating uncertainty from visual observations to planning decisions. The key idea is to use a transformer-based perceptual front-end coupled with graph neural network relational reasoning to extract probabilistic symbolic states with calibrated confidences, and an uncertainty-aware symbolic planner that triggers information gathering when confidence is low. The framework is demonstrated on tabletop robotic manipulation using 10,047 synthetic scenes, achieving overall F1=0.68 for symbol prediction and 90.7% average success rate across three benchmarks.

## Method Summary
The framework consists of a neural-symbolic translator that maps RGB images to probabilistic symbolic predicates, followed by an uncertainty-aware symbolic planner. The translator uses a ResNet-18 backbone with self-attention and graph neural networks to extract 18-dimensional edge features representing spatial relationships between objects. These features are processed to generate probabilistic predicates (On, LeftOf, CloseTo, Clear, Touching) with calibrated confidences. The planner uses PDDL to generate action plans, triggering information-gathering actions (look_closer) when predicate confidence falls below threshold τ_plan=0.7. The system is trained on 10,047 synthetic PyBullet scenes with weighted focal loss and relation-specific thresholds.

## Key Results
- Overall symbol prediction F1=0.68, with On relation at F1=0.52
- Average task success rate of 90.7% across three benchmarks (Simple Stack: 94%, Deep Stack: 90%, Clear+Stack: 88%)
- Planning time of 10-15 ms per decision
- Exceeds strongest POMDP baseline by 10-14 percentage points
- Theoretical bounds linking uncertainty calibration to planning convergence validated within 17% of theory

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Relational Reasoning for Geometry
The combination of transformer-based attention with Graph Neural Networks (GNNs) allows the system to handle variable object counts while enforcing strict geometric constraints required for manipulation. The Transformer uses self-attention to aggregate global context and select relevant objects from the visual feature map. The GNN then processes these objects as nodes in a graph, using 18-dimensional edge features to learn spatial constraints via message passing. Core assumption: Objects are detectable via bounding boxes, and their geometric features are sufficient to disambiguate spatial relations like "On" or "LeftOf" despite occlusion.

### Mechanism 2: Uncertainty-Driven Information Gathering
Explicitly quantifying uncertainty allows the system to defer risky actions and gather information, improving success rates over deterministic planners. The "Neural-Symbolic Translator" outputs probabilistic predicates (e.g., On(A, B) = 0.46). If the confidence falls within the uncertain range (1-τ_plan, τ_plan), the planner triggers an information-gathering action (like look_closer) to reduce entropy before committing to a manipulation primitive. Core assumption: The uncertainty reduction rate α (approx. 0.29) remains roughly constant per sensing action.

### Mechanism 3: Calibration-Dependent Convergence
The theoretical guarantee of planning convergence depends strictly on the calibration quality (Expected Calibration Error, ECE) of the perceptual model. The paper establishes Theorem 3, which bounds the number of information-gathering steps required to reach a confident state. This bound explicitly includes the calibration error; well-calibrated models ensure the planner converges quickly, while miscalibrated models invalidate the guarantee. Core assumption: The calibration measured in the training/validation domain transfers to the deployment domain.

## Foundational Learning

- **Concept: Markov Random Fields (MRFs) & Loopy Belief Propagation**
  - Why needed: The system uses an MRF to model dependencies between predicates (e.g., On(A,B) implies Clear(B) is false). Understanding this is necessary to grasp how the system refines raw neural confidences into consistent symbolic states.
  - Quick check: If the neural network predicts high confidence for both On(A,B) and Clear(B), how does the MRF layer resolve this conflict?

- **Concept: Expected Calibration Error (ECE)**
  - Why needed: The theoretical convergence guarantees depend on the ECE. You cannot evaluate if the planner will theoretically converge without measuring how well the confidence scores match empirical accuracy.
  - Quick check: If a model predicts "On" with 80% confidence, and it is correct 80% of the time across 100 samples, is it perfectly calibrated?

- **Concept: PDDL (Planning Domain Definition Language)**
  - Why needed: The final planning stage uses a PDDL planner. You need to understand how to define actions (pick, place), predicates (On, Clear), and preconditions to modify the domain for new tasks.
  - Quick check: What is the difference between a predicate (e.g., Clear(obj)) and a function in PDDL, and which one does this framework primarily rely on?

## Architecture Onboarding

- **Component map:** RGB Image -> ResNet-18 -> Self-Attention -> Bounding Boxes -> GNN (18-dim Edge Features) -> Probabilistic Predicates -> MRF Refinement -> Calibrated Beliefs -> PDDL Planner

- **Critical path:** The transformation from raw relative 3D poses (Edge Features in GNN) to Probabilistic Predicates. If the 3D pose extraction is inaccurate, the GNN cannot learn "On" relations, causing the MRF to propagate incorrect uncertainties, and the planner will either loop infinitely or fail.

- **Design tradeoffs:** ResNet-18 vs. ResNet-50 for better speed-accuracy trade-off (15ms vs 21ms inference despite ~2% drop in success rate). Fixed vs. Adaptive Thresholds: Adaptive thresholds drastically improve F1 (0.68 vs 0.28) but require a validation set for tuning.

- **Failure signatures:** Occlusion-heavy "On" relations: F1 drops to 0.52 (vs 0.68 overall) because the 3D geometry is obscured. Sim-to-real gap: The paper notes that "Calibration Transfer" is a critical assumption; real-world sensor noise may violate the ECE bounds assumed by the convergence theorem.

- **First 3 experiments:**
  1. Validation of Calibration: Compute ECE on a held-out test set to ensure Theorem 3 bounds apply (Target: ECE ≈ 0.073).
  2. Threshold Ablation: Vary τ_plan (e.g., 0.5, 0.7, 0.9) on a simple stacking task to observe the trade-off between planning time and success rate.
  3. Relation-Specific Testing: Evaluate "On" vs. "LeftOf" accuracy independently to verify that the geometric edge features in the GNN are functioning correctly for vertical stacking tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the theoretical guarantees linking uncertainty calibration to planning convergence transfer from synthetic PyBullet environments to physical robotic platforms?
- Basis in paper: The authors state in the Limitations section that calibration transfer "requires real-world validation" and that theoretical guarantees may not hold if calibration degrades on real sensors.
- Why unresolved: The study relies on simulated sensor noise and physics; real-world factors like complex lighting, material properties, and mechanical backlash are not modeled.
- What evidence would resolve it: Empirical validation on physical robots (e.g., Franka Emika Panda) showing that the measured Expected Calibration Error (ECE) remains within bounds that satisfy the convergence criteria derived in Theorem 3.

### Open Question 2
- Question: Can the framework be applied to non-robotic domains, such as grid world navigation or visual question answering, without domain-specific architectural modifications?
- Basis in paper: In the Future Work section, the authors list "Cross-Domain Validation" as a goal, noting that validation on domains like grid worlds or scene understanding "remains future work due to scope limitations."
- Why unresolved: While the authors claim the framework is domain-agnostic, all empirical results are derived solely from tabletop manipulation scenarios.
- What evidence would resolve it: Successful implementation of the neuro-symbolic translator and planner on a non-manipulation task (e.g., grid world) demonstrating maintained performance and theoretical validity.

### Open Question 3
- Question: Can multi-view fusion or explicit 3D geometric reasoning modules significantly improve the detection accuracy of occluded "On" relations?
- Basis in paper: The authors identify "On Relation Detection" as a specific limitation (F1=0.52) and explicitly suggest that "Further improvements could be achieved through enhanced 3D reasoning or multi-view fusion."
- Why unresolved: The current single-view architecture struggles with the geometric complexity and occlusion inherent in stacking tasks.
- What evidence would resolve it: An ablation study integrating 3D reasoning into the GNN edge features, demonstrating a statistically significant increase in the F1 score for "On" predicates compared to the current 2D spatial validation method.

## Limitations
- The theoretical convergence bounds rely on strict calibration transfer from synthetic to real domains, which may not hold under sensor noise or domain shift. The 17% empirical-theory gap suggests practical bounds are looser than ideal.
- The framework assumes objects are detectable via bounding boxes; severe occlusion or textureless objects could break the geometric reasoning pipeline.
- Fixed thresholds (τ_On=0.5, τ_plan=0.7) were tuned on synthetic data; real-world performance may degrade without adaptive threshold selection.

## Confidence

- **High Confidence:** Task success rates (90.7% avg, 94% Simple Stack) and F1 scores (0.68 overall) are directly measured on synthetic test sets with ground truth.
- **Medium Confidence:** The 10-14% POMDP baseline improvement assumes the baseline is correctly implemented and comparable; details of the baseline's perceptual front-end are not fully specified.
- **Low Confidence:** Sim-to-real transfer claims lack empirical validation—no real-robot experiments are reported to verify calibration holds under actual sensor noise.

## Next Checks

1. **Calibration Transfer Test:** Measure ECE on real sensor data (e.g., RealSense) and compare to synthetic ECE to validate Theorem 3 bounds hold in practice.
2. **Occlusion Robustness:** Systematically occlude "On" objects in synthetic scenes and measure F1 degradation to quantify the geometric reasoning failure point.
3. **Threshold Sensitivity:** Sweep τ_plan from 0.5 to 0.9 on a held-out validation set to map the planning time vs. success rate trade-off curve.