---
ver: rpa2
title: Is Trust Correlated With Explainability in AI? A Meta-Analysis
arxiv_id: '2504.12529'
source_url: https://arxiv.org/abs/2504.12529
tags:
- trust
- explainability
- systems
- ieee
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges the assumption that explainability in AI
  systems inherently increases user trust. Through a meta-analysis of 90 studies,
  it reveals a statistically significant but moderate positive correlation (r = 0.194)
  between AI explainability and trust.
---

# Is Trust Correlated With Explainability in AI? A Meta-Analysis

## Quick Facts
- **arXiv ID:** 2504.12529
- **Source URL:** https://arxiv.org/abs/2504.12529
- **Reference count:** 40
- **Primary result:** Meta-analysis reveals moderate positive correlation (r = 0.194) between AI explainability and user trust across 90 studies

## Executive Summary
This meta-analysis examines the relationship between AI explainability and user trust by synthesizing findings from 90 studies. The research reveals a statistically significant but modest positive correlation (r = 0.194) between explainability and trust, challenging the common assumption that explainability is the primary driver of trust in AI systems. The findings suggest that while explainability contributes to building trust, it is not the predominant factor, and a more holistic approach to AI trustworthiness is needed. The study emphasizes the importance of considering ethical considerations, bias management, and socio-technical understanding in developing trustworthy AI systems, particularly in critical domains such as healthcare and justice.

## Method Summary
The study conducted a systematic meta-analysis of 90 studies examining the relationship between AI explainability and user trust. The researchers analyzed correlation coefficients across diverse domains and user populations to quantify the overall relationship. Statistical analysis was performed to assess the strength and significance of the correlation, accounting for heterogeneity across studies in terms of methodologies, explainability types, and application contexts.

## Key Results
- Statistically significant but moderate positive correlation (r = 0.194) between AI explainability and trust
- Explainability contributes to trust but is not the predominant factor
- Effect size remains relatively small, suggesting other factors play crucial roles in AI trustworthiness

## Why This Works (Mechanism)
The moderate correlation between explainability and trust operates through multiple psychological pathways. When users receive explanations about AI decisions, it reduces uncertainty and helps them understand the system's reasoning, which can increase confidence. However, the modest effect size suggests that trust formation is influenced by a complex interplay of factors including perceived fairness, accountability, reliability, and alignment with user values. The relationship appears context-dependent, varying based on the stakes of the decision, user expertise, and domain-specific requirements.

## Foundational Learning
- **Correlation vs Causation** - Understanding that statistical association does not prove explainability directly causes trust changes; necessary to avoid overinterpreting the relationship
- **Effect Size Interpretation** - Learning to contextualize r = 0.194 as a small-to-moderate effect that has practical implications but isn't overwhelming; quick check: compare to typical effect sizes in social science research
- **Publication Bias Awareness** - Recognizing that studies showing stronger correlations may be more likely to be published; quick check: examine funnel plots and conduct fail-safe N calculations
- **Heterogeneity Sources** - Identifying how domain differences, user populations, and explainability types create variation in the correlation; quick check: test for moderator effects
- **Trust Multi-dimensionality** - Understanding that trust encompasses competence, benevolence, and integrity dimensions; quick check: map explainability's contribution to each dimension
- **Meta-analysis Methodology** - Grasping how combining multiple studies provides more robust estimates than individual studies; quick check: verify inclusion/exclusion criteria transparency

## Architecture Onboarding
**Component Map:** Studies -> Effect Sizes -> Correlation Pool -> Statistical Analysis -> Overall Correlation (r = 0.194)
**Critical Path:** Study Selection → Data Extraction → Effect Size Calculation → Statistical Aggregation → Interpretation
**Design Tradeoffs:** Comprehensive inclusion increases generalizability but introduces heterogeneity; focused inclusion reduces heterogeneity but limits applicability
**Failure Signatures:** Publication bias skewing results toward positive findings; methodological inconsistencies across studies; domain-specific factors masking general patterns
**First 3 Experiments:**
1. Subgroup analysis by risk domain to test correlation consistency
2. Sensitivity analysis excluding methodologically weak studies
3. Experimental testing of explainability-trust interaction with other trust factors

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Moderate effect size (r = 0.194) suggests explainability alone has limited impact on trust
- High heterogeneity across 90 studies in domains, populations, and explainability types
- Potential publication bias favoring studies showing stronger correlations

## Confidence
- **Main Finding:** Medium - Statistical analysis is sound but effect size is modest and field may have publication bias
- **Generalizability:** Medium - Results hold across diverse studies but specific contexts remain unclear
- **Causal Claims:** Low - Correlation does not establish causation between explainability and trust

## Next Checks
1. Conduct subgroup meta-analysis examining the explainability-trust correlation across different risk domains (healthcare, finance, consumer applications) to identify whether the relationship varies by consequence severity.
2. Perform sensitivity analysis by systematically excluding studies with potential methodological weaknesses (small samples, convenience sampling, lack of control groups) to test robustness of the correlation.
3. Design experimental studies specifically testing interaction effects between explainability and other trust factors (transparency, accountability, fairness) to quantify relative contributions rather than examining each in isolation.