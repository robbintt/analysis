---
ver: rpa2
title: 'Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic
  Reasoning'
arxiv_id: '2509.04083'
source_url: https://arxiv.org/abs/2509.04083
tags:
- language
- formal
- reasoning
- llms
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the impact of formal language choice on\
  \ neurosymbolic LLM reasoning. The authors conduct an extensive empirical study\
  \ comparing four formal languages (Pyke, ASP, NLTK, FOL) across three datasets (ProntoQA,\
  \ ProofWriter, FOLIO) and seven LLMs (8B\u2013671B parameters)."
---

# Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning

## Quick Facts
- arXiv ID: 2509.04083
- Source URL: https://arxiv.org/abs/2509.04083
- Reference count: 34
- Key outcome: FOL outperforms ASP, NLTK, and Pyke as intermediate formal language for neurosymbolic reasoning, with up to 85.45% accuracy on DeepSeek-R1

## Executive Summary
This paper investigates how formal language choice affects neurosymbolic reasoning with LLMs. The authors conduct an extensive empirical study comparing four formal languages (Pyke, ASP, NLTK, FOL) across three datasets and seven LLMs ranging from 8B to 671B parameters. The core method involves translating natural language reasoning problems into formal languages using LLMs, then solving them with symbolic solvers. The primary finding is that FOL achieves the best reasoning performance, with syntax differences significantly impacting LLM reasoning performance through translation errors that vary across formal languages.

## Method Summary
The method translates natural language reasoning problems into formal languages using LLMs with in-context learning, then solves them with symbolic solvers. The study uses three datasets: ProntoQA (500 samples, CWA, depth-5), ProofWriter (600 samples, OWA, depth-5), and FOLIO (204 samples, OWA). Seven LLMs are tested (8B-671B parameters): GPT-4o-mini, Ministral-8B, Llama-8B, DeepSeek-8B, DeepSeek-32B, DeepSeek-V3, DeepSeek-R1. The approach uses temperature=0, max_tokens=2048 (20480 for DeepSeek-R1), with one hand-crafted ICL example per dataset/formal language combination. Solvers include Clingo (ASP), Pyke implementation, and Prover9 via NLTK (NLTK/FOL).

## Key Results
- FOL achieves highest average accuracy (65.29% ± 2.52%) across experiments
- Small language models (SLMs) show the largest performance improvements when using appropriate formal languages
- Pyke has lowest performance due to formatting errors (missing line breaks)
- ASP maintains high execution-rate but struggles with negation semantics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FOL outperforms logic programming languages as an intermediate formal language for neurosymbolic reasoning
- Mechanism: FOL's syntax appears more frequently in LLM training corpora (textbooks, mathematical logic resources), enabling better autoformalization through stronger pattern matching during in-context learning
- Core assumption: Performance differences correlate with training data prevalence
- Evidence anchors: FOL achieves 65.29% average accuracy vs. 53.94% for ASP and 45.83% for Pyke

### Mechanism 2
- Claim: Small Language Models (SLMs) show the largest performance variance and potential gains from formal language selection
- Mechanism: SLMs have less robust syntactic pattern recognition, making them more sensitive to language-specific error modes
- Core assumption: SLM performance variance stems from weaker syntactic generalization
- Evidence anchors: Llama-8B showed ASP at 6.78% vs. NLTK at 54.94%, while DeepSeek-8B showed Pyke at 1.43% vs. FOL at 33.15%

### Mechanism 3
- Claim: Language-specific error modes create distinct execution-rate vs. execution-accuracy tradeoffs
- Mechanism: Each formal language has characteristic failure patterns affecting the decomposition of overall accuracy
- Core assumption: Error patterns are systematic and language-intrinsic
- Evidence anchors: Pyke fails on formatting, ASP struggles with negation, NLTK/FOL share similar errors but FOL achieves higher execution-rate

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The translation step from natural language to formal language relies entirely on ICL—providing instructions and examples without weight updates
  - Quick check question: Can you explain why ICL performance would vary across formal languages with different prevalence in pre-training corpora?

- Concept: **Execution-Rate vs. Execution-Accuracy Decomposition**
  - Why needed here: The paper uses this decomposition to diagnose where failures occur
  - Quick check question: If ASP has 80% execution-rate and 70% execution-accuracy, what is its overall accuracy? What if FOL has 75% execution-rate and 85% execution-accuracy?

- Concept: **Autoformalization**
  - Why needed here: This is the core task—automatic translation from natural language to formal representation
  - Quick check question: Why might autoformalization to FOL succeed where autoformalization to ASP fails for the same natural language problem?

## Architecture Onboarding

- Component map: Natural Language Problem → [LLM + ICL Prompt] → Formal Language Representation → [Symbolic Solver] → Solution

- Critical path:
  1. Select formal language based on task requirements
  2. Craft ICL example with correct syntax for chosen language
  3. Set temperature=0, max-output-tokens appropriately
  4. Execute translation; check execution-rate (syntactic validity)
  5. If valid, pass to symbolic solver; check execution-accuracy (semantic correctness)

- Design tradeoffs:
  - FOL: Highest average performance; requires classical logic semantics
  - ASP: Handles non-monotonic reasoning well; negation confusion errors
  - NLTK: Strong execution-accuracy; wrapper around Prover9
  - Pyke: Lowest performance overall; formatting errors dominate

- Failure signatures:
  - Pyke: Missing line breaks in output
  - ASP: Invalid negation syntax
  - NLTK/FOL: Parentheses errors, predicate arity inconsistencies
  - Truncation: Insufficient max-output-tokens for reasoning models

- First 3 experiments:
  1. Run Std. and CoT prompting on your target dataset with your chosen LLM to establish non-neurosymbolic baselines
  2. Test all four formal languages on 50-100 samples from your dataset with temperature=0 to identify which language performs best
  3. For the best-performing language, manually inspect failed translations to categorize error types and adjust ICL examples

## Open Questions the Paper Calls Out

- Can fine-tuning Small Language Models (SLMs) on custom-designed intermediate languages outperform their performance on standard formal languages like FOL or ASP? The authors plan to explore crafting custom intermediate languages and fine-tuning SLMs on these languages in future work.

- To what extent does the prevalence of specific formal languages in pre-training data cause the observed performance differences across different LLMs? The authors hypothesize that performance differences can be explained with a lack or abundance of the formal languages in the training data.

- How can logic programming languages be adapted to effectively handle the classical logic concepts required by datasets like FOLIO? The authors excluded ASP and Pyke from the FOLIO dataset experiments because instances require classical logic concepts which are effectively impossible to encode in standard logic programming.

## Limitations
- Limited formal language space (four languages) may not capture the full landscape of potential intermediate representations
- Hand-crafted ICL examples not shared, raising concerns about reproducibility and prompt quality confounding language effects
- Performance differences assumed to correlate with training data prevalence without direct empirical validation

## Confidence

- **High Confidence**: FOL outperforming ASP/NLTK/Pyke on average; execution-rate/accuracy decomposition framework; SLM sensitivity to language choice
- **Medium Confidence**: Mechanism linking training data prevalence to performance differences; Pyke's formatting errors being the primary failure mode
- **Low Confidence**: The 20480 token setting for DeepSeek-R1 being universally optimal; ASP's strong performance on DeepSeek-R1 being replicable across other reasoning models

## Next Checks
1. Conduct corpus analysis of formal language exposure in LLM training data to test the prevalence-performance hypothesis
2. Replicate error taxonomy analysis across all languages using automated syntactic parsing to quantify error type distributions
3. Test the 20480 token configuration on other reasoning-focused LLMs (Claude-3-5-Sonnet, o1) to verify whether this setting generalizes beyond DeepSeek-R1