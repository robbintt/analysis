---
ver: rpa2
title: 'Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form
  Mental Health Support'
arxiv_id: '2509.14851'
source_url: https://arxiv.org/abs/2509.14851
tags:
- reasoning
- health
- responses
- support
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of generating high-quality,\
  \ empathetic responses for long-form mental health counseling texts in Chinese,\
  \ where existing models often lack structured reasoning and therapeutic depth. The\
  \ authors introduce Empathy-R1, a framework that integrates Chain-of-Empathy (CoE)\
  \ reasoning with reinforcement learning to guide models through a structured, multi-layered\
  \ analysis of emotions, causes, and intentions\u2014inspired by cognitive-behavioral\
  \ therapy."
---

# Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support

## Quick Facts
- **arXiv ID:** 2509.14851
- **Source URL:** https://arxiv.org/abs/2509.14851
- **Reference count:** 7
- **Primary result:** Empathy-R1 achieves 44.30% Win@1 rate in human evaluations for Chinese mental health counseling, outperforming strong baselines through Chain-of-Empathy reasoning and reinforcement learning.

## Executive Summary
This paper addresses the challenge of generating high-quality, empathetic responses for long-form mental health counseling texts in Chinese, where existing models often lack structured reasoning and therapeutic depth. The authors introduce Empathy-R1, a framework that integrates Chain-of-Empathy (CoE) reasoning with reinforcement learning to guide models through a structured, multi-layered analysis of emotions, causes, and intentions—inspired by cognitive-behavioral therapy. To support this, they construct and release Empathy-QA, a large-scale Chinese dataset of long counseling texts. Empirical results show that Empathy-R1 significantly outperforms strong baselines on automatic metrics and achieves a 44.30% Win@1 rate in human evaluations, demonstrating superior performance in generating contextually appropriate and empathetic responses.

## Method Summary
Empathy-R1 combines Chain-of-Empathy (CoE) reasoning with a two-stage training pipeline. First, Supervised Fine-Tuning (SFT) on 200 professionally reviewed examples instills the CoE structure, forcing the model through a four-layer analysis (emotions, causes, intent, response strategy) inspired by CBT. Second, Group Relative Policy Optimization (GRPO) refines therapeutic quality using a composite reward (format adherence + answer quality), normalizing within groups of candidate outputs to handle multiple valid responses. The framework is built on Qwen3-8B and uses a contrastive-learning reward model trained on positive/negative response pairs.

## Key Results
- **Human evaluation:** 44.30% Win@1 rate on Empathy-QA test set, significantly outperforming MentraSuite (38.50%) and other baselines.
- **Automatic metrics:** Consistent improvements across BLEU-1, ROUGE-L, METEOR, and Distinct-1 on both Empathy-QA and PsyQA datasets.
- **CoE effectiveness:** The Chain-of-Empathy structure enables deeper therapeutic reasoning compared to direct generation approaches.

## Why This Works (Mechanism)

### Mechanism 1: Structured Multi-Layer Reasoning Decomposition
- **Claim:** Decomposing empathetic response generation into a sequential four-layer analysis improves therapeutic relevance over direct generation.
- **Mechanism:** The Chain-of-Empathy (CoE) forces the model through a cognitively-grounded pipeline: L1 identifies emotions in context (grounded in Appraisal Theory), L2 infers causes and cognitive biases (inspired by mentalizing), L3 discerns communication intent (from Davis's multi-dimensional empathy), and L4 synthesizes a response strategy (from active listening/NURSE frameworks). This prevents superficial pattern matching by requiring explicit intermediate reasoning.
- **Core assumption:** Therapeutic reasoning principles from cognitive-behavioral therapy transfer effectively to autoregressive language model reasoning chains.
- **Evidence anchors:** [abstract], [Section 3.1], [corpus]

### Mechanism 2: SFT-GRPO Training Synergy
- **Claim:** Combining supervised fine-tuning with group-relative reinforcement learning produces better results than either approach alone.
- **Mechanism:** SFT first instills the CoE structural scaffolding using a small high-quality dataset (200 professionally vetted examples), creating a predictable policy. GRPO then refines therapeutic quality by optimizing relative quality within sampled groups, using a composite reward (format adherence + answer quality). The key insight is that SFT provides structural stability that makes subsequent RL optimization tractable.
- **Core assumption:** A small curated dataset can successfully cold-start the reasoning format, and relative quality optimization is sufficient to improve therapeutic nuance without collapsing structure.
- **Evidence anchors:** [Section 4.3.1, Table 3], [Section 3.2], [corpus]

### Mechanism 3: Group-Relative Reward Normalization
- **Claim:** Normalizing rewards within groups of candidate outputs handles the multiple-valid-responses problem in counseling better than absolute reward signals.
- **Mechanism:** For each query, the model generates G=4 candidates. Rewards are normalized using group mean and standard deviation (Equation 1), computing advantages as deviation from group average rather than absolute scores. This emphasizes outputs that are relatively superior within context, avoiding the need to define universal quality thresholds that may not exist for subjective therapeutic responses.
- **Core assumption:** Mental health counseling admits multiple valid responses, and relative quality is more learnable than absolute quality.
- **Evidence anchors:** [Section 3.3.1], [Section 4.1.1], [corpus]

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF):**
  - Why needed here: GRPO is a variant of policy optimization used in RLHF; understanding KL penalties, policy ratios, and advantage computation is essential.
  - Quick check question: Can you explain why the KL divergence term in Equation 2 prevents the policy from deviating too far from the SFT reference?

- **Cognitive Behavioral Therapy (CBT) Principles:**
  - Why needed here: The CoE layers are explicitly grounded in CBT concepts (cognitive biases, belief-emotion links); understanding this informs debugging of reasoning chains.
  - Quick check question: In CBT, what is the relationship between automatic thoughts, intermediate beliefs, and core beliefs—and which CoE layer targets each?

- **Contrastive Learning:**
  - Why needed here: The reward model uses triplet loss with margin-based contrastive learning to distinguish empathetic from non-empathetic responses.
  - Quick check question: In Equation 5, what happens to the loss when the margin m is increased, and how does this affect reward model selectivity?

## Architecture Onboarding

- **Component map:** Qwen3-8B -> SFT Module (LLaMA-Factory) -> Reward Model (contrastive-learning encoder) -> GRPO Optimizer (Verl) -> Output Format (XML-style CoE tags + answer)

- **Critical path:**
  1. Prepare Empathy-QA data (filter, anonymize)
  2. Generate CoE chains for 200 samples using Deepseek-R1 + professional review
  3. SFT training to instill CoE format
  4. Train reward model on positive/negative response pairs
  5. GRPO training for 1,000 steps with G=4 candidates per query

- **Design tradeoffs:**
  - Small SFT dataset (200) vs. larger: Authors chose quality over quantity for cold-start; tradeoff is potential overfitting to specific CoE phrasings
  - Binary format reward vs. soft: Binary ensures strict structure but may be brittle if model explores valid variations
  - ROUGE-L vs. BLEU/METEOR: Paper argues lower ROUGE-L with higher BLEU/METEOR indicates paraphrasing over copying, but this requires human validation

- **Failure signatures:**
  - SFT-only outputs: Rigid, formulaic responses that follow structure but lack therapeutic nuance (Table 3 shows NAvg drop from 0.246→0.232)
  - GRPO-only training: Unstable optimization without structural foundation
  - Reward model misalignment: High reward scores but poor human preference (monitor Win@1 vs. automatic metrics divergence)

- **First 3 experiments:**
  1. **Format ablation:** Test whether removing each CoE layer (L1-L4 individually) degrades performance to validate each layer's contribution
  2. **Reward threshold sensitivity:** Vary the threshold T in Equation 6 across {0.3, 0.5, 0.7} to find optimal precision-recall tradeoff for answer rewards
  3. **Cross-dataset generalization:** Evaluate on PsyQA (out-of-domain) vs. Empathy-QA (in-domain) to measure whether CoE structure transfers across counseling corpora (Table 2 shows 44.30%→37.50% Win@1 drop, worth investigating why)

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- **Small SFT dataset:** Only 200 samples used for cold-starting CoE structure, raising concerns about generalization across 29 diverse topics.
- **Underspecified reward model:** Critical details like embedding architecture and threshold T are not provided, making reproduction difficult.
- **Limited baseline comparison:** Strong results shown against only two baselines, with incremental improvement over MentraSuite (38.50%→44.30%).

## Confidence
- **High confidence** in the core mechanism: Chain-of-Empathy's structured four-layer reasoning process is well-grounded in CBT principles and clearly articulated.
- **Medium confidence** in empirical results: While automatic metrics show consistent improvements and human evaluation demonstrates strong preference, the small SFT dataset and limited baseline comparisons introduce uncertainty about real-world generalizability.
- **Low confidence** in reward model specifics: Critical implementation details (embedding architecture, threshold selection, reward function parameters) are not provided, making it difficult to assess whether the RL optimization actually learned therapeutic reasoning versus optimizing for surface-level patterns.

## Next Checks
1. **Format ablation study**: Systematically remove each CoE layer (L1-L4) individually and measure performance degradation to validate that each layer contributes meaningfully rather than creating structural overhead.
2. **Cross-dataset generalization**: Evaluate Empathy-R1 on PsyQA (out-of-domain) versus Empathy-QA (in-domain) to quantify whether the CoE structure transfers across counseling corpora. The 44.30%→37.50% Win@1 drop suggests domain dependence worth investigating.
3. **Reward threshold sensitivity**: Vary the answer reward threshold T across multiple values (0.3, 0.5, 0.7) to find optimal precision-recall tradeoff and assess whether current threshold selection was empirically justified or arbitrary.