---
ver: rpa2
title: Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction
  Models via Future State Perturbation
arxiv_id: '2505.06134'
source_url: https://arxiv.org/abs/2505.06134
tags:
- attack
- adversarial
- trajectory
- prediction
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for generating realistic adversarial
  attacks on trajectory prediction models in autonomous vehicles. The key innovation
  is perturbing not only the past but also the future states of adversarial agents,
  while constraining the resulting trajectories to be dynamically feasible and tactically
  consistent with the original behavior.
---

# Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation

## Quick Facts
- arXiv ID: 2505.06134
- Source URL: https://arxiv.org/abs/2505.06134
- Reference count: 33
- Primary result: Novel method for generating realistic adversarial attacks on trajectory prediction models by perturbing control inputs and constraining future states to ensure dynamic feasibility and tactical consistency

## Executive Summary
This paper introduces a novel method for generating realistic adversarial attacks on trajectory prediction models in autonomous vehicles. The key innovation is perturbing not only the past but also the future states of adversarial agents, while constraining the resulting trajectories to be dynamically feasible and tactically consistent with the original behavior. The approach perturbs control inputs (acceleration and curvature) rather than positions directly, and introduces constraints on future states to ensure realistic behavior throughout the trajectory. Experiments on the L-GAP dataset show that this method produces attacks that are both effective at deceiving prediction models and physically realizable, unlike existing position-based attacks that generate infeasible trajectories.

## Method Summary
The method generates adversarial attacks by perturbing control inputs (acceleration and curvature) extracted from trajectories via a kinematic bicycle model, rather than directly manipulating positions. This ensures dynamic feasibility of the perturbed trajectories. The attack optimization uses Projected Gradient Descent with constraints on both past and future states to maintain tactical consistency. A novel false negative collision attack objective is introduced that deceives models into predicting non-colliding trajectories when collisions are actually imminent. The approach constrains future state perturbations through regularization terms that penalize deviations, particularly at the final time step, which propagates constraints backward through the dynamic model.

## Key Results
- Position-based attacks generate physically impossible trajectories with accelerations exceeding 3g, while control-action attacks maintain realistic acceleration bounds
- Constraining future state perturbations preserves tactical behavior, resulting in perturbed trajectories far more similar to ground truth
- The false negative collision attack successfully deceives models into predicting non-colliding trajectories when collisions are actually imminent in over 70% of cases
- Trade-off exists between attack efficiency and realism: tighter constraints reduce perturbation magnitude but also reduce prediction error increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining future state perturbations preserves tactical behavior, making attacks more meaningful.
- Mechanism: By perturbing control inputs (acceleration, curvature) and applying trajectory-specific constraints to both past and future states, the method ensures the adversarial trajectory remains within the same maneuver class as the original. The paper introduces a time-trajectory-specific regularization term that penalizes deviations, particularly at the final time step, which propagates constraints backward through the dynamic model.
- Core assumption: The tactical behavior is primarily defined by the path curvature and longitudinal profile, and the kinematic bicycle model is sufficient for feasibility.
- Evidence anchors:
  - [section 4.4.2]: "For example, when making a left turn... it is crucial that, at prediction time t = 0, the target agent can still feasibly complete the turn... Therefore we introduce a combined regularization function."
  - [section 6.2]: "imposing constraints on future positions... results in perturbed trajectories which are far more similar to the ground truth... likely because adding the constraints on future perturbed trajectories impose further limits on the latter timesteps of the observed trajectory."
- Break condition: The regularization parameters are too loose, allowing the trajectory to drift into a different maneuver class, or the dynamic model is too simple for high-speed maneuvers.

### Mechanism 2
- Claim: Perturbing control inputs rather than positions guarantees dynamic feasibility.
- Mechanism: Instead of directly manipulating position coordinates (which can yield physically impossible accelerations), the method extracts and perturbs control inputs (acceleration a, curvature κ) via an inverse kinematic bicycle model. Hard constraints on these control inputs ensure the reconstructed trajectory is executable by a real vehicle.
- Core assumption: The vehicle's motion can be adequately modeled by a kinematic bicycle model.
- Evidence anchors:
  - [section 4.1]: "To ensure dynamic feasibility of the perturbed trajectories, we follow Cao et al. [4] and perturb not the recorded positions Xtar... but instead the control inputs u = {a, κ} underlying this trajectory."
  - [section 6.1]: "For both established methods... the average acceleration magnitude a exceeds 30 ms⁻² ≈ 3g... Trajectories with such extreme acceleration and curvature values cannot be reproduced in real vehicles."
- Break condition: The vehicle is operating at friction limits where the kinematic bicycle model is inaccurate, or control input constraints are set too wide.

### Mechanism 3
- Claim: A false negative collision attack objective can deceive a model into a false sense of safety.
- Mechanism: The novel attack objective minimizes the distance between the adversarial prediction and the ground truth prediction while simultaneously minimizing the distance between the perturbed adversarial path and the ego vehicle. This creates a scenario where the adversarial agent is on a collision course, but the model predicts a safe, non-colliding trajectory.
- Core assumption: The prediction model relies heavily on recent past trajectory history and is insensitive to subtle, dynamically feasible deviations that alter the future outcome.
- Evidence anchors:
  - [abstract]: "We introduce a novel 'false negative collision attack' objective that successfully deceives models into predicting non-colliding trajectories when collisions are actually imminent."
  - [section 6.3]: "Across all three approaches... at least 70% of generated trajectories were on a collision course while the model believed they were not."
- Break condition: The prediction model incorporates robust reasoning about future goals or environmental constraints, making it less susceptible to being fooled by perturbed histories that lead to collisions.

## Foundational Learning

- Concept: **Projected Gradient Descent (PGD) for Constrained Optimization**
  - Why needed here: The method uses PGD to iteratively find adversarial perturbations. Understanding the alternating gradient step (to maximize loss) and projection step (to satisfy constraints) is essential for grasping how feasible attacks are generated.
  - Quick check question: In the PGD process, what is the role of the projection step onto the feasible set C?

- Concept: **Kinematic Bicycle Model**
  - Why needed here: The paper uses this model to translate between trajectory positions and control inputs (acceleration, curvature). Without this, the link between perturbing control inputs and generating feasible trajectories would be unclear.
  - Quick check question: Given a vehicle's current state (x, y, θ, v) and control inputs (a, κ), how does the model compute the next state?

- Concept: **Trajectory Prediction as a Probabilistic Task**
  - Why needed here: The paper attacks models that output a distribution of possible future trajectories (K predictions). The attack objectives (ADE, FDE, collision rate) are defined over this distribution, which is a key aspect of how attacks are evaluated.
  - Quick check question: Why does the paper report collision rate as the max over timesteps of a binary collision function, rather than a single distance metric?

## Architecture Onboarding

- Component map: Raw trajectory -> Initial state extraction -> Control input extraction -> **PGD Loop** (perturb controls -> reconstruct trajectory -> get model prediction -> compute loss & gradients -> project perturbations) -> Final adversarial trajectory
- Critical path: Raw trajectory -> Initial state extraction -> Control input extraction -> **PGD Loop** (perturb controls -> reconstruct trajectory -> get model prediction -> compute loss & gradients -> project perturbations) -> Final adversarial trajectory
- Design tradeoffs:
  - **Attack Efficiency vs. Realism**: Tighter constraints on future states and control inputs reduce the magnitude of adversarial perturbations, making attacks more realistic but potentially less effective at increasing prediction error (Section 6.2)
  - **Constraint Type**: Time-specific constraints limit deviation at each timestep, while trajectory-specific constraints allow more longitudinal flexibility, potentially creating more diverse but realistic attacks (Section 4.4.2)
  - **White-box vs. Black-box**: The method is white-box, requiring model gradients. This is powerful but may not reflect real-world threat models (Section 2.1)
- Failure signatures:
  - **Infeasible Trajectories**: Output trajectories have extreme accelerations (> 3g) or curvatures, indicating constraints are too loose or the dynamic model is inadequate
  - **Tactical Inconsistency**: The perturbed future trajectory executes a different maneuver than the original (e.g., turns right instead of left), violating the attack's premise
  - **Low Attack Success**: For false negative collision attacks, a low CRFNC indicates the optimization is failing to find a colliding path that deceives the model
- First 3 experiments:
  1. **Reproduce Feasibility Results**: Implement the position-based vs. control-action-based attack on a simple trajectory. Verify that the control-action method produces significantly lower and more realistic acceleration/curvature values, matching the paper's qualitative findings (Section 6.1)
  2. **Ablate Future State Constraints**: Run the control-action attack with and without constraints on the future trajectory. Measure the change in attack efficiency (ADE/FDE) and perturbation magnitude (D_max, D_mean) to confirm the trade-off discussed in Section 6.2
  3. **Test False Negative Collision Attack**: Implement the FNC loss and run it against the Trajectron++ model (or a similar surrogate). Report the collision rate along the perturbed path (CRFNC) and the predicted collision rate (CR_pred) to validate the >70% deception rate claimed in Section 6.3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the efficiency of adversarial attacks be improved without compromising dynamic feasibility and realism?
- Basis in paper: [explicit] The authors note that keeping trajectories realistic "can dramatically reduce attack efficiency" and state that "future research needs to focus on improving efficiency of dynamically feasible, realistic, and meaningful attacks."
- Why unresolved: There is a trade-off between realism and impact; realistic control-action attacks yielded significantly lower prediction errors (e.g., ADE 0.25m) compared to infeasible position-based attacks (ADE 4.0m).
- What evidence would resolve it: Novel optimization methods or constraints that allow control-action perturbations to achieve higher displacement errors comparable to unconstrained attacks while maintaining physically realizable accelerations.

### Open Question 2
- Question: How can trajectory prediction models be hardened against "false negative collision attacks"?
- Basis in paper: [explicit] The authors demonstrate that this novel attack successfully deceives models into predicting safe trajectories when collisions are imminent, concluding that "more research is required to improve robustness of prediction models towards this kind of attacks."
- Why unresolved: The experiments showed a high success rate (over 70%) for this attack, indicating current models are vulnerable to being deceived into a false "sense of safety."
- What evidence would resolve it: Architectural modifications or training protocols that significantly reduce the success rate of false negative collision attacks without increasing false positive collision rates.

### Open Question 3
- Question: Do the findings regarding control-action perturbations generalize to diverse, real-world driving datasets?
- Basis in paper: [explicit] The authors acknowledge that using the L-GAP dataset (limited to intersections) introduces bias and suggest that "evaluating the generalizability of the attacks would be greatly enhanced by applying them to datasets with more diverse scenarios such as NuScenes."
- Why unresolved: The current results are derived from a specific scenario (left-turning vehicles), and it is unknown if the constraints on future states behave similarly in complex environments with multiple lane types or agent interactions.
- What evidence would resolve it: Reproducing the attack methodology on large-scale datasets like NuScenes or Waymo to verify that future-state constraints preserve tactical behavior across varied traffic scenarios.

### Open Question 4
- Question: What defense mechanisms are effective against dynamically feasible, control-action-based adversarial attacks?
- Basis in paper: [explicit] The authors list studying responses to these attacks as future work, specifically mentioning "fine tuning models on the perturbed input data, or the use of randomized smoothing."
- Why unresolved: The paper focused entirely on the generation and evaluation of attacks; no defensive strategies were implemented or tested.
- What evidence would resolve it: Evaluations showing that models trained with adversarial robustness techniques (like randomized smoothing) maintain prediction accuracy when subjected to the proposed control-action perturbations.

## Limitations
- The L-GAP dataset availability and exact preprocessing details (filter parameters, trajectory selection criteria) are not publicly documented, potentially limiting reproducibility
- The absolute acceleration bounds (amin, amax) are defined as dataset-specific extrema but not numerically reported, creating ambiguity in the constraint formulation
- The paper assumes white-box access to prediction models, which may not reflect realistic threat models in deployed systems

## Confidence
- **High Confidence**: The core technical contribution of perturbing control inputs instead of positions to ensure dynamic feasibility is well-supported by quantitative evidence (Section 6.1 showing position-based attacks generate >3g accelerations)
- **Medium Confidence**: The effectiveness of future-state constraints in preserving tactical behavior is demonstrated qualitatively through trajectory similarity metrics, though the specific regularization parameters appear somewhat arbitrary
- **Medium Confidence**: The false negative collision attack objective successfully deceives models in the reported experiments, but the generalizability across different prediction architectures remains untested

## Next Checks
1. Implement the control-action vs. position-based attack comparison on a small trajectory set to verify the dramatic difference in feasibility metrics (acceleration/curvature bounds) reported in Section 6.1
2. Conduct an ablation study systematically varying the future state constraint strength to quantify the trade-off between attack efficiency and realism as discussed in Section 6.2
3. Test the false negative collision attack against a different prediction model (e.g., Multipath or VectorNet) to assess whether the deception rate holds across architectures, addressing a key limitation of the current evaluation