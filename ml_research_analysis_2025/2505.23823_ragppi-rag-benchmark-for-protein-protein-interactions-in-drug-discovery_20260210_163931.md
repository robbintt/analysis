---
ver: rpa2
title: 'RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery'
arxiv_id: '2505.23823'
source_url: https://arxiv.org/abs/2505.23823
tags:
- ppis
- protein
- dataset
- abstract
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAGPPI, a benchmark dataset for evaluating
  retrieval-augmented generation (RAG) systems in drug discovery, specifically focusing
  on protein-protein interactions (PPIs). The dataset contains 4,420 question-answer
  pairs derived from the BioGRID database, with 500 expert-verified gold-standard
  examples and 3,720 silver-standard examples generated using an ensemble auto-evaluation
  model.
---

# RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery

## Quick Facts
- **arXiv ID:** 2505.23823
- **Source URL:** https://arxiv.org/abs/2505.23823
- **Reference count:** 40
- **Primary result:** Introduces a benchmark dataset for evaluating RAG systems in drug discovery, focusing on protein-protein interactions (PPIs)

## Executive Summary
This paper introduces RAGPPI, a benchmark dataset for evaluating retrieval-augmented generation (RAG) systems in drug discovery, specifically focusing on protein-protein interactions (PPIs). The dataset contains 4,420 question-answer pairs derived from the BioGRID database, with 500 expert-verified gold-standard examples and 3,720 silver-standard examples generated using an ensemble auto-evaluation model. Experiments with six different models (LLM-based and RAG-based) show that current systems struggle with factual precision in this domain, with LLM-based models generally outperforming RAG-based approaches. The benchmark addresses a critical gap in evaluating RAG systems for target identification in drug discovery, where understanding the biological impacts of PPIs is essential for therapeutic development.

## Method Summary
The authors constructed RAGPPI by extracting 75,000 PPIs from the BioGRID database and generating 4,420 question-answer pairs. They stratified PPIs by literature frequency (low/medium/high) and categorized them into 7 interaction types (Reaction, Activation, Catalysis, Binding, Ptmod, Inhibition, Expression). 500 gold-standard examples were created through expert annotation, while 3,720 silver-standard examples were generated using an ensemble auto-evaluation model that uses three specialized LLMs to assess factual correctness based on atomic fact-level similarity and low-similarity fact counts.

## Key Results
- RAGPPI contains 4,420 QA pairs: 500 gold-standard (expert-verified) and 3,720 silver-standard (auto-evaluated)
- The ensemble auto-evaluation model achieves 93.71% accuracy in labeling factual correctness
- LLM-based models generally outperform RAG-based approaches on this benchmark
- Current RAG pipelines struggle with factual precision in the drug discovery domain

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** An ensemble of three specialized LLMs can approximate expert judgment on factual correctness by evaluating atomic fact-level semantic alignment.
- **Mechanism:** Answers and abstracts are decomposed into atomic facts. Two features are extracted: (F1) average cosine similarity between answer facts and abstract facts, and (F2) count of low-similarity outliers (below mean - 2×std). Three GPT-4o sub-models evaluate these features (M1 on F1 distribution, M2 on F2 distribution, M3 on global semantic alignment with ground truth), with final labels determined by majority voting.
- **Core assumption:** Expert labeling characteristics for domain-specific factual correctness can be captured through embedding similarity at the atomic fact level.
- **Evidence anchors:**
  - [abstract]: "The auto-evaluation model uses three specialized LLMs to assess factual correctness based on atomic fact-level similarity and low-similarity fact counts, achieving 93.71% accuracy."
  - [section]: Table 3 shows the ensemble model achieving 4.29% total error rate vs 42-53% for non-prompted GPT-4o variants. Independent t-tests confirmed significant differences between Correct and Incorrect groups for both F1 (p < 0.001, means 0.83 vs 0.72) and F2 (p < 0.001, means 0.22 vs 1.37).
  - [corpus]: Weak direct evidence; related work (GraPPI arXiv:2501.16382) uses GraphRAG for PPI exploration but does not address evaluation methodologies.
- **Break condition:** If domain-specific biological relationships cannot be adequately captured through embedding cosine similarity, or if atomic fact decomposition fails on complex multi-step biological pathways.

### Mechanism 2
- **Claim:** Frequency stratification and PPI type balancing create a representative benchmark that covers both validation of known mechanisms and exploration of novel interactions.
- **Mechanism:** PPIs are categorized by literature frequency (Low: frequency=1, Medium: 1 < freq < μ, High: freq ≥ μ) and 7 interaction types (Reaction, Activation, Catalysis, Binding, Ptmod, Inhibition, Expression). Equal sampling preserves balance across groups.
- **Core assumption:** Expert-identified criteria (balanced PPI types and frequency distribution) correlate with real-world utility in target identification workflows.
- **Evidence anchors:**
  - [abstract]: "Through interviews with experts, we identified criteria for a benchmark dataset, such as a type of QA and source."
  - [section]: Section 2.1 documents interviews with 5 drug discovery researchers (7-10 years experience). All experts reported using STRING/BioGRID and identified balanced distribution as essential. Table 1 shows 33-34% distribution across frequency levels.
  - [corpus]: PRING (arXiv:2507.05101) critiques existing PPI benchmarks for focusing on isolated pairwise evaluations rather than network-level reconstruction—a different but complementary concern.
- **Break condition:** If literature frequency does not correlate with task difficulty or practical relevance; if the 7 predefined PPI types fail to capture emerging interaction categories.

### Mechanism 3
- **Claim:** Current RAG pipelines underperform LLM-only approaches in this domain due to suboptimal retrievers that surface context misaligned with ground-truth biological pathways.
- **Mechanism:** LLM-based models generate answers from internal knowledge without retrieved context; RAG systems retrieve abstracts and condition generation on them. If retrieval is poor, the conditioned generation produces less factually aligned outputs than the model's internal knowledge.
- **Core assumption:** The retriever is the bottleneck; with better retrieval, RAG would outperform LLM-only approaches.
- **Evidence anchors:**
  - [abstract]: "Experiments with six different models... show that current systems struggle with factual precision in this domain, with LLM-based models generally outperforming RAG-based approaches."
  - [section]: Table 4 shows Gemini 2.0 Flash (LLM) achieves 56.18% ensemble accuracy vs GraPPI (RAG) at 13.44%. F1 scores show LLMs have higher atomic fact similarity (0.72-0.74) than RAG systems (0.63-0.71).
  - [corpus]: GraPPI (arXiv:2501.16382) specifically addresses large-scale PPI exploration with a retrieve-divide-solve pipeline, suggesting retrieval architecture significantly impacts performance.
- **Break condition:** If the issue is corpus quality rather than retrieval algorithms; if RAG underperformance stems from generator inability to synthesize retrieved context rather than retrieval quality.

## Foundational Learning

- **Concept: Protein-Protein Interactions (PPIs) in Drug Discovery**
  - **Why needed here:** The entire benchmark evaluates QA on PPI biological impacts—understanding that PPIs are cascading pathways (not binary interactions) is essential for interpreting evaluation results.
  - **Quick check question:** Can you explain why a single PPI might have multiple downstream biological effects, and why "FANCD2>BRCA2" represents a directed interaction?

- **Concept: Atomic Fact Decomposition for Evaluation**
  - **Why needed here:** The auto-evaluation model relies on decomposing answers into atomic facts for similarity scoring. Without this, you cannot understand why ensemble evaluation outperforms single-model evaluation.
  - **Quick check question:** Given the answer "FHL1 translocates to the nucleus and binds BCLAF1 to promote tumor growth," what are the atomic facts, and how would you compute their similarity to an abstract?

- **Concept: Retrieval-Augmented Generation Architecture**
  - **Why needed here:** The benchmark specifically targets RAG systems for Target ID. Understanding the retriever-generator split is critical for diagnosing why RAG underperforms.
  - **Quick check question:** If a RAG system retrieves an irrelevant abstract for a PPI question, what failure modes would you expect in the generated answer (vs. an LLM-only approach)?

## Architecture Onboarding

- **Component map:**
  - BioGRID (75K PPIs, 2.7M papers) -> Frequency stratification (H/M/L) -> PPI type classification (7 types, multi-label) -> Expert annotation (10 experts, Perfect/Acceptable/Incorrect) -> Gold-standard (500 PPIs)
  - Answer + Abstract -> Atomic fact decomposition (GPT-4o) -> Embedding (text-embedding-3-small) -> Feature extraction (F1 mean similarity, F2 outlier count) -> Ensemble (M1/M2/M3 via majority vote) -> Silver-standard (3,720 PPIs)
  - 6 systems (3 LLM, 3 RAG) -> 372 sampled PPIs -> Generated answers -> Auto-eval ensemble -> Accuracy metrics

- **Critical path:**
  1. Replicate the 500 gold-standard PPIs from BioGRID using the frequency/type stratification logic (Tables 1, 8).
  2. Validate the ensemble auto-evaluation model on the 70-sample test set (35 Correct, 35 Incorrect) to confirm 93.71% accuracy.
  3. Run baseline experiments with Gemini 2.0 Flash and GraPPI to establish the LLM > RAG performance gap before testing modifications.

- **Design tradeoffs:**
  - **Gold vs. Silver standard:** Gold (500, expert-verified) is high-quality but costly; Silver (3,720, auto-labeled) scales but inherits auto-eval errors (6.29% residual).
  - **Ensemble size:** 3 sub-models balance robustness vs. cost; fewer models increase variance, more models have diminishing returns.
  - **Threshold for F2 (low-similarity outliers):** Set at mean - 2×std (0.61); stricter thresholds increase precision but may reject valid domain-specific phrasing.

- **Failure signatures:**
  - **Hallucination in domain QA:** High F2 (many low-similarity atomic facts) indicates facts not supported by the abstract.
  - **RAG retrieval mismatch:** Low F1 (below 0.72) suggests retrieved context is misaligned with the ground-truth biological pathway.
  - **Evaluator over-prediction of "Correct":** Non-prompted GPT-4o shows 31.43% error on Accurate but 74.29% on Inaccurate (Table 3), indicating bias toward positive labels.

- **First 3 experiments:**
  1. **Retrieval ablation:** Replace BioGRID abstracts with STRING database descriptions as the retrieval corpus for GraPPI; measure if F1 scores improve (testing the retriever bottleneck hypothesis).
  2. **Feature importance:** Run the ensemble with only M1 or only M2 to quantify the contribution of each feature (F1 vs. F2) to the 93.71% accuracy.
  3. **Domain fine-tuning:** Fine-tune a small LLM (e.g., Llama-8B) on the gold-standard 500 QA pairs and compare ensemble accuracy against MedLlama (25.27%) to test whether domain adaptation closes the LLM-RAG gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do LLM-based models outperform RAG-based systems on RAGPPI, and what retrieval improvements could close this gap?
- Basis in paper: [explicit] Results show "LLM-based models outperform RAG-based counterparts," with the authors suggesting "current RAG pipelines may suffer from a suboptimal retriever for references."
- Why unresolved: The paper documents the finding but does not diagnose specific retrieval failures or propose remediation strategies.
- What evidence would resolve it: Ablation studies isolating retriever quality, retrieval context relevance, and retrieval-to-generation alignment on RAGPPI samples.

### Open Question 2
- Question: How does the ensemble auto-evaluation model's performance vary across PPI types and frequency levels?
- Basis in paper: [inferred] The benchmark stratifies by frequency (high/medium/low) and PPI types (7 categories), but auto-evaluation accuracy (93.71%) is reported only in aggregate.
- Why unresolved: It is unknown whether the auto-evaluator is equally reliable for rare PPI types or low-frequency interactions versus well-studied cases.
- What evidence would resolve it: Per-stratum accuracy and error analysis of M1, M2, M3 sub-models across frequency and PPI-type subgroups.

### Open Question 3
- Question: Can RAGPPI's benchmark design generalize to other drug discovery tasks such as ligand generation and drug repurposing?
- Basis in paper: [explicit] The conclusion states plans to "expand the applicability of this benchmark by applying diverse drug discovery domains, such as ligand generation, and drug repurposing."
- Why unresolved: The current benchmark is narrowly scoped to PPI biological impacts; criteria, QA formats, and evaluation metrics for other tasks remain undefined.
- What evidence would resolve it: Pilot adaptation of RAGPPI methodology to ligand-generation or repurposing QA, with expert-validated gold and silver sets.

## Limitations

- **Evaluator generalizability:** The auto-evaluation model achieves 93.71% accuracy on the test set but this validation is limited to 70 samples (35 correct, 35 incorrect). The model's performance on diverse PPI scenarios outside the benchmark distribution remains untested.
- **Corpus coverage:** All questions derive from BioGRID abstracts, which may not represent the full spectrum of PPI literature or emerging interaction types. The benchmark's applicability to other protein interaction databases is uncertain.
- **RAG architecture specificity:** Results show LLM-based models outperform RAG-based approaches, but this comparison uses a specific RAG configuration (single abstract retrieval). Performance may differ with multi-hop retrieval or different retriever-generator architectures.

## Confidence

- **High confidence:** The benchmark construction methodology (frequency stratification, PPI type balancing, expert interviews) is well-documented and reproducible. The auto-evaluation model architecture and its feature-based design are clearly specified.
- **Medium confidence:** The 93.71% auto-evaluation accuracy is supported by statistical testing on the validation set, but the small sample size (n=70) limits confidence in generalization. The LLM > RAG performance gap is consistently observed but attributed primarily to retrieval quality without ruling out generator limitations.
- **Low confidence:** Claims about the benchmark's utility for "real-world" target identification workflows lack empirical validation beyond the constructed evaluation scenarios.

## Next Checks

1. **Evaluator stress test:** Evaluate the auto-evaluation model on a held-out set of 200 PPIs from STRING database (not BioGRID) to assess cross-database generalizability and identify failure modes.
2. **RAG architecture ablation:** Test a multi-document RAG system that retrieves and synthesizes information from 3-5 abstracts per query, measuring if F1 scores improve relative to the single-abstract baseline.
3. **Expert ground-truth validation:** Have domain experts independently evaluate a random sample of 100 auto-labeled silver-standard answers to quantify residual error rate and identify systematic evaluator biases.