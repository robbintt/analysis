---
ver: rpa2
title: Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking
arxiv_id: '2507.17788'
source_url: https://arxiv.org/abs/2507.17788
tags:
- bias
- position
- number
- repetitions
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking

## Quick Facts
- arXiv ID: 2507.17788
- Source URL: https://arxiv.org/abs/2507.17788
- Authors: Ali Vardasbi; Gustavo Penha; Claudia Hauff; Hugues Bouchard
- Reference count: 24
- Primary result: Reduces LLM calls by ~81-87% while preserving ranking accuracy through adaptive repetition

## Executive Summary
This paper addresses position bias in LLM-based pairwise ranking by proposing adaptive repetition strategies that leverage Repetition Consistency (RC). The key insight is that for most candidate pairs, an LLM exhibits RC for at least one of the two orderings (a,b) or (b,a), allowing early termination when a conclusive majority vote is reached. Two methods are proposed: early stopping when the majority vote is conclusive, and confidence-based adaptation that uses a linear model to predict when sufficient repetitions have been made based on confidence gaps.

## Method Summary
The method implements adaptive repetition for pairwise LLM comparisons. For each candidate pair, it prompts the LLM with both orderings (a,b) and (b,a), repeating each ordering n times up to a maximum of 12. The algorithm stops early when a majority vote is conclusive (not a tie), or uses a confidence-based approach that estimates the probability gap from confidence scores to bound repetitions. The approach is evaluated on TREC-DL 2019/2020 and four DPO alignment datasets, showing significant reductions in LLM calls while maintaining accuracy.

## Key Results
- Reduces LLM calls by 81% on average using early stopping when majority vote is conclusive
- Confidence-based method achieves 87% reduction in calls with only slight accuracy trade-off
- Observation 1 holds with violation rates of 0.7-6.3% across different LLMs
- Position bias direction varies substantially within datasets, affecting both early and later items

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Early stopping is viable because for the vast majority of candidate pairs, the LLM exhibits Repetition Consistency (RC) for at least one of the two orderings (a,b) or (b,a).
- **Mechanism:** The authors hypothesize that an LLM's outcome stabilizes when its inherent preference for a candidate aligns with its position bias (e.g., the model prefers candidate A, and A is placed in the preferred position). This alignment reduces variance, resulting in a consistent verdict across repetitions for that specific ordering.
- **Core assumption:** The alignment between inherent preference and position bias creates a "stable" ordering that remains consistent even if the reverse ordering is unstable.
- **Break condition:** If an instance violates Observation 1 (i.e., neither ordering is RC), the method cannot guarantee convergence to the consensus outcome without hitting the maximum repetition cap.

### Mechanism 2
- **Claim:** A dynamic repetition strategy reduces LLM calls by ~81% while preserving accuracy by only extending repetitions for "hard" instances where the outcome is initially unclear.
- **Mechanism:** The algorithm increments repetitions ($n$) for both orderings (a,b) and (b,a) in parallel. It stops as soon as a conclusive majority voting outcome is reached (i.e., the consensus is not a tie). Instances where one ordering is stable are resolved quickly (often $n=1$), while contradictory cases consume more calls.
- **Core assumption:** A majority vote at a lower $n$ (e.g., $n=3$) predicts the consensus outcome at maximum $n$ (e.g., $n=12$) with high fidelity for non-violating instances.
- **Break condition:** If the two orderings produce contradictory stable decisions, the algorithm must continue repetition up to the maximum limit, negating efficiency gains for that specific instance.

### Mechanism 3
- **Claim:** Using a "confidence gap" to bound repetitions reduces calls by ~87% by predicting the probability gap between candidates and aborting early on distinct mismatches.
- **Mechanism:** The method prompts the LLM for a confidence value alongside its judgment. It assumes a correlation between the gap in confidence scores (when the LLM chooses A vs B) and the actual probability gap $g = |P_a - P_b|$. A larger gap implies the winner is obvious, allowing the stop criterion to trigger even sooner (capped at $(1-g)n_M + 1$).
- **Core assumption:** The LLM's self-reported confidence is a reliable proxy for the latent probability gap between candidates, and this relationship is roughly linear or predictable via a small fitted model.
- **Break condition:** If the confidence gap is small (implying a close call), the maximum number of repetitions is still required, offering no efficiency gain over the standard early-stopping method.

## Foundational Learning

- **Concept:** **Permutation Consistency (PC) vs. Repetition Consistency (RC)**
  - **Why needed here:** The paper hinges on the distinction that PC is about stability when swapping items, while RC is about stability when repeating the *same* prompt. Understanding that low RC implies model uncertainty (or high temperature effects) is vital for grasping why repetition helps.
  - **Quick check question:** If a model outputs (A, A, A) for ordering (a, b) but (A, B, A) for ordering (b, a), is it Repetition Consistent for the first ordering?

- **Concept:** **Primacy vs. Recency Bias**
  - **Why needed here:** The paper demonstrates that bias is not unidirectional; a model might favor the first item in one instance and the last in another. Mitigation strategies cannot assume a fixed "correction" vector.
  - **Quick check question:** Does a high "Primacy Biased" ratio in a dataset imply the model always prefers the first item? (Answer: No, it implies it prefers the first item *in biased instances*, but direction varies).

- **Concept:** **Consensus Outcome via Majority Voting**
  - **Why needed here:** This is the "ground truth" mechanism the paper tries to approximate efficiently. One must understand that the goal is to recover the outcome of the expensive 24-call consensus using fewer calls.
  - **Quick check question:** In a pairwise comparison with $2n$ repetitions, what result triggers the "Early Stopping" condition? (Answer: Any non-tie majority).

## Architecture Onboarding

- **Component map:** Prompt Orchestrator -> LLM Ranker -> History Accumulator -> Stopping Criteria Controller
- **Critical path:**
  1. Initialize $n=1$
  2. Prompt LLM with both orderings (a,b) and (b,a)
  3. Check if Majority Vote is conclusive (not a tie)
  4. **Confidence Extension:** Check if estimated probability gap allows for early termination
  5. If not conclusive, $n++$ and repeat
  6. Stop if $n = n_{max}$ (12) or criteria met
- **Design tradeoffs:**
  - **Temperature:** Paper uses 0.1. Higher temp increases diversity (helps majority voting) but lowers RC, potentially increasing the required repetitions
  - **Static vs. Dynamic:** Static (24 calls) is robust but expensive. Dynamic is cheap but relies on Observation 1 holding
  - **Accuracy vs. Cost:** The Confidence-Based method pushes cost lower (87% reduction) but admits a "slight accuracy trade-off"
- **Failure signatures:**
  - **High Violation Rate:** If Table 1 violation rates spike (e.g., >10%), Observation 1 fails, and accuracy will drop as the algorithm terminates on unstable/inconclusive results
  - **Confidence Calibration Failure:** If the linear model mapping confidence to probability gap is mis-calibrated, the confidence-based method may stop too early on difficult examples, degrading accuracy
- **First 3 experiments:**
  1. **Baseline RC Check:** Run 100 instances with $n=12$ for both orderings. Calculate violation rates of Observation 1 (percentage of pairs where *neither* ordering is RC) to ensure the fundamental assumption holds for your specific model/domain
  2. **Efficiency Validation:** Compare "Swap Once" (n=1), "Early Stopping," and "Static Consensus" (n=12) on a test set. Plot Accuracy vs. Average LLM Calls to visualize the trade-off curve
  3. **Confidence Calibration:** Extract confidence scores for a sample of judgments. Fit a linear model to predict the probability gap (derived from actual vote counts) from the confidence gap. Evaluate if this model generalizes to a held-out set

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What underlying input features or semantic properties cause the direction of position bias (primacy vs. recency) to vary between instances within the same dataset?
- **Basis in paper:** The authors explicitly call for "analyzing the reasons behind the varying direction of position bias within a single dataset" in the Conclusion
- **Why unresolved:** While the paper quantifies the variance in bias direction, it does not investigate the causal factors, noting only that they could be architectural or representational
- **What evidence would resolve it:** A mechanistic interpretability study correlating specific attention head patterns or tokenization artifacts with the observed direction of bias for individual instances

### Open Question 2
- **Question:** Can the proposed early-stopping methods be effectively extended to multi-item listwise ranking scenarios without incurring the factorial complexity of exhaustive permutations?
- **Basis in paper:** The Conclusion identifies "extending our early-stopping methods to more complex judgment scenarios, such as multi-item comparisons" as a primary avenue for future work
- **Why unresolved:** The current methodology is validated only on pairwise comparisons; the paper notes that generalizing to listwise ranking introduces exponential complexity regarding permutations
- **What evidence would resolve it:** An implementation of the early-stopping algorithm using linear-complexity cyclic permutations evaluated on standard listwise re-ranking benchmarks

### Open Question 3
- **Question:** Does the observation that "consistency occurs when inherent judgment aligns with bias" generalize to other cognitive biases in LLMs, such as length bias or self-enhancement bias?
- **Basis in paper:** The Conclusion states that the "observation of consistency when the inherent judgment aligns with position bias could be tested for other types of bias"
- **Why unresolved:** The paper establishes this correlation only for positional bias; it remains unverified if this behavior is a universal property of LLM reasoning under various bias conditions
- **What evidence would resolve it:** Experiments manipulating non-positional biases to see if the alignment between the model's inherent preference and the bias vector predicts repetition consistency

## Limitations

- The method's effectiveness fundamentally depends on Observation 1 holding, which is empirically observed but not theoretically guaranteed
- The confidence-based method introduces additional uncertainty through its linear regression model, which assumes a predictable relationship between confidence gaps and probability gaps
- The paper does not investigate the causal factors behind the varying direction of position bias within datasets

## Confidence

**High Confidence:** The existence and quantification of position bias in LLMs, the efficiency gains of early stopping (81% reduction), and the mechanism of Repetition Consistency

**Medium Confidence:** The asymmetric stability mechanism is plausible given the data but relies on interpreting Observation 1's implications. The majority voting consensus as "ground truth" is reasonable but assumes voting stability across repetitions.

**Low Confidence:** The confidence gap proxy for probability gaps shows correlation in the paper but lacks theoretical justification. The claim of "only slight accuracy trade-off" for the 87% efficiency gain is not quantitatively specified.

## Next Checks

1. **Cross-Domain Validation:** Test Observation 1 and the early stopping efficiency on non-TREC datasets (e.g., DPO alignment data, product recommendations) to verify the method generalizes beyond document retrieval tasks

2. **Temperature Sensitivity Analysis:** Systematically vary temperature from 0.0 to 1.0 and measure how violation rates and RC stability change to reveal the method's robustness to stochasticity

3. **Confidence Model Generalization:** Instead of using 10% of instances for calibration, implement k-fold cross-validation on the confidence gap model. Also test non-linear models (e.g., random forest) to determine if the linear assumption is optimal