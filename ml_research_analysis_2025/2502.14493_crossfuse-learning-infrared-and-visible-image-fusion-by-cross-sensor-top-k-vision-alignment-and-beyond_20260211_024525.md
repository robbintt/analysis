---
ver: rpa2
title: 'CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K
  Vision Alignment and Beyond'
arxiv_id: '2502.14493'
source_url: https://arxiv.org/abs/2502.14493
tags:
- fusion
- image
- data
- visible
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of out-of-distribution (OOD)
  data in infrared and visible image fusion (IVIF) for real-world applications. The
  authors propose a multi-view data augmentation framework to enhance model robustness
  and generalization.
---

# CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K Vision Alignment and Beyond

## Quick Facts
- **arXiv ID:** 2502.14493
- **Source URL:** https://arxiv.org/abs/2502.14493
- **Reference count:** 40
- **Primary result:** Proposes a multi-view data augmentation framework for infrared and visible image fusion (IVIF) that achieves superior OOD generalization through external and internal augmentation strategies.

## Executive Summary
This paper addresses the challenge of out-of-distribution (OOD) data in infrared and visible image fusion (IVIF) for real-world applications. The authors propose a multi-view data augmentation framework to enhance model robustness and generalization. The approach combines external data augmentation using Top-k Selective Channel Alignment to mitigate distribution shifts between datasets, and internal data augmentation employing weak-aggressive augmentation for self-supervised learning. A frequency-aware fusion network is also introduced to extract and integrate global and local features. Experimental results on multiple datasets demonstrate superior performance and robustness compared to state-of-the-art methods, with the proposed method achieving top rankings across various metrics including MI, SCD, VIF, Qabf, and SSIM. The framework significantly enhances the reliability and stability of IVIF tasks in practical applications.

## Method Summary
The CrossFuse framework addresses OOD generalization in IVIF through a three-pronged approach: external data augmentation via Top-k Selective Channel Alignment to align auxiliary data distribution with the target, internal data augmentation using weak-aggressive augmentation with self-supervised learning, and a frequency-aware fusion network architecture. The external augmentation crops images into patches, selects Top-K patches with similar intensity distributions between datasets, and applies RGB-wise gamma transformations to align them. The internal augmentation generates weak (random crop) and aggressive (Gaussian blur) views of fused outputs and minimizes their difference through a self-supervised loss. The fusion network uses a hybrid CNN-Transformer architecture with separate long-scale (Transformer) and short-scale (CNN) feature extractors for global and local features respectively.

## Key Results
- Achieves state-of-the-art performance on multiple IVIF datasets with top rankings in MI, SCD, VIF, Qabf, and SSIM metrics
- Demonstrates superior robustness to out-of-distribution data compared to baseline methods
- Ablation studies show significant contributions from both external Top-k alignment and internal self-supervised learning components
- Cross-dataset testing validates enhanced generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1: External Data Augmentation via Top-k Selective Channel Alignment
- **Claim:** Mitigates distribution shifts between auxiliary and target datasets by aligning RGB-wise intensity distributions through selective gamma transformations
- **Mechanism:** Crops images into patches, calculates average pixel intensity per channel for target and auxiliary patches, selects Top-K auxiliary patches most similar to target, applies gamma correction to align distributions
- **Core assumption:** Pixel intensity distribution is a sufficient proxy for broader feature distribution shifts
- **Evidence anchors:** Abstract mentions Top-k Selective Vision Alignment; Section III-C.1 details the algorithm with equations and visualizations in Fig. 3 & 4
- **Break condition:** Fails if distribution shift is not captured by mean intensity or requires complex spatially-varying transformations

### Mechanism 2: Internal Self-Supervised Augmentation
- **Claim:** Forces model to learn robust feature representations through weak-aggressive augmentation invariance
- **Mechanism:** Generates weak (random crop) and aggressive (Gaussian blur) views of fused outputs, minimizes their difference through self-supervised loss with decaying weight
- **Core assumption:** Fusion features should be invariant to weak and aggressive perturbations, encoding fundamental structural content
- **Evidence anchors:** Abstract states internal augmentation with self-supervised learning; Section III-C.2 describes process and loss function; Table VI ablation shows performance drop when removed
- **Break condition:** Fails if aggressive augmentation destroys critical information or learned features don't transfer to fusion objective

### Mechanism 3: Frequency-Aware Network Architecture
- **Claim:** Facilitates extraction and integration of complementary multi-scale features through hybrid CNN-Transformer design
- **Mechanism:** Uses Transformer blocks for long-scale (global/low-frequency) features and CNN layers for short-scale (local/high-frequency) features, then fuses them
- **Core assumption:** Infrared provides global thermal context (low-frequency) while visible provides fine texture (high-frequency)
- **Evidence anchors:** Abstract introduces frequency-aware fusion network; Section III-D describes multi-scale extraction-fusion process; Table VI ablation shows performance degradation when replacing either component
- **Break condition:** Underperforms when visible contributes critical global context or infrared provides crucial fine details

## Foundational Learning

- **Concept:** Out-of-Distribution (OOD) Generalization
  - **Why needed here:** Core problem being addressed - test data comes from different statistical distribution than training data
  - **Quick check question:** If a model is trained on daytime driving scenes, will it work on night scenes, and why or why not according to this paper?

- **Concept:** Data Augmentation for Domain Alignment
  - **Why needed here:** Proposed solution for OOD - transforming external dataset to match target distribution
  - **Quick check question:** How does Top-k Selective Channel Alignment reduce the distribution gap between M3FD and RoadScene datasets?

- **Concept:** Self-Supervised Contrastive Learning
  - **Why needed here:** Core of internal augmentation mechanism - training by contrasting different views without explicit labels
  - **Quick check question:** What two augmented views are contrasted by the model's internal self-supervised learning loss, and what is the goal?

## Architecture Onboarding

- **Component map:**
  1. External Augmentation Pipeline: Aligns auxiliary data to target using Top-k selective gamma correction
  2. Fusion Network Backbone: Hybrid CNN-Transformer architecture with shallow feature extractor, long-scale (Transformer) and short-scale (CNN) extractors, fusion blocks, and reconstruction blocks
  3. Internal Augmentation & Loss Module: Self-supervised branch applying weak (crop) and aggressive (blur) augmentations to fused output

- **Critical path:**
  1. Data Preparation: Apply Top-k channel alignment to M3FD dataset using RoadScene statistics
  2. Training Stage I: Train feature extractors and reconstruction blocks using reconstruction loss on aligned data
  3. Training Stage II: Introduce fusion blocks and self-supervised loss; total loss combines multiple terms with decaying SSL weight
  4. Inference: Feed IR and visible images through network to get fused output

- **Design tradeoffs:**
  - Performance vs. Simplicity: Multi-view augmentation adds complexity; lower inference efficiency than simpler models due to Transformers
  - Augmentation Strength: Specific choice of weak (crop) and aggressive (blur) augmentations
  - Generalization vs. ID Performance: Optimized for OOD scenarios with primary gains in cross-dataset tests

- **Failure signatures:**
  - Overly blurry fused images or lack of fine detail indicates incorrect balance between frequency paths or too strong aggressive augmentation
  - Failure on new datasets suggests Top-k alignment on single auxiliary dataset is insufficient
  - Artifacts or unnatural color shifts indicate issues with channel alignment or loss function weights

- **First 3 experiments:**
  1. Validate OOD Problem: Train baseline on RoadScene, test on MSRS/TNO; compare with proposed model in same cross-dataset setting
  2. Ablation on Augmentation: Train without external Top-k alignment and without internal self-supervised loss; compare on TNO using Table VI metrics
  3. Visualize Feature Separation: Extract and visualize feature maps from long-scale and short-scale extractors to confirm distinct global vs. local representations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the CrossFuse framework be adapted to maintain performance when facing extreme distribution shifts where the disparity between auxiliary and target datasets is conspicuous?
- **Basis in paper:** Authors explicitly state in Limitations that performance might deteriorate with extreme distribution shifts
- **Why unresolved:** Current Top-k alignment assumes some overlap between domains that may not exist in extreme OOD scenarios
- **What evidence would resolve it:** Experiments on datasets with significantly larger domain gaps or theoretical analysis proving convergence under wider distribution divergences

### Open Question 2
- **Question:** Can architectural complexity be reduced to improve inference efficiency without compromising multi-scale feature extraction?
- **Basis in paper:** Efficiency Results note lower running efficiency compared to Densefuse and TarDAL due to Transformer structure
- **Why unresolved:** Reliance on Restormer and Transformer blocks is computationally expensive; optimal speed-accuracy trade-off not established
- **What evidence would resolve it:** Derived lightweight model achieving comparable MI and SCD scores with significantly reduced FLOPs and inference time

### Open Question 3
- **Question:** Can multi-view data augmentation strategy be generalized to improve performance in high-level vision tasks like object detection or semantic segmentation?
- **Basis in paper:** Conclusion states core tenets hold promise for benefiting tasks such as object detection and semantic segmentation
- **Why unresolved:** Improved fusion quality doesn't always linearly correlate with improved performance in downstream semantic tasks; alignment strategy may not suit semantic feature spaces
- **What evidence would resolve it:** Experiments demonstrating improved mAP or mIoU on downstream tasks when using images fused via this cross-domain alignment strategy

## Limitations
- Exact value of K in Top-k Selective Channel Alignment algorithm is not provided
- Specific architecture details for Restormer and Transformer blocks (depth, channel dimensions) are omitted
- Batch size used during training is not stated
- Approach relies heavily on pixel intensity distribution as proxy for broader environmental shifts
- Model shows reduced inference efficiency compared to simpler methods due to Transformer components

## Confidence
- **External Data Augmentation Mechanism:** Medium confidence - shows theoretical soundness and visual alignment, but relies on simplifying assumption with weak corpus support
- **Internal Self-Supervised Augmentation:** Medium confidence - novel strategy with positive ablation results, but choice of blur may destroy task-critical information and lacks direct corpus support
- **Frequency-Aware Network Architecture:** High confidence - aligns with current architectural trends, ablation studies support frequency-based separation, consistent performance gains across metrics

## Next Checks
1. **Validate Top-k Selection Sensitivity:** Systematically vary K (e.g., K=25, 50, 100, 200) in channel alignment algorithm and measure impact on cross-dataset test performance
2. **Test Alternative Aggressive Augmentations:** Replace Gaussian blur with other aggressive augmentations (high JPEG compression, strong noise, cutout) and compare performance
3. **Analyze Frequency Path Contributions:** Conduct quantitative ablation study zeroing out either low-frequency or high-frequency path outputs to measure resulting fusion quality