---
ver: rpa2
title: On Joint Regularization and Calibration in Deep Ensembles
arxiv_id: '2511.04160'
source_url: https://arxiv.org/abs/2511.04160
tags:
- ensemble
- validation
- joint
- performance
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the \"ensemble optimality gap\"\u2014\
  the difference between tuning models individually versus tuning the full ensemble\
  \ jointly\u2014in deep learning. The authors systematically explore this gap across\
  \ three key areas: weight decay tuning, temperature scaling for calibration, and\
  \ early stopping."
---

# On Joint Regularization and Calibration in Deep Ensembles

## Quick Facts
- arXiv ID: 2511.04160
- Source URL: https://arxiv.org/abs/2511.04160
- Reference count: 40
- Primary result: Joint optimization (early stopping, temperature scaling) can yield modest but consistent improvements over individual tuning for deep ensembles.

## Executive Summary
This paper investigates the "ensemble optimality gap"—the performance difference between tuning ensemble models individually versus jointly—across three key areas: weight decay tuning, temperature scaling for calibration, and early stopping. The authors propose an overlapping holdout validation strategy that balances data efficiency with the ability to jointly evaluate ensemble performance. They find that while individual tuning provides a strong baseline, jointly optimizing for ensemble performance can yield consistent improvements, particularly for early stopping and temperature scaling. The study highlights the practical benefits of ensemble-aware validation and provides concrete guidance for practitioners aiming to improve deep ensemble reliability and performance.

## Method Summary
The authors systematically compare individual versus joint optimization strategies for deep ensembles across four datasets (CIFAR-10, NCI1, Covertype, AG News) and four architectures (WRN, GCN, MLP, BiLSTM). They explore three key aspects: weight decay tuning via grid search, temperature scaling for calibration via L-BFGS optimization, and early stopping with patience-based criteria. The novel overlapping holdout validation strategy allocates the validation set into M portions, each shared by M-1 models, enabling pairwise joint evaluation while minimizing data loss. Experiments use ensemble sizes of M=4 or M=8 depending on architecture, with batch size 128 and various learning rates and training durations specific to each model type.

## Key Results
- Individual temperature scaling can degrade ensemble calibration, while joint temperature scaling improves it.
- Joint early stopping consistently reduces classification error and improves calibration across all datasets and architectures tested.
- The overlapping holdout strategy offers a practical compromise in data-scarce settings, with 1-5% validation data showing optimal performance.
- For BatchEnsemble with Gaussian initialization, individual and joint optimization yield similar results due to data leakage issues.

## Why This Works (Mechanism)
The ensemble optimality gap exists because individual model optimization focuses on maximizing each model's performance independently, while joint optimization considers the collective behavior of the ensemble. This is particularly important for calibration, where temperature scaling must account for how models interact when their outputs are combined. Early stopping benefits from joint optimization because it can account for the ensemble's collective generalization behavior rather than individual model trajectories. The overlapping holdout strategy enables efficient joint evaluation by allowing each model to share validation data with every other model, approximating full ensemble evaluation without requiring separate validation sets for each possible combination.

## Foundational Learning

**Deep ensembles**: Multiple independently trained models combined to improve predictive performance and uncertainty estimation. Needed to understand the baseline approach being compared against joint optimization strategies. Quick check: Can you explain how ensemble predictions are typically combined (e.g., averaging)?

**Temperature scaling**: A post-hoc calibration method that adjusts model outputs by a learned parameter to improve reliability of predicted probabilities. Needed to understand how calibration interacts with ensemble behavior. Quick check: Do you know the difference between applying temperature scaling to individual models versus the entire ensemble?

**ECE (Expected Calibration Error)**: A metric measuring the discrepancy between predicted probabilities and actual accuracy. Needed to evaluate calibration quality in the experiments. Quick check: Can you explain why well-calibrated models are important for reliable uncertainty estimates?

## Architecture Onboarding

**Component map**: Data preprocessing -> Model training (individual or joint) -> Validation (shared/disjoint/overlapping holdout) -> Ensemble evaluation (NLL, ECE, error) -> Hyperparameter optimization (weight decay sweep, temperature scaling, early stopping)

**Critical path**: The key experimental flow is: train individual models with various hyperparameters → evaluate on validation sets using different holdout strategies → select optimal hyperparameters → evaluate final performance on test set. The overlapping holdout validation is the novel critical component.

**Design tradeoffs**: Shared holdout maximizes training data but prevents joint evaluation; disjoint holdout enables joint evaluation but reduces training data; overlapping holdout balances both but adds complexity. Individual optimization is simpler but may miss ensemble-level improvements; joint optimization is more computationally expensive but can capture ensemble dynamics.

**Failure signatures**: Individual temperature scaling degrading ensemble ECE; large validation percentages (>10%) reducing training data quality; BatchEnsemble with Gaussian initialization showing data leakage (validation/test metrics diverging).

**First experiments**:
1. Reproduce weight decay sweep (Fig 1) using shared holdout: train models at log-spaced WD values, evaluate individual vs ensemble NLL.
2. Implement overlapping holdout validation with M=4 portions for CIFAR-10; verify partition construction and validation stability.
3. Test BatchEnsemble with random sign initialization on CIFAR-10; confirm individual vs joint optimization differences disappear.

## Open Questions the Paper Calls Out

**Open Question 1**: Does the ensemble optimality gap persist or change in magnitude when applied to very large-scale models and datasets? The authors note they "did not investigate our hypotheses on very large-scale models and datasets," suggesting dynamics might differ due to computational costs.

**Open Question 2**: Can Gaussian initialization yield sufficient diversity for BatchEnsemble in very large models, despite failing in smaller-scale experiments? The authors note that while Gaussian initialization failed in their experiments, its viability "remains an open question" given conflicting reports in large-scale literature.

**Open Question 3**: Do the benefits of joint optimization transfer to implicit ensembles like MC-dropout or Bayesian Neural Networks (BNNs)? The conclusion states that "The implications of our findings for other types of ensembles (e.g., BNNs, MC-dropout) also warrant investigation."

**Open Question 4**: Does implementing higher-order overlapping holdout strategies (e.g., triplet-wise overlaps) provide significant gains over the proposed pairwise strategy? The authors suggest future work should "investigate variations of the overlapping holdout strategy—such as structures enabling higher-order joint evaluation."

## Limitations
- Empirical scope limited to four datasets and specific architectures (WRN, GCN, MLP, BiLSTM).
- Overlapping holdout strategy's optimal parameters (number of portions M, partition construction) are not fully specified.
- Results may not generalize to all ensemble methods or learning problems.
- Lack of theoretical analysis for why joint calibration/tuning works better in some cases.

## Confidence
- **High confidence**: Claims about ensemble optimality gap being non-zero, especially for temperature scaling and early stopping.
- **Medium confidence**: Claims about practical benefits of overlapping holdout as data-efficient compromise.
- **Low confidence**: Generalizability of findings to all ensemble methods and all problem domains.

## Next Checks
1. Reproduce weight decay sweep and early stopping comparison (Figs 1, 3-4) using official code on CIFAR-10/WRN-16-4; verify individual temperature scaling degrades ensemble ECE and joint early stopping improves both accuracy and calibration.
2. Implement overlapping holdout validation with M=4 portions for CIFAR-10; check if validation NLL curves are stable across different partition constructions.
3. Test BatchEnsemble on CIFAR-10 using random sign initialization; confirm individual vs joint calibration/tuning differences disappear as predicted.