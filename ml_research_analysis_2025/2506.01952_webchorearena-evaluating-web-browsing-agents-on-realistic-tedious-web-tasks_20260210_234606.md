---
ver: rpa2
title: 'WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks'
arxiv_id: '2506.01952'
source_url: https://arxiv.org/abs/2506.01952
tags:
- tasks
- agent
- webarena
- webchorearena
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces WebChoreArena, a benchmark designed to evaluate
  web browsing agents on complex, tedious tasks beyond general web browsing. It extends
  the WebArena benchmark with 532 human-curated tasks across four simulated websites,
  focusing on three key challenges: Massive Memory (retrieving large amounts of information),
  Calculation (mathematical reasoning), and Long-Term Memory (across multiple webpages).'
---

# WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks

## Quick Facts
- arXiv ID: 2506.01952
- Source URL: https://arxiv.org/abs/2506.01952
- Reference count: 39
- State-of-the-art LLMs show significant performance drops on WebChoreArena compared to WebArena

## Executive Summary
WebChoreArena is a benchmark designed to evaluate web browsing agents on complex, tedious tasks that go beyond general web browsing. It extends the WebArena benchmark with 532 human-curated tasks across four simulated websites, focusing on three key challenges: Massive Memory (retrieving large amounts of information), Calculation (mathematical reasoning), and Long-Term Memory (across multiple webpages). The benchmark is fully reproducible and enables direct comparison with WebArena.

Experiments with state-of-the-art LLMs (GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Pro) and agents (AgentOccam, BrowserGym) show that even the latest models struggle significantly on WebChoreArena. GPT-4o achieves only 6.8% accuracy compared to 42.8% on WebArena. Gemini 2.5 Pro, the strongest model tested, reaches 44.9% on WebChoreArena versus 59.2% on WebArena, indicating substantial room for improvement. The benchmark effectively differentiates model performance and highlights the need for more advanced capabilities in web agents.

## Method Summary
WebChoreArena extends the WebArena benchmark by creating a comprehensive evaluation framework for web browsing agents. The benchmark includes 532 tasks distributed across four simulated websites: travel agency, task management, shopping, and flight booking. Each task is designed to be tedious and complex, requiring agents to handle massive amounts of information, perform calculations, and maintain long-term memory across multiple webpages. The tasks are human-curated and tested to ensure reproducibility. The benchmark maintains compatibility with WebArena, allowing direct comparison between the two evaluation frameworks.

## Key Results
- GPT-4o achieves 6.8% accuracy on WebChoreArena compared to 42.8% on WebArena
- Gemini 2.5 Pro reaches 44.9% accuracy on WebChoreArena versus 59.2% on WebArena
- The benchmark effectively differentiates between model capabilities, with even the strongest models struggling on complex tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic task design that mirrors actual tedious web interactions. By focusing on Massive Memory, Calculation, and Long-Term Memory challenges, WebChoreArena creates scenarios where agents must process extensive information, perform multi-step reasoning, and maintain context across multiple webpages. This design exposes the limitations of current models in handling real-world web browsing complexities that go beyond simple information retrieval or single-page interactions.

## Foundational Learning
The benchmark implicitly requires agents to develop foundational capabilities in information synthesis, mathematical reasoning, and persistent context management. These skills are essential for navigating real-world web tasks but are often overlooked in simpler benchmarks. The tedious nature of the tasks forces agents to demonstrate sustained attention and systematic problem-solving approaches rather than relying on quick pattern matching or surface-level understanding.

## Architecture Onboarding
The benchmark's architecture is designed for seamless integration with existing web browsing agent frameworks. By building upon the WebArena foundation, WebChoreArena provides a familiar evaluation environment while introducing more challenging tasks. The four-domain structure (travel, task management, shopping, flights) allows researchers to test agents across diverse web interaction scenarios while maintaining consistent evaluation metrics and task complexity levels.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the significant performance gap between WebArena and WebChoreArena suggests several important research directions. These include investigating how to improve long-term memory mechanisms in web agents, developing better calculation and reasoning capabilities, and understanding how to handle massive amounts of information efficiently during web interactions.

## Limitations
- Limited domain coverage with focus on four specific domains may not capture full diversity of real-world web interactions
- Accuracy as primary evaluation metric may oversimplify agent performance assessment
- Simulated environment may not fully represent complexity and unpredictability of actual open web
- The tedious nature of tasks may not reflect all types of real-world web interactions

## Confidence
**High Confidence Claims**:
- WebChoreArena effectively extends WebArena to evaluate more complex web tasks
- State-of-the-art models show significant performance drops on WebChoreArena compared to WebArena
- The benchmark successfully differentiates between model capabilities

**Medium Confidence Claims**:
- The identified challenges (Massive Memory, Calculation, Long-Term Memory) represent the key bottlenecks for web agents
- Performance on WebChoreArena indicates substantial room for improvement in web agents
- The benchmark provides meaningful insights into web agent capabilities

## Next Checks
1. Test selected agents from WebChoreArena on actual websites to assess how well simulated performance translates to real-world web interactions.

2. Expand the benchmark to include additional domains and task types, particularly those involving more dynamic content and unexpected scenarios.

3. Implement and compare alternative evaluation approaches beyond simple accuracy, such as partial credit scoring or task completion time analysis, to provide more nuanced performance assessment.