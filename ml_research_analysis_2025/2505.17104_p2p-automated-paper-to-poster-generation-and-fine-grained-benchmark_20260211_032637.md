---
ver: rpa2
title: 'P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark'
arxiv_id: '2505.17104'
source_url: https://arxiv.org/abs/2505.17104
tags:
- poster
- arxiv
- evaluation
- generation
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "P2P introduces the first multi-agent LLM framework for automated\
  \ academic poster generation from research papers, addressing semantic richness\
  \ and visual-textual integration challenges. The system employs three specialized\
  \ agents\u2014Figure Agent, Section Agent, and Orchestrate Agent\u2014each with\
  \ dedicated checker modules for iterative refinement, generating HTML-rendered posters."
---

# P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark

## Quick Facts
- arXiv ID: 2505.17104
- Source URL: https://arxiv.org/abs/2505.17104
- Reference count: 40
- P2P outperforms template-based approaches and achieves competitive results against original author-created posters, particularly when using reasoning-enhanced LLMs

## Executive Summary
P2P introduces the first multi-agent LLM framework for automated academic poster generation from research papers, addressing semantic richness and visual-textual integration challenges. The system employs three specialized agents—Figure Agent, Section Agent, and Orchestrate Agent—each with dedicated checker modules for iterative refinement, generating HTML-rendered posters. A large-scale instruction dataset (30K+ examples) and comprehensive benchmark with dual evaluation (Universal and Fine-Grained) were established. Experiments show HTML as optimal output format, with P2P outperforming template-based approaches and achieving competitive results against original author-created posters, particularly when using reasoning-enhanced LLMs.

## Method Summary
P2P uses a multi-agent LLM framework where three specialized agents process distinct subtasks: Figure Agent extracts and describes visual elements using DocLayout-YOLO and MLLMs; Section Agent generates structured content based on inferred poster schema; Orchestrate Agent assembles HTML layouts with CSS. Each agent includes checker modules that trigger reflection loops for iterative refinement. The system was fine-tuned on P2PInstruct (30K examples) and evaluated using P2PEval benchmark (121 paper-poster pairs) with both Universal (XGBoost-calibrated LLM scoring) and Fine-Grained checklist-based metrics.

## Key Results
- HTML output format achieves 65.40 FineGrain score vs. 52.74 (SVG) and 56.88 (LaTeX)
- Full multi-agent system with checkers scores 65.40 FineGrain vs. 60.72 without specialized components (7.7% improvement)
- P2P outperforms template-based approaches and achieves competitive results against original author-created posters
- HTML enables flexible layouts through CSS content-structure decoupling, reducing rendering errors compared to vector formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent decomposition with specialized checkers improves poster quality over single-pass generation
- Mechanism: Three agents (Figure, Section, Orchestrate) process distinct subtasks—visual extraction, content synthesis, and layout assembly—each with a checker module that triggers reflection loops when validation criteria fail. This decomposition separates concerns that otherwise compete in a single model's attention
- Core assumption: Poster generation requires cognitively distinct operations (visual parsing, summarization, layout) that benefit from modular specialization
- Evidence anchors: Ablation study (Table 4): full system scores 65.40 FineGrain vs. 60.72 without specialized components—a 7.7% improvement

### Mechanism 2
- Claim: Converting figures to textual descriptions before integration reduces MLLM interpretive burden
- Mechanism: The Figure Describer generates semantic summaries (avg. 192 tokens) of visual elements. These descriptions guide content placement, rather than forcing MLLMs to reason directly from raw images during text generation
- Core assumption: Text-based reasoning about visual elements is more reliable than direct image-to-text integration for layout decisions
- Evidence anchors: Ablation: removing Figure Describer drops FineGrain score from 65.40 to 63.74

### Mechanism 3
- Claim: HTML output format outperforms SVG and LaTeX for LLM-generated posters
- Mechanism: LLMs demonstrate greater proficiency in HTML/CSS code generation, producing fewer rendering errors. HTML's content-structure decoupling via CSS enables adaptive layouts without hardcoding dimensions
- Core assumption: Current LLMs have stronger prior training on HTML/CSS than SVG or LaTeX poster templating
- Evidence anchors: Table 3: HTML achieves 65.40 FineGrain vs. 52.74 (SVG) and 56.88 (LaTeX)

## Foundational Learning

- Concept: Document Layout Detection (DocLayout-YOLO)
  - Why needed here: The Figure Agent must accurately locate figures, tables, and captions in PDFs before extraction. Misalignment propagates errors through entire pipeline
  - Quick check question: Can you explain how bounding-box confidence thresholds affect figure-caption pairing accuracy?

- Concept: LLM-as-a-Judge Evaluation
  - Why needed here: P2PEval uses GPT-4o to score posters against human-annotated checklists. Understanding calibration between LLM and human judgments is critical for interpreting benchmark results
  - Quick check question: Why does XGBoost regression achieve R²=0.92 when mapping LLM scores to human ratings?

- Concept: Reflection/Refinement Loops in Multi-Agent Systems
  - Why needed here: Each checker module can reject outputs and trigger regeneration. Understanding when to iterate vs. terminate prevents infinite loops
  - Quick check question: What criteria should the Section Checker use to decide if content is "good enough" versus requiring revision?

## Architecture Onboarding

- Component map:
  - Figure Agent: DocLayout-YOLO (extraction) → Figure Describer (MLLM captioning) → Figure Checker (validation)
  - Section Agent: Section Generator (schema inference) → Content Generator (LLM text) → Section Checker (coherence/fidelity)
  - Orchestrate Agent: HTML Generator (Markdown→HTML) → Poster Checker (layout validation)
  - Shared resources: P2PInstruct dataset (30K examples), P2PEval benchmark (121 paper-poster pairs)

- Critical path: Figure extraction → Description synthesis → Content generation → HTML rendering. The Figure Describer output is a hard dependency for Section Agent; both feed Orchestrate Agent

- Design tradeoffs:
  - Speed vs. quality: Each reflection iteration improves output but increases latency (3-agent system with checkers vs. single-pass)
  - Generalization vs. domain specificity: P2PInstruct fine-tuning (Qwen3-P2P-8B) achieves best ROUGE but may overfit to conference poster styles

- Failure signatures:
  - Figure-caption mismatch: Detected elements exceed captions → confidence threshold lowered iteratively
  - Layout imbalance: Blank space >19% or column height ratio >1.7 (see Table 5 Claude vs. GPT comparison)
  - Content hallucination: Section Checker flags claims unsupported by source paper

- First 3 experiments:
  1. Run P2P on a paper outside the training distribution (e.g., mathematics or theoretical physics) to assess generalization boundaries
  2. Ablate one checker module at a time to quantify each agent's contribution to final quality
  3. Compare HTML output rendered in different browsers to verify CSS cross-compatibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated systems overcome the "strict preference" gap to consistently outperform human authors in poster design?
- Basis in paper: Table 2 shows that while P2P is competitive, human evaluators strictly prefer the original author-created poster over the P2P version in the majority (64.41%) of cases
- Why unresolved: The paper demonstrates feasibility and competitiveness but does not analyze the specific semantic or aesthetic factors that cause humans to reject generated posters in favor of originals
- What evidence would resolve it: A targeted ablation study identifying whether the preference gap stems from layout rigidity, figure resolution, or nuance loss in text summarization

### Open Question 2
- Question: How can the framework be extended to synthesize novel visual summaries rather than solely extracting existing figures?
- Basis in paper: Section 2.1 describes the Figure Agent as strictly "extracting" and "describing" existing visual elements rather than generating new diagrams or charts
- Why unresolved: Effective academic posters often require simplified schematics or composite figures that do not exist in the source paper, a capability the current architecture lacks
- What evidence would resolve it: Experiments integrating a visual synthesis module to create summary diagrams, comparing user comprehension rates against extraction-only posters

### Open Question 3
- Question: Does the reliance on HTML as an intermediate representation limit the structural complexity of generated posters compared to vector graphics?
- Basis in paper: Section 4.3 concludes HTML is optimal, but attributes this to current LLMs' "greater proficiency" in HTML over LaTeX/SVG, implying the choice is model-driven rather than representationally superior
- Why unresolved: It is unclear if HTML provides sufficient typographic and layout precision for complex mathematical or multi-column scientific layouts as LLMs improve
- What evidence would resolve it: A follow-up evaluation using a model fine-tuned equally on SVG/LaTeX to isolate the format's structural potential from the model's coding bias

## Limitations
- Generalization boundaries remain unclear: P2P's performance on papers outside computer science or with unconventional figure types is untested
- Checker module thresholds are underspecified: The reflection termination criteria lack precise numerical bounds, potentially affecting reproducibility
- HTML-specific advantages may not transfer: CSS-based layout flexibility might not address domain-specific poster design constraints

## Confidence

- **High Confidence**: HTML outperforms SVG/LaTeX (empirical result from controlled comparison with 65.40 vs. 52.74/56.88 scores); Multi-agent architecture improves quality (ablation shows 7.7% gain)
- **Medium Confidence**: Figure description mechanism reduces MLLM burden (mechanism supported by score improvements but lacks independent validation); P2P surpasses template-based methods (limited baseline comparison)
- **Low Confidence**: Fine-tuning Qwen3-P2P-8B achieves best ROUGE (no ablation on fine-tuning vs. zero-shot performance); Poster quality approaches human authors (preference rates 54.5-67.8% may reflect benchmark bias)

## Next Checks

1. Test P2P on papers from non-ML domains (mathematics, humanities) to measure domain generalization
2. Implement controlled ablation of each checker module with quantitative quality impact analysis
3. Conduct user study comparing HTML outputs across browsers and PDF export scenarios to verify rendering consistency