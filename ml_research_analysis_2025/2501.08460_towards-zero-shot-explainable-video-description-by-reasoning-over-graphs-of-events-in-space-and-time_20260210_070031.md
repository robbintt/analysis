---
ver: rpa2
title: Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs
  of Events in Space and Time
arxiv_id: '2501.08460'
source_url: https://arxiv.org/abs/2501.08460
tags:
- video
- language
- vision
- videos
- gest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an explainable approach for video description
  by reasoning over Graphs of Events in Space and Time (GESTs). It combines state-of-the-art
  computer vision models for object detection, action recognition, and semantic segmentation
  with natural language processing techniques.
---

# Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time

## Quick Facts
- arXiv ID: 2501.08460
- Source URL: https://arxiv.org/abs/2501.08460
- Authors: Mihai Masala; Marius Leordeanu
- Reference count: 40
- Primary result: State-of-the-art zero-shot video description on Videos-to-Paragraphs dataset using GESTs

## Executive Summary
This paper introduces a novel explainable approach for zero-shot video description by reasoning over Graphs of Events in Space and Time (GESTs). The method combines specialized computer vision models for object detection, action recognition, and semantic segmentation with natural language processing techniques. A GEST is built from video frames and converted into a proto-language, which is then translated into a natural textual description by a large language model. The approach achieves state-of-the-art results across multiple datasets, particularly excelling on the Videos-to-Paragraphs dataset with complex multi-actor scenarios.

## Method Summary
The method processes video frames through specialized vision models (action detection, object detection/tracking, semantic segmentation, depth estimation) to construct a GEST. Events are nodes with attributes (actor, objects, timeframe, bounding box) connected via spatial and temporal edges. The graph is serialized into proto-language rather than being mapped directly to text. An LLM translates the proto-language into a natural, rich textual description. Person re-identification and action filtering reduce tracker noise, enabling coherent event aggregation over time. The LLM compensates for vision model uncertainty by selecting contextually appropriate objects from an over-complete candidate list.

## Key Results
- State-of-the-art performance on Videos-to-Paragraphs dataset, particularly in multi-actor scenarios
- Superior text similarity metrics (BLEU, ROUGE) compared to existing open models
- High qualitative rankings by LLM-based jury (Claude 3.5, GPT-4o, Qwen2-VL)
- Reduced hallucinations compared to end-to-end models like VidIL while maintaining rich descriptions

## Why This Works (Mechanism)

### Mechanism 1
An explicit spatio-temporal graph representation provides a grounded, inspectable intermediate between vision and language that end-to-end neural approaches lack. Raw video frames are processed by specialized vision models whose outputs are aggregated into events with attributes and connected via explicitly computed spatial and temporal edges. This graph is serialized into proto-language rather than being mapped directly to text. The quality of the final description depends primarily on the completeness and accuracy of detected events and relations.

### Mechanism 2
Multi-stage person re-identification and action filtering reduce tracker noise, enabling coherent event aggregation over time. Short-term ID inconsistencies are resolved by unifying person IDs within 10 frames with IoU > 0.4. Long-term re-identification uses HSV histogram features with cosine similarity. Actions are filtered by confidence (> 0.75), limited to top-2 per frame, and validated by an 11-frame voting window requiring ≥5 appearances. Consecutive actions are merged into events.

### Mechanism 3
Delegating ambiguous object selection and action refinement to an LLM compensates for vision model uncertainty while preserving explainability. Rather than hard-selecting the most probable object per event, the proto-language lists all candidate objects. The LLM is prompted to select contextually appropriate objects, modify action names, or delete inconsistent events. A small VLM provides scene context prepended to the proto-language.

## Foundational Learning

- Concept: **Graph-based spatio-temporal representation**
  - Why needed here: GEST is the central data structure; understanding nodes (events), edges (relations), and graph traversal is essential
  - Quick check question: Given two events with overlapping timeframes, how would you compute a spatial proximity edge?

- Concept: **Object tracking and re-identification**
  - Why needed here: The pipeline relies on consistent person IDs across frames; tracker drift directly degrades event coherence
  - Quick check question: What visual features could supplement HSV histograms for robust person re-ID?

- Concept: **Prompt engineering for structured-to-natural language conversion**
  - Why needed here: The LLM prompt controls object selection, action modification, and output style—prompt quality directly affects description quality
  - Quick check question: How would you modify the prompt to reduce hallucinations while preserving fluency?

## Architecture Onboarding

- Component map: VideoMAE (action) -> YOLO (objects/tracking) -> Mask2Former (segmentation) -> Marigold (depth) -> Person unification -> Action filtering/voting -> Event aggregation -> GEST builder -> Proto-language generator -> Scene classifier (VLM) -> LLM refinement

- Critical path: Action detection → Person tracking → Event aggregation → GEST construction → Proto-language → LLM. Errors propagate forward; missing detections cannot be recovered later.

- Design tradeoffs:
  - Fixed action vocabulary limits expressiveness but ensures grounded verbs vs. VidIL's rich but hallucinated descriptions
  - Over-complete object lists deferred to LLM vs. early hard selection—trades proto-language verbosity for contextual disambiguation
  - Rule-based thresholds (empirically set) vs. learned modules—simpler but may not generalize

- Failure signatures:
  - Fragmented events: Many short events instead of coherent ones (voting window too strict)
  - Missing actors: Person re-ID failure (check HSV similarity threshold)
  - Robotic text: LLM not receiving scene context or prompt insufficient

- First 3 experiments:
  1. **Ablate tracking unification**: Disable short-term and long-term person ID unification; measure event fragmentation and description coherence on Videos-to-Paragraphs
  2. **Vary action confidence threshold**: Test thresholds from 0.5 to 0.9; plot precision/recall of detected events vs. ground truth SVO annotations
  3. **Prompt sensitivity**: Replace the object selection prompt with a hard-selection baseline; compare BLEU/ROUGE and LLM-as-Jury rankings

## Open Questions the Paper Calls Out

### Open Question 1
Can a robust semantic-based solution for person re-identification be developed to handle long-term tracking inconsistencies without sacrificing the processing speed required for real-time application? The authors explicitly state the need for a "semantic-based solution that is powerful enough for person re-identification while being very fast," noting that their current HSV histogram approach is a substitute for this ideal solution.

### Open Question 2
How can the system overcome the limitation of fixed action vocabularies to prevent the description of complex activities as sequences of lower-level movements? The paper notes that descriptions "lack flexibility" and often decompose complex actions (e.g., describing "mopping" merely as "holding an object while walking") due to the constraints of the action recognizer's training data.

### Open Question 3
To what extent does delegating object selection to the LLM introduce hallucinations compared to developing a visual grounding mechanism within the graph construction phase? The method passes an "over-complete" set of possible objects to the LLM for selection, explicitly avoiding a hard decision in the visual pipeline.

## Limitations
- Threshold sensitivity: Empirically-set thresholds (10-frame window, 0.4 IoU, 0.75 confidence, 5/11 voting) were tuned on 20-25 examples, raising generalization concerns
- LLM hallucination risk: While reducing hallucinations compared to end-to-end models, the LLM may still introduce inaccuracies during object selection or action refinement
- No open-source implementation: Lack of released code or model weights makes independent verification difficult

## Confidence
- **High**: The core pipeline design (vision model extraction → GEST construction → proto-language → LLM refinement) is well-described and logically sound
- **Medium**: Reported improvements over baselines are significant, but absence of ablation studies on key components limits confidence in which elements drive the gains
- **Low**: Robustness to videos with rapid motion, extreme occlusion, or scenes with many similar objects is not evaluated, and HSV-based person re-identification may fail in these cases

## Next Checks
1. **Ablation of tracking unification**: Disable both short-term and long-term person ID unification and measure the impact on event coherence and description quality on the Videos-to-Paragraphs dataset
2. **Threshold sensitivity analysis**: Systematically vary the action confidence threshold (0.5 to 0.9) and the voting window parameters; plot precision/recall of detected events against ground truth SVO annotations
3. **Hallucination audit**: Manually inspect a sample of generated descriptions for factual consistency with the input video, focusing on whether the LLM introduces plausible but incorrect objects or actions during refinement