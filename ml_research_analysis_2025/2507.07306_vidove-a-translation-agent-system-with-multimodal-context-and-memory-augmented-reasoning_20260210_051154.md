---
ver: rpa2
title: 'ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented
  Reasoning'
arxiv_id: '2507.07306'
source_url: https://arxiv.org/abs/2507.07306
tags:
- translation
- agent
- memory
- video
- vidove
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ViDove is a multimodal translation agent system that integrates
  visual, auditory, and textual inputs to improve translation quality. It employs
  a multi-agent framework with specialized translators, proofreaders, and editors,
  supported by a long-short term memory system enriched with domain-specific knowledge.
---

# ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning

## Quick Facts
- arXiv ID: 2507.07306
- Source URL: https://arxiv.org/abs/2507.07306
- Reference count: 23
- ViDove achieves 28% improvement in BLEU and 15% improvement in SubER compared to state-of-the-art baselines on DoveBench

## Executive Summary
ViDove is a multimodal translation agent system designed to improve long-form video subtitling and translation quality by integrating visual, auditory, and textual inputs. The system employs a multi-agent framework with specialized translators, proofreaders, and editors, supported by a hierarchical memory system that maintains both short-term context and long-term domain knowledge. On a newly introduced 17-hour benchmark called DoveBench, ViDove demonstrates significant improvements over existing methods, particularly in handling domain-specific terminology and maintaining contextual consistency across extended video content.

## Method Summary
ViDove processes long-form video inputs through a multi-agent pipeline that extracts multimodal cues from audio and visual streams. The system uses an auditory agent for speech recognition, a vision agent for scene understanding, and a translation agent with multi-agent post-editing (translator, proofreader, editor) to produce context-aware translations. A hierarchical memory system (short-term memory M_s and long-term memory M_l) maintains context consistency and domain knowledge throughout the translation process. The framework is implemented using GPT-4o for translation tasks and Gemini-2.5-flash for perception tasks, with LlamaIndex managing the memory system.

## Key Results
- Achieves 28% improvement in BLEU score compared to Whisper + DelTA baseline on DoveBench
- Reduces SubER by 15% on DoveBench, demonstrating better subtitle timing and accuracy
- Maintains competitive performance on BigVideo benchmark, showing generalizability across different evaluation datasets
- Ablation studies show multi-agent post-editing provides the largest quality gains, with significant drops when agents are removed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multimodal grounding (visual/audio) reduces lexical ambiguity and improves entity resolution in translation.
- **Mechanism:** The Vision Agent ($L^*$) extracts scene descriptions and object keywords, storing them in short-term memory ($M_s$). The Translation Agent ($L$) retrieves these cues to constrain the translation space, distinguishing between homonyms (e.g., "fold" in cooking vs. origami) or recognizing specific game units (e.g., "Spire" → "飞龙塔").
- **Core assumption:** The visual encoder correctly identifies objects relevant to the spoken text, and the text encoder can effectively utilize these non-textual embeddings.
- **Evidence anchors:**
  - [Section 3.4]: "This visual cue allows the agent to resolve textually ambiguousness and accurately interpret domain-specific terminology."
  - [Section 5.4]: "While the visual module has limited impact on BLEU... it helps the editor correct entity-level terms."
  - [Corpus]: Neighbors like *WorldMM* and *UserCentrix* support the general efficacy of multimodal memory in agents, though they do not verify ViDove's specific translation gains.
- **Break condition:** Fails if visual cues are generic (e.g., "a person talking") or if the ASR transcription drift is too severe for the Vision Agent to align with the visual context.

### Mechanism 2
- **Claim:** Hierarchical memory (Short-term $M_s$ + Long-term $M_l$) enforces intra-video consistency and domain accuracy better than a fixed context window.
- **Mechanism:** $M_s$ maintains a sliding window of translation history to ensure pronouns and tone remain consistent across chunks ($C_i$). $M_l$ (implemented via LlamaIndex) retrieves domain-specific glossaries (e.g., StarCraft terminology) independent of the immediate video context, preventing the LLM from hallucinating generic translations for specialized terms.
- **Core assumption:** The retrieval mechanism successfully surfaces the *relevant* domain knowledge and *correct* historical segments without overwhelming the prompt.
- **Evidence anchors:**
  - [Section 3.5.2]: "Long-term memory... accumulates knowledge over time... [ensuring] accurate video translations for a diverse, multilingual audience."
  - [Table 2]: Removing domain memory reduces BLEU (15.84 → 14.86), demonstrating reliance on external knowledge.
- **Break condition:** Fails in low-resource domains where $M_l$ is sparse or unstructured, leading to retrieval of irrelevant "web knowledge" that confuses the translation agent.

### Mechanism 3
- **Claim:** Multi-agent post-editing (Proofreader → Editor) catches logical and stylistic errors that single-pass generation misses.
- **Mechanism:** The Proofreader ($L_{pr}$) acts as a critic, flagging terminology errors using domain memory ($M_l$). The Editor ($L_{ed}$) acts as the decision-maker, verifying these flags against multimodal short-term memory ($M_s$) to accept or reject changes.
- **Core assumption:** The LLMs assigned to Proofreader and Editor roles possess distinct capabilities or prompt adherence that allows one to identify errors the other missed.
- **Evidence anchors:**
  - [Table 2]: Ablation shows removing the Proofreader causes the "sharpest quality drop" (BLEU 15.84 → 13.56).
  - [Section A.1.1]: Logs show the Proofreader detecting "pilum" vs "pylon" confusion and correcting "Spire" translation.
- **Break condition:** If the Proofreader is overly pedantic or hallucinates constraints, the Editor may introduce "corrections" that degrade the original translation quality.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** ViDove relies on LlamaIndex to inject external domain glossaries ($M_l$) and historical context ($M_s$) into the prompt. Understanding vector search and chunk retrieval is required to debug why the system might retrieve irrelevant context.
  - **Quick check question:** How does the system ensure that a 20-minute-old segment in a video influences the translation of the current segment without re-processing the entire video history?

- **Concept: Multimodal Large Language Models (MLLMs)**
  - **Why needed here:** The system uses MLLMs (e.g., GPT-4o) not just for translation, but for "watching" video frames via the Vision Agent. You need to understand that these models map visual inputs into a semantic space shared with text.
  - **Quick check question:** In Figure 1, how do the "Visual Cues" from the Vision Agent get formatted for the text-based Translation Agent?

- **Concept: Agent Role-Play / Persona**
  - **Why needed here:** ViDove uses specific prompts (Appendix A.3) to force the same base LLM to act as a "Translator," "Proofreader," or "Editor." The system's success depends on these personas adhering to their specific constraints (e.g., Editor checks logic, Proofreader checks grammar).
  - **Quick check question:** If the Proofreader suggests a change that contradicts the visual evidence seen by the Editor, which agent wins?

## Architecture Onboarding

- **Component map:** Video ($V$) → Chunker (Pyannote) → Auditory Agent (ASR/Whisper) + Vision Agent (VLM/CLIP) → Memory (Ms, Ml) → Translation Agent ($L$) → Proofreader ($L_{pr}$) → Editor ($L_{ed}$) → Output SRT/Video

- **Critical path:** The **Translation Agent** is the bottleneck. It requires both the ASR transcript (Auditory) and the Visual Cues to be fully formed and stored in $M_s$ before execution. If the Vision Agent lags, the translation pipeline stalls.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The multi-agent loop (Translator → Proofreader → Editor) triples the inference time compared to a single-pass model like Gemini.
  - **Chunk Size:** Smaller chunks ($C_i$) reduce memory load but risk losing sentence context at boundaries; larger chunks improve context but increase hallucination risk in MLLMs.

- **Failure signatures:**
  - **High SubER (Timing errors):** Often caused by the ASR/Auditory Agent failing to align timestamps during the "Chunk Splitting" phase.
  - **Terminology Drift:** Suggests Long-term Memory ($M_l$) retrieval is failing or the Proofreader is not enforcing domain constraints.
  - **Hallucination in Silence:** The Vision Agent describes a scene, and the Translation Agent invents dialogue to match the visual description even if no speech is detected.

- **First 3 experiments:**
  1. **Ablation on Modality:** Run ViDove on DoveBench with the Vision Agent disabled to isolate the specific BLEU contribution of visual cues (expect a drop in entity accuracy).
  2. **Context Window Stress Test:** Feed a 60-minute video into the system to observe how the Short-term Memory ($M_s$) handles "forgetting" early context compared to the full-length baseline.
  3. **Agent Loop Limit:** Force the Editor to accept *all* Proofreader suggestions vs. *none* to determine if the Editor's "decision-making" capability is actually beneficial or if simple post-processing rules would suffice.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the utility of visual grounding be better quantified when standard metrics like BLEU and BLEURT fail to capture improvements in factual accuracy?
- **Basis in paper:** [explicit] Page 6 notes that while the visual module has "limited impact on BLEU or BLEURT," it helps correct entity-level terms and improves "factual accuracy... beyond what metrics capture."
- **Why unresolved:** There is a discrepancy between the system's qualitative improvements in entity handling and the lack of signal in standard automated metrics.
- **What evidence would resolve it:** Introduction of evaluation benchmarks specifically designed for visually ambiguous terminology or human evaluation studies focusing on entity accuracy.

### Open Question 2
- **Question:** What specific bottlenecks prevent current systems from achieving "fully satisfactory" results on long-form video subtitling despite multi-agent collaboration?
- **Basis in paper:** [explicit] Page 5 highlights that despite state-of-the-art results, "absolute scores... remain relatively modest," noting that "no existing system has achieved fully satisfactory results" due to the task's "intrinsic difficulty."
- **Why unresolved:** While ViDove improves over baselines, the authors acknowledge the performance ceiling remains low, leaving the core difficulty of the task unresolved.
- **What evidence would resolve it:** A detailed error analysis of ViDove's outputs on DoveBench identifying whether errors stem from context truncation, agent miscommunication, or ASR failures.

### Open Question 3
- **Question:** Is the multi-agent, multi-model architecture viable for real-time or cost-sensitive applications compared to cascaded baselines?
- **Basis in paper:** [inferred] The framework relies on a complex pipeline of distinct agents (Auditory, Vision, Translator, Proofreader, Editor) powered by large proprietary models (GPT-4o, Gemini), which typically incurs high latency and cost.
- **Why unresolved:** The paper evaluates translation quality but does not provide data on inference speed, computational cost, or scalability constraints.
- **What evidence would resolve it:** Reporting tokens-per-second processing rates and financial cost per minute of video compared to the Whisper + DelTA baseline.

## Limitations
- DoveBench dataset availability is unclear, preventing independent validation
- Domain knowledge base construction details are insufficient for adapting to new domains
- Memory retrieval effectiveness lacks ablation studies isolating M_s vs. M_l contributions

## Confidence
- **High confidence:** Multi-agent post-editing framework improves translation quality (supported by ablation studies)
- **Medium confidence:** Multimodal grounding reduces lexical ambiguity (qualitative evidence, but limited quantitative isolation)
- **Low confidence:** Memory system scalability for long videos and low-resource domains (untested scenarios)

## Next Checks
1. Confirm DoveBench dataset availability and obtain it to reproduce results
2. Run ViDove with Vision Agent disabled to quantify visual cues' specific contribution to BLEU and SubER
3. Evaluate system performance on 60-minute video to assess short-term memory effectiveness in maintaining extended context