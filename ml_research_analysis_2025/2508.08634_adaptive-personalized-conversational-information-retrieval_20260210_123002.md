---
ver: rpa2
title: Adaptive Personalized Conversational Information Retrieval
arxiv_id: '2508.08634'
source_url: https://arxiv.org/abs/2508.08634
tags:
- query
- personalization
- personalized
- retrieval
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses adaptive personalization in conversational
  information retrieval (CIR), where not all queries require personalization. The
  proposed framework, APCIR, explicitly identifies the required personalization level
  for each query turn and integrates personalized queries with other reformulations
  for ranking fusion using dynamic weights based on the identified level.
---

# Adaptive Personalized Conversational Information Retrieval

## Quick Facts
- arXiv ID: 2508.08634
- Source URL: https://arxiv.org/abs/2508.08634
- Reference count: 40
- One-line primary result: Achieves state-of-the-art performance on TREC iKAT datasets with NDCG@3 scores of 31.1% (iKAT-2023) and 50.8% (iKAT-2024) by adaptively fusing personalized and non-personalized query results.

## Executive Summary
This paper addresses the challenge of adaptive personalization in conversational information retrieval (CIR), where not all user queries require personalization. The proposed framework, APCIR, explicitly identifies the required personalization level for each query turn (non, partial, or full) and integrates personalized queries with non-personalized reformulations for ranking fusion using dynamic weights based on the identified level. Evaluated on TREC iKAT datasets, APCIR demonstrates state-of-the-art performance, significantly outperforming existing methods by avoiding over-personalization while still leveraging user context when beneficial.

## Method Summary
APCIR employs a three-stage approach: First, an LLM with Chain-of-Thought reasoning identifies the personalization level (Non, Partial, or Full) for each query using the user profile and conversation history. Second, the system generates multiple query variants including personalized and non-personalized reformulations, along with pseudo-responses for expansion. Third, retrieval results from these variants are fused using dynamic weights optimized via grid search on validation data. The framework uses SPLADE-v3 for retrieval and monoT5 for re-ranking, with fusion weights specifically tuned for each personalization level to optimize NDCG@3 performance.

## Key Results
- Achieves NDCG@3 scores of 31.1% on iKAT-2023 and 50.8% on iKAT-2024, outperforming existing methods
- Adaptive fusion approach significantly outperforms both static fusion and single-pass personalization strategies
- Demonstrates that explicitly identifying personalization needs prevents the performance degradation caused by over-personalization

## Why This Works (Mechanism)

### Mechanism 1: Three-Level Personalization Classification
The system uses an LLM with Chain-of-Thought reasoning to classify each query into non, partial, or full personalization levels. This explicit classification prevents the errors caused by forcing personalization on generic queries that don't require user context.

### Mechanism 2: Parallel Query Generation with Hedging
The framework generates multiple query variants (personalized and non-personalized) along with pseudo-responses, creating parallel retrieval streams that hedge against misclassification of user intent and improve coverage.

### Mechanism 3: Dynamic Fusion Weighting
Unlike static fusion approaches, APCIR applies pre-optimized fusion weights based on the predicted personalization level, allowing the system to adapt the influence of personalized results according to the query's specific needs.

## Foundational Learning

- **Concept: Query Drift / Over-personalization**
  - **Why needed here:** The core thesis is that adding user context when unnecessary hurts performance
  - **Quick check question:** If a user asks "Compare prices of X", does adding "I like blue" to the query help or hurt the ranking of price comparison pages?

- **Concept: Rank Fusion (Linear Combination)**
  - **Why needed here:** Final output blends multiple ranking lists from different retrievers
  - **Quick check question:** How do you handle score normalization when fusing results from a sparse retriever (BM25) and an LLM-generated query?

- **Concept: In-Context Learning (ICL) & Chain-of-Thought (CoT)**
  - **Why needed here:** The classifier is a generic LLM prompted with examples and reasoning steps
  - **Quick check question:** Why might an LLM fail to classify a query correctly without the CoT "reasoning" step preceding the final label?

## Architecture Onboarding

- **Component map:** User Profile + History + Current Query -> LLM Classifier -> Query Reformulator -> SPLADE-v3/BM25 Retriever -> Fusion Module -> Final Ranking
- **Critical path:** The transition from LLM Classifier output to Fusion Module is crucial; incorrect level identification leads to wrong weight application and retrieval failure
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Multiple LLM calls for identification, reformulation, and pseudo-response generation significantly increase inference time
  - **Data Dependency:** Requires validation set with relevance judgments for grid search optimization, not a zero-shot setup
- **Failure signatures:**
  - **Uniform Results:** Identical results for different users suggest Level Identifier stuck on "Non-Personalized" or fusion weight near zero
  - **Irrelevant Results:** Obscure user details in results indicate wrong "Personalized" assignment with too-high fusion weight
- **First 3 experiments:**
  1. **Ablation of Levels:** Force all queries to "Personalized" vs. "Non-Personalized" vs. "Adaptive" to validate adaptive fusion superiority
  2. **Weight Sensitivity:** Perturb fusion weights (Â±0.1) to test grid search stability and graceful degradation
  3. **Backbone Swap:** Replace GPT-4o with Llama-3-8B for classification to measure sensitivity to model size

## Open Questions the Paper Calls Out
- Can a mechanism be developed to determine optimal fusion weights dynamically for each specific retrieval instance rather than relying on pre-computed weights associated with discrete personalization levels?
- How can the latency of the APCIR framework be optimized to enable real-time conversational interactions while maintaining state-of-the-art retrieval accuracy?
- Can LLMs leverage reasoning to exploit implicit knowledge not explicitly present in the user profile or conversational context to further improve personalized query reformulation?

## Limitations
- Dependency on grid-search optimized fusion weights requires validation set with relevance judgments, limiting zero-shot applicability
- Effectiveness of LLM-based level identification without fine-tuning may not generalize across domains with different user profile formats
- Fusion weights optimized on iKAT datasets may not be robust to shifts in query distribution or user profile complexity

## Confidence
- **High Confidence:** Adaptive fusion based on predicted personalization levels is well-supported by experimental results
- **Medium Confidence:** LLM-based level identification reliability across diverse queries is plausible but not fully validated
- **Low Confidence:** Scalability of grid-search fusion weight optimization for larger parameter spaces or different datasets

## Next Checks
1. **Cross-Dataset Weight Transfer:** Apply iKAT-2023 optimized weights to OR-QuAC dataset without re-optimization to test robustness
2. **Level Identification Error Analysis:** Manually annotate 100 queries with true personalization levels and compare against LLM predictions to quantify misclassification rates
3. **Open-Source LLM Swap:** Replace GPT-4o with Llama-3-8B for level identification and measure NDCG@3 drop to assess black box dependency cost