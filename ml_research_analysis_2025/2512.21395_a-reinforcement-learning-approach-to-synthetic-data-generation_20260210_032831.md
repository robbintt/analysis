---
ver: rpa2
title: A Reinforcement Learning Approach to Synthetic Data Generation
arxiv_id: '2512.21395'
source_url: https://arxiv.org/abs/2512.21395
tags:
- data
- synthetic
- real
- rlsyn
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RLSyn, a reinforcement learning approach
  to synthetic data generation for biomedical datasets. Unlike existing methods like
  GANs and diffusion models, RLSyn models the generator as a stochastic policy and
  uses Proximal Policy Optimization (PPO) with discriminator-derived rewards, making
  it more stable and data-efficient.
---

# A Reinforcement Learning Approach to Synthetic Data Generation

## Quick Facts
- arXiv ID: 2512.21395
- Source URL: https://arxiv.org/abs/2512.21395
- Reference count: 40
- Primary result: RLSyn achieves comparable utility to diffusion models on MIMIC-IV and outperforms them on the smaller AI-READI dataset, with higher fidelity and lower privacy risks.

## Executive Summary
This paper introduces RLSyn, a reinforcement learning approach to synthetic data generation for biomedical tabular datasets. Unlike existing methods like GANs and diffusion models, RLSyn models the generator as a stochastic policy and uses Proximal Policy Optimization (PPO) with discriminator-derived rewards, making it more stable and data-efficient. Experiments on two biomedical datasets (AI-READI and MIMIC-IV) show that RLSyn performs comparably to state-of-the-art methods on larger datasets and outperforms them on smaller ones.

## Method Summary
RLSyn treats synthetic data generation as a reinforcement learning problem where the generator is a stochastic policy optimized via PPO. The generator outputs parameters for Gaussian and Bernoulli distributions, creating a stochastic sampling process. A discriminator provides scalar rewards indicating how "real" synthetic samples appear. The method uses PPO's clipped surrogate objective to stabilize training, entropy regularization to encourage diversity, and a mean-matching penalty for continuous features. The framework decouples generator and discriminator training, avoiding the minimax game of GANs.

## Key Results
- RLSyn achieves similar predictive utility (S2R AUC 0.902 vs 0.906) compared to diffusion models on MIMIC-IV
- Higher fidelity on MIMIC-IV (NMI 0.001 vs 0.003) and significantly better performance on smaller AI-READI dataset
- Lower privacy risks with membership inference AUC of 0.50 compared to diffusion models (0.601 on AI-READI)
- Superior performance on smaller datasets where GANs and diffusion models struggle with overfitting

## Why This Works (Mechanism)

### Mechanism 1: Clipped Surrogate Objective for Training Stability
RLSyn uses PPO with a clipped surrogate objective to constrain policy updates, preventing large destructive changes to the probability distribution. This bounded objective avoids the instability common in GAN training where the generator and discriminator compete in a non-stationary game. The stability is particularly valuable in high-dimensional tabular data generation where small perturbations can have large effects.

### Mechanism 2: Stochastic Policy for Distributional Coverage
The generator outputs distribution parameters (means and variances) rather than point estimates, creating a stochastic sampling process. Combined with entropy regularization, this encourages sample diversity and reduces overfitting to training records. This contrasts with deterministic methods that may memorize training data, especially in small-sample regimes.

### Mechanism 3: Discriminator-Derived Scalar Reward
The discriminator serves only as a reward estimator, providing scalar feedback about sample "realness" rather than direct gradient flow. This decouples the generator from the discriminator's gradients, avoiding the minimax game pathologies of GANs. The generator updates via policy gradient to maximize expected reward, while the discriminator is trained independently.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed: PPO is the core engine that constrains policy updates to a trust region, preventing catastrophic forgetting in the generator.
  - Quick check: If the probability ratio $\rho = \exp(\log p_{new} - \log p_{old})$ is 1.5 and the clip $\epsilon$ is 0.2, what value is used in the objective?

- **Concept: Policy Gradient Methods (Actor-Critic)**
  - Why needed: RLSyn learns via trial-and-error rather than supervised learning, requiring understanding of how advantage ($\hat{A} = r - V_{baseline}$) guides updates.
  - Quick check: Why is a value baseline (critic) subtracted from the reward when calculating the Advantage?

- **Concept: Membership Inference Attacks**
  - Why needed: This is the primary privacy metric; an AUC of 0.5 implies synthetic data reveals no traces of specific training records.
  - Quick check: If a generative model memorizes a training record and outputs a nearly identical copy, how would this affect the Membership Inference AUC (towards 0.5 or 1.0)?

## Architecture Onboarding

- **Component map:**
  Generator (Policy) -> Discriminator (Reward Function) -> PPO Optimizer (Generator) and Adam Optimizer (Discriminator)

- **Critical path:**
  1. Sample noise $z$
  2. Generator outputs distribution parameters; sample synthetic record $x$
  3. Discriminator evaluates $x$ -> produces reward $r$
  4. Calculate Advantage: $A = r - \text{ValueHead}(z)$
  5. Update Generator: Use PPO clipped objective based on Advantage + Mean-Matching Penalty + Entropy Bonus
  6. Update Discriminator: Standard BCE loss on Real/Synthetic batches

- **Design tradeoffs:**
  - PPO Clip $\epsilon$: Lower values (e.g., 0.1) ensure stability but might slow convergence; higher values allow faster learning but risk instability
  - Mean-Matching Penalty $\lambda_m$: Helps align continuous feature distributions but might restrict diversity
  - Data Regime: RLSyn favored for small-to-medium datasets; for massive datasets, PPO overhead vs. standard diffusion varies

- **Failure signatures:**
  - Reward Hacking: Generator produces limited diversity (low entropy). Fix: Increase entropy coefficient $\beta$
  - Discriminator Collapse: Discriminator loss drops to zero instantly; generator receives no gradient signal. Fix: Increase discriminator learning rate or reduce generator update frequency
  - Privacy Leakage: High membership inference AUC (>0.6). Fix: Check for near-duplicate synthetic/real pairs indicating overfitting

- **First 3 experiments:**
  1. Replicate AI-READI experiment to confirm RLSyn achieves lower Membership Inference AUC than Diffusion baseline (< 0.55)
  2. Remove PPO clipping mechanism and observe if training diverges compared to standard GAN baseline
  3. Sweep entropy coefficient ($\beta$) to visualize tradeoff between S2R AUC (utility) and Membership Inference AUC (privacy)

## Open Questions the Paper Calls Out

### Open Question 1
Can RLSyn effectively scale to non-tabular modalities such as longitudinal electronic health records, clinical narrative text, and medical imaging? The authors note their experiments focused on cross-sectional tabular data and explicitly state future work should explore efficacy for higher-dimensional generation problems.

### Open Question 2
Which specific architectural components (e.g., PPO clipping, stochastic policy sampling) are primarily responsible for RLSyn's stability and data efficiency? The authors acknowledge mechanisms are not yet clear and suggest future work should attribute which components drive gains.

### Open Question 3
Can RLSyn be optimized using alternative reward formulations that eliminate the need for a discriminator network? The authors highlight flexibility of the RL framework and suggest future work could explore alternative reward formulations that eliminate the need to train a discriminator altogether.

## Limitations
- Stability and convergence of PPO in tabular synthetic data generation under varying dataset sizes remains uncertain
- Optimal values for mean-matching penalty and entropy regularization coefficients are not established across biomedical domains
- Method's robustness to extreme class imbalance or high-cardinality categorical features remains untested

## Confidence
- **High Confidence**: RLSyn's comparable performance to diffusion models on MIMIC-IV and superior performance on AI-READI is well-supported by presented metrics
- **Medium Confidence**: Claimed mechanisms of PPO stability and anti-memorization are theoretically sound but lack direct ablation evidence
- **Low Confidence**: Assertion that RLSyn is more "data-efficient" is based on convergence speed without rigorous comparison of sample complexity or wall-clock time

## Next Checks
1. Remove the PPO clipping mechanism (set $\epsilon$ very high) and compare training stability and final performance against the standard GAN baseline on both datasets
2. Systematically vary the entropy coefficient ($\beta$) and plot the tradeoff curve between S2R AUC (utility) and Membership Inference AUC (privacy)
3. Train RLSyn with discriminators of varying depth and width to determine sensitivity of reward signal quality and final synthetic data quality to this critical component