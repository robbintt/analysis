---
ver: rpa2
title: Translation as a Scalable Proxy for Multilingual Evaluation
arxiv_id: '2601.11778'
source_url: https://arxiv.org/abs/2601.11778
tags:
- latn
- translation
- language
- multilingual
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical evaluation paradox in multilingual
  large language models (LLMs), where comprehensive non-machine-translated benchmarks
  exist for fewer than 30 languages, leaving over 98% of the world's 7,000 languages
  in an empirical void. The authors propose using translation quality as a scalable
  proxy for multilingual capabilities.
---

# Translation as a Scalable Proxy for Multilingual Evaluation

## Quick Facts
- arXiv ID: 2601.11778
- Source URL: https://arxiv.org/abs/2601.11778
- Reference count: 40
- Translation quality is a strong, inexpensive first-pass proxy for multilingual performance, enabling translation-first screening with targeted follow-up for specific tasks

## Executive Summary
This paper addresses the critical evaluation paradox in multilingual large language models (LLMs), where comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving over 98% of the world's 7,000 languages in an empirical void. The authors propose using translation quality as a scalable proxy for multilingual capabilities. Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, they find that translation performance is a strong indicator of downstream task success, with median Pearson correlations of 0.89 for METRICX, 0.91 for XCOMET, and 0.87 for SSA-COMET. These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding.

## Method Summary
The study evaluates 14 multilingual LLMs (1B-72B parameters) across three translation datasets (FLORES-200, WMT24++, NTREX) and nine downstream benchmarks (Global MMLU, BELEBELE, MLQA, MGSM, AfriMMLU, AfriXNLI, HELLASWAG, TruthfulQA, INCLUDE). Models perform zero-shot English-to-target translations using consistent prompts and temperature settings per model family. Translation outputs are scored using seven MT metrics (XCOMET, METRICX, BLEU, CHRF, METEOR, SSA-COMET, COMET). Correlations between translation metric scores and benchmark performance are computed per language using Pearson and Spearman correlations, then aggregated via Fisher's z-transformation. The primary finding is that neural MT metrics (XCOMET, METRICX, SSA-COMET) show strong correlations (median r > 0.85) with downstream benchmark performance across diverse tasks and languages.

## Key Results
- Translation quality strongly predicts downstream multilingual task performance with median Pearson correlations of 0.89 for METRICX, 0.91 for XCOMET, and 0.87 for SSA-COMET
- Neural MT metrics outperform traditional metrics, with XCOMET achieving median r = 0.91 and BLEU at median r = 0.79
- Translation-proxy correlation strength varies by task category: semantic tasks (BELEBELE, HELLASWAG) show strong alignment (median r > 0.85), while specialized tasks (MGSM, INCLUDE) show weaker correlations (median r = 0.62-0.72)

## Why This Works (Mechanism)

### Mechanism 1
Translation quality serves as a proxy for multilingual capability because it tests the linguistic interfaces that gate access to a model's shared computation space. Translation requires encoding an input language into a shared representation space, applying core reasoning within that space, and decoding into the target language. Models that translate well demonstrate reliable encoding/decoding pathways, which are prerequisites for most downstream multilingual tasks. Core assumption: multilingual models have functional separation between language-specific processing and language-agnostic computation. Break condition: if a downstream task requires competencies not mediated by linguistic encoding/decoding (e.g., specialized mathematical reasoning), translation quality will be an incomplete proxy.

### Mechanism 2
Neural MT metrics (XCOMET, METRICX, SSA-COMET) capture semantic equivalence better than lexical overlap metrics, making them stronger predictors of downstream performance. These metrics are trained on human judgments and use encoder representations to measure semantic similarity rather than surface n-gram matching. This aligns with how multilingual benchmarks evaluate meaning preservation. Core assumption: semantic similarity in translation correlates with semantic understanding in downstream tasks. Break condition: for low-resource languages underrepresented in neural metric training data, metric scores may reflect metric-specific bias rather than true model capability.

### Mechanism 3
Translation-proxy correlation strength varies by task category; tasks gated by broad semantic understanding show stronger alignment than those requiring specialized competencies. Benchmarks like reading comprehension (BELEBELE) and commonsense reasoning (HELLASWAG) depend primarily on faithful semantic mapping. Specialized tasks (MGSM for math, INCLUDE for regional knowledge) require additional competencies that translation does not exercise. Core assumption: translation primarily exercises cross-lingual semantic mapping, not domain-specific reasoning. Break condition: when evaluating specialized competencies, translation-first screening must be supplemented with task-specific benchmarks.

## Foundational Learning

- **Pearson vs. Spearman Correlation**
  - Why needed here: The paper uses both to establish linear (Pearson) and rank-based (Spearman) relationships between translation quality and benchmark performance
  - Quick check question: If translation scores are 0.4, 0.6, 0.8 and benchmark scores are 50, 70, 90, what would Pearson r approximately be? (Answer: ~1.0, strong positive linear relationship)

- **Neural MT Metrics (COMET family, METRICX)**
  - Why needed here: These metrics are the strongest predictors; understanding their design helps interpret proxy validity and limitations
  - Quick check question: Why would BLEU (n-gram overlap) underperform XCOMET (semantic embedding) for evaluating translation quality? (Answer: BLEU misses semantic equivalence when surface forms differ)

- **Translation Dataset Characteristics (FLORES-200, WMT24++, NTREX)**
  - Why needed here: The paper uses three datasets to ensure results aren't corpus-specific; each has different coverage and domain characteristics
  - Quick check question: If WMT24++ correlations are lower than FLORES-200 for some metrics, what might explain this? (Answer: Domain differences, smaller language overlap, or different text characteristics)

## Architecture Onboarding

- **Component map:**
  Translation Evaluation Pipeline: Input (English text from FLORES-200/WMT24++/NTREX) → LLM (zero-shot translation prompt) → Translation output → MT metrics (XCOMET, METRICX, BLEU, etc.) → Correlation analysis against benchmark scores → Proxy validity assessment per language/task

- **Critical path:**
  1. Select translation dataset and target languages
  2. Generate translations with consistent prompting (zero-shot, temperature per model family)
  3. Compute neural MT metrics (XCOMET recommended as default)
  4. Compare against benchmark scores using Pearson correlation with permutation tests (N=2000)
  5. If r > 0.75, translation is likely a reliable proxy; if r < 0.65, supplement with task-specific evaluation

- **Design tradeoffs:**
  - XCOMET vs. BLEU: XCOMET has higher median correlation (0.91 vs 0.79) but requires GPU inference; BLEU is CPU-fast but less sensitive
  - FLORES-200 vs. WMT24++: FLORES-200 has broader language coverage (200+ languages) but WMT24++ may better reflect news-domain performance
  - Per-language vs. aggregate analysis: Per-language analysis is statistically fragile due to sparse language overlap across benchmarks

- **Failure signatures:**
  - Negative correlation between translation and benchmark (e.g., Gemma3-27B on MGSM/SSA-COMET: r = -0.74): indicates model-scale instability or metric-dataset mismatch
  - Very high correlation on small n (e.g., n ≤ 3 on WMT24++ subsets): statistically unreliable, likely artifact
  - Large metric spread within a task (e.g., MLQA: METEOR r = 0.26 vs. METRICX r = 0.95): suggests proxy validity is metric-dependent for that task

- **First 3 experiments:**
  1. Baseline validation: Run zero-shot translation on FLORES-200 English-to-target for your model across 10 diverse languages; compute XCOMET scores; correlate with BELEBELE accuracy if available (expected r > 0.80)
  2. Metric ablation: Compare XCOMET vs. BLEU vs. METRICX correlations on GlobalMMLU across same languages to determine which metric is most predictive for your model family
  3. Task boundary test: Compute translation-benchmark correlation for both a semantic task (HELLASWAG) and a specialized task (MGSM or domain-specific benchmark) to identify where translation proxy degrades

## Open Questions the Paper Calls Out

- Does translation quality causally influence multilingual task performance, or is the observed correlation driven by shared training data, model representations, or other confounds? (Basis: "Correlational evidence does not establish causation... controlled causal studies would be needed to establish why translation quality influences downstream success.")

- Does the translation-benchmark correlation persist across non-standard text types such as dialectal variation, informal registers, code-switching, and domain-specific language? (Basis: "Translation datasets are curated... may under-represent dialectal variation, informal registers, code-switching, and domain-specific language.")

- How do correlations differ under alternative translation directions (e.g., target-to-English) or alternative prompting strategies? (Basis: "Translation is elicited via a single, direct, zero-shot prompt... correlations could differ under alternative prompting strategies, decoding settings, or translation directions.")

- Do translation-based proxy metrics remain reliable for low-resource languages where neural MT evaluators have limited training signal? (Basis: The paper notes that neural metrics "can be sensitive to the distribution and language coverage of their training data," yet low-resource languages are exactly where such coverage is sparsest.)

## Limitations
- Translation-proxy relationship appears weaker for specialized domains (INCLUDE, MGSM), suggesting the approach is insufficient for comprehensive multilingual evaluation across all task types
- The study's focus on English-centric translation (English-to-target) may not capture bidirectional translation dynamics that better reflect real-world multilingual use
- Strong correlations could partly reflect shared benchmark construction artifacts rather than pure model capability

## Confidence
- **High Confidence**: Neural MT metrics (XCOMET, METRICX, SSA-COMET) consistently show strong correlations (median r > 0.85) with downstream benchmark performance across diverse tasks
- **Medium Confidence**: Translation quality serves as a reliable first-pass proxy for multilingual capability, given that correlations drop for specialized tasks and the relationship may not hold for low-resource languages
- **Low Confidence**: Translation-proxy correlation strength is primarily determined by whether tasks are "gated by broad semantic understanding" versus "specialized competencies"

## Next Checks
1. **Low-resource language validation**: Test the translation-proxy correlation for languages with <100k Wikipedia articles to determine if neural MT metrics maintain their predictive power for truly low-resource scenarios
2. **Bidirectional translation analysis**: Evaluate whether English-to-target translation quality correlates equally well with target-to-English benchmark performance, as this better reflects real-world multilingual usage patterns
3. **Cross-dataset generalization test**: Apply the translation-proxy framework to a non-English-centric translation benchmark (e.g., Masakhane MT or Flores-101) to verify that correlations hold when English is not the source language