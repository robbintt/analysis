---
ver: rpa2
title: Visual-Guided Key-Token Regularization for Multimodal Large Language Model
  Unlearning
arxiv_id: '2601.22020'
source_url: https://arxiv.org/abs/2601.22020
tags:
- unlearning
- often
- answer
- tokens
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal large language model
  (MLLM) unlearning, which aims to prevent the model from revealing private information
  when queried about target images while maintaining coherence and retention of non-target
  knowledge. Existing methods treat all answer tokens uniformly and focus only on
  the language modality, ignoring visual cues for identifying key tokens.
---

# Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning

## Quick Facts
- **arXiv ID:** 2601.22020
- **Source URL:** https://arxiv.org/abs/2601.22020
- **Reference count:** 40
- **Primary result:** ViKeR achieves 32.0% unlearning accuracy on MLLMU 15% task with 52.7% ROUGE and 22.0% BLEU for retention, compared to 0.1% ROUGE and 0.0% BLEU for standard gradient ascent.

## Executive Summary
This paper addresses the problem of multimodal large language model (MLLM) unlearning, which aims to prevent the model from revealing private information when queried about target images while maintaining coherence and retention of non-target knowledge. Existing methods treat all answer tokens uniformly and focus only on the language modality, ignoring visual cues for identifying key tokens. To address this, the authors propose Visual-Guided Key-Token Regularization (ViKeR), which uses irrelevant visual inputs to estimate ideal post-unlearning token distributions and regularizes the unlearning process accordingly. Experiments on MLLMU and CLEAR benchmarks with LLaVA-7B demonstrate that ViKeR effectively performs unlearning while outperforming existing methods in retention and response coherence.

## Method Summary
ViKeR performs MLLM unlearning by leveraging irrelevant visual inputs to estimate ideal post-unlearning token-level distributions. For each forget set sample, k reference images are paired with the original question and passed through a frozen pre-unlearning model to compute averaged token distributions. These distributions serve as targets for regularization, where normal tokens (structurally necessary) retain sharp distributions while key tokens (identity-specific) lose identity-specific peaks. The method combines gradient ascent with KL divergence regularization, implicitly reweighting token-level gradients to amplify updates on key tokens while dampening updates on normal tokens. The approach is validated on LLaVA-7B using MLLMU and CLEAR benchmarks.

## Key Results
- ViKeR achieves 32.0% unlearning accuracy on MLLMU 15% task with 52.7% ROUGE and 22.0% BLEU for retention
- Outperforms standard gradient ascent which achieves 0.1% ROUGE and 0.0% BLEU for retention on same task
- Maintains response coherence with GIB scores comparable to or better than existing methods
- Shows consistent improvement across different forget-set sizes (10% and 15% tasks)

## Why This Works (Mechanism)

### Mechanism 1: Visual-Guided Distribution Estimation as Unlearning Target
Irrelevant visual inputs generate token distributions that approximate the ideal post-unlearning state, where normal tokens retain sharp distributions while key tokens lose identity-specific peaks. When k irrelevant images are paired with the original question and fed through the full MLLM, the averaged token distributions naturally suppress private information while preserving structural tokens, providing a gradient-free reference target for what "good unlearning" looks like.

### Mechanism 2: Gradient Reweighting via KL-Regularized Loss
The ViKeR loss implicitly reweights token-level gradients, amplifying updates on key tokens while dampening updates on normal tokens. The loss decomposes into token-level gradients where for normal tokens the reweighting factor approaches 1-λ (reducing unlearning pressure), while for key tokens the factor can exceed 1 for non-ground-truth tokens, actively pushing probability mass away from the ground truth.

### Mechanism 3: Entropy-Based Key Token Identification
Key tokens are formally defined via the entropy of their ideal post-unlearning distribution, distinguishing them from normal tokens where entropy approaches zero. The ideal distribution for normal tokens is near-deterministic (concentrated on the ground-truth token), yielding H(R_y^i) → 0, while key tokens have high entropy because the reference distribution spreads probability mass across many candidates when visual guidance is removed.

## Foundational Learning

- **Concept: Gradient Ascent for Unlearning**
  - Why needed here: GA is the baseline unlearning method that ViKeR modifies. Understanding that GA simply maximizes log-likelihood of the forget set (inverting standard training) explains why it over-forgets normal tokens.
  - Quick check question: Given a forget set sample (I, x, y), does GA increase or decrease p(y|I, x; θ)? (Answer: It increases it, which seems counterintuitive but "forgets" by pushing the model away from correct predictions through the inverted loss.)

- **Concept: KL Divergence as Distributional Regularization**
  - Why needed here: ViKeR uses KL(̂R||̂Q) to constrain the unlearning process. Understanding that KL measures how one distribution diverges from another (asymmetrically) clarifies why ̂R (the reference) appears first.
  - Quick check question: If ̂R_y^i is uniform over vocabulary and ̂Q_y^i concentrates 90% on one token, is the KL divergence high or low? (Answer: High—the model's confident prediction diverges from the reference's uncertainty.)

- **Concept: Information Entropy for Uncertainty Quantification**
  - Why needed here: The paper uses entropy to define key vs. normal tokens. Understanding that entropy measures "spread" of probability mass explains why high-entropy tokens are treated as key.
  - Quick check question: A token with R_y^i = [0.99, 0.01, ...] has what entropy property compared to R_y^i = [0.1, 0.1, ...]? (Answer: The first has near-zero entropy; the second has high entropy.)

## Architecture Onboarding

- **Component map:** Full MLLM (θ_full) -> Reference Image Set (I') -> Distribution Estimator -> Unlearning Model (θ) -> Loss Computer
- **Critical path:** Load θ_full and freeze it; for each (I, x, y) in forget set D_f: sample k reference images from I', compute ̂R_y^i for each token position via k forward passes through frozen θ_full, compute L_ViKeR using current θ and precomputed ̂R, backpropagate and update θ (LoRA weights only), evaluate on forget/retain/generalization/real sets
- **Design tradeoffs:**
  - λ (regularization strength): Higher λ → better retention/coherence but weaker unlearning. Paper uses λ=0.05 (10% task) or λ=0.5 (15% task). Tuning required per forget-set size.
  - k (number of reference images): More images → more stable ̂R but higher compute. Paper finds k≥5 stable; diminishing returns beyond.
  - Reference image source: "Irrelevant people" (Celeb) vs. pets/scenes/patterns. Paper shows pets/retain images hurt retention; patterns give best unlearning but "people" balances both.
  - Regularizer type: KL vs. JSD vs. Cosine Similarity. JSD ≈ KL performance but 2× compute; CoS preserves coherence but weak unlearning.
- **Failure signatures:** Incoherent outputs (low GIB) → λ too low or k too small; insufficient unlearning (high Forget ACC) → λ too high; retention degradation → reference images leak task-relevant information; gibberish outputs → GA component removed entirely
- **First 3 experiments:**
  1. Baseline comparison: Run GA, NPO, IdkPO, and ViKeR on MLLMU 10% task with identical hyperparameters. Record Forget/Generalization ACC, Retain/Real ROUGE-BLEU, and GIB.
  2. Ablation study: Remove each component: (a) "w/o Reg" (just GA), (b) "w/o GA" (just KL minimization), (c) "w/o Vis" (use target image's own distribution as reference).
  3. Reference image sensitivity: Test ViKeR with different reference sets: forget-set images, retain-set images, pets, scenes, textures, and irrelevant celebrities. Plot Forget ACC vs. Retain ROUGE.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal strategy for selecting the reference image set I' to maximize the trade-off between unlearning efficacy and retention, specifically regarding the semantic diversity and distributional distance from the forget set?
- **Basis in paper:** The paper explicitly investigates different reference sets (irrelevant celebrities, patterns, scenes, pets) in Section 6 and Appendix C.6, noting that "using 'people' images achieves a better balance" but observing significant variance where "scene" images perform poorly.
- **Why unresolved:** While the paper demonstrates that the choice of reference images impacts performance, it does not provide a theoretical framework or automated metric for selecting these images optimally for arbitrary datasets.
- **What evidence would resolve it:** A theoretical analysis correlating the entropy of the reference distribution ̂R with unlearning success, or an algorithmic approach for constructing I' that guarantees the suppression of key tokens while strictly preserving normal tokens across different domains.

### Open Question 2
- **Question:** Can the regularization strength λ be adapted dynamically on a token-level basis rather than globally, to better handle inputs with varying densities of sensitive information?
- **Basis in paper:** Section 5 defines "Normal" and "Key" tokens based on distinct entropy thresholds, yet the loss function applies a single global hyperparameter λ for regularization. The hyperparameter analysis shows a delicate, task-dependent trade-off controlled by λ.
- **Why unresolved:** A global λ forces a uniform trade-off for all tokens, potentially over-regularizing sparse sensitive information or under-regularizing dense sensitive information.
- **What evidence would resolve it:** Experiments demonstrating that an adaptive λ_i (regularization strength per token i), potentially derived from the information entropy H(̂R_i) itself, yields higher ROUGE/BLEU scores on the 'Retain' set without compromising 'Forget' accuracy compared to the static λ.

### Open Question 3
- **Question:** Does the visual-guided assumption—that irrelevant images produce stable distributions for "normal tokens"—hold for video-based MLLMs or complex reasoning tasks where visual grounding is temporal or logic-dependent?
- **Basis in paper:** The method is validated on static image benchmarks (MLLMU, CLEAR) using LLaVA. The paper states that MLLM research covers video, but the proposed method relies on static visual features to separate key tokens from normal ones.
- **Why unresolved:** In video or complex reasoning tasks, "normal" structural tokens might be highly dependent on the visual context. Removing the specific visual context might destabilize these tokens, violating the definition of "normal tokens" established in Definition 1.
- **What evidence would resolve it:** Application of ViKeR to video-language benchmarks showing that the entropy of "normal" structural tokens remains low when conditioned on irrelevant video inputs, maintaining the efficacy of the regularization.

## Limitations
- Distribution estimation dependency on carefully selected irrelevant reference images without specified selection criteria
- Entropy threshold sensitivity that may misclassify tokens with intermediate entropy values
- Gradient reweighting assumptions that may not hold if irrelevant identities don't generalize to forgotten identities
- Task-specific tuning requirements with different λ values needed for different forget-set sizes

## Confidence

**High Confidence:**
- ViKeR outperforms standard gradient ascent in retention and coherence metrics while maintaining comparable unlearning accuracy
- The entropy-based key token identification provides a principled criterion for token importance without manual annotation
- Visual guidance is critical for preventing the degeneration into incoherent outputs seen with GA alone

**Medium Confidence:**
- The visual-guided distribution estimation mechanism reliably approximates ideal post-unlearning token distributions
- The KL-regularized loss effectively reweights gradients to amplify key token updates while dampening normal token updates
- The method generalizes across different reference image sources with consistent performance

**Low Confidence:**
- The entropy threshold ε can be set consistently across different datasets and tasks
- The method's performance is robust to variations in reference image selection beyond the "irrelevant people" specification
- The gradient reweighting analysis holds under all training conditions and forget-set compositions

## Next Checks
1. **Reference Image Sensitivity Analysis:** Systematically test ViKeR with different reference image sets including forget-set images, retain-set images, pets, scenes, textures, and irrelevant celebrities. Plot Forget ACC vs. Retain ROUGE to empirically validate the claim that "people" images provide the best tradeoff, and quantify the performance variance across different reference sources.

2. **Entropy Threshold Robustness Test:** Implement the entropy-based key token identification and evaluate its classification accuracy on a held-out validation set where token importance is manually annotated. Test multiple entropy threshold values (ε) to determine the optimal setting and measure classification consistency across different identities and question types.

3. **Gradient Reweighting Validation:** Instrument the training process to track the actual gradient magnitudes for normal vs. key tokens during ViKeR training. Compare these empirical gradients against the theoretical predictions from Propositions 2 and 3, and verify that the KL regularization term produces the expected damping effect on normal tokens and amplification effect on key tokens.