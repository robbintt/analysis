---
ver: rpa2
title: Differentiable Adaptive Kalman Filtering via Optimal Transport
arxiv_id: '2508.07037'
source_url: https://arxiv.org/abs/2508.07037
tags:
- drift
- noise
- noise-statistics
- filtering
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OTAKNet, the first online solution to noise-statistics
  drift within learning-based adaptive Kalman filtering. The method uses optimal transport
  to connect the state estimate and the drift via one-step predictive measurement
  likelihood, enabling fully online adaptation without ground-truth labels or retraining.
---

# Differentiable Adaptive Kalman Filtering via Optimal Transport

## Quick Facts
- arXiv ID: 2508.07037
- Source URL: https://arxiv.org/abs/2508.07037
- Authors: Yangguang He, Wenhao Li, Minzhe Li, Juan Zhang, Xiangfeng Wang, Bo Jin
- Reference count: 6
- Primary result: OTAKNet is the first online solution for noise-statistics drift in learning-based adaptive Kalman filtering, using optimal transport to connect state estimates and drift via one-step predictive measurement likelihood.

## Executive Summary
This paper introduces OTAKNet, a novel online solution for noise-statistics drift in learning-based Kalman filtering that leverages optimal transport (OT) to enable fully label-free adaptation without retraining. The method constructs source and target distributions from the filter's predictive state and innovation history, then uses OT to align these distributions and adapt filter parameters online. OTAKNet outperforms classical adaptive Kalman filtering and offline learning-based methods on both synthetic (Lorenz attractor) and real-world (NCLT robot localization) datasets, particularly under limited training data conditions.

## Method Summary
OTAKNet extends a pre-trained neural Kalman filter backbone by adding an online OT-driven adaptation module. At each timestep, it constructs a source distribution from Monte Carlo samples of the filter's predictive state estimate and covariance, and a target distribution from current observations shifted by past innovations. The Wasserstein-2 distance between these distributions, computed via IPOT, serves as a differentiable loss that drives online parameter updates. The method includes a linear warm-up schedule to stabilize early steps when the residual buffer is small. Unlike existing approaches that require offline fine-tuning or ground-truth labels, OTAKNet performs fully online adaptation using only the measurement stream.

## Key Results
- OTAKNet achieves lower MSE than classical adaptive Kalman filtering and offline learning-based methods across various noise drift levels
- The method maintains performance under limited training data (3 trajectories vs. full 13) on real-world NCLT dataset
- Ablation studies show warm-up schedule contributes 0.97 dB improvement and OT loss contributes 0.20 dB improvement in MSE
- Computational overhead is significant (21.02s vs 0.53s for EKF on Lorenz attractor) but acceptable for offline or low-frequency applications

## Why This Works (Mechanism)

### Mechanism 1
The one-step predictive measurement likelihood characterizes noise-statistics drift impact on state estimation. A source distribution is constructed from the filter's prior state estimate and innovation covariance via Monte Carlo sampling, approximating the predictive measurement distribution under drifted noise statistics. A target distribution is built from current observation shifted by past innovations, encoding temporal drift information. The discrepancy between these distributions reflects the degree of noise-statistics drift. Core assumption: innovations are second-order stationary and ergodic.

### Mechanism 2
Wasserstein-2 distance provides geometry-aware, stable gradients for detecting and correcting noise-statistics drift. Unlike KL divergence, Wasserstein distance respects the metric structure of measurement space and yields non-vanishing gradients even under large distributional shifts. IPOT algorithm computes optimal transport plan differentiably, enabling gradient propagation to filter parameters. Core assumption: Polyak-Łojasiewicz condition holds for OT loss.

### Mechanism 3
Online parameter updates via OT loss achieve label-free adaptation without retraining. At each timestep, OT loss is backpropagated through neural filter parameters using Adam optimizer with linear warm-up schedule. Over successive steps, source distribution converges toward target, aligning filter's internal covariance with true innovation covariance. Core assumption: sufficient temporal diversity in innovation window to approximate drift statistics.

## Foundational Learning

- **Kalman filtering basics (predict-update cycle, innovation, covariance propagation)**: Essential for understanding how OTAKNet extends standard Kalman architecture and constructs source distributions. Quick check: Can you derive the one-step predictive measurement distribution for a linear Gaussian system?

- **Optimal transport fundamentals (Wasserstein distance, Sinkhorn/IPOT algorithms)**: Core to understanding how OTAKNet uses OT as differentiable loss and why coupling matrices preserve geometry. Quick check: What is computational complexity of Sinkhorn iterations and why does entropic regularization enable differentiability?

- **Differentiable programming and gradient-based online learning**: Critical for understanding test-time optimization, gradient flow through sampling operations, and PL condition for convergence. Quick check: Why might single-sample pointwise gradients fail for online adaptation and how does distributional OT address this?

## Architecture Onboarding

- **Component map**: Neural Kalman Filter -> Source Sampler -> Innovation Buffer -> Target Constructor -> OT Solver (IPOT) -> Optimizer (Adam) -> Updated Filter Parameters

- **Critical path**: 1) Run prior prediction to obtain mean and covariance 2) Sample N source particles 3) Compute current innovation and update buffer 4) Construct W target pseudo-measurements 5) Compute cost matrix and run IPOT 6) Backpropagate OT loss and update parameters 7) Run Kalman update with adapted parameters

- **Design tradeoffs**: Window size W (larger improves drift estimation but increases latency and memory; paper uses W=20), Particle count N (more particles improve source approximation but cost O(N²) per IPOT iteration), Learning rate warm-up (stabilizes early steps but delays adaptation)

- **Failure signatures**: Early-step instability (caused by insufficient residual history; mitigated by warm-up), Slow convergence under rapid drift (if drift occurs faster than W steps), Gradient explosion (check cost matrix normalization and epsilon settings)

- **First 3 experiments**: 1) Static drift validation: Train KalmanNet at ν=0dB, test OTAKNet across ν∈{-10,0,10,20,30}dB on Lorenz attractor; compare MSE vs. KalmanNet and EKF oracle 2) Ablation study: Remove warm-up and OT components separately; measure MSE degradation to confirm each mechanism's contribution 3) Limited training robustness: Train on only 3 NCLT trajectories vs. full 13; compare OTAKNet vs. KalmanNet under limited data

## Open Questions the Paper Calls Out

- **Computational overhead reduction**: The OT-based adaptation module requires ~40x more computation than standard EKF, raising questions about real-time applicability. The paper validates accuracy but doesn't propose optimization methods for IPOT algorithm or parallelization to meet strict timing constraints of embedded systems.

- **Stationarity assumption under abrupt drift**: The theoretical convergence guarantees depend on innovations remaining constant over the window W, which may not hold during sudden environmental changes like wind gusts. While the introduction claims handling of highly maneuvering scenarios, the stationarity assumption may fail under non-ergodic or discontinuous drift.

- **Adaptive selection of hyperparameters**: The sliding window length W and entropic regularization ε are treated as fixed hyperparameters without theoretical guidance. A fixed W imposes rigid memory on drift estimation, and optimal values may vary with drift frequency and amplitude.

## Limitations

- Computational overhead is substantial (21.02s vs 0.53s for EKF), limiting real-time applications
- Stationarity and ergodicity assumptions may not hold under rapid or non-ergodic drift
- Specific architectural and hyperparameter choices are underspecified, preventing exact reproduction

## Confidence

- **Main adaptive filtering claims**: Medium - OTAKNet shows improved performance over baselines, but theoretical guarantees are limited to ideal conditions
- **Learning-based components**: Medium - Standard approaches but underspecified details prevent exact reproduction
- **Online adaptation without retraining**: Low - Still requires pre-trained backbone and assumes bounded drift within adaptation window
- **Real-world robustness**: Medium - Small dataset (17 trajectories) and unclear temporal resolution of drift

## Next Checks

1. **Assumption A1 verification**: Measure empirical stationarity and ergodicity of innovations under different drift regimes to bound validity of source-target distribution alignment

2. **PL condition robustness**: Test convergence under non-PL landscapes (e.g., multi-modal drift) and quantify gradient vanishing under extreme distributional shifts

3. **Window size sensitivity**: Sweep W and learning rate η across varying drift speeds to identify regimes where adaptation lags or overfits