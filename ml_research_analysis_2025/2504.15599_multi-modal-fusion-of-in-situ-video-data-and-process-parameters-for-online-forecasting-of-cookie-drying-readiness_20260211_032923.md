---
ver: rpa2
title: Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online
  Forecasting of Cookie Drying Readiness
arxiv_id: '2504.15599'
source_url: https://arxiv.org/abs/2504.15599
tags:
- drying
- data
- video
- multi-modal
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an end-to-end multi-modal fusion framework
  for real-time online forecasting of cookie drying readiness. The method integrates
  in-situ video data with process parameters using modality-specific encoders and
  a transformer-based decoder to predict the remaining drying time.
---

# Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness

## Quick Facts
- arXiv ID: 2504.15599
- Source URL: https://arxiv.org/abs/2504.15599
- Reference count: 40
- Primary result: Achieves 15s average MAE for cookie drying readiness prediction, outperforming video-only and state-of-the-art fusion methods

## Executive Summary
This study proposes an end-to-end multi-modal fusion framework for real-time online forecasting of cookie drying readiness. The method integrates in-situ video data with process parameters using modality-specific encoders and a transformer-based decoder to predict the remaining drying time. Experimental results on sugar cookie drying show the model achieves an average prediction error of only 15 seconds, outperforming state-of-the-art fusion methods by 65.69% and a video-only model by 11.30%. The approach demonstrates strong robustness, flexibility, and generalizability across various configurations, balancing accuracy, model size, and computational efficiency for industrial applications. The framework is extensible to other industrial modality fusion tasks for online decision-making.

## Method Summary
The proposed method employs a modality-specific encoder-decoder architecture that fuses video observations with process parameters. Video frames are processed through a pretrained ResNet-18 followed by a GRU layer to capture spatial and temporal features, while tabular parameters are encoded via a fully connected network. The embeddings are concatenated along the temporal dimension and processed by a transformer decoder, producing a final regression output for time-to-ready prediction. The model is trained using smooth L1 loss and evaluated with leave-one-group-out cross-validation across different environmental conditions.

## Key Results
- Achieves 15-second average MAE on sugar cookie drying readiness prediction
- Outperforms video-only models by 11.30% and state-of-the-art fusion methods by 65.69%
- Demonstrates strong robustness and generalizability across various temperature and fan speed configurations
- Successfully balances accuracy, model size, and computational efficiency for industrial deployment

## Why This Works (Mechanism)

### Mechanism 1: Disentangled State and Rate Encoding
Fusing visual state data with environmental rate parameters enables generalization to unseen drying conditions better than single-modality approaches. The architecture separates encoding of intrinsic sample properties from extrinsic process conditions, learning to correlate specific visual stages with drying rates defined by parameters rather than memorizing a single average trajectory. This correlation may weaken if drying dynamics become non-linear such that visual cues decouple from thermal inputs.

### Mechanism 2: Tokenization of Static Context
Treating static process parameters as a temporal "frame" allows the Transformer decoder to attend to environmental context dynamically alongside video features. The model projects the small tabular parameter vector into a 32-dimension embedding and concatenates it to the temporal dimension of the video sequence, enabling the self-attention mechanism to compute relationships between video frames and specific oven settings. If sequence length becomes very large, the single static token might be "diluted" in attention computation.

### Mechanism 3: Inductive Bias for Small Data Regimes
A hybrid ResNet-GRU encoder provides better inductive bias for small, non-IID industrial datasets than large-scale Transformer-based video models. Explicit temporal modeling via GRU and spatial modeling via ResNet imposes stricter structure on feature extraction compared to fully learnable attention maps, acting as a regularizer that prevents overfitting on the limited 72-batch dataset. If dataset size were to increase by orders of magnitude, the restrictive bias of GRU might limit performance compared to fully attention-based models.

## Foundational Learning

- **Non-IID Data and Leave-One-Group-Out Cross-Validation (LOGOCV)**: Industrial drying datasets are often small and autocorrelated. Standard random splitting causes "data leakage" because frames from the same drying batch might appear in both training and testing, falsely inflating performance. Quick check: If you randomly split frames from a 9-minute video, why might your model fail to predict the readiness time for a cookie baked at a different temperature?

- **Modality-Specific Encoders**: Video data (high-dimensional, unstructured) and tabular data (low-dimensional, structured) have different statistical properties. Using a single network to process both often results in the simpler modality being ignored or the complex one being under-utilized. Quick check: Why project the 2-dimensional tabular input into a 32-dimensional vector before fusing it with the video embedding?

- **The "Ready" Moment Identification**: The model is a supervised learner requiring ground truth labels. In this paper, y is not a subjective quality score but a specific timestamp identified by a physical signal (temperature stabilization) validated by experts. Quick check: How does defining the "ready" moment based on chamber temperature change rather than just visual appearance improve the reliability of the training labels?

## Architecture Onboarding

- **Component map**: Input (Video Clip + Process Params) -> Video Encoder (ResNet-18 → GRU) -> Tabular Encoder (FCN → 32-dim) -> Fusion (Concatenate along temporal axis) -> Decoder (Transformer Encoder) -> Output (FCN)

- **Critical path**: 
  1. Segmentation: Raw video must be segmented using SAM to isolate cookie from background noise
  2. Frame Alignment: Ensure tabular parameters correspond exactly to timestamps of video frames
  3. Label Calculation: Convert absolute timestamps into "time-to-ready" values for regression target

- **Design tradeoffs**:
  - Accuracy vs. Latency: 7 frames selected as optimal clip length; increasing frames improves accuracy but sharply increases inference time
  - Complexity vs. Data Size: TimeSFormer/VideoMAE provide state-of-the-art accuracy on big data but fail here due to overfitting on 72 batches
  - Modality Reliance: Video-only model is more flexible but 11% less accurate; multi-modal requires sensor integration but is robust to unseen conditions

- **Failure signatures**:
  - Constant Prediction (MAE ~40s+): Model outputs average ready time; failed to learn features. Check encoder capacity or learning rate.
  - High Variance between Folds: Model works for Condition A but fails for Condition B. Check if LOGOCV is used and if process parameters are correctly fed into tabular encoder.
  - Early Stopping Plateau: Model stops improving because late-stage visual changes are too subtle for ResNet to distinguish.

- **First 3 experiments**:
  1. Baseline LOGOCV: Train model using standard random K-fold vs. paper's LOGOCV to verify need for specific validation strategy
  2. Frame Ablation: Run inference using 1 frame vs. 7 frames to quantify contribution of temporal dynamics vs. static spatial features
  3. Encoder Swap: Replace ResNet-18+GRU with 3D-CNN or TimeSFormer to verify sensitivity of architecture choice

## Open Questions the Paper Calls Out

- Can the multi-modal framework generalize to distinct industrial processes (e.g., ultrasonic welding) by incorporating additional sensor modalities? The conclusion proposes exploring generalizability across applications like ultrasonic welding using real-time sensor measurements, but current validation is restricted to sugar cookie drying.

- Can transfer learning strategies effectively mitigate data scarcity and process variability in drying applications? The conclusion identifies transfer learning and quantification of dataset similarities as necessary next steps, but the current model was trained from scratch on a limited dataset.

- How can state-of-the-art video architectures be optimized for small, non-i.i.d. industrial datasets where they currently underperform? Section 5.3 shows advanced models failed on the small dataset, indicating need for architectures suited to limited, specialized data, but it's unclear if poor performance is due to overfitting or lack of relevant pre-training data.

## Limitations
- Dataset size remains modest at 72 batches despite large number of frames per batch, raising questions about scalability
- Method requires in-situ sensor integration for process parameters, limiting deployment in legacy systems without instrumentation
- Model's performance on different food types or industrial drying processes beyond cookies remains unproven

## Confidence
- **High Confidence**: Core empirical finding that multi-modal fusion outperforms both single-modality baselines and state-of-the-art video models on this dataset; LOGOCV methodology is sound and well-justified
- **Medium Confidence**: Claimed mechanisms (disentangled state/rate encoding, tokenization of static context) are plausible based on architectural description and ablation studies but could benefit from additional qualitative analysis
- **Low Confidence**: Generalizability claims to "other industrial modality fusion tasks" are aspirational rather than demonstrated; assertion that video-only model's 11.30% accuracy deficit is due to missing process context is reasonable but not definitively proven

## Next Checks
1. **Attention Pattern Analysis**: Visualize self-attention weights in Transformer decoder to verify static tabular token receives appropriate weighting relative to video frames across different drying stages, providing direct evidence for "tokenization of static context" mechanism

2. **Cross-Food Generalization**: Apply trained model to different food drying dataset (e.g., bread or fruit drying) without retraining to test claimed generalizability and measure performance degradation

3. **Sensor-Less Deployment Test**: Implement video-only variant in real oven and compare practical utility against multi-modal version to validate whether 11.30% accuracy gap translates to meaningful operational differences in actual industrial settings