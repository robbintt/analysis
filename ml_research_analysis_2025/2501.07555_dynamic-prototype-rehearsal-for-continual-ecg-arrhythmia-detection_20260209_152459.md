---
ver: rpa2
title: Dynamic Prototype Rehearsal for Continual ECG Arrhythmia Detection
arxiv_id: '2501.07555'
source_url: https://arxiv.org/abs/2501.07555
tags:
- learning
- memory
- arrhythmia
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DREAM-CL, a novel continual learning method
  for ECG arrhythmia detection that dynamically selects challenging samples as prototypes
  for rehearsal memory. The method clusters training data based on learning behavior,
  applies Lambert W transform to compress extreme values and remove outliers, then
  selects the most difficult samples from each cluster to form the memory buffer.
---

# Dynamic Prototype Rehearsal for Continual ECG Arrhythmia Detection

## Quick Facts
- arXiv ID: 2501.07555
- Source URL: https://arxiv.org/abs/2501.07555
- Authors: Sana Rahmani; Reetam Chatterjee; Ali Etemad; Javad Hashemi
- Reference count: 28
- Primary result: Achieves state-of-the-art performance in continual ECG arrhythmia detection with AvgAUC 0.78-0.79 in class-incremental learning and 0.72-0.73 in lead-incremental learning using dynamic prototype selection.

## Executive Summary
This paper introduces DREAM-CL, a continual learning method for ECG arrhythmia detection that dynamically selects challenging samples as prototypes for rehearsal memory. The method clusters training data based on learning behavior, applies Lambert W transform to compress extreme values and remove outliers, then selects the most difficult samples from each cluster to form the memory buffer. Evaluated across time-incremental, class-incremental, and lead-incremental scenarios on Chapman and PTB-XL datasets, DREAM-CL demonstrates robust performance across sessions and shows that sample selection strategy is more critical than memory size.

## Method Summary
DREAM-CL tracks per-sample loss updates across training epochs, forming loss-delta vectors that capture learning behavior. These vectors are clustered using k-means (k=5) to group samples with similar training dynamics. The method applies Lambert W transform to normalized difficulty scores to compress extreme values and prevent outlier dominance. Within each cluster, the hardest samples (by transformed difficulty) are selected for the rehearsal memory buffer. During training, the model jointly optimizes on current session data and replayed prototypes from all previous sessions using a compound loss function.

## Key Results
- State-of-the-art AvgAUC scores of 0.78-0.79 in class-incremental learning scenarios
- Robust performance with AvgAUC 0.72-0.73 in lead-incremental learning scenarios
- Consistent results across different buffer sizes (0.25-0.75), demonstrating selection strategy matters more than memory size
- Strong backward transfer (BWT) with average 0.10 in class-incremental learning when using Lambert W transform

## Why This Works (Mechanism)

### Mechanism 1
Clustering samples by training dynamics captures diverse learning behaviors, improving memory representativeness. For each sample, the method computes loss deltas across epochs (∆L), creating a trajectory vector ν that reflects how the model responds to that sample. Samples with similar trajectories are grouped via k-means, then prototypes are drawn from each cluster. This ensures the memory buffer spans distinct training behaviors rather than redundant examples. Core assumption: Loss update patterns are a meaningful proxy for "learning behavior" and cluster membership generalizes to unseen sessions.

### Mechanism 2
Lambert W transform compresses extreme loss values, preventing outliers from monopolizing memory selection. After computing normalized difficulty scores d, the method applies δ = e^(-W(0.5 × max(-2/e, d))) where W is the Lambert W function. This smoothly compresses large values, allowing a balanced ranking that includes hard-but-not-extreme samples. Core assumption: Extreme loss values mostly represent outliers or noisy labels rather than maximally informative hard examples.

### Mechanism 3
Prioritizing harder samples for rehearsal improves backward transfer and reduces forgetting. Within each cluster, samples are ranked by transformed difficulty δ, and the top |M|/k hardest samples are stored. Higher-loss samples are more likely to belong to underrepresented classes and decision boundary regions, making them more effective for retention. Core assumption: Harder samples correlate with class imbalance or boundary cases, and replaying them preserves critical decision surfaces.

## Foundational Learning

- **Concept: Rehearsal-based Continual Learning**
  - Why needed here: DREAM-CL is fundamentally a replay method; understanding why storing past samples mitigates forgetting is prerequisite.
  - Quick check question: Can you explain why replaying old data prevents weight drift from affecting previously learned classes?

- **Concept: Training Dynamics and Sample Difficulty**
  - Why needed here: The method ranks samples by loss trajectories; grasping why loss patterns reveal difficulty is essential.
  - Quick check question: What does a consistently high loss across epochs suggest about a sample's relationship to the decision boundary?

- **Concept: Lambert W Function Basics**
  - Why needed here: Used for value compression; intuition about how it dampens extremes without thresholding is useful.
  - Quick check question: How does Lambert W differ from simple clipping or log transform for handling outliers?

## Architecture Onboarding

- **Component map:** Training loop → per-sample loss tracking across epochs → loss-delta vectors ν → k-means clustering on ν → k clusters of training behaviors → Lambert W transform on normalized loss → difficulty scores δ → Top-|M|/k selection per cluster → rehearsal memory M → Compound loss: L_session + L_memory (replay from all prior memories)

- **Critical path:**
  1. Run standard training on current session D^t while logging loss per sample per epoch
  2. Compute ∆L vectors and cluster (k=5 per paper)
  3. Apply Lambert W scoring and select hardest samples per cluster
  4. Merge new prototypes into M and train with joint loss

- **Design tradeoffs:**
  - Memory size: Paper shows 0.25–0.75 yields nearly identical results; selection quality matters more than buffer size
  - Cluster count: Sensitivity analysis (Fig. 3) shows plateau after k=3–4; paper uses k=5
  - Clustering method: k-means slightly outperforms GMM in ablation; difference is minor

- **Failure signatures:**
  - BWT dropping significantly → check if Lambert W was skipped or random sampling used
  - High variance across runs → inspect cluster stability; may need to seed k-means consistently
  - Poor performance on rare classes → verify δ-ranking isn't being overwhelmed by a single large cluster

- **First 3 experiments:**
  1. Reproduce class-incremental baseline with rM=0.25; verify AvgAUC ~0.73 and BWT ~0.10
  2. Ablate Lambert W (use raw loss for ranking); expect BWT degradation per Table II
  3. Run sensitivity on cluster count (k=2 to k=9) to confirm plateau behavior on your dataset

## Open Questions the Paper Calls Out

The paper identifies lead-incremental learning as particularly challenging, noting "lead-incremental seems more challenging for CL as evidenced by the fluctuations," but provides no specific mechanism beyond the general prototype selection to address this instability. The authors also do not analyze computational overhead or resource usage, leaving the feasibility for resource-constrained devices unclear. While concluding that selection strategy is more critical than memory size based on tests with 25-75% buffer sizes, the effectiveness in extreme low-memory regimes (<10%) remains untested.

## Limitations

- Method's reliance on loss trajectories assumes training dynamics are stable across sessions; dramatic distribution shifts may cause poor clustering
- Lambert W transform efficacy depends on assumption that extreme losses are mostly outliers rather than informative hard examples
- Memory buffer selection is sensitive to k-means initialization, though the paper seeds it consistently
- Computational overhead of tracking loss dynamics and clustering is not analyzed for resource-constrained devices

## Confidence

- **High Confidence**: Prototype rehearsal improves backward transfer compared to no-memory baselines
- **Medium Confidence**: Dynamic clustering by learning behavior yields better memory representativeness than random sampling
- **Medium Confidence**: Lambert W compression is beneficial; ablation shows clear BWT drops without it

## Next Checks

1. Test sensitivity to cluster count (k=2 to k=9) on your dataset to confirm the plateau effect observed in the paper
2. Compare Lambert W transform against simpler alternatives (log, clipping) to verify its unique contribution to outlier handling
3. Analyze memory buffer composition to ensure it covers underrepresented classes and decision boundary regions, not just globally hard samples