---
ver: rpa2
title: 'WETBench: A Benchmark for Detecting Task-Specific Machine-Generated Text on
  Wikipedia'
arxiv_id: '2507.03373'
source_url: https://arxiv.org/abs/2507.03373
tags:
- wikipedia
- arxiv
- text
- detectors
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces WETBench, a multilingual benchmark for detecting
  machine-generated text in Wikipedia-specific editing tasks. Unlike prior work focusing
  on generic generation, WETBench evaluates three editor-driven tasks: Paragraph Writing,
  Summarisation, and Text Style Transfer, implemented across English, Portuguese,
  and Vietnamese using two new datasets.'
---

# WETBench: A Benchmark for Detecting Task-Specific Machine-Generated Text on Wikipedia

## Quick Facts
- **arXiv ID**: 2507.03373
- **Source URL**: https://arxiv.org/abs/2507.03373
- **Authors**: Gerrit Quaremba; Elizabeth Black; Denny Vrandečić; Elena Simperl
- **Reference count**: 40
- **Key outcome**: Training-based detectors achieve 78% average accuracy on Wikipedia editing tasks while zero-shot detectors average 58%, with detection hardest for Text Style Transfer and lowest-resource languages.

## Executive Summary
This paper introduces WETBench, a multilingual benchmark for detecting machine-generated text in Wikipedia-specific editing tasks. Unlike prior work focusing on generic generation, WETBench evaluates three editor-driven tasks: Paragraph Writing, Summarisation, and Text Style Transfer, implemented across English, Portuguese, and Vietnamese using two new datasets. The authors test various prompts, generate MGT with four LLMs, and benchmark eight detectors. Results show training-based detectors achieve 78% average accuracy while zero-shot detectors average 58%, with substantial variation across languages, models, and tasks. Detection is hardest for Text Style Transfer and lowest-resource languages. These findings highlight that current detectors struggle with realistic Wikipedia editing scenarios, emphasizing the need for task-specific evaluation to assess reliability in real-world contexts.

## Method Summary
WETBench evaluates MGT detection on three Wikipedia editing tasks using two new datasets: WikiPS (lead-infobox-body triplets for Paragraph Writing and Summarisation) and mWNC (biased-neutralised pairs for Text Style Transfer). The benchmark generates MGT using four LLMs (GPT-4o mini, Gemini 2.0 Flash, Qwen2.5-7B-Instruct, Mistral-7B-Instruct) with task-specific prompts (RAG for PW, one-shot for SUM, five-shot for TST). Eight detectors are evaluated: two training-based (XLM-RoBERTa, mDeBERTa) and six zero-shot (Binoculars, LLR, FastDetectGPT white-box; Revise-Detect, GECScore, FastDetectGPT black-box). The evaluation uses 2,700 human-written texts per task-language subset, with accuracy and F1-score as primary metrics.

## Key Results
- Training-based detectors achieve 78% average accuracy across all tasks and languages
- Zero-shot detectors average 58% accuracy, with substantial variation across detector families
- Text Style Transfer shows near-random detection (52-56% accuracy for zero-shot methods)
- Lowest-resource languages (Vietnamese, Portuguese) show reduced detection performance compared to English
- Mistral-7B exhibits anomalous behavior on Vietnamese tasks, producing English or incomplete outputs

## Why This Works (Mechanism)

### Mechanism 1
Task-specific editing scenarios produce MGT that is harder to detect than open-ended generation. Task-specific prompts constrain outputs toward factual grounding and stylistic similarity to existing Wikipedia content, reducing distributional divergence between MGT and HWT. Core assumption: Detectors trained on generic generation learn high-level patterns that don't transfer to constrained, editor-driven tasks. Evidence: Training-based detectors achieve 78% accuracy while zero-shot detectors average 58%. Break condition: Retraining detectors on task-specific MGT distributions should narrow the performance gap.

### Mechanism 2
The degree of LLM contribution to final text predicts detection difficulty. Tasks requiring more original generation (Paragraph Writing > Summarisation > Text Style Transfer) leave more detectable signals; minimal-edit tasks like TST preserve human-written characteristics. Core assumption: Supervised detectors can exploit subtle syntactic/statistical differences even in minimal-edit scenarios. Evidence: Zero-shot detectors show 52-56% accuracy for sentence-level TST, while paragraph-level improves by 18%. Break condition: Reframing TST as paragraph-level rather than sentence-level improves detection.

### Mechanism 3
Resource-level asymmetry in training data amplifies multilingual detection failures. English provides abundant training data while Portuguese and Vietnamese have less, causing model internals and token-level patterns to diverge unpredictably. Core assumption: Multilingual models' representations are not equally calibrated across resource levels. Evidence: Substantial anomalies in Mistral's Vietnamese output; training-based methods maintain 75-86% accuracy but with higher variance. Break condition: Training detectors on balanced multilingual task-specific data should decrease language-based performance variance.

## Foundational Learning

- **Machine-Generated Text Detection Taxonomy**: Understanding detector families (training-based, zero-shot white-box, zero-shot black-box) and their access requirements (logits vs. API-only) is essential for interpreting results. Quick check: Given only API access to an LLM, which detector families can you use?

- **Prompt Engineering for Constrained Generation**: The benchmark's validity depends on prompts that produce realistic editor-driven outputs. Quick check: Why does the Naive RAG prompt outperform Minimal prompts for Paragraph Writing (QAFactEval 0.38 vs. 0.06)?

- **Wikipedia Content Policies (NPOV, Verifiability, Lead Section Style)**: The TST task targets NPOV violations while Summarisation follows Manual of Style for lead sections. Quick check: What makes a Wikipedia lead section different from a generic summary?

## Architecture Onboarding

- **Component map**: WikiPS corpus (Paragraph Writing, Summarisation) -> mWNC corpus (Text Style Transfer) -> Prompt evaluation (BLEU, ROUGE, BERTScore, QAFactEval, style classifiers) -> MGT generation (4 LLMs × 3 tasks × 3 languages) -> Detection evaluation (8 detectors × accuracy/F1 metrics)

- **Critical path**: 1) Sample 2,700 HWT per task-language subset (length-stratified) -> 2) Generate MGT using task-optimal prompts -> 3) Run all 8 detectors -> 4) Compute accuracy and F1 -> 5) Analyze variance across tasks, languages, generators, detector families

- **Design tradeoffs**: Pre-ChatGPT data cutoff ensures no MGT contamination but may not reflect current patterns; sentence vs. paragraph-level TST balances data quantity with detection difficulty; prompt selection via automatic metrics may not perfectly align with human judgments

- **Failure signatures**: Mistral Vietnamese anomaly (model fails to follow prompts, producing English/incomplete text with 98% supervised accuracy but random zero-shot performance); TST sentence-level collapse (zero-shot detectors at ~50% accuracy across all generators); cross-generator variance (Gemini 2.0 Summarisation shows 6-17% lower detection than other generators)

- **First 3 experiments**: 1) Baseline replication: Run XLM-RoBERTa and mDeBERTa on English Paragraph Writing to verify ~83-87% accuracy range; 2) Ablate prompt conditioning: Compare Minimal vs. RAG prompts for Paragraph Writing across all detectors; 3) Cross-task transfer test: Train detectors on Summarisation MGT, evaluate on TST MGT to measure generalization gap

## Open Questions the Paper Calls Out
None

## Limitations
- Uses pre-ChatGPT Wikipedia data (before Nov 2022), which may not reflect current editing patterns
- TST detection performance relies on noisy style classifiers as ground truth, particularly problematic for Vietnamese and English paragraph-level tasks
- Benchmark only evaluates four LLMs and eight detectors, limiting coverage of the broader landscape
- Language coverage restricted to three languages with substantial resource asymmetry between English and other two

## Confidence
- **High Confidence**: Training-based detectors (78% average accuracy) significantly outperform zero-shot detectors (58% average accuracy) across Wikipedia editing tasks
- **Medium Confidence**: Text Style Transfer is hardest to detect, with near-random zero-shot performance (52-56%), though noisy ground truth makes definitive conclusions difficult
- **Medium Confidence**: Multilingual detection becomes more challenging for lower-resource languages, with the Mistral Vietnamese anomaly providing strong evidence but potentially not generalizing to other generators

## Next Checks
1. **Cross-task transfer evaluation**: Train detectors on one Wikipedia task (e.g., Paragraph Writing) and test on another (e.g., Text Style Transfer) to determine whether detectors learn task-specific vs. general MGT patterns

2. **Ground truth verification for TST**: Manually annotate a subset of TST outputs to verify accuracy of style classifier-based ground truth and quantify noise level

3. **Language model robustness test**: Replicate detection evaluation using different set of four LLMs (e.g., Claude, Llama, Phi-3, GPT-4) to assess whether observed performance patterns hold across different generator families