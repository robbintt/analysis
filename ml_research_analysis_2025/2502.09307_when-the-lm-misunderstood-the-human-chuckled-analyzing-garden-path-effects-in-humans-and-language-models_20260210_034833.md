---
ver: rpa2
title: 'When the LM misunderstood the human chuckled: Analyzing garden path effects
  in humans and language models'
arxiv_id: '2502.09307'
source_url: https://arxiv.org/abs/2502.09307
tags:
- while
- sentence
- sentences
- llms
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether Large Language Models (LLMs) and
  humans experience similar comprehension difficulties with garden-path sentences,
  a well-known challenge in human psycholinguistics. The authors propose three non-mutually
  exclusive hypotheses: (1) garden-path syntax is inherently harder due to reanalysis
  demands; (2) misinterpretation is driven by semantic plausibility of the noun as
  a verb object; (3) transitive verbs lead to more misinterpretation than reflexive
  or unaccusative verbs.'
---

# When the LM misunderstood the human chuckled: Analyzing garden path effects in humans and language models

## Quick Facts
- arXiv ID: 2502.09307
- Source URL: https://arxiv.org/abs/2502.09307
- Reference count: 18
- Key outcome: LLMs and humans show similar garden-path sentence comprehension difficulties, with larger models exhibiting stronger human-like correlations

## Executive Summary
This study investigates whether Large Language Models (LLMs) experience comprehension difficulties similar to humans when processing garden-path sentences. Through controlled experiments with human participants and multiple LLM families, the authors find that both humans and LLMs struggle with garden-path structures, with misinterpretation driven by syntactic reanalysis demands and semantic plausibility. Larger, more capable models show stronger correlations with human comprehension patterns. Additional validation tasks (paraphrasing and image generation) confirm that LLMs often misinterpret garden-path sentences in ways similar to humans, supporting their potential as models for human-like language processing.

## Method Summary
The study used 69 sentence sets (228 sentences total) with controlled manipulations of garden-path structure, semantic plausibility, and verb type. Human participants completed word-by-word reading tasks with comprehension questions, while LLMs were evaluated using few-shot prompting across 8 prompt variants. Accuracy on binary comprehension questions was measured, and correlations with human performance were computed using Kendall Tau (item-level) and Spearman (condition-level) metrics. Additional validation tasks included automatic paraphrasing evaluation and manual image generation analysis to assess whether misinterpretations propagated to downstream tasks.

## Key Results
- Both humans and LLMs show reduced accuracy on garden-path sentences compared to non-garden-path structures
- Accuracy is lower when the noun is a plausible direct object, with plausibility effects stronger than syntactic effects
- Larger LLMs exhibit higher Kendall Tau correlation with human judgements across all model families
- DALL-E 3 generated images showing misinterpretations (e.g., boy washing dog) confirm comprehension failures propagate to downstream tasks

## Why This Works (Mechanism)

### Mechanism 1: Syntactic Reanalysis Demand
Garden-path sentences cause comprehension failures because initial misparses persist despite reanalysis cues at the main verb. Readers incrementally attach post-verbal nouns as direct objects, and when the second verb arrives, reanalysis is often incomplete, leaving the initial interpretation active alongside the correct one. Evidence: accuracy consistently higher for non-GP structures, and models abandon structure for semantic shortcuts when parsing is hard.

### Mechanism 2: Semantic Plausibility Bias
Semantic plausibility of the noun as a verb object drives misinterpretation independently of syntax. When a noun is a plausible direct object (e.g., "hunt the deer"), readers attach it to the verb even without syntactic support; implausible nouns resist attachment. Evidence: accuracy lower when the noun is a plausible direct object, and the plausibility effect was more pronounced than the syntactic effect.

### Mechanism 3: Model Scale-Human Alignment Correlation
Larger and more capable LLMs exhibit stronger correlation with human comprehension patterns. Increased model capacity and training data improve modeling of incremental processing pressures, including syntactic and semantic biases. Evidence: larger models exhibit higher Kendall Tau correlation with human judgements across all model families, with monotonic increase in correlation with parameter count.

## Foundational Learning

- **Concept: Garden-path sentences**
  - Why needed: Understanding why temporary ambiguity causes persistent misinterpretation is central to interpreting study results
  - Quick check: In "While the boy washed the dog barked," what is the correct syntactic role of "the dog"?

- **Concept: Incremental processing**
  - Why needed: Both humans and LLMs process tokens sequentially, creating opportunity for early misparses to persist
  - Quick check: Why might a model with full sentence context still show garden-path effects?

- **Concept: Kendall Tau vs Spearman correlation**
  - Why needed: Kendall Tau measures item-level difficulty ranking alignment; Spearman measures condition-level ranking
  - Quick check: Which metric better captures whether specific sentences are hard for both humans and LLMs?

## Architecture Onboarding

- **Component map**: Stimuli generation -> Human evaluation -> LLM evaluation -> Validation tasks
- **Critical path**: 1) Construct sentence sets with controlled manipulations 2) Collect human accuracy data 3) Extract LLM response probabilities 4) Compute Kendall Tau and Spearman correlations
- **Design tradeoffs**: Single-trial human design prevents learning effects but limits data; few-shot prompting standardizes evaluation but may not reflect zero-shot deployment; automatic metrics may miss subtle misinterpretations
- **Failure signatures**: Models answering "Yes" to GP questions indicates lingering misinterpretation; DALL-E 3 generating images showing misinterpretation confirms comprehension failure propagates; flat accuracy across conditions suggests model is not sensitive to manipulated factors
- **First 3 experiments**: 1) Replicate with additional GP types (reduced relative clauses, coordination ambiguities) 2) Vary prompt context (zero-shot vs few-shot vs chain-of-thought) 3) Add reading time or surprisal measures for LLMs to test difficulty localization

## Open Questions the Paper Calls Out

- **Open Question 1**: Do LLMs exhibit similar comprehension failures in other garden-path constructions (e.g., NP/S ambiguity) as observed in Subject/Object structures?
  - Basis: The authors state their focus was limited to Subject/Object sentences and that exploring other types would be interesting
  - Why unresolved: The study only designed materials for one specific subtype of syntactic ambiguity
  - What evidence would resolve it: Experiments utilizing materials from other garden-path categories tested on the same LLM suite

- **Open Question 2**: Do human eye-gaze or reading-time metrics on garden-path sentences correlate with specific LLM internal representations?
  - Basis: The authors note they did not collect data on metrics beyond reading comprehension and suggest gathering such metrics could provide valuable insights
  - Why unresolved: The study relied on offline comprehension question accuracy rather than real-time processing measures
  - What evidence would resolve it: Parallel data collection of human eye-tracking or self-paced reading times compared against LLM surprisal or attention weights

- **Open Question 3**: Why do LLMs fail to correct garden-path misinterpretations despite processing the entire sentence simultaneously?
  - Basis: Section 4.2 notes that LLM imperfection is "perhaps surprising" since there is no reason to suspect they should suffer from the same processing difficulties humans do
  - Why unresolved: The study identifies behavioral correlation but does not isolate architectural or training reasons for inability to override the "initial" parse
  - What evidence would resolve it: Probing experiments to determine if models lack capacity for reanalysis or if misinterpretation arises from semantic priors dominating context window

## Limitations

- Few-shot prompting methodology may artificially induce garden-path effects through instruction bias
- Single-trial human design limits statistical power and makes individual differences unmeasurable
- Comprehension question format may not fully capture whether models genuinely misinterpret versus strategically answering incorrectly

## Confidence

- **High Confidence**: Larger LLMs show stronger correlation with human garden-path effects (Section 5 findings)
- **Medium Confidence**: Semantic plausibility drives garden-path misinterpretation (Section 3.2 findings)
- **Medium Confidence**: Models exhibit "incremental processing" similar to humans (Section 4.3 observations)

## Next Checks

1. Run the same garden-path experiments with zero-shot prompting to determine whether few-shot examples artificially induce the misinterpretation effects
2. Extract token-level surprisal or latency measures from transformer-based LLMs to test whether processing difficulty concentrates at the disambiguating verb position
3. Replace binary yes/no questions with multiple-choice or forced-choice tasks to distinguish between genuine comprehension failures and strategic answering