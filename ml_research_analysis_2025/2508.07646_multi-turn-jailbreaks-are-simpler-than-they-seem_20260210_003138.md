---
ver: rpa2
title: Multi-Turn Jailbreaks Are Simpler Than They Seem
arxiv_id: '2508.07646'
source_url: https://arxiv.org/abs/2508.07646
tags:
- multi-turn
- attacks
- single-turn
- jailbreak
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges the perceived sophistication of multi-turn
  jailbreak attacks on large language models. The authors find that when accounting
  for retry mechanisms after refusals, multi-turn jailbreaking approaches are approximately
  equivalent to simply resampling single-turn attacks multiple times.
---

# Multi-Turn Jailbreaks Are Simpler Than They Seem

## Quick Facts
- arXiv ID: 2508.07646
- Source URL: https://arxiv.org/abs/2508.07646
- Reference count: 40
- This study challenges the perceived sophistication of multi-turn jailbreak attacks on large language models.

## Executive Summary
This study challenges the perceived sophistication of multi-turn jailbreak attacks on large language models. The authors find that when accounting for retry mechanisms after refusals, multi-turn jailbreaking approaches are approximately equivalent to simply resampling single-turn attacks multiple times. Using the StrongREJECT benchmark across models including GPT-4, Claude, and Gemini variants, they demonstrate that attack success rates correlate strongly among models from the same provider, making newly released models predictably vulnerable. Additionally, for reasoning models, higher reasoning effort often leads to higher attack success rates, contrary to expectations. The results suggest that defense efforts should focus on fundamental robustness rather than detecting multi-turn patterns, and that safety evaluations must incorporate systematic resampling to accurately assess model vulnerabilities.

## Method Summary
The paper evaluates jailbreak attacks using a three-component pipeline: an attacker model generates adversarial prompts based on tactics (Direct Request, Command, Crowding, Emotional Appeal), a target model processes these prompts, and an evaluator model scores responses using the StrongREJECT rubric (0-1 scale based on specificity and convincingness). Experiments compare multi-turn attacks (up to 8 turns with 10 refusals) against single-turn attacks with equivalent retry opportunities. The study uses GPT-4o-mini for both attacker and evaluator roles, testing against various frontier models including GPT-4, Claude, and Gemini variants via OpenRouter API.

## Key Results
- Multi-turn jailbreak attacks show no fundamental advantage over single-turn attacks when controlling for retry attempts after refusals
- Attack success rates are strongly correlated within model families, making newly released models predictably vulnerable to existing attacks
- For reasoning models, increased reasoning effort paradoxically leads to higher jailbreak success rates rather than improved safety

## Why This Works (Mechanism)

### Mechanism 1: Multi-Turn Equivalence to Resampling
The attacker model receives feedback from failed attempts (refusals) and modifies subsequent prompts. This functions similarly to resampling single-turn attacks, with additional refusal context allowing the attacker to avoid failed phrasings and explore alternatives, offering a marginal but not qualitative advantage. Effectiveness stems primarily from the number of attempts and learning from failures, not from a unique multi-turn conversational dynamic.

### Mechanism 2: Attack Success Correlation Within Model Families
Models from the same provider share similar training methodologies and safety measures, leading to correlated responses to similar attack prompts. This allows attackers to easily transfer successful attacks to newly released models, as shared training data, alignment techniques, and architectures create persistent, correlated vulnerabilities.

### Mechanism 3: Increased Reasoning Effort Paradoxically Aids Attacks
More reasoning effort may lead the model to find a "harmless framing" for a dual-use harmful request, or produce outputs that appear more specific and convincing, accidentally fooling the evaluator. Given more computation, reasoning models may optimize for answering the query (finding a compliant path) over adhering to a safety refusal, especially for dual-use requests.

## Foundational Learning

- **Concept: Retry-Adjusted Attack Success Rate**
  - Why needed here: To fairly compare multi-turn and single-turn attacks. Naive comparisons are confounded by the number of attempts. You must normalize for `n' = n_turns + n_refusals` to measure true efficacy.
  - Quick check question: If a single-turn attack is allowed 5 retries after refusal, and a multi-turn attack uses 3 turns with 2 refusals, which has more "effective attempts" and why might the multi-turn attack still have a slight edge?

- **Concept: Attack Transferability**
  - Why needed here: To predict the vulnerability of new models. The paper shows high correlation within model families, meaning a successful attack on `Claude-3.5-Sonnet` is a strong prior for `Claude-Sonnet-4`. This informs red-teaming prioritization.
  - Quick check question: Why might an attack that successfully jailbreaks `GPT-4o` fail on `Claude-3.5-Sonnet`, despite both being state-of-the-art models?

- **Concept: Dual-Use Evasion**
  - Why needed here: To understand why reasoning models may be more vulnerable. Many harmful requests have innocuous interpretations (dual-use). A reasoning model may spend compute finding the innocuous path, which the evaluator then scores as a successful jailbreak if the response is sufficiently specific.
  - Quick check question: Give an example of a "dual-use" harmful request. How might a reasoning model's extended "thinking" process lead it to answer this request in a way a non-reasoning model would not?

## Architecture Onboarding

- **Component map:** Attacker Model (`M_A`) -> Target Model (`M_T`) -> Evaluator Model (`M_E`)
- **Critical path:**
  1. Initialization: Load harmful behaviors and attacker tactic prompts
  2. Attack Loop: `M_A` generates prompt, `M_T` responds, `M_E` scores response
  3. Adaptation: If score is low (refusal), controller logs failure and re-prompts `M_A` with full history, or re-samples attack from scratch
  4. Termination: Loop ends after max_turns or max_refusals, maximum score across all turns is recorded

- **Design tradeoffs:**
  - Evaluator Accuracy vs. Cost: Using more capable evaluator improves accuracy but increases cost; Direct Request tactic has lowest false positive rate
  - Attacker Model Capability: Stronger attacker generates more effective attacks but increases cost; current setup may introduce shared-model bias
  - Resampling vs. Retry-with-Context: Resampling is computationally cheaper but less effective; retry-with-context is more effective but requires managing conversation state

- **Failure signatures:**
  - High False Positive Rate: Evaluator gives score of 1.0 for vague or misinterpreted responses (more common with "Crowding" or "Emotional Appeal" tactics)
  - Over-Refusal: Model refuses innocuous requests, leading to artificially low scores (suspected in `Claude 3.5 Sonnet`)
  - Attack Stagnation: Attacker model gets stuck in loop, repeating similar failed prompts without progress

- **First 3 experiments:**
  1. Baseline Equivalence Test: Run "Direct Request" attacks against 2-3 target models, comparing multi-turn vs single-turn with retries vs single-turn without retries, plotting score vs effective attempts
  2. Reasoning-Effort Ablation: Select reasoning model, run fixed attack while varying reasoning token budget, plot final StrongREJECT score
  3. Tactic Robustness Check: Run "Direct Request" and "Command" tactics on 10 harmful behaviors, manually re-evaluate successful jailbreaks to estimate false positive rates

## Open Questions the Paper Calls Out

- Does the observed equivalence between multi-turn attacks and repeated single-turn attacks hold for sophisticated context-dependent attacks like Crescendo?
- What mechanisms cause higher reasoning effort to increase jailbreak success rates in reasoning models?
- Does shared-model bias between attacker and evaluator inflate measured jailbreak success rates?

## Limitations

- StrongREJECT evaluator reliability is compromised by shared-model bias between attacker and evaluator, with high false positive rates for certain tactics
- The study doesn't address whether equivalence holds under realistic resource constraints or whether certain tactics become more effective with deeper conversation trees
- Correlation within model families may not persist across architectural shifts to non-transformer models

## Confidence

- **High Confidence**: Multi-turn jailbreaks are equivalent to resampled single-turn attacks when controlling for retry attempts
- **Medium Confidence**: Attack transferability claims within model families
- **Low Confidence**: Increased reasoning effort leads to higher attack success rates

## Next Checks

1. Systematically measure StrongREJECT evaluator false positive rates across all five tactics and multiple model pairs to establish reliability bounds
2. Replicate multi-turn equivalence experiments with reduced n_turns and n_refusals parameters to determine whether equivalence holds under realistic resource constraints
3. Test attack transferability hypothesis by including at least one non-transformer model to determine whether correlations are architecture-specific or more fundamental