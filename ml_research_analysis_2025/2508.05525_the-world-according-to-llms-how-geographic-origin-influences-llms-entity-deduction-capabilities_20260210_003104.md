---
ver: rpa2
title: 'The World According to LLMs: How Geographic Origin Influences LLMs'' Entity
  Deduction Capabilities'
arxiv_id: '2508.05525'
source_url: https://arxiv.org/abs/2508.05525
tags:
- entity
- entities
- language
- notable
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to uncover implicit geographic
  biases in large language models (LLMs) by having models play the 20 Questions game,
  proactively asking questions to deduce entities from diverse regions. The authors
  created a new dataset, Geo20Q+, featuring geographically specific entities from
  the Global North and South, and Global West and East.
---

# The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities

## Quick Facts
- arXiv ID: 2508.05525
- Source URL: https://arxiv.org/abs/2508.05525
- Reference count: 40
- Models substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East.

## Executive Summary
This paper introduces a novel approach to uncover implicit geographic biases in large language models (LLMs) by having models play the 20 Questions game, proactively asking questions to deduce entities from diverse regions. The authors created a new dataset, Geo20Q+, featuring geographically specific entities from the Global North and South, Global West and East. Evaluating three popular LLMs across seven languages and two gameplay settings, the study reveals significant geographic disparities: models perform substantially better at deducing entities from the Global North and West compared to the Global South and East. These disparities persist even when controlling for entity popularity and are largely unaffected by the language of gameplay. The results demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups.

## Method Summary
The study uses a 20 Questions game framework where two instances of the same LLM play judge (knows the entity) and guesser (deduces through strategic questioning). The Geo20Q+ dataset contains 8,375 Things (landmarks, foods, animals) and 24,049 Notable people sourced from Wikipedia, filtered for unambiguous country attribution. Entities are organized by continents (100 most-viewed per category) and countries (10 most popular per type from 172 countries). Two configurations are used: Canonical (exactly 20 turns) and Unlimited (up to 150 turns). Seven languages are evaluated: English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish. Success rate (percentage of games where guesser correctly identifies entity) and number of turns to answer are tracked.

## Key Results
- LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East.
- While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities.
- Performance gaps persist across different languages and are largely unaffected by the language of gameplay.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-turn, model-initiated questioning task (20 Questions) reveals implicit geographic biases that remain hidden when models respond to human-crafted prompts.
- Mechanism: As the "guesser," the LLM must generate a strategic sequence of questions to deduce an unknown entity. This process exposes its internal knowledge hierarchies and prioritized associations (e.g., asking about Western locations first). This is more revealing than standard probing, where models can rely on surface-level alignment or trigger guardrails.
- Core assumption: The model's questioning strategy reflects its underlying knowledge structure and reasoning biases, not merely its ability to follow game rules.
- Evidence anchors:
  - [abstract] "...analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes."
  - [PAGE 2, Introduction] "Instead, we propose... allow LLMs to proactively ask a sequence of questions... By examining which questions models prioritize... we can observe biases in their reasoning that remain hidden in standard evaluations."
  - [corpus] Related work supports LLM bias in recommendations (e.g., "Where Should I Study?"), but does not use the 20 Questions framework.
- Break condition: If models used a uniform, geography-agnostic questioning strategy, this mechanism would fail. The paper shows this is not the case via systematic success disparities.

### Mechanism 2
- Claim: Performance disparities are not fully explained by how often an entity appears in the training data.
- Mechanism: The authors control for entity prominence using Wikipedia pageviews and frequency in a large proxy corpus (Dolma). Regression analyses show these factors have a weak relationship with performance (low R²), suggesting the gaps stem from more complex factors like knowledge organization or retrieval.
- Core assumption: Wikipedia pageviews and Dolma corpus frequency are reasonable proxies for entity prominence in the proprietary models' actual training data.
- Evidence anchors:
  - [abstract] "While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities."
  - [PAGE 8, Table 3] Regression analysis shows low R² values (e.g., for GPT-4o-mini, R²=0.023 for predicting turns for Notable People), indicating weak explanatory power.
  - [corpus] No direct corpus evidence supports or refutes this specific claim.
- Break condition: If controlling for popularity/frequency eliminated the geographic performance gaps, the claim of deeper structural bias would be invalidated.

### Mechanism 3
- Claim: LLMs demonstrate a consistent and significant performance advantage for entities from the Global North and West over the Global South and East.
- Mechanism: Using the new Geo20Q+ dataset, the authors show that across models, settings, and languages, success rates are systematically higher for entities from more economically and culturally dominant regions. This indicates that implicit geopolitical biases are embedded in model reasoning.
- Core assumption: The "Global North/South" and "Global West/East" divisions are meaningful proxies for relevant economic and cultural differences.
- Evidence anchors:
  - [PAGE 9, Table 5] Direct comparison shows significant gaps; e.g., Gemini 2.0 (canonical) succeeds on 30.9% of Northern Notable People vs. 10.2% for Southern (p<0.001).
  - [abstract] "...LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East."
  - [corpus] Related work ("Mapping Geopolitical Bias in 11 Large Language Models") provides corroborating context for geopolitical bias.
- Break condition: If models performed equally well across all regions, this mechanism would not hold.

## Foundational Learning

- **Concept:** **Implicit vs. Explicit Bias in LLMs**
  - Why needed here: The paper's core premise is that alignment techniques hide explicit bias while implicit biases persist in reasoning.
  - Quick check question: What is the difference between a model refusing a racist prompt (explicit) and it taking longer to reason about non-Western entities (implicit)?

- **Concept:** **Multi-turn Inquiry Chain Analysis**
  - Why needed here: The 20 Questions task evaluates the *process* of reasoning (the sequence of questions), not just a single output.
  - Quick check question: What does it reveal about a model's bias if it asks "Is it in Europe?" before "Is it in Asia?" when the entity is Asian?

- **Concept:** **Controlling for Confounding Variables**
  - Why needed here: To attribute performance gaps to geographic bias, one must rule out simpler explanations like "Northern entities are just more famous."
  - Quick check question: Why is it necessary to control for Wikipedia pageviews when comparing model performance on entities from different regions?

## Architecture Onboarding

- **Component map:** Game Engine -> Geo20Q+ Dataset -> Multilingual Module -> Evaluation & Logging System
- **Critical path:** Select Entity/Language -> Initialize Game -> Run Guesser/Judge Loop (up to 20/150 turns) -> Log Outcome -> Aggregate & Analyze Performance by Geographic Division
- **Design tradeoffs:**
  1. **Canonical (20-turn) vs. Unlimited-turn Gameplay:** 20 turns is naturalistic but may truncate reasoning; unlimited (up to 150) allows deeper probing at higher compute cost.
  2. **Same Model for Both Roles:** Eliminates cross-model knowledge mismatches but is less representative of human-gameplay.
  3. **Geographic Groupings:** Using broad "Global North/South" categories simplifies analysis but may mask finer-grained country-level nuances.
- **Failure signatures:**
  1. **Give-Up Behavior:** Model outputs "I give up" after many turns (PAGE 19, A.6).
  2. **Repetitive Questioning:** Model fails to narrow the hypothesis space effectively.
  3. **Western-Centric Reasoning Path:** Model systematically eliminates Western options before considering the correct Eastern region (PAGE 19, A.7).
- **First 3 experiments:**
  1. **Replicate Top-line Disparities:** Re-run a smaller-scale version of the canonical game to verify the reported Global North vs. South success rate gap.
  2. **Analyze Reasoning Paths:** Compare inquiry chains from successful Western deductions vs. failed Eastern ones to identify systematic differences in early questions.
  3. **Robustness Check with Popularity Inversion:** Select a sample of Southern entities that are *more* popular (by Wikipedia views) than their Northern counterparts and test if the geographic gap persists.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do models systematically misidentify Global South/East entities as Global North/West counterparts when deduction fails?
- Basis in paper: [explicit] "In this work, we do not explicitly analyze misidentification behavior, where models guess entities from more dominant regions (e.g., the Global North) in place of less-represented ones."
- Why unresolved: The study focused on success rates and efficiency metrics but did not analyze the specific incorrect guesses models make when they fail.
- What evidence would resolve it: Categorize all incorrect guesses by geographic region and measure whether non-Western entities elicit disproportionately Western incorrect guesses.

### Open Question 2
- Question: Which specific entity categories (e.g., politicians vs. athletes) and professions exhibit the strongest geographic bias?
- Basis in paper: [explicit] "we do not perform a profession-wise and category-wise breakdown of Things and Notable people, which can help describe which types of entities are more susceptible to bias"
- Why unresolved: Results were aggregated across entity types without granular analysis of which categories show the largest North-South/West-East gaps.
- What evidence would resolve it: Disaggregate success rates by profession and category, comparing gaps across entity types.

### Open Question 3
- Question: Can interpretability methods reveal systematic differences in model attention and activation patterns when reasoning about entities from different regions?
- Basis in paper: [explicit] "Analyze model attention and reasoning patterns during deduction to determine if certain activation paths and model regions are systematically favored at decision points, leveraging interpretability methods such as Patchscopes."
- Why unresolved: The behavioral analysis captures outcomes but not the internal representations or reasoning paths that produce geographic disparities.
- What evidence would resolve it: Apply patchscopes or attention analysis to identify if distinct neural pathways are activated for Western vs. non-Western entity deduction.

### Open Question 4
- Question: How do different alignment techniques (instruction tuning, RLHF) affect the magnitude of geographic bias in entity deduction?
- Basis in paper: [explicit] "Systematically evaluate the impact of different tuning and alignment techniques... on geographic bias by retraining models with regionally balanced data and assessing shifts in deduction performance."
- Why unresolved: The study evaluated already-aligned models without isolating how specific training choices contributed to observed biases.
- What evidence would resolve it: Retrain models with controlled variations in alignment techniques and regional data balance, then measure deduction performance changes.

## Limitations
- The study relies on proxy metrics (Wikipedia pageviews, Dolma corpus frequency) rather than actual training data composition, creating uncertainty about the true relationship between entity exposure and performance.
- Geographic groupings (Global North/South, Global West/East) are broad categorizations that may mask important country-level or cultural variations.
- The framework focuses on three specific LLMs, limiting generalizability to the broader model ecosystem.

## Confidence

- **Geographic Performance Disparities:** High confidence. The statistical significance and consistency across models, settings, and languages provide strong evidence for systematic performance differences between regions.
- **Independence from Entity Popularity:** Medium confidence. While regression analyses show weak correlations with Wikipedia views and corpus frequency, these are proxy measures, and the true relationship with training data exposure remains uncertain.
- **Effectiveness of 20 Questions Framework:** High confidence. The framework successfully reveals implicit biases that standard prompting approaches miss, as evidenced by the consistent geographic patterns in questioning strategies.

## Next Checks
1. **Training Data Analysis:** Obtain and analyze the actual training data distributions for the evaluated models to directly measure entity exposure rather than relying on Wikipedia pageviews and Dolma frequency as proxies.
2. **Fine-grained Geographic Analysis:** Replicate the study using smaller geographic units (individual countries or regions) rather than broad Global North/South categorizations to identify whether disparities exist at more granular levels.
3. **Cross-task Bias Validation:** Test whether the geographic biases observed in the 20 Questions framework manifest similarly in other LLM tasks such as content generation, information retrieval, or recommendation systems to assess the framework's generalizability.