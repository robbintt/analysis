---
ver: rpa2
title: Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation
arxiv_id: '2506.12879'
source_url: https://arxiv.org/abs/2506.12879
tags:
- design
- support
- agent
- https
- designers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored voice-based metacognitive support agents for
  designers working with generative AI (GenAI) in mechanical design. Designers often
  struggle with intent formulation, problem exploration, and outcome evaluation when
  using GenAI tools.
---

# Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation

## Quick Facts
- arXiv ID: 2506.12879
- Source URL: https://arxiv.org/abs/2506.12879
- Reference count: 40
- Primary result: Voice-based metacognitive support agents significantly improved feasibility of designs generated with generative AI, with SocratAIs questions most effective for intent formulation

## Executive Summary
This study explores how voice-based metacognitive support agents can enhance human-AI co-creation in mechanical design workflows. Through a Wizard of Oz experiment with 20 mechanical engineers, three agent types were tested: SocratAIs (asking reflective questions), HephAIstus (prompting planning and sketching with suggestions), and Expert-Freeform (human expert support). The agents targeted three critical design phases: intent formulation, problem exploration, and outcome evaluation. Results showed that agent-supported users produced significantly more feasible designs (average score 3.5/5) compared to unsupported users (average score 1.0/5). SocratAIs' questions were particularly effective for intent formulation and problem exploration, while HephAIstus helped with software navigation and visualization.

## Method Summary
The study used a between-subjects Wizard of Oz design with 20 participants (5 per condition) who designed a ship engine mounting bracket using Autodesk Fusion 360's Generative Design extension. Participants were mechanical engineers with 2+ years CAD experience but no prior generative design experience. Wizards communicated via a web interface using Google TTS to provide agent support through Zoom audio. The study compared three agent types against a no-support control, measuring design feasibility through a 5-point rubric covering structural soundness, load case setup, mass optimization, fastener clearance, and part size. Impact was also measured by counting "observable new considerations" raised by users in response to agent interventions.

## Key Results
- Agent-supported users produced significantly more feasible designs (average score 3.5/5) compared to unsupported users (average score 1.0/5)
- SocratAIs' questions were particularly effective for intent formulation and problem exploration
- HephAIstus' suggestions helped users navigate the software and visualize design alternatives
- Designers appreciated agent support but highlighted trade-offs between cognitive engagement and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Socratic questioning improves intent formulation and problem exploration in GenAI design workflows
- Mechanism: Open-ended questions prompt users to mentally simulate real-world constraints and articulate assumptions they would otherwise skip, reducing the risk of underspecifying GenAI inputs
- Core assumption: Users possess sufficient domain knowledge to answer questions meaningfully; reflection alone can surface misconceptions
- Evidence anchors:
  - [abstract] "SocratAIs' questions were particularly effective for intent formulation and problem exploration"
  - [section 6.1.3] "questions were especially effective... when prompting users to mentally simulate the real-world aspects of the bracket... An agent question prompting them to reflect on the part's function from a real-world perspective helped the user update their mental model and load case"
  - [corpus] "Enhancing Critical Thinking in Generative AI Search with Metacognitive Prompts" discusses metacognitive engagement effects but does not test Socratic questioning in design specifically—support is indirect
- Break condition: When users have solidified incorrect assumptions, questioning alone fails (section 6.1.3D: questions "reinforced their assumptions, preventing them from correcting issues")

### Mechanism 2
- Claim: Externalization through sketching and structured planning reduces cognitive load and improves GenAI input specification accuracy
- Mechanism: Sketching free-body diagrams and using planning documents shifts working memory load to external representations, creating conversational anchors that reveal inconsistencies between intent and actual parameter settings
- Core assumption: Users will engage with external tools mid-task; the externalization format aligns with their mental model of the problem
- Evidence anchors:
  - [section 6.1.4] "all users followed the link and used the board to sketch out diagrams... In several cases, the sketched free-body diagram served as a conversational anchor"
  - [section 6.1.4] "H4 had first sketched an FBD with feasible load cases, they then incorrectly specified the load case in Fusion360. Later, the agent pointed out the inconsistency... which led the designer to correct the input specification"
  - [corpus] Corpus evidence on sketching in GenAI workflows exists ("TalkSketch," "Exploring Visual Prompts") but focuses on ideation, not specification accuracy—direct evidence absent
- Break condition: If users do not revisit external artifacts during the session, benefit is limited (section 6.1.4: "only two users revisited the document later in the session")

### Mechanism 3
- Claim: Timely, context-aware intervention improves support effectiveness, but timing and strategy must match user state
- Mechanism: Agents that monitor user verbalizations, screen actions, and task transitions can deliver support at moments when users are receptive (e.g., during subtask transitions, when hedging, or after errors surface in evaluation)
- Core assumption: Real-time context monitoring is feasible; wizards or automated agents can accurately infer user state from observable signals
- Evidence anchors:
  - [section 5.4.1] wizards were instructed to "pay special attention to moments in which designers transition between design sub-tasks, as well as when designers show hesitation or use hedging expressions"
  - [section 6.1.5] "experts also deliberately delayed messages when the user moved on to a different sub-task too quickly and waited to reintroduce the topic later at a more opportune moment"
  - [section 6.2.4] users reported valuing questions that came at the right time, but some found redundant questions distracting
  - [corpus] No direct corpus evidence on timing strategies in metacognitive design agents—this is a gap
- Break condition: Interventions delivered too frequently or at wrong moments cause over-reliance or frustration (section 6.1.3D: repeated questioning amplified cognitive offloading in one case)

## Foundational Learning

- Concept: **Metacognition and Self-Regulated Learning (SRL)**
  - Why needed here: The entire agent approach builds on SRL theory—users must plan, monitor, and evaluate their own thinking. Without this foundation, you cannot design effective prompting or sketching interventions
  - Quick check question: Can you explain the difference between cognitive strategies (solving the problem) and metacognitive strategies (monitoring how you're solving it)?

- Concept: **Cognitive Offloading in GenAI Workflows**
  - Why needed here: The core problem these agents address is that GenAI tools reduce user engagement, leading to underspecified inputs and poor evaluation. Understanding this tradeoff is essential to avoid building agents that increase dependency
  - Quick check question: Give an example where automation improved efficiency but degraded the operator's ability to detect errors

- Concept: **GenAI Design Workflow Constraints (Intent Formulation, Problem Exploration, Outcome Evaluation)**
  - Why needed here: The three-phase framing structures when each support strategy is most effective. Engineers designing agents must map interventions to phases
  - Quick check question: In a generative CAD workflow, why is specifying all constraints upfront different from traditional iterative modeling?

## Architecture Onboarding

- Component map: Observation Layer -> State Inference Module -> Strategy Selection -> Delivery Interface -> Logging & Impact Tracking
- Critical path:
  1. User begins task -> observation layer streams verbalizations and screen
  2. State inference detects phase transition or inconsistency -> triggers strategy selector
  3. Strategy selector chooses question/suggestion/sketch prompt based on phase and history
  4. Delivery interface renders response in appropriate modality
  5. Logging captures impact (did user verbalize new consideration or take corrective action?)
- Design tradeoffs:
  - Questions vs. Suggestions: Questions better for intent formulation (section 6.1.2: SocratAIs had 41% impact on loads vs. HephAIstus 18%); suggestions better for tool fluency
  - Voice vs. Visual: Voice reduces cognitive load for visual-spatial tasks, but screen annotations preferred for precise GUI guidance (section 6.2.2)
  - Proactive vs. On-Demand: Proactive support catches missed steps; on-demand reduces interruption. Users differ in preference by experience level (section 6.2.4)
  - Single vs. Blended Strategies: No agent was one-size-fits-all; combining questioning with sketching may outperform either alone
- Failure signatures:
  - Over-reliance: User defers to agent without reflection; seen when repeated questioning caused user to second-guess correct assumptions (section 6.1.3D)
  - Ignored Suggestions: Users with strong prior (incorrect) mental models may reject direct hints (section 6.1.4D: "participants decided not to follow the agent's suggestions")
  - Timing Mismatch: Support delivered during focused subtask causes distraction; support delayed too long allows errors to compound
  - Modality Clash: Voice impractical in shared workspaces; visual annotations missed if user not looking at highlighted region
- First 3 experiments:
  1. Ablation by Strategy: Run controlled study comparing pure questioning, pure suggestion, and blended agents on same task; measure outcome feasibility score and count of observable new considerations per message
  2. Timing Sensitivity Test: Vary intervention frequency (0.2, 0.4, 0.6 messages/minute) and trigger type (transition-based, hedging-based, error-based); measure user satisfaction and error correction rate
  3. Externalization Benefit: Compare conditions with/without sketching board and planning doc; measure rate of inconsistencies between sketched intent and specified parameters, plus revisitation frequency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would combining metacognitive support strategies (e.g., Socratic questioning plus sketching prompts) produce better design outcomes than any single strategy alone?
- Basis in paper: [explicit] The authors state: "This suggests that combining multiple strategies may ultimately prove more effective in practice, and future work should explore systems with blended approaches."
- Why unresolved: Each probe in the study implemented only one strategy; no combinations were tested.
- What evidence would resolve it: A controlled study comparing blended-strategy agents to single-strategy agents on design outcome quality and user satisfaction.

### Open Question 2
- Question: When should metacognitive support agents "ask" versus "tell" users to maximize reflection without causing over-reliance?
- Basis in paper: [explicit] The authors state: "A challenge will lie in determining when 'asking' versus 'telling' the user would be most appropriate."
- Why unresolved: The study showed questions helped intent formulation but sometimes reinforced incorrect assumptions; suggestions helped software operation but triggered less metacognitive reflection.
- What evidence would resolve it: A study that dynamically varies support modality based on detected user state (e.g., uncertainty level, task phase) and measures cognitive engagement.

### Open Question 3
- Question: Can automated NLP systems generate contextually appropriate Socratic questions for design tasks as effectively as human wizards?
- Basis in paper: [explicit] The authors note: "Future research should explore design task-specific question generation to prompt designers' self-reflection and critical thinking aligned to specific design domains."
- Why unresolved: All agent messages in this study were authored by human wizards; automation feasibility and quality remain untested.
- What evidence would resolve it: A comparison study where LLM-generated questions are evaluated against human-authored questions for relevance, timing, and impact on design process outcomes.

### Open Question 4
- Question: Do metacognitive support agents generalize to design domains beyond mechanical engineering (e.g., architecture, graphic design)?
- Basis in paper: [explicit] The authors state: "While this work explores metacognitive support agents for GenAI-assisted mechanical part creation, the findings and design considerations offer promising avenues for research in other AI-assisted workflows."
- Why unresolved: All participants were mechanical engineers; no other design domains were tested.
- What evidence would resolve it: Replication studies with designers from other disciplines using domain-specific GenAI tools, measuring similar outcome and process metrics.

## Limitations
- Small sample size (n=20) in controlled Wizard of Oz setting limits generalizability
- Human wizard variability in intervention timing and strategy introduces potential confounds
- Impact metrics depend on coder interpretation of "observable new considerations"
- Results may not transfer to other design domains or GenAI applications

## Confidence
- **High Confidence**: The core finding that agent-supported users produced more feasible designs than unsupported users (3.5 vs 1.0 on feasibility scale)
- **Medium Confidence**: The differential effectiveness of SocratAIs questions for intent formulation versus HephAIstus suggestions for tool navigation
- **Low Confidence**: Claims about optimal intervention timing and frequency, as these were guided by wizard discretion rather than systematic variation

## Next Checks
1. Conduct a larger-scale (n≥60) randomized controlled trial with automated agents following predefined intervention protocols to validate the feasibility score improvements and eliminate wizard variability

2. Perform a within-subjects study where participants experience all three agent types on different design tasks, enabling direct comparison of questioning versus suggestion strategies on the same participants

3. Implement a longitudinal study tracking the same users across multiple design sessions with and without agent support to assess whether metacognitive assistance builds lasting self-regulation skills or creates dependency