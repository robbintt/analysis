---
ver: rpa2
title: 'DynamicBench: Evaluating Real-Time Report Generation in Large Language Models'
arxiv_id: '2506.21343'
source_url: https://arxiv.org/abs/2506.21343
tags:
- report
- llms
- information
- generation
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DynamicBench introduces a benchmark for evaluating large language\
  \ models\u2019 ability to process real-time information through a dual-path retrieval\
  \ system combining web searches and local report databases. Unlike traditional static\
  \ benchmarks, it requires up-to-date domain-specific knowledge for accurate responses."
---

# DynamicBench: Evaluating Real-Time Report Generation in Large Language Models

## Quick Facts
- arXiv ID: 2506.21343
- Source URL: https://arxiv.org/abs/2506.21343
- Reference count: 3
- Key outcome: Outperforms GPT4o by 7.0% and 5.8% in document-free and document-assisted scenarios respectively

## Executive Summary
DynamicBench introduces a benchmark for evaluating large language models' ability to process real-time information through a dual-path retrieval system combining web searches and local report databases. Unlike traditional static benchmarks, it requires up-to-date domain-specific knowledge for accurate responses. The benchmark assesses models under two scenarios: with and without external document assistance, measuring independent knowledge storage and contextual enhancement capabilities. A robust report generation system was developed to handle dynamic information synthesis through systematic planning, search, and writing phases.

## Method Summary
The benchmark evaluates LLMs on real-time report generation across 4 domains using dual-path retrieval (web search + local database of 148,589 annual reports from 10,338 companies). The 4-phase report generation system includes Section Planning, Section Search (with iterative query refinement), Section Writing, and Merge Sections. Evaluation uses 5 metrics on 1-10 scales: Accuracy, Completeness, Readability, Applicability, and Length. Two scenarios are tested: document-free (internal knowledge) and document-assisted (external documents provided).

## Key Results
- System achieves accuracy of 74.8%, completeness of 73.7%, readability of 78.0%, applicability of 71.7%, and length of 74.4%
- Outperforms GPT4o by 7.0% and 5.8% in document-free and document-assisted scenarios respectively
- Surpasses baseline models including Claude3.7, DeepSeek-v3, and GPT4o across multiple metrics
- Capability-enhanced models (LongWriter, Suri) showed performance decline when given external documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-path retrieval provides both domain-specific depth and temporal currency
- Mechanism: Local RAG retrieves structured reports via cosine similarity; web search supplies real-time data
- Core assumption: Information needs decompose into stable domain knowledge and time-sensitive updates
- Evidence anchors: Abstract confirms dual-path pipeline; FinS-Pilot validates real-time data necessity in financial domains

### Mechanism 2
- Claim: Iterative self-assessment prevents premature drafting with insufficient evidence
- Mechanism: Model generates K queries, retrieves results, self-assesses sufficiency, iterates until adequate
- Core assumption: LLMs can reliably judge information sufficiency for writing tasks
- Evidence anchors: Section 3.2 describes self-assessment loop; no corpus validation exists

### Mechanism 3
- Claim: Four-phase decomposition improves report coherence
- Mechanism: Planning → Search → Write → Merge prevents coherence drift in single-pass generation
- Core assumption: Section-level context windows suffice for coherent writing
- Evidence anchors: Systematic approach guarantees thorough creation; LongWriter/Suri performance decline supports structured decomposition

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) with dual encoders**
  - Why needed here: Prerequisite to debugging retrieval failures and tuning embedding quality
  - Quick check question: Given a query embedding Eq and context embedding Ec, how would you compute their similarity score, and what threshold indicates a "good" match?

- Concept: **Iterative query refinement**
  - Why needed here: The self-assessment loop requires understanding when retrieval results are insufficient
  - Quick check question: If a section on "Apple revenue 2021" returns only product launches but no financial data, what type of follow-up query would close this gap?

- Concept: **Document-assisted vs. document-free evaluation**
  - Why needed here: Evaluates two distinct capabilities with different failure modes
  - Quick check question: Why might a model fine-tuned for long generation perform worse when given external documents?

## Architecture Onboarding

- Component map: Query Encoder → Context Encoder → Cosine Similarity → Top-K Retrieval → Dual-Path Router (parallel: Local DB + Web Search) → Self-Assessment Module → Section Planner → Section Writer → Section Merger

- Critical path: Query → Dual Retrieval → Self-Assessment → Section Planning → Per-Section Retrieval → Section Writing → Merge. Self-assessment checkpoint is primary latency variable.

- Design tradeoffs: Local DB provides structured data but lacks currency; web search provides currency but variable quality; iterative retrieval improves completeness but increases latency.

- Failure signatures: High accuracy, low completeness (early termination); low accuracy, high readability (irrelevant sources); capability-enhanced models declining with documents.

- First 3 experiments:
  1. Ablate web search only: Measure contribution of real-time data to accuracy scores
  2. Vary self-assessment threshold: Test early termination vs. aggressive refinement for latency-quality tradeoff
  3. Cross-domain robustness: Compare performance across four categories to identify domain-specific gaps

## Open Questions the Paper Calls Out

- Why do capability-enhanced models (Suri, LongWriter) exhibit performance declines when provided with external documents while general LLMs improve?
- How can DynamicBench mitigate evaluation biases from discrepancies or inaccuracies in external web search and local database sources?
- How can the benchmark be extended to assess deeper domain-specific expertise required for highly specialized fields?
- What is the optimal weighting or fusion strategy between local report databases and web search results?

## Limitations
- Specific LLM model and embedding models used are not disclosed
- Evaluation methodology unclear (human vs. automated scoring)
- Prompt templates for all system phases are missing
- Iterative retrieval parameters (K value, stopping criteria) undefined

## Confidence
- High confidence: Dual-path retrieval mechanism validity (supported by OkraLong and FinS-Pilot evidence)
- Medium confidence: Performance improvement claims (lacks open-source evaluation code)
- Low confidence: Self-assessment sufficiency mechanism (no corpus validation)

## Next Checks
1. Ablation study of web search contribution: Run benchmark with local database only versus dual-path retrieval
2. Self-assessment calibration testing: Vary sufficiency threshold and iteration limits to map latency-quality tradeoff
3. Cross-domain performance analysis: Compare benchmark performance across Tech & Science versus International & Politics categories