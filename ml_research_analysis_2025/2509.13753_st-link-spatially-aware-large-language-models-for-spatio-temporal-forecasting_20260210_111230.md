---
ver: rpa2
title: 'ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting'
arxiv_id: '2509.13753'
source_url: https://arxiv.org/abs/2509.13753
tags:
- forecasting
- spatio-temporal
- spatial
- traffic
- st-link
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ST-LINK, a novel framework that enhances
  large language models (LLMs) for spatio-temporal forecasting by addressing their
  spatial modeling limitations. The key innovation is SE-Attention, which extends
  rotary position embeddings to incorporate spatial relationships as rotational transformations
  within attention, preserving the LLM's sequential processing while enabling spatial
  learning.
---

# ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting

## Quick Facts
- **arXiv ID**: 2509.13753
- **Source URL**: https://arxiv.org/abs/2509.13753
- **Reference count**: 40
- **Primary result**: Novel framework extends LLMs for spatio-temporal forecasting by integrating spatial awareness through SE-Attention and historical pattern retrieval via MRFFN, achieving lower error rates than conventional deep learning approaches.

## Executive Summary
ST-LINK addresses the challenge of adapting large language models for spatio-temporal forecasting tasks, which require capturing both spatial dependencies and temporal dynamics. The framework introduces SE-Attention, which extends rotary position embeddings to incorporate spatial relationships as rotational transformations, and MRFFN, a memory retrieval mechanism that dynamically accesses historical patterns. Experiments on traffic flow and demand forecasting datasets demonstrate that ST-LINK outperforms conventional deep learning and LLM approaches, achieving lower error rates across multiple metrics and horizons. The framework effectively captures both regular traffic patterns and abrupt changes, validating the potential of LLMs for spatio-temporal forecasting tasks.

## Method Summary
ST-LINK builds on GPT-2 and integrates two key innovations: SE-Attention and MRFFN. SE-Attention extends rotary position embeddings by applying separate temporal and spatial rotations to query/key vectors, using learnable node embeddings to scale base frequencies for spatial relationships. MRFFN stores learnable key-value memory pairs and retrieves top-k relevant patterns based on similarity scoring, with retrieved embeddings routed through a mixture-of-experts layer. The framework employs Partially Frozen Attention, freezing lower LLM layers while training upper layers to balance pre-trained knowledge preservation with domain adaptation. Training uses batch size 64 with learning rates of 0.0001 for demand datasets and 0.00025 for speed datasets.

## Key Results
- ST-LINK achieves lower MAE, RMSE, and MAPE than conventional deep learning and LLM approaches across METR-LA, PEMS-BAY, NYCTaxi, and Citi Bike datasets
- Framework demonstrates superior performance in capturing both regular traffic patterns and abrupt changes across multiple forecasting horizons
- Ablation studies confirm the contributions of both SE-Attention and MRFFN mechanisms to overall performance gains

## Why This Works (Mechanism)

### Mechanism 1
Extending rotary position embeddings (RoPE) to encode spatial relationships enables LLMs to capture spatial dependencies without disrupting their sequential processing structure. SE-Attention applies separate temporal rotations (RoPE_T) and spatial rotations (RoPE_S) to query/key vectors, using learnable node embeddings to scale base frequencies for spatial rotations. The outputs are concatenated and projected back to original dimension.

### Mechanism 2
Memory retrieval integrated with MoE routing improves long-term forecasting stability by dynamically accessing relevant historical patterns. MRFFN stores learnable key-value memory pairs and retrieves top-k memory slots via similarity scoring. The retrieved embedding is concatenated with input and attention summary to route through MoE experts, with memory keys updated via EMA during training.

### Mechanism 3
Freezing lower LLM layers while training upper layers balances pre-trained knowledge preservation with domain adaptation. Partially Frozen Attention (PFA) freezes layers 1 to L-U entirely while keeping only LayerNorm trainable, allowing upper U layers to specialize for spatio-temporal forecasting without disrupting foundational representations.

## Foundational Learning

- **Rotary Position Embeddings (RoPE)**: Core mechanism being extended; understanding how rotation matrices encode relative position is prerequisite for SE-Attention. Quick check: Can you derive why (R_m q)^T (R_n k) = q^T R_{m-n} k enables relative position encoding?

- **Mixture-of-Experts (MoE)**: MRFFN uses MoE for dynamic expert routing conditioned on retrieved memory patterns. Quick check: How does top-k expert selection trade off capacity against computational cost?

- **Reversible Instance Normalization (RevIN)**: Applied before SE-Attention to handle node-specific scale variations in spatio-temporal data. Quick check: Why must denormalization occur after attention rather than training directly on normalized outputs?

## Architecture Onboarding

- **Component map**: Input (B × T_in × N × F_in) → Patch Embedding + Temporal/Node Embeddings → H^(0) → LLM Block (×L): RevIN → SE-Attention → MRFFN → LayerNorm → Output: Prediction Head

- **Critical path**: 1) Learnable node embeddings N(node_i) determine spatial rotation frequencies per node. 2) Memory keys k_m are updated via EMA (α controls adaptation speed); values v_m updated via backprop. 3) Upper U layers trainable; lower L-U layers frozen except LayerNorm.

- **Design tradeoffs**: Memory size vs. retrieval cost (more slots capture more patterns but increase search latency); Frozen vs. trainable layers (more frozen reduces overfitting risk but limits adaptation); Spatial RoPE dimension allocation (paper uses d/2 for spatial, d/2 for temporal; alternatives unexplored).

- **Failure signatures**: Over-smoothing (spatial rotations converging across nodes); Memory drift (EMA updates too aggressive or too slow); Scale mismatch (RevIN cross-node contamination).

- **First 3 experiments**: 1) Ablate SE-Attention on METR-LA to isolate spatial encoding contribution. 2) Sweep memory size (16, 64, 256 slots) and α (0.05, 0.1, 0.2) on NYCTaxi to characterize memory dynamics. 3) Vary frozen layer ratio (0%, 50%, 75% frozen) on NYCTaxi to validate PFA assumptions.

## Open Questions the Paper Calls Out

### Open Question 1
How do SE-Attention and MRFFN performance and computational overhead scale when implemented within significantly larger foundational models compared to the relatively small GPT-2 backbone used in this study? The authors validate ST-LINK on GPT-2 but note the need to examine scalability to larger models.

### Open Question 2
Can the MRFFN memory update mechanism be refined to prevent catastrophic forgetting during rapid domain shifts or continuous learning scenarios? While rapid adaptation is advantageous, the authors identify the stability-plasticity balance as requiring further research.

### Open Question 3
How effectively does the spatially-extended RoPE generalize to spatio-temporal domains with non-grid or highly irregular spatial structures distinct from road networks? Current evaluation is limited to traffic/demand datasets, but future research will focus on generalizing to wider spatio-temporal domains.

## Limitations
- SE-Attention mechanism assumes spatial relationships can be meaningfully captured through frequency-based rotations, which may not hold for arbitrary graph structures with complex hierarchies
- Limited ablation studies make it difficult to attribute performance gains specifically to individual mechanisms
- Memory dynamics in MRFFN are underspecified, making it unclear how memory size and update parameters affect long-term stability

## Confidence
- **High confidence**: Overall framework effectiveness (consistent improvements across multiple datasets and metrics)
- **Medium confidence**: SE-Attention mechanism (supported by mathematical formulation but limited ablation evidence)
- **Low confidence**: MRFFN contribution (retrieval augmentation is novel but memory dynamics are underspecified)

## Next Checks
1. **Ablation isolation**: Remove SE-Attention and run on METR-LA to quantify spatial encoding contribution versus baseline GPT-2
2. **Memory sensitivity**: Sweep memory slot count (16→64→256) and EMA momentum (0.05→0.2) on NYCTaxi to characterize retrieval stability
3. **Topology stress test**: Apply ST-LINK to non-road graph data (social networks, citation graphs) to test spatial encoding limits beyond transportation scenarios