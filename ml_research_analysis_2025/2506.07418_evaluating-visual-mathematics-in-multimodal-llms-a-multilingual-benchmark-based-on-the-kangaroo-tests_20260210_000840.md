---
ver: rpa2
title: 'Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark
  Based on the Kangaroo Tests'
arxiv_id: '2506.07418'
source_url: https://arxiv.org/abs/2506.07418
tags:
- visual
- reasoning
- problems
- mathematical
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multimodal large language models (MLLMs) on
  visually presented mathematics problems from the Kangaroo competition across four
  languages. A benchmark dataset was created with image-based and text-only questions
  spanning geometry, visual algebra, logic, patterns, and combinatorics.
---

# Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests

## Quick Facts
- **arXiv ID:** 2506.07418
- **Source URL:** https://arxiv.org/abs/2506.07418
- **Reference count:** 40
- **Primary result:** Multimodal LLMs show moderate accuracy on visual math problems from the Kangaroo competition, with models performing better on text-only questions than image-based ones, indicating underutilization of visual information.

## Executive Summary
This study evaluates multimodal large language models on visually presented mathematics problems from the Kangaroo competition across four languages. A benchmark dataset was created with image-based and text-only questions spanning geometry, visual algebra, logic, patterns, and combinatorics. Eight models were tested, including GPT-4o, Gemini 2.0 Flash, Qwen-VL, and Llama variants. Results show overall moderate precision, with no single model excelling across all topics. Models performed better on text-only questions than image-based ones, indicating underutilization of visual information. Gemini 2.0 Flash achieved the highest accuracy on image-based tasks (45.4%), followed by Qwen-VL 2.5 72B (43.5%) and GPT-4o (40.2%). None approached human-level performance, especially on complex visual reasoning tasks. Models with more parameters generally performed better, but even the best struggled with advanced geometry and combinatorial reasoning. Gemini and GPT-4o demonstrated more structured reasoning, while Pixtral and Llama often defaulted to heuristics or random choices when solutions did not align with answer options.

## Method Summary
The study evaluated eight multimodal LLMs on Kangaroo Mathematics Competition problems across English, French, Spanish, and Catalan. The benchmark included image-based and text-only multiple-choice questions spanning geometry, visual algebra, logic, patterns, and combinatorics. Models were prompted to show reasoning before answering. Larger models used cloud APIs while smaller models ran locally. Performance was measured by accuracy on image-based versus text-only questions, analyzed by language, difficulty, and topic. The dataset comprised questions from 2014-2024 with accompanying images where applicable.

## Key Results
- Gemini 2.0 Flash achieved highest accuracy on image-based tasks (45.4%), followed by Qwen-VL 2.5 72B (43.5%) and GPT-4o (40.2%)
- Models performed better on text-only questions than image-based ones, indicating systematic underutilization of visual information
- No model approached human-level performance, especially on complex visual reasoning tasks involving advanced geometry and combinatorial reasoning
- Larger parameter models generally performed better, but scale improvements were modest (10-15% gain for 10× parameter increase)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale positively correlates with multimodal mathematical reasoning performance, particularly for visual integration tasks.
- Mechanism: Larger parameter counts provide greater capacity for cross-modal alignment between visual features and mathematical reasoning patterns. The visual encoder and language model must jointly learn to map geometric/spatial features to symbolic operations.
- Core assumption: Performance gains from scale reflect improved cross-modal representations rather than mere memorization of training patterns.
- Evidence anchors:
  - [abstract] "Models with more parameters generally performed better, but even the best struggled with advanced geometry and combinatorics reasoning."
  - [section] Table 1 shows Qwen-VL 72B achieving 43.5% on image-based tasks vs Qwen-VL 7B at 32.5%; Gemini 2.0 Flash at 45.4% vs Lite at 39.4%.
  - [corpus] Related work M3Kang similarly evaluates multilingual multimodal math reasoning, confirming the domain relevance, though corpus lacks direct replication of this scale-performance finding.
- Break condition: If smaller models with specialized vision-language alignment layers matched or exceeded larger models, the scale mechanism would be weakened.

### Mechanism 2
- Claim: MLLMs systematically underutilize visual information when solving mathematical problems, treating images as optional context rather than essential reasoning inputs.
- Mechanism: Vision encoders extract features, but the cross-modal attention mechanism fails to weight diagrammatic information appropriately during multi-step reasoning. Models may treat visual tokens as supplementary rather than structuring the solution pathway.
- Core assumption: The gap between text-only and image-based performance reflects a failure in visual-to-reasoning integration, not merely harder image-based problems.
- Evidence anchors:
  - [abstract] "while most models see improved accuracy with questions that do not have images, the gain is often limited; performance for some remains nearly unchanged without visual input"
  - [section] Page 13 states: "their performance notably declines in tasks requiring precise measurements derived from scales or grids explicitly represented within the figures"
  - [corpus] MathVerse (referenced in paper) similarly found models fail to "truly see" diagrams, supporting this mechanism.
- Break condition: If removing visual inputs caused consistent performance collapse (rather than modest declines), it would indicate models *do* rely on visual information.

### Mechanism 3
- Claim: Structured reasoning capability—not parameter count alone—distinguishes models that genuinely reason from those that default to pattern-matching heuristics.
- Mechanism: Models with stronger reasoning scaffolding maintain coherent solution paths even when intermediate answers don't match provided options; weaker models collapse to random selection or "no answer" responses when their heuristic fails.
- Core assumption: The ability to continue reasoning despite option-mismatch reflects genuine multi-step reasoning rather than surface pattern completion.
- Evidence anchors:
  - [abstract] "Gemini and GPT-4o stand out for their structured reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less consistent reasoning, often defaulting to heuristics or randomness"
  - [section] Page 14: "when the derived solution does not correspond to any of the provided options, they default to 'No answer'" for Pixtral/Llama, while "Gemini and GPT-4o demonstrated more coherent and structured reasoning"
  - [corpus] "Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems" explores reasoning limitations but doesn't directly address this heuristic-vs-reasoning distinction (weak corpus support).
- Break condition: If "heuristic-defaulting" models achieved comparable accuracy on novel problems (not in training distribution), the reasoning-quality distinction would be undermined.

## Foundational Learning

- Concept: **Cross-modal attention mechanisms**
  - Why needed here: The paper's central finding—that models underutilize visual information—requires understanding how vision and language streams are combined. Without this, you cannot diagnose whether poor image-based performance stems from vision encoding failure or integration failure.
  - Quick check question: Given a multimodal transformer, can you sketch where visual tokens would enter the attention computation and how they might be under-weighted during mathematical reasoning steps?

- Concept: **Parameter scaling laws for multimodal models**
  - Why needed here: The paper documents scale-performance relationships (Qwen 7B→72B, Gemini Lite→Flash). Understanding why scale helps—and where it plateaus—is essential for interpreting these results.
  - Quick check question: Why might a 10× parameter increase yield only a ~10-15% accuracy gain on visual math tasks? What factors limit scaling benefits?

- Concept: **Reasoning vs. retrieval in LLMs**
  - Why needed here: The paper explicitly tests whether models "reason or simply recite" (p. 14). Understanding chain-of-thought mechanisms and memorization pitfalls is critical for interpreting this distinction.
  - Quick check question: If a model solves a novel Kangaroo problem correctly, what evidence would distinguish genuine reasoning from training-data recitation?

## Architecture Onboarding

- Component map:
  Vision encoder (ViT-based) -> Vision-language projector -> Multimodal LLM backbone with cross-modal attention -> Output head

- Critical path:
  1. Image preprocessing → Vision encoder → Visual token sequence
  2. Visual tokens + text tokens → Projector → Unified embedding space
  3. Unified sequence → LLM with cross-modal attention → Reasoning trace
  4. Reasoning trace → Answer selection → Final response

- Design tradeoffs:
  - **Specialized math vision encoders** vs. **general-purpose vision models**: Math-LLaVA and G-LLaVA adapt architectures for geometry, but the paper shows even these lag behind dedicated reasoners on complex visual tasks (p. 4).
  - **Closed-source APIs** vs. **open-source local deployment**: Closed models (Gemini, GPT-4o) achieve higher accuracy but offer no architectural transparency; open-source (Llama, Qwen) allow inspection but underperform.
  - **Multilingual training balance**: Models perform inconsistently across English/French/Spanish/Catalan, suggesting uneven multilingual training (p. 9).

- Failure signatures:
  - **Visual information neglect**: Accuracy similar with/without images → vision encoder output ignored during reasoning
  - **Heuristic collapse**: Returning "no answer" when derived solution doesn't match options → reasoning path abandoned prematurely
  - **Geometric miscomputation**: Correct text reasoning but wrong final answer on figures → vision encoder misreads angles/dimensions
  - **3D→2D projection failure**: "All elements incorrectly perceived as existing in a single two-dimensional plane" (p. 13)

- First 3 experiments:
  1. **Ablate visual input**: Run same Kangaroo problems in text-only mode vs. full image mode. Measure accuracy delta per model. Expect Gemini/Qwen to show larger gaps than Llama/Pixtral (confirming underutilization pattern).
  2. **Cross-lingual consistency check**: Present identical problems in English vs. Catalan. Identify where accuracy diverges by >10% to flag language-specific reasoning failures.
  3. **Option-mismatch stress test**: Modify problems so correct reasoning yields answers not in the option set. Count "no answer" vs. random selection responses per model to quantify reasoning-vs-heuristic behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MLLM architectures be improved to ensure visual information is actively utilized in reasoning rather than ignored?
- Basis in paper: [explicit] The authors state that performance for some models remains nearly unchanged without visual input, "indicating underutilization of diagrammatic information."
- Why unresolved: Current vision encoders or projection layers may fail to translate geometric features into reasoning tokens effectively, causing the language model to rely primarily on text.
- What evidence would resolve it: Demonstrating that models achieve significantly higher accuracy on image-based versions of problems compared to text-only versions, rather than the current trend of text-only superiority.

### Open Question 2
- Question: What specific training interventions are required to enable MLLMs to correctly perceive depth and overlapping elements in geometric diagrams?
- Basis in paper: [inferred] The Discussion notes that models consistently misunderstand stacking or overlapping elements, perceiving them as existing in a single 2D plane.
- Why unresolved: Standard image-text pre-training likely lacks sufficient density of complex spatial relationship data to learn these specific perceptual invariants.
- What evidence would resolve it: Evaluation results showing a marked reduction in errors related to occlusion and 3D interpretation on the "Geometry and Figures" benchmark category.

### Open Question 3
- Question: How can smaller, open-source models (e.g., Llama, Pixtral) be stabilized to prevent defaulting to random choices when logical deduction fails?
- Basis in paper: [explicit] The authors observe that Pixtral and Llama "often default to heuristics or randomness when unable to align their outputs with the given answer options."
- Why unresolved: These models lack the robust uncertainty calibration or "verbalized reasoning" capabilities found in top-tier models like GPT-4o.
- What evidence would resolve it: A decrease in random selection behaviors and an increase in explicit uncertainty declaration or structured step-by-step attempts in the model outputs.

## Limitations

- Limited visibility into exact prompt engineering details, which may significantly influence reasoning outputs and consistency
- No architectural transparency for closed-source models (GPT-4o, Gemini), preventing direct analysis of cross-modal attention mechanisms
- Benchmark performance evaluated only at inference time without fine-tuning, potentially underestimating model capabilities
- Small sample size per language-topic combination may mask systematic failures or biases
- No error analysis of why specific visual elements are misinterpreted (e.g., measurement misreading vs. structural misunderstanding)

## Confidence

- **High confidence:** Scale-performance correlation (Qwen 7B→72B shows consistent improvement; larger models achieve higher accuracy)
- **Medium confidence:** Visual information underutilization (models show improved performance without images, but causality unclear)
- **Medium confidence:** Reasoning-vs-heuristic distinction (systematic "no answer" responses for option mismatches suggests heuristic collapse)
- **Low confidence:** Multilingual consistency claims (limited cross-linguistic performance data; language-specific training effects unmeasured)

## Next Checks

1. **Cross-lingual transfer validation:** Present identical visual math problems across all four languages to each model, measuring performance variance to isolate language-specific reasoning failures
2. **Vision ablation study:** Systematically remove visual inputs from image-based questions while preserving text descriptions, measuring whether performance changes reflect genuine visual reasoning or superficial pattern matching
3. **Novel problem stress test:** Generate mathematically valid but never-before-seen Kangaroo-style problems where correct solutions don't match any provided options, quantifying heuristic vs. reasoning behavior across models