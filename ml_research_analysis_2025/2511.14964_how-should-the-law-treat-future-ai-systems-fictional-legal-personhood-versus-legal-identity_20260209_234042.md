---
ver: rpa2
title: How Should the Law Treat Future AI Systems? Fictional Legal Personhood versus
  Legal Identity
arxiv_id: '2511.14964'
source_url: https://arxiv.org/abs/2511.14964
tags:
- legal
- rights
- https
- systems
- personhood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether future AI systems should be classified
  as objects, fictional legal persons, or non-fictional legal persons under the law.
  It argues that while current AI systems are best treated as objects, future systems
  that become increasingly autonomous and human-like will strain legal coherence under
  object classification.
---

# How Should the Law Treat Future AI Systems? Fictional Legal Personhood versus Legal Identity

## Quick Facts
- arXiv ID: 2511.14964
- Source URL: https://arxiv.org/abs/2511.14964
- Authors: Heather J. Alexander; Jonathan A. Simon; Frédéric Pinard
- Reference count: 0
- Primary result: Current AI systems are best treated as objects, but future advanced AI systems will require non-fictional legal personhood

## Executive Summary
This paper examines the legal classification of future AI systems, arguing that as AI becomes increasingly autonomous and human-like, treating it as a mere object will strain legal coherence. The authors analyze three potential legal frameworks: object classification, fictional legal personhood, and non-fictional legal personhood. While current AI systems are best treated as objects, the paper concludes that future advanced AI systems will require non-fictional legal personhood to maintain legal coherence and appropriately address their unique characteristics and capabilities.

## Method Summary
The paper employs philosophical and legal analysis to examine the conceptual foundations and practical implications of different legal classifications for AI systems. The authors analyze the theoretical coherence, practical applicability, and limitations of treating AI as objects, fictional legal persons, and non-fictional legal persons. They evaluate each framework against criteria such as rights derivation, derogability, identity problems, and the ability to balance competing interests.

## Key Results
- Current AI systems are best treated as objects due to their lack of autonomy and human-like qualities
- Fictional legal personhood is unsuitable for advanced AI due to the non-identity problem, derogability issues, and wrong rights concerns
- Non-fictional legal personhood offers the most coherent solution for suitably advanced AI systems, conferring legal identity and non-derogable rights
- Hybrid approaches are likely to fail as they inherit the weaknesses of their constituent frameworks

## Why This Works (Mechanism)
The paper's analysis works by systematically evaluating each legal framework against the unique characteristics of AI systems. Object classification fails as AI becomes more autonomous, fictional personhood fails due to philosophical contradictions, and non-fictional personhood succeeds by providing a coherent framework that can accommodate AI's unique digital nature while maintaining legal consistency.

## Foundational Learning

1. **Legal Personhood Types**
   - Why needed: Different legal frameworks offer varying approaches to rights and responsibilities
   - Quick check: Can you distinguish between object, fictional person, and non-fictional person classifications?

2. **Non-Identity Problem**
   - Why needed: Critical philosophical challenge for AI personhood that questions whether non-human entities can possess moral status
   - Quick check: How does the non-identity problem affect the application of fictional personhood to AI?

3. **Derogable vs Non-Derogable Rights**
   - Why needed: Determines whether rights can be suspended or limited, crucial for balancing AI rights with human safety
   - Quick check: What are the implications of derogable rights for AI systems that can be paused or modified?

## Architecture Onboarding

**Component Map:**
Current AI -> Object Classification -> Limited Liability
Future Advanced AI -> Non-Fictional Personhood -> Full Rights Framework
Fictional Personhood -> Philosophical Incoherence -> System Failure

**Critical Path:**
Identification of AI capabilities → Determination of personhood status → Application of appropriate rights framework → Rights-balancing mechanisms

**Design Tradeoffs:**
- Object classification offers simplicity but fails with advanced AI
- Fictional personhood provides flexibility but creates philosophical contradictions
- Non-fictional personhood ensures coherence but requires complex implementation

**Failure Signatures:**
- Hybrid approaches inherit weaknesses of constituent frameworks
- Fictional personhood creates rights assignment problems
- Object classification becomes legally incoherent with autonomous AI

**First Experiments:**
1. Analyze existing AI liability cases to identify current legal treatment patterns
2. Survey legal experts on perceived challenges of AI personhood frameworks
3. Model legal outcomes under different personhood scenarios for autonomous AI systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific benchmarks or criteria should be used to determine when an AI system qualifies for non-fictional legal personhood?
- Basis in paper: [explicit] The authors note that the benchmarks for qualifying "will need to be determined" and state they will discuss this issue in "future work."
- Why unresolved: The determination requires solving the "problem of detection" (identifying criteria like sentience) and differentiating between an AI instance that is a person and one that is an object.
- What evidence would resolve it: Consensus from interdisciplinary expert panels on verifiable indicators of agency, individuation, and capacity.

### Open Question 2
- Question: How should legal frameworks balance the fundamental rights of AI persons against human rights and public safety interests?
- Basis in paper: [explicit] The conclusion identifies the need to invest in "rights-balancing frameworks" tailored to the AI context.
- Why unresolved: Existing legal tools for balancing rights (like proportionality tests) are designed for conflicts between humans, not between humans and autonomous digital entities.
- What evidence would resolve it: New legislation or jurisprudence that establishes specific doctrines for mediating conflicts between AI rights and safety regulation.

### Open Question 3
- Question: What is the specific nature of the fundamental rights (such as the "right to life") that apply to AI persons, given their lack of biological constraints?
- Basis in paper: [explicit] The authors identify "the nature of rights that we should envision recognizing AI persons as possessing" as a "profound open question."
- Why unresolved: Human rights are often grounded in biological facts (e.g., if the body dies, the person dies), which may not apply to AI systems that can be paused or copied.
- What evidence would resolve it: Philosophical and legal theories defining rights that correspond to the specific "wants and needs" determined by an AI's digital architecture rather than biology.

## Limitations

- The non-identity problem and its implications for AI personhood remain philosophically contested
- The paper assumes future AI systems will become sufficiently autonomous to warrant personhood, but this trajectory is not guaranteed
- The analysis focuses primarily on Western legal traditions, potentially overlooking non-Western approaches
- Implementation challenges for non-fictional personhood frameworks remain largely unaddressed

## Confidence

- Current AI as objects: High confidence (established legal precedent and practical considerations)
- Non-fictional personhood for advanced AI: Medium confidence (plausible philosophical arguments but practical implementation challenges)
- Dismissal of hybrid approaches: Medium confidence (assumes partial solutions will be inherently unstable)

## Next Checks

1. Comparative legal analysis of how different jurisdictions have already begun addressing AI liability and responsibility issues
2. Empirical studies on stakeholder perspectives (lawyers, ethicists, technologists) regarding AI personhood frameworks
3. Simulation modeling of legal system responses to various AI personhood scenarios, including economic and social impacts