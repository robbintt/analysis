---
ver: rpa2
title: 'Sparse Deep Additive Model with Interactions: Enhancing Interpretability and
  Predictability'
arxiv_id: '2509.23068'
source_url: https://arxiv.org/abs/2509.23068
tags:
- sdami
- effects
- main
- interactions
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Sparse Deep Additive Model with Interactions
  (SDAMI), a structured deep learning framework designed for high-dimensional regression
  problems with limited samples. SDAMI addresses the challenge of simultaneously achieving
  interpretability and predictive accuracy by explicitly modeling main effects and
  interactions through dedicated neural subnetworks.
---

# Sparse Deep Additive Model with Interactions: Enhancing Interpretability and Predictability

## Quick Facts
- arXiv ID: 2509.23068
- Source URL: https://arxiv.org/abs/2509.23068
- Reference count: 34
- Primary result: SDAMI achieves lowest MSE and highest TPR/FPR in high-dimensional regression with interpretable additive+interaction structure

## Executive Summary
This paper introduces the Sparse Deep Additive Model with Interactions (SDAMI), a structured deep learning framework designed for high-dimensional regression problems with limited samples. SDAMI addresses the challenge of simultaneously achieving interpretability and predictive accuracy by explicitly modeling main effects and interactions through dedicated neural subnetworks. Central to the approach is the Effect Footprint principle, which posits that variables contributing solely through interactions leave detectable marginal signals, enabling principled feature screening. The method employs a two-stage procedure: first identifying strong main effects (and footprint variables) via sparse additive screening, then disentangling true main effects from interactions using structured regularization such as group lasso.

## Method Summary
SDAMI uses a two-stage approach to build an interpretable deep additive model. Stage 1 applies a sparse additive model (SpAM) to screen variables based on marginal signal strength, capturing both true main effects and "footprint variables" that participate in interactions. Stage 2 employs group lasso with orthogonal basis expansion to partition the selected variables into main effects and interaction candidates. The final model trains dedicated neural subnetworks for each identified component, with norm constraints ensuring sparsity. The architecture enforces that interaction complexity is only paid for when the collective signal justifies it, preventing overfitting in small sample regimes.

## Key Results
- In simulations with n=150, SDAMI achieved MSE as low as 0.270 (Case 6) while maintaining TPRs near 1.0 for main effects
- Outperformed deep neural networks, sparse additive models, and LASSO across all test cases
- Demonstrated superior interpretability through visualization of component functions revealing nonlinear patterns and interaction surfaces
- Applied successfully to neuroscience (V1 fMRI), reliability analysis (chip device lifetime), and medical diagnostics (diabetes progression) datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Screening variables using marginal signals can effectively capture variables that contribute exclusively through interactions.
- **Mechanism:** SDAMI posits an "Effect Footprint," where a variable $X_j$ involved in an interaction $f(X_j, X_k)$ exhibits a non-zero marginal expectation $E[f|X_j] \neq 0$. This allows a sparse additive model (SpAM) in Stage 1 to select variables that appear to have main effects but are actually proxies for interactions.
- **Core assumption:** The interaction functions are not perfectly symmetric or antisymmetric (e.g., XOR logic) such that the marginal projection cancels out.
- **Evidence anchors:** [abstract] Mentions the "Effect Footprint principle, which posits that variables contributing solely through interactions leave detectable marginal signals." [section] Theorem 4.1 (Page 5) formally characterizes when footprints vanish (Hoeffdingâ€“Sobol decomposition), noting that in practice, noise and correlation typically preserve footprints.

### Mechanism 2
- **Claim:** Structured disentanglement prevents interaction terms from overfitting to noise in small sample regimes.
- **Mechanism:** The model uses a two-stage process: Stage 1 identifies a superset of active variables ($\hat{S}$), and Stage 2 uses group lasso to separate true main effects ($\hat{M}$) from interaction candidates ($\hat{I}$). This forces the model to "pay" for interaction complexity only when the grouped signal justifies it.
- **Core assumption:** The "Strong Heredity" or "Weak Heredity" principle holds implicitly; interactions are only sought among variables with detectable footprints.
- **Evidence anchors:** [abstract] Describes the "two-stage strategy: first, identify strong main effects... second, exploit this information through structured regularization." [section] Section 3 (Page 4) details the partitioning of the active set $\hat{S}$ into main and interaction sets using group lasso.

### Mechanism 3
- **Claim:** Norm-constrained deep subnetworks can approximate nonlinear functions while maintaining structural sparsity.
- **Mechanism:** Instead of a dense network, SDAMI assigns dedicated subnetworks ($NN^{(j)}$) to selected variables. Norm constraints on the first-layer weights ($||W_{M,j}^{(1)}||_\infty \leq \kappa$) act as gates, effectively pruning the subnetwork if the functional norm $\|f_j\|$ is zero.
- **Core assumption:** The additive structure (sum of subnetworks) is sufficient to model the response; complex entangled representations (standard DNN) are not required for these specific data types.
- **Evidence anchors:** [abstract] Claims the "deep additive structure achieves higher predictive accuracy than classical additive models." [section] Equation (3) (Page 4) defines the constraint mechanism linking weights to functional norms.

## Foundational Learning

- **Concept:** **Sparse Additive Models (SpAM)**
  - **Why needed here:** This is the statistical engine for Stage 1. You must understand how non-parametric functions $f_j$ are penalized (e.g., via spline smoothness or $L_1$ penalties on coefficients) to interpret the screening results.
  - **Quick check question:** Can you explain why a standard linear LASSO would fail to select a variable that has a strong but purely nonlinear relationship with the target?

- **Concept:** **Hoeffding-Sobol Decomposition / ANOVA Decomposition**
  - **Why needed here:** This is the theoretical backing for the "Effect Footprint." It defines how a function is split into main effects and interactions.
  - **Quick check question:** If a function is purely $f(x_1, x_2) = x_1 \cdot x_2$ and inputs are uniform on $[-1, 1]$, does $x_1$ leave a marginal footprint?

- **Concept:** **Group Lasso**
  - **Why needed here:** Used in Stage 2 to treat interaction terms (which involve multiple basis functions) as groups. It allows the model to drop an entire interaction block if the collective signal is weak.
  - **Quick check question:** How does Group Lasso differ from standard Lasso in terms of feature selection granularity?

## Architecture Onboarding

- **Component map:** Input Layer -> Screening Module (SpAM) -> Disentanglement Module (Group Lasso) -> Deep Subnetworks -> Aggregation Layer
- **Critical path:** The **Screening Module**. If a variable participating in an interaction has a weak footprint (low signal-to-noise ratio), it is dropped before the DNN ever sees it. The success of the final model hinges on the sensitivity of Stage 1.
- **Design tradeoffs:**
  - **Interpretability vs. Accuracy:** You are trading the raw predictive power of a fully connected dense network for the auditability of additive components.
  - **Footprint Sensitivity:** Tuning the SpAM penalty ($\lambda_1$) is critical. Too high $\to$ missed interactions; Too low $\to$ computational overhead and noise in the interaction network.
- **Failure signatures:**
  - **Symmetric XOR failure:** Model predicts near-zero accuracy on pure interaction data where main effects are perfectly centered (the "break condition" of the footprint mechanism).
  - **"Ghost" Main Effects:** The model identifies a variable as a main effect when it is actually a footprint for an interaction, potentially leading to suboptimal functional approximation if the DNN subnetwork for the main effect is too shallow.
- **First 3 experiments:**
  1. **Sanity Check (Linear + XOR):** Generate data $y = x_1 + x_2 + x_3 \cdot x_4$. Verify SDAMI correctly assigns $x_1, x_2$ to $\hat{M}$ and $x_3, x_4$ to $\hat{I}$.
  2. **Footprint Vanishing Test:** Generate data $y = x_3 \cdot x_4$ where $X$ is centered and independent. Confirm if SDAMI fails to recover the structure (as predicted by Theorem 4.1) and compare against a standard DNN.
  3. **Scalability Check:** Run the full pipeline (SpAM $\to$ Group Lasso $\to$ DNN) on the provided V1 fMRI dataset (p=11,000) to ensure the sparse screening effectively reduces the dimension before the expensive DNN training step.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the initial SpAM estimation phase be replaced by Sure Independence Screening (SIS) to improve computational efficiency without sacrificing selection consistency?
  - **Basis:** [explicit] The authors note that "Screening methods such as Sure Independence Screening (SIS)... could accelerate this step by prioritizing variable selection over full function estimation."
  - **Why unresolved:** The current two-stage procedure relies on estimating function norms via SpAM, which is computationally demanding for ultrahigh-dimensional settings.
  - **What evidence would resolve it:** A theoretical extension of the selection consistency proof for SIS-based screening and empirical runtime comparisons on datasets with significantly higher dimensionality ($p > 10,000$).

- **Open Question 2:** What are the minimax convergence rates for SDAMI, and can finite-sample theoretical guarantees be established?
  - **Basis:** [explicit] The paper states that "current theoretical results establish effect-level consistency but not convergence rates" and suggests incorporating advances in nonparametric learning could provide these guarantees.
  - **Why unresolved:** While the paper proves prediction convergence in probability, it does not quantify the speed of convergence or provide finite-sample error bounds.
  - **What evidence would resolve it:** Theoretical derivation of the convergence rate (e.g., $O_p(n^{-\alpha})$) and simulations verifying these rates align with finite-sample behavior.

- **Open Question 3:** How does SDAMI perform in data regimes where the Effect Footprint theoretically vanishes, such as antisymmetric interactions (e.g., XOR)?
  - **Basis:** [inferred] Theorem 4.1 defines conditions where footprints vanish (e.g., independence with centering or perfect symmetry), implying the screening stage may fail to detect variables that contribute solely through these specific interaction types.
  - **Why unresolved:** The method relies on marginal signals to trigger subnetwork activation; interactions that perfectly cancel out marginally leave no "footprint" to detect.
  - **What evidence would resolve it:** Simulation studies specifically designed with XOR-like or antisymmetric interaction structures to test if SDAMI's false negative rate increases significantly compared to methods that do not rely on marginal screening.

## Limitations

- **Symmetric Interaction Failure:** The Effect Footprint principle relies on non-zero marginal signals from interaction terms. For perfectly symmetric functions (e.g., $X_1 \cdot X_2$ with centered inputs), this footprint vanishes, potentially causing SDAMI to miss true interactions.
- **Implementation Complexity of Norm Constraints:** The constraint $\|W^{(1)}\|_\infty \leq \kappa \|f\|$ is central to SDAMI's pruning mechanism, but the paper lacks explicit implementation details, making faithful reproduction challenging.
- **Two-Stage Dependency Risk:** Stage 1's SpAM screening must successfully capture footprint variables for Stage 2 to work. Type II errors in the screening phase are permanent - missed interaction variables cannot be recovered in later stages.

## Confidence

- **High Confidence:** MSE improvements over baselines (LASSO, SpAM, DNN) are well-supported by simulation results across multiple sample sizes and real-world datasets.
- **Medium Confidence:** The theoretical guarantees (Theorem 4.1 on Effect Footprint, Theorems 4.2-4.3 on selection consistency) are sound, but their practical relevance depends on real-world data meeting the "generic" conditions stated.
- **Low Confidence:** The specific implementation of norm-constrained deep subnetworks and the orthogonal basis expansion for Group Lasso lack sufficient detail for exact replication.

## Next Checks

1. **Footprint Vanishing Test:** Generate data $y = x_3 \cdot x_4$ with centered, independent inputs. Verify SDAMI fails to recover the interaction structure (as predicted by Theorem 4.1) and compare performance against standard DNNs.

2. **Stage 1 Sensitivity Analysis:** Systematically vary the SpAM penalty parameter and measure the resulting TPR/FPR for interaction variables across different signal-to-noise ratios to quantify the risk of permanently missing interactions.

3. **Implementation Reproduction:** Build a minimal PyTorch implementation of the constrained subnetwork architecture (widths 15-12-10) and test on a simple Case 3 simulation to verify that the norm constraints effectively prune connections as intended.