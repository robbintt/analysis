---
ver: rpa2
title: 'Adapting Definition Modeling for New Languages: A Case Study on Belarusian'
arxiv_id: '2507.09536'
source_url: https://arxiv.org/abs/2507.09536
tags:
- definition
- data
- modeling
- language
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to adapt existing definition modeling systems
  to Belarusian. The authors propose a new dataset of 43,150 definitions and experiment
  with finetuning multilingual models on subsets of this data.
---

# Adapting Definition Modeling for New Languages: A Case Study on Belarusian

## Quick Facts
- arXiv ID: 2507.09536
- Source URL: https://arxiv.org/abs/2507.09536
- Reference count: 18
- One-line primary result: Even 1% of training data suffices for Belarusian definition modeling adaptation, though larger datasets improve informativeness and reduce circularity.

## Executive Summary
This paper investigates adapting existing definition modeling systems to Belarusian, a low-resource language. The authors create a new dataset of 43,150 definitions and experiment with fine-tuning multilingual models on subsets of this data. Results show that minimal data (1%) enables the model to generate Belarusian definitions with high fluency, while larger datasets significantly improve informativeness and reduce circular definitions. The study also reveals gaps in current automatic evaluation metrics, with character-level and specific BLEURT variants better capturing quality than standard metrics. The findings suggest minimal data suffices for language adaptation, but better evaluation methods are needed for definition modeling quality assessment.

## Method Summary
The study fine-tunes MT0-XL, a 3.7B parameter multilingual encoder-decoder, on Belarusian definition data. The authors create the TSBM dataset from the Skarnik online dictionary, containing 40,105 training, 1,486 validation, and 1,159 test instances. They experiment with logarithmically-spaced data subsets (1%, 3%, 10%, 31%, 100%) across three base models (Russian, Norwegian, English fine-tuned variants). Input format uses a Russian interrogative frame "[EXAMPLE] Что такое [HEADWORD]?" with definition glosses as targets. Evaluation combines automatic metrics (BLEU, BERTScore, BLEURT variants, chrF++) with manual annotation for fluency, informativeness, and circularity.

## Key Results
- Minimal data (1% of dataset) enables models to generate Belarusian definitions with high fluency scores
- Larger datasets significantly improve informativeness and reduce circular definitions (from 26% to 11% at full data)
- Language similarity between base and target languages does not significantly affect adaptation performance
- Standard automatic metrics poorly align with human judgment; chrF++ and BLEURT-D12 better capture informativeness

## Why This Works (Mechanism)

### Mechanism 1: Low-Data Language Adaptation via Fine-Tuning
- Claim: Minimal fine-tuning data (1% of dataset, ~400 examples) can shift a pretrained multilingual model's output language while preserving fluency.
- Mechanism: The base MT0-XL model already contains multilingual representations. Fine-tuning on even small Belarusian data activates and biases these representations toward target-language generation patterns, rapidly aligning output language without requiring full retraining.
- Core assumption: The base model's multilingual pretraining has already encoded sufficient cross-lingual transfer capacity.
- Evidence anchors:
  - [Section 4, Figure 2]: "With only 1% of the training data, the model already achieves a fluency score of 0.78."
  - [Section 4, Figure 1]: "Any amount of training data immediately gears all three models toward producing Belarusian."
  - [Corpus]: Related work on multilingual definition modeling (FMR=0.61) suggests multilingual models can generalize across languages, supporting this assumption.
- Break condition: If the target language lacks representation in the base model's pretraining data, adaptation may require significantly more examples or fail entirely.

### Mechanism 2: Language-Independent Transfer for Definition Generation
- Claim: Adaptation performance does not meaningfully depend on linguistic similarity between the base model's training language and the target language.
- Mechanism: Definition modeling relies on abstract semantic-to-text generation capabilities that are largely language-agnostic in multilingual models. The model learns a general "definition schema" that transfers across languages, with fine-tuning primarily handling surface realization.
- Core assumption: Definition structure (gloss patterns, explanatory frames) generalizes across languages more than it diverges.
- Evidence anchors:
  - [Section 4]: "Performances with a Russian model re-trained for Belarusian are on par with what we observe with the Norwegian or English baselines. This strongly suggests that adaptation does not depend on the similarity of the languages considered."
  - [Section 3.2]: All three base models (Russian, Norwegian, English) use the same MT0-XL architecture.
  - [Corpus]: No direct corpus evidence on cross-linguistic similarity effects for definition modeling.
- Break condition: May not hold for typologically distant language pairs or languages with fundamentally different definition conventions (e.g., languages without a dictionary tradition).

### Mechanism 3: Asymmetric Scaling of Fluency vs. Informativeness
- Claim: Fluency stabilizes quickly with minimal data, while informativeness requires substantially more training examples to approach human-quality semantic accuracy.
- Mechanism: Grammatical patterns and natural phrasing are learned from distributional regularities that are abundant even in small corpora. Semantic precision requires learning sense distinctions, avoiding circularity, and capturing nuanced glosses—capabilities that depend on exposure to diverse definitional contexts.
- Core assumption: Fluency is primarily a surface-level phenomenon; informativeness requires deeper semantic grounding.
- Evidence anchors:
  - [Section 4]: "Fluency remains consistently high across all data sizes... Informativeness shows a more significant improvement as the amount of training data increases. Starting from a modest score of 0.32 in 1%, informativeness increases to 0.60 when the entire dataset is used."
  - [Section 4, Table 4]: Full circularity decreases from 26% (1% data) to 11% (100% data).
  - [Corpus]: Related work on definition generation quality notes similar fluency-semantic gaps, though no direct quantitative comparison found.
- Break condition: If the training data contains systematically circular or low-quality definitions, more data may not improve informativeness.

## Foundational Learning

- Concept: **Definition Modeling**
  - Why needed here: The task requires generating dictionary-style definitions for words in context. Understanding that this differs from standard text generation helps frame why evaluation is challenging.
  - Quick check question: Can you explain why generating a definition for "bank" in "river bank" vs. "bank account" requires context awareness?

- Concept: **Fine-Tuning vs. Zero-Shot Transfer**
  - Why needed here: The paper's core intervention is fine-tuning pretrained models on limited target-language data. Understanding this distinction clarifies why 0% (zero-shot) performs poorly while 1% data shows dramatic gains.
  - Quick check question: If you have a model trained on Russian and want it to generate Belarusian, what is the difference between zero-shot inference and fine-tuning on 100 Belarusian examples?

- Concept: **NLG Evaluation Metrics (BLEU, BERTScore, BLEURT, chrF++)**
  - Why needed here: The paper critiques automatic metrics' misalignment with human judgment. Understanding what each metric captures (n-gram overlap vs. semantic similarity) is essential for interpreting results.
  - Quick check question: Why might BLEU score be high for a fluent but factually incorrect definition?

## Architecture Onboarding

- Component map:
  Base Model: MT0-XL (3.7B parameters), pretrained multilingual encoder-decoder
  -> Source Models: Three variants fine-tuned on Russian, Norwegian, or English definition data (from Kutuzov et al., 2024)
  -> Input Format: [EXAMPLE] <context sentence> Что такое <HEADWORD>? (using Russian interrogative frame)
  -> Target Output: Definition gloss in Belarusian
  -> Dataset (TSBM): 40,105 train / 1,486 validation / 1,159 test instances; headwords strictly separated across splits
  -> Evaluation Pipeline: Automatic metrics (BLEU, BERTScore, BLEURT variants, chrF++) + manual annotation (fluency, informativeness, circularity)

- Critical path:
  1. Select base model (language of original fine-tuning appears non-critical)
  2. Prepare Belarusian definition data with POS tags and example sentences; filter functional words
  3. Fine-tune using logarithmically-spaced data subsets (1%, 3%, 10%, 31%, 100%) to identify minimal viable dataset size
  4. Generate definitions for held-out test headwords
  5. Evaluate with multiple automatic metrics; validate with manual annotation on homograph and non-homograph subsets
  6. Use language identification to confirm output language shift

- Design tradeoffs:
  - Data efficiency vs. quality: 1% data achieves adequate fluency but 100% needed for semantic accuracy; choose based on application constraints
  - Metric selection: chrF++ and BLEURT-D12 correlate best with informativeness; BLEURT-D6 for fluency; standard BLEU/BERTScore are suboptimal for definition modeling
  - Base model selection: No significant performance difference across Russian/Norwegian/English base models; prioritize availability over linguistic similarity

- Failure signatures:
  - Zero-shot output in wrong language: Base model generates in its original training language (e.g., Russian definitions for Belarusian headwords)
  - Circular definitions: Model uses headword or its morphological variants in the generated definition (26% rate at 1% data)
  - Sense disambiguation errors: Incorrect meaning selected for polysemous/homographic entries
  - Metric-human misalignment: High automatic scores but low manual informativeness ratings

- First 3 experiments:
  1. Replicate with a different Slavic low-resource language (e.g., Ukrainian or Slovak) to test whether language similarity to Russian actually affects transfer, using the same logarithmic data scaling approach.
  2. Ablate the input format by replacing the Russian interrogative frame (Что такое) with target-language equivalents or neutral tokens to test whether source-language prompts bias output.
  3. Evaluate BLEURT-D12 and chrF++ as primary metrics on an existing English definition dataset to confirm whether the metric recommendations generalize beyond Belarusian.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop or adapt automatic evaluation metrics to reliably capture fluency and semantic nuances in definition modeling?
- Basis in paper: [explicit] The authors conclude that "further research is necessary in order to properly automatize the assessment" and explicitly note that current metrics show poor correlation with human judgments of fluency and vary in their ability to capture informativeness.
- Why unresolved: The study finds that standard metrics like BLEU and BERTScore fail to align with human evaluation, particularly for fluency, and that even learning-based metrics like BLEURT behave inconsistently compared to other NLG tasks.
- What evidence would resolve it: The proposal of a new metric or the fine-tuning of an existing one that demonstrates a statistically significant high correlation with human judgments across both fluency and informativeness in definition generation tasks.

### Open Question 2
- Question: Does the similarity between the base model's language and the target language truly have no impact on adaptation performance?
- Basis in paper: [explicit] The introduction asks, "Do we need base models trained for similar languages?" The results (Section 4) surprisingly show that performance is stable regardless of whether the base model was Russian (related) or English/Norwegian (less related).
- Why unresolved: The finding contradicts the intuition that linguistic proximity should aid transfer. It remains unclear if this stability is an artifact of the specific models used (mT5-based) or if the zero-shot baselines were universally too low to reveal the benefits of relatedness.
- What evidence would resolve it: A comparative study adapting definition models to a wider variety of target languages (including non-Indo-European ones) showing whether related base languages consistently offer no advantage over unrelated ones.

### Open Question 3
- Question: How can low-resource definition modeling overcome the dependency on large datasets to reduce circularity and semantic errors?
- Basis in paper: [inferred] The manual evaluation reveals that while fluency is achievable with minimal data (1%), informativeness is low (0.32) and circularity is high (26%) in low-data settings. The authors note models often rely on morphological patterns over semantic accuracy.
- Why unresolved: The paper demonstrates that scaling up data resolves these issues, but this solution is paradoxical for low-resource languages where large datasets are unavailable.
- What evidence would resolve it: A method (e.g., constraint-based decoding or specific data augmentation) that maintains high fluency while significantly reducing circularity and improving semantic accuracy in the 1% data setting.

## Limitations
- Training hyperparameters (learning rate, batch size, epochs) are not specified, affecting reproducibility
- Manual evaluation sample size for validating automatic metrics is not reported
- Dataset construction lacks details on exact criteria for filtering "incorrect entries" and typo correction methodology
- Language similarity claims based on comparison across only three languages may not generalize

## Confidence

- **High Confidence**: The finding that minimal data (1%) suffices for Belarusian language generation while larger datasets improve informativeness is well-supported by systematic experiments across multiple data scales.
- **Medium Confidence**: The recommendations for specific automatic metrics (chrF++ and BLEURT-D12 for informativeness, BLEURT-D6 for fluency) are based on manual evaluation of a limited sample and require broader validation.
- **Low Confidence**: The mechanism explaining why fluency stabilizes faster than informativeness (surface-level vs. semantic learning) remains speculative without direct probing experiments or analyses of model internal representations.

## Next Checks

1. Replicate the language adaptation experiment with a different Slavic low-resource language (e.g., Ukrainian or Slovak) to test whether the claim about language similarity holds across a broader typological range.
2. Conduct ablation studies on the input format by replacing the Russian interrogative frame (Что такое) with target-language equivalents or neutral tokens to determine whether prompt language biases output generation.
3. Validate the automatic metric recommendations (chrF++ and BLEURT-D12) on an existing English definition dataset to confirm whether these metrics generalize beyond Belarusian definition modeling.