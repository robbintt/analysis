---
ver: rpa2
title: 1$^{st}$ Place Solution of WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction
  Challenge
arxiv_id: '2505.03543'
source_url: https://arxiv.org/abs/2505.03543
tags:
- multimodal
- item
- learning
- embedding
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the 1st place solution to the WWW 2025 EReL@MIR
  Workshop Multimodal CTR Prediction Challenge, focusing on improving click-through
  rate prediction by integrating multimodal embeddings into recommender systems. The
  proposed approach combines sequential modeling with feature interaction learning,
  using a Transformer-based architecture to capture user interest patterns and DCNv2
  for modeling high-order feature interactions.
---

# 1$^{st}$ Place Solution of WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction Challenge

## Quick Facts
- **arXiv ID**: 2505.03543
- **Source URL**: https://arxiv.org/abs/2505.03543
- **Reference count**: 15
- **Primary result**: Achieves 0.9839 AUC on WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction Challenge

## Executive Summary
This paper presents the winning solution to the WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction Challenge, achieving 0.9839 AUC by integrating multimodal embeddings with sequential modeling and explicit feature interactions. The approach combines a Transformer-based sequential feature learning module with DCNv2 for high-order feature interaction modeling. Multimodal embeddings are concatenated with item embeddings, though the authors note this was due to time constraints and future work will explore better alignment methods. Extensive ablation studies confirm the effectiveness of both the Transformer and DCNv2 components, with results showing that proper integration of multimodal information is crucial for optimal performance.

## Method Summary
The solution integrates multimodal embeddings into a CTR prediction framework using a two-stage architecture: sequential feature learning followed by feature interaction modeling. The sequential module uses a Transformer encoder to capture user interest patterns, with target item concatenation enabling context-aware encoding. The feature interaction module employs DCNv2 to model high-order feature interactions through explicit cross networks. Multimodal embeddings (PCA-reduced concatenation of BERT+CLIP) are frozen and concatenated with item embeddings. The model uses binary cross-entropy loss, Adam optimizer (lr=5e-4), and early stopping, achieving 0.9839 AUC on the leaderboard.

## Key Results
- Achieves 0.9839 AUC on WWW 2025 EReL@MIR Workshop Multimodal CTR Prediction Challenge leaderboard
- Ablation shows DCNv2 removal causes sharpest performance drop (0.9776→0.9632 AUC)
- Target-concatenated Transformer encoding outperforms generic sequence encoding
- k=16 short-term window with max-pooling long-term representation optimal for multi-timescale preferences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Concatenating target item embedding with each historical item before Transformer encoding improves sequential interest capture compared to encoding history independently.
- **Mechanism**: By concatenating target item embedding to each position in the history sequence, the self-attention mechanism learns position-wise relevance weights between user's historical behaviors and the current candidate item, creating query-aware sequence representation.
- **Core assumption**: User interest is contextually activated by the target item—relevant history items should be weighted differently depending on what's being predicted.
- **Evidence anchors**:
  - [section 2.2.2]: "The target item embedding e_target is concatenated with every item embedding in the history sequence to form the input sequence"
  - [corpus]: "Quadratic Interest Network" paper similarly targets multimodal CTR with explicit interest modeling, suggesting target-aware encoding is a productive pattern
- **Break condition**: If target item is appended only once (not per-position), or if positional encoding is removed entirely, the mechanism degrades to generic sequence encoding.

### Mechanism 2
- **Claim**: DCNv2's cross network provides explicit bounded-degree feature interactions that complement the implicit interactions learned by the deep MLP.
- **Mechanism**: The cross layer performs element-wise multiplication between input features and learned projections, creating polynomial feature interactions of increasing order with each layer. This is fused with a parallel deep network, allowing both memorization (cross) and generalization (deep).
- **Core assumption**: CTR prediction benefits from explicit feature crosses (e.g., "user_age × item_category") that may be hard for pure MLPs to learn efficiently.
- **Evidence anchors**:
  - [section 2.2.3]: "To explicitly model the interactions between features, we adopt DCNv2 as the feature interaction module"
  - [section 3.2]: "Removing the DCNv2 layer (w/o DCNv2) sharply degrades performance" (AUC drops from 0.9776 to 0.9632)
  - [corpus]: Related MMCTR challenge solutions similarly rely on cross-network architectures for feature interaction
- **Break condition**: If cross layers are removed and only deep MLP remains, high-order feature interactions must be learned implicitly, which may require more parameters or data.

### Mechanism 3
- **Claim**: Combining the last k Transformer outputs (short-term) with max-pooled full sequence (long-term) captures multi-timescale user preferences.
- **Mechanism**: Selects the last k=16 positions for fine-grained short-term interest while applying max pooling across all positions for coarse long-term signal. This dual representation is flattened and passed to the interaction module.
- **Core assumption**: User behavior follows recency bias (recent items more predictive) but long-term preferences still provide discriminative signal.
- **Evidence anchors**:
  - [section 2.2.2]: "Following [13], the latest k outputs are selected as the representation of user's short-term interest preference. And the max pooling operation is adopted to represent the user's long-term interest preference"
  - [section 3.3]: Parameter sensitivity shows k=16 is optimal among [0, 2, 4, 8, 16, 24], with k=0 (no short-term) degrading performance
- **Break condition**: If k is set too large (e.g., k=24), the model overfits to recent noise; if k=0, short-term signal is lost entirely.

## Foundational Learning

- **Concept: Transformer self-attention for sequences**
  - Why needed here: The Sequential Feature Learning Module uses Transformer encoders to model user history. Without understanding attention weights, positional encoding, and multi-head attention, debugging the sequence representation is difficult.
  - Quick check question: Can you explain why concatenating the target item to each history position changes what the attention mechanism learns?

- **Concept: Cross networks (DCN/DCNv2)**
  - Why needed here: DCNv2 is the core feature interaction module. Understanding how cross layers differ from standard MLP layers is essential for tuning depth and diagnosing underfitting.
  - Quick check question: What feature interaction order does a 3-layer cross network produce at most?

- **Concept: AUC as a ranking metric**
  - Why needed here: The challenge evaluates on AUC, not accuracy. Hyperparameter choices (e.g., dropout=0.2) may look suboptimal under log loss but improve AUC by better ranking.
  - Quick check question: If AUC improves but log loss worsens, what does this indicate about model calibration vs. ranking ability?

## Architecture Onboarding

- **Component map**: Embedding Layer -> Sequential Feature Learning (Transformer) -> Feature Interaction (DCNv2) -> Prediction (MLP)
- **Critical path**:
  1. Verify embedding concatenation produces correct dimension (item features × 64 + multimodal dim)
  2. Confirm padding mask is applied in Transformer for variable-length sequences
  3. Check DCNv2 input includes all three sources: e_target, e_side, S_o
  4. Validate binary cross-entropy loss with proper label handling
- **Design tradeoffs**:
  - Frozen vs. learnable multimodal embeddings: Frozen reduces training cost but may misalign with CTR task (authors note baseline DIN + multimodal actually hurts performance)
  - k=16 short-term window: Trades off short-term granularity vs. noise sensitivity
  - Parallel DCNv2: Cross + deep branches increase capacity but require more careful regularization
- **Failure signatures**:
  - AUC ~0.85-0.88 with multimodal enabled but ~0.97 without: Multimodal embeddings not aligned with task (observed in baseline DIN)
  - AUC drops sharply when removing DCNv2: Cross-feature interactions are critical
  - Model collapse at high learning rate (1e-3): Reduce to 5e-4
- **First 3 experiments**:
  1. **Baseline sanity check**: Run without Transformer, without DCNv2, and full model on validation set. Confirm ablation results match Table 2 trends.
  2. **Multimodal impact**: Compare with vs. without frozen multimodal embeddings. If performance degrades, investigate embedding alignment or add a small learnable projection layer.
  3. **k-sensitivity sweep**: Test k ∈ {4, 8, 16, 24} on validation. Plot AUC vs. k to verify local optimum near 16.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can quantization methods transform frozen multimodal embeddings into semantic and learnable item embeddings that outperform simple concatenation for CTR prediction?
  - Basis in paper: [explicit] The conclusion states: "In the future, we will explore quantization methods to transform the frozen multimodal embeddings into semantic and learnable item embeddings."
  - Why unresolved: Time constraints limited the authors to simple concatenation; no alternative integration methods were tested.
  - What evidence would resolve it: Comparative experiments on the MicroLens benchmark showing quantized learnable embeddings achieving higher AUC than frozen concatenated embeddings.

- **Open Question 2**: How can prior knowledge encoded in pretrained multimodal embeddings effectively guide the learning of collaborative ID embeddings?
  - Basis in paper: [explicit] The authors explicitly identify this as a "worthy research direction" in the conclusion.
  - Why unresolved: The paper uses frozen embeddings throughout and does not propose any knowledge transfer mechanism to ID embeddings.
  - What evidence would resolve it: A method that leverages multimodal embeddings to regularize or initialize ID embeddings, with empirical gains demonstrated on CTR metrics.

- **Open Question 3**: Why do certain architectures (DIN, models without Transformer) suffer performance degradation when multimodal embeddings are incorporated?
  - Basis in paper: [inferred] The authors observe that "the baseline DIN model and our model without Transformer suffer a substantial performance degradation when multimodal embeddings are added" and hypothesize misalignment, but do not investigate causes.
  - Why unresolved: The phenomenon is documented but no ablation or architectural analysis explains which components enable multimodal compatibility.
  - What evidence would resolve it: Systematic probing of how Transformer attention patterns interact with multimodal features, or architectural modifications that restore compatibility in simpler models.

- **Open Question 4**: What specific feature interactions does DCNv2 capture that are critical for multimodal CTR prediction but cannot be captured by sequential modeling alone?
  - Basis in paper: [inferred] Removing DCNv2 causes the sharpest AUC drop (from 0.9776 to 0.9632), yet the paper treats it as a black-box component without analyzing which interactions drive gains.
  - Why unresolved: No interpretability analysis or cross-layer inspection was conducted to identify the most valuable learned interactions.
  - What evidence would resolve it: Ablation on specific cross-layer outputs, feature importance attribution, or attention visualization revealing key feature combinations.

## Limitations

- Multimodal embedding integration uses simple concatenation rather than more sophisticated alignment methods
- Sequence length N is not specified, creating implementation uncertainty
- Frozen embeddings may limit task-specific adaptation despite computational benefits
- Limited exploration of multimodal embedding alignment mechanisms

## Confidence

- **High confidence**: The effectiveness of the DCNv2 feature interaction module (confirmed by ablation: 0.9776→0.9632 AUC drop when removed) and the overall model architecture design choices.
- **Medium confidence**: The sequential modeling approach with target-concatenated history and dual timescale representation (short-term k=16 + long-term max-pool) based on parameter sensitivity studies.
- **Low confidence**: The multimodal embedding integration strategy and the optimal window size k=16, as these were determined under time constraints without exploring alternatives.

## Next Checks

1. **Ablation validation**: Replicate the three core ablations (w/o Transformer, w/o DCNv2, full model) on validation set to confirm the reported performance trends and mechanism effectiveness.

2. **Multimodal alignment experiment**: Test the impact of adding a small learnable projection layer to transform multimodal embeddings before concatenation, addressing the alignment issue noted by the authors.

3. **k-sensitivity validation**: Systematically test k ∈ {4, 8, 16, 24} on validation set with proper plotting of AUC vs. k to verify the local optimum at k=16 and understand the short-term/long-term tradeoff.