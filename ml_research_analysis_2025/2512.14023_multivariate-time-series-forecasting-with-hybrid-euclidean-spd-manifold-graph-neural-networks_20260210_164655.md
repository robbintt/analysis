---
ver: rpa2
title: Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph
  Neural Networks
arxiv_id: '2512.14023'
source_url: https://arxiv.org/abs/2512.14023
tags:
- graph
- manifold
- forecasting
- time
- euclidean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HSMGNN introduces a novel approach for multivariate time series
  forecasting by leveraging hybrid Euclidean-Riemannian geometric representations.
  The method employs a Submanifold-Cross-Segment embedding to project time series
  data into both Euclidean and SPD manifold spaces, capturing diverse spatial-temporal
  dependencies.
---

# Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks

## Quick Facts
- arXiv ID: 2512.14023
- Source URL: https://arxiv.org/abs/2512.14023
- Reference count: 40
- HSMMGNN achieves up to 13.8% improvement in forecasting accuracy over state-of-the-art baselines

## Executive Summary
This paper introduces a novel approach for multivariate time series forecasting by leveraging hybrid Euclidean-Riemannian geometric representations. The method employs a Submanifold-Cross-Segment embedding to project time series data into both Euclidean and SPD manifold spaces, capturing diverse spatial-temporal dependencies. An Adaptive-Distance-Bank layer with a trainable memory mechanism approximates Riemannian distances efficiently, reducing computational overhead. The Fusion Graph Convolutional Network integrates features from both geometric spaces through a learnable fusion operator for accurate predictions. Experiments on three benchmark datasets demonstrate significant performance improvements over existing methods.

## Method Summary
The HSMGNN architecture consists of three main components: a Submanifold-Cross-Segment (SCS) embedding module that projects input time series into both Euclidean and SPD manifold spaces using 1D CNN and sliding window covariance calculations; an Adaptive-Distance-Bank (ADB) layer with a trainable memory bank that approximates Riemannian distances efficiently; and a Fusion Graph Convolutional Network (FGCN) that integrates features from both geometric spaces through weighted fusion. The model is trained with batch size 32, Adam optimizer (lr=1e-4), and early stopping for up to 80 epochs.

## Key Results
- Achieves up to 13.8% improvement in forecasting accuracy over state-of-the-art baselines
- Reduces computational complexity from O(E³) to O(E² + N²) using the ADB layer
- Demonstrates effectiveness across three diverse benchmark datasets: C-MAPSS (RUL prediction), UCI-HAR (human activity recognition), and ISRUC-S3 (sleep stage classification)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Projecting MTS data into both Euclidean and SPD manifolds captures spatio-temporal dependencies that single-space models miss.
- **Mechanism**: SCS embedding uses 1D CNN for Euclidean features and computes sample covariance matrices over sliding windows for SPD features.
- **Core assumption**: The manifold hypothesis holds for input MTS, meaning high-dimensional sensor data lies on a lower-dimensional curved geometry.
- **Evidence anchors**: Abstract states SCS captures ST variations; section 4.2 describes covariance matrix computation; general MTS literature supports ST dependency complexity.
- **Break condition**: If input data lacks correlation structure, SPD projection adds noise rather than signal.

### Mechanism 2
- **Claim**: A trainable memory bank can approximate expensive Riemannian distance calculations efficiently.
- **Mechanism**: ADB uses trainable memory matrix and Nonlinear Distance Vector to modulate adjacency matrix via residual connection.
- **Core assumption**: Historical patterns stored in memory bank are sufficient to approximate local geometric distances for new data points.
- **Evidence anchors**: Abstract mentions ADB reduces computational overhead; section 4.5 states complexity reduction from O(E³) to O(E² + N²).
- **Break condition**: If manifold curvature is highly irregular or non-stationary, fixed-size memory bank may fail to track geometric shifts.

### Mechanism 3
- **Claim**: Fusing features from hybrid geometric spaces via learnable weighted operator improves forecasting accuracy.
- **Mechanism**: FGCN aggregates features from both SPD and Euclidean streams and combines them using learnable weights.
- **Core assumption**: Euclidean and Riemannian features provide complementary information that must be balanced dynamically.
- **Evidence anchors**: Abstract mentions FGCN integrates features via learnable fusion operator; section 5.4.3 shows balanced weights outperform single-space reliance.
- **Break condition**: If one domain contains significantly higher noise, fusion mechanism may propagate errors unless weights are heavily regularized.

## Foundational Learning

- **Concept**: Riemannian Geometry & SPD Manifolds
  - **Why needed here**: Euclidean space distorts true distances of correlated time-series data; understanding SPD matrices as curved cones is crucial.
  - **Quick check question**: Can you explain why computing the arithmetic mean of two covariance matrices might yield a result that does not respect the geometry of the data manifold?

- **Concept**: Graph Neural Networks (GNNs) for Time Series
  - **Why needed here**: Architecture relies on constructing graphs where nodes are sensors and edges are dependencies.
  - **Quick check question**: How does a GNN propagate information differently than an RNN when processing multivariate sensor data?

- **Concept**: Sample Covariance Matrices
  - **Why needed here**: This is the explicit bridge used by SCS module to transform Euclidean windows into SPD manifold points.
  - **Quick check question**: Does increasing the sliding window size make the sample covariance estimate more stable or more noisy?

## Architecture Onboarding

- **Component map**: Input MTS -> SCS Module (1D CNN + Sliding Window Covariance) -> Graph Construction (Standard dot-product + ADB refinement) -> FGCN (Parallel Graph Convolutions + Weighted Fusion) -> MLP Predictor
- **Critical path**: The projection from Euclidean time patches to SPD covariance matrices (SCS) is the linchpin. If this transformation is noisy, subsequent ADB and FGCN cannot recover the geometric signal.
- **Design tradeoffs**:
  - *Accuracy vs. Complexity*: ADB approximates Riemannian distances to avoid O(E³) cost, but may lose geometric precision.
  - *Window Size*: Smaller windows capture short-term dynamics but result in noisier covariance estimates.
- **Failure signatures**:
  - *Mode Collapse*: If ADB weights saturate, SPD adjacency matrix degrades to simple dot product.
  - *Fusion Imbalance*: If validation loss plateaus while training loss drops, check if weights are diverging.
- **First 3 experiments**:
  1. **Sanity Check (Ablation)**: Run "w/o FGCN" vs. "Complete" on C-MAPSS subset to verify SPD path contribution.
  2. **Hyperparameter Sensitivity**: Test cross-decomposition ratio δ on UCI-HAR to find optimal sliding window length.
  3. **Memory Size Analysis**: Vary query size in ADB to observe trade-off between computational overhead and RMSE on FD002/FD004.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical error bounds of the Adaptive-Distance-Bank (ADB) approximation compared to exact Riemannian metrics?
- Basis: Section 4.5 states ADB approximates geometric relationships without explicit decomposition to reduce complexity from O(E³) to O(E²).
- Why unresolved: While paper demonstrates empirical success, it does not quantify geometric distortion or fidelity loss introduced by replacing expensive Riemannian distance calculations with trainable memory bank.
- What evidence would resolve it: Theoretical analysis bounding divergence between ADB-approximated distances and true geodesic distances, or tests on synthetic data with known manifold properties.

### Open Question 2
- Question: Is the linear weighted fusion mechanism sufficient to capture complex interactions between Euclidean and SPD manifold features?
- Basis: Equation 15 defines final prediction as linear weighted sum of two geometric representations.
- Why unresolved: Simple weighted sum assumes linear relationship between spaces; unclear if this fails to model non-linear cross-manifold correlations.
- What evidence would resolve it: Comparative studies substituting weighted sum with non-linear fusion operators to see if accuracy improves on datasets with complex inter-sensor dynamics.

### Open Question 3
- Question: Does the non-overlapping partition strategy in SCS embedding introduce boundary effects that disrupt temporal continuity?
- Basis: Section 4.2 describes partitioning temporal dimension into non-overlapping blocks to construct SPD matrices.
- Why unresolved: Standard patch-based processing can isolate dependencies at window boundaries; paper does not analyze if overlapping windows would better preserve continuous temporal evolution.
- What evidence would resolve it: Ablation experiments varying stride of SCS embedding to determine if overlapping windows yield better performance than strictly non-overlapping approach.

## Limitations
- Empirical validation depth lacks rigorous theoretical guarantees for why SPD manifold projections capture complementary dependencies
- Memory bank approximation may degrade under highly non-stationary manifold geometries
- Computational complexity reduction claims depend on assumption that O(E³) operations can be safely approximated without significant accuracy loss

## Confidence
- **High Confidence**: Experimental results showing 13.8% accuracy improvement on three benchmark datasets; architecture is well-specified and reproducible
- **Medium Confidence**: Claim that SPD manifold projections capture complementary spatial-temporal dependencies; evidence is primarily empirical rather than theoretical
- **Low Confidence**: Assertion that memory bank approximation maintains geometric fidelity; complexity reduction is clear but trade-off between efficiency and accuracy remains uncertain

## Next Checks
1. **Geometric Fidelity Analysis**: Quantify approximation error introduced by memory bank mechanism by comparing ADB outputs against exact Riemannian distance calculations on synthetic manifolds with known geometry.
2. **Cross-Decomposition Ratio Sensitivity**: Conduct systematic grid search over δ values on UCI-HAR to determine if there's theoretically optimal range based on sample covariance matrix stability.
3. **Non-Stationary Manifold Testing**: Evaluate HSMGNN on time series with known structural breaks or regime shifts to assess whether memory bank can adapt to changing manifold geometries or if performance degrades when SPD space becomes non-stationary.