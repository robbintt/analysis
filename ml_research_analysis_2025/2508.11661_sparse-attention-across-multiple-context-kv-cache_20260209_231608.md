---
ver: rpa2
title: Sparse Attention across Multiple-context KV Cache
arxiv_id: '2508.11661'
source_url: https://arxiv.org/abs/2508.11661
tags:
- cache
- attention
- query
- caches
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SamKV, the first method for KV Cache sparsification
  across multiple contexts in RAG scenarios. SamKV addresses the challenge of efficiently
  serving large language models with long sequences by combining personalized query
  embedding, KV selection, and selective recomputation.
---

# Sparse Attention across Multiple-context KV Cache

## Quick Facts
- arXiv ID: 2508.11661
- Source URL: https://arxiv.org/abs/2508.11661
- Authors: Ziyi Cao; Qingyi Si; Jingbin Zhang; Bingquan Liu
- Reference count: 11
- Primary result: First method for KV Cache sparsification across multiple contexts in RAG scenarios, achieving 15% cache compression while maintaining accuracy

## Executive Summary
SamKV introduces a novel approach for efficient KV Cache sparsification in RAG scenarios by addressing the challenge of serving large language models with long sequences across multiple contexts. The method combines three key components: personalized query embedding with inter-document consensus, KV selection based on attention analysis, and selective recomputation of a subset of tokens to recover cross-attention. This comprehensive approach enables significant memory compression while maintaining model accuracy compared to full-recomputation baselines. The method is particularly effective for QA tasks in LongBench datasets, where it achieves comparable F1 scores while reducing GPU memory usage and computational overhead.

## Method Summary
SamKV is designed to address the computational and memory challenges of serving LLMs with long sequences in RAG scenarios by implementing a three-pronged approach. First, it generates context-specific query vectors through a personalized embedding mechanism that incorporates inter-document consensus, allowing the model to better understand relationships between multiple documents. Second, it performs KV selection by analyzing attention patterns to identify and retain only the most important key-value pairs, significantly reducing cache size. Third, it employs selective recomputation, where a carefully chosen subset of tokens is recomputed to recover cross-attention information that might be lost during sparsification. This combination allows SamKV to compress KV cache size to 15% of the original while maintaining comparable accuracy to full recomputation baselines.

## Key Results
- Compresses KV Cache size to 15% of original while maintaining accuracy compared to full-recomputation baselines
- Achieves comparable F1 scores on LongBench QA datasets while significantly reducing GPU memory usage and computational overhead
- Ablation study shows that both personalized bias and selective recomputation components contribute to improved performance

## Why This Works (Mechanism)
SamKV works by addressing the fundamental challenge of maintaining cross-attention information while significantly reducing memory footprint in multi-context RAG scenarios. The personalized query embedding with inter-document consensus allows the model to create context-aware representations that capture document relationships, while the attention-based KV selection identifies and retains only the most semantically important information. The selective recomputation phase then intelligently recovers any lost cross-attention signals by recomputing a minimal subset of tokens, ensuring that the compressed cache still provides sufficient information for accurate downstream task performance.

## Foundational Learning

**Key-Value Cache Sparsification**
- Why needed: Traditional KV caches store all attention keys and values, leading to quadratic memory growth with sequence length
- Quick check: Verify that cache size scales linearly rather than quadratically with input length

**Personalized Query Embedding**
- Why needed: Standard query vectors don't account for inter-document relationships in multi-context RAG scenarios
- Quick check: Ensure personalized embeddings can be generated efficiently without becoming a bottleneck

**Attention-based KV Selection**
- Why needed: Not all KV pairs contribute equally to model performance, allowing for intelligent pruning
- Quick check: Validate that important KV pairs are correctly identified through attention analysis

**Selective Recomputation**
- Why needed: Sparsification can lose critical cross-attention information that needs recovery
- Quick check: Confirm that recomputed tokens successfully restore necessary attention patterns

## Architecture Onboarding

**Component Map**
Personalized Query Generator -> KV Selector -> Selective Recomputation Engine -> Sparse Attention Module

**Critical Path**
The critical execution path involves generating personalized query embeddings, performing attention analysis for KV selection, creating the sparse cache, and executing the selective recomputation phase before final attention computation.

**Design Tradeoffs**
- Memory vs. Accuracy: 15% compression achieved with minimal accuracy loss
- Computational Overhead: Personalized embedding generation adds preprocessing time
- Complexity vs. Performance: Three-component approach increases implementation complexity but provides better results

**Failure Signatures**
- Cache underflow: Insufficient KV pairs retained leading to poor attention coverage
- Excessive recomputation: Too many tokens marked for recomputation negating memory savings
- Consensus failure: Poor inter-document relationship modeling in personalized embeddings

**First 3 Experiments**
1. Baseline comparison: Full recomputation vs. SamKV on LongBench QA tasks
2. Ablation study: Evaluate each component's contribution to overall performance
3. Memory analysis: Measure actual GPU memory usage across different sequence lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused primarily on QA tasks from LongBench, limiting generalizability to other RAG scenarios
- Memory bandwidth savings and compute overhead from personalized query embedding generation not fully quantified
- Performance with extremely long sequences (>32K tokens) and multi-tenant serving scenarios unexplored

## Confidence

**High Confidence**: Technical implementation of KV selection and selective recomputation mechanisms are well-described and technically sound. Ablation study results are reproducible.

**Medium Confidence**: F1 score comparisons with full recomputation baselines are convincing, but generalizability across different RAG use cases needs validation.

**Low Confidence**: Scalability claims for production environments lack comprehensive experimental validation, particularly regarding personalized query embedding overhead.

## Next Checks
1. Multi-domain Evaluation: Test SamKV performance across diverse RAG scenarios including code generation, medical literature review, and legal document analysis to validate generalization beyond LongBench QA tasks.

2. Memory Bandwidth Analysis: Measure the actual memory bandwidth savings and compute overhead introduced by the personalized query embedding generation and consensus computation phases in production GPU configurations.

3. Long-sequence Scalability: Evaluate SamKV with sequences exceeding 32K tokens and assess performance degradation patterns, particularly focusing on the effectiveness of selective recomputation as sequence length increases.