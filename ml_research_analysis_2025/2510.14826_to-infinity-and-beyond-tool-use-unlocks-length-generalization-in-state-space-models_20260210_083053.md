---
ver: rpa2
title: 'To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space
  Models'
arxiv_id: '2510.14826'
source_url: https://arxiv.org/abs/2510.14826
tags:
- length
- arxiv
- tool
- memory
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: State Space Models (SSMs) cannot solve long-form generation tasks
  due to their fixed memory size, limiting their performance compared to Transformers.
  This work shows that allowing SSMs interactive access to external tools overcomes
  this limitation, enabling length generalization on tasks like arithmetic, reasoning,
  and coding.
---

# To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models

## Quick Facts
- **arXiv ID**: 2510.14826
- **Source URL**: https://arxiv.org/abs/2510.14826
- **Reference count**: 40
- **Primary result**: Interactive tool-use enables SSMs to achieve length generalization on computationally tractable long-form tasks by augmenting fixed internal memory with external tools.

## Executive Summary
State Space Models (SSMs) like Mamba are efficient alternatives to Transformers but are fundamentally limited by fixed-size internal memory, preventing length generalization on long-form tasks. This paper demonstrates that allowing SSMs interactive access to external tools overcomes this limitation, enabling them to solve tasks like multi-digit arithmetic, logical reasoning, and code fixing at lengths far beyond their training distribution. The key insight is that interactive tool-use (multiple interleaved command-observation cycles) is both necessary and sufficient for length generalization, while single-turn approaches remain fundamentally limited. Experiments show tool-augmented SSMs match or exceed Transformer performance on large problems when trained on interactive tool-use trajectories.

## Method Summary
The method involves training SSMs on synthetically constructed tool-use trajectories that simulate algorithmic execution traces. Models use special tokens for thoughts, commands, and observations, interacting with external tools (pointer-based or search-based memory) through multiple turns. Training uses next-token prediction with teacher forcing, masking observation tokens from loss since they're generated by the oracle. The approach is evaluated on arithmetic (multi-digit addition/multiplication), reasoning (Tower of Hanoi, logical graphs), and code fixing tasks, comparing interactive vs single-turn tool use and SSMs vs Transformers.

## Key Results
- Interactive tool-augmented SSMs achieve 100% accuracy on 1,000-digit addition trained only on ≤5-digit examples
- Single-turn tool training fails on complex tasks while interactive training maintains accuracy on large codebases
- Mamba-1.4B trained on interactive SWE-agent trajectories matches Pythia-1.4B performance on code fixing, with SSMs excelling at larger problems
- No-CoT or no-tool variants show minimal length generalization, confirming both components are necessary

## Why This Works (Mechanism)

### Mechanism 1: External Memory Augmentation Bypasses Fixed-State Bottleneck
External tools provide unbounded storage that compensates for SSMs' fixed-size internal state. The model simulates a Turing machine using thoughts to track machine state, commands to move read/write heads, and external memory as the tape—allowing the effective output space to grow beyond |S| (internal state cardinality). Interactive multi-turn tool-use is necessary because single-turn approaches remain fundamentally limited by bounded internal state before generation.

### Mechanism 2: Algorithmic Trajectory Training Localizes Computation
Training on synthetically constructed tool-use trajectories that mirror algorithmic execution enables length generalization. Trajectories encode Turing machine traces—each step includes current state, symbol read, symbol written, and head movement. The model learns string-matching between observed (state, symbol) pairs and correct transitions, generalizing when all necessary pairs appear in training.

### Mechanism 3: Interactive Multi-Turn Tool-Use is Necessary and Sufficient
Interactive tool-use (multiple interleaved command-observation cycles) is both necessary and sufficient; single-turn tool-use remains fundamentally limited. In single-turn, the model's internal state before output generation is bounded by |S|, limiting possible outputs regardless of tool output. Interactive use allows state updates through external memory across turns, effectively growing representable outputs.

## Foundational Learning

- **State Space Models (SSMs) and Bounded Memory**
  - Why needed here: SSMs have fixed-size memory |S| regardless of sequence length, unlike Transformers with unbounded KV-cache
  - Quick check question: Why does a 1.4B parameter Mamba model have the same memory capacity whether processing 100 or 1,000,000 tokens?

- **Length Generalization**
  - Why needed here: The paper formally defines length generalization (Definition 2.3)—achieving low error on complexity n ≥ n₀ after training only on complexities up to n₀
  - Quick check question: If a model trained on ≤5-digit addition achieves 100% accuracy on 1,000-digit addition, what does this demonstrate about its learning?

- **ReAct Framework (Reasoning + Acting)**
  - Why needed here: The paper builds on ReAct for agent trajectories with interleaved thoughts (internal reasoning), commands (tool calls), and observations (tool outputs)
  - Quick check question: In the ReAct paradigm, what is the difference between a thought token, a command token, and an observation token?

## Architecture Onboarding

- **Component map**: Base SSM (Mamba-1.4B or Mamba-130M) -> Tool oracle O with memory state M_t and position index i_t -> Token taxonomy ([THINK]...[\THINK], [TOOL]...[\TOOL], [OBS]...[\OBS], output stream) -> Memory tool variants (pointer-based or search-based)

- **Critical path**: 1) Define task and construct synthetic trajectories simulating algorithm execution, 2) Train with next-token prediction, masking observation tokens from loss, 3) Evaluate exact trajectory match accuracy on problems with complexity n' > n (training max)

- **Design tradeoffs**: Pointer-based vs. search-based tools (structured vs. flexible), synthetic trajectories vs. agent distillation (perfect vs. noisy traces), training complexity n₀ (coverage vs. cost)

- **Failure signatures**: Single-turn tool training → high accuracy on small problems, collapse on large; no tool-use or no-CoT → minimal length generalization; high token accuracy but low sequence accuracy → error accumulation over long outputs

- **First 3 experiments**:
  1. Replicate multi-digit addition: Train Mamba-130M on ≤5-digit addition with pointer-based memory tool; evaluate on up to 1,000-digit
  2. Ablation on tool paradigm: Compare CoT-only, single-turn tool, and interactive tool on same arithmetic task
  3. Code fixing transfer: Finetune Mamba-1.4B and Pythia-1.4B on SWE-agent trajectories; evaluate on larger codebases

## Open Questions the Paper Calls Out

- **Question**: Does the theoretical guarantee of length generalization hold for standard gradient-based learning algorithms, or is it restricted to the "simple" string-matching learning algorithm analyzed in the proofs?
- **Basis**: Section 2.4 footnote notes the algorithm analyzed is simple string-matching and believes similar results can be obtained for more natural algorithms at the cost of more involved analysis
- **Why unresolved**: The mathematical proof is currently limited to a specific learning paradigm; it remains unproven whether gradient descent dynamics would yield the same guarantees
- **What evidence would resolve it**: A theoretical extension covering gradient descent, or empirical evidence showing gradient descent consistently matches string-matching performance

## Limitations

- Reliance on synthetically constructed training data that perfectly mirrors algorithmic execution traces, which may not generalize to noisier real-world tasks
- Limited scalability analysis—success demonstrated on 1,000 digits or 16-file codebases but unclear how approaches perform on problems requiring terabytes of memory
- Methodological ambiguities in Transformer comparisons, including potential confounding factors from pretraining data quality and architectural differences

## Confidence

- **High confidence**: Theoretical framework proving necessity and sufficiency of interactive tool-use for length generalization (Theorem 2.2)
- **Medium confidence**: Empirical demonstrations on synthetic arithmetic tasks
- **Medium confidence**: Code fixing experiments with SWE-agent trajectories

## Next Checks

1. **Scaling law analysis**: Systematically vary training complexity n₀ and measure length generalization curves to identify diminishing returns and failure points
2. **Real-world trajectory quality ablation**: Compare performance when training on synthetic trajectories versus distilled agent trajectories versus noisy human demonstrations for the same task
3. **Memory-bounded stress test**: Design tasks requiring external memory that provably exceeds what single-turn approaches can encode to validate theoretical claims about fundamental limitations