---
ver: rpa2
title: How Well Do LLMs Understand Tunisian Arabic?
arxiv_id: '2511.16683'
source_url: https://arxiv.org/abs/2511.16683
tags:
- tunisian
- arabic
- dataset
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well large language models (LLMs) understand
  Tunisian Arabic, a low-resource dialect spoken by over twelve million people. The
  research introduces a manually curated dataset of 100 examples, annotated for transliteration
  (Tunizi to Arabic script), translation (Tunisian Arabic to English), and sentiment
  classification (Positive, Negative, Neutral).
---

# How Well Do LLMs Understand Tunisian Arabic?

## Quick Facts
- arXiv ID: 2511.16683
- Source URL: https://arxiv.org/abs/2511.16683
- Reference count: 0
- Primary result: Proprietary LLMs (Gemini 2.5 Flash, Claude Sonnet 4.5, GPT-4o Mini) significantly outperform open-source models on Tunisian Arabic tasks, with CER as low as 0.15 for transliteration.

## Executive Summary
This study evaluates large language models' ability to process Tunisian Arabic, a low-resource dialect spoken by over twelve million people. The research introduces a manually curated dataset of 100 examples annotated for transliteration, translation, and sentiment classification. Seven state-of-the-art LLMs were benchmarked, revealing significant performance disparities: proprietary models like Gemini 2.5 Flash achieved strong results (CER 0.15, METEOR 0.45), while open-source models like Mistral and Qwen performed poorly. The findings highlight the challenges of modeling low-resource dialects and underscore the need for expanded datasets and tailored benchmarks to improve linguistic inclusivity in AI systems.

## Method Summary
The study benchmarks seven LLMs on three Tunisian Arabic tasks using a manually curated dataset of 100 social media examples. Each example includes Tunizi text, Arabic script, English translation, and sentiment label. Models were evaluated zero-shot on transliteration (Tunizi to Arabic script), translation (Tunisian Arabic to English), and sentiment classification (Positive/Negative/Neutral). Performance was measured using CER, Levenshtein distance, LCS for transliteration; BLEU, METEOR, BERTScore for translation; and Accuracy, F1, Kappa, MCC for sentiment. All inference was run on November 9, 2025, with task-specific prompts and multi-metric evaluation.

## Key Results
- Gemini 2.5 Flash achieved the best overall performance with CER 0.15 for transliteration and METEOR 0.45 for translation
- GPT-4o Mini reached sentiment classification F1 of 0.69 for positive sentiment
- Open-source models (Mistral, Qwen) lagged significantly, with Qwen showing CER 3.16 and METEOR 0.09

## Why This Works (Mechanism)

### Mechanism 1: Cross-Script Phonetic Generalization
- Claim: Proprietary LLMs can transliterate Latin-script Tunisian Arabic (Tunizi) to Arabic script by leveraging phonetic correspondences learned from multilingual code-switched content during pretraining.
- Mechanism: Models map phoneme sequences across scripts without dialect-specific fine-tuning by recognizing patterns from Arabic-French-English mixed text and Romanized Arabic seen in web-scale corpora.
- Core assumption: Training corpora contain sufficient code-switched Arabic content for the model to internalize Latin-to-Arabic phoneme correspondences.
- Evidence anchors:
  - [abstract] "transliteration CERs as low as 0.15 for top models"
  - [section] "Gemini 2.5 Flash achieves the best overall performance with a CER of 0.15... capturing fine-grained phonetic details while maintaining orthographic consistency"
  - [corpus] LinTO dataset and TEDxTN corpus confirm existence of speech/text resources for Tunisian Arabic that may appear in training data
- Break condition: When model training data contains minimal North African dialectal content or Latin-script Arabic representations, phonetic mapping fails (observed: Mistral CER=1.29, Qwen CER=3.16).

### Mechanism 2: Semantic Transfer via Dialectal Cognates
- Claim: Translation quality for low-resource dialects depends on semantic embedding overlap between the target dialect and related high-resource languages in the model's representation space.
- Mechanism: Tunisian Arabic shares vocabulary with Modern Standard Arabic, French, and Turkish. Models with strong multilingual representations can activate semantic clusters via cognates or loanwords, enabling translation even without direct dialect exposure.
- Core assumption: Semantic representations transfer from Modern Standard Arabic and French to Tunisian Arabic variants.
- Evidence anchors:
  - [abstract] "translation METEOR scores around 0.45"
  - [section] "Gemini 2.5 Flash... METEOR score of 0.45 and a BERTScore F1 of 0.91, confirming its superior ability to preserve both lexical and semantic alignment"
  - [corpus] Weak direct evidence; neighboring papers focus on speech recognition, not semantic transfer for Tunisian dialect
- Break condition: When dialectal expressions use highly localized idioms with no cognates in training languages, semantic alignment degrades (observed: Qwen METEOR=0.09, BLEU=0.02).

### Mechanism 3: Asymmetric Class-Specific Pattern Recognition
- Claim: Sentiment classification accuracy varies by class because models develop stronger recognition for sentiment categories with more distinctive or frequent linguistic markers in training data.
- Mechanism: Positive sentiment often correlates with explicit marker words; negative sentiment may involve negation patterns; neutral sentiment requires context understanding. Models develop asymmetric proficiency based on marker salience.
- Core assumption: Sentiment expressions in Tunisian Arabic share structural patterns with other Arabic dialects present in training data.
- Evidence anchors:
  - [abstract] "sentiment classification accuracy up to 0.60"
  - [section] "GPT-4o Mini achieves... F1 for Positive sentiment (0.69)... Claude Sonnet 4.5 demonstrates competitive performance with high F1 for Neutral sentiment (0.64)"
  - [corpus] No corpus neighbors directly address sentiment classification for Tunisian Arabic
- Break condition: When sentiment is expressed through sarcasm, culturally-specific idioms, or implicit context absent from training data, classification approaches random (observed: Mistral Kappa=0.10).

## Foundational Learning

- **Character Error Rate (CER) and Edit Distance**
  - Why needed here: Transliteration quality is evaluated via CER and Levenshtein distance; understanding these metrics is essential for interpreting Table 2 results.
  - Quick check question: If ground truth is "كيفاش" (4 chars) and prediction is "كيفش" (3 chars, one deletion), what is the CER?

- **Code-Switching and Dialectal Arabic**
  - Why needed here: Tunisian Arabic mixes Arabic, French, English, Turkish, and Amazigh; models must handle hybrid lexical forms without standard orthography.
  - Quick check question: Why does the lack of standardized spelling for Tunisian words increase difficulty for tokenization?

- **Low-Resource Evaluation Limitations**
  - Why needed here: The 100-sample dataset limits statistical power; results are "indicative rather than statistically conclusive."
  - Quick check question: Why might BLEU scores underreport translation quality for dialectal languages compared to semantic metrics like BERTScore?

## Architecture Onboarding

- **Component map:**
  - Data collection (social media) -> Manual filtering -> Anonymization -> Triple annotation (Tunizi, Arabic, English, sentiment) -> Task-specific prompts -> Zero-shot LLM inference -> Multi-metric computation

- **Critical path:**
  1. Curate non-indexed social media data to avoid memorization contamination
  2. Native-speaker manual annotation for ground truth
  3. Standardized prompt design for all models
  4. Multi-metric evaluation across three distinct tasks

- **Design tradeoffs:**
  - 100 samples enables rapid iteration but limits statistical conclusiveness
  - Single annotator ensures consistency but introduces potential annotator bias
  - Zero-shot evaluation tests generalization but doesn't measure few-shot potential
  - Non-indexed data prevents contamination but may limit dialectal diversity

- **Failure signatures:**
  - CER >1.0: Model produces fundamentally wrong character sequences or wrong script entirely
  - METEOR <0.15 with BLEU near 0: Semantic drift, hallucination, or complete mistranslation
  - Cohen's Kappa <0.2: Classification barely above random baseline
  - Normalization to Modern Standard Arabic: Model refuses dialectal forms (observed in mid-tier models)

- **First 3 experiments:**
  1. Baseline replication: Run transliteration task across all 7 models with paper's exact prompts; verify CER distribution matches reported results (Gemini ~0.15, Qwen >3.0).
  2. Failure mode analysis: For worst performer, manually categorize 20 errors into: wrong script, MSA normalization, partial transliteration, or complete failure.
  3. Few-shot sensitivity test: Add 3 exemplars to sentiment classification prompt; measure accuracy delta to quantify in-context learning potential for Tunisian Arabic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the causal factors driving Gemini and GPT's superior performance on Tunisian Arabic compared to open-source models?
- Basis in paper: [explicit] The authors state: "While the exact reasons behind this dominance remain multifaceted and partly speculative... Future work could explore these factors in a controlled setting, potentially shedding light on the relative impact of data, model architecture, and human knowledge on dialectal performance."
- Why unresolved: The paper only speculates about training data diversity, regional expertise, and infrastructure advantages; no controlled experiments isolate these variables.
- What evidence would resolve it: Ablation studies varying training corpus composition, architecture components, and fine-tuning strategies across models while holding other factors constant.

### Open Question 2
- Question: Would fine-tuning open-source models (e.g., Mistral, Qwen) on Tunisian Arabic data close the performance gap with proprietary models?
- Basis in paper: [inferred] The authors note that open-source models "still require targeted fine-tuning with dialect-specific parallel data to reach competitive translation fidelity," but only evaluate zero-shot performance.
- Why unresolved: All experiments use off-the-shelf models without fine-tuning; no experiments test whether adaptation improves low-resource dialect handling.
- What evidence would resolve it: Fine-tune Mistral and Qwen on Tunisian parallel corpora and re-evaluate on the three tasks, comparing gains against proprietary baselines.

### Open Question 3
- Question: How does single-annotator labeling affect dataset reliability and model evaluation validity for subjective tasks like sentiment classification?
- Basis in paper: [inferred] The paper states: "The annotation process was carried out manually by a single annotator (the author), ensuring consistency and high quality." No inter-annotator agreement metrics are reported.
- Why unresolved: Sentiment labeling involves subjectivity; without multiple annotators, systematic bias or annotation errors cannot be detected or quantified.
- What evidence would resolve it: Have 2-3 additional native Tunisian speakers annotate the dataset independently; report inter-annotator agreement (e.g., Cohen's Kappa) and analyze discrepancies.

## Limitations

- Small dataset size (100 samples) limits statistical power and generalizability of results
- Single annotator ground truth eliminates inter-annotator agreement assessment and may embed individual linguistic preferences
- Zero-shot evaluation framework cannot capture models' few-shot learning capabilities that might significantly improve performance for low-resource dialects

## Confidence

- **High Confidence:** The relative ranking of model performance across tasks (Gemini 2.5 Flash > Claude Sonnet 4.5 ≈ GPT-4o Mini >> Mistral/Qwen) is robust given the multi-metric evaluation framework and clear performance gaps between model tiers.
- **Medium Confidence:** The specific metric values (CER=0.15, METEOR=0.45, accuracy=0.60) should be interpreted cautiously due to dataset size limitations.
- **Low Confidence:** The assumption that models learn transliteration through phonetic generalization from multilingual code-switched content is speculative and lacks direct verification.

## Next Checks

1. **Dataset Expansion Validation:** Replicate the evaluation using progressively larger random subsets (25, 50, 75, 100 samples) to quantify metric stability and identify the sample size threshold where performance rankings stabilize.

2. **Cross-Annotator Agreement Study:** Re-annotate 20 randomly selected samples with multiple native speakers to measure inter-annotator agreement for transliteration, translation, and sentiment tasks.

3. **Few-Shot Performance Benchmark:** Evaluate all models with 3-5 exemplars in prompts for each task to measure in-context learning capabilities and compare zero-shot versus few-shot performance.