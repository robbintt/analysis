---
ver: rpa2
title: Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning
arxiv_id: '2509.25534'
source_url: https://arxiv.org/abs/2509.25534
tags:
- vous
- training
- rappel
- wang
- avez
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Rewarding Rubric-Based Reinforcement
  Learning for Open-Ended Reasoning, a lightweight framework that uses a model's own
  rubric-based scoring to train reasoning capabilities, eliminating the need for separate
  reward models. The approach substantially reduces resource consumption and training
  time while improving performance on hard open-ended tasks.
---

# Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning

## Quick Facts
- arXiv ID: 2509.25534
- Source URL: https://arxiv.org/abs/2509.25534
- Reference count: 40
- Qwen3-32B trained on 4000 HealthBench Easy samples with self-rewarding achieves performance exceeding GPT-5 on HealthBench Hard

## Executive Summary
This paper introduces Self-Rewarding Rubric-Based Reinforcement Learning (SRRRL), a lightweight framework that uses a model's own rubric-based scoring to train reasoning capabilities, eliminating the need for separate reward models. The approach substantially reduces resource consumption and training time while improving performance on hard open-ended tasks. On Qwen3-32B, training with only 4000 easy HealthBench samples using self-rewarding signals achieves performance exceeding GPT-5 on the HealthBench Hard subset, demonstrating the effectiveness of transparent rubric-based evaluation. Incorporating a small amount of teacher-graded data further benefits weaker models like Qwen3-8B, though stronger models achieve best results with purely self-rewarding training.

## Method Summary
SRRRL extends GRPO to open-ended reasoning by using the policy model itself as a rubric-based grader, eliminating the need for a separate reward model. The method generates G responses per prompt, then evaluates each against task-specific rubrics using the same model in a generative judging mode. Each rubric is scored independently, creating a normalized composite reward. This approach reduces computational overhead by ~30% while maintaining or improving performance, and preserves the model's grading ability while inducing longer, more informative responses through a virtuous cycle of quality improvement.

## Key Results
- Qwen3-32B trained on 4000 HealthBench Easy samples with self-rewarding exceeds GPT-5 performance on HealthBench Hard
- Self-rewarding reduces step time by ~25-35% and reward computation time by ~50% compared to external GRM
- Grading ability (HealthBench Meta MF1) slightly improves during self-rewarding training, not degrades
- Qwen3-8B requires small amount of teacher-graded data to avoid training crashes, while Qwen3-32B succeeds with pure self-rewarding

## Why This Works (Mechanism)

### Mechanism 1: Rubric-Grounded Self-Reward Signal
Using the policy model itself as a rubric-based grader provides effective reward signals for open-ended reasoning tasks. The model generates responses and evaluates them against task-specific rubrics, creating a normalized composite reward. This assumes the model's grading ability remains sufficiently aligned with target evaluation criteria throughout training. Evidence shows slight MF1 improvement, but degradation would break this mechanism.

### Mechanism 2: Virtuous Cycle of Response Length and Grading Quality
Self-rewarding training induces a feedback loop where longer responses improve grading ability, which in turn provides better rewards. As RL progresses, response length increases and HealthBench Meta MF1 improves synchronously. This creates reinforcement: better responses → better self-grading → more precise rewards → further improvement. If responses become verbose without quality, the cycle breaks.

### Mechanism 3: Resource-Efficient On-Policy Training via Unified Actor-Grader
Eliminating a separate reward model reduces computational overhead while maintaining or improving performance. The policy model serves as both actor and grader, removing the need for additional inference GPUs. This reduces step time by ~25-35% and reward computation time by ~50%. Weaker models may need mixed data if self-grading becomes unreliable.

## Foundational Learning

- **Reinforcement Learning from Verifiable Rewards (RLVR)**: SRRRL extends RLVR to open-ended domains by making rewards verifiable through rubrics. Quick check: Can you explain how rubrics make open-ended task rewards "verifiable" compared to scalar human preferences?

- **Group Relative Policy Optimization (GRPO)**: SRRRL builds on GRPO, modifying the reward source but retaining advantage estimation via group normalization. Quick check: In GRPO, how is the advantage Âi,t computed from group rewards, and why does this remove the need for a value function?

- **LLM-as-a-Judge with Rubric-Based Scoring**: HealthBench evaluation and SRRRL rewards both depend on structured rubric judgments rather than holistic scores. Quick check: What are the trade-offs between point-wise rubric scoring vs. pairwise comparison for open-ended evaluation?

## Architecture Onboarding

- **Component map**: Prompt + Rubric Dataset -> Policy Model (πθ) -> Rollout Engine -> Self-Grading Module -> Advantage Computer -> Optimizer
- **Critical path**: 1) Sample batch of (prompt, rubrics) from D 2) Generate G responses per prompt using πθold 3) Query πθold to judge each rubric criterion 4) Compute composite reward S for each response 5) Compute group-relative advantages 6) Update policy via gradient ascent
- **Design tradeoffs**: Self-grading saves ~50% reward time but risks reward hacking; point-wise rubric rewards provide fine-grained signals vs. simpler pairwise comparisons; KL penalty omission speeds training but may increase drift risk
- **Failure signatures**: Grading MF1 degradation indicates reward quality decline; reward hacking if models game rubrics; training instability with weak models like Qwen3-8B; communication quality decline as outputs lengthen
- **First 3 experiments**: 1) Validate self-grading stability: Train Qwen3-32B monitoring HealthBench Hard score and Meta MF1 2) Compare to external GRM baseline: Train with external grader vs self-grading 3) Test weaker model rescue: Train Qwen3-8B with self-rewarding vs mixed data

## Open Questions the Paper Calls Out

**Cross-domain generalization**: Does SRRRL generalize to open-ended reasoning tasks beyond the medical domain? All experiments were on HealthBench; future work should explore broader domains.

**Grading ability mechanism**: Why does self-rewarding training improve the model's own grading ability (HealthBench Meta MF1) despite no explicit grading supervision? The paper observes this phenomenon but provides no mechanistic explanation.

**LLM-generated rubrics**: Can LLM-generated rubrics match or exceed expert-curated rubrics for SRRRL training? The paper suggests this as promising direction but only used expert-annotated HealthBench rubrics.

## Limitations
- Experiments restricted to HealthBench in medical domain; generalization to other open-ended tasks untested
- Response length increases may degrade communication clarity without quantitative metrics beyond HealthBench evaluation
- Critical performance threshold exists where self-grading becomes reliable; weaker models like Qwen3-8B require teacher data to avoid training crashes

## Confidence
- **High confidence**: Resource efficiency claims well-supported by concrete timing data (30% reduction in step time, 50% reduction in reward computation)
- **Medium confidence**: Performance improvements on HealthBench Hard compelling but domain-specific; generalizability to other tasks untested
- **Low confidence**: "Virtuous cycle" claim between response length and grading quality is plausible but not rigorously proven; shows correlation without establishing causation

## Next Checks
1. **Grading stability across domains**: Apply SRRRL to a different open-ended reasoning task and monitor grading MF1 throughout training to verify self-grading reliability beyond healthcare

2. **Teacher data dosage response**: Systematically vary proportion of teacher-graded data (0%, 10%, 25%, 50%) when training Qwen3-8B to characterize minimum effective dosage and identify performance threshold where self-grading becomes reliable

3. **Communication quality metrics**: Add quantitative measures for response conciseness and coherence (e.g., BERTScore, readability indices) to track trade-off between performance gains and communication quality degradation as outputs lengthen