---
ver: rpa2
title: Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio
arxiv_id: '2501.11378'
source_url: https://arxiv.org/abs/2501.11378
tags:
- hallucinations
- speech
- audio
- whisper
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Whisper ASR hallucinations induced by non-speech
  audio. The authors first conduct experiments on a large dataset of non-speech sounds
  to generate an exhaustive list of hallucinations, then analyze the patterns of hallucinations
  in terms of sound types and durations.
---

# Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio

## Quick Facts
- arXiv ID: 2501.11378
- Source URL: https://arxiv.org/abs/2501.11378
- Authors: Mateusz Barański; Jan Jasiński; Julitta Bartolewska; Stanisław Kacprzak; Marcin Witkowski; Konrad Kowalczyk
- Reference count: 26
- Key outcome: Post-processing with Bag of Hallucinations (BoH) + delooping reduces WER from ~90% to ~20% on speech-augmented with non-speech sounds, but cannot fully eliminate hallucinations.

## Executive Summary
This paper systematically investigates how Whisper ASR generates hallucinations when processing non-speech audio. The authors conduct extensive experiments across 25 sound categories, revealing that Whisper frequently produces coherent but irrelevant text outputs instead of silence. They identify that specific hallucinations correlate with training data artifacts (particularly video transcription credits) and demonstrate that both audio duration and positioning significantly impact hallucination rates. To address this, they create a Bag of Hallucinations (BoH) by filtering common hallucinations using n-gram language models and frequency thresholds, then propose a post-processing pipeline combining BoH removal with delooping. Experiments show this approach, especially when combined with Voice Activity Detection (VAD), significantly reduces Word Error Rate (WER) from ~90% to ~20% on speech-augmented non-speech audio, though complete elimination of hallucinations remains elusive.

## Method Summary
The authors first run Whisper on a large dataset of 25 non-speech sound categories to exhaustively catalog hallucinations. They filter these using n-gram language model log probability (< -10) and frequency threshold (> 4) to create a Bag of Hallucinations (BoH). For inference on speech-augmented non-speech audio, they apply a pipeline: VAD pre-filtering (SileroVAD) → Whisper transcription → delooping to remove repetitive outputs → Aho-Corasick string matching to remove BoH entries. They also implement forced phoneme alignment using wav2vec2.0 as an optional verification layer. The approach is evaluated on multiple datasets showing significant WER reduction while reducing hallucination detection rates from 13-18% to near zero.

## Key Results
- Whisper generates coherent hallucinations (e.g., "thanks for watching," "transcript emily beynon") when processing pure non-speech audio instead of outputting silence
- Hallucination rates increase noticeably when audio duration exceeds 30 seconds, matching Whisper's internal segment allocation
- BoH filtering combined with delooping reduces WER from ~90% to ~20% on speech-augmented non-speech audio
- VAD + BoH + delooping achieves near-zero hallucination detection rates (0.2%) while maintaining reasonable WER (~9%)

## Why This Works (Mechanism)

### Mechanism 1
Whisper hallucinations are strongly influenced by training data distribution, particularly video transcription artifacts. When Whisper receives acoustically ambiguous input (non-speech audio), the model defaults to high-probability sequences from its training distribution rather than outputting silence or uncertainty. The frequency of specific hallucinated phrases reflects their prevalence in Whisper's training corpus, as evidenced by common hallucinations like "thanks for watching" and "transcript emily beynon."

### Mechanism 2
Audio segment duration and positioning relative to Whisper's 30-second decoding window directly modulates hallucination probability. Whisper allocates 30-second segments for decoding, and when audio exceeds this threshold, additional heuristics are invoked that appear to increase hallucination likelihood. Very short segments may lack sufficient context for the model's silence-detection mechanisms, contributing to increased hallucination rates at duration boundaries.

### Mechanism 3
Post-hoc filtering using a curated Bag of Hallucinations (BoH) combined with delooping reduces WER by intercepting predictable failure modes. BoH is constructed by collecting hallucinations from non-speech inference, then filtering using n-gram language model log probability (< -10) and occurrence frequency (> 4). During inference, Aho-Corasick string matching detects and removes BoH entries while delooping handles repetitive outputs separately, significantly improving transcription accuracy.

## Foundational Learning

- **ASR Hallucinations vs. Phonetic Errors**: The paper explicitly distinguishes hallucinations (no semantic/phonetic connection to input) from mishearing errors. Understanding this distinction is essential for interpreting the BoH approach. Quick check: If Whisper outputs "the sky" when the audio contains "this guy," is this a hallucination per the paper's definition?

- **Whisper's 30-Second Segmentation**: The duration-based hallucination patterns directly reference Whisper's internal segment allocation. Engineers debugging hallucination rates need to understand this architectural constraint. Quick check: What happens when audio input exceeds 30 seconds in Whisper's processing pipeline?

- **N-gram Language Model Filtering**: BoH construction uses log probability from an external n-gram model to filter out common English phrases. Engineers must understand why this prevents false positives. Quick check: Why would filtering out phrases with log probability > -10 reduce false positive removals?

## Architecture Onboarding

- Component map:
  ```
  Non-speech audio dataset → Whisper inference → Raw hallucination list → N-gram LM filtering + frequency threshold → Bag of Hallucinations (BoH)
                                                                                   ↓
  Speech + non-speech audio → Whisper → Delooping → Aho-Corasick BoH matching → Clean transcription
                                                                                   ↑
  Optional: SileroVAD pre-filtering
  ```

- Critical path:
  1. Collect hallucinations from pure non-speech inference across diverse sound types
  2. Filter to create BoH (log p < -10, occurrences > 4)
  3. For production: VAD pre-process → Whisper → deloop → BoH removal
  4. Forced phoneme alignment (wav2vec2.0) available as optional confirmation layer

- Design tradeoffs:
  - BoH specificity vs. recall: Stricter filtering (lower log p threshold) reduces false positives but may miss hallucinations
  - VAD aggressiveness: SileroVAD nearly eliminates hallucinations (0.2% detection rate) but may clip valid speech
  - Computational overhead: Forced alignment adds cost; deloop+BoH is lightweight post-processing

- Failure signatures:
  - High looping percentage (Table I: up to 18.5% for original files) indicates delooping not applied or insufficient
  - Hallucinations at 30s+ durations suggest VAD not used or segment boundary issues
  - Offensive/violent outputs (e.g., profanity) indicate BoH incomplete—manual review required
  - WER > 100% (Table VII, non-processed baseline) signals severe hallucination cascades

- First 3 experiments:
  1. Replicate Experiment 1 on your target audio domain: Run Whisper on domain-specific non-speech audio to identify whether BoH generalizes or requires customization.
  2. Test BoH false positive rate: Apply BoH removal to clean speech transcriptions; measure deletion rate of valid content.
  3. Ablate VAD + BoH combination: Compare SileroVAD-only, BoH-only, and combined approaches on your specific SNR conditions to determine optimal configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism causing Whisper to map acoustically diverse non-speech inputs to a limited set of specific text hallucinations?
- Basis in paper: The study identifies that a small subset of hallucinations accounts for the majority of errors (e.g., "Subtitle credits") but finds "only a limited correlation... between audio content and hallucination."
- Why unresolved: The research characterizes the external behavior (transcription outputs) but does not analyze the internal model weights or activations causing acoustically distinct inputs to converge on identical text.

### Open Question 2
- Question: What are the optimal filtering criteria for the Bag of Hallucinations (BoH) to minimize the removal of valid speech (false positives)?
- Basis in paper: The authors apply specific heuristic criteria (n-gram log probability < -10, occurrence > 4) but explicitly state that "in general, any other filtration criteria could be used."
- Why unresolved: The paper validates the effectiveness of one specific configuration but does not perform a sensitivity analysis on the thresholds or evaluate the false positive rate on clean, non-hallucinated speech.

### Open Question 3
- Question: Is it possible to completely eliminate non-speech induced hallucinations in Whisper using strictly pre-processing (VAD) or post-processing techniques?
- Basis in paper: The paper concludes that while the proposed methods reduce Word Error Rate, "none of these approaches can be considered a complete solution that fully protects against errors of this kind."
- Why unresolved: The best configuration (SileroVAD + delooping + BoH) significantly reduces hallucinations but does not reduce the detection rate to absolute zero, indicating the model is still vulnerable to non-speech input.

## Limitations

- **Generalizability of Hallucination Patterns**: The identified patterns may not transfer to real-world scenarios where non-speech sounds co-occur with speech in varying SNR conditions.
- **Distribution Shift in Non-speech Categories**: The hallucination patterns for underrepresented sound categories in the experimental dataset may not generalize well to real-world environments.
- **Temporal Boundary Effects**: The causal mechanism linking 30-second segmentation to hallucination intensification is not fully established—correlation does not definitively prove causation.

## Confidence

**High Confidence**:
- The existence of systematic hallucinations induced by non-speech audio
- The effectiveness of the BoH + delooping approach in reducing WER when combined with VAD
- The correlation between audio duration and hallucination rates

**Medium Confidence**:
- The claim that specific hallucination phrases reflect training data distribution
- The assertion that the BoH approach cannot fully eliminate hallucinations
- The mechanism linking 30-second segmentation to hallucination intensification

**Low Confidence**:
- The generalizability of hallucination patterns across all possible non-speech sound types
- The complete transferability of BoH from pure non-speech to speech-augmented scenarios
- The relative contribution of VAD vs. BoH filtering to overall performance gains

## Next Checks

1. **Domain Transfer Validation**: Apply the BoH approach to real-world datasets with naturally occurring speech + non-speech mixtures (e.g., meeting recordings, YouTube videos, field recordings). Measure hallucination rates before and after BoH filtering.

2. **Ablation Study on Duration Effects**: Systematically test the 30-second boundary hypothesis by creating synthetic audio files with identical content but varying durations (just under 30s, just over 30s, multiples of 30s). Control for sound type, SNR, and speech content.

3. **False Positive Analysis of BoH Filtering**: Conduct a comprehensive evaluation of BoH removal on clean speech transcriptions across multiple domains. Calculate the false positive rate and develop metrics for acceptable filtering thresholds.