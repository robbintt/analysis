---
ver: rpa2
title: Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful
  without Hint Verbalization
arxiv_id: '2512.23032'
source_url: https://arxiv.org/abs/2512.23032
tags:
- hint
- faithful
- professor
- black
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the reliability of the Biasing Features\
  \ metric for evaluating CoT faithfulness. The authors argue that this metric conflates\
  \ unfaithfulness with incompleteness\u2014the necessary compression of a model\u2019\
  s distributed computation into a linear narrative."
---

# Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization

## Quick Facts
- arXiv ID: 2512.23032
- Source URL: https://arxiv.org/abs/2512.23032
- Reference count: 40
- Key outcome: The paper challenges Biasing Features as a faithfulness metric, showing that many non-verbalizing CoTs are actually incomplete rather than unfaithful, with causal mediation analysis demonstrating that CoTs can causally influence predictions even without explicit hint verbalization.

## Executive Summary
This paper challenges the reliability of the Biasing Features metric for evaluating CoT faithfulness. The authors argue that this metric conflates unfaithfulness with incompleteness—the necessary compression of a model's distributed computation into a linear narrative. Using datasets like OpenbookQA, StrategyQA, and ARC-Easy with Llama-3 and Gemma-3 models, they show that CoTs labeled unfaithful by Biasing Features are often faithful under alternative metrics like Filler Tokens and FUR, with over 50% of flagged CoTs judged faithful by other measures. They introduce faithful@k, showing that larger inference-time budgets increase hint verbalization rates up to 90%, suggesting many "unfaithful" CoTs are incomplete rather than misleading. Causal Mediation Analysis further demonstrates that non-verbalized hints can still causally influence predictions via the CoT. The authors caution against relying solely on hint-based evaluations and advocate for a broader interpretability toolkit.

## Method Summary
The authors evaluate CoT faithfulness using multiple complementary methods: Biasing Features (hint verbalization detection), Filler Tokens (checking if replacing CoT with filler tokens changes predictions), faithful@k (sampling multiple CoTs to detect verbalization), FUR (First-Uncommitted Restore for agreement-based filtering), and Causal Mediation Analysis (decomposing prediction changes into direct and indirect effects via CoT). They test these methods across OpenbookQA, StrategyQA, and ARC-Easy using Llama-3 and Gemma-3 models with various hint types including Professor, Metadata, and Black Squares hints. The experimental design includes both greedy decoding for baseline comparison and sampling-based approaches to reveal incompleteness.

## Key Results
- Biasing Features flags over 50% of non-verbalizing CoTs as unfaithful, but Filler Tokens and FUR show most are actually faithful
- faithful@k shows verbalization rates increase from ~30% (k=1) to ~90% (k=16), indicating incompleteness rather than unfaithfulness
- Causal Mediation Analysis reveals non-zero Natural Indirect Effects even for non-verbalizing CoTs, proving CoTs causally mediate predictions
- Different hint types show varying patterns: Professor hints show high incompleteness, while Metadata/Black Squares hints show genuine unfaithfulness

## Why This Works (Mechanism)

### Mechanism 1: Causal Mediation Through Non-Verbalizing CoTs
- Claim: CoTs can causally mediate hint influence on predictions even without explicit hint verbalization.
- Mechanism: When a hint is injected, it alters CoT generation (the mediator), which then influences the final prediction. The Natural Indirect Effect (NIE) quantifies this pathway. If CoTs were purely post-hoc rationalization, NIE would be near zero.
- Core assumption: Causal mediation via CoT indicates the CoT participates in the decision process, not merely justifies it.
- Evidence anchors:
  - [abstract] "Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT."
  - [section 6] "In Figure 7, all NIE confidence intervals exclude zero, indicating that CoTs generated under hinted inputs have a significant causal effect on predictions even when the hint is not explicitly verbalized."
  - [corpus] Related work on CoT monitorability (arXiv:2511.08525) supports that reasoning traces can serve as meaningful intermediate signals.
- Break condition: If NIE confidence intervals include zero across tasks and models, mediation mechanism fails; CoTs would be post-hoc only.

### Mechanism 2: Incompleteness as Compression Artifact
- Claim: Non-verbalization of hints often reflects token-budget constraints (incompleteness), not deliberate omission (unfaithfulness).
- Mechanism: Transformer computation is distributed and parallel. Mapping this to a sequential natural language narrative requires lossy compression. Increasing inference budget (sampling multiple CoTs) raises the probability that at least one verbalizes the hint.
- Core assumption: If non-verbalization were genuine unfaithfulness, faithful@k would remain flat as k increases.
- Evidence anchors:
  - [abstract] "larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits."
  - [section 5] "The steady upward trend as k increases, together with the large gap between faithful@1 and faithful@16, suggests that a substantial portion of the unfaithfulness attributed to CoTs is consistent with incompleteness."
  - [corpus] Limited corpus support; related work on reasoning effectiveness (arXiv:2510.09388) focuses on navigation, not compression dynamics.
- Break condition: If faithful@k remains flat across k values for all hint types, the incompleteness explanation fails.

### Mechanism 3: Probability Redistribution Via CoT-Mediated Suppression
- Claim: CoTs influence predictions not only by boosting the hinted option but by suppressing alternatives.
- Mechanism: Hint-induced CoTs can bypass reasoning paths that would otherwise support the default prediction, redistributing probability mass away from non-hinted options.
- Core assumption: Genuine causal mediation should produce measurable indirect effects on the full output distribution, not just the target token.
- Evidence anchors:
  - [section 6] "CoTs can shift predictions by decreasing the probability of non-hinted options, not only by increasing the hinted option."
  - [section 6, Figure 8] "The NIE confidence intervals remain non-zero, while some NDEs are not significantly different from zero... more instances where the indirect effect is larger in magnitude than the direct effect when reducing the probability of non-hinted options."
  - [corpus] No direct corpus support for this specific redistribution mechanism.
- Break condition: If indirect effects on non-hinted option probabilities are uniformly zero or negligible, suppression mechanism fails.

## Foundational Learning

### Concept: Causal Mediation Analysis
- Why needed here: Distinguishes whether CoTs are intermediate causal steps (NIE > 0) or post-hoc justifications (NIE ≈ 0). Essential for interpreting Figures 7-8 and understanding the paper's core claim.
- Quick check question: If adding a hint changes the prediction, but corrupting the CoT eliminates this change, what does this suggest about the CoT's causal role?

### Concept: Faithfulness vs. Plausibility
- Why needed here: The paper critiques Biasing Features for conflating these. Faithfulness = alignment with actual computation; plausibility = human-judged reasonableness. A CoT can be one without the other.
- Quick check question: A CoT mentions a hint but the hint didn't influence the decision. Is this CoT faithful, plausible, both, or neither?

### Concept: Distributed vs. Sequential Computation
- Why needed here: Transformers compute attention across all positions in parallel. CoTs are sequential narratives. Understanding this mismatch explains why "incompleteness" is structural, not behavioral.
- Quick check question: Why is it unreasonable to expect a linear CoT to capture all parallel attention-mediated influences in a transformer?

## Architecture Onboarding

### Component Map
Input (x) -> Hint Injection (h) -> Hinted Input (x_h) -> CoT Generation -> c_h -> Prediction Head (from c_h) -> Prediction ŷ_h
                                      │
                                      ▼
                              Corruption Path (Filler Tokens)
                                      │
                                      ▼
                              Corrupted Pred
                                      │
                        ┌───────────┴───────────┐
                        ▼                       ▼
                Biasing Features        Causal Mediation
                (verbalization?)       (NDE/NIE decomposition)

### Critical Path
1. **Hint injection** -> Must flip prediction to hinted answer for inclusion in analysis
2. **CoT generation** -> Check for verbalization (LLM-as-judge)
3. **If non-verbalizing** -> Apply Filler Tokens, FUR, faithful@k, and CMA

### Design Tradeoffs
- **Greedy vs. sampling**: Greedy matches prior work but underestimates verbalization; sampling (faithful@k) reveals incompleteness
- **LLM-as-judge vs. lexical matching**: Judge reduces false positives but introduces ~20% disagreement with human labels
- **FUR applicability**: Requires CoT and no-CoT predictions to agree; excludes many instances for larger models

### Failure Signatures
- **Empty/degenerate CoTs**: Llama-3-8B produces many empty CoTs under hints -> excluded from Filler Tokens/FUR
- **Flat faithful@k**: Black Squares and Metadata hints show no improvement with k -> genuine unfaithfulness, not incompleteness
- **NIE = 0**: If CMA shows no indirect effect, CoT is post-hoc; mechanism 1 invalid

### First 3 Experiments
1. **Reproduce Biasing Features baseline**: Run greedy decoding on OpenbookQA with Professor hint; confirm >80% unfaithfulness rate matches Figure 2.
2. **Implement Filler Tokens check**: For non-verbalizing CoTs, replace with "..." and measure prediction change. Target 20-40% faithful rate per Figure 3.
3. **Run faithful@k with k=16**: Sample 128 CoTs per instance; compute probability of at least one verbalization. Professor hint should show upward trend; Metadata/Black Squares should stay flat.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do these findings generalize to larger models (e.g., 70B+ parameters) and specialized reasoning models (e.g., DeepSeek-R1, o1) that produce longer CoTs by design?
- Basis in paper: [explicit] The Limitations section states: "our experiments do not include larger models or specialized reasoning models due to computational constraints."
- Why unresolved: FUR experiments are memory-intensive, and faithful@k requires expensive sampling of longer CoTs, making these settings impractical under current resource constraints.
- What evidence would resolve it: Running faithful@k and Causal Mediation Analysis on larger models and reasoning-specialized models across the same tasks and hint types.

### Open Question 2
- Question: How can verbalization training methods generalize beyond held-out evaluations that closely match training data to expose implicit, real-world decision factors?
- Basis in paper: [explicit] Future Work section: "Verbalization Finetuning (VFT)... its generalization is unclear because held-out evaluations closely match training data. Future work should aim to improve CoTs... by encouraging models to expose implicit, real-world factors through broader, more generalizable objectives."
- Why unresolved: Current methods optimize for verbalizing simplistic or toy interventions rather than training models to articulate real-world implicit biases.
- What evidence would resolve it: Developing and testing verbalization training on diverse real-world scenarios (e.g., demographic biases, reward-hacking behaviors) with held-out evaluations substantially different from training data.

### Open Question 3
- Question: What structural properties of hints determine whether the CoT serves primarily as post-hoc rationalization (high NDE) versus a meaningful intermediate in decision-making (high NIE)?
- Basis in paper: [inferred] The paper finds that NDE dominates under the Metadata hint while NIE dominates under Black Squares, noting "this aligns with hint structure: the Metadata hint directly reveals the answer... whereas the subtler Black Squares hint lets the model use the hint implicitly."
- Why unresolved: The mechanism connecting hint perceptibility, answer transparency, and causal pathway through CoT remains uncharacterized.
- What evidence would resolve it: Systematic variation of hint properties (explicitness, answer-revealing vs. implicit, social vs. visual) combined with mediation analysis to identify which properties shift the balance between NDE and NIE.

## Limitations

- The incompleteness explanation is plausible but not definitively proven—non-verbalization could still be a mixture of incompleteness and genuine unfaithfulness
- The LLM-as-judge component introduces uncertainty with approximately 20% disagreement between LLM and human judgments
- The FUR metric's restriction to cases where CoT and no-CoT predictions agree limits its applicability and may bias the evaluation
- Focus on relatively simple reasoning tasks (OpenbookQA, StrategyQA, ARC-Easy) raises questions about generalization to more complex reasoning scenarios

## Confidence

**High Confidence (90-95%)**:
- The Biasing Features metric conflates unfaithfulness with incompleteness
- The faithful@k metric validly captures the incompleteness phenomenon
- Non-verbalized hints can causally influence predictions through CoTs (CMA evidence)

**Medium Confidence (70-85%)**:
- Most "unfaithful" CoTs are actually incomplete rather than misleading
- The CoT's role is better characterized as lossy compression than post-hoc rationalization
- Hint-induced CoTs redistribute probability via suppression of alternatives

**Low Confidence (50-65%)**:
- The exact proportion of incomplete vs. genuinely unfaithful non-verbalizing CoTs
- Whether LLM-as-judge verbalization detection is reliable at scale
- Generalization to complex reasoning tasks beyond the evaluated datasets

## Next Checks

1. **Cross-Model Mediation Consistency**: Apply CMA to additional model families (e.g., Qwen, DeepSeek) and reasoning-specific models (o1, o3) to test whether non-verbalized hints consistently show non-zero NIE across architectures. This would validate whether the causal mediation mechanism is a general property of CoT generation or model-specific.

2. **Token Budget Manipulation Experiment**: Systematically vary maximum token limits (100, 200, 500, 1000 tokens) while holding inference budget (k) constant. If faithful rates increase monotonically with token limits, this provides stronger evidence for the incompleteness hypothesis independent of sampling effects.

3. **Complex Task Transfer**: Apply the complete evaluation suite (Biasing Features, Filler Tokens, FUR, faithful@k, CMA) to multi-step reasoning tasks from GSM8K and MATH. These tasks require chaining multiple reasoning steps, which should amplify any compression artifacts and provide a stricter test of the incompleteness vs. unfaithfulness distinction.