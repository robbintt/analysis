---
ver: rpa2
title: Exact Synthetic Populations for Scalable Societal and Market Modeling
arxiv_id: '2512.07306'
source_url: https://arxiv.org/abs/2512.07306
tags:
- constraints
- synthetic
- constraint
- populations
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a constraint-programming framework for generating
  synthetic populations that exactly reproduce target statistics while enforcing full
  individual consistency. Unlike data-driven approaches, the method directly encodes
  aggregated statistics and structural relations, enabling precise control of demographic
  profiles without requiring microdata.
---

# Exact Synthetic Populations for Scalable Societal and Market Modeling

## Quick Facts
- **arXiv ID:** 2512.07306
- **Source URL:** https://arxiv.org/abs/2512.07306
- **Reference count:** 25
- **Primary result:** Constraint programming framework generates synthetic populations matching target statistics exactly while enforcing full individual consistency without requiring microdata

## Executive Summary
This paper introduces a constraint programming framework for generating synthetic populations that exactly reproduce target statistics while enforcing full individual consistency. Unlike data-driven approaches, the method directly encodes aggregated statistics and structural relations, enabling precise control of demographic profiles without requiring microdata. The approach uses a batch-based generation process with distribution constraints to match target categorical distributions exactly, validated on official demographic sources.

## Method Summary
The framework formulates population generation as a constraint optimization problem where target percentages are encoded as soft global cardinality constraints. Using OR-Tools CP-SAT, it solves for individual attribute assignments that minimize deviation from target distributions while enforcing hard logical consistency rules (e.g., no retired minors). The generation proceeds in batches with extension-preserving optimality, allowing the method to scale to thousands of individuals while maintaining statistical accuracy.

## Key Results
- Achieved mean absolute percentage error under 5% even under complex constraints
- Exact matching of most distributions validated on official demographic sources
- Scales to populations of thousands (tested 5,000â€“55,000 individuals)
- Demonstrated high accuracy compared to traditional polling methods in virtual polling applications

## Why This Works (Mechanism)

### Mechanism 1: Declarative Distribution Matching via Soft Constraints
The system achieves near-zero distribution error (MAPE < 5%) by formulating population generation as a constraint optimization problem rather than a sampling problem. Target percentages are encoded as soft global cardinality constraints with a cost variable aggregating deviations, allowing declarative control distinct from statistical approximations of GANs or IPF. Core assumption: target marginal distributions and inter-feature dependencies are logically consistent and feasible to satisfy simultaneously.

### Mechanism 2: Batch-Based Extension-Preserving Optimization
The approach scales to thousands of individuals by dividing generation into batches while preserving global optimality. Distribution constraints possess a monotonicity property such that local batch decisions do not irreversibly violate global optimality. Core assumption: the extension-preserving property holds across different constraint structures, allowing batch decisions to align with global targets adjusted for previously generated individuals.

### Mechanism 3: Semantic Coherence via Hard Logical Constraints
Generated individuals serve as valid proxies for human respondents because they satisfy hard logical consistency rules (e.g., "no retired minors"). The framework applies individual constraints to every synthetic person, ensuring internal semantic validity that prevents hallucinated inconsistencies when LLMs query these agents.

## Foundational Learning

- **Constraint Programming (CP) vs. Generative Models:** CP is superior when exact statistical matching is required without microdata. Quick check: Do you need to learn a distribution from training data, or satisfy mathematical rules and targets?

- **Global Cardinality Constraints:** The core technical contribution implements distribution matching as a variant of global cardinality constraints. Quick check: How does a solver count occurrences of a variable value across individuals and minimize difference from target count?

- **Largest Remainder Method (LRM):** Converts continuous percentages into discrete integer counts without accumulating rounding errors. Quick check: If you need 100 people and a target is 33.3%, do you assign 33 or 34 people?

## Architecture Onboarding

- **Component map:** JSON Specification -> Solver Core (OR-Tools CP-SAT) -> Constraint Encoder -> Batch Manager -> Synthetic Population
- **Critical path:** Definition of the objective variable and decomposition of the distribution constraint. Weak primitive constraints cause propagation failure; overly complex ones cause timeouts.
- **Design tradeoffs:** Batch size affects solve time vs. MAPE (small batches faster but higher error). Hard constraints (individual consistency) must be satisfied; soft constraints (distribution matching) are minimized.
- **Failure signatures:** Infeasibility (contradictory constraints), high MAPE (>5% indicates timeout or overly tight constraints).
- **First 3 experiments:** 1) Unit test: Generate 100 individuals with 2 features, verify exact counts match targets. 2) Stress test: Scale population from 1k to 10k, observe solve time vs. MAPE. 3) Integration test: Generate district population, serialize to JSON, verify semantic coherence with LLM prompt.

## Open Questions the Paper Calls Out
None

## Limitations
- Extension-preserving property is critical for scaling but not empirically validated across diverse constraint structures
- Assumes complete, coherent target distributions are available, which real-world statistics often lack
- MAPE metric alone doesn't capture preservation of higher-order statistical relationships beyond pairwise marginals

## Confidence
- **High Confidence:** Core CP-based methodology and batch generation approach are technically sound
- **Medium Confidence:** Claims about exact matching and scalability supported by limited test cases
- **Low Confidence:** Broader applicability claims for LLM-based simulations not rigorously validated

## Next Checks
1. **Stress Test Individual Constraints:** Systematically vary tightness of individual coherence rules while measuring distribution error and solve time to reveal practical limits of extension-preserving property.

2. **Cross-Validation with Real Data:** Generate synthetic populations where microdata is available but not used, then compare joint distributions against ground truth to assess preservation of higher-order relationships.

3. **Infeasibility Analysis:** Deliberately introduce inconsistent target distributions and document solver behavior, error messages, and diagnostic tools to test framework robustness to real-world data quality issues.