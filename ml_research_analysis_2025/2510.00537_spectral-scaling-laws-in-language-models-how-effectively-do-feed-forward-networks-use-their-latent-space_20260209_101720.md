---
ver: rpa2
title: 'Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward
  Networks Use Their Latent Space?'
arxiv_id: '2510.00537'
source_url: https://arxiv.org/abs/2510.00537
tags:
- spectral
- rank
- utilization
- width
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how effectively FFN width scales with utilization
  of latent space in large language models. It introduces spectral diagnostics (hard
  rank, soft rank, spectral concentration, SUI) to measure how many latent directions
  are meaningfully activated across different model families.
---

# Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?

## Quick Facts
- **arXiv ID**: 2510.00537
- **Source URL**: https://arxiv.org/abs/2510.00537
- **Reference count**: 26
- **Primary result**: Asymmetric spectral scaling law: soft rank grows nearly linearly with FFN width while hard rank plateaus sublinearly, indicating widening adds under-utilized tail directions.

## Executive Summary
This paper investigates how effectively feed-forward network (FFN) width scales with utilization of latent space in large language models. The authors introduce spectral diagnostics (hard rank, soft rank, spectral concentration, SUI) to measure how many latent directions are meaningfully activated across different model families. They find that widening FFNs exhibits an asymmetric scaling law: soft rank grows nearly linearly with width while hard rank plateaus sublinearly, suggesting that added capacity primarily populates low-energy tail directions rather than expanding high-variance core subspaces. LayerNorm placement critically affects this trade-off, with Mix-LN providing the best balance between core and tail utilization.

## Method Summary
The authors conduct width sweeps on LLaMA, GPT-2, and nGPT architectures, training models with FFN hidden dimensions ranging from 1× to 8× the embedding dimension. They sample post-activation outputs from each FFN layer, compute covariance matrices, and perform eigendecomposition to calculate spectral metrics. The key measurements include hard rank (participation ratio), soft rank (Shannon entropy), spectral concentration (power-law decay exponent), and SUI (harmonic mean of normalized ranks). Models are trained from scratch on C4 dataset with 20K-100K training steps depending on model scale, and metrics are tracked throughout training.

## Key Results
- Soft rank follows an almost perfect power law with FFN width (β≈1.07, R²≈0.93) while hard rank grows only sublinearly (β≈0.60, R²≈0.68)
- LayerNorm placement modulates the trade-off: Pre-LN amplifies tail capacity, Post-LN suppresses it, Mix-LN balances both
- Post-LN models exhibit spectral collapse at large widths (hard-rank <10⁻³, concentration≈1.0) without stabilization techniques
- Spectral concentration follows truncated power-law with intermediate values (α≈1.1-1.3) balancing expressivity and compactness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Widening FFNs allocates capacity via tail-first growth, where low-energy directions expand before high-energy core subspaces saturate
- Mechanism: As FFN hidden dimension D increases, post-activation eigenspectrum shows near-linear scaling of soft rank but sublinear scaling of hard rank, indicating added dimensions primarily populate the long tail of the spectrum
- Core assumption: Post-activation covariance eigenvalues reflect functionally utilized latent directions
- Evidence anchors: [abstract] "soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance"; [Section 4.1] LLaMA-130M: SRank β≈1.07 vs HRank β≈0.60
- Break condition: If eigendecomposition costs dominate at extreme widths, or if tail directions encode task-critical information, tail-pruning could harm performance

### Mechanism 2
- Claim: LayerNorm placement modulates the trade-off between dominant-mode and tail capacity utilization
- Mechanism: Pre-LN normalizes before sub-blocks, allowing variance to spread into tail directions (amplifying soft rank). Post-LN normalizes after transformations, curbing variance spread and suppressing tail growth. Mix-LN combines both, preserving tail coverage while improving dominant-mode scaling
- Core assumption: Normalization positioning directly controls activation variance distribution across latent directions
- Evidence anchors: [Section 5.1] Pre-LN: SRank β≈0.88-1.07, HRank β≈0.45-0.60. Post-LN: SRank β≈0.71-0.82, HRank β≈0.52-0.56. Mix-LN: SRank β≈0.97-1.10, HRank β≈0.59-0.63
- Break condition: If normalization interacts with other architectural choices (e.g., attention mechanisms, residual connections) in untested ways, effects may not transfer

### Mechanism 3
- Claim: Spectral concentration follows a truncated power-law (λₖ ∝ k⁻ᵅ), where α controls front-loading of variance; intermediate values (α≈1.1-1.3) balance expressivity and compactness
- Mechanism: Higher α concentrates variance in leading eigenvalues (spectral collapse risk); lower α distributes variance uniformly (dilution risk). Empirical FFN spectra cluster near α≈1.1-1.3, avoiding extremes
- Core assumption: Eigenvalue decay rate predicts functional capacity trade-offs
- Evidence anchors: [Section 4.3] Table 3: α=1.5 captures >90% variance in top 10% PCs; α=0.8 requires >50% PCs for 80% variance; "activations in prevalent models such as LLaMA typically exhibit intermediate spectral concentration (α≈1.1-1.3)"
- Break condition: If tasks require either highly sparse or highly distributed representations, intermediate α may be suboptimal

## Foundational Learning

- Concept: Participation ratio (hard rank)
  - Why needed here: Measures effective dimensionality of high-energy subspace; central to detecting dominant-mode saturation
  - Quick check question: Given eigenvalues [10, 1, 0.1, 0.01], compute PR=(Σλᵢ)²/Σλᵢ². Does it approach 1 (spike) or 4 (uniform)?

- Concept: Shannon entropy-based rank (soft rank)
  - Why needed here: Captures uniformity across all spectral directions; detects long-tail utilization missed by hard rank
  - Quick check question: Convert eigenvalues to probabilities pᵢ=λᵢ/Σλⱼ, then compute e^H where H=-Σpᵢlog(pᵢ). How does it differ from PR for the same spectrum?

- Concept: Power-law scaling (log-log fits)
  - Why needed here: Quantifies how spectral metrics scale with width; β exponents reveal linear vs. sublinear growth
  - Quick check question: If y ∝ x^β, what does β<1 imply about marginal returns when doubling x? What if β≈1?

## Architecture Onboarding

- Component map: W_gate → activation → elementwise product with W_up → W_down → Spectral probe
- Critical path: 1) Choose FFN width multiplier α (typical: 2.67-4× for efficiency, up to 8× for experiments) 2) Select LayerNorm placement (Pre-LN for stability, Post-LN for tail suppression, Mix-LN for balance) 3) Compute spectral metrics at checkpoints (hard/soft rank, concentration, SUI, eDim) 4) Monitor for collapse (hard-rank <10⁻³) or dilution (soft-rank utilization flat while hard-rank declines)
- Design tradeoffs: Width vs. efficiency (beyond α≈4×, eDim grows sublinearly; consider depth or MoE instead); LayerNorm (Pre-LN maximizes tail capacity but risks over-tailing; Post-LN reduces tail inflation but may collapse at large widths without weight normalization); Normalization augmentation (WeightNorm stabilizes spectra and improves perplexity)
- Failure signatures: Spectral collapse (hard-rank plunges to <10⁻³, concentration→1.0, perplexity spikes); Spectral dilution (hard-rank utilization declines with width, soft-rank utilization flat); Training instability (Post-LN at large widths without normalization)
- First 3 experiments: 1) Width sweep diagnostic: Train LLaMA-130M with α∈{1, 2, 2.67, 4, 6, 8}; plot hard/soft rank vs. D on log-log axes; verify β_soft≈1, β_hard<1. 2) LayerNorm ablation: Compare Pre-LN, Post-LN, Mix-LN at fixed α=2.67 and α=4; measure SUI, eDim, and perplexity. 3) Stabilization test: For Post-LN at α≥4, add WeightNorm to FFN layers; confirm hard-rank remains >10⁻² and perplexity improves vs. vanilla.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does actively maximizing spectral utilization (SUI) causally improve downstream performance, or is it merely a correlated proxy?
- Basis in paper: [explicit] The Limitations section states that "causality remains unproven" between spectral metrics and perplexity
- Why unresolved: The current study observes and correlates metrics but does not intervene to force specific spectral shapes during training
- What evidence would resolve it: Training models with an explicit spectral regularization loss (maximizing SUI) and measuring performance changes against a baseline with identical compute

### Open Question 2
- Question: Do asymmetric spectral scaling laws persist in encoder-decoder architectures or multilingual models beyond 250M parameters?
- Basis in paper: [explicit] The Limitations section notes the study is "limited to English decoder-only models up to 250M parameters"
- Why unresolved: Different inductive biases (cross-attention in encoders) and data distributions (multilingual vocabularies) may alter covariance dynamics and rank saturation
- What evidence would resolve it: Replicating the width sweep analysis on large-scale encoder-decoder (e.g., T5) or multilingual models (e.g., mGPT) to verify if hard/soft rank exponents remain consistent

### Open Question 3
- Question: What is the functional role of the low-energy "tail" directions; do they store critical rare information or represent under-utilized noise?
- Basis in paper: [inferred] The paper concludes widening adds "low-energy tail directions," while the Related Work notes small singular values may encode critical information
- Why unresolved: The paper quantifies the variance distribution (spectral concentration) but does not perform ablations to test the semantic contribution of these specific directions
- What evidence would resolve it: Targeted ablation of low-energy principal components to measure the impact on specific linguistic tasks versus the high-energy dominant modes

### Open Question 4
- Question: How can spectral diagnostics be operationalized into a training algorithm for dynamic, layer-wise width allocation?
- Basis in paper: [inferred] The Conclusion suggests "width-efficient designs via layer-wise scheduling and pruning," but provides no mechanism to achieve this
- Why unresolved: The paper offers a diagnostic tool (eDim) but stops short of proposing an algorithm that uses this signal to modify architecture during or before training
- What evidence would resolve it: A training pipeline that dynamically adjusts layer widths or prunes neurons based on real-time eDim plateaus, demonstrating FLOP reduction without performance degradation

## Limitations
- Empirical validation is constrained to English decoder-only models up to 250M parameters, limiting generalizability to other architectures and scales
- The interpretation of spectral metrics as measures of functional utilization conflates statistical variance with task-relevant information without direct ablation studies
- LayerNorm placement effects are demonstrated through correlation rather than controlled causal experiments, as normalization interacts with other architectural choices

## Confidence

- **High Confidence**: The spectral measurement methodology (hard/soft rank, spectral concentration) is technically sound and the basic observation that soft rank scales more linearly than hard rank with width is well-supported by the data
- **Medium Confidence**: The asymmetric scaling law interpretation and its implications for capacity utilization are plausible but require additional validation across different architectures and tasks
- **Low Confidence**: The specific β exponent values and their interpretation as universal scaling laws are uncertain, as they may depend on training duration, dataset characteristics, and optimization dynamics not controlled in the experiments

## Next Checks

1. **Cross-architecture validation**: Test the asymmetric spectral scaling law on non-transformer architectures (CNNs, MLPs, or other attention variants) and different task types (vision, speech, reinforcement learning) to determine if the width-utility relationship is architecture-specific or more universal

2. **Temporal stability analysis**: Track how spectral metrics evolve throughout training beyond the 20K-100K step window, particularly examining whether the asymmetric scaling emerges early, develops gradually, or only appears after certain training milestones like loss plateaus

3. **Ablation of normalization effects**: Systematically isolate the effects of LayerNorm placement by controlling for other variables (attention mechanisms, residual connections, activation functions) through factorial experimental designs to establish causal relationships rather than correlational patterns