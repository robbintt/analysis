---
ver: rpa2
title: Assessing the Business Process Modeling Competences of Large Language Models
arxiv_id: '2601.21787'
source_url: https://arxiv.org/abs/2601.21787
tags:
- process
- quality
- llms
- bpmn
- b-instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEF4LLM, a comprehensive evaluation framework
  for assessing LLM-generated BPMN models across syntactic, pragmatic, semantic, and
  validity dimensions. Using 39 established metrics, the framework enables detailed
  quality assessment of LLM-generated process models.
---

# Assessing the Business Process Modeling Competences of Large Language Models

## Quick Facts
- arXiv ID: 2601.21787
- Source URL: https://arxiv.org/abs/2601.21787
- Reference count: 40
- Introduces BEF4LLM evaluation framework for LLM-generated BPMN models across 4 quality dimensions using 39 metrics

## Executive Summary
This paper presents BEF4LLM, a comprehensive evaluation framework for assessing large language models' ability to generate business process models in BPMN notation. The framework evaluates models across four quality dimensions: syntactic, pragmatic, semantic, and validity, using 39 established metrics. The study tests 11 open-source LLMs on 105 text-to-BPMN pairs, revealing that while LLMs excel in syntactic and pragmatic quality (consistently above 0.8), they struggle significantly with semantic understanding, achieving only moderate scores. The research finds no clear correlation between model size and quality, and highlights that LLMs can approach human expert performance in some dimensions while falling short in others.

## Method Summary
The authors developed BEF4LLM by combining 39 established metrics from existing BPMN evaluation frameworks to assess four quality dimensions. They evaluated 11 open-source LLMs on 105 text-BPMN pairs, measuring syntactic quality (structural correctness), pragmatic quality (functional completeness), semantic quality (conceptual accuracy), and validity (XML compliance). The study used automated metrics rather than human expert evaluation, comparing results against a benchmark of human-generated models to establish baseline performance levels.

## Key Results
- LLMs achieve pragmatic scores consistently above 0.8 but semantic quality peaks at only 0.5768
- Only Llama 3.1 70b achieves over 90% valid XML generation across all tested models
- No consistent relationship found between model size and BPMN generation quality
- LLMs match human experts in pragmatic and syntactic dimensions but lag in semantic quality

## Why This Works (Mechanism)
The framework's effectiveness stems from its multi-dimensional approach that captures different aspects of BPMN model quality. By combining 39 established metrics across four distinct quality dimensions, BEF4LLM provides a comprehensive evaluation that reveals LLMs' strengths in structural and functional aspects while exposing their weaknesses in conceptual understanding. The automated nature of the evaluation allows for consistent, scalable assessment across multiple models and datasets.

## Foundational Learning
The evaluation framework builds upon established BPMN quality assessment methodologies, incorporating metrics from previous research on process model evaluation. The study demonstrates that LLMs can learn to generate syntactically correct and functionally complete BPMN models through pattern recognition in training data, but struggle with the deeper semantic understanding required for accurate conceptual representation of business processes.

## Architecture Onboarding
Component Map: Text Input -> LLM Processing -> BPMN Generation -> BEF4LLM Evaluation Framework (syntactic, pragmatic, semantic, validity metrics)

Critical Path: Text description → Model interpretation → BPMN element mapping → Diagram construction → XML serialization → Quality assessment

Design Tradeoffs: Automated metric evaluation vs. human expert assessment; breadth of model testing vs. depth of individual analysis; established metrics vs. novel quality dimensions

Failure Signatures: Low semantic scores indicate conceptual misunderstanding; XML invalidity suggests technical implementation issues; pragmatic gaps reveal functional completeness problems

First Experiments:
1. Test LLMs on edge cases with ambiguous process descriptions to stress semantic understanding
2. Evaluate cross-model consistency by comparing outputs for identical inputs across different LLMs
3. Assess transfer learning by fine-tuning models on BPMN-specific training data and measuring quality improvements

## Open Questions the Paper Calls Out
- How can LLMs be improved to better capture semantic relationships in business processes?
- What training approaches might help LLMs develop deeper conceptual understanding of BPMN notation?
- Can domain-specific fine-tuning enhance LLMs' ability to generate semantically accurate BPMN models?
- How do different BPMN dialects and extensions affect LLM performance across quality dimensions?

## Limitations
- Narrow scope with only 105 text-BPMN pairs may not represent real-world complexity
- Established metrics may not capture all relevant quality dimensions for BPMN modeling
- Absence of human expert evaluation beyond benchmark pairs introduces uncertainty about metric alignment
- Limited testing of domain-specific BPMN extensions and variants

## Confidence
- Syntactic and pragmatic quality: Medium (consistent results across metrics and models)
- Semantic quality: Low (moderate scores with potential metric limitations)
- Validity dimension: Medium (important but may not correlate directly with semantic correctness)

## Next Checks
1. Expand evaluation dataset to include more diverse and complex business process descriptions, focusing on multi-stakeholder scenarios and edge cases

2. Conduct comprehensive human expert validation study to compare automated metric results with human judgments, especially for semantic quality

3. Test LLM robustness across different BPMN versions and dialects, including domain-specific extensions and modeling conventions