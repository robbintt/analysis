---
ver: rpa2
title: 'MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language
  Models via a Mixture of Experts Framework'
arxiv_id: '2506.02460'
source_url: https://arxiv.org/abs/2506.02460
tags:
- safety
- helpfulness
- preference
- midpo
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing safety and helpfulness
  in large language models (LLMs). The authors propose MidPO, a Mixture of Experts
  (MoE) framework that uses a dynamic routing mechanism to optimize both safety and
  helpfulness simultaneously.
---

# MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework

## Quick Facts
- arXiv ID: 2506.02460
- Source URL: https://arxiv.org/abs/2506.02460
- Reference count: 40
- Primary result: MidPO achieves safety win rate of 86.15% and helpfulness win rate of 63.09% on benchmark datasets while maintaining inference efficiency

## Executive Summary
This paper addresses the challenge of balancing safety and helpfulness in large language models by proposing MidPO, a Mixture of Experts (MoE) framework that optimizes both objectives simultaneously. The method separates safety and helpfulness optimization into dedicated experts trained via single-preference enhanced direct preference optimization (SPE-DPO), then integrates them using a dynamic routing mechanism. MidPO outperforms state-of-the-art methods, achieving significant improvements in both safety and helpfulness metrics while maintaining reasonable inference efficiency through LoRA-based expert parameterization.

## Method Summary
MidPO fine-tunes two independent experts for safety and helpfulness using SPE-DPO with margin-based preference amplification. These experts are LoRA adapters on the MLP down_proj layer, integrated into an MoE framework with a dynamic router that allocates expert contributions based on input characteristics. The router learns to prioritize safety expert for unsafe inputs and helpfulness expert for safe inputs, with L1 regularization preventing collapse to single-expert solutions.

## Key Results
- Safety win rate of 86.15% vs. Alpaca-7B baseline
- Helpfulness win rate of 63.09% vs. Alpaca-7B baseline
- Reduces over-refusal while maintaining safety performance
- Inference overhead of +0.49s vs. Safe RLHF baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating safety and helpfulness optimization into dedicated experts avoids gradient conflicts present in joint optimization.
- Mechanism: SPE-DPO introduces a homogeneous preference margin that amplifies within-category distinctions while setting cross-category margins to zero, preventing interference between safety and helpfulness objectives during expert training.
- Core assumption: Safety and helpfulness preferences have conflicting gradient directions that compound when optimized jointly; decoupling allows each expert to specialize without compromise.
- Evidence anchors: [abstract] "fine-tunes the two independent experts for optimal safety or helpfulness performance"; [section 3.1.1] "amplifies the distinction between y_w and y_l within the same preference category"; [corpus] Tangent Space Fine-Tuning paper confirms directional conflicts in multi-dimensional preference optimization.

### Mechanism 2
- Claim: Dynamic routing enables input-dependent expert weighting, reducing over-refusal on safe prompts while maintaining refusal on unsafe ones.
- Mechanism: Router maps hidden states to expert weights via learned linear projections; L1 regularization prevents collapse to single-expert solutions. The router learns to upweight safety expert for unsafe inputs and helpfulness expert for safe inputs.
- Core assumption: Input hidden states encode learnable safety characteristics distinguishable by a shallow network.
- Evidence anchors: [abstract] "dynamic routing mechanism to allocate contributions from each expert adaptively"; [section 4.3.3, Figure 5] Router assigns higher safety-expert weights to unsafe questions (median ~0.8) vs. safe questions (median ~0.3); [corpus] No corpus papers directly validate hidden-state-based safety routing.

### Mechanism 3
- Claim: LoRA-based expert parameterization maintains inference efficiency by avoiding full model duplication.
- Mechanism: Experts share frozen base weights with separate low-rank adapters. During inference, MoE output combines base model with weighted LoRA contributions, requiring only router computation as overhead.
- Core assumption: MLP layers capture sufficient preference-specific behavior for effective expert specialization.
- Evidence anchors: [section 3.1.2] "fine-tuning the multi-layer perceptron (MLP) layer of π_θ using LoRA"; [section E.1] Inference overhead vs. Safe RLHF: +0.49s for +13.29% safety and +8.21% helpfulness improvement; [corpus] Mix-or-Merge paper explores model merging for 3H optimization but uses full-weight merging.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed here: MidPO extends DPO with margin-based modifications (SPE-DPO); understanding the baseline DPO loss is prerequisite.
  - Quick check question: Can you explain why DPO avoids training an explicit reward model?

- **Mixture of Experts (MoE)**
  - Why needed here: MidPO's architecture combines two experts via learned routing; distinguishing sparse-gated MoE from MidPO's dense-weighted combination is essential.
  - Quick check question: How does MidPO's routing differ from top-k gating in traditional MoE?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Experts are LoRA adapters, not full models; understanding rank, alpha, and target module selection affects implementation.
  - Quick check question: Why might down_proj be a better LoRA target than attention layers for preference optimization?

## Architecture Onboarding

- **Component map:** Base Model (Alpaca-7B) → LoRA adapters (Safety Expert, Helpfulness Expert) → Router (two parallel networks R_s, R_h) → MoE Layer (weighted LoRA combination) → Output

- **Critical path:** (1) Train Safety Expert via SPE-DPO on safety-labeled pairs → (2) Train Helpfulness Expert via SPE-DPO on helpfulness pairs → (3) Freeze experts, train Router on dual-preference dataset D_dual

- **Design tradeoffs:** Router capacity (d_r=512) vs. overfitting risk; single-layer LoRA (down_proj only) vs. multi-layer; L1 regularization strength; assumption that down_proj captures preference behavior.

- **Failure signatures:** Safety expert dominates → over-refusal on benign prompts; Helpfulness expert dominates → harmful responses on unsafe prompts; Router collapses to uniform weights → performance similar to static model merging; Margin clipping triggers frequently → reward model misaligned with preference labels.

- **First 3 experiments:**
  1. **Ablation: SPE-DPO vs. vanilla DPO for experts.** Train experts with standard DPO, compare single-preference scores. Expected: SPE-DPO experts outperform DPO on target metric.
  2. **Router validation on held-out distribution.** Plot weight distributions on out-of-distribution prompts to detect generalization gaps.
  3. **Layer sensitivity.** Compare down_proj-only LoRA vs. q_proj+down_proj LoRA. Assumption: Down_proj captures preference behavior; if attention layers improve results, this assumption breaks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does applying the MidPO framework to other linear layers within the transformer architecture yield comparable or superior performance compared to the specific "down_proj" layer utilized in the study?
- Basis in paper: [explicit] The authors state in the Limitations section that while they selected the "down_proj" layer for LoRA fine-tuning, "it remains unclear whether applying our framework to other linear layers would achieve comparable performance."
- Why unresolved: The study restricted the expert fine-tuning to the MLP "down_proj" layers without conducting ablation studies on other components to determine if they are more effective targets for safety or helpfulness optimization.
- What evidence would resolve it: An ablation study comparing the safety and helpfulness scores of experts fine-tuned on different linear layers versus the current setting.

### Open Question 2
- Question: How can MidPO be efficiently extended to optimize for more than two alignment objectives without suffering from the computational cost of pretraining supplementary reward models?
- Basis in paper: [explicit] Section 7 notes that "Applying MidPO to additional alignment objectives would require pretraining supplementary reward models, thereby reducing the efficiency of the fine-tuning process."
- Why unresolved: The current framework relies on distinct experts trained via specific reward models. Adding a third objective introduces linear scaling in reward modeling requirements, creating a bottleneck.
- What evidence would resolve it: A demonstration of MidPO functioning with a third expert that uses a shared or implicit reward signal, maintaining training efficiency while improving multi-objective metrics.

### Open Question 3
- Question: Does the dynamic routing mechanism maintain its balance and inference efficiency when applied to significantly larger base models or different model architectures?
- Basis in paper: [inferred] The experiments are limited to the Alpaca-7B model, and while the authors claim the inference overhead is justifiable, they do not demonstrate if the routing logic scales effectively to models with different capacities or architectural quirks.
- Why unresolved: The router's ability to distinguish between safe and unsafe inputs is learned on a 7B parameter scale; it is unproven whether this specific routing architecture transfers to larger models without re-tuning or suffering from increased latency.
- What evidence would resolve it: Experimental results applying MidPO to a larger model family reporting both win rates and the marginal inference time increase relative to the larger base.

## Limitations
- Effectiveness depends on assumption that safety and helpfulness gradients are sufficiently conflicting to warrant expert separation
- Router's ability to generalize to edge cases (e.g., adversarial prompts) remains untested
- LoRA-based expert design assumes preference-relevant behavior is captured in MLP layers, potentially missing attention-based preference signals

## Confidence
- **High confidence:** Separating safety and helpfulness optimization into dedicated experts is well-supported by ablation studies showing superior performance over joint training approaches
- **Medium confidence:** Dynamic routing mechanism's ability to reduce over-refusal while maintaining safety is demonstrated on benchmark datasets but lacks validation on adversarial inputs
- **Medium confidence:** LoRA-based expert parameterization maintains inference efficiency is supported by measured overhead metrics, though the assumption about MLP-layer sufficiency remains partially untested

## Next Checks
1. **Generalization test:** Evaluate router performance on adversarial prompts designed to appear safe while requesting harmful information to assess robustness beyond benchmark distributions
2. **Attention-layer ablation:** Train experts with LoRA adapters on both attention and MLP layers to quantify the impact of the down_proj-only design assumption on preference capture
3. **Gradient orthogonality measurement:** Quantify the angle between safety and helpfulness gradients in joint training to empirically validate the conflict assumption underlying expert separation