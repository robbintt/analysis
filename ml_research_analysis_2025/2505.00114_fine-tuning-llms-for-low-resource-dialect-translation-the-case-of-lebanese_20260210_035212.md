---
ver: rpa2
title: 'Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese'
arxiv_id: '2505.00114'
source_url: https://arxiv.org/abs/2505.00114
tags:
- translation
- lebanese
- language
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the effectiveness of fine-tuning Large Language\
  \ Models (LLMs) for translating the low-resource Lebanese dialect. The study compares\
  \ three fine-tuning approaches\u2014Basic (Instruct-MT), contrastive (Instruct-Cont),\
  \ and grammar-hint (Instruct-Grammar)\u2014using the Aya23-8B model."
---

# Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese

## Quick Facts
- arXiv ID: 2505.00114
- Source URL: https://arxiv.org/abs/2505.00114
- Reference count: 19
- Primary result: Contrastive fine-tuning with culturally authentic data outperforms larger non-native datasets for Lebanese dialect translation

## Executive Summary
This paper demonstrates that fine-tuning Large Language Models for low-resource dialect translation benefits more from culturally authentic data than from larger translated datasets. Using the Aya23-8B model, the authors compare three fine-tuning approaches on Lebanese Arabic to English translation: basic translation, contrastive fine-tuning with bad examples, and grammar-hint instruction. Experiments show that models trained on a smaller culturally-aware Lebanese dataset (3K sentences) consistently outperform those trained on larger non-native datasets (140K sentences). The best results come from contrastive fine-tuning paired with contrastive prompting at inference, highlighting the value of exposing models to translation errors during training.

## Method Summary
The study fine-tunes the Aya23-8B model using QLoRA (rank=64, batch=16, grad_accum=16, 3 epochs) on three types of instruction-formatted datasets: basic translation pairs, contrastive pairs with chosen/rejected translations, and grammar-hint pairs. Two data sources are used: non-native data (MADAR 12K + OpenSubtitles 128K = 140K sentences) and culturally-aware LanguageWave (LW) dataset (~3K sentences). The authors evaluate using xCOMET-10.7B on both FLoRes and a new native Lebanese benchmark (LebEval). Prompting strategies include zero-shot, 3-shot, and contrastive 3-shot (C3-shot) at inference.

## Key Results
- Models fine-tuned on LW dataset outperform those trained on larger non-native datasets despite 47x size difference
- Contrastive fine-tuning with contrastive prompting achieves highest scores (74.4 xCOMET on LebEval)
- Grammar-hint instruction format underperforms basic translation and contrastive approaches
- Large performance gap between FLoRes (85.5) and LebEval (68.7) reveals limitations of translated benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Cultural Authenticity Over Data Quantity
Smaller culturally-authentic datasets (3K sentences) outperform larger translated datasets (140K sentences) because native cultural data captures idioms, regional expressions, and contextual nuances that back-translated or non-native corpora systematically miss. The LW dataset, derived from Lebanese podcasts, preserves linguistic patterns that larger non-native datasets fail to encode.

### Mechanism 2: Contrastive Fine-Tuning with Negative Examples
Exposing models to "bad" translation examples during training improves output quality, especially when paired with contrastive prompting at inference. Contrastive instruction formatting teaches the model to discriminate quality differences rather than merely pattern-matching source-target pairs, creating an implicit error-awareness signal absent from standard MT fine-tuning.

### Mechanism 3: Authentic Benchmarking Reveals True Performance Gaps
Standard translated benchmarks (FLoRes) inflate model performance compared to native-content benchmarks (LebEval). Translated benchmarks inadvertently test the model's ability to reverse translation patterns rather than handle authentic dialectal variation, masking the cultural-linguistic gap that native benchmarks expose.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) with Instruction Templates**
  - Why needed here: All three training approaches reformulate parallel data into prompt/response format with structured fields (preamble, ###Input, ###Response, ###Hint)
  - Quick check question: Can you explain why instruction-tuning differs from continued pre-training for adapting translation behavior?

- Concept: **Contrastive Learning / Preference Signals**
  - Why needed here: The contrastive instruction format requires understanding how chosen/rejected pairs create a ranking signal rather than absolute quality labels
  - Quick check question: How does concatenating good/bad translations with a `<rather than>` delimiter create a different learning signal than presenting only correct pairs?

- Concept: **Catastrophic Forgetting in Multi-Stage Training**
  - Why needed here: Curriculum learning approaches failed to improve performance, attributed to forgetting
  - Quick check question: Why might sequential fine-tuning on different instruction types cause performance degradation on earlier-learned tasks?

## Architecture Onboarding

- Component map:
  ```
  Base Model: Aya23-8B (Cohere, multilingual, 8B params)
       ↓
  Fine-tuning: QLoRA (rank=64, batch=16, grad_accum=16, 3 epochs)
       ↓
  Instruction Types:
    - Instruct-MT: Basic translation (source → target)
    - Instruct-Cont: Contrastive (chosen <rather than> rejected)
    - Instruct-Grammar: Hint-annotated (rule + source → target)
       ↓
  Prompting Strategies (inference):
    - 0-shot, 3-shot, C3-shot (contrastive 3-shot)
       ↓
  Evaluation: xCOMET-10.7B (reference-free) on FLoRes + LebEval
  ```

- Critical path:
  1. Obtain culturally-authentic parallel data (LW) OR non-native data (MADAR+OS)
  2. Construct instruction datasets (MT / Contrastive / Grammar)
  3. Fine-tune Aya23-8B with QLoRA
  4. Evaluate with both 0-shot and contrastive prompting
  5. Report xCOMET on LebEval (not just FLoRes)

- Design tradeoffs:
  - **LW vs NN data**: LW (3K, authentic) vs NN (140K, translated) — paper shows LW wins despite 47× size difference
  - **Instruct-Cont vs CPO**: Contrastive SFT outperformed Contrastive Preference Optimization (Table 2: CPO-LW 67.1 vs Instruct-Cont-LW 73.6 on LebEval)
  - **Curriculum vs Single-Task**: Curriculum learning showed no gains; single-task contrastive training recommended
  - **Grammar-hint synthesis**: Claude 3.5 Sonnet used for synthetic grammatical data (2,836 pairs), but Instruct-Grammar underperformed (Table 1: 70.1 max on LebEval)

- Failure signatures:
  - Curriculum training shows no improvement → likely catastrophic forgetting
  - CPO underperforms baseline → rejected samples from same model may lack contrast quality
  - Grammar-hint underperforms → synthetic data may not capture real grammatical complexity
  - FLoRes >> LebEval scores → model overfits to translated benchmark artifacts

- First 3 experiments:
  1. **Baseline replication**: Fine-tune Aya23-8B on LW with Instruct-MT format, evaluate 0-shot and 3-shot on both FLoRes and LebEval. Expect LebEval xCOMET ~72-74.
  2. **Contrastive ablation**: Create contrastive instructions using base model outputs as rejected samples. Compare Instruct-Cont-LW with C3-shot prompting vs 3-shot. Expect ~1-2 point gain on LebEval.
  3. **Data quality stress test**: Fine-tune on NN data (140K) with identical contrastive setup. Verify that smaller LW (3K) still outperforms. This validates the "quality over quantity" claim for your target dialect.

## Open Questions the Paper Calls Out

### Open Question 1
Does Contrastive Preference Optimization (CPO) outperform Supervised Fine-Tuning when the preference datasets utilize human-curated rejected translations focused on cultural alignment rather than model-generated errors? The experiments relied on synthetic "bad" examples generated by the base model, which may have lacked the specific nuances required to teach the model to reject culturally inauthentic translations effectively.

### Open Question 2
Can a multi-agent framework with specialized models for grammar, contrastive reasoning, and translation successfully mitigate the "catastrophic forgetting" observed in single-model curriculum learning? The paper only tested multi-stage training on a single model, where later stages caused the model to forget earlier lessons, and the proposed agentic architecture was not implemented.

### Open Question 3
How does the application of a Mixture of Experts (MoE) approach impact the efficiency and translation quality of fine-tuning for low-resource dialects compared to the dense fine-tuning methods explored? The study was constrained to standard fine-tuning techniques (QLoRA) on dense models, leaving the potential efficiency gains or performance shifts of sparse architectures unexplored.

### Open Question 4
Do the benefits of contrastive instruction tuning and culturally authentic data transfer to larger model architectures or Arabic-centric LLMs? The findings are specific to the Aya23-8B model; it remains unclear if larger models inherently possess enough cultural knowledge to negate the need for specific fine-tuning strategies, or if they would show similar relative gains.

## Limitations

- Dataset accessibility: The LanguageWave (LW) dataset availability and access method are not fully specified, creating potential barriers to replication
- Benchmark size: The LebEval benchmark contains only 70 sentences, raising concerns about statistical robustness and domain coverage
- Model scope: Experiments are limited to Aya23-8B, with no testing on larger models or Arabic-centric LLMs due to computational constraints

## Confidence

- **High Confidence**: The finding that culturally-authentic data (LW) outperforms larger non-native datasets is well-supported by direct comparisons in Table 1 and grounded in the established principle that translation quality depends on source data authenticity.
- **Medium Confidence**: The contrastive fine-tuning benefit (Instruct-Cont vs Instruct-MT) is demonstrated but relies on the quality of base-model-generated rejected translations, which isn't fully characterized.
- **Low Confidence**: The grammar-hint approach underperforms, but the synthetic data generation process (Claude 3.5 Sonnet prompts) isn't fully specified, making it unclear whether this reflects a genuine limitation of the approach or implementation artifacts.

## Next Checks

1. **Data Authenticity Validation**: Replicate the core finding (LW vs NN) using a different culturally-authentic Lebanese dataset or by synthetically corrupting NN data to simulate cultural gaps. This tests whether the "quality over quantity" effect is robust to dataset choice.

2. **Contrastive Signal Quality**: Analyze the base model's rejected translations for diversity and quality. If they're too similar to chosen translations, the contrastive gain may be an artifact. Consider using human-curated rejections or automatically detected errors as an ablation.

3. **Benchmark Robustness**: Evaluate the best model (Instruct-Cont-LW + C3-shot) on a larger, manually curated dialect benchmark (if available) or test LebEval performance across multiple random 70-sentence subsets to assess score variance.