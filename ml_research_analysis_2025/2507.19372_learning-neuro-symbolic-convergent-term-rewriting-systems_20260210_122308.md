---
ver: rpa2
title: Learning neuro-symbolic convergent term rewriting systems
arxiv_id: '2507.19372'
source_url: https://arxiv.org/abs/2507.19372
tags:
- neural
- formulas
- rewriting
- selector
- formula
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a neuro-symbolic framework for learning convergent
  term rewriting systems, aiming to solve formula simplification problems with strong
  out-of-distribution generalization. Two models are presented: the Neural Rewriting
  System (NRS) and its more efficient variant, the Fast Neural Rewriting System (FastNRS).'
---

# Learning neuro-symbolic convergent term rewriting systems

## Quick Facts
- arXiv ID: 2507.19372
- Source URL: https://arxiv.org/abs/2507.19372
- Reference count: 23
- Key outcome: Neuro-symbolic models for convergent term rewriting achieve strong OOD generalization in formula simplification, outperforming both neural and large language model baselines

## Executive Summary
This paper introduces a neuro-symbolic framework for learning convergent term rewriting systems to solve formula simplification problems with strong out-of-distribution generalization. The authors present two models—Neural Rewriting System (NRS) and Fast Neural Rewriting System (FastNRS)—that decompose the rewriting process into three neural modules: Selector, Solver, and Combiner. These models are evaluated on logic, list operations, arithmetic, and algebra domains, demonstrating significant improvements over baselines including GPT-4 and Neural Data Router, especially on complex OOD samples. FastNRS offers up to 500x faster inference while maintaining accuracy, and both models can handle multi-domain settings.

## Method Summary
The framework learns convergent term rewriting systems by decomposing the problem into three neural modules: Selector identifies subexpressions to rewrite, Solver generates appropriate rewrite rules, and Combiner applies the rewrite to produce the simplified expression. Two architectures are proposed: NRS, which uses a standard transformer-based approach, and FastNRS, which employs a text-segmentation technique for more efficient inference. Both models are trained on synthetic datasets representing different mathematical domains and evaluated on their ability to generalize to longer, more complex expressions not seen during training.

## Key Results
- FastNRS and NRS outperform Neural Data Router baseline and GPT-4 on complex out-of-distribution samples
- Models match or surpass OpenAI's o1-preview on hardest test cases
- FastNRS achieves up to 500x faster inference with fewer parameters while maintaining accuracy
- Both models succeed in multi-domain settings, solving multiple formula types within a single architecture

## Why This Works (Mechanism)
The neuro-symbolic approach works by explicitly mirroring the structure of classical rewriting algorithms through neural modules. By decomposing the rewriting process into distinct steps (selecting subexpressions, solving for appropriate rewrites, and combining results), the models maintain interpretability while leveraging neural networks' pattern recognition capabilities. The convergent property ensures termination and confluence, preventing infinite rewriting loops. The FastNRS's text-segmentation approach accelerates inference by reducing computational overhead while preserving the core rewriting logic.

## Foundational Learning
- Term Rewriting Systems: Why needed - Provides theoretical foundation for formula simplification; Quick check - Verify understanding of confluence and termination properties
- Neural Module Integration: Why needed - Enables decomposition of symbolic algorithms into learnable components; Quick check - Trace data flow through Selector, Solver, and Combiner
- Out-of-Distribution Generalization: Why needed - Critical for assessing real-world applicability beyond training data; Quick check - Examine performance gaps between in-distribution and OOD test sets
- Transformer Architecture: Why needed - Serves as backbone for neural modules and attention mechanisms; Quick check - Review attention patterns in Selector module
- Text Segmentation: Why needed - Enables computational efficiency in FastNRS variant; Quick check - Verify segmentation boundaries align with logical subexpression boundaries

## Architecture Onboarding

Component map: Input Formula -> Selector -> Solver -> Combiner -> Output Formula

Critical path: Input sequence flows through Selector (identifies rewrite candidates) → Solver (generates rewrite rules) → Combiner (applies rewrites) → Output sequence

Design tradeoffs: The three-module decomposition trades some end-to-end optimization potential for improved interpretability and modularity. FastNRS sacrifices some architectural flexibility for significant speed gains through text-segmentation.

Failure signatures: Selector may misidentify rewrite candidates in complex nested expressions; Solver may generate incorrect rewrite rules for unseen patterns; Combiner may produce invalid syntax when applying rewrites; FastNRS may fail when text-segmentation boundaries don't align with logical subexpression boundaries.

First experiments: 1) Test Selector module on simple in-distribution examples to verify correct subexpression identification, 2) Validate Solver module generates correct rewrite rules for known patterns, 3) Confirm Combiner maintains expression validity when applying simple rewrites.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the system be extended to handle rewriting rules that act on patterns across different, non-contiguous parts of the sequence?
- Basis in paper: [explicit] The authors explicitly state that future work should expand the system to handle rules acting on patterns across different parts of the sequence, necessitating a redesign of the Selector to generalize on non-local patterns.
- Why unresolved: The current architecture restricts the Selector to localized attention windows and text-segmentation, making it unable to identify or manipulate global, disjoint patterns.
- What evidence would resolve it: A modified architecture capable of successfully identifying and substituting non-contiguous sub-expressions (e.g., global term collection) in out-of-distribution formulas.

### Open Question 2
- Question: Can the algorithmic blueprint (the specific decomposition into Selector, Solver, and Combiner) be inferred directly from data rather than being manually defined?
- Basis in paper: [explicit] The conclusion identifies designing an end-to-end learned system where the blueprint is inferred from data as a challenging and interesting venue for future research to overcome current scope limitations.
- Why unresolved: The current design imposes a strong inductive bias by hard-coding the three-module structure based on the classical rewriting algorithm.
- What evidence would resolve it: An autonomous system that, when trained on raw formula simplification data, develops internal modules or processing steps that functionally map to the select, solve, and combine operations without architectural supervision.

### Open Question 3
- Question: How can the framework be adapted to support reasoning over hierarchical data structures or visual inputs?
- Basis in paper: [explicit] The limitations section notes that the current sequence-only framework cannot address many real-world tasks that involve hierarchical structures or visual reasoning.
- Why unresolved: The current implementation relies on sequential token processing (transformers) and specific text-segmentation techniques that do not generalize to graph or image data.
- What evidence would resolve it: An extension of the model utilizing graph neural networks or visual encoders that maintains strong out-of-distribution generalization on non-sequential reasoning tasks.

## Limitations
- Evaluation restricted to synthetic and controlled datasets in limited mathematical domains
- Generalization claims based on specific OOD splits without testing on diverse or adversarial perturbations
- FastNRS text-segmentation approach may introduce brittleness across different formula types
- Comparisons to GPT-4 and o1-preview use small test sets without considering deployment costs

## Confidence
- Core claims about Selector-Solver-Combiner decomposition effectiveness: High
- Out-of-distribution generalization results: Medium
- Efficiency gains in real-world deployment: Low

## Next Checks
1. Test models on real-world mathematical datasets or noisy, human-written expressions to assess robustness beyond synthetic data
2. Evaluate performance under adversarial perturbations (e.g., slight syntactic variations, irrelevant subexpressions) to probe brittleness in the rewriting process
3. Conduct large-scale efficiency benchmarking, including end-to-end deployment scenarios, to verify practical utility of FastNRS's speed and parameter savings