---
ver: rpa2
title: Zero-Shot Complex Question-Answering on Long Scientific Documents
arxiv_id: '2503.02695'
source_url: https://arxiv.org/abs/2503.02695
tags:
- answer
- answers
- language
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot pipeline framework for complex
  question-answering on full-length scientific documents, specifically targeting social
  science research papers. The framework addresses multi-span extraction, multi-hop
  reasoning, and long-answer generation through a combination of pre-trained language
  models including encoder-only and decoder-only Transformers.
---

# Zero-Shot Complex Question-Answering on Long Scientific Documents

## Quick Facts
- arXiv ID: 2503.02695
- Source URL: https://arxiv.org/abs/2503.02695
- Authors: Wanting Wang
- Reference count: 20
- Primary result: Zero-shot pipeline achieves 0.825-0.890 Similar Match scores on complex QA over 52 social psychology papers

## Executive Summary
This paper presents a zero-shot pipeline framework for complex question-answering on full-length scientific documents, specifically targeting social science research papers. The framework addresses multi-span extraction, multi-hop reasoning, and long-answer generation through a combination of pre-trained language models including encoder-only and decoder-only Transformers. The approach integrates Retrieval Augmented Generation (RAG) for entity extraction, decomposes multi-hop questions into single-hop sub-questions, and employs answer ensemble strategies. Evaluated on MLPsych, a novel dataset of social psychology papers with complex question annotations, the framework achieves strong performance across all four question types.

## Method Summary
The framework uses a zero-shot multi-stage pipeline with encoder-only extractive models (DeBERTa-v3-large, ALBERT-xxlarge, ELECTRA-large, RoBERTa-large, BERT-large) fine-tuned on SQuAD v2.0, combined with RAG-enhanced entity extraction using LLaMA-3-8B. The approach decomposes multi-hop questions into single-hop sub-questions, employs answer ensemble strategies, and retains top-k answers per question type. The system processes PDF documents through text extraction, runs extractive QA, applies RAG enhancement for entity categorization, and combines predictions through ensemble methods.

## Key Results
- Achieves Similar Match scores ranging from 0.825 to 0.890 across all four question types
- Demonstrates significant improvements over baseline methods with F1 scores from 0.365 to 0.685
- Shows 67.1% improvement on Q4 multi-hop reasoning through single-hop decomposition strategy

## Why This Works (Mechanism)

### Mechanism 1
RAG-enhanced extraction improves answer quality by filtering and normalizing raw encoder outputs through a generative model. Encoder-only models produce raw text spans that serve as retrieved context for LLaMA-3-8B, which generates structured, categorized answers with deduplication and format standardization. This works under the assumption that the generative model can reliably map noisy extracted spans to canonical entity names without hallucinating.

### Mechanism 2
Decomposing multi-hop questions into multiple single-hop sub-questions reduces reasoning complexity. First extract bridge entities (e.g., ML techniques from Q1), then generate sub-questions like "What was [technique X] used for?" for each entity. Each sub-question requires only single-hop retrieval. This approach assumes bridge entities can be exhaustively extracted in the first phase, though missing entities cascade into incomplete multi-hop answers.

### Mechanism 3
Answer ensemble from complementary models captures diverse extraction patterns. Combine top-k answers from two models with different inductive biases (e.g., DeBERTa + ALBERT). Union of predictions increases recall while overlap confirms confidence. This relies on the assumption that models make uncorrelated errors, though if models share systematic biases, ensemble provides no gain.

## Foundational Learning

- **Encoder-only vs. Decoder-only Transformers**: Pipeline uses encoder-only models (BERT family) for extraction and decoder-only (LLaMA) for answer normalization—each optimized for different tasks. Quick check: Given a span extraction task, which architecture would you use and why?

- **Multi-hop Reasoning with Bridge Entities**: Q4 requires identifying a bridge entity (ML technique) before answering the follow-up question (its purpose). Quick check: For "What software was used for the classification task?", what prior information constitutes the bridge entity?

- **Similar Match vs. Exact Match Metrics**: Multi-span answers have semantic equivalents ("XGBoost" ≈ "extreme gradient boosting"); traditional F1/EM fail to capture this. Quick check: Why does cosine similarity on embeddings help for answer evaluation but not for training?

## Architecture Onboarding

- **Component map**: PDF → Text Extraction (Apache Tika/Grobid) → Preprocessing → Q1-Q4: Encoder-only QA → Q1/Q2: RAG Enhancement (LLaMA-3-8B) → Answer Ensemble → Q3: Direct Answer Ensemble → Q4: Bridge Entity Extraction → Sub-question Generation → Multi-pass Extraction

- **Critical path**: Q4 is the most fragile—depends on Q1 bridge entity extraction quality. A missed technique in Q1 propagates as a missing sub-question in Q4.

- **Design tradeoffs**: Zero-shot (no fine-tuning) vs. performance ceiling—authors accept suboptimal individual model performance for accessibility. Retrieving top-20 answers (Q1) increases recall but introduces noise; RAG filtering mitigates but doesn't eliminate. Multi-single-hop improves Q4 by 67% but increases compute ~10x (9.76 sub-questions average).

- **Failure signatures**: High F1 but low Exact Match → semantic equivalence not captured by metric. Many short predicted spans with low Similar Match → encoder extracting fragments, not entities. Q4 performance collapse → check Q1 bridge entity recall first. RAG outputs more verbose than gold → check LLaMA prompt for verbosity bias.

- **First 3 experiments**: 1) Run DeBERTa on Q1 with top-5 vs. top-20; verify span count correlates with answer coverage. 2) Manually inspect Q1 outputs on 3 documents; confirm all ML techniques are captured before running Q4. 3) Compare single-model vs. DeBERTa+ALBERT ensemble on Q1; measure precision/recall tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
Can the computational overhead of the multi-single-hop decomposition strategy be reduced to enable scalability on corpora significantly larger than the 52-document MLPsych dataset? The "Limitations" section states that because inference cost scales linearly with the number of bridge entities, the approach faces "significant scalability challenges for larger corpora, where the total inference time would grow prohibitively large." This remains unresolved as the authors demonstrated efficacy on a small dataset but did not test or optimize the linear scaling of the multi-hop decomposition for large-scale applications.

### Open Question 2
How does the framework's performance vary when evaluated using metrics that penalize redundant predictions, such as micro-averaged F1, compared to the current best-match metrics? The "Discussion" notes that current metrics do not penalize redundancy, suggesting that "Alternative metrics that account for prediction volume, such as the micro-averaged F1 score... may provide more comprehensive performance assessment." This remains unresolved as the paper relies on F1, Exact Match, and Similar Match, which are computed using the best-matching span and thus may inflate performance by ignoring excessive false positives.

### Open Question 3
To what extent does the model-assisted validation step introduce systematic bias into the MLPsych dataset's ground truth annotations? The "Limitations" section notes that while model-assisted validation helped identify missed spans, "this approach may introduce potential biases in the gold standard annotations." This remains unresolved as the annotations were refined by comparing human work against model outputs, potentially creating a feedback loop where the ground truth favors the very models used for assistance.

## Limitations
- Zero-shot constraint inherently caps performance below fine-tuned alternatives
- Bridge-entity extraction for multi-hop reasoning is fragile with cascading failure modes
- MLPsych dataset's small size (52 documents, 151 QA pairs) and single-annotator construction raises generalizability concerns
- Computational overhead of multi-hop decomposition (~10x) may limit scalability

## Confidence
- **High confidence**: Similar Match and Mentions metrics effectively capture semantic equivalence in multi-span answers where traditional F1/EM fail
- **Medium confidence**: RAG-enhanced entity extraction improves answer quality through generative normalization
- **Medium confidence**: Multi-hop decomposition into single-hop sub-questions reduces reasoning complexity and improves Q4 performance by 67.1%
- **Medium confidence**: Answer ensemble strategies capture diverse extraction patterns and improve performance across all metrics

## Next Checks
1. Manually inspect Q1 outputs on 10 randomly selected documents to verify that all machine learning techniques mentioned in the text are captured as bridge entities, calculating recall rate against a ground truth checklist
2. Reconstruct and test the exact LLaMA-3-8B prompt templates for RAG-enhanced categorization using partial examples in Figures 2-3, running controlled experiments comparing outputs with and without the generative normalization step
3. Measure inference time and GPU memory usage for the full pipeline on documents of varying lengths (5, 15, 30 pages), calculating per-document cost at scale (e.g., 1000 documents) and comparing against performance gains from multi-hop decomposition