---
ver: rpa2
title: A Versatile Framework for Designing Group-Sparse Adversarial Attacks
arxiv_id: '2510.16637'
source_url: https://arxiv.org/abs/2510.16637
tags:
- attacks
- adversarial
- attack
- sparsity
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes ATOS (Attack Through Overlapping Sparsity),\
  \ a novel adversarial attack framework for generating sparse and structured perturbations\
  \ against deep neural networks (DNNs). Unlike previous attacks that neglect perturbation\
  \ sparsity, ATOS introduces the Overlapping Smoothed \u21130 (OSL0) function, which\
  \ promotes sparse and structured perturbations while ensuring convergence."
---

# A Versatile Framework for Designing Group-Sparse Adversarial Attacks

## Quick Facts
- arXiv ID: 2510.16637
- Source URL: https://arxiv.org/abs/2510.16637
- Reference count: 40
- Primary result: ATOS achieves 100% attack success rate on CIFAR-10 and ImageNet with significantly sparser perturbations than prior methods

## Executive Summary
This paper introduces ATOS (Attack Through Overlapping Sparsity), a novel adversarial attack framework that generates sparse, structured perturbations while maintaining 100% attack success rate on standard image classification models. Unlike previous attacks that neglect perturbation sparsity, ATOS employs the Overlapping Smoothed ℓ0 (OSL0) function to promote sparse and structured perturbations while ensuring convergence through a differentiable optimization approach. The framework supports three sparsity modes—element-wise, pixel-wise, and group-wise—by leveraging overlapping grouping strategies, and uses LSEAp for adaptive perturbation intensity control.

## Method Summary
ATOS is a white-box adversarial attack framework that uses first-order optimization with OSL0 regularization for sparsity and LSEAp for ℓ∞ control. The method operates in three modes (element-wise, pixel-wise, group-wise) determined by grouping parameters (n_v, s). The optimization uses a two-phase approach: convex initialization with large σ followed by gradual decrease, with step sizes controlled by Lipschitz constants. Key hyperparameters are specified per model/dataset in Tables 2-5, including iteration counts (N_i=200), group windows (n=4,s=1 for CIFAR-10; n=16,s=2 for ImageNet), and decay rates (s_σ=0.5).

## Key Results
- Achieves 100% attack success rate on CIFAR-10 and ImageNet datasets
- Produces significantly sparser perturbations than prior methods (lower NPP and ℓ0 norms)
- Group-wise attack highlights critical regions from DNN's perspective, offering improved interpretability as counterfactual explanations
- Maintains effectiveness against robust models with 98.52% ASR compared to FMR's 0.67%

## Why This Works (Mechanism)

### Mechanism 1
The Overlapping Smoothed ℓ0 (OSL0) function promotes sparse, structured perturbations while maintaining differentiability for gradient-based optimization. OSL0 groups input elements with overlap (stride s < group size n_v) and applies a smoothed ℓ0 surrogate. Each group contributes loss only if any element is non-zero, encouraging the optimizer to concentrate perturbations into fewer groups. The exponential smoothing (σ parameter) makes the function differentiable while approaching true ℓ0 as σ → 0.

### Mechanism 2
The LSEAp function provides adaptive perturbation intensity control without requiring preset per-image thresholds. LSEAp approximates ℓ∞ gradient via a soft maximum over absolute perturbation values. The relaxation parameter p controls approximation quality—the function concentrates weight on maximal elements while remaining differentiable. This allows the optimizer to automatically balance attack success against perturbation magnitude per input.

### Mechanism 3
The two-phase optimization (convex initialization with large σ, then gradual decrease) ensures convergence to stationary points even in the non-convex regime. Theorem 4.1 establishes convexity when σ ≥ x_m√(2n_v-1). The algorithm starts in this convex region, then gradually decreases σ while using Lipschitz-guided step sizes. The KL property guarantees sequence convergence to critical points.

## Foundational Learning

- **ℓ_p norms and their gradients**: ATOS fundamentally trades off ℓ0 (sparsity), ℓ∞ (perturbation magnitude), and cross-entropy (attack success). Understanding why ℓ0 is non-differentiable and how surrogates work is essential. Quick check: Why can't we directly compute ∇∥x∥₀, and what property must a surrogate function have to be useful in gradient descent?

- **Group sparsity with overlap**: The paper extends standard group sparsity to overlapping groups, which is the core novelty. Without this, you cannot understand why stride s < n_v creates "indirect connections" between elements. Quick check: If groups have size n_v = 4 and stride s = 2, how many groups does element x_5 belong to, and how does this affect its gradient weight?

- **Lipschitz continuity and step size selection**: The convergence proofs rely on setting step size η ≤ 1/L where L is the Lipschitz constant. This is standard in optimization but critical here since OSL0 becomes non-convex. Quick check: If a function has Lipschitz constant L = 100, what is the maximum step size that guarantees descent?

## Architecture Onboarding

- **Component map**: Input X, target y_t -> [Categorizing Rule R] <- Config: (n_v, s) for element/pixel/group modes -> [OSL0 Regularizer] <- σ (starts large, decreases by s_σ each step) -> [LSEAp Intensity Control] <- p parameter -> [Composite Loss] = λ·CE + λ_s·σ²·OSL0 + λ_∞·LSEAp -> [Gradient Descent] <- step size μ ≤ 1/L -> [Clip to [0,1]] -> [Quantize] -> Output Δ

- **Critical path**: 1) Initialize σ₀ ≥ x_m√(2n_v-1) for convexity, 2) Run N_i gradient iterations per σ level, 3) Decrease σ ← σ × s_σ for N_s steps, 4) Quantize final perturbation, 5) If attack fails: increase λ and restart

- **Design tradeoffs**: Group size (n) vs. stride (s): larger n and smaller s increase overlap (better structure) but computational cost is O(n²/s²). σ decay rate (s_σ): faster decay enforces sparsity sooner but risks poor local minima. λ_∞ weight: higher values reduce perturbation visibility but may fail on robust models.

- **Failure signatures**: High NPP with low ASR → λ_s too low. ASR < 100% → λ too low or iteration count insufficient. Scattered, non-structured perturbations → stride s too large or group size n too small. Divergence → step size μ exceeds 1/L bound.

- **First 3 experiments**: 1) Reproduce pixel-wise attack on CIFAR-10 CNN with parameters from Table 2 (μ=1.4, λ_s=1, n_v=3) to validate basic implementation. 2) Ablate σ initialization by comparing convex σ₀ vs. small σ. 3) Test LSEAp vs. threshold-based ℓ∞ by comparing ATOS-PW-ℓ∞ against PGD-(ℓ₀,ℓ∞) on ImageNet.

## Open Questions the Paper Calls Out

### Open Question 1
Can the ATOS framework be adapted to black-box settings where model gradients are inaccessible? The paper explicitly contrasts white-box and black-box attacks and states it aims to design a white-box attack, leaving the black-box scenario unaddressed. The ATOS optimization relies heavily on backpropagation to compute gradients for OSL0 and LSEAp functions; gradient-free or query-based optimization requires a different theoretical approach to maintain convergence guarantees. Evidence that would resolve this: An extension utilizing gradient estimation techniques (e.g., Natural Evolutionary Strategies) that maintains the 100% ASR and sparsity properties without direct weight access.

### Open Question 2
How can the grouping parameters (window size n and stride s) be optimized adaptively for specific images rather than fixed globally? Section 5.1 states that window parameters (n=4 for CIFAR-10, n=16 for ImageNet) are selected to maximize neighborhood coverage while managing computational complexity, implying a heuristic, dataset-level choice rather than an adaptive one. Fixed grids may not align with semantic boundaries of objects in every image, potentially degrading the quality of counterfactual explanations. Evidence that would resolve this: A comparative study showing adaptive grouping (e.g., guided by image segmentation) results in statistically lower ℓ₂,₀ scores or higher Interpretability Scores (IS) compared to fixed-grid baseline.

### Open Question 3
Do the structured perturbations generated by ATOS exhibit better or worse transferability to other models compared to unstructured sparse attacks? The paper notes black-box attacks are useful for "fooling inaccessible DNNs," yet experiments focus solely on white-box success. The specific structured nature of ATOS perturbations might influence how well they transfer across architectures. Evidence that would resolve this: Empirical results measuring ASR when applying perturbations generated on a source model (e.g., ResNet152) to a target model (e.g., ViT-s16), compared against unstructured methods like PGD-ℓ₀.

### Open Question 4
How can the semantic validity of the generated counterfactual explanations be quantitatively validated beyond the Interpretability Score (IS)? The conclusion claims the group-wise mode serves as an effective tool for counterfactual explanations, relying primarily on visual inspection and IS (alignment with saliency maps) for validation. While IS measures alignment with model's sensitivity, it does not necessarily confirm that the perturbation is a "meaningful" or "semantically coherent" change from a human perspective. Evidence that would resolve this: A user study or causal metric confirming humans identify the correct connection between perturbed region and target class significantly better with ATOS than with StrAttack.

## Limitations
- Lacks precise CNN architecture specifications for CIFAR-10 experiments, described only as "simple yet effective CNN blocks"
- Hyperparameter search methodology is underspecified with no ranges, trial counts, or detailed ordering provided
- Quantitative comparisons may depend heavily on undisclosed implementation details and evaluation protocols

## Confidence

- **High confidence**: Theoretical framework (OSL0 function design, convergence proofs, LSEAp mechanism) is mathematically rigorous and well-specified
- **Medium confidence**: Empirical claims (100% ASR) depend on undisclosed CNN architecture and hyperparameter search details; interpretability analysis relies on specific visualization techniques
- **Low confidence**: Quantitative comparisons (98.52% ASR on robust models vs. FMR 0.67) depend on exact implementation details and evaluation protocols

## Next Checks

1. Implement ATOS-PW with minimal parameters (μ=1.4, λ_s=1, n_v=3) on CIFAR-10 and verify NPP falls in the reported 16-61 range, confirming basic functionality
2. Ablate σ initialization strategy by comparing convex initialization (σ₀ ≥ x_m√(2n_v-1)) versus small σ initialization. Expect improved ASR and stability with convex initialization
3. Benchmark LSEAp against threshold-based ℓ∞ control by comparing ATOS-PW-ℓ∞ with PGD-(ℓ₀,ℓ∞) on ImageNet. Measure both ASR and NPP to evaluate the adaptive control mechanism