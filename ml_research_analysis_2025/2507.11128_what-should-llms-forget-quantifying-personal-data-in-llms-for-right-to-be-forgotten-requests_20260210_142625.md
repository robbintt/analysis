---
ver: rpa2
title: What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten
  Requests
arxiv_id: '2507.11128'
source_url: https://arxiv.org/abs/2507.11128
tags:
- data
- llms
- language
- memorization
- personal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of identifying personal data
  in large language models (LLMs) for compliance with the EU's Right to Be Forgotten
  (RTBF). Existing machine unlearning methods assume the data to forget is known,
  but no method exists to identify which individual-fact associations are stored in
  LLMs.
---

# What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests

## Quick Facts
- **arXiv ID**: 2507.11128
- **Source URL**: https://arxiv.org/abs/2507.11128
- **Reference count**: 40
- **Primary result**: Introduces WikiMem dataset and NLL-based metric to quantify human-fact associations in LLMs for RTBF compliance, finding memorization correlates with web presence and model scale.

## Executive Summary
This paper addresses the critical challenge of identifying which personal data is memorized by large language models, a prerequisite for implementing the EU's Right to Be Forgotten. The authors introduce WikiMem, a dataset of over 5,000 natural language canaries covering 243 human-related properties from Wikidata, and develop a model-agnostic metric using calibrated negative log-likelihood to quantify human-fact associations. By evaluating 200 individuals across 15 LLMs ranging from 410M to 70B parameters, the study demonstrates that memorization rates correlate with subject web presence and model scale, enabling systematic identification of data that should be subject to RTBF requests.

## Method Summary
The method quantifies human-fact associations using calibrated negative log-likelihood across paraphrased prompts. For each (subject, property) pair, it generates 11 canary templates (1 declarative baseline + 10 paraphrases), computes NLL for 101 candidates (ground truth + 100 counterfactuals) per template, applies calibration using generic subject baseline and similar-name adjustment, and ranks candidates. Memorization is declared if ground truth ranks first across all 11 variants, with strength measured by z-score margin. The approach uses WikiMem dataset with 5,650 canaries covering 243 Wikidata properties for 200 subjects stratified by web presence, evaluated across 15 LLMs with 4-bit quantization.

## Key Results
- Memorization rates correlate strongly with subject web presence: ~38.8% for well-known vs. ~22.4% for lesser-known subjects in LLaMA 3.1-8B
- Model scale increases memorization strength (z* scores) even when coverage plateaus across 8B-70B parameter range
- Strict threshold (all 11 paraphrases must succeed) yields 20-40% memorization rates; lenient threshold (>80%) may satisfy "reasonably accessible" legal standards
- Instruct models show near-zero memorization for gender/sex properties, suggesting explicit safety filtering

## Why This Works (Mechanism)

### Mechanism 1: Calibrated NLL-based Association Quantification
The method computes NLL for candidate completions, then applies subject calibration using a generic subject baseline to remove property-level priors, and similar-name adjustment using phonetic variants to neutralize name-based biases. If ground truth ranks first among 100 counterfactuals, the association is considered memorized. This isolates genuine subject-specific knowledge from linguistic priors and name-based biases.

### Mechanism 2: Paraphrase-based Robustness Testing
Requiring consistent rank-1 predictions across 11 paraphrased templates distinguishes robust memorization from prompt-dependent extraction. The 11 templates provide sufficient linguistic diversity to capture underlying association strength independent of phrasing, reducing false positives from prompt artifacts.

### Mechanism 3: Web Presence Correlation with Memorization
The paper stratifies 200 subjects by web presence score (Wikipedia page views, article length, language editions). Well-known subjects show significantly higher memorization rates (~38.8% vs. ~22.4%), enabling prioritization of RTBF audits based on practical memorization likelihood.

## Foundational Learning

- **Concept**: Negative Log-Likelihood (NLL)
  - Why needed here: The entire metric relies on comparing NLL values across candidate completions. Understanding that lower NLL = higher model confidence is essential for interpreting scores.
  - Quick check question: Given "Jane's profession is [X]", if NLL("biologist") = 2.1 and NLL("accountant") = 3.4, which completion does the model prefer?

- **Concept**: Calibration in Model Scoring
  - Why needed here: Raw NLL conflates genuine associations with property-level priors (e.g., "doctor" is generally more likely than "acupuncturist") and name-based biases. The calibration step isolates subject-specific knowledge.
  - Quick check question: Why might a model assign lower NLL to "John's occupation is doctor" than "Jane's occupation is doctor" even if neither fact appears in training data?

- **Concept**: Eidetic vs. Approximate Memorization
  - Why needed here: The paper targets factual associations rather than verbatim text. Understanding this distinction clarifies what the metric detects (semantic relationships vs. extractable strings).
  - Quick check question: If training data says "Her occupation is software developer" and model outputs "She works as a software engineer," is this eidetic or approximate memorization?

## Architecture Onboarding

- **Component map**: WikiMem Dataset -> Canary Generator -> NLL Scorer -> Calibration Module -> Ranking Engine -> Strength Calculator

- **Critical path**:
  1. Load LLM with native tokenizer (4-bit NF4 quantization acceptable)
  2. For each (subject, property), retrieve ground truth and 100 counterfactuals
  3. Generate 11 canary templates; compute NLL for all 101 candidates per template
  4. Apply calibration: s(h, vᵢ) = [NLL(h₀, vᵢ) - NLL(h, vᵢ)] - α·E[NLL(h₀, vᵢ) - NLL(h̃, vᵢ)]
  5. Rank candidates; declare memorization if any ground truth ranks #1
  6. Compute strength z* = (Δ* - μ) / σ where Δ* is the margin over best counterfactual
  7. Aggregate rates across subjects/properties for model-level metrics

- **Design tradeoffs**:
  - Strict vs. Lenient Threshold: Strict (all 11 paraphrases must succeed) yields 20-40% rates; lenient (any succeeds) yields >80%
  - Quantization: 4-bit preserves accuracy; 2-bit/1-bit degrades fact retrieval
  - Contextualized Canaries: Intended to reduce ambiguity, but consistently lowered memorization rates
  - Counterfactual Sampling: Random sampling may not match difficulty; no guarantee of uniform discrimination

- **Failure signatures**:
  - Zero memorization for prominent subjects: Check label alignment (e.g., Wikidata "logician" vs. common "mathematician")
  - High paraphrase variance: Indicates prompt sensitivity; consider more templates
  - Large model underperformance: Investigate training data filtering (Mistral-Small-24B showed 6.92% on lesser-known vs. expected ~12-18%)
  - Sex/gender near-zero in instruct models: Likely explicit safety filtering

- **First 3 experiments**:
  1. Replicate Table 1 on LLaMA-3.1-8B base vs. instruct for 20 subjects across 5 properties, verifying the reported ~39%/22% split
  2. Ablate counterfactual set size: test 25/50/100/200 counterfactuals to measure ranking stability
  3. Test contextualized vs. non-contextualized canaries on 10 ambiguous subjects (common names) to validate paper's counterintuitive finding

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the plateau in memorization rate (despite increasing strength) persist for models with parameters significantly larger than 70B?
  - Basis in paper: The authors note that while strength increases, coverage appeared to plateau in the 8B–70B range, stating, "Verifying whether this trend persists at greater scales (e.g., LLaMA 450B) is left for future work."
  - Why unresolved: The study was limited to models up to 70B parameters; behavior at frontier scales (100B+) remains empirically unverified.
  - What evidence would resolve it: Evaluating the WikiMem metric on current frontier models (e.g., Llama 3.1 405B) to observe if memorization rates remain static while confidence increases.

- **Open Question 2**: Can this quantification method effectively identify memorized data for private individuals in proprietary models using only black-box access?
  - Basis in paper: The conclusion states that future work includes "black-box evaluations of proprietary models and user studies involving individuals outside of Wikidata."
  - Why unresolved: The current methodology was tested exclusively on open-weight models using public figures with high web presence.
  - What evidence would resolve it: A user study where private individuals verify if the metric successfully flags data the subject provided, accessed only via API.

- **Open Question 3**: Does the reliance on Wikidata labels cause the metric to conflate factual recall with the model's preference for specific canonical phrasings?
  - Basis in paper: The limitations section notes the metric may "partially conflate factual recall with a model’s ability to parse or produce canonical Wikidata phrasing," as labels may not align with natural language.
  - Why unresolved: The current dataset uses specific Wikidata strings which might skew NLL scores based on grammatical surprise rather than factual knowledge.
  - What evidence would resolve it: A comparative analysis using natural language value labels versus strict Wikidata labels to see if ranking performance diverges.

## Limitations

- **Counterfactual sampling quality** remains a critical vulnerability. The paper uses random sampling of 100 type-consistent counterfactuals per canary without analysis of their relative difficulty or discrimination power, which could confound the claimed correlation between web presence and memorization rates.

- **Paraphrase generation and alignment** introduces multiple failure points. The paper acknowledges Wikidata labels may not match natural language usage, and while it suggests using sentence embedding similarity >0.75 and label aliases as remedies, these are not empirically validated.

- **Web presence as memorization proxy** assumes that online visibility directly translates to training data frequency and subsequent memorization. This may not hold for individuals with significant offline presence but minimal web footprint, or those affected by deduplication and filtering during training.

## Confidence

- **High confidence**: The core technical approach of using calibrated NLL for association detection is sound and internally consistent. The stratified evaluation design (well-known vs. lesser-known subjects) and the basic correlation findings with model scale are well-supported by the data presented.

- **Medium confidence**: The specific web presence correlation coefficients and the absolute memorization rates (20-40% for strict threshold) are contingent on the counterfactual sampling quality and paraphrase alignment, which lack detailed validation. The claim that larger models show higher strength scores even when coverage plateaus is plausible but not thoroughly tested across all size ranges.

- **Low confidence**: The practical utility for RTBF compliance depends on real-world legal standards and the feasibility of constructing forget sets at scale, neither of which are explored beyond the technical detection capability. The paper's discussion of "reasonably accessible" knowledge and legal thresholds remains speculative without engagement with actual RTBF case law or regulatory frameworks.

## Next Checks

1. **Counterfactual discrimination analysis**: For 10 subjects across all 5 properties, manually evaluate the plausibility and difficulty of the 100 counterfactuals. Compute the variance in NLL scores between top and bottom counterfactuals to ensure they provide meaningful discrimination. If variance is low (<1.0 nats), the sampling method needs refinement.

2. **Label alignment audit**: For all false negatives where memorization is expected (high web presence subjects), use sentence embedding similarity and Wikidata aliases to identify semantically equivalent alternatives to the canonical label. Report the percentage of false negatives that could be rescued through this alignment step to quantify the impact of label mismatch.

3. **Scale-dependent strength validation**: Replicate the memorization strength analysis (z* scores) for a continuous range of model sizes (e.g., 1B, 3B, 7B, 13B, 34B, 70B) on the same subject set. Plot z* vs. parameter count to verify whether strength continues increasing beyond the point where memorization rates plateau, as claimed.