---
ver: rpa2
title: How Small Can You Go? Compact Language Models for On-Device Critical Error
  Detection in Machine Translation
arxiv_id: '2511.09748'
source_url: https://arxiv.org/abs/2511.09748
tags:
- translation
- gemma
- qwen
- error
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether compact, sub-2B-parameter language\
  \ models can perform Critical Error Detection (CED) in English-to-German machine\
  \ translation with sufficient accuracy for on-device deployment. Using a unified\
  \ prompt-based framework with few-shot exemplars, majority voting, and lightweight\
  \ logit-bias calibration, the study benchmarks five compact models\u2014LFM2-350M,\
  \ Qwen 3-0.6B/1.7B, Llama 3.2-1B-Instruct, and Gemma 3-1B\u2014across WMT21, WMT22,\
  \ and SynCED-EnDe 2025."
---

# How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation

## Quick Facts
- arXiv ID: 2511.09748
- Source URL: https://arxiv.org/abs/2511.09748
- Reference count: 40
- Compact sub-2B-parameter models achieve strong CED performance with sub-400ms latency on capable hardware

## Executive Summary
This paper investigates whether compact language models can perform Critical Error Detection (CED) in English-to-German machine translation with sufficient accuracy for on-device deployment. Using a unified prompt-based framework with few-shot exemplars, majority voting, and lightweight logit-bias calibration, the study benchmarks five compact models across WMT21, WMT22, and SynCED-EnDe 2025. Results show that 1B-parameter models like Gemma 3-1B and Llama 3.2-1B-Instruct achieve strong performance, with Gemma 3-1B reaching MCC=0.77 and F1-ERR=0.98 on SynCED-EnDe 2025 after merged-weights fine-tuning, all while maintaining sub-400ms latency on a MacBook Pro M4 Pro. Performance saturates around 1B parameters, with diminishing returns beyond 1.7B.

## Method Summary
The study employs a unified prompt-based framework for CED that incorporates few-shot exemplars, majority voting across multiple prompt variations, and lightweight logit-bias calibration. Five compact models are evaluated: LFM2-350M, Qwen 3-0.6B/1.7B, Llama 3.2-1B-Instruct, and Gemma 3-1B. The models are tested on three datasets (WMT21, WMT22, SynCED-EnDe 2025) with both in-context learning and merged-weights fine-tuning approaches. Latency measurements are conducted on a MacBook Pro M4 Pro with Apple Silicon. The evaluation focuses on key metrics including MCC, F1-ERR, and F1-NER, while also assessing model efficiency in terms of VRAM usage and inference speed.

## Key Results
- 1B-parameter models (Gemma 3-1B, Llama 3.2-1B-Instruct) achieve strong CED performance with MCC up to 0.77 and F1-ERR up to 0.98
- Performance saturates around 1B parameters, with minimal gains from 1B to 1.7B models
- Sub-400ms latency achieved on MacBook Pro M4 Pro for all tested models
- Ultra-small 0.6B models remain usable with calibration but under-detect entity and number errors

## Why This Works (Mechanism)
The unified prompt-based framework with majority voting and logit-bias calibration effectively addresses the CED task by providing multiple perspectives on translation quality assessment. The few-shot exemplars help models understand the error detection task, while majority voting reduces the impact of individual prediction errors. Logit-bias calibration specifically targets the imbalance between error and non-error classifications, improving detection accuracy for critical errors.

## Foundational Learning
- **Critical Error Detection (CED)**: Task of identifying severe translation errors that compromise meaning; needed to ensure translation quality and safety in real-world applications
- **Few-shot learning**: Technique of learning from limited examples; quick check: model performance with varying numbers of exemplars in prompts
- **Logit-bias calibration**: Method to adjust model output probabilities; quick check: calibration effectiveness across different error types
- **Majority voting**: Ensemble technique combining multiple predictions; quick check: improvement in performance metrics compared to single predictions
- **Merged-weights fine-tuning**: Parameter-efficient adaptation method; quick check: performance comparison between in-context learning and fine-tuning
- **Latency profiling**: Measurement of inference speed; quick check: correlation between model size and latency on target hardware

## Architecture Onboarding
**Component Map**: Input Translation -> Prompt Generation -> Model Inference -> Logit-Bias Calibration -> Majority Voting -> CED Output

**Critical Path**: The critical path involves prompt generation, model inference, and majority voting, with calibration applied to individual model outputs before aggregation.

**Design Tradeoffs**: Model size vs. accuracy (saturates at ~1B parameters), in-context learning vs. fine-tuning (merged-weights provides consistent gains), privacy vs. performance (on-device deployment vs. cloud inference).

**Failure Signatures**: Under-detection of entity and number errors in ultra-small models, performance degradation on low-resource language pairs, calibration instability across different translation domains.

**3 First Experiments**:
1. Compare CED performance between 1B and 1.7B models on SynCED-EnDe 2025 to validate saturation claim
2. Test majority voting effectiveness by comparing single-model vs. ensemble predictions
3. Evaluate calibration impact by measuring performance with and without logit-bias adjustment

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the identified 1B-parameter "sweet spot" for efficiency and accuracy generalize to typologically diverse language pairs?
- Basis in paper: [explicit] Section VI-D states the need for "multilingual extensions of SynCEDâ€“EnDe to typologically diverse languages where annotation remains scarce."
- Why unresolved: The current study exclusively evaluates English-to-German translation, leaving performance on structurally different languages unknown.
- What evidence would resolve it: Benchmarks of compact models on low-resource or non-Indo-European language pairs to verify if the scaling trends hold.

### Open Question 2
- Question: Can hybrid architectures successfully balance privacy and accuracy by combining on-device screening with cloud-scale reasoning?
- Basis in paper: [explicit] Section VI-D proposes that "future work will explore hybrid pipelines that combine on-device screening with cloud-scale reasoning."
- Why unresolved: The study evaluates on-device and cloud-scale capabilities in isolation but does not test interaction or routing logic between them.
- What evidence would resolve it: A system evaluation measuring accuracy and privacy trade-offs in a cascaded architecture where low-confidence local predictions trigger cloud fallbacks.

### Open Question 3
- Question: Do the low latency and memory footprint of sub-2B models directly correlate with reduced energy consumption on edge hardware?
- Basis in paper: [inferred] While the paper claims sustainability, Section VII notes that "Incorporating power and energy metrics will further substantiate the claim of sustainable, edge-ready CED."
- Why unresolved: The study relies on latency and VRAM as proxies for efficiency but lacks empirical wattage or battery-life measurements.
- What evidence would resolve it: Profiling data reporting actual energy consumption (e.g., via Apple Energy Diagnostics) during sustained inference.

## Limitations
- Evaluation limited to English-to-German translation, restricting generalizability to other language pairs
- Single hardware configuration (MacBook Pro M4 Pro) used for latency measurements
- No security analysis of on-device deployment vulnerabilities or model tampering risks
- Calibration effectiveness not tested across extended deployment periods or varying quality distributions

## Confidence
- **High confidence**: Core finding that 1B-parameter models achieve strong CED performance with sub-400ms latency on capable hardware
- **Medium confidence**: Saturation claim around 1B parameters, as gains between 1B and 1.7B models are minimal and dataset-dependent
- **Medium confidence**: Calibration effectiveness demonstrated but alternative methods and long-term stability not explored
- **Low confidence**: Generalizability to other language pairs, hardware constraints, or real-world deployment scenarios

## Next Checks
1. Evaluate model performance and latency on resource-constrained devices (e.g., smartphones, IoT devices) to assess true on-device feasibility across hardware tiers
2. Test the unified framework across diverse language pairs and translation domains to validate generalizability beyond English-German translation
3. Implement longitudinal studies measuring calibration stability and performance drift over extended deployment periods with varying translation quality distributions