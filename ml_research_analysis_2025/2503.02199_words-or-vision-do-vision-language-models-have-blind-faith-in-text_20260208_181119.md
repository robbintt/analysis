---
ver: rpa2
title: 'Words or Vision: Do Vision-Language Models Have Blind Faith in Text?'
arxiv_id: '2503.02199'
source_url: https://arxiv.org/abs/2503.02199
tags:
- text
- fvlm
- data
- arxiv
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Vision-language models exhibit a \u201Cblind faith in text\u201D\
  \ phenomenon, where they prefer textual data over visual information when inconsistencies\
  \ arise, leading to significant performance drops under corrupted text. The authors\
  \ investigated this bias across four vision-centric tasks using ten models, finding\
  \ that text preference increases with text relevance, token order, and language\
  \ model size."
---

# Words or Vision: Do Vision-Language Models Have Blind Faith in Text?

## Quick Facts
- arXiv ID: 2503.02199
- Source URL: https://arxiv.org/abs/2503.02199
- Reference count: 40
- Vision-language models prefer textual data over visual information when inconsistencies arise, leading to significant performance drops under corrupted text.

## Executive Summary
Vision-language models (VLMs) exhibit a "blind faith in text" phenomenon, where they prioritize textual data over visual information when inconsistencies arise, leading to significant performance drops under corrupted text. The authors investigated this bias across four vision-centric tasks using ten models, finding that text preference increases with text relevance, token order, and language model size. Instruction prompts and supervised fine-tuning with text augmentation were explored to mitigate the bias, with fine-tuning showing greater effectiveness. A theoretical analysis suggests the bias stems from an imbalance of pure text and multi-modal data during training. These findings highlight the need for balanced training and careful consideration of modality interactions to enhance VLM robustness in real-world applications.

## Method Summary
The authors evaluated modality preference in VLMs using four vision-centric tasks (VQAv2, DocVQA, MathVista, Brand Detection) with 16,500 samples across three text variations (match, corruption, irrelevance). They measured Text Preference Ratio (TPR) and accuracy metrics when visual and textual inputs conflicted. Ten VLMs were tested including LLaVA-NeXT-7B/13B/34B, Phi3.5, Molmo-7B-D, Qwen2-VL-7B, GPT-4o, and Claude-3.5-Sonnet. Text corruption was generated using GPT-4o with a specific prompt template. Mitigation strategies included instruction prompts and supervised fine-tuning with text augmentation using 1,000 samples across different text conditions.

## Key Results
- Text preference increases with text relevance, token order, and language model size
- VLMs show significant accuracy drops under corrupted text (e.g., LLaVA-NeXT-7B accuracy fell from ~79% to ~29% on VQAv2)
- SFT with text augmentation was more effective than prompting (~43% TPR reduction vs ~2% accuracy gain)
- Token order significantly affects bias, with text-first inputs increasing text preference

## Why This Works (Mechanism)

### Mechanism 1: Data Imbalance-Driven Prioritization
The "blind faith in text" phenomenon is primarily driven by a severe imbalance in pre-training data, where pure text data (N) vastly outnumbers multi-modal data (M), causing the model's loss minimization to favor its stronger textual prior. During Empirical Risk Minimization, the model's parameters are optimized to minimize a combined loss. When N ≫ M, the gradient signals from the abundant text data dominate, effectively training the model to treat textual patterns as the primary source of truth and visual signals as secondary or supplementary.

### Mechanism 2: Autoregressive Positional Bias
The sequential nature of autoregressive generation in VLMs creates a positional bias where earlier tokens disproportionately influence the model's internal state, causing it to commit to textual patterns before fully integrating visual evidence. When text tokens precede image tokens, the model's initial hidden states are conditioned primarily on language. This creates a coherent linguistic context that the model is then biased to preserve. When later attending to image tokens, it may interpret or de-emphasize conflicting visual information to maintain consistency with its already-formed textual context.

### Mechanism 3: Miscalibrated Modality Certainty
VLMs use an implicit confidence score to choose between modalities, but this certainty is miscalibrated, often assigning higher predictive likelihood (certainty) to plausible but incorrect text than to accurate visual data. The model computes an internal certainty score (e.g., length-normalized likelihood) for answers derived from text-only (Ptxt) and image-only (Pimg) contexts. Since text is often coherent and semantically rich, the language model component may assign it a high Ptxt. In contrast, the mapping from vision to text is noisier, potentially resulting in a lower Pimg, even when the visual answer is correct. The model defaults to the higher-certainty modality (text).

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architecture**
  - Why needed here: This paper's analysis is about the tension between a model's visual and textual components. Understanding that a VLM is fundamentally an LLM augmented with a visual encoder is critical. The "blind faith" is the LLM reverting to its native modality (text) under pressure.
  - Quick check question: Can you describe the two main components of a standard VLM and explain how the LLM component processes the output of the vision encoder?

- **Concept: Autoregressive Generation and Attention**
  - Why needed here: The paper identifies token order as a key exacerbating factor. This only makes sense if you grasp that these models generate output token-by-token, with each new token attending to all previous ones. This sequential dependency is the root of the positional bias the paper describes.
  - Quick check question: How does the causal nature of autoregressive generation create a potential bias based on which modality (image or text) is presented first in the input sequence?

- **Concept: Model Calibration and Certainty**
  - Why needed here: The paper uses "Uni-Modal Certainty" to explain behavior. You must understand that a model's predicted probability (its "certainty") is not the same as correctness. The core failure mode is the model being highly *certain* in the wrong (textual) answer.
  - Quick check question: What does it mean for a model to be "miscalibrated," and how could high certainty in a textual description lead a VLM to ignore contradictory visual evidence?

## Architecture Onboarding

**Component Map:**
- Inputs: (Image, Text/Context, Question) -> Vision Encoder (e.g., CLIP) -> Visual Embeddings & Tokenizer -> Text Embeddings
- Core Processor: Visual & Text Embeddings -> Multi-Modal Projector -> Combined Token Sequence -> Transformer LLM (with multi-head attention)
- Output Generator: Final Layer Hidden States -> Autoregressive Head -> Probability Distribution over Vocabulary -> Sampled Token
- Key Metric (Derived): Text Preference Ratio (TPR) = p_txt / (p_txt + p_img)

**Critical Path:**
1. Tokenization & Ordering: How the combined sequence of visual and text tokens is constructed is the first critical decision point. The order (e.g., [TEXT, IMAGE] vs. [IMAGE, TEXT]) is shown to directly influence the bias.
2. Attention & Fusion: Within the Transformer LLM layers, the cross-attention between text and image tokens is where information is fused. If attention weights skew towards text tokens, the visual signal is lost here.
3. Certainty-Guided Generation: The final output token is sampled based on the model's predicted probabilities. This is where the implicit certainty (Pimg vs. Ptxt) in the hidden states translates into a final answer favoring one modality.

**Design Tradeoffs:**
- Data Quantity vs. Balance: The standard approach uses vast amounts of text-only data (cheap) and less multi-modal data (expensive). This tradeoff creates the N ≫ M imbalance, directly fueling text bias. Fixing it requires costly balanced data curation.
- Mitigation: Prompting vs. Fine-Tuning (SFT): Prompting is a lightweight inference-time intervention. SFT is a heavyweight training-time intervention. The paper shows prompting is easy but weak (~2% accuracy gain), while SFT is powerful (~43% TPR drop) but requires data and compute.
- Modality Certainty Calibration: Trying to fix the certainty mechanism is theoretically sound but complex to implement. The paper's chosen mitigation (SFT with text augmentation) is a more indirect but practical approach to achieving better balance.

**Failure Signatures:**
- High Text Preference Ratio (TPR): A TPR significantly above 50% on corruption tasks is the defining quantitative signature of this failure mode.
- Corruption-Induced Accuracy Drop: A large performance gap between a clean baseline and a corruption task (e.g., accuracy falling from ~79% to ~29% on VQAv2 for LLaVA-NeXT-7B).
- "Correct but Groundless" Generations: A qualitative signature where the model produces a fluent, confident answer that perfectly matches the corrupt text but contradicts the visual input.

**First 3 Experiments:**
1. Token Order Ablation: Replicate the paper's core finding by running a VLM on a corruption task with two prompts: one where the corrupt text is placed *before* the image tokens and one where it's placed *after*. Measure and compare the Text Preference Ratio (TPR).
2. Mitigation Technique Comparison: Implement both the "Focus on Image" prompt and a basic SFT with text augmentation (using a small held-out dataset). Compare their effectiveness in reducing TPR on a corruption task to see the magnitude of the prompting vs. fine-tuning tradeoff firsthand.
3. Certainty Correlation Analysis: For a set of inconsistent inputs, compute the model's Uni-Modal Certainty (Ptxt and Pimg) for each sample. Plot the model's choice (Image, Text, or Other answer) against these certainty scores to verify the paper's claim that the model's decision boundary is driven by this implicit certainty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning (SFT) methods be optimized to mitigate the "blind faith" bias without sacrificing the model's ability to utilize helpful (matched) textual information?
- Basis in paper: Explicit (Table 4 shows SFT reduces "Match" accuracy compared to the baseline; Conclusion notes "balancing robustness and effectiveness... remains challenging").
- Why unresolved: The results demonstrate a trade-off where increasing robustness to corrupted text causes the model to reject or perform worse on correct text, reducing overall utility in scenarios with reliable text.
- Evidence: A training regimen where the model maintains or improves upon the baseline "Match" accuracy while simultaneously reducing the Text Preference Ratio (TPR) in corruption cases.

### Open Question 2
- Question: Does empirically adjusting the ratio of pure-text to multi-modal data during the pre-training phase resolve the "blind faith" bias, as suggested by the theoretical bounds?
- Basis in paper: Inferred (Theorem A.5 suggests the bias stems from an imbalance of pure-text N and multi-modal M data; Section 5 analyzes this theoretically but empirical validation is limited to fine-tuning).
- Why unresolved: The paper establishes a theoretical link between data imbalance and bias but does not conduct controlled experiments to verify if balancing these datasets during the initial pre-training stage eliminates the root cause.
- Evidence: Training VLMs from scratch with varying ratios of text-to-multi-modal data and observing the corresponding change in Text Preference Ratio (TPR).

### Open Question 3
- Question: How can text bias mitigation strategies be adapted to generalize effectively to tasks requiring distinct reasoning skills, such as mathematical reasoning (MathVista)?
- Basis in paper: Explicit (Section 4.2 notes that generalization improvements are "smallest on MathVista, likely due to a greater distribution shift").
- Why unresolved: While SFT with text augmentation works for general VQA and document tasks, the method fails to transfer robustness effectively to mathematical domains, indicating a limitation in the current mitigation approach.
- Evidence: A mitigation technique that yields statistically significant reductions in TPR on the MathVista benchmark comparable to the reductions seen on VQAv2 or DocVQA.

## Limitations

- The core phenomenon may be more pronounced in autoregressive architectures than in non-autoregressive alternatives
- The text corruption method relies on GPT-4o for consistency, introducing variability in how well corrupted text remains semantically plausible
- The SFT mitigation uses only 1,000 samples, which may not scale to larger models or more complex tasks
- The theoretical analysis provides intuition but doesn't establish causation
- The study focuses on English and may not capture cross-lingual or multimodal interaction effects

## Confidence

- **Text preference phenomenon exists**: High confidence
- **Data imbalance causes the bias**: Medium confidence
- **Token order exacerbates the bias**: High confidence
- **SFT with text augmentation is the best mitigation**: Medium confidence

## Next Checks

1. **Data imbalance manipulation**: Train or fine-tune two versions of the same VLM—one with M ≈ N (balanced) and one with N ≫ M (imbalanced)—then compare their TPR on the same corruption task to directly test the causal link between data imbalance and text bias.

2. **Token-order reversal across modalities**: Extend the token order ablation beyond just text-before-image to include image-only prompts with prepended irrelevant text, measuring whether the positional bias persists when text is not task-relevant.

3. **Cross-lingual robustness test**: Apply the corruption framework to a multilingual VQA dataset (e.g., X-VLM or multilingual DocVQA) to determine whether the "blind faith" phenomenon generalizes across languages or is specific to English-language pretraining.