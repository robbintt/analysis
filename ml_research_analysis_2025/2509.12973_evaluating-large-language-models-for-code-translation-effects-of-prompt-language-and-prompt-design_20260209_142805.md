---
ver: rpa2
title: 'Evaluating Large Language Models for Code Translation: Effects of Prompt Language
  and Prompt Design'
arxiv_id: '2509.12973'
source_url: https://arxiv.org/abs/2509.12973
tags:
- prompt
- code
- translation
- language
- java
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models for code translation
  among C++, Java, Python, and C, comparing them with traditional baselines. Using
  BLEU and CodeBLEU metrics, the authors assess translation quality under different
  prompt styles (concise vs.
---

# Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design

## Quick Facts
- arXiv ID: 2509.12973
- Source URL: https://arxiv.org/abs/2509.12973
- Authors: Aamer Aljagthami; Mohammed Banabila; Musab Alshehri; Mohammed Kabini; Mohammad D. Alahmadi
- Reference count: 15
- Primary result: Detailed prompts improve translation quality; English prompts outperform Arabic by 13-15% in CodeBLEU.

## Executive Summary
This study evaluates large language models (GPT-4o, Gemini 2.0, DeepSeek-V3, Claude 3.7) for code translation among C++, Java, Python, and C#, comparing them against the TransCoder baseline. Using BLEU and CodeBLEU metrics, the authors assess translation quality under different prompt styles (concise vs. detailed) and languages (English vs. Arabic). Experiments show that detailed prompts consistently improve performance across all models and translation directions. English prompts outperform Arabic prompts by 13-15%. The top-performing model achieves the highest CodeBLEU scores on challenging pairs like Java to C# and Python to C++. All evaluated LLMs surpass the traditional TransCoder baseline.

## Method Summary
The study uses a random sample of 24 functions from the MuST dataset, filtered to include only entries with parallel implementations across C++, Java, Python, and C#. Each function is translated in all 12 directed pairs using two prompt styles (concise instruction vs. detailed 10-requirement specification) and two prompt languages (English and Arabic). Zero-shot API calls are made to four LLMs and TransCoder baseline. Outputs are evaluated using BLEU (n-gram overlap) and CodeBLEU (syntax and data-flow aware) metrics, aggregated by model, direction, prompt style, and language.

## Key Results
- Detailed prompts deliver consistent gains across models and translation directions in both BLEU and CodeBLEU.
- English prompts outperform Arabic prompts by 13-15% in CodeBLEU, with similar trends in BLEU.
- The top-performing model achieves highest CodeBLEU on challenging pairs such as Java → C# and Python → C++.
- All evaluated LLMs outperform the TransCoder baseline.

## Why This Works (Mechanism)

### Mechanism 1
Detailed prompt specifications improve code translation quality across all models and language pairs. Explicit constraints reduce ambiguity in the translation task, narrowing the hypothesis space for the model. The 10-item requirement list provides structured guidance that aligns model output with syntactic and semantic expectations. Core assumption: models' pre-training includes sufficient exposure to both source and target languages to map detailed natural language specifications to code transformations. Break condition: when source code contains idioms or library calls with no direct target-language equivalent.

### Mechanism 2
English prompts outperform Arabic prompts by 13-15% in CodeBLEU scores. Pre-training corpora for evaluated LLMs are English-dominant in code-related contexts, creating stronger conditioning for English instructions mapping to code transformations compared to Arabic. Core assumption: the performance gap reflects training data distribution rather than inherent linguistic limitations of Arabic. Break condition: models specifically fine-tuned on multilingual instruction-following data may show reduced or reversed gaps.

### Mechanism 3
Translation difficulty is direction-dependent, with stricter type-system targets (C++, C#) proving harder than dynamic targets (Python). Translating from dynamically-typed to statically-typed languages requires inferring type annotations, memory management patterns, and explicit declarations absent in source. Asymmetries reflect information loss vs. information synthesis requirements. Core assumption: evaluation metrics capture meaningful differences in translation difficulty. Break condition: when source code includes explicit type hints, the asymmetry may diminish.

## Foundational Learning

- **BLEU vs. CodeBLEU metrics**
  - Why needed: BLEU captures surface n-gram overlap; CodeBLEU adds AST matching and data-flow analysis. Understanding this distinction is necessary to interpret model performance differences.
  - Quick check: If a translation preserves all variable names but changes the control-flow structure, which metric would show a larger drop?

- **Zero-shot code translation**
  - Why needed: All evaluated LLMs operate in zero-shot mode, meaning performance reflects pre-trained capabilities activated purely through prompting.
  - Quick check: What does "zero-shot" imply about how the model learned to translate between programming languages?

- **Type system strictness gradient**
  - Why needed: The paper's directionality findings hinge on understanding that C++/C# enforce static typing while Python is dynamic.
  - Quick check: Why would translating Python list operations to C++ require more decisions than translating C++ std::vector operations to Python?

## Architecture Onboarding

- **Component map**: CSV Input -> Prompt Construction -> Model Layer (4 LLMs + TransCoder) -> Output CSV -> BLEU/CodeBLEU Evaluation
- **Critical path**: 1) Sample functions from MuST dataset 2) Construct prompts per direction × style × language 3) Execute model inference 4) Compute BLEU and CodeBLEU 5) Aggregate by model, direction, prompt style, and language
- **Design tradeoffs**: Small dataset (24 functions) enables controlled analysis but limits generalizability; BLEU is widely comparable but misses structural correctness; zero-shot reflects real-world deployment but fine-tuned models can approach LLM performance
- **Failure signatures**: Low BLEU/high CodeBLEU suggests correct logic but different conventions; high BLEU/low CodeBLEU indicates structural errors; large gaps between prompt languages signal poor conditioning for non-English instructions; direction reversal asymmetry >20% indicates systematic difficulty
- **First 3 experiments**: 1) Baseline replication with concise English prompts on 10-function subset 2) Prompt style A/B test for single model on 5 hard directions 3) Language gap isolation for single direction across 2 models

## Open Questions the Paper Calls Out

1. What specific prompt designs or tuning strategies can effectively close the 13-15% performance gap between English and non-English (Arabic) prompts?
2. Do the high BLEU and CodeBLEU scores achieved by LLMs correlate with functional correctness when evaluated using execution-based checks?
3. Do the observed benefits of detailed prompt specifications and English prompts persist when scaling the dataset or broadening language coverage?

## Limitations
- 24-function sample size may not capture full diversity of programming patterns and idioms
- Performance gap between English and Arabic prompts sensitive to exact wording of Arabic translations
- Zero-shot API evaluation lacks specification of decoding parameters (temperature, max tokens, top-p)

## Confidence

- **High Confidence**: Detailed prompts improve translation quality (consistent gains across all models and directions)
- **Medium Confidence**: 13-15% English/Arabic performance gap (directional claim well-supported but lacks full reproducibility)
- **Medium Confidence**: Direction-dependent difficulty claims (plausible but limited by sample size and absence of statistical significance tests)

## Next Checks

1. **Prompt fidelity replication**: Translate concise and detailed prompts into Arabic using standardized technical glossary; run 10-function subset across 2 models to verify 13-15% CodeBLEU gap
2. **Direction difficulty stress test**: Select 3 hard and 3 easy directions; run 20 functions per direction across 2 models; compute BLEU/CodeBLEU per direction; perform pairwise t-tests
3. **Reproducibility audit**: Reconstruct 24-function sample using same MuST dataset snapshot and random seed; rerun concise English prompts on 2 models; compare scores to published results and document deviations