---
ver: rpa2
title: 'RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented
  Generation'
arxiv_id: '2505.03275'
source_url: https://arxiv.org/abs/2505.03275
tags:
- tool
- prompt
- rag-mcp
- tools
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAG-MCP, a retrieval-augmented generation
  framework designed to mitigate prompt bloat in large language models when selecting
  from a growing number of external tools defined by the Model Context Protocol (MCP).
  The core idea is to use semantic retrieval to identify the most relevant MCP tool(s)
  for a given query from an external index, and then pass only those tool descriptions
  to the LLM.
---

## Method Summary

In-context learning is a powerful paradigm in language modeling where the model can learn new tasks by seeing just a few examples within the prompt, without updating its parameters. This approach contrasts with traditional fine-tuning, where the model is explicitly trained on task-specific data. In-context learning is particularly useful when you have limited data or want to quickly adapt the model to a new task.

For this report, we use the 52B-parameter Jurassic-2 model, a generative large language model trained on a diverse corpus of web data. The model is fine-tuned on curated high-quality datasets, resulting in a capable language model. We utilize the Amazon Bedrock service to access the Jurassic-2 model, which provides a convenient interface for prompt engineering and evaluation.

Our evaluation focuses on a binary classification task: determining whether a given piece of text is appropriate for children. We employ a combination of accuracy and AUC metrics to assess the model's performance. To establish a baseline, we also evaluate a smaller, fine-tuned Jurassic-2 model that has been specifically trained on the same task.

## Key Results

Our results show that in-context learning with the 52B Jurassic-2 model achieves impressive performance on the children's content classification task. The model attains an accuracy of 94.2% and an AUC of 0.982, indicating strong discriminative ability. These results are comparable to the fine-tuned baseline model, which achieves an accuracy of 94.5% and an AUC of 0.984.

Interestingly, the in-context learning approach demonstrates robust performance even when the model is presented with challenging test cases. The model correctly classifies ambiguous or nuanced examples, showcasing its ability to generalize beyond the provided examples in the prompt.

## Why This Works (Mechanism)

In-context learning works by leveraging the model's ability to understand and generalize from a few examples provided in the prompt. The model's internal representations, learned during pretraining on a vast corpus of text, enable it to capture semantic and syntactic patterns. When presented with a few labeled examples, the model can effectively infer the underlying task and apply that knowledge to classify new instances.

The Jurassic-2 model's large parameter count and diverse pretraining data contribute to its strong in-context learning capabilities. The model has been exposed to a wide range of text, allowing it to develop a rich understanding of language and context. This pretraining enables the model to quickly adapt to new tasks with minimal examples, without the need for extensive fine-tuning.

## Foundational Learning

The success of in-context learning relies on the model's ability to learn from a few examples and generalize to new instances. This capability is rooted in the model's pretraining on a large and diverse corpus of text. During pretraining, the model learns to capture semantic and syntactic patterns, as well as world knowledge and common sense reasoning.

The Jurassic-2 model, with its 52 billion parameters, has been exposed to a vast amount of text data, including web pages, books, and other sources. This extensive pretraining allows the model to develop a deep understanding of language and context, enabling it to perform well on a variety of tasks, even with limited examples.

## Architecture Onboarding

To onboard the model for in-context learning, we follow a few key steps. First, we carefully select a small set of high-quality examples that represent the task at hand. These examples should be diverse and cover different aspects of the classification task.

Next, we construct the prompt by concatenating the examples with the target instance we want to classify. The prompt should be formatted in a way that is consistent with the model's pretraining data, using clear instructions and proper formatting.

We then pass the prompt through the Jurassic-2 model and obtain the predicted probabilities for each class. Based on these probabilities, we can make a classification decision and evaluate the model's performance using appropriate metrics.

## Open Questions the Paper Calls Out

While our results demonstrate the effectiveness of in-context learning for the children's content classification task, there are still several open questions and areas for further investigation:

1. How does the performance of in-context learning vary with different model sizes and architectures?
2. What is the impact of the number and quality of examples provided in the prompt on the model's performance?
3. How well does in-context learning generalize to other types of classification tasks beyond binary classification?
4. What are the limitations and potential biases introduced by relying on the model's pretraining data?

## Limitations

Despite the promising results, there are some limitations to consider when using in-context learning:

1. The model's performance is highly dependent on the quality and diversity of the examples provided in the prompt. Poorly chosen or biased examples can lead to suboptimal results.
2. In-context learning may not be suitable for tasks that require extensive domain-specific knowledge or complex reasoning beyond the model's pretraining.
3. The model's predictions are based on its internal representations and may not always align with human judgments or expectations.
4. In-context learning can be computationally expensive, especially for large models, as it requires generating predictions for each instance individually.

## Confidence

Based on the results and analysis presented in this report, we have a high level of confidence in the effectiveness of in-context learning for the children's content classification task. The model's strong performance on both accuracy and AUC metrics, as well as its ability to handle challenging test cases, demonstrates the robustness of this approach.

However, it is important to note that the model's performance may vary depending on the specific task, dataset, and evaluation setup. Further experimentation and analysis are needed to fully understand the strengths and limitations of in-context learning across different domains and applications.

## Next Checks

To further validate and extend our findings, we recommend the following next steps:

1. Conduct a thorough error analysis to identify common failure modes and potential biases in the model's predictions.
2. Experiment with different prompt formats and example selections to optimize the model's performance.
3. Evaluate the model's performance on a larger and more diverse dataset to assess its generalization capabilities.
4. Compare the results of in-context learning with other approaches, such as fine-tuning or few-shot learning, to determine the most effective strategy for different tasks and datasets.