---
ver: rpa2
title: 'Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi
  Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks'
arxiv_id: '2501.09609'
source_url: https://arxiv.org/abs/2501.09609
tags:
- indoor
- adversarial
- positioning
- ensemble
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the enhancement of Wi-Fi-based indoor positioning
  systems' robustness against adversarial attacks, specifically Wi-Fi spoofing and
  signal strength manipulation. The research introduces a novel approach combining
  adversarial training and ensemble modeling, utilizing Kolmogorov-Arnold Networks
  (KAN) as the underlying architecture.
---

# Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks

## Quick Facts
- **arXiv ID**: 2501.09609
- **Source URL**: https://arxiv.org/abs/2501.09609
- **Reference count**: 40
- **Primary result**: Robust model achieves 10% reduction in positioning error; ensemble model shows lowest error rates under adversarial conditions

## Executive Summary
This paper introduces a novel approach to enhance Wi-Fi-based indoor positioning systems' robustness against adversarial attacks, specifically Wi-Fi spoofing and signal strength manipulation. The method combines adversarial training and ensemble modeling using Kolmogorov-Arnold Networks (KAN) as the underlying architecture. Three models are developed and evaluated: a baseline model, an adversarially trained robust model, and an ensemble model that combines predictions from the base and robust models. The robust model is trained on adversarially perturbed data to improve resilience, while the ensemble model further optimizes performance by leveraging the strengths of both the base and robust models. Experimental results demonstrate significant improvements in positioning accuracy, with the robust model achieving a 10% reduction in error compared to the baseline, and the ensemble model showing the lowest error rates under adversarial conditions.

## Method Summary
The study proposes a three-pronged approach to secure indoor Wi-Fi positioning: (1) a baseline KAN model trained on clean RSSI data, (2) a robust model trained with adversarially perturbed samples using Gaussian noise injection and multiplicative scaling, and (3) an ensemble model that combines predictions from both base and robust models with optimized weights. All models use RobustScaler normalization, Huber loss function, and are trained on the ESP32C3 WiFi FTM RSSI Indoor Localization dataset. The robust model is trained on a union of clean and adversarial data, while the ensemble model uses weighted averaging of base and robust predictions. The architecture employs 15 parallel MLP inner functions and an aggregation layer, implemented as a custom KAN rather than standard library versions.

## Key Results
- Baseline model achieves 1.9m RMSE on clean data, degrading to 2.07-2.30m under high attack strength
- Robust model shows 10% error reduction compared to baseline under adversarial conditions
- Ensemble model achieves lowest error rates (2.01m for spoofing, 2.05m for manipulation) among all models tested
- All three models maintain similar performance on clean data, with differences emerging under attack scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training improves resilience to spoofing and signal manipulation by exposing the model to perturbed RSSI patterns during training.
- Mechanism: The robust model (M_Rob) is trained on an augmented dataset containing adversarially perturbed samples. Wi-Fi spoofing is modeled as Gaussian noise injection (x'i = xi + ε, ε~N(0, σ²In)), while signal strength manipulation is modeled as element-wise scaling (x'i = xi ⊙ (1n + U(-α, α))). This forces the network to learn invariances to these perturbation types.
- Core assumption: Attack perturbations in deployment will resemble the perturbation distributions used during training.
- Evidence anchors:
  - [abstract] "The robust model is trained with adversarially perturbed data"
  - [section III.B] Equations (6) and (7) define the adversarial perturbation models
  - [corpus] Limited direct corpus evidence on adversarial training for IPS; related work focuses on GAN-based augmentation (LiGen) but not specifically adversarial robustness
- Break condition: If attackers use perturbation distributions significantly different from N(0, σ²) or U(-α, α), robustness gains may not transfer.

### Mechanism 2
- Claim: Weighted ensemble combination of base and robust predictions provides marginal accuracy improvements while maintaining stability across attack strengths.
- Mechanism: The ensemble prediction is computed as ŷ_ens = (1-λ) · M_Base(x) + λ · M_Rob(x), where λ is optimized via grid search on validation data. This leverages M_Base's accuracy on clean data and M_Rob's reliability under attacks.
- Core assumption: The optimal λ found on validation data generalizes to unseen attack scenarios and environments.
- Evidence anchors:
  - [abstract] "The ensemble model combines predictions from both the base and robust models"
  - [section III.B] Equation (8) defines the ensemble combination
  - [corpus] No corpus papers directly validate ensemble weighting schemes for adversarial IPS
- Break condition: If base and robust models produce systematically correlated errors, ensemble benefits diminish.

### Mechanism 3
- Claim: RobustScaler normalization reduces sensitivity to outliers in RSSI measurements by centering on median and scaling by interquartile range.
- Mechanism: x'i = (xi - median(X)) / IQR(X). This contrasts with standard z-score normalization by using robust statistics that are less influenced by extreme signal fluctuations common in indoor environments.
- Core assumption: RSSI outliers are noise rather than signal; the IQR captures the meaningful variation range.
- Evidence anchors:
  - [section III.A] Equation (1) defines RobustScaler; text states it "ensures robustness to outliers"
  - [section III.A] "This normalization approach ensures that the features are scaled consistently"
  - [corpus] No corpus papers explicitly compare normalization strategies for IPS
- Break condition: If legitimate extreme RSSI values carry positioning information, robust scaling may attenuate useful signal.

## Foundational Learning

- Concept: Kolmogorov-Arnold Representation Theorem
  - Why needed here: KAN architecture is inspired by this theorem, which states any multivariate continuous function can be represented as superpositions of univariate functions. Understanding this explains why the architecture uses inner functions (m=15 MLPs) and outer aggregation functions.
  - Quick check question: Can you explain why KAN decomposes the mapping into inner univariate functions and outer aggregation, versus a standard MLP's approach?

- Concept: Huber Loss Function
  - Why needed here: All three models use Huber loss, which is quadratic for small errors and linear for large errors. This matters for understanding why the models handle outliers differently than MSE.
  - Quick check question: For what error threshold δ does Huber loss transition from quadratic to linear, and how does this affect gradient behavior during training?

- Concept: Adversarial Training Paradigm
  - Why needed here: The paper's core contribution relies on training with adversarially perturbed data. Understanding the difference between "training on clean data" vs "training on augmented data" is essential for reproducing results.
  - Quick check question: How does the perturbation strategy for Wi-Fi spoofing (additive Gaussian) differ fundamentally from signal strength manipulation (multiplicative uniform)?

## Architecture Onboarding

- Component map:
  - Input layer: n-dimensional RSSI vector from n access points
  - Preprocessing: RobustScaler normalization
  - Kolmogorov layer: 2n+1 nodes applying weighted transformations
  - Inner functions: m=15 parallel MLPs (Dense → ReLU → BatchNorm → Dropout)
  - Arnold layer: Applies tanh activation for non-linearity
  - Outer functions: Weighted aggregation producing 2D coordinate output
  - Ensemble layer (M_Ens only): Weighted combination of M_Base and M_Rob outputs

- Critical path: RSSI input → RobustScaler → Kolmogorov layer → Inner MLPs (f1j through f15j) → Arnold transformations → Outer aggregation → 2D coordinates. For ensemble: base output and robust output → weighted sum → final prediction.

- Design tradeoffs:
  - M_Base: Optimized for clean data accuracy, vulnerable to attacks (RMSE degrades to 2.07-2.30m under high attack strength)
  - M_Rob: Improved resilience (~10% error reduction) but requires adversarial data generation overhead
  - M_Ens: Marginal improvement over M_Rob (2.01m vs 2.03m for spoofing) with additional inference complexity
  - KAN architecture vs standard MLP: More flexible function approximation but increased architectural complexity

- Failure signatures:
  - M_Base under attack: RMSE increases linearly with attack strength (1.9m → 2.3m as dBm increases 0.05 → 0.30)
  - M_Rob still degrades under attack but at slower rate
  - Ensemble may over-rely on one sub-model if λ is poorly tuned
  - Assumption: RobustScaler may fail if IQR approaches zero for low-variance features

- First 3 experiments:
  1. Reproduce M_Base vs M_Rob vs M_Ens comparison on the U-Victoria EOW dataset with reported parameters (ε=0.1, α=0.2, lr=0.001, RMSprop, batch=16, 100 epochs). Verify RMSE values match Table I within tolerance.
  2. Ablation study: Train M_Rob with only Wi-Fi spoofing augmentation vs only signal strength manipulation augmentation to isolate which perturbation type contributes more to robustness.
  3. Sensitivity analysis: Vary ε and α beyond the 0.1-0.2 range to identify the attack strength threshold where M_Rob's advantage over M_Base begins to collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated ensemble techniques (e.g., stacking or boosting) or hybrid architectures achieve a statistically significant performance improvement over the current Robust model, surpassing the marginal gains observed in the proposed Ensemble model?
- Basis in paper: [explicit] The Conclusion states, "Future research could explore more sophisticated ensemble techniques or hybrid approaches to further improve system resilience," noting that the current Ensemble model offered only a marginal advantage over the Robust model.
- Why unresolved: The authors observed that the difference in performance between the Robust and Ensemble models was minimal, suggesting the current simple linear combination (Equation 8) may be insufficient to maximize resilience.
- What evidence would resolve it: Experimental results from advanced ensemble meta-learners demonstrating a larger reduction in RMSE (significantly below 1.975m) under the same attack conditions.

### Open Question 2
- Question: How does the KAN-based model perform against gradient-based white-box adversarial attacks (e.g., PGD or FGSM) compared to the random noise perturbations used in this study?
- Basis in paper: [inferred] The methodology defines attacks using Gaussian noise (Equation 6) and uniform scaling (Equation 7) rather than optimized perturbations designed to maximize the specific loss function of the model.
- Why unresolved: While the models are robust against random signal variance, real-world "adversarial" attacks often involve optimization algorithms that exploit model gradients, which were not simulated in the experimental setup.
- What evidence would resolve it: A comparative analysis of model accuracy when subjected to gradient-based perturbations specifically crafted to fool the KAN architecture versus the random noise attacks presented.

### Open Question 3
- Question: Does the adversarial robustness of the proposed models generalize to heterogeneous indoor environments with significantly different multipath propagation characteristics or distinct hardware configurations?
- Basis in paper: [inferred] The experimental validation is limited to a specific setting (University of Victoria, EOW 3rd/5th floors) using a specific hardware setup (ESP32C3), while the abstract claims applicability to "mission-critical environments" broadly.
- Why unresolved: Indoor signal propagation is highly environment-dependent; it is unclear if the adversarial training overfits the model to the specific geometries and signal distributions of the tested building.
- What evidence would resolve it: Cross-domain evaluation results showing that the model maintains <2m error rates when trained on the current dataset and tested on a completely different indoor floor plan or with different receiving hardware.

## Limitations
- Sample weighting scheme for adversarial training is underspecified, creating uncertainty about the robust model's training dynamics
- KAN architecture implementation deviates from standard library definitions, raising questions about whether claimed KAN benefits are realized
- Limited experimental validation to a single indoor environment with specific hardware configuration

## Confidence
- **High confidence**: Baseline model performance (1.9m RMSE) and the general effectiveness of adversarial training (10% improvement over baseline)
- **Medium confidence**: Ensemble method's marginal improvements and the specific perturbation distributions chosen for Wi-Fi spoofing and signal manipulation
- **Low confidence**: Sample weighting algorithm details and the architectural implementation of KAN inner functions

## Next Checks
1. Clarify the sample weighting scheme by contacting authors to confirm whether ŷᵢ represents a teacher model, previous epoch predictions, or another source
2. Compare KAN vs standard MLP baselines on the same dataset to isolate whether architecture choice or adversarial training drives the improvements
3. Test model robustness against attack types not seen during training (e.g., correlated noise patterns or adversarial examples generated by white-box attacks)