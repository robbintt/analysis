---
ver: rpa2
title: 'Partial Action Replacement: Tackling Distribution Shift in Offline MARL'
arxiv_id: '2511.07629'
source_url: https://arxiv.org/abs/2511.07629
tags:
- learning
- offline
- agents
- action
- spacql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles distribution shift in offline multi-agent reinforcement
  learning (MARL) by introducing partial action replacement (PAR), which updates only
  one or a few agents' actions while keeping others fixed to behavioral data, reducing
  distribution shift compared to full joint-action updates. The authors develop SPaCQL,
  an algorithm that dynamically weights different PAR strategies based on uncertainty
  estimates from Q-function ensembles, adapting between conservative single-agent
  updates and coordinated multi-agent updates.
---

# Partial Action Replacement: Tackling Distribution Shift in Offline MARL

## Quick Facts
- **arXiv ID:** 2511.07629
- **Source URL:** https://arxiv.org/abs/2511.07629
- **Reference count:** 35
- **Key outcome:** Partial Action Replacement (PAR) reduces distribution shift scaling from exponential to linear in the number of deviating agents under factorized behavior policies, enabling SPaCQL to outperform state-of-the-art baselines on 10 of 16 tasks.

## Executive Summary
This paper addresses distribution shift in offline multi-agent reinforcement learning by introducing Partial Action Replacement (PAR), which updates only a subset of agents' actions while keeping others fixed to behavioral data. The proposed SPaCQL algorithm dynamically weights different PAR strategies based on Q-function ensemble uncertainty estimates, allowing it to adapt between conservative single-agent updates and coordinated multi-agent updates. Theoretical analysis proves that under factorized behavior policies, PAR reduces distribution shift scaling from exponential to linear in the number of deviating agents, yielding tighter value-error bounds.

## Method Summary
SPaCQL implements PAR by constructing a target value as a convex combination of operators that replace k agents (k ∈ {1,...,n}) while keeping n-k agents' actions fixed to the dataset. For each transition, the algorithm samples k agents, constructs hybrid actions, and queries a Q-ensemble to estimate uncertainty. Weights are assigned inversely proportional to ensemble variance, creating an uncertainty-weighted mixture of Bellman operators. The method uses a shared Q-function architecture with decentralized policies, trained with CQL regularization to prevent overestimation. The approach requires maintaining 10 Q-networks for uncertainty estimation and relies on specific data loader formats that provide access to behavioral actions.

## Key Results
- SPaCQL outperforms state-of-the-art baselines on 10 of 16 tasks in MPE and MaMujoco benchmarks
- Superior performance on all Random and Medium-Replay datasets where agent behaviors are less coordinated
- Theoretical proof shows distribution shift penalty scales linearly rather than exponentially under factorized behavior policies
- Adaptive weighting successfully shifts between conservative (k=1) and coordinated (k>1) updates based on dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1: Linear Distribution Shift Scaling
Standard offline MARL updates all n agents simultaneously, potentially querying OOD points in joint-action space. PAR updates only k agents while keeping n-k actions fixed, reducing occupancy measure shift from exponential to linear growth in deviating agents. This requires factorized behavior policies where agents act independently.

### Mechanism 2: Uncertainty-Weighted Adaptive Backups
SPaCQL mixes Bellman operators using weights inversely proportional to ensemble variance. High uncertainty (likely OOD) forces conservative backups (small k), while low uncertainty permits coordinated backups (large k). This creates a dynamic balance between stability and coordination.

### Mechanism 3: Implicit Coordination via Shared Q-Functions
Even ICQL-QS (fixed k=1) can learn coordinated behaviors through a shared Q-function. While each update appears myopic, the shared parameters are updated by errors from all agents, forcing the value landscape to reflect inter-dependencies.

## Foundational Learning

- **Concept:** Factorized vs. Correlated Behavior Policies
  - **Why needed:** The linear error scaling guarantee breaks down if offline data was generated by dependent agents
  - **Quick check:** For expert demonstrations with tight choreography, will PAR's linear scaling assumption hold?

- **Concept:** Epistemic Uncertainty in Ensembles
  - **Why needed:** SPaCQL relies on ensemble standard deviation to detect OOD risk
  - **Quick check:** Why does high variance in the Q-ensemble lead to down-weighting strategies?

- **Concept:** The Bellman Operator Mixture
  - **Why needed:** Understanding convex combinations of contractive operators explains SPaCQL's stability
  - **Quick check:** If T^(1) and T^(n) are γ-contractions, is their weighted sum T^SP also a contraction?

## Architecture Onboarding

- **Component map:** Environment -> MPE/MaMujoco -> SPaCQL -> Q-Ensemble (10 networks) -> Policy Heads -> Target Networks
- **Critical path:**
  1. Sample transition (s, a, r, s')
  2. For k ∈ {1,...,n}, construct hybrid action a'^(k)
  3. Query Q-Ensemble for variance (uncertainty) for each k
  4. Compute weights w_k and aggregate targets into Y^SP
  5. Update shared Q-parameters via TD loss + CQL regularizer
- **Design tradeoffs:**
  - Larger ensembles improve uncertainty estimation but increase compute costs
  - CQL penalty weight must balance preventing OOD overestimation vs. learning coordination
  - Requires behavioral actions from dataset for fixed agents
- **Failure signatures:**
  - Stagnation on Expert Data: w_k incorrectly collapses to k=1, preventing learning coordination
  - Correlation Collapse: High correlation coefficient κ degrades theoretical bounds
- **First 3 experiments:**
  1. Replicate Figure 3 to verify w_1 dominates for Random datasets while w_{>1} activates for Expert datasets
  2. Run SPaCQL vs. ICQL-QS (fixed k=1) on tasks requiring tight coordination
  3. Train on synthetic data with varying correlation levels to verify theoretical bounds

## Open Questions the Paper Calls Out

- Can more sophisticated uncertainty estimation techniques improve SPaCQL's ability to balance stability and coordination on high-quality expert datasets?
- Does PAR inherently limit discovery of superior policies in scenarios where offline data exhibits tight action correlations?
- How does SPaCQL's computational cost scale with the number of agents given the requirement to evaluate replacements for all k ∈ {1,...,n} agents?

## Limitations

- Theoretical guarantee requires factorized behavior policies, which may not hold for real-world coordinated datasets
- Performance on expert datasets is weaker than on random/medium datasets, suggesting limitations for highly coordinated behaviors
- Ensemble variance as uncertainty proxy may fail when all ensemble members are jointly overconfident
- Computational overhead from maintaining 10 Q-networks and increased hyperparameter sensitivity

## Confidence

- **High confidence:** Linear scaling under factorized policies (Theorem 1) and empirical superiority on Random/Medium-Replay datasets
- **Medium confidence:** Adaptive weighting mechanism's effectiveness across diverse coordination requirements
- **Medium confidence:** Practical applicability to real-world scenarios with non-factorized behavior policies

## Next Checks

1. Systematically vary correlation coefficient κ in synthetic datasets to measure degradation of PAR's theoretical bounds and empirical performance
2. Compare ensemble variance against alternative uncertainty estimation methods on OOD detection accuracy in MARL scenarios
3. Identify minimum coordination level required in offline data for PAR to outperform full joint-action updates