---
ver: rpa2
title: Cost and Reward Infused Metric Elicitation
arxiv_id: '2501.00696'
source_url: https://arxiv.org/abs/2501.00696
tags:
- metric
- cost
- elicitation
- algorithm
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of metric elicitation in machine
  learning, which involves selecting performance metrics that best reflect an individual's
  implicit preferences for a given application. The authors propose a novel algorithm,
  cost and reward infused metric elicitation, that extends the multiclass metric elicitation
  framework to account for bounded costs and rewards in addition to accuracy values
  encoded within a given model's confusion matrix.
---

# Cost and Reward Infused Metric Elicitation

## Quick Facts
- **arXiv ID:** 2501.00696
- **Source URL:** https://arxiv.org/abs/2501.00696
- **Reference count:** 2
- **Primary result:** Algorithm converges logarithmically to true metric weights on synthetic data with L1 errors decreasing as iterations increase

## Executive Summary
This paper introduces a novel algorithm for metric elicitation that extends multiclass metric elicitation to handle bounded costs and rewards in addition to accuracy. The method works by sampling weight ratios for each classifier attribute relative to class 1 accuracy, finding optimal classifiers for each sample, and using oracle pairwise queries to narrow the search space. The approach demonstrates logarithmic convergence on synthetic data, with estimated weights converging to true metric weights while accounting for practical constraints like costs and rewards.

## Method Summary
The algorithm elicits a linear performance metric by performing binary search on weight ratios for each classifier attribute. For accuracy attributes, it uses restricted Bayes optimal (RBO) classifiers that predict between only two classes. For cost/reward attributes, it constructs optimal classifiers along a quarter-ellipse Pareto frontier between accuracy and the cost/reward. The method queries an oracle with pairwise comparisons of classifiers and updates weight ratio bounds based on responses. Each attribute's weight ratio is recovered independently through binary search, with normalization applied to the final weight vector.

## Key Results
- L1 errors decrease logarithmically with number of queries on synthetic data
- Estimated weights converge to true metric weights
- Algorithm handles both accuracy and cost/reward attributes
- Linear scaling with number of attributes (O(n log(1/ε)) query complexity)

## Why This Works (Mechanism)

### Mechanism 1
Binary search on weight ratios converges to true metric weights with logarithmic query complexity. The algorithm exploits that linear metrics depend only on weight ratios, not absolute values. It samples weight ratios for each attribute relative to class 1 accuracy, constructs optimal classifiers for each sample, queries the oracle with pairwise comparisons, and narrows the search interval based on oracle responses. This creates a binary search that halves the uncertainty interval each iteration.

### Mechanism 2
Optimal classifiers for cost-accuracy trade-offs can be determined analytically when the Pareto frontier shape is known. For a metric weighting only class 1 accuracy and a single cost/reward, the optimal classifier occurs where the Pareto frontier slope matches the metric's level curve slope (determined by weight ratio). With a quarter-ellipse Pareto frontier, this reduces to computing angle θ = tan⁻¹(η₁m / A(1-m)), which directly yields optimal classifier attributes.

### Mechanism 3
Pairwise oracle queries on two-attribute variations isolate individual weight ratios independently. By varying only two classifier statistics while holding others constant, the metric difference simplifies to depend only on the ratio of their weights. This decomposes the multivariate weight recovery problem into independent binary searches per attribute.

## Foundational Learning

- **Concept: Confusion Matrix Geometry**
  - **Why needed here:** Understanding that the space of achievable confusion matrices forms a convex region with specific vertices (trivial classifiers) is essential for reasoning about which classifiers can be constructed for queries.
  - **Quick check question:** Given k=3 classes, what classifier achieves vertex ζ₂e₂ of the confusion space?

- **Concept: Pareto Frontier**
  - **Why needed here:** The algorithm fundamentally relies on trading off accuracy against costs/rewards along a Pareto frontier. Without understanding this trade-off surface, the optimal classifier construction is opaque.
  - **Quick check question:** If the Pareto frontier between accuracy and latency is a quarter-ellipse from (0.9 accuracy, 0ms) to (0.6 accuracy, 100ms), what accuracy level maximizes metric ψ = 0.7×accuracy - 0.003×latency?

- **Concept: Restricted Bayes Optimal (RBO) Classifier**
  - **Why needed here:** For accuracy-only queries, the algorithm uses RBO classifiers that predict between only two classes. Understanding RBO construction is prerequisite to implementing the query generation.
  - **Quick check question:** For classes k₁ and k₂ with weights aₖ₁=0.6, aₖ₂=0.4 and class probabilities ηₖ₁(x)=0.3, ηₖ₂(x)=0.5, which class does the RBO classifier predict?

## Architecture Onboarding

- **Component map:**
  ELICITATION (main loop) -> For each attribute i: Binary search on weight ratio m ∈ [0,1] -> Sample 5 points (a,c,d,e,b) -> CONSTRUCT_RBO_CLASSIFIER (for accuracy attributes) or CONSTRUCT_COST_ACCURACY_CLASSIFIER (for cost/reward attributes) -> Query oracle with 4 pairwise comparisons -> Update binary search bounds based on oracle responses -> Normalize and return estimated weight vector

- **Critical path:** Binary search convergence per attribute → each attribute requires O(log(1/ε)) queries. With n attributes, total complexity O(n log(1/ε)). The per-iteration bottleneck is oracle query latency, not computation.

- **Design tradeoffs:**
  - Quarter-ellipse vs. real Pareto frontier: Synthetic ellipse guarantees slope coverage but may not reflect real cost-accuracy trade-offs
  - DLPM vs. full LPM: Using only diagonal terms simplifies to O(n) searches but ignores off-diagonal confusion terms
  - 5-sample vs. 3-sample binary search: Current implementation samples 5 points per iteration for robustness to oracle noise

- **Failure signatures:**
  - Non-convergence/oscillation: L1 error plateaus or oscillates near convergence
  - Weight estimates stuck at bounds: Binary search bounds don't update
  - Inconsistent oracle responses: Same comparison yields different results
  - Classifier construction fails: Cannot achieve target accuracy/cost combination

- **First 3 experiments:**
  1. Validate on known metric with synthetic oracle: Set true weights a*=(0.3, 0.5, 0.2) for k=2 classes + 1 cost. Run elicitation with ε=0.01. Verify ||â - a*||₁ < ε and plot convergence curve.
  2. Test sensitivity to Pareto frontier misspecification: Generate data with true frontier as quarter-circle. Run algorithm assuming ellipse with different aspect ratio. Measure weight recovery error.
  3. Stress test with additional attributes: Scale to k=5 classes, 3 costs, 2 rewards (10 total attributes). Measure total queries and verify linear scaling.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on consistent oracle providing pairwise preferences for a fixed linear metric
- Assumes known, monotonic Pareto frontiers that cover all required slopes
- Synthetic experiments don't validate performance on real-world Pareto frontiers
- Decomposition into independent binary searches may fail with non-linear metric interactions

## Confidence
- **High Confidence:** Binary search mechanism for weight ratio recovery is mathematically sound and converges logarithmically under ideal conditions
- **Medium Confidence:** Algorithm works well on synthetic data with quarter-ellipse Pareto frontiers, but real-world application requires estimating actual Pareto frontiers
- **Low Confidence:** Decomposition into independent binary searches assumes linear metric structure and may fail with complex, non-linear preference patterns

## Next Checks
1. **Real Pareto Frontier Validation:** Test the algorithm on a real-world dataset where the Pareto frontier must be estimated using the suggested bisection method. Compare weight recovery accuracy against ground truth user preferences.

2. **Oracle Noise Robustness:** Introduce stochasticity into the oracle and measure convergence degradation. Determine if the 5-sample per iteration approach sufficiently handles realistic oracle uncertainty.

3. **Off-Diagonal Confusion Term Recovery:** Extend beyond diagonal linear performance metrics to full linear performance metrics that include misclassification type preferences. Evaluate whether the binary search framework scales to O(k²) attributes.