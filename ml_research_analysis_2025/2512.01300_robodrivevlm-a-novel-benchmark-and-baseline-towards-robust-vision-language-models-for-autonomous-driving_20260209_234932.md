---
ver: rpa2
title: 'RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language
  Models for Autonomous Driving'
arxiv_id: '2512.01300'
source_url: https://arxiv.org/abs/2512.01300
tags:
- driving
- corruption
- autonomous
- arxiv
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoboDriveBench, the first benchmark for evaluating
  the robustness of vision-language model (VLM)-based end-to-end autonomous driving
  systems under real-world corruptions. The benchmark systematically tests 11 simulated
  scenarios, including 6 sensor corruption types (fog, snow, rain, brightness/darkness
  variations, motion blur) and 5 prompt corruption types (bit errors, transmission
  corruption, conversational interference, command overwrite, malicious attacks).
---

# RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving

## Quick Facts
- arXiv ID: 2512.01300
- Source URL: https://arxiv.org/abs/2512.01300
- Authors: Dacheng Liao, Mengshi Qi, Peng Shu, Zhining Zhang, Yuxin Lin, Liang Liu, Huadong Ma
- Reference count: 40
- Key outcome: Introduces RoboDriveBench benchmark and RoboDriveVLM baseline showing VLM vulnerability to prompt corruptions while demonstrating multimodal fusion and test-time adaptation improvements

## Executive Summary
This paper addresses the critical need for robustness evaluation of vision-language model (VLM)-based end-to-end autonomous driving systems under real-world corruptions. The authors introduce RoboDriveBench, the first comprehensive benchmark systematically testing 11 simulated corruption scenarios across sensor and prompt domains. They propose RoboDriveVLM, a novel framework that enhances robustness through multimodal sensor fusion and test-time adaptation, and demonstrate that while current VLM systems show moderate sensor corruption resilience, they remain highly vulnerable to prompt attacks.

## Method Summary
The paper introduces RoboDriveBench for evaluating VLM-based autonomous driving robustness under sensor corruptions (fog, snow, rain, brightness/darkness, motion blur) and prompt corruptions (bit errors, transmission corruption, conversational interference, command overwrite, malicious attacks). The proposed RoboDriveVLM framework enhances robustness by fusing LiDAR, radar, and camera data into a unified latent space through BEV projection with height/velocity encoding. A Test-Time Adaptation method based on cross-modal knowledge distillation selects the highest-probability trajectory sequence across modalities and uses it to update the model during inference. The approach is evaluated on nuScenes validation set with 64,559 trajectory prediction cases per scenario.

## Key Results
- Existing VLM-based systems exhibit moderate robustness to sensor corruptions but remain highly vulnerable to prompt corruptions
- RoboDriveVLM achieves lower Mean Corruption L2 (MCL2) and Mean Corruption Collision (MCC) than camera-only baselines across all sensor corruption types
- RoboDriveVLM-TTA reduces invalid predictions by over 90% under malicious attacks, from 2,916 to 32
- TTA improves MCL2 from 150.95% to 99.69% under malicious attack scenarios

## Why This Works (Mechanism)

### Mechanism 1
Unified latent-space fusion of LiDAR, radar, and camera inputs improves robustness to sensor corruption compared to camera-only VLMs. LiDAR provides spatial structure (BEV maps with Z-axis encoded as channels), radar contributes velocity information (encoded as connecting lines), and camera supplies semantic detail. These are concatenated and jointly processed by the VLM, reducing reliance on any single degraded modality. The core assumption is that when one modality is corrupted, other modalities retain sufficient uncorrupted signal to compensate. Break condition: if corruption simultaneously degrades all modalities, fusion benefits collapse.

### Mechanism 2
Test-time adaptation via cross-modal knowledge distillation reduces invalid predictions and improves collision metrics without labeled data. During inference, the model generates independent trajectory token sequences from LiDAR-only, camera-only, and all-modalities inputs. The sequence with highest joint probability is selected as the "optimal" output and used to distill knowledge back into the model through gradient updates, restoring corrupted-modality feature extraction. The core assumption is that the modality with highest joint probability is reliably correct. Break condition: if all modalities produce low-confidence outputs, maximum-likelihood selection may still choose an incorrect trajectory.

### Mechanism 3
Explicitly penalizing invalid predictions in evaluation metrics (MCC, MCL2) reveals VLM vulnerabilities hidden by standard L2/collision metrics. Standard metrics exclude samples with invalid outputs, artificially improving scores. MCC and MCL2 multiply corruption ratios by (1 + invalid_nums/sample_nums), ensuring invalid predictions increase penalty. The core assumption is that invalid predictions are safety-relevant failures rather than benign exclusions. Break condition: if invalid outputs correlate with conservative behaviors rather than unsafe trajectories, the penalty may overstate risk.

## Foundational Learning

- **Concept: Bird's Eye View (BEV) Representation**
  - Why needed here: LiDAR and radar point clouds are projected to BEV images to align with VLM's 2D-image processing pipeline while preserving spatial structure.
  - Quick check question: Can you explain how encoding Z-axis height as image channels preserves 3D information in a 2D representation?

- **Concept: Chain-of-Thought (CoT) Reasoning in VLMs**
  - Why needed here: Compared methods (DriveVLM, OpenEMMA) use multi-turn CoT for scene analysis, but the paper finds this accumulates errors under corruption. Understanding CoT tradeoffs is critical.
  - Quick check question: Why might multi-turn CoT amplify prompt corruption effects compared to single-turn trajectory prediction?

- **Concept: Maximum Likelihood Sequence Selection**
  - Why needed here: TTA mechanism relies on selecting the highest joint-probability token sequence across modalities as pseudo-ground-truth for distillation.
  - Quick check question: How does the chain rule compute joint probability of a token sequence, and what does maximizing it imply about output reliability?

## Architecture Onboarding

- **Component map:** Input Preprocessing -> Fusion Module -> VLM Backbone -> Token Decoding -> Trajectory Output. For TTA: add Parallel Single-Modality Inference -> Joint Probability Comparison -> Model Update.
- **Critical path:** BEV projection → Feature concatenation → VLM inference → Token decoding → Trajectory output. For TTA: add parallel single-modality inference branches → joint probability comparison → model update.
- **Design tradeoffs:** Multimodal fusion adds 28.08% runtime overhead but reduces MCL2 by ~50%. TTA uses 32 offline samples before testing, adding ~10% total overhead without affecting inference latency. Short-dialogue prompting vs. CoT: former more robust to prompt corruption, latter provides richer scene reasoning.
- **Failure signatures:** High invalid prediction counts under malicious attacks (>2,000 for camera-only). MCC exceeding 400% under transmission corruption indicates 4x+ collision rate increase. CoT-based models show error propagation: bit errors in early reasoning stages corrupt final trajectory.
- **First 3 experiments:** 1) Reproduce clean vs. sensor corruption baseline: Run RoboDriveVLM on fog, rain, snow at severity levels 1/3/5; verify MCL2 increases with severity. 2) Validate TTA on prompt corruption: Apply transmission corruption to prompts; compare invalid prediction counts between RoboDriveVLM and RoboDriveVLM-TTA. 3) Ablate modality contribution: Disable LiDAR or radar input individually under motion blur; measure whether MCC/avgL2 degrades to camera-only levels.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of VLM-based systems on RoboDriveBench's simulated corruptions correlate with performance in physical real-world driving scenarios involving actual sensor degradation? The paper uses simulated scenarios rather than real-world failure data, and there is often a "sim-to-real" gap where synthetic noise models do not perfectly match physical sensor failures.

### Open Question 2
Can the proposed Test-Time Adaptation (TTA) method be reformulated to support online, real-time adaptation during inference without introducing latency that exceeds safety margins? The current implementation uses an offline manner with 32 samples prior to evaluation, which works for benchmarks but real-world driving requires immediate adaptation to sudden, unpredictable environmental shifts.

### Open Question 3
What architectural modifications beyond TTA are required to immunize VLMs against malicious prompt injection attacks, which currently induce high rates of invalid outputs? While TTA mitigates symptoms, the paper notes the "inherent vulnerability" remains, suggesting that simple adaptation is insufficient for securing the prompt interface against semantic hijacking.

## Limitations
- Evaluation covers only 6 sensor corruption types and 5 prompt corruption types, leaving many real-world failure modes untested
- TTA mechanism assumes corrupted modalities can recover through knowledge distillation, which may fail when corruption severely degrades feature extraction
- Safety implications of invalid predictions are not fully characterized - it's unclear whether these represent conservative behaviors or dangerous failures

## Confidence
- **High confidence:** Baseline robustness evaluation showing VLM vulnerability to prompt corruptions is well-supported by extensive testing across 11 corruption scenarios
- **Medium confidence:** Multimodal fusion mechanism's effectiveness is demonstrated but relies on the assumption that at least one modality remains reliable during corruption
- **Medium confidence:** TTA method's performance gains are impressive, but the mechanism assumes the highest-likelihood sequence is reliably correct, which may not hold under severe prompt corruption

## Next Checks
1. **Simultaneous Multimodal Corruption Test:** Evaluate RoboDriveVLM under scenarios where camera, LiDAR, and radar are all corrupted simultaneously (e.g., heavy snow affecting all sensors) to verify fusion benefits persist under extreme conditions.

2. **TTA Failure Boundary Analysis:** Systematically increase prompt corruption severity until TTA begins selecting incorrect trajectories, establishing the method's operational limits and failure conditions.

3. **Safety Characterization of Invalid Predictions:** Analyze the nature of invalid outputs (e.g., requesting human takeover vs. dangerous trajectories) through expert review of failure cases to better understand safety implications.