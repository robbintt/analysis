---
ver: rpa2
title: A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations
arxiv_id: '2506.03425'
source_url: https://arxiv.org/abs/2506.03425
tags:
- audio
- ieee
- methods
- diffusion
- deepfake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating explanations for
  audio deepfake detection, where traditional XAI methods like SHAP and LPR fail to
  provide accurate attributions. The authors propose a data-driven approach that leverages
  paired real and vocoded audio to create ground truth explanations by computing the
  difference in their time-frequency representations.
---

# A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations
arXiv ID: 2506.03425
Source URL: https://arxiv.org/abs/2506.03425
Authors: Petr Grinberg; Ankur Kumar; Surya Koppisetti; Gaurav Bharaj
Reference count: 0
Primary result: Proposed diffusion-based method SpecSegDiff outperforms classical XAI techniques (GDice 57.46% on VocV4) at identifying deepfake artifacts in audio.

## Quick Facts
- arXiv ID: 2506.03425
- Source URL: https://arxiv.org/abs/2506.03425
- Authors: Petr Grinberg; Ankur Kumar; Surya Koppisetti; Gaurav Bharaj
- Reference count: 0
- Primary result: Proposed diffusion-based method SpecSegDiff outperforms classical XAI techniques (GDice 57.46% on VocV4) at identifying deepfake artifacts in audio.

## Executive Summary
This paper addresses the challenge of generating explanations for audio deepfake detection, where traditional XAI methods like SHAP and LPR fail to provide accurate attributions. The authors propose a data-driven approach that leverages paired real and vocoded audio to create ground truth explanations by computing the difference in their time-frequency representations. A diffusion model is trained to predict artifact regions in spoof audio, using either the spectrogram or intermediate features from a pretrained audio deepfake detector as conditioning. The proposed method, SpecSegDiff, outperforms classical XAI techniques in both qualitative and quantitative evaluations, achieving higher segmentation metrics like Generalized Dice (57.46% on VocV4) and better alignment with ground truth annotations. The diffusion model effectively highlights deepfake artifacts, offering more faithful and interpretable explanations for audio deepfake detection.

## Method Summary
The method generates ground truth explanations by computing the spectrogram difference between paired real and vocoded audio, applying Gaussian smoothing and 95% quantile binarization. Two diffusion models are trained: SpecSegDiff conditions on log-magnitude spectrograms, while ADDSegDiff conditions on intermediate features from a Wav2Vec2-AASIST ADD model. The SegDiff architecture (UNet encoder-decoder with 2 residual blocks per level) iteratively denoises random noise into artifact heatmaps. SpecSegDiff uses 12 RRDB blocks for spectrogram preprocessing, while ADDSegDiff uses 1 RRDB block with projected ADD features. Inference averages 32 stochastic predictions. The approach enables both classifier-agnostic segmentation (SpecSegDiff) and classifier-specific faithfulness explanations (ADDSegDiff).

## Key Results
- SpecSegDiff achieves 57.46% Generalized Dice on VocV4 vs. 12.65% for AttnLRP, demonstrating superior segmentation accuracy
- Cross-dataset transfer shows SpecSegDiff generalizes to LibriSeVoc (51.73% GDice) despite different vocoder architectures
- ADDSegDiff provides classifier-specific explanations with better faithfulness metrics but lower segmentation performance (48.26% GDice)
- Classical XAI methods produce similar outputs across different vocoders on same source audio, highlighting content rather than artifacts
- The diffusion model effectively captures dense artifact regions while classical methods miss critical deepfake patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paired real-vocoded audio differences provide valid ground-truth supervision for artifact localization
- Mechanism: Parallel samples share identical speaker and content; their spectrogram difference isolates vocoder-introduced artifacts. Gaussian smoothing (kernel size 3×11, variance 3×5) removes noise while preserving dense artifact regions, followed by 95% quantile binarization.
- Core assumption: Vocoded audio differs from real audio predominantly through localized vocoder artifacts rather than global distortions.
- Evidence anchors:
  - [abstract]: "We consider paired real and vocoded audio, and use the difference in time-frequency representation as the ground-truth explanation."
  - [section 3.2]: "By subtracting bona fide spectrogram Mb from a parallel spoof spectrogram Ms, we obtain a heatmap that showcases the most manipulated regions."
  - [corpus]: Weak support. Neighbor paper "Evaluating Model Explanations without Ground Truth" discusses challenges in defining ground truth for explanations broadly, but doesn't address this specific approach.
- Break condition: Misaligned audio pairs (LibriSeVoc required DTW correction) or vocoders producing diffuse, non-localized artifacts would invalidate the assumption.

### Mechanism 2
- Claim: Diffusion models trained on ground-truth masks outperform classical XAI methods at artifact segmentation
- Mechanism: SegDiff architecture (UNet encoder-decoder with 2 residual blocks per level) iteratively denoises random noise → artifact heatmap, conditioned on either spectrograms (SpecSegDiff) or ADD model features (ADDSegDiff). Inference averages 32 stochastic predictions.
- Core assumption: Artifact patterns across vocoders are learnable and generalizable within the training distribution.
- Evidence anchors:
  - [abstract]: "The proposed method, SpecSegDiff, outperforms classical XAI techniques... achieving higher segmentation metrics like Generalized Dice (57.46% on VocV4)."
  - [section 4.2, Table 1]: SpecSegDiff achieves 57.46% GDice vs. 12.65% for AttnLRP on VocV4; similar performance on LibriSeVoc (51.73%) demonstrates cross-dataset transfer.
  - [corpus]: No direct corpus support for diffusion-based XAI in audio domain.
- Break condition: Severe distribution shift to unseen vocoder architectures or recording conditions not represented in training data.

### Mechanism 3
- Claim: Conditioning on ADD model intermediate features yields classifier-specific faithful explanations
- Mechanism: Extract features from Wav2Vec2-AASIST transformer layers {0, 4, 9, 14, 19, 23}, project 1024→320 dims via 2-layer MLP, use as diffusion conditioning. Faithfulness metrics (AI, AD, AG, Fid-In) measure whether highlighted regions actually affect classifier decisions.
- Core assumption: Intermediate ADD model features preserve spatial correspondence to time-frequency artifact locations.
- Evidence anchors:
  - [section 3.3]: "If we get similar outputs compared to SpecSegDiff and ground-truth annotations, then we can comment that the features from the ADD model actually utilize the annotated artifacts."
  - [section 4.2]: ADDSegDiff outperforms classical XAI on faithfulness metrics (AI↑, AD↓) but underperforms SpecSegDiff on segmentation, attributed to "complex spatial relationship that ADDSegDiff needs to learn between latent space of the ADD model and time-frequency representation."
  - [corpus]: No corpus evidence addresses encoder-only models' spatial preservation.
- Break condition: Pretrained backbones (Wav2Vec2) may not preserve spatial information needed for accurate localization—evidenced by SpecSegDiff outperforming ADDSegDiff.

## Foundational Learning

- **Short-Time Fourier Transform (STFT) and Spectrograms**:
  - Why needed here: The entire method operates on time-frequency representations; RawWrapper technique requires understanding perfect reconstruction via ISTFT.
  - Quick check question: Can you explain why the paper uses log-magnitude spectrograms for visualization but exponentiates before ISTFT?

- **Diffusion Models for Image Segmentation (SegDiff)**:
  - Why needed here: The core architecture adapts SegDiff for audio artifact detection; understanding conditioning mechanisms is essential for extending the approach.
  - Quick check question: Why does inference average 32 predictions from different noise samples rather than using a single deterministic forward pass?

- **Vocoder Architectures and Artifacts**:
  - Why needed here: Different vocoders (GAN-based, flow-based, diffusion, autoregressive) produce distinct artifact patterns; understanding this informs dataset construction and model generalization.
  - Quick check question: The paper notes Hn-NSF and WaveGlow produce artifacts in very different frequency/time regions—what does this imply for training data diversity?

## Architecture Onboarding

- **Component map**:
  - Data pipeline: Paired real/fake audio → STFT → Gaussian-smoothed difference → 95% quantile binarization → ground truth masks
  - SpecSegDiff: Spoof spectrogram Ms → 12 RRDB blocks (preprocessing) → SegDiff UNet → artifact heatmap
  - ADDSegDiff: Spoof audio → frozen Wav2Vec2-AASIST → transformer layer features → 2-layer MLP → SegDiff UNet → artifact heatmap
  - Baseline comparison: RawWrapper (ISTFT as conv layer) enables DeepSHAP, GradientSHAP, AttnLRP in time-frequency domain

- **Critical path**:
  1. Ground truth mask quality (Section 3.2) determines supervision quality
  2. Conditioning choice (spectrogram vs. ADD features) trades classifier-agnostic accuracy vs. classifier-specific faithfulness
  3. RawBoost augmentation with identical noise vectors for paired samples maintains artifact-only differences

- **Design tradeoffs**:
  - SpecSegDiff: Higher segmentation accuracy (GDice 57.46%), classifier-agnostic, no faithfulness guarantees
  - ADDSegDiff: Lower segmentation (GDice 48.26%) but provides classifier-specific explanations with faithfulness metrics
  - Preprocessing: 12 RRDB blocks for spectrogram vs. 1 RRDB for ADD features—reflects input complexity difference

- **Failure signatures**:
  - Classical XAI methods (DeepSHAP, GradientSHAP, AttnLRP) produce similar outputs for different vocoders on same source audio → highlighting content, not artifacts (Figure 3)
  - ADDSegDiff missing dense regions or predicting extra columns → latent-space spatial mapping degradation
  - Misaligned audio pairs (LibriSeVoc silences) → requires DTW preprocessing

- **First 3 experiments**:
  1. Reproduce ground truth mask generation on a small VocV4 subset: verify Gaussian smoothing parameters and 95% quantile threshold produce meaningful dense regions matching Figure 1.
  2. Train SpecSegDiff on single vocoder type, evaluate on held-out vocoder: measure generalization gap to understand vocoder-specific vs. universal artifact patterns.
  3. Implement RawWrapper + single classical XAI method (e.g., GradientSHAP) on Wav2Vec2-AASIST: confirm time-frequency attributions are computable and compare visually against Figure 3 baselines.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can scaling the training dataset and incorporating large pretrained diffusion backbones close the performance gap between the classifier-specific (ADDSegDiff) and classifier-agnostic (SpecSegDiff) models?
- Basis in paper: [explicit] The authors state in Section 4.2 and Section 5 that resolving the performance gap and improving robustness through scaling and pretrained backbones are potential future directions.
- Why unresolved: The ADDSegDiff model currently underperforms SpecSegDiff, likely due to the complex spatial relationship between the Wav2Vec2 latent space and the time-frequency representation, which the authors hope to mitigate with more data.
- What evidence would resolve it: Experimental results showing ADDSegDiff matching or exceeding SpecSegDiff segmentation metrics (e.g., Generalized Dice) after training on larger, more diverse datasets with pretrained weights.

### Open Question 2
- Question: To what extent does the proposed data-driven approach generalize to unseen vocoders, diverse speakers, and "in-the-wild" acoustic conditions?
- Basis in paper: [explicit] The conclusion identifies generalization to unseen vocoders, speakers, and other conditions as a critical aspect for future work given the nature of deep learning techniques.
- Why unresolved: While the method performs well on VocV4 and LibriSeVoc, the authors note that robustness across varied conditions remains a key challenge for practical deployment.
- What evidence would resolve it: Evaluation of the trained diffusion models on out-of-distribution datasets containing spoofing algorithms and speakers excluded from the training set.

### Open Question 3
- Question: Can this methodology be adapted to explain deepfakes generated by non-vocoder methods (e.g., end-to-end synthesis) where paired genuine audio is unavailable for ground truth generation?
- Basis in paper: [inferred] The methodology relies on paired real and vocoded audio to compute ground truth via spectrogram subtraction. The authors do not address how to generate explanations for deepfakes created without a parallel real reference.
- Why unresolved: The current supervision signal is fundamentally dependent on the availability of time-synchronized genuine audio, which limits the applicability of the training pipeline to vocoder-based attacks.
- What evidence would resolve it: A modified training framework that generates accurate explanations for unpaired or end-to-end synthesized deepfakes without relying on subtraction-based ground truth masks.

## Limitations
- Ground truth mask quality depends on Gaussian smoothing parameters and 95% quantile threshold selection, which may not generalize across vocoder types
- The method requires paired real-vocoded audio, limiting applicability to vocoder-based deepfakes and excluding end-to-end synthesis methods
- Generalization to unseen vocoder architectures and "in-the-wild" conditions remains unproven despite cross-dataset results within similar families
- ADDSegDiff's lower segmentation performance suggests pretrained ADD model features may not preserve spatial information needed for artifact localization

## Confidence
- **High confidence**: The segmentation performance gap between SpecSegDiff and classical XAI methods (GDice 57.46% vs 12.65%) is well-supported by quantitative metrics and visual examples.
- **Medium confidence**: The faithfulness metrics for ADDSegDiff are methodologically sound, but the comparison to classical XAI is indirect since those methods don't optimize for faithfulness.
- **Medium confidence**: Cross-dataset generalization (LibriSeVoc results) demonstrates robustness but within similar vocoder families.

## Next Checks
1. **Ground truth quality verification**: Recompute ground truth masks on VocV4 subset with varied Gaussian smoothing parameters to assess sensitivity and validate the 95% quantile threshold choice.
2. **Unseen vocoder evaluation**: Test trained models on audio vocoded with entirely different architectures (e.g., new neural vocoders) to measure true generalization beyond training distribution.
3. **Classical XAI faithfulness comparison**: Implement a modified classical XAI method that optimizes for faithfulness (e.g., using relevance redistribution) to provide fairer comparison with ADDSegDiff's faithfulness metrics.