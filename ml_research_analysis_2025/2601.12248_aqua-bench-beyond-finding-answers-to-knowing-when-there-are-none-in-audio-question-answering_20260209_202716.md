---
ver: rpa2
title: 'AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio
  Question Answering'
arxiv_id: '2601.12248'
source_url: https://arxiv.org/abs/2601.12248
tags:
- audio
- question
- unanswerable
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AQUA-Bench, a new benchmark to evaluate how
  audio-aware large language models handle unanswerable questions. While models excel
  at standard audio question answering, they struggle when faced with scenarios where
  the correct answer is missing, answer choices are incompatible, or the audio lacks
  sufficient information.
---

# AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering

## Quick Facts
- arXiv ID: 2601.12248
- Source URL: https://arxiv.org/abs/2601.12248
- Authors: Chun-Yi Kuan; Hung-yi Lee
- Reference count: 0
- One-line primary result: Audio-aware LLMs struggle to recognize unanswerable questions, but CoT prompting and explicit abstention instructions significantly improve their ability to abstain.

## Executive Summary
AQUA-Bench is a new benchmark that evaluates how audio-aware large language models handle unanswerable questions in audio question answering tasks. While these models excel at standard audio QA, they struggle when the correct answer is missing, answer choices are incompatible with the question, or the audio lacks sufficient information. The benchmark defines three types of unanswerable cases: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Audio Question Detection (IAQD). Experiments reveal that top models exhibit a "forced-choice" bias, selecting an answer even when none is correct, but reasoning models and those with explicit guidance perform significantly better. AQUA-Bench exposes a critical blind spot in current audio-language systems, highlighting the need for models that can recognize and appropriately abstain from answering unanswerable queries.

## Method Summary
The benchmark evaluates audio-language models on three types of unanswerable scenarios using multiple-choice format with "None of the above" or "Unanswerable" options. It uses ESC-50, Music Instrument Sounds, VocalSound, and MMAU datasets with 15 instruction templates per task type. The evaluation has two stages: answerable questions first, then unanswerable variants (removed correct answers, incompatible answer sets, or irrelevant questions). Accuracy is measured on answerable tasks, while Conditional Accuracy (CA) measures performance on unanswerable cases only when corresponding answerable questions were correct. Chain-of-Thought prompting and explicit abstention instructions are tested as interventions.

## Key Results
- Top audio-language models show sharp accuracy drops on unanswerable questions, revealing a "forced-choice" bias
- Chain-of-Thought prompting significantly improves unanswerability detection across all models
- Explicit abstention instructions provide universal performance improvements in recognizing unanswerable scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought (CoT) prompting recovers latent unanswerability detection by forcing verification steps before selection.
- Mechanism: Models default to semantic matching (selecting the best fit among options). CoT instructions insert a verification phase that checks audio grounding and option compatibility, overriding the forced-choice bias.
- Core assumption: The model possesses sufficient internal world knowledge and audio-text alignment capability to perform the verification if explicitly prompted.
- Evidence anchors:
  - [section 4.4] "CoT helps all models... transition from near-random guessing to competitive accuracy."
  - [section 4.4] "Qwen2.5-Omni... MMAU AAD accuracy improving from 28.3% to 59.3%."
  - [corpus] "Speech-Hands" abstract supports this, noting the need for "self-reflection" to know when to trust perception vs. consult external tools.
- Break condition: Models lacking sufficient parametric knowledge to recognize incompatibility (e.g., failing to know that "color" is not audible) will not benefit from reasoning steps alone.

### Mechanism 2
- Claim: Explicit abstention instructions activate dormant decision boundaries for "unanswerable" categories.
- Mechanism: Standard training creates a bias toward high-confidence selection. Adding instructions like "Select None of the above if..." acts as a conditional rule that shifts the decision boundary, allowing the model to map high-uncertainty or low-match states to a specific "unanswerable" token rather than the highest-probability distractor.
- Core assumption: The model has been instruction-tuned sufficiently to follow negative constraints.
- Evidence anchors:
  - [section 4.3] "We observed an universal improvement in performance across all models" with explicit guidance.
  - [section 4.3] "Models... often fail to apply this capability without direct instruction."
  - [corpus] Corpus evidence for this specific "prompting to abstain" mechanism in audio is weak; nearest neighbors focus on general reasoning or retrieval.
- Break condition: If the model's confidence calibration is extremely poor (i.e., it is highly confident in wrong answers), explicit instructions may fail to override the hallucination.

### Mechanism 3
- Claim: Robust performance on Incompatible Answer Set Detection (IASD) relies on hierarchical semantic organization rather than perceptual verification.
- Mechanism: In IASD (e.g., asking for an instrument but providing emotion options), the model detects a category mismatch between the question embedding and the answer set cluster. This does not require deep audio analysis but rather a semantic consistency check between the question type and the option labels.
- Core assumption: The text-encoder portion of the model has structured concept embeddings (e.g., "instrument" ≠ "emotion").
- Evidence anchors:
  - [section 4.3] "Models performed significantly better on [IASD] compared to AAD... suggests... strong internal understanding of semantic categories."
  - [section 4.3] "They can effectively recognize that 'happy' is not a valid answer for 'What animal...'."
  - [corpus] "Learning to Answer from Correct Demonstrations" suggests models learn acceptable answer distributions, which aids in rejecting incompatible sets.
- Break condition: If distractor options are semantically close to the correct category (e.g., "cat" vs "dog" when audio is a "lion"), this semantic filter fails.

## Foundational Learning

- Concept: **Forced-Choice Bias in LLMs**
  - Why needed here: This is the baseline failure mode the benchmark exposes. Without understanding that ALLMs are trained to almost always select an option, the performance drops (e.g., 96.4% to 0.7%) seem unintelligible.
  - Quick check question: If a model is 99% confident the answer is "None," but the loss function penalizes abstention, what will the model learn to do during fine-tuning?

- Concept: **Conditional Accuracy (CA)**
  - Why needed here: The paper uses CA to distinguish between "model doesn't know the answer" and "model doesn't know it can't answer." It filters out performance drops caused by simple deafness or incompetence.
  - Quick check question: Why is reporting raw accuracy on unanswerable questions potentially misleading regarding a model's "abstention" capability?

- Concept: **Negative Sample Construction (Audio-Text Mismatch)**
  - Why needed here: Creating unanswerable data requires specific strategies (AAD, IASD, IAQD). Understanding the distinction between "missing answer" and "incompatible question" is necessary for diagnosing model failures.
  - Quick check question: Which unanswerable type (AAD vs. IAQD) requires the model to reason about the physical limitations of the audio modality (e.g., "can you hear a color?")?

## Architecture Onboarding

- Component map:
  Audio Encoder (Whisper/BEATs) + Text Tokenizer (LLM) → Audio-Language Alignment Layer (Q-Former/Linear Projector) → Frozen/LoRA LLM Backbone → Vocabulary Head (generates option tokens)

- Critical path:
  Audio Input → Audio Features → Projection to LLM Space → Concatenation with Question Text → LLM Logits → **Extraction Regex** (maps free-form text to "a", "b", or "None of the above")

- Design tradeoffs:
  - **Multiple-Choice vs. Open-Ended**: The paper notes multiple-choice is cleaner for evaluating "unanswerability" (clear signal) but open-ended is more realistic. *Tradeoff*: Diagnostic clarity vs. deployment realism.
  - **Reasoning vs. Cost**: Chain-of-Thought (CoT) recovers performance but increases inference latency and token cost.

- Failure signatures:
  - **High Ori / Zero AAD**: Model is "deaf" to the absence of correct options (Classic Forced-Choice Bias). seen in Audio Flamingo 3 results (Table 1).
  - **High Ori / Low IAQD**: Model lacks world-knowledge of sensory limits (hallucinates visual details from audio).
  - **High IASD / Low AAD**: Model understands semantic categories (text-text coherence) but cannot verify specific acoustic details (audio-text coherence).

- First 3 experiments:
  1. **Baseline Compliance Check**: Run a capable ALLM (e.g., Qwen2-Audio) on the AAD subset without any prompt modification. *Goal*: Confirm the forced-choice bias exists (Accuracy should drop significantly vs. Original).
  2. **CoT Intervention**: Re-run the same AAD subset with the specific CoT prompt: "First reason step-by-step about whether the question is answerable..." *Goal*: Quantify the "hidden" unanswerability capability.
  3. **Cross-Modal Sanity (IAQD)**: Test the model on IAQD tasks (e.g., "What color is the instrument?"). *Goal*: Determine if the failure is due to audio misinterpretation or a lack of physical/commonsense reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can audio-language models be trained to inherently recognize unanswerable questions without requiring explicit prompting or Chain-of-Thought guidance?
- Basis in paper: [explicit] The authors show that models possess "latent ability to identify unanswerable questions, but often fail to apply this capability without direct instruction," and that CoT prompting recovers this ability. They conclude that models "tend to default to forced-choice behavior unless explicitly guided."
- Why unresolved: The paper demonstrates the problem and a prompting-based workaround, but does not propose or evaluate training methodologies that would embed this abstention capability natively.
- What evidence would resolve it: Training experiments showing models achieving high unanswerability detection without explicit guidance prompts, compared against baseline instruction-tuned models.

### Open Question 2
- Question: What evaluation protocols can reliably assess model abstention behavior in open-ended, free-form response settings?
- Basis in paper: [explicit] Section 6.3 states: "Future research will explore robust evaluation protocols for free-form responses, potentially utilizing advanced LLMs as judges to assess how well models abstain in open-ended settings."
- Why unresolved: The current benchmark uses multiple-choice format for controlled evaluation, but this does not reflect real-world interactions where users pose open-ended questions.
- What evidence would resolve it: A validated evaluation framework for free-form responses, with inter-annotator agreement metrics and correlation with human judgments of appropriate abstention.

### Open Question 3
- Question: How does unanswerability detection performance generalize to complex "in-the-wild" scenarios involving cross-modal reasoning (e.g., inferring speaker identity, visual attributes, or spatial context from audio)?
- Basis in paper: [explicit] Section 6.3 notes: "Real-world scenarios are often more complex, involving queries about visual attributes, specific speaker identities, or object appearances that cannot be inferred from audio alone. Extending the evaluation to cover these 'in-the-wild' scenarios is a priority for our future work."
- Why unresolved: The benchmark focuses on controlled audio-question mismatches, which may not capture the full complexity of real-world unanswerable queries.
- What evidence would resolve it: Evaluation results on extended benchmarks with diverse, naturally-occurring unanswerable queries, showing whether current performance patterns hold.

## Limitations
- The "hidden" capability to detect unanswerability may not be uniformly distributed across models, limiting the generalizability of prompting-based solutions.
- The benchmark relies on carefully constructed negative samples that may not fully capture the diversity of real-world unanswerable scenarios.
- The reliance on multiple-choice format limits the ecological validity of the results, as real-world interactions typically involve open-ended questions.

## Confidence
- **High Confidence**: The identification of the "forced-choice bias" as a fundamental limitation in current audio-language models, supported by robust experimental evidence.
- **Medium Confidence**: The specific mechanisms by which Chain-of-Thought prompting and explicit abstention instructions improve performance, as the exact internal processes are not fully elucidated.
- **Low Confidence**: The extent to which the findings generalize to more complex, open-ended audio question answering tasks and to models with significantly different architectures or training paradigms.

## Next Checks
1. **Cross-Modal Generalization Test**: Evaluate the effectiveness of the AQUA-Bench methodology on visual question answering, where unanswerability concepts are also relevant, to test if the identified mechanisms are modality-agnostic.
2. **Model-Agnostic Intervention Comparison**: Systematically compare the effectiveness of Chain-of-Thought prompting and explicit abstention instructions across a wider range of audio-language models with varying sizes and architectures.
3. **Open-Ended Ablation Study**: Adapt a subset of the AQUA-Bench tasks to an open-ended format and evaluate the same models to test if the forced-choice bias is exacerbated or mitigated in a less constrained setting.