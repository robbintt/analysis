---
ver: rpa2
title: Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers
arxiv_id: '2503.01805'
source_url: https://arxiv.org/abs/2503.01805
tags:
- graph
- each
- width
- embedding
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the expressive power of transformers with varying
  depth and width for graph reasoning tasks. The authors establish a hierarchy showing
  that linear width suffices for constant-depth transformers to solve problems like
  cycle detection and connectivity, while more complex tasks like Eulerian cycle verification
  require near-quadratic width.
---

# Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers

## Quick Facts
- arXiv ID: 2503.01805
- Source URL: https://arxiv.org/abs/2503.01805
- Reference count: 40
- Primary result: Linear width suffices for constant-depth transformers to solve many graph reasoning tasks while offering faster training/inference than deeper models

## Executive Summary
This paper analyzes the expressive power of transformers with varying depth and width for graph reasoning tasks. The authors establish a hierarchy showing that linear width suffices for constant-depth transformers to solve problems like cycle detection and connectivity, while more complex tasks like Eulerian cycle verification require near-quadratic width. The work demonstrates that shallow and wide transformers can match the accuracy of deeper models while offering faster training and inference times. Theoretical lower bounds are complemented by empirical results on synthetic graph tasks, validating the efficiency advantages of the proposed scaling regime.

## Method Summary
The paper uses standard transformer encoder architectures with Multi-Head Self-Attention, FFN, LayerNorm, and residual connections. Models are constrained to approximately 100k parameters with varying depth-width configurations: (1,125), (2,89), (4,63), (8,45), (10,40). Synthetic graphs are generated using Erdős-Rényi, Random Geometric, Scale-Free, and SBM models with 50-400 nodes. Three tokenization schemes are tested: adjacency rows (primary), edge list, and Laplacian eigenvectors. Training uses 100 epochs with learning rates {10⁻⁴, 5·10⁻⁵}, batch sizes {32, 64}, dropout 0.1, and ReLU activations.

## Key Results
- Linear width (m=O(n)) suffices for constant-depth transformers to solve graph reasoning tasks like cycle detection and connectivity
- Different graph tasks require fundamentally different minimum width scaling laws (constant to quadratic)
- Shallow and wide transformers achieve comparable accuracy to deep and narrow models with faster training and inference times
- Critical width for success scales roughly linearly with graph size for counting tasks

## Why This Works (Mechanism)

### Mechanism 1: Width as a Compute Substitutor for Depth
Increasing embedding dimension to linear scale relative to input size allows constant-depth models to solve graph reasoning tasks that would otherwise require deeper architectures. The self-attention mechanism acts as a communication round, and with linear width, each token can store and access a significantly larger portion of the graph's global information, effectively parallelizing operations that would require sequential message-passing steps in narrower models.

### Mechanism 2: Task-Dependent Width Complexity Hierarchy
Different graph reasoning tasks require fundamentally different minimum width scaling laws (from constant to quadratic) to be solved by constant-depth transformers. The width requirement is determined by the communication complexity and the amount of information that must be aggregated to solve a specific problem, with simpler local tasks needing less width and global dense tasks requiring more.

### Mechanism 3: Parallelization-Driven Hardware Efficiency
Shallow and wide transformers achieve comparable accuracy to deep and narrow models but with significantly faster training and inference times on modern parallel hardware. Modern hardware is highly optimized for parallelizing matrix operations, and increasing the width of a single layer allows for greater parallelism within that layer's computation compared to increasing depth.

## Foundational Learning

**Communication Complexity**: Understanding communication complexity concepts is essential to grasp why certain tasks require minimum width, as the paper's core theoretical tool for proving width lower bounds reduces graph problems to problems in communication complexity.

Quick check: How does the lower bound for the "set disjointness" problem in a two-party communication model translate into a lower bound on the width of a transformer?

**Input Tokenization Schemes for Graphs**: The paper's theoretical results are highly sensitive to how the graph is represented as input tokens. Understanding alternatives like edge-list or Laplacian-eigenvector tokenizations is crucial for interpreting the results' generality.

Quick check: According to the paper, why might Laplacian-eigenvector tokenization be better for a task like connectivity, but worse for a task like computing node degrees?

**Transformer Architecture Fundamentals**: A clear understanding of the transformer's components—specifically, how self-attention moves information between tokens and how the MLP processes token-wise information—is necessary to follow the constructive proofs that show how specific algorithms are implemented within the architecture.

Quick check: In a constant-depth transformer, how can increasing the embedding dimension (width) compensate for the lack of layers (depth) in propagating information across the graph?

## Architecture Onboarding

**Component map**: Graph $G$ with $n$ nodes -> Node-Adjacency Tokenization (token $i$ = row $i$ of adjacency matrix + positional encoding) -> Transformer Encoder ($L$ layers, embedding dimension $m$) -> Task-dependent outputs (classification/regression/aggregated)

**Critical path**: 1. Encoding: Convert graph to chosen tokenization format (e.g., adjacency rows) 2. Embedding: Project $n$ input tokens into embedding dimension $m$ 3. Self-Attention Layers: Pass tokens through $L$ sequential attention and MLP layers 4. Readout: Use final token embeddings to predict target property

**Design tradeoffs**:
- **Shallow-Wide vs. Deep-Narrow**: Shallow-Wide models are empirically faster on parallel hardware and can solve many problems requiring depth, but some complex tasks may require minimum depth or very large width
- **Tokenization Choice**: Node-Adjacency (efficient for dense graphs, strong for local tasks), Laplacian-Eigenvector (captures global structure, loses local detail), Edge-List (expressive but computationally expensive)
- **Width vs. Task Complexity**: Task-dependent lower bounds exist; simpler tasks may work with $m = O(\log n)$, complex ones may require $m = O(n^{2-\epsilon})$

**Failure signatures**:
- Under-parameterized Width: Model unable to fit training data (loss plateaus) even with more layers
- Inefficient Depth: Increasing depth under fixed parameter budget leads to slower training/inference without improving accuracy

**First 3 experiments**:
1. **Critical Width Measurement**: For target task (e.g., 4-Cycle Count), train 1-layer transformer varying graph size $n \in \{50, 100, \dots\}$ and embedding dimension $m$, identify critical width where model successfully learns
2. **Depth-Width Efficiency Tradeoff**: For fixed parameter count (~100k), train models with different depth-width configs (e.g., $L=1, m=125$ vs $L=10, m=40$) on graph reasoning task, compare accuracy, training time, and inference latency
3. **Tokenization Ablation**: Compare Node-Adjacency tokenization against Edge-List and Laplacian-Eigenvector tokenizations on OGB molecular property prediction datasets

## Open Questions the Paper Calls Out

**Open Question 1**: Can transformers learn to solve graph problems using standard optimization methods (like SGD) to match the theoretical expressivity bounds, or is there a generalization gap?

**Open Question 2**: How do the depth-width tradeoffs change when using spectral tokenization schemes, such as Laplacian eigenvectors, instead of node-adjacency encodings?

**Open Question 3**: Is the exponential size of the MLP in the sub-quadratic subgraph counting construction fundamental, or can constant-depth transformers solve this with polynomial computation?

## Limitations

- Theoretical results hinge critically on the *node-adjacency* tokenization scheme, limiting generalizability
- Quadratic width lower bound for Eulerian cycle verification relies on the unresolved "1 vs. 2 cycles conjecture"
- Empirical validation focuses on models with approximately 100k parameters, unclear whether efficiency advantages scale to larger models

## Confidence

**High Confidence**: The empirical demonstration that shallow-wide transformers can match the accuracy of deeper models while offering faster training and inference times (Section 6.1).

**Medium Confidence**: The theoretical hierarchy establishing that different graph tasks require different minimum width scaling laws, though conditional results and tokenization restrictions introduce uncertainty.

**Low Confidence**: The claim that linear width suffices for constant-depth transformers to solve a "host of graph-based problems" without qualification, as the paper provides constructive proofs for specific problems but not comprehensive characterization.

## Next Checks

**Next Check 1**: Reproduce critical width experiments using alternative tokenizations (edge-list and Laplacian eigenvectors) to determine whether linear scaling laws hold across different input representations.

**Next Check 2**: Attempt to prove or disprove the 1 vs. 2 cycles conjecture or find alternative unconditional lower bounds for Eulerian cycle verification.

**Next Check 3**: Extend empirical depth-width efficiency experiments to models with 1M+ parameters to determine whether observed advantages persist at larger scales.