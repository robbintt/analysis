---
ver: rpa2
title: 'More Similar than Dissimilar: Modeling Annotators for Cross-Corpus Speech
  Emotion Recognition'
arxiv_id: '2509.12295'
source_url: https://arxiv.org/abs/2509.12295
tags:
- annotator
- annotators
- data
- target
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting speech emotion recognition
  (SER) models to new annotators across different datasets without expensive model
  retraining. The authors propose leveraging inter-annotator similarity by identifying
  a "most similar" pre-trained source annotator for each new target annotator using
  limited enrollment data.
---

# More Similar than Dissimilar: Modeling Annotators for Cross-Corpus Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2509.12295
- Source URL: https://arxiv.org/abs/2509.12295
- Reference count: 35
- This paper proposes leveraging inter-annotator similarity for lightweight cross-corpus speech emotion recognition without model retraining

## Executive Summary
This paper addresses the challenge of adapting speech emotion recognition (SER) models to new annotators across different datasets without expensive model retraining. The authors propose leveraging inter-annotator similarity by identifying a "most similar" pre-trained source annotator for each new target annotator using limited enrollment data. Their method selects the pre-trained annotator model that performs best on the target annotator's training data, then applies this model to the target's test data. The approach significantly outperforms other off-the-shelf methods across three datasets (MSP-Improv, IEMOCAP, MuSE), achieving CCC values of 0.610-0.719 for aggregate predictions compared to 0.438-0.619 for random mapping.

## Method Summary
The method trains a multi-head MLP on MSP-Podcast with separate heads for each annotator, using frozen WavLM and BERT embeddings as input features. For adaptation to new target annotators, the approach uses each target's training data (enrollment set) to evaluate all source heads and select the one with highest CCC. This selected source head is then applied to the target's test data. The approach requires minimal enrollment data (15-20 samples per annotator) and demonstrates stable annotator pairing across multiple folds and seeds.

## Key Results
- IA PT-Mapped approach achieves CCC values of 0.610-0.719 for aggregate predictions across datasets
- Outperforms random mapping baseline (0.438-0.619 CCC) by 0.129-0.206 absolute points
- Stable annotator pairing across different folds and random seeds
- Works effectively with minimal enrollment data (15-20 samples per annotator)

## Why This Works (Mechanism)
The method works because annotators are more similar than dissimilar in their emotion rating patterns. By pre-training on a large source dataset with many annotators, the model learns a rich space of annotation behaviors. The CCC-based similarity matching effectively identifies which source annotator's rating patterns most closely match each new target annotator, enabling personalized predictions without retraining.

## Foundational Learning
- **Speech Emotion Recognition fundamentals**: Understanding dimensional emotion spaces (activation/valence) and evaluation metrics like CCC
  - Why needed: Core task domain and performance measurement
  - Quick check: Can explain difference between CCC and Pearson correlation

- **Cross-corpus adaptation**: Techniques for transferring models between datasets with different speakers, recording conditions, and annotation schemes
  - Why needed: Central problem being solved
  - Quick check: Can describe domain adaptation vs. cross-corpus adaptation

- **Inter-annotator agreement**: Patterns in how different human raters perceive and rate emotional content in speech
  - Why needed: Basis for the similarity-matching approach
  - Quick check: Can explain why annotators show systematic differences in ratings

## Architecture Onboarding

**Component Map:** WavLM/BERT feature extraction -> Multi-head MLP (shared layers + individual annotator heads) -> CCC-based similarity matching -> Target inference

**Critical Path:** Feature extraction → Source model training → Enrollment CCC evaluation → Head selection → Test prediction

**Design Tradeoffs:** Lightweight adaptation via head selection vs. fine-tuning entire model; frozen features for consistency vs. potential gains from end-to-end training

**Failure Signatures:** Low enrollment CCC values indicate no sufficiently similar source annotator exists; high variance in head selection suggests unstable mappings

**First Experiments:**
1. Reproduce baseline CCC scores on MSP-Podcast source dataset to verify feature extraction pipeline
2. Test head selection stability by running enrollment phase multiple times with different random seeds
3. Validate CCC calculation implementation on synthetic emotion prediction tasks

## Open Questions the Paper Calls Out
1. How much performance improvement can be achieved by fine-tuning the annotator-specific models compared to the off-the-shelf mapping approach?
2. Why do fine-tuned aggregate models outperform IA PT-Mapped for valence prediction on IEMOCAP and MSP-Improv specifically?
3. Can failure cases be detected when no sufficiently similar source annotator exists for a given target annotator?
4. Can the annotator similarity mapping approach generalize to categorical emotion recognition or other subjective annotation tasks?

## Limitations
- Limited to dimensional emotion (activation/valence) rather than categorical emotion labels
- Specific optimizer configuration and training hyperparameters not fully specified
- Focuses on three datasets, limiting generalizability claims
- Does not establish thresholds for when the mapping approach should be abandoned

## Confidence
- High confidence: Core methodological contribution and theoretical justification
- High confidence: Empirical superiority over baseline methods
- Medium confidence: Stability claims across random seeds due to limited detail on seed management
- Medium confidence: Minimal data requirement claims given empirical sensitivity analysis

## Next Checks
1. Verify feature extraction pipeline by reproducing baseline CCC scores on MSP-Podcast
2. Test mapping stability by running source head selection process multiple times with different enrollment data samples
3. Validate CCC calculation implementation by comparing against known benchmark implementations on synthetic tasks