---
ver: rpa2
title: How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?
arxiv_id: '2509.21732'
source_url: https://arxiv.org/abs/2509.21732
tags:
- questions
- llms
- gpt-4o
- question
- navigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how accurately large language models (LLMs)
  can answer multiple Yes/No questions about conversational transcripts in a single
  inference pass. The authors propose a batch prompting approach where up to 50 questions
  are grouped with a transcript, and models generate structured JSON responses containing
  both a binary judgment and a supporting utterance index.
---

# How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?

## Quick Facts
- arXiv ID: 2509.21732
- Source URL: https://arxiv.org/abs/2509.21732
- Reference count: 6
- Primary result: Fine-tuned 8-billion-parameter models surpass GPT-4o in Judgment accuracy for larger question groups, with JSON decode errors substantially reduced after fine-tuning.

## Executive Summary
This study evaluates large language models' ability to answer multiple Yes/No questions about conversational transcripts in a single inference pass. The authors propose a batch prompting approach where up to 50 questions are grouped with a transcript, and models generate structured JSON responses containing both a binary judgment and a supporting utterance index. Experiments compare proprietary models (GPT-4o, GPT-4o-mini, Gemini-1.5-flash) against fine-tuned public models (Llama-3, Qwen-2.5) on real customer support call data. While GPT-4o achieves the best overall performance, fine-tuned 8-billion-parameter models surpass GPT-4o in Judgment accuracy when answering larger question groups, demonstrating their strong potential for cost-effective deployment. JSON decoding errors, common in instruction-tuned public models, are substantially reduced after fine-tuning.

## Method Summary
The study employs batch prompting where up to 50 Yes/No questions are presented to LLMs alongside a conversational transcript in a single inference pass. Models output structured JSON containing binary judgments and supporting utterance indices. The evaluation uses 2,800 ASR-generated call transcripts from contact centers, with 50 questions per transcript. Proprietary models (GPT-4o, GPT-4o-mini, Gemini-1.5-flash) are compared against fine-tuned public models (Llama-3.2-1B, Llama-3.1-8B, Qwen2.5-7B). Fine-tuning uses variable question sampling (K∈[5,N] pairs per instance) to avoid generation artifacts, with 3 epochs, learning rate 3e-5, and BFloat-16 precision on 8× A100 GPUs. GPT-4o responses at N=10 serve as pseudo-ground truth.

## Key Results
- GPT-4o achieves the best overall performance in multi-question answering on conversational transcripts
- Fine-tuned 8-billion-parameter models surpass GPT-4o in Judgment accuracy for larger question groups (N>30)
- JSON decode errors decrease from 30-70% in instruction-tuned models to negligible levels after fine-tuning

## Why This Works (Mechanism)
The batch prompting approach enables efficient processing of multiple questions in a single inference pass, reducing computational overhead compared to sequential querying. Variable question sampling during fine-tuning prevents models from learning to always generate exactly N responses regardless of input, addressing a common failure mode in multi-turn generation tasks. The structured JSON output format with enforced schema ensures consistent response parsing and reduces ambiguity in evaluation.

## Foundational Learning
- **ASR-generated transcripts**: Automatic speech recognition output serves as the primary input format, requiring models to handle potential transcription errors and conversational noise. Quick check: Evaluate model performance on human-transcribed vs. ASR-generated versions of the same conversations.
- **Multi-question batching**: Grouping up to 50 questions per transcript enables efficient inference but increases complexity in maintaining context and consistency. Quick check: Measure performance degradation as N increases from 10 to 50.
- **Structured JSON output**: Enforcing output schema with binary judgments and utterance indices standardizes evaluation and enables downstream processing. Quick check: Compare decode error rates between models with and without fine-tuning on the output format.
- **Variable sampling during fine-tuning**: Using K∈[5,N] question pairs per training instance prevents models from learning fixed-length generation patterns. Quick check: Compare model behavior when trained with fixed vs. variable N values.

## Architecture Onboarding
- **Component map**: ASR transcripts -> Batch prompting template -> LLM inference -> JSON output -> Evaluation metrics (Judgment Accuracy, Navigation F1, MAE, Decode Error Rate)
- **Critical path**: Transcript preprocessing → Question batching → Model inference → JSON decoding → Metric computation
- **Design tradeoffs**: Single-pass inference vs. sequential querying (efficiency vs. potential context degradation); larger models vs. fine-tuned smaller models (accuracy vs. cost); pseudo-ground truth vs. human annotation (scalability vs. potential bias)
- **Failure signatures**: Fixed N training causing generation artifacts; high JSON decode error rates in instruction-tuned models; performance degradation at larger N values
- **First experiments**: 1) Test variable vs. fixed N training on a small subset; 2) Evaluate decode error rates on instruction-tuned vs. fine-tuned base models; 3) Measure performance at different N values (10, 20, 30, 40, 50)

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Proprietary dataset limits independent verification of results and generalizability
- Fine-tuning hyperparameters (batch size, optimizer specifics beyond learning rate) remain underspecified
- Evaluation relies on GPT-4o pseudo-ground truth, introducing potential label bias

## Confidence
- **High confidence**: GPT-4o and fine-tuned 8B models achieve strong Judgment accuracy in single-pass inference; fine-tuning substantially reduces JSON decode errors
- **Medium confidence**: Public models surpass GPT-4o in Judgment accuracy for larger question groups, given dependence on proprietary evaluation data and pseudo-ground truth
- **Medium confidence**: Navigation F1 and MAE metrics are consistently reported, though absolute values are tied to the internal dataset

## Next Checks
1. Implement variable K∈[5, N] sampling during fine-tuning to confirm that fixed N training causes generation artifacts
2. Evaluate fine-tuned Llama3.1-8B on a public conversational QA dataset (e.g., MultiWOZ) using the same JSON prompt format to test generalization
3. Test whether the reported performance degradation at N>30 persists when training with larger N values to assess robustness