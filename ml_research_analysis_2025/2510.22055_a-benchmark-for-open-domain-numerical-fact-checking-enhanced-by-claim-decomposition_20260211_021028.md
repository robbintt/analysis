---
ver: rpa2
title: A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition
arxiv_id: '2510.22055'
source_url: https://arxiv.org/abs/2510.22055
tags:
- claim
- claims
- evidence
- queries
- numerical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QuanTemp++, a benchmark for open-domain numerical
  fact-checking enhanced by claim decomposition. The dataset addresses limitations
  in existing benchmarks, such as temporal leakage and lack of focus on numerical
  claims, by collecting 15,000 natural numerical claims and 165,700 evidence records.
---

# A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition

## Quick Facts
- **arXiv ID:** 2510.22055
- **Source URL:** https://arxiv.org/abs/2510.22055
- **Reference count:** 40
- **One-line primary result:** QuanTemp++ dataset with FCDecomp improves retrieval (Recall@10: 0.54, MRR: 0.68) and downstream verification (Macro-F1: 58.92%).

## Executive Summary
This paper introduces QuanTemp++, a benchmark for open-domain numerical fact-checking enhanced by claim decomposition. The dataset addresses limitations in existing benchmarks, such as temporal leakage and lack of focus on numerical claims, by collecting 15,000 natural numerical claims and 165,700 evidence records. The authors propose FCDecomp, a claim decomposition method that generates high-quality queries by emulating human fact-checkers' search processes, ensuring comprehensive coverage of numerical and temporal aspects. Evidence collection is performed with temporal and gold justification leakage filters to ensure realism. Evaluation shows that FCDecomp improves retrieval performance (Recall@10: 0.54, MRR: 0.68) and downstream fact-verification accuracy (Macro-F1: 58.92%) compared to existing methods. The study highlights the importance of decomposition-based query planning and identifies a gap between retrieval and downstream performance, offering insights for future research in automated fact-checking.

## Method Summary
The method introduces QuanTemp++, a dataset of 15,000 natural numerical claims and 165,700 evidence records, with strict temporal and gold leakage filters. FCDecomp, a claim decomposition method, uses few-shot learning with gpt-3.5-turbo and justification documents to generate 6-7 diverse sub-queries per claim, filtered by MMR. Evidence is collected via search APIs with "before:" temporal filters and exclusion of 150+ fact-checking domains. A distilled version (QGen) uses FLAN-T5-LARGE to generate queries without justification documents. Retrieval is performed with Contriever, aggregated via CombMAX-Norm, and verified with roberta-large-mnli. The approach achieves Recall@10: 0.54 and Macro-F1: 58.92%.

## Key Results
- FCDecomp improves retrieval performance: Recall@10: 0.54, MRR: 0.68.
- Downstream fact-verification accuracy: Macro-F1: 58.92%.
- Temporal and gold leakage filters prevent overfitting and ensure realism.
- QGen, a distilled version, achieves competitive performance (Macro-F1: 57.28%) with lower computational cost.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing claims into diverse search queries emulating human fact-checkers improves evidence retrieval for complex numerical claims.
- **Mechanism:** FCDecomp uses an LLM in a few-shot setting, conditioned on the claim and a human-authored justification document, to generate sub-queries targeting explicit and implicit numerical aspects. It filters these using Maximal Marginal Relevance (MMR) to ensure diversity.
- **Core assumption:** The justification document contains the implicit search strategy and reasoning steps of an expert fact-checker, which can be extracted and generalized.
- **Evidence anchors:** Abstract states FCDecomp "generates high-quality queries by emulating human fact-checkers' search processes." Section 3.1 describes using few-shot learning with justification documents. "Retrieve-Refine-Calibrate" (arXiv:2601.16555) supports decomposition paradigms with MMR to mitigate noise.

### Mechanism 2
- **Claim:** Enforcing strict temporal and "gold" evidence constraints during data collection creates a realistic open-domain retrieval setting that prevents model overfitting.
- **Mechanism:** The pipeline applies two filters: 1) removing evidence from 150+ fact-checking domains (gold leakage), and 2) using search engine date operators (`before:`) to exclude documents published after the claim date (temporal leakage).
- **Core assumption:** Validating claims requires the same informational constraints as a real-time fact-checker—specifically, no access to future knowledge or the final verdict article.
- **Evidence anchors:** Abstract notes evidence collection uses "temporal and gold justification leakage filters to ensure realism." Section 3.2 describes filtering 150+ fact-checking domains and using `before:` filters. "FactIR" (arXiv:2502.06006) supports the need for realistic leakage-free benchmarks.

### Mechanism 3
- **Claim:** A smaller language model can effectively approximate the query generation capability of a larger LLM (FCDecomp) for efficient inference.
- **Mechanism:** The approach (QGen) distills the query generation process by fine-tuning a Flan-T5-Large model on (Claim, FCDecomp Queries) pairs, allowing it to generate decomposed queries without the expensive LLM or justification document at test time.
- **Core assumption:** The mapping from a raw claim to an optimal set of sub-queries is a learnable function that does not strictly require the reasoning context of the justification document at inference time.
- **Evidence anchors:** Section 3.3 describes training a smaller language model to "imitate the LLM's decomposition behavior." Table 5 shows QGen achieves Macro-F1: 57.28%, statistically significantly better than "Claim-Only" (55.28%). Corpus evidence for specific distillation techniques is weak in provided neighbors; this appears to be a novel contribution.

## Foundational Learning

- **Concept: Temporal Leakage**
  - **Why needed here:** The paper defines "realism" by the absence of future information. Understanding this is critical for evaluating why standard retrieval metrics might be inflated in other datasets.
  - **Quick check question:** Does your evidence corpus contain documents with publication dates *after* the claim was made?

- **Concept: Claim Decomposition**
  - **Why needed here:** Numerical claims often contain multiple implicit aspects (e.g., a range, a specific date, an entity). A single query typically fails to capture all evidence needs.
  - **Quick check question:** Can a single Google search query verify the claim, or are multiple searches for sub-components required?

- **Concept: Rank Fusion (CombMAX-Norm)**
  - **Why needed here:** When a claim is decomposed into 6+ queries, you retrieve multiple lists of evidence. You need a principled method to merge them into a single set of top-k evidence for the verification model.
  - **Quick check question:** How do you combine relevance scores from two different queries (e.g., "Providence budget 2003" vs "Cicilline reserves") into one ranking?

## Architecture Onboarding

- **Component map:** Input (Claim + Date) -> Query Generator (FCDecomp/QGen) -> Retrieval (Search API + Contriever) -> Aggregation (CombMAX-Norm) -> Verifier (RoBERTa-large-mnli).

- **Critical path:** The quality of the **Query Generator** drives the Recall, which in turn limits the Verifier's ability to handle "Conflicting" verdicts. The paper notes a "gap" where retrieval gains don't fully translate to verification gains, suggesting the Verifier is a secondary bottleneck.

- **Design tradeoffs:**
  - **FCDecomp vs. QGen:** FCDecomp uses expensive LLMs and justification docs (unavailable at test time) but sets the upper bound (Recall@10: 0.54). QGen is cheaper and deployable but slightly lower performance.
  - **Recall vs. Precision:** The paper optimizes for high Recall@100 (0.82) via decomposition, necessitating a robust fusion strategy to manage the noise.

- **Failure signatures:**
  - **Homogeneous Evidence:** Retrieving multiple copies of the same article (solved by MMR filtering).
  - **Temporal Drift:** Verifier accuracy drops sharply if temporal filtering is disabled in the retrieval step.
  - **Numerical Hallucination:** The paper notes LLMs struggle with "numerical contextualization," leading to incorrect verdicts even with correct evidence.

- **First 3 experiments:**
  1. **Establish Baseline:** Run retrieval using the raw claim only ("Claim-Only") vs. FCDecomp queries on QuanTemp++ to measure the upper bound gain from decomposition.
  2. **Validate Leakage Filters:** Ablate the temporal filter to demonstrate the inflation of NLI performance when "future" evidence is allowed.
  3. **Test Distillation:** Train the QGen (Flan-T5) student model and compare its Recall@10 against the teacher (FCDecomp) to quantify the efficiency-performance tradeoff.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can iterative claim decomposition strategies, where intermediate retrieval results inform subsequent query generation, match the performance of methods relying on gold justifications?
  - **Basis in paper:** The authors state, "we hypothesize that iterative approaches that claim decomposition and retrieval are necessary for optimal claim decomposition" to overcome the lack of access to justification documents at test time.
  - **Why unresolved:** FCDecomp relies on gold justifications to generate queries, which are unavailable during realistic inference. Static decomposition baselines (ClaimDecomp, PgmFC) perform significantly worse, leaving a gap in dynamic query planning.
  - **What evidence would resolve it:** Developing and testing an iterative retrieval system on QuanTemp++ that uses retrieved evidence to refine queries dynamically, demonstrating retrieval metrics comparable to the FCDecomp upper bound.

- **Open Question 2:** Does optimizing the claim decomposition module for downstream verification utility (end-to-end) reduce the observed performance gap between retrieval and verification?
  - **Basis in paper:** The paper suggests "one could train the claim decomposition model by optimizing for downstream fact-verification performance in an end-end manner to reduce the observed gap in the performance of retrieval and downstream NLI."
  - **Why unresolved:** Significant gains in retrieval metrics (e.g., Recall@10) achieved by FCDecomp did not proportionally translate into downstream NLI accuracy (Macro-F1), indicating that relevance-based training is misaligned with verification utility.
  - **What evidence would resolve it:** An experiment fine-tuning the query generator using a loss function derived from the downstream verifier’s accuracy rather than retrieval relevance metrics.

- **Open Question 3:** How can fact-verification models be improved to better handle numerical contextualization, such as understanding ranges and thresholds?
  - **Basis in paper:** The authors identify "fundamental numerical contextualization and understanding limitations in claim verification models" which lack the ability to parse ranges and thresholds effectively.
  - **Why unresolved:** Despite improved retrieval, the discrepancy between retrieval performance and downstream verification persists, largely attributed to the NLI model's inability to reason over the numerical aspects of the provided evidence.
  - **What evidence would resolve it:** Architectural modifications or neuro-symbolic approaches for the verification step that specifically target numerical reasoning, resulting in a higher correlation between evidence relevance and veracity prediction accuracy.

## Limitations

- The core claim of achieving "comprehensive coverage" through claim decomposition is limited by the assumption that justification documents contain sufficient reasoning traces for the LLM to extract.
- The temporal leakage filter introduces a potential confound: if the claim date is ambiguous or the search engine's date metadata is inaccurate, the filtering may artificially suppress relevant evidence.
- The distillation method (QGen) assumes that numerical reasoning patterns can be captured without justification documents at inference time, but the paper does not validate whether this assumption holds for claims requiring complex temporal or quantitative reasoning.
- The "gap" between retrieval and downstream performance (Recall@10: 0.54 vs. Macro-F1: 58.92%) suggests that the verification model may be a bottleneck, but the paper does not explore whether this is due to numerical reasoning limitations in the NLI model or the quality of the retrieved evidence.

## Confidence

- **High confidence:** The retrieval metrics (Recall@10: 0.54, MRR: 0.68) and the ablation showing temporal leakage inflation are well-supported by the experimental setup.
- **Medium confidence:** The claim that QGen achieves "competitive" performance (Macro-F1: 57.28%) is supported by Table 5, but the statistical significance of this improvement over "Claim-Only" (55.28%) is not explicitly tested.
- **Low confidence:** The paper asserts that the "gap" between retrieval and verification is due to numerical reasoning limitations, but does not provide ablation studies isolating the NLI model's performance on numerical vs. non-numerical claims.

## Next Checks

1. **Test QGen on out-of-distribution claims:** Evaluate the distilled model on claims involving numerical reasoning patterns not present in the training set to quantify its generalization limits.
2. **Ablate the NLI model:** Replace the RoBERTa-large-mnli verifier with a version fine-tuned only on non-numerical claims to isolate whether the "gap" is due to numerical reasoning or evidence quality.
3. **Stress-test temporal filtering:** Deliberately introduce claims with ambiguous dates and measure the impact on retrieval and verification performance to quantify the robustness of the temporal leakage filter.