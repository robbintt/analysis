---
ver: rpa2
title: 'MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models'
arxiv_id: '2512.24693'
source_url: https://arxiv.org/abs/2512.24693
tags:
- multi-turn
- arxiv
- music
- instruction
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating multi-turn conversations
  in large language models (LLMs), which is crucial for developing capable conversational
  AI systems. Standard preference datasets for training reward models (RMs) often
  only contrast responses based on the final conversational turn, providing insufficient
  signal to capture the nuances of multi-turn interactions.
---

# MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models

## Quick Facts
- arXiv ID: 2512.24693
- Source URL: https://arxiv.org/abs/2512.24693
- Reference count: 13
- Key outcome: MUSIC successfully enhances reward models' ability to identify and promote higher-quality multi-turn interactions without sacrificing performance on standard single-turn benchmarks

## Executive Summary
This paper addresses the challenge of evaluating multi-turn conversations in large language models by proposing MUlti-Step Instruction Contrast (MUSIC), an unsupervised data augmentation strategy. Standard preference datasets for training reward models typically only contrast responses based on the final conversational turn, providing insufficient signal to capture multi-turn interaction nuances. MUSIC uses LLM-based user and assistant simulators to generate paired conversations with guided quality differences across multiple turns, enabling reward models to better distinguish conversational quality throughout the interaction.

## Method Summary
The MUSIC approach synthesizes contrastive conversation pairs exhibiting differences across multiple turns by using LLM-based user and assistant simulators. A contrastive instruction prompt guides the assistant simulator to produce lower-quality responses for one conversation in each pair at every turn. The authors apply this augmentation to the Skywork preference dataset and fine-tune a Gemma-2-9B-Instruct model on the augmented data. The MUSIC-augmented reward model is then evaluated against a baseline (trained without MUSIC) on multi-turn Best-of-N inference and RewardBench tasks, demonstrating improved performance in identifying higher-quality multi-turn interactions.

## Key Results
- Conversations guided by the MUSIC-augmented reward model are preferred over baseline-guided conversations, with preference gap widening as candidate number N increases
- MUSIC-augmented RM achieves slightly better or comparable accuracy on RewardBench, including notable improvement in the Reasoning category
- The approach successfully enhances RM's ability to identify and promote higher-quality multi-turn interactions without sacrificing single-turn benchmark performance

## Why This Works (Mechanism)
MUSIC works by creating synthetic training data that explicitly captures quality differences across multiple conversational turns, addressing the fundamental limitation of existing datasets that only provide single-turn contrast signals. By using simulator-based augmentation with contrastive instructions, the method generates diverse conversation pairs where quality differences are distributed throughout the interaction rather than concentrated at the final turn. This multi-turn signal enables the reward model to learn more nuanced distinctions between conversation qualities, improving its ability to evaluate ongoing interactions rather than just endpoints.

## Foundational Learning

**LLM-based simulation**: Using large language models to simulate user and assistant roles in generating synthetic conversations - needed to create diverse training data at scale; quick check: verify simulators can maintain consistent personas across multiple turns

**Unsupervised data augmentation**: Generating training data without human annotations - needed to address data scarcity for multi-turn evaluation; quick check: measure distribution shift between synthetic and real conversations

**Contrastive learning in RLHF**: Training models to distinguish between high and low-quality responses - needed for reward model training; quick check: verify that contrastive pairs show clear quality differences

**Multi-turn evaluation metrics**: Methods for assessing conversational quality across multiple turns - needed to validate MUSIC's effectiveness; quick check: ensure metrics capture temporal aspects of conversation quality

## Architecture Onboarding

**Component map**: LLM simulator -> Contrastive instruction prompt -> Conversation pair generation -> Reward model training -> Evaluation (Best-of-N, RewardBench)

**Critical path**: The core innovation is the contrastive instruction prompt that guides the assistant simulator to generate quality differences at each turn, creating the multi-turn signal that enables better reward model learning.

**Design tradeoffs**: MUSIC trades computational cost of generating synthetic data for improved multi-turn evaluation capability, accepting potential simulator-induced distributional shifts to overcome real data scarcity.

**Failure signatures**: Poor-quality contrastive pairs where the quality difference is not perceptible across turns, or where simulator behavior introduces unrealistic conversation patterns that could bias the reward model.

**3 first experiments**:
1. Generate a small sample of MUSIC-augmented conversation pairs and manually verify that quality differences are consistently distributed across turns
2. Train a reward model on MUSIC-augmented data and test on synthetic conversations to verify it can correctly rank quality differences
3. Conduct ablation by training without the contrastive instruction prompt to measure its specific contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-based simulation may introduce distributional shifts between synthetic and real user interactions
- Effectiveness of contrastive instruction prompt in consistently producing meaningful quality differences across multiple turns is not fully characterized
- Evaluation methodology lacks exploration of potential degradation in other conversational qualities like empathy or coherence maintenance
- No analysis of how MUSIC augmentation affects reward model behavior in terms of specific conversational capabilities

## Confidence
- High confidence: The technical implementation of the MUSIC augmentation strategy is sound and well-described
- Medium confidence: The empirical improvements demonstrated on the specific evaluation tasks
- Low confidence: The generalizability of results to other conversational domains and the long-term stability of the approach

## Next Checks
1. Conduct ablation studies removing the contrastive instruction prompt to quantify its specific contribution to the observed improvements
2. Test the MUSIC-augmented RM across multiple diverse conversational datasets beyond Skywork to assess domain robustness
3. Perform human evaluation studies to verify that automated preference judgments align with human perceptions of conversation quality across multiple turns