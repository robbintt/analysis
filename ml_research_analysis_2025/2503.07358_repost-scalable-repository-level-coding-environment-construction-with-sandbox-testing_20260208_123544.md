---
ver: rpa2
title: 'RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox
  Testing'
arxiv_id: '2503.07358'
source_url: https://arxiv.org/abs/2503.07358
tags:
- function
- test
- code
- question
- repo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REPO ST constructs execution-based environments for repository-level
  code generation by sandboxing target functions and their local dependencies into
  isolated test scripts. This avoids the complexity of building entire repositories
  and enables scalable dataset construction.
---

# RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing

## Quick Facts
- arXiv ID: 2503.07358
- Source URL: https://arxiv.org/abs/2503.07358
- Authors: Yiqing Xie; Alex Xie; Divyanshu Sheth; Pengfei Liu; Daniel Fried; Carolyn Rose
- Reference count: 35
- Primary result: Training on REPOST-Train improves model performance by 5.5% Pass@1 on HumanEval and 3.5% Pass@1 on RepoEval

## Executive Summary
REPO ST addresses the challenge of constructing execution-based coding environments for repository-level code generation by introducing sandbox testing. The framework isolates target functions with their local dependencies into standalone evaluation scripts, eliminating the need to build entire repositories. This approach enables scalable dataset construction while maintaining functional equivalence through iterative LLM-guided debugging and test coverage improvement. The resulting REPOST-Train and REPOST-Eval datasets demonstrate significant performance gains over traditional synthetic benchmarks.

## Method Summary
REPO ST constructs scalable coding environments by sandboxing target functions with their local dependencies into isolated test scripts. The pipeline extracts call-graph-reachable code, mocks external dependencies, and generates equivalence tests using LLMs. Iterative debugging and coverage improvement ensure high-quality evaluation scripts, which are then used for training via rejection sampling. The framework generates synthetic tests with LLM-based equivalence checking, iteratively debugs and improves test coverage, and validates functionality equivalence and test quality.

## Key Results
- Training on REPOST-Train improves model performance by 5.5% Pass@1 on HumanEval and 3.5% Pass@1 on RepoEval
- REPOST-Eval benchmarks 12 models with GPT-4o achieving 39.53 Pass@1
- Rejection sampling outperforms SFT by 4.3% Pass@1 on HumanEval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Isolating target functions with their local dependencies into standalone evaluation scripts reduces environment setup complexity compared to full repository construction, enabling scalable dataset generation.
- Mechanism: Sandboxing extracts only the call-graph-reachable code and mocks external dependencies (APIs, databases, file I/O), eliminating the need to resolve repository-wide installation issues.
- Core assumption: The external dependencies of individual functions are substantially simpler than whole-repository dependency graphs.
- Evidence anchors:
  - [abstract] "Sandbox testing reduces the complexity of external dependencies and enables constructing environments at a large scale."
  - [section 2.2] "By isolating the function and its local dependencies, we can execute the function by only installing the necessary packages."
  - [corpus] SWE-Bench++ and DockSmith both identify environment construction as the primary bottleneck for repo-level evaluation.

### Mechanism 2
- Claim: Iterative LLM-guided debugging and test coverage improvement yields executable evaluation scripts with high-quality tests at scale without human intervention.
- Mechanism: Pipeline executes scripts, feeds error messages back to LLM for repair, measures branch coverage, prompts LLM to expand tests if below threshold, and applies final quality filters (AST comparison, functionality equivalence, test correctness checks).
- Core assumption: LLMs can reliably diagnose execution errors and generate coverage-expanding test cases when given targeted feedback.
- Evidence anchors:
  - [section 2.4] "If there are any execution errors... we provide the error message as the context and prompt an LLM to debug the evaluation script."
  - [table 2] Achieved 97.8% and 100% average branch coverage on train and eval sets.

### Mechanism 3
- Claim: Training with execution-verified solutions via rejection sampling improves generalization to both algorithmic (HumanEval) and repo-level (RepoEval) benchmarks.
- Mechanism: Candidate solutions are generated, executed against sandbox tests, and only passing solutions are retained for fine-tuning—this filters out hallucinated but syntactically valid code.
- Core assumption: Passing sandbox tests correlates with functional correctness on held-out benchmarks.
- Evidence anchors:
  - [abstract] "Training with the execution feedback provided by REPOST-Train leads to a performance gain of 5.5% Pass@1 on HumanEval and 3.5% Pass@1 on RepoEval."
  - [table 5] Rejection sampling (Distill) outperforms SFT by 4.3% Pass@1 on HumanEval.

## Foundational Learning

- Concept: Call graph extraction and dependency analysis
  - Why needed here: Sandboxing requires identifying which functions, classes, and global variables a target function transitively depends on.
  - Quick check question: Given a function that imports `utils.helpers.process_data`, how would you determine if `process_data` needs to be included in the sandbox?

- Concept: Mock objects and dependency injection for testing
  - Why needed here: External APIs and I/O must be replaced with deterministic mocks to enable isolated, reproducible execution.
  - Quick check question: If a function calls `requests.get()`, what properties must a mock have to preserve the function's observable behavior?

- Concept: Branch coverage and test adequacy metrics
  - Why needed here: The pipeline uses coverage thresholds to determine when tests are sufficient; understanding this helps diagnose why examples are filtered.
  - Quick check question: A function has 10 branches but tests only cover 6—what information would you provide to an LLM to improve coverage?

## Architecture Onboarding

- Component map:
  - Repository Curator -> Sandbox Builder -> Test Generator -> Quality Controller -> Training Pipeline

- Critical path: Sandbox Builder → Test Generator → Quality Controller (debugging iterations) → Coverage Improvement → Final Filtering → Training Data Output

- Design tradeoffs:
  - Strict AST matching (81.7% pass rate) vs LLM-based functionality equivalence check (recovers more examples but introduces noise)
  - 80% coverage threshold for train set vs 100% for eval set trades scale against benchmark reliability
  - Self-distillation vs external distillation trades data authenticity against candidate solution quality

- Failure signatures:
  - Low yield from Sandbox Builder often indicates functions with GPU dependencies not caught by keyword filters
  - Persistent debugging loop failures typically stem from package version mismatches or mock API behavior divergence
  - Low test coverage after improvement rounds suggests functions with implicit behavior (e.g., print output) rather than return values

- First 3 experiments:
  1. Run the pipeline on 10 repositories, manually inspect sandbox fidelity (compare original vs sandboxed function ASTs) to validate equivalence preservation.
  2. Ablate the coverage improvement step to measure yield vs quality tradeoff—track how many additional examples pass at 60%, 80%, 100% thresholds.
  3. Train a small model (e.g., StarCoder2-7B) with SFT-only vs rejection sampling on a 500-example subset to validate execution feedback contribution before scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RepoST sandbox testing framework be effectively adapted for repository-level tasks beyond function generation, such as issue-solving or code refactoring?
- Basis in paper: [explicit] Section 8 states, "In the future, one can also adapt our framework to other repo-level tasks such as issue-solving... code refactoring, or code translation."
- Why unresolved: The current framework focuses on completing isolated functions, whereas tasks like issue-solving require synthesizing context from natural language descriptions and performing multi-file edits.
- What evidence would resolve it: A modified RepoST pipeline that successfully generates executable environments for GitHub issues and evaluates multi-file patches.

### Open Question 2
- Question: How can RepoST datasets be utilized to train or evaluate autonomous coding agents that interact dynamically with the repository environment?
- Basis in paper: [explicit] Section 8 notes, "It is also possible to use our datasets to train and evaluate coding agents. We also leave it to future works."
- Why unresolved: The current work aggregates static context in a prompt, whereas agents require stateful interaction, tool use, and navigation of the original repository structure.
- What evidence would resolve it: Experiments demonstrating improved performance in agentic benchmarks (e.g., SWE-bench) for models fine-tuned on RepoST data in an agent-specific format.

### Open Question 3
- Question: Can training on RepoST-Train significantly close the performance gap between open-source and proprietary models on repository-level code generation?
- Basis in paper: [explicit] Section 5.2 highlights a gap of 5.07 Pass@1 between the best open-source and proprietary models, explicitly calling for "future work to improve open-source models by training."
- Why unresolved: While fine-tuning improves base models, it is unverified if open-source models can reach parity with proprietary models (like GPT-4o) solely through scaling up data with methods like RepoST.
- What evidence would resolve it: An open-source model fine-tuned on RepoST-Train achieving Pass@1 scores statistically indistinguishable from GPT-4o on RepoST-Eval.

## Limitations
- Reliance on LLM-generated equivalence tests introduces uncertainty about true functional coverage
- Keyword-based filtering for GPU/cloud dependencies may miss functions with implicit hardware requirements
- Assumption that passing sandbox tests correlates with real-world functional correctness remains unverified

## Confidence

- **High Confidence**: The sandboxing mechanism for reducing environment complexity is well-supported by direct evidence showing simplified dependency graphs and successful execution of isolated functions.
- **Medium Confidence**: The iterative LLM debugging pipeline achieves high coverage rates (97.8% train, 100% eval), but lacks direct comparison to alternative approaches, making the LLM's debugging capability assumption less certain.
- **Medium Confidence**: The performance gains from execution-verified training (5.5% HumanEval, 3.5% RepoEval) are demonstrated, but the correlation between sandbox test passing and real-world functional correctness remains an assumption requiring validation.

## Next Checks

1. **Coverage Validation**: Compare human-written tests against LLM-generated tests for a sample of functions to measure true functional coverage gaps and identify systematic weaknesses in the test generation process.

2. **Environment Fidelity Assessment**: Run sandboxed functions on a held-out dataset of their original repository contexts to measure execution fidelity and identify failure modes where mocks diverge from real behavior.

3. **Generalization Benchmark**: Test trained models on independent real-world code generation tasks (not HumanEval or RepoEval) to validate whether sandbox-verified solutions generalize beyond the constructed evaluation environment.