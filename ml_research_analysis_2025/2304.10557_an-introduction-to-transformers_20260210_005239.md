---
ver: rpa2
title: An Introduction to Transformers
arxiv_id: '2304.10557'
source_url: https://arxiv.org/abs/2304.10557
tags:
- transformer
- sequence
- attention
- across
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a mathematically precise introduction to the
  transformer architecture, detailing its components and design choices. The transformer
  is a neural network component used to learn useful representations of sequences
  or sets of data-points, driving recent advances in natural language processing,
  computer vision, and spatio-temporal modeling.
---

# An Introduction to Transformers

## Quick Facts
- **arXiv ID:** 2304.10557
- **Source URL:** https://arxiv.org/abs/2304.10557
- **Reference count:** 8
- **Primary result:** Provides a mathematically precise introduction to the transformer architecture and its components

## Executive Summary
This paper offers a rigorous, mathematical introduction to the transformer architecture, detailing its core components and design rationale. The transformer is a neural network component used to learn useful representations of sequences or sets of data-points, driving recent advances in natural language processing, computer vision, and spatio-temporal modeling. The core method involves two stages: self-attention across the sequence and multi-layer perceptron (MLP) across features. The paper details the mechanisms, assumptions, and potential failure modes of the architecture, providing a foundational understanding for practitioners and researchers.

## Method Summary
The method formalizes the standard Transformer block for sequence/set processing (classification or auto-regression). The architecture uses stacked blocks of Multi-Head Self-Attention (MHSA) and Multi-Layer Perceptron (MLP). MHSA computes attention weights using learned query and key matrices, then aggregates values. Layer Normalization is applied per token, and residual connections are used for both stages. Training uses the Adam optimizer with gradient clipping to mitigate instabilities. The paper provides mathematical equations for each component but leaves some practical details (like specific MLP activation) to assumptions.

## Key Results
- The transformer architecture is formalized as a composition of content-based attention and feature-wise MLP operations
- The design choices of residual connections and layer normalization are motivated by the need for training stability
- The paper identifies the $O(N^2)$ attention matrix as a computational bottleneck for long sequences
- Positional encodings are necessary to break the permutation invariance of the self-attention mechanism

## Why This Works (Mechanism)

### Mechanism 1: Content-Based Addressing via Self-Attention
- **Claim:** The transformer creates context-aware representations by dynamically weighting the importance of other tokens in the sequence based on content similarity rather than proximity.
- **Mechanism:** The architecture computes an attention matrix $A$ using dot products between learnable transformations of the input (Queries $U_q$ and Keys $U_k$). This matrix dictates how much token $n'$ contributes to the representation of token $n$. This allows the model to "retrieve" relevant information from distant parts of the sequence, decoupling the attention computation from the specific content values.
- **Core assumption:** Relevant relationships between tokens can be captured by linear projections followed by dot-product similarity.
- **Evidence anchors:**
  - [Section 2.1]: "An alternative is to perform the same operation on a linear transformation... decoupling the attention computation from the content."
  - [Section 2.1]: "Intuitively speaking $A^{(m)}_{n',n}$ will take a high value for locations in the sequence $n'$ which are of high relevance for location $n$."
  - [Corpus]: "Introduction to Sequence Modeling with Transformers" supports the centrality of attention as the main working horse.
- **Break condition:** If the relationship between tokens is non-linear or cannot be captured by the specific $U_q, U_k$ projection rank, the attention mechanism may fail to route relevant signals.

### Mechanism 2: Iterative Refinement via Interleaved Processing
- **Claim:** The architecture builds complex representations by alternating between mixing information across the sequence (horizontal) and processing features within a token (vertical).
- **Mechanism:** A transformer block alternates between Multi-Head Self-Attention (MHSA), which aggregates data across the sequence dimension, and a Multi-Layer Perceptron (MLP), which applies non-linear transformations across the feature dimension. Residual connections allow these stages to perform "mild non-linear transformations" that compose over depth.
- **Core assumption:** Complex functions can be decomposed into sequence-mixing and feature-refining operations.
- **Evidence anchors:**
  - [Section 2]: "The block itself comprises two stages: one operating across the sequence and one operating across the features."
  - [Section 2.3]: "Over many layers, these mild non-linear transformations compose to form large transformations."
  - [Corpus]: Weak direct evidence in provided neighbors; "Finding Clustering Algorithms" implies internal structural discovery but does not detail the interleaving mechanism explicitly.
- **Break condition:** If the sequence mixing (Attention) and feature mixing (MLP) require strong coupling within a single layer to solve a specific sub-problem, the sequential nature of the block might limit optimization speed.

### Mechanism 3: Training Stability via Residual Parameterization and Normalization
- **Claim:** Deep transformer networks remain trainable because they are parameterized to start near the identity function and are explicitly normalized to prevent magnitude explosion.
- **Mechanism:** Residual connections ($x^{(m)} = x^{(m-1)} + \text{res}_{\theta}(\dots)$) allow the model to learn deviations from the input. Layer Normalization (TokenNorm) explicitly centers and scales the feature vector for each token, preventing unbounded activation growth.
- **Core assumption:** The desired mapping is close enough to the identity that residual learning provides a useful optimization landscape.
- **Evidence anchors:**
  - [Section 2.3]: "This type of parameterisation is used... with the idea that each applies a mild non-linear transformation."
  - [Section 2.3]: "This transform stops feature representations blowing up in magnitude as non-linearities are repeatedly applied."
  - [Corpus]: No specific corpus evidence contradicts this, though "TriagerX" mentions PLMs attending to irrelevant tokens, suggesting stability/training challenges remain in the attention mechanism specifically.
- **Break condition:** If the optimal function requires a complete transformation of the input distribution (far from identity) or if normalization washes out critical magnitude-based signals, performance may degrade.

## Foundational Learning

- **Concept:** Linear Transformations & Matrix Multiplication
  - **Why needed here:** The paper describes the entire architecture in terms of matrix operations ($Y = X A$, $q = U x$). Understanding dimensions ($D \times N$) is critical for implementing the "token vs. feature" distinction.
  - **Quick check question:** If $X$ is $D \times N$ (features $\times$ tokens), what are the dimensions of the Query matrix $U_q$ if it projects to a space of size $K$?

- **Concept:** Softmax Function
  - **Why needed here:** It is the engine of normalization for both the attention weights (sum to 1) and the output probability distributions in language modeling heads.
  - **Quick check question:** Why is the softmax applied to the dot product of the query and key before multiplying by the value?

- **Concept:** Multi-Layer Perceptron (MLP)
  - **Why needed here:** This is the "Stage 2" of the block where non-linear feature processing happens. You must understand how a simple feed-forward network acts on a vector.
  - **Quick check question:** In a transformer, the MLP is applied independently to every token vector $y_n$. Does the MLP at position 1 share weights with the MLP at position 2?

## Architecture Onboarding

- **Component map:**
  1.  **Input:** Matrix $X^{(0)}$ of size $D \times N$ (Features $\times$ Tokens).
  2.  **Stage 1 (MHSA):** Projects $X$ to Queries ($Q$) and Keys ($K$) via learned matrices. Computes Attention $A = \text{softmax}(Q^\top K)$. Projects input via Values ($V$) and aggregates: $Y = V X A$.
  3.  **Residual/Add & Norm:** Add $Y$ to previous input, normalize features per token.
  4.  **Stage 2 (MLP):** Apply feed-forward network to each column (token) of the matrix independently.
  5.  **Residual/Add & Norm:** Add result to previous stage, normalize again.

- **Critical path:** The matrix multiplication $X A^{(m)}$ in the attention head. This is where the $O(N^2)$ memory bottleneck occurs. Correctly implementing the masking (upper triangular) here is essential for auto-regressive tasks to prevent "future leakage."

- **Design tradeoffs:**
  - **Positional Encoding:** The paper notes the transformer is permutation invariant (treats data as a set). You *must* add positional encodings (learned or fixed sinusoids) if order matters (e.g., text), but you might omit them for true sets.
  - **Normalization Location:** While the paper describes a standard setup (Norm before MHSA/MLP), it notes configurations differ. TokenNorm (LayerNorm) is standard; BatchNorm is found to be unstable.

- **Failure signatures:**
  - **Permutation Invariance:** The model outputs the same representation for "dog bites man" and "man bites dog" if position encodings are missing or insufficient.
  - **Training Instability:** The paper explicitly mentions that despite normalization, transformers can be unstable as training progresses, requiring gradient clipping and learning rate decay.
  - **Memory Explosion:** Processing long sequences fails due to the $N \times N$ attention matrix storage.

- **First 3 experiments:**
  1.  **Dimensionality Check:** Implement the forward pass with random matrices. Pass an input $X$ of shape $D \times N$ and verify the output is also $D \times N$.
  2.  **Permutation Test:** Feed a sequence, then feed the shuffled sequence (without position encodings). Verify the output features are identical but permuted correspondingly.
  3.  **Causal Masking Validation:** For an auto-regressive setup, verify that attention weights for position $n$ are zero for all positions $n' > n$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the loss of representational power caused by auto-regressive masking be mitigated by increasing the capacity of the model (e.g., increasing token dimensionality $D$)?
- Basis in paper: [explicit] The author states in Footnote 22: "Itâ€™s an open question as to how significant this is and whether increasing the capacity of the model can mitigate it e.g. by using higher dimensional tokens, i.e. increasing $D$."
- Why unresolved: Masking restricts the attention mechanism to previous tokens only, which theoretically limits the model's ability to capture relationships compared to bidirectional attention, but the trade-off between this restriction and model width is not fully mapped.
- What evidence would resolve it: Empirical studies comparing the performance of auto-regressive models with varying embedding dimensions against unmasked baselines on tasks requiring complex structural understanding.

### Open Question 2
- Question: Is there experimental evidence to support the necessity of separate query and key matrices ($U_q \neq U_k$) over symmetric similarity measures?
- Basis in paper: [explicit] Footnote 9 notes: "However, I do not know of experimental evidence to support using $U_q \neq U_k$."
- Why unresolved: While asymmetric matrices allow for directed relationships (e.g., 'caulking iron' $\to$ 'tool'), it is unclear if this flexibility is essential for performance or if symmetric similarity suffices for most tasks.
- What evidence would resolve it: Ablation studies constraining $U_q = U_k$ across standard benchmarks (like NLP translation or image classification) to quantify any performance degradation.

### Open Question 3
- Question: How can the quadratic computational bottleneck of the $N \times N$ attention matrix be overcome for long sequences?
- Basis in paper: [inferred] The paper identifies in Footnote 5 that "The need for transformers to store and compute $N \times N$ attention arrays can be a major computational bottleneck, which makes processing of long sequences challenging."
- Why unresolved: The standard self-attention mechanism requires comparing every token pair, leading to $O(N^2)$ complexity. While sparse attention exists, the paper implies a general need for solutions to this inherent scaling limit.
- What evidence would resolve it: The development of an attention mechanism with provably sub-quadratic complexity (e.g., $O(N \log N)$) that maintains the expressivity and accuracy of the standard mechanism on long-sequence benchmarks.

### Open Question 4
- Question: Why does the additive construction of position embeddings work effectively, and is it strictly superior to concatenation?
- Basis in paper: [inferred] The paper mentions regarding position encoding: "E.g. by simply adding the position embedding (surprisingly this works) or concatenating."
- Why unresolved: Theoretically, adding a position vector to a content vector might seem like noise or interference, yet it is the standard. The paper mathematically hints that a linear transform can recover the additive construction from concatenation, but the empirical success of the additive approach remains somewhat counter-intuitive.
- What evidence would resolve it: Analysis of the vector space showing how addition preserves semantic direction while encoding phase/frequency information, or comparative studies showing failure modes of concatenation approaches in deep networks.

## Limitations
- The paper is a theoretical introduction without empirical validation of the mechanisms or training procedures
- Practical implementation details like specific MLP activation functions and exact training schedules are left as assumptions
- The confidence in the mechanisms described is primarily based on mathematical consistency and widespread adoption, not direct evidence from this work

## Confidence
- **Mechanism 1 (Content-Based Addressing via Self-Attention):** High. The mathematical formulation is explicit and well-defined.
- **Mechanism 2 (Iterative Refinement via Interleaved Processing):** High. The alternating structure is clearly described as the core design principle.
- **Mechanism 3 (Training Stability via Residual Parameterization and Normalization):** Medium. The paper states this is the intention of the design, but does not provide empirical evidence.

## Next Checks
1. **Implement the Forward Pass:** Code the transformer block from the mathematical definitions in the paper (Sections 2.1-2.3). Verify that the attention weights $A$ sum to 1 over the column dimension and that the output dimension matches the input dimension ($D \times N$).
2. **Test Permutation Invariance:** Feed a sequence and its shuffled version into the model (with position encodings disabled). Verify that the output feature representations are identical but permuted, confirming the model's set-like behavior.
3. **Validate Causal Masking:** For an auto-regressive task, implement the causal mask in the attention computation. Verify that for position $n$, the attention weights for all positions $n' > n$ are zero, preventing "future leakage" in the prediction.