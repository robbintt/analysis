---
ver: rpa2
title: 'The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus
  Training on Model Values and Biases'
arxiv_id: '2508.12411'
source_url: https://arxiv.org/abs/2508.12411
tags:
- cultural
- language
- arxiv
- biases
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether Large Language Models (LLMs) inherit\
  \ distinct cultural orientations\u2014conceptualized as \"cultural genes\"\u2014\
  from their training data. The authors introduce a Cultural Probe Dataset (CPD) of\
  \ 200 prompts targeting Individualism-Collectivism (IDV) and Power Distance (PDI)\
  \ dimensions, and evaluate two models: GPT-4 (Western-centric) and ERNIE Bot (Eastern-centric)."
---

# The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases

## Quick Facts
- arXiv ID: 2508.12411
- Source URL: https://arxiv.org/abs/2508.12411
- Authors: Emanuel Z. Fenech-Borg; Tilen P. Meznaric-Kos; Milica D. Lekovic-Bojovic; Arni J. Hentze-Djurhuus
- Reference count: 40
- This study finds that LLMs function as statistical mirrors of their cultural corpora, with GPT-4 aligning with USA (IDV CAI ≈ 0.91; PDI CAI ≈ 0.88) and ERNIE Bot aligning with China (IDV CAI ≈ 0.85; PDI CAI ≈ 0.81).

## Executive Summary
This study investigates whether Large Language Models (LLMs) inherit distinct cultural orientations—conceptualized as "cultural genes"—from their training data. The authors introduce a Cultural Probe Dataset (CPD) of 200 prompts targeting Individualism-Collectivism (IDV) and Power Distance (PDI) dimensions, and evaluate two models: GPT-4 (Western-centric) and ERNIE Bot (Eastern-centric). Using standardized zero-shot prompts and human annotation, they find statistically significant differences in cultural alignment. GPT-4 scores high on individualism and low on power distance (IDV ≈ 1.21; PDI ≈ -1.05), while ERNIE Bot shows the opposite pattern (IDV ≈ -0.89; PDI ≈ 0.76). Both models closely align with Hofstede's scores for the USA and China, respectively, with Cultural Alignment Indices ranging from 0.81 to 0.91. Qualitative case studies confirm these patterns in ethical reasoning. The results demonstrate that LLMs act as statistical mirrors of their cultural corpora, raising concerns about algorithmic cultural hegemony and underscoring the need for culturally aware AI evaluation and deployment.

## Method Summary
The study constructs a Cultural Probe Dataset (CPD) of 200 prompts across two Hofstede dimensions (IDV and PDI), using three probe types: Value-Dilemma Probes, Scenario-Judgment Probes, and Stereotype-Association Probes. Two models (GPT-4 and ERNIE Bot) are evaluated using zero-shot prompting at temperature 0.7, with human annotation mapping responses to Cultural Dimension Scores (CDS) on a -2 to +2 scale. Cultural Alignment Index (CAI) scores are calculated by comparing model CDS against Hofstede's national cultural scores for the USA and China. Statistical significance is assessed via independent samples t-tests.

## Key Results
- GPT-4 exhibits individualistic and low-power-distance orientation (IDV ≈ 1.21; PDI ≈ -1.05)
- ERNIE Bot exhibits collectivistic and high-power-distance orientation (IDV ≈ -0.89; PDI ≈ 0.76)
- GPT-4 aligns closely with USA cultural scores (IDV CAI ≈ 0.91; PDI CAI ≈ 0.88)
- ERNIE Bot aligns closely with China cultural scores (IDV CAI ≈ 0.85; PDI CAI ≈ 0.81)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs function as statistical mirrors that reflect cultural value distributions embedded in their training corpora.
- Mechanism: During pretraining, models learn statistical associations between linguistic patterns and culturally-situated concepts. When prompted, they generate responses by sampling from these learned distributions, producing outputs that align with dominant cultural norms in the training data.
- Core assumption: Cultural values are encoded in language patterns consistently enough to be captured by statistical learning.
- Evidence anchors:
  - [abstract] "Our results support the view that LLMs function as statistical mirrors of their cultural corpora"
  - [section 5.4.1] "When a model produces a 'collectivistic' response, it is because the statistical associations between the prompt's concepts and collectivistic language patterns are stronger in its training corpus"
  - [corpus] Related work on cultural value adaptation in LLMs (arXiv:2511.03980) confirms that prompt language and cultural framing impact model outputs, supporting the statistical mirroring hypothesis
- Break condition: If models were explicitly fine-tuned to override cultural priors through RLHF or other alignment techniques targeting cultural neutrality, the mirroring effect could be attenuated or masked.

### Mechanism 2
- Claim: Cultural alignment is quantifiable through forced-choice value dilemmas that reveal underlying prioritization hierarchies.
- Mechanism: Value-Dilemma Probes (VDPs) present scenarios where optimal choices differ across cultural frameworks. Models must resolve conflicts between competing values (e.g., individual achievement vs. group harmony), revealing which cultural priors dominate in their latent representations.
- Core assumption: Cultural dimensions like IDV and PDI represent coherent value clusters that models encode consistently.
- Evidence anchors:
  - [section 5.3] "Value-Dilemma Probes (VDPs) elicited the strongest cultural signals from both models, yielding the highest average absolute CDS values (M_W: 1.35, M_E: 1.02)"
  - [section 5.1.1] GPT-4 achieved IDV score of 1.21 (individualistic), ERNIE achieved -0.89 (collectivistic), with p < 0.001
  - [corpus] Work on Japanese-specific bias evaluation (arXiv:2509.24468) similarly uses culturally-grounded scenarios to assess model alignment
- Break condition: If models learn to recognize probing patterns and strategically evade revealing values, measured alignment would underestimate actual cultural priors.

### Mechanism 3
- Claim: Cultural Alignment Index (CAI) provides external validation by correlating model outputs with established cultural dimension scores.
- Mechanism: Model responses are scored on cultural dimensions, then compared against Hofstede's national cultural indices. High CAI indicates the model's value orientation statistically matches the cultural profile of a specific population.
- Core assumption: Hofstede's framework adequately captures meaningful cultural variation that transfers to model behavior.
- Evidence anchors:
  - [abstract] "GPT-4 aligns more closely with the USA (IDV CAI ≈ 0.91; PDI CAI ≈ 0.88) whereas ERNIE Bot aligns more closely with China (IDV CAI ≈ 0.85; PDI CAI ≈ 0.81)"
  - [section 5.1.2] "These CAI scores demonstrate that the biases we measured correspond directly to the dominant cultural frameworks of the regions where the models' training data primarily originated"
  - [corpus] Limited direct corpus validation of CAI methodology; related work focuses on prompt-level cultural framing rather than index-based measurement
- Break condition: If Hofstede's framework lacks granularity for intra-cultural diversity or fails to capture modern cultural dynamics, CAI scores may misrepresent actual model alignment.

## Foundational Learning

- Concept: **Hofstede's Cultural Dimensions Theory**
  - Why needed here: The entire probing methodology is grounded in IDV (Individualism-Collectivism) and PDI (Power Distance) dimensions. Without understanding these constructs, you cannot interpret CDS scores or CAI alignments.
  - Quick check question: Can you explain why a collectivistic culture might prioritize group harmony over individual achievement in a workplace dilemma?

- Concept: **Zero-shot Probing**
  - Why needed here: The methodology uses standardized zero-shot prompts to elicit intrinsic model properties without fine-tuning or few-shot examples that could contaminate results.
  - Quick check question: Why would zero-shot prompting be preferred over few-shot prompting when attempting to measure a model's "default" cultural orientation?

- Concept: **Statistical vs. Semantic Bias**
  - Why needed here: The paper argues that cultural "genes" are not surface-level stereotypes but systematic statistical patterns. Distinguishing between explicit bias (what a model says overtly) and implicit statistical associations is critical for interpreting SAP results.
  - Quick check question: Why did Stereotype-Association Probes yield smaller CDS magnitudes (0.89, 0.65) compared to Value-Dilemma Probes (1.35, 1.02)?

## Architecture Onboarding

- Component map:
  Cultural Probe Dataset (CPD) -> Model inference (zero-shot, temperature 0.7) -> Human annotation -> CDS computation -> CAI calculation against Hofstede reference scores

- Critical path: Probe design → Model inference (zero-shot, temperature 0.7) → Human annotation → CDS computation → CAI calculation against Hofstede reference scores

- Design tradeoffs:
  - VDPs maximize signal strength but force artificial dichotomies; SJPs offer more naturalistic scenarios; SAPs reduce obtrusiveness but yield weaker signals
  - Using Hofstede's national scores provides external validity but averages over intra-cultural heterogeneity
  - Single-query per probe (no sampling distribution) limits variance estimation

- Failure signatures:
  - Low CDS variance across dimensions suggests probes fail to elicit value conflicts
  - CAI scores near 0.5 for all reference cultures indicates measurement noise or model ambiguity
  - High alignment with "wrong" culture (e.g., GPT-4 aligning with China) signals methodology failure or data contamination

- First 3 experiments:
  1. **Baseline replication**: Run CPD on a new Western-centric model (e.g., Claude) and verify IDV > 0 and PDI < 0 alignment with USA
  2. **Probe-type ablation**: Isolate VDP-only vs. SAP-only scoring to quantify signal strength differential for your target model
  3. **Temperature sensitivity**: Test whether higher sampling temperatures (e.g., 1.0 vs. 0.7) increase variance in CDS without changing mean alignment direction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mechanistic interpretability techniques pinpoint where and how cultural representations are encoded within transformer architectures?
- Basis in paper: [explicit] "Future work could use techniques from mechanistic interpretability... to pinpoint exactly where and how these cultural representations are stored in the network."
- Why unresolved: The current study is purely behavioral (input-output analysis) and does not examine internal activations, attention patterns, or layer-wise representations.
- What evidence would resolve it: Causal tracing or activation patching studies identifying specific neurons, heads, or layers that encode cultural value associations.

### Open Question 2
- Question: Do additional Hofstede dimensions (Uncertainty Avoidance, Masculinity-Femininity, Long-Term Orientation, Indulgence) show similar cross-corpus cultural alignment patterns?
- Basis in paper: [explicit] "Our analysis was limited to two cultural dimensions... Future work should expand this framework to include more dimensions (e.g., Uncertainty Avoidance)."
- Why unresolved: The CPD only probes IDV and PDI; other dimensions remain untested.
- What evidence would resolve it: Constructing probes for additional dimensions and computing CAI scores across diverse models.

### Open Question 3
- Question: What training or fine-tuning interventions can effectively steer or mitigate ingrained cultural biases beyond superficial debiasing?
- Basis in paper: [explicit] "The next critical step is prescriptive: developing methods to effectively steer or mitigate these ingrained cultural biases, moving beyond simple debiasing techniques."
- Why unresolved: The study is diagnostic; no mitigation strategies were tested.
- What evidence would resolve it: Experiments comparing RLHF, data rebalancing, or cultural fine-tuning showing reduced CAI asymmetries while maintaining model capability.

### Open Question 4
- Question: How do cultural genes manifest in models from underrepresented cultural spheres (e.g., India, Middle East, Africa)?
- Basis in paper: [explicit] "Future work should expand this framework to include... a wider array of models, including those from other cultural spheres (e.g., India, the Middle East)."
- Why unresolved: Only Western (GPT-4) and Eastern Chinese (ERNIE) models were compared.
- What evidence would resolve it: Extending the CPD evaluation to models primarily trained on Hindi, Arabic, or Swahili corpora and computing alignment with respective Hofstede scores.

## Limitations
- The CPD construction lacks transparency regarding probe selection criteria and cultural equivalence across dimensions
- The CAI formula remains unspecified despite being central to the main conclusions
- The paper does not address potential temporal shifts in model behavior as models receive additional fine-tuning or safety updates

## Confidence
- **High confidence** in the finding that GPT-4 and ERNIE Bot show statistically different cultural orientations (p < 0.001)
- **Medium confidence** in the CAI methodology and its interpretation as measuring "alignment with USA/China"
- **Low confidence** in generalizability beyond the two specific models tested

## Next Checks
1. Replicate the study with a third Western-centric model (e.g., Claude or Llama) to test whether IDV > 0, PDI < 0 pattern generalizes beyond GPT-4
2. Apply the CPD to a multilingual model (e.g., mGPT or BLOOM) to determine whether cultural orientation varies with language of interaction or remains consistent
3. Conduct a sensitivity analysis on probe selection by randomly sampling subsets of the CPD to assess stability of CDS and CAI scores across different probe combinations