---
ver: rpa2
title: Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image
  Models
arxiv_id: '2509.21360'
source_url: https://arxiv.org/abs/2509.21360
tags:
- image
- prompt
- prompts
- content
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of bypassing safety filters in
  text-to-image models to generate Not-Safe-for-Work (NSFW) content. The proposed
  method, Multimodal Prompt Decoupling Attack (MPDA), utilizes image modality to separate
  harmful semantic components from original unsafe prompts.
---

# Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models

## Quick Facts
- **arXiv ID:** 2509.21360
- **Source URL:** https://arxiv.org/abs/2509.21360
- **Reference count:** 40
- **Primary result:** MPDA achieves 93% bypass success rate on NSFW content across major T2I models, outperforming text-only methods by up to 29%.

## Executive Summary
This paper presents Multimodal Prompt Decoupling Attack (MPDA), a novel method for bypassing safety filters in text-to-image models to generate NSFW content. The attack operates in three stages: decomposing unsafe prompts into semantic components using an LLM, generating a benign base image from pseudo-safe components, and iteratively rewriting harmful components into adversarial prompts guided by a VLM. MPDA achieves an average bypass success rate of 93% across four unsafe prompt datasets on leading T2I models, demonstrating that current safety filters lack robust cross-modal intent analysis. The authors also propose a Reconstructed Prompt Security Check (RPSC) defense, though its effectiveness remains to be fully validated.

## Method Summary
MPDA decouples unsafe prompts into six semantic components (subject, action, condition, environment, atmosphere, style) using DeepSeek-v3 LLM, classifying them as pseudo-safe or harmful. A base image is generated from pseudo-safe components, while harmful components are iteratively rewritten into adversarial prompts validated by LlamaGuard. The final NSFW image is produced by fusing the base image with the adversarial prompt using IP-Adapter or commercial image-prompting features. The process includes a feedback loop using VLM captioning and CLIP similarity scoring to ensure semantic fidelity across up to 10 iterations.

## Key Results
- Achieves 93% average bypass success rate across four unsafe prompt datasets
- Outperforms previous methods by 29% on pornographic content attacks against Midjourney
- Demonstrates that multimodal fusion distributes harmful semantics across modalities, evading text-only and image-only filters

## Why This Works (Mechanism)

### Mechanism 1
- Decomposing unsafe prompts into fine-grained semantic components reduces detection by unimodal safety filters
- LLM splits prompts into six independent sub-prompts; safety filters evaluate each individually rather than reconstructing full intent
- Core assumption: Safety filters lack holistic intent analysis to reconstruct harmful intent from separated benign components
- Evidence: LlamaGuard shows reduction in harmful prompt detection rates from 44% to 6% for violent content after decoupling

### Mechanism 2
- Multimodal text-image fusion bypasses safety filters by distributing harmful semantics across two modalities
- Pseudo-safe prompt generates benign base image; adversarial prompt is combined with base image as joint input
- Core assumption: T2I model safety mechanisms don't robustly evaluate emergent harmful intent from benign image + obfuscated text fusion
- Evidence: MPDA achieves higher bypass rates than text-only methods, particularly on commercial models with strong text filters

### Mechanism 3
- Feedback loop using VLM guidance refines adversarial prompts while maintaining semantic fidelity
- Iterative process: generate image → VLM caption → CLIP similarity check → LLM prompt rewrite
- Core assumption: VLM provides accurate feedback and LLM can successfully interpret feedback to refine prompts
- Evidence: Algorithm 1 details iterative loop; convergence achieved within 10 iterations for semantic similarity ≥ 0.26

## Foundational Learning

- **Text-to-Image (T2I) Model Safety Filters**
  - Why needed: Understanding target defense is critical; multi-stage filters (textual prompt filtering, post-generation image moderation) are designed to block NSFW content
  - Quick check: What are the two primary stages of defense in a modern commercial T2I model like Midjourney?

- **Prompt Decoupling and Semantic Components**
  - Why needed: Core technique of attack; decomposing complex prompts into six independent semantic components is essential for understanding attack and proposed defense
  - Quick check: What are the six semantic components used by MPDA to decouple an unsafe prompt?

- **Cross-Modal Fusion and Image Weight**
  - Why needed: Attack succeeds by fusing information from pseudo-safe image and adversarial text prompt; understanding "image weight" parameter is crucial
  - Quick check: What happens to attack's success if image weight parameter is set too high or too low?

## Architecture Onboarding

- **Component map:** Unsafe Prompt → Prompt Decoupler → [Pseudo-safe Prompts → Base Image Generator] and [Harmful Prompts → Adversarial Rewriter → Safety Validator] → Multimodal Image Generator → [Generated Image → VLM → CLIP → Feedback to Adversarial Rewriter]

- **Critical path:** `Unsafe Prompt -> Prompt Decoupler`. This produces two parallel streams: `Pseudo-safe Prompts -> Base Image Generator` and `Harmful Prompts -> Adversarial Rewriter`. These streams converge at the `Multimodal Image Generator`, which takes both the Base Image and the Adversarial Prompt to produce the final image.

- **Design tradeoffs:** Attack success is sensitive to "image weight" parameter. High weight prioritizes benign base image, risking loss of harmful semantics. Low weight prioritizes adversarial text, risking filter detection. Iterative feedback loop adds computational cost but is necessary for semantic fidelity.

- **Failure signatures:**
  - Semantic Drift: Generated image is benign or unrelated to original intent; VLM/CLIP feedback loop intended to correct this
  - Filter Blockage: Adversarial prompt repeatedly detected by safety filters; LLM rewriter must find new evasion strategy
  - Incoherent Fusion: Final image appears as disjointed overlay, indicating poor fusion by T2I model or suboptimal parameter settings

- **First 3 experiments:**
  1. Reproduce Ablation on Midjourney: Compare bypass rate and harmfulness of pure text-based attack vs. full MPDA multimodal attack using same dataset
  2. VLM Feedback Ablation: Run iterative rewriting loop with and without VLM-derived captions (using only CLIP scores) to quantify VLM's impact
  3. Image Weight Parameter Sweep: Systematically vary image weight parameter (0.1, 1.0, 3.0) on target model to determine optimal balance for different unsafe prompt types

## Open Questions the Paper Calls Out

### Open Question 1
- Can delicate balance between textual and visual guidance weights be automated to ensure optimal attack success without manual calibration?
- Basis: Section IV.G states "We aim to address these challenges in future work by exploring methods to... automate weight optimization"
- Why unresolved: Current method relies on specific default weight values (e.g., 1.0 for Midjourney), with performance dropping significantly if text or image guidance is too dominant
- What evidence would resolve it: Adaptive weighting algorithm that dynamically adjusts parameters based on iterative feedback, demonstrating stable success rates without human intervention

### Open Question 2
- How effective is RPSC defense in neutralizing MPDA attacks, particularly regarding VLM captioning fidelity?
- Basis: Section IV.F introduces RPSC but concludes "The effectiveness of this approach depends on the fidelity of the description provided by VLM, and further exploration is needed"
- Why unresolved: Defense strategy is conceptual; authors did not implement or empirically validate whether VLM captions can reliably reconstruct harmful intent from decoupled image-text pairs
- What evidence would resolve it: Experimental results showing detection accuracy of RPSC against MPDA, measuring semantic gap between reconstructed prompt and original harmful intent

### Open Question 3
- Can multimodal generation process be streamlined to reduce high computational costs associated with pay-per-use commercial platforms?
- Basis: Section IV.G notes "The method's reliance on generating a base image inherently increases computational costs, particularly when using pay-per-use commercial platforms"
- Why unresolved: Attack requires generating base image and iteratively refining prompts, which is resource-intensive and financially costly for attackers using commercial APIs
- What evidence would resolve it: Modified attack framework that minimizes number of required generation steps or utilizes lower-cost surrogate models without sacrificing bypass success rates

## Limitations

- **VLM Dependency and Generalization**: Attack critically depends on VLM's ability to accurately caption generated images and identify semantic drift; effectiveness varies with VLM vision-language capabilities
- **Black-Box Assumptions and Defense Specificity**: Efficacy on models with fundamentally different architectures or stronger integrated multimodal safety classifiers is not validated
- **Image Weight Sensitivity**: Optimal setting may be highly model-specific and prompt-dependent; paper lacks systematic study of parameter impact across different scenarios

## Confidence

- **Bypass Success Rates (93% average, 29% higher than baselines)**: High confidence - clear experimental methodology with established datasets and metrics
- **Ablation Study (Importance of Multimodal Fusion and VLM Feedback)**: Medium confidence - limited to single model, doesn't fully explore VLM contribution
- **RPSC Defense Effectiveness (Mitigating MPDA)**: Low confidence - only evaluated against specific MPDA attack, not diverse attack vectors

## Next Checks

1. **VLM Generalization Test**: Replicate MPDA attack on I2P dataset subset using different VLMs (LLaVA, GPT-4o with vision) to assess dependence on specific model's captioning accuracy
2. **Defense Breadth Evaluation**: Test RPSC defense against spectrum of prompt attacks including pure text-based methods, single-modality image-based attacks, and other prompt rewriting strategies
3. **Image Weight Parameter Analysis**: Conduct systematic sweep of image weight parameter (0.1 to 3.0) on Midjourney for different prompt categories to quantify impact on bypass rate and semantic consistency