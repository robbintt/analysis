---
ver: rpa2
title: 'ICU-TSB: A Benchmark for Temporal Patient Representation Learning for Unsupervised
  Stratification into Patient Cohorts'
arxiv_id: '2506.06192'
source_url: https://arxiv.org/abs/2506.06192
tags:
- patient
- learning
- clustering
- stratification
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICU-TSB, a novel benchmark for evaluating
  temporal patient representation learning in unsupervised stratification tasks using
  ICU EHR data. The core method involves comparing statistical and deep learning approaches,
  including LSTM and GRU models, to generate patient embeddings for clustering.
---

# ICU-TSB: A Benchmark for Temporal Patient Representation Learning for Unsupervised Stratification into Patient Cohorts

## Quick Facts
- arXiv ID: 2506.06192
- Source URL: https://arxiv.org/abs/2506.06192
- Reference count: 5
- Primary result: Temporal representation learning achieves v-measure scores of 0.46 at top ICD level and 0.40 at lowest level for unsupervised patient stratification

## Executive Summary
This paper introduces ICU-TSB, a benchmark for evaluating temporal patient representation learning in unsupervised stratification tasks using ICU EHR data. The benchmark compares statistical and deep learning approaches, including LSTM and GRU models, to generate patient embeddings for clustering. Results show that temporal representation learning can rediscover clinically meaningful patient cohorts, with majority vote label assignment proving most effective for cluster interpretation. The work addresses the challenge of unsupervised patient stratification in healthcare, providing a reproducible framework for future research in this area.

## Method Summary
The benchmark uses hierarchical ICD and CCS taxonomies to evaluate model performance across multiple disease classification levels. The core method involves comparing statistical and deep learning approaches to generate patient embeddings from temporal ICU data. Models are trained using autoregressive self-supervised objectives to predict next-timestep feature values. Embeddings are clustered using k-Means and evaluated against ICD labels using metrics like v-measure and AMI. The framework is validated across three ICU datasets (MIMIC-IV, eICU, and Sicilian ICU) with different feature sets and sample sizes.

## Key Results
- Temporal representation learning can rediscover clinically meaningful patient cohorts
- V-measure scores range from 0.46 at the top taxonomy level to 0.40 at the lowest level
- Majority vote label assignment strategy proved most effective for cluster interpretation
- Performance degrades at granular ICD levels (L2-L4) due to sample size limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Autoregressive sequence models trained to predict next-timestep feature values produce patient embeddings that capture disease-relevant patterns without explicit supervision.
- **Mechanism:** The model minimizes MSE loss on predicting feature values at t+1, forcing the hidden state to encode sufficient information about the clinical trajectory. This self-supervised objective creates representations that implicitly cluster by disease similarity because similar disease trajectories share predictive patterns.
- **Core assumption:** Clinical trajectories that share predictive dynamics also share underlying disease classifications.
- **Evidence anchors:**
  - [abstract] "Our results demonstrate that temporal representation learning can rediscover clinically meaningful patient cohorts"
  - [section: Methods] "We trained the two deep learning baselines using autoregressive unidirectional LSTM and GRU models... for the autoregressive task of inferring the feature values of the next time step"
  - [corpus] Weak direct evidence; neighbor paper "Serialized EHR make for good text representations" suggests sequential modeling benefits, but no direct validation of the autoregressive mechanism.
- **Break condition:** If disease categories do not produce distinct temporal patterns in monitored features, the predictive task will not yield discriminative embeddings.

### Mechanism 2
- **Claim:** Hierarchical clustering aligned with ICD taxonomy levels enables extrinsic evaluation of unsupervised patient stratification without ground-truth labels.
- **Mechanism:** The ICD hierarchy (L1→L4) provides clinically validated groupings at increasing granularity. By clustering patients and measuring alignment (v-measure, AMI) with each taxonomy level, the benchmark evaluates whether embeddings capture disease relationships that mirror medical knowledge.
- **Core assumption:** ICD classifications represent clinically meaningful disease groupings that should emerge from temporal clinical data.
- **Evidence anchors:**
  - [abstract] "v-measure scores ranging from 0.46 at the top taxonomy level to 0.40 at the lowest level"
  - [section: Results] "performance drops in L2→L4 and L3→L4, in the more granular ICD hierarchy level"
  - [corpus] Neighbor paper "UnPaSt" uses similar biclustering for patient stratification but focuses on omics data, not temporal EHR.
- **Break condition:** If ICD categories do not correspond to distinct temporal clinical signatures, alignment scores will remain low regardless of model quality.

### Mechanism 3
- **Claim:** Majority-vote label assignment produces more clinically interpretable cluster labels than centroid or medoid approaches.
- **Mechanism:** Majority vote selects the most frequent true label within a cluster, which naturally aligns with the dominant disease category. Centroid/medoid approaches may select atypical patients whose embeddings happen to be geometrically central but whose diagnoses are outliers.
- **Core assumption:** Clinically meaningful clusters should contain a dominant disease category that represents the majority of patients.
- **Evidence anchors:**
  - [abstract] "The majority vote label assignment strategy proved most effective for cluster interpretation"
  - [section: Results] "The majority vote outperformed both metrics across datasets, while the medoid performed consistently worse"
  - [corpus] No direct corpus evidence on label assignment strategies.
- **Break condition:** If disease subtypes are truly multimodal (multiple valid diagnoses per cluster), majority vote will oversimplify and miss clinically relevant heterogeneity.

## Foundational Learning

- **Concept: Recurrent Neural Networks (LSTM/GRU) for sequences**
  - Why needed here: The paper uses LSTM/GRU to encode variable-length ICU stays into fixed-dimensional patient representations via the final hidden state.
  - Quick check question: Can you explain why the hidden state at the final timestep represents the entire sequence?

- **Concept: Self-supervised learning via autoregression**
  - Why needed here: Models are trained to predict next-timestep values without requiring disease labels, enabling unsupervised stratification.
  - Quick check question: How does predicting the next observation force the model to learn useful representations?

- **Concept: External clustering evaluation metrics (V-measure, AMI)**
  - Why needed here: The benchmark evaluates clusters against ICD taxonomy using these metrics rather than internal measures like Silhouette Score.
  - Quick check question: Why would internal clustering metrics fail to capture clinical relevance?

## Architecture Onboarding

- **Component map:** Data preprocessing (ricu R package) -> Representation learning (STAT/LSTM/GRU) -> Dimensionality reduction (t-SNE) -> Clustering (k-Means) -> Label assignment (majority vote/centroid/medoid) -> Evaluation (v-measure, AMI)
- **Critical path:**
  1. Ensure hourly time granularity across all datasets (SiC uses minute-level, requires aggregation)
  2. Train autoregressive model on training split only (avoid leakage)
  3. Extract final hidden state as patient embedding
  4. Cluster with k matching ICD category count at target level
  5. Evaluate alignment with held-out ICD labels
- **Design tradeoffs:**
  - LSTM vs. GRU: Paper shows inconsistent differences; LSTM better on eICU, worse on SiC. Assumption: Dataset-specific feature count matters more than architecture choice.
  - STAT vs. neural: STAT requires no training but underperforms; neural captures non-linearities but requires regularization on smaller datasets.
  - t-SNE before clustering: Assumption helps visualization but may distort distances; paper optimizes parameters via Optuna.
- **Failure signatures:**
  - V-measure drops sharply from L1→L4: Sample sizes too small at granular levels; consider relaxing minimum cluster size.
  - GRU outperforms LSTM on small datasets (SiC): Possible overfitting; reduce model capacity.
  - Medoid label assignment underperforms: Geometric center may not represent clinical majority; switch to majority vote.
- **First 3 experiments:**
  1. **Reproduce L1 clustering on MIMIC-IV with LSTM**: Extract embeddings, cluster with k=25 (top ICD categories), report v-measure. Baseline target: ~0.46.
  2. **Ablate representation method**: Compare STAT vs. LSTM vs. GRU on eICU at L2. Hypothesis: LSTM should outperform STAT; magnitude of gap indicates non-linearity importance.
  3. **Test label assignment strategies**: On a single dataset/model combination, compare centroid/medoid/majority vote accuracy. Expect majority vote to achieve highest top-1 accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformer-based architectures or adaptive time-series models outperform the current LSTM and GRU baselines in capturing long-range dependencies and handling irregularly sampled ICU data?
- Basis in paper: [explicit] The authors state in the Discussion that "Future work could explore transformer-based architectures and adaptive time-series to capture long-range dependencies in EHR sequences."
- Why unresolved: The current benchmark only evaluated statistical methods, LSTMs, and GRUs, which may not optimally handle the irregular intervals and missing values inherent in ICU records.
- What evidence would resolve it: Benchmark results (V-measure, AMI) showing transformers significantly improving stratification performance on the same ICU datasets.

### Open Question 2
- Question: To what extent can contrastive learning or semi-supervised approaches mitigate the performance degradation observed at granular ICD hierarchy levels (L3–L4) caused by long-tailed label distributions?
- Basis in paper: [explicit] The Discussion notes performance drops at finer-grained levels likely due to sample sizes and suggests "Addressing this imbalance through data augmentation, contrastive learning, or semi-supervised approaches could improve stratification."
- Why unresolved: The current study focused on self-supervised autoregressive models, which struggled with the class imbalance and data sparsity of specific disease subcategories.
- What evidence would resolve it: Improved clustering accuracy or V-measure scores at the L3 and L4 hierarchy levels when employing contrastive or semi-supervised learning frameworks.

### Open Question 3
- Question: Does the integration of multimodal data sources, such as clinical notes and imaging, yield richer patient representations that outperform temporal EHR data alone in unsupervised stratification?
- Basis in paper: [explicit] The Conclusion lists "multimodal integration by fusing clinical notes, imaging, and genomics for richer patient representations" as a specific avenue for future work.
- Why unresolved: The ICU-TSB benchmark is currently limited to temporal features (108 variables) and static demographics, excluding unstructured or imaging data available in the source repositories.
- What evidence would resolve it: A comparative study showing multimodal embeddings achieve higher alignment with clinical taxonomies than the unimodal temporal baselines.

### Open Question 4
- Question: Do graph-based clustering metrics or clinical validation studies provide a more reliable evaluation of clinical relevance than intrinsic metrics like the Silhouette Score or AMI?
- Basis in paper: [explicit] The Discussion highlights that "Intrinsic clustering quality does not always correlate with clinical relevance" and suggests "Future research could investigate graph-based clustering metrics or clinical validation studies."
- Why unresolved: The paper relied on extrinsic metrics like AMI and V-measure against taxonomies, but the link between these mathematical scores and actual clinical utility remains an open challenge.
- What evidence would resolve it: A correlation analysis demonstrating that graph-based metrics predict expert-validated clinical utility better than standard clustering metrics.

## Limitations

- Sample sizes become critically small at granular ICD levels (L2-L4), with some datasets having only single-digit clusters at L4
- The 6-hour observation window before first ICD code assignment may introduce label noise and miss pre-hospitalization conditions
- The mechanism by which temporal patterns map to disease categories remains heuristic rather than theoretically grounded

## Confidence

- **High Confidence:** The benchmark framework design and evaluation methodology are sound and reproducible. The v-measure results across multiple datasets and taxonomy levels are statistically robust.
- **Medium Confidence:** The claim that majority vote label assignment is superior is supported by the data but lacks theoretical justification for why this should hold across different clustering scenarios.
- **Low Confidence:** The scalability of results to rare diseases or outpatient settings remains unproven, as does the generalizability beyond the three studied ICU datasets.

## Next Checks

1. Test the benchmark on MIMIC-III to validate reproducibility across similar but distinct datasets
2. Evaluate performance on non-ICU settings (e.g., outpatient EHR) to assess generalizability beyond critical care
3. Compare against supervised baselines to quantify the cost of true unsupervised learning