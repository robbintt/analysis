---
ver: rpa2
title: 'Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems:
  A Graph-RAG based Approach'
arxiv_id: '2501.18320'
source_url: https://arxiv.org/abs/2501.18320
tags:
- modeling
- knowledge
- sasp
- optimization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automated optimization modeling
  (AOM) for sensor array signal processing (SASP) problems, where traditional prompt-based
  techniques fail due to a lack of domain-specific knowledge. The proposed approach,
  MAG-RAG, combines a multi-agent (MA) structure with a graph-based retrieval-augmented
  generation (Graph-RAG) process to enhance modeling results.
---

# Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach

## Quick Facts
- **arXiv ID**: 2501.18320
- **Source URL**: https://arxiv.org/abs/2501.18320
- **Authors**: Tianpeng Pan; Wenqiang Pu; Licheng Zhao; Rui Zhou
- **Reference count**: 28
- **Key result**: MAG-RAG achieves superior completeness and correctness metrics for automated optimization modeling of 10 classical SASP problems compared to prompt-based AOM approaches

## Executive Summary
This paper addresses the challenge of automated optimization modeling (AOM) for sensor array signal processing (SASP) problems, where traditional prompt-based techniques fail due to a lack of domain-specific knowledge. The proposed approach, MAG-RAG, combines a multi-agent (MA) structure with a graph-based retrieval-augmented generation (Graph-RAG) process to enhance modeling results. The MA structure decomposes the problem-solving process into manageable sub-tasks, while Graph-RAG retrieves relevant domain-specific knowledge to improve the optimization modeling.

The experimental results on ten classical SASP problems demonstrate that MAG-RAG outperforms several AOM benchmarks, achieving better completeness and correctness metrics. The approach effectively integrates domain knowledge into the modeling process, leading to more accurate and relevant solutions. The system uses a four-layer knowledge graph (Problem Type → System Model → Optimization Formulation → Optimization Algorithm) to guide retrieval and generation, addressing the domain knowledge gap that limits traditional LLM-based AOM approaches.

## Method Summary
MAG-RAG employs a two-stage pipeline for automated optimization modeling of SASP problems. First, a four-layer knowledge graph is constructed using domain documents, with edges weighted by cosine similarity and containing knowledge nodes across problem types, system models, optimization formulations, and algorithms. Second, a three-agent system processes each problem: the Terminology Agent standardizes queries using SASP-specific terminology, Graph-RAG retrieves relevant knowledge (top-3 nodes from the Problem Type layer), and the Optimization Modeling Agent generates the final mathematical formulation. The approach uses text-embedding-3-small for embeddings and all-MiniLM-L6-v2 for keyword extraction, with human evaluation by three domain experts across five metrics (completeness, standardization, correctness, relevance, readability) totaling 100 points.

## Key Results
- MAG-RAG outperforms several AOM benchmarks on 10 classical SASP problems
- Achieves better completeness (30 points) and correctness (30 points) metrics
- Effective integration of domain knowledge leads to more accurate and relevant solutions
- Human evaluation by three domain experts confirms superior performance

## Why This Works (Mechanism)
MAG-RAG addresses the fundamental limitation of LLM-based AOM approaches: insufficient domain-specific knowledge. Traditional prompt-based methods fail on complex SASP problems because they lack access to the rich mathematical and algorithmic knowledge required for optimization modeling. By constructing a four-layer knowledge graph from domain documents and using Graph-RAG for retrieval, MAG-RAG provides the LLM with targeted, relevant domain knowledge that significantly improves the quality of generated optimization models. The multi-agent decomposition further enhances performance by specializing different aspects of the problem-solving process.

## Foundational Learning
- **Graph-RAG retrieval**: Knowledge graphs enable structured retrieval of domain-specific information, essential for complex optimization problems where context matters. Quick check: Verify cosine similarity scores for retrieved nodes are above threshold ε.
- **Multi-agent decomposition**: Breaking complex tasks into specialized sub-tasks (terminology standardization, retrieval, modeling) improves overall performance. Quick check: Compare agent outputs against baseline single-LLM approach.
- **Four-layer knowledge structure**: Organizing domain knowledge as Problem Type → System Model → Optimization Formulation → Optimization Algorithm creates effective retrieval paths. Quick check: Validate edge weights and node connectivity in constructed graph.
- **Human evaluation metrics**: Five-dimensional scoring (completeness, standardization, correctness, relevance, readability) provides comprehensive assessment of AOM quality. Quick check: Calculate inter-rater reliability between three domain experts.
- **Domain knowledge integration**: Retrieval-augmented generation bridges the gap between general LLMs and specialized problem domains. Quick check: Measure performance difference with and without Graph-RAG retrieval.

## Architecture Onboarding

**Component map**: Problem Description -> Terminology Agent -> Graph-RAG Retrieval -> Optimization Modeling Agent -> Optimization Model

**Critical path**: The optimization modeling quality depends on successful Graph-RAG retrieval, which in turn relies on accurate terminology standardization by the Terminology Agent. The knowledge graph construction quality directly impacts retrieval relevance.

**Design tradeoffs**: The system trades computational complexity (graph construction, multiple agent calls) for improved modeling accuracy. Using top-3 retrieval balances context richness against attention mechanism limitations.

**Failure signatures**: Poor retrieval quality manifests as irrelevant or incomplete optimization models. If readability scores drop significantly with retrieval enabled, knowledge overload may be degrading attention mechanism performance.

**First experiments**:
1. Reconstruct the four-layer knowledge graph using 5-10 SASP domain documents and test retrieval relevance on the 10 evaluation problems
2. Implement the three-agent pipeline and compare MAG-RAG outputs against Pure LLM baseline on beam pattern matching problem
3. Conduct human evaluation on 3 sample problems with all five metrics to validate scoring framework

## Open Questions the Paper Calls Out
None

## Limitations
- Knowledge base quality and coverage are critical but unspecified - exact number of domain documents and evaluation of knowledge base coverage not provided
- Human evaluation relies on subjective scoring without established inter-rater reliability or detailed scoring rubrics
- Effectiveness for problems outside the 10 classical SASP domains remains unverified
- Similarity threshold ε for DD edge creation is mentioned but not specified, significantly impacting retrieval quality

## Confidence
**High confidence**: The technical methodology of combining multi-agent decomposition with graph-based retrieval is sound and well-described.

**Medium confidence**: The reported experimental results are likely valid for the tested SASP problems, but limited problem set (10 cases) and absence of statistical significance testing reduce confidence in broader claims.

**Low confidence**: Claims about generalizability to arbitrary optimization problems and robustness of human evaluation scores due to lack of detailed validation procedures.

## Next Checks
1. **Knowledge Base Quality Validation**: Reconstruct the four-layer graph using 5-10 SASP domain documents and evaluate retrieval relevance for the 10 test problems, measuring cosine similarity distributions and retrieval accuracy.

2. **Statistical Significance Testing**: Conduct paired t-tests or Wilcoxon signed-rank tests comparing MAG-RAG against at least two baseline AOM methods across all 10 problems, reporting p-values for each of the five evaluation metrics.

3. **Inter-rater Reliability Assessment**: Have three independent domain experts score the same subset of modeling outputs and calculate Cohen's kappa or Fleiss' kappa to establish the reliability of the human evaluation framework.