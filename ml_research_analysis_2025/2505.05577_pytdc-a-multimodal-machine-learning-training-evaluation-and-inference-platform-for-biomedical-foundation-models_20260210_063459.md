---
ver: rpa2
title: 'PyTDC: A multimodal machine learning training, evaluation, and inference platform
  for biomedical foundation models'
arxiv_id: '2505.05577'
source_url: https://arxiv.org/abs/2505.05577
tags:
- data
- learning
- pytdc
- cell
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PyTDC addresses the lack of unified platforms for multimodal biomedical
  AI by introducing an API-first system that integrates heterogeneous, continuously
  updated single-cell and therapeutic data sources. It provides standardized benchmarking,
  model retrieval, and inference endpoints for context-aware foundation models.
---

# PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models

## Quick Facts
- arXiv ID: 2505.05577
- Source URL: https://arxiv.org/abs/2505.05577
- Reference count: 40
- Primary result: PyTDC introduces context-specific metrics and API-first multimodal data access for biomedical foundation models, enabling reproducible training and evaluation of therapeutic AI models

## Executive Summary
PyTDC addresses the lack of unified platforms for multimodal biomedical AI by introducing an API-first system that integrates heterogeneous, continuously updated single-cell and therapeutic data sources. It provides standardized benchmarking, model retrieval, and inference endpoints for context-aware foundation models. The platform enables reproducible training, evaluation, and fine-tuning of models across modalities like gene expression, perturbations, protein-peptide interactions, and clinical outcomes.

## Method Summary
PyTDC implements an API-first dataset architecture using model-view-controller design pattern to enable multimodal data views. The platform provides three main capabilities: unified access to distributed, heterogeneous, continuously updated data sources through an API-first approach; standardized benchmarking with domain-specific metrics for evaluation; and model inference via a server that loads foundation models and provides pretrained weights. The system supports tasks including single-cell drug-target nomination, perturbation outcome prediction, and T-cell receptor epitope prediction across multiple data modalities.

## Key Results
- PINNACLE context-aware geometric deep learning model achieves AP@5 Top-20 CT of 0.917 (RA) and 0.885 (IBD), outperforming GATv2 (0.334 RA) and Node2Vec (0.299 RA) on context-specific metrics
- Context-specific metrics reveal that domain-specific and state-of-the-art graph models underperform, while context-aware PINNACLE excels but lacks generalization to unseen cell types
- PyTDC's unified platform enables reproducible training, evaluation, and fine-tuning of models across heterogeneous biomedical data sources

## Why This Works (Mechanism)

### Mechanism 1
API-first architecture with MVC pattern enables unified access to heterogeneous, continuously-updated biomedical data sources without manual integration. The Model-View-Controller abstraction separates data logic (DataLoader), processing/transformation (View with data splits and processing functions), and control flow (Domain-Specific Language controller). This allows data views to be composed from multiple modalities—single-cell gene expression atlases, PPI networks, knowledge graphs—through declarative configurations rather than bespoke pipelines.

### Mechanism 2
Context-specific metrics expose model failures that aggregate metrics hide, particularly for therapeutic relevance where cell-type-specific performance matters. Rather than computing AUROC/AP across all samples, PyTDC computes metrics within biological "slices" (e.g., per-cell-type subgraphs), then aggregates over top-K performing contexts. This surfaces whether models achieve strong performance on therapeutically relevant contexts (e.g., disease-relevant cell types) versus averaging across irrelevant contexts.

### Mechanism 3
Pre-computed embeddings from foundation models (e.g., Geneformer, PINNACLE) provide transferable representations that reduce data requirements for downstream therapeutic tasks. Foundation models trained on large-scale single-cell atlases learn gene-gene co-expression patterns and network hierarchies. Fine-tuning these representations on limited therapeutic-task-specific data leverages pre-learned biological priors rather than learning from scratch.

## Foundational Learning

- **Graph Neural Networks and Attention Mechanisms**
  - Why needed here: The scDTN task operates on protein-protein interaction networks. Understanding message passing, attention-based neighborhood aggregation (GATv2), and context-free vs. context-aware representations is essential for interpreting benchmark results.
  - Quick check question: Can you explain why GATv2 with one-hot cell type encoding might underperform PINNACLE's context-aware PPI networks?

- **Transfer Learning and Foundation Models**
  - Why needed here: PyTDC's model server provides pretrained models (Geneformer, scGPT, PINNACLE) intended for fine-tuning. Understanding the pretrain-finetune paradigm and when to freeze vs. fine-tune layers is critical.
  - Quick check question: What criteria would determine whether to use frozen Geneformer embeddings vs. fine-tune the full model for a perturbation-response prediction task?

- **Single-Cell Transcriptomics Fundamentals**
  - Why needed here: Datasets include scRNA-seq atlases (CZI CellXGene), perturbation screens (scPerturb), and cell-type-specific networks. Familiarity with gene expression matrices, cell/feature annotations, batch effects, and normalization is assumed.
  - Quick check question: How would you interpret the difference between context-specific AUROC improving while aggregate AUROC remains flat?

## Architecture Onboarding

- **Component map:** `tdc_ml.resource` -> Data access layer (CellXGene Census, PrimeKG, protein-peptide datasets) -> `tdc_ml.benchmark_group` -> Evaluation modules (scDTNGroup, PerturbOutcomeGroup, TCREpitopeGroup) -> `tdc_ml.model_server` -> Foundation model loading and inference (HuggingFace hub integration, tokenizers) -> `tdc_ml.multi_pred` -> Task-specific data loaders with pre-built splits

- **Critical path:** 1. Identify task (e.g., `scdtn_group.SCDTNGroup`) 2. Retrieve data via benchmark group (`get_train_valid_split()`) 3. Load foundation model via `tdc_hf_interface` and appropriate tokenizer 4. Extract embeddings or fine-tune 5. Run predictions and evaluate via `group.evaluate(preds)`

- **Design tradeoffs:**
  - API-first vs. local data: API access enables automatic updates but requires network; large datasets (CellXGene Census) use TileDB-SOMA for memory-efficient streaming at the cost of initial query overhead
  - Pre-computed vs. on-demand embeddings: Pre-computed storage reduces inference time but limits flexibility; on-demand inference supports custom tokenization and fine-tuning
  - Context-specific vs. aggregate evaluation: Context-specific metrics are more therapeutically relevant but require sufficient samples per context; sparse contexts may yield high-variance estimates

- **Failure signatures:**
  - Tokenizer mismatch: Geneformer expects Ensembl IDs; using gene names without mapping causes embedding extraction failures
  - Cell type sparsity: Context-specific metrics return NaN or high variance when cell types have <10 samples
  - Cold split leakage: Using random splits instead of cold splits (by protein, perturbation, or cell line) inflates performance on benchmark leaderboards

- **First 3 experiments:**
  1. Baseline reproduction: Load PINNACLE embeddings for RA/IBD scDTN task, evaluate with provided splits, and verify Table 2 AP@5 Top-20 CT scores (~0.91 RA, ~0.88 IBD)
  2. Ablation on context count: Evaluate PINNACLE with Top-5 vs. Top-50 cell types to quantify sensitivity to context selection in AP@R metrics
  3. Foundation model comparison: Extract Geneformer vs. scGPT embeddings from the same scPerturb dataset and compare perturbation-response prediction MSE to establish which pretrained representation better captures drug-response biology

## Open Questions the Paper Calls Out

- How can context-aware models be improved to generalize predictions to unseen cell types in drug-target nomination? The authors state that while PINNACLE outperforms baselines, "the model is unable to generalize to unseen cell types."

- What architectures enable the effective integration of additional biological modalities into single-cell foundation models? The authors highlight that current models fail to "incorporate additional modalities," limiting their ability to capture the full complexity of biological systems.

- Can a unified model accurately predict transcriptional responses for both genetic and chemical perturbations? The paper notes in Appendix A.2 that current models are "limited to either genetic or chemical perturbations without being able to generalize to the other."

## Limitations

- PINNACLE's performance on diseases beyond RA/IBD remains untested, limiting claims about cross-disease generalization
- Baseline models (GATv2, Node2Vec) use unspecified architectures and training configurations, making exact reproduction challenging
- The assumption that cell-type-specific performance correlates with therapeutic relevance is not empirically validated across diverse therapeutic areas

## Confidence

- High confidence: API-first architecture with MVC pattern (Section 3.1) - directly specified and consistent with implementation
- Medium confidence: Context-specific metrics expose model failures (Section 4.2) - demonstrated on RA/IBD but not validated on other diseases
- Low confidence: Foundation model embeddings reduce data requirements (Section C.2) - theoretical justification provided but limited empirical validation across tasks

## Next Checks

1. Reproduce Table 2 AP@5 Top-20 CT scores for PINNACLE on RA/IBD scDTN task using provided splits and evaluate standard deviation across 10 runs
2. Test PINNACLE's performance on a third disease (e.g., asthma) to assess cross-disease generalization of context-specific metrics
3. Compare frozen Geneformer embeddings vs. fine-tuned model on a perturbation-response prediction task to quantify transfer learning benefits