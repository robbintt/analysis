---
ver: rpa2
title: 'Revisiting Knowledge Distillation: The Hidden Role of Dataset Size'
arxiv_id: '2510.15516'
source_url: https://arxiv.org/abs/2510.15516
tags:
- distillation
- teacher
- data
- training
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the role of dataset size in knowledge\
  \ distillation, revealing that distillation is most effective in low-data regimes.\
  \ The authors systematically vary the training dataset fraction \u03BA and demonstrate\
  \ that distillation provides substantial performance gains when \u03BA < 1, with\
  \ improvements of up to 10% in test accuracy for image classification and 25% for\
  \ language modeling tasks."
---

# Revisiting Knowledge Distillation: The Hidden Role of Dataset Size

## Quick Facts
- **arXiv ID**: 2510.15516
- **Source URL**: https://arxiv.org/abs/2510.15516
- **Reference count**: 33
- **Primary result**: Distillation provides up to 10% accuracy gains in image classification and 25% in language modeling when training data is scarce (κ < 1)

## Executive Summary
This paper systematically investigates how dataset size affects knowledge distillation (KD) performance, revealing that KD is most effective in low-data regimes. Through controlled experiments varying the training dataset fraction κ, the authors demonstrate that distillation provides substantial performance gains when κ < 1, with improvements far exceeding those reported in traditional self-distillation studies at κ = 1. The data efficiency of distillation is shown to be robust across architectures (CNNs and Transformers) and modalities (vision and language), challenging the conventional view that KD primarily serves model compression purposes.

## Method Summary
The study employs knowledge distillation with temperature scaling, where students are trained on varying fractions κ of the teacher's training data using a combination of KL divergence loss on softened teacher outputs and standard cross-entropy. The temperature τ ∈ [0.1, 1, 3, 5, 10, 20, 100] is tuned per dataset. Experiments systematically vary κ across vision (CIFAR10-5m, CIFAR100, ImageNet) and language (Languini Books) tasks, measuring Performance Increment PI = R(KD) - R(LT) between distilled and label-trained students. Feature kernel alignment is quantified using Centered Kernel Alignment (CKA), and various ablations test mechanisms including label smoothing and temperature effects.

## Key Results
- Distillation provides up to 10% accuracy gains in image classification and 25% in language modeling when κ < 1
- Feature kernel alignment between teacher and student is strongly correlated with performance gains, particularly in low-data regimes
- Label smoothing alone cannot explain distillation benefits, showing only constant PI across κ values while KD shows amplified PI at low κ
- Temperature tuning is crucial, with smoother teacher distributions (higher τ) significantly improving data efficiency

## Why This Works (Mechanism)

### Mechanism 1: Feature Kernel Alignment via Dark Knowledge Transfer
- **Claim**: Distillation transfers inter-class similarity structure encoded in teacher logits, inducing representational alignment between student and teacher feature spaces in low-data regimes
- **Core assumption**: The student can recover feature structure from output distributions alone, without direct access to teacher features
- **Evidence anchors**: Section 5.2 shows KD induces higher feature kernel alignment than label training, particularly in low-data regimes; Figure 5 demonstrates distilled students form compact clusters with significantly higher pairwise kernel alignment

### Mechanism 2: Variance Reduction in Low-Data Regimes
- **Claim**: Distillation acts as a variance-reducing regularizer when training data is scarce, compensating for increased bias from following an imperfect teacher
- **Core assumption**: The teacher's predictions encode a useful inductive bias that generalizes better than random initialization when data is limited
- **Evidence anchors**: Section B.2 provides theoretical framing showing variance dominates error in low-data regimes; Section 6.1 demonstrates performance gains diminish as κ → 1

### Mechanism 3: Soft Label Smoothness Enables Efficient Knowledge Transfer
- **Claim**: Higher temperatures produce smoother teacher distributions that spread probability mass across non-target classes, enabling more efficient gradient-based learning in data-scarce settings
- **Core assumption**: Non-target class probabilities contain "dark knowledge" about class relationships that one-hot labels discard
- **Evidence anchors**: Section 6.2, Figure 8 shows smoother labels obtained via higher temperatures significantly improve data efficiency; Table 2 demonstrates label smoothing yields ~constant PI across κ while KD shows amplified PI at low κ

## Foundational Learning

- **Concept: KL Divergence with Temperature Scaling**
  - **Why needed here**: The distillation objective uses KL divergence between softened teacher and student distributions; understanding how τ affects gradient magnitude and distribution smoothness is essential
  - **Quick check question**: What happens to the KL divergence gradient as τ → ∞? (Answer: Approaches squared error on softened distributions)

- **Concept: Centered Kernel Alignment (CKA)**
  - **Why needed here**: The paper uses CKA to measure feature kernel similarity between networks; this is the primary evidence for the dark knowledge mechanism
  - **Quick check question**: Why use CKA instead of direct neuron-wise comparison? (Answer: CKA is invariant to neuron permutations and orthogonal transformations)

- **Concept: Bias-Variance Decomposition in Supervised Learning**
  - **Why needed here**: Section B.2 provides theoretical framing for why distillation helps in low-data regimes—variance dominates error when data is scarce
  - **Quick check question**: Why does distillation performance plateau near teacher performance as κ → ∞? (Answer: Bias term converges to constant determined by teacher accuracy and optimization procedure)

## Architecture Onboarding

- **Component map**: Teacher network (pre-trained, frozen) -> Student network (trained with combined loss) -> Temperature τ (applied to logits) -> α parameter (interpolation weight)

- **Critical path**: 
  1. Train teacher on full dataset with standard cross-entropy
  2. For each κ value, train paired students (KD vs. label training) with identical hyperparameters
  3. Tune τ per dataset using grid search [0.1, 1, 3, 5, 10, 20, 100]
  4. Measure PI = R(KD) - R(LT) across κ values

- **Design tradeoffs**: 
  - Higher τ → smoother labels → better data efficiency but potential underfitting at κ=1
  - Larger student relative to teacher → lower κ* (crossover point) → distillation helps less
  - Data efficiency ≠ computational efficiency: larger teachers improve performance but increase FLOPs

- **Failure signatures**:
  - KD underperforms labels when κ >> 1 (student has more data than teacher was trained on)
  - Very low τ (< 1) reduces gains to near-zero (hard targets)
  - Transfer learning with large κ shows reversed gains

- **First 3 experiments**:
  1. **Baseline sweep**: Train teacher, then sweep κ ∈ [0.02, 0.1, 0.2, 0.4, 1.0] with fixed τ=20, comparing KD vs. label training PI on CIFAR10/100
  2. **Temperature ablation**: At κ=0.1, vary τ ∈ [1, 3, 10, 20, 100] to find optimal smoothness; verify soft labels outperform hard labels
  3. **Fidelity correlation**: Measure Top-1 agreement between student and teacher predictions at test time across κ values; confirm correlation with PI in low-data regime

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does Neural Collapse explain the feature kernel alignment observed during distillation?
- **Basis in paper**: Appendix B.3.4 suggests investigating if Neural Collapse helps explain the alignment phenomena
- **Why unresolved**: The paper observes alignment but lacks a mechanistic proof linking it to weight/feature geometry convergence
- **Evidence**: Analysis of feature/weight geometry (Simplex ETF) in the final layers of distilled students

### Open Question 2
- **Question**: Why does distillation induce feature alignment in vision but not in language modeling?
- **Basis in paper**: Section 5.2 notes alignment is high for vision but lower than baseline for language, calling for further research
- **Why unresolved**: The mechanisms for "dark knowledge" transfer appear task-dependent, and current theories do not account for this divergence
- **Evidence**: Comparative analysis of optimization dynamics and gradient noise in language models vs. CNNs under distillation

### Open Question 3
- **Question**: How does the teacher's generalization error relative to the student's capacity affect the distillation crossover point κ*?
- **Basis in paper**: Appendix B.1 suspects the difference between ViT and ResNet students is due to the teacher's lower test error, stating experiments are needed
- **Why unresolved**: The study focuses on dataset size, but the teacher's quality (test error) is identified as a potential confounding variable not fully isolated
- **Evidence**: Controlled experiments varying teacher test error while keeping capacity constant to observe changes in κ*

## Limitations

- **Unknown teacher quality**: The paper does not investigate how teacher accuracy relative to student capacity affects the distillation crossover point κ*
- **Synthetic data concerns**: Experiments on CIFAR10-5m use ~6M synthetic images, raising questions about generalizability to real-world data
- **Temperature tuning**: The temperature selection method is dataset-specific without a principled selection approach, potentially limiting practical applicability

## Confidence

- **High confidence**: Distillation provides substantial performance gains in low-data regimes (κ < 1), as evidenced by controlled experiments across multiple datasets and architectures with consistent PI patterns
- **Medium confidence**: Feature kernel alignment correlates with performance gains, though the causal relationship between alignment and accuracy is inferred rather than directly established through ablation studies
- **Low confidence**: The variance reduction mechanism is theoretically plausible but not empirically validated beyond the observed performance trends

## Next Checks

1. **Teacher quality ablation**: Train teachers to varying performance levels on the same dataset, then measure how teacher accuracy affects distillation gains across κ values to test the bias-variance hypothesis

2. **Cross-dataset teacher transfer**: Use a teacher trained on one dataset (e.g., CIFAR100) to distill students on a different dataset (e.g., CIFAR10) at multiple κ values to assess whether gains stem from dataset-specific knowledge or general representational priors

3. **Direct feature space comparison**: Instead of relying on CKA, directly measure feature similarity between teacher and student by extracting intermediate activations during distillation to validate the dark knowledge transfer mechanism