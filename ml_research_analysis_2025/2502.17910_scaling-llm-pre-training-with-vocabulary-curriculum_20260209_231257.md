---
ver: rpa2
title: Scaling LLM Pre-training with Vocabulary Curriculum
arxiv_id: '2502.17910'
source_url: https://arxiv.org/abs/2502.17910
tags:
- vocabulary
- curriculum
- tokens
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces vocabulary curriculum learning, a dynamic
  tokenization approach that alternates between entropy-guided vocabulary expansion
  and model optimization. Unlike static vocabularies used in modern language models,
  this method adapts the vocabulary size and structure during training based on the
  model's entropy patterns.
---

# Scaling LLM Pre-training with Vocabulary Curriculum

## Quick Facts
- arXiv ID: 2502.17910
- Source URL: https://arxiv.org/abs/2502.17910
- Authors: Fangyuan Yu
- Reference count: 21
- One-line primary result: Vocabulary curriculum learning achieves log-linear scaling gains, improving bits-per-character (BPC) by a steeper slope (-0.147 vs -0.109) compared to static vocabularies.

## Executive Summary
This paper introduces vocabulary curriculum learning, a dynamic tokenization approach that adapts vocabulary size and structure during LLM pre-training based on model entropy patterns. Unlike static vocabularies used in modern language models, this method alternates between entropy-guided vocabulary expansion and model optimization. Experiments with small-scale GPT models on the enwiki8 dataset show that this approach achieves lower bits-per-character (BPC) compared to fixed-vocabulary training, with log-linear scaling gains. The results demonstrate that longer tokens become increasingly predictable while shorter tokens remain harder to predict, forming a natural hierarchical structure.

## Method Summary
The method uses entropy-guided vocabulary expansion that alternates between model optimization and vocabulary growth. Starting with characters (92-token vocabulary), the model identifies sequences where next-token entropy is monotonically decreasing and below threshold ε (0.3). These sequences are merged into single tokens, with new embeddings initialized using the model's hidden state representations. The dataset is re-tokenized after each expansion, and training resumes. This process repeats through 5 iterations, growing from 92 to 18,276 tokens, creating a curriculum that first learns character-level dependencies before progressing to larger units.

## Key Results
- Vocabulary curriculum learning achieves log-linear scaling gains with BPC slope of -0.147 vs -0.109 for static approaches
- Longer tokens capture predictable content while shorter tokens focus on complex, harder-to-predict contexts
- The method demonstrates improved scaling efficiency compared to baseline training on enwiki8 dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entropy-guided vocabulary expansion improves compute efficiency by compressing predictable sequences into single tokens.
- **Mechanism:** The model identifies sequences where the next-token entropy is monotonically decreasing and below a threshold (ε). It then merges these sequences into single tokens, reducing sequence length for predictable data and allowing more capacity for high-entropy regions.
- **Core assumption:** High-entropy regions benefit disproportionately from increased model capacity compared to low-entropy regions.
- **Evidence anchors:** [abstract] "longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts." [section 3.1] "A sequence is considered mergeable if all tokens after the first position exhibit monotonically decreasing entropy below threshold ε."
- **Break condition:** If ε is too high, the model may merge distinct semantic units solely because they are statistically frequent.

### Mechanism 2
- **Claim:** Initializing new token embeddings with the final hidden state of the merged sequence preserves learned knowledge.
- **Mechanism:** When tokens are merged, the new embedding vector is initialized using the model's existing hidden state representation of that sequence, acting as a warm-start for the new vocabulary entry.
- **Core assumption:** The pre-expansion model's hidden states form a sufficiently good initialization for post-expansion embeddings.
- **Evidence anchors:** [section 3.1] Equation 1 defines expansion: "WE[vnew] = h(L)t... where h(L)t represents the final hidden state for the merged sequence."
- **Break condition:** If the model architecture changes significantly between steps, hidden state dimensions might mismatch new embedding requirements.

### Mechanism 3
- **Claim:** Curriculum learning creates a hierarchical difficulty allocation that improves scaling slopes.
- **Mechanism:** Starting with characters and iteratively growing the vocabulary, the model first learns character-level dependencies before "freezing" them into larger tokens, resulting in log-linear scaling improvement.
- **Core assumption:** Models learn more efficiently when exposed to fine-grained units first rather than initializing with large static vocabulary.
- **Evidence anchors:** [abstract] "approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size." [section 4.2] "curriculum learning demonstrating a steeper improvement curve compared to baseline training."
- **Break condition:** If re-tokenization overhead exceeds computational gains from improved scaling efficiency.

## Foundational Learning

- **Concept: Next-Token Entropy**
  - **Why needed here:** The core signal for vocabulary expansion is the entropy H(st|s1:t-1). Low entropy means the model is confident (predictable), while high entropy means uncertainty.
  - **Quick check question:** If a token sequence has an entropy of 0.01, is it a good or bad candidate for merging into a new single token?

- **Concept: Bits-Per-Character (BPC)**
  - **Why needed here:** BPC rather than perplexity measures performance across different vocabulary sizes since token count varies, making token-level perplexity an unfair comparison.
  - **Quick check question:** Why is BPC a better metric for comparing a model with 92-token vocabulary versus one with 18,000-token vocabulary?

- **Concept: Byte Pair Encoding (BPE)**
  - **Why needed here:** The paper contrasts its method with BPE, which merges based on frequency statistics. Understanding this difference helps appreciate the entropy-guided approach.
  - **Quick check question:** Does standard BPE consider the neural model's ability to predict the text when deciding to merge tokens?

## Architecture Onboarding

- **Component map:** GPT model -> Entropy calculator -> Trie-based tokenizer -> Embedding resizer -> Dataset re-tokenizer

- **Critical path:**
  1. Initialize: Train base model on characters (V0)
  2. Diagnose: Calculate entropy for all possible sequences
  3. Select: Filter sequences where H < ε
  4. Merge: Create new token IDs for these sequences
  5. Inject: Expand model embedding layers; copy hidden states to new embeddings
  6. Re-tokenize: Encode entire dataset with new vocabulary
  7. Resume: Continue training on newly tokenized dataset

- **Design tradeoffs:**
  - Threshold ε: Too low = slow vocabulary growth; Too high = aggressive merging
  - Merge Frequency: Every epoch is expensive; too rarely loses curriculum effect
  - Vocabulary Size Limit: Caps at 18k to avoid "vocabulary curse" for 10M param model

- **Failure signatures:**
  - BPC Regression: Poor embedding initialization spikes loss
  - Stalling: Model fails to lower entropy below ε for new sequences
  - Token Fragmentation: Overly aggressive merging loses semantic precision

- **First 3 experiments:**
  1. Sanity Check: Compare fixed BPE (5k vocab) vs. Vocabulary Curriculum starting from chars and growing to 5k on 1MB text
  2. Ablation on Initialization: Compare random vs. hidden-state initialization for new embeddings
  3. Entropy Threshold Sweep: Run with ε ∈ {0.1, 0.3, 0.5} and plot vocabulary growth rate vs. final BPC

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the observed log-linear scaling efficiency persist or improve when applied to significantly larger model architectures (>1B parameters)?
- **Basis in paper:** [explicit] Section 4.4 states "we suspect the scaling improvement might be better for bigger model size. We'll work on extending our experiments therein."
- **Why unresolved:** Current study restricted to small-scale GPT models (10M parameters), leaving interaction with large-scale compute budgets unverified.
- **What evidence would resolve it:** Running vocabulary curriculum training on models from 100M to 7B+ parameters and comparing BPC slope against baselines.

### Open Question 2
- **Question:** Can the entropy-guided vocabulary expansion strategy effectively transfer to non-text modalities like byte sequences representing images or audio?
- **Basis in paper:** [explicit] Section 4.4 suggests "potential application in other modality than text, for instance, in bGPT... where the scaling power of vocabulary curriculum could be leveraged."
- **Why unresolved:** Current validation relies solely on enwiki8 text dataset; unknown if "predictable content" heuristic holds for visual or acoustic byte patterns.
- **What evidence would resolve it:** Applying method to multimodal byte dataset via bGPT and measuring compression improvements.

### Open Question 3
- **Question:** Is the vocabulary curriculum approach robust across diverse linguistic domains with different structural properties, such as programming code or mathematical reasoning?
- **Basis in paper:** [explicit] Abstract notes authors "plan to extend our experiments to... diverse domains."
- **Why unresolved:** Current results from Wikipedia-style prose; code or math requires precise tokenization that might be disrupted by dynamic merging.
- **What evidence would resolve it:** Evaluating on code repositories or arithmetic datasets to check if dynamic merging degrades syntactic integrity.

## Limitations
- Scale Validation Gap: All experiments use small GPT models (10M parameters) on enwiki8, leaving log-linear scaling unverified at LLM scales
- Hyperparameter Sensitivity: Critical hyperparameters appear chosen without systematic ablation or sensitivity analysis
- Dataset Generalization: enwiki8 is unusually homogeneous; entropy patterns may not transfer to more diverse datasets

## Confidence

**High Confidence:** The core mechanism of entropy-guided vocabulary expansion is technically sound and experimental methodology is rigorous. The observation that BPC improves with dynamic vocabulary adaptation on enwiki8 is well-supported.

**Medium Confidence:** The claimed log-linear scaling improvement (-0.147 vs -0.109) is demonstrated on the specific experimental setup but requires validation at scale. Architectural choices appear reasonable but lack comparative justification.

**Low Confidence:** Claims about computational efficiency gains are not directly measured. The paper focuses on BPC improvement but doesn't report training time, memory usage, or FLOPs comparisons.

## Next Checks

1. **Scale-Up Validation:** Replicate curriculum learning with medium-scale GPT model (100M-1B parameters) on standard word-level dataset like WikiText-103 or C4. Compare vocabulary size vs. BPC log-slope against static BPE vocabularies and original small-scale results.

2. **Hyperparameter Sensitivity Study:** Systematically vary ε (try 0.1, 0.2, 0.4, 0.5), maximum merge counts (1K, 2K, 4K), and expansion frequency (every 10%, 25%, 50% of training) on enwiki8. Plot resulting BPC and vocabulary growth curves.

3. **Efficiency Benchmarking:** Measure wall-clock training time, peak GPU memory usage, and total FLOPs for both curriculum learning and compute-matched static vocabulary baselines across multiple runs. Include overhead of entropy calculation, re-tokenization, and embedding expansion.