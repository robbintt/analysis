---
ver: rpa2
title: An Empirical Comparison of Cost Functions in Inductive Logic Programming
arxiv_id: '2503.07554'
source_url: https://arxiv.org/abs/2503.07554
tags:
- cost
- functions
- hypothesis
- hypotheses
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale empirical study comparing
  seven standard cost functions for optimal inductive logic programming (ILP) hypotheses
  across 25 domains and over 1,000 tasks. The authors extend the POPPER system to
  support seven lexico-linear cost functions, including training error minimization,
  description length, and hypothesis size minimization.
---

# An Empirical Comparison of Cost Functions in Inductive Logic Programming

## Quick Facts
- arXiv ID: 2503.07554
- Source URL: https://arxiv.org/abs/2503.07554
- Reference count: 21
- This paper presents the first large-scale empirical study comparing seven standard cost functions for optimal inductive logic programming (ILP) hypotheses across 25 domains and over 1,000 tasks

## Executive Summary
This paper presents the first large-scale empirical study comparing seven standard cost functions for optimal inductive logic programming (ILP) hypotheses. The authors extend the POPPER system to support seven lexico-linear cost functions and evaluate them across 25 domains and over 1,000 tasks. Their results show that no single cost function consistently outperforms others, but minimizing training error or description length provides the best overall performance. The study provides practical guidance for selecting cost functions based on dataset properties and reveals that minimizing hypothesis size does not always improve generalization.

## Method Summary
The authors extended the POPPER ILP system to support seven lexico-linear cost functions: training error minimization, description length (MDL), hypothesis size minimization, and four variations combining these metrics. They evaluated these cost functions across 25 benchmark domains from the ILP literature, conducting over 1,000 experiments. The evaluation measured predictive accuracy on test sets and examined how cost function performance varied with domain characteristics and training data volume. Statistical significance testing was used to compare cost function performance across domains.

## Key Results
- No single cost function consistently outperformed others across all domains
- Minimizing training error or description length provided the best overall performance
- MDL performed poorly with few training examples but excelled with abundant data
- Minimizing hypothesis size did not always improve generalization

## Why This Works (Mechanism)
The empirical comparison reveals that cost function performance depends on domain characteristics and data availability. Different domains have different inductive biases that interact with specific cost functions. The study shows that the relationship between hypothesis complexity and generalization is not monotonic - simpler hypotheses do not always generalize better. MDL's performance variation with training data volume demonstrates how different cost functions implicitly trade off model complexity and data fit differently.

## Foundational Learning

**Inductive Logic Programming**: A form of machine learning that learns logical rules from examples and background knowledge. Why needed: ILP is the target domain where cost function performance is being evaluated. Quick check: Can you explain the difference between ILP and traditional symbolic learning?

**Lexico-linear cost functions**: Cost functions that can be expressed as linear combinations of multiple metrics with lexicographic ordering. Why needed: The study focuses on this tractable class of cost functions that can be efficiently optimized by ILP systems. Quick check: Can you describe how lexico-linear functions differ from arbitrary non-linear functions?

**Minimum Description Length (MDL)**: A principle that favors hypotheses that minimize the total description length of both the model and the data encoded using the model. Why needed: MDL is one of the key cost functions evaluated and shows interesting behavior with varying training data. Quick check: Can you explain how MDL balances model complexity against data fit?

## Architecture Onboarding

**Component map**: POPPER system -> Cost function module -> Hypothesis search space -> Evaluation on training/test sets -> Performance comparison across domains

**Critical path**: (1) Load domain and data, (2) Apply cost function to guide hypothesis search, (3) Generate hypothesis, (4) Evaluate on test set, (5) Compare performance across cost functions

**Design tradeoffs**: The choice of lexico-linear cost functions enables efficient optimization but may miss performance gains from non-linear alternatives. The focus on established ILP benchmarks ensures comparability but may limit generalizability to novel domains.

**Failure signatures**: Poor generalization despite low training error suggests overfitting; MDL underperformance with limited data indicates insufficient model capacity; hypothesis size minimization failing to improve performance suggests the bias toward simplicity is inappropriate for certain domains.

**First experiments**: (1) Run a single domain with all seven cost functions to verify implementation, (2) Test performance sensitivity to training data volume for MDL, (3) Compare hypothesis size distributions across cost functions on a simple domain

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- The study focuses exclusively on lexico-linear cost functions, potentially missing performance gains from non-linear alternatives
- Domain selection from ILP literature may introduce bias toward problems already amenable to ILP approaches
- Domain-specific recommendations rely on coarse categorizations of domain characteristics without investigating granular feature interactions

## Confidence
- **High confidence**: The empirical methodology is sound, the implementation of multiple cost functions in POPPER is technically competent, and the core finding that cost function performance varies by domain is well-supported
- **Medium confidence**: The domain-specific recommendations and the relationship between training data volume and MDL performance are supported but could benefit from additional validation across broader problem classes
- **Low confidence**: The generalizability of the hypothesis size minimization findings to other ILP systems and the potential impact of non-lexico-linear cost functions remain uncertain

## Next Checks
1. Replicate the study using a different ILP system (e.g., Aleph or TILDE) to verify that the cost function performance patterns are not implementation-dependent
2. Test the cost functions on non-traditional ILP domains, such as those with high attribute counts or from real-world industrial applications, to assess generalizability
3. Conduct a follow-up study with non-lexico-linear cost functions, including combinations of existing metrics or domain-specific weighting schemes, to explore whether better overall performance can be achieved