---
ver: rpa2
title: 'STEB: In Search of the Best Evaluation Approach for Synthetic Time Series'
arxiv_id: '2505.21160'
source_url: https://arxiv.org/abs/2505.21160
tags:
- data
- measures
- synthetic
- time
- measure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STEB is the first benchmark framework for automated, large-scale
  evaluation of synthetic time series measures. It uses controlled data transformations
  with intensity modulation to create pseudo-synthetic datasets and tests 41 measures
  across 10 diverse datasets, assessing reliability, consistency, and running time.
---

# STEB: In Search of the Best Evaluation Approach for Synthetic Time Series

## Quick Facts
- **arXiv ID:** 2505.21160
- **Source URL:** https://arxiv.org/abs/2505.21160
- **Reference count:** 40
- **Primary result:** First benchmark framework for automated, large-scale evaluation of synthetic time series measures

## Executive Summary
STEB introduces a benchmark framework for evaluating synthetic time series generation methods by applying controlled data transformations with intensity modulation. The framework tests 41 different quality measures across 10 diverse datasets, assessing their reliability, consistency, and computational efficiency. Results reveal significant variability in measure performance, with embedder choice often having a greater impact on scores than the measures themselves.

## Method Summary
STEB uses controlled data transformations (noise addition, mode collapse, misalignment, etc.) with intensity modulation to create pseudo-synthetic datasets from real data. For each transformation, the framework systematically increases corruption intensity and tracks how evaluation measures respond. Measures are categorized into Fidelity, Generalization, Privacy, and Representativeness, with specific expected behaviors defined for each transformation-category pair. The framework computes reliability indicators based on monotonicity of score degradation and consistency across random seeds and datasets.

## Key Results
- α-precision excels in fidelity assessment, while ACS performs best for generalization testing
- Autocorrelation is most reliable for privacy evaluation, and Context-FID for representativeness
- All measures show high reliability variability, with scores often fluctuating non-monotonically
- Embedder choice significantly impacts scores, with MAPE differences often exceeding 1000% between TS2Vec and Catch22
- Running times range from near-instant for simple correlations to minutes for complex deep-learning-based metrics

## Why This Works (Mechanism)

### Mechanism 1: Controlled Distribution Modulation
The framework creates a proxy for ground truth by systematically degrading real data to test if measures react as expected. By applying transformations with increasing intensity and tracking score degradation, STEB assumes valid measures must respond monotonically to gradual data corruption.

### Mechanism 2: Category-Specific Expectation Mapping
STEB decouples "quality" into four distinct categories and validates measures against specific expected behaviors for each transformation. The system defines how measures should react (Improve, Worsen, Constant) based on the quality aspect being tested.

### Mechanism 3: Embedder Sensitivity Analysis
The choice of upstream time series embedding introduces massive variance, often overwhelming the actual signal of quality measures. STEB systematically swaps embedding components while keeping measures constant to quantify this impact through MAPE calculations.

## Foundational Learning

- **Time Series Embeddings (TS2Vec vs. Catch22)**
  - Why needed: Many measures operate on vector representations, not raw data. Understanding DL-based vs. feature-based embeddings is critical for diagnosing score variability.
  - Quick check: If a measure returns a poor score, is it the generator's fault or the embedding model's inability to capture relevant features?

- **Synthetic Data Quality Dimensions**
  - Why needed: "Quality" is ambiguous. STEB enforces a taxonomy (Fidelity, Generalization, Privacy, Representativeness) that users must understand.
  - Quick check: Does a measure that detects "Mode Collapse" check for Fidelity or Generalization? (Answer: Representativeness/Generalization).

- **Monotonicity and Reliability Indicators**
  - Why needed: The core logic tests if scores degrade monotonically with transformation intensity. Understanding rank correlation is key to interpreting reliability scores.
  - Quick check: If a score fluctuates up and down as noise intensity increases, is the reliability indicator high or low? (Answer: Low).

## Architecture Onboarding

- **Component map:** Data Ingestion → Embedding (expensive) → Transformation Loop (κ steps) → Measure Execution → Reliability Aggregation
- **Critical path:** Data Ingestion → Embedding (TS2Vec/Catch22) → Transformation Loop → Measure Execution → Reliability Aggregation
- **Design tradeoffs:** Pseudo-synthetic vs. Real Generators (controlled vs. realistic); Memory vs. Speed (caching enabled for runtime reduction)
- **Failure signatures:** CUDA OOM during DOMIAS/deep embedder steps; Time Limit Exceeded for Context-FID/Discriminative Score; High StD in reliability indicating unpredictability
- **First 3 experiments:**
  1. Run miMAE example measure against Gaussian Noise transformation to verify reliability ≈ 1.0
  2. Run Context-FID on Sine dataset with TS2Vec vs. Concat to confirm MAPE differences
  3. Run 5 top measures (α-precision, ACS) on Appliances energy dataset to verify ranking and runtime logging

## Open Questions the Paper Calls Out

- **Standardizing embeddings:** How to develop a standardized time series embedding model to minimize >1000% MAPE variability between different embedders?
- **Dataset size sensitivity:** To what extent do reliability indicators depend on absolute sizes of real and synthetic datasets?
- **Extending evaluation categories:** How to evaluate quality aspects beyond current four, specifically fairness and separation of diversity from utility?

## Limitations

- Reliance on pseudo-synthetic data limits realism compared to generator-based synthetic data
- 41 measures may not represent full spectrum of possible evaluation approaches
- Category-specific expectations may not generalize to real-world synthetic data scenarios

## Confidence

- **High Confidence:** Reliability assessment methodology using monotonicity along transformation paths is well-grounded and reproducible
- **Medium Confidence:** Embedder sensitivity findings are robust but numerical values depend on exact implementation
- **Medium Confidence:** Running time measurements are reliable for tested hardware but may not generalize
- **Low Confidence:** Generalizability of category-specific expectations to real-world synthetic data remains uncertain

## Next Checks

1. Apply top-performing measures (α-precision, ACS, autocorrelation, Context-FID) to data from actual time series generators (GANs, VAEs) to test real-world applicability
2. Test whether reliability rankings hold when applying measures across different application domains (financial vs. medical vs. sensor data)
3. Systematically evaluate whether ensemble embeddings or domain-specific embeddings can mitigate >1000% MAPE issues observed between TS2Vec and Catch22