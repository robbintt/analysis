---
ver: rpa2
title: Empowering Small Language Models with Factual Hallucination-Aware Reasoning
  for Financial Classification
arxiv_id: '2601.01378'
source_url: https://arxiv.org/abs/2601.01378
tags:
- reasoning
- slms
- factual
- language
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether mitigating factual hallucinations
  in small language models (SLMs) improves financial classification performance. A
  three-step pipeline (AAAI: Association Identification, Automated Detection, and
  Adaptive Inference) is proposed to detect and correct reasoning errors.'
---

# Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification

## Quick Facts
- arXiv ID: 2601.01378
- Source URL: https://arxiv.org/abs/2601.01378
- Authors: Han Yuan; Yilin Wu; Li Zhang; Zheng Ma
- Reference count: 2
- Primary result: AAAI pipeline improves SLM financial classification by detecting and correcting factual hallucinations in reasoning

## Executive Summary
This study demonstrates that factual hallucinations in small language model (SLM) reasoning are positively correlated with classification errors in financial decision-making. The authors propose a three-step AAAI pipeline (Association Identification, Automated Detection, and Adaptive Inference) that detects hallucinated reasoning steps and uses feedback to trigger SLM reasoning refinement. Experiments on three SLMs (Llama-3.2-3B, Gemma-2-2B, Phi-3.5-3.8B) show that encoder-based verifiers can discriminate hallucinated from factual reasoning with high accuracy, and that oracle feedback consistently improves classification performance while verifier and self-reflection feedback show mixed results depending on model steerability.

## Method Summary
The method employs a three-step AAAI pipeline for financial classification using SLMs. First, SLMs generate structured outputs with decision and reasoning. Second, encoder-based verifiers (DeBERTa, RoBERTa, BART) fine-tuned on annotated reasoning-error pairs predict hallucination probabilities for each reasoning segment. Third, adaptive inference uses feedback (oracle, verifier, or self-reflection) to trigger reasoning regeneration. The pipeline was tested on the German Credit Dataset using three SLMs, with performance measured by F1 score and weighted cost (5×FN + 1×FP). Verifier evaluation used AUPRC and balanced accuracy metrics.

## Key Results
- Factual hallucinations show positive correlation with misclassifications (Pearson correlation 0.0447–0.1990)
- Encoder-based verifiers achieve high AUPRC (74.91–100.00) and balanced accuracy (72.66–96.15) in detecting hallucinations
- Oracle feedback improves F1 scores (up to 80.67) and reduces weighted costs (down to 31)
- Verifier and self-reflection feedback effectiveness varies significantly by model (Gemma benefits, Phi shows no improvement)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factual hallucinations in reasoning are positively associated with classification errors
- Mechanism: SLMs that generate factually incorrect reasoning steps (contradicting provided context) are more likely to produce incorrect final classifications, suggesting reasoning quality constrains decision quality
- Core assumption: The correlation is causal-directional rather than both being symptoms of a third factor (e.g., general model confusion)
- Evidence anchors:
  - [abstract] "factual hallucinations are positively correlated with misclassifications (Pearson correlation 0.0447–0.1990)"
  - [section] Table 1 shows positive risk differences (1.35–8.75) indicating higher misclassification risk when hallucinations are present
  - [corpus] Weak direct support; neighbor papers focus on retrieval/external knowledge, not self-contained reasoning factuality
- Break condition: If the relationship is merely correlational, intervening on hallucinations may not improve classification

### Mechanism 2
- Claim: Fine-tuned encoder-based verifiers can discriminate factual hallucinations in SLM reasoning
- Mechanism: Transformer encoders (DeBERTa, RoBERTa, BART) fine-tuned on annotated reasoning-error pairs learn to assign higher probabilities to hallucinated reasoning segments, enabling automated detection without ground-truth labels
- Core assumption: Patterns distinguishing hallucinated from factual reasoning are learnable and generalize within-task
- Evidence anchors:
  - [abstract] "encoder-based verifiers achieve high AUPRC (74.91–100.00) and balanced accuracy (72.66–96.15)"
  - [section] Figure 2 shows Wilcoxon p-values <0.01 for most verifier-model combinations, confirming discriminative ability
  - [corpus] MedTrust-RAG paper similarly uses verification mechanisms for biomedical QA, suggesting cross-domain plausibility
- Break condition: If annotation quality is poor or sample size too small (paper notes N=100), verifiers may overfit or fail to generalize

### Mechanism 3
- Claim: Feedback on factual errors can trigger SLM reasoning refinement, but effectiveness depends on feedback quality and model steerability
- Mechanism: When SLMs receive specific error feedback, they regenerate reasoning and decisions; oracle (human-annotated) feedback consistently helps, but verifier/self-reflection feedback quality varies—some SLMs (Gemma) benefit, others (Phi) resist steering
- Core assumption: SLMs can "notice" and correct reasoning when explicitly told about errors, rather than just generating plausible-looking refinements
- Evidence anchors:
  - [abstract] "oracle feedback improves F1 scores (up to 80.67) and reduces weighted costs (down to 31), while verifier and self-reflection feedback yield mixed results"
  - [section] Table 3 shows Phi unchanged across all feedback types; Gemma sometimes improves more with verifier than oracle
  - [corpus] No direct corpus support for this specific adaptive inference pattern in financial domains
- Break condition: If SLMs cannot genuinely self-correct (as Huang et al. 2024 suggests), gains may stem from prompt effects rather than true reasoning repair

## Foundational Learning

- Concept: Factual hallucination vs. logical error distinction
  - Why needed here: The pipeline targets factuality (reasoning contradicting provided context), not logical soundness—conflating these leads to wrong intervention design
  - Quick check question: Can you distinguish "stating a customer has income of X when the data shows Y" from "drawing an incorrect conclusion from correct premises"?

- Concept: Process verification vs. outcome verification
  - Why needed here: Real-world deployment cannot assume ground-truth labels exist; detecting errors in reasoning steps is practical only when process-level signals are available
  - Quick check question: If you only know the final classification was wrong, can you identify which reasoning step failed?

- Concept: Model steerability
  - Why needed here: Different SLMs respond differently to identical feedback; Phi shows near-zero steerability while Llama and Gemma are more responsive
  - Quick check question: If you give a model corrective feedback and its output doesn't change, is that a bug or a feature?

## Architecture Onboarding

- Component map: SLM backbone -> Verifier -> Feedback loop -> Adaptive inference
- Critical path:
  1. Generate initial response with structured reasoning output
  2. Segment reasoning into discrete points for verification
  3. Run verifier to flag hallucinated segments (threshold ≥0.5)
  4. Construct feedback prompt with specific error citations
  5. Regenerate with corrective instruction

- Design tradeoffs:
  - Single-point vs. entire-content granularity: Single-point provides stronger instructional signal for steerable models; entire-content may work better for low-steerability models like Phi
  - Verifier vs. self-reflection: Verifiers require annotation effort but are more reliable; self-reflection needs no external resources but often fails
  - Multiple rounds: Paper shows performance fluctuates across rounds—no clear benefit from additional iterations

- Failure signatures:
  - Phi-style low steerability: No output change regardless of feedback quality
  - Over-correction: Models that were correct initially may degrade after feedback
  - Verifier false positives: Incorrect feedback may mislead otherwise correct reasoning
  - Round-to-round oscillation: Performance fluctuates rather than monotonically improving

- First 3 experiments:
  1. Establish baseline: Run your target SLM on financial classification with reasoning output; manually annotate ~50 samples for hallucinations and compute correlation with errors
  2. Train verifiers: Fine-tune encoder models on annotated reasoning-error pairs using 3-fold cross-validation; verify discriminability with probability density plots and Wilcoxon tests
  3. Test adaptive inference with oracle feedback: Use human-annotated errors as feedback; compare F1 and weighted cost before/after to establish upper bound on improvement potential

## Open Questions the Paper Calls Out

- Question: Does the AAAI pipeline generalize to diverse financial tasks beyond the German Credit dataset and to a wider range of SLM architectures?
  - Basis in paper: [explicit] "To validate the generalizability of our findings, future work should include more tasks and SLMs."
  - Why unresolved: The study was constrained by annotation budgets to 100 cases from a single public dataset and three specific SLMs (Llama, Gemma, Phi).
  - Evidence would resolve it: Replication of the correlation and classification improvement results across diverse financial benchmarks (e.g., forecasting, fraud detection) and other model families.

- Question: Can providing few-shot examples improve the capability of SLMs to detect factual errors during self-reflection?
  - Basis in paper: [explicit] "The current self-reflection feedback relies on SLMs' zero-shot capability, while providing sufficient few-shot examples may improve their capability."
  - Why unresolved: Experiments showed mixed results for zero-shot self-reflection; the potential for in-context learning to enhance this specific feedback loop remains untested.
  - Evidence would resolve it: Ablation studies comparing the accuracy of zero-shot versus few-shot self-reflection feedback and their subsequent impact on classification F1 scores.

- Question: Does adaptive inference based on factuality feedback improve the logical quality of the reasoning path itself?
  - Basis in paper: [explicit] "Although adaptive inference enhances classification, we do not evaluate its impact on reasoning."
  - Why unresolved: The evaluation focused solely on the final classification label (F1 score) and weighted cost, ignoring whether the regenerated reasoning text became factually accurate.
  - Evidence would resolve it: Human or automated evaluation of the reasoning text generated during the adaptive inference step to verify if factual errors were actually corrected.

## Limitations
- The positive correlation between factual hallucinations and misclassifications is statistically present but weak (0.0447–0.1990), raising questions about causal mechanisms
- Verifier performance metrics are impressive but rely on small annotation sets (n=100), creating high variance risk and potential overfitting concerns
- Mixed feedback effectiveness across SLMs suggests fundamental steerability differences that may limit generalizability
- The study focuses on factual hallucinations only, not logical reasoning errors, potentially missing important error modes in financial decision-making

## Confidence
- **High confidence**: Encoder-based verifiers can detect factual hallucinations in SLM reasoning
- **Medium confidence**: Factual hallucinations are positively correlated with classification errors
- **Medium confidence**: Oracle feedback improves performance
- **Low confidence**: Verifier and self-reflection feedback provide consistent benefits

## Next Checks
1. **Replication with larger annotation sets**: Expand the reasoning point annotations from n=100 to n=500+ to test whether verifier performance remains stable and whether the hallucination-error correlation strengthens with more data
2. **Steerability intervention comparison**: Test whether different prompt engineering approaches (chain-of-thought vs direct instruction) improve low-steerability models like Phi, or whether steerability is fundamentally model-dependent
3. **Logical error detection extension**: Adapt the AAAI pipeline to detect and correct logical reasoning errors (not just factual hallucinations) by modifying verifier training to focus on reasoning structure rather than factuality against context