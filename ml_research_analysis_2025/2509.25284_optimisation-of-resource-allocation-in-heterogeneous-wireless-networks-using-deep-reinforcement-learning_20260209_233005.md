---
ver: rpa2
title: Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using
  Deep Reinforcement Learning
arxiv_id: '2509.25284'
source_url: https://arxiv.org/abs/2509.25284
tags:
- policy
- power
- learning
- allocation
- resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses dynamic resource allocation in heterogeneous
  wireless networks, focusing on jointly optimizing transmit power, bandwidth, and
  scheduling under varying user loads and channel conditions. A deep reinforcement
  learning framework is proposed to balance throughput, energy efficiency, and fairness
  through a multi-objective reward function.
---

# Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2509.25284
- **Source URL:** https://arxiv.org/abs/2509.25284
- **Reference count:** 40
- **Primary result:** Deep reinforcement learning (PPO and TD3) significantly outperforms heuristic methods for resource allocation in heterogeneous wireless networks, with PPO achieving superior fairness and TD3 faster initial convergence.

## Executive Summary
This study addresses the challenge of dynamic resource allocation in heterogeneous wireless networks by proposing a deep reinforcement learning framework that jointly optimizes transmit power, bandwidth, and scheduling. The framework uses a multi-objective reward function balancing throughput, energy efficiency, and fairness, implemented through Twin Delayed Deep Deterministic Policy Gradient (TD3) and Proximal Policy Optimization (PPO) algorithms. Both algorithms are evaluated against three heuristic baselines across multiple network scenarios using real base station locations, demonstrating significant performance improvements in optimizing resource allocation.

## Method Summary
The research formulates resource allocation as a Markov Decision Process, where a centralized agent learns to allocate power, bandwidth, and scheduling scores based on network state information. The state includes power levels, interference, association matrices, and user locations. Actions are normalized to stabilize training, then scaled to physical units. The reward function combines throughput, power consumption, and Jain's fairness index through weighted coefficients. Two DRL algorithms (TD3 and PPO) are trained for 1 million steps across 10 random seeds and compared against greedy, interference-pricing, and proportional-fair heuristics in dense, sparse, and hotspot network scenarios.

## Key Results
- Both PPO and TD3 significantly outperform heuristic baselines in optimizing resource allocation metrics
- PPO achieves superior overall performance with higher rewards and better fairness across all scenarios
- TD3 exhibits faster initial convergence but PPO shows more stable long-term learning trajectories
- The multi-objective reward function effectively balances competing network goals when properly weighted

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A linearly weighted multi-objective reward function may enable a single agent to balance competing network goals (throughput, energy, fairness) more effectively than single-objective heuristics.
- **Mechanism:** The reward $r_t$ aggregates throughput ($T_U$), power consumption ($P_{BS}$), and Jain's fairness index into a single scalar signal. By tuning coefficients ($\kappa, \beta, \phi$), the gradient descent process is forced to find a policy that maximizes a specific trade-off, rather than optimizing one metric at the expense of others.
- **Core assumption:** The selected weights ($\kappa, \beta, \phi$) accurately reflect the desired operational priorities, and the linear combination does not obscure the gradient signal for any individual objective.
- **Evidence anchors:**
  - [abstract] "...multi-objective reward balancing throughput, energy efficiency, and fairness."
  - [section II] Equation (9) and (10); Table II showing weights $\kappa=1.0, \beta=0.01, \phi=0.96$.
  - [corpus] "ReaCritic" (arXiv:2505.10992) notes that diverse user requirements and time-varying conditions introduce decision complexity, validating the need for multi-objective handling.

### Mechanism 2
- **Claim:** Normalizing action outputs to a bounded range (e.g., $[0, 1]$) likely stabilizes neural network training by decoupling policy outputs from physical unit scales.
- **Mechanism:** The actor network outputs a normalized value $a_p \in [0, 1]$, which is then linearly scaled to physical power $P_{BS} = P_{min} + a_p \cdot (P_{max} - P_{min})$. This prevents exploding gradients caused by large physical values directly influencing network weights and allows the same architecture to adapt to different hardware capabilities via affine transformation.
- **Core assumption:** The relationship between the normalized action and the physical resource is linear and deterministic.
- **Evidence anchors:**
  - [section II] "To facilitate a stable learning process, these action components are normalised... preventing exploding gradients."
  - [section II] Equation (4) defining the affine transformation.

### Mechanism 3
- **Claim:** PPO's clipped surrogate objective appears to yield higher asymptotic performance and fairness compared to TD3's off-policy approach, potentially due to more robust exploration.
- **Mechanism:** PPO constrains policy updates using a clipping coefficient $\epsilon$, preventing destructive large updates. While TD3 (off-policy) converges faster initially via replay buffers, PPO (on-policy) appears to explore the state space more thoroughly over time, resulting in superior long-term fairness and reward in this specific HetNet scenario.
- **Core assumption:** The "superior performance" is conditional on the specific network topology and reward weights defined; it may not generalize to all continuous control tasks.
- **Evidence anchors:**
  - [abstract] "PPO achieves superior overall performance with higher rewards and better fairness, while TD3 exhibits faster initial convergence."
  - [section V] "PPO shows a slower but more stable and superior learning trajectory... conservative and stochastic policy updates encourage exploration."

## Foundational Learning

- **Concept:** **Markov Decision Process (MDP) Formulation**
  - **Why needed here:** The paper models the wireless network as an MDP ($S, A, P, R, \gamma$). You must understand how to map continuous wireless states (interference, location) and continuous actions (power, bandwidth) into this tuple to implement the solver.
  - **Quick check question:** Does the state $s_t$ defined in Equation (2) include the history of previous actions, or does it rely solely on current observables to satisfy the Markov property?

- **Concept:** **Jain's Fairness Index**
  - **Why needed here:** This is the specific metric used in the reward function ($\phi \cdot \text{Fairness}_t$) to penalize inequality. Understanding its quadratic nature ($(\sum z_U)^2 / N \sum z_U^2$) is necessary to predict how the reward changes as user allocation variances increase.
  - **Quick check question:** If all users receive identical resources, what is the value of the index? (Answer: 1).

- **Concept:** **Actor-Critic Architecture**
  - **Why needed here:** Both TD3 and PPO are Actor-Critic methods. You need to distinguish the **Actor** (determining the policy/action distribution) from the **Critic** (estimating the value function/Q-value) to debug divergence issues.
  - **Quick check question:** In TD3, why are there two Critic networks but only one Actor policy?

## Architecture Onboarding

- **Component map:**
  - **Environment:** Custom simulator using real Cape Town BS coordinates.
    - **State Generator:** Concatenates Power ($\vec{p}$), Interference ($\vec{I}$), Association Matrix ($A$), and Coordinates.
    - **Channel Model:** Computes SINR (Eq 7) and Throughput (Eq 8) using path loss/shadowing.
  - **Agent:**
    - **TD3:** Twin Critic networks + Deterministic Actor + Replay Buffer.
    - **PPO:** Stochastic Actor + Critic + Generalized Advantage Estimation (GAE).
  - **Interface:** Normalization layer (mapping physical units to $[0,1]$) and Reward Calculator (Eq 9).

- **Critical path:**
  1. Environment generates State $s_t$ (BS/User locations + Channel conditions).
  2. Agent (Actor) samples normalized Action $a_t$ (Power, BW, Score).
  3. Environment denormalizes $a_t$, applies to channel model, calculates SINR/Throughput.
  4. Reward function computes $r_t$ using Weighted Sum (Throughput - Power + Fairness).
  5. Agent updates weights via backpropagation (PPO clip or TD3 min-Q).

- **Design tradeoffs:**
  - **TD3 vs. PPO:** Select TD3 for rapid deployment/online learning where sample collection is expensive. Select PPO for offline training or scenarios requiring high stability and maximum long-term fairness.
  - **Centralized vs. Distributed:** The paper uses a single centralized agent. Scalability to hundreds of BSs may require decomposition (not covered here).

- **Failure signatures:**
  - **Gradient Explosion:** If actions are not strictly normalized to $[0,1]$ before passing to the network.
  - **Greedy Collapse:** If $\beta$ (power penalty) or $\phi$ (fairness) are too low relative to $\kappa$ (throughput), the agent learns a "G-OFDMA" style policy that starves edge users.
  - **Critic Overestimation:** If TD3's target policy smoothing noise is too low, the Q-values may grow unbounded, destabilizing the actor.

- **First 3 experiments:**
  1. **Sanity Check (Random vs. Heuristic):** Run the environment with random actions vs. the PF-EQ heuristic to establish a baseline reward range and verify the reward function implementation.
  2. **Convergence Test (TD3 vs. PPO):** Train both agents for a reduced number of steps (e.g., 100k) on a fixed seed to verify if TD3 shows its characteristic fast initial rise compared to PPO.
  3. **Reward Weight Sensitivity:** Ablation study on the fairness weight $\phi$. Reduce it significantly and confirm that the agent's "Scheduling Score" metric drops (validating that the agent is responding to the specific reward shaping).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed centralized framework be adapted to a multi-agent architecture to improve scalability in ultra-dense network deployments?
- Basis in paper: [explicit] The conclusion states that "Future work will focus on extending this framework to multi-agent scenarios."
- Why unresolved: The current study utilizes a single central agent (Section II-A) which may struggle with the state/action space dimensionality and communication overhead of massive networks.
- What evidence would resolve it: A comparative performance analysis of centralized vs. decentralized agents in networks with significantly higher base station and user densities.

### Open Question 2
- Question: How do high-velocity user mobility patterns affect the stability and convergence speed of the PPO and TD3 agents?
- Basis in paper: [explicit] The authors identify "incorporating the effects of user mobility" as a specific direction for future work.
- Why unresolved: The current experiments assume users are static within episodes (Section IV), which simplifies the Markov property but neglects the dynamic channel fading and handover challenges inherent to mobile environments.
- What evidence would resolve it: Evaluation of reward convergence and handover success rates under simulation scenarios involving trajectory-based user movement.

### Open Question 3
- Question: Is the observed superiority of PPO over TD3 robust across different weightings of the multi-objective reward function ($\kappa, \beta, \phi$)?
- Basis in paper: [inferred] The paper fixes reward weights to specific values (Table II) to reflect "desired network operating priorities" (Section II), leaving the sensitivity of the algorithm ranking unexplored.
- Why unresolved: It is unclear if PPO remains the dominant algorithm if the objective function is altered to prioritize energy efficiency ($\beta$) much more heavily than throughput or fairness.
- What evidence would resolve it: A sensitivity analysis comparing TD3 and PPO performance across a grid of varying reward weight configurations.

## Limitations

- The centralized single-agent architecture may face scalability challenges in larger networks with hundreds of base stations
- The specific reward function weights significantly influence agent behavior, and alternative weightings could produce different algorithm rankings
- The simulation environment uses simplified channel models without considering mobility or traffic dynamics beyond static 50-user episodes

## Confidence

- **High Confidence**: The core comparative finding that PPO outperforms TD3 in long-term fairness and asymptotic reward while TD3 converges faster initially
- **Medium Confidence**: The specific claim that PPO's superior performance stems from its exploration mechanics via clipped updates
- **Medium Confidence**: The generalization of findings to other HetNet scenarios across different weightings

## Next Checks

1. **Reward Weight Sensitivity Analysis**: Systematically vary the fairness weight Ï† (e.g., 0.5, 0.7, 0.96) to determine if the PPO vs. TD3 performance ranking remains consistent across different fairness priorities

2. **Architecture Scaling Test**: Implement a distributed multi-agent version of the DRL framework and evaluate performance degradation as the number of base stations increases from 13 to 50+ to assess real-world scalability

3. **Cross-Algorithm Ablation Study**: Disable specific PPO components (e.g., entropy bonus, value function clipping) and specific TD3 components (e.g., target policy smoothing, twin critics) to isolate which algorithmic features contribute most to the observed performance differences