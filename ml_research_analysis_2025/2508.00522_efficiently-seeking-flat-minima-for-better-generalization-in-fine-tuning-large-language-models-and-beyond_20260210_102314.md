---
ver: rpa2
title: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large
  Language Models and Beyond
arxiv_id: '2508.00522'
source_url: https://arxiv.org/abs/2508.00522
tags:
- lora
- efmlora
- fmlora
- learning
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving generalization
  in fine-tuning large language models by efficiently seeking flat minima in the loss
  landscape. The authors propose Flat Minima LoRA (FMLoRA), which theoretically demonstrates
  that perturbations in the full parameter space can be equivalently transferred to
  the low-rank subspace, avoiding interference between multiple matrices.
---

# Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond

## Quick Facts
- **arXiv ID:** 2508.00522
- **Source URL:** https://arxiv.org/abs/2508.00522
- **Reference count:** 17
- **Primary result:** Proposes EFMLoRA to efficiently seek flat minima in LoRA fine-tuning, achieving near-LoRA computational efficiency while attaining comparable or better performance on GLUE, E2E NLG, CLIP, and Qwen-VL-Chat.

## Executive Summary
This paper addresses the challenge of improving generalization in fine-tuning large language models by efficiently seeking flat minima in the loss landscape. The authors propose Flat Minima LoRA (FMLoRA), which theoretically demonstrates that perturbations in the full parameter space can be equivalently transferred to the low-rank subspace, avoiding interference between multiple matrices. They further introduce an efficient version, EFMLoRA, which uses exponential moving average to accelerate optimization. Extensive experiments on RoBERTa, GPT-2, CLIP, and Qwen-VL-Chat show that EFMLoRA achieves near-LoRA computational efficiency while attaining comparable or better performance.

## Method Summary
The paper proposes two methods to efficiently seek flat minima in LoRA fine-tuning. FMLoRA transfers perturbations from the full parameter space to a single low-rank parameter space by projecting the perturbation onto matrix B using the pseudo-inverse of A, avoiding interference between independent perturbations in A and B. EFMLoRA builds on FMLoRA by using an Exponential Moving Average (EMA) strategy to accelerate optimization, reducing the computational overhead from 2x to 1x without significantly sacrificing sharpness minimization. The methods theoretically guarantee that the low-rank perturbation aligns with the true flatness objective while maintaining computational efficiency.

## Key Results
- EFMLoRA achieves near-LoRA computational efficiency while attaining comparable or better performance.
- On GLUE with RoBERTa-large, EFMLoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively.
- On vision-language models like Qwen-VL-Chat, EFMLoRA achieves improvements of 1.5% and 1.0% on SQA and VizWiz datasets, respectively.
- These results demonstrate that reducing sharpness is closely related to improved generalization in parameter-efficient fine-tuning methods.

## Why This Works (Mechanism)

### Mechanism 1
Mapping perturbations from the full parameter space to a single LoRA matrix (B) eliminates interference caused by independent perturbations in low-rank subspaces. Standard LoRA decomposes weights into A and B. A naive application of Sharpness-Aware Minimization (SAM) perturbs both matrices independently (A+E_A, B+E_B), which distorts the loss landscape geometry. This method calculates the perturbation as if it were applied to the full matrix W, then projects this perturbation exclusively onto the B matrix using the pseudo-inverse of A (E_B ≈ 1/s * E_W * A^+). This ensures the local sharpness measure in the low-rank subspace aligns with the true sharpness of the full model. Core assumption: The condition number of matrix A remains low (≈3), ensuring the pseudo-inverse accurately approximates the full-space gradient.

### Mechanism 2
Using an Exponential Moving Average (EMA) of historical perturbations reduces computational overhead from 2× to 1× without significantly sacrificing sharpness minimization. Standard SAM requires two forward-backward passes (one to find the perturbation, one to update weights). EFMLoRA reuses the perturbation direction computed in the previous step (E_hat_{t-1}) to evaluate the current gradient. It assumes the optimal perturbation direction changes slowly enough that a weighted average of past directions is a valid proxy for the current adversarial ascent step. Core assumption: The loss landscape is locally convex or smooth enough that the perturbation estimate E_hat_t lags minimally behind the true optimal perturbation E_tilde_t.

### Mechanism 3
Explicitly optimizing for flat minima in the low-rank subspace correlates directly with improved generalization on downstream tasks. By minimizing the loss uniformly within a neighborhood ρ (flatness), the model becomes less sensitive to input noise and distribution shifts. The paper provides empirical validation that this geometric property, often lost in standard PEFT, is a primary driver of performance gains over standard LoRA. Core assumption: The "flat minima" hypothesis, previously established for full fine-tuning, holds true and is accessible within the constrained rank-r subspace of LoRA.

## Foundational Learning

### Sharpness-Aware Minimization (SAM)
Why needed: This is the core mathematical tool the paper modifies. You must understand how SAM seeks to minimize max_{||ε||≤ρ} L(w+ε) rather than just L(w).
Quick check: Why does standard SAM require roughly twice the compute of standard Stochastic Gradient Descent (SGD)?

### LoRA (Low-Rank Adaptation)
Why needed: You need to distinguish the roles of matrices A and B to understand why the paper targets B for perturbation and A for stability.
Quick check: In the update W = W_0 + BA, which matrix is typically initialized to zero, and what implication does this have for gradient flow at the start of training?

### Pseudo-inverse & Matrix Calculus
Why needed: Mechanism 1 relies on approximating the gradient of the full matrix W using the gradients of A and B via pseudo-inverse (A^+).
Quick check: If matrix A is singular or ill-conditioned, why does the pseudo-inverse operation in Eq. (12) become unreliable?

## Architecture Onboarding

### Component map:
Frozen Backbone -> LoRA Adapter (A: Project-down, B: Project-up) -> Perturbation Buffer (stores EMA vector E_B) -> Hyperparameters (ρ: Perturbation radius, β: EMA momentum)

### Critical path:
1. Input: Pass x through W_0
2. Perturbed Forward: Compute adapter output using perturbed weights: y = (B + E_hat_B)Ax
3. Gradient: Compute gradients ∇L_B, ∇L_A at the perturbed point
4. Project: Estimate full-space gradient ∇L_W and project back to find the new optimal perturbation E_B^new
5. Update EMA: E_hat_B ← (1-β)E_hat_B + βE_B^new
6. Step Weights: Update A and B using the gradients from Step 3

### Design tradeoffs:
Memory vs. Speed: EFMLoRA trades slightly higher memory (storing E_hat_B) for 1× training speed (vs 2× for naive SAM)
Perturbation Target: The paper perturbs B and freezes the perturbation on A, arguing A captures domain-common features. Perturbing A might work for highly distinct domains but risks instability.

### Failure signatures:
Exploding Loss: The perturbation radius ρ is too large for the EMA approximation to remain stable
No Improvement over LoRA: The EMA coefficient β is too low, causing the estimated perturbation to lag too far behind the true optimal ascent direction
NaN Gradients: The condition number of A exploded, causing the pseudo-inverse calculation in the gradient projection to fail

### First 3 experiments:
1. Sanity Check (GLUE/MNLI): Run EFMLoRA vs. Standard LoRA. Verify that training time is roughly equal (1.1×) while accuracy increases
2. Ablation on Radius (ρ): Run a sweep of ρ (e.g., 0.05, 0.1, 0.2) on a validation set. Identify the "sweet spot" where sharpness is reduced without diverging
3. EMA vs. Full SAM: Compare EFMLoRA (1-pass) against FMLoRA (2-pass). Quantify the performance gap to see if the EMA approximation holds for your specific dataset

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal perturbation radius ρ be determined automatically for specific downstream tasks without manual tuning? The paper notes performance is sensitive to ρ, with different tasks requiring significantly different radii, but lacks a theoretical or automated method for its determination.

### Open Question 2
Is perturbing only the matrix B theoretically optimal for all LoRA architectures, or do specific scenarios benefit from perturbing matrix A? The method section states the decision to perturb B relies on external observations rather than a theoretical proof of optimality within the FMLoRA framework.

### Open Question 3
How does the theoretical guarantee of the EMA approximation behave in non-convex loss landscapes outside of the fine-tuning regime? Theorem 2 relies on convexity, which the authors admit is reasonable only because fine-tuning occurs near a local minimum where the landscape is approximately convex.

### Open Question 4
Does the correlation between sharpness reduction and improved generalization hold for parameter-efficient fine-tuning in modalities other than text and vision? The experiments are limited to NLU, NLG, and Vision-Language tasks, and it is unclear if the "flat minima" hypothesis translates effectively to other domains.

## Limitations
- The pseudo-inverse projection mechanism's stability depends critically on the condition number of matrix A remaining low, with no explicit regularization for edge cases
- The EMA approximation assumes the optimal perturbation direction changes slowly enough to be captured by a weighted average of past directions, validated only through Theorem 2 bounds
- The "flat minima" generalization hypothesis, while established for full fine-tuning, is assumed to hold within the constrained low-rank subspace of LoRA without direct theoretical proof

## Confidence
- **High:** Experimental results demonstrating EFMLoRA's computational efficiency gains and effectiveness across multiple model architectures and tasks
- **Medium:** Theoretical demonstration that perturbations can be equivalently transferred between full and low-rank spaces, given the assumption of low condition numbers
- **Medium:** Claim that EMA approximation maintains sufficient sharpness minimization to justify computational savings, based on Theorem 2 error bounds

## Next Checks
1. **Numerical Stability Test:** Implement monitoring to track the condition number of matrix A throughout training. Run experiments with and without explicit regularization on A to determine if pseudo-inverse projection fails in any scenarios.
2. **Perturbation Direction Fidelity:** Compute and visualize the cosine similarity between the EMA-approximated perturbation direction E_hat_B and the true optimal perturbation direction computed via full SAM. Determine if EMA approximation maintains high fidelity (>0.8) across different learning rates and datasets.
3. **Flatness Generalization Link:** Design an ablation study that explicitly measures flatness (e.g., using PAC-Bayes bounds or empirical sharpness measures) and correlates these metrics with generalization performance on held-out test sets.