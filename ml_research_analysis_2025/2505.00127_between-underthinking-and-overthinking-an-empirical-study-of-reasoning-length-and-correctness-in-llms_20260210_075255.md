---
ver: rpa2
title: 'Between Underthinking and Overthinking: An Empirical Study of Reasoning Length
  and correctness in LLMs'
arxiv_id: '2505.00127'
source_url: https://arxiv.org/abs/2505.00127
tags:
- length
- reasoning
- questions
- responses
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically studies the relationship between reasoning
  length and answer correctness in large language models. It finds that models tend
  to overthink simple problems, generating unnecessarily long outputs, while underthinking
  harder ones, failing to extend reasoning when needed.
---

# Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs

## Quick Facts
- arXiv ID: 2505.00127
- Source URL: https://arxiv.org/abs/2505.00127
- Reference count: 18
- This paper empirically studies the relationship between reasoning length and answer correctness in large language models, finding that models tend to overthink simple problems while underthinking harder ones, and that accuracy exhibits a non-monotonic relationship with reasoning length.

## Executive Summary
This paper investigates a fundamental challenge in large language model reasoning: models struggle to calibrate their response length appropriately to problem difficulty. Through empirical analysis on mathematical reasoning benchmarks (GSM8K and MATH), the authors demonstrate that models generate unnecessarily long responses for simple problems (overthinking) while failing to extend reasoning sufficiently for harder problems (underthinking). The study reveals that correctness exhibits a non-monotonic relationship with reasoning length—initially improving with longer responses but declining when reasoning becomes excessively long. The authors propose length-based preference optimization that can reduce token generation by 30-60% while maintaining acceptable accuracy, even without ground-truth supervision.

## Method Summary
The study analyzes two 1.5B parameter reasoning models (DeepSeek-1.5B-Distill and DeepScaler-1.5B-Preview) on GSM8K and MATH benchmarks. For analysis, the method generates N=10 samples per question using top-p sampling (p=1.0) with max_tokens=8k, then ranks responses by length to examine accuracy-length relationships. The study computes accuracy at each rank position (Acc_r), average lengths for correct (L✓) and incorrect (L×) responses, and Pearson/Spearman correlations. For length-based preference optimization, the method uses SimPO to train models preferring shorter responses without correctness labels, using 2-sample pairs per question with gradient accumulation.

## Key Results
- Incorrect responses are significantly longer than correct responses (L× >> L✓)
- Accuracy exhibits non-monotonic relationship with reasoning length, peaking at r*=1 or r*=3
- Models overthink simple problems and underthink harder ones, failing to calibrate length to difficulty
- Length-based preference optimization reduces generation length by 30%-60% while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1: Non-Monotonic Length-Correctness Curve
- **Claim:** For a fixed question, increasing reasoning length improves accuracy only up to a specific optimal point, after which additional tokens degrade performance due to noise introduction.
- **Core assumption:** The model's sampling process reflects a distribution where "correctness" is a finite state that can be diluted by excessive exploration.
- **Evidence anchors:**
  - [abstract] "Correctness exhibits a non-monotonic relationship... initially improving... but declining when reasoning becomes excessively long."
  - [section 4.1] "Accuracy initially improves with increasing reasoning length, but beyond a certain point, further increases in length lead to a decline... peak accuracy is achieved at r*=1 or r*=3."
  - [corpus] "DeepCompress" (arXiv:2510.27419) corroborates the existence of cognitive inefficiencies where reasoning extends beyond utility.

### Mechanism 2: Asymmetric Difficulty Calibration
- **Claim:** Models successfully allocate more tokens to "perceived" difficulty increases within their capability range but fail to extend reasoning for problems far outside their knowledge boundaries.
- **Core assumption:** "Underthinking" stems from a misclassification of problem difficulty rather than a hard limit on generation capacity.
- **Evidence anchors:**
  - [abstract] "Models tend to overthink simple problems... and underthink harder ones, failing to appropriately calibrate their response length."
  - [section 5.1] "Models may struggle to be aware of, and respond to, increased difficulty in hard questions... [generating] responses that are shorter than needed."
  - [corpus] "Think Right" (arXiv:2510.01581) supports the need for adaptive allocation aligned with task difficulty.

### Mechanism 3: Implicit Pruning via Length Preference
- **Claim:** Optimizing for brevity using a length-only reward signal (without ground truth) preserves accuracy by disproportionately penalizing incorrect responses, which naturally exhibit higher variance and length.
- **Core assumption:** The distribution of incorrect answers is shifted towards longer token counts compared to correct answers for the same prompt.
- **Evidence anchors:**
  - [abstract] "Generation length can be reduced by 30%-60%... with most reduction coming from incorrect responses."
  - [section 6] "Incorrect responses are generally much longer than correct ones... the observed reduction in overall generation length is mainly driven by shortening the incorrect responses."
  - [corpus] "From Long to Short" (arXiv:2509.06174) confirms LLMs can effectively trim reasoning chains without explicit correctness labels.

## Foundational Learning

- **Concept:** Non-Monotonic Performance Scaling
  - **Why needed here:** Engineers often assume "more compute = better performance." This paper demonstrates an inverted-U curve where more tokens actively harm accuracy after an optimal point.
  - **Quick check question:** If increasing `max_tokens` from 4k to 8k lowers accuracy on a validation set, is it a model failure or an "overthinking" artifact?

- **Concept:** Implicit Reward Signals
  - **Why needed here:** The paper uses "length" as a proxy reward without ground truth labels. Understanding how to derive supervision signals from structural properties (like token count) is key to low-cost alignment.
  - **Quick check question:** Can you identify a structural property in your data (e.g., response entropy, formatting consistency) that correlates with quality and could serve as a training signal?

- **Concept:** Difficulty Calibration vs. Capability
  - **Why needed here:** The paper distinguishes between "is the model capable?" and "does the model know if it's capable?" Poor calibration leads to wasted compute on easy problems and insufficient reasoning on hard ones.
  - **Quick check question:** Does the model's token usage correlate with the ground-truth difficulty label, or does it flatline regardless of complexity?

## Architecture Onboarding

- **Component map:** Sampler -> Ranker -> Optimizer (SimPO)
- **Critical path:**
  1. **Diversity Generation:** You must sample multiple responses (N ≥ 10) to reveal the distribution of lengths and correctness. Single-sample evaluation will miss the "overthinking" trend.
  2. **Correlation Analysis:** Validate the negative correlation between length and correctness on your specific dataset before applying length penalties. If your task requires long reasoning (e.g., summarization of books), this architecture will fail.

- **Design tradeoffs:**
  - **Label-Free vs. Accuracy Loss:** Training without correctness labels is cheap and scalable, but you risk pruning long, correct, complex chains.
  - **Inference Cost vs. Calibration:** Allowing the model to "decide" length requires it to have good self-awareness, which smaller models (e.g., 1.5B) lack.

- **Failure signatures:**
  - **Length Collapse:** The model learns to output the answer immediately without reasoning (CoT collapse), causing accuracy to plummet on harder datasets.
  - **Stagnant Perplexity:** If perplexity does not decrease as accuracy increases (Section 5.1), the model is overconfident on difficult problems, indicating calibration failure.

- **First 3 experiments:**
  1. **Optimal Length Sweep:** Plot accuracy vs. token length rank (r) to find the peak (is it r=1 or r=5?).
  2. **Unlabeled Preference Run:** Train using SimPO preferring shorter responses on a held-out set without labels; measure the trade-off curve between token reduction and accuracy drop.
  3. **Stratified Length Analysis:** Separate the dataset into "Easy" (100% solve rate) and "Hard" (0% solve rate) to verify if the model genuinely "underthinks" the hard set (shorter than expected) or if it simply fails to solve it regardless of length.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed non-monotonic relationships between reasoning length and correctness emerge primarily from pretraining or from post-training processes such as supervised fine-tuning or reinforcement learning?
- **Basis in paper:** [explicit] The authors state: "Extending this analysis to a broader set of models—with different training strategies—and dataset with varying difficulty levels would help assess the generality of our observations and clarify whether these patterns emerge primarily from pretraining or post-training processes such as supervised fine-tuning or reinforcement learning."
- **Why unresolved:** The study only examines two reasoning-capable models (DeepSeek-1.5B-Distill and DeepScaler-1.5B-Preview), which share similar training paradigms, making it impossible to isolate the training stage responsible for length-calibration failures.

### Open Question 2
- **Question:** When and why do self-correction mechanisms succeed, and how does their effectiveness relate to the model's internal assessment of problem difficulty?
- **Basis in paper:** [explicit] The authors note: "The distinct behaviors of models towards easy questions and more challenging questions found in our paper hint at a deeper connection between model calibration and self-correction."
- **Why unresolved:** The paper demonstrates that models fail to recognize difficulty on hard problems (underthinking) but does not investigate whether self-correction capabilities differ between problems the model knows it can solve versus those beyond its capabilities.

### Open Question 3
- **Question:** Would incorporating simple length-based heuristics—such as deprioritizing excessively long generations—improve accuracy when aggregating multiple reasoning chains via self-consistency?
- **Basis in paper:** [explicit] The authors state: "Our results also motivate practical refinements to test-time inference strategies that aggregate multiple reasoning chains, such as self-consistency. Incorporating simple heuristics—e.g., deprioritizing excessively long generations—may improve both accuracy and efficiency."
- **Why unresolved:** The paper shows that excessively long responses are more likely incorrect, but does not empirically validate whether this signal can improve self-consistency voting.

### Open Question 4
- **Question:** Can models be explicitly trained to better recognize problem difficulty and adaptively calibrate reasoning length, overcoming the underthinking tendency on challenging problems?
- **Basis in paper:** [inferred] The paper shows models underthink hard problems—they fail to extend reasoning when it is most needed and "may struggle to be aware of, and respond to, increased difficulty in hard questions." This suggests a gap in difficulty-aware length adaptation.
- **Why unresolved:** While SimPO experiments show length can be reduced for easy problems, the paper does not explore whether models can be trained to recognize when longer reasoning is genuinely necessary.

## Limitations

- Analysis is limited to mathematical reasoning benchmarks (GSM8K and MATH), which may not generalize to other reasoning domains
- Experiments conducted only on 1.5B parameter models, limiting conclusions about scaling behavior
- Length-based preference optimization uses fixed sampling parameters without exploring temperature or diversity variations

## Confidence

**High Confidence:** The empirical observation that incorrect responses are significantly longer than correct responses on mathematical reasoning tasks.

**Medium Confidence:** The claim that accuracy exhibits a non-monotonic relationship with reasoning length, with an optimal point beyond which additional tokens degrade performance.

**Medium Confidence:** The assertion that models systematically overthink simple problems while underthinking harder ones.

**Low Confidence:** The generalizability of length-based preference optimization to produce state-of-the-art performance across diverse reasoning tasks.

## Next Checks

1. **Cross-Domain Replication:** Apply the length-correctness analysis to non-mathematical reasoning tasks (e.g., commonsense reasoning datasets like StrategyQA or ARC) to test whether the non-monotonic relationship holds across reasoning domains.

2. **Model Scale Sweep:** Repeat the preference optimization experiments across a range of model sizes (e.g., 7B, 34B, 70B parameters) to determine whether the 30-60% length reduction without accuracy loss scales proportionally with model capacity.

3. **Semantic Quality Analysis:** Implement a qualitative assessment where human evaluators rate the coherence and completeness of reasoning chains from different length ranks (r=1, r=3, r=5, r=10) to determine whether shorter correct responses sacrifice valuable reasoning steps compared to longer correct responses.