---
ver: rpa2
title: Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning
arxiv_id: '2506.15544'
source_url: https://arxiv.org/abs/2506.15544
tags:
- learning
- gradient
- deep
- depth
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the scalability challenges in deep reinforcement
  learning, particularly focusing on the interaction between non-stationarity and
  gradient pathologies in large neural networks. Through empirical analyses, the authors
  demonstrate that deeper and wider networks suffer from vanishing gradients, reduced
  activations, and degraded representation expressivity under non-stationary conditions.
---

# Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.15544
- Source URL: https://arxiv.org/abs/2506.15544
- Reference count: 40
- This work investigates the scalability challenges in deep reinforcement learning, particularly focusing on the interaction between non-stationarity and gradient pathologies in large neural networks.

## Executive Summary
This paper addresses critical scalability challenges in deep reinforcement learning by investigating how non-stationarity and gradient pathologies interact in large neural networks. Through comprehensive empirical analyses, the authors demonstrate that deeper and wider networks suffer from vanishing gradients, reduced activations, and degraded representation expressivity under non-stationary conditions. The research proposes two key interventions—multi-skip residual connections and Kronecker-factored optimization—that significantly improve training stability and performance across multiple benchmarks. These innovations enable effective parameter scaling in deep RL, achieving substantial performance gains on standard test suites.

## Method Summary
The authors tackle scalability issues in deep RL through a two-pronged approach. First, they introduce multi-skip residual connections that enable direct gradient propagation from the encoder to all subsequent layers, mitigating vanishing gradient problems in deep networks. Second, they employ Kronecker-factored optimization, which leverages curvature information for more stable parameter updates. These interventions are evaluated across multiple RL agents including PQN and PPO, tested on benchmarks such as Atari-10 and DeepMind Control Suite. The methodology combines empirical analysis of gradient pathologies with targeted architectural modifications to address identified issues.

## Key Results
- The proposed interventions achieve median improvements of 83.27% on the full ALE suite
- Performance gains of 31.40% on Atari-10 benchmark demonstrate cross-domain effectiveness
- Combined approach enables stable training at larger scales, validating gradient stabilization as critical for effective parameter scaling in deep RL

## Why This Works (Mechanism)
The effectiveness of the proposed approach stems from addressing fundamental gradient pathologies that emerge during large-scale RL training. Non-stationarity in RL creates moving target problems where the data distribution shifts during learning, exacerbating gradient issues in deep networks. Multi-skip residual connections provide shortcut paths for gradient flow, preventing the vanishing gradient problem that typically plagues deep architectures under non-stationary conditions. Kronecker-factored optimization improves update stability by incorporating second-order curvature information, which is particularly valuable when dealing with the noisy gradients characteristic of RL. Together, these mechanisms maintain gradient magnitude and direction stability throughout training, enabling deeper and wider networks to learn effectively.

## Foundational Learning

**Non-stationarity in RL** - The data distribution shifts during learning as the policy changes, creating a moving target problem that complicates optimization. Why needed: Understanding this is crucial because it's the primary source of instability in deep RL training. Quick check: Verify that gradient variance increases over training epochs.

**Vanishing gradients** - Gradients diminish exponentially as they propagate through deep networks, preventing effective learning in lower layers. Why needed: This is the core pathological behavior that prevents scaling deep networks in RL. Quick check: Monitor gradient norms across layers during training.

**Residual connections** - Skip connections that bypass intermediate layers to enable direct gradient flow. Why needed: Essential for training very deep networks by providing gradient shortcut paths. Quick check: Compare training curves with and without skip connections.

**Kronecker-factored approximation** - Method for approximating the Fisher information matrix using Kronecker products to enable efficient second-order optimization. Why needed: Provides curvature-aware updates that stabilize training in noisy RL environments. Quick check: Measure training stability metrics with different optimizers.

**Representation expressivity** - The capacity of neural networks to capture and represent relevant features from input data. Why needed: Degradation of expressivity directly impacts learning performance and is a key indicator of gradient pathologies. Quick check: Evaluate feature diversity metrics during training.

## Architecture Onboarding

**Component map**: Input -> Encoder -> Multi-skip Residual Blocks -> Output. The multi-skip connections create parallel gradient paths from encoder to all subsequent layers.

**Critical path**: The primary learning signal flows through the encoder to residual blocks, with gradients flowing back through both direct and residual paths. The Kronecker-factored optimizer processes these gradients to determine parameter updates.

**Design tradeoffs**: Multi-skip connections increase parameter count and computational overhead but provide critical gradient stability. Kronecker-factored optimization offers better convergence but requires additional computation for curvature estimation. The tradeoff favors stability and performance over computational efficiency.

**Failure signatures**: Vanishing gradients manifest as exponentially decaying gradient norms across layers. Reduced activations appear as shrinking activation magnitudes over training. Degraded expressivity shows as reduced feature diversity and collapsed representations.

**First experiments**: 1) Measure gradient norms across layers during baseline training to establish pathology baselines. 2) Implement multi-skip connections and verify improved gradient flow. 3) Compare Kronecker-factored optimization against standard optimizers on training stability metrics.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The analysis focuses on specific network architectures and may not generalize to all RL settings
- The study does not investigate how interventions perform with alternative RL algorithms beyond PQN and PPO
- Computational overhead of Kronecker-factored optimization in distributed training scenarios is not fully characterized

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Empirical findings demonstrate strong improvements in large-scale RL settings | High |
| Effectiveness of multi-skip residual connections and Kronecker-factored optimization | Medium |
| Scalability claims within tested domains | High |
| Generalizability to all RL settings | Low |

## Next Checks
1. Test the proposed interventions with additional RL algorithms (e.g., SAC, TD3) to assess cross-algorithm robustness
2. Evaluate performance across a broader range of network architectures, including convolutional transformers and vision transformers
3. Investigate the computational overhead of Kronecker-factored optimization in distributed training scenarios to verify practical scalability benefits