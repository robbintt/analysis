---
ver: rpa2
title: 'Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing'
arxiv_id: '2510.16040'
source_url: https://arxiv.org/abs/2510.16040
tags:
- edram
- kelle
- vectors
- cache
- refresh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kelle is a software-hardware co-design solution that integrates
  embedded DRAM (eDRAM) with large language model (LLM) serving to improve efficiency
  on edge devices. It addresses the high memory and energy costs of key-value (KV)
  caching during LLM inference by using eDRAM as the primary on-chip storage for KV
  vectors.
---

# Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing

## Quick Facts
- arXiv ID: 2510.16040
- Source URL: https://arxiv.org/abs/2510.16040
- Authors: Tianhua Xia; Sai Qian Zhang
- Reference count: 40
- Primary result: 3.9× speedup and 4.5× energy savings over baselines

## Executive Summary
Kelle addresses the high memory and energy costs of KV caching in LLM inference on edge devices by integrating eDRAM with intelligent cache management. The solution introduces attention-based eviction and recomputation policy (AERP) to selectively retain critical tokens while recomputing others, and a two-dimensional adaptive refresh policy (2DRP) that varies refresh frequency based on token importance and bit position. A dedicated accelerator with specialized hardware further optimizes computation patterns. Evaluations show Kelle achieves significant speedups and energy savings while maintaining model accuracy.

## Method Summary
Kelle is a software-hardware co-design solution that uses eDRAM as primary storage for KV vectors, implementing AERP for cache management and 2DRP for refresh optimization. The system includes a reconfigurable systolic array for matrix multiplications, a systolic evictor for on-the-fly importance scoring, and a scheduler that minimizes data lifetime in eDRAM. The approach selectively stores input vectors for popular tokens to be recomputed later, while varying refresh rates based on bit position and token importance to reduce energy consumption.

## Key Results
- Achieves 3.9× speedup compared to baseline solutions
- Delivers 4.5× energy savings through optimized cache management and refresh policies
- Maintains LLM accuracy while significantly reducing memory footprint and refresh overhead

## Why This Works (Mechanism)

### Mechanism 1: Attention-Guided Selective Retention (AERP)
Storing only critical tokens and recomputing others on demand reduces KV cache footprint without destabilizing the LLM. Kelle calculates an "importance score" for each token by summing attention scores across heads, evicting low-score tokens. For popular tokens (retained in >50% of heads), it stores only input vectors rather than KV vectors, recomputing K and V on the fly. This leverages the observation that recomputation cost is often lower than memory access cost for long sequences.

### Mechanism 2: Risk-Weighted Refresh (2DRP)
Significant energy savings are achievable by reducing refresh frequency for data that minimally impacts model accuracy if corrupted. eDRAM requires periodic refresh to prevent charge leakage. Kelle varies refresh frequency along two dimensions: Bit Position (MSBs refreshed more than LSBs) and Token Importance (High-score tokens refreshed more than Low-score tokens). This exploits the finding that LLMs are robust to errors in LSBs and unimportant tokens.

### Mechanism 3: Transient Data Lifetime Optimization
Reducing the time data resides in volatile memory directly lowers refresh energy. The Kelle scheduler reorders operations to minimize the gap between when data is written and when it is consumed. By parallelizing weight loading (SRAM) and KV loading (eDRAM), it reduces the "data lifetime" of activations from baseline to Kelle levels.

## Foundational Learning

- **Concept**: KV Cache Dynamics
  - Why needed here: The entire paper revolves around managing the linear growth of the KV cache during decoding. You must understand that the KV cache stores previous states to avoid redundant computations.
  - Quick check question: Does the KV cache size depend on the batch size or the sequence length? (Answer: Sequence length, linearly)

- **Concept**: eDRAM Retention and Refresh
  - Why needed here: Unlike SRAM, eDRAM cells leak charge. Kelle optimizes the "refresh" operation (reading and rewriting data to maintain integrity) which is usually a fixed, power-hungry overhead.
  - Quick check question: Why is eDRAM denser than SRAM but slower than SRAM? (Answer: Fewer transistors per cell (1T/3T vs 6T), but requires periodic refresh cycles)

- **Concept**: Attention Scores as Saliency
  - Why needed here: Kelle uses attention scores not just for output generation, but as a proxy for token importance.
  - Quick check question: If a token has a low attention score across all heads, what happens to it in the Kelle AERP policy? (Answer: It is evicted/not stored)

## Architecture Onboarding

- **Component map**: Input vector arrives -> RSA computes Q,K,V -> Systolic Evictor calculates scores and identifies lowest-score token -> Controller decides to store x_N (recompute later) or KV vectors based on popularity -> Controller triggers refresh pulses only for banks/groups where interval timer has expired

- **Critical path**: 1. Load: Input vector x_N arrives. 2. Compute: RSA computes Q,K,V. 3. Evict: Systolic Evictor calculates scores and identifies lowest-score token index. 4. Manage: Controller decides to store x_N (recompute later) or KV vectors based on "popularity." 5. Refresh: Controller triggers refresh pulses only for banks/groups where the interval timer has expired.

- **Design tradeoffs**:
  - Area vs. Capacity: Using eDRAM saves ~50% area vs. SRAM, but adds controller complexity
  - Compute vs. Memory: Recomputation saves memory space/bandwidth but consumes RSA cycles
  - Accuracy vs. Energy: 2DRP saves energy by risking data corruption; the "Uniform" vs. "2D" policy gap shows the tradeoff

- **Failure signatures**:
  - Accuracy Collapse: Sudden spike in Perplexity (PPL) indicates refresh interval is too long
  - Latency Stalls: If Systolic Evictor backpressures RSA, throughput drops
  - Energy Plateau: If energy savings don't scale with sequence length, check if scheduler is falling back to serial execution

- **First 3 experiments**:
  1. Baseline Refresh Profiling: Run LLM with uniform, safe refresh interval (e.g., 45μs). Measure energy and accuracy to establish "ground truth" reference.
  2. 2DRP Stress Test: Implement 2DRP but artificially increase temperature or retention time. Plot PPL vs. Refresh Interval to find the "knee" where accuracy drops.
  3. Recomputation Threshold Tuning: Vary the "popularity" threshold. Measure ratio of stored x vectors vs. KV vectors. Determine point where RSA utilization hits 100%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can LLMs tolerate data corruption in the KV cache without compromising accuracy, and does this tolerance vary across different model architectures (e.g., Mixture-of-Experts)?
- Basis in paper: Section 3.3.1 explicitly poses the question to motivate the design of the refresh policy.
- Why unresolved: While the paper establishes a tolerance threshold for specific dense models, it does not explore if this tolerance holds for sparsely activated models like MoE or models with significantly different attention mechanisms.
- What evidence would resolve it: A sensitivity analysis applying the retention failure injection method to MoE architectures or architectures with non-standard attention to compare failure rate thresholds.

### Open Question 2
- Question: Can a refresh policy be developed that is finer-grained than the proposed two-dimensional approach to further reduce energy consumption?
- Basis in paper: Section 4.2 asks if a finer-grained refresh policy could support even lower refresh frequencies while maintaining accuracy.
- Why unresolved: The proposed 2DRP creates four distinct groups. It is unclear if further granularity (e.g., per-channel, per-layer, or dynamic frequency scaling) could yield additional energy savings without exceeding complexity limits.
- What evidence would resolve it: Simulation of a 3D or higher-dimensional refresh policy that measures marginal energy gains against hardware overhead of increased logic complexity.

### Open Question 3
- Question: How can the system dynamically identify the optimal trade-off point between KV cache recomputation and memory access to avoid becoming compute-bound?
- Basis in paper: Section 8.3.2 demonstrates that "Over Recomp" shifts the bottleneck from memory to compute engine, yet AERP relies on a static heuristic to decide when to recompute.
- Why unresolved: The paper establishes a fixed threshold for token popularity to trigger recomputation. It does not provide a mechanism to adjust this threshold dynamically if the systolic array approaches saturation.
- What evidence would resolve it: A dynamic scheduler that monitors systolic array utilization and adjusts the recomputation threshold in real-time to maintain the system at the "memory-bound" roofline ridge point.

### Open Question 4
- Question: Does the assumption that "token popularity" remains stable during decoding hold for complex reasoning tasks where attention patterns shift significantly mid-sequence?
- Basis in paper: Section 4.1.2 assumes "empirical evidence shows limited fluctuation" in token popularity, fixing storage format after prefilling stage.
- Why unresolved: The "limited fluctuation" claim is based on specific datasets. Complex reasoning tasks might exhibit "late-blooming" attention where tokens deemed unimportant early become critical later.
- What evidence would resolve it: Evaluation of AERP policy on datasets requiring dynamic context switching or long-horizon dependencies to track eviction regret rates.

## Limitations

- eDRAM Refresh Tolerance Thresholds: The specific relationship between refresh interval, temperature, and error accumulation is not fully characterized. Real-world edge deployments may experience temperature variations that could push bit-flip rates beyond the claimed $10^{-3}$ tolerance.
- Recomputation Overhead Scaling: The scalability with larger batch sizes remains unclear. The compute-bound threshold is mentioned but not explicitly quantified.
- Cross-Model Generalization: The techniques are validated on LLaMA models, but effectiveness may vary significantly across different LLM architectures.

## Confidence

**High Confidence**: The fundamental observation that LLMs are robust to certain types of bit errors (particularly in LSBs) is well-supported by experimental results. The architectural design of separating weight SRAM from KV eDRAM is straightforward and verifiable.

**Medium Confidence**: The AERP mechanism's effectiveness depends on the assumption that recomputation is consistently cheaper than storage for popular tokens. While the paper provides theoretical justification, the actual break-even point is not precisely characterized across different sequence lengths and batch sizes.

**Low Confidence**: The 2DRP refresh policy's real-world effectiveness under variable operating conditions (temperature fluctuations, voltage variations) is not thoroughly validated. The paper demonstrates controlled experiments but doesn't address how the system maintains accuracy guarantees across the full range of edge device operating environments.

## Next Checks

1. **Temperature Sensitivity Analysis**: Conduct experiments varying ambient temperature (0-70°C) while monitoring PPL degradation and bit-flip rates. This would validate whether the 2DRP refresh intervals maintain the claimed $10^{-3}$ error tolerance across the full operating range of edge devices.

2. **Batch Size Scaling Study**: Systematically increase batch sizes from 1 to 32 while measuring RSA utilization, memory bandwidth usage, and overall energy efficiency. Identify the exact batch size threshold where AERP's memory savings are negated by recomputation overhead.

3. **Cross-Architecture Generalization**: Implement Kelle's techniques on at least two additional LLM architectures (e.g., OPT, Falcon) with different attention mechanisms and quantization schemes. Compare the effectiveness of attention-based importance scoring and bit-level error tolerance across these models to establish the technique's generalizability.