---
ver: rpa2
title: Serialized EHR make for good text representations
arxiv_id: '2510.13843'
source_url: https://arxiv.org/abs/2510.13843
tags:
- data
- language
- serialbehrt
- arxiv
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SerialBEHRT, a domain-aligned foundation
  model extending SciBERT through additional pretraining on structured EHR sequences.
  SerialBEHRT aims to address the challenge of reconciling the tabular and event-based
  nature of Electronic Health Records (EHRs) with the sequential priors of natural
  language models.
---

# Serialized EHR make for good text representations

## Quick Facts
- arXiv ID: 2510.13843
- Source URL: https://arxiv.org/abs/2510.13843
- Authors: Zhirong Chou; Quan Qin; Shi Li
- Reference count: 22
- Primary result: Domain-aligned foundation model extending SciBERT through EHR text serialization shows superior antibiotic susceptibility prediction performance

## Executive Summary
This paper introduces SerialBEHRT, a domain-aligned foundation model extending SciBERT through additional pretraining on structured EHR sequences. The core innovation addresses the challenge of reconciling the tabular and event-based nature of Electronic Health Records (EHRs) with the sequential priors of natural language models. By leveraging text serialization to transform EHR data into a textual format suitable for pretraining, SerialBEHRT encodes temporal and contextual relationships among clinical events. The model is evaluated on antibiotic susceptibility prediction, demonstrating superior and more consistent performance compared to state-of-the-art EHR representation strategies.

## Method Summary
SerialBEHRT extends SciBERT through additional pretraining on serialized EHR data, addressing the fundamental challenge of applying natural language processing models to structured clinical data. The approach involves transforming structured EHR fields into temporally ordered textual representations, then using these serialized sequences to continue pretraining SciBERT with Masked Language Modeling. The serialized format preserves the temporal and contextual relationships inherent in clinical events while making them compatible with transformer-based architectures. The model is initialized from SciBERT, expands the vocabulary through SentencePiece BPE training on the serialized EHR corpus, and continues pretraining on a combined corpus of scientific text and serialized EHR data before fine-tuning on downstream clinical prediction tasks.

## Key Results
- SerialBEHRT achieved high F1 scores, ROC-AUC, and PRC-AUC values across various antibiotics in antibiotic susceptibility prediction
- The model demonstrated superior and more consistent performance compared to state-of-the-art EHR representation strategies
- Results highlight the effectiveness of temporal serialization in foundation model pretraining for healthcare applications

## Why This Works (Mechanism)
The approach works by reconciling the fundamental mismatch between structured EHR data and the sequential nature of transformer models. By converting tabular EHR events into temporally ordered textual representations, the model can leverage the powerful contextual learning capabilities of BERT architectures while preserving the temporal relationships critical for clinical decision-making. The serialization process encodes clinical events in a format that captures both the sequential nature of patient care and the domain-specific terminology needed for medical applications.

## Foundational Learning
- **SentencePiece BPE tokenization**: Needed to handle domain-specific clinical vocabulary and expand SciBERT's vocabulary from 32K to 42K tokens; quick check: verify merged vocab size equals 42K
- **Masked Language Modeling**: Core pretraining objective that teaches the model to understand contextual relationships in clinical text; quick check: ensure 15% mask ratio is applied consistently
- **EHR serialization schema**: Converts structured clinical data into sequential text format preserving temporal relationships; quick check: reconstruct sample EHR records to verify T(X) transformation
- **Temporal ordering in clinical events**: Critical for capturing the sequence of patient care and treatment decisions; quick check: verify chronological ordering of events in serialized output
- **Domain alignment of scientific text**: Combines general biomedical knowledge with EHR-specific patterns; quick check: inspect training corpus composition ratio

## Architecture Onboarding

**Component Map**: Serialized EHR -> SentencePiece Tokenizer -> Expanded SciBERT -> MLM Pretraining -> Fine-tuned Classifier -> Antibiotic Susceptibility Prediction

**Critical Path**: The critical path involves the serialization of EHR data into textual format, expansion of the tokenizer vocabulary, and subsequent pretraining on the combined corpus. This path directly determines the model's ability to understand clinical context and temporal relationships.

**Design Tradeoffs**: The approach trades off perfect preservation of structured data relationships for the ability to leverage powerful transformer architectures. While some tabular relationships may be lost in serialization, the gain in contextual understanding and temporal reasoning appears to outweigh this cost for clinical prediction tasks.

**Failure Signatures**: 
- Poor performance on PRC-AUC suggests class imbalance issues in the antibiotic susceptibility labels
- Inconsistent results across different antibiotics may indicate insufficient domain-specific pretraining
- Tokenizer vocabulary mismatches can cause embedding misalignment and degraded performance

**First Experiments**:
1. Verify tokenizer vocabulary expansion by checking token coverage on sample EHR records
2. Test serialization schema by reconstructing a small set of EHR records and verifying temporal ordering
3. Evaluate class distribution in antibiotic susceptibility labels to assess potential imbalance effects

## Open Questions the Paper Calls Out
- **Multi-site generalizability**: The model was validated exclusively on MIMIC-IV (Beth Israel Deaconess Medical Center), leaving robustness to diverse clinical settings and coding practices unproven. Future work intends to include multiple sites.
- **Generalization to other clinical tasks**: While evaluated on antibiotic susceptibility prediction, the model's utility for other longitudinal clinical outcomes like hospital readmission, mortality prediction, and phenotype prediction remains untested.
- **Extension to general tabular data**: It's unclear if the specific serialization priors transfer to non-medical tabular domains. The authors aim to develop a general tabular foundation model by serializing large numbers of datasets.

## Limitations
- The exact serialization schema for converting structured EHR fields into textual format is unspecified, creating uncertainty in reproducing the precise model behavior
- The antibiotic susceptibility prediction task may be sensitive to class imbalance and temporal confounding factors not fully addressed in the reported results
- Lack of multi-site evaluation limits understanding of the model's robustness to different clinical coding practices and EHR systems

## Confidence
- **High confidence**: The general approach of extending SciBERT through additional pretraining on serialized EHR data for improved clinical representation learning
- **Medium confidence**: The reported performance improvements, given the meaningful clinical task but potential sensitivity to data splits and class imbalance
- **Low confidence**: The exact implementation details required for faithful reproduction due to missing serialization schema specifications

## Next Checks
1. Verify the serialization schema by reconstructing a small sample of EHR records from MIMIC-IV using the described T(X) transformation and comparing against expected temporal ordering
2. Test the merged SentencePiece tokenizer by checking that vocabulary size reaches exactly 42K tokens and that clinical terms from EHR data are properly represented
3. Evaluate class distribution across all antibiotics in the downstream dataset to assess potential imbalance effects on PRC-AUC scores, and test whether class weighting improves results consistency