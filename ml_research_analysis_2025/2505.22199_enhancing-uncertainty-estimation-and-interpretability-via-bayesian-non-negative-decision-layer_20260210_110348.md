---
ver: rpa2
title: Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative
  Decision Layer
arxiv_id: '2505.22199'
source_url: https://arxiv.org/abs/2505.22199
tags:
- bndl
- uncertainty
- neural
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Bayesian Non-negative Decision Layer (BNDL)
  to address the limitations of deep neural networks in uncertainty estimation and
  interpretability. BNDL reformulates DNNs as a conditional Bayesian non-negative
  factor analysis, incorporating stochastic latent variables to model complex dependencies
  and provide robust uncertainty estimation.
---

# Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer

## Quick Facts
- arXiv ID: 2505.22199
- Source URL: https://arxiv.org/abs/2505.22199
- Reference count: 36
- Primary result: BNDL achieved 79.82% accuracy on CIFAR-100, outperforming the baseline ResNet-18 model (74.62%).

## Executive Summary
This paper introduces the Bayesian Non-negative Decision Layer (BNDL) to address the limitations of deep neural networks in uncertainty estimation and interpretability. BNDL reformulates DNNs as a conditional Bayesian non-negative factor analysis, incorporating stochastic latent variables to model complex dependencies and provide robust uncertainty estimation. The sparsity and non-negativity of the latent variables encourage disentangled representations and decision layers, improving interpretability. The authors offer theoretical guarantees for effective disentangled learning and develop a variational inference method using a Weibull variational inference network to approximate the posterior distribution of the latent variables. Experiments on benchmark datasets, including CIFAR-10, CIFAR-100, and ImageNet-1k, demonstrate that BNDL improves model accuracy, provides reliable uncertainty estimation, and enhances interpretability.

## Method Summary
BNDL replaces the final linear layer of a DNN with a Bayesian non-negative factor analysis framework. The model learns local latent variables θ (feature activations) and global latent variables Φ (decision layer weights) as distributions, using Gamma priors to enforce non-negativity and sparsity. A Weibull variational inference network approximates the intractable Gamma posterior for stable optimization. The method is trained via maximizing the Evidence Lower Bound (ELBO), which combines a categorical log-likelihood term with KL divergence terms for θ and Φ.

## Key Results
- Achieved 79.82% accuracy on CIFAR-100, outperforming the baseline ResNet-18 model (74.62%)
- Demonstrated improved uncertainty estimation through lower PAvPU scores compared to MC Dropout baselines
- Showed enhanced interpretability with higher SEPIN@k scores, indicating more disentangled features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing stochastic latent variables into the decision layer allows a network to model complex dependencies and capture aleatoric uncertainty.
- Mechanism: The BNDL reformulates the final layer as a conditional Bayesian non-negative factor analysis. Instead of deterministic weights and features, it models local latent variables θ (feature activations) and global latent variables Φ (decision layer weights) as distributions. The model learns to approximate the posterior distributions q(θ|x) and q(Φ), enabling sampling during inference to generate a distribution of predictions rather than a single point estimate.
- Core assumption: The uncertainty in a DNN's prediction can be primarily captured by stochasticity in the final decision layer, rather than requiring full Bayesian treatment of all network parameters. Assumption: The non-negative factor analysis structure is a suitable generative model for classification labels.

### Mechanism 2
- Claim: Enforcing non-negativity and sparsity on the decision layer weights and latent variables encourages the learning of disentangled representations, improving interpretability.
- Mechanism: BNDL uses a Gamma distribution as a prior for both local (θ) and global (Φ) latent variables. The Gamma distribution naturally enforces non-negativity and encourages sparsity. This constraint encourages the model to decompose the data into additive, non-negative parts. Theoretically, this connects to Non-negative Matrix Factorization (NMF), which is known for learning parts-based, disentangled representations.
- Core assumption: Disentangled, interpretable features are non-negative and sparse. The "selective window" assumption—that certain latent classes appear uniquely in the dataset—holds sufficiently for partial identifiability. Assumption: The Gamma distribution is a suitable prior for these variables.

### Mechanism 3
- Claim: A Weibull variational inference network can efficiently approximate the intractable posterior of the Gamma-distributed latent variables.
- Mechanism: Since the true posterior p(θ, Φ | Y, X) is intractable, the authors use variational inference. Directly reparameterizing the Gamma distribution for gradient estimation is problematic due to high variance. To solve this, they use a Weibull distribution, which is similar in shape to the Gamma but has a simple, differentiable reparameterization trick, enabling stable optimization via stochastic gradient descent (SGD) on the Evidence Lower Bound (ELBO).
- Core assumption: The Weibull distribution is a sufficiently flexible and accurate approximation for the conditional posteriors of the Gamma-distributed latent variables. Assumption: Stochastic gradient descent on the ELBO is sufficient for learning.

## Foundational Learning

- Concept: Bayesian Neural Networks & Uncertainty Estimation
  - Why needed here: BNDL is fundamentally a Bayesian method applied to the last layer of a DNN. Understanding the difference between point estimation (standard DNNs) and distributional estimation (BNNs), and the concepts of aleatoric vs. epistemic uncertainty, is crucial for grasping the problem BNDL solves.
  - Quick check question: How does a standard neural network output differ from a Bayesian neural network's output in terms of uncertainty?

- Concept: Variational Inference (VI) & Evidence Lower Bound (ELBO)
  - Why needed here: BNDL uses VI to train its parameters. The entire learning process is framed as maximizing the ELBO. Without understanding VI, the loss function and optimization procedure will be opaque.
  - Quick check question: Why is it necessary to maximize the Evidence Lower Bound (ELBO) instead of the likelihood directly in variational inference?

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: The core of BNDL's design is inspired by NMF. Understanding NMF's property of learning parts-based, additive, and sparse representations is key to understanding the interpretability claims and the non-negativity constraint.
  - Quick check question: What property of Non-negative Matrix Factorization makes it suitable for learning interpretable, "parts-based" representations?

## Architecture Onboarding

- Component map:
  - Backbone (ResNet/ViT) -> BNDL Layer -> Classification Output
  - BNDL Layer contains: Local Inference Network (f_λ, f_k), Global Latent Variables (Φ)

- Critical path:
  1. Forward pass input through the backbone to get feature h_j
  2. Pass h_j through Local Inference Network to get Weibull parameters for θ_j
  3. Sample θ_j and Φ from their respective Weibull distributions using the reparameterization trick
  4. Compute class probabilities via y_j = Category(θ_j Φ)
  5. Compute loss as the sum of negative log-likelihood and KL divergences
  6. Backpropagate and update backbone, inference network, and global variables

- Design tradeoffs:
  - Weibull vs. Gamma Posterior: The paper chooses Weibull for its tractable reparameterization, trading off some theoretical purity (since the prior is Gamma) for stable optimization
  - Last Layer vs. Full Network Bayesian Treatment: BNDL only applies Bayesian modeling to the last layer, trading off a more complete uncertainty estimate for scalability and ease of integration with existing architectures
  - Sparsity vs. Performance: A hyperparameter α in the activation function (f(x) = ReLU(x - α)) controls sparsity. Increasing sparsity may degrade accuracy, as shown in Figure 3

- Failure signatures:
  - High uncertainty on all inputs: May indicate a poorly converged variational posterior or a mismatch between the Weibull approximation and the true posterior
  - Degraded accuracy vs. baseline: May result from excessive sparsity constraints (high α) or an inadequacy of the non-negativity constraint for the dataset's features
  - No improvement in interpretability: May occur if the learned features remain entangled, potentially violating the "selective window" assumption required for identifiability

- First 3 experiments:
  1. Integrate BNDL with a ResNet-18 on CIFAR-10. Train from scratch and compare accuracy and PAvPU (uncertainty metric) against a standard ResNet-18 and an MC Dropout baseline
  2. Perform an ablation study on the sparsity parameter α for a pre-trained ImageNet model. Fine-tune only the BNDL layer and plot the trade-off curve between test accuracy and decision layer sparsity (1-sparsity) to replicate Figure 3
  3. Conduct a qualitative interpretability check. For a set of correctly classified images from ImageNet, visualize the LIME results for the most activated feature θ and compare against a baseline to see if BNDL features are more semantically meaningful, as in Figure 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the partial identifiability of features be theoretically guaranteed without relying on the restrictive "Selective Window" assumption, which requires unique samples for latent classes?
- Basis: Proposition 1 relies on the "Selective Window" assumption (that a latent class appears uniquely in the dataset) to guarantee identifiability. The authors argue this is reasonable but do not verify if standard datasets strictly satisfy it.
- Why unresolved: Real-world datasets often violate strict uniqueness assumptions. The current theoretical guarantee may be brittle if the data distribution does not contain these specific "anchor" samples for every class.
- What evidence would resolve it: A theoretical proof of identifiability under relaxed conditions (e.g., sufficiently scattered conditions) or an empirical sensitivity analysis showing how identifiability degrades as the "uniqueness" of samples is removed.

### Open Question 2
- Question: To what extent does the use of a Weibull distribution to approximate the Gamma posterior introduce bias into the uncertainty estimation?
- Basis: The authors utilize Weibull variational inference because direct reparameterization of the Gamma distribution results in high noise.
- Why unresolved: While the Weibull distribution is "similar" to the Gamma and allows for reparameterization, it is an approximation. The paper does not quantify the divergence between the true posterior and this approximation or its specific impact on calibration.
- What evidence would resolve it: A comparative study on synthetic data where the true Gamma posterior is known, measuring the KL divergence between the Weibull approximation and the ground truth.

### Open Question 3
- Question: Does the BNDL framework generalize to non-image domains, such as natural language processing, where the assumption of non-negative, additive feature representations may not hold intuitively?
- Basis: The conclusion suggests BNDL can serve as a "versatile tool," but all experimental validation is restricted to image classification datasets (CIFAR, ImageNet) with CNN or ViT backbones.
- Why unresolved: The "multifaceted" neuron problem and the benefits of sparse, non-negative factorization are demonstrated on visual concepts. It is unclear if text embeddings or sequential features benefit from the same non-negative disentanglement constraints.
- What evidence would resolve it: Application of BNDL to transformer-based NLP tasks (e.g., BERT fine-tuning) to evaluate if interpretability and uncertainty metrics improve over standard dense decision layers.

## Limitations
- The core mechanism relies on reformulating DNNs as a conditional Bayesian non-negative factor analysis, which assumes the underlying data can be effectively modeled by this structure. This assumption may not hold for all datasets or feature spaces.
- The use of Weibull variational inference to approximate Gamma-distributed posteriors introduces an approximation gap that could affect uncertainty estimation quality, especially if the true posterior is multimodal.
- The theoretical guarantees for disentangled learning rely on partial identifiability conditions (selective window assumption and sparsity constraints) that may not be met in practice.

## Confidence
- High confidence: The experimental results showing improved accuracy (e.g., 79.82% on CIFAR-100 vs. 74.62% baseline) and the general mechanism of Bayesian treatment for uncertainty estimation.
- Medium confidence: The interpretability improvements and disentanglement claims, as they rely on specific data properties and the effectiveness of the non-negativity constraint.
- Medium confidence: The Weibull variational inference approach, as it's a novel choice with limited corpus evidence for this specific application.

## Next Checks
1. Conduct an ablation study on CIFAR-10 using different sparsity hyperparameters (α) to quantify the trade-off between accuracy and interpretability metrics (SEPIN@k), replicating the analysis in Figure 3.
2. Perform a qualitative analysis on ImageNet by visualizing LIME explanations for BNDL features and comparing them against a standard ResNet baseline to verify the interpretability claims, as shown in Figure 4.
3. Test BNDL on a dataset where the selective window assumption is likely violated (e.g., a dataset with highly correlated features) to assess the robustness of the identifiability guarantees and disentanglement performance.