---
ver: rpa2
title: Improving GUI Grounding with Explicit Position-to-Coordinate Mapping
arxiv_id: '2510.03230'
source_url: https://arxiv.org/abs/2510.03230
tags:
- grounding
- wang
- tokens
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of GUI grounding, which requires
  mapping natural language instructions to pixel coordinates on a screen. The core
  difficulty is that current approaches rely on implicit position-to-coordinate mappings
  that break when moving to unseen, high-resolution displays.
---

# Improving GUI Grounding with Explicit Position-to-Coordinate Mapping

## Quick Facts
- arXiv ID: 2510.03230
- Source URL: https://arxiv.org/abs/2510.03230
- Reference count: 19
- Primary result: Improves GUI grounding accuracy from 31.1% to 37.2% on ScreenSpot-Pro benchmark

## Executive Summary
This paper addresses the challenge of GUI grounding, where natural language instructions must be mapped to precise pixel coordinates on a screen. The key problem is that existing models rely on implicit position-to-coordinate mappings that fail when applied to unseen, high-resolution displays. The authors propose RULER tokens as explicit coordinate markers and Interleaved MRoPE (I-MRoPE) to improve positional embedding balance across spatial dimensions. Their approach achieves significant performance gains on multiple ScreenSpot benchmarks, demonstrating consistent improvements across different screen resolutions.

## Method Summary
The authors introduce RULER tokens, which explicitly encode coordinate information in the model's input, replacing implicit position-to-coordinate mappings. They also develop Interleaved MRoPE (I-MRoPE), a technique that improves the balance of positional embeddings across spatial dimensions. The approach involves finetuning existing Vision-Language Models (VLMs) with RULER tokens and training from scratch using both RULER and I-MRoPE. This method addresses the resolution generalization problem by making coordinate mappings explicit rather than learned implicitly.

## Key Results
- Achieves 37.2% accuracy on ScreenSpot-Pro benchmark, up from 31.1%
- Demonstrates consistent improvements across ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro benchmarks
- Performance gains are maintained across different screen resolutions
- Adds minimal computational overhead to existing VLM architectures

## Why This Works (Mechanism)
The method works by explicitly encoding spatial coordinate information through RULER tokens rather than relying on implicit positional embeddings. This explicit mapping eliminates the resolution-dependent bias present in traditional approaches. The I-MRoPE component further enhances this by ensuring balanced positional representations across all spatial dimensions, preventing the model from developing resolution-specific patterns that don't generalize.

## Foundational Learning
- **GUI Grounding**: The task of mapping natural language instructions to specific screen coordinates - needed because traditional VLMs lack spatial precision for GUI interaction tasks; quick check: verify the model can locate buttons based on descriptive text.
- **Vision-Language Models (VLMs)**: Neural architectures that process both visual and textual inputs simultaneously - needed to handle the multimodal nature of GUI grounding; quick check: confirm the model can process screen images alongside instruction text.
- **Positional Embeddings**: Vector representations that encode spatial information in neural networks - needed because standard embeddings fail to generalize across resolutions; quick check: test embedding consistency across different screen sizes.
- **Resolution Generalization**: The ability of models to perform consistently across varying screen resolutions - needed because GUI applications must work on diverse display configurations; quick check: validate performance across multiple resolution settings.

## Architecture Onboarding

**Component Map**: Input Screen + Instructions -> RULER Token Encoder -> I-MRoPE Processor -> VLM Backbone -> Coordinate Output

**Critical Path**: Natural language instruction → RULER token encoding → positional embedding enhancement (I-MRoPE) → visual feature extraction → coordinate prediction

**Design Tradeoffs**: Explicit coordinate encoding (RULER) vs. implicit learning provides better generalization but requires architectural modification; I-MRoPE adds complexity but improves spatial consistency

**Failure Signatures**: 
- Poor performance on resolutions significantly different from training data
- Degradation when instructions involve relative positioning
- Reduced accuracy for complex GUI layouts with overlapping elements

**First 3 Experiments to Run**:
1. Test resolution generalization by evaluating on unseen screen sizes
2. Compare RULER-only vs. RULER+I-MRoPE performance to isolate contributions
3. Evaluate on dynamic GUI scenarios with moving elements or changing layouts

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertain generalization to real-world dynamic GUIs beyond ScreenSpot benchmarks
- Potential overfitting to specific ScreenSpot datasets without external validation
- Computational costs of finetuning VLMs with RULER tokens not thoroughly analyzed
- Unverified robustness to varying screen aspect ratios and multi-monitor setups

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Improved accuracy on ScreenSpot benchmarks | High |
| Generalization across resolutions | Medium |
| Minimal computational overhead | Low |

## Next Checks
1. Evaluate the method on real-world dynamic GUI datasets, including multi-monitor and varying aspect ratio setups, to assess robustness and generalization
2. Conduct ablation studies to quantify individual contributions of RULER tokens and I-MRoPE to performance gains
3. Measure computational overhead (memory usage, inference latency) of finetuning VLMs with RULER tokens on larger-scale datasets