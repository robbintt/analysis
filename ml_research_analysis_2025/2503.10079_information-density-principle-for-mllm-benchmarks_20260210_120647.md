---
ver: rpa2
title: Information Density Principle for MLLM Benchmarks
arxiv_id: '2503.10079'
source_url: https://arxiv.org/abs/2503.10079
tags:
- uni00000013
- uni00000011
- uni00000015
- uni00000010
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Information Density Principle to evaluate
  multimodal benchmarks for MLLMs, focusing on four dimensions: Fallacy, Difficulty,
  Redundancy, and Diversity. The study addresses the challenge of selecting suitable
  benchmarks for MLLM evaluation by proposing a theoretical framework based on information
  entropy.'
---

# Information Density Principle for MLLM Benchmarks

## Quick Facts
- arXiv ID: 2503.10079
- Source URL: https://arxiv.org/abs/2503.10079
- Authors: Chunyi Li; Xiaozhe Li; Zicheng Zhang; Yuan Tian; Ziheng Jia; Xiaohong Liu; Xiongkuo Min; Jia Wang; Haodong Duan; Kai Chen; Guangtao Zhai
- Reference count: 40
- This paper introduces the Information Density Principle to evaluate multimodal benchmarks for MLLMs, focusing on four dimensions: Fallacy, Difficulty, Redundancy, and Diversity.

## Executive Summary
This paper addresses the challenge of selecting suitable benchmarks for multimodal large language model (MLLM) evaluation by proposing a theoretical framework based on information entropy. The authors introduce the Information Density Principle, which evaluates benchmarks across four dimensions: Fallacy (ground truth reliability), Difficulty (model challenge level), Redundancy (content overlap), and Diversity (sample variety). Experiments conducted on 19 benchmarks with over 10,000 samples reveal that while the latest benchmarks offer more insights than previous ones, there is still room for improvement in their information density. The paper presents a Human-Model-Data evaluation pipeline to assess these dimensions, providing both design and selection references for MLLM developers and benchmark developers.

## Method Summary
The study evaluates 19 mainstream multimodal benchmarks using a comprehensive pipeline that measures information density across four dimensions. For Difficulty, the protocol uses multiple models (GPT-4o, InternVL-2.5, QwenVL-2.5) to predict both the best option and an alternative, classifying samples where average predictions fail. Redundancy is measured by calculating accuracy when image or text modalities are hidden, using only QwenVL-2.5 due to its unique ability to process incomplete inputs. Diversity is computed through CLIP embeddings, K-means clustering, and semantic deduplication. Fallacy is determined through human-labeled error rates in ground truth. The Data Eval component uses Random Forest models with image features (Laplace, etc.) and text features (syntax depth, CLIP distance) to predict evaluation scores. The entire pipeline processes 1,000 samples per benchmark with circular evaluation across five seeds.

## Key Results
- Latest benchmarks provide more insights than previous ones but still have room for improvement in information density
- There is a "mutual antagonism" between benchmark dimensions, where optimizing one dimension inevitably lags in others
- Diversity has shown no significant improvement over time despite advances in other dimensions
- The MME-Realworld benchmark demonstrates high Difficulty but has Fallacy rates approaching 0.5

## Why This Works (Mechanism)
The Information Density Principle works by applying information entropy theory to benchmark evaluation, quantifying the amount of unique, challenging, and reliable information contained in each benchmark. By decomposing information density into four orthogonal dimensions (Fallacy, Difficulty, Redundancy, and Diversity), the framework provides a comprehensive assessment that goes beyond simple accuracy metrics. The circular evaluation with multiple seeds and cross-model validation ensures robustness in the measurements, while the Human-Model-Data pipeline allows for both resource-intensive precise evaluation and automated screening.

## Foundational Learning
- **Information Entropy Theory**: Understanding how entropy quantifies uncertainty and information content in multimodal data
  - Why needed: Forms the theoretical foundation for defining and measuring the four dimensions
  - Quick check: Verify that higher Difficulty correlates with lower model prediction accuracy

- **Semantic Deduplication**: Techniques for identifying and removing semantically similar samples
  - Why needed: Critical for accurate Diversity measurement to avoid inflated diversity scores
  - Quick check: Confirm that CLIP embedding distances align with human semantic similarity judgments

- **Modality Ablation**: Systematically removing image or text inputs to test model dependence
  - Why needed: Essential for measuring Redundancy by identifying samples solvable with single modalities
  - Quick check: Ensure models consistently fail on ablated samples that humans can solve

- **Multi-dimensional Evaluation**: Framework for simultaneously assessing multiple quality dimensions
  - Why needed: Prevents over-optimization of single metrics at the expense of overall benchmark quality
  - Quick check: Verify that high-performing benchmarks maintain balance across all four dimensions

## Architecture Onboarding

Component Map:
Human Eval -> Model Eval (GPT-4o, InternVL-2.5, QwenVL-2.5) -> Data Eval (Random Forest) -> Final Information Density Scores

Critical Path:
1. Sample 1,000 instances per benchmark with fixed seed alignment
2. Run Model Eval pipeline (Difficulty, Redundancy, Diversity calculations)
3. Perform Human Eval for Fallacy ground truth
4. Train Data Eval models on low-level features
5. Generate final benchmark rankings and insights

Design Tradeoffs:
- Resource Intensity vs. Accuracy: Model Eval provides most accurate results but requires 70B+ parameter models
- Automation vs. Reliability: Data Eval offers scalability but currently lacks human-level precision
- Modality Completeness vs. Model Compatibility: Redundancy measurement limited to QwenVL-2.5 due to refusal issues

Failure Signatures:
- Low Redundancy scores with high Difficulty may indicate benchmarks requiring multimodal integration
- High Fallacy rates suggest unreliable ground truth annotations
- Stagnant Diversity scores despite new benchmarks indicate dataset collection limitations

First Experiments:
1. Replicate the 1,000-sample extraction process for a single benchmark to verify sampling methodology
2. Run single-model Difficulty evaluation (GPT-4o) on a small benchmark subset to test pipeline functionality
3. Implement CLIP embedding extraction and K-means clustering for Diversity calculation on sample images

## Open Questions the Paper Calls Out
- How can the accuracy of the automated "Data Eval" paradigm be improved to match human-level assessment without relying on resource-intensive "Model Eval" inference?
- Can the observed "mutual antagonism" between benchmark dimensions be resolved, specifically allowing for a benchmark that is simultaneously high in Difficulty and Diversity while remaining low in Fallacy?
- What specific mechanisms are causing the stagnation of Diversity in recent MLLM benchmarks, and how can this trend be reversed?

## Limitations
- The Redundancy scores are completely dependent on QwenVL-2.5's unique ability to process incomplete modalities without refusal
- The pipeline requires hosting two 70B+ parameter models simultaneously, creating significant computational barriers
- Semantic deduplication hyperparameters (K value and τ threshold) are not explicitly defined in the main text
- The current Data Eval method uses simple random forest models that may fail to capture complex semantic nuances

## Confidence
- **High Confidence**: The theoretical framework connecting information entropy to the four dimensions is internally consistent and mathematically sound
- **Medium Confidence**: The relative rankings between benchmarks are likely reliable, as multiple models show similar patterns
- **Low Confidence**: The Redundancy scores are particularly uncertain due to their complete dependence on QwenVL-2.5's unique behavior

## Next Checks
1. Examine the repository code to extract the exact K-means clustering parameters (K value) and semantic deduplication threshold (τ) used for Diversity calculations
2. Test the Redundancy calculation pipeline with multiple models (including those that refuse incomplete inputs) to establish whether the scores are truly intrinsic to the benchmarks
3. Systematically vary the prompt templates for "best option and alternative" prediction across a small subset of benchmarks to quantify the impact on Difficulty classifications