---
ver: rpa2
title: 'Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge
  Editing in Same-Subject'
arxiv_id: '2502.06868'
source_url: https://arxiv.org/abs/2502.06868
tags:
- editing
- knowledge
- rdfs
- wordnet
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the challenge of editing multiple related
  knowledge pieces for the same subject in large language models. Through experiments
  with the proposed S2RKE benchmark, it identifies a phenomenon called "related knowledge
  perturbation," where subsequent edits interfere with earlier ones, causing performance
  degradation.
---

# Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject

## Quick Facts
- arXiv ID: 2502.06868
- Source URL: https://arxiv.org/abs/2502.06868
- Reference count: 40
- This paper identifies "related knowledge perturbation" where editing multiple facts about the same subject causes earlier edits to degrade in subsequent locate-then-edit methods.

## Executive Summary
This paper investigates a critical failure mode in knowledge editing for large language models: when multiple related facts about the same subject are edited sequentially, later edits interfere with earlier ones, causing performance degradation. Through the S2RKE benchmark, the authors demonstrate that mainstream locate-then-edit methods like ROME and MEMIT suffer from this "related knowledge perturbation" phenomenon due to their over-reliance on subject information when computing edit keys. The analysis reveals that these methods produce highly similar keys for same-subject edits, leading to interference in the parameter space. In contrast, non-locate-then-edit methods show minimal sensitivity to this issue. The findings highlight fundamental limitations in current knowledge editing approaches and call for new solutions to handle same-subject editing scenarios.

## Method Summary
The study compares six knowledge editing methods (ROME, MEMIT, PMET, FT, MEND, KN) on three language models (GPT-2 XL, GPT-J, LLaMA-2) using the S2RKE benchmark. Experiments evaluate both sequential-editing (batch_size=1) and batch-editing (consecutive_edits=1) strategies across different subject densities (High: 3 edits/subject, Medium: 2, Low: 1). The benchmark measures Efficacy Success (ES), Paraphrase Success (PS), Neighborhood Success (NS), and Score (S = harmonic mean of ES, PS, NS). The study introduces Homogeneous-Editing (first and last edits target same subject) vs Heterogeneous-Editing (different subjects) to isolate related knowledge perturbation effects.

## Key Results
- Locate-then-edit methods (ROME, MEMIT, PMET) show significant performance degradation when editing multiple facts about the same subject, with Score Difference (SD) values strongly negative
- Same-subject edits produce highly similar keys (cosine similarity approaching 1.0), causing interference between edits in parameter space
- Non-locate-then-edit methods (FT, MEND, KN) show minimal sensitivity to subject density, maintaining stable performance across editing scenarios
- Related knowledge perturbation occurs in both sequential and batch editing strategies, though the interference mechanism differs

## Why This Works (Mechanism)

### Mechanism 1
Locate-then-edit methods store factual associations as key-value pairs in MLP weight matrices, where the key derives primarily from subject token representations. These methods solve W·k = v, where k* is computed as the mean of MLP layer outputs across sampled prefixes concatenated with the subject prompt. This design assumes MLP modules function as linear key-value associative memories for factual knowledge.

### Mechanism 2
Related knowledge perturbation occurs because same-subject edits produce highly similar keys, causing subsequent edits to overwrite or interfere with earlier modifications. When editing multiple facts (s, r₁, o₁*), (s, r₂, o₂*) for the same subject s, the computed keys k₁* and k₂* are nearly identical since both derive from subject s's token representation. The weight update ΔW for the second edit affects the same parameter subspace as the first, degrading the first edit's efficacy.

### Mechanism 3
Non-locate-then-edit methods show minimal related knowledge perturbation because they use different parameter modification strategies that don't rely on subject-derived key addressing. Fine-tuning directly optimizes loss with parameter constraints; MEND uses hypernetworks to predict gradient decompositions; KN modifies specific neuron rows rather than solving key-value equations. These approaches distribute updates differently across parameters, avoiding concentrated interference in subject-specific subspaces.

## Foundational Learning

- **Concept: Locate-then-edit paradigm (ROME, MEMIT)**
  - Why needed here: Understanding how these methods identify knowledge storage locations (causal tracing → MLP layers) and compute parameter updates (key-value pair optimization) is essential for diagnosing why same-subject edits interfere.
  - Quick check question: Given a fact (s, r, o), what component of the Transformer does ROME identify as the primary storage location, and what mathematical operation does it use to update this knowledge?

- **Concept: Key-value associative memory in MLPs**
  - Why needed here: The paper's core diagnosis hinges on the Geva et al. (2020) hypothesis that MLP layers function as key-value memories. Understanding this abstraction explains why key similarity leads to value interference.
  - Quick check question: In the equation W·k = v for an MLP down-projection layer, what do k and v represent in terms of a factual triplet (subject, relation, object)?

- **Concept: Sequential vs. batch editing strategies**
  - Why needed here: The paper evaluates both strategies and finds perturbation occurs in both, but the interference mechanism differs (sequential: temporal overwriting; batch: simultaneous optimization conflicts).
  - Quick check question: In sequential editing with batch_size=1, how does the model state after edit N affect the computation of edit N+1? What constraint does batch editing relax?

## Architecture Onboarding

- **Component map:**
  Input Prompt (s, r, ?) → [Embedding Layer] → [Transformer Layers 1-L] → [LM Head] → Output logits
  Critical edit targets: W_proj in early-to-middle MLP layers (ROME: single layer; MEMIT: layers 3-8 in GPT-J)

- **Critical path:**
  1. Causal tracing → Identifies which layer(s) store the target fact
  2. Key computation → Extracts subject representation: k* = mean(K(prefixes + subject))
  3. Value computation → Encodes (relation, new_object) association: v*
  4. Weight update → Solves least-squares for ΔW such that (W + ΔW)k* = v*
  5. Interference point → If prior edit used similar k, ΔW_new partially overwrites ΔW_old

- **Design tradeoffs:**
  - Subject-only keys vs. relation-aware keys: Current design ensures subject coherence but causes same-subject collision; incorporating relation tokens would reduce interference but may fragment subject knowledge.
  - Single-layer vs. multi-layer updates: ROME (single layer) is more precise but more fragile; MEMIT (multi-layer) distributes updates but increases collision surface.
  - Sequential vs. batch: Sequential is simpler but accumulates errors; batch enables joint optimization but requires pre-knowledge of all edits.

- **Failure signatures:**
  - Efficacy Success (ES) drops for earlier edits in same-subject sequences → Key collision causing value overwriting
  - Score Difference (SD) strongly negative → Related knowledge perturbation (e.g., MEMIT on GPT-J shows SD ≈ -0.4)
  - Neighborhood Success (NS) stable while ES degrades → Specificity preserved but targeted edits failing
  - Cosine similarity of keys ≈ 1.0 for same-subject edits → Diagnostic confirmation of perturbation source

- **First 3 experiments:**
  1. Reproduce perturbation baseline: Run MEMIT sequential-editing on GPT-J using S2RKE benchmark with 2 same-subject edits vs. 2 different-subject edits. Measure ES for first edit and key cosine similarity. Expected: ES drops ~20-30% for same-subject; cosine similarity >0.95.
  2. Key composition ablation: Modify MEMIT key computation to include relation token: k* = mean(K(prefixes + subject + relation)). Compare same-subject edit performance against original. This tests whether relation-aware keys reduce perturbation.
  3. Layer-wise interference mapping: For a pair of same-subject edits, trace which specific neurons/weights in W_proj receive updates from both edits. Measure overlap ratio (Jaccard similarity of affected parameter sets). High overlap confirms spatial collision hypothesis.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the "related knowledge perturbation" phenomenon be effectively mitigated in locate-then-edit methods to allow successful sequential editing for the same subject? The authors explicitly call for future research to develop mitigation strategies, as they do not propose a comprehensive solution.

- **Open Question 2**: Does the "related knowledge perturbation" phenomenon persist or worsen in Large Language Models with parameter counts significantly larger than 7B? The study is restricted to models up to 7B parameters, leaving uncertainty about how the effect scales.

- **Open Question 3**: Can the key calculation mechanism in locate-then-edit methods be re-engineered to incorporate relation information, thereby preventing the high cosine similarity that causes interference? The paper diagnoses the over-reliance on subject information but does not experiment with alternative key formulations.

## Limitations

- The mechanistic explanation relies heavily on the assumption that MLP layers function as linear key-value associative memories, which has not been independently verified for the specific models examined.
- The S2RKE benchmark's focus on YAGO3.0.3 triples may not generalize to more diverse knowledge domains or complex factual structures.
- Comparative analysis between locate-then-edit and non-locate-then-edit methods is constrained by different parameter scales these methods typically operate on, making direct performance comparison challenging.

## Confidence

- **Low**: The core phenomenon is well-demonstrated, but the mechanistic explanation relies on unverified assumptions about knowledge distribution in MLPs.
- **Medium**: The benchmark construction introduces potential confounding factors, and the study's scope is limited to specific knowledge types.
- **Medium**: Architectural differences between editing methods make direct comparisons difficult, as they may be solving different sub-problems.

## Next Checks

1. **Key-Value Distribution Analysis**: Conduct comprehensive examination of factual knowledge distribution across model layers using causal tracing to validate or challenge the key similarity hypothesis.

2. **Relation-Aware Key Modification**: Implement modified MEMIT that incorporates relation tokens into key computation and compare same-subject editing performance to determine if relation-aware keys reduce perturbation.

3. **Cross-Domain Generalization Test**: Evaluate related knowledge perturbation on a more diverse knowledge base beyond YAGO triples to determine whether the phenomenon is universal or specific to certain knowledge types.