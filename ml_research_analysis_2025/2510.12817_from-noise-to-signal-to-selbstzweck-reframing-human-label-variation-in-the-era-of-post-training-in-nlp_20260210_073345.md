---
ver: rpa2
title: 'From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the
  Era of Post-training in NLP'
arxiv_id: '2510.12817'
source_url: https://arxiv.org/abs/2510.12817
tags:
- human
- alignment
- computational
- systems
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This position paper argues that human label variation (HLV)\u2014\
  legitimate disagreement among annotators\u2014should be preserved as a Selbstzweck\
  \ (intrinsic value) in preference dataset construction for LLM alignment. The authors\
  \ trace HLV's evolution from noise to signal, then advocate elevating it as essential\
  \ for pluralistic alignment."
---

# From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP

## Quick Facts
- arXiv ID: 2510.12817
- Source URL: https://arxiv.org/abs/2510.12817
- Reference count: 32
- Human label variation (HLV) should be preserved as intrinsic value in LLM alignment datasets

## Executive Summary
This position paper argues that human label variation (HLV)—legitimate disagreement among annotators—should be preserved as a Selbstzweck (intrinsic value) in preference dataset construction for LLM alignment. The authors trace HLV's evolution from noise to signal, then advocate elevating it as essential for pluralistic alignment. They identify current datasets' limitations in flattening diverse perspectives through aggregation and propose actionable strategies: releasing annotator-level preferences, leveraging diverse feedback types, carefully selecting annotator pools to reflect target populations, and documenting pluralistic reasoning alongside final decisions. The paper emphasizes that preserving HLV is crucial for capturing pluralistic human values and enhancing sociotechnical safety evaluation.

## Method Summary
This is a conceptual position paper that synthesizes existing research on human label variation and proposes strategies for preserving it in preference datasets for LLM alignment. The authors review the evolution of HLV from being treated as noise to signal, then argue it should be treated as intrinsic value (Selbstzweck). They propose four strategies: (1) release annotator-level preferences rather than aggregating, (2) leverage diverse feedback types beyond binary comparisons, (3) carefully select annotator pools to reflect target populations, and (4) document pluralistic reasoning alongside final decisions. While the paper references existing datasets like HelpSteer2 and MultiPref, it does not present new empirical experiments or implementations.

## Key Results
- Current preference-learning datasets routinely aggregate multiple annotations into single labels, flattening diverse perspectives
- 30%+ of examples in datasets like HelpSteer2 contain disagreement, with <25% attributable to annotation error
- Survey methodology offers guidance for managing annotator selection bias through stratified quotas and multi-channel recruitment
- Preserving annotator-level labels enables distributional reward modeling that captures preference heterogeneity

## Why This Works (Mechanism)

### Mechanism 1
Preserving annotator-level preferences enables distributional reward modeling that captures preference heterogeneity. Individual annotations maintain the variance structure of human disagreement; downstream models can learn conditional distributions rather than point estimates, allowing outputs to reflect legitimate pluralism instead of forced consensus. Core assumption: The disagreement captured is systematic HLV (legitimate variation), not annotation error. Break condition: If disagreement is predominantly noise (not HLV), preserving it may introduce harmful variance without signal gain.

### Mechanism 2
Annotator pool composition directly determines which human perspectives are represented in the preference signal. Selecting annotators through stratified sampling aligned with target populations ensures that the distribution of preferences reflects intended demographics; opaque or narrow pools risk embedding unexamined biases into the reward signal. Core assumption: The target population and its relevant demographic stratification can be clearly defined for the alignment task. Break condition: If demographic categories do not meaningfully predict preference variation, stratified sampling adds cost without benefit.

### Mechanism 3
Heterogeneous feedback protocols (ratings, pairwise comparisons, demonstrations) capture orthogonal dimensions of preference that any single protocol misses. Binary pairwise comparisons favor assertive responses and collapse nuance; scalar ratings preserve intensity; demonstrations encode behavioral preferences—combining them provides richer signal for pluralistic alignment. Core assumption: Each feedback type captures distinct, complementary information about human preferences that survives integration. Break condition: If feedback types are redundant or conflict irreconcilably, integration complexity may not justify gains.

## Foundational Learning

- **Human Label Variation (HLV) vs. Annotation Error**
  - Why needed here: The paper's central argument depends on distinguishing legitimate disagreement (which should be preserved) from mistakes (which should be corrected); conflating them leads to either over-aggregation or under-cleaning.
  - Quick check question: Given an annotator disagreement, can you articulate three potential causes and classify each as HLV or error?

- **RLHF and Preference Learning Pipeline**
  - Why needed here: The paper targets preference dataset construction for post-training; understanding how reward models consume annotations clarifies why aggregation shape matters downstream.
  - Quick check question: Sketch the data flow from raw annotator labels → reward model → policy optimization; where does label aggregation occur?

- **Sociotechnical Evaluation Layers**
  - Why needed here: The paper frames HLV preservation as essential for systemic safety evaluation, not just capability testing; this requires understanding evaluation beyond model accuracy.
  - Quick check question: For a hate speech classifier, give one evaluation question at each layer: capability, human interaction, systemic impact.

## Architecture Onboarding

- Component map: Target Population Definition → Annotator Pool Selection → Feedback Protocol Design → Annotation Collection (preserve individual labels) → Preference Dataset → Distributional Reward Modeling → Conditional Policy → Pluralistic Evaluation

- Critical path: 1. Define whose perspectives matter (target population) 2. Recruit annotators reflecting that population (stratified, documented) 3. Collect multiple feedback types per instance 4. Do not aggregate—store annotator-level labels with metadata 5. Train reward models that predict distributions, not scalars

- Design tradeoffs: Transparency vs. privacy (releasing annotator-level labels enables research but may expose identities); Diversity vs. coherence (heterogeneous pools increase HLV but may produce inconsistent reward signals); Cost vs. coverage (multiple feedback protocols per example increases annotation cost substantially)

- Failure signatures: "False consensus" (>70% agreement on items where population surveys show genuine disagreement); "Minority regret" (systematic utility loss for underrepresented demographic groups in held-out evaluation); "Flat reward distribution" (reward model outputs near-identical scores across diverse inputs, suggesting signal loss)

- First 3 experiments: 1. HLV audit: Take an existing preference dataset (e.g., HelpSteer2); compute per-item agreement rates; manually code a sample to estimate HLV vs. error proportion. 2. Aggregation ablation: Train reward models on aggregated vs. annotator-level labels from the same data; compare performance on minority-preference held-out set. 3. Feedback protocol comparison: For the same prompt-response pairs, collect both pairwise comparisons and scalar ratings; measure correlation and identify systematic divergence cases.

## Open Questions the Paper Calls Out

### Open Question 1
How can evaluation metrics like "pluralism fidelity" and "minority regret" be operationalized and validated to measure whether models faithfully capture the distribution of human preferences rather than collapsed consensus? The authors explicitly identify these as future work: "future work could explore new evaluation metrics, for example by developing metrics such as: Pluralism fidelity... Minority regret..." No empirical validation or comparison of these metrics against existing alignment evaluation methods has been provided.

### Open Question 2
What specific strategies from survey methodology (stratified quotas, multi-channel recruitment, post-stratification weighting) most effectively mitigate annotator selection bias when constructing pluralistic preference datasets? The paper advocates borrowing from survey methodology but does not provide empirical evidence that these techniques actually improve downstream model alignment or reduce bias in preference datasets.

### Open Question 3
How can alignment algorithms be modified to model HLV distributionally rather than collapsing it through scalarization, while remaining computationally tractable? While the paper reviews multi-objective RLHF approaches, it notes these "often obscure minority preferences or assume static, universal objectives." The practical implementation of distributional modeling that preserves minority viewpoints remains unspecified.

### Open Question 4
How should AI systems preserve pluralism through reasoning documentation when a single operative decision is unavoidable, and what are the trade-offs between preserving dissenting opinions and decision utility? The paper provides the legal analogy but does not specify how this would be implemented technically, how dissenting perspectives would be represented, or whether preserving such documentation improves user outcomes or accountability in practice.

## Limitations
- The paper presents a conceptual argument without empirical validation of the proposed mechanisms
- Causal links between annotator pool diversity and downstream alignment quality are inferred from survey methodology rather than demonstrated empirically
- No specific model architectures, training procedures, or evaluation protocols are provided for the proposed metrics

## Confidence

**Major Uncertainties and Limitations**
- High: The conceptual framework distinguishing HLV from annotation error and the critique of aggregation practices are well-grounded in existing literature
- Medium: The mechanisms linking HLV preservation to improved pluralistic alignment are plausible but under-tested; related work shows promise but lacks conclusive superiority evidence
- Low: Specific implementation details for proposed strategies (e.g., exact demographic stratification methods, heterogeneous feedback integration protocols) are underspecified

## Next Checks

1. **HLV Audit**: Take an existing preference dataset (e.g., HelpSteer2); compute per-item agreement rates; manually code a sample to estimate HLV vs. error proportion
2. **Aggregation Ablation**: Train reward models on aggregated vs. annotator-level labels from the same data; compare performance on minority-preference held-out set
3. **Feedback Protocol Comparison**: For the same prompt-response pairs, collect both pairwise comparisons and scalar ratings; measure correlation and identify systematic divergence cases