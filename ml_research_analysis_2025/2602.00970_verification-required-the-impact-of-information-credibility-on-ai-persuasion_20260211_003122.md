---
ver: rpa2
title: 'Verification Required: The Impact of Information Credibility on AI Persuasion'
arxiv_id: '2602.00970'
source_url: https://arxiv.org/abs/2602.00970
tags:
- receiver
- sender
- verification
- information
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MixTalk, a strategic communication game for
  LLM-to-LLM interaction that models information credibility. In MixTalk, a sender
  agent strategically combines verifiable and unverifiable claims to communicate private
  information, while a receiver agent allocates a limited budget to costly verification
  and infers the underlying state from prior beliefs, claims, and verification outcomes.
---

# Verification Required: The Impact of Information Credibility on AI Persuasion

## Quick Facts
- arXiv ID: 2602.00970
- Source URL: https://arxiv.org/abs/2602.00970
- Reference count: 28
- Key outcome: MixTalk game reveals inverse sender-receiver competence trade-off and improves receiver robustness via TOPD distillation

## Executive Summary
This paper introduces MixTalk, a strategic communication game modeling information credibility for LLM-to-LLM interaction. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state. The study evaluates state-of-the-art LLM agents across recruitment, insurance claims, and used-car listing scenarios, revealing that models excelling as senders tend to perform poorly as receivers, with non-transitive interactions and pronounced role specialization. The paper proposes Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time, significantly improving receiver robustness to persuasion.

## Method Summary
The MixTalk environment defines private multi-attribute states with verifiable and unverifiable attributes, where senders construct messages mixing both claim types and receivers allocate limited verification budgets. Five LLM agents (gpt-5m, grok-4.1f, kimi-k2t, gem-3fpr, gpt-4.1m) are evaluated in round-robin tournaments across 30 environment variants using ReAct-style reasoning loops. Performance is measured through Bradley-Terry scores, α-Rank, and Tournament Oracle Regret (TOR), with behavioral metrics tracking strategic patterns like omission, fabrication, and verification propensity. TOPD improves receiver performance by distilling high-performing tournament behaviors into statistical summaries injected as in-context guidance with budget headroom.

## Key Results
- Inverse sender-receiver competence trade-off: models strong as senders perform poorly as receivers and vice versa
- Non-transitive interactions observed with role specialization across tournament environments
- TOPD reduces regret by 23.57% and increases utility by 6.47% in large environments
- Credibility constraints shift strategic behavior toward fabrication, exaggeration, or omission depending on verification costs

## Why This Works (Mechanism)

### Mechanism 1: Inverse Sender-Receiver Competence Trade-off
- Claim: Models that excel as persuasive senders tend to perform poorly as skeptical receivers, reflecting a fundamental offense-defense asymmetry in strategic reasoning.
- Mechanism: Persuasion requires generating coherent narratives that exploit verification gaps, while receiving requires calibrated distrust, cost-aware verification, and conservative belief updates. These demand opposing reasoning modes that do not transfer.
- Core assumption: Strategic communication skills are role-specialized rather than generalizable across perspectives.
- Evidence anchors:
  - [abstract] "models excelling as senders tend to perform poorly as receivers, and vice versa, with non-transitive interactions and pronounced role specialization"
  - [section] Table 3: kimi-k2t achieves BT=1.38 (combined sender) but only 0.74 (combined receiver); gem-3fpr shows the inverse (0.64 sender, 1.66 receiver)
  - [corpus] Weak direct evidence; related papers (Bakhtin et al., Akata et al.) study coordination and repeated games but not role specialization under credibility constraints
- Break condition: If models are explicitly trained with counterfactual role-switching or if verification costs are negligible (collapsing to disclosure).

### Mechanism 2: TOPD Distills Tournament Oracle into In-Context Guidance
- Claim: Offline extraction of high-performing tournament behaviors into statistical summaries, injected as prompts, reduces regret without weight updates.
- Mechanism: TOPD samples oracle episodes (best receiver per sender-episode), filters by opponent utility to prioritize robustness, computes structure-aware statistics (verification propensity, budget usage), and injects these as explicit guidance with a 1.25× budget headroom heuristic.
- Core assumption: Tournament oracles encode recoverable strategic patterns that generalize across episodes and can be compressed into prompt-usable summaries.
- Evidence anchors:
  - [abstract] "TOPD significantly improves receiver robustness to persuasion, reducing regret and increasing utility across environments"
  - [section] Table 5: Large environment shows −23.57% TOR, +6.47% utility, +18.68% frugality improvement
  - [corpus] Shinn et al. (Reflexion) and Zhang et al. demonstrate effective prompt-based adaptation from execution traces
- Break condition: When tournament coverage is sparse, when oracle policies are highly heterogeneous, or when the task structure shifts significantly from the tournament distribution.

### Mechanism 3: Credibility Constraints Shift Strategic Behavior
- Claim: Partial verifiability creates a strategic middle ground where fabrication, exaggeration, and omission are differentially optimal depending on verification costs and tool reliability.
- Mechanism: When verification is costly or noisy, senders can safely omit or exaggerate unverifiable attributes; punishment for detected lies on verifiable attributes shifts strategy toward selective disclosure and soft persuasion. Receivers compensate with increased pessimism and paranoia as credibility weakens.
- Core assumption: LLM agents reason about incentive structures and adjust strategies accordingly.
- Evidence anchors:
  - [section] Figure 4: CheapTalk variant shows highest fabrication/exaggeration; Disclosure variant shifts to strategic omission; MixTalk sits between
  - [abstract] "credibility mechanisms shape strategies, shifting agents toward fabrication, exaggeration, or omission depending on verifiability"
  - [corpus] Dasgupta (2023) provides theoretical foundation for mixed soft/hard information; limited empirical LLM work in this regime
- Break condition: When agents fail to model verification incentives, or when verification is uniformly cheap (full unraveling) or uniformly unavailable (pure cheap-talk).

## Foundational Learning

- Concept: **Cheap-talk vs. Disclosure Games**
  - Why needed here: MixTalk is explicitly designed as a hybrid between these classical extremes. Without understanding why cheap-talk collapses into uninformative pools and why disclosure drives unraveling, you cannot interpret the behavioral results or design appropriate counterfactuals.
  - Quick check question: In Crawford & Sobel (1982), why does cheap-talk become uninformative when sender-receiver preferences diverge? In Milgrom (1981), why does full verifiability lead to progressive revelation?

- Concept: **ReAct-style Agent Architecture**
  - Why needed here: The paper evaluates off-the-shelf LLMs using a ReAct reasoning loop (interleaved thought-action-observation). Understanding this is essential for interpreting agent traces and diagnosing where strategic reasoning fails.
  - Quick check question: In a ReAct loop, how does an agent decide between calling a verification tool versus producing a final estimate? What happens if the reasoning trace is truncated?

- Concept: **Empirical Game-Theoretic Metrics (Bradley–Terry, α-Rank, Regret)**
  - Why needed here: Performance is opponent-dependent and non-transitive; simple win rates are misleading. The paper uses multiple complementary metrics and introduces Tournament Oracle Regret (TOR) as a best-in-tournament surrogate.
  - Quick check question: Why does the paper report both Bradley–Terry scores and α-Rank? What does TOR measure that mean utility does not?

## Architecture Onboarding

- Component map:
  Environment Layer (30 variants) -> Agent Layer (5 LLMs × sender/receiver) -> Tournament Layer (round-robin, 90 episodes) -> Analysis Layer (BT, α-Rank, TOR, behavioral metrics) -> Improvement Layer (TOPD distillation and injection)

- Critical path:
  1. Define environment config (attributes, costs, priors, story mapping)
  2. Sample θ from prior; sender generates claims + statement
  3. Receiver runs ReAct loop: call tools up to budget, then output estimate
  4. Compute utilities (accuracy − verification cost for receiver; persuasion − claim cost for sender)
  5. Aggregate across tournament; compute rankings and behavioral profiles
  6. For TOPD: extract oracle episodes, summarize, inject into receiver prompt, re-evaluate

- Design tradeoffs:
  - Episode alignment ensures fair comparison but may reduce statistical independence across conditions
  - Tournament oracle is tractable but approximates true best-response; TOR underestimates actual regret
  - Statistical summaries in TOPD are cheap but may miss nuanced strategic patterns; LLM-based summarization could capture more but adds cost and variability

- Failure signatures:
  - gpt-4.1m: Non-reasoning model shows near-random receiver behavior (BT≈0.07, TOR≈0.86–0.97), minimal strategic communication as sender
  - High paranoia: gpt-5m receiver shows elevated paranoia (0.22), over-disbelieving truthful claims, trading accuracy for caution
  - Low frugality: grok-4.1f receiver has highest judgment but overspends on verification; TOPD targets this explicitly
  - Excessive omission: gem-3fpr sender omits >50% of attributes, resulting in high receiver pessimism and poor persuasion

- First 3 experiments:
  1. Minimal tournament validation: Run 2 models × 2 environment variants × 20 episodes each. Verify episode alignment, payoff matrix construction, and that behavioral metrics extract correctly from traces
  2. Behavioral profile extraction: Implement the 9 behavioral metrics from Table 1. Validate on a single episode trace (e.g., Appendix D) before scaling
  3. TOPD ablation: Apply TOPD to the worst-performing receiver (e.g., gpt-4.1m) using only the statistical summary. Compare TOR reduction vs. the reported grok-4.1f improvement to test whether gains scale with baseline competence

## Open Questions the Paper Calls Out
None

## Limitations
- Behavioral mechanism claims rely on opponent-dependent performance but underlying trace-level data is not publicly available for independent verification
- TOPD's improvement depends on stable oracle policies and sufficient tournament coverage; sensitivity to oracle heterogeneity not tested
- Behavioral metrics extraction logic and threshold definitions are not fully specified, limiting reproducibility

## Confidence

- **High**: The inverse sender-receiver competence trade-off is directly observable in reported BT scores and TOR values; pattern is internally consistent and statistically supported
- **Medium**: TOPD mechanism is technically coherent and shows significant utility gains, but offline oracle distillation step is not independently validated with ablation or alternative methods
- **Medium**: Strategic behavioral shifts under credibility constraints are plausible given incentive structure, but claim that agents "reason about incentives" is inferred from outcomes rather than directly measured

## Next Checks
1. Trace-level behavioral validation: Reconstruct the 9 behavioral metrics from a sample episode trace (e.g., Appendix D) to confirm extraction logic matches reported profiles before scaling to full tournaments
2. TOPD sensitivity analysis: Test TOPD with alternative distillation targets (e.g., average receiver policy instead of tournament oracle) to isolate contribution of oracle-based selection step
3. Distributional robustness test: Evaluate TOPD receivers on MixTalk variants not included in original tournament (e.g., different attribute sizes or cost structures) to assess generalization beyond training distribution