---
ver: rpa2
title: 'Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs'
arxiv_id: '2511.22402'
source_url: https://arxiv.org/abs/2511.22402
tags:
- uncertainty
- layers
- epistemic
- linguistic
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) represent
  linguistic uncertainty, particularly epistemic modality cues in clinical text. Unlike
  prior work on uncertainty quantification, it probes input-side sensitivity to uncertainty
  by analyzing activation-level changes across transformer layers.
---

# Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs
## Quick Facts
- arXiv ID: 2511.22402
- Source URL: https://arxiv.org/abs/2511.22402
- Reference count: 7
- Key outcome: This paper investigates how large language models (LLMs) represent linguistic uncertainty, particularly epistemic modality cues in clinical text. Unlike prior work on uncertainty quantification, it probes input-side sensitivity to uncertainty by analyzing activation-level changes across transformer layers. The authors introduce a novel metric, Model Sensitivity to Uncertainty (MSU), which measures layerwise activation shifts between certain and uncertain sentence variants. They curate a dataset of 3,114 clinical sentence pairs differing only in epistemic markers (e.g., "must" vs. "might") and evaluate three small LLMs (Qwen2.5-0.5B-Instruct, Qwen1.5-0.5B-Chat, LLaMA-3.2-1B-Instruct). Results show that sensitivity to linguistic uncertainty is a progressive, depth-dependent phenomenon, with MSU scores increasing monotonically in later layers, indicating that epistemic modality is encoded as high-level semantic features. PCA visualizations further reveal that uncertainty-related signals become more linearly separable in deeper layers, suggesting late-stage semantic reorganization.

## Executive Summary
This study introduces a novel approach to understanding how LLMs encode linguistic uncertainty by examining activation-level changes across transformer layers. The authors focus specifically on epistemic modality in clinical text, creating a curated dataset of sentence pairs that differ only in uncertainty markers. They develop the Model Sensitivity to Uncertainty (MSU) metric to quantify how activation patterns change when certain sentences are transformed into uncertain variants. Their findings reveal that sensitivity to uncertainty is not uniform across layers but increases progressively in later layers, suggesting that epistemic modality is processed as a high-level semantic feature rather than an early syntactic element.

## Method Summary
The authors construct a dataset of 3,114 clinical sentence pairs where each pair differs by only one epistemic modality marker (e.g., "must" vs. "might"). They evaluate three small LLMs (Qwen2.5-0.5B-Instruct, Qwen1.5-0.5B-Chat, LLaMA-3.2-1B-Instruct) by feeding both certain and uncertain variants of each sentence pair through the models. Using their novel MSU metric, they measure layerwise activation differences between certain and uncertain sentence pairs. The MSU score quantifies how much the activation pattern changes when epistemic modality shifts, with higher scores indicating greater sensitivity to uncertainty. They complement this quantitative analysis with PCA visualizations to examine how uncertainty-related signals evolve across layers.

## Key Results
- Sensitivity to linguistic uncertainty increases monotonically across transformer layers, with MSU scores highest in later layers
- PCA visualizations show that uncertainty-related signals become more linearly separable in deeper layers
- Three small LLMs consistently demonstrate progressive sensitivity to epistemic modality markers
- Epistemic modality appears to be encoded as high-level semantic features rather than early syntactic elements

## Why This Works (Mechanism)
The progressive increase in MSU scores across layers indicates that transformer models gradually build up uncertainty representations as information flows through the network. Early layers likely process surface-level syntactic differences between certain and uncertain sentences, while deeper layers extract and amplify semantic distinctions related to epistemic modality. This hierarchical processing aligns with established findings about how transformers encode linguistic information, where shallow layers handle local syntactic patterns and deeper layers capture abstract semantic relationships.

## Foundational Learning
- **Epistemic modality**: The linguistic expression of uncertainty or possibility (e.g., "might," "could," "must"). Understanding this is crucial because it represents the primary uncertainty signal being studied.
- **Model Sensitivity to Uncertainty (MSU)**: A novel metric measuring activation-level differences between certain and uncertain sentence variants. This metric is needed to quantify how much models "notice" linguistic uncertainty.
- **Transformer layerwise processing**: How information flows and transforms through successive layers. Quick check: Verify that activation differences increase monotonically across layers.
- **Activation pattern analysis**: Examining how model internal representations change in response to input variations. Quick check: Confirm that PCA visualizations show increasing separability.
- **Clinical text uncertainty**: Specific challenges in representing medical uncertainty where precision matters for patient outcomes. Quick check: Validate dataset covers clinically relevant uncertainty scenarios.

## Architecture Onboarding
**Component map**: Input sentences -> Tokenizer -> Embedding layer -> Transformer layers (N) -> MSU metric calculation -> PCA visualization
**Critical path**: Sentence pairs → Layer activations → MSU computation → Pattern analysis
**Design tradeoffs**: Small models (0.5-1.5B parameters) offer computational efficiency but may miss uncertainty patterns present in larger models; single-sentence analysis simplifies interpretation but may miss context-dependent uncertainty.
**Failure signatures**: If MSU scores don't increase across layers, this could indicate the model doesn't progressively build uncertainty representations; if PCA shows no separability, uncertainty signals may be too weak to detect.
**First experiments**: 1) Test MSU metric on random sentence pairs (should yield low scores), 2) Compare MSU across different epistemic markers to identify which generate strongest responses, 3) Validate that MSU correlates with human judgments of uncertainty salience.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on epistemic modality markers may not generalize to broader uncertainty phenomena
- Only three small-scale LLMs evaluated (≤1.5B parameters) limits conclusions about larger models
- Curated dataset may not capture full complexity of clinical uncertainty in real-world scenarios
- MSU metric measures activation shifts without establishing functional significance or downstream performance impact

## Confidence
- **High Confidence**: Layerwise sensitivity progression (monotonic increase in MSU scores across layers)
- **Medium Confidence**: Epistemic modality as high-level semantic features in later layers
- **Low Confidence**: Clinical applicability and MSU sufficiency as uncertainty handling proxy

## Next Checks
1. Validate MSU findings across diverse larger LLMs (7B+ parameters) to assess scalability
2. Conduct experiments linking MSU scores to clinical decision-making performance under uncertainty
3. Expand dataset to include context-rich clinical passages rather than isolated sentence pairs