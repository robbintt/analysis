---
ver: rpa2
title: 'Get away with less: Need of source side data curation to build parallel corpus
  for low resource Machine Translation'
arxiv_id: '2601.08629'
source_url: https://arxiv.org/abs/2601.08629
tags:
- data
- sentences
- sentence
- lalita
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the challenge of developing effective Machine
  Translation (MT) systems for low-resource languages, where acquiring sufficient
  high-quality parallel data is prohibitively expensive. The study introduces LALITA
  (LexicalAndLinguisticallyInformedTextAnalysis), a framework that uses linguistic
  and lexical features to assess and select structurally complex source sentences
  for optimal MT training.
---

# Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation

## Quick Facts
- arXiv ID: 2601.08629
- Source URL: https://arxiv.org/abs/2601.08629
- Reference count: 40
- Demonstrates data reduction by over 50% while maintaining translation quality

## Executive Summary
This research introduces LALITA, a framework for source-side data curation in low-resource machine translation. The framework leverages linguistic and lexical features to identify and select structurally complex sentences for optimal MT training. By focusing on complex sentences rather than simple ones, LALITA achieves significant improvements in translation quality while dramatically reducing the amount of training data required. The approach is validated across multiple language pairs including Hindi, Odia, Nepali, Norwegian Nynorsk, and German.

## Method Summary
LALITA employs a feature-based approach to evaluate sentence complexity, using both linguistic features (such as dependency parsing metrics, constituency tree metrics, and sentence length) and lexical features (including word frequency, out-of-vocabulary words, and n-gram diversity). These features are extracted from source sentences and combined into a feature vector, which is then analyzed using Principal Component Analysis (PCA) to compute a LALITA score that quantifies sentence complexity. The framework curates training data by selecting sentences with higher complexity scores, thereby ensuring that the training corpus contains more challenging and diverse linguistic structures that improve the model's ability to handle complex translations.

## Key Results
- LALITA reduces training data requirements by over 50% while maintaining translation quality
- Systems trained on LALITA-curated data achieve comparable or superior performance to full dataset baselines
- Framework demonstrates effectiveness across multiple language pairs and resource scenarios
- Significant improvements in translation quality for complex sentence structures

## Why This Works (Mechanism)
The framework works by strategically selecting structurally complex sentences that provide richer linguistic information for training. By focusing on complexity rather than volume, LALITA ensures that the model learns to handle challenging linguistic phenomena that are crucial for high-quality translation. The use of PCA to combine multiple linguistic and lexical features into a single complexity score allows for systematic and reproducible data curation.

## Foundational Learning
- **Linguistic Complexity Metrics**: Used to quantify sentence structure difficulty; needed to identify valuable training examples that improve model robustness
- **Principal Component Analysis**: Reduces feature dimensionality while preserving variance; needed to create a single interpretable complexity score from multiple features
- **Lexical Diversity Measures**: Captures vocabulary richness and rare word usage; needed to ensure training data covers diverse linguistic phenomena
- **Dependency Parsing Features**: Analyzes grammatical relationships; needed to identify syntactically complex structures
- **Cross-lingual Transfer**: Applies complexity-based curation across different language families; needed to validate framework generalizability

## Architecture Onboarding

**Component Map**: Source sentences -> Feature Extraction -> Feature Vector -> PCA Analysis -> LALITA Score -> Data Curation -> MT Training

**Critical Path**: The pipeline from feature extraction through PCA analysis to LALITA score computation is critical, as these components directly determine which sentences are selected for training.

**Design Tradeoffs**: The framework trades simplicity of sentence selection for quality of translation outcomes. While filtering out simple sentences reduces training data volume, it ensures the model learns more challenging linguistic patterns.

**Failure Signatures**: Poor translation quality on simple sentences, over-reliance on complex structures, or failure to generalize across domains may indicate suboptimal feature selection or score computation.

**First Experiments**:
1. Baseline translation quality comparison between LALITA-curated and random data subsets
2. Ablation study on individual feature contributions to LALITA score
3. Cross-lingual validation across different language families

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation limited to five specific languages and language pairs
- Reliance on linguistic complexity features may not capture domain-specific translation challenges
- Potential biases introduced by complexity-based filtering that could exclude important simple sentence patterns

## Confidence
- High confidence: 50% data reduction while maintaining translation quality is well-supported
- Medium confidence: Effectiveness in both low-resource and high-resource scenarios
- Medium confidence: Cost reduction and sustainability claims lack explicit computational efficiency analysis

## Next Checks
1. Test LALITA on additional language pairs from different language families (Asian, African, indigenous languages)
2. Conduct domain adaptation experiments for specialized domains (medical, legal, technical)
3. Perform ablation studies on individual linguistic features to determine optimal feature combinations