---
ver: rpa2
title: Exploring GPT's Ability as a Judge in Music Understanding
arxiv_id: '2501.13261'
source_url: https://arxiv.org/abs/2501.13261
tags:
- music
- beat
- chord
- concepts
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of text-based large language models
  (LLMs) for music information retrieval (MIR) tasks through prompt engineering. The
  authors convert music data into symbolic representations and test GPT-3.5's ability
  to detect annotation errors in beat tracking, chord extraction, and key estimation
  tasks.
---

# Exploring GPT's Ability as a Judge in Music Understanding

## Quick Facts
- arXiv ID: 2501.13261
- Source URL: https://arxiv.org/abs/2501.13261
- Reference count: 0
- Key outcome: GPT-3.5 achieves error detection accuracies of 65.20%, 64.80%, and 59.72% for beat tracking, chord extraction, and key estimation tasks respectively, all exceeding random baselines.

## Executive Summary
This study explores using text-based large language models for music information retrieval (MIR) tasks through prompt engineering. The authors convert music data into symbolic representations and test GPT-3.5's ability to detect annotation errors in beat tracking, chord extraction, and key estimation tasks. A concept augmentation method is proposed to evaluate how the amount of music knowledge in prompts affects performance. Results show that GPT-3.5 achieves error detection accuracies of 65.20%, 64.80%, and 59.72% for the three tasks respectively, all exceeding random baselines. The study finds a positive correlation between GPT's performance and the amount of music concepts provided in prompts, establishing a foundation for future LLM-based MIR research.

## Method Summary
The study converts music data to symbolic inputs and evaluates LLMs' ability in detecting annotation errors without training. The method involves parsing MIDI datasets into JSON format with musical notes and labels, synthetically injecting errors at controlled rates, constructing prompts with six components (Background, Concepts, Format, Steps, Output, Data Input), and applying concept augmentation (introduction/masking). GPT-3.5 is used as the LLM judge to identify errors, with outputs parsed and evaluated against ground truth using task-specific metrics. Datasets include MAPS for beat tracking and POP909 for chord and key estimation tasks.

## Key Results
- GPT-3.5 achieves error detection accuracies of 65.20% for beat tracking, 64.80% for chord extraction, and 59.72% for key estimation
- All task performances exceed random baselines
- Positive correlation between GPT's performance and amount of music concepts provided in prompts
- GPT exhibits general time-series analysis abilities even when musical concepts are masked

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based LLMs can perform music reasoning on symbolic representations when prompted with structured domain knowledge.
- Mechanism: The LLM parses music encoded as JSON-like symbolic data (MIDI-derived) and uses its pre-trained pattern-matching and reasoning capabilities. The prompt engineering provides scaffolding: task definition, domain concepts, and step-by-step reasoning guides. This transforms the MIR problem into a text-based logic puzzle.
- Core assumption: The LLM has sufficient general reasoning ability and has been exposed to enough musical text during pre-training to map provided symbolic note data to described musical concepts.
- Evidence anchors: [abstract]: "We convert the music data to symbolic inputs and evaluate LLMs' ability in detecting annotation errors..."; [section] Section 3.2 & Figure 1: The prompt structure is detailed, showing "Music Concepts Description" and "Techniques and Steps" as key components.

### Mechanism 2
- Claim: Performance is causally linked to the specificity of musical concepts provided in the prompt.
- Mechanism: The "concept augmentation" strategy varies the amount of domain knowledge in the prompt. Introducing concepts gives the LLM a more precise heuristic for judgment, leading to better performance. Masking concepts removes these heuristics, forcing the LLM to rely on weaker pattern recognition.
- Core assumption: The LLM's performance improves because it is correctly applying the provided concepts to the data, not simply because the prompt is longer or more complex.
- Evidence anchors: [abstract]: "...positive correlation between GPT's performance and the amount of music concepts provided in prompts..."; [section] Table 2 & Section 4.3: Results show a clear performance trend.

### Mechanism 3
- Claim: LLMs possess generalizable time-series analysis capabilities that provide a non-random baseline even without domain-specific musical concepts.
- Mechanism: Even when all musical concepts are masked, the LLM is presented with a sequence of data points and asked to find inconsistencies. It applies general logic to detect anomalies in the data structure.
- Core assumption: The LLM's pre-training includes sufficient exposure to structured data and logical reasoning tasks to perform this abstract error detection.
- Evidence anchors: [section] Section 5 (Conclusion): "GPT exhibits general time series analysis abilities even when music concepts are all masked..."; [section] Figure 2 & Table 2: Shows performance under "Domain Masking."

## Foundational Learning

- Concept: Symbolic Music Representation (e.g., MIDI as JSON)
  - Why needed here: The entire methodology hinges on converting audio-based MIR problems into text-based symbolic problems (notes as `{onset, duration, pitch, velocity}`). Without understanding this translation, the prompt engineering makes little sense.
  - Quick check question: How does representing a musical chord as a JSON object with a "notes" array (each with "pitch") allow a text model to perform reasoning that an audio model would do differently?

- Concept: Prompt Engineering (Chain-of-Thought, Few-Shot)
  - Why needed here: The paper's core contribution is a systematic prompt structure. Understanding techniques like "Techniques and Steps" (which encourages chain-of-thought) is essential for interpreting why the model succeeds and how to improve it.
  - Quick check question: According to the paper's prompt structure, what is the purpose of the "Techniques and Steps" component, and how does it relate to the chain-of-thought prompting technique?

- Concept: Music Information Retrieval (MIR) Tasks (Beat, Chord, Key)
  - Why needed here: The specific tasks (beat tracking, chord extraction, key estimation) are the benchmarks. Understanding what each task entails is necessary to interpret the results and the specific "concepts" that were introduced.
  - Quick check question: Why is the evaluation metric for beat tracking error detection (CPR, EDRP, EDRN) more complex than a simple F1-score, and what types of errors does it aim to capture?

## Architecture Onboarding

- Component map: Data Parser -> Error Injector -> Prompt Assembler -> LLM Interface -> Output Parser -> Evaluator
- Critical path: Data Preparation (parsing, error injection) -> Prompt Construction (concept augmentation) -> Model Inference -> Evaluation. The most critical step is Prompt Construction, as the paper demonstrates it directly controls model performance.
- Design tradeoffs:
  - Synthetic vs. Real Errors: The study uses synthetic errors for controlled evaluation. Assumption: This may not fully represent real-world MIR system failures.
  - GPT-3.5 vs. Larger Models: The study uses GPT-3.5 (cost-effective, accessible). Tradeoff: More capable models might show higher baseline performance.
  - Symbolic Proxy vs. Audio: Using symbolic MIDI avoids complex audio-to-text alignment modules. Tradeoff: It discards timbral and expressive information present in audio.
- Failure signatures:
  - Hallucination: Model invents notes or applies concepts incorrectly.
  - Format Non-Compliance: Model fails to output in the specified JSON format.
  - High Sensitivity: Small prompt changes cause large performance variations.
  - Abstract Failure: Performance collapses under domain masking if the model cannot map the problem to any known reasoning pattern.
- First 3 experiments:
  1. Baseline Reproduction: Run the "Basic Concepts" prompt on the provided datasets to reproduce reported F1-scores and establish a baseline.
  2. Concept Ablation: Systematically apply "Music Attribute Masking" to one task to verify reported performance drop and understand which concepts are most critical.
  3. Concept Introduction Test: Add a novel, relevant concept to the prompt and measure if performance improves, testing the mechanism's extensibility.

## Open Questions the Paper Calls Out
- Can LLMs effectively detect naturally occurring errors in MIR annotations, or is their performance limited to the synthetic error patterns tested? The authors state they will consider evaluating LLMs' judging ability on real MIR errors instead of synthetic ones.
- Does supervised fine-tuning significantly improve LLM performance in music reasoning tasks compared to the training-free prompt engineering approach? The authors list using fine-tuning techniques as a specific future direction.
- Can the reasoning capabilities of text-based LLMs be effectively aligned with raw audio perception to improve MIR tasks without prohibitive training costs? The introduction frames symbolic input as a "practical initial step" to avoid the "substantial data requirements" of audio models.

## Limitations
- Reliance on synthetic error injection rather than real-world MIR system failures
- Use of GPT-3.5 rather than more capable models like GPT-4
- Performance gains from concept augmentation may reflect prompt length/complexity effects
- Symbolic representation discards audio-level information that could be relevant

## Confidence

- **High Confidence**: The basic premise that GPT-3.5 can perform symbolic music error detection with structured prompts and exceed random baselines.
- **Medium Confidence**: The positive correlation between concept augmentation and performance, and the claim that GPT exhibits general time-series analysis abilities.
- **Low Confidence**: That these results establish a foundation for broader LLM-based MIR research.

## Next Checks

1. Apply the same prompt engineering framework to detect errors in actual outputs from existing MIR systems rather than synthetic errors, to assess practical utility.
2. Repeat key experiments using GPT-4 or GPT-3.5 with different parameter settings to determine whether performance improvements are model-dependent and to assess hallucination sensitivity.
3. Extend the framework to incorporate audio-derived features alongside symbolic data to evaluate whether combining modalities improves performance beyond symbolic-only reasoning.