---
ver: rpa2
title: Evaluating the Diversity and Quality of LLM Generated Content
arxiv_id: '2504.12522'
source_url: https://arxiv.org/abs/2504.12522
tags:
- diversity
- semantic
- arxiv
- function
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework for measuring effective semantic\
  \ diversity\u2014diversity among high-quality LLM outputs\u2014to better reflect\
  \ real-world utility. The authors define a validity function to filter outputs by\
  \ quality and a semantic function based on program execution to assess semantic\
  \ uniqueness."
---

# Evaluating the Diversity and Quality of LLM Generated Content

## Quick Facts
- arXiv ID: 2504.12522
- Source URL: https://arxiv.org/abs/2504.12522
- Reference count: 40
- Primary result: Preference-tuned models achieve higher effective semantic diversity by generating more valid outputs, not by increasing diversity among valid outputs.

## Executive Summary
This paper introduces a framework for measuring effective semantic diversity—diversity among high-quality LLM outputs—to better reflect real-world utility. The authors define a validity function to filter outputs by quality and a semantic function based on program execution to assess semantic uniqueness. Experiments on open-ended programming tasks show that preference-tuned models (especially RL-based) achieve higher effective semantic diversity than SFT or base models, primarily due to generating more valid outputs, even though their lexical and syntactic diversity decreases. Larger models produce greater semantic diversity without sacrificing form diversity, while smaller models are more parameter-efficient at generating unique content. These findings reveal that preference tuning reduces syntactic/lexical diversity while preserving semantic diversity, highlighting a distinction between diversity of form and content.

## Method Summary
The framework evaluates 19 models across LLaMA 2/3.1 families and other architectures on 108 open-ended programming tasks adapted from CODE NET. For each prompt, K=32 generations are sampled and executed against test suites to determine validity (error-free execution with non-null output). Semantic uniqueness is assessed via output traces across test cases. Effective semantic diversity is computed using pairwise diversity measures that account for varying validity rates. The study compares base, SFT, DPO, PPO, and GRPO models, along with scale variants (7B vs 70B), measuring lexical, syntactic, neural, and semantic diversity.

## Key Results
- Preference-tuned models (especially RL-based) achieve higher effective semantic diversity than SFT or base models
- This increase comes from generating more valid outputs, not from increasing diversity among valid outputs
- Larger models increase semantic diversity without reducing form diversity, unlike preference tuning which trades form for content
- Smaller models are more parameter-efficient at generating unique content per parameter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference tuning increases effective semantic diversity primarily by improving output validity rates, not by increasing diversity among already-valid outputs.
- Mechanism: RL-based and DPO methods shape the policy distribution toward higher-reward regions. This increases the probability mass on valid outputs (measured by the validity function V), expanding the denominator of valid generations from which semantic diversity is computed. The paper shows preference-tuned models have lower semantic diversity when restricting only to valid outputs (Table 3, SFT vs. RL: d = -3.12), but higher overall effective diversity (Table 2) because validity gains offset this reduction.
- Core assumption: The validity function V correctly captures task-specific quality thresholds, and semantic equivalence via test case execution reliably proxies meaningful content differences.
- Evidence anchors:
  - [abstract] "they produce greater effective semantic diversity... not from increasing diversity among high-quality outputs, but from generating more high-quality outputs overall"
  - [section 5.1] "Semantic diversity driven by higher quality... preference-tuned models generally have more semantic duplicates than their SFT-tuned counterparts"
  - [corpus] Limited direct support; NoveltyBench addresses mode collapse but doesn't examine the quality-diversity interaction mechanism.
- Break condition: If validity rates saturate near 100%, further preference tuning cannot increase effective diversity through this mechanism and may only reduce form diversity.

### Mechanism 2
- Claim: Preference tuning decouples form diversity (syntactic/lexical) from content diversity (semantic), reducing the former while preserving the latter.
- Mechanism: RL optimization with KL-regularization constrains the policy to remain near the base distribution while shifting probability toward high-reward outputs. The paper suggests this narrows the syntactic and lexical modes the model explores (converging to canonical solution patterns) while preserving semantic distinctiveness among valid solutions. RL shows larger syntactic/lexical reductions than DPO (Table 2, Syntactic d = -1.16 for DPO vs. RL).
- Core assumption: The observed correlation between RL training and reduced form diversity reflects a causal effect of the training procedure, not confounding factors like dataset or architecture differences.
- Evidence anchors:
  - [abstract] "preference-tuning reduces syntactic diversity while preserving semantic diversity—revealing a distinction between diversity in form and diversity of content"
  - [section 5.1] "preference-tuning broadly induces a dramatic decrease in the syntactic and lexical diversity of generations"
  - [corpus] Not directly addressed in neighboring papers; mechanism remains specific to this work.
- Break condition: If tasks require form diversity as part of utility (e.g., creative writing with stylistic variation), this decoupling may reduce practical utility despite preserved semantic diversity.

### Mechanism 3
- Claim: Larger models achieve higher effective semantic diversity without sacrificing form diversity, unlike preference tuning which trades form for content.
- Mechanism: Increased model capacity enables representation of more solution modes and semantic patterns from training data. Unlike preference tuning which actively reshapes the distribution, scaling preserves the base distribution's richness while improving validity through better capability. The paper finds larger models show positive effect sizes on semantic diversity (d = 0.26) without significant syntactic/lexical reduction (Table 2).
- Core assumption: Model families compared (7B vs. 70B) differ primarily in size, with other factors controlled; scaling effects generalize across architectures.
- Evidence anchors:
  - [section 5.2] "Larger models increase semantic diversity without reducing the diversity of form... larger models have a higher capacity to model a greater amount of distributions"
  - [section 1] "increases in effective diversity are not associated with a reduction in lexical or syntactic diversity"
  - [corpus] Weak external validation; corpus papers don't examine scale-diversity relationships.
- Break condition: If scaling laws for diversity saturate, further parameter increases yield diminishing returns on semantic diversity per unit of compute.

## Foundational Learning

- **Concept: Validity functions and quality thresholds**
  - Why needed here: The framework's core innovation is conditioning diversity on validity. Understanding how to define V for different domains is essential for applying this approach.
  - Quick check question: For a code generation task, would you define validity as (a) syntax correctness only, (b) passing a test suite, or (c) passing tests plus style requirements? How does each choice affect measured diversity?

- **Concept: Semantic equivalence via execution traces**
  - Why needed here: The paper uses program execution outputs to determine if two programs are semantically equivalent, avoiding LLM-as-judge reliability issues.
  - Quick check question: Two programs produce identical outputs on 10 test cases. Are they semantically equivalent? What could cause them to differ on untested inputs?

- **Concept: Preference tuning taxonomy (SFT, DPO, PPO, GRPO)**
  - Why needed here: The paper's conclusions differ by post-training method; RL shows stronger form-diversity reduction than DPO.
  - Quick check question: Why might KL-regularized RL (PPO) reduce form diversity more than DPO, even though both optimize for preferences?

## Architecture Onboarding

- **Component map:**
  - Prompt dataset (D) -> Generation sampler (K=32 outputs) -> Validity function V (execution check) -> Semantic function S (output trace) -> Diversity metrics (Div_pair, Distinct-N, AST, CODE-BERTScore)

- **Critical path:**
  1. Generate K outputs per prompt at specified temperature
  2. Execute each output against test suite → validity label
  3. For valid outputs, compute execution traces → semantic equivalence clusters
  4. Compute Div_pair using d_sem distance function (Equation 4)
  5. Aggregate via AvgDiv_m across all prompts (Equation 1)

- **Design tradeoffs:**
  - Test case coverage: More test cases reduce false semantic equivalence but increase compute cost; the paper acknowledges edge cases may be missed
  - Pairwise vs. fixed-count diversity: Equation 2 is sample-size sensitive; Equation 3 is robust but more complex
  - Domain specificity: Execution-based semantic equivalence works for code; NLP tasks require different S functions (potentially LLM-based, with reliability concerns)

- **Failure signatures:**
  - Validity rates near 0%: Base models at low temperature may produce no valid outputs, making semantic diversity undefined
  - Test case insufficiency: Programs that differ only on untested inputs appear semantically equivalent, underestimating true diversity
  - Temperature extremes: Very high temperature increases neural/lexical diversity but crashes validity (Figure 1)
  - Model comparison across families: Different tokenizers and training data confound diversity comparisons

- **First 3 experiments:**
  1. Replicate the temperature sweep (Figure 1) on your target model family to identify the validity-diversity sweet spot before running larger experiments.
  2. Validate the semantic equivalence function: Manually inspect 50 pairs classified as semantically equivalent to estimate false-positive rate from test coverage gaps.
  3. Compare Div_pair (Equation 3) vs. Div_fixed (Equation 2) on a subset with varying valid-output counts to confirm robustness to sample size effects before full-scale analysis.

## Open Questions the Paper Calls Out
None

## Limitations
- The validity function may not capture all quality dimensions relevant to real-world utility (e.g., runtime efficiency, edge case handling)
- Semantic equivalence via execution traces risks underestimating true semantic diversity if test cases miss corner cases
- The pairwise diversity measure Div_pair is computationally prohibitive for larger K values
- The observed correlation between preference tuning and reduced form diversity assumes causation rather than confounding factors

## Confidence
- **High confidence**: Validity rates significantly improve with preference tuning, directly increasing effective semantic diversity (Mechanism 1)
- **Medium confidence**: Preference tuning decouples form diversity from semantic diversity (Mechanism 2), based on observed correlations
- **Medium confidence**: Larger models achieve higher semantic diversity without form diversity reduction (Mechanism 3), assuming controlled comparisons
- **Low confidence**: The semantic equivalence function reliably captures meaningful content differences for all programming tasks

## Next Checks
1. **False equivalence rate validation**: Manually inspect 100 program pairs classified as semantically equivalent to estimate how often test case coverage misses meaningful semantic differences.
2. **Cross-domain generalization**: Apply the framework to a non-code domain (e.g., story generation with quality metrics) to test whether preference tuning similarly decouples form and content diversity.
3. **Edge case robustness**: Design test suites that intentionally include corner cases where semantically different programs produce identical outputs on most inputs, then measure whether semantic diversity captures these distinctions.