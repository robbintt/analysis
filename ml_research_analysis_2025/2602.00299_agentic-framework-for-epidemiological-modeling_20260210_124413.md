---
ver: rpa2
title: Agentic Framework for Epidemiological Modeling
arxiv_id: '2602.00299'
source_url: https://arxiv.org/abs/2602.00299
tags:
- epidemiological
- scenario
- modeling
- graph
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces EPIAGENT, an agentic framework for epidemiological
  modeling that automatically synthesizes, calibrates, and verifies mechanistic simulators
  from natural-language scenario descriptions. A central innovation is the use of
  an epidemiological flow graph as an intermediate representation, enabling modular
  correctness checks before code generation.
---

# Agentic Framework for Epidemiological Modeling

## Quick Facts
- arXiv ID: 2602.00299
- Source URL: https://arxiv.org/abs/2602.00299
- Reference count: 40
- Primary result: EPIAGENT achieves accurate fits to observed trajectories, produces epidemiologically consistent counterfactual projections, and converges more reliably than unguided approaches

## Executive Summary
This work introduces EPIAGENT, an agentic framework for epidemiological modeling that automatically synthesizes, calibrates, and verifies mechanistic simulators from natural-language scenario descriptions. A central innovation is the use of an epidemiological flow graph as an intermediate representation, enabling modular correctness checks before code generation. EPIAGENT integrates retrieval-augmented knowledge, multi-agent verification, and iterative refinement to enforce structural validity, epidemiological consistency, and scenario fidelity. Evaluation on COVID-19 and behavioral SEIR case studies shows that EPIAGENT achieves accurate fits to observed trajectories, produces epidemiologically consistent counterfactual projections, and converges more reliably than unguided approaches. Agentic feedback significantly accelerates convergence toward valid models by mimicking expert workflows.

## Method Summary
EPIAGENT automatically synthesizes, calibrates, and verifies compartmental epidemic simulators from natural-language scenario descriptions. The system uses retrieval-augmented generation (RAG) with FAISS-based vector search to ground LLM outputs in epidemiological knowledge. It synthesizes an epidemiological flow graph as an intermediate representation, which is verified against structural constraints before being compiled into ordinary differential equations. A multi-agent architecture (Planner, Verification, Validation agents) iteratively refines the model, enforcing constraints like non-negativity and mass conservation. Calibration is performed via Adam optimizer with MSE loss on infections and deaths. The framework was evaluated on COVID-19 Scenario Modeling Hub data and behavioral SEIR datasets with deaths, mobility, and seasonality covariates.

## Key Results
- EPIAGENT achieves accurate fits to observed trajectories on COVID-19 and behavioral SEIR case studies
- Agentic feedback accelerates convergence toward valid models by mimicking expert workflows
- Knowledge guidance reduces the average number of graph-generation iterations by ~20% compared to unguided LLM generation

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Flow Graph Representation
- **Claim:** EPIAGENT improves the reliability of mechanistic model synthesis by decoupling structural validity from equation generation using an intermediate flow graph representation.
- **Mechanism:** Instead of mapping natural language directly to code, the system first maps scenario descriptions to a directed epidemiological flow graph $G=(V, E)$. This graph is verified against hard structural constraints (e.g., valid compartment transitions like $S \to E \to I$, conservation of mass) before compilation into ordinary differential equations (ODEs).
- **Core assumption:** Structural validity (correct causal pathways) is a necessary precondition for parameter calibration and can be verified independently of the specific parameter values.
- **Evidence anchors:**
  - [abstract]: "A central innovation is the use of an epidemiological flow graph as an intermediate representation... enabling modular correctness checks before code generation."
  - [section 4.2]: "This graph-level verification prevents invalid structures from propagating into executable code."
  - [corpus]: Corpus evidence is weak for this specific architectural pattern; related work like *EpiLLM* focuses on direct forecasting rather than structural synthesis.
- **Break condition:** If the verification logic fails to capture implicit constraints (e.g., scenario-specific eligibility for vaccination), the system may generate structurally valid but behaviorally invalid graphs.

### Mechanism 2: Retrieval-Augmented Constraint Grounding
- **Claim:** Augmenting prompts with retrieved epidemiological knowledge reduces the frequency of structural errors and accelerates convergence compared to unguided LLM generation.
- **Mechanism:** The framework uses a FAISS-based vector index to retrieve relevant passages from epidemiological textbooks based on the input scenario. This knowledge explicitly constrains the LLM's generation space, preventing it from defaulting to oversimplified canonical models (e.g., standard SEIR) when complex dynamics (e.g., waning immunity) are required.
- **Core assumption:** General LLM parametric knowledge is insufficient for precise epidemiological rigor and must be grounded in external, curated domain text.
- **Evidence anchors:**
  - [section 4.1]: "Retrieved passages are combined with the specification... grounding model construction in established epidemiological theory."
  - [section 5.1]: "Without external knowledge retrieval... the LLM defaults to the simplest SEIRD-style graphs... Knowledge guidance reduces the average number of graph-generation iterations... by ~20%."
  - [corpus]: *EpiPlanAgent* supports the use of LLMs for automated planning, reinforcing the utility of agentic structures in this domain.
- **Break condition:** If the retrieval corpus lacks diversity or contains conflicting information, the grounding may misguide the synthesis agent.

### Mechanism 3: Multi-Agent Verification & Iterative Repair
- **Claim:** A multi-agent architecture mimicking expert workflows (planning, verification, feedback) prevents degenerative solutions and ensures epidemiological consistency.
- **Mechanism:** A "Planner" agent generates code, which is then audited by "Verification" and "Validation" agents. These agents check for mathematical validity (non-negativity, mass conservation) and scenario fidelity. Violations trigger structured feedback for the Planner to revise the model, rather than relying on naive parameter clipping.
- **Core assumption:** Iterative, explicit feedback is more effective for correcting structural errors in code than single-pass generation or standard optimization tricks (like ReLU).
- **Evidence anchors:**
  - [abstract]: "Agentic feedback significantly accelerates convergence toward valid models by mimicking expert workflows."
  - [section 4.5]: "The verification agent enforces non-negativity, population conservation, and monotonicity, preventing such degenerate but low-loss solutions..."
  - [corpus]: *Epi$^2$-Net* utilizes physics-inspired constraints, which parallels EPIAGENT's use of hard constraints to enforce physical realism.
- **Break condition:** The system may enter cyclic failure if the feedback loop fails to provide actionable insights or if the "retry" limit is reached without resolving numerical instability.

## Foundational Learning

- **Compartmental Modeling (SEIR/ODEs):**
  - **Why needed here:** The system's output is a system of ODEs defined by a flow graph. Understanding stock-and-flow dynamics is required to debug why a model might violate mass conservation.
  - **Quick check question:** Can you identify why a direct transition from Susceptible ($S$) to Recovered ($R$) might be structurally invalid in a standard infection model?

- **Retrieval-Augmented Generation (RAG):**
  - **Why needed here:** The quality of the generated model depends heavily on the context retrieved from the epidemiological corpus.
  - **Quick check question:** How does semantic search (e.g., FAISS/vector embeddings) differ from keyword search in retrieving relevant context for "immune escape"?

- **Inverse Problems & Calibration:**
  - **Why needed here:** The architecture involves fitting (calibrating) unknown parameters $\theta$ to observed data using gradient descent.
  - **Quick check question:** Why might a model achieve a low loss (MSE) but still be epidemiologically invalid (e.g., negative parameters)?

## Architecture Onboarding

- **Component map:**
  1. RAG Module: (Sentence-Transformers + FAISS) retrieves epi-knowledge
  2. Graph Synthesizer: (LLM) outputs $G=(V,E)$ based on scenario + retrieved knowledge
  3. Graph Verifier: (Rule-based/LLM) checks structural constraints (e.g., valid transitions)
  4. Planner Agent: (LLM + Skeleton Code) compiles $G$ into differentiable PyTorch code
  5. V&V Agents: Monitor training for constraint violations (e.g., non-negativity) and provide feedback

- **Critical path:** Scenario Text → RAG Retrieval → Flow Graph Generation → Verification → Code Synthesis → Calibration → Validation. *Note:* Failure at the Graph Verification stage prevents wasted compute on code generation.

- **Design tradeoffs:**
  - Time-invariant vs. Time-variant: The system defaults to time-invariant parameters ($\beta, \gamma$) for interpretability. Neural-augmented dynamics are available but reduce mechanistic clarity.
  - Strict constraints vs. Flexibility: Hard constraints (e.g., no $S \to R$) ensure validity but may restrict novel model discovery; the skeleton code enforces execution consistency but limits architectural creativity.

- **Failure signatures:**
  - Structural Drift: Generated equations imply invalid causal pathways (e.g., deaths from Susceptible states)
  - Numerical Instability: "NaNs" or "Infs" during calibration, often due to solver stiffness or invalid initial conditions
  - Scenario Collapse: Different input scenarios produce identical projection trajectories, suggesting the model ignored scenario-specific interventions

- **First 3 experiments:**
  1. Ablation on RAG: Run the framework with the RAG module disabled. Verify if the output defaults to a simple SEIR model regardless of complex prompt requirements (as suggested in Section 5.1).
  2. Graph Verification Stress Test: Intentionally inject a "false" constraint into the verifier (e.g., allow $S \to D$) and observe if the resulting simulations produce physically impossible trajectories (e.g., deaths without infection).
  3. Calibration Check: Using a pre-verified flow graph, attempt calibration on noisy data. Check if the V&V agents successfully flag "low-loss but invalid" solutions (e.g., negative infection rates).

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on the quality and coverage of the epidemiology knowledge corpus, which is not specified in detail
- Multi-agent architecture introduces potential for cyclic failures when feedback loops fail to resolve structural errors
- Evaluation focuses on specific case studies (COVID-19 and behavioral SEIR) without broader validation across diverse epidemiological scenarios

## Confidence

- **High Confidence:** The intermediate flow graph representation mechanism and its role in enabling modular correctness checks before code generation
- **Medium Confidence:** The retrieval-augmented constraint grounding mechanism's effectiveness in reducing structural errors and accelerating convergence
- **Medium Confidence:** The multi-agent verification and iterative repair approach's ability to prevent degenerative solutions and ensure epidemiological consistency

## Next Checks

1. **Knowledge Corpus Completeness Test:** Systematically evaluate model performance across scenarios requiring different levels of epidemiological complexity (basic SEIR, waning immunity, behavioral feedback) while varying the knowledge corpus size and coverage. This would quantify the minimum knowledge requirements for reliable synthesis.

2. **Cross-Scenario Generalization Test:** Apply the trained EPIAGENT framework to entirely new epidemiological scenarios (e.g., different diseases, novel intervention types) not represented in the COVID-19 Scenario Modeling Hub or behavioral SEIR datasets. Measure whether the flow graph verification and multi-agent feedback mechanisms generalize beyond the training distribution.

3. **Constraint Relaxation Impact Analysis:** Systematically relax the hard constraints in the graph verifier (e.g., allow S→R transitions, permit death from susceptible states) and measure the impact on calibration accuracy, computational efficiency, and epidemiological validity. This would clarify the optimal constraint strictness for different modeling objectives.