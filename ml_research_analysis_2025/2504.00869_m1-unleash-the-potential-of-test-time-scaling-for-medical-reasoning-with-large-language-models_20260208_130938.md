---
ver: rpa2
title: 'm1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with
  Large Language Models'
arxiv_id: '2504.00869'
source_url: https://arxiv.org/abs/2504.00869
tags:
- medical
- reasoning
- data
- scaling
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents m1, the first comprehensive investigation of
  test-time scaling for medical reasoning with large language models. The core idea
  is to improve medical reasoning by allowing models to generate longer chains of
  thought during inference, while addressing the unique challenges of the medical
  domain.
---

# m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2504.00869
- Source URL: https://arxiv.org/abs/2504.00869
- Reference count: 40
- Key result: Lightweight models under 10B parameters achieve state-of-the-art medical reasoning performance with test-time scaling

## Executive Summary
This paper presents m1, the first comprehensive investigation of test-time scaling for medical reasoning with large language models. The method improves medical reasoning by allowing models to generate longer chains of thought during inference while addressing unique challenges in the medical domain. Through careful data curation, fine-tuning, and test-time controls, the approach enables lightweight models to achieve new state-of-the-art performance on medical QA benchmarks, with 32B models rivaling previous 70B-scale medical LLMs.

## Method Summary
The m1 method involves curating high-quality medical QA data with detailed reasoning chains, fine-tuning models on this data using supervised fine-tuning, and applying test-time controls to manage reasoning length during inference. The data pipeline filters for difficulty, generates reasoning traces via DeepSeek-R1 with correctness validation, and samples for diversity across medical domains. Models are trained on the "question → reasoning → answer" format using Qwen2.5-Instruct base models, then evaluated with controlled thinking token budgets at inference time.

## Key Results
- Test-time scaling consistently enhances medical reasoning, with optimal performance at ~4K reasoning tokens
- Lightweight fine-tuned models under 10B parameters achieve new state-of-the-art performance on medical benchmarks
- 32B model rivals previous 70B-scale medical LLMs in performance
- Performance degrades beyond optimal token budget due to overthinking
- Budget forcing provides limited benefits and can introduce errors

## Why This Works (Mechanism)

### Mechanism 1
Increasing the thinking token budget during inference improves medical QA accuracy up to an optimal threshold (~4K tokens). Allocating more generation tokens allows the model to decompose complex medical questions into multi-step reasoning chains, retrieving relevant knowledge, evaluating differential diagnoses, and verifying conclusions sequentially. Beyond ~4K tokens, overthinking introduces degradation as models overanalyze or second-guess correct answers.

### Mechanism 2
Supervised fine-tuning on verified-correct reasoning traces enables models to productively use extended thinking budgets at inference. The training data undergoes difficulty filtering, reasoning generation via DeepSeek-R1, validation to discard incorrect chains, and diversity sampling across MeSH domains. SFT teaches the model the "question → reasoning → answer" format, though its effectiveness depends on the quality and correctness of training reasoning traces.

### Mechanism 3
Medical knowledge sufficiency is the primary bottleneck for test-time scaling effectiveness. Extended reasoning cannot compensate for absent or incorrect medical knowledge—when models lack foundational facts, longer reasoning chains retrieve incorrect associations or fail to access required information. Budget forcing exacerbates this by prompting reconsideration of correct answers with faulty knowledge.

## Foundational Learning

- Concept: Chain-of-thought (CoT) prompting and test-time scaling
  - Why needed here: m1 builds on the principle that extended reasoning improves complex task performance. Understanding CoT basics explains why token budget control matters.
  - Quick check question: Can you explain why generating reasoning steps before an answer might improve accuracy on multi-step problems?

- Concept: Knowledge vs. reasoning decomposition in LLMs
  - Why needed here: The paper's central insight is that medical domains differ from mathematical reasoning—knowledge gaps cannot be resolved through extended reasoning alone.
  - Quick check question: If a model lacks the knowledge that "aspirin inhibits platelet aggregation," will prompting it to "think longer" help it answer a question about aspirin's mechanism?

- Concept: Supervised fine-tuning (SFT) data quality vs. quantity tradeoffs
  - Why needed here: m1 achieves strong results with only 1K-23K examples, but relies on rigorous filtering. Understanding this tradeoff is essential for reproduction.
  - Quick check question: Why might 1K carefully filtered examples outperform 100K randomly sampled examples for medical reasoning training?

## Architecture Onboarding

- Component map: Raw QA pool (196K) → Difficulty filtering (37K) → Reasoning generation via DeepSeek-R1 → Correctness validation → Diversity sampling (1K m1K or 23K m23K) → SFT on Qwen2.5-Instruct (7B or 32B) → 5 epochs training → SGLang inference with token budget control (128-8192 tokens) → Optional budget forcing

- Critical path: Difficulty filtering is essential—training on "easy" questions provides no reasoning signal; reasoning validation prevents training on incorrect chains; token budget control at inference (not budget forcing) is the primary performance lever.

- Design tradeoffs: 1K vs. 23K training data balances knowledge coverage against compute requirements; 7B vs. 32B model affects knowledge richness and scaling benefits; budget forcing generally harmful in medical domain, may help in math domains.

- Failure signatures: Performance plateaus or degrades beyond 4K tokens indicates overthinking threshold reached; budget forcing reduces accuracy when models reconsider correct answers with faulty knowledge; small models underperform on OOD benchmarks due to knowledge capacity insufficiency.

- First 3 experiments: 1) Reproduce token budget scaling curve on MedMCQA at budgets [128, 256, 512, 1024, 2048, 4096] to verify ~4K optimal threshold; 2) Ablate difficulty filtering by training on random 1K vs. difficulty-filtered 1K samples and compare accuracy on MedQA-USMLE; 3) Test budget forcing degradation by applying 1-3 "Wait" injections to 20 questions answered correctly at budget 2048 and measuring error introduction rate.

## Open Questions the Paper Calls Out

1. Why does budget forcing degrade medical QA performance unlike in mathematical reasoning where it improves accuracy? The paper documents the phenomenon but doesn't investigate why medical knowledge differs from mathematical knowledge in this regard.

2. What causes the "overthinking" phenomenon where performance degrades beyond ~4K token budget, and does this optimal threshold vary systematically by question type, difficulty, or model size? The phenomenon is observed empirically but not explained mechanistically.

3. How can medical knowledge gaps that prevent test-time scaling benefits be systematically identified and remediated in LLMs? While the bottleneck is identified, the paper doesn't provide methods to detect specific knowledge gaps or targeted remediation strategies.

4. How robust is test-time scaling for medical reasoning across diverse clinical tasks beyond multiple-choice QA? The paper evaluates exclusively on QA benchmarks without exploring generalizability to diagnosis, treatment planning, or clinical note generation.

## Limitations
- Data quality bottleneck: Relies on DeepSeek-R1-generated reasoning traces that may contain systematic errors amplified through SFT
- Knowledge vs. reasoning disentanglement: Doesn't provide systematic methods to distinguish knowledge failures from reasoning failures
- Generalization across medical subdomains: Performance gains demonstrated but not analyzed for uniformity across different medical specialties

## Confidence
- High confidence: The empirical observation that test-time scaling improves medical reasoning up to ~4K tokens is well-supported by systematic experiments
- Medium confidence: The claim that medical knowledge sufficiency is the primary bottleneck has strong supporting evidence but relies on indirect inference
- Medium confidence: The effectiveness of the specific data curation pipeline is demonstrated but not extensively ablated

## Next Checks
1. Conduct systematic error analysis for questions where test-time scaling fails to categorize whether failures stem from absent knowledge, incorrect knowledge, or reasoning process issues.

2. Replicate the token budget scaling experiments separately for different medical subdomains to determine if optimal token budgets and scaling benefits vary by specialty.

3. Design an experiment where models are augmented with external medical knowledge retrieval during reasoning to test whether providing missing knowledge during inference improves scaling effectiveness.