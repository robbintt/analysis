---
ver: rpa2
title: 'GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant'
arxiv_id: '2507.01259'
source_url: https://arxiv.org/abs/2507.01259
tags:
- legal
- score
- answer
- retrieval
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces gAIus, a legal knowledge-based assistant that
  improves large language model performance on Polish legal tasks by combining retrieval
  with explicit legal act segmentation. Instead of using embeddings, gAIus retrieves
  relevant legal articles using a custom scoring function that matches text substrings,
  enabling better explainability and performance.
---

# GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant

## Quick Facts
- arXiv ID: 2507.01259
- Source URL: https://arxiv.org/abs/2507.01259
- Authors: Michał Matak; Jarosław A. Chudziak
- Reference count: 4
- Primary result: Outperformed raw LLMs on Polish law exam QA by 173-419% using substring-matching retrieval and article-level chunking

## Executive Summary
GAIus is a legal knowledge-based assistant that improves LLM performance on Polish legal tasks by combining retrieval with explicit legal act segmentation. Instead of using embeddings, gAIus retrieves relevant legal articles using a custom scoring function that matches text substrings, enabling better explainability and performance. Evaluated on a dataset of 146 Polish law exam questions, gAIus significantly outperformed raw LLMs, improving GPT-3.5-turbo-0125 by 419% and GPT-4o-mini by 173%, achieving joint accuracy scores of 109 and 126 respectively, surpassing even GPT-4o. The approach proved more effective than embedding-based retrieval while providing clearer legal references.

## Method Summary
The system prepares a legal corpus by splitting the Polish Civil Code into individual articles (avg. ~140 tokens each), then implements a custom fuzzy substring-matching retriever that slides a window across each chunk to score relevance based on character-level matches. An LLM agent reformulates user queries into generalized search terms and retrieves the top-50 scoring chunks, which are then provided to the LLM for answer generation with citations. The system uses LangChain with OpenAI GPT-3.5-turbo-0125 or GPT-4o-mini (temperature=0) and evaluates results using answer accuracy, citation accuracy, and joint scores.

## Key Results
- Outperformed raw GPT-3.5-turbo-0125 by 419% and GPT-4o-mini by 173% on joint accuracy score
- Achieved joint accuracy scores of 109 (GPT-3.5) and 126 (GPT-4o-mini), surpassing GPT-4o
- Proved more effective than embedding-based retrieval while providing clearer legal references

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A fuzzy substring-matching retrieval function can outperform embedding-based vector search for statutory law QA by better aligning exam question phrasing with article text.
- Mechanism: The Civil Code is split into article-level chunks. For a query, the retriever slides a window across each chunk and scores the window by counting character-level matches (fuzzy text search). The top-k scoring chunks (k=50) are returned to the LLM, which produces an answer with citations.
- Core assumption: Exam questions contain lexical cues that directly overlap with article text; an LLM can filter noise from many retrieved chunks.
- Evidence anchors:
  - [abstract] "Instead of using embeddings, gAIus retrieves relevant legal articles using a custom scoring function that matches text substrings, enabling better explainability and performance."
  - [section 3.2] "To assess the relevance of the document to the query, we score the documents using a custom function and retrieve the top-k documents with the highest scores (Algorithm 1 and 2)."
  - [corpus] No direct validation in neighboring papers; corpus support is weak. Related legal RAG work (e.g., HalluGraph, ACORD) focuses on embedding/keyword hybrids, not pure substring scoring.
- Break condition: Queries with paraphrased or abstract wording lacking lexical overlap may cause recall to collapse; increasing k may not help if relevant articles rank low.

### Mechanism 2
- Claim: Article-level segmentation plus high-k retrieval improves recall while the LLM handles precision via in-context filtering.
- Mechanism: Each article becomes a chunk (avg. ~140 tokens). The system retrieves up to 50 chunks to maximize recall, fitting within context limits. The LLM then identifies the correct answer and citations from the provided context.
- Core assumption: Articles are coherent, self-contained units; the LLM can robustly select relevant signal among many chunks; token budgets tolerate ~50 chunks.
- Evidence anchors:
  - [abstract] "...combining retrieval with explicit legal act segmentation."
  - [section 3.2] "We chose to split the entire Code into individual articles... average length of approximately 140 tokens... we chose to retrieve a relatively large number of chunks (k=50)."
  - [corpus] Corpus papers discuss segmentation (e.g., rhetorical roles in TraceRetriever) but do not specifically validate article-level chunks with high-k retrieval for statutory QA.
- Break condition: If questions require synthesizing many articles or articles are long, k=50 may still miss context or exceed windows.

### Mechanism 3
- Claim: An LLM-based query generalization step (specific question → broader term) improves retrieval effectiveness.
- Mechanism: The agent first rewrites a specific question into a generalized retrieval query (e.g., "Who can be incapacitated?" → "Incapacitation"). It can call the retriever multiple times with different reformulations if needed.
- Core assumption: Exam questions use specific phrasing not always present verbatim in statutes; generalized terms better match article text.
- Evidence anchors:
  - [abstract] Not explicitly described.
  - [section 3.1] "For example, a specific question such as 'Who can be incapacitated?' would be transformed into the more general query 'Incapacitation.' ... The agent can use the retriever multiple times."
  - [corpus] No direct corpus evidence on LLM query rewriting for statutory law; corpus support is weak.
- Break condition: Reformulation can drift or introduce terms absent from the code, degrading retrieval; iterative retries may amplify errors.

## Foundational Learning

- Concept: RAG with chunking and top-k selection
  - Why needed here: gAIus is a RAG system where chunk size and top-k shape the recall/precision trade-off and token budget.
  - Quick check question: How would decreasing chunk size but increasing k affect context window usage and answer accuracy?

- Concept: Statute law vs. case law retrieval
  - Why needed here: The paper targets statutory articles with precise citations; case law retrieval uses different units and ranking criteria.
  - Quick check question: What is the difference in retrieval objectives between finding a civil code article and finding a precedent case in common law?

- Concept: Explainability via citations
  - Why needed here: Evaluation includes context score and joint answer+context; explainability is a core goal.
  - Quick check question: How would you evaluate whether a citation is correct and not overly broad?

## Architecture Onboarding

- Component map: User query -> LLM agent (query reformulation) -> Retriever (substring scoring over article-level chunks) -> Top-k article chunks -> LLM generates answer with citations -> Evaluation agent extracts answer and citations to JSON
- Critical path:
  1. Prepare corpus: PDF -> TXT -> strip metadata -> split by articles
  2. Implement retriever: Algorithms 1 & 2 (sliding window, character matching) -> rank -> select top-k=50
  3. Agent loop: LLM rewrites query -> retriever call -> optional retries -> prompt with chunks -> answer+citations
- Design tradeoffs:
  - Substring vs. embeddings: Better explainability and (per paper) higher accuracy, but brittle to paraphrase; no semantic generalization
  - Article-level chunks: Natural boundaries and clearer citations, but may split related provisions; higher k required
  - High-k retrieval: Higher recall at higher token cost; relies on LLM noise filtering
- Failure signatures:
  - Wrong citations: Query reformulation drift or lexical mismatch in scoring
  - Token overflow: Too many long articles; cap tokens or summarize
  - Missing context: Multi-article questions may be underserved by k=50 if relevant chunks rank low
- First 3 experiments:
  1. Replicate retrieval on a held-out slice of the 146-question dataset; compare substring vs. embedding retriever on context and joint scores
  2. Ablate top-k (10, 25, 50, 75) to map recall vs. token-cost trade-offs
  3. Ablate query reformulation: test raw question vs. LLM-generalized query and measure changes in answer/context scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the single-agent architecture be adapted or scaled to retrieve information from multiple distinct legal acts simultaneously?
- Basis in paper: [explicit] The authors state that "a natural path for further development would be to expand the assistant to cover additional legal acts," suggesting the integration of additional retrieval tools or multi-agent architectures.
- Why unresolved: The current evaluation and implementation are strictly limited to the Polish Civil Code, utilizing a single retriever tool.
- What evidence would resolve it: Successful deployment and evaluation of a multi-agent system or routing architecture that queries a corpus of diverse legal statutes.

### Open Question 2
- Question: To what extent does the gAIus retrieval architecture maintain performance when applied to complex legal cases requiring multi-step reasoning?
- Basis in paper: [explicit] The authors note it would be valuable to "examine how large language models perform in a more realistic legal environment, where reasoning and multi-step analysis are required."
- Why unresolved: The current benchmark relies on entrance exam questions which are acknowledged to be "relatively simple" and focused on direct queries rather than extensive reasoning.
- What evidence would resolve it: Evaluation results from a dataset of complex, realistic legal scenarios requiring chained inferential steps to reach a conclusion.

### Open Question 3
- Question: What is the impact of integrating case law retrieval on the performance and argumentation quality of a statute-based legal assistant?
- Basis in paper: [explicit] The paper identifies "the addition of case retrieval relevant to the legal issue at hand" as a significant potential enhancement.
- Why unresolved: The current system focuses exclusively on statutory law (the Civil Code) and does not incorporate case law, which serves as a "powerful instrument for argumentation."
- What evidence would resolve it: A comparative study of system performance on legal tasks with and without a case retrieval module, specifically in a statutory law context.

### Open Question 4
- Question: Is the proposed substring-based scoring function sufficiently robust against semantic variations where user queries use different wording than the legal text?
- Basis in paper: [inferred] The retrieval algorithm (Algorithm 1) relies on counting matching letters between the query and document parts, which inherently favors lexical overlap over semantic understanding.
- Why unresolved: While the agent reformulates queries, the fundamental retrieval mechanism is a fuzzy text search, which may fail if a legal concept is phrased using synonyms not present in the literal statutory text.
- What evidence would resolve it: An ablation study testing retrieval recall on a dataset of paraphrased questions where lexical similarity to the original articles is intentionally minimized.

## Limitations

- The evaluation dataset is small (146 questions) and domain-specific to Polish Civil Code exam questions, limiting generalizability
- No runtime or token usage metrics are reported, which are critical for production deployment given the high-k retrieval strategy
- The paper does not compare against modern embedding-based retrievers with hybrid keyword+semantic search techniques

## Confidence

- High confidence: That gAIus outperforms raw LLM baselines on this specific dataset (GPT-3.5 and GPT-4o-mini improvements are dramatic and clearly reported)
- Medium confidence: That substring matching is "more effective than embedding-based retrieval" - based on single comparison without reporting baseline implementation details
- Low confidence: That the LLM-based query reformulation step is necessary and beneficial (no ablation provided)

## Next Checks

1. **Ablation study on query reformulation**: Run the system with raw questions instead of LLM-generalized queries on the full 146-question dataset. Measure the delta in answer/context/joint scores to quantify the benefit/cost of the reformulation step.

2. **Embedding baseline comparison with modern techniques**: Implement a strong embedding-based retriever using a legal-domain embedding model (e.g., CaseLawBERT or similar) with hybrid keyword+semantic search. Compare substring matching vs. embeddings on context and joint scores, reporting token usage and retrieval time for both.

3. **Generalization test on different legal corpus**: Apply gAIus to a different legal domain (e.g., US case law headnotes or a different country's civil code) with at least 50 test questions. Measure whether the substring-matching advantage holds or if paraphrase sensitivity causes performance collapse.