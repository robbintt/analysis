---
ver: rpa2
title: 'Using large language models to produce literature reviews: Usages and systematic
  biases of microphysics parametrizations in 2699 publications'
arxiv_id: '2503.21352'
source_url: https://arxiv.org/abs/2503.21352
tags:
- parameterizations
- publications
- microphysics
- precipitation
- parameterization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used the large language model GPT-4 Turbo to systematically
  analyze 2,699 publications on precipitation simulations using the WRF model. The
  goal was to understand how different microphysics parameterizations were used and
  their performance biases.
---

# Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications

## Quick Facts
- arXiv ID: 2503.21352
- Source URL: https://arxiv.org/abs/2503.21352
- Reference count: 22
- Primary result: GPT-4 Turbo extracted information from 2,699 WRF microphysics papers with 94% accuracy, revealing that 7/9 parameterizations tend to overestimate precipitation

## Executive Summary
This study demonstrates how large language models can efficiently process thousands of scientific publications to extract structured information about model configurations and performance. Using GPT-4 Turbo, the authors analyzed 2,699 publications on WRF microphysics parameterizations, extracting configuration details, regional usage patterns, and systematic performance biases. The approach achieved 94% average accuracy in information extraction while revealing that seven out of nine parameterizations tend to overestimate precipitation, with regional variations in bias patterns. The WSM6 and Thompson schemes emerged as the most commonly used. This work establishes a scalable methodology for synthesizing complex research domains through LLM-powered literature reviews.

## Method Summary
The study employed a two-stage GPT-4 Turbo pipeline to analyze WRF microphysics literature. First, papers were collected from Web of Science and Scopus using keyword searches, then filtered to remove duplicates and non-relevant publications. GPT-4 Turbo classified abstracts for relevance with 90.3% accuracy. For relevant papers, full-text PDFs were parsed and eight structured questions were used to extract microphysics configurations, performance metrics, and regional information. The temperature parameter was set to 0.12 to ensure deterministic outputs. Manual validation on 300 papers achieved 94% average accuracy across extraction questions, with higher accuracy for configuration details (98%) than performance metrics (86%).

## Key Results
- Seven out of nine microphysics parameterizations tend to overestimate precipitation globally
- WSM6 and Thompson schemes are the most commonly used parameterizations
- Systematic biases vary by region: overestimation concentrated in China, southeast Asia, and western US
- Single-moment schemes dominated before 2020, with double-moment schemes becoming more prevalent post-2020
- GPT-4 Turbo achieved 94% average accuracy in extracting structured information from scientific literature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured prompt engineering with low-temperature sampling enables reliable information extraction from scientific PDFs
- Mechanism: Eight targeted questions guide GPT-4 Turbo to extract specific configuration and performance fields from publication text. Temperature set to 0.12 reduces output randomness, trading creativity for determinism
- Core assumption: The relevant information appears in extractable text; tables and figures are secondary
- Evidence anchors: GPT-4 Turbo extracted information about model configurations and performance from the text of 2699 publications; temperature parameter ranges from 0 to 2; we chose a setting of 0.12 after several tests; weak direct evidence in corpus

### Mechanism 2
- Claim: Two-stage relevance filtering (keyword + LLM) reduces false positives while maintaining recall
- Mechanism: Excel keyword filtering removes coupled models (WRF-Chem, WRF-Hydro); GPT-4 Turbo then classifies abstract relevance with 90.3% accuracy against manual labels
- Core assumption: Keyword filtering does not eliminate relevant publications; LLM can generalize beyond exact term matching
- Evidence anchors: 810 out of 897 (90.3%) GPT-4 Turbo answers were accurate for relevance identification; ProfOlaf and related SLR tools validate multi-stage filtration as a standard design pattern

### Mechanism 3
- Claim: Aggregated extraction across thousands of papers reveals systematic patterns invisible in individual studies
- Mechanism: By extracting overestimation/underestimation labels and regional domains, the study identifies that 7/9 parameterizations tend to overestimate precipitation, with region-specific bias patterns
- Core assumption: Reported biases across heterogeneous study designs are comparable enough to aggregate
- Evidence anchors: Seven out of nine parameterizations tended to overestimate precipitation; systematic biases differed by regionâ€”overestimation concentrated in China, southeast Asia, western US; no direct corpus validation of aggregation methodology for bias meta-analysis

## Foundational Learning

- Concept: Bulk vs. bin microphysics parameterizations
  - Why needed here: 95% of papers used bulk schemes; understanding this distinction is required to interpret popularity trends and performance comparisons
  - Quick check question: Can you explain why bulk schemes predict particle size distribution parameters rather than discrete size bins?

- Concept: Single-moment vs. double-moment schemes
  - Why needed here: Temporal shift from one-moment (pre-2020) to two-moment (post-2020) dominance is a core finding; understanding what additional prognostic variable double-moment schemes predict is essential
  - Quick check question: What does a double-moment scheme predict that a single-moment scheme does not?

- Concept: LLM temperature parameter
  - Why needed here: The study explicitly tuned temperature to 0.12; understanding the tradeoff between determinism and creativity is required to reproduce or adapt the pipeline
  - Quick check question: What happens to output variance as temperature approaches 2.0?

## Architecture Onboarding

- Component map: Literature query -> deduplication -> keyword filtering -> relevance classification -> PDF parsing -> structured extraction -> validation -> aggregation
- Critical path: Query design -> prompt engineering -> temperature tuning -> validation sampling -> aggregation logic. Errors in prompt design propagate through all downstream analysis
- Design tradeoffs: Temperature 0.12 prioritizes consistency over nuanced interpretation; may miss context-dependent answers; aggregating "overestimation" and "underestimation" labels while ignoring "mixed" results (18% of papers) potentially oversimplifies real-world complexity; relying on text extraction misses metrics in tables/figures, reducing completeness for RMSE/CC questions
- Failure signatures: Hallucination: GPT-4 Turbo provides answers not grounded in source text; Incomplete extraction: Multiple RMSE values from multi-case studies are not fully captured; Geographic ambiguity: Domain descriptions like "southeast China" require secondary geocoding step, introducing error
- First 3 experiments: 1) Replicate relevance classification on 100-paper sample with varied temperature settings (0.0, 0.12, 0.5) to validate robustness; 2) Test extraction accuracy when prompts include explicit instructions to check tables/figures for RMSE/CC values; 3) Apply the same pipeline to a different domain (e.g., cumulus parameterizations) to assess generalizability and prompt reuse requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific physical mechanisms or structural deficiencies within the microphysics parameterizations are responsible for the systematic overestimation of precipitation observed in seven of the nine analyzed schemes?
- Basis in paper: The authors state in the conclusion that "we were unable to confirm the exact causes within this paper" regarding the overestimation tendency, and describe this gap as a "critical need for future research"
- Why unresolved: The methodology relied on text extraction to identify patterns of bias, but the LLM could not synthesize the diverse, qualitative physical explanations scattered across 2,699 papers into a unified causal mechanism
- What evidence would resolve it: Targeted sensitivity modeling experiments that isolate specific process rates (e.g., graupel conversion, snow fallout) to correlate directly with the documented overestimation biases

### Open Question 2
- Question: Why do bulk microphysics parameterizations that generally overestimate precipitation globally (such as Thompson and Morrison) exhibit a specific tendency to underestimate precipitation over the Eastern United States?
- Basis in paper: The results clearly map a "systematic bias" where schemes overestimate in China/Western US but underestimate in the Eastern US, yet the text provides no hypothesis for this distinct regional reversal
- Why unresolved: The study successfully identified the spatial heterogeneity of errors but did not correlate these errors with local environmental factors (e.g., humidity profiles, aerosol concentrations) that differ between the Eastern US and other regions
- What evidence would resolve it: A comparative regional analysis linking the spatial bias patterns to specific thermodynamic profiles or dominant weather regimes unique to the Eastern United States

### Open Question 3
- Question: How can LLM-based review methodologies be advanced to accurately extract quantitative performance metrics (e.g., RMSE, correlation coefficients) that are primarily presented in figures and tables rather than text?
- Basis in paper: The authors explicitly note a limitation in Section 2e and 5: "GPT-4 Turbo had difficulty finding these two metrics in the publications if the metrics appeared, not within the text, but within tables or figures"
- Why unresolved: GPT-4 Turbo is a text-based model and lacks robust multi-modal capabilities to reliably parse numerical data from image-based charts or complex table formatting without manual rectification
- What evidence would resolve it: The integration of vision-capable models or specialized table-parsing tools into the workflow that can convert non-text data into structured formats with accuracy comparable to the text-extraction success rates

## Limitations

- Information extraction from PDFs is limited for performance metrics, with only 86% accuracy for RMSE and correlation coefficient compared to 98% for configuration details, primarily because numerical values are often embedded in figures or tables
- Geographic mapping introduces uncertainty through secondary geocoding of domain descriptions like "southeast China" to country-level regions, which may introduce location errors
- Papers with multiple configurations or regions present ambiguity, as the pipeline does not clearly specify how to handle papers comparing multiple microphysics schemes

## Confidence

- High confidence: GPT-4 Turbo can extract structured information from scientific literature with ~94% accuracy overall
- Medium confidence: Systematic bias patterns showing 7/9 parameterizations tend to overestimate precipitation
- Medium confidence: Temporal shift from single-moment to double-moment scheme dominance post-2020

## Next Checks

1. Validate extraction completeness for performance metrics by manually reviewing 100 papers where RMSE/CC values are expected, specifically identifying those with metrics in tables/figures
2. Test multi-scheme paper handling by identifying 50 papers comparing multiple microphysics schemes and verifying whether GPT-4 captures all configurations or defaults to one
3. Geographic mapping accuracy check for 100 papers with regional descriptions, verifying the geocoding process from domain text to country-level regions