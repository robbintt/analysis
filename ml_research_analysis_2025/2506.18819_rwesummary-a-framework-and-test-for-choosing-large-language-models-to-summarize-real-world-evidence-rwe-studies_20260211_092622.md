---
ver: rpa2
title: 'RWESummary: A Framework and Test for Choosing Large Language Models to Summarize
  Real-World Evidence (RWE) Studies'
arxiv_id: '2506.18819'
source_url: https://arxiv.org/abs/2506.18819
tags:
- outcome
- summary
- visit
- prompt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RWESummary introduces a framework and benchmark for evaluating\
  \ LLMs on summarizing real-world evidence (RWE) studies. It includes one scenario\
  \ and three evaluation metrics\u2014direction of effect, accurate numbers, and completeness\u2014\
  using an LLM jury for scoring."
---

# RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies

## Quick Facts
- arXiv ID: 2506.18819
- Source URL: https://arxiv.org/abs/2506.18819
- Reference count: 0
- Primary result: Introduces framework and benchmark for evaluating LLMs on summarizing real-world evidence studies, with Gemini 2.5 models performing best

## Executive Summary
RWESummary introduces a novel framework and benchmark for evaluating large language models on summarizing real-world evidence (RWE) studies. The framework includes one scenario and three evaluation metrics—direction of effect, accurate numbers, and completeness—using an LLM jury for scoring. Tested across nine models on 13 RWE studies, Gemini 2.5 models (Flash and Pro) performed best overall. The framework enables objective comparison of LLM performance for RWE summarization tasks.

## Method Summary
The RWESummary framework was developed to evaluate LLM performance on summarizing real-world evidence studies. It consists of a single scenario involving summarization of RWE studies, with three specific metrics for evaluation: direction of effect (whether the model correctly identifies the direction of relationships), accurate numbers (whether key numerical findings are preserved), and completeness (whether the summary captures all essential elements). An LLM jury was used to score model outputs against these metrics. The framework was tested on nine different LLMs using 13 RWE studies as input.

## Key Results
- Gemini 2.5 Flash and Pro models outperformed other LLMs on the RWESummary benchmark
- Three evaluation metrics (direction of effect, accurate numbers, completeness) provided structured assessment of summarization quality
- The LLM jury approach enabled automated scoring of model performance across the benchmark

## Why This Works (Mechanism)
The framework's effectiveness likely stems from its targeted evaluation of RWE-specific elements that are critical for clinical decision-making. By focusing on direction of effect, accurate numbers, and completeness, the framework addresses the key requirements for RWE summarization: correctly identifying treatment effects, preserving quantitative findings, and ensuring comprehensive coverage of study results. The use of an LLM jury for scoring may also align well with the language-based nature of both the input (RWE studies) and output (summaries), potentially capturing nuanced aspects of summarization quality that might be missed by simpler automated metrics.

## Foundational Learning
The framework's design suggests that LLMs can be effectively evaluated for domain-specific tasks when the evaluation criteria are closely aligned with the practical requirements of that domain. The success of the three metrics indicates that RWE summarization quality can be decomposed into distinct, measurable components. The use of an LLM jury for scoring also implies that large language models may be capable of providing reliable assessments of other language models' outputs in specialized contexts, potentially reducing the need for human evaluation in certain benchmarking scenarios.

## Architecture Onboarding
The framework appears to leverage standard large language model architectures without requiring specialized modifications for the RWE summarization task. The key architectural consideration seems to be the ability of models to handle domain-specific terminology and complex quantitative relationships inherent in RWE studies. Models with stronger contextual understanding and numerical reasoning capabilities (like Gemini 2.5) performed better, suggesting that the framework effectively distinguishes between models based on their core language understanding and reasoning capabilities rather than task-specific adaptations.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark covers only 13 RWE studies, representing a relatively small sample size
- The one-scenario approach may not capture the full complexity of real-world RWE summarization tasks
- The use of an LLM jury for scoring introduces potential bias compared to human expert judgment
- The framework's applicability to different types of RWE studies (observational, registry-based, claims data) has not been fully explored

## Confidence
High Confidence: The relative performance ranking of different LLMs within the tested framework is reliable, as the methodology provides consistent and reproducible evaluation criteria.

Medium Confidence: The generalizability of the RWESummary framework to broader RWE summarization tasks is supported but requires additional validation across more diverse scenarios and study types.

Medium Confidence: The claim that Gemini 2.5 models perform best overall is based on the specific evaluation metrics and study corpus, but may not hold across all RWE summarization contexts.

## Next Checks
1. Expand the benchmark to include a larger and more diverse corpus of RWE studies (minimum 50 studies) spanning multiple therapeutic areas, study designs, and data sources to validate the framework's robustness and generalizability.

2. Conduct a head-to-head comparison between LLM jury scoring and human expert evaluation across the same RWE summaries to assess the validity and reliability of the automated scoring approach.

3. Test the framework's performance with additional summarization scenarios beyond the single scenario used in the current study, including comparative effectiveness analyses and longitudinal outcome summaries, to evaluate its applicability to real-world clinical decision support contexts.