---
ver: rpa2
title: 'The Distracting Effect: Understanding Irrelevant Passages in RAG'
arxiv_id: '2505.06914'
source_url: https://arxiv.org/abs/2505.06914
tags:
- distracting
- passages
- effect
- passage
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of distracting passages in Retrieval
  Augmented Generation (RAG) systems, where irrelevant but semantically related passages
  mislead language models during answer generation. The authors formalize a quantifiable
  measure of a passage's distracting effect with respect to a query and LLM, demonstrating
  robustness across different models through high correlation in distraction scores.
---

# The Distracting Effect: Understanding Irrelevant Passages in RAG

## Quick Facts
- arXiv ID: 2505.06914
- Source URL: https://arxiv.org/abs/2505.06914
- Reference count: 40
- Authors: Chen Amiraz; Florin Cuconasu; Simone Filice; Zohar Karnin
- One-line primary result: Fine-tuning LLMs with adversarially-selected distracting passages achieves up to 7.5% improvement in answering accuracy over conventional RAG training

## Executive Summary
This paper addresses the problem of distracting passages in Retrieval Augmented Generation (RAG) systems, where irrelevant but semantically related passages mislead language models during answer generation. The authors formalize a quantifiable measure of a passage's distracting effect with respect to a query and LLM, demonstrating robustness across different models through high correlation in distraction scores. They introduce multiple methods to obtain hard distracting passages, including answer-skewed retrieval and synthetic generation across four categories (related topic, hypothetical, negation, modal statement). Experiments show that retrieved passages become more distracting at higher ranks, and combining diverse retrieval and generation methods yields significantly more distracting passages than any single approach. The authors demonstrate practical value by fine-tuning LLMs on their hard distracting dataset, achieving up to 7.5% improvement in answering accuracy over conventional RAG training, particularly for ungrounded examples.

## Method Summary
The paper formalizes a quantifiable distracting effect metric (DE_q(p) = 1 - p_LLM(NO-RESPONSE|q,p)) and introduces a comprehensive pipeline for generating and measuring distracting passages. The method combines multiple approaches: standard retrieval (Rst), answer-skewed retrieval (Rsk) using query-answer embedding subtraction with λ=1, and four categories of synthetic distracting passages generated via few-shot prompting with Claude 3.5 Sonnet. Passages are filtered using an NLI model to exclude relevant content. For fine-tuning, they create balanced datasets (50% grounded with 4 distractors, 50% ungrounded with 5 distractors) and use LoRA-based training on Llama-3.2-3B or Llama-3.1-8B with specific hyperparameters (rank 64/128, learning rates 3e-5/5e-5, 3 epochs, neftune noise α=5, max grad norm 0.3).

## Key Results
- Fine-tuning with adversarially-selected distracting passages improves RAG robustness, achieving up to 7.5% increase in answering accuracy
- The distracting effect metric shows high correlation (0.47-0.76) across seven different LLMs from three model families, indicating model-independent properties
- Retrieved passages become more distracting at higher ranks, with reranked results showing higher distraction than non-reranked
- Combining diverse retrieval and generation methods yields significantly more distracting passages than any single approach (Figure 3)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with adversarially-selected distracting passages improves RAG robustness by exposing the model to diverse failure modes during training. The method combines multiple sources of hard negatives—retrieved passages from strong retrievers (Rst+), answer-skewed retrieval (Rsk+), and four categories of synthetic passages (Grel, Ghypo, Gneg, Gmodal). During fine-tuning, the model learns to identify and suppress irrelevant information even when it shares surface-level features with the query. This adversarial training creates a decision boundary that requires deeper semantic understanding rather than relying on simple lexical or semantic similarity cues. Core assumption: The model can learn a generalizable "irrelevance detection" capability that transfers to unseen distracting passage types.

### Mechanism 2
The distraction effect metric (DE_q(p)) captures a passage property that is largely model-independent, suggesting it reflects intrinsic information content characteristics rather than model-specific vulnerabilities. The metric measures the probability that an LLM fails to abstain when prompted with only an irrelevant passage. The high Spearman correlations (0.47-0.76) across seven different LLMs from three model families indicate that passages have consistent "distracting potential" regardless of model architecture. This consistency emerges because distracting passages exploit fundamental aspects of language understanding—semantic relatedness, plausible-sounding but incorrect assertions, and pragmatic implicature—that are common across transformer-based language models. Core assumption: The distraction effect measured in isolation (single passage + query) generalizes to the multi-passage setting typical of RAG systems.

### Mechanism 3
Stronger retrievers paradoxically increase distraction risk by ranking semantically-similar-but-irrelevant passages higher. As retriever quality improves, it becomes better at finding passages with high surface-level similarity to the query. However, this similarity doesn't distinguish between relevant (contains answer) and distracting (related topic but wrong answer) content. The reranking stage compounds this by further promoting passages that appear relevant to a cross-encoder, which may still miss the distinction between "related to query topic" and "contains correct answer." Higher-ranked positions receive more attention due to positional bias, amplifying the distraction effect. Core assumption: The semantic similarity functions learned by retrievers and rerankers don't inherently encode the distinction between topical relevance and answer correctness.

## Foundational Learning

- **Concept: Dense Retrieval and Semantic Similarity**
  - **Why needed here:** The paper's analysis depends on understanding how dense retrievers encode queries and documents, why semantically similar passages can be retrieved even without the answer, and how embedding space arithmetic (subtraction/projection) can modify retrieval behavior.
  - **Quick check question:** Given query embedding EQ(q) and answer embedding ED(a), what does the modified query embedding EQ(q) - λ·ED(a) retrieve when λ > 0?

- **Concept: Cross-Encoder Reranking**
  - **Why needed here:** The paper shows that reranking increases distraction effect at top positions. Understanding how cross-encoders process query-document pairs differently from bi-encoders explains why they can amplify rather than reduce distraction.
  - **Quick check question:** Why might a cross-encoder rank a related-but-incorrect passage higher than a bi-encoder would?

- **Concept: Fine-tuning with Adversarial Examples**
  - **Why needed here:** The paper's main application uses hard distracting passages as adversarial training data. Understanding how adversarial examples differ from standard negatives, and why they improve robustness, is essential for implementing and extending this approach.
  - **Quick check question:** How does training on synthetically generated distracting passages (Gmodal, Gneg) differ from training on retrieved negatives, and what failure modes might each address?

## Architecture Onboarding

- **Component map:**
  1. **Distracting Effect Calculator** (Algorithm 1, Page 12): Takes query q, passage p, LLM M → outputs DE_q(p) score. Uses prompt template (Figure 6) with NO-RESPONSE target.
  2. **Hard Negative Generation Pipeline**: Two parallel tracks:
     - Retrieval track: Standard retriever (Rst) and answer-skewed retriever (Rsk), each with optional reranking (Rst+, Rsk+)
     - Synthetic track: Four generators (Grel, Ghypo, Gneg, Gmodal) using Claude 3.5 Sonnet with few-shot prompts (Figures 22-25)
  3. **Relevance Filter** (Page 4, Section 3.2): NLI model from Honovich et al. (2022) to exclude false negatives
  4. **Training Data Builder** (Page 8, Section 5.1): Combines retrieved + synthetic distracting passages with relevant passages, shuffles, and creates (q, a*, P) triplets
  5. **Fine-tuning Engine**: LoRA-based training on Llama-3.2-3B or Llama-3.1-8B

- **Critical path:**
  1. Start with standard retrieval + NLI filtering to establish baseline distracting passages
  2. Implement distraction effect calculator to rank candidates
  3. Add answer-skewed retriever (Esub formulation with λ=1) for diversity
  4. Implement one synthetic generator (start with Gmodal - highest average distraction per Figure 1)
  5. Combine methods and select highest-scoring distracting passage per query
  6. Build mixed training set (50% grounded with 4 distractors, 50% ungrounded with 5 distractors)
  7. Fine-tune and evaluate on both in-distribution and OOD test sets

- **Design tradeoffs:**
  - **Retrieval vs. Synthetic Distractors:** Retrieval provides realistic noise but may miss distractor types absent from corpus; synthetic provides coverage but may not reflect inference-time distribution
  - **Single vs. Combined Methods:** Figure 3 shows no single method wins more than 52% of queries; combining provides ~10-20 percentage point improvement in distraction scores
  - **Grounded vs. Ungrounded Training Mix:** Paper uses 50/50 split; could tune this ratio based on target deployment scenario (more ungrounded for robust abstention, more grounded for accuracy with retrieved context)
  - **NLI Filtering Threshold:** Stricter filtering reduces false negatives but may discard valuable hard negatives that happen to contain partial answer information

- **Failure signatures:**
  - **Over-abstention:** Model rejects relevant passages; check if NLI filter is too aggressive or if training set has too many false negatives labeled as distracting
  - **Synthetic Pattern Memorization:** Model improves on synthetic distractor types but not on retrieved distractors; indicates need for more diverse training mix
  - **Positional Overfitting:** Model performs well when distracting passage is in training position but fails when shuffled; ensure training includes position randomization
  - **Answer-Format Mismatch:** Model learns to reject passages without exact answer span but fails on paraphrased answers; NLI filter should use entailment rather than exact match

- **First 3 experiments:**
  1. **Baseline Distraction Effect Distribution:** For your target model, compute DE_q(p) scores for top-10 retrieved passages across 100 queries. Plot distribution to understand current model's susceptibility. Compare with Figure 1 distributions to identify if your model is more/less distractable than Llama-3.1-8B.
  
  2. **Single-Method Ablation:** Implement Gmodal generator and compare its distraction scores against top-1 retrieved passages (Rst). Measure: (a) mean distraction effect, (b) unique wins % (queries where one method outperforms the other), (c) accuracy drop when combined with gold passage. Expected: Gmodal should win on 10-15% of queries where retrieval fails.
  
  3. **Training Set Size Sensitivity:** Build three training sets with 200, 800 (paper default), and 2000 queries. Fine-tune and measure: (a) overall accuracy improvement, (b) ungrounded accuracy specifically, (c) generalization to OOD datasets. This establishes the data efficiency of the approach and identifies if your domain requires more/fewer training examples than the NQ-derived sets used in the paper.

## Open Questions the Paper Calls Out

- **Question:** Can the taxonomy of distracting passages be expanded to account for additional rhetorical strategies beyond the four categories (related topic, hypothetical, negation, modal) identified?
  - **Basis in paper:** The Limitations section states that the generated categories "do not necessarily capture the full range of distracting passage types" and that expanding this taxonomy remains an open research question.
  - **Why unresolved:** The current study limited synthetic generation to four specific semantic categories; it is unknown if other linguistic structures (e.g., complex logical fallacies or domain-specific jargon) produce higher distracting effects.
  - **What evidence would resolve it:** Successful identification of new rhetorical categories that yield statistically significant increases in the distracting effect (DE_q(p)) compared to the current best methods (e.g., G_modal).

- **Question:** Does the quantifiable measure of distracting effect and the associated fine-tuning benefits generalize to non-QA RAG tasks, such as summarization or fact-checking?
  - **Basis in paper:** The authors explicitly state that their research "primarily investigated the question-answering task" and that extending the study to additional tasks "will provide a more complete picture" left for future work.
  - **Why unresolved:** The distracting effect is defined based on the probability of an LLM answering a query; tasks requiring synthesis rather than extraction may exhibit different sensitivity to irrelevant context.
  - **What evidence would resolve it:** Application of the DE_q(p) metric and "Hard" fine-tuning strategy to datasets like CNN/DailyMail or FEVER, demonstrating improved accuracy or hallucination reduction.

- **Question:** Is the observed robustness of the distracting effect measure consistent across non-English languages with different syntactic structures?
  - **Basis in paper:** While the methodology is described as language-agnostic, the authors note that "formal verification of this hypothesis remains to be carried out" as experiments were limited to English.
  - **Why unresolved:** The semantic relationships used to create hard negatives (e.g., negation or hypothetical statements) may interact differently with morphological features in non-English languages.
  - **What evidence would resolve it:** High Spearman correlation of distraction scores across multilingual models and consistent accuracy improvements when fine-tuning on non-English distracting datasets.

## Limitations

- The paper's findings are limited to English-language question-answering tasks and may not generalize to other languages or task types.
- The effectiveness of synthetic generation depends on the capabilities of the specific model (Claude 3.5 Sonnet) used, raising questions about cross-model generalization.
- The assumption that single-passage distraction scores correlate with multi-passage RAG performance hasn't been fully validated experimentally.

## Confidence

**High Confidence**: The empirical finding that retrieved passages become more distracting at higher ranks (Figure 2) is well-supported by the experimental design and consistent across different retrievers. The observation that combining multiple distraction generation methods outperforms any single approach (Figure 3) also shows strong statistical evidence.

**Medium Confidence**: The claim that distraction effect is largely model-independent (correlation matrix in Figure 4) is supported but based on a limited sample of 7 LLMs from 3 model families. The cross-model consistency might not extend to non-transformer architectures or smaller models.

**Low Confidence**: The fine-tuning results showing up to 7.5% improvement are promising but derived from a single training configuration (800 queries per dataset, 50/50 grounded/ungrounded split) without sensitivity analysis. The OOD generalization results (TIGER, SQuAD, HotpotQA) show mixed outcomes, suggesting the approach may not transfer equally well across all domains.

## Next Checks

1. **Cross-Model Generalization Test**: Compute distraction effect scores for the same set of passages across a diverse set of LLMs including non-transformer architectures (e.g., Mamba-based models) and smaller models (<1B parameters). Verify whether the high Spearman correlations observed in the paper persist across this broader model spectrum.

2. **Multi-Passage Distraction Validation**: Design an experiment where models process 5-passage contexts with varying distraction configurations (0, 1, 2, 3 distracting passages alongside gold passage). Measure whether single-passage distraction scores accurately predict accuracy drops in the multi-passage setting, or if interaction effects between multiple distractors create emergent failure modes not captured by the single-passage metric.

3. **Robustness to Synthetic Generation Model**: Reproduce the entire pipeline using different synthetic generation models (e.g., GPT-4, Llama-3.1-8B) with the same prompts. Compare the resulting distraction effect distributions and training outcomes to determine whether the approach's effectiveness depends on the specific generation model's capabilities or if it generalizes across different LLMs.