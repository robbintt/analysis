---
ver: rpa2
title: Unsupervised Hallucination Detection by Inspecting Reasoning Processes
arxiv_id: '2509.10004'
source_url: https://arxiv.org/abs/2509.10004
tags:
- arxiv
- statement
- uncertainty
- hallucination
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised hallucination detection in LLMs,
  focusing on identifying false or hallucinated content without labeled data. The
  proposed method, IRIS, leverages internal representations from LLM verification
  processes to train a classifier.
---

# Unsupervised Hallucination Detection by Inspecting Reasoning Processes

## Quick Facts
- **arXiv ID:** 2509.10004
- **Source URL:** https://arxiv.org/abs/2509.10004
- **Reference count:** 12
- **Primary result:** Proposed method IRIS significantly outperforms existing unsupervised hallucination detection methods, achieving accuracy improvements of 3.2%, 7.0%, and 10.2% on True-False, HaluEval2, and HELM datasets respectively.

## Executive Summary
This paper addresses unsupervised hallucination detection in large language models by leveraging internal representations from LLM verification processes. The proposed IRIS method prompts the LLM to carefully verify statement truthfulness and uses the uncertainty in its response as soft pseudolabels. Contextualized embeddings from this verification process serve as informative features for training a lightweight probe classifier. The approach is fully unsupervised, computationally efficient, and achieves significant accuracy improvements over existing methods while requiring minimal training data.

## Method Summary
IRIS extracts internal representations from a frozen LLM during a verification reasoning process. The method prompts the LLM to carefully verify statements with chain-of-thought reasoning, then extracts the last-token contextualized embeddings from a specific layer. The LLM's verbalized confidence in its verification serves as soft pseudolabels (0-1 values). A lightweight MLP probe (256-128-64 units) is trained on these embeddings using symmetric cross-entropy loss with soft bootstrapping to map internal states to truth values. The approach is fully unsupervised, requiring no human-annotated ground truth.

## Key Results
- IRIS achieves 3.2%, 7.0%, and 10.2% accuracy improvements over unsupervised baselines on True-False, HaluEval2, and HELM datasets respectively
- The method works effectively with minimal training data and maintains performance across different model architectures
- IRIS demonstrates robustness to adversarial prompts while maintaining computational efficiency suitable for real-time detection

## Why This Works (Mechanism)

### Mechanism 1: Verification-Induced Representation Alignment
Forcing the model to "carefully verify" statements shifts internal states to encode factual correctness features rather than syntactic completion patterns. The reasoning step retrieves relevant knowledge, making hidden states more linearly separable regarding truthfulness. This works when the model possesses parametric knowledge about the statement, but fails when the model lacks knowledge about obscure facts.

### Mechanism 2: Uncertainty-Based Pseudo-Supervision
Verbalized confidence serves as continuous soft labels (0-1) that act as noisy proxies for truth values. The probe learns to map hidden states to these confidence scores, distilling the model's self-assessment. This assumes verbalized confidence correlates with actual factuality and is better calibrated than token-entropy uncertainty measures.

### Mechanism 3: Noise-Robust Loss Optimization
Symmetric Cross Entropy combined with soft bootstrapping prevents overfitting to incorrect pseudolabels. The reverse cross-entropy term penalizes overconfident wrong predictions while bootstrapping blends noisy labels with model predictions. This provides better generalization when pseudolabels contain noise.

## Foundational Learning

- **Contextualized Embeddings (Hidden States):** IRIS extracts the vector representation of the last token from a specific Transformer layer during verification. Why needed: These embeddings capture the model's internal reasoning state about truthfulness rather than just the input statement. Quick check: Does the probe use the embedding of the input statement or the verification response? (Answer: The verification response).

- **Soft Pseudolabeling:** Instead of binary labels, continuous values (0.0 to 1.0) derived from verbalized confidence are used. Why needed: Preserves information about model confidence level and enables training without ground truth. Quick check: How does the target change during training with standard bootstrapping? (Answer: It mixes the fixed pseudolabel with the model's current prediction).

- **Calibration:** The method assumes verbalized confidence matches reality. Why needed: If a model says "90% confident" but is wrong 50% of the time, poor calibration degrades performance. Quick check: Why choose verbalized confidence over entropy-based uncertainty? (Answer: Verbalized confidence was found to be better calibrated).

## Architecture Onboarding

- **Component map:** Statement -> Prompt Template -> LLM Inference -> Confidence Extraction + Embedding Extraction -> MLP Probe -> Classification
- **Critical path:** 1) Input Statement → Prompt Construction (Append "Is this true? Let's think step by step") 2) LLM Inference → Extract verification text and hidden state 3) Parse confidence → Soft Label 4) Train Probe: Map Hidden State → Soft Label
- **Design tradeoffs:** Layer selection shows "middle layers" often peak in accuracy but varies by dataset; prompt design is robust to some variations but sensitive to adversarial constructions that force zero confidence
- **Failure signatures:** Confidence collapse (all outputs at 0 or 1), OOD generalization drops (~3%), adversarial reasoning that concludes falsely despite logical steps
- **First 3 experiments:** 1) Layer ablation: Extract embeddings from layers {16, 24, 32} to identify optimal depth 2) Label strategy comparison: Verbalized vs entropy vs random labels 3) Prompt stress test: Test with "Suggested Answer: False" to verify hidden states retain truth information

## Open Questions the Paper Calls Out

1. **Multi-layer fusion architecture:** Can sophisticated architectures effectively fuse embeddings across multiple layers to outperform single-layer selection? The paper notes preliminary fusion attempts failed but suggests further examination is required. Evidence needed: Multi-layer fusion architecture demonstrating statistically significant improvements over single-layer baselines.

2. **Mechanistic relationship between activations and truthfulness:** What is the mechanistic relationship between internal activation patterns and statement truthfulness? The paper identifies this as an open problem, noting the probe functions as a black box. Evidence needed: Causal tracing or ablation studies identifying specific neural circuits responsible for storing and retrieving truthful knowledge.

3. **Passage-level hallucination detection:** How can the framework be adapted to detect hallucinations in full passages containing mixed factual and non-factual claims? The paper identifies the need for comprehensive systems to decompose passages and filter statements. Evidence needed: A pipeline that automates claim decomposition and applies IRIS to long-form text while maintaining high detection accuracy.

## Limitations

- Effectiveness depends heavily on the proxy LLM's parametric knowledge coverage, with performance degrading when the model lacks knowledge about specific entities or domains
- Performance drops approximately 3% on out-of-distribution datasets, indicating limited generalization across domains
- Layer selection optimization is empirical rather than principled, requiring significant hyperparameter tuning for different model architectures

## Confidence

**High Confidence:** Empirical performance improvements over unsupervised baselines (3.2%, 7.0%, 10.2% accuracy gains) are well-supported with clear methodology and reproducible results.

**Medium Confidence:** Verification-induced representations being more linearly separable is supported by ablation studies but lacks theoretical grounding for why specific layers or reasoning steps contribute most.

**Low Confidence:** Verbalized confidence being "better calibrated" than entropy-based uncertainty is based on limited comparison without broader calibration analysis across multiple models and domains.

## Next Checks

1. **Knowledge Gap Sensitivity Analysis:** Systematically evaluate IRIS performance across statements requiring different levels of parametric knowledge (common facts vs. obscure entities vs. domain-specific knowledge) to quantify the method's limitations.

2. **Cross-Domain Generalization Test:** Train IRIS on one dataset and test on completely different domains (medical, legal, technical) to assess whether multi-domain training improves robustness compared to single-domain training.

3. **Mechanism Isolation Experiment:** Create controlled ablations testing each component independently: statement-only vs verification-response embeddings, verbalized confidence vs entropy uncertainty, standard CE vs SCE loss to quantify relative contributions.