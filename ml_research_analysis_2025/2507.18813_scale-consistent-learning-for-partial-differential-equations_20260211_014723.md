---
ver: rpa2
title: Scale-Consistent Learning for Partial Differential Equations
arxiv_id: '2507.18813'
source_url: https://arxiv.org/abs/2507.18813
tags:
- scale
- domain
- neural
- equation
- operator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scale-consistent learning framework for
  partial differential equations (PDEs) that addresses the limitation of neural operators
  being trained on specific scales and unable to generalize across different scales.
  The authors propose a data augmentation scheme based on the scale-consistency properties
  of PDEs, which allows the model to create new training instances with various scales
  via sub-sampling and super-sampling.
---

# Scale-Consistent Learning for Partial Differential Equations

## Quick Facts
- **arXiv ID**: 2507.18813
- **Source URL**: https://arxiv.org/abs/2507.18813
- **Reference count**: 40
- **Primary result**: Scale-consistent framework achieves 34% average error reduction on PDEs by enabling neural operators to generalize across different scales

## Executive Summary
This paper addresses a fundamental limitation in neural operators for partial differential equations: their inability to generalize across different spatial and temporal scales. The authors introduce a scale-consistency framework that leverages the inherent scale-invariance properties of PDEs to create a data augmentation scheme. By sub-sampling and super-sampling training instances, the model learns to handle various scales without requiring explicit multi-scale training data. The approach combines a scale-consistency loss with a scale-informed neural operator architecture that takes scale parameters as input, enabling extrapolation to scales not seen during training.

## Method Summary
The scale-consistency framework introduces two key innovations. First, a data augmentation scheme that creates new training instances by rescaling existing data through sub-sampling (zooming in) and super-sampling (zooming out) operations. Second, a scale-informed neural operator architecture that incorporates the scale parameter directly into the model with weight-sharing parameterization. The architecture uses an adaptive U-shape design to capture a wide range of scales effectively. A domain-decomposition algorithm is also proposed for iterative refinement at test time, particularly useful for boundary value problems. The framework is validated across four benchmark PDEs: Burgers' equation, Darcy Flow, Helmholtz equation, and Navier-Stokes equations.

## Key Results
- Scale-consistency loss achieves 34% average error reduction compared to baseline FNO models across all test problems
- Domain-decomposition refinement algorithm reduces error by 40% specifically on Darcy Flow problems
- The framework successfully extrapolates to scales not present in the training data, demonstrating genuine scale generalization
- Performance improvements are consistent across both forward and inverse PDE problems

## Why This Works (Mechanism)
The scale-consistency framework works by exploiting the inherent scale-invariance properties of PDEs. Since many PDEs maintain their form under spatial and temporal rescaling, the model can learn to solve the same underlying equation at different scales. By training with augmented data that spans multiple scales, the neural operator develops scale-agnostic representations. The scale parameter input allows the model to adapt its internal representations based on the problem scale, while the adaptive U-shape architecture provides the flexibility to capture features at different resolutions. The domain-decomposition refinement leverages local information to iteratively improve predictions, particularly effective for problems with strong boundary effects.

## Foundational Learning

**Scale-invariance in PDEs**: Many PDEs maintain their mathematical form under spatial or temporal rescaling, allowing solutions at different scales to be related through simple transformations. This property is crucial because it enables the data augmentation approach - if the PDE doesn't fundamentally change with scale, then solving it at one scale provides information about all scales.

*Why needed*: Without scale-invariance, rescaling training data would not produce valid training examples for the same underlying PDE.

*Quick check*: Verify that the target PDEs exhibit scale-invariance by checking if they remain unchanged under appropriate coordinate transformations.

**Neural operator generalization**: Standard neural operators trained on specific discretizations struggle to generalize to different resolutions or domains because they learn scale-dependent features. The scale-consistency framework addresses this by forcing the model to learn scale-invariant representations.

*Why needed*: Traditional neural operators are essentially interpolators within their training scale range, unable to handle resolution changes or extrapolation to new scales.

*Quick check*: Test whether a standard FNO model's performance degrades significantly when evaluated at scales different from training.

**Adaptive U-shape architectures**: These architectures dynamically adjust their receptive fields based on input scale parameters, allowing the model to process both fine-scale and coarse-scale features effectively within the same framework.

*Why needed*: Fixed receptive fields in standard architectures limit their ability to capture multi-scale phenomena efficiently.

*Quick check*: Compare performance with and without the adaptive components when handling problems spanning multiple orders of magnitude in scale.

## Architecture Onboarding

**Component map**: Data augmentation (sub-sampling/super-sampling) -> Scale-informed neural operator (with adaptive U-shape) -> Scale-consistency loss -> Domain-decomposition refinement

**Critical path**: The scale-consistency loss optimization during training is the critical path, as it forces the model to learn scale-invariant representations. The data augmentation provides the diverse scale training data, while the scale-informed architecture enables effective processing of the scale parameters.

**Design tradeoffs**: The framework trades increased training complexity (due to augmented data and scale parameters) for improved generalization across scales. The adaptive U-shape architecture adds parameters but provides better scale coverage than fixed architectures. The domain-decomposition refinement adds inference-time computation but significantly improves accuracy for boundary problems.

**Failure signatures**: The approach may fail on PDEs without strong scale-invariance properties, or when the scale augmentation doesn't adequately sample the relevant scale space. The domain-decomposition algorithm might struggle with highly nonlinear problems where local refinements don't converge to the global solution.

**3 first experiments**:
1. Test scale-consistency loss effectiveness by training with and without it on the same augmented dataset
2. Evaluate the impact of different augmentation ratios (sub-sampling/super-sampling factors) on final performance
3. Compare the adaptive U-shape architecture against fixed receptive field alternatives on multi-scale problems

## Open Questions the Paper Calls Out
None

## Limitations
- Data augmentation relies on sub-sampling and super-sampling that may not fully capture complexity of highly nonlinear or chaotic PDE systems
- Adaptive U-shape architecture performance primarily validated on standard benchmark problems, with uncertain effectiveness on complex real-world PDEs
- Domain-decomposition refinement algorithm tested mainly on Darcy Flow, with limited validation across different PDE categories

## Confidence

**Confidence Labels:**
- Scale-consistency loss effectiveness: High
- Data augmentation methodology: Medium
- Adaptive U-shape architecture performance: Medium
- Domain-decomposition refinement algorithm: Low

## Next Checks

1. Test the framework on highly nonlinear PDEs with chaotic solutions, such as the Kuramoto-Sivashinsky equation, to assess robustness beyond the current benchmark set.

2. Evaluate the scale-consistency approach with different sampling strategies (e.g., adaptive sampling based on solution gradients) to determine if the current fixed sub-sampling/super-sampling approach is optimal.

3. Conduct ablation studies specifically isolating the contribution of the adaptive U-shape architecture versus the scale-consistency loss to better understand which components drive the performance improvements.