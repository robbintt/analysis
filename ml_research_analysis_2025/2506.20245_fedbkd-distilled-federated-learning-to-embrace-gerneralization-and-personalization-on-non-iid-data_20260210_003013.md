---
ver: rpa2
title: 'FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization
  on Non-IID Data'
arxiv_id: '2506.20245'
source_url: https://arxiv.org/abs/2506.20245
tags:
- data
- local
- learning
- global
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedBKD addresses the challenge of federated learning on non-IID
  data by proposing a bidirectional knowledge distillation framework that achieves
  both strong global generalization and effective personalization. The method introduces
  a data-free generator that synthesizes high-quality client-alike data using local
  models as discriminators, eliminating the need for public datasets and reducing
  privacy risks.
---

# FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data

## Quick Facts
- arXiv ID: 2506.20245
- Source URL: https://arxiv.org/abs/2506.20245
- Authors: Yushan Zhao; Jinyuan He; Donglai Chen; Weijie Luo; Chong Xie; Ri Zhang; Yonghong Chen; Yan Xu
- Reference count: 40
- Primary result: FedBKD achieves 88% accuracy on CIFAR-10 with 100 clients and 20 classes per client, outperforming existing federated learning methods

## Executive Summary
FedBKD addresses the challenge of federated learning on non-IID data by proposing a bidirectional knowledge distillation framework that achieves both strong global generalization and effective personalization. The method introduces a data-free generator that synthesizes high-quality client-alike data using local models as discriminators, eliminating the need for public datasets and reducing privacy risks. FedBKD then employs dual distillation—global-to-local and local-to-global—to enhance feature extraction in local models and improve the global model's adaptability. Experiments on CIFAR-10, CIFAR-100, FEMNIST, and Sent140 under various non-IID settings show that FedBKD achieves state-of-the-art performance in both personalization and generalization.

## Method Summary
FedBKD combines federated learning with knowledge distillation and a data-free generator to address non-IID data challenges. Local models are trained on private data using a FedRep-style approach, then uploaded to the server. The server aggregates these models, then trains a generator to produce synthetic data that mimics client distributions without accessing real data. This synthetic data enables bidirectional knowledge distillation: global-to-local transfers generalizable features to improve local models, while local-to-global teaches the global model to adapt to diverse distributions. The method eliminates the need for public datasets while achieving superior personalization and generalization performance.

## Key Results
- FedBKD achieves 88% accuracy on CIFAR-10 with 100 clients (20 classes per client), outperforming FedAvg and other baselines
- On FEMNIST with 100 clients (10 classes each), FedBKD reaches 84.65% personalization accuracy
- Sent140 personalization accuracy reaches 84.28%, with 10-shot adaptation accuracy of 76.4%
- Ablation studies show both the data-free generator and bidirectional distillation are critical for performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data-free generator produces synthetic data that captures real client data patterns, enabling effective distillation without privacy leakage
- Mechanism: Local models serve as frozen discriminators in a GAN framework. The generator maximizes discriminator confidence (cross-entropy loss) while diversity regularization prevents mode collapse. This produces feature maps similar to real data distributions without accessing actual client data
- Core assumption: Frozen pre-trained classifiers contain sufficient information about data distributions to guide meaningful synthetic data generation
- Evidence anchors:
  - [abstract] "data-free generator that synthesizes high-quality client-alike data using local models as discriminators, eliminating the need for public datasets"
  - [section 3.2] "Loh = 1/n * Σ Hcross(D(xi_G), ti)" and "LG = Loh + λLms"
  - [section 4.6] Figure 2 shows L1 distance between synthetic and real data is lower and more concentrated than random data

### Mechanism 2
- Claim: Global-to-local distillation improves local models' feature extraction ability by transferring knowledge from the globally aggregated model
- Mechanism: After parameter averaging, the global model's representation layers contain diverse patterns from multiple clients. Local models freeze their classification layers and use KL divergence to align their representation layers with the global model on synthetic data, absorbing cross-client knowledge
- Core assumption: Global model's representation layers encode useful generalizable features that local models can leverage without overwriting client-specific knowledge
- Evidence anchors:
  - [abstract] "global-to-local and local-to-global—to enhance feature extraction in local models"
  - [section 3.3] "Li→g = KL[θR_i(t), θR_g(t)](xi_G)" with classification layer frozen
  - [section 4.5] Ablation shows removing global-to-local distillation drops CIFAR-10 accuracy from 88% to 86.51%

### Mechanism 3
- Claim: Local-to-global distillation reduces parameter drift and improves global model generalization by teaching it to adapt to diverse local distributions
- Mechanism: The global model sequentially distills from each local model's representation layers using their respective synthetic data. This teaches the global model to recognize patterns across heterogeneous distributions, making it a better initialization point for new clients
- Core assumption: Sequential distillation from local models can approximate ensemble knowledge transfer without requiring simultaneous access to all local data
- Evidence anchors:
  - [abstract] "improve the global model's adaptability"
  - [section 3.3] "Lg→i = kl[θR_g(t), θR_i(t)](xi_G)" and "freezing classification layer to better maintain its generalizability"
  - [section 4.5] Without local-to-global distillation, generalization accuracy drops (e.g., CIFAR-10 from 91.37% to lower performance)

## Foundational Learning

- **Concept: Knowledge Distillation (KL Divergence)**
  - Why needed here: Core mechanism for bidirectional knowledge transfer between global and local models
  - Quick check question: Can you explain why KL divergence measures how one probability distribution differs from a reference distribution?

- **Concept: Federated Learning with Non-IID Data**
  - Why needed here: FedBKD specifically addresses weight divergence caused by heterogeneous data distributions across clients
  - Quick check question: Why does FedAvg's simple parameter averaging fail when client data distributions differ significantly?

- **Concept: GAN Training with Frozen Discriminators**
  - Why needed here: The data-free generator relies on this technique to produce synthetic data without accessing real client data
  - Quick check question: What happens to generator output quality if the discriminator is not a good representation of the target data distribution?

## Architecture Onboarding

- **Component map**: Client Side: Local model (θi = θR_i ∘ θC_i), local dataset Di, local training loop → Server Side: Global model θg, generator G, aggregation module, distillation coordinator → Communication: Model parameters (θi) uploaded, global model (θg) downloaded, synthetic data generated server-side

- **Critical path**:
  1. Client initialization: θR_i ← θR_g (copy global representation layers)
  2. Local training: τ epochs on classifier, then update representation layers
  3. Upload: Client sends θi to server
  4. Aggregation: θg ← average of uploaded models
  5. Generator training: G trained against sampled local models as discriminators
  6. Synthetic data generation: xi_G ← G(random vectors)
  7. Bidirectional distillation: Global→Local (4 epochs), then Local→Global (1 epoch)
  8. Distribute updated models back to clients

- **Design tradeoffs**:
  - Synthetic data quantity vs. computation: Paper uses 5000 for CIFAR-10 (s=20), 1000 for others; more data increases distillation quality but costs computation
  - Distillation epochs: Global→Local (4) vs Local→Global (1) asymmetry reflects that local models need more knowledge absorption
  - Feature-level synthesis vs. raw data: Generating feature maps (not images) provides additional privacy but limits interpretability

- **Failure signatures**:
  - GAN overfitting: After ~6 epochs (Figure 4), synthetic data loses diversity, distillation quality drops
  - Extreme non-IID (100 clients, 2 classes each): Performance gaps widen; bidirectional distillation helps but cannot fully close
  - Cold start: First round has poor synthetic data quality since local models are randomly initialized

- **First 3 experiments**:
  1. **Reproduce ablation on CIFAR-10 (100, 5)**: Run FedBKD with and without the data-free generator (random vs. GAN synthetic data). Expect ~2-3% accuracy gap per Table 2
  2. **Test distillation directionality**: Compare (global→local only), (local→global only), and (bidirectional) on Sent140. Verify that bidirectional outperforms both unidirectional variants
  3. **Generalization stress test**: Train on 90% of clients, hold out 10%. Fine-tune global model on held-out clients with frozen representation layers. Measure few-shot adaptation speed compared to FedAvg and FedRep baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the FedBKD framework be effectively extended to non-IID tasks beyond classification, such as regression or generation tasks?
- Basis in paper: [explicit] The conclusion states, "Currently, FedBKD can only work for non-IID classification. In the future, we will extend FedBKD’s capability to cover more complicated machine learning tasks."
- Why unresolved: The current methodology relies on classification-specific loss functions (e.g., cross-entropy for the generator's discriminator loss $L_{oh}$ in Eq. 6) and argmax operations for synthetic data labeling (Eq. 4).
- What evidence would resolve it: A modification of the distillation and generator losses to support continuous labels, demonstrating comparable performance on regression benchmarks.

### Open Question 2
- Question: How does FedBKD perform under system heterogeneity where clients possess different model architectures?
- Basis in paper: [inferred] The algorithm aggregates global parameters via direct averaging (Eq. 13: $\theta_g = \frac{1}{m} \sum \theta_i$) and distills representation layers assuming consistent dimensions (Eq. 14).
- Why unresolved: The paper assumes a homogeneous environment where all clients share the same network structure, allowing for direct weight averaging and feature matching, which is often infeasible in cross-device FL.
- What evidence would resolve it: Experimental results using distinct client backbones (e.g., varying layers or widths) utilizing knowledge distillation for aggregation rather than weight averaging.

### Open Question 3
- Question: Can the training process for the data-free generator be automated to prevent overfitting without requiring manual tuning of training epochs?
- Basis in paper: [inferred] The discussion notes that model performance degrades if GAN training exceeds a specific threshold (Fig. 4) due to overfitting, suggesting this hyperparameter is sensitive and currently requires manual optimization.
- Why unresolved: The paper identifies the risk of the generator losing general feature information but does not propose an adaptive stopping criterion or regularization method to automate this critical hyperparameter.
- What evidence would resolve it: The inclusion of an adaptive regularization technique or convergence metric that maintains high accuracy without manually fixing the number of GAN training epochs.

## Limitations
- Architecture specifications are incomplete: The exact neural network architectures for client classifiers and the data-free generator are not provided, creating potential reproducibility gaps
- Hyperparameter sensitivity: Critical values like diversity regularization weight λ, local update epochs τ, and GAN training settings are unspecified, likely affecting performance
- Evaluation scope: While personalization and generalization are evaluated, the paper does not address computational overhead of the bidirectional distillation process or communication costs

## Confidence

- **High confidence**: FedBKD achieves SOTA performance on personalization metrics across multiple datasets (CIFAR-10 88%, FEMNIST 84.65%, Sent140 84.28%) and demonstrates clear gains from bidirectional distillation
- **Medium confidence**: The data-free generator reliably produces high-quality synthetic data that captures client data distributions without privacy leakage, though GAN stability could vary with different architectures
- **Medium confidence**: The generalization improvements from local-to-global distillation are well-supported, but the sequential nature of the process may create order-dependent effects not fully explored

## Next Checks
1. **Architecture sensitivity test**: Reproduce core results using different client model architectures (e.g., ResNet-18 vs. smaller CNNs) to verify performance gains are architecture-agnostic
2. **Gradient-based attribution analysis**: Apply Grad-CAM to synthetic data and distilled representations to verify that the generator and distillation process are capturing semantically meaningful features
3. **Robustness to initialization**: Evaluate FedBKD's performance across multiple random seeds and different starting model weights to quantify variance in the GAN and distillation stability