---
ver: rpa2
title: 'Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language
  Models for Better Understanding of AI'
arxiv_id: '2505.02859'
source_url: https://arxiv.org/abs/2505.02859
tags:
- evaluation
- artifact
- chatbot
- shap
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel reference architecture for improving
  machine learning model interpretability by integrating fine-tuned large language
  models (LLMs) with explainable AI (XAI) techniques. The approach leverages a fine-tuned
  LLM to provide interactive, context-aware explanations of SHAP values, making complex
  model predictions more understandable to users.
---

# Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI

## Quick Facts
- **arXiv ID:** 2505.02859
- **Source URL:** https://arxiv.org/abs/2505.02859
- **Reference count:** 7
- **Primary result:** A reference architecture that fine-tunes LLMs to provide interactive, context-aware explanations of SHAP values significantly improves model interpretability for users with less XAI expertise.

## Executive Summary
This paper presents a novel reference architecture for improving machine learning model interpretability by integrating fine-tuned large language models (LLMs) with explainable AI (XAI) techniques. The approach leverages a fine-tuned LLM to provide interactive, context-aware explanations of SHAP values, making complex model predictions more understandable to users. The reference architecture was instantiated in a battery state-of-health prediction use case and evaluated through expert interviews and an online survey (n=61). Results show the solution significantly improves interpretability and clarity, especially for users with less XAI expertise. Fine-tuning the LLM in three steps—domain-specific, global explanation, and human alignment—led to improved performance across all evaluated metrics. The artifact offers a generalizable blueprint for enhancing trust and transparency in ML applications across various domains.

## Method Summary
The method employs a three-step sequential fine-tuning approach using LoRA adapters on Llama-2-13b-chat-hf. First, the model is trained on domain-specific texts about SHAP and batteries. Second, it learns to summarize SHAP analysis results. Finally, it is aligned with human preferences through structured question-answer pairs. The system takes CatBoost model predictions and SHAP values as input, providing interactive explanations via a chatbot interface. Evaluation was conducted through expert interviews and an online survey measuring interpretability, clarity, and cognitive effort.

## Key Results
- The fine-tuned LLM chatbot significantly improves interpretability and clarity of SHAP explanations compared to baseline methods.
- Users with less XAI expertise particularly benefit from the interactive explanation approach.
- Sequential three-step fine-tuning (domain-specific → global explanation → human alignment) consistently improved model performance across all evaluation metrics.
- The reference architecture demonstrates generalizability beyond the specific battery state-of-health use case.

## Why This Works (Mechanism)
The approach works by transforming static, complex SHAP value visualizations into interactive, context-aware explanations through fine-tuned language models. The three-step fine-tuning process progressively specializes the LLM: first grounding it in domain knowledge, then teaching it to interpret SHAP outputs, and finally aligning its explanations with human communication preferences. This creates a system that can translate technical model outputs into accessible narratives while maintaining accuracy and relevance to the specific prediction context.

## Foundational Learning
- **SHAP Values:** A game-theoretic approach to explain individual model predictions by quantifying feature contributions. *Why needed:* Provides the quantitative basis for explanations that the LLM must interpret and communicate. *Quick check:* Can you explain how SHAP values differ from feature importance scores?
- **LoRA Fine-Tuning:** A parameter-efficient method that injects low-rank matrices into pre-trained model weights during adaptation. *Why needed:* Enables effective specialization of large LLMs without full retraining. *Quick check:* Does LoRA preserve general capabilities while adding domain-specific knowledge?
- **Explainable AI (XAI):** Methods and techniques that make machine learning model decisions understandable to humans. *Why needed:* The fundamental problem space this architecture addresses. *Quick check:* What are the key differences between global and local XAI explanations?
- **Context Window Management:** Techniques for effectively using the limited token capacity of LLMs to include relevant information. *Why needed:* Critical for ensuring SHAP values and prediction context fit within model constraints. *Quick check:* How does context window size affect the comprehensiveness of explanations?
- **Human-AI Alignment:** The process of training AI systems to produce outputs that match human expectations and preferences. *Why needed:* Ensures explanations are not just technically correct but also useful and accessible. *Quick check:* What metrics can evaluate whether an explanation aligns with human understanding?

## Architecture Onboarding

**Component Map:** CatBoost Model → SHAP Value Generator → LLM Fine-Tuning Pipeline → Chatbot Interface → User Feedback Loop

**Critical Path:** Prediction generation → SHAP computation → Context construction → LLM inference → User interaction → Feedback collection

**Design Tradeoffs:** The architecture trades model complexity and training time for improved interpretability. Using LoRA instead of full fine-tuning reduces computational costs but may limit adaptation depth. The sequential fine-tuning approach balances specialization with maintaining general capabilities, though it requires careful curriculum design.

**Failure Signatures:** Hallucinations occur when the model generates factually incorrect information not supported by the SHAP context. Catastrophic forgetting manifests as loss of general conversational ability after specialized training. Poor interpretability results from explanations that are technically accurate but fail to connect with user mental models.

**First Experiments:**
1. **Sanity Check:** Test the base LLM's ability to interpret sample SHAP values before any fine-tuning.
2. **Domain Grounding:** Evaluate the model after Step 1 fine-tuning to ensure it understands battery and SHAP terminology.
3. **Explanation Quality:** Compare explanations from Step 2 vs. Step 3 fine-tuning to measure improvement in human alignment.

## Open Questions the Paper Calls Out
- How does the artifact perform when adapted for unsupervised ML models or different XAI techniques beyond SHAP?
- Can Retrieval-Augmented Generation (RAG) effectively reduce hallucinations compared to the current fine-tuning approach?
- How does the utility of the chatbot evolve during long-term usage in a naturalistic environment?
- How can the system be adapted to maintain utility for users with high XAI proficiency?

## Limitations
- The approach is currently validated only on supervised models using SHAP values, limiting generalizability.
- The system may struggle with complex, high-dimensional SHAP outputs that exceed context window limits.
- Expert users may find the explanations overly simplified, suggesting a need for adaptive explanation depth.
- The three-step fine-tuning process requires substantial domain-specific data and expertise to implement effectively.

## Confidence
- **Method Validity:** High - The sequential fine-tuning approach is well-established and appropriately applied.
- **Evaluation Design:** Medium - While the sample size is reasonable, the study lacks longitudinal assessment of real-world usage.
- **Generalizability:** Medium - Results are promising but limited to one specific use case and XAI technique.
- **Technical Implementation:** High - The architecture leverages proven techniques (LoRA, SHAP, LLMs) in a coherent manner.

## Next Checks
1. Replicate the three-step fine-tuning process with the specified hyperparameters and evaluate perplexity reduction at each stage.
2. Conduct a user study comparing the fine-tuned chatbot against baseline explanation methods on a different XAI technique.
3. Test the system's robustness by introducing edge cases where SHAP values are ambiguous or contradictory.