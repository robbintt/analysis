---
ver: rpa2
title: 'Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models
  to Ask for Information'
arxiv_id: '2508.11252'
source_url: https://arxiv.org/abs/2508.11252
tags:
- problems
- information
- lrms
- problem
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the gap in evaluating large reasoning models
  (LRMs) for their ability to ask clarifying questions on incomplete mathematical
  problems, rather than only solving well-defined problems. To this end, the authors
  introduce CRITIC-math, a dataset containing 1.3K test and 5.3K training samples
  with two types of incompleteness: missing goal and missing premises.'
---

# Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information

## Quick Facts
- **arXiv ID**: 2508.11252
- **Source URL**: https://arxiv.org/abs/2508.11252
- **Reference count**: 40
- **Primary result**: Current large reasoning models struggle to proactively ask clarifying questions on incomplete mathematical problems, achieving only 25% clarification ratio on implicit prompts and 40% accuracy even with explicit prompting.

## Executive Summary
This paper addresses a critical gap in evaluating large reasoning models (LRMs) beyond their ability to solve well-defined mathematical problems. The authors introduce CRITIC-math, a dataset of 1.3K test and 5.3K training samples containing incomplete mathematical problems with either missing goals or missing premises. Through systematic evaluation of state-of-the-art LRMs, the research reveals that current models struggle significantly with proactively seeking clarification, exhibiting overthinking and hallucination instead of asking questions. The study demonstrates that supervised fine-tuning can improve information-seeking capabilities, though a fundamental dilemma exists between deep-thinking and information-seeking behaviors in LRMs.

## Method Summary
The authors created CRITIC-math by rewriting existing well-defined mathematical problems from benchmarks like GSM8K, MATH, and SVAMP to introduce incompleteness through either missing goals (problem context missing) or missing premises (key information missing). The dataset was manually verified for quality and contains 1.3K test and 5.3K training samples. State-of-the-art LRMs were evaluated using both implicit prompts (asking them to solve incomplete problems) and explicit prompts (directly instructing them to ask for missing information). The evaluation measured clarification ratios (whether models asked questions) and accuracy of the questions asked. Additionally, the authors performed supervised fine-tuning on the Qwen2.5-32B-Instruct model using CRITIC-math, creating CRITIC-Qwen, to assess whether training on incomplete problems could improve information-seeking behavior.

## Key Results
- LRMs achieve only 25% clarification ratio on implicit prompts and 50% with explicit prompting when asked to solve incomplete mathematical problems
- Accuracy of asking appropriate questions improves from 40% to 65% with explicit prompting
- CRITIC-Qwen (SFT model) achieves 87.86% accuracy on well-defined problems and 78.42% on missing premises
- Overthinking and hallucination are identified as primary failure modes when LRMs fail to ask clarifying questions
- SFT models trained on both well-defined and incomplete problems outperform those trained only on well-defined problems

## Why This Works (Mechanism)
The research reveals that LRMs' failure to ask clarifying questions stems from their training paradigm, which emphasizes deep reasoning on well-defined problems rather than interactive problem-solving. When faced with incomplete information, models default to generating plausible but potentially incorrect solutions rather than seeking clarification, a behavior pattern reinforced by standard training objectives. The mechanism involves a tension between the model's trained tendency to produce confident answers and the need to recognize uncertainty and request additional information.

## Foundational Learning
**Why needed**: Understanding how LRMs process incomplete information requires grasping the difference between problem-solving and interactive dialogue.
**Quick check**: Can the model distinguish between missing premises and missing goals?

**Why needed**: The distinction between implicit and explicit prompting reveals how LRMs interpret task instructions.
**Quick check**: Does explicit instruction to ask questions significantly improve clarification ratios?

**Why needed**: SFT methodology demonstrates how targeted training can reshape model behavior.
**Quick check**: Does training on incomplete problems improve performance on well-defined problems?

**Why needed**: Identifying overthinking vs. hallucination failure modes helps diagnose model behavior.
**Quick check**: Can we categorize model responses into seeking clarification vs. generating solutions?

**Why needed**: Understanding the tradeoff between deep-thinking and information-seeking reveals architectural limitations.
**Quick check**: Does improved information-seeking come at the cost of reasoning depth?

## Architecture Onboarding

**Component map**: Input problem → Clarification detection module → Information request generator → Response formulation

**Critical path**: Incomplete problem → Model reasoning → Decision point (ask vs. solve) → Output generation

**Design tradeoffs**: The tension between producing confident answers (trained behavior) versus admitting uncertainty and asking questions represents a fundamental architectural challenge in LRMs.

**Failure signatures**: Overthinking manifests as excessive reasoning without seeking clarification; hallucination appears as plausible but incorrect assumptions about missing information.

**3 first experiments**: 1) Test CRITIC-Qwen on cross-domain incomplete problems (science, logic puzzles) to assess generalization. 2) Implement a confidence threshold mechanism that triggers clarification requests when uncertainty exceeds a threshold. 3) Compare human performance on CRITIC-math problems versus LRM performance to establish baseline expectations.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset size (1.3K test samples) may not capture the full diversity of real-world incomplete problems
- The construction method relies on rewriting existing problems, potentially missing novel forms of incompleteness
- The effectiveness of SFT may be limited by dataset size and could lead to overfitting
- Results are based on English-language mathematical problems, limiting generalizability to other domains and languages

## Confidence

**High Confidence**: Empirical findings on LRMs' poor performance with implicit clarification prompts (25% CR) and improvement with explicit prompting (50% CR) are well-supported. Identification of overthinking and hallucination as failure modes is well-documented.

**Medium Confidence**: SFT methodology shows improvement in information-seeking but may be dataset-specific. The effectiveness of training on both well-defined and incomplete problems shows promise but needs broader validation.

**Low Confidence**: Generalizability of the "dilemma between deep-thinking and information-seeking" beyond mathematical domains remains unproven. Specific mechanisms for mode transitions need more rigorous investigation.

## Next Checks
1. **Dataset Expansion and Diversity Test**: Expand CRITIC-math to include problems from diverse mathematical domains and multiple languages to validate robustness across different problem types and linguistic contexts.

2. **Cross-Model Generalization**: Evaluate the SFT methodology across a broader range of LRMs to determine if improvements in information-seeking are model-agnostic or specific to the Qwen2.5-32B-Instruct baseline.

3. **Real-World Deployment Analysis**: Conduct a user study with human experts interacting with both baseline and SFT-enhanced LRMs on incomplete real-world mathematical problems to assess practical utility and identify additional failure modes.