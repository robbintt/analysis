---
ver: rpa2
title: 'Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for
  Ukrainian Exam Tasks'
arxiv_id: '2503.13988'
source_url: https://arxiv.org/abs/2503.13988
tags:
- tasks
- language
- answer
- solution
- letter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap of compact language models
  on reasoning-intensive tasks in underrepresented languages like Ukrainian. The authors
  fine-tune LLaMA and Gemma models using parameter-efficient techniques combined with
  chain-of-thought prompting and task-specific knowledge generation.
---

# Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought for Ukrainian Exam Tasks

## Quick Facts
- **arXiv ID:** 2503.13988
- **Source URL:** https://arxiv.org/abs/2503.13988
- **Reference count:** 40
- **Primary result:** Fine-tuned LLaMA and Gemma models with CoT achieve up to 17.4% improvement on complex matching tasks and 1.6% overall, outperforming GPT-4o mini and Mistral Large on Ukrainian exam tasks.

## Executive Summary
This paper demonstrates that compact language models can achieve competitive performance on reasoning-intensive tasks in underrepresented languages through parameter-efficient fine-tuning with chain-of-thought prompting. The authors fine-tune LLaMA and Gemma models on Ukrainian language exam tasks using low-rank adapters and CoT annotation, achieving significant improvements over answer-only tuning. Their approach is particularly effective for complex matching tasks, where models must maintain logical consistency across multiple output tokens. The study validates that efficient fine-tuning of compact models can deliver performance competitive with larger proprietary models while maintaining interpretability through explicit reasoning steps.

## Method Summary
The authors fine-tune LLaMA 3.1-8B, LLaMA 3.2-3B, and Gemma 2-9B models on Ukrainian exam tasks using parameter-efficient techniques. They apply 4-bit quantization (QLoRA) to fit models on a single A100 GPU, then train LoRA adapters (rank 16) on the frozen base models. Training uses instruction-tuned variants with learning rate 3e-04 for 4 epochs. The key innovation is incorporating chain-of-thought reasoning by training models to generate task topics before solutions. Models are evaluated on a test set of 108 tasks, with answers extracted after the "Відповідь:" keyword. Best checkpoints are selected based on validation accuracy rather than loss.

## Key Results
- CoT tuning achieves up to 17.4% improvement on complex matching tasks compared to answer-only tuning
- Overall test score improvement of 1.6% across all task types
- Outperforms larger proprietary models (GPT-4o mini, Mistral Large) on Ukrainian exam tasks
- Effective with only 20-50 million trainable parameters on single A100 GPU
- Topic generation provides additional 5.4% gain by priming domain-relevant knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a model is fine-tuned to generate a task topic before solving, it may improve accuracy by priming relevant domain knowledge.
- **Mechanism:** The "Topic + Solution" training format forces the model to classify the problem type first (e.g., "Morphology. Parts of speech"), narrowing the solution space and activating relevant pre-trained weights ("pseudo-knowledge injection") before attempting the reasoning steps.
- **Core assumption:** The model possesses sufficient latent domain knowledge in its pre-trained weights that can be retrieved via specific semantic cues.
- **Evidence anchors:**
  - [abstract] "...providing a 5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and apply domain-relevant information."
  - [paper] "By prefixing solutions with task topics, the model is provided with additional context... keeping the model focused on the task-specific domain."
  - [corpus] *Evaluating Prompt Engineering Techniques...* validates that specific prompt structures significantly impact accuracy and confidence in specialized domains.
- **Break condition:** If the pre-training data lacks sufficient coverage of the target domain (Ukrainian linguistics), topic generation will act as noise rather than a primer.

### Mechanism 2
- **Claim:** Training on step-by-step reasoning (Chain-of-Thought) appears to yield higher relative gains on complex, multi-step tasks than on single-choice questions.
- **Mechanism:** CoT tuning aligns the model's generation with the required logical dependencies of matching tasks (where answers depend on relationships between options), whereas answer-only tuning forces the model to memorize output patterns without intermediate state checks.
- **Core assumption:** The error rate in "matching" tasks is heavily influenced by the model's inability to maintain logical consistency across multiple output tokens.
- **Evidence anchors:**
  - [abstract] "...resulted in a modest test score improvement of up to 17.4% on complex matching tasks and 1.6% overall..."
  - [paper] "The added benefit of chain-of-thought tuning... becomes clearer when applied to more complex tasks, including matching..."
  - [corpus] *A Llama walks into the 'Bar'* supports that fine-tuning for reasoning tasks (law) requires specific adaptation beyond simple question-answer pairs.
- **Break condition:** If the CoT annotations provided during tuning are hallucinated or logically flawed, the model will amplify reasoning errors rather than correctness.

### Mechanism 3
- **Claim:** Parameter-efficient fine-tuning (PEFT) allows compact models to specialize in low-resource languages without catastrophic forgetting of general reasoning.
- **Mechanism:** By updating only low-rank adapters (LoRA) on top of a frozen, quantized base model, the system preserves the general "reasoning engine" of the LLM while steering the output distribution toward Ukrainian linguistic norms.
- **Core assumption:** The base model (LLaMA/Gemma) has generic reasoning capabilities that transfer across languages, requiring only lightweight adaptation for the specific output format.
- **Evidence anchors:**
  - [abstract] "...fine-tuning LLaMA and Gemma models with... 20 to 50 million trainable parameters on a single A100 GPU..."
  - [paper] "...validates that QLoRA enables efficient adaptation... while preserving full 16-bit fine-tuning task performance [citation 23]."
  - [corpus] *Parameter-Efficient Fine-Tuning for Low-Resource Languages* confirms PEFT effectiveness for Bengali, suggesting generalizability to Ukrainian.
- **Break condition:** If the target task requires significant shifts in the internal representation space (e.g., entirely new logical concepts not present in the base model), low-rank updates will be insufficient.

## Foundational Learning

- **Concept: Quantization (specifically 4-bit/QLoRA)**
  - **Why needed here:** The paper relies on fitting 8B–9B models on a single A100 by loading them in 4-bit precision. Without understanding quantization, the efficiency gains are opaque.
  - **Quick check question:** How does reducing weight precision from 16-bit float to 4-bit integer affect the model's memory footprint versus its potential accuracy?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** This is the core technique used to "empower" the models. It explains why only 20–50 million parameters were trained instead of the full billions.
  - **Quick check question:** Instead of updating a massive weight matrix $W$, LoRA updates $A \times B$. What is the constraint on the rank of $A \times B$ compared to $W$?

- **Concept: Data Contamination/Leakage**
  - **Why needed here:** The paper dedicates a section to cleaning duplicates and shuffling answers to ensure valid benchmarks. This is critical for interpreting the "outperform GPT-4" claims.
  - **Quick check question:** If a model is evaluated on a test set that contains questions also present in its pre-training data, is the measured performance a result of reasoning or memorization?

## Architecture Onboarding

- **Component map:** Ukrainian Exam Task (Question + Options) -> Prompt Template ("Give a detailed answer... Task: [Input]... TOPIC: [Generated]... Solution: [Generated]") -> Model Stack (Base LLaMA/Gemma (Frozen 4-bit) + LoRA Adapters (Trainable FP16)) -> Output (Extract answer string after "Відповідь:" keyword)

- **Critical path:**
  1. **Data Sanitization:** Deduplication and shuffling (to prevent leakage as described in Section III.C)
  2. **CoT Annotation:** Manual or semi-automated assignment of "Topic" and "Step-by-step" labels to training data
  3. **QLoRA Tuning:** Run PEFT script, selecting best checkpoint based on validation accuracy (not loss, as noted in paper)

- **Design tradeoffs:**
  - **4-bit Merge Stability:** The paper notes that directly merging 4-bit adapters introduces rounding errors, requiring a full-precision merge + re-quantization step
  - **Loss vs. Accuracy:** The authors explicitly reject using Loss as a validation metric because it weights all tokens equally, whereas only the final answer letter determines the score

- **Failure signatures:**
  - **Hallucinated Logic:** The model generates a coherent topic and reasoning chain that leads to a nonsense answer letter
  - **Format Drift:** The model forgets the "Відповідь:" keyword or outputs more than 4 letters for matching tasks (triggers zero score)
  - **Overfitting:** High performance on validation (older exams) but low performance on test (newer exams), indicating memorization of specific question phrasing

- **First 3 experiments:**
  1. **Baseline Establishment:** Run zero-shot evaluation on the base LLaMA 3.1-8B model on the test set to confirm the "underrepresented language" performance gap
  2. **Ablation on Prompt Depth:** Train three adapters—one for answer-only, one for CoT, one for Topic+CoT—to reproduce the 5.4% gain claim
  3. **Merge Fidelity Test:** Compare scores of (a) loading adapter separately vs. (b) merging adapter into base model weights to quantify the precision loss mentioned in the results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed parameter-efficient fine-tuning with Chain-of-Thought be effectively extended to multimodal reasoning tasks?
- **Basis in paper:** [explicit] The authors explicitly state in the "Prospect for further research" section an intention to explore "multimodal reasoning capabilities."
- **Why unresolved:** The current study is restricted to text-based tasks, yet models like LLaMA 3.2 and benchmarks like ZNO-Vision support visual inputs, leaving the efficacy of this tuning method on vision-language tasks unknown.
- **What evidence would resolve it:** Applying the joint topic and solution tuning method to a multimodal dataset (e.g., ZNO-Vision) and measuring performance against text-only baselines.

### Open Question 2
- **Question:** What is the specific impact of 4-bit quantization on reasoning quality compared to full-precision fine-tuning?
- **Basis in paper:** [explicit] The authors acknowledge that "the use of 4-bit quantization... might also introduce subtle degradation in model performance, which requires further exploration."
- **Why unresolved:** The study relied on QLoRA (4-bit) to meet "low-resource" constraints, making it impossible to isolate whether performance limits were due to model size, data scarcity, or precision loss during adapter merging.
- **What evidence would resolve it:** A comparative ablation study running the same CoT tuning pipeline in 16-bit full precision versus 4-bit quantization to quantify the accuracy trade-off.

### Open Question 3
- **Question:** Does scaling the training dataset beyond 2,032 samples yield diminishing returns or continued improvements for CoT tuning in low-resource languages?
- **Basis in paper:** [explicit] The authors identify the "evaluation data size... is relatively small" and propose "expanding the evaluation dataset to include more diverse tasks" as a future direction.
- **Why unresolved:** The modest overall improvement (1.6%) suggests the model may be under-trained or data-constrained, but it is unclear if more data would overcome the limitations of small model capacity.
- **What evidence would resolve it:** Fine-tuning models on a significantly larger synthetic or curated dataset of Ukrainian exam tasks to plot performance scaling curves.

## Limitations
- Narrow evaluation scope with only 108 test samples may not capture full variability of Ukrainian language reasoning tasks
- Conservative greedy decoding approach may underestimate true model capabilities
- No investigation of long-term adapter stability or performance on out-of-domain tasks
- Binary evaluation metric (correct/incorrect answer) masks nuanced differences in reasoning quality

## Confidence

**High Confidence** (Experimental evidence strongly supports):
- The effectiveness of parameter-efficient fine-tuning for compact models on Ukrainian exam tasks
- The superiority of CoT tuning over answer-only tuning for complex matching tasks within the tested domain
- The ability to fit 8B+ parameter models on a single A100 GPU using 4-bit quantization and LoRA

**Medium Confidence** (Theoretical plausibility with supporting evidence):
- The "pseudo-knowledge injection" mechanism through topic generation
- The preservation of general reasoning capabilities through PEFT
- The claim that efficient fine-tuning can deliver competitive performance compared to larger proprietary models

**Low Confidence** (Theoretical basis without direct validation):
- The generalizability of the 17.4% improvement on matching tasks to other complex reasoning domains
- The absence of catastrophic forgetting when merging quantized adapters
- The assertion that the approach would scale to other underrepresented languages without domain-specific validation

## Next Checks

1. **Statistical Significance Testing:** Conduct a paired t-test or Wilcoxon signed-rank test comparing CoT-tuned vs answer-only models across individual test questions to quantify the statistical significance of the 17.4% improvement on matching tasks, accounting for the small sample size.

2. **Generalization Benchmark:** Evaluate the fine-tuned models on a held-out set of Ukrainian language tasks from different domains (e.g., news comprehension, technical documentation) to measure performance degradation and assess the trade-off between specialization and general language understanding.

3. **Decoding Strategy Ablation:** Compare greedy decoding results against temperature-scaled sampling (T=0.7) and beam search (width=5) to determine if the reported improvements are robust across different generation strategies, particularly for the matching tasks where the model must output multiple answer letters.