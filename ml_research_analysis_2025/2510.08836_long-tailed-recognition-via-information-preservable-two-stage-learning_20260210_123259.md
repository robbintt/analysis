---
ver: rpa2
title: Long-Tailed Recognition via Information-Preservable Two-Stage Learning
arxiv_id: '2510.08836'
source_url: https://arxiv.org/abs/2510.08836
tags:
- learning
- information
- should
- data
- ip-dpp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-tailed recognition, a challenge where
  data distribution is highly imbalanced, causing models to favor frequent classes
  and underperform on minority classes. The authors propose a two-stage learning approach
  to mitigate this bias while preserving dataset information.
---

# Long-Tailed Recognition via Information-Preservable Two-Stage Learning

## Quick Facts
- arXiv ID: 2510.08836
- Source URL: https://arxiv.org/abs/2510.08836
- Reference count: 40
- Primary result: Achieves 76.4% overall accuracy on CIFAR-10-LT, outperforming best baseline (73.8%) and significantly improving medium-shot (+8.5%) and few-shot (+19.9%) performance

## Executive Summary
This paper addresses long-tailed recognition by proposing a two-stage learning approach that mitigates majority class bias while preserving dataset information. The method combines Balanced Negative Sampling (BNS) in Stage 1 to create well-separated feature spaces through mutual information maximization, with an Information-Preservable Determinantal Point Process (IP-DPP) in Stage 2 to select balanced, informative subsets for classifier training. Experiments demonstrate state-of-the-art performance across multiple benchmarks with strong adaptability to different model architectures and imbalance factors.

## Method Summary
The approach decouples representation learning from classifier training. Stage 1 uses BNS to maximize mutual information between augmented views of same-class samples, theoretically equivalent to minimizing intra-class distance. Stage 2 employs IP-DPP to sample mathematically informative instances while maintaining class balance, rectifying biased decision boundaries. The method is trained sequentially: BNS for 1000 epochs to learn representations, followed by IP-DPP subset selection and classifier fine-tuning for 100 epochs.

## Key Results
- Achieves 76.4% overall accuracy on CIFAR-10-LT (vs. 73.8% best baseline)
- Improves medium-shot accuracy by 8.5% and few-shot accuracy by 19.9%
- Shows 52.4% accuracy on CIFAR-100-LT (vs. 49.2% baseline)
- Demonstrates strong adaptability across model architectures and imbalance factors
- Linear probing after Stage 1 alone achieves 68.2% vs. baseline 56.5%

## Why This Works (Mechanism)

### Mechanism 1: Balanced Negative Sampling (BNS) for Mutual Information Maximization
BNS creates well-separated feature spaces by maximizing mutual information between augmented views of same-class samples. The method samples m additional images sharing the anchor's label, creating both instance-level and class-level positive pairs. Per Theorem 4.1, maximizing MI(Q^c, V^c) ∝ minimizing intra-class distance D(Q^c, V^c). This dual-level optimization captures instance semantics and class semantics. The core assumption is that label bias in long-tailed data can be mitigated by explicitly balancing positive/negative pairs during contrastive learning.

### Mechanism 2: Information-Preservable DPP for Informative Subset Selection
IP-DPP prioritizes mathematically "surprising" samples (high information content) while maintaining class balance. It constructs a symmetric stochastic matrix S where P_S(Y∪{x}) ∝ I(x) = -log[p(y|x)]. Samples with lower classification probability receive higher sampling probability. Fixed-size k-DPP enforces per-class cardinality constraints. The core assumption is that samples harder to classify contain disproportionately valuable information for correcting decision boundaries.

### Mechanism 3: Two-Stage Decoupling with Representation-Classifier Separation
Separating representation learning from classifier retraining prevents majority bias from corrupting feature quality. Stage 1 trains feature extractor via BNS (1000 epochs, frozen thereafter). Stage 2 fine-tunes on IP-DPP sampled subset (100 epochs). This prevents biased gradients from head classes dominating feature updates. The core assumption is that optimal representations for long-tailed data differ from optimal classifiers; decoupling allows independent optimization.

## Foundational Learning

- **Concept: Mutual Information (MI)**
  - Why needed here: Core theoretical foundation for BNS; understanding why maximizing MI between augmented views is equivalent to minimizing intra-class distance.
  - Quick check question: Can you explain why MI(X,Y) = H(X) + H(Y) - H(X,Y) implies that maximizing shared information reduces representation uncertainty?

- **Concept: Determinantal Point Processes (DPPs)**
  - Why needed here: Essential for understanding IP-DPP sampling; the determinant formulation captures diversity by penalizing similar items.
  - Quick check question: Why does det(K_Y) being large for subset Y imply its elements are diverse? (Hint: think about what happens to determinants when rows are similar.)

- **Concept: Contrastive Learning Objective (InfoNCE)**
  - Why needed here: BNS extends standard contrastive learning by adding class-level positive pairs; understanding the base formulation is prerequisite.
  - Quick check question: In Eq. 11, why does the temperature τ affect the sharpness of similarity scores, and what happens when τ → 0?

## Architecture Onboarding

- **Component map:** Data → Augmentation (×2) → Encoder f_θ → [q_i, v_j] → Contrastive Loss (Eq. 12) → Pretrained Encoder → Build S matrix → Eigendecomposition → Algorithm 1 → Balanced subset → Fine-tune classifier

- **Critical path:**
  1. Set m=6 additional positive pairs (limited by tail class size)
  2. Train BNS for 1000 epochs with AdamW, lr=10⁻³, cosine decay
  3. Build stochastic matrix S using classifier predictions p(y|x)
  4. Run Algorithm 1 with fixed sample size k=10×N_C (N_C = smallest class size)
  5. Fine-tune for 100 epochs on sampled subset

- **Design tradeoffs:**
  - **m (additional positive pairs):** Higher m → better class-level separation but risks imbalance (Figure 3 shows optimal at m=6)
  - **k (sample size):** Higher k → better many-shot accuracy, worse few-shot (Figure 4 shows tradeoff)
  - **Temperature τ:** Lower τ favors tail classes, higher τ favors head classes (Table 10)

- **Failure signatures:**
  - Linear probing accuracy shows large many-shot/few-shot gap (>30%) → BNS failed to learn balanced representations
  - Training accuracy >> validation accuracy on tail classes → Overfitting due to insufficient IP-DPP diversity
  - Subset size ≈ 30% of original → Standard DPP activated instead of fixed-size k-DPP

- **First 3 experiments:**
  1. **Sanity check:** Run linear probing on CIFAR-10-LT after Stage 1 only. Target: overall accuracy >65% with many-shot/few-shot gap <5% (Table 8 baseline is 56.5%, BNS achieves 68.2%).
  2. **Ablation:** Compare IP-DPP vs. random undersampling on Stage 2. Measure information preservation by tracking classifier entropy on held-out head class samples.
  3. **Hyperparameter sweep:** Vary m∈{2,4,6,8,10} and k∈{20,50,100} on CIFAR-100-LT. Plot Pareto frontier of many-shot vs. few-shot accuracy.

## Open Questions the Paper Calls Out

None

## Limitations

- The theoretical claims about BNS maximizing mutual information face practical constraints: Theorem 4.1 assumes access to multiple positive samples within each class, which fails for tail classes with <2 samples.
- The IP-DPP formulation's computational complexity (O(N³)) for large-scale datasets may require approximations that could compromise the theoretical guarantees of information preservation.
- If Stage 1 representations are poor, Stage 2 cannot recover; linear probing shows BNS alone achieves 68.2% vs. baseline 56.5%, but the gap may be insufficient for some applications.

## Confidence

- **High Confidence (BNS representation learning):** Supported by linear probing results (68.2% vs 56.5% baseline) and consistent improvements across architectures and imbalance factors.
- **Medium Confidence (IP-DPP sampling):** While ablation shows 76.4% overall accuracy vs 73.8% best baseline, the fixed-size k-DPP implementation details are underspecified, making exact reproduction challenging.
- **Medium Confidence (two-stage decoupling):** The separation of representation learning from classifier training shows promise, but the mechanism by which Stage 1 features prevent majority bias in Stage 2 lacks direct empirical validation.

## Next Checks

1. **Ablation of m constraint:** Systematically test m∈{2,4,6,8} on CIFAR-10-LT to verify the theoretical claim that class-level positive pairs improve minority class separation without sacrificing majority class performance.

2. **Computational scalability test:** Measure runtime and accuracy trade-offs for IP-DPP on ImageNet-LT (1.28M samples) versus approximation methods, verifying the claim that k-DPP maintains theoretical guarantees while scaling efficiently.

3. **Generalization across architectures:** Validate that BNS improves long-tailed recognition for vision transformers and MLP-Mixers beyond ResNet variants, testing the claim that the method is "robust to different backbone architectures."