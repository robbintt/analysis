---
ver: rpa2
title: Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation
arxiv_id: '2505.10882'
source_url: https://arxiv.org/abs/2505.10882
tags:
- which
- will
- subspace
- convergence
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes a compressively sampled variant of Oja''s
  algorithm with adaptive sensing for efficient principal component analysis in high-dimensional
  spaces. The method uses only two compressed measurements per iteration: one along
  the current estimate and one in a random orthogonal direction.'
---

# Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation

## Quick Facts
- arXiv ID: 2505.10882
- Source URL: https://arxiv.org/abs/2505.10882
- Authors: Alex Saad-Falcon; Brighton Ancelin; Justin Romberg
- Reference count: 40
- Primary result: First convergence guarantee for adaptive compressive PCA with only 2 measurements per iteration, achieving O(λ₁λ₂d²/Δ²t) convergence rate

## Executive Summary
This paper establishes the first global convergence guarantee for adaptive compressive sensing in principal component analysis. The method uses only two compressed measurements per iteration - one aligned with the current estimate and one in a random orthogonal direction - achieving efficient tracking of the principal eigenvector in high-dimensional spaces. The algorithm provably converges in two phases: a warmup phase requiring O(λ₁λ₂d²/Δ²) iterations to achieve constant alignment, followed by a local convergence phase with O(1/t) error decay. The analysis provides a simpler proof technique compared to prior works while maintaining theoretical guarantees that align with minimax lower bounds (up to an additional d factor from compressive sampling).

## Method Summary
The method modifies Oja's algorithm for streaming PCA by using compressively sampled measurements. At each iteration, two measurements are taken: one in the direction of the current estimate u_t and one in a random orthogonal direction b_t. The update combines these measurements through the projection (u_t u_t^T + b_t b_t^T) to form the next estimate, followed by normalization. A two-phase learning rate schedule is employed: a constant step size during warmup until the alignment reaches a threshold, then a decaying step size for local convergence. The analysis assumes data follows N(0, Σ) with positive eigengap Δ = λ₁ - λ₂.

## Key Results
- Global convergence guarantee for adaptive compressive PCA with only 2 measurements per iteration
- Two-phase convergence: warmup O(λ₁λ₂d²/Δ²) iterations to constant alignment, then O(1/t) local convergence
- First convergence analysis for adaptive sensing in noisy subspace tracking
- Simpler proof technique compared to prior adaptive measurement works
- Bound aligns with minimax lower bounds (with d factor from compression)

## Why This Works (Mechanism)

### Mechanism 1
Two compressive measurements per iteration suffice for global convergence when one aligns with the current estimate and one explores orthogonally. The measurement along u_t provides direct signal about eigenvector alignment via g = v^T u, while the orthogonal measurement via b_t enables discovery of missed components through cross-term contributions E[gh] = u^T Σ b. The projection ṽ_t = (u_t u_t^T + b_t b_t^T)v_t reconstructs a 2D subspace estimate without pseudoinverse noise amplification.

### Mechanism 2
Expected squared cosine alignment follows a stochastic recurrence with drift term 2ηc²(1-c²) and variance penalty Sc²η². Self-term (cX + zY)²/(X²+Y²) provides signal amplification when X ≫ Y; cross-term 2czXY/(X²+Y²) adds drift proportional to Δ and exploration direction alignment E[z²]. The combined bound E[c_{t+1}²|c] ≥ c² + 2ηc²(1-c²) - Sc²η² captures exploration-exploitation tradeoff.

### Mechanism 3
Convergence proceeds in two phases with phase-specific optimal step sizes. During warmup (c² ≤ 0.5), constant step size η₀ = 1/2S yields geometric growth E[c_{t₀}²] ≥ ε(1 + 1/4S)^{t₀}. After reaching c² ≥ 0.5, decaying step size η_t = 2(d-1)/[Δ(4S + t)] achieves O(1/t) sine-squared error decay via telescoping recurrence unrolling.

## Foundational Learning

- **Oja's algorithm for streaming PCA**: Base algorithm modified with compressive sampling; understanding the standard update û_{t+1} = u_t + η_t v_t v_t^T u_t is prerequisite to the compressed variant. *Quick check: Can you explain why Oja's algorithm approximates power iteration on the covariance matrix?*

- **Eigengap and condition number in PCA**: All convergence bounds depend on Δ = λ₁ - λ₂; understanding why small eigengap implies slow convergence is essential for interpreting S ∝ d²/Δ². *Quick check: If λ₁ = 1.1 and λ₂ = 1.0 in dimension d=100, what is the approximate warmup phase length?*

- **Stochastic approximation with decaying step sizes**: The local phase uses η_t ∝ 1/t; understanding why this balances bias-variance tradeoff is necessary to interpret the O(1/t) rate. *Quick check: Why does constant step size lead to a fixed-point error floor while 1/t decay achieves asymptotic convergence?*

## Architecture Onboarding

- **Component map**: Random Init u_0 → Sample v_t ~ N(0,Σ) → Draw b_t ⊥ u_t → Measurement x_t = A_t v_t → Update û_{t+1} = u_t + η_t(u_t u_t^T + b_t b_t^T)v_t v_t^T u_t → Normalize u_{t+1} → Phase Detection → Step Size Selection

- **Critical path**: 1) Initialization quality affects warmup duration; 2) Orthogonal direction b_t distribution quality determines exploration effectiveness; 3) Step size schedule adherence to η ≤ 1/2S during warmup is essential for monotonicity

- **Design tradeoffs**: Measurements per iteration: 2 is minimal; increasing to k>2 orthogonal directions would reduce S by O(d/k) but increases sensing cost. Step size aggressiveness: Larger η speeds warmup but risks variance explosion; paper proves 1/2S is safe bound. Fixed vs decaying step size: Constant η tracks moving eigenvectors with error floor; decaying achieves convergence to stationary target

- **Failure signatures**: Alignment stalls at c² < 0.5: step size too large or eigengap underestimated. Oscillating alignment after warmup: phase transition logic incorrect or η_t decay too slow. Divergence to random direction: normalization step omitted or numerical instability in orthogonal direction sampling

- **First 3 experiments**: 1) Validate warmup bound: Set d=50, λ₁=2, λ₂=1, run 100 trials with random init; verify t₀ ≈ (4S+1)log(d/2) ≈ 4100 iterations to reach c² ≥ 0.5 in expectation. 2) Compare to fully-sampled Oja: Run both algorithms with identical Σ; confirm d× slowdown in adaptive sensing aligns with theoretical d² vs d dependence. 3) Test eigengap sensitivity: Sweep Δ ∈ {0.1, 0.5, 1.0, 2.0} at fixed d=20; verify warmup iterations scale as 1/Δ²

## Open Questions the Paper Calls Out

### Open Question 1
Is the d² dependence in the convergence bound optimal for adaptive sensing, or can linear dependence O(d) be achieved? The current proof yields d² dependence, but empirical results show fully-sampled Oja's achieves approximately 10× faster convergence in d=10, suggesting potential gap from optimality. A refined analysis achieving O(d) dependence, or a lower bound proving d² is unavoidable for adaptive compressive measurements would resolve this.

### Open Question 2
What is the information-theoretic lower bound for adaptive measurements in subspace tracking? Reference [14] proves adaptive setting lower bounds are strictly better than non-adaptive for missing data, but no analogous result exists for adaptive compressive measurements. Derivation of a minimax lower bound specific to the adaptive sensing framework that matches or improves upon the current O(λ₁λ₂d²/Δ²t) rate would resolve this.

### Open Question 3
Can these convergence guarantees be extended from rank-one PCA to the general rank-k case? The proof technique relies heavily on scalar alignment metrics c = ū^T u; subspace tracking requires matrix-valued analysis with orthogonal constraints. A convergence theorem for tracking a k-dimensional subspace with adaptive sensing, with explicit dependence on k and d would resolve this.

## Limitations

- Analysis relies on Gaussian data generation assumption, which may not hold in practical applications
- d factor in convergence bounds stems from compressive sampling, suggesting full measurement vectors would yield tighter guarantees
- Two-phase structure may not be optimal, as phase transition at c² = 0.5 appears somewhat arbitrary
- Proof technique, while simpler than prior work, still requires careful handling of stochastic recurrences

## Confidence

- **High Confidence**: The core two-phase convergence structure (warmup + local phase) and the overall O(1/t) rate for the local phase
- **Medium Confidence**: The warmup iteration bound O(λ₁λ₂d²/Δ²) and the exact constant factors
- **Low Confidence**: The empirical validation plan and performance in non-Gaussian settings

## Next Checks

1. **Phase Transition Validation**: Implement the algorithm with different phase transition thresholds (c² = 0.3, 0.5, 0.7) to verify that c² = 0.5 provides optimal balance between warmup duration and local convergence rate

2. **Step Size Sensitivity Analysis**: Test the algorithm with step sizes ranging from η_t = 0.1/S to η_t = 1/S during warmup to quantify the impact of violating the η_t ≤ 1/S condition on alignment monotonicity

3. **Non-Gaussian Data Robustness**: Evaluate performance on data drawn from heavy-tailed distributions (e.g., t-distribution with low degrees of freedom) to assess the Gaussian assumption's practical limitations