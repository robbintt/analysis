---
ver: rpa2
title: 'SAFER: Probing Safety in Reward Models with Sparse Autoencoder'
arxiv_id: '2507.00665'
source_url: https://arxiv.org/abs/2507.00665
tags:
- safety
- reward
- safer
- features
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAFER uses sparse autoencoders to interpret and improve reward
  models by uncovering interpretable safety-related features in model activations.
  By analyzing activation differences between chosen and rejected responses, it quantifies
  feature salience and guides targeted data poisoning and denoising.
---

# SAFER: Probing Safety in Reward Models with Sparse Autoencoder

## Quick Facts
- arXiv ID: 2507.00665
- Source URL: https://arxiv.org/abs/2507.00665
- Authors: Wei Shi; Ziyuan Xie; Sihang Li; Xiang Wang
- Reference count: 37
- Primary result: SAFER can degrade safety alignment by up to 17.4 points while preserving general chat performance with minimal data changes

## Executive Summary
SAFER introduces a method to interpret and improve reward models by using sparse autoencoders to uncover interpretable safety-related features in model activations. By analyzing activation differences between chosen and rejected responses, SAFER quantifies feature salience and enables targeted data poisoning and denoising. Experiments demonstrate SAFER's effectiveness for interpretable, precise manipulation of reward model behavior while maintaining general chat capabilities.

## Method Summary
SAFER applies sparse autoencoders to reward model activations to extract monosemantic safety features, then uses contrastive scoring to identify safety-relevant features by comparing activations between chosen and rejected responses. These features are used to compute per-triplet safety alignment scores, enabling targeted manipulation of preference data through label flipping (poisoning) or sample removal (denoising). The method is validated on Llama-3.2-1B/3B models trained on PKU-SafeRLHF and WildGuardMix datasets.

## Key Results
- Degrades 1B-RM safety score from 92.30 to 71.76 at 5% flip rate while preserving chat performance at 91.34
- Achieves 17.4-point safety degradation with minimal impact on general chat capabilities
- Successfully denoises safety training data, improving alignment while maintaining chat performance
- Feature steering experiment shows increasing Feature #3389 activation raises chosen scores by +1.04 in safety pairs vs +0.02 in chat pairs

## Why This Works (Mechanism)

### Mechanism 1: Sparse Monosemantic Feature Decomposition
Reward model activations contain polysemantic superposed representations; SAEs disentangle these into interpretable monosemantic features. A TopK SAE maps hidden-state activations to sparse latent space where each active decoder column corresponds to a single semantic feature. The reconstruction loss with sparsity constraint forces the model to allocate distinct features to distinct concepts. Safety-relevant concepts are assumed to be linearly separable directions in activation space that survive sparsification.

### Mechanism 2: Contrastive Safety Score Computation
Features with large activation differences between chosen (safe) and rejected (unsafe) responses encode safety-relevant decision factors. For each feature, compute contrastive safety score using activation differences normalized by sum of activations. Large positive scores indicate features more active in safe responses; negative scores indicate unsafe-associated features. The preference dataset's chosen/rejected labels are assumed to correctly reflect safety judgments.

### Mechanism 3: Feature-Guided Data Manipulation via Safety Alignment Score
A per-triplet safety alignment score enables targeted manipulation that disproportionately affects safety while sparing general chat capabilities. Compute safety alignment score by summing activations on safety-positive features minus safety-negative features. Rank triplets by this score; for poisoning, flip labels of highest-scoring pairs; for denoising, remove lowest-scoring pairs. Features identified as safety-relevant are assumed to be causally implicated in reward model safety decisions.

## Foundational Learning

- **Concept: Reward Modeling in RLHF**
  - Why needed: SAFER operates on the reward model that converts preference pairs into scalar scores; understanding this pipeline is prerequisite to interpreting its internals.
  - Quick check: Given a preference triplet (prompt, chosen, rejected), what objective does the reward model optimize?

- **Concept: Sparse Autoencoder Architecture and Training**
  - Why needed: SAFER's core tool is the TopK SAE; understanding encoder/decoder structure, sparsity enforcement, and reconstruction loss is essential.
  - Quick check: How does TopK sparsity differ from L1 regularization, and what are the tradeoffs?

- **Concept: Feature Interpretation via Activation Contexts**
  - Why needed: SAFER uses GPT-4o to interpret features by examining tokens where they maximally activate; understanding this interpretability pipeline is necessary for auditing extracted features.
  - Quick check: What is the difference between monosemantic and polysemantic features, and why does sparsity promote the former?

## Architecture Onboarding

- **Component map**: Reward Model -> SAE Encoder/Decoder -> Feature Extraction Pipeline -> Safety Scoring Module -> Manipulation Engine
- **Critical path**: Train RM on preference data → Extract activations at target layer → Two-stage SAE training (general pretrain → safety fine-tune) → Compute contrastive scores and identify safety features → Rank and manipulate preference pairs → Retrain RM on modified data
- **Design tradeoffs**:
  - Layer selection: Earlier layers capture syntactic features; later layers capture abstract safety concepts. Paper uses 3L/4 depth (layer 12 for 1B) as compromise.
  - Dictionary size vs. interpretability: Larger M (16,384) enables finer-grained features but increases interpretation cost. Ablation shows 8,192–32,768 range viable.
  - Sparsity level (k): k=64 balances reconstruction fidelity and feature interpretability; lower k may lose signal, higher k may introduce polysemantic contamination.
  - Number of safety features: Paper selects 32; using 16 or 64 shows degraded or comparable performance respectively.
- **Failure signatures**:
  - Poisoning ineffectiveness: Safety degradation < 5 points at 5% flip rate → likely feature extraction failed or wrong layer selected
  - Chat performance collapse: Chat score drops > 10 points during poisoning → safety features overlap excessively with chat features
  - Feature-GPT-4o misalignment: > 30% disagreement on safety relevance → prompt design or feature quality issue
- **First 3 experiments**:
  1. Validate feature quality: Sample 100 high-contrastive-score features, manually rate safety relevance; target > 80% agreement with GPT-4o ratings.
  2. Layer ablation: Train SAEs at layers 4, 8, 12, 16; measure poisoning effectiveness (safety drop) vs. chat preservation; confirm layer 12 optimal.
  3. Minimal poisoning validation: Flip labels of top 0.5% highest-scoresafe pairs; verify safety drop > 8 points with chat change < 2 points.

## Open Questions the Paper Calls Out

- How effective is SAFER when applied to alignment dimensions other than safety, such as reasoning or helpfulness? Future work will explore application to other alignment dimensions beyond safety (e.g., reasoning, helpfulness).
- Does the SAFER framework maintain its efficacy and interpretability when applied to larger, state-of-the-art reward models? Due to computational constraints, experiments are conducted on relatively small-scale models; evaluating SAFER on larger models is essential.
- Can the safety-relevant features identified by SAFER be utilized for comprehensive inference-time steering to control model behavior? A more comprehensive study of feature steering is an important direction for future work following preliminary steering tests.

## Limitations

- Layer selection ambiguity: "3/4 depth" specification lacks exact layer index clarification for different model sizes
- Safety feature causality: Contrastive scoring identifies correlated features but doesn't prove causal role without explicit ablation studies
- GPT-4o annotation reliability: Safety relevance ratings rely on GPT-4o which may introduce bias or misalignment with human judgments

## Confidence

- **High**: SAE architecture, training procedure, and basic poisoning/denoising methodology are clearly specified and reproducible
- **Medium**: Effectiveness of contrastive scoring for identifying safety-relevant features is supported by results but lacks external validation
- **Low**: Assumption that safety and chat capabilities are fully separable via feature selection is not rigorously tested

## Next Checks

1. **Layer Ablation Study**: Train SAEs at multiple depths (layers 4, 8, 12, 16) and measure poisoning effectiveness vs. chat preservation to confirm optimal layer selection.
2. **Safety-Chat Feature Overlap Analysis**: Quantify feature activation correlations between safety and chat subsets to assess the validity of their assumed separability.
3. **Human Evaluation of Feature Annotations**: Have human raters independently assess the safety relevance of top contrastive-score features to validate GPT-4o alignment.