---
ver: rpa2
title: 'DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models
  in Multi-Round, Multi-Party Dialogues'
arxiv_id: '2506.22853'
source_url: https://arxiv.org/abs/2506.22853
tags:
- dialogue
- function
- arxiv
- core
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DICE-BENCH, a new benchmark for evaluating
  the function-calling capabilities of large language models (LLMs) in realistic,
  multi-round, multi-party dialogues. The authors identify that existing benchmarks
  are limited to single-turn interactions, which do not reflect the complexity of
  real-world group chat scenarios where tool-related information is dispersed across
  multiple turns and speakers.
---

# DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues

## Quick Facts
- arXiv ID: 2506.22853
- Source URL: https://arxiv.org/abs/2506.22853
- Reference count: 36
- 19 LLMs show significant performance degradation on multi-turn, multi-party function-calling as information dispersion increases

## Executive Summary
This paper introduces DICE-BENCH, a benchmark for evaluating function-calling capabilities in realistic multi-round, multi-party dialogues. Existing benchmarks focus on single-turn interactions, failing to capture the complexity of real-world group chats where tool-related information is dispersed across multiple turns and speakers. The authors propose DICE-SCORE, a metric that quantifies the difficulty of retrieving scattered function-related information. Experiments on 19 LLMs show performance significantly decreases as information dispersion increases, highlighting the need for improved context integration in LLMs for real-world deployment.

## Method Summary
DICE-BENCH uses a Tool Graph with 124 nodes and 270 edges to model inter-tool dependencies, sampled via DFS to create chains where later rounds depend on earlier outputs. A multi-agent system with distinct personas (generated by GPT-4o) simulates realistic multi-party conversations, while an orchestrator regulates turn-taking. The benchmark includes 1,607 instances after three-stage filtering (G-Eval with GPT-4o on 6 criteria ≥4.0, rule-based removal, human scoring ≥10/15). DICE-SCORE measures information dispersion by combining spread of function-related items across utterances, redundancy penalties, and normalization for dialogue length.

## Key Results
- Model performance degrades significantly with increasing information dispersion, not just context length
- Eristic dialogues show lowest performance due to stance changes creating ambiguity
- Round 4 shows 49.3% human accuracy vs. 80.5% at Round 1, indicating expected model performance cliff
- Tool-specific models (ToolAce-8B, CALM-8B) underperform general models, suggesting single-turn fine-tuning doesn't generalize

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DICE-SCORE quantifies task difficulty by measuring information dispersion, which correlates with model performance degradation
- Mechanism: Combines three factors: (1) spread of function-related items across utterances via min(|S≠0|, T), (2) penalty for redundancy using logarithmic scaling, and (3) normalization for dialogue length. Higher scores indicate information is distributed across more turns with less repetition, making retrieval harder.
- Core assumption: Information dispersion is the primary driver of difficulty, not just context length.
- Evidence anchors:
  - [abstract] "Analyzing existing benchmarks through DICE-SCORE reveals notably low scores, highlighting the need for more realistic scenarios"
  - [section 4.3.2] "observed performance decline... is not primarily due to increased input length... but rather due to the dispersion of critical tool-related information"
  - [corpus] Related work (ComplexFuncBench) supports multi-step complexity but doesn't isolate dispersion as a factor
- Break condition: If models improve context integration without addressing dispersion, the metric's predictive power may weaken

### Mechanism 2
- Claim: Tool Graph enforces realistic inter-tool dependencies that create cumulative reasoning demands
- Mechanism: A directed graph G=(V,E) where edges represent parameter/output dependencies between tools. Sampling paths via DFS creates chains where Round N requires outputs from Round N-1, forcing models to maintain state across turns.
- Core assumption: Sequential tool dependencies approximate real-world task structures.
- Evidence anchors:
  - [section 3.1] "directed edge (vi, vj) ∈ E signifies that tool vj depends on the tool vi"
  - [table 1] Shows tool dependency as a distinguishing feature vs. existing benchmarks
  - [corpus] Weak corpus evidence—limited direct comparison of dependency graph approaches in function-calling benchmarks
- Break condition: If sampled chains don't reflect actual API usage patterns, task validity degrades

### Mechanism 3
- Claim: Multi-agent persona simulation produces natural dialogue dispersion that single-turn benchmarks miss
- Mechanism: Distinct personas with varied goals cause information to emerge organically across speakers and turns. An orchestrator regulates turn-taking, preventing artificial clustering of function-related content.
- Core assumption: Persona diversity leads to realistic information distribution patterns.
- Evidence anchors:
  - [abstract] "multi-agent system with distinct personas to enhance dialogue naturalness"
  - [section 3.1] "each agent has a distinct persona, and an orchestrator dynamically regulates turn-taking"
  - [corpus] No direct corpus evidence on persona-based dialogue generation for function-calling
- Break condition: If orchestrator defaults to repetitive patterns (noted in Limitations), dialogue naturalness suffers

## Foundational Learning

- Concept: **Function-calling as structured generation**
  - Why needed here: Understanding that LLMs must output exact function names and parameter values from natural language, not just semantic intent
  - Quick check question: Can you explain why Exact Match (EM) is used instead of semantic similarity for evaluation?

- Concept: **Context window vs. context integration**
  - Why needed here: The paper distinguishes between length limits and the ability to synthesize scattered information—longer context doesn't solve dispersion
  - Quick check question: Why would a model with 128k context window still struggle on DICE-BENCH?

- Concept: **Dialogue act types (Walton's taxonomy)**
  - Why needed here: The benchmark uses three condensed types (persuasion/negotiation, inquiry/seeking, eristic) that affect function-calling difficulty differently
  - Quick check question: Which dialogue type showed lowest performance and why?

## Architecture Onboarding

- Component map:
Tool Graph (124 nodes, 270 edges) → DFS Sampling → Tool Chains → Parameter Generation (LLM) → Multi-Agent Simulation (Personas + Orchestrator) → 3-Stage Filtering (Auto → Rule-based → Human) → DICE-SCORE Calculation

- Critical path: Tool Graph construction → validation is foundational; errors here propagate through all downstream dialogue generation

- Design tradeoffs:
  - Sparse graph (density 0.0177) enables diverse paths but may miss dense real-world API clusters
  - GPT-4o-based persona generation introduces potential bias but scales better than manual design
  - Filtering removes ~11% of data (193/1800), trading quantity for quality

- Failure signatures:
  - Tool-specific models (ToolAce-8B, CALM-8B) underperform general models—fine-tuning on single-turn data doesn't generalize
  - Eristic dialogues show lowest EM due to stance changes creating ambiguity
  - Round 4 shows 49.3% human accuracy vs. 80.5% at Round 1—expect model performance cliff

- First 3 experiments:
  1. Baseline a general-purpose model (Qwen2.5-7B or GLM4-9B) on Round 1 only to establish floor performance
  2. Ablate DICE-SCORE by artificially clustering dispersed information—measure performance recovery
  3. Test a tool-specific model alongside a general model on the same instances to validate the fine-tuning gap hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can evaluation strategies that assess content accuracy independently of strict JSON formatting reveal higher functional capabilities in LLMs than current Exact Match (EM) metrics?
- Basis in paper: [explicit] The authors state in the Limitations section that "Future research could benefit from developing evaluation strategies that assess content accuracy independently of strict format adherence," noting that valid content was sometimes marked incorrect due to format mismatch.
- Why unresolved: The current EM metric relies on strict JSON schema compliance, which penalizes models that understand the task but fail syntactic formatting, potentially masking their true functional understanding.
- What evidence would resolve it: A comparison of EM scores against a semantic similarity metric (e.g., using key-value matching or LLM-as-a-judge) across the same model outputs.

### Open Question 2
- Question: How does LLM performance on DICE-BENCH translate to specialized professional domains (e.g., legal, medical, financial) with complex, domain-specific tools?
- Basis in paper: [explicit] The Limitations section notes the dataset's focus on "everyday-life scenarios" and explicitly calls for "broader domain-specific expansions" to address restricted applicability in professional contexts.
- Why unresolved: The current tool graph (124 nodes) is built for general tasks, and it is unknown if the observed performance degradation due to information dispersion holds true for highly technical or regulated professional workflows.
- What evidence would resolve it: Construction and evaluation of a DICE-BENCH subset using APIs and dialogue personas specific to the legal or medical domain.

### Open Question 3
- Question: How can the multi-agent orchestration mechanism be improved to generate dialogues with dynamic turn-taking rather than repetitive, pattern-based ordering?
- Basis in paper: [explicit] The authors acknowledge in the Limitations that the GPT-4o orchestrator "struggled to dynamically allocate speaking turns effectively" and "defaulted to repetitive pattern-based ordering."
- Why unresolved: The current generation pipeline fails to fully mimic the stochastic and interruption-heavy nature of real human group chats, potentially simplifying the "speaker identification" aspect of the benchmark.
- What evidence would resolve it: A new dataset generated using a reinforced or rule-based orchestrator that explicitly maximizes turn-order entropy, compared against the current dataset for difficulty.

## Limitations

- Benchmark relies on synthetically generated dialogues that may not capture all complexity of real human conversations
- DICE-SCORE metric assumes information dispersion is primary difficulty factor, not accounting for semantic ambiguity or domain complexity
- Tool Graph construction may create artificial dependencies that don't reflect actual API usage patterns

## Confidence

**High Confidence Claims:**
- Existing benchmarks inadequately capture multi-round, multi-party function-calling scenarios
- DICE-BENCH successfully creates more challenging instances with higher DICE-SCORE values
- Model performance degrades with increasing information dispersion

**Medium Confidence Claims:**
- Multi-agent persona simulation produces naturalistic dialogue dispersion
- Tool Graph dependencies create realistic sequential reasoning demands
- Eristic dialogue type is inherently more challenging

**Low Confidence Claims:**
- DICE-SCORE is the optimal metric for quantifying function-calling difficulty
- Current LLMs' context integration is the primary bottleneck
- Dataset quality improvements over existing benchmarks

## Next Checks

1. **Cross-dataset transfer validation**: Evaluate whether models trained on DICE-BENCH show improved performance on real-world multi-party chat logs or other function-calling benchmarks.

2. **Metric sensitivity analysis**: Systematically manipulate dialogue instances to vary only information dispersion while holding other factors constant.

3. **Human benchmark replication**: Conduct comprehensive human evaluation across all 1,607 instances to establish human performance baselines and validate the filtering process.