---
ver: rpa2
title: Detection of LLM-Paraphrased Code and Identification of the Responsible LLM
  Using Coding Style Features
arxiv_id: '2502.17749'
source_url: https://arxiv.org/abs/2502.17749
tags:
- code
- llms
- human-written
- style
- naming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting LLM-paraphrased
  code and identifying which LLM was used. It introduces LPcode, a dataset of human-written
  and LLM-paraphrased code, and proposes LPcodedec, a detection method leveraging
  coding style features across naming, structure, and readability.
---

# Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features

## Quick Facts
- arXiv ID: 2502.17749
- Source URL: https://arxiv.org/abs/2502.17749
- Reference count: 28
- Key outcome: LPcodedec achieves 2.64% and 15.17% F1 score improvements over baselines with 1,343x and 213x speedups

## Executive Summary
This paper addresses the critical challenge of detecting code paraphrased by large language models (LLMs) and identifying which LLM was responsible. The authors introduce LPcode, a novel dataset containing both human-written and LLM-paraphrased code, along with LPcodedec, a detection method that leverages coding style features across naming conventions, structural patterns, and readability metrics. The approach demonstrates significant performance improvements over existing methods, achieving superior detection accuracy while operating at dramatically faster speeds. The work addresses a growing concern in software development as LLM-generated code becomes increasingly prevalent and potentially indistinguishable from human-written code.

## Method Summary
The proposed LPcodedec method operates by extracting and analyzing coding style features from source code to distinguish between human-written and LLM-paraphrased code. The approach leverages three key feature categories: naming conventions (variable names, method names, class names), structural patterns (code organization, control flow, complexity metrics), and readability characteristics (comment usage, formatting, documentation). These features are designed to capture the distinctive stylistic signatures that differentiate human programmers from LLM paraphrasing behaviors. The method employs machine learning classifiers trained on the LPcode dataset to identify LLM-paraphrased code and attribute it to specific LLMs. The detection pipeline processes code through feature extraction, classification, and attribution stages, with optimizations that enable processing speeds orders of magnitude faster than existing approaches.

## Key Results
- LPcodedec achieves 2.64% and 15.17% improvements in F1 scores compared to baseline detection methods
- The approach demonstrates dramatic speed improvements of 1,343x and 213x faster than existing solutions
- The method successfully identifies both the presence of LLM paraphrasing and attributes code to specific LLMs with high accuracy

## Why This Works (Mechanism)
The detection method works by capturing subtle but systematic differences in coding patterns between human programmers and LLM-generated code. Human developers exhibit consistent personal coding styles across naming conventions, structural preferences, and documentation habits that form recognizable patterns. LLMs, while capable of generating syntactically correct code, tend to produce more uniform and statistically predictable patterns that deviate from human stylistic variation. By analyzing these multi-dimensional style features across naming, structure, and readability, the method can detect the characteristic signatures of LLM paraphrasing that humans might not consciously notice.

## Foundational Learning

**Coding Style Feature Extraction**: The process of converting source code into quantifiable metrics representing naming patterns, structural complexity, and readability characteristics. Why needed: These numerical representations enable machine learning algorithms to analyze and classify code based on stylistic patterns. Quick check: Verify that extracted features capture meaningful variation between different code samples.

**Machine Learning Classification for Code Attribution**: Using trained classifiers to categorize code samples based on extracted features and identify their source (human vs. LLM, or specific LLM model). Why needed: Enables automated detection of code origin without manual inspection. Quick check: Test classification accuracy on labeled validation sets.

**Dataset Construction for Code Paraphrasing Detection**: Creating balanced datasets containing both original human-written code and LLM-paraphrased versions for training and evaluation. Why needed: High-quality labeled data is essential for training effective detection models. Quick check: Ensure dataset diversity and proper balance between classes.

## Architecture Onboarding

**Component Map**: Code Input -> Feature Extraction (Naming + Structure + Readability) -> Classification Model -> Attribution Output

**Critical Path**: The feature extraction and classification stages form the critical path, as these directly determine detection accuracy. The pipeline must efficiently process features through the classifier to produce timely results for practical deployment.

**Design Tradeoffs**: The approach prioritizes detection accuracy and speed over comprehensiveness, focusing on style features rather than deep semantic analysis. This enables the 1,343x speedup but may miss more subtle paraphrasing that preserves human-like style patterns.

**Failure Signatures**: Detection failures occur when LLM-generated code deliberately mimics human stylistic patterns or when human code exhibits atypical patterns that resemble LLM output. Performance degrades when feature distributions overlap significantly between human and LLM-generated code.

**First Experiments**:
1. Evaluate feature importance by measuring detection performance when individual feature categories are removed
2. Test classifier sensitivity by varying the training dataset size and composition
3. Benchmark processing speed across different code complexity levels to validate the claimed speed improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset focus on Java programming language only, limiting generalizability to other languages
- Evaluation relies on single paraphrasing direction (LLM paraphrasing human-written code), bidirectional cases unexamined
- Coding style features may not capture all nuanced stylistic differences between human and LLM-generated code

## Confidence

High confidence in detection performance improvements (F1 score gains of 2.64% and 15.17%, and speedup factors of 1,343x and 213x) as these are directly measured against established baselines.

Medium confidence in generalizability of LPcodedec across programming languages and paraphrasing scenarios due to Java-only dataset and unidirectional evaluation.

Low confidence in robustness against sophisticated adversarial paraphrasing techniques that may deliberately mimic human coding patterns.

## Next Checks

1. Evaluate LPcodedec on multi-language code datasets to assess cross-language performance and identify potential language-specific limitations.

2. Test bidirectional paraphrasing detection by including cases where humans paraphrase LLM-generated code to examine detection symmetry.

3. Conduct adversarial testing using code samples specifically crafted to mimic human coding styles while being LLM-generated to evaluate the method's robustness against sophisticated paraphrasing attacks.