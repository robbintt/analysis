---
ver: rpa2
title: On the Predictive Power of Representation Dispersion in Language Models
arxiv_id: '2506.24106'
source_url: https://arxiv.org/abs/2506.24106
tags:
- perplexity
- dispersion
- distance
- llama-3
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Representation dispersion\u2014the average pairwise cosine distance\
  \ among a language model\u2019s contextual embeddings\u2014strongly correlates with\
  \ model perplexity across multiple model families (LLaMA, Qwen, Mistral, Phi, Gemma)\
  \ and domains (Wikipedia, news, medical abstracts). Lower perplexity aligns with\
  \ more dispersed embeddings, even within semantically similar clusters, suggesting\
  \ broader embedding geometry enables sharper next-token predictions."
---

# On the Predictive Power of Representation Dispersion in Language Models
## Quick Facts
- arXiv ID: 2506.24106
- Source URL: https://arxiv.org/abs/2506.24106
- Authors: Yanhong Li; Ming Li; Karen Livescu; Jiawei Zhou
- Reference count: 40
- Primary result: Representation dispersion strongly correlates with model perplexity and enables unsupervised prediction of downstream task performance

## Executive Summary
This paper establishes that representation dispersion—measured as the average pairwise cosine distance among contextual embeddings—strongly correlates with language model perplexity across multiple model families (LLaMA, Qwen, Mistral, Phi, Gemma) and domains (Wikipedia, news, medical abstracts). The research demonstrates that models with lower perplexity exhibit more dispersed embeddings, even within semantically similar clusters, suggesting that broader embedding geometry enables sharper next-token predictions. This relationship is leveraged to create practical applications including unsupervised prediction of downstream task accuracy, improved performance in domain-specific tasks through dispersion-aware model selection, and enhanced retrieval-augmented methods.

The study introduces a push-away training objective that directly increases representation dispersion, showing that this approach reduces perplexity in both single and cross-domain scenarios. The findings position representation dispersion as both a diagnostic tool for understanding model behavior and a training signal for improving language model performance, offering a geometry-based framework for more effective and interpretable language models.

## Method Summary
The research analyzes representation dispersion by computing the average pairwise cosine distance among contextual embeddings produced by various language models. The study examines multiple model families including LLaMA, Qwen, Mistral, Phi, and Gemma across different domains such as Wikipedia, news articles, and medical abstracts. The authors establish the dispersion-perplexity relationship through systematic measurements and then demonstrate practical applications by using dispersion to predict downstream task performance without labels, select optimal layers for retrieval-augmented methods, and incorporate dispersion into training objectives through a push-away loss term.

## Key Results
- Lower perplexity strongly correlates with more dispersed embeddings across multiple model families and domains
- Dispersion measured on unlabeled text accurately predicts downstream task accuracy without requiring labeled data
- Incorporating a push-away objective that increases dispersion directly reduces perplexity in both single-domain and cross-domain scenarios
- Models with larger dispersion gaps between domain-specific and general tokens achieve higher performance in specialized tasks like math and code generation

## Why This Works (Mechanism)
The paper demonstrates that representation dispersion serves as a geometric indicator of model predictive capability. When embeddings are more dispersed, the model can better distinguish between semantically similar contexts, leading to sharper probability distributions over next tokens and consequently lower perplexity. This geometric property appears to capture the model's ability to encode fine-grained distinctions in language understanding, which translates directly to improved predictive performance.

## Foundational Learning
- **Representation dispersion**: The average pairwise cosine distance among contextual embeddings; needed to quantify embedding geometry and its relationship to model performance; quick check: compute cosine distances between all token pairs in a batch
- **Perplexity**: A measure of how well a probability model predicts a sample; needed as the primary performance metric to correlate with dispersion; quick check: calculate cross-entropy loss and exponentiate
- **Cosine similarity**: A measure of similarity between two non-zero vectors; needed to compute pairwise distances in embedding space; quick check: dot product divided by product of vector magnitudes
- **Push-away objective**: A training loss that encourages greater dispersion between embeddings; needed to validate the causal relationship between dispersion and performance; quick check: implement as negative cosine similarity loss between token embeddings
- **Layer-wise dispersion analysis**: Measuring dispersion at different network depths; needed to identify optimal layers for specific tasks like retrieval; quick check: compute dispersion for each transformer layer separately

## Architecture Onboarding
- **Component map**: Input text -> Token embeddings -> Transformer layers -> Contextual embeddings -> Dispersion computation -> Perplexity calculation -> Downstream task evaluation
- **Critical path**: Token embeddings flow through transformer layers to produce contextual representations, which are then used for both dispersion measurement and next-token prediction
- **Design tradeoffs**: Higher dispersion improves perplexity but may affect generation diversity; layer selection balances representation quality with computational efficiency
- **Failure signatures**: Low dispersion may indicate mode collapse or over-smoothing; inconsistent dispersion across domains may suggest poor generalization
- **First experiments**: 1) Measure dispersion-perplexity correlation across model families, 2) Test dispersion-based layer selection for kNN-LM, 3) Implement push-away objective and measure perplexity improvements

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Analysis focuses primarily on perplexity as the performance metric, with limited downstream task evaluations
- The causal mechanisms linking embedding geometry to predictive performance are not fully characterized
- Impact of dispersion-aware training on other model properties (generation quality, training stability) requires further investigation
- Relationship between dispersion and model scale or architecture differences needs systematic study

## Confidence
- High Confidence: Empirical observation that dispersion correlates with perplexity across diverse model families and domains
- Medium Confidence: Dispersion can predict downstream task performance without labels, though validation across broader task sets needed
- Medium Confidence: Push-away objective improves perplexity in controlled experiments but requires validation across different model scales and architectures

## Next Checks
1. Evaluate the dispersion-perplexity relationship across a broader set of downstream tasks including classification, summarization, and long-form generation to assess universal predictive capability
2. Systematically investigate how the dispersion-perplexity relationship scales with model size across multiple orders of magnitude
3. Compare dispersion against other geometric properties of embedding spaces (isotropy measures, manifold curvature) to determine unique information capture