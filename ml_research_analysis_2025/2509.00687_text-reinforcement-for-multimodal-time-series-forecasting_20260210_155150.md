---
ver: rpa2
title: Text Reinforcement for Multimodal Time Series Forecasting
arxiv_id: '2509.00687'
source_url: https://arxiv.org/abs/2509.00687
tags:
- text
- series
- time
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unstable performance in multimodal
  time series forecasting (TSF) due to low-quality text inputs that fail to capture
  information from historical time series. The authors propose a Text Reinforcement
  (TeR) model that generates reinforced text using a large language model (LLM) to
  address potential weaknesses in the original text.
---

# Text Reinforcement for Multimodal Time Series Forecasting

## Quick Facts
- **arXiv ID:** 2509.00687
- **Source URL:** https://arxiv.org/abs/2509.00687
- **Reference count:** 40
- **Primary result:** TeR model achieves lower MSE/MAE than strong baselines on Time-MMD benchmark by generating reinforced text via LLM and DPO optimization.

## Executive Summary
This paper addresses the problem of unstable multimodal time series forecasting (TSF) performance due to low-quality or misaligned text inputs. The authors propose Text Reinforcement (TeR), a framework that uses a large language model to generate reinforced text that captures temporal patterns more effectively than the original sparse text. The LLM is guided by a dual-objective reward function evaluating prediction accuracy and task relevance, optimized through reinforcement learning with Direct Preference Optimization (DPO). Experiments on the Time-MMD benchmark demonstrate that TeR outperforms state-of-the-art multimodal TSF models, achieving significant improvements in mean squared error and mean absolute error across multiple domains.

## Method Summary
The TeR framework generates reinforced text by converting numerical time series to 4-level precision text plus statistical descriptors, concatenating with original text and task prompts, then using a Qwen3-1.7B LLM to generate candidates. These candidates are scored using a dual reward function measuring forecasting improvement (negative MSE) and keyword relevance (task-specific terms like "trend" and "seasonality"). The top and bottom candidates form preference pairs for DPO training, which iteratively refines the LLM over 4 rounds with 2 candidates per round. The reinforced text is then used as input to a multimodal TSF model (PatchTST encoder + Llama-3.1-8B text embeddings with cross-attention fusion) for final prediction.

## Key Results
- TeR achieves avg MSE of 0.631 and MAE of 0.482 on Time-MMD benchmark, outperforming state-of-the-art baselines
- Dual-reward DPO (accuracy + task relevance) outperforms single-reward variants, confirming the importance of keyword alignment
- Optimal configuration uses k=2 candidates and m=4 DPO rounds; performance degrades beyond these values due to overfitting or extreme preference pairs
- Smaller LLMs (1.7B) outperform larger ones (8B) on TSF tasks due to reduced overfitting risk on limited time series data

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Text Reinforcement via LLM
Converting numerical time series to text and processing through an LLM produces semantically richer representations that capture temporal patterns more effectively than raw numerical inputs alone. The TeR model takes time series data S and available text E, converts S to fixed 4-level precision text (Stxt) plus statistical descriptors (Atxt such as mean, variance), concatenates with original text E and task-specific prompts P, then generates reinforced text Eaug = fTeR(Stxt, Atxt, E, P). This reinforced text is then used as input to the multimodal TSF model. Core assumption: LLMs can infer and articulate temporal relationships (trends, seasonality, fluctuations) from numerical sequences better than the original sparse or misaligned text. Evidence: t-SNE visualization shows reinforced text forms distinct clusters with more temporally-relevant tokens like "trend" and "seasonality"; LLM-PS paper confirms LLMs have powerful in-contextual modeling capabilities for TSF tasks.

### Mechanism 2: Dual-Objective Reward-Guided Quality Assessment
A composite reward function balancing prediction accuracy and task relevance can effectively proxy text quality without direct human annotation. Reward r = w1·r1 + w2·r2 where r1 = negative MSE (measures forecasting improvement from text) and r2 = normalized count of task-relevant keywords from a predefined set K (e.g., "peak," "fluctuation," "seasonality"). Higher scores indicate text that both improves predictions and uses domain-appropriate terminology. Core assumption: Text that minimizes forecasting error and contains domain keywords is genuinely "higher quality" for TSF tasks; the keyword set K adequately captures task relevance across domains. Evidence: TSF+TeR+r12 (dual reward) outperforms TSF+TeR+r1 (single reward) with avg MSE 0.631 vs 0.689; dual reward jointly optimizes forecasting utility and task relevance.

### Mechanism 3: Iterative Preference Optimization via DPO
Direct Preference Optimization can iteratively refine LLM text generation by learning from self-generated quality rankings without external human labels. For m rounds: (1) TeR generates k candidate reinforced texts {E¹aug, E²aug, ..., Ekaug}; (2) Reward generator scores each candidate; (3) Highest-ranked becomes positive example E⁺aug, lowest-ranked becomes negative E⁻aug; (4) DPO loss maximizes likelihood gap between preferred and dispreferred outputs; (5) Updated TeR regenerates candidates for next round. Loss: L = E log[exp(βr⁺)/(exp(βr⁺) + exp(βr⁻))]. Core assumption: The reward function provides sufficiently reliable quality signals that DPO can leverage; k=2 candidates and m=4 rounds represent optimal exploration-exploitation balance. Evidence: Performance peaks at m=4, degrades beyond due to overfitting; k=2 optimal; k>2 introduces low-quality texts that create extreme preference pairs, causing DPO to miss nuanced learning signals.

## Foundational Learning

- **Concept: Multimodal Time Series Forecasting**
  - Why needed here: The paper assumes familiarity with combining numerical time series with auxiliary modalities (text). Understanding why text helps—capturing external context, explaining anomalies, providing domain knowledge—is essential for grasping why text reinforcement matters.
  - Quick check question: Can you explain why adding news sentiment to stock price data might improve forecasts compared to using prices alone?

- **Concept: Reinforcement Learning from Human/AI Feedback (RLHF/DPO)**
  - Why needed here: The core innovation uses DPO to optimize an LLM. Understanding preference-based learning—how models learn from comparative rankings rather than absolute labels—is critical for understanding the training pipeline.
  - Quick check question: How does DPO differ from traditional supervised fine-tuning, and why might it be preferred when direct quality labels are unavailable?

- **Concept: Time Series Representation Learning**
  - Why needed here: The paper uses PatchTST to encode time series and cross-attention to fuse with text embeddings. Understanding patch-based tokenization and attention-based multimodal fusion explains how reinforced text integrates with numerical patterns.
  - Quick check question: What are the trade-offs between patch-based and point-based time series representations for capturing long-range temporal dependencies?

## Architecture Onboarding

- **Component map:**
  Data Preprocessing Module -> Text Reinforcement Model (TeR) -> Multimodal TSF Model -> Reward Generator (RG) -> DPO Optimizer -> LoRA Update TeR

- **Critical path:**
  Time series S + original text E → Preprocessing → TeR generates k candidates → Multimodal TSF model produces predictions → Reward Generator scores each candidate → DPO selects best/worst → LoRA update TeR → Repeat for m rounds → Final TeR produces Eaug for inference

- **Design tradeoffs:**
  - **k (candidates per round)**: k=2 optimal; k>2 introduces low-quality texts that create extreme preference pairs, causing DPO to miss nuanced learning signals
  - **m (training rounds)**: m=4 optimal; m<4 underfits, m>4 overfits to limited training data
  - **LLM size for TeR**: Smaller models (Qwen3-1.7B) outperform larger ones (4B, 8B) due to overfitting risk with limited TSF data; optimal size is task-dependent
  - **Reward weights (w1, w2)**: Paper uses equal weighting implicitly; domain-specific tuning may be needed

- **Failure signatures:**
  - **Overfitting in DPO**: MSE/MAE increase after m=4 rounds—model memorizes training patterns, loses generalization
  - **Extreme preference pairs**: Degraded performance when k>2—model only sees best vs worst, ignores middle-quality examples needed for gradual learning
  - **Large LLM underperformance**: 8B models worse than 1.7B—overfitting to small TSF datasets
  - **Text-token misalignment**: Original TSF+TeR (no RL) performs worse than baselines—unguided LLM generates irrelevant text that harms predictions

- **First 3 experiments:**
  1. **Baseline replication**: Implement TSF-only (PatchTST without text) and TSF+Text (with original text) on a single Time-MMD domain (e.g., Energy). Verify that text helps in some domains but not others, confirming the quality sensitivity problem.
  2. **TeR ablation without RL**: Generate reinforced text using a pretrained LLM (Qwen3-1.7B) with prompt-only guidance (no DPO). Compare MSE/MAE against TSF+Text to confirm that unguided generation degrades performance (replicating Table 2 TSF+TeR result).
  3. **Single vs. dual reward**: Implement DPO training with r1 only (prediction accuracy), then add r2 (task relevance). Compare final MSE/MAE to validate the dual-reward benefit shown in Table 2 (TSF+TeR+r1 vs. TSF+TeR+r12). Monitor generated text for keyword density changes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the TeR framework be adapted to effectively leverage larger Large Language Models (e.g., >3B parameters) without experiencing the performance degradation caused by overfitting on time-series datasets?
- **Basis in paper:** Section 5.4 notes that increasing model size to 8B often fails to improve performance, and the analysis suggests this is due to time series datasets being smaller than text corpora, causing overfitting.
- **Why unresolved:** The authors demonstrate that smaller models (1.7B) perform best, but they do not propose methods (e.g., regularization, adapter modules) to unlock the potential reasoning capacity of larger models without succumbing to data scarcity.
- **What evidence would resolve it:** An experiment showing that a specific regularization technique or hierarchical fine-tuning strategy allows an 8B parameter model to outperform the 1.7B baseline on the Time-MMD dataset.

### Open Question 2
- **Question:** Can the static, keyword-based "Task Relevance Reward" be replaced by a dynamic, embedding-based metric to eliminate the need for manual domain expert intervention?
- **Basis in paper:** Section 3.3.1 states that the keyword set K is "predefined... based on domain expertise," implying a manual engineering step that varies by domain.
- **Why unresolved:** The current reliance on predefined keywords limits the model's ability to generalize to new domains where expert-curated keyword lists are unavailable or insufficient to capture nuanced temporal dynamics.
- **What evidence would resolve it:** A comparative study showing that an automated, learnable reward function (e.g., using cosine similarity between generated text embeddings and temporal features) achieves equivalent or superior performance without manual keyword definition.

### Open Question 3
- **Question:** How can the DPO training process be stabilized to benefit from generating a larger number of candidate texts (k > 2), rather than suffering from "excessively extreme quality differences"?
- **Basis in paper:** Section 5.2 reports that increasing the number of candidates k beyond 2 degrades performance because the resulting preference pairs exhibit "excessively extreme quality differences," forcing the model to learn with "excessively large steps."
- **Why unresolved:** The framework currently identifies the problem of extreme contrast in preference pairs but does not offer a mechanism to filter or weight intermediate candidates to create a smoother learning curve.
- **What evidence would resolve it:** A modified training strategy (e.g., using margin-based sampling or curriculum learning on preference pairs) that results in lower MSE/MAE when k=8 or k=10 compared to the current optimal setting of k=2.

## Limitations

- **Incomplete keyword specification**: The task relevance reward depends on a predefined keyword list that is only partially specified ("peak," "fluctuation," "seasonality"), limiting reproducibility and cross-domain generalization.
- **Unstated reward weights**: The dual-reward function uses scalar weights w₁ and w₂ that are not explicitly stated, potentially affecting optimization outcomes and making exact replication difficult.
- **Limited real-world noise testing**: While the Time-MMD benchmark is diverse, it may not fully represent real-world scenarios with severe text noise or extreme time series volatility, creating uncertainty about real-world applicability.

## Confidence

- **Keyword Set Completeness**: Low confidence - partial keyword specification creates uncertainty about reproducibility and domain generalization
- **Reward Weighting**: Medium confidence - dual-reward mechanism effectiveness demonstrated, but specific weight values unknown
- **Generalization to Noisy Domains**: Medium confidence - benchmark diversity limited, real-world noise scenarios not extensively tested

## Next Checks

1. **Keyword Sensitivity Analysis**: Test the model with different keyword sets (expanded, reduced, domain-specific) to quantify the impact on forecasting performance and text quality. This validates whether the keyword-based reward is robust or brittle.

2. **Ablation on Candidate Size and Rounds**: Systematically vary k (1, 2, 4, 8) and m (1, 2, 4, 8) to confirm the claimed optimal values (k=2, m=4) and identify the overfitting threshold under different data conditions.

3. **Cross-Domain Transfer Test**: Train the model on one domain (e.g., Energy) and evaluate on another (e.g., Traffic) without fine-tuning. Measure MSE/MAE degradation and analyze whether the reinforced text remains temporally relevant across domains.