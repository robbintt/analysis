---
ver: rpa2
title: 'Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity
  Gap with LLMs'
arxiv_id: '2510.11313'
source_url: https://arxiv.org/abs/2510.11313
tags:
- skill
- ontology
- zero-shot
- decomposition
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of automated skill decomposition
  using large language models (LLMs), aiming to generate fine-grained sub-skills from
  broad skill labels while maintaining alignment with expert ontologies. The core
  method introduces a rigorous framework that standardizes prompting, candidate generation,
  normalization, and alignment with ontology nodes, alongside two evaluation metrics:
  a semantic F1-score for content accuracy and a hierarchy-aware F1-score for granularity
  assessment.'
---

# Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs

## Quick Facts
- arXiv ID: 2510.11313
- Source URL: https://arxiv.org/abs/2510.11313
- Reference count: 36
- Primary result: Few-shot prompting with ontology-derived exemplars improves granularity and hierarchy-aware alignment, especially for medium-scale models.

## Executive Summary
This paper tackles the challenge of decomposing broad skill labels into fine-grained sub-skills using large language models, while ensuring alignment with expert ontologies. The authors introduce a rigorous framework that combines standardized prompting, candidate generation, normalization, and alignment with ontology nodes. Two evaluation metrics—semantic F1-score and hierarchy-aware F1-score—are proposed to assess content accuracy and granularity. Experiments on the ROME-ESCO-DecompSkill benchmark show that few-shot prompting with ontology exemplars consistently outperforms zero-shot baselines, particularly for medium-scale models, by acting as structural priors to stabilize granularity and reduce hallucinations.

## Method Summary
The framework takes a broad skill label and decomposes it into exactly k (5-12) fine-grained sub-skills. It employs either zero-shot or few-shot prompting with ontology-derived exemplars, normalizes LLM outputs, and aligns them to ontology nodes using multilingual Sentence-BERT embeddings and cosine similarity. Optimal 1-1 matching (Hungarian algorithm) is used to compute semantic and hierarchy-aware F1-scores. The method ensures schema compliance and evaluates generalization using label-disjoint exemplars.

## Key Results
- Few-shot prompting with ontology exemplars improves phrasing, granularity, and hierarchy-aware alignment, especially for medium-scale models.
- Semantic alignment via SBERT embeddings effectively grounds free-text LLM outputs to expert ontology nodes better than lexical matching.
- Exemplar-guided prompts can reduce generation latency compared to unconstrained zero-shot baselines due to schema-compliant completions.

## Why This Works (Mechanism)

### Mechanism 1
Ontology-derived exemplars act as structural priors that stabilize granularity and hierarchy-aware alignment, particularly for medium-scale models. Few-shot prompting uses label-disjoint exemplars to demonstrate the desired level of specificity and phrasing style, regularizing the LLM's output and reducing granularity drift.

### Mechanism 2
Semantic alignment via embedding-based matching effectively grounds free-text LLM outputs to expert ontology nodes better than lexical matching. The system uses multilingual Sentence-BERT embeddings and cosine similarity, followed by Hungarian matching, to maximize semantic overlap.

### Mechanism 3
Enforcing schema-compliant completions via exemplars can reduce generation latency compared to unconstrained zero-shot baselines. Few-shot prompts define a strict output schema, potentially leading to earlier termination of generation due to more structured outputs.

## Foundational Learning

- **Concept: Hierarchical F1 vs. Semantic F1**
  - **Why needed here:** Standard F1 only checks if content matches. Hierarchical F1 is required to evaluate if the generated sub-skill sits at the correct depth (granularity) in the taxonomy.
  - **Quick check question:** If a model predicts "Machine Learning" instead of "Statistical Modeling" for a "Data Analysis" parent, which metric penalizes the depth error?

- **Concept: Leakage-Safe Exemplars**
  - **Why needed here:** To evaluate generalization, the few-shot prompt must not contain the test data. Understanding how to select "label-disjoint" examples is critical for building the training set.
  - **Quick check question:** If your target skill is "Java Programming," would "Python Scripting" be a safe exemplar candidate? (Answer: Yes, if it shares the same parent structure but is not a child of the target).

- **Concept: Ontology as "Ruler" vs. "Source"**
  - **Why needed here:** The paper explicitly treats the ontology as an evaluation tool (gold standard), not a training database. This distinction is vital for architecting the pipeline (the ontology is offline/external).
  - **Quick check question:** In this framework, does the LLM query the ontology during generation to find answers?

## Architecture Onboarding

- **Component map:** Input (broad skill + controls) -> Prompt Builder (zero-shot/few-shot context) -> LLM Generator (raw list) -> Normalizer (lowercase, dedupe, filter) -> Aligner (SBERT + cosine + Hungarian) -> Scorer (Semantic F1 + Hierarchy-aware F1).

- **Critical path:** The Alignment Operator (α). This is where raw LLM text is mapped to the structured ontology. Errors here invalidate all subsequent scoring.

- **Design tradeoffs:**
  - **Prompt Strategy:** Zero-shot is faster to implement but suffers "granularity drift." Few-shot requires curated exemplars but stabilizes hierarchy.
  - **Model Scale:** Larger models have stronger priors but may suffer recall drops if few-shot exemplars constrain them too much. Medium models benefit most from structural priors.

- **Failure signatures:**
  - **Over-regularization:** High Precision but low Recall in Few-shot (model copies style of exemplar but misses diverse gold children).
  - **Hallucination Drift:** High semantic match but low hierarchy score (model finds valid concepts but places them at incorrect depths).
  - **Latency Bloat:** Few-shot takes longer than Zero-shot (exemplar token processing dominates decoding gains).

- **First 3 experiments:**
  1. **Baseline Probe:** Run Zero-shot on 10 parents; verify normalization pipeline strips verbs/tools correctly.
  2. **Threshold Sensitivity:** Vary the alignment threshold τ (e.g., 0.7 vs 0.78 vs 0.85) to optimize Semantic F1 without accepting false positives.
  3. **Exemplar Ablation:** Compare random exemplars vs. depth-aligned exemplars for a specific mid-scale model (e.g., Llama4 Scout) to quantify the "structural prior" gain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can retrieval-augmented grounding using masked ontology evidence further reduce domain drift compared to leakage-safe few-shot prompting?
- **Basis in paper:** The conclusion states an intent to "investigate retrieval-augmented grounding with masked ontology evidence."
- **Why unresolved:** The current study relies on leakage-safe, label-disjoint exemplars to guide style; it is unknown if providing masked, relevant ontology context can improve specificity without compromising the "closed world" evaluation fairness.
- **What evidence would resolve it:** Experiments comparing the proposed masked RAG approach against the current zero-shot and few-shot baselines on the ROME-ESCO-DecompSkill benchmark, measuring semantic F1 and hallucination rates.

### Open Question 2
- **Question:** Does an adaptive exemplar selection strategy outperform the current depth-proximity heuristic in stabilizing granularity?
- **Basis in paper:** The conclusion lists "adaptive exemplar selection" as a future direction, while Section 3.3 notes current exemplars use a "depth-proximity heuristic."
- **Why unresolved:** The paper observes that poorly matched exemplars can constrain coverage in large models (precision–recall trade-off); adaptive selection might optimize this trade-off dynamically per skill.
- **What evidence would resolve it:** Implementation of an adaptive retrieval mechanism for exemplars and a comparative analysis of hierarchy-aware F1-scores against the static heuristic baseline.

### Open Question 3
- **Question:** Does the efficacy of the proposed framework generalize to skills with extreme branching factors (i.e., parents with fewer than 5 or more than 12 children)?
- **Basis in paper:** Section 4.1 explicitly restricts the evaluation to "parents with between 5 and 12 direct children," despite the dataset containing a median of 2 and a maximum of 270 children per parent.
- **Why unresolved:** It remains untested if the latency benefits and "schema-compliant completions" hold when the target output cardinality k is very small (1-4) or very large (>12).
- **What evidence would resolve it:** Extending the experiments to the excluded nodes in the ontology and reporting performance metrics (Semantic F1 and Latency) across the full distribution of branching factors.

## Limitations
- Specific LLM checkpoints (GPT-5, Llama4 Scout, Qwen3) are not publicly available, making exact replication uncertain.
- The "depth-proximity heuristic" for exemplar selection is not fully specified, leaving ambiguity in the few-shot pipeline.
- Latency advantages of exemplar-guided prompts over zero-shot are reported but lack independent validation.

## Confidence
- **High confidence:** Semantic alignment via SBERT embeddings effectively grounds free-text LLM outputs to ontology nodes, as the embedding model and matching algorithm are standard and well-documented.
- **Medium confidence:** Few-shot prompting with ontology-derived exemplars improves granularity and hierarchy-aware alignment, particularly for medium-scale models, though exact gains depend on exemplar quality and model access.
- **Low confidence:** Latency advantages of exemplar-guided prompts over zero-shot are reported but lack independent validation, as no neighbor papers address this specific trade-off.

## Next Checks
1. **Threshold Sensitivity:** Vary the alignment threshold τ (e.g., 0.7, 0.78, 0.85) to optimize Semantic F1 and assess false positive risk.
2. **Exemplar Ablation:** Compare random versus depth-aligned exemplars for a specific mid-scale model (e.g., Llama4 Scout) to quantify the "structural prior" effect on granularity.
3. **Schema Compliance Test:** Enforce strict regex parsing and prompt constraints to verify that LLM outputs conform to noun phrase schema and reduce non-compliance errors.