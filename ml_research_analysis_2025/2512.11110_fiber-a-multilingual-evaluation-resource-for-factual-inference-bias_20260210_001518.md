---
ver: rpa2
title: 'FIBER: A Multilingual Evaluation Resource for Factual Inference Bias'
arxiv_id: '2512.11110'
source_url: https://arxiv.org/abs/2512.11110
tags:
- bias
- factual
- language
- languages
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIBER, a multilingual benchmark for evaluating
  factual knowledge in large language models (LLMs). It addresses the limitations
  of existing benchmarks by including both single- and multi-entity answers across
  English, Italian, and Turkish.
---

# FIBER: A Multilingual Evaluation Resource for Factual Inference Bias

## Quick Facts
- arXiv ID: 2512.11110
- Source URL: https://arxiv.org/abs/2512.11110
- Reference count: 0
- Key result: FIBER shows 31% of topics exhibit factual inference bias >0.5, with Turkish prompts showing higher bias than Italian in 83% of cases

## Executive Summary
This paper introduces FIBER, a multilingual benchmark for evaluating factual knowledge and inference bias in large language models across English, Italian, and Turkish. The dataset addresses limitations in existing benchmarks by including both single- and multi-entity answers, enabling assessment of factual inference bias and performance differences between entity types. Using FIBER, the authors examine how prompt language affects entity selection and compare model performance on single- versus multi-entity questions. Results reveal systematic biases in language-aligned entity selection and performance degradation as the number of correct answers increases.

## Method Summary
FIBER employs rank-based probing without training, computing cumulative log-probabilities for all candidates in a predefined surface set, then ranking candidates and computing Average Precision (AP) against gold answers. Factual Inference Bias is measured by comparing the proportion of language-specific entities appearing in top-ranked positions. The evaluation uses four models (Gemma-3-4B, Qwen-2.5-3B, Qwen-2.5-7B, Llama-3.1-8B) across three languages with both sentence completion and question-answering prompt formats, covering 14,281 entries across 8 single-entity and 8 multi-entity topics.

## Key Results
- 31% of topics exhibit factual inference bias score greater than 0.5
- Turkish prompts show higher bias than Italian in 83% of topics
- Models perform significantly better on single-entity questions (up to 38% MAP in English) than multi-entity ones (as low as 12% MAP)
- English achieves highest MAP (38%) while Turkish and Italian show similar performance (31-32%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt language causally influences entity selection by activating language-locale associations embedded in the model's training distribution.
- **Mechanism:** When a prompt is provided in Italian or Turkish, the model's probability ranking over candidate entities shifts toward entities culturally/geographically associated with that language's region, even when factually incorrect. This occurs because the model has learned statistical correlations between language tokens and locale-specific entities during pretraining.
- **Core assumption:** The model encodes joint distributions over (language, entity) pairs from training data, and prompting in a specific language conditions the sampling distribution toward that language's associated entities.
- **Evidence anchors:** [abstract] "31% of the topics exhibit factual inference bias score greater than 0.5... Turkish prompts show higher bias compared to Italian in 83% of the topics"; [section 4.2.1] "Polyglot Celebrities, Official Languages, Capital Cities, and Original Languages of the Books are the top four topics with the highest Factual Inference Bias, ranging between 0.86 and 0.93"; [corpus] Related work (Kim & Kim, 2025) documents similar language-aligned response patterns, but corpus lacks direct mechanistic evidence for why Turkish exhibits higher bias than Italian
- **Break condition:** If the model were fine-tuned with explicit de-biasing objectives or if entities had no geographical/cultural associations, this mechanism would weaken.

### Mechanism 2
- **Claim:** Multi-entity retrieval requires maintaining multiple valid probability peaks simultaneously, which degrades as entity count increases.
- **Mechanism:** Single-entity questions map to a unimodal distribution where the correct answer dominates. Multi-entity questions require the model to assign high probability to multiple tokens that don't share common prefixes, fragmenting probability mass. The ranking-based evaluation (Average Precision) penalizes this dispersion.
- **Core assumption:** Models optimize for single-best prediction during training, not calibrated multi-label ranking.
- **Evidence anchors:** [abstract] "Models perform better on single-entity questions (up to 38% MAP in English) than multi-entity ones (as low as 12% MAP)"; [section 4.2.2] "as the entity count of the subject decreases, the average precision increases across all languages and topics"; [corpus] Multi-span QA datasets (MultiSpanQA, MyriadLAMA) exist but corpus lacks comparative mechanistic studies of multi-entity probability fragmentation
- **Break condition:** If models were explicitly trained with multi-label ranking losses or beam-aware objectives, performance gaps would narrow.

### Mechanism 3
- **Claim:** Performance disparity across languages correlates with training data resource levels, amplifying factual errors in lower-resource languages.
- **Mechanism:** English-dominated pretraining creates richer factual representations for English prompts. Italian and Turkish, as lower-resource languages, have fewer factual exemplars during training, leading to weaker entity-relation bindings and lower MAP scores.
- **Core assumption:** Factual knowledge is stored language-specifically rather than in a language-agnostic latent space with shared cross-lingual retrieval.
- **Evidence anchors:** [abstract] "The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores"; [section 4.2.3] "English achieves the highest overall results, with a mean average precision (MAP) of 38%. Both Italian and Turkish demonstrate similar performance levels, with 32% and 31% MAP, respectively"; [corpus] Qiu et al. (2023) and Chataigner et al. (2024) document higher hallucination rates in non-English languages, supporting resource-disparity hypothesis
- **Break condition:** If cross-lingual representation alignment were sufficiently strong, factual retrieval would transfer across languages without performance gaps.

## Foundational Learning

- **Concept: Factual Knowledge Probing (Rank-Based Evaluation)**
  - **Why needed here:** FIBER evaluates models not by generation quality but by probability ranking over candidate entities. Understanding that evaluation uses log-probability accumulation and Average Precision is essential.
  - **Quick check question:** Given a prompt and candidate answers [A, B, C, D], if the model assigns log-probs [-1.2, -2.5, -0.8, -3.1], what is Precision@2 if A and D are ground truth?

- **Concept: Inference Bias vs. Factual Inference Bias**
  - **Why needed here:** The paper distinguishes general inference bias (frequency of language-aligned responses) from factual inference bias (probability ranking shifts for factual questions). The latter uses token-level log-probabilities rather than generation counts.
  - **Quick check question:** If a model answers "Rome" to "What is the capital of France?" when prompted in Italian, is this inference bias, factual inference bias, both, or neither?

- **Concept: Multi-Entity Answer Structures**
  - **Why needed here:** FIBER introduces one-to-many subject-entity relations (e.g., Canada → {English, French}). Evaluating multi-entity recall requires metrics like Average Precision rather than exact-match accuracy.
  - **Quick check question:** For a question with 3 correct answers among 10 candidates, if the model ranks correct answers at positions 1, 4, and 7, what is the Average Precision?

## Architecture Onboarding

- **Component map:** Prompt templates -> Surface set (candidate answers) -> Log-probability extractor -> Ranking module -> Bias detector
- **Critical path:** Load subject-relation-entity triplets for target topic → Construct prompts in all three languages (both SC and QA formats) → For each candidate in surface set: compute cumulative log-prob via iterative token scoring → Rank candidates; compute AP for factual recall, compare against Q(l,t) for bias score → Aggregate MAP across subjects; report bias by topic, language, model
- **Design tradeoffs:** Closed-set vs. open-ended (FIBER uses predefined candidate sets for controlled comparison, but cannot detect hallucinations outside the surface set); Rank-based vs. generation-based (Log-prob evaluation avoids sampling variance but may not reflect real-world deployment behavior); Topic selection (Focus on country/language-related topics maximizes bias observability but limits domain generality)
- **Failure signatures:** Tokenization mismatches (Turkish suffix variants cause duplicate entries or missed candidates); Translation quality (Entity names may differ across languages (e.g., "Germany" vs. "Almanya"); surface set must include all variants); Log-prob NaN (Multi-token candidates with out-of-vocabulary subwords produce invalid scores)
- **First 3 experiments:** Baseline reproduction (Run FIBER evaluation on Llama-3.1-8B across all topics in English; verify MAP values match reported ~38% for single-entity tasks); Bias isolation (For Official Languages topic, compare entity ranking distributions between Italian and Turkish prompts; confirm Turkish bias exceeds Italian in >80% of subjects); Entity count ablation (Stratify MAP by entity count (1, 2-3, 4-6, 7+); plot the inverse relationship and identify the count threshold where MAP drops below 20%)

## Open Questions the Paper Calls Out

- **Question:** Can mechanistic interpretability techniques pinpoint the specific internal components responsible for the higher factual inference bias observed in Turkish compared to Italian?
  - **Basis in paper:** [explicit] The Conclusion states that "mechanistic interpretability methods... should be employed to uncover the underlying causes of this bias."
  - **Why unresolved:** The current study quantifies the bias (e.g., Turkish prompts showing higher bias than Italian in 83% of topics) but does not investigate the internal model architecture or weights driving this behavior.
  - **What evidence would resolve it:** Causal tracing or activation patching experiments that identify specific attention heads or neurons which, when ablated, equalize the bias scores between Turkish and Italian prompts.

- **Question:** Does factual inference bias decrease or persist when evaluating larger-scale models beyond the 3B–8B parameter range tested in this study?
  - **Basis in paper:** [explicit] The Limitations section notes that the study "focuses on smaller models in the range of 3B to 8B" and suggests "further evaluations should also be conducted on larger-scaled models."
  - **Why unresolved:** It is unclear if the observed bias is an artifact of model capacity or a systemic issue across all scales.
  - **What evidence would resolve it:** Benchmarking FIBER on models with parameters significantly exceeding 8B (e.g., 70B or 405B models) to compare the resulting factual inference bias scores against the current baselines.

- **Question:** How does factual inference bias manifest in open-ended text generation compared to the log-probability ranking approach used in FIBER?
  - **Basis in paper:** [explicit] The Limitations section states the analysis is restricted to log-probabilities and "can be extended to open-ended text generation... rather than focusing solely on factual correctness."
  - **Why unresolved:** The current methodology limits the candidate answers to a fixed surface set; it is unknown if the model hallucinates different or more severe language-aligned entities when forced to generate free-form text.
  - **What evidence would resolve it:** A comparative evaluation where models generate open-ended answers for the FIBER prompts, analyzed for hallucination rates and bias metrics distinct from the ranking-based approach.

## Limitations

- The dataset focuses on country/language-related topics, which may not reflect bias patterns in other domains
- The closed-set evaluation using predefined candidate answers cannot detect hallucinations beyond the surface set, potentially underestimating model weaknesses
- While the log-probability ranking approach avoids sampling variance, it may not fully capture real-world generation behavior where models use sampling strategies

## Confidence

- **High confidence**: The empirical finding that 31% of topics exhibit factual inference bias >0.5 and that Turkish prompts show higher bias than Italian in 83% of cases. The systematic performance advantage of single-entity questions over multi-entity ones (MAP difference up to 26 percentage points) is well-supported.
- **Medium confidence**: The mechanistic explanation that prompt language activates language-locale associations through training distribution correlations. While the bias patterns are clear, direct evidence for the causal mechanism requires further validation.
- **Medium confidence**: The resource-level explanation for cross-linguistic performance gaps. The correlation between English dominance and higher performance is observed, but without training corpus analysis, the causal link remains inferential.

## Next Checks

1. **Cross-domain bias validation**: Apply FIBER evaluation to non-geopolitical topics (e.g., scientific concepts, historical events) to determine if the language-aligned bias pattern persists across domains
2. **Open-ended generation comparison**: Supplement closed-set ranking with open-ended generation experiments using the same prompts to assess whether log-probability evaluation underestimates actual bias in deployment scenarios
3. **Cross-lingual representation analysis**: Conduct probing experiments to determine whether factual knowledge is stored language-specifically or in shared cross-lingual latent spaces, validating the resource-disparity hypothesis