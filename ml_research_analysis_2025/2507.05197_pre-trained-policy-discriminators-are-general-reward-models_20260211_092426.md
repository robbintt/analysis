---
ver: rpa2
title: Pre-Trained Policy Discriminators are General Reward Models
arxiv_id: '2507.05197'
source_url: https://arxiv.org/abs/2507.05197
tags:
- reward
- arxiv
- policy
- polar
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Policy Discriminative Learning (POLAR), a
  novel reward modeling approach that frames reward models as policy discriminators
  quantifying differences between training and target policies. Instead of traditional
  absolute preference modeling, POLAR pre-trains reward models to recognize identical
  policies and discriminate different ones through contrastive learning, enabling
  criterion-agnostic initialization.
---

# Pre-Trained Policy Discriminators are General Reward Models

## Quick Facts
- arXiv ID: 2507.05197
- Source URL: https://arxiv.org/abs/2507.05197
- Reference count: 40
- Pre-trained reward models achieve 81.0% preference accuracy on STEM tasks vs 54.8% baseline

## Executive Summary
This paper introduces Policy Discriminative Learning (POLAR), a novel reward modeling approach that frames reward models as policy discriminators quantifying differences between training and target policies. Instead of traditional absolute preference modeling, POLAR pre-trains reward models to recognize identical policies and discriminate different ones through contrastive learning, enabling criterion-agnostic initialization. The method demonstrates superior performance across benchmarks and reveals power-law scaling relationships between model size/compute and performance.

## Method Summary
POLAR consists of two stages: pre-training and fine-tuning. In pre-training, a reward model is trained on synthetic data from 184 diverse LLMs to distinguish whether two trajectories originated from the same policy (positive pairs) or different policies (negative pairs) using Bradley-Terry loss. This creates a criterion-agnostic policy discriminator. In fine-tuning, the model is trained on human-annotated preference triplets to align the policy discrimination signal with human preferences. The input format concatenates prompt, reference, and candidate trajectories with special tokens. The model is initialized from InternLM2.5 and fine-tuned using standard RLHF techniques.

## Key Results
- POLAR-7B achieves 81.0% preference accuracy on STEM tasks (vs 54.8% baseline) and 85.5% on creative writing tasks (vs 57.9% baseline)
- RLHF training performance improves from 47.36% to 56.33% on average across 20 benchmarks
- Power-law scaling relationships show correlation coefficients approaching 0.99 between model size/compute and performance

## Why This Works (Mechanism)

### Mechanism 1: Criterion-Agnostic Policy Discrimination
Pre-training a Reward Model to distinguish whether two trajectories originated from the same policy imparts a fundamental, transferable representation of "policy similarity," independent of specific human preference criteria. The model learns latent features characterizing a policy's behavioral distribution rather than just surface-level semantic quality.

### Mechanism 2: Distributional Alignment via Density Ratio
The reward signal generated by the discriminator approximates the divergence between the training policy and the target policy. Theoretically, the optimal reward function under KL-constrained RL optimization is proportional to the log-density ratio of the optimal policy vs. the initial policy, and POLAR implicitly learns this density ratio.

### Mechanism 3: Predictable Scaling (Power Law)
Performance scales predictably with model size and compute as the model capacity increases, allowing it to better model the complex, high-dimensional distributions of diverse policies. The validation loss decreases as a power law function of parameters and compute.

## Foundational Learning

- **Concept: Bradley-Terry Model (Pairwise Ranking)**
  - Why needed here: This is the loss function used in both pre-training and fine-tuning stages. Understanding $P(A \succ B) = \sigma(r(A) - r(B))$ is crucial to grasping how the model learns relative "distance."
  - Quick check question: If Reward(A) - Reward(B) is large, what does the sigmoid function output, and how does that affect the loss?

- **Concept: KL-Divergence in RLHF**
  - Why needed here: The paper derives its validity from the link between the reward function and the KL-divergence between policies. You must understand that minimizing KL-divergence forces two probability distributions to overlap.
  - Quick check question: Does minimizing the KL-divergence between Policy A and Policy B make Policy A generate outputs *identical* to Policy B, or just *probabilistically similar*?

- **Concept: Contrastive Learning (Positive/Negative Pairs)**
  - Why needed here: The core pre-training innovation is contrastive: treating outputs from the same policy as "positive" pairs and different policies as "negative" pairs.
  - Quick check question: In the POLAR pre-training phase, is the "positive" pair defined by human preference or by the identity of the generating model?

## Architecture Onboarding

- **Component map:**
  - Input: `[Prompt] + [Reference] + <|split_token|> + [Prompt] + [Candidate] + <|reward_token|>`
  - Backbone: InternLM2.5 (Autoregressive Transformer)
  - Head: Linear layer on the hidden state of `<|reward_token|>`
  - Data Engine: Policy Pool of 100+ LLMs used to generate synthetic pre-training corpus (3.6T tokens)

- **Critical path:**
  1. Initialize: Load InternLM2.5 weights
  2. Pre-train (Contrastive): Train on synthetic triples to learn policy identity
  3. Fine-tune (SFT): Train on human-annotated triples to align "policy identity" with "human preference"
  4. Inference (RFT): Feed (Prompt, Ref, Candidate) to get scalar reward

- **Design tradeoffs:**
  - Reference Dependence: POLAR requires a reference trajectory at inference time, improving accuracy but increasing inference context length
  - Synthetic Bias: Pre-training relies entirely on synthetic data from existing LLMs, potentially inheriting biases before SFT correction

- **Failure signatures:**
  - Reward Hacking via Mimicry: Model might reward simple stylistic mimicry of the reference rather than semantic correctness
  - Reference Sensitivity: Poor reference trajectories can mislead the reward signal

- **First 3 experiments:**
  1. Sanity Check (Identity Discrimination): Verify pre-trained model can distinguish if two unlabelled responses came from the same model using validation loss curve
  2. Ablation on References: Run preference evaluation with "Reference-Free" vs "Reference-Included" prompts to confirm reference mechanism
  3. Scaling Law Verification: Train smaller proxy models and plot validation loss vs. compute to reproduce power law coefficients

## Open Questions the Paper Calls Out

- **Cross-prompt referencing strategies:** Can strategies that use trajectories from other prompts with similar reasoning structures reduce annotation costs without compromising accuracy? The authors plan systematic evaluation of this approach.

- **Integration with test-time scaling:** How can POLAR's pre-training paradigm be integrated with test-time scaling techniques to synergistically enhance reward model performance? The current paper focuses on pre-training and fine-tuning, leaving this interaction unexplored.

- **Correlation with RLHF effectiveness:** Does high performance on standard preference benchmarks correlate with effective reward signals for downstream RLHF? The paper observed discrepancies between preference accuracy and RLHF performance, suggesting traditional evaluation methodologies may be inadequate.

## Limitations
- Dependence on synthetic pre-training data from 184 LLMs without clear details about policy pool diversity and quality
- Reference-dependence mechanism creates failure modes where poor reference trajectories can mislead the reward signal
- Scaling law predictions assume sufficient pre-training data volume, but the relationship between model size and required data volume isn't explicitly characterized

## Confidence

- **High Confidence:** Empirical performance improvements on preference accuracy benchmarks and downstream RLHF tasks (81.0% vs 54.8%, 85.5% vs 57.9%)
- **Medium Confidence:** Theoretical mechanism linking policy discrimination to reward modeling through density ratios
- **Medium Confidence:** Power-law scaling relationships with correlation coefficients approaching 0.99

## Next Checks

1. **Policy Pool Diversity Audit:** Analyze the 184 LLMs in the pre-training pool for behavioral diversity using clustering or embedding techniques to ensure the policy identity features are sufficiently general.

2. **Reference Quality Sensitivity Test:** Systematically evaluate POLAR's performance using reference trajectories of varying quality (strong, average, weak models) on the same prompt sets to test the hypothesis that references from high-quality policies are necessary.

3. **Scaling Law Stress Test:** Train additional model sizes between reported 1.8B and 7B points to verify power-law coefficients and test whether scaling relationships hold when training on proportionally larger pre-training corpora.