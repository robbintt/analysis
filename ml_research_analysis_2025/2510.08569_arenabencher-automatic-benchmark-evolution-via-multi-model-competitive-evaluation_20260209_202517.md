---
ver: rpa2
title: 'ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation'
arxiv_id: '2510.08569'
source_url: https://arxiv.org/abs/2510.08569
tags:
- arxiv
- benchmark
- test
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARENABENCHER is a framework for automatic benchmark evolution that
  addresses data contamination in LLM evaluation by updating test cases while preserving
  task comparability. It extracts the core ability of each test case, generates candidate
  queries that preserve the original objective, and uses an LLM judge to verify correctness
  and intent.
---

# ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation

## Quick Facts
- arXiv ID: 2510.08569
- Source URL: https://arxiv.org/abs/2510.08569
- Reference count: 35
- Primary result: Framework for automatic benchmark evolution that updates test cases while preserving task comparability and exposing shared model weaknesses

## Executive Summary
ArenaBencher is a framework that automatically evolves test benchmarks to address data contamination in LLM evaluation. The system extracts the core ability of each test case, generates new candidate queries that preserve the original objective, and uses an LLM judge to verify correctness and intent. Multi-model feedback is aggregated to select candidates that consistently degrade performance across diverse models, revealing shared failure patterns. The framework iteratively refines test cases using in-context demonstrations to steer generation toward more challenging and diagnostic cases.

## Method Summary
ArenaBencher extracts a structured "core ability" from each test case using an LLM, then generates candidate queries conditioned on this ability description and in-context demonstrations. An LLM verifier checks candidate correctness and alignment with the original task. Multi-model feedback is aggregated (sampling m=⌈√K⌉ models) to select candidates causing the most consistent performance degradation. The process iterates R times, reusing top candidates as demonstrations, before selecting final updated test cases that are more difficult, discriminative, and maintain alignment and fairness across models.

## Key Results
- Generated updates are more difficult, showing lower model accuracy compared to original benchmarks
- Updates demonstrate higher separability between models, revealing shared failure patterns
- Framework maintains alignment and fairness across model families while increasing difficulty
- Successfully applied to GSM8K, CommonsenseQA, and safety datasets with measurable improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting a structured "core ability" from each test case enables the generation of new candidate queries that preserve the original task's evaluation intent.
- Mechanism: An LLM is prompted to analyze a given test case (query, answer) and produce a structured JSON object (`a_i`). This object codifies the specific capability, core concept, required operations, and difficulty aspects. This structured description then conditions a generator LLM to produce new query-answer pairs that target the same skill profile.
- Core assumption: The LLM performing the extraction can accurately and completely verbalize the implicit reasoning skill or decision criterion embedded in a test case.

### Mechanism 2
- Claim: Aggregating performance feedback from a strategically sampled subset of models (`m = ⌈√K⌉`) selects for candidate test cases that expose shared, generalizable weaknesses across a diverse model pool.
- Mechanism: For each candidate query, a loss score is computed for each model in a sampled subset. These scores are averaged. Candidates with the highest average loss (i.e., cause the most consistent performance degradation) are selected. This acts as a model-agnostic, ensemble-style filter to avoid overfitting to any single model's idiosyncratic failures.
- Core assumption: Failure modes that appear across multiple, diverse models are more likely to reflect fundamental capability gaps than failure modes unique to one model.

### Mechanism 3
- Claim: Iteratively reusing the highest-scoring candidate test cases as in-context demonstrations progressively steers the generator toward producing more challenging and diagnostic variants.
- Mechanism: After each round of candidate generation and multi-model scoring, the top-`k` candidates are formatted as demonstrations. These are prepended to the prompt for the next generation round. This creates a feedback loop where the generator learns from successful examples of "hard" test cases to produce even harder ones.
- Core assumption: The properties that make a test case challenging (as measured by multi-model loss) can be captured and imitated via few-shot learning from positive examples.

## Foundational Learning

- **Data Contamination in LLMs**
  - Why needed here: ArenaBencher's core motivation is that static benchmarks are likely memorized by models during pretraining, making scores unreliable.
  - Quick check question: Can you explain how a model achieving near-perfect accuracy on a benchmark could still fail on a semantically equivalent but rephrased version of the same test?

- **In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The iterative refinement mechanism (Mechanism 3) relies entirely on providing successful candidate test cases as in-context examples to guide the LLM generator in subsequent rounds.
  - Quick check question: How does the role of in-context demonstrations in ArenaBencher differ from their typical use in prompt engineering for task performance?

- **Ensemble Methods & Bias-Variance Tradeoff**
  - Why needed here: The multi-model feedback scoring (Mechanism 2) is an application of ensemble reasoning. Sampling multiple models and averaging their feedback reduces variance and the risk of overfitting to a single model's bias.
  - Quick check question: Why is the √K rule used for model sampling instead of using all K models? What tradeoff is being managed?

## Architecture Onboarding

- Component map: Original Test Case -> Ability Extraction -> [Candidate Generation -> Verification -> Multi-Model Scoring -> Selection -> Update Demos] (Loop R times) -> Final Selection -> Updated Benchmark

- Critical path: The multi-model scoring and selection form the critical, model-agnostic evaluation core

- Design tradeoffs:
  1. Diversity vs. Cost in Model Sampling: Choosing `m`. Small `m` is cheap but may yield noisy, model-specific scores. Large `m` is robust but computationally expensive.
  2. Generation Freedom vs. Control: The generator must be free to create novel cases but constrained enough to preserve alignment. This is managed by conditioning on the structured ability `a_i` and in-context demos.
  3. Iteration Depth vs. Risk of Drift: More iterations (`R`) can increase difficulty but also risk semantic drift from the original intent, even with alignment checks.

- Failure signatures:
  1. Alignment Drift: The updated test case requires a different or more complex skill not present in the original. Detected by a drop in the alignment score.
  2. Unsolvable Queries: The generated question is malformed or missing critical information. The verifier should catch this, but some may slip through.
  3. Unfair Difficulty Distribution: The updated benchmark disproportionately degrades performance for a specific model family, indicating model-specific bias.

- First 3 experiments:
  1. Ablation on Multi-Model Feedback: Run with `m=1` vs. `m=√K` and compare resulting benchmarks on four quality desiderata.
  2. Sensitivity to Ability Extraction: Manually inspect and perturb extracted ability JSON and measure resulting alignment scores.
  3. Iterative Refinement Trajectory: Run for `R=1, 2, 3` iterations and plot trajectory of average loss of selected candidates.

## Open Questions the Paper Calls Out

- **Can the multi-model competitive evaluation framework be effectively extended to multimodal domains?**
  - Basis in paper: The conclusion states, "Future work will broaden the scope to multimodal settings..."
  - Why unresolved: Current implementation and experiments are restricted to text-based domains.
  - What evidence would resolve it: Successful application to vision-language benchmarks showing increased difficulty and preserved alignment.

- **Do structure-aware constraints effectively prevent the generation of underspecified or unsolvable test cases?**
  - Basis in paper: The conclusion proposes strengthening validity checks using "structure-aware constraints" to mitigate generation failures.
  - Why unresolved: Case study shows generator can omit necessary constraints or alter reasoning operations, rendering questions unsolvable.
  - What evidence would resolve it: Comparative analysis showing reduction in "invalid" questions when structure-aware constraints are applied.

- **Does an ensemble of calibrated judges improve the detection of misaligned test cases compared to a single LLM judge?**
  - Basis in paper: Authors list "ensembles of calibrated judges" as a direction for future work to strengthen validity checks.
  - Why unresolved: Current system relies on a single verifier (GPT-4o), which may inherit specific biases or fail to catch subtle logical errors.
  - What evidence would resolve it: Experiments demonstrating that ensemble approach yields higher correlation with human expert judgments.

## Limitations

- Ability Extraction Reliability: Framework's reliance on LLM to extract core ability introduces critical dependency - if extraction is inaccurate, generation will drift from original task intent
- Model Sampling Heuristic: √K rule for model sampling lacks task-specific justification and comparative analysis of different sampling strategies
- Iterative Refinement Saturation: No explicit mechanism to prevent semantic drift or overfitting to in-context demonstrations during iterations

## Confidence

- **High Confidence**: Framework's ability to generate more difficult and discriminative test cases supported by direct experimental comparisons
- **Medium Confidence**: Claims about maintaining fairness and alignment supported by reported metrics but could be further validated
- **Low Confidence**: Specific mechanism of √K sampling rule optimizing diversity-cost tradeoff not empirically validated

## Next Checks

1. **Ability Extraction Ablation**: Manually perturb extracted ability descriptions for sample test cases and measure resulting alignment and difficulty scores
2. **Model Sampling Sensitivity**: Repeat update process with different sampling strategies (m=1, m=K, stratified) and compare resulting benchmarks
3. **Iterative Saturation Analysis**: Run framework for larger number of iterations (R > 3) and plot trajectory of average loss and alignment scores to identify diminishing returns or semantic drift