---
ver: rpa2
title: 'Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach'
arxiv_id: '2506.20197'
source_url: https://arxiv.org/abs/2506.20197
tags:
- samples
- distribution
- algorithm
- anubis
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a zero-shot attribution framework, Anubis,
  for large language models using distribution testing techniques. The core idea is
  to treat LLMs as distributions over token sequences and use sampling and density
  estimates to attribute code samples without requiring training data.
---

# Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach

## Quick Facts
- **arXiv ID**: 2506.20197
- **Source URL**: https://arxiv.org/abs/2506.20197
- **Reference count**: 40
- **Primary result**: Anubis achieves AUROC ≥ 0.9 in code attribution using only ~2000 samples

## Executive Summary
This paper introduces Anubis, a zero-shot attribution framework for large language models that leverages distribution testing techniques. The approach treats LLMs as distributions over token sequences and uses sampling and density estimates to attribute code samples without requiring training data. By framing attribution as a hypothesis testing problem, Anubis can distinguish between different code models (DeepSeek-Coder, CodeGemma, Stable-Code) with high accuracy using only approximately 2000 samples. The framework addresses the challenges of high dimensionality and post-processing through oracle-enhanced distribution testing, outperforming existing methods like detectGPT in code attribution tasks.

## Method Summary
Anubis proposes a novel framework that reframes zero-shot attribution as a distribution testing problem. The method samples code from candidate models, computes likelihoods of these samples under each model, and uses statistical tests to determine attribution. The framework leverages the fact that code distributions are sparse and constrained, making them amenable to distribution testing approaches. By using oracle-enhanced distribution testing, Anubis can effectively attribute code samples without requiring labeled training data, addressing a key limitation of traditional fine-tuning approaches. The method demonstrates that even with limited samples (~2000), it can achieve reliable attribution across different code generation models.

## Key Results
- Achieves AUROC scores ≥ 0.9 in distinguishing between code generation models
- Requires only ~2000 samples for reliable attribution, compared to traditional fine-tuning approaches
- Outperforms detectGPT in code attribution tasks where token distributions are sparse and constrained
- Demonstrates effectiveness with as few as 1000 samples in some scenarios

## Why This Works (Mechanism)
The framework works by treating LLMs as distributions over token sequences and applying distribution testing techniques. Since code has a more constrained and sparse token distribution compared to natural language, statistical differences between models become more detectable. The oracle-enhanced distribution testing allows the framework to effectively distinguish between models by leveraging the likelihood of samples under different candidate models. This approach avoids the need for training data while still providing reliable attribution through statistical hypothesis testing.

## Foundational Learning

**Distribution Testing**: The statistical framework for determining whether two distributions are identical or significantly different. Needed because attribution requires determining if a sample came from one model versus another. Quick check: Understand the difference between uniformity testing and identity testing.

**Hypothesis Testing**: Statistical method for making decisions using data. Needed because attribution is framed as a hypothesis testing problem (H0: sample from model A vs H1: sample from model B). Quick check: Understand Type I and Type II error rates and their tradeoff.

**Likelihood Estimation**: Computing the probability of data under a model. Needed because Anubis computes likelihoods of samples under candidate models to determine attribution. Quick check: Be able to explain how log-likelihood differs from likelihood in numerical computation.

**Oracle Enhancement**: Using an external decision-maker to improve testing accuracy. Needed because the oracle helps distinguish samples from the true model more effectively. Quick check: Understand the limitations when oracles are imperfect or unavailable.

**Sparse Distributions**: Distributions where most possible outcomes have zero or near-zero probability. Needed because code distributions are sparse, making them more amenable to distribution testing. Quick check: Recognize why sparse distributions are easier to test than dense ones.

## Architecture Onboarding

**Component Map**: Data Sampler -> Likelihood Calculator -> Statistical Test Engine -> Attribution Decision
**Critical Path**: Sample generation → Likelihood computation → Statistical aggregation → Attribution decision
**Design Tradeoffs**: Sample efficiency vs. accuracy (more samples improve accuracy but increase cost), oracle dependency vs. standalone performance, computational overhead vs. attribution reliability
**Failure Signatures**: Poor oracle performance leads to attribution errors; insufficient samples cause statistical uncertainty; highly similar models produce ambiguous results; post-processing that changes token distributions reduces accuracy
**3 First Experiments**: 1) Verify distribution testing works on synthetic sparse distributions 2) Test attribution accuracy with increasing sample sizes 3) Compare performance against detectGPT on controlled code generation tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on oracle accuracy, which may not be practically achievable in real-world scenarios
- 2000-sample requirement, while modest, still represents computational cost for real-time attribution
- Framework's effectiveness on open-domain text generation tasks remains unclear, as evaluation focuses exclusively on code attribution

## Confidence

**High**: The framework's core theoretical foundation using distribution testing is sound, and empirical results showing AUROC ≥ 0.9 are convincing for tested code attribution scenarios.

**Medium**: The claimed superiority over detectGPT needs further validation across different model architectures and domains beyond code, as comparison is limited to specific models.

**Low**: Practical scalability to extremely large language models (GPT-4 class) and robustness to adversarial modifications of code samples are not thoroughly evaluated.

## Next Checks

1. Test Anubis's performance on attribution tasks involving mixtures of code and natural language text to assess generalization beyond purely code-based scenarios.

2. Evaluate the framework's robustness when the oracle itself is imperfect or when samples undergo various transformations (comment stripping, variable renaming, etc.).

3. Conduct a scalability study measuring attribution accuracy and computational requirements as model size increases from 7B to 70B+ parameters.