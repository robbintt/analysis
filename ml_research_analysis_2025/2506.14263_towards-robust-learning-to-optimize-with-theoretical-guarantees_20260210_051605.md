---
ver: rpa2
title: Towards Robust Learning to Optimize with Theoretical Guarantees
arxiv_id: '2506.14263'
source_url: https://arxiv.org/abs/2506.14263
tags:
- diag
- gdiag
- convergence
- following
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of theoretical guarantees for Learning
  to Optimize (L2O) methods under out-of-distribution (OOD) scenarios. It introduces
  a framework linking OOD and in-distribution (InD) performance through virtual features
  and trajectories.
---

# Towards Robust Learning to Optimize with Theoretical Guarantees

## Quick Facts
- arXiv ID: 2506.14263
- Source URL: https://arxiv.org/abs/2506.14263
- Authors: Qingyu Song; Wei Lin; Juncheng Wang; Hong Xu
- Reference count: 40
- Key outcome: This paper addresses the lack of theoretical guarantees for Learning to Optimize (L2O) methods under out-of-distribution (OOD) scenarios. It introduces a framework linking OOD and in-distribution (InD) performance through virtual features and trajectories. Theoretical analysis shows that L2O models' convergence deteriorates with increased input feature magnitudes in OOD settings. Based on these insights, the authors propose GO-Math-L2O, which uses gradient-only features and gradient-based history modeling. Experiments demonstrate that GO-Math-L2O achieves up to 10× faster convergence than state-of-the-art methods in OOD scenarios while maintaining strong InD performance. The approach provides both theoretical guarantees and practical improvements for robust optimization.

## Executive Summary
This paper tackles the critical challenge of Out-of-Distribution (OOD) generalization in Learning to Optimize (L2O) methods. Current L2O approaches lack theoretical guarantees for performance when the optimization problem deviates from training distributions. The authors introduce a novel theoretical framework that characterizes the relationship between OOD and InD performance through "virtual features" and "virtual trajectories." They prove that convergence deterioration in OOD settings is directly related to the magnitude of input feature shifts. Based on this analysis, they propose GO-Math-L2O, a gradient-only L2O architecture that eliminates variable-based features and uses gradient-based history modeling. Experiments show GO-Math-L2O achieves up to 10× faster convergence in OOD scenarios while maintaining strong InD performance.

## Method Summary
The paper introduces GO-Math-L2O, a Learning to Optimize method designed for robust OOD generalization. The method takes as input the gradient $\nabla f(x)$ and subgradient bounds $\partial r(x)_{lb}, \partial r(x)_{ub}$ (normalized), processes them through a 2-layer LSTM to generate diagonal matrices $R_k, Q_k, B_k$, and uses these parameters in a proximal operator to compute the next iterate $x_k$. The key innovation is the "gradient-only" approach that excludes variable $x$ from inputs, reducing virtual feature magnitude in OOD settings. Training uses 32,000 synthetic instances for LASSO and Logistic Regression problems, with Adam optimizer (LR=0.01, batch=128, 3 epochs) and truncated backpropagation (20 steps). The method is evaluated on OOD scenarios including initial point shifts and objective function shifts.

## Key Results
- GO-Math-L2O achieves up to 10× faster convergence than state-of-the-art methods in OOD scenarios
- The method maintains strong InD performance while improving OOD robustness
- Theoretical analysis shows convergence deterioration in OOD is upper-bounded by input feature magnitude differences
- Gradient-only features significantly reduce virtual feature magnitude, tightening OOD convergence bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The degradation of convergence in Out-of-Distribution (OOD) scenarios is upper-bounded by the magnitude of the difference between OOD and In-Distribution (InD) input features.
- **Mechanism:** The authors model the OOD trajectory as a deviation from the InD trajectory using "virtual features" ($s'$) and a "virtual Jacobian" ($J_d$). Theoretical derivation (Corollary 2 and 3) reveals that the per-iteration convergence improvement in OOD is penalized by a term proportional to $\|s'\|^2$.
- **Core assumption:** The L2O model is a differentiable function with bounded Jacobian (L-smoothness of the learning model).
- **Evidence anchors:**
  - [Section 4.1, Corollary 2]: "The decline in the convergence improvement of OOD is determined by the magnitude of the input (virtual) feature $s'$..."
  - [Section 3.1]: Derives $d(z') = d(z) + J_d s'$.
  - [Corpus]: Related works (e.g., "Learning Provably Improves the Convergence of Gradient Descent") acknowledge the gap in rigorous theoretical backing for L2O training, which this mechanism attempts to bridge.
- **Break condition:** If the underlying function to optimize is not convex or the L2O model's Jacobian is unbounded, the $\|s'\|$ penalty may no longer predict convergence failure.

### Mechanism 2
- **Claim:** Eliminating variable-based features from the L2O input vector reduces the "virtual feature" magnitude, thereby tightening the OOD convergence bound.
- **Mechanism:** Standard L2O inputs include $[x, \nabla f(x)]$. In OOD shifts, the variable $x$ can deviate significantly. By defining the input $z$ solely as gradients $[\nabla f(x), \partial r(x)]$, the term $\|s\|$ (variable shift) is removed from the virtual feature norm $\|s'\|$, directly reducing the theoretical upper bound for error accumulation.
- **Core assumption:** The optimal solution is characterized by gradients (approaching zero), making variable values secondary for determining update steps.
- **Evidence anchors:**
  - [Section 5]: "This gradient-only approach enables a more concise virtual feature in OOD settings."
  - [Section 4.1, Eq 13]: Shows $\|s'\|^2 = \|s\|^2 + \|\dots\|^2$, implying removing $\|s\|$ lowers the bound.
  - [Corpus]: Weak direct evidence; neighbors focus on data augmentation or feasibility, not feature ablation.
- **Break condition:** If the optimization landscape requires absolute position information (not just local gradient shape) to determine the step size or direction correctly.

### Mechanism 3
- **Claim:** Gradient-based history modeling provides a more robust status indicator than variable-based history for OOD generalization.
- **Mechanism:** The model uses a recurrent structure (LSTM) where history is modeled using gradients ($G_k$) rather than variable sequences. This aligns with the "gradient-only" philosophy, ensuring that the historical momentum does not amplify variable-based distribution shifts.
- **Core assumption:** Historical gradient norms and directions sufficiently capture the curvature and momentum required for convergence.
- **Evidence anchors:**
  - [Section 5, Eq 9]: Defines history modeling $v_k$ using gradient and subgradient sequences.
  - [Abstract]: Mentions "novel gradient-based history modeling method."
  - [Corpus]: N/A (Specific implementation detail).
- **Break condition:** If the optimization problem has pathological curvature where gradient history is misleading (e.g., oscillating gradients), this mechanism may fail.

## Foundational Learning

- **Concept: L-Smoothness and Strong Convexity**
  - **Why needed here:** The theoretical guarantees rely on defining the function space $F_L(\mathbb{R}^n)$ where gradients are Lipschitz continuous. This allows the derivation of quadratic upper bounds used in the proofs.
  - **Quick check question:** Can you explain why knowing the Lipschitz constant $L$ allows us to set a safe learning rate (e.g., $1/L$) in gradient descent?

- **Concept: Proximal Gradient Method**
  - **Why needed here:** The paper deals with composite objectives $F(x) = f(x) + r(x)$ where $r(x)$ is non-smooth (e.g., L1 norm). The architecture uses a proximal operator to solve for $x_{k+1}$.
  - **Quick check question:** How does the proximal operator handle non-smooth regularization differently than a standard gradient step?

- **Concept: Mean Value Theorem (Vector Form)**
  - **Why needed here:** This is the mathematical tool used to relate the L2O model's behavior in OOD ($d(z')$) to its behavior in InD ($d(z)$) via the Jacobian.
  - **Quick check question:** In the context of this paper, what does the "virtual Jacobian" $J_d$ physically represent regarding the model's sensitivity to distribution shifts?

## Architecture Onboarding

- **Component map:**
  1. Calculate Gradient $\to$ Normalize & Concatenate $\to$ LSTM $\to$ Generate $R, Q, B$ $\to$ Compute Gradient Map $G$ $\to$ Solve Proximal Operator $\to$ Update $x$

- **Critical path:**
  `Calculate Gradient` $\to$ `Normalize & Concatenate` $\to$ `LSTM` $\to$ `Generate $R, Q, B$` $\to$ `Compute Gradient Map $G$` $\to$ `Solve Proximal Operator` $\to$ `Update $x$`.

- **Design tradeoffs:**
  - **Robustness vs. Absolute Info:** Removing $x$ from inputs improves OOD robustness (Mechanism 2) but discards absolute position data. This assumes optimality is translation-invariant regarding the variable space.
  - **Complexity:** The "Inverse Gradient Map" requires computing $x_k$ via an `arg min` (proximal operator) rather than a direct neural network output, adding computational cost per iteration compared to black-box L2O.

- **Failure signatures:**
  - **Exploding Gradients in LSTM:** If inputs aren't normalized, the theoretical bounds suggest OOD shifts will cause divergence.
  - **Non-convergence in OOD:** If the shift $s'$ is so large that the $LC^2$ terms in the upper bound dominate the gradient gain term.

- **First 3 experiments:**
  1. **Feature Ablation (InD vs. OOD):** Train two models—one with $[x, \nabla f]$ and one with $[\nabla f]$. Test on a shifted dataset (e.g., translate the objective function) to verify that the gradient-only model maintains convergence speed while the variable-based model degrades.
  2. **Bound Verification:** Plot the actual convergence gap $F'(x_k) - F'(x^*)$ against the theoretical upper bound derived in Corollary 3 (scaled by $\|s'\|$) to see if the penalty term accurately predicts performance drops.
  3. **Subgradient Recovery Test:** For the non-smooth case (e.g., LASSO), inspect the recovered subgradients $g_k$. Verify they satisfy the inclusion $g_k \in \partial r(x_k)$ to ensure the "implicit subgradient" mechanism is mathematically consistent.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework assumes strongly convex objectives and L-smoothness, which may not hold for all practical optimization problems
- The "virtual feature" framework relies on differentiability assumptions for the L2O model itself
- Empirical validation focuses on specific problem classes (LASSO, Logistic Regression) and synthetic data
- Practical applicability beyond convex problems is not validated

## Confidence

**Confidence Labels:**
- Theoretical framework and bounds: **High** - The mathematical derivation is rigorous and clearly presented
- Gradient-only feature effectiveness: **Medium** - Strong theoretical backing but limited empirical validation across diverse scenarios
- 10× speedup claim: **Medium** - Based on specific experimental conditions; magnitude depends heavily on the OOD shift configuration
- Practical applicability beyond convex problems: **Low** - Not validated; theoretical assumptions explicitly require convexity

## Next Checks

1. **Stress Test OOD Bounds**: Systematically vary the magnitude of OOD shifts (both initial point and objective function shifts) and verify that convergence degradation follows the predicted $||s'||^2$ scaling relationship
2. **Non-Convex Extension**: Test GO-Math-L2O on a non-convex problem (e.g., neural network training) to assess practical limitations of the theoretical assumptions
3. **Real-World Data Evaluation**: Apply the method to a real optimization problem with naturally occurring distribution shifts (e.g., federated learning across heterogeneous devices) to validate synthetic data conclusions