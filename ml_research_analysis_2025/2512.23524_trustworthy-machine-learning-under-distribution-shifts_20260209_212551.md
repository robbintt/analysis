---
ver: rpa2
title: Trustworthy Machine Learning under Distribution Shifts
arxiv_id: '2512.23524'
source_url: https://arxiv.org/abs/2512.23524
tags:
- data
- learning
- which
- generalization
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis presents a systematic study of Trustworthy Machine
  Learning under Distribution Shifts, focusing on both capability and responsibility
  in various practical applications. It addresses perturbation shifts, domain shifts,
  and modality shifts, proposing novel frameworks such as HOOD, SharpDRO, EVIL, MVT,
  and COX to enhance robustness, explainability, and adaptability of AI models.
---

# Trustworthy Machine Learning under Distribution Shifts

## Quick Facts
- **arXiv ID:** 2512.23524
- **Source URL:** https://arxiv.org/abs/2512.23524
- **Authors:** Zhuo Huang
- **Reference count:** 0
- **Primary result:** Proposes five novel frameworks (HOOD, SharpDRO, EVIL, MVT, COX) addressing capability and responsibility in ML under distribution shifts

## Executive Summary
This thesis presents a systematic study of Trustworthy Machine Learning under Distribution Shifts, focusing on both capability and responsibility in various practical applications. It addresses perturbation shifts, domain shifts, and modality shifts, proposing novel frameworks such as HOOD, SharpDRO, EVIL, MVT, and COX to enhance robustness, explainability, and adaptability of AI models. The research demonstrates significant performance improvements across diverse benchmarks, validates theoretical analyses, and provides insights for future general-purpose AI development with alignment to human values.

## Method Summary
The thesis presents five distinct frameworks for Trustworthy ML under distribution shifts. HOOD uses causal disentanglement to separate content and style for OOD detection. SharpDRO combines Distributionally Robust Optimization with Sharpness-Aware Minimization for photon-limited corruptions. EVIL employs sparse training for invariant learning across domains. MVT uses Multimodal LLMs to rectify vision model predictions via denoising in-context learning. COX enables out-of-modal generalization through information-theoretic methods connecting in-modal and out-of-modal data. These frameworks address various shift types including perturbation, domain, and modality shifts.

## Key Results
- HOOD achieves significant improvements in open-set domain adaptation through content-style disentanglement
- SharpDRO demonstrates superior robustness to photon-limited corruptions compared to standard DRO methods
- MVT successfully uses MLLMs to correct OOD vision model errors, improving generalization
- EVIL's sparse training reduces overfitting to spurious correlations while maintaining accuracy
- COX enables knowledge transfer across modalities, enhancing out-of-modal generalization performance

## Why This Works (Mechanism)

### Mechanism 1: Causal Disentanglement for Style/Content Separation (HOOD)
- Claim: Separating "content" (class-specific features) from "style" (domain-specific perturbations) via variational inference allows models to distinguish benign OOD data (same content, new style) from malign OOD data (new content).
- Mechanism: A Structural Causal Model (SCM) is constructed where content $C$ causes label $Y$ and style $S$ causes domain $D$. The model maximizes an approximated Evidence Lower Bound (ELBO) to break spurious correlations by regularizing the encoders.
- Core assumption: The data generating process is factorizable into invariant content and variant style components, and benign shifts arise solely from style changes.
- Break condition: If the semantic class is inherently defined by the "style" (e.g., texture is the class), the disentanglement assumption fails and style-augmentation will distort class identity.

### Mechanism 2: Worst-Case Sharpness Minimization (SharpDRO)
- Claim: Generalization under severe corruption improves by explicitly minimizing the loss landscape sharpness of the worst-performing distribution (severity), rather than just minimizing the worst-case loss.
- Mechanism: Combines Distributionally Robust Optimization (DRO) with Sharpness-Aware Minimization (SAM). Identifies the distribution with highest loss and applies SAM to find a flat minimum for that specific distribution.
- Core assumption: Corruption severity follows a distribution (e.g., Poisson for photon-limited noise), and over-parameterized models converge to sharp minima on scarce worst-case data.
- Break condition: If corruption is "adversarial" rather than "natural" (does not follow smooth severity distribution), flat minimum search may not correspond to true error geometry.

### Mechanism 3: Denoising In-Context Learning via MLLMs (MVT)
- Claim: Vision models under distribution shift can be "therapized" (corrected) by treating OOD errors as label noise and using Multimodal LLMs to rectify predictions via "positive/negative" exemplar prompt.
- Mechanism: Estimates transition matrix to find likely error classes, retrieves positive (correct class) and negative (confusing class) exemplars, forms in-context prompt for MLLM to judge query image.
- Core assumption: MLLMs possess superior zero-shot generalization to semantic similarity; distribution shifts manifest as instance-dependent label noise.
- Break condition: If MLLM lacks alignment with specific visual domain, "therapy" may hallucinate corrections or reinforce biases.

## Foundational Learning

- **Concept: Structural Causal Models (SCM)**
  - Why needed here: Essential for Mechanism 1 (HOOD). Must understand how to model data generation as graph of latent variables to justify separating content from style.
  - Quick check question: If $Style \rightarrow Class$ is a spurious correlation, how would intervening on $Style$ affect a classifier that hasn't disentangled these factors?

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: Essential for Mechanism 2 (SharpDRO). Need to grasp that generalization correlates with flatness of loss basin, not just depth of minimum.
  - Quick check question: Why does perturbing weights $\theta + \epsilon$ and minimizing resulting loss encourage a "flat" minimum?

- **Concept: Multimodal In-Context Learning (ICL)**
  - Why needed here: Essential for Mechanism 3 (MVT). Need to understand how to format visual inputs as exemplars within language prompt to trigger reasoning in LLMs.
  - Quick check question: How does providing (Positive Example, Negative Example, Query) triplet differ from standard zero-shot prompting in terms of semantic alignment?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Raw Image/Data + optional Modality Tags
  - HOOD Encoder: Dual-branch VAE (Content Encoder $g_c$, Style Encoder $g_s$)
  - SharpDRO Optimizer: Wrapper around optimizer computing gradient ascent steps ($\theta + \rho \cdot \text{sign}(\nabla L)$) before descent step
  - EVIL Mask: Binary mask generator dynamically pruning weights based on gradient variance across domains
  - MVT Interface: Retrieval module (support set) + MLLM prompt constructor + small vision fine-tuning loop

- **Critical path:**
  1. Standard Pipeline: Input $\to$ HOOD Encoders $\to$ Latent $C, S$ $\to$ Classifier
  2. Robustness Injection: During training, wrap optimizer with SharpDRO to perturb weights; dynamically update EVIL mask to prune variant parameters
  3. Adaptation: If OOD error detected, trigger MVT $\to$ Retrieve Exemplars $\to$ Query MLLM $\to$ Pseudo-label $\to$ Fine-tune Vision Model

- **Design tradeoffs:**
  - Compute vs. Robustness: SharpDRO requires 2 backward passes per step (expensive) but reduces error on severe corruptions
  - Plasticity vs. Stability: EVIL reduces overfitting to spurious correlations but may drop accuracy if initial mask suboptimal
  - Automation vs. Supervision: MVT reduces human labeling needs but relies on "black box" reasoning of MLLMs

- **Failure signatures:**
  - HOOD: "Style leakage" where Style encoder perfectly predicts Class, indicating disentanglement failed
  - SharpDRO: "Gradient explosion" or NaN loss if perturbation radius $\rho$ set too high relative to learning rate
  - MVT: "Semantic Drift" where MLLM corrections systematically bias vision model toward common classes in support set

- **First 3 experiments:**
  1. Sanity Check (HOOD): Train on CIFAR-10 with synthetic style shifts (e.g., color jitter). Verify OOD score (Eq 3.11) is high for malign (content-shifted) data and low for benign (style-shifted) data.
  2. Robustness Stress Test (SharpDRO): Train on ImageNet-30 with Poisson-distributed noise severities. Plot accuracy vs. corruption severity.
  3. Modality Therapy (MVT): Fine-tune CLIP model on PACS dataset (DomainBed) using MVT. Compare accuracy against "Vanilla FT" baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Out-of-Modal (OOM) generalization methods like COX effectively transfer knowledge when target modality data originates from different dataset with large domain gap?
- Basis in paper: Chapter 6.5 states, "our OOM generalization is mostly conducted within the modalities from the same dataset. In the future, we hope to discover scenarios where the OOM data is from a different dataset with a large modality gap."
- Why unresolved: COX validated on multi-modal datasets where modalities share source, but not tested on cross-distribution tasks where relationship between source and target modalities is more abstract or unrelated.
- Evidence: Applying COX framework to setup where In-Modal data comes from large-scale dataset (e.g., Conceptual Captions) and OOM data comes from specialized, disjoint dataset (e.g., medical imaging tactile data) without instance-level alignment.

### Open Question 2
- Question: How can perturbation magnitude ($\rho$) in SharpDRO be dynamically optimized to balance robustness against severe corruption without degrading performance on cleaner distributions?
- Basis in paper: Chapter 3, Section 3.5.1 notes that "aggressive setting a large perturbation scale... enhance[s] the generalization of severely corrupted data but degrade[s] the performance of slightly corrupted data."
- Why unresolved: Current method relies on fixed $\rho$ parameter, creating static trade-off between robust generalization and clean accuracy; adaptive mechanism not defined.
- Evidence: Developing algorithm that dynamically adjusts $\rho$ based on real-time estimates of corruption severity (e.g., photon-limited noise levels) and validating that this adaptive approach outperforms fixed-parameter SharpDRO across all severity levels on ImageNet-C.

### Open Question 3
- Question: Can computational overhead of HOOD framework's causal disentanglement and adversarial training be reduced to facilitate practical deployment in large-scale settings?
- Basis in paper: Chapter 2, Table 2.6 and Section 2.4.5 acknowledge method's efficiency limitations, noting that "the training time is increased" compared to baselines due to complexity of variational inference framework.
- Why unresolved: While effective for robustness, framework requires significantly longer execution times (e.g., 11.4h vs 7.8h for baselines), limiting applicability in resource-constrained environments.
- Evidence: Implementing lightweight approximation of content-style disentanglement module or sparse gradient updates, and showing comparable AUROC scores on Open-Set DA task with training time reduction of >30%.

## Limitations
- HOOD's causal disentanglement assumption may fail when semantic classes are inherently defined by "style" features (e.g., texture-based classification)
- SharpDRO's effectiveness depends on corruption following smooth severity distributions, potentially limiting performance against adversarial perturbations
- MVT framework's reliance on MLLM alignment introduces brittleness when MLLMs lack domain-specific training data
- EVIL's sparse training may drop accuracy if initial weight masks are suboptimal
- COX's architecture details for connection module are underspecified, affecting reproducibility

## Confidence
- **HOOD Content/Style Disentanglement:** Medium confidence - strong theoretical basis but dependent on factorizability assumption that may not hold in all domains
- **SharpDRO Robust Optimization:** High confidence - well-motivated theoretical framework with clear empirical validation on photon-limited corruptions
- **EVIL Sparse Invariant Learning:** Medium confidence - innovative approach with promising results, but regularization schedules lack precise specification
- **MVT MLLM Therapy:** Medium confidence - demonstrates effectiveness but vulnerable to MLLM hallucination and domain misalignment
- **COX Out-of-Modal Generalization:** Low confidence - method described but critical architectural details missing

## Next Checks
1. **HOOD Factorizability Test:** Apply HOOD to texture-based classification task (e.g., DTD dataset) where "style" features directly define classes. Measure whether disentanglement assumption breaks down and style augmentation distorts class identity.

2. **SharpDRO Adversarial Transferability:** Evaluate SharpDRO on adversarially perturbed ImageNet data rather than natural corruptions. Compare whether worst-case flatness principle still correlates with robustness against gradient-based attacks.

3. **MVT Domain Generalization:** Test MVT's "therapy" capability on medical imaging dataset (e.g., CheXpert) where MLLMs lack pre-training. Measure whether framework can still correct systematic OOD errors without domain-aligned MLLM supervision.