---
ver: rpa2
title: Adversarial Evasion Attacks on Computer Vision using SHAP Values
arxiv_id: '2601.10587'
source_url: https://arxiv.org/abs/2601.10587
tags:
- shap
- attacks
- values
- arxiv
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a white-box adversarial attack on computer vision
  models using SHAP values to induce misclassifications while remaining imperceptible
  to human observers. The proposed method leverages SHAP values to identify and neutralize
  the most influential pixels in an image, achieving more stable and effective misclassifications
  compared to traditional FGSM attacks.
---

# Adversarial Evasion Attacks on Computer Vision using SHAP Values

## Quick Facts
- arXiv ID: 2601.10587
- Source URL: https://arxiv.org/abs/2601.10587
- Authors: Frank Mollard; Marcus Becker; Florian Roehrbein
- Reference count: 7
- Primary result: SHAP-based adversarial attacks achieve 73-98% misclassification rates while remaining visually imperceptible

## Executive Summary
This paper introduces a novel white-box adversarial attack methodology that leverages SHAP (SHapley Additive exPlanations) values to induce misclassifications in computer vision models while maintaining visual imperceptibility. The approach identifies and neutralizes the most influential pixels in an image by targeting those with the highest SHAP attributions, effectively reducing the model's confidence in correct classifications. Experiments across multiple datasets (Animal Faces, Cats and Dogs, MNIST, Woman and Man Faces) and various model architectures demonstrate that SHAP attacks achieve more consistent and stable misclassifications compared to traditional FGSM attacks, which suffer from gradient masking issues. The method successfully balances attack effectiveness with stealth, though at the cost of significant computational overhead.

## Method Summary
The methodology centers on using SHAP values to identify the most influential pixels for classification in an image. Unlike traditional gradient-based attacks that perturb pixels based on model gradients, this approach computes SHAP values to quantify each pixel's contribution to the model's prediction. The attack then iteratively reduces the values of the most influential pixels (either by direct reduction or noise injection) until the model's confidence drops below a threshold, causing misclassification. This process exploits the model's reliance on specific high-influence pixels, making it more stable than gradient-based methods that can be thwarted by gradient masking. The implementation requires computing SHAP values for every pixel, which is computationally intensive but yields more consistent attack success across different model architectures and datasets.

## Key Results
- SHAP attacks achieved misclassification rates between 73% and 98% across all tested datasets and models
- FGSM attacks showed inconsistent results, failing in some cases due to gradient masking
- SHAP attacks maintained visual imperceptibility while effectively reducing classification confidence
- The method demonstrated robustness across diverse model architectures and image types
- Computational requirements were significantly higher than traditional attacks due to SHAP value calculations

## Why This Works (Mechanism)
The attack exploits a fundamental vulnerability in current computer vision models: their tendency to rely heavily on specific high-influence pixels for classification decisions. By using SHAP values to quantify each pixel's contribution to the prediction, the attack can precisely target the most critical features that the model depends upon. This targeted neutralization disrupts the model's reasoning process more effectively than random perturbations or gradient-based methods, which may be less precise or fail entirely due to gradient masking. The SHAP-based approach provides a more stable and reliable way to find and exploit these vulnerabilities, as it doesn't depend on potentially unreliable gradient signals.

## Foundational Learning
- **SHAP Values**: Game-theoretic approach to explain individual predictions by quantifying feature contributions; needed to identify most influential pixels for targeted attacks
- **Adversarial Evasion Attacks**: Techniques to fool machine learning models into making incorrect predictions; needed as the core attack framework
- **Gradient Masking**: When models obscure or distort gradients to defend against attacks; needed to understand why traditional methods fail
- **Model Interpretability**: Methods to understand model decision-making; needed to bridge explainability and vulnerability discovery
- **Pixel Attribution**: Quantifying how much each pixel contributes to predictions; needed for targeted attack strategy
- **Quick Check**: Verify SHAP implementation by comparing computed values against known interpretable examples

## Architecture Onboarding

Component Map: Input Image -> SHAP Value Calculation -> Pixel Selection -> Pixel Modification -> Model Prediction -> Confidence Check

Critical Path: The attack succeeds when SHAP values correctly identify influential pixels, those pixels are effectively modified, and the model's confidence drops below threshold for misclassification.

Design Tradeoffs: Computational efficiency versus attack stability - SHAP values provide more reliable targeting but require significantly more processing time than gradient methods.

Failure Signatures: Attack fails when SHAP values don't align with true influential pixels, when pixel modifications don't sufficiently impact model confidence, or when gradient masking affects SHAP computation.

First Experiments:
1. Verify SHAP values correctly identify known influential regions in simple test images
2. Test attack success rate on a single model architecture with one dataset
3. Compare visual changes between successful and unsuccessful attack attempts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can defense mechanisms be specifically developed to mitigate the risks posed by SHAP-based evasion attacks?
- Basis in paper: The conclusion states, "Future research should focus on improving the defence mechanisms of computer vision models to mitigate the risks posed by these attacks."
- Why unresolved: The paper focuses entirely on the methodology and execution of the attack, establishing its potency, but does not propose or test specific countermeasures.
- What evidence would resolve it: A study demonstrating a specific defense strategy (e.g., unique adversarial training) that significantly reduces the success rate of SHAP attacks compared to undefended baselines.

### Open Question 2
- Question: Does training models to distribute feature importance more evenly across pixels effectively reduce susceptibility to SHAP attacks?
- Basis in paper: The authors suggest, "A development towards better generalized models would be desirable, where the influence of individual pixels is more balanced across the image and the classification relies less on individual pixels."
- Why unresolved: The paper establishes that current models rely on specific high-influence pixels which SHAP attacks exploit, but it does not verify if balancing this influence actually improves robustness.
- What evidence would resolve it: Experiments comparing attack success rates on standard models versus models regularized to enforce balanced pixel attribution.

### Open Question 3
- Question: Can the computational efficiency of SHAP attacks be improved to enable practical application on high-resolution images or video?
- Basis in paper: The paper notes that "high computing resources may also be required due to the high computational complexity," limiting practical application for high-resolution data.
- Why unresolved: While effective, the method relies on calculating SHAP values for every pixel, which is computationally expensive; the paper does not explore optimization strategies for this bottleneck.
- What evidence would resolve it: A modified algorithm or approximation method that maintains high misclassification rates while significantly reducing the processing time and resource requirements for large inputs.

## Limitations
- High computational cost due to SHAP value calculations for every pixel
- Limited evaluation of attack transferability to black-box settings
- No quantitative metrics for visual imperceptibility (relied on subjective assessment)
- Tested only on relatively small datasets and standard model architectures

## Confidence
- High: The core methodology of using SHAP values for adversarial attacks is technically sound and well-implemented, with clear experimental demonstrations across multiple datasets.
- Medium: Claims about SHAP attacks being "more stable" than FGSM are supported by the data but could benefit from additional statistical validation across more diverse model architectures and datasets.
- Medium: The assertion that SHAP values reveal model vulnerabilities is compelling but requires further investigation into whether these vulnerabilities generalize across different model families or training regimes.

## Next Checks
1. Benchmark runtime performance comparing SHAP-based attacks against traditional gradient methods across different hardware configurations
2. Evaluate attack transferability from white-box to black-box settings and test against common defense mechanisms
3. Conduct quantitative analysis of visual imperceptibility using established image quality metrics and larger-scale human studies