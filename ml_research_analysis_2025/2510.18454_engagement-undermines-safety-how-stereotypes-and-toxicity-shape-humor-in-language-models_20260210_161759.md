---
ver: rpa2
title: 'Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in
  Language Models'
arxiv_id: '2510.18454'
source_url: https://arxiv.org/abs/2510.18454
tags:
- humor
- stereotypical
- toxicity
- uncertainty
- jokes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that modern LLM humor generation systems amplify
  harmful stereotypes and toxicity when optimizing for engagement. Using six state-of-the-art
  models and diverse evaluation methods including LLM-based raters, specialized classifiers,
  and incongruity metrics, we find that role-based prompting (e.g., impersonating
  comedians) increases stereotypical content by 5 percentage points and toxic content
  by 4-5 percentage points.
---

# Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models

## Quick Facts
- arXiv ID: 2510.18454
- Source URL: https://arxiv.org/abs/2510.18454
- Reference count: 40
- Modern LLM humor systems amplify harmful stereotypes and toxicity when optimizing for engagement

## Executive Summary
This study systematically investigates how modern language models generate humor while inadvertently amplifying harmful stereotypes and toxic content. Through controlled experiments with six state-of-the-art models, the research demonstrates that role-based prompting (such as impersonating comedians) increases stereotypical content by 5 percentage points and toxic content by 4-5 percentage points. More concerning, the study reveals a bias amplification loop where humor evaluators trained on engagement data reward harmful content with higher scores, creating structural incentives for generators to produce more offensive material. The findings expose critical safety vulnerabilities in single-objective humor optimization pipelines and call for multi-objective approaches that explicitly balance humor with safety constraints.

## Method Summary
The study used 10K neutral joke setups from the r/Jokes dataset, filtered to remove stereotypical content from the setups themselves. Six LLMs (OLMo-2 7B/13B/32B, Llama-3.1-8B, Ministral-8B, Mistral-Small-24B) generated five punchlines per setup under both base and persona conditions. Generation was performed with temperature 0.6 and max_tokens 256. Three evaluation methods were employed: LLM-based ordinal classification (3-point humor/stereotype/toxicity scales), specialized classifiers (ALBERT-v2 MGS for stereotypes, HateBERT-ToxiGen for toxicity), and incongruity metrics (entropy and surprisal computed at punchline tokens). A counterfactual condition using "complete without being offensive" prompts tested safety interventions. Human evaluation on satire data validated the findings.

## Key Results
- Role-based prompting increases stereotypical content by 5 percentage points and toxic content by 4-5 percentage points
- Harmful outputs receive 10-21% higher humor scores for stereotypical jokes and 20% higher for toxic jokes
- Harmful content concentrates among the funniest joke categories, creating a bias amplification loop
- Information-theoretic analysis shows harmful cues can widen predictive uncertainty and reduce surprisal for some models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Humor evaluators trained on engagement data reward harmful content with higher scores, creating a bias amplification loop.
- **Mechanism**: Evaluators learn statistical associations from training data (e.g., Reddit upvotes) that correlate stereotype/toxicity with perceived funniness. Generators optimizing for these scores exploit these correlations, producing more harmful content which evaluators then score higher.
- **Core assumption**: Engagement signals (upvotes) reflect genuine humor preference rather than other factors (controversy, shock value).
- **Evidence anchors**:
  - [abstract] "stereotypical jokes gain 10–21% and toxic jokes gain 20% in mean humor score"
  - [section 4.2] "humor scores show a rise of up to 7% while moving up in stereotype levels"
  - [section 4.7] "bias loop in which generators exploit harmful cues that evaluators reward"
  - [corpus] Limited—neighbor papers discuss humor generation but don't directly validate the amplification loop claim
- **Break condition**: If evaluator training data lacks engagement-harmfulness correlation, or if explicit safety constraints override engagement signals during evaluation.

### Mechanism 2
- **Claim**: Harmful humor patterns are structurally embedded in learned distributions, not merely imitated through role-prompting.
- **Mechanism**: Information-theoretic analysis reveals that for some models (OLMo family), stereotypical punchlines exhibit *lower* surprisal (negative log-likelihood), meaning the model expects these completions. This indicates harmful patterns are learned during pretraining, not injected via prompting.
- **Core assumption**: Lower surprisal reflects genuine learned probability rather than artifacts of tokenizer or architecture.
- **Evidence anchors**:
  - [abstract] "harmful cues... can even make harmful punchlines more expected for some models, suggesting structural embedding"
  - [section 4.4] "surprisal reduces from 2.83→2.79→2.76 with increasing stereotypes for the OLMo family"
  - [section 4.6] Counterfactual non-offensive prompting reduced but didn't eliminate harmful content
  - [corpus] Not directly validated—corpus papers don't address information-theoretic embedding
- **Break condition**: If surprisal reductions stem from prompt artifacts or tokenization rather than distributional learning.

### Mechanism 3
- **Claim**: Role-based persona prompting primes models toward riskier, more harmful outputs by activating stylistic associations between "comedian" and boundary-pushing humor.
- **Mechanism**: Persona instructions ("Speak like Robin Williams") retrieve associated stylistic patterns from training data, which include edgier, stereotype-based comedy. This shifts the generation distribution toward higher variance and more harmful content.
- **Core assumption**: Persona associations in training data correlate with harmful humor styles; models can and do retrieve these associations.
- **Evidence anchors**:
  - [abstract] "role-based prompting increases stereotypical content by 5 percentage points and toxic content by 4-5 percentage points"
  - [table 1] Base→Persona stereotypes: 54.9%→59.11% (classifier); toxicity: 70.92%→75.78%
  - [section 4.1] "comedian personas prime models toward edgier, more biased humor"
  - [corpus] Limited indirect support—corpus discusses persona effects but not specifically comedian-harm associations
- **Break condition**: If persona associations are safety-filtered during training, or if explicit safety instructions override implicit style retrieval.

## Foundational Learning

- **Concept: Incongruity Theory in Computational Humor**
  - **Why needed here**: The paper uses entropy (uncertainty) and surprisal as proxies for incongruity—a core theory that humor arises from expectation violation. Understanding this is essential to interpret why harmful content might score as "funnier."
  - **Quick check question**: Why would higher entropy at the punchline position indicate more plausible humorous completions rather than just confused generation?

- **Concept: Distributional Bias Amplification in RLHF-like Systems**
  - **Why needed here**: The bias loop between generators and evaluators mirrors reward hacking dynamics. Without this lens, the correlation between harm and humor scores may appear coincidental rather than structural.
  - **Quick check question**: If an evaluator is trained on human engagement data that contains implicit bias, what happens when a generator is optimized against it?

- **Concept: Information-Theoretic Metrics (Entropy vs. Surprisal)**
  - **Why needed here**: The paper distinguishes uncertainty (how many plausible paths exist) from surprisal (how unexpected the actual path was). These reveal different aspects of harmful content embedding.
  - **Quick check question**: If entropy increases but surprisal decreases for stereotypical content, what does this tell you about how the model has learned to complete jokes?

## Architecture Onboarding

- **Component map**: Neutral joke setups -> (base prompt OR persona-conditioned prompt) -> LLM -> 5 punchline candidates per setup -> Humor scorer (STELLA-400M) + Stereotype classifier (ALBERT-v2) + Toxicity classifier (HateBERT-ToxiGen) -> LLM-based ordinal classifier + Incongruity metrics (entropy U, surprisal S) -> External satire validation

- **Critical path**:
  1. Sample 10K neutral joke setups (stereotypical jokes with stereotypical content filtered from setup)
  2. Generate punchlines with 6 LLMs under base and persona conditions
  3. Score all generations on humor, stereotype, toxicity
  4. Compute incongruity metrics on punchline tokens using the generating model's own probability distribution
  5. Correlate harm levels with humor scores and surprisal/uncertainty

- **Design tradeoffs**:
  - Single-task humor regressor vs. LLM-based rater: Regressor is faster but inherits Reddit bias; LLM rater is slower but can incorporate safety instructions
  - Neutral setups filtered for stereotype vs. synthetic setups: Neutral setups isolate model invention but may limit ecological validity
  - Five generations per setup vs. one: Multiple generations enable selection effects analysis but increase compute cost

- **Failure signatures**:
  1. High stereotype/toxicity rates in base condition (>50%) suggest training data contamination rather than persona effects
  2. Non-monotonic surprisal patterns (dip at severe toxicity) may indicate tokenizer artifacts or model-specific overfitting
  3. Divergence between classifier and LLM-based toxicity ratings suggests evaluation rubric ambiguity

- **First 3 experiments**:
  1. **Counterfactual safety prompt ablation**: Run the "complete without being offensive" prompt across all 6 models and quantify residual harmful content. This tests whether harm is prompt-controllable or structurally embedded.
  2. **Evaluator debiasing test**: Retrain or fine-tune the humor scorer on a filtered dataset with stereotype/toxicity penalties. Measure whether generator selection shifts toward safer content.
  3. **Cross-model surprisal analysis**: Compute surprisal of harmful punchlines using a different model than the generator. If surprisal patterns persist, this suggests shared distributional patterns across models rather than model-specific artifacts.

## Open Questions the Paper Calls Out

- **Question**: How can multi-objective optimization frameworks be designed to effectively balance humor generation with safety constraints without sacrificing engagement?
- **Basis in paper**: [explicit] The Conclusion states that single-objective pipelines provide weak guardrails and motivates "multi-objective generation and evaluation that explicitly trade off humor and safety."
- **Why unresolved**: The paper identifies the necessity for this trade-off and exposes the bias amplification loop, but does not propose, implement, or test specific multi-objective architectures or loss functions to solve it.
- **What evidence would resolve it**: Empirical results from a modified pipeline using constrained decoding or reinforcement learning that penalizes toxicity while maintaining competitive humor scores.

## Limitations

- The study relies exclusively on English-language Reddit data, limiting cross-cultural generalizability
- Incongruity metric interpretation conflates distributional learning with tokenizer/architecture effects
- The humor evaluator's bias inheritance is assumed but not empirically validated

## Confidence

- **Humor-Engagement Bias Amplification Loop**: Medium - Supported by correlation patterns but relies on untested assumptions about evaluator training data
- **Structural Embedding of Harmful Humor**: Medium-Low - Incongruity metrics show patterns but interpretation is confounded by tokenizer/architecture effects
- **Persona Priming to Harmful Content**: High - Large effect sizes (5-5 percentage points) across multiple models and evaluation methods
- **Multi-Objective Safety Necessity**: High - Consistent findings across humor, stereotype, and toxicity metrics with clear practical implications

## Next Checks

1. **Evaluator bias isolation**: Train a humor scorer on filtered Reddit data where stereotype/toxicity content is explicitly downweighted. Run the same generation-evaluation pipeline and measure whether the humor-harm correlation persists. This directly tests whether engagement optimization inherently couples with harmful content.

2. **Cross-model incongruity validation**: Compute surprisal and entropy for harmful punchlines using a different model than the generator (e.g., use Llama-3.1 to score OLMo-generated punchlines). If harmful content shows similar surprisal patterns across models, this strengthens the distributional embedding claim; if patterns differ, this suggests model-specific artifacts.

3. **Safety prompt ablation study**: Systematically vary safety prompt strength and phrasing across all 6 models and all harm types. Measure both harm reduction and humor score impact to quantify the safety-fun tradeoff curve and identify whether harm reduction comes at the cost of funniness or through better prompt engineering.