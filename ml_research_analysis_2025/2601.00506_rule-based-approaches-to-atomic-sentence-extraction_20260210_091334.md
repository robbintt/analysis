---
ver: rpa2
title: Rule-Based Approaches to Atomic Sentence Extraction
arxiv_id: '2601.00506'
source_url: https://arxiv.org/abs/2601.00506
tags:
- atomic
- sentences
- sentence
- extraction
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated which linguistic structures most challenge
  rule-based atomic sentence extraction systems. Using the WikiSplit dataset and spaCy's
  dependency parser, a rule-based system was developed to decompose complex sentences
  into single-idea units.
---

# Rule-Based Approaches to Atomic Sentence Extraction

## Quick Facts
- arXiv ID: 2601.00506
- Source URL: https://arxiv.org/abs/2601.00506
- Reference count: 6
- Primary result: Rule-based atomic sentence extraction achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898

## Executive Summary
This study investigates which linguistic structures most challenge rule-based atomic sentence extraction systems. Using the WikiSplit dataset and spaCy's dependency parser, a rule-based system was developed to decompose complex sentences into single-idea units. The system achieved moderate performance with ROUGE-1 F1 = 0.6714 and BERTScore F1 = 0.5898, indicating reasonable lexical and structural alignment but weaker phrasal and semantic preservation. Error analysis revealed that complex structures like relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions caused the majority of failures, with missing objects (44%) being the most frequent issue.

## Method Summary
The study employed a rule-based approach to atomic sentence extraction using spaCy's dependency parser on the WikiSplit dataset. The system processed 100 complex sentences manually annotated into 2-5 atomic sentences each (252 aligned pairs total). Rules detected coordinating conjunctions ("and"/"or" and "conj" tag) and subordinate clauses ("advcl", "relcl"), with subject propagation to reconstruct arguments. Performance was evaluated using ROUGE-1/2/L metrics and BERTScore with RoBERTa-large. Error analysis categorized failures into 44% missing objects, 5% coordination errors, and 4% missing subjects, with additional issues in handling nested clauses and appositions.

## Key Results
- System achieved ROUGE-1 F1 = 0.6714, indicating reasonable lexical alignment with gold standards
- ROUGE-2 F1 = 0.478 shows moderate phrasal preservation capability
- BERTScore F1 = 0.5898 demonstrates fair semantic alignment
- Error analysis revealed 44% of failures due to missing objects and 5% due to coordination errors

## Why This Works (Mechanism)
The rule-based approach works by leveraging dependency parsing to identify clause boundaries and coordinating structures, then applying deterministic rules to split sentences while preserving subject arguments. The system's effectiveness stems from spaCy's accurate dependency parsing for common structures and the propagation of subjects across split clauses. However, the mechanism struggles with complex nested structures and fails to recover complete argument structures in many cases.

## Foundational Learning
- **Dependency parsing**: Identifies grammatical relationships between words; needed to locate clause boundaries and argument structures
- **Subject propagation**: Copies subjects from main clauses to split coordinated predicates; needed to maintain grammatical completeness
- **Clause boundary detection**: Identifies relative clauses ("relcl"), adverbial clauses ("advcl"), and appositions; needed to determine split points
- **Argument recovery**: Restores missing objects and other dependents; needed to complete split predicates
- **ROUGE metrics**: Measures n-gram overlap between generated and reference sentences; needed for quantitative evaluation
- **BERTScore**: Evaluates semantic similarity using contextual embeddings; needed to assess meaning preservation

## Architecture Onboarding
- **Component map**: Input text -> spaCy pipeline (tokenization, POS tagging, dependency parsing) -> Rule-based splitter (coordinator detection, clause boundary identification, subject propagation) -> Output atomic sentences
- **Critical path**: Text processing -> Rule application -> Argument propagation -> Sentence generation
- **Design tradeoffs**: Simplicity and interpretability of rules vs. coverage of complex linguistic phenomena
- **Failure signatures**: Missing objects, coordination errors, incomplete subject propagation, incorrect clause boundary detection
- **3 first experiments**: 1) Test rule coverage on simple coordinated sentences; 2) Evaluate subject propagation on conjoined predicates; 3) Measure performance degradation with increasing clause nesting depth

## Open Questions the Paper Calls Out
1. Can integrating argument recovery components into rule-based pipelines significantly reduce the high rate of "Missing Object" errors (44%)?
2. To what extent does rule-based atomic sentence extraction improve performance in downstream tasks like question answering and fact verification?
3. Can expanding the rule set to specifically target nested clauses and coordination patterns improve phrasal preservation (ROUGE-2)?

## Limitations
- Rule-based approach struggles with complex syntactic structures and nested clauses
- Missing objects represent 44% of errors, indicating inadequate argument recovery
- Manual annotation introduces potential subjectivity in clause boundary decisions
- Limited coverage of passive constructions and appositional structures

## Confidence
- **Medium** confidence in reported performance metrics due to unknown alignment methodology
- **High** confidence in error analysis findings from systematic examination
- **Low** confidence in generalizability to other rule-based systems due to unspecified implementations

## Next Checks
1. Implement the exact rule-based system using described spaCy dependencies and subject propagation algorithm, then compare performance on the same 100-sentence sample
2. Conduct ablation studies by removing individual rule components to quantify their contribution to overall performance
3. Evaluate the system on a held-out test set of complex sentences not included in the error analysis to verify error patterns generalize beyond the analyzed sample