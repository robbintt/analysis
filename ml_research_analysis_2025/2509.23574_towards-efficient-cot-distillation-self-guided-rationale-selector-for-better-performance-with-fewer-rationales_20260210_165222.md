---
ver: rpa2
title: 'Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better
  Performance with Fewer Rationales'
arxiv_id: '2509.23574'
source_url: https://arxiv.org/abs/2509.23574
tags:
- rationale
- rationales
- distillation
- student
- morsd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MoRSD improves Chain-of-Thought (CoT) distillation by selecting\
  \ high-quality rationales based on accuracy, diversity, and difficulty, using a\
  \ Rationale Difficulty (RD) metric to measure a student model\u2019s ability to\
  \ generate correct answers under given rationales. Experiments on seven datasets\
  \ across three reasoning tasks show that MoRSD achieves an average 4.6% accuracy\
  \ improvement using fewer rationales than baselines, demonstrating that selective,\
  \ high-quality rationale sets enhance student model reasoning more effectively than\
  \ full datasets."
---

# Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales

## Quick Facts
- **arXiv ID:** 2509.23574
- **Source URL:** https://arxiv.org/abs/2509.23574
- **Reference count:** 40
- **Primary result:** MoRSD achieves 4.6% average accuracy improvement over baselines using fewer rationales by selecting high-quality data based on accuracy, diversity, and difficulty.

## Executive Summary
MoRSD introduces a self-guided rationale selection framework for Chain-of-Thought (CoT) distillation that significantly improves student model performance while using fewer training samples. The method uses the student model itself to evaluate the quality of teacher-generated rationales through a novel Rationale Difficulty (RD) metric, which measures how much a rationale reduces the student's uncertainty about the correct answer. By filtering rationales based on accuracy, diversity, and difficulty, MoRSD addresses the problem of noisy or incorrect information transfer in traditional CoT distillation. Experiments across seven datasets and three reasoning tasks demonstrate that selective, high-quality rationale sets enhance student reasoning more effectively than full datasets.

## Method Summary
MoRSD operates through a three-stage rationale selection pipeline. First, a teacher model (GPT-3) generates multiple rationales per question. Second, an offline student model calculates the Rationale Difficulty (RD) metric by comparing perplexity when predicting answers from questions alone versus questions with rationales. Third, rationales are filtered through accuracy selection (removing incorrect answers), diversity selection (removing N-gram similar duplicates), and difficulty selection (choosing rationales with lowest RD). The resulting curated dataset is then used for standard fine-tuning of the student model. The method demonstrates that student models can effectively self-evaluate the pedagogical value of teacher-generated rationales, leading to more efficient knowledge transfer.

## Key Results
- MoRSD achieves an average 4.6% accuracy improvement over baselines across seven datasets
- Uses significantly fewer rationales than traditional CoT distillation while maintaining or improving performance
- Ablation studies show each selection stage (accuracy, diversity, difficulty) contributes meaningfully to performance gains
- Particularly effective on arithmetic and mathematical reasoning tasks where rationale quality is critical
- Demonstrates that selective, high-quality training data outperforms brute-force approaches using full datasets

## Why This Works (Mechanism)

### Mechanism 1: Rationale Difficulty Selection
The RD metric ($PPL(a|r,q)/PPL(a|q)$) identifies rationales that specifically reduce student uncertainty about correct answers. Lower RD values indicate rationales that effectively guide students toward correct reasoning paths. This pedagogical matching ensures the student learns from rationales most relevant to its current capability level, rather than generic complex reasoning chains.

### Mechanism 2: Diversity Selection via N-gram Filtering
Jaccard similarity on 3-grams removes redundant rationales, forcing the student to learn broader reasoning patterns rather than memorizing specific token sequences. This prevents overfitting to teacher phrasing and promotes generalization across different problem formulations of the same underlying logic.

### Mechanism 3: Accuracy Selection Thresholding
Filtering out rationales with incorrect final answers prevents the transfer of confident but wrong reasoning patterns. This hard gate eliminates "hallucinations" from the teacher model that could mislead the student during training, ensuring only pedagogically sound examples are used.

## Foundational Learning

- **Concept: Perplexity (PPL) and Negative Log-Likelihood (NLL)**
  - Why needed here: MoRSD relies on student model PPL to evaluate rationale quality through the RD metric
  - Quick check question: If $PPL(\text{answer}|\text{rationale})$ is lower than $PPL(\text{answer}|\text{question})$, what does that mathematically imply about the rationale's utility?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed here: The paper operates on the premise that a large Teacher (GPT-3) transfers reasoning capabilities to a smaller Student (Flan-T5)
  - Quick check question: Why can't we simply fine-tune the student on the ground truth answer alone if we want to teach "reasoning"?

- **Concept: Data-Centric AI (Curriculum Learning)**
  - Why needed here: The core thesis is that better data beats more data in model training
  - Quick check question: What is the risk of training a small model on a massive, uncurated dataset containing 50% incorrect reasoning chains?

## Architecture Onboarding

- **Component map:** Teacher Generator -> Offline Student Scorer -> Selector Pipeline -> Student Trainer
- **Critical path:**
  1. Generation: Prompt Teacher for raw rationales
  2. Pre-Scoring: Run untrained Student to compute RD scores for all candidates
  3. Filtering: Apply Accuracy → Diversity → Difficulty selection stages
  4. Distillation: Fine-tune Student on selected subset

- **Design tradeoffs:** Requires sufficient student pre-training for meaningful PPL scores; trades generation+scoring compute for reduced training data size
- **Failure signatures:** High RD inversion (selecting worst rationales), empty dataset from aggressive filtering, over-pruning from excessive diversity filtering
- **First 3 experiments:**
  1. Replicate Figure 6: Train on top/bottom 10% RD rationales to verify correlation
  2. Ablate accuracy selection on SVAMP to observe performance degradation
  3. Cross-model RD transfer: Calculate with Small, train with Base/Large to test transferability

## Open Questions the Paper Calls Out

- **Question 1:** Does MoRSD generalize effectively to multilingual settings and non-mathematical domains where rationale structures differ significantly?
- **Question 2:** Can integrating Rationale Difficulty with process reward models or structural coherence metrics yield a more robust selection mechanism?
- **Question 3:** Can the efficiency of the rationale generation phase be improved to avoid generating full datasets before selection?

## Limitations
- RD-based selection depends heavily on student model pre-training quality; fails with randomly initialized students
- The RD metric's correlation with "learnability" is not rigorously validated across all reasoning types
- Fixed selection thresholds (k=3, K=6) may not be optimal across different datasets and model scales

## Confidence
- **High Confidence:** Accuracy selection improves distillation by filtering incorrect rationales (strong ablation evidence)
- **Medium Confidence:** Diversity selection improves generalization (plausible but limited direct evidence)
- **Medium Confidence:** RD-based difficulty selection improves performance (supported by Figure 6 but needs broader validation)

## Next Checks
1. RD Metric Robustness Test: Generate rationales for new dataset, train on top/bottom 10% RD to verify correlation
2. Ablate Diversity Stage: Disable 3-gram filtering on GSM8K, compare accuracy and dataset size to original
3. Cross-Model RD Transferability: Calculate RD with Small, train Base and Large models to test if "customized" data benefits different sizes