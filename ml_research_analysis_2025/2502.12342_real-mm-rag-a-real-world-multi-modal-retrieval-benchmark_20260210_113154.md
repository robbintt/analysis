---
ver: rpa2
title: 'REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark'
arxiv_id: '2502.12342'
source_url: https://arxiv.org/abs/2502.12342
tags:
- retrieval
- query
- queries
- rephrasing
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REAL-MM-RAG introduces a multi-modal retrieval benchmark designed
  to evaluate retrieval-augmented generation systems in realistic scenarios. The benchmark
  addresses limitations in existing datasets by incorporating multi-modal documents,
  enhanced difficulty with long documents and sub-domain coverage, realistic RAG queries
  without explicit page references, and accurate labeling through exhaustive verification.
---

# REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark

## Quick Facts
- arXiv ID: 2502.12342
- Source URL: https://arxiv.org/abs/2502.12342
- Reference count: 29
- Primary result: Introduces a multi-modal retrieval benchmark with rephrasing and table-heavy document evaluation, revealing significant weaknesses in current models

## Executive Summary
REAL-MM-RAG introduces a multi-modal retrieval benchmark designed to evaluate retrieval-augmented generation systems in realistic scenarios. The benchmark addresses limitations in existing datasets by incorporating multi-modal documents, enhanced difficulty with long documents and sub-domain coverage, realistic RAG queries without explicit page references, and accurate labeling through exhaustive verification. It introduces a multi-level rephrasing scheme to evaluate semantic understanding beyond keyword matching and includes table-heavy financial documents to test model robustness. The benchmark reveals significant weaknesses in current models, particularly in handling rephrased queries and table-heavy documents. To address these issues, the authors curate two specialized training datasets: a rephrased dataset to improve semantic understanding and a table-heavy finance dataset to enhance retrieval on tabular content. Fine-tuning on these datasets yields state-of-the-art performance, demonstrating the effectiveness of targeted training data in improving retrieval models.

## Method Summary
The REAL-MM-RAG benchmark is constructed through a pipeline of automated steps: document collection and rendering to images, query generation using Pixtral-12B (10 query-answer pairs per page), RAG suitability verification with Mixtral-8x22B-v0.1 to filter out page-referencing or overly broad questions, three-level query rephrasing using Mixtral-8x22B-v0.1, and exhaustive false-negative verification using Pixtral-12B against all pages. The benchmark contains 8,000 pages across four sub-domains: FinReport, FinSlides, TechReport, and TechSlides. Fine-tuning uses ColPali training set with rephrased queries (LLaMA-3-70B) and a 46k triplet table-focused dataset from FinTabNet.

## Key Results
- Vision-based models (ColPali, ColQwen) significantly outperform text-based approaches (BM25, BGE-M3 with OCR) across all benchmarks
- NDCG@5 drops from 71.3 (level 0) to 56.6 (level 3) for ColPali under query rephrasing, exposing keyword-matching dependency
- RobTabColQwen achieves 67.1 NDCG@5 on FinReport (+25.3 over ColQwen baseline) and 61.6 on FinSlides (+30.5 improvement)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level query rephrasing exposes whether retrieval models rely on keyword matching versus semantic understanding.
- Mechanism: By applying three progressively aggressive rephrasing levels (minor word changes → structural restructuring) to queries while preserving meaning, the benchmark forces models to retrieve based on intent rather than surface-form overlap. Performance degradation at higher rephrasing levels indicates over-reliance on lexical cues.
- Core assumption: Rephrased queries accurately simulate real-world user variability, and LLM-based rephrasing preserves semantic equivalence (verified via a secondary LLM validation step).
- Evidence anchors:
  - [abstract] "we propose a multi-difficulty-level scheme based on query rephrasing to evaluate models' semantic understanding beyond keyword matching"
  - [section 5.1, Table 3] NDCG@5 drops from 71.3 (level 0) to 56.6 (level 3) for ColPali; BM25 drops most severely (52.7→27.1), confirming lexical fragility
  - [corpus] Related work (Zuccon et al., 2016; Penha et al., 2022) documents retrieval vulnerability to query variation, but no prior multi-modal benchmark standardized this evaluation
- Break condition: If rephrasing unintentionally alters query semantics (despite verification), observed performance drops may reflect query-answer misalignment rather than semantic weakness.

### Mechanism 2
- Claim: Training on rephrased queries and table-heavy documents improves retrieval robustness through forced semantic learning and layout understanding.
- Mechanism: Rephrased training data prevents models from memorizing query-document lexical patterns, requiring generalization to paraphrased inputs. Table-heavy finance data (FinTabNet-derived, 46k triplets) exposes models to dense tabular layouts, improving visual comprehension of structured content.
- Core assumption: Improvements transfer to held-out benchmarks; training data quality depends on VLM query generation accuracy (Pixtral-12B or Qwen2-VL-72B).
- Evidence anchors:
  - [abstract] "Fine-tuning on these datasets enables models to achieve state-of-the-art retrieval performance"
  - [section 5.2-5.3, Table 2] RobTabColQwen achieves 67.1 NDCG@5 on FinReport (+25.3 over ColQwen baseline) and 61.6 on FinSlides (+30.5 improvement)
  - [corpus] Related multi-modal RAG benchmarks (ViDoRAG, MHier-RAG) address retrieval complexity but do not isolate rephrasing robustness or provide targeted training data
- Break condition: If training data contains systematic biases (e.g., VLM-generated queries share artifacts), models may overfit to generation patterns rather than learn transferable semantics.

### Mechanism 3
- Claim: Exhaustive false-negative verification improves benchmark reliability by ensuring all relevant pages are correctly labeled.
- Mechanism: Each query is evaluated against every page in the corpus using a VLM (Pixtral-12B), identifying additional relevant pages missed by the initial generation process. Only queries with verified single-page answers are retained, reducing false negatives that would otherwise inflate perceived retrieval errors.
- Core assumption: VLM verification is sufficiently accurate to identify all relevant query-page pairs; computational cost (O(query×pages)) limits corpus scale.
- Evidence anchors:
  - [abstract] "accurate labeling through exhaustive verification"
  - [section 4, Human Evaluation] False negative rate: REAL-MM-RAG 31.9% vs. ViDoRe 86.9% and MMLongBench 77.8%
  - [corpus] ViDoRe V3 (neighbor paper) similarly emphasizes complex real-world evaluation but does not explicitly address false-negative labeling
- Break condition: If VLM verification misses valid matches (especially for multi-page reasoning), retained queries may still have incomplete labels.

## Foundational Learning

- Concept: **Late Interaction Retrieval (ColBERT-style)**
  - Why needed here: The paper builds on ColPali/ColQwen, which use multi-vector embeddings with late interaction. Understanding token-level matching versus single-vector similarity is essential for interpreting why these models outperform OCR+BM25 baselines.
  - Quick check question: Can you explain why late interaction (comparing all query-document token embeddings) is more expressive than single-vector dense retrieval?

- Concept: **Query Variation Robustness**
  - Why needed here: The benchmark's core contribution is evaluating how retrieval degrades under rephrasing. Understanding that users rarely match document wording clarifies why this matters for real-world RAG systems.
  - Quick check question: If a user queries "Q4 earnings" and the document says "fourth quarter revenue," should a robust retriever rank this document highly? What signals would it need?

- Concept: **Multi-Modal Document Representation**
  - Why needed here: Vision-based models (ColPali, ColQwen) embed page images directly, bypassing OCR. Understanding the trade-offs between OCR-pipeline approaches and end-to-end visual encoding is critical for architecture decisions.
  - Quick check question: What information might a VLM capture from a page image that OCR would miss? What might OCR capture that a VLM misses?

## Architecture Onboarding

- Component map:
  - Document Collection → Query Generation (VLM) → Query Filtering (LLM) → Query Rephrasing (LLM, 3 levels) → False-Negative Verification (VLM exhaustive check)

- Critical path:
  1. Rephrasing robustness evaluation (Table 3) → identifies semantic understanding gaps
  2. Table-heavy benchmark results (FinReport, FinSlides) → identifies visual/layout weaknesses
  3. Targeted fine-tuning → validates that specialized training data closes performance gaps

- Design tradeoffs:
  - **Exhaustive verification vs. scale**: Computing query×page VLM checks ensures accurate labels but limits corpus to 8,000 pages
  - **VLM query generation vs. human queries**: Automated generation enables scale but may miss natural query diversity (acknowledged as a limitation)
  - **Single-page vs. multi-page reasoning**: Benchmark restricts to single-page answers for retrieval evaluation purity, at the cost of not testing cross-document synthesis

- Failure signatures:
  - Sudden NDCG drop from rephrasing level 0→3 indicates keyword dependency
  - Consistently lower scores on FinSlides vs. TechSlides (Table 2) indicates table comprehension weakness
  - High false-negative rate (as in ViDoRe/MMLongBench) suggests labeling unreliable, not model failure

- First 3 experiments:
  1. **Baseline reproduction**: Run ColQwen on REAL-MM-RAG at rephrasing level 0 and level 3 to confirm reported degradation (41.8→31.1 on FinReport). This validates your evaluation pipeline.
  2. **Ablation on training data**: Fine-tune ColQwen on the rephrased ColPali training set only (without table-heavy data) and measure improvement on financial vs. technical benchmarks. This isolates the rephrasing robustness contribution.
  3. **Cross-domain generalization test**: Evaluate RobTabColQwen on an external multi-modal benchmark (e.g., ViDoRe or MMLongBench) to assess whether improvements transfer beyond the training distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated pipelines be adapted to generate multi-page reasoning queries for evaluating complex multi-modal retrieval?
- Basis in paper: [explicit] Section 7 states the benchmark does not assess multi-page reasoning and suggests exploring automated query generation that combines multiple pages.
- Why unresolved: The current construction pipeline explicitly filters and verifies queries against single pages, lacking a methodology for creating cross-page dependencies.
- Evidence: The development of a query generation mechanism that links multiple distinct pages to a single answer and metrics for evaluating multi-hop retrieval.

### Open Question 2
- Question: Does the improvement from fine-tuning on the finance-table-heavy dataset generalize to complex tables in non-financial domains?
- Basis in paper: [inferred] The training data is derived from FinTabNet (S&P 500 reports) and evaluated on IBM financial documents, leaving performance on other table-heavy domains (e.g., scientific or medical) uncertain.
- Why unresolved: While the authors show improved performance on financial benchmarks, it is unclear if the model learns generic table structure understanding or overfits to financial tabular patterns.
- Evidence: Evaluation of the TabCol models on a new benchmark containing complex tables from diverse fields such as biomedical research or engineering.

### Open Question 3
- Question: To what extent does the specific VLM used for query generation (Pixtral-12B) bias the benchmark's difficulty and "Realistic-RAG" alignment?
- Basis in paper: [explicit] Section 7 notes that queries generated by a VLM may not capture the full range of plausible user queries and that errors in labeling may occur.
- Why unresolved: The reliance on a single VLM for the entire generation pipeline may introduce systematic biases in phrasing or difficulty that affect how "realistic" the benchmark truly is.
- Evidence: A comparative analysis of benchmark quality and model rankings when query generation is performed using different VLMs or human annotators.

## Limitations

- Benchmark focuses on single-page answers, limiting applicability to real-world RAG scenarios requiring multi-document reasoning
- Reliance on automated VLM-based query generation and verification may introduce systematic biases in query phrasing and labeling accuracy
- Computational intensity of exhaustive VLM verification constrains corpus size to 8,000 pages, potentially limiting generalizability

## Confidence

**High Confidence**: Vision-based models significantly outperform text-based approaches across all benchmarks; targeted fine-tuning efficacy demonstrated through controlled experiments.

**Medium Confidence**: Claims about addressing real-world RAG limitations are substantiated by design choices, but rely on automated VLM-based labeling achieving human-level accuracy.

**Low Confidence**: Transferability of fine-tuned model improvements to entirely different document domains remains unproven; assertion that benchmark comprehensively represents "real-world" RAG scenarios is limited by single-page answer constraint.

## Next Checks

1. **Human Validation Study**: Conduct a blind human evaluation comparing VLM-generated labels against expert judgments on a stratified sample of query-page pairs across all rephrasing levels to quantify labeling accuracy and identify systematic biases in automated verification.

2. **Cross-Domain Generalization Test**: Evaluate the best-performing fine-tuned models (RobTabColQwen, RobColQwen) on at least two external multi-modal benchmarks (e.g., ViDoRe V3 and MMLongBench) to assess whether improvements transfer beyond the training distribution and identify domain-specific limitations.

3. **Multi-Page Reasoning Extension**: Modify the benchmark to include a subset of queries requiring cross-page synthesis (e.g., "Compare Q3 and Q4 revenue across all annual reports"), measuring whether current models can extend their retrieval capabilities beyond single-page contexts while maintaining accuracy.