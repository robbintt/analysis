---
ver: rpa2
title: A Survey on Large Language Model Benchmarks
arxiv_id: '2508.15361'
source_url: https://arxiv.org/abs/2508.15361
tags:
- language
- benchmarks
- arxiv
- evaluation
- monolingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically reviews 283 benchmarks for evaluating
  large language models (LLMs), categorizing them into three types: general capabilities,
  domain-specific, and target-specific. The study highlights the evolution from early
  single-task evaluations to complex, multi-dimensional frameworks.'
---

# A Survey on Large Language Model Benchmarks

## Quick Facts
- arXiv ID: 2508.15361
- Source URL: https://arxiv.org/abs/2508.15361
- Reference count: 40
- Key outcome: Systematic review of 283 LLM benchmarks categorizing them into general capabilities, domain-specific, and target-specific types, identifying critical issues including data contamination and cultural bias

## Executive Summary
This survey provides a comprehensive analysis of 283 benchmarks used to evaluate Large Language Models (LLMs), systematically categorizing them into three primary types: General Capabilities (linguistic, knowledge, reasoning), Domain-Specific (sciences, humanities, engineering), and Target-Specific (safety, agents, others). The study traces the evolution from early single-task evaluations to complex multi-dimensional frameworks and identifies critical challenges including data contamination, cultural bias, and the need for process-level evaluation. The authors emphasize the importance of dynamic, inclusive, and robust evaluation methodologies to ensure fair and trustworthy LLM assessments across diverse applications.

## Method Summary
The authors conducted a systematic literature review, identifying 283 LLM benchmark papers and datasets from academic sources including arXiv and conference proceedings. They applied qualitative analysis to categorize these benchmarks into a hierarchical taxonomy based on their primary focus areas. The methodology involved mapping individual benchmarks to the proposed framework (e.g., categorizing MMLU under Knowledge and HarmBench under Safety). The selection criteria for "representative" benchmarks were based on citation counts and impact, though exact inclusion/exclusion criteria were not fully specified.

## Key Results
- 283 benchmarks categorized into three primary types: General Capabilities, Domain-Specific, and Target-Specific
- Identification of data contamination as a critical issue causing inflated scores across major benchmarks
- Evolution documented from single-task evaluations to multi-dimensional frameworks like HELM
- Three major challenges identified: data contamination, cultural bias, and static evaluation approaches

## Why This Works (Mechanism)

### Mechanism 1
Benchmarks function as both measurement tools and development guides for LLMs. They provide quantitative performance signals that create optimization targets for model training while simultaneously revealing capability gaps that inform research priorities and architectural innovations. The core assumption is that improvement on benchmark metrics correlates with genuine capability advancement rather than dataset-specific memorization. Evidence shows MMLU established rigorous standards catalyzing an arms race in both model development and benchmark design. Break condition occurs when models optimize for benchmark performance through memorization rather than developing transferable capabilities.

### Mechanism 2
Comprehensive LLM evaluation requires transitioning from isolated single-task metrics to multi-dimensional capability profiling. Multi-dimensional benchmarks expose capability boundaries across diverse domains, revealing systematic strengths and weaknesses that single-metric evaluations mask, enabling more targeted model improvement. The core assumption is that performance variations across benchmark categories reflect genuine differences in underlying capabilities. Evidence documents progression from GLUE's 9 tasks to HELM's living benchmark concept with continuous scenario expansion. Break condition occurs when multi-dimensional benchmarks become contaminated or when models develop superficial strategies that transfer across tasks without genuine reasoning.

### Mechanism 3
Data contamination—overlap between training data and benchmark test sets—artificially inflates performance metrics. Models exposed to benchmark examples during training can achieve high scores through pattern recognition and memorization rather than genuine generalization, creating a false impression of capability. The core assumption is that a model's score on contaminated benchmarks overestimates its performance on truly novel, unseen problems. Evidence identifies inflated scores caused by data contamination as a critical issue, with discussion of how contamination makes evaluation an open-book exam that cannot truly measure reasoning capabilities. Break condition occurs when contamination detection methods fail to identify all contaminated samples or when models achieve inflated scores through means other than direct memorization.

## Foundational Learning

- **Concept: Benchmark Taxonomy and Purpose**
  - Why needed here: The survey categorizes 283 benchmarks into three types (general capabilities, domain-specific, target-specific). Understanding this taxonomy is essential for selecting appropriate evaluation strategies and interpreting results correctly.
  - Quick check question: Given a model intended for medical diagnosis assistance, which benchmark categories would you prioritize for evaluation, and why?

- **Concept: Evaluation Metric Limitations**
  - Why needed here: The paper discusses how single metrics like accuracy fail to capture nuanced capabilities (e.g., reasoning process, safety, consistency). Understanding these limitations prevents misinterpreting benchmark scores.
  - Quick check question: A model achieves 95% accuracy on a knowledge benchmark but struggles with multi-step reasoning. What evaluation approaches beyond accuracy would reveal this limitation?

- **Concept: Benchmark Validity Threats**
  - Why needed here: The survey identifies data contamination, cultural bias, and static evaluation as key validity threats. Recognizing these helps assess whether benchmark results generalize to real-world performance.
  - Quick check question: How would you determine whether a model's strong performance on a benchmark reflects genuine capability versus memorization of contaminated training examples?

## Architecture Onboarding

- **Component map**: The evaluation ecosystem consists of: (1) Benchmark design (task definition, data curation, metric selection), (2) Model-benchmark interface (inference protocol, prompting strategy), (3) Result interpretation (score aggregation, cross-benchmark comparison, contamination analysis), and (4) Feedback loop (identifying capability gaps, informing training improvements).

- **Critical path**: Select benchmarks aligned with target deployment → Verify contamination status → Execute evaluation with standardized protocols → Aggregate results across multiple benchmarks → Identify systematic patterns in failures → Translate findings into actionable training or architecture modifications.

- **Design tradeoffs**: 
  - Breadth vs. depth: Comprehensive coverage (e.g., MMLU's 57 disciplines) vs. deep probing of specific capabilities (e.g., GPQA's expert-level questions)
  - Static vs. dynamic: Fixed test sets enable reproducibility but risk obsolescence; dynamic benchmarks (e.g., LiveBench) resist contamination but complicate comparison
  - Automated vs. human evaluation: Scalability vs. nuanced judgment (e.g., LLM-as-judge approaches may introduce systematic biases)

- **Failure signatures**:
  - Suspiciously high scores across all benchmarks may indicate widespread contamination
  - Large performance gaps between similar benchmarks suggest format-specific artifacts
  - Consistent failures on specific capability types (e.g., multi-hop reasoning) indicate architectural limitations
  - Performance degradation on perturbed versions of benchmarks (e.g., MATH-P) suggests reliance on superficial patterns

- **First 3 experiments**:
  1. **Baseline profiling**: Evaluate your model across representative benchmarks from each category (general, domain-specific, target-specific) to establish a capability baseline. Include at least one contamination-resistant benchmark (e.g., LiveBench, GPQA) to calibrate for potential inflation.
  2. **Contamination audit**: Apply detection methods (e.g., Min-k% probability from WikiMIA) to estimate contamination levels in your primary benchmarks. Compare performance on contaminated vs. clean benchmark subsets to quantify inflation magnitude.
  3. **Failure mode analysis**: Systematically analyze incorrect predictions across benchmarks to identify recurring failure patterns (e.g., specific reasoning types, knowledge domains, or prompt formats). This reveals whether failures stem from capability gaps or evaluation artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can future benchmarks systematically prevent inflated scores caused by data contamination without sacrificing scalability?
- Basis in paper: [explicit] The Abstract identifies "inflated scores caused by data contamination" as a critical issue, and Section 3.2 highlights the need for "Google-Proof" benchmarks to mitigate this.
- Why unresolved: As training datasets grow to encompass the web, the probability of test set leakage increases, making it difficult to distinguish between generalization and memorization.
- Evidence would resolve it: A standardized framework for detecting memorization or a widely adopted dynamic benchmark (e.g., LiveBench) that maintains low contamination rates over time.

### Open Question 2
- Question: How can the field overcome the "self-referential trap" where LLM-as-Judge methodologies circularly validate their own generative patterns?
- Basis in paper: [explicit] Section 3.1.3 warns that when frontier models assess conversational depth or instruction fidelity, they risk "circularly validating their own generative patterns" rather than authentic capability.
- Why unresolved: The field lacks scalable alternatives to using strong LLMs for open-ended evaluation, and human evaluation is often too resource-intensive.
- Evidence would resolve it: The development of adversarial auditing frameworks or specialized, domain-tuned judge ensembles that demonstrably diverge from the biases of the judge model's training data.

### Open Question 3
- Question: How can benchmarks effectively evaluate the faithfulness and logical consistency of a model's reasoning process rather than just the correctness of the final output?
- Basis in paper: [explicit] Section 3.3.4 notes that current evaluations predominantly focus on the final output and must shift to scrutinizing the reasoning process itself (faithfulness and efficiency). Section 4.1.6 reiterates this in the context of "shortcut learning."
- Why unresolved: Evaluating intermediate reasoning steps (chains of thought) requires complex annotations and new metrics that are difficult to automate reliably.
- Evidence would resolve it: A benchmark that successfully penalizes models for reaching correct answers via invalid logical steps, or the adoption of verifiable, program-guided reasoning evaluation methods.

## Limitations
- The selection methodology for "representative" benchmarks remains unclear, potentially introducing selection bias toward English-language, academic benchmarks
- Quantitative assessment of contamination prevalence across benchmarks lacks empirical validation, relying instead on qualitative analysis of known cases
- Analysis of future directions and improvement strategies remains largely theoretical without concrete implementation pathways or empirical evidence

## Confidence
- High Confidence (4/5): The categorization framework and its detailed breakdown into 10 subcategories appears methodologically sound with clear differentiation criteria
- Medium Confidence (3/5): Identification of critical issues is well-supported by examples, though quantitative contamination assessment lacks validation
- Low Confidence (2/5): Future directions and improvement strategies are theoretical with limited empirical evidence of effectiveness

## Next Checks
1. **Contamination Audit**: Apply the Min-k% probability method (from WikiMIA) to estimate contamination levels in MMLU, MATH, and other widely-used benchmarks. Compare the actual inflation magnitude with the survey's qualitative assessment.

2. **Cross-Cultural Evaluation**: Test the cultural bias claims by evaluating models on both Western-centric benchmarks (e.g., MMLU) and culturally diverse alternatives (e.g., Multi-Text). Measure performance gaps and analyze whether current benchmarks systematically disadvantage non-Western knowledge and reasoning patterns.

3. **Process vs. Outcome Evaluation**: Implement a controlled study comparing traditional accuracy-based evaluation with process-level assessment for a reasoning task (e.g., GSM8K). Track whether models that achieve similar final accuracy scores employ fundamentally different reasoning strategies, validating the survey's call for more sophisticated evaluation methodologies.