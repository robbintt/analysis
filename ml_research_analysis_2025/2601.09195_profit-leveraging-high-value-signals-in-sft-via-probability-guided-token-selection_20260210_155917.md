---
ver: rpa2
title: 'ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token
  Selection'
arxiv_id: '2601.09195'
source_url: https://arxiv.org/abs/2601.09195
tags:
- tokens
- arxiv
- profit
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of surface-level overfitting in
  supervised fine-tuning (SFT) of large language models, where models overfit to non-core
  expressions due to the one-to-many nature of language. The proposed method, ProFit,
  leverages token probability as a proxy for semantic importance, selectively masking
  low-probability tokens during training to focus optimization on high-value core
  semantics.
---

# ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection

## Quick Facts
- arXiv ID: 2601.09195
- Source URL: https://arxiv.org/abs/2601.09195
- Reference count: 40
- ProFit improves SFT performance by up to 11.4% average accuracy on reasoning and math tasks by masking low-probability tokens

## Executive Summary
This paper addresses surface-level overfitting in supervised fine-tuning (SFT) where models memorize specific phrasings rather than core reasoning patterns. The proposed method, ProFit, uses token prediction probabilities as proxies for semantic importance, selectively masking low-probability tokens during training to focus optimization on high-value core semantics. Extensive experiments demonstrate consistent improvements across reasoning, mathematics, and instruction-following benchmarks, with theoretical analysis confirming that masking low-probability tokens prevents their large gradients from overshadowing critical semantic signals.

## Method Summary
ProFit modifies standard SFT by computing a binary mask Mt = I[sg(πθ(y*t|x, y*<t)) > τ] using stop-gradient on token probabilities before thresholding. Only tokens exceeding threshold τ contribute to the loss calculation, with loss normalized by sequence length T rather than the number of retained tokens. The method trains on 2,000 high-quality samples from BAAI-InfinityInstruct Dataset using Qwen3-4B-Base, evaluating across reasoning (GPQA-Diamond), mathematics (GSM8K, MATH-500, AIME'24), and instruction-following (IFEval) benchmarks.

## Key Results
- 11.4% average accuracy improvement on Qwen3-4B-Base compared to vanilla SFT
- Reverses performance drops seen in vanilla SFT on reasoning benchmarks
- Consistent improvements across multiple task types with optimal threshold τ ≈ 0.1-0.5
- GPQA-Diamond shows slight degradation at higher thresholds, suggesting knowledge tasks may need lower τ

## Why This Works (Mechanism)

### Mechanism 1: Probability as a Proxy for Semantic Importance
Token prediction probability correlates with semantic necessity—high-probability tokens encode core reasoning logic while low-probability tokens represent stylistic variations. The model's own probability estimates serve as real-time indicators of token importance during forward propagation.

### Mechanism 2: Gradient Amplification from Low-Probability Tokens
Low-probability tokens generate disproportionately large gradients that can dominate parameter updates and distort optimization direction. From Equation 2, the logit gradient for a token is ∂ℓt/∂zt,v = pt,v - I[v=y*t]. When pt,y*t is low (model uncertain), the residual (1 - pt,y*t) is large, creating stronger gradient signals.

### Mechanism 3: Selective Hard Masking with Stop-Gradient
Hard masking (complete exclusion) of low-probability tokens prevents gradient interference more effectively than soft reweighting. ProFit computes a binary mask Mt = I[sg(πθ(y*t|x, y*<t)) > τ] using stop-gradient to decouple mask computation from backpropagation.

## Foundational Learning

- **Cross-Entropy Loss and Gradient Dynamics**
  - Why needed here: Understanding why vanilla SFT overfits requires grasping how cross-entropy gradients behave—specifically, how ∇zℓ = p - ey*t creates stronger signals for uncertain predictions.
  - Quick check question: Given a token with predicted probability 0.3 vs 0.8, which generates a larger gradient magnitude during SFT, and why?

- **Stop-Gradient Operations**
  - Why needed here: ProFit's masking uses sg(·) to prevent mask computation from affecting backpropagation. Without this, the indicator function's discontinuity would cause training instability.
  - Quick check question: What would happen to training stability if the mask Mt were computed without stop-gradient and τ were treated as learnable?

- **Surface-Level vs. Semantic Overfitting**
  - Why needed here: The paper distinguishes memorizing specific phrasings (surface) from failing to generalize reasoning patterns (semantic). ProFit targets the former to improve the latter.
  - Quick check question: Why might multi-reference SFT exacerbate rather than alleviate overfitting, as observed in Figure 2's GPQA-Diamond results?

## Architecture Onboarding

- **Component map:**
Input (x, y*) → Forward Pass → Per-token probabilities πθ(y*t|x, y*<t) → Stop-gradient → Binary mask: Mt = I[π > τ] → Masked loss: -Σ(Mt · log πθ) → Backprop (masked tokens only)

- **Critical path:**
1. Standard forward pass computes token probabilities
2. Stop-gradient detaches probabilities before threshold comparison
3. Binary mask zeros out low-probability token contributions
4. Loss aggregates only over retained tokens (normalized by sequence length T, not mask sum)

- **Design tradeoffs:**
  - **Threshold τ (0.1-0.9 range tested):** Lower retains more tokens (conservative); higher focuses on highest-confidence core (aggressive). Paper uses τ≈0.1-0.5 for most experiments.
  - **Hard masking vs. soft reweighting:** Hard masking (ProFit) completely excludes trivial tokens; soft reweighting (DFT) scales their contribution. Hard masking is computationally simpler but less forgiving of threshold mis-specification.
  - **Static vs. adaptive thresholds:** Current implementation uses fixed τ per run. Adaptive thresholds could handle task heterogeneity but add complexity.

- **Failure signatures:**
  - Performance drops on knowledge-intensive benchmarks (long-tail facts masked as "low probability")
  - Instability if stop-gradient omitted (indicator function discontinuity propagating gradients)
  - Threshold too high: excessive masking leaves insufficient supervision signal
  - Creative tasks where "stylistic" tokens actually matter for quality

- **First 3 experiments:**
1. **Threshold sweep on held-out validation:** Run ProFit with τ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} on Qwen3-4B with 500-sample validation split. Plot accuracy vs. τ per task type (reasoning, math, instruction-following) to identify task-specific optima.
2. **Gradient norm comparison:** Log ∥∇θℓ∥2 per-token for both masked and unmasked tokens during a short training run. Verify that masked tokens indeed show higher average gradient norms, confirming Theorem 1 empirically in your setup.
3. **Ablation: Hard vs. soft masking:** Compare ProFit (hard mask at τ=0.3) against a soft reweighting baseline that scales loss by (1 - τ) for low-probability tokens rather than zeroing them. Use same τ to isolate the masking strategy effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProFit perform on creative generation tasks where low-probability tokens may contribute to stylistic diversity rather than representing non-essential noise?
- Basis in paper: [explicit] Limitations section states: "our core assumption that low-probability tokens represent non-core expressions primarily holds for logic-intensive tasks (e.g., reasoning, mathematics). For creative generation tasks, such tokens may contribute to stylistic diversity, which requires further investigation."
- Why unresolved: All experiments focus exclusively on reasoning, mathematics, and instruction-following benchmarks; no creative writing or open-ended generation tasks were evaluated.
- What evidence would resolve it: Experiments applying ProFit to creative generation benchmarks (e.g., story completion, dialogue) with diversity and quality evaluations comparing masked vs. full-token training.

### Open Question 2
- Question: Can adaptive, instance-specific probability thresholds improve ProFit's performance compared to the static threshold approach?
- Basis in paper: [explicit] "ProFit currently employs a static probability threshold across all samples... we acknowledge that future iterations could further enhance performance by exploring adaptive mechanisms that dynamically adjust the threshold based on instance-specific difficulty."
- Why unresolved: The paper uses a fixed threshold τ for all samples; no dynamic or difficulty-aware thresholding mechanisms were explored.
- What evidence would resolve it: Comparative experiments with adaptive thresholding strategies (e.g., difficulty-calibrated, entropy-based) demonstrating consistent improvements over static thresholds.

### Open Question 3
- Question: What are the failure modes of ProFit on knowledge-intensive tasks where specific long-tail entities may fall into the low-probability range and get incorrectly masked?
- Basis in paper: [inferred] Figure 4(b) shows GPQA-Diamond performance declining at higher thresholds: "knowledge-intensive tasks like GPQA-Diamond show a slight performance decline as the threshold increases, likely due to specific long-tail entities falling into the low-probability range."
- Why unresolved: The paper does not analyze which knowledge types or entity categories are most susceptible to incorrect masking, nor propose mitigation strategies.
- What evidence would resolve it: Token-level analysis of masked content on knowledge benchmarks, identifying systematically masked factual tokens; evaluation of hybrid masking strategies for knowledge-intensive domains.

## Limitations
- Task-Threshold Heterogeneity: Optimal threshold varies significantly across task types, with knowledge tasks showing degradation at higher τ values
- Generalization Across Model Sizes: All experiments use Qwen3-4B-Base; scaling effects to larger models remain untested
- Creative Task Performance: Explicitly noted as unsuitable, but no empirical validation of this limitation

## Confidence
- **High Confidence**: The core mechanism linking token probability to semantic importance is well-supported by theoretical analysis (Theorem 1) and empirical validation (p=1×10⁻⁶ hypothesis test)
- **Medium Confidence**: The surface-level overfitting problem and experimental improvements (11.4% average gain) are well-documented, but generalization across model scales and task types remains uncertain
- **Low Confidence**: Claims about ProFit's superiority over soft reweighting methods and its limitations for creative tasks lack direct empirical comparison

## Next Checks
- **Validation Check 1: Cross-Model Threshold Validation** Run ProFit with τ ∈ {0.1, 0.3, 0.5, 0.7} on Qwen3-7B-Base and Qwen3-13B-Base using the same BAAI-InfinityInstruct subset. Plot accuracy vs. τ curves for each model size to test whether the 0.1-0.5 range remains optimal or whether larger models require different thresholds.
- **Validation Check 2: Knowledge vs. Reasoning Ablation** Create a stratified evaluation suite with three components: (a) pure reasoning tasks (GSM8K, MATH-500), (b) knowledge-intensive tasks (GPQA-Diamond, MMLU), and (c) mixed tasks (IFEval). Run ProFit with τ ∈ {0.1, 0.5, 0.9} on each category separately. Compare against vanilla SFT to determine whether knowledge degradation occurs at higher thresholds and whether reasoning gains persist independently.
- **Validation Check 3: Hard vs. Soft Masking Efficiency** Implement a soft reweighting baseline where low-probability tokens contribute loss scaled by (1 - τ) rather than being zeroed. Run both ProFit (hard mask) and soft baseline with τ=0.3 on Qwen3-4B-Base for exactly 3,000 steps (same computational budget). Measure: (a) final accuracy on all benchmarks, (b) wall-clock training time, (c) GPU memory usage.