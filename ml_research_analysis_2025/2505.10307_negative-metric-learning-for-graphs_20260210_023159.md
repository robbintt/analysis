---
ver: rpa2
title: Negative Metric Learning for Graphs
arxiv_id: '2505.10307'
source_url: https://arxiv.org/abs/2505.10307
tags:
- negatives
- negative
- 'false'
- learning
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the false negative problem in graph contrastive
  learning (GCL), where positive samples are incorrectly treated as negatives, degrading
  downstream task performance. The authors propose Negative Metric Learning (NML)
  enhanced GCL (NML-GCL), which employs a learnable Negative Metric Network (NMN)
  to build a negative metric space where false negatives can be distinguished from
  true negatives based on their distance to anchor nodes.
---

# Negative Metric Learning for Graphs

## Quick Facts
- arXiv ID: 2505.10307
- Source URL: https://arxiv.org/abs/2505.10307
- Authors: Yiyang Zhao; Chengpei Wu; Lilin Zhang; Ning Yang
- Reference count: 40
- This paper addresses the false negative problem in graph contrastive learning (GCL), where positive samples are incorrectly treated as negatives, degrading downstream task performance. The authors propose Negative Metric Learning (NML) enhanced GCL (NML-GCL), which employs a learnable Negative Metric Network (NMN) to build a negative metric space where false negatives can be distinguished from true negatives based on their distance to anchor nodes. To overcome the lack of explicit supervision signals, NML-GCL uses a joint training scheme with bi-level optimization that iteratively optimizes the encoder and NMN using self-supervision signals. The method is theoretically justified by proving that NML achieves a tighter lower bound of mutual information compared to traditional InfoNCE loss. Experiments on six widely-used datasets show that NML-GCL consistently outperforms state-of-the-art GCL methods, achieving up to 1.64% improvement in node classification accuracy and 1.96% improvement in FMI over the best baseline. The learned negative metric space successfully separates false negatives from true negatives, enhancing the discriminative power of node embeddings.

## Executive Summary
This paper introduces Negative Metric Learning (NML) enhanced Graph Contrastive Learning (NML-GCL) to address the false negative problem in GCL, where nodes with the same labels are incorrectly treated as negatives. The method employs a learnable Negative Metric Network (NMN) to construct a negative metric space that distinguishes false negatives from true negatives based on their distance to anchor nodes. Through bi-level optimization, the encoder and NMN mutually reinforce each other without explicit supervision signals. The approach is theoretically justified by proving a tighter mutual information lower bound compared to InfoNCE loss, and experimentally validated on six benchmark datasets with consistent improvements over state-of-the-art GCL methods.

## Method Summary
NML-GCL extends standard GCL by introducing a learnable Negative Metric Network (NMN) that outputs normalized distance scores between node pairs. The method uses bi-level optimization: an inner loop updates the NMN to minimize the NML loss, while an outer loop updates the encoder using the learned metric as soft labels. This joint training scheme allows the encoder and NMN to reinforce each other without explicit false negative supervision. The approach is theoretically justified by proving that NML achieves a tighter mutual information lower bound compared to traditional InfoNCE loss. The method is evaluated on six benchmark datasets with two-layer GCN encoders and MLPs for the NMN, using standard augmentation techniques.

## Key Results
- NML-GCL consistently outperforms state-of-the-art GCL methods across six benchmark datasets
- Achieves up to 1.64% improvement in node classification accuracy and 1.96% improvement in FMI over the best baseline
- The learned negative metric space successfully separates false negatives from true negatives
- NML loss achieves a tighter mutual information lower bound compared to InfoNCE loss

## Why This Works (Mechanism)

### Mechanism 1: Learnable Negative Metric Space Construction
- **Claim:** The Negative Metric Network (NMN) learns to distinguish false negatives from true negatives by outputting smaller distance scores for nodes more likely to be false negatives.
- **Mechanism:** The NMN M maps node pairs (u_i, v_j) to normalized distances m_ij ∈ [0,1]. When minimizing L_NML w.r.t. M, larger similarity θ(u_i, v_j) produces smaller m_ij. Since false negatives share labels with anchors, they exhibit higher similarity, receiving smaller weights that reduce their penalization during contrastive learning.
- **Core assumption:** Same-class nodes (false negatives) have higher embedding similarity to anchors than different-class nodes (true negatives).
- **Evidence anchors:**
  - [abstract]: "NML-GCL employs a learnable Negative Metric Network (NMN) to build a negative metric space, in which false negatives can be distinguished better from true negatives based on their distance to anchor node."
  - [section 3.2, p.3]: "It is obvious that when minimizing L(i)_NML w.r.t. M, the bigger θ(u_i, v_j), the smaller m_ij."
  - [corpus]: Related work on false negatives (FALCON, GRAPE) addresses similar problems but with different approaches; no direct comparison in corpus.
- **Break condition:** Fails in highly heterophilic graphs where same-class nodes are structurally dissimilar, or when initial embeddings have poor discriminability.

### Mechanism 2: Bi-level Optimization for Mutual Reinforcement
- **Claim:** Joint training enables encoder and NMN to reinforce each other without explicit false negative supervision.
- **Mechanism:** In iteration t+1: (1) Inner minimization updates M^(t+1) with optimal m^(t+1)_ik inversely proportional to θ(u^(t)_i, v^(t)_k); (2) Outer minimization updates E^(t+1) where larger m^(t+1)_ij forces embeddings farther apart. This creates a self-training loop where better M improves E's supervision, and better E provides more reliable similarity signals for M.
- **Core assumption:** Self-supervision from positive pairs (u_i, v_i) is sufficient to bootstrap both components.
- **Evidence anchors:**
  - [section 3.3, p.4]: "the training of E can be regarded as a form of self-training... where the supervision signal is provided by E via M."
  - [section 3.2, p.4]: "via bigger θ(u_i, v_i), the self-supervision signals can strengthen M's ability to recognize false negatives."
  - [corpus]: No corpus evidence for bi-level optimization in GCL specifically.
- **Break condition:** Poor initial encoder may cause convergence to suboptimal local minima where M and E reinforce incorrect distinctions.

### Mechanism 3: Tighter Mutual Information Lower Bound
- **Claim:** NML-GCL achieves a tighter MI lower bound than InfoNCE, improving generalization.
- **Mechanism:** While InfoNCE uses fixed weights (1/(N-1)) for negatives, NML adaptively learns weights m_ij ∈ [0,1]. Theorem 1 proves I(U;V) ≥ I_NML(U;V) ≥ I_NCE(U;V), allowing NML to capture more information about view relationships.
- **Core assumption:** Tighter MI bound directly translates to better downstream performance (empirically validated but not causally proven).
- **Evidence anchors:**
  - [section 4.1, p.5]: "Theorem 1. I(U;V) ≥ I_NML(U;V) ≥ I_NCE(U;V)"
  - [appendix B.1, p.11]: "InfoNCE simply sets m_ij = 0 if i=j otherwise m_ij = 1/(N-1) while INML seeks the optimum m_ij ∈ [0,1]."
  - [corpus]: No corpus evidence on MI bounds in related work.
- **Break condition:** Poorly trained NMN may not realize tighter bound; regularization weight α too large forces uniform weights, reverting to InfoNCE-like behavior.

## Foundational Learning

- **Concept: Graph Contrastive Learning (GCL)**
  - **Why needed here:** NML-GCL extends standard GCL's augmentation-encoding-contrasting pipeline. Understanding InfoNCE loss, contrastive views, and positive/negative pairs is essential.
  - **Quick check question:** Explain how InfoNCE loss treats all negatives equally and why this degrades performance when false negatives exist.

- **Concept: Bi-level Optimization**
  - **Why needed here:** Core training uses nested loops—inner updates NMN, outer updates encoder. Understanding gradient flow and inter-level dependencies is critical.
  - **Quick check question:** What happens if inner loop optimization is insufficient before outer loop updates?

- **Concept: Mutual Information Estimation**
  - **Why needed here:** Theoretical justification relies on proving tighter MI lower bounds. Understanding why InfoNCE bounds MI helps evaluate the improvement claim.
  - **Quick check question:** Why does a tighter lower bound of MI matter for representation quality?

## Architecture Onboarding

- **Component map:** Input G → [Augment: DropEdge 40% + FeatureMask 10%] → G_U, G_V → [Encoder E: 2-layer GCN, d=512] → U, V ∈ R^(N×512) → [NMN M: MLP(512→512→512) + Softmax] → M ∈ R^(N×N) → [Loss: L_NML + α·KL(uniform||M)]

- **Critical path:**
  1. Generate augmentations; compute embeddings U, V with frozen E
  2. Inner loop (T_M iterations): Update M minimizing L_NML
  3. Compute M matrix; outer loop: Update E using M as soft labels
  4. Repeat T_E epochs

- **Design tradeoffs:**
  - **T_M (inner iterations):** More iterations → better M convergence but higher cost. Paper uses 2–8.
  - **α (regularization):** Small α → overfitting (one-hot weights); large α → uniform weights, losing discriminability. Paper uses 0.05–0.2.
  - **NMN capacity:** Larger MLP captures nuanced relationships but risks overfitting.

- **Failure signatures:**
  - **NaN loss:** Numerical instability in softmax; check initialization.
  - **No improvement over InfoNCE:** NMN not learning; verify T_M sufficient, α tuned.
  - **Memory overflow on large graphs:** N×N matrix expensive; consider sampling/mini-batching.
  - **Uniform weights (all ≈1/N):** α too large; reduce regularization.

- **First 3 experiments:**
  1. **Reproduce Cora/CiteSeer results:** Train with defaults; verify accuracy (84.93%/73.37%) and that false negatives receive smaller m_ij than true negatives.
  2. **Ablation: NMN vs. cosine similarity:** Compare against "repl. cosine sim." variant; expect 1–3% improvement from learnable NMN.
  3. **Hyperparameter sweep (α, T_M):** Grid search α ∈ {0.05, 0.1, 0.2}, T_M ∈ {2, 5, 8} on Photo; verify U-shaped α curve and convergence behavior.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rest on idealized assumptions about node similarity distributions that may not hold in practice
- Requires computing an NxN metric matrix, creating O(N²) memory complexity that limits scalability
- Bi-level optimization introduces significant computational overhead through multiple inner-loop iterations per outer update
- Absence of comparison to more recent GCL methods like GRACE or MVGRL represents a notable gap in experimental evaluation

## Confidence
- **High Confidence:** The NML loss formulation and bi-level optimization algorithm are clearly specified and reproducible. The ablation studies showing performance gains over InfoNCE baselines are well-supported.
- **Medium Confidence:** The theoretical proof of tighter MI bounds is mathematically sound, but the practical significance of this improvement remains unproven. The claim that improved MI bounds directly cause better downstream performance lacks causal evidence.
- **Low Confidence:** The claim that NMN specifically learns to distinguish false negatives based on structural similarity is not directly validated. The paper does not analyze whether NMN weights actually correlate with node label similarity in a meaningful way.

## Next Checks
1. **False Negative Detection Validation:** For Cora/CiteSeer datasets, compute correlation between NMN weights m_ij and actual node label similarity (true/false negatives) to verify NMN actually learns the intended discrimination.
2. **Scalability Test:** Implement NML-GCL on a larger graph (e.g., OGBN-Proteins with ~132K nodes) and measure memory usage and training time compared to standard GCL methods to quantify the O(N²) overhead.
3. **Alternative Negative Metrics:** Replace the learned NMN with cosine similarity or learned attention weights, and compare performance to assess whether the specific architecture choices matter beyond having any learnable negative metric.