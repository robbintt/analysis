---
ver: rpa2
title: 'Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical
  Guarantees'
arxiv_id: '2510.24754'
source_url: https://arxiv.org/abs/2510.24754
tags:
- prediction
- confidence
- intervals
- scores
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying uncertainty in
  predictions made by uncertain knowledge graph embedding (UnKGE) models. While UnKGE
  methods can predict confidence scores for unseen triples, they only provide point
  estimates without capturing predictive uncertainty.
---

# Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees

## Quick Facts
- arXiv ID: 2510.24754
- Source URL: https://arxiv.org/abs/2510.24754
- Reference count: 28
- Key outcome: UNKGCP constructs prediction intervals for UnKGE outputs with guaranteed coverage ≥α, demonstrating sharp and informative intervals on standard benchmarks

## Executive Summary
This paper addresses the problem of quantifying uncertainty in predictions made by uncertain knowledge graph embedding (UnKGE) models. While UnKGE methods can predict confidence scores for unseen triples, they only provide point estimates without capturing predictive uncertainty. The authors propose UNKGCP, a framework that constructs prediction intervals guaranteed to contain the true score with a user-specified confidence level. UNKGCP builds on conformal prediction and introduces a novel nonconformity measure tailored to UnKGE methods. The method also employs an efficient procedure for interval construction. Theoretical guarantees on the coverage of the ground truth by the prediction intervals are provided, and extensive experiments on standard benchmarks demonstrate that the intervals are sharp, informative, and effectively capture predictive uncertainty.

## Method Summary
UNKGCP constructs prediction intervals for UnKGE model outputs using inductive conformal prediction. The method partitions data into training (85%), calibration (7%), and test (8%) sets. A UnKGE model is trained on the training set and remains fixed. Nonconformity scores are computed on calibration triples using an entropy-normalized measure: |predicted - actual| / H(predicted), where H is binary entropy. A threshold is selected via quantile estimation from sorted calibration scores. For test queries, prediction intervals are constructed as [M(q) - ε, M(q) + ε] where ε = threshold × H(M(q)). The framework guarantees coverage ≥α under exchangeability assumptions.

## Key Results
- UNKGCP achieves coverage ≥0.90 on in-distribution test triples across all datasets and UnKGE backbones
- Prediction intervals effectively capture model uncertainty, with width positively correlated with prediction error (conditionality)
- UNKGCP produces sharper intervals than baseline conformal prediction methods while maintaining validity guarantees
- Coverage drops to 0.7-0.8 on synthetic negative triples, indicating distribution shift detection

## Why This Works (Mechanism)

### Mechanism 1: Inductive Conformal Prediction for Efficient Interval Construction
- Claim: UNKGCP constructs prediction intervals without retraining the UnKGE model for each candidate confidence score
- Mechanism: Data is partitioned into proper training and calibration sets. The UnKGE model is trained once and remains fixed. Nonconformity scores are computed on calibration set, and a threshold is selected via quantile estimation to construct intervals for new queries
- Core assumption: Calibration and test triples are exchangeable (weaker than i.i.d.)
- Evidence anchors: [abstract] "UNKGCP builds on the conformal prediction framework but introduces... an efficient procedure for interval construction"; [Section 4.2] "We employ inductive conformal prediction... to construct prediction intervals efficiently"
- Break condition: If calibration set is too small (<5-10% of data), quantile estimates become unstable

### Mechanism 2: Entropy-Normalized Nonconformity Measure for Query-Adaptive Intervals
- Claim: The entropy-normalized nonconformity measure produces prediction intervals whose width adapts to the model's predictive uncertainty for each query
- Mechanism: Nonconformity score is defined as |M(q) - c| / H(M(q)), where H(M(q)) is binary entropy of predicted score. Higher entropy (lower confidence) tolerates larger residuals, resulting in wider intervals
- Core assumption: Model entropy correlates with genuine epistemic uncertainty; entropy is not systematically miscalibrated
- Evidence anchors: [abstract] "The length of the intervals reflects the model's predictive uncertainty"; [Section 4.3] Equation (18) defines entropy-normalized measure; Figure 2 shows interval length increases with prediction error
- Break condition: If UnKGE model is systematically overconfident (low entropy but high error), intervals will be inappropriately narrow

### Mechanism 3: Exchangeability-Based Statistical Validity Guarantees
- Claim: Under exchangeability, constructed prediction intervals provably contain ground truth with probability ≥α
- Mechanism: Exchangeability ensures nonconformity scores from calibration and test are invariant under permutation. (ℓ+1) possible ranks of test score are equally likely, so probability it falls within ⌈α(ℓ+1)⌉ smallest is ≥α
- Core assumption: Weighted triples in T and test triple are exchangeable (holds under i.i.d.)
- Evidence anchors: [abstract] "guaranteed to contain the true score with a user-specified level of confidence"; [Proposition 1, Section 4.2.2] Formal proof of conservative coverage and asymptotic exact validity
- Break condition: Under distribution shift (e.g., synthetically generated negative triples), exchangeability is violated and coverage drops

## Foundational Learning

- **Knowledge Graph Embeddings (KGE)**: Understanding how UnKGE methods extend KGE to predict continuous confidence scores is prerequisite. Quick check: Can you explain how UKGE maps a triple's plausibility score to [0,1]?

- **Conformal Prediction Fundamentals**: The entire UNKGCP framework rests on conformal prediction's validity guarantees and role of nonconformity measures. Quick check: What is the difference between transductive and inductive conformal prediction, and why does ICP avoid retraining?

- **Exchangeability vs. I.I.D.**: The validity guarantees rely on exchangeability, a weaker assumption than i.i.d.; understanding this distinction clarifies when guarantees hold or break. Quick check: If calibration triples are synthetically corrupted negatives (different distribution from positives), does exchangeability hold?

## Architecture Onboarding

- **Component map**: Data Partitioner -> UnKGE Backbone -> Nonconformity Scorer -> Threshold Selector -> Interval Constructor

- **Critical path**: Data partition → UnKGE training → Calibration scoring → Threshold selection → Test interval construction. The threshold selection is the only step that depends on α

- **Design tradeoffs**:
  - Larger calibration set → more stable thresholds but less training data for UnKGE
  - Entropy normalization improves conditionality but assumes entropy reflects true uncertainty
  - Conservative validity (≥α) vs. exact validity (requires large ℓ and no ties)

- **Failure signatures**:
  - Coverage systematically below α → likely distribution shift or insufficient calibration data
  - Intervals uniformly wide regardless of query difficulty → entropy normalization not working or model overconfident everywhere
  - High variance in coverage across runs → calibration set too small

- **First 3 experiments**:
  1. Sanity check: Reproduce Table 1 on one dataset-backbone pair; verify coverage ≥α and compare sharpness of UNKGCP vs. baseline CP
  2. Conditionality test: Plot interval length vs. prediction error (replicate Figure 2); confirm positive correlation
  3. Calibration sensitivity: Vary calibration set size (10, 20, 40... triples) and plot coverage/sharpness curves (replicate Figure 3); identify minimum stable calibration size

## Open Questions the Paper Calls Out

- **Handling interval-valued confidence scores**: How can the framework be extended to handle interval-valued confidence scores in the input graph rather than single scalar values? [explicit] Current method assumes single-valued scores; proposes challenge of extending model to handle cases where confidence is expressed as intervals. [Unresolved] Current nonconformity measure and calibration step are designed for scalar distributions. [Evidence needed] Modified procedure representing input intervals by mean and length, analyzing their joint distribution.

- **Preserving guarantees under distribution shift**: How can formal statistical guarantees be preserved when applying the framework to synthetically generated negative triples that violate the i.i.d. assumption? [explicit] Section 6 highlights i.i.d. assumption is easily violated during negative sampling, resulting in loss of formal validity guarantees. [Unresolved] Validity proofs rely on exchangeability, broken by systematic generation process of negative samples. [Evidence needed] Derivation of theoretical bound or integration of weighted conformal prediction techniques.

- **Decomposing aleatoric and epistemic uncertainty**: Can prediction intervals be decomposed to distinguish between dataset noise (aleatoric uncertainty) and model inadequacy (epistemic uncertainty)? [inferred] Section 7 notes large average interval lengths on CN15k indicate predictions are "rather random," suggesting limitations in "either the dataset quality or model expressiveness." [Unresolved] Current framework provides single aggregate interval reflecting total uncertainty without mechanisms to attribute uncertainty source. [Evidence needed] Methodological extension correlating interval width with specific error modes or providing separate bounds for data noise versus model error.

## Limitations

- Coverage guarantees rely on exchangeability assumption, which breaks under distribution shift (e.g., synthetic negative triples)
- Entropy-normalized nonconformity measure assumes model entropy correlates with true epistemic uncertainty, which may not hold for systematically overconfident models
- Calibration set size critically affects threshold stability—too small (<10 triples) leads to high variance in coverage and sharpness

## Confidence

- **High confidence**: Coverage guarantees under exchangeability (Proposition 1), efficiency of inductive conformal prediction (avoiding retraining), empirical coverage ≥α on in-distribution test sets
- **Medium confidence**: Entropy normalization improves conditionality (based on Figure 2 correlation), UNKGCP outperforms baseline CP in sharpness (Table 1), coverage drop on negatives indicates distribution shift detection
- **Low confidence**: Absolute sharpness values across datasets are directly comparable (different normalization schemes), entropy always reflects true uncertainty (systematic overconfidence cases untested)

## Next Checks

1. **Distribution shift sensitivity**: Test coverage on artificially corrupted calibration sets (e.g., injected noise, adversarial triples) to quantify exchangeability violation effects
2. **Entropy calibration**: Compare entropy values against actual prediction error distributions to verify correlation and identify systematic overconfidence patterns
3. **Calibration set scaling**: Systematically vary calibration set size (5, 10, 20, 40, 80 triples) and plot coverage/sharpness curves to identify minimum stable calibration threshold