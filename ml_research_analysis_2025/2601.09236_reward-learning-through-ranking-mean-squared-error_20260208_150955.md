---
ver: rpa2
title: Reward Learning through Ranking Mean Squared Error
arxiv_id: '2601.09236'
source_url: https://arxiv.org/abs/2601.09236
tags:
- reward
- learning
- feedback
- each
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reward design in reinforcement
  learning by proposing a novel method, R4, that learns reward functions from human
  ratings rather than manually engineered rewards. R4 uses a ranking mean squared
  error (rMSE) loss that treats ratings as ordinal targets, enabling it to leverage
  richer feedback compared to binary preferences.
---

# Reward Learning through Ranking Mean Squared Error

## Quick Facts
- **arXiv ID:** 2601.09236
- **Source URL:** https://arxiv.org/abs/2601.09236
- **Reference count:** 40
- **Primary result:** Novel R4 method learns reward functions from trajectory ratings using ranking MSE loss, outperforming existing rating and preference-based RL methods while requiring less feedback

## Executive Summary
This paper addresses the challenge of reward design in reinforcement learning by proposing R4, a method that learns reward functions from human ratings rather than manually engineered rewards. R4 uses a ranking mean squared error (rMSE) loss that treats ratings as ordinal targets, enabling it to leverage richer feedback compared to binary preferences. The method employs a differentiable sorting operator to rank trajectories by predicted returns and minimizes the MSE between these ranks and teacher-provided ratings. Theoretically, R4 is proven to be minimal and complete under mild assumptions, distinguishing it from prior rating-based approaches. Empirically, R4 is evaluated in both offline and online feedback settings across robotic locomotion tasks from OpenAI Gym and DeepMind Control Suite, consistently matching or outperforming existing rating and preference-based RL methods while requiring significantly less feedback.

## Method Summary
R4 learns reward functions from trajectory ratings by computing predicted returns for each trajectory, ranking them using a differentiable sorting operator to produce continuous "soft ranks," and minimizing the mean squared error between these soft ranks and the teacher-provided ratings. The method operates in both offline settings (where a fixed dataset of trajectories and ratings is provided) and online settings (where the policy generates trajectories that are rated by a teacher and used to update the reward model). R4 uses an ensemble of three reward models with L2 regularization, batch size 64, and Soft Actor-Critic (SAC) as the policy optimization algorithm. The differentiable sorting operator enables gradient-based optimization over ranking objectives by making the sorting operation differentiable, while the rMSE loss preserves intra-class diversity and enforces only correct ordering across rating classes.

## Key Results
- R4 matches or outperforms existing rating-based (RbRL) and preference-based (PEBBLE) methods across multiple environments while requiring less feedback
- R4 is robust to the number of rating classes and exhibits better performance under high label noise compared to RbRL
- The method achieves superior or comparable performance in both offline and online feedback settings
- R4's theoretical guarantees of minimality and completeness distinguish it from prior rating-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The rMSE loss enables gradient-based optimization over ranking objectives by making the sorting operation differentiable
- **Mechanism:** A differentiable sorting operator (Blondel et al., 2020) produces continuous "soft ranks" rather than discrete hard ranks. These soft ranks approximate true ordinal positions while remaining smooth enough for gradient propagation. The MSE between soft ranks and teacher ratings provides a supervised learning signal that adjusts the reward model's parameters.
- **Core assumption:** The soft ranking operator approximates true ranks with bounded error (Assumption 5: ϵ < √(2n−2)/(n−2))
- **Evidence anchors:**
  - [abstract] "employs a differentiable sorting operator to rank trajectories by predicted returns and minimizes the MSE between these ranks and the teacher-provided ratings"
  - [Section 4.1] "Since the soft ranks are differentiable with respect to the reward parameters θ, minimizing this loss allows the model to adjust r̂θ to better align with the ratings"
- **Break condition:** If regularization strength for soft ranking is poorly tuned, soft ranks deviate significantly from true ranks (see Appendix A.4 and Figure 15 showing sensitivity to regularization)

### Mechanism 2
- **Claim:** Treating ratings as ordinal targets preserves intra-class diversity and avoids collapsing all trajectories in a class to a single midpoint value
- **Mechanism:** Unlike prior RbRL cross-entropy loss, which encourages predicted returns to concentrate at class midpoints (Bi + Bi+1)/2, rMSE only enforces correct ordering across classes. Trajectories within the same class can have varied predicted returns as long as the ranking constraint is satisfied.
- **Core assumption:** Teacher ratings reflect true ordinal quality relationships: if c(τa) < c(τb), then G*(τa) < G*(τb) under the teacher's implicit reward
- **Evidence anchors:**
  - [Section 4.1] "The RbRL objective encourages all trajectories in a class to have predicted returns close to the midpoint... In contrast, the rMSE objective does not enforce such a constraint"
  - [Proposition 1] Shows r* is in the rMSE solution set but not guaranteed to be in RbRL's solution set
- **Break condition:** If teacher ratings are inconsistent (e.g., due to noise or subjective variance), the ordinal assumption is violated. Paper shows robustness up to 80% noise (Figure 12), but higher corruption breaks alignment

### Mechanism 3
- **Claim:** The rMSE objective's solution set is provably minimal and complete under mild assumptions
- **Mechanism:** Minimality means no other objective can produce a smaller solution set without additional assumptions; completeness means all valid reward functions consistent with the rating ordering are included. This is achieved because rMSE's loss is zero if and only if predicted returns preserve the exact class ordering.
- **Core assumption:** Deterministic reward realizability (r* exists in hypothesis class), binning by return intervals, and exact (or bounded-error) differentiable ranking
- **Evidence anchors:**
  - [abstract] "theoretically, R4 is proven to be minimal and complete under mild assumptions, distinguishing it from prior rating-based approaches"
  - [Theorem 1] "rθ ∈ R ⇐⇒ rθ ∈ arg minθ LrMSE(θ), ∀rθ"
  - [Theorem 2] Extends guarantees under relaxed ranking error bounds
- **Break condition:** If the hypothesis class cannot represent r* (Assumption 3 violated), completeness fails—some valid reward functions will be unreachable

## Foundational Learning

- **Concept: Markov Decision Processes without Rewards (MDP\R)**
  - Why needed here: R4 operates in settings where no engineered reward exists; the goal is to infer one from trajectory ratings
  - Quick check question: Can you explain why the discount factor γ is fixed rather than learned in reward learning?

- **Concept: Differentiable Sorting / Soft Ranking**
  - Why needed here: Standard sorting is non-differentiable; soft ranking enables backpropagation through ranking operations
  - Quick check question: Given input values [3.2, 1.0, 4.5], what would soft ranks approximately output versus hard ranks?

- **Concept: Ordinal vs. Categorical Label Treatment**
  - Why needed here: R4 treats ratings as ordered (ordinal) rather than unordered classes, exploiting the structure that "good" > "neutral" > "bad."
  - Quick check question: Why would treating ratings as unordered categories (standard cross-entropy) waste information?

## Architecture Onboarding

- **Component map:** Reward Model -> Return Aggregator -> Soft Rank Module -> rMSE Loss -> Backprop to Reward Model -> RL Policy
- **Critical path:** Trajectory sampling → Return prediction → Soft ranking → Loss computation → Backprop to reward model → Policy training on learned rewards. In online setting: policy generates new trajectories → teacher rates → reward model updates → replay buffer relabeling.
- **Design tradeoffs:**
  - **Batch size B:** Larger batches reduce gradient variance but increase memory; paper uses B=64
  - **Ranking regularization strength:** Too low → soft ranks don't approximate true ranks; too high → gradients vanish. Paper finds 0.065–1.0 works (Figure 15)
  - **Number of rating classes:** R4 is robust to this choice (Figure 13); RbRL is sensitive
- **Failure signatures:**
  - Loss doesn't decrease: Check soft rank regularization; may be too weak/strong
  - Policy learns unintended behaviors: Check rating quality; noise >80% degrades performance significantly
  - Reward model collapses to constant: Check L2 regularization coefficient β (paper uses 0.005–0.01)
- **First 3 experiments:**
  1. **Offline sanity check:** Train R4 on Reacher with simulated ratings; compare policy trained on learned reward vs. environment reward. Verify R4 matches baseline with fewer labeled trajectories.
  2. **Ablation on soft ranking:** Replace soft ranks with hard ranks (non-differentiable) using a non-gradient optimizer; confirm performance drops or optimization fails.
  3. **Noise robustness test:** Inject label noise (randomly flip 10–80% of ratings); compare R4 vs. RbRL degradation curves to validate claimed robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does R4 maintain its theoretical and empirical advantages over preference-based methods in large-scale studies with real human raters who exhibit high variance and non-uniform rating distributions?
- Basis: [inferred] The paper relies primarily on simulated teachers, and the human pilot study (Section B.1) was limited to N=5 participants, revealing substantial noise and imperfections in the ratings (e.g., overlapping returns) that were not present in the main experiments.
- Why unresolved: The theoretical guarantees (Assumptions 1-3) assume consistent binning based on ground-truth returns, but human "binning" is noisy and subjective. It is unclear if the rMSE loss is robust to the specific types of labeling errors humans introduced in the pilot (Figure 6-10).
- What evidence would resolve it: A user study with statistical power comparing R4 against preference-based baselines (e.g., PEBBLE) using real human feedback, analyzing the correlation between human label noise and performance degradation.

### Open Question 2
- Question: Can the heuristic "dynamic rating class" mechanism be formalized to automatically track human "response shift" without requiring manual tuning of merging thresholds or bin ranges?
- Basis: [inferred] Section 4.2 discusses "response shift and recalibration," noting that humans adjust internal standards over time. The paper implements dynamic classes heuristically (merging bins, introducing new ones), but does not provide a rigorous method for detecting *when* a shift occurs.
- Why unresolved: The current implementation appears to rely on environment-specific knowledge or manual intervention to decide when to merge or split bins. A general solution would need to infer the teacher's current internal scale solely from their feedback history.
- What evidence would resolve it: An algorithm that adaptively updates bin structures based on incoming ratings and demonstrates stability across environments with different reward scales, outperforming the current heuristic approach.

### Open Question 3
- Question: How does the regularization strength of the differentiable sorting operator impact the tightness of the theoretical completeness guarantees (Theorem 2) in high-dimensional function approximation settings?
- Basis: [inferred] Theorem 2 requires Assumption 5 (Bounded Ranking Error, $\epsilon < \frac{\sqrt{2n-2}}{n-2}$). However, Figure 15 shows that the regularization strength of the soft ranking operator significantly impacts policy performance, suggesting the "softness" of the rank might violate the bounded error assumption if tuned incorrectly.
- Why unresolved: The paper proves the bounds exist but does not theoretically or empirically map the relationship between the optimizer's hyperparameters (regularization strength) and the bound $\epsilon$ required for the solution set $R_{rMSE}$ to remain valid.
- What evidence would resolve it: An empirical or theoretical analysis linking specific regularization strengths to the resulting ranking error $\epsilon$, showing where the learned reward function falls outside the guaranteed solution set.

## Limitations
- Theoretical guarantees rely on strong assumptions (deterministic reward realizability, bounded soft ranking error) that may not hold in practice
- Performance with real human raters exhibiting high variance and non-uniform rating distributions remains untested at scale
- Without code release, reproducing exact hyperparameter choices (particularly soft rank regularization strength and dynamic bin thresholds) is challenging

## Confidence
- Theoretical minimality/completeness guarantees: **Medium** (proofs are provided but rely on strong assumptions)
- Empirical performance superiority: **High** (multiple environments, multiple baselines, consistent results)
- Robustness to rating class granularity and noise: **Medium-High** (shown in ablation studies but within tested ranges)

## Next Checks
1. Test R4 performance on environments with different state-action dimensionalities and dynamics to verify generalizability beyond the tested locomotion tasks
2. Evaluate the method's sensitivity to the choice of differentiable sorting algorithm (Blondel et al., 2020 vs. alternative approaches) to isolate the contribution of soft ranking
3. Conduct a systematic study of teacher rating consistency by varying the number of raters and measuring performance degradation to quantify the method's robustness to human subjectivity