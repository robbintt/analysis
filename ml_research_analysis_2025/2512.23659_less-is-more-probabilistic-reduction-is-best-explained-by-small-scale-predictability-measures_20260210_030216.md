---
ver: rpa2
title: 'Less is more: Probabilistic reduction is best explained by small-scale predictability
  measures'
arxiv_id: '2512.23659'
source_url: https://arxiv.org/abs/2512.23659
tags:
- language
- words
- word
- reduction
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that n-gram statistics, not large language
  models, better explain probabilistic reduction in phonetic duration. Across four
  speech corpora, n-gram probabilities derived from short-range phrase sequences provided
  superior model fits to word durations compared to long-range LLM probabilities.
---

# Less is more: Probabilistic reduction is best explained by small-scale predictability measures

## Quick Facts
- **arXiv ID**: 2512.23659
- **Source URL**: https://arxiv.org/abs/2512.23659
- **Reference count**: 20
- **Primary result**: N-gram statistics outperform large language models in explaining phonetic duration reduction across four speech corpora.

## Executive Summary
This study compares n-gram and large language model (LLM) probabilities for predicting word duration reduction in spontaneous speech. Across four corpora, n-gram probabilities derived from short-range phrase sequences provided superior model fits compared to long-range LLM probabilities. LLM probabilities were confounded with utterance position and failed to show consistent reduction effects. The findings support incremental, phrase-based planning and retrieval as the cognitive mechanism underlying probabilistic reduction, rather than whole-utterance probability computation.

## Method Summary
The study segments speech corpora into inter-pause units and extracts word duration, frequency, phoneme count, and relative position. Two types of predictability measures are computed: n-gram probabilities from corpus text and fine-tuned Pythia-160M LLM probabilities. Mixed-effects regression models predict log word duration using these predictors along with lexical covariates. Model comparison focuses on ΔLog-Likelihood and coefficient directionality between n-gram and LLM-based predictors across four speech corpora.

## Key Results
- N-gram probabilities consistently outperformed LLM probabilities in predicting word duration reduction
- LLM probabilities were confounded with utterance position, leading to worse model fits
- Effect sizes of probabilistic reduction were extremely small compared to prosodic factors
- Fine-tuning Pythia-160M on dialect-specific corpora did not improve predictive power over n-grams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local, phrase-level predictability from n-grams is a better and more cognitively plausible predictor of word duration than long-range LLM probabilities.
- Mechanism: Speakers incrementally plan and retrieve multiword sequences (phrases) from memory. Frequent local transitions (bigrams/trigrams) are stored as chunks; when retrieved together, they promote articulatory fluency and reduction in duration.
- Core assumption: Production planning is incremental and phrase-based, not whole-utterance; speakers do not encode or use full sentential context to estimate probabilities.
- Evidence anchors:
  - [abstract] "They demonstrate that LLM probabilities are confounded with utterance position... and that the effect of predictability on duration is extremely small. The results support multiword phrase representations as the plausible cognitive units for production..."
  - [section] Introduction (pg 2-3) and General Discussion (pg 21-24) argue for incremental, phrase-based planning and retrieval.
  - [corpus] arXiv 2506.09641 directly compares NDL and information-theoretic predictors for probabilistic reduction, supporting the relevance of alternative statistical models beyond LLMs.
- Break condition: If critical duration effects are shown to depend on dependencies spanning >3-4 words that n-grams cannot capture, the sufficiency claim would weaken.

### Mechanism 2
- Claim: LLM probabilities are confounded with utterance position, which degrades their ability to predict duration independently of prosodic effects.
- Mechanism: LLMs implicitly encode positional structure; words at utterance edges receive systematically different probability scores. Since final words are also lengthened prosodically, this correlation introduces a confound that weakens the LLM-duration relationship.
- Core assumption: Prosodic factors like phrase-final lengthening and anticipatory shortening are major determinants of duration and are partially aligned with positional patterns learned by LLMs.
- Evidence anchors:
  - [abstract] "They demonstrate that LLM probabilities are confounded with utterance position, leading to worse model fits..."
  - [section] Study 2 (pg 11-12, Figure 3) visualizes the complex non-linear relationship between position, LLM probability, and duration.
  - [corpus] Direct corpus evidence for this specific confound is weak; claim is primarily supported by internal re-analysis.
- Break condition: If positional confounds can be explicitly modeled or removed from LLM representations, their predictive power for duration may improve.

### Mechanism 3
- Claim: The effect size of probabilistic reduction on word duration is extremely small.
- Mechanism: Predictability influences duration, but the magnitude is minute (single-digit milliseconds), often dwarfed by prosodic and lexical factors. Standard model comparison metrics (ΔLL) do not convey effect size or direction, masking the practical insignificance.
- Core assumption: Significant improvements in model fit do not imply large or interpretable effects on the outcome variable.
- Evidence anchors:
  - [abstract] "...and that the effect of predictability on duration is extremely small."
  - [section] Effect size of probabilistic reduction (pg 17-20, Figure 7) shows coefficient magnitudes for predictability are an order of magnitude smaller than position or phoneme count.
  - [corpus] External corpus support for exact effect sizes is limited; the claim is based on internal re-analysis across four corpora.
- Break condition: If substantial predictability-duration effects are found in other languages, speech styles, or experimental settings, the "extremely small" generalization may not hold.

## Foundational Learning

### Concept: Probabilistic reduction
- Why needed here: Central phenomenon; the paper debates what statistical representations best explain it.
- Quick check question: Can you name two factors that influence word duration independently of predictability?

### Concept: N-gram vs. neural language models
- Why needed here: Core comparison; n-grams use fixed local context, LLMs use long-range dependencies.
- Quick check question: What is a key architectural difference between n-gram and transformer LLMs regarding context handling?

### Concept: Effect size vs. statistical significance
- Why needed here: The paper warns that model comparison metrics can hide tiny effect sizes.
- Quick check question: If a predictor significantly improves model log-likelihood, does that guarantee a large effect on duration?

## Architecture Onboarding

### Component map:
- Word tokens from pause-segmented inter-pause units
- → Lexical features (frequency, phoneme count)
- → Predictability measures (n-gram or LLM probabilities)
- → Mixed-effects regression model
- → Log word duration prediction

### Critical path:
1. Segment speech corpora into inter-pause intervals
2. Extract duration, position, lexical covariates
3. Compute forward/backward probabilities (n-gram or LLM)
4. Fit regression models; compare LLM vs. n-gram fit and coefficient directionality

### Design tradeoffs:
- n-gram: Lower compute, better fit, cognitively plausible but ignores long-distance dependencies
- LLM: Higher compute, worse fit due to position confound, but captures long-range context
- Fine-tuned LLM: May reduce dialect bias but not necessary for observing reduction

### Failure signatures:
- LLM coefficients show positive or inconsistent signs (lengthening instead of reduction)
- Model fits improve but coefficients are tiny or directionally wrong
- Non-linear patterns in LLM probability vs. position indicate confounding

### First 3 experiments:
1. Replicate duration regression with n-gram probabilities on a new speech corpus; compare fit and coefficients to an LLM-based predictor
2. Plot LLM probability vs. relative position to visualize confounding
3. Compare standardized coefficient magnitudes of position, phoneme count, and predictability to assess relative effect sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does label noise in corpus annotations negatively impact statistical power to detect differences in probabilistic reduction patterns between content and function words?
- Basis in paper: [explicit] The authors note that "future work should determine the extent to which label noise negatively impacts statistical power" given that dataset categories contain inconsistencies, such as pronouns being coded as content words.
- Why unresolved: The paper identifies significant inconsistencies in the Switchboard-NXT dataset's content/function distinctions (e.g., pronouns appearing in both categories), but the re-analysis did not fully disentangle the impact of these errors from the underlying effects.
- What evidence would resolve it: A study using a dataset with manually verified, strictly defined word class annotations, or a controlled experiment where word class is a manipulated variable rather than an observed one.

### Open Question 2
- Question: Does probabilistic reduction primarily serve a resource conservation function for the speaker, or is it an audience design effect for the listener?
- Basis in paper: [explicit] The authors state that the link between shortening durations and resource conservation "is less than clear, and further experimentation is needed to test this alternative explanation."
- Why unresolved: The study confirms that reduction occurs and is predicted by n-grams, but the correlational nature of the data cannot distinguish between the "rational production" hypothesis (helping the listener) and the "ease of retrieval" hypothesis (helping the speaker).
- What evidence would resolve it: Experiments that measure articulatory effort or planning load independently of the listener's needs, or studies that manipulate the communicative context to see if reduction is modulated by the presence or absence of a listener.

### Open Question 3
- Question: How do speakers efficiently estimate a word's probability in context during production if they do not use large-scale language model computations?
- Basis in paper: [explicit] The authors state, "it is unknown what knowledge speakers draw on during language production, and how a speaker might efficiently estimate a word’s probability in context."
- Why unresolved: While the paper demonstrates that local n-grams are better predictors than LLMs, the specific cognitive mechanism or algorithm speakers use to compute or retrieve these probabilities on the fly remains unconfirmed.
- What evidence would resolve it: Computational cognitive modeling that implements the proposed sparse network retrieval mechanism (described on page 23-24) to see if it predicts reaction times and durations as accurately as the n-gram regressions.

### Open Question 4
- Question: Are the observed reduction effects driven by conditional probability or by the surface frequency of multi-word sequences (bigrams/trigrams)?
- Basis in paper: [inferred] The authors discuss on page 22 that what appears to be sensitivity to conditional probabilities "may instead reflect other processes... [like] bigram or trigram frequency," creating an ambiguity in the underlying cause.
- Why unresolved: Conditional probabilities and phrase frequencies are often highly correlated in natural corpora, making it difficult to isolate which statistical variable is the actual driver of phonetic reduction.
- What evidence would resolve it: Factorial experimental designs or targeted corpus analyses where phrase frequency and conditional probability are orthogonally manipulated to observe their independent effects on duration.

## Limitations

- LLM positional confound is primarily supported by internal re-analysis rather than independent validation
- Effect size quantification relies on internal comparisons that may not generalize across languages
- Choice of Pythia-160M may not represent full spectrum of modern language models
- Fine-tuning impact on reducing dialect bias remains unclear

## Confidence

- **High confidence**: N-gram probabilities consistently outperform LLM probabilities in model fit across multiple corpora; effect sizes of probabilistic reduction are small compared to prosodic factors
- **Medium confidence**: The specific positional confound affecting LLM probabilities; the sufficiency of phrase-shaped representations for explaining reduction
- **Low confidence**: Generalizability of effect sizes to other languages or experimental settings; whether newer or larger LLMs would show different patterns

## Next Checks

1. Replicate the positional confound analysis using a different LLM architecture (e.g., GPT-2 or OPT) to test whether the confound is model-specific or general to transformer-based approaches
2. Conduct an experimental production study with controlled predictability conditions to verify whether observed effect sizes match those found in spontaneous speech corpora
3. Test the n-gram sufficiency claim on a morphologically rich language where long-distance dependencies may be more critical for prediction