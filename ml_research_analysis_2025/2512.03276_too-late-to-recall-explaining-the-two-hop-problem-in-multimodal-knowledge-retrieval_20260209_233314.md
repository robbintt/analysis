---
ver: rpa2
title: 'Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge
  Retrieval'
arxiv_id: '2512.03276'
source_url: https://arxiv.org/abs/2512.03276
tags:
- factual
- entity
- recall
- llav
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLMs trained on vision-language tasks show degraded factual recall
  compared to their LLM backbones, as visual representations bypass early MLP layers
  critical for retrieving factual knowledge. By analyzing 14 VLMs across different
  architectures and sizes, the study finds that 11 models underperform their LLM counterparts
  on factual recall tasks.
---

# Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval

## Quick Facts
- arXiv ID: 2512.03276
- Source URL: https://arxiv.org/abs/2512.03276
- Reference count: 38
- Primary result: VLMs trained on vision-language tasks show degraded factual recall compared to their LLM backbones

## Executive Summary
VLMs consistently underperform their LLM backbones on factual recall tasks because visual representations bypass early MLP layers critical for retrieving factual knowledge. Through benchmarking 14 VLMs against their LLM counterparts, the study found that 11 models show degraded factual accuracy. Mechanistic analysis reveals that visual entities are resolved too late in the forward pass, missing early-layer factual recall mechanisms. Patching early MLP outputs from LLMs into VLMs recovers factual accuracy, while chain-of-thought prompting mitigates performance gaps. The findings highlight that effective multimodal alignment requires early integration of visual representations into the LLM's factual recall circuit.

## Method Summary
The study benchmarks 14 VLMs against their LLM backbones on 15,000 multimodal factual-recall questions. For each VLM/LLM pair, 1000 valid samples are collected where the VLM correctly identifies the entity. Attribution patching with Grad×Delta is performed on 100 correct examples per model, comparing per-layer attribution patterns between VLMs and LLM backbones. Heuristic activation patching of early MLP outputs from LLMs is applied to VLMs. Layer-wise linear probing is used to track entity representation emergence across layers. Chain-of-thought evaluation tests inference-time reasoning capabilities. Noise multiplier α is ablated per model (values 1-6) to achieve target KL divergence of ~2.

## Key Results
- 11 out of 14 VLMs underperform their LLM counterparts on factual recall accuracy
- Visual entity representations emerge significantly later in VLMs compared to LLMs
- Patching early MLP outputs from LLMs into VLMs recovers factual accuracy
- Chain-of-thought prompting closes performance gaps in larger VLMs but degrades performance in smaller models

## Why This Works (Mechanism)
VLMs degrade factual recall because visual representations bypass early MLP layers where factual knowledge is retrieved. The "two-hop problem" occurs when visual entities are resolved too late in the forward pass, missing the MLP circuits that handle factual recall in LLMs. This timing misalignment prevents visual information from engaging the factual recall mechanisms that exist in early layers of the LLM backbone.

## Foundational Learning
- **Multimodal knowledge retrieval**: Combining visual and textual information to answer questions about entities in images - needed to understand the core problem of integrating vision with language models.
- **Attribution patching**: A mechanistic interpretability technique using gradient × activation differences to identify which components contribute most to model outputs - needed to diagnose where VLMs fail compared to LLMs.
- **Chain-of-thought prompting**: A prompting technique that encourages models to reason step-by-step before answering - needed to understand why this approach helps mitigate the two-hop problem in larger models.
- **Linear probing**: A method to measure how well representations at different layers encode specific information by training simple classifiers on top - needed to track when entity representations emerge during the forward pass.
- **MLP vs attention layers**: Different neural network components with distinct computational roles - needed to understand why early MLP layers are critical for factual recall.
- **Gaussian noise corruption**: A technique to test model robustness and identify important components by adding controlled noise - needed for the attribution analysis.

## Architecture Onboarding

**Component Map**
VLM Backbone -> Early MLP Layers -> Attention Layers -> Late MLP Layers -> Output

**Critical Path**
Image Input -> Visual Encoder -> Early MLP Layers (bypassed) -> Attention Layers -> Late MLP Layers -> Factual Recall

**Design Tradeoffs**
- Bypass early MLP layers for speed vs. maintain early MLP engagement for factual accuracy
- Large-scale multimodal training vs. adapter-based approaches
- Direct visual integration vs. chain-of-thought reasoning as mitigation strategies

**Failure Signatures**
- Delayed emergence of entity representations in layer-wise probing
- Lower attribution scores in early MLP layers compared to LLM backbones
- Consistent factual recall degradation across multiple VLM architectures
- Performance recovery when early MLP outputs are patched from LLMs

**3 First Experiments**
1. Run layer-wise linear probing to measure entity representation emergence timing in both VLM and LLM
2. Perform attribution patching with Grad×Delta to identify critical sublayers for factual recall
3. Test chain-of-thought prompting across different model sizes to observe performance variations

## Open Questions the Paper Calls Out

### Open Question 1
Does the "two-hop" timing failure affect other high-level cognitive circuits in the LLM backbone, such as in-context learning or multi-step reasoning, or is it specific to factual recall?

The study strictly benchmarks factual recall tasks and does not test other distinct mechanistic circuits known to exist in LLMs. Replicating the attribution patching and probing experiments on tasks requiring induction heads or logical reasoning would determine if similar delays occur in other circuits.

### Open Question 2
Can adapter mechanisms be explicitly regularized or trained to shift visual entity resolution to earlier layers without requiring the massive compute of native pretraining?

The paper demonstrates that current adapters fail to align early, while successful models rely on "massive multimodal fine-tuning" or native training. Training VLMs with a loss term penalizing late emergence of entity probes could test whether efficient adapter-only solutions are possible.

### Open Question 3
Why does chain-of-thought prompting recover performance in large VLMs but fail or degrade performance in smaller models?

The authors observe this model-dependent effect but do not identify the mechanistic cause. Analyzing attention patterns during CoT generation in smaller vs. larger models could reveal whether smaller models fail to attend effectively to generated text entity tokens.

## Limitations
- Attribution patching uses ad hoc heuristic interventions rather than theoretically grounded approaches
- Analysis focuses on specific VLM architectures (primarily LLaVA and Qen variants) and factual recall tasks
- Chain-of-thought prompting shows inconsistent results across model sizes without clear explanation

## Confidence
- Factual recall degradation in VLMs vs LLMs: High
- Late emergence of visual entity representations: Medium
- MLP early layers as critical for factual recall: Medium
- Chain-of-thought as effective mitigation: Low (based on limited evidence)
- Two-hop problem as the primary explanation: Medium

## Next Checks
1. Test the proposed mechanism on additional VLM architectures beyond LLaVA and Qwen variants, particularly those using different visual encoding strategies
2. Validate whether the early-layer MLP importance holds for other knowledge-intensive tasks beyond factual recall
3. Conduct ablation studies varying the timing of visual representation integration to test whether earlier integration truly improves factual recall performance