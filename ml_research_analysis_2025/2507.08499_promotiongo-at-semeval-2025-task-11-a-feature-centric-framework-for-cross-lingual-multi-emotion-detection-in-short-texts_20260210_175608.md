---
ver: rpa2
title: 'PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual
  Multi-Emotion Detection in Short Texts'
arxiv_id: '2507.08499'
source_url: https://arxiv.org/abs/2507.08499
tags:
- fasttext
- tf-idf
- sbert
- languages
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a feature-centric framework for cross-lingual\
  \ multi-emotion detection in short texts, designed to dynamically adapt document\
  \ representations and learning algorithms for optimal language-specific performance.\
  \ The study evaluates three key components\u2014document representation, dimensionality\
  \ reduction, and model training\u2014across 28 languages, with detailed analysis\
  \ on five languages."
---

# PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts

## Quick Facts
- **arXiv ID:** 2507.08499
- **Source URL:** https://arxiv.org/abs/2507.08499
- **Reference count:** 8
- **Primary result:** Feature-centric framework achieves strong cross-lingual emotion detection across 28 languages, with TF-IDF excelling for low-resource languages

## Executive Summary
This paper introduces a feature-centric framework for cross-lingual multi-emotion detection in short texts, dynamically adapting document representations and learning algorithms for optimal language-specific performance. The study evaluates three key components—document representation, dimensionality reduction, and model training—across 28 languages, with detailed analysis on five languages. Results show that TF-IDF remains highly effective for low-resource languages, while contextual embeddings like FastText and transformer-based representations (e.g., Sentence-BERT) exhibit language-specific strengths. Principal Component Analysis (PCA) reduces training time without compromising performance, particularly benefiting FastText and neural models such as Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores the trade-off between model complexity and processing cost.

## Method Summary
The framework processes raw text through tokenization, document representation selection (TF-IDF, FastText, or SBERT), optional PCA dimensionality reduction, and multi-label classification using decision trees, KNN, random forests, SVM, voting ensembles, or MLP. Language family mapping approximates embeddings for unseen languages. Grid search optimizes hyperparameters per language. The system handles 6 emotion categories with binary classification per label.

## Key Results
- TF-IDF achieves highest F1-macro in 3/5 selected languages (Marathi: 0.7438, Spanish: 0.6561, Russian: 0.7107)
- PCA reduces TF-IDF+MLP training from 200.8s to 115.0s while improving FastText+Voting F1 from 0.4046 to 0.4407
- MLP consistently outperforms traditional ML and ensemble voting when paired with semantically rich embeddings (SBERT, FastText)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TF-IDF remains competitive with or superior to contextual embeddings for low-resource language emotion detection when training data is limited.
- **Mechanism:** TF-IDF computes term importance based on corpus-local frequency statistics rather than relying on external pretraining data. This reduces dependency on large-scale pretrained corpora that may underrepresent target languages.
- **Core assumption:** Emotional signals in low-resource languages can be captured through lexical frequency patterns without requiring deep semantic context from pretraining.
- **Evidence anchors:**
  - [abstract] "TF-IDF remains highly effective for low-resource languages"
  - [section 4.1/Table 2] TF-IDF achieves highest F1-macro in 3/5 selected languages (Marathi: 0.7438, Spanish: 0.6561, Russian: 0.7107)
  - [corpus] Neighbor papers (JNLP, UoB-NLP) similarly address low-resource challenges but use generative/adapters—no direct corpus comparison on TF-IDF vs. embeddings
- **Break condition:** If emotional expression heavily relies on word order, negation, or sarcasm, TF-IDF's bag-of-words assumption may fail regardless of language resource level.

### Mechanism 2
- **Claim:** PCA-based dimensionality reduction accelerates training for neural models (MLP) with minimal performance loss, but degrades TF-IDF performance.
- **Mechanism:** PCA projects high-dimensional sparse features into dense lower-dimensional space. Neural models (MLP) benefit from noise reduction and faster convergence; tree-based models (DT) and TF-IDF representations lose discriminative high-dimensional sparsity patterns critical for their decision boundaries.
- **Core assumption:** The principal components preserve emotion-relevant variance while discarding noise; this holds better for dense embeddings (FastText, SBERT) than sparse TF-IDF vectors.
- **Evidence anchors:**
  - [abstract] "PCA reduces training time without compromising performance, particularly benefiting FastText and neural models such as MLP"
  - [section 4.3/Table 4-5] PCA reduces TF-IDF+MLP training from 200.8s to 115.0s; FastText+Voting F1 improves from 0.4046 to 0.4407 with PCA; TF-IDF+Voting drops from 0.6561 to 0.3931
  - [corpus] No corpus evidence found on PCA effects in neighbor papers
- **Break condition:** If PCA components exceed ~95% variance threshold or emotion signals are distributed across many small-variance dimensions, critical affective nuance may be lost.

### Mechanism 3
- **Claim:** MLP classifiers consistently outperform traditional ML and ensemble voting when paired with semantically rich embeddings (SBERT, FastText).
- **Mechanism:** MLP's multi-layer architecture learns non-linear combinations of embedding dimensions, capturing emotion co-occurrence patterns that linear or tree-based models cannot. Grid search optimizes language-specific hyperparameters.
- **Core assumption:** Emotion labels have complex interdependencies that require non-linear decision boundaries; sufficient training data exists to fit MLP parameters without severe overfitting.
- **Evidence anchors:**
  - [section 4.2/Table 3] Performance hierarchy MLP > Voting > DT consistent across all 5 languages with SBERT (e.g., Marathi: 0.8389 vs. 0.6654 vs. 0.4275)
  - [section 2.3.2] "Grid Search to evaluate multiple parameter combinations and select the one that maximizes the F1-macro score"
  - [corpus] Lotus paper uses RoBERTa+Llama-3 explanations for multi-label; Team A uses multilingual models—different architectures, no direct MLP comparison
- **Break condition:** If training samples per language fall below ~1000-2000, MLP may overfit; if label imbalance is severe (e.g., Hindi anger: 78% negative), recall for minority emotions collapses.

## Foundational Learning

- **Concept: Multi-label classification with label co-occurrence**
  - **Why needed here:** Emotions are not mutually exclusive; a single text can express joy AND surprise simultaneously. Models must predict multiple binary labels per instance rather than a single class.
  - **Quick check question:** Can you explain why a softmax output layer is inappropriate for multi-label emotion detection?

- **Concept: Cross-lingual transfer via language family mapping**
  - **Why needed here:** Low-resource languages like Oromo lack pretrained embeddings. The system uses LLMs to identify linguistically similar supported languages and transfers their embeddings.
  - **Quick check question:** If Oromo maps to Somali via language family, what assumptions are being made about emotional expression across these languages?

- **Concept: Bias-variance trade-off in dimensionality reduction**
  - **Why needed here:** PCA reduces overfitting risk (variance) but may discard emotion-relevant features (bias). The paper shows this trade-off manifests differently across representation types.
  - **Quick check question:** Why does PCA help FastText+Voting performance but hurt TF-IDF+Voting?

## Architecture Onboarding

- **Component map:** Raw Text → Tokenization (GemmaTokenizer) → Document Representation (TF-IDF | FastText | SBERT) → Normalization → PCA (optional) → Classifier (DT | KNN | RF | SVM | Voting | MLP) → Multi-label Output (6 emotions)

- **Critical path:** Document representation choice → PCA configuration → MLP hyperparameters. Representation selection has the largest F1-macro variance (0.30-0.84 range).

- **Design tradeoffs:**
  | Representation | Speed | Low-resource Performance | Semantic Nuance |
  |----------------|-------|-------------------------|-----------------|
  | TF-IDF         | Fast  | High                    | Low             |
  | FastText       | Medium| Low-Medium              | Medium          |
  | SBERT          | Slow  | Medium-High             | High            |

- **Failure signatures:**
  - Very low F1 (<0.15): FastText on unseen languages without language family mapping
  - High specificity, low recall (>0.94, <0.30): Severe label imbalance (e.g., Hindi anger)
  - PCA causes >10% F1 drop: Applied to TF-IDF with tree-based classifiers

- **First 3 experiments:**
  1. **Baseline audit:** Run TF-IDF + MLP on 5 diverse languages (Marathi, Spanish, Hindi, Romanian, Russian) to establish F1-macro baseline; expect 0.61-0.76 range.
  2. **PCA ablation:** Compare TF-IDF+MLP vs. TF-IDF+MLP+PCA (n_components=0.95) on Spanish; expect ~2% F1 drop but 40%+ training time reduction.
  3. **Embedding comparison:** Test SBERT+MLP on Romanian and Hindi (where SBERT outperforms TF-IDF); verify >0.56 F1-macro threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training custom FastText word vectors for low-resource languages yield significantly better emotion detection performance than the current method of mapping unseen languages to linguistically similar supported languages via LLMs?
- **Basis in paper:** [explicit] The conclusion states the authors plan to "training FastText word vectors to improve representation quality, particularly for low-resource languages," contrasting with the current method described in Section 2.1.2.
- **Why unresolved:** The current system relies on approximating embeddings for unseen languages (e.g., Oromo) using LLM-mapped similar languages, which may introduce representational errors compared to native embeddings.
- **What evidence would resolve it:** A comparative study measuring F1-macro scores on low-resource languages (like Oromo or Tigrinya) using native FastText vectors versus the current LLM-mapped approximation approach.

### Open Question 2
- **Question:** What specific PCA configurations (variance thresholds or component counts) optimize the balance between training efficiency and semantic preservation for transformer-based representations like SBERT?
- **Basis in paper:** [explicit] The authors explicitly aim to focus on "determining optimal PCA configurations and evaluating its impact on the performance across different languages."
- **Why unresolved:** The ablation study (Section 4.3) indicates that while PCA speeds up training, it can strip "contextual nuances critical for emotion differentiation" in SBERT, reducing performance.
- **What evidence would resolve it:** Experiments varying the number of PCA components for SBERT features to identify the "elbow point" where efficiency gains no longer justify the loss in F1-macro score.

### Open Question 3
- **Question:** Can an automated selection mechanism for feature-classifier pairs improve upon the static "one-size-fits-all" or manual selection approach for specific languages?
- **Basis in paper:** [explicit] The conclusion proposes "optimizing feature-classifier selection for each language" as a primary goal for future work.
- **Why unresolved:** The results (Table 2) show high variance; TF-IDF excels for Marathi, while SBERT excels for Hindi. The paper currently lacks a mechanism to predict which pipeline suits a given language without brute-force testing.
- **What evidence would resolve it:** Implementation of a meta-learner or heuristic model that predicts the best feature-classifier combination based on language features (e.g., morphological complexity, dataset size) and outperforms the current manual selection.

## Limitations

- Low-resource language performance may degrade for truly unseen languages outside mapped language families
- Severe label imbalance in some languages (e.g., Hindi anger: 78% negative samples) may inflate F1 scores
- Computational efficiency claims don't account for full pipeline costs including embedding generation time

## Confidence

- **High confidence:** TF-IDF effectiveness for low-resource languages, PCA acceleration benefits for neural models, MLP superiority with semantically rich embeddings
- **Medium confidence:** Cross-lingual transfer via language family mapping, computational efficiency trade-offs, generalizability across 28 languages
- **Low confidence:** Handling of label imbalance effects, performance on truly unseen languages, real-world deployment feasibility

## Next Checks

1. **Imbalance stress test:** Apply SMOTE or class weighting to Marathi and Hindi datasets, then re-evaluate TF-IDF+MLP and SBERT+MLP F1-macro to quantify performance changes under balanced conditions.

2. **Language family boundary test:** Select an African language (e.g., Yoruba) outside the Indo-European/Finno-Ugric families, implement manual language mapping to a distant relative (e.g., Swahili), and measure performance degradation relative to mapped languages.

3. **End-to-end latency benchmark:** Measure total processing time (embedding generation + training + inference) for TF-IDF vs. SBERT pipelines on Spanish and Hindi to validate computational efficiency claims across the full workflow.