---
ver: rpa2
title: Gradient Multi-Normalization for Stateless and Scalable LLM Training
arxiv_id: '2502.06742'
source_url: https://arxiv.org/abs/2502.06742
tags:
- adam
- norm
- arxiv
- training
- norms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gradient Multi-Normalization for Stateless
  and Scalable LLM Training. The authors address the challenge of training large language
  models with memory-efficient optimizers by proposing a novel framework that normalizes
  stochastic gradients according to multiple norms.
---

# Gradient Multi-Normalization for Stateless and Scalable LLM Training

## Quick Facts
- arXiv ID: 2502.06742
- Source URL: https://arxiv.org/abs/2502.06742
- Authors: Meyer Scetbon; Chao Ma; Wenbo Gong; Edward Meeds
- Reference count: 40
- Primary result: SinkGD achieves 3× speedup over Adam with 3× less memory while maintaining competitive perplexity on LLaMA models

## Executive Summary
This paper introduces Gradient Multi-Normalization (MNGD) as a general framework for stateless and scalable LLM training. The authors propose alternating projection across multiple normalization constraints to produce memory-efficient gradient preprocessing. They demonstrate that the existing SWAN optimizer is a special case of their framework, then design SinkGD - a more efficient variant that relaxes SWAN's spectral normalization to column-wise normalization while maintaining memory efficiency. Experiments on LLaMA models show SinkGD achieves 3× speedup over Adam with significantly reduced memory requirements.

## Method Summary
The method combines MultiNorm gradient preprocessing with SGD to create a stateless optimizer. MultiNorm uses alternating projections across multiple norms (row-wise ℓ2 and column-wise ℓ2 for SinkGD) to normalize gradients without storing optimizer states. SR-Sinkhorn algorithm implements this efficiently through L iterations of alternating row and column scaling. The framework applies to linear layers in transformer blocks while using Adam for embeddings and normalization layers. The approach eliminates memory overhead from momentum and second-moment estimates while maintaining competitive performance.

## Key Results
- SinkGD achieves 3× speedup over Adam on LLaMA 1.3B model with 2.8× effective throughput improvement
- Memory consumption reduced by 3× compared to Adam (0.93G vs 2.05G at 350M scale)
- Maintains competitive perplexity (~22.75) compared to Adam baseline
- Speedup advantage scales with model size (1.6× on 60M, 2.8× on 1.3B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Alternating projection across multiple norms produces a gradient direction approximately normalized with respect to all specified constraints.
- **Mechanism:** The MultiNorm algorithm iteratively applies normalized projections P_g1, P_g2, ..., P_gK. For two norms with constant ℓ2 projections (Assumption 3.3), Theorem 3.6 shows d(x_n, F) → 0, where F is the fixed-point set where both normalization constraints are satisfied.
- **Core assumption:** Projection operators for each norm are efficiently computable (true for ℓp-norms, Schatten p-norms).
- **Evidence anchors:**
  - [Section 3.1] "We propose a simple alternating projection scheme... our procedure can produce, up to an arbitrary precision, a fixed-point of the problem"
  - [Section 3.2] Theorem 3.6 provides convergence guarantees under Assumption 3.3
  - [corpus] AlphaGrad paper (arXiv:2504.16020) explores related gradient normalization themes, but direct multi-norm alternating projection is not discussed
- **Break condition:** When norms have projections with incompatible ℓ2 magnitudes that cannot be rescaled to equality, convergence to a common fixed-point may fail (see Remark 3.7 for rescaling workaround).

### Mechanism 2
- **Claim:** Replacing SWAN's spectral normalization (whitening) with column-wise ℓ2 normalization retains most benefits while reducing complexity from O(m²(m+n)) to O(mn).
- **Mechanism:** SWAN uses row-wise ℓ2 + spectral norm; SinkGD uses row-wise ℓ2 + column-wise ℓ2. The SR-Sinkhorn procedure (Algorithm 3) alternates row/column scaling, which is equivalent to square-root Sinkhorn iterations (Eq. 9). This produces a gradient where all rows have norm √n and all columns have norm √m at convergence.
- **Core assumption:** The relaxed constraint (column-wise vs. spectral) still captures enough structure to precondition gradients effectively.
- **Evidence anchors:**
  - [Section 4] "Algorithm 3 can be seen as a simple reparameterization of the updates presented in (8)... the linear convergence of Algorithm 3 follows directly from the convergence rate of Sinkhorn"
  - [Table 1] SinkGD matches or outperforms SWAN† across all model sizes
  - [corpus] No direct corpus comparison of spectral vs. column-wise relaxation; this is novel to the paper
- **Break condition:** If gradient matrices have highly non-uniform singular value distributions where spectral whitening is critical, the relaxation may underperform. Paper does not characterize this boundary.

### Mechanism 3
- **Claim:** Stateless optimization eliminates memory overhead by computing all preprocessing from instantaneous gradients only, without accumulating momentum/second-moment estimates.
- **Mechanism:** Unlike Adam (stores m_t, s_t), SinkGD computes X̂_t = SR-Sinkhorn(∇_t, L) and updates θ_{t+1} = θ_t - η_t X̂_t. No persistent state beyond parameters. Memory = model weights + gradients (same as SGD).
- **Core assumption:** Instantaneous gradient preprocessing provides sufficient adaptation; historical statistics are not necessary for LLM training dynamics.
- **Evidence anchors:**
  - [Abstract] "eliminates the need for optimizer states while achieving performance comparable to Adam"
  - [Table 1] Memory footprint of SinkGD (0.23G-2.98G across scales) matches other stateless methods and is ~3× lower than Adam
  - [corpus] AlphaGrad and SPAM papers also pursue stateless/near-stateless designs, suggesting a broader trend, but corpus evidence for this specific mechanism is weak
- **Break condition:** If training requires long-range gradient history (e.g., sparse features with infrequent updates), stateless methods may lose critical adaptation. Paper does not evaluate such regimes.

## Foundational Learning

- **Concept: Steepest descent under a norm**
  - **Why needed here:** The paper frames optimization as finding z that maximizes ⟨∇, z⟩ subject to ‖z‖ ≤ 1. Understanding this connects MNGD to classical optimization.
  - **Quick check question:** Given gradient ∇ ∈ R^d, what is the steepest descent direction under ℓ∞ norm? (Answer: sign(∇), i.e., normalized projection onto ℓ∞ ball)

- **Concept: Dual norms and projection operators**
  - **Why needed here:** Normalized projection P_{‖·‖}(x) = argmax_{z:‖z‖=1} ⟨x, z⟩ relies on dual norm structure. Lemma 3.5 uses dual norm properties to prove Pg ∘ Pg = Pg.
  - **Quick check question:** What is the dual norm of ℓ2? Of ℓ1? (Answer: ℓ2 is self-dual; dual of ℓ1 is ℓ∞)

- **Concept: Sinkhorn algorithm and doubly stochastic scaling**
  - **Why needed here:** SR-Sinkhorn is a square-root variant of Sinkhorn's row/column balancing. Linear convergence guarantees transfer directly.
  - **Quick check question:** Given positive matrix A ∈ R^{m×n}, what does Sinkhorn iteration produce? (Answer: Diagonal scaling Q, R such that QAR has uniform row sums = n and column sums = m)

## Architecture Onboarding

- **Component map:** Linear layers in transformer blocks (attention Q/K/V/O, MLP up/down projections) -> SinkGD with SR-Sinkhorn preprocessing -> parameter updates; Embedding layer, RMSNorm layers, final output projection -> Adam optimizer

- **Critical path:**
  1. Compute per-batch gradient ∇_t via backprop
  2. For each linear weight matrix W ∈ R^{m×n}, extract gradient component G
  3. Run SR-Sinkhorn(G, L): initialize X = G; for ℓ=1..L: X ← √n·Q(X)^{-1}X, then X ← √m·X·R(X)^{-1}
  4. Apply learning rate scaling: effective_lr = α × η_t (α=0.05 per paper)
  5. Update: W ← W - effective_lr × X̂

- **Design tradeoffs:**
  - L (Sinkhorn iterations): Higher L → better fixed-point approximation but more compute. Ablation (Table 4) shows L=1 vs L=5 gives marginal improvement on 130M model; paper uses L=5 conservatively.
  - α (learning rate scaling): Must compensate for √(nm) Frobenius norm of preprocessed gradient. Paper uses α=0.05; tuning may be needed for different architectures.
  - Mixed optimizer strategy: Using Adam on non-linear layers adds memory overhead but stabilizes training. Fully stateless requires additional validation.

- **Failure signatures:**
  - Gradient explosion: If input gradients have near-zero rows/columns, Q(X)^{-1} or R(X)^{-1} will blow up. Add ε to diagonal denominators.
  - Slow convergence on small models: Table 1 shows 1.6× speedup on 60M vs 2.8× on 1.3B. Benefit scales with model size—expect weaker results on small models.
  - Non-uniform layer sensitivity: If certain layers dominate loss, uniform α may underperform. Consider per-layer α tuning.

- **First 3 experiments:**
  1. **Reproduce 130M LLaMA result:** Train on C4 subset with L=5, α=0.05, global_lr=0.02. Target: test perplexity ~22.75 at 20K steps (match Table 1).
  2. **Ablate L ∈ {1, 3, 5, 10}:** Measure perplexity and wall-clock time tradeoff. Determine compute-optimal L for your hardware.
  3. **Memory profiling:** Compare peak GPU memory for SinkGD vs Adam at 350M scale. Verify reported 0.93G vs 2.05G (Table 1). Check if mixed Adam/SinkGD strategy is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can alternative normalization schemes beyond row-wise and column-wise ℓ2 norms further enhance the efficiency or convergence speed of stateless optimizers?
- **Basis in paper:** [explicit] The Conclusion states, "Future research directions include exploring alternative normalization schemes to further enhance the efficiency of stateless optimizers."
- **Why unresolved:** The current work focuses specifically on SinkGD, which relaxes SWAN's constraints using a specific choice of norms (row-wise/column-wise ℓ2) to achieve O(mn) complexity.
- **What evidence would resolve it:** Empirical or theoretical analysis comparing SinkGD against MNGD instances using different matrix norms (e.g., Schatten p-norms with varying p).

### Open Question 2
- **Question:** Is SinkGD effective for training regimes other than LLM pre-training, such as fine-tuning or reinforcement learning from human feedback (RLHF)?
- **Basis in paper:** [explicit] The Conclusion proposes "extending the applicability of SinkGD to other training regimes beyond LLM pre-training."
- **Why unresolved:** The experimental section (Section 5) validates SinkGD exclusively on LLaMA pre-training tasks using the C4 dataset.
- **What evidence would resolve it:** Benchmark results showing SinkGD's performance and stability compared to Adam on downstream tasks like instruction tuning or RLHF.

### Open Question 3
- **Question:** Does the MultiNorm procedure provide formal convergence guarantees to a stationary point of the non-convex loss function, rather than just convergence to a fixed-point of the normalization operator?
- **Basis in paper:** [inferred] Theorem 3.6 guarantees convergence to a fixed-point of the normalization operator Pg1 and Pg2, but the paper does not provide a theoretical convergence rate for the resulting optimizer on the non-convex loss landscape.
- **Why unresolved:** The paper establishes that the gradient preprocessing converges, but standard convergence analysis for the overall optimization loop (combining SGD steps with MultiNorm) is not derived.
- **What evidence would resolve it:** A formal proof establishing a convergence rate (e.g., O(1/√T)) for the MNGD update sequence on non-convex objectives.

### Open Question 4
- **Question:** Can a fully stateless optimizer be achieved by applying SinkGD to all layers, including embeddings and layer norms, without sacrificing training stability?
- **Basis in paper:** [inferred] In Section 5.1, the authors note that for "all other modules... Adam optimizer... is used," implying SinkGD is currently applied only to linear projection weights.
- **Why unresolved:** The paper demonstrates memory savings on linear modules but relies on Adam for normalization layers and embeddings, suggesting potential stability issues if SinkGD were applied universally.
- **What evidence would resolve it:** Successful training runs of LLaMA-scale models where SinkGD (or a modified variant) is applied to every layer, completely eliminating optimizer states.

## Limitations
- The convergence guarantees for MultiNorm rely on assumptions about projection operator properties that may not hold for all gradient distributions
- The relaxation from spectral to column-wise normalization lacks theoretical guarantees for all gradient matrix structures
- The mixed optimizer strategy (Adam for non-linear layers) partially undermines the "stateless" claim, though this is acknowledged as a pragmatic choice

## Confidence
- **High confidence**: Memory efficiency claims (Table 1 shows 3× reduction vs Adam with identical implementation); the stateless nature of SinkGD is unambiguous and verifiable
- **Medium confidence**: Speedup claims (3× faster) - while Table 1 shows consistent throughput gains, these depend heavily on hardware, batch size, and gradient characteristics. The claim that SinkGD "achieves performance comparable to Adam" on perplexity is supported but could vary with hyperparameter tuning
- **Low confidence**: Theoretical convergence guarantees for general multi-norm settings. The proof relies on specific assumptions about projection ℓ2 constants that may not hold in practice, and Remark 3.7 acknowledges rescaling limitations without quantifying failure modes

## Next Checks
1. **Convergence boundary testing**: Systematically vary gradient matrix properties (singular value spread, sparsity, condition number) and measure when SinkGD's relaxed column-wise normalization begins to underperform spectral normalization. Characterize the failure threshold.

2. **Ablation of mixed optimizer strategy**: Train a fully stateless variant (all layers using SinkGD) vs the proposed mixed approach. Quantify perplexity and memory tradeoffs to determine if Adam's contribution is essential or merely beneficial.

3. **Scaling limit validation**: Extend experiments beyond 1.3B parameters to 7B+ scale. Measure whether the 3× speedup and memory benefits persist, or if new bottlenecks emerge (e.g., SR-Sinkhorn computation cost, gradient synchronization overhead).