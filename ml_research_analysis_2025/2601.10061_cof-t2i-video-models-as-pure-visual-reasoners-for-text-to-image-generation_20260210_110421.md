---
ver: rpa2
title: 'CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation'
arxiv_id: '2601.10061'
source_url: https://arxiv.org/abs/2601.10061
tags:
- reasoning
- generation
- video
- visual
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CoF-T2I, a text-to-image model that repurposes
  video generation models as pure visual reasoners by leveraging Chain-of-Frame reasoning.
  Instead of relying on text-based planning, CoF-T2I uses intermediate video frames
  as explicit reasoning steps, progressively refining images from coarse layouts to
  high-fidelity outputs.
---

# CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation

## Quick Facts
- **arXiv ID**: 2601.10061
- **Source URL**: https://arxiv.org/abs/2601.10061
- **Reference count**: 40
- **Primary result**: Achieves 0.86 on GenEval and 7.468 on Imagine-Bench using video models as pure visual reasoners

## Executive Summary
CoF-T2I repurposes video generation models as pure visual reasoners for text-to-image generation through Chain-of-Frame reasoning. Instead of relying on text-based planning, CoF-T2I uses intermediate video frames as explicit reasoning steps, progressively refining images from coarse layouts to high-fidelity outputs. The approach leverages a 64K dataset of structured visual refinement trajectories and employs frame-wise encoding to avoid motion artifacts. Experiments demonstrate that CoF-T2I significantly outperforms its video backbone and achieves state-of-the-art results on key benchmarks.

## Method Summary
CoF-T2I initializes from a pretrained video model (Wan2.1-T2V-14B) and fine-tunes it for text-to-image generation using Chain-of-Frame reasoning. The model generates 3-frame latent sequences where each frame represents progressive refinement (coarse layout → semantic correction → aesthetic refinement), with only the final frame decoded as output. Training uses a quality-aware CoF trajectory supervision approach with the CoF-Evol-Instruct dataset, employing independent frame-wise VAE encoding to prevent motion artifacts. The training objective is based on rectified flow, and the system prompt prefix is added during both training and inference.

## Key Results
- Achieves 0.86 GenEval score, surpassing target-only training (0.81) by 5 points
- Reaches 7.468 on Imagine-Bench, demonstrating strong imaginative concept transformations
- Frame-wise evaluation shows monotonic improvement: Frame 1 (0.56) → Frame 2 (0.79) → Frame 3 (0.86) on GenEval
- Independent frame encoding improves quality from 0.83 to 0.86 on GenEval

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Frame (CoF) Visual Reasoning
Video models can perform iterative visual self-correction by treating frame sequences as explicit reasoning steps, improving semantic alignment and aesthetic quality monotonically across frames. The model learns to generate a 3-frame latent sequence where each frame represents progressive refinement. Video backbone's spatiotemporal prior enables frame-to-frame corrections without external textual planning. Evidence: GenEval scores improve monotonically across frames, and MME-CoF benchmark confirms video models exhibit emergent zero-shot CoF reasoning.

### Mechanism 2: Independent Frame-wise VAE Encoding
Encoding each frame independently through the video VAE prevents motion artifacts that would otherwise degrade reasoning-step independence. The VAE slides along the temporal axis so the target frame is always in the initial context window, restricting encoding to spatial-only compression. Evidence: 5-point GenEval improvement from independent encoding (0.83 → 0.86) demonstrates the importance of this design choice.

### Mechanism 3: Quality-Aware CoF Trajectory Supervision
Training with explicitly progressive CoF sequences provides superior supervision compared to target-only fine-tuning. The quality-aware generation pipeline routes anchor images by quality tier to three construction strategies using a Unified Editing Primitive agent system. Evidence: Target-only SFT achieves 0.81 vs. CoF-T2I at 0.86, confirming the value of intermediate supervision.

## Foundational Learning

- **Concept: Rectified Flow / Flow Matching**
  - **Why needed here**: CoF-T2I uses rectified flow to learn straight paths from noise to data. Understanding Eq. (1) is essential for modifying the training objective.
  - **Quick check question**: Can you explain why rectified flow learns to predict the velocity field (x₁ - x₀) rather than the noise or data directly?

- **Concept: Causal Video VAE Compression**
  - **Why needed here**: The Wan2.1 VAE uses 8× spatial + 4× temporal compression with causal dependencies. Understanding this explains why independent encoding is necessary.
  - **Quick check question**: Why does standard causal video VAE encoding of a 3-frame sequence require padding to 5 frames?

- **Concept: Chain-of-Thought Reasoning (from LLMs)**
  - **Why needed here**: CoF is the visual analog of textual CoT. Understanding CoT helps frame why intermediate supervision improves reasoning.
  - **Quick check question**: How is CoF reasoning similar to and different from textual CoT in language models?

## Architecture Onboarding

- **Component map**: Text Prompt → Text Encoder (frozen) → [Noise z₀] → DiT Backbone (14B params, fine-tuned) → [z₁, z₂, z₃] → Wan2.1 VAE Decoder (frozen) → Final Image
- **Critical path**: 
  1. Independent VAE encoding (must use frame-wise sliding window, NOT standard video encoding)
  2. 3-frame latent sequence generation during training
  3. Decode only final frame at inference
- **Design tradeoffs**:
  - 3-frame vs. longer chains: Chosen to balance semantic correction + aesthetic refinement while avoiding diminishing returns
  - Independent vs. continuous VAE encoding: 3-point GenEval improvement from independence (0.83 → 0.86)
  - Dataset size: 64K trajectories from 68K prompts after filtering
- **Failure signatures**:
  - Motion blur or temporal artifacts between frames → independent VAE encoding not working
  - Non-progressive quality across frames → CoF training objective not converged
  - Subject identity drift across frames → UEP construction producing inconsistent chains
  - Semantic errors persisting in final frame → insufficient semantic correction stage training
- **First 3 experiments**:
  1. Reproduce ablation: Train Target-only SFT variant (final frames only) and compare against full CoF-T2I. Expected gap: ~5 points on GenEval overall.
  2. Validate independent encoding: Run inference with standard causal VAE encoding and measure quality degradation vs. independent encoding.
  3. Analyze trajectory progression: Decode and evaluate all 3 frames (F₁, F₂, F₃) on GenEval subcategories to verify monotonic improvement pattern matches Table 4.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can reinforcement learning (RL) techniques be effectively integrated with video-based CoF reasoning to enable more adaptive visual refinements in T2I generation? (Limitations section states RL remains underexplored)
- **Open Question 2**: Is the fixed three-frame CoF trajectory optimal, or would variable-length reasoning chains improve generation quality? (Paper chose fixed length of three without ablation on other lengths)
- **Open Question 3**: Can the CoF-T2I paradigm be extended to video generation and 3D synthesis tasks while maintaining reasoning quality? (Paper has not systematically explored extension to broader task domains)
- **Open Question 4**: What are the computational cost trade-offs between independent frame encoding and continuous video VAE encoding at larger frame counts? (Paper shows quality improvement but doesn't analyze computational overhead)

## Limitations
- The paper's confidence in video models as "pure visual reasoners" is somewhat overstated given that the approach still requires text prompts as input
- Key components like Wan2.1-T2V-14B checkpoint access and Qwen-Image-Edit-2509 model availability are not publicly specified, creating reproducibility challenges
- The dataset construction pipeline involving multiple specialized models is complex and difficult to replicate without access to these specific models
- Inference settings such as denoising steps, sampler choice, and CFG scale are unspecified but likely impact final quality

## Confidence
- **High confidence**: The empirical demonstration that CoF reasoning improves generation quality (0.86 GenEval vs 0.81 baseline) and the effectiveness of independent frame-wise VAE encoding
- **Medium confidence**: The generalizability of the approach to other video models and the robustness of the dataset construction pipeline
- **Low confidence**: The claim that video models can serve as "pure visual reasoners" without significant architectural modification

## Next Checks
1. **Replicate the Target-only SFT ablation**: Train a version using only final frames for supervision and verify the ~5-point GenEval gap (0.81 vs 0.86) to confirm the importance of intermediate frame supervision
2. **Validate independent VAE encoding**: Implement standard causal video VAE encoding (pad to 5 frames as described) and measure the expected quality degradation (paper reports 0.83 vs 0.86) to confirm this design choice
3. **Analyze trajectory consistency**: Decode and evaluate all three frames (F1, F2, F3) on GenEval subcategories to verify the monotonic improvement pattern matches Table 4 and check for subject identity drift across frames