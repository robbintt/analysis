---
ver: rpa2
title: Improving Code-Switching Speech Recognition with TTS Data Augmentation
arxiv_id: '2601.00935'
source_url: https://arxiv.org/abs/2601.00935
tags:
- speech
- data
- synthetic
- code-switching
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that fine-tuning a multilingual TTS model
  (CosyVoice2) on conversational code-switching data (SEAME) produces high-quality
  synthetic speech that effectively augments low-resource ASR training. The synthetic
  data captures realistic conversational prosody, informal language mixing, and speaker
  diversity.
---

# Improving Code-Switching Speech Recognition with TTS Data Augmentation

## Quick Facts
- arXiv ID: 2601.00935
- Source URL: https://arxiv.org/abs/2601.00935
- Reference count: 22
- Primary result: Synthetic speech with speaker diversity improves low-resource code-switching ASR MER by ~2 points absolute

## Executive Summary
This paper presents a data augmentation approach for low-resource code-switching speech recognition by fine-tuning a multilingual TTS model (CosyVoice2) on conversational code-switching data (SEAME). The method generates high-quality synthetic speech with diverse speaker identities that captures realistic conversational prosody and informal language mixing patterns. Experiments show that augmenting real speech with synthetic speech reduces mixed error rate (MER) from 12.1% to 10.1% on DevMan and from 17.8% to 16.0% on DevSGE, with speaker diversity identified as the critical factor for improvement.

## Method Summary
The approach fine-tunes only the LLM component (QwenLM2-0.5B) of the CosyVoice2 multilingual TTS model on SEAME code-switching transcripts and speech tokens, keeping the speech tokenizer, flow-matching decoder, and vocoder frozen. Synthetic speech is generated by resynthesizing transcripts with randomly sampled speaker embeddings (x-vectors) to introduce speaker diversity. The real (100h) and synthetic (100-300h) speech data are combined to fine-tune Whisper-small ASR models. The method successfully transfers to a new code-switching corpus (ASCEND), demonstrating cross-domain applicability while confirming that synthetic data complements rather than replaces real data.

## Key Results
- Speaker diversity in synthetic data is critical: Random speaker embeddings (TTS-R) reduce MER more than original speakers (TTS-O)
- Optimal augmentation ratio: 2-3x synthetic to real data provides maximum benefit before diminishing returns
- Cross-domain transfer: Method works on ASCEND corpus, narrowing performance gap with real data
- Quality maintenance: UTMOS scores for synthetic speech remain high (3.2) despite being below real speech (3.6)

## Why This Works (Mechanism)

### Mechanism 1
Speaker diversity in synthetic speech is the critical factor for ASR improvement, not merely data quantity. Randomly sampled speaker embeddings introduce novel timbres, pitch ranges, and prosodic variations that expand the acoustic space the ASR model learns from, whereas original-speaker synthesis largely recapitulates existing speaker characteristics. This explains why TTS-R (random speakers) outperforms TTS-O (original speakers) by 1-2 percentage points absolute MER.

### Mechanism 2
Fine-tuning only the LLM component of a multilingual TTS system suffices to capture domain-specific code-switching patterns without full model retraining. The QwenLM learns to predict speech-token sequences reflecting target corpus language alternation patterns and conversational prosody, while frozen speech tokenizer and vocoder preserve acoustic fidelity from large-scale pretraining. This partial fine-tuning approach is computationally efficient while maintaining quality.

### Mechanism 3
Synthetic data works as a complement to real data but cannot fully replace it; optimal ratio is approximately 2-3x synthetic to real. Synthetic speech provides speaker and prosodic diversity but may contain subtle artifacts. Real data anchors the model to genuine acoustic conditions, while synthetic data expands coverage. Beyond 2-3x ratio, marginal returns diminish sharply, and synthetic-only training underperforms real-only baselines.

## Foundational Learning

- **Code-switching (intra-sentence vs. inter-sentence)**: Code-switching involves rapid language alternation within utterances, requiring ASR systems to handle language-dependent intonation and pronunciation shifts. Quick check: Can you explain why audio splicing (concatenating monolingual segments) fails to capture realistic code-switching prosody?

- **Speaker embeddings (x-vectors)**: Compact vector representations of speaker identity used to generate diverse synthetic voices. Quick check: What would happen if all synthetic utterances used the same speaker embedding?

- **Flow-matching decoder**: CosyVoice2 uses flow-matching (not diffusion) to convert discrete tokens to mel-spectrograms, explaining why only the LLM needs fine-tuning. Quick check: Why might freezing the vocoder and decoder during fine-tuning preserve acoustic quality?

## Architecture Onboarding

- **Component map**:
  Text Input → Text Tokenizer (BPE)
  ↓
  Ground-truth Speech → Speech Tokenizer (VQ over ASR encoder)
  ↓
  QwenLM2-0.5B (autoregressive token prediction) ← FINE-TUNED
  ↓
  Flow-Matching Decoder (frozen)
  ↓
  HiFT Vocoder (frozen)
  ↓
  Synthetic Speech → Whisper ASR (fine-tuned)

- **Critical path**:
  1. Fine-tune QwenLM on 100h SEAME transcripts + speech tokens (200 epochs, lr=1e-4, 10k warmup)
  2. Generate synthetic speech by resynthesizing transcripts with randomly sampled speaker embeddings
  3. Mix real (100h) + synthetic (100-300h) data
  4. Fine-tune Whisper-small with SpecAugment, AdamW, inverse-square-root lr decay

- **Design tradeoffs**:
  - TTS-O vs. TTS-R: Original speakers preserve domain match but lack diversity; random speakers add diversity but may introduce domain mismatch
  - Fine-tuning scope: LLM-only is lightweight but assumes frozen components are sufficient; full fine-tuning may improve quality
  - Synthetic data volume: More data helps until ~300h, then diminishing returns

- **Failure signatures**:
  - MER doesn't improve: Check if synthetic data volume is too low (<100h) or speaker diversity is insufficient (using TTS-O)
  - UTMOS score low (<3.0): TTS model may need more fine-tuning data; increase from 10h to 50h+
  - Cross-domain transfer fails: Ensure source TTS fine-tuning corpus shares conversational characteristics with target domain

- **First 3 experiments**:
  1. Baseline comparison: Train Whisper on ground-truth only (100h) vs. ground-truth + TTS-R (100h each) to confirm speaker diversity benefit
  2. Fine-tuning data ablation: Fine-tune CosyVoice on 10h, 50h, 100h of SEAME and measure UTMOS + downstream ASR MER
  3. Volume sweep: Generate 100h, 200h, 300h, 400h, 500h of synthetic speech and fine-tune Whisper to identify inflection point

## Open Questions the Paper Calls Out

### Open Question 1
Can large language models (LLMs) be effectively designed or adapted to generate controlled, linguistically valid code-switching text that enhances textual diversity in TTS augmentation pipelines? Current augmentation is "constrained by the limited textual diversity inherent in existing transcriptions"—the synthetic speech merely reproduces existing transcript content with speaker variation, not novel linguistic patterns.

### Open Question 2
Does fine-tuning additional components of CosyVoice2 beyond the language model (e.g., speech tokenizer or flow-matching decoder) yield further improvements in synthetic code-switching speech quality? The methodology notes that "Only the QwenLM language-model component is updated; the speech tokeniser, flow-matching decoder and vocoder are kept fixed" during fine-tuning.

### Open Question 3
What mechanisms cause the diminishing returns in MER reduction when synthetic training data exceeds 2-3x the real data volume? Table III shows the relative MER gain drops from 23.04% (100→200h) to 10.29% (200→300h) to 4.06% (300→400h) to 2.54% (400→500h), indicating a saturation effect the paper describes as "rapidly diminishing returns" but does not explain.

## Limitations

- **Model accessibility**: Requires specific CosyVoice2 checkpoint with QwenLM2-0.5B, HiFT vocoder, and VQ tokenizer; exact checkpoint provenance unspecified
- **Speaker embedding pool**: Size, diversity, and source of x-vector speaker embedding set used for TTS-R are unspecified
- **Architecture dependence**: Results reported only for Whisper-small/Whisper-Large-v3; performance on other ASR architectures may differ

## Confidence

- **High confidence**: Speaker diversity in synthetic data drives MER improvement; combining real and synthetic data outperforms either alone; synthetic data is complementary, not substitutive
- **Medium confidence**: The optimal fine-tuning duration for CosyVoice is ~100 epochs on 10-50h SEAME data; the ideal synthetic:real ratio is 2-3x for Whisper-small
- **Low confidence**: Exact speaker embedding pool composition; absolute reproducibility of UTMOS scores without identical CosyVoice checkpoint; generalization to ASR architectures beyond Whisper

## Next Checks

1. **Ablation on speaker embedding diversity**: Generate synthetic speech using (a) original speaker embeddings (TTS-O), (b) 10 randomly sampled embeddings repeated, (c) 100 diverse embeddings. Fine-tune Whisper-small on each and measure MER on DevMan/DevSGE to isolate the impact of embedding diversity versus data quantity.

2. **Cross-ASR validation**: Repeat the real+synthetic training pipeline using a hybrid TDNN/HMM ASR system (e.g., Kaldi TED-LIUM recipe adapted for code-switching) and compare MER to Whisper baselines. This tests architecture independence of the augmentation benefit.

3. **Synthetic data quality audit**: Sample 50 synthetic utterances from TTS-R and 50 from TTS-O. Have native Chinese-English code-switchers rate naturalness and conversational authenticity on a 1-5 Likert scale. Correlate perceived quality with MER improvement to verify that speaker diversity improves both acoustic realism and ASR performance.