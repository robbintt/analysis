---
ver: rpa2
title: (Almost) Free Modality Stitching of Foundation Models
arxiv_id: '2507.10015'
source_url: https://arxiv.org/abs/2507.10015
tags:
- hyma
- image
- training
- performance
- connector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimal pairing and stitching
  of uni-modal models to create multi-modal models, particularly Vision-Language Models
  (VLMs). The challenge lies in the computational expense of grid search for optimal
  model pairs and the lack of correlation between uni-modal and multi-modal performance.
---

# (Almost) Free Modality Stitching of Foundation Models

## Quick Facts
- arXiv ID: 2507.10015
- Source URL: https://arxiv.org/abs/2507.10015
- Authors: Jaisidh Singh; Diganta Misra; Boris Knyazev; Antonio Orvieto
- Reference count: 36
- Primary result: 10× computational efficiency gain over grid search for optimal VLM pairing while matching ranking performance

## Executive Summary
This paper addresses the computationally expensive problem of optimal pairing and stitching of uni-modal models to create multi-modal models, particularly Vision-Language Models (VLMs). The authors propose Hypernetwork Model Alignment (HYMA), which uses a hypernetwork to predict parameters for all N×M connector modules simultaneously rather than training each individually. By jointly learning to generate connectors for all possible combinations, HYMA reduces search costs by 10× while maintaining comparable ranking and performance to exhaustive grid search. Experiments across diverse benchmarks demonstrate strong performance with significantly fewer computational resources compared to baselines.

## Method Summary
HYMA uses a hypernetwork to jointly predict connector parameters for all possible uni-modal model pairs in a zoo. The hypernetwork takes learnable embeddings representing specific model pairs as input and generates connector parameters on-the-fly. Training uses dual mini-batching over both data samples and model combinations, allowing each model pair to be evaluated on the same data batch while reducing per-pair data exposure. The hypernetwork is trained to minimize a multi-modal loss averaged over all pairs, forcing it to learn common structures in stitching tasks across different uni-modal model pairs.

## Key Results
- 10× reduction in computational cost compared to grid search for optimal model pair identification
- Strong rank correlation (NDCG@k, Spearman's ρ) with grid search oracle on standard VLM benchmarks
- Competitive zero-shot performance on ImageNet-1K, CIFAR-100, MSCOCO, and Flickr-8K
- Efficiency gains maintained across diverse model zoos including Vision-Language and Multi-modal Large Language Models

## Why This Works (Mechanism)

### Mechanism 1
HYMA's hypernetwork jointly predicts connector parameters for all N×M model pairs, enabling efficient search by learning a shared mapping from model-pair identity to effective connector weights. A hypernetwork Hϕ takes a learnable embedding ck (representing a specific model pair) as input and generates the parameters θ for a connector module (an MLP). The hypernetwork is trained to minimize a multi-modal loss averaged over a batch of model combinations, forcing it to learn common structures in the stitching task across different uni-modal model pairs.

### Mechanism 2
A dual mini-batching strategy over data and model combinations reduces per-pair data exposure, acting as implicit random data pruning to achieve computational efficiency. HYMA samples a batch of Bd data points and a batch of Bm model combinations. Each model pair is evaluated on the same data batch. Over the full training run, any single model pair is evaluated only a fraction of the time, seeing less data than in independent training. This reduces total FLOPs while the hypernetwork still learns from the entire dataset across all pairs.

### Mechanism 3
HYMA uses the rank correlation between its predicted connectors' performance and a grid search oracle as the primary metric for successful model pair identification. After training, HYMA generates a connector for every possible pair and evaluates them on a validation set to produce a performance ranking. Success is measured by high correlation (via Spearman's ρ or NDCG@k) with the ranking from an exhaustive grid search, indicating HYMA can correctly identify the best pairs without the search cost.

## Foundational Learning

- **Hypernetworks (Ha et al., 2016)**: A hypernetwork outputs the weights for another "target" network. During training, gradients flow back through the target network's loss to update the hypernetwork, not the target weights directly. Quick check: During training, are the gradients used to update the connector's parameters θ or the hypernetwork's parameters ϕ?

- **Contrastive Learning (InfoNCE loss)**: This is the primary training objective for VLMs in the paper. The connector is trained to align positive image-text pairs and separate negative pairs in a shared embedding space. Quick check: In the InfoNCE loss formula, what does the similarity function sim(·, ·) measure?

- **Multi-modal Optimal Pairing and Stitching (M-OPS)**: This is the problem the paper formalizes, separating the challenge into two sub-problems: (1) finding the best pair of models from a zoo and (2) training the connector to stitch them. Quick check: What are the two sub-problems of M-OPS?

## Architecture Onboarding

- **Component map**: Uni-modal Encoders (frozen pretrained) -> Hypernetwork (Hϕ with lookup table, layer embeddings, parameter decoder) -> Connector (fθ) -> Loss Module (multi-modal loss) -> Gradients update hypernetwork

- **Critical path**:
  1. Sample a batch of Bm model pair IDs and a batch of Bd data samples
  2. For each pair ID, retrieve its embedding ck; for each connector layer j, combine ck with layer embedding ej, pass through decoder Fϱ, and assemble connector parameters θ
  3. Stitch uni-modal encoders with generated connector and compute multi-modal loss on data batch
  4. Average losses from all Bm pairs and update hypernetwork parameters

- **Design tradeoffs**: Efficiency vs. Performance (HYMA is ~10x cheaper but may show small performance gap vs. oracle); Model Batch Size (Bm) affects stability and memory; Connector Complexity (Linear/MLP1 vs. deeper) impacts training stability

- **Failure signatures**: Training Instability (loss divergence/NaNs, especially with certain architectures like MaxVIT); Low Rank Correlation (fails to replicate oracle ranking, particularly in MLLM experiments); Underfitting (generated connector performs poorly due to reduced per-pair data)

- **First 3 experiments**:
  1. Reproduce N×M=3 VLM Baseline: Train HYMA on 3-image/1-text encoder zoo with MLP1 connector on CC558K; verify ~4.4x efficiency gain and comparable accuracy on ImageNet-1K vs. grid search
  2. Ablate Model Batch Size (Bm): Using N×M=3 setup, run HYMA with Bm=1 and Bm=3; compare final performance and training time
  3. Test MLLM Setting: Train HYMA on 1-image/3-LLM zoo for MSCOCO image captioning; evaluate if it can identify best LLM, noting ranking accuracy drops vs. VLMs

## Open Questions the Paper Calls Out

- What mechanisms drive the reduced ranking accuracy of HYMA in Multi-modal Large Language Models (MLLMs) compared to Vision-Language Models (VLMs)? The paper notes this requires investigating the effects of causal modeling loss versus representation space alignment.

- How can the training dynamics of HYMA be stabilized to prevent the exclusion of certain model architectures? The paper mentions excluding models like MaxVIT due to instability but calls for deeper investigation into gradient interference from diverse model combinations.

- Can the efficiency of hypernetwork-based stitching be generalized to modalities beyond image and text? While the paper suggests audio-text as a possibility, empirical validation is strictly limited to vision-language tasks.

## Limitations

- Performance degradation on highly dissimilar model pairs where shared hypernetwork learning may fail
- Training instability with certain architectures (e.g., MaxVIT) requiring exclusion from training zoo
- Reduced rank correlation in Multi-modal Large Language Models compared to Vision-Language Models

## Confidence

- **High Confidence**: The efficiency gains demonstrated through dual mini-batching and the fundamental mechanism of hypernetwork-based parameter prediction are well-supported by experimental results and align with established hypernetwork literature
- **Medium Confidence**: The claim that HYMA matches grid search ranking performance is supported by experiments on standard VLMs but shows weakness in MLLM setting where rank correlation drops
- **Low Confidence**: The generalizability of HYMA to highly heterogeneous model zoos and the specific failure modes when dealing with very different architectures are not thoroughly explored

## Next Checks

1. **Reproduce N×M=3 VLM baseline**: Train HYMA on a 3-image/1-text encoder zoo (ViT-S, DeiT-S, DeiT3-S with MiniLM-L) using LLaVA-CC558K, then validate zero-shot ImageNet-1K accuracy and measure FLOPs compared to grid search

2. **Ablate Model Batch Size (Bm)**: Using the same 3×1 setup, systematically vary Bm (1, 3, 9) while monitoring both training stability and final accuracy to quantify the tradeoff between efficiency and stability

3. **Test MLLM Setting**: Train HYMA on a 1-image/3-LLM zoo (e.g., Vicuna variants) for MSCOCO image captioning; evaluate whether HYMA can identify the best LLM and measure rank correlation with grid search