---
ver: rpa2
title: Quantile Advantage Estimation for Entropy-Safe Reasoning
arxiv_id: '2509.22611'
source_url: https://arxiv.org/abs/2509.22611
tags:
- entropy
- advantage
- zhang
- reasoning
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unstable entropy dynamics in
  reinforcement learning with verifiable rewards (RLVR), where training often oscillates
  between entropy collapse and explosion. The core issue is traced to the mean baseline
  in value-free RL methods like GRPO and DAPO, which improperly penalizes negative-advantage
  samples under reward outliers.
---

# Quantile Advantage Estimation for Entropy-Safe Reasoning

## Quick Facts
- arXiv ID: 2509.22611
- Source URL: https://arxiv.org/abs/2509.22611
- Authors: Junkang Wu; Kexin Huang; Jiancan Wu; An Zhang; Xiang Wang; Xiangnan He
- Reference count: 40
- One-line result: QAE stabilizes entropy and improves pass@1 on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023 via quantile-baseline advantage estimation

## Executive Summary
This paper addresses unstable entropy dynamics in RLVR training, where methods like GRPO and DAPO oscillate between entropy collapse and explosion due to improper mean-baseline advantage estimation. The core insight is that mean baselines penalize negative-advantage samples under reward outliers, destabilizing exploration. The proposed Quantile Advantage Estimation (QAE) replaces the mean baseline with a group-wise K-quantile baseline, creating a response-level two-regime gate that selectively reinforces rare successes on hard queries and targets residual failures on easy queries. Under first-order softmax updates, QAE provides two-sided entropy safety with provable bounds, preventing both collapse and explosion.

## Method Summary
QAE modifies standard value-free RL methods by replacing the mean baseline with a K-quantile baseline computed per query group. For binary rewards with group success rate p(q), the baseline is b_K(q) = 0 if p(q) ≤ 1-K, else b_K(q) = 1. Advantages are standardized: Âi = (Ri - b_K(q)) / (std({Rj}) + ε). This creates sparse advantages where ~80% are zero, concentrating learning on informative samples near decision boundaries. The method uses dynamic sampling ensuring 0 < |correct| < G responses, with K=0.4, ε_high=0.28, ε_low=0.2, and standard RLVR hyperparameters.

## Key Results
- QAE stabilizes entropy throughout training while maintaining or improving pass@1 accuracy on AIME 2024/2025 and AMC 2023
- Sparsifies credit assignment with ~80% of responses receiving zero advantage when K is properly tuned
- Demonstrates sustained performance gains on Qwen3-8B/14B-Base compared to mean-baseline methods
- Provides provable entropy safety bounds under first-order softmax updates

## Why This Works (Mechanism)

### Mechanism 1: Quantile Baseline Creates Response-Level Two-Regime Gate
Replacing the mean baseline with a K-quantile baseline creates a deterministic gate that selectively updates either rare successes or residual failures based on query difficulty. For binary rewards, the baseline reduces to a threshold on p(q): b_K(q) = 0 if p(q) ≤ 1-K, 1 if p(q) > 1-K. On hard queries (low success rate), only correct responses get positive advantage; on easy queries (high success rate), only incorrect responses get negative advantage. This zeros out the advantage for one outcome type per regime.

### Mechanism 2: Two-Sided Entropy Safety via Baseline Extremality
The K-quantile baseline provides provable bounds on one-step entropy change by attaining extremal values (0 or 1) that minimize entropy increase (preventing explosion) or maximize it (preventing collapse). Under first-order softmax updates, entropy change ΔH(q;b) is strictly increasing in baseline b ∈ [0,1]. The K-quantile takes extreme values—0 for hard queries (minimal entropy increase) and 1 for easy queries (maximal entropy increase).

### Mechanism 3: Sparse Credit Assignment Concentrates Learning Signal
With tuned K (≈0.4), roughly 80% of responses receive zero advantage, concentrating gradient updates on the most informative samples near the decision boundary. The quantile thresholding zeros out advantage for one outcome type per query, focusing learning on either rare successes (hard queries) or residual failures (easy queries).

## Foundational Learning

- **Concept: Advantage Estimation in Policy Gradient**
  - Why needed: QAE is fundamentally an advantage estimation modification. Understanding that advantages measure "how much better than expected" is essential to grasp why baseline choice controls entropy dynamics.
  - Quick check: In GRPO/DAPO, what happens to the advantage of a correct response when one other response in the group is also correct, vs. when no other response is correct?

- **Concept: Policy Entropy and Exploration**
  - Why needed: The paper is motivated by entropy collapse (premature convergence) and explosion (unstable exploration). You must understand why entropy governs the exploration-exploitation tradeoff.
  - Quick check: If a model's policy entropy drops to near-zero during RLVR training, what does this imply about its exploration behavior and potential performance ceiling?

- **Concept: Quantiles vs. Means as Robust Statistics**
  - Why needed: The core insight is that mean baselines are outlier-sensitive while quantile baselines provide controlled, interpretable thresholds.
  - Quick check: Given reward values [0, 0, 0, 0, 1], what is the mean baseline vs. the 0.8-quantile baseline? Which responses receive positive advantage under each?

## Architecture Onboarding

- **Component map**: Input query q, group of G responses {o_i}, binary rewards {R_i} -> Compute p(q) = (1/G) Σ R_i -> Determine b_K(q) = 0 if p(q) ≤ 1-K else 1 -> Compute standardized advantages Âi = (Ri - b_K(q)) / (std({Rj}) + ε) -> Pass to token-level policy gradient loss

- **Critical path**: 
  1. Sample G responses per query from π_old
  2. Compute binary rewards via correctness verification
  3. [QAE] Compute p(q), determine b_K(q) per threshold
  4. [QAE] Compute standardized advantages (Eq. 3)
  5. Pass to standard token-level policy gradient loss

- **Design tradeoffs**:
  - K selection: Lower K (0.2) → more positives, lower entropy, collapse risk; Higher K (0.8) → more negatives, higher entropy, explosion risk. Default: K=0.4 with Clip-Higher (ϵ_high=0.28)
  - Group size G: Small G yields noisy p(q); larger G stabilizes threshold reliability
  - Drop-in vs. deep integration: One-line swap works; K tuning requires entropy monitoring

- **Failure signatures**:
  - Entropy surge after step ~100: K too high → reduce K
  - Early plateau with low entropy: K too low → increase K
  - Volatile response length: Entropy instability → check K
  - No sparsity (few zero advantages): K not creating clean threshold → verify binary rewards and group statistics

- **First 3 experiments**:
  1. Baseline comparison: Run DAPO mean-baseline vs. QAE (K=0.4) on held-out reasoning benchmark; plot entropy and pass@1 curves
  2. K sensitivity sweep: Train with K ∈ {0.2, 0.4, 0.6, 0.8}; plot entropy/accuracy/length
  3. Masking ablation: Implement POS-MASK and NEG-MASK separately (Eqs. 7-8) under different ϵ_high; identify which regime drives gains

## Open Questions the Paper Calls Out

- **Open Question 1**: Can an adaptive K schedule outperform fixed K, or does fixed K already approximate optimal exploration-exploitation trade-offs?
  - Basis: Limitations mention exploring schedules or entropy/gradient-variance feedback
  - Why unresolved: Paper uses single fixed K (0.4) tuned once; no adaptive scheme evaluated
  - Evidence needed: Compare training curves with K scheduled vs. fixed across benchmarks

- **Open Question 2**: Does the quantile baseline retain theoretical entropy-safety guarantees when integrated into value-based methods like PPO?
  - Basis: Limitations mention embedding quantile-baseline idea into PPO's whitening/normalization
  - Why unresolved: Current proofs and experiments target value-free methods; PPO interaction untested
  - Evidence needed: Implement batch-wise quantile baselines in PPO and measure entropy bounds, stability, and pass@1

- **Open Question 3**: How does QAE perform under non-binary reward distributions where quantile baseline may not reduce to simple threshold?
  - Basis: Theoretical analysis explicitly assumes binary rewards R ∈ {0,1}
  - Why unresolved: Method and theory are specialized to binary rewards; non-binary extension requires new analysis
  - Evidence needed: Evaluate QAE on tasks with non-binary verifiable rewards and compare to mean baseline

## Limitations
- Theoretical entropy safety proofs assume first-order softmax dynamics, which may not hold with large step sizes or non-softmax policies
- Binary reward assumption limits applicability to tasks with partial credit or continuous correctness scores
- K=0.4 is empirically motivated but lacks theoretical justification for why this value optimally balances entropy regimes

## Confidence
**High Confidence**: The quantile baseline mechanism and two-regime behavior are well-supported by mathematical formulation and empirical evidence. The entropy safety bounds under first-order softmax updates are formally proven.

**Medium Confidence**: The claim that baseline design is the primary mechanism for scaling RLVR is plausible but not definitively proven without ablation studies isolating baseline effects.

**Low Confidence**: The assertion that QAE "solves" entropy instability is overstated, as the paper doesn't address scenarios with unreliable quantile thresholds or non-binary rewards.

## Next Checks
1. **Group Size Sensitivity Analysis**: Systematically vary group size G while keeping K=0.4 fixed, measuring entropy stability, sparsity levels, and pass@1 accuracy across all three evaluation benchmarks.

2. **Non-Binary Reward Robustness Test**: Implement a multi-class reward scheme and evaluate whether QAE's quantile thresholding still provides meaningful advantage estimation and entropy control.

3. **First-Order Dynamics Validation**: Compare empirical entropy change trajectories against theoretical bounds by measuring actual policy parameter changes and computing covariance terms in Proposition 4.2 across training steps.