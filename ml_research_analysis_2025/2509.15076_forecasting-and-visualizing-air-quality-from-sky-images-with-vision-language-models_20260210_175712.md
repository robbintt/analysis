---
ver: rpa2
title: Forecasting and Visualizing Air Quality from Sky Images with Vision-Language
  Models
arxiv_id: '2509.15076'
source_url: https://arxiv.org/abs/2509.15076
tags:
- visual
- pollution
- quality
- forecasting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of air pollution forecasting by
  proposing a hybrid AI agent that combines pollution classification with vision-language
  model (VLM)-guided generative synthesis. The method uses multi-scale feature extraction
  from sky images, including Gabor filters and CNN embeddings, to classify pollution
  levels.
---

# Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models

## Quick Facts
- **arXiv ID**: 2509.15076
- **Source URL**: https://arxiv.org/abs/2509.15076
- **Reference count**: 22
- **Primary result**: 89.4% classification accuracy, 89.6% semantic consistency in generated images

## Executive Summary
This paper presents a hybrid AI system for air quality forecasting that combines sky image analysis with vision-language model (VLM)-guided generative synthesis. The method extracts multi-scale features from sky images using Gabor filters and CNN embeddings to classify pollution levels into five AQI categories. A VLM (BLIP-2) then conditions a diffusion generator to synthesize realistic sky scenes reflecting the predicted pollution grade. The system achieves 89.4% classification accuracy and demonstrates 89.6% semantic consistency between predicted and generated images, while improving interpretability through visual counterfactuals.

## Method Summary
The approach uses a dual-stream feature extraction pipeline: Gabor filters extract directional texture responses from sky images, and statistical moments from these responses are combined with CNN semantic embeddings. A Random Forest classifier maps these features to AQI categories (Good, Moderate, Unhealthy for Sensitive Groups, Unhealthy, Very Unhealthy). For visualization, BLIP-2 translates predicted AQI labels into descriptive prompts that condition a diffusion model to generate corresponding sky scenes. Generated images are validated by re-classification with the original CNN to ensure semantic fidelity.

## Key Results
- Classification accuracy: 89.4% on five AQI categories
- Semantic consistency of generated images: 89.6%
- User study interpretability improvement: 18% increase with visual counterfactuals
- Feature ablation shows 8.3% F1-score drop when Gabor features are removed

## Why This Works (Mechanism)

### Mechanism 1: Multi-Scale Texture-to-Pollution Mapping
The system combines Gabor filter responses (sensitive to haze density and cloud patterns) with CNN embeddings to classify air quality from sky images alone. This hybrid approach captures both statistical texture features and semantic representations.

### Mechanism 2: VLM-Guided Semantic-to-Visual Bridging
BLIP-2 maps predicted AQI categories to descriptive prompts that condition a diffusion generator to produce semantically consistent sky scene variants, translating pollution semantics into visual attributes.

### Mechanism 3: Classifier-Generator Semantic Consistency Loop
Generated images are re-classified using the same CNN architecture to validate semantic fidelity, with 89.6% agreement indicating genuine alignment between generative and classification modules.

## Foundational Learning

- **Gabor Filters and Texture Analysis**: Understanding orientation and spatial frequency parameters is essential for extracting haze and cloud patterns from sky images.
  - Quick check: Given a sky image with uniform haze, what would you expect the Gabor filter response distribution to look like compared to a clear, cloud-textured sky?

- **Vision-Language Model Architecture (BLIP-2)**: The Q-former module bridges frozen image encoders and LLMs, critical for understanding prompt-to-visual alignment.
  - Quick check: How does BLIP-2 differ from a standard image captioning model, and why does this matter for controllable generation?

- **Diffusion-Based Conditional Image Generation**: Understanding how conditioning signals modulate the denoising process is critical for troubleshooting semantic misalignment.
  - Quick check: If generated images for "Unhealthy" AQI consistently appear too similar to "Moderate," what conditioning or prompt engineering adjustments might you investigate first?

## Architecture Onboarding

- **Component map**: Input preprocessing → Gabor + CNN feature extraction → classification → AQI label → prompt generation (BLIP-2) → diffusion synthesis → output visualization
- **Critical path**: Input image → feature extraction → classification → AQI label → prompt generation → diffusion synthesis → output visualization
- **Design tradeoffs**: Random Forest on Gabor features (85.2% accuracy) is more interpretable but less accurate than best CNN (89.4%); current implementation processes static images only; edge deployment claims are aspirational
- **Failure signatures**: Low accuracy on minority classes suggests class imbalance; poor semantic consistency in generated images indicates prompt engineering issues; high computational cost suggests need for green CNN optimization
- **First 3 experiments**: (1) Feature ablation comparing Gabor-only, CNN-only, and combined approaches; (2) Prompt sensitivity analysis measuring semantic consistency across different descriptions; (3) Cross-dataset generalization testing on geographically held-out images

## Open Questions the Paper Calls Out

- **Integration of auxiliary data**: Can multimodal data (meteorological variables, satellite observations) with recurrent architectures improve forecasting robustness compared to the current static, image-only approach?
- **Edge deployment feasibility**: Can a "Green CNN" with FPGA-based incremental learning maintain ~89% accuracy while achieving required energy efficiency for real-time edge deployment?
- **User impact**: How does exposure to VLM-generated visual counterfactuals specifically influence user trust, risk perception, and behavioral intent compared to numerical AQI data alone?
- **Robustness to occlusions**: What is the performance degradation when applied to raw, unsegmented images containing buildings, trees, or other occlusions?

## Limitations
- Dataset generalization limited by unspecified geographic and temporal coverage
- VLM-generation pipeline lacks end-to-end learning and comprehensive prompt ablation
- Edge deployment claims are aspirational without implemented green CNN or FPGA validation

## Confidence
- **High**: Classification accuracy (89.4%), macro F1 (88.1%), and feature ablation results (8.3% F1 drop)
- **Medium**: Semantic consistency of generated images (89.6%) and user study interpretability gains (18%)
- **Low**: Cross-dataset generalization, edge deployment latency, and long-term robustness

## Next Checks
1. Geographic hold-out test: Train on subset of regions, test on geographically disjoint cities to measure accuracy drop
2. Temporal robustness: Train on summer data, test on winter data to assess seasonal generalization
3. Prompt ablation study: Systematically vary prompt descriptors for each AQI level and measure semantic consistency to identify minimum viable prompt complexity