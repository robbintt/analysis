---
ver: rpa2
title: 'DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward
  Variance Management'
arxiv_id: '2505.12951'
source_url: https://arxiv.org/abs/2505.12951
tags:
- reward
- dgro
- reasoning
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DGRO, a novel reinforcement learning algorithm
  for enhancing LLM reasoning by decoupling the traditional regularization coefficient
  into two independent hyperparameters: one controlling the policy gradient strength
  and the other regulating the policy distance penalty. This decoupling enables more
  precise control over the exploration-exploitation trade-off and can be seamlessly
  extended to existing reward optimization frameworks.'
---

# DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management

## Quick Facts
- arXiv ID: 2505.12951
- Source URL: https://arxiv.org/abs/2505.12951
- Reference count: 40
- Primary result: DGRO achieves 96.9% accuracy on Logic dataset and state-of-the-art performance across mathematical benchmarks

## Executive Summary
This paper introduces DGRO, a novel reinforcement learning algorithm for enhancing LLM reasoning by decoupling the traditional regularization coefficient into two independent hyperparameters: one controlling the policy gradient strength and the other regulating the policy distance penalty. This decoupling enables more precise control over the exploration-exploitation trade-off and can be seamlessly extended to existing reward optimization frameworks. The authors also analyze the impact of reward variance on convergence speed and model performance, showing that higher variance facilitates faster learning. Empirically, DGRO achieves state-of-the-art performance on the Logic dataset with 96.9% accuracy and demonstrates strong generalization across mathematical benchmarks, significantly outperforming base models and other RL methods like GRPO and Online DPO.

## Method Summary
DGRO modifies the standard KL-regularized RL objective by decoupling the regularization coefficient β into two independent parameters: β1 scales the policy gradient term for exploration control, while β2 regulates the KL divergence penalty for exploitation control. The method uses online RL where the sampling policy equals the source policy, and approximates the intractable soft value function with group-wise mean rewards. Training involves generating multiple rollouts per prompt, scoring responses with rule-based rewards, computing group mean rewards, and updating the policy using the decoupled gradient formulation. The approach is evaluated on logic reasoning tasks (K&K dataset) and mathematical benchmarks (AIME, MATH, AMC) using base models like Qwen2.5-7B-Instruct and DeepSeek-R1-Distill-Qwen-7B.

## Key Results
- Achieves 96.9% pass@1 accuracy on K&K Logic dataset, outperforming GRPO and Online DPO
- Demonstrates strong generalization across mathematical benchmarks including AIME, MATH, and AMC
- Shows that reward variance in range [-5, 5] converges faster than lower variance ranges [-1, 1]
- Decoupled coefficients enable more precise exploration-exploitation control than unified regularization

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Exploration-Exploitation Control
Separating the regularization coefficient into β1 and β2 enables independent control of exploration and exploitation. β1 scales the policy gradient term while β2 independently regulates KL divergence from the old policy. This allows high exploration with controlled policy deviation, or vice versa. The assumption is that optimal balance varies during training and across tasks, requiring independent control. Evidence comes from the abstract and Section 3.2 describing the mechanism, though corpus support is limited. Break condition: If β1 and β2 have strong interaction effects that cannot be tuned independently, the decoupling benefit diminishes.

### Mechanism 2: Reward Variance Accelerates Convergence
Higher reward variance produces larger policy gradients, facilitating faster learning under bounded conditions. Theorem 3.4 bounds the policy gradient norm by a function proportional to Var[r]^(1/3). Higher variance increases the gradient signal magnitude, speeding up policy updates toward high-reward regions. The assumption is that rewards are bounded with finite variance and the policy uses Softmax parameterization. Evidence includes Theorem 3.4 and empirical results showing r ∈ [-5, 5] converges faster than r ∈ [-1, 1]. Break condition: If reward variance becomes too high relative to reward scale, gradient estimates may become unstable.

### Mechanism 3: Mean Reward Approximates Soft Value Function
Using group-wise mean reward r̄(x) as a surrogate for the intractable soft value function V(x, β) introduces bounded error that remains manageable. V(x, β) = β log Z(x) is intractable due to the partition function. Theorem 3.3 bounds the error, which diminishes as β → 0 or as the policy improves. The assumption is that the sampling policy equals the source policy in the online setting. Evidence includes Theorem 3.3's error bound derivation, though practical error quantification is limited. Break condition: When β is small and reward distribution has high spread, approximation error could bias optimization significantly.

## Foundational Learning

- Concept: **KL-regularized reinforcement learning objective**
  - Why needed here: DGRO modifies the standard KL-regularized reward maximization framework. Understanding the base formulation is essential to see what decoupling changes.
  - Quick check question: Can you explain why penalizing KL divergence from a reference policy stabilizes RL training for LLMs?

- Concept: **Soft value function and partition function**
  - Why needed here: The paper's theoretical analysis centers on approximating V(x, β) = β log Z(x). Without understanding why V(x, β) is intractable, the motivation for mean-reward approximation is unclear.
  - Quick check question: Why does computing Z(x) = Σ_y π_src(y|x) exp(r(x,y)/β) become intractable for language models with large vocabularies and long sequences?

- Concept: **Exploration-exploitation trade-off in policy optimization**
  - Why needed here: The core contribution is finer control over this trade-off. Practitioners need intuition for how β1 and β2 affect exploration vs. exploitation to tune them effectively.
  - Quick check question: If you observe high training entropy but slow accuracy improvement, which coefficient should you adjust and in what direction?

## Architecture Onboarding

- Component map: Rollout Generator -> Reward Scorer -> Group Aggregator -> Loss Computer -> Policy Updater

- Critical path:
  1. Generate G rollouts per prompt from π_old
  2. Score each response with rule-based rewards
  3. Compute group mean reward r̄(x)
  4. Calculate DGRO gradient: -β1(r - r̄)∇log π + β2∇(KL²)
  5. Update policy; synchronize π_old

- Design tradeoffs:
  - Higher G (rollout count): More accurate r̄(x) estimate, but increased compute cost
  - Higher reward range: Faster convergence (higher variance) but potential instability
  - β1 vs β2 tuning: β1 ↑ increases exploration (faster learning, risk of instability); β2 ↑ increases exploitation (stable but potentially slower)

- Failure signatures:
  - Entropy collapse early in training: β2 too high relative to β1 → premature convergence
  - Divergent loss with high variance rewards: β1 too high or reward scale too large
  - Slow convergence despite many steps: β1 too low or reward variance too small

- First 3 experiments:
  1. Baseline comparison: Replicate GRPO and Online DPO on K&K dataset with identical base model and training steps; measure accuracy gap.
  2. Ablation on (β1, β2): Sweep β1 ∈ {0.5, 1, 2} and β2 ∈ {0.01, 0.1, 1} on held-out validation set; plot convergence curves to identify optimal region.
  3. Reward variance impact: Train with r ∈ [-1, 1], [-3, 3], [-5, 5] on small subset; log gradient norms and convergence speed to validate Theorem 3.4 empirically.

## Open Questions the Paper Calls Out
- What are the optimal adaptive strategies for tuning β₁ and β₂ during training, and how do interactions between these parameters affect convergence?
- Can a principled, theoretically-grounded method be developed for automatic reward variance management instead of manual scaling?
- How does the approximation error of using ¯r(x) instead of V(x,β) affect final model performance in the practical regime where β is neither very small nor very large?
- What explains the divergent entropy dynamics between logic tasks (entropy decreases) and math tasks (entropy increases), and should entropy management be task-specific?

## Limitations
- Hyperparameter sensitivity and generalization across tasks not systematically explored
- Theoretical approximations under practical conditions not empirically validated
- Reward variance upper bounds not established, potentially leading to instability

## Confidence
- **High Confidence**: Mathematical derivations in Section 3, empirical results on Logic dataset, core decoupling mechanism
- **Medium Confidence**: Generalization across mathematical benchmarks, reward variance impact on convergence, comparison with GRPO/Online DPO
- **Low Confidence**: Practical impact of approximation error, independent tunability of β1 and β2, transferability of hyperparameters

## Next Checks
1. Conduct comprehensive grid search over (β1, β2) combinations on held-out validation set to test independent tunability
2. Implement exact soft value function computation for small subset and measure actual approximation error compared to r̄(x)
3. Systematically vary reward ranges beyond [-5, 5] while monitoring gradient norms, training stability, and convergence speed