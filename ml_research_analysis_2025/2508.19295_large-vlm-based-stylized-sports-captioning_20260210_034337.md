---
ver: rpa2
title: Large VLM-based Stylized Sports Captioning
arxiv_id: '2508.19295'
source_url: https://arxiv.org/abs/2508.19295
tags:
- sports
- lvlm
- caption
- images
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-level fine-tuned LVLM pipeline for
  generating stylized sports captions from football images in real-time. The pipeline
  addresses the challenge of accurate entity identification (players, coaches, celebrities)
  and natural language description in a specific caption format, overcoming limitations
  of existing models that lack domain-specific sports jargon.
---

# Large VLM-based Stylized Sports Captioning

## Quick Facts
- arXiv ID: 2508.19295
- Source URL: https://arxiv.org/abs/2508.19295
- Reference count: 16
- Generates stylized sports captions from football images in real-time with 91.2% BERT score

## Executive Summary
This paper presents a two-level LVLM pipeline for real-time stylized sports captioning, addressing the challenge of accurate entity identification and natural language description in sports contexts. The system processes football game images during live broadcasts, generating over 1000 captions during Super Bowl LIX at 6 images every 3-5 seconds. The approach uses 4-bit quantized models for memory efficiency (>10× VRAM savings) while maintaining low latency and meeting operational requirements.

## Method Summary
The pipeline employs a two-level fine-tuning approach: Level 1 uses SFT for player identification with confidence-based labeling and team roster context, while Level 2 generates captions with correct entity-action associations and stylized formatting. Level 1 outputs team name and jersey number with HIGH/LOW confidence labels, avoiding direct player name generation to reduce hallucination. Level 2 receives only HIGH-confidence entities and uses outputs from traditional vision models alongside image metadata to produce captions in the required Imagn format.

## Key Results
- 91.2% BERT score on generated captions
- 8-10% F1 score improvement for entity identification using confidence-based labeling
- Processes 6 images every 3-5 seconds during live games
- >10× VRAM savings via 4-bit quantization
- Generated over 1000 captions during Super Bowl LIX

## Why This Works (Mechanism)

### Mechanism 1
Decomposing caption generation into two sequential specialized levels with confidence-based filtering improves F1 scores by 8-10% over direct fine-tuning. Level 1 outputs only team name and jersey number with HIGH/LOW confidence labels, avoiding direct player name generation. This constrained vocabulary reduces hallucination by enabling deterministic roster lookup. Level 2 receives only HIGH-confidence entities, focusing attention on prominent players while filtering noise from background figures.

### Mechanism 2
Explicit HIGH/LOW confidence annotation during SFT teaches the LVLM to weight attention toward in-focus players, boosting true positive rates while reducing false positives from background figures. Annotators label visibility confidence based on player prominence (size, focus, occlusion). The model learns visual-feature-to-confidence mappings. At inference, this learned signal propagates forward, improving entity-level precision without manual thresholds.

### Mechanism 3
Decoder-based architectures for Level 1 enable post-training prompt optimization for out-of-distribution scenarios without full retraining. Decoder models generate tokens autoregressively conditioned on visual input, allowing test-time prompt adjustments to shift behavior. When training data lacks specific conditions, prompt engineering can compensate rather than requiring data collection and retraining.

## Foundational Learning

- **Large Vision-Language Models (LVLMs)**: The entire pipeline relies on LVLMs that fuse visual and linguistic information. Understanding cross-modal attention and how visual features condition text generation is essential for debugging both levels. Quick check: How does a decoder-based LVLM differ from a pipeline that runs an image classifier followed by a text generator?

- **Supervised Fine-Tuning (SFT) Templates**: Both levels use custom SFT templates that structure input format, context (roster), and output constraints. Template design directly affects hallucination rates. Quick check: What three fields would you include in an SFT template for player identification beyond just the player name?

- **Quantization (4-bit)**: The paper claims >10× VRAM reduction via 4-bit quantization, enabling production deployment. Understanding precision-accuracy tradeoffs is critical for resource planning. Quick check: If you quantize from 16-bit to 4-bit, what accuracy degradation might you expect, and how would you measure it?

## Architecture Onboarding

- Component map:
  ```
  [Image + Metadata + Roster]
           │
           ▼
  ┌─────────────────────────┐
  │ Level 1: LVLM Player    │  ← SFT, decoder-based, 4-bit
  │ Output: Team, Jersey #, │
  │         Confidence      │
  └─────────────────────────┘
           │
           ▼ (HIGH-confidence only)
  ┌─────────────────────────┐
  │ Auxiliary Vision Models │  ← Celebrity faces, logos, OCR, generic description
  └─────────────────────────┘
           │
           ▼
  ┌─────────────────────────┐
  │ Level 2: LVLM Caption   │  ← SFT, 4-bit
  │ Output: Stylized caption│
  └─────────────────────────┘
           │
           ▼
  [Imagn format: Date; Location; Entity-Action; Credits]
  ```

- Critical path: Image ingest → Level 1 (entity + confidence) → filter to HIGH → Level 2 (with auxiliary inputs) → formatted caption. Latency measured at 6 images per 3-5 seconds.

- Design tradeoffs:
  - Two-level vs. single: Adds deployment complexity but separates concerns (identification vs. language), reducing hallucination
  - Decoder vs. encoder for Level 1: Enables prompt-based OOD adaptation but authors acknowledge this needs more research
  - 4-bit quantization: >10× VRAM savings claimed, but paper does not report accuracy impact—validate before production

- Failure signatures:
  - Low entity recall: Check if confidence thresholds too aggressive or training data lacks occlusion variety
  - Hallucinated player names: Verify Level 1 outputs only jersey numbers (not names); check roster mapping integrity
  - Wrong action association: Level 2 may need more diverse action examples; review auxiliary vision model outputs
  - Style drift: Level 2 SFT data may need stronger style regularization

- First 3 experiments:
  1. Held-out game validation: Run pipeline on a completely unseen game (different teams/stadium). Compare F1 and BERT scores against zero-shot Llama3.2 11B to confirm 8-10% gains generalize.
  2. Confidence ablation: Compare Level 2 performance with all Level 1 outputs vs. HIGH-only filtering to quantify filtering's contribution.
  3. Quantization accuracy test: Run Level 1 and Level 2 at full precision vs. 4-bit on identical inputs. The paper claims efficiency but omits accuracy delta—this is essential for production decisions.

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific comparative advantages of decoder-based models versus encoder-based models for visual and video Large Language Models (LVLMs) when handling out-of-distribution sports data? The authors state this is ongoing research, having demonstrated decoder benefits for Level 1 but without systematic comparison.

### Open Question 2
How can scalable and accurate hyperparameter tuning be achieved for LVLMs under Supervised Fine-Tuning (SFT) or ORPO settings to reduce performance variability? The authors observed high variability in fine-tuned performance, suggesting current methodologies are unstable.

### Open Question 3
Why did preference alignment using ORPO fail to yield significant improvements in the captioning pipeline, and under what conditions might it be effective? The paper mentions ORPO did not improve performance but does not analyze the failure or compare with alternative alignment techniques.

## Limitations

- The claimed 8-10% F1 improvement from confidence-based annotation lacks direct ablation studies against baselines without confidence labels
- 4-bit quantization efficiency gain is reported without corresponding accuracy degradation metrics
- System performance on low-visibility conditions (night games, rain) and novel team formations is untested
- Annotation process required significant manual effort (confidence labeling, entity-action pairing), which may not scale

## Confidence

**High Confidence:**
- Two-level pipeline architecture and operational metrics (91.2% BERT score, 6 images/3-5 seconds)
- Memory efficiency benefit of 4-bit quantization is well-established
- Super Bowl LIX deployment results appear reliable

**Medium Confidence:**
- 8-10% F1 improvement from confidence-based annotation (plausible but lacks direct comparative evidence)
- Decoder-based generalization claim for OOD scenarios (reasonable but insufficiently validated)
- Entity-action association accuracy depends on auxiliary vision models

**Low Confidence:**
- BERT score of 91.2% lacks context about comparison baselines
- Hallucination reduction mechanism is theoretically sound but not empirically isolated
- Style consistency across different game contexts is asserted but not measured

## Next Checks

1. **Zero-shot baseline comparison**: Run the same test set through Llama3.2 11B with zero-shot prompting and compare F1 scores to validate the claimed 8-10% improvement.

2. **Quantization accuracy impact**: Systematically compare Level 1 and Level 2 outputs at full precision versus 4-bit across multiple metrics (F1 score, BERT score, hallucination rates).

3. **Confidence threshold ablation**: Create three variants (all outputs, HIGH-only, soft weighting) and compare downstream caption quality and entity recall to isolate confidence-based filtering's contribution.