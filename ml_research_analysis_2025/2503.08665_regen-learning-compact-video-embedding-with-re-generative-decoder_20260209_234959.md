---
ver: rpa2
title: 'REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder'
arxiv_id: '2503.08665'
source_url: https://arxiv.org/abs/2503.08665
tags:
- video
- latent
- compression
- decoder
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REGEN, a novel approach to video embedding
  that leverages a diffusion transformer (DiT) as a generative decoder rather than
  the traditional VAE-based architecture. The core idea is that effective latent representations
  for generative modeling should enable visually plausible synthesis rather than exact
  reconstruction.
---

# REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder

## Quick Facts
- **arXiv ID:** 2503.08665
- **Source URL:** https://arxiv.org/abs/2503.08665
- **Reference count:** 40
- **Key outcome:** Introduces REGEN, achieving superior video reconstruction quality at high compression ratios (up to 32× temporal) using a diffusion transformer decoder and content-aware positional encoding.

## Executive Summary
REGEN proposes a novel approach to video embedding by replacing traditional VAE-based decoders with a diffusion transformer (DiT) as a generative decoder. The key insight is that effective latents for generative modeling need only enable visually plausible synthesis rather than exact pixel reconstruction, allowing much higher compression ratios without sacrificing quality. The method employs content-aware positional encoding that enables generalization across arbitrary resolutions and aspect ratios. Experiments demonstrate that REGEN achieves state-of-the-art reconstruction quality at high compression ratios and enables efficient text-to-video generation with approximately 5× fewer latent frames than current approaches.

## Method Summary
REGEN learns compact video embeddings through an encoder-generator framework where the encoder produces spatiotemporal latents (content and motion) and a DiT-based generative decoder reconstructs the video from these latents. The core innovation is using a diffusion model as the decoder with a "visually plausible synthesis" objective rather than exact reconstruction, enabling higher compression ratios. A content-aware positional encoding mechanism, derived from the latents via a SIREN network, allows the DiT to generalize to arbitrary resolutions and aspect ratios. The approach supports flexible decoding schemes including one-step sampling without external distillation, and enables 4× to 32× temporal compression while maintaining high reconstruction quality.

## Key Results
- Achieves PSNR 29.27 on MCL-JCV at 4× temporal compression, outperforming MAGVIT-v2 by 0.85 points
- Maintains stable reconstruction quality (PSNR ~29) even with one-step sampling, demonstrating strong latent conditioning
- Enables 32× temporal compression while preserving semantic content, with 5× fewer latent frames needed for text-to-video generation
- Content-aware positional encoding improves resolution generalization from PSNR 23.39 to 29.41 at 2× resolution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing exact pixel reconstruction with "visually plausible synthesis" enables higher compression ratios without degrading generation quality.
- **Mechanism:** The encoder focuses only on preserving essential semantic and structural information, while the DiT decoder synthesizes realistic finer details during denoising. This bypasses the information bottleneck that limits traditional VAE decoders.
- **Core assumption:** The latent need not enable pixel-perfect reconstruction—only semantic fidelity sufficient for conditional generation.
- **Evidence anchors:** [abstract] "focus on synthesizing visually plausible reconstructions"; [section 1] "encoder can focus only on preserving the essential semantic and structural information"; [corpus] Progressive Growing of Video Tokenizers confirms temporal compression beyond 4× is challenging for standard tokenizers.
- **Break condition:** If downstream tasks require precise pixel correspondence (e.g., medical video analysis), the relaxed objective may fail.

### Mechanism 2
- **Claim:** Content-aware positional encoding enables DiT decoders to generalize to arbitrary resolutions and aspect ratios unseen during training.
- **Mechanism:** Instead of fixed coordinate-to-embedding maps, the latent expansion module generates positional embeddings conditioned on latents via a SIREN network and projector. This makes PE a function of content, not absolute coordinates.
- **Core assumption:** Content-aware PE preserves spatial/temporal structure without overfitting to training resolutions.
- **Evidence anchors:** [section 3.2] Fixed PE struggles with generalizing to unseen input sizes; [section 5, Table 3] In-context conditioning degrades from PSNR 25.71 → 23.39 at 2× resolution; REGEN improves 26.04 → 29.41; [corpus] No corpus paper explicitly addresses DiT generalization via content-aware PE.
- **Break condition:** If the SIREN network underfits or latents lack sufficient spatial detail, PE may not convey accurate position information, causing misalignment in generated frames.

### Mechanism 3
- **Claim:** Strong conditioning from latents enables few-step or one-step sampling without external distillation.
- **Mechanism:** The denoising task is easier than unconditional or text-conditional generation because latents provide dense, content-specific guidance. The model learns to map noisy inputs to clean outputs with minimal steps when the condition is informative.
- **Core assumption:** The latent is sufficiently expressive to guide the denoising trajectory in few steps.
- **Evidence anchors:** [section 5, Figure 7] PSNR and rFVD remain stable or improve from 1 to 100 steps; 1-step PSNR ~29 on MCL-JCV at 4× compression; [section 5] "our decoder is in fact tasked with an easier generation problem compared to text-to-video generation as it is equipped with a very strong conditioning signal"; [corpus] DGAE paper also uses diffusion-guided decoding for latent learning but does not report one-step sampling capability.
- **Break condition:** If latents are too compressed (e.g., 64× temporal) or the encoder fails to capture motion, one-step sampling may produce blurry or incoherent frames.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - **Why needed here:** REGEN is fundamentally an LDM component—the embedder defines the latent space where diffusion operates.
  - **Quick check question:** Can you explain why modeling in latent space is more efficient than pixel space for diffusion?

- **Concept: Diffusion Transformers (DiT) vs U-Net**
  - **Why needed here:** The decoder uses DiT, not U-Net; understanding the architectural differences (patches vs convolutions, scaling behavior) is essential.
  - **Quick check question:** What are the trade-offs between patch-based DiT and convolutional U-Net for video decoding?

- **Concept: Implicit Neural Representations (INR) / SIREN**
  - **Why needed here:** The latent conditioning module uses SIREN for time-coordinate mapping; understanding sinusoidal activations and continuous representation is key.
  - **Quick check question:** Why might a SIREN network be preferable to an MLP with ReLU for encoding continuous temporal coordinates?

## Architecture Onboarding

- **Component map:** Spatiotemporal Encoder -> Latent Conditioning Module -> DiT Decoder
- **Critical path:** Encoder output (z_c, z_m) → C_e produces z_e (same spatiotemporal size as target) → added to token + timestep embeddings → DiT blocks → unpatchify to video
- **Design tradeoffs:**
  - Patch size 8 matches spatial downsampling (8×); larger patches reduce cost but may lose detail
  - Encoder mirrors MAGVIT-v2 for compatibility; changing this affects latent semantics
  - Operating in pixel space (not latent) for decoding increases memory; mitigated by large patches
- **Failure signatures:**
  - Gridding artifacts at unseen resolutions → fixed PE used instead of content-aware PE
  - Temporal jumping at chunk boundaries → chunk-wise encoding without latent extrapolation
  - Blurry reconstructions at 1-step → encoder under-capacity or insufficient training
- **First 3 experiments:**
  1. Reproduce Table 1 comparison at 4× and 16× temporal compression on DAVIS 2019 (256×256); verify PSNR gap vs MAGVIT-v2
  2. Ablate conditioning: replace C_e with in-context concatenation; measure PSNR drop at 2× resolution (per Table 3)
  3. Train a small DiT decoder (e.g., 12 blocks) with same encoder; assess whether one-step sampling degrades and by how much

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the training efficiency of the diffusion-based decoder be improved through architectural optimization, specifically regarding the number of transformer blocks?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "training of diffusion-based decoders remains computationally expensive" and they "could not conduct extensive ablation on architecture configuration with the number of transformer blocks."
- **Why unresolved:** The high computational cost of training diffusion models prevented the authors from exploring the optimal model scaling laws or architectural efficiencies for this specific encoder-generator framework.
- **What evidence would resolve it:** An ablation study varying the depth (number of blocks) and width of the DiT decoder to plot the trade-off between training FLOPs and reconstruction metrics (rFVD/PSNR).

### Open Question 2
- **Question:** Can the "jumping" artifacts at chunk boundaries be fully eliminated through architectural changes rather than the current training-free mitigation?
- **Basis in paper:** [explicit] The authors note that while latent extension mitigates temporal discontinuities, it "cannot fully remove the jumping issue in such a training-free manner."
- **Why unresolved:** The chunk-wise encoding scheme is necessary for processing arbitrary video lengths, but it inherently introduces discontinuities at boundaries that inference-time tricks cannot completely smooth out.
- **What evidence would resolve it:** A modification to the training objective or encoder architecture (e.g., overlapping context windows) that results in seamless temporal consistency across chunk boundaries without requiring inference-time guidance.

### Open Question 3
- **Question:** Does operating the decoder in pixel space with a large patch size (8) impose a ceiling on generation quality compared to latent-space decoding?
- **Basis in paper:** [explicit] The paper lists as a limitation that the decoder operates in pixel space, requiring a "large patch size for efficiency (8 in our case), which can degrade generation quality... compared to the commonly used patch size of 1 or 2."
- **Why unresolved:** While the large patch size enables efficiency, it may compromise the decoder's ability to synthesize fine-grained details, creating a trade-off between the proposed method's efficiency and the maximal fidelity achievable by standard approaches.
- **What evidence would resolve it:** A comparative experiment where the DiT decoder operates in a latent space (allowing smaller patch sizes) to isolate the quality loss attributable specifically to the pixel-space constraint.

## Limitations
- **Dataset Dependence:** Trained on proprietary dataset of 15M videos and 300M images; generalization to other domains is unproven
- **SIREN Architecture Details:** Critical hyperparameters for the content-aware positional encoding module are underspecified
- **Diffusion Hyperparameters:** Training setup lacks detailed specification of noise scheduler, learning rate schedule, and weight decay parameters

## Confidence

- **High Confidence:** The general architecture design (DiT decoder with generative objective) and the core claim that relaxed reconstruction objectives enable higher compression ratios are well-supported by experimental results across multiple metrics and datasets
- **Medium Confidence:** The content-aware positional encoding mechanism's effectiveness is demonstrated empirically, but the underlying SIREN implementation details are incomplete
- **Low Confidence:** The one-step sampling capability is impressive but relies on strong latent conditioning that may not generalize well to more challenging video distributions or extreme compression ratios

## Next Checks

1. **Dataset Generalization Test:** Train REGEN on a public dataset (e.g., Kinetics-600 or WebVid-10M) and evaluate on the same test sets (MCL-JCV, DAVIS 2019). Compare performance degradation to proprietary dataset results to quantify dataset dependency.

2. **SIREN Hyperparameter Sensitivity:** Systematically vary the SIREN network architecture (layer depth, frequency scaling, hidden dimensions) while keeping other components fixed. Measure the impact on positional encoding quality and reconstruction performance, particularly at out-of-distribution resolutions.

3. **Compression Ratio Scaling Analysis:** Evaluate REGEN at compression ratios beyond 32× (e.g., 64× temporal) and measure the degradation in PSNR, SSIM, and rFVD. Compare with VAE-based approaches to determine the practical limits of the generative approach and identify the compression ceiling where the relaxed objective fails.