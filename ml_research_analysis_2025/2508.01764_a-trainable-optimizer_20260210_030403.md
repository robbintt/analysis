---
ver: rpa2
title: A Trainable Optimizer
arxiv_id: '2508.01764'
source_url: https://arxiv.org/abs/2508.01764
tags:
- trainable
- optimizer
- adam
- convex
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel trainable optimizer framework that
  jointly optimizes model parameters and gradient approximation during training. The
  key innovation is a pseudo-linear approximation of the full gradient that achieves
  O(1/t) convergence for strongly convex losses while simultaneously reducing variance
  to zero.
---

# A Trainable Optimizer

## Quick Facts
- arXiv ID: 2508.01764
- Source URL: https://arxiv.org/abs/2508.01764
- Reference count: 40
- Primary result: Novel trainable optimizer achieves O(1/t) convergence for strongly convex losses while reducing variance to zero

## Executive Summary
This paper introduces a trainable optimizer (TO) framework that jointly optimizes model parameters and gradient approximation during training. The key innovation is a pseudo-linear approximation of the full gradient that achieves O(1/t) convergence for strongly convex losses while simultaneously reducing variance to zero. The framework is computationally efficient, requiring only minimal additional tensor multiplications beyond standard gradient computation. Two memory-efficient variants - Diagonal TO and RankOne TO - are proposed to reduce the O(d²) parameter count of the full version to O(d), making the approach scalable to large models.

## Method Summary
The trainable optimizer framework approximates the gradient using a linear function of model weights: ∇F(wt) ≈ At·wt + bt. Both the model weights and optimizer variables (At, bt) are trained simultaneously through gradient descent on the approximation loss. The framework includes three variants: Full-TO (d² + d parameters), Diagonal TO (2d parameters), and RankOne TO (2d + d parameters). Theoretical analysis proves that both the approximation error and optimality gap converge at the same O(1/t) rate for strongly convex losses, while memory-efficient variants maintain this convergence property with significantly reduced parameter counts.

## Key Results
- TO variants significantly outperform ADAM in 16 out of 17 strongly convex cases, with 15 cases showing statistical significance (p<0.05)
- On non-convex tasks including ResNet training and Llama fine-tuning, TO achieves faster convergence and better final solutions
- Memory-efficient variants achieve 99.98% memory reduction (400.8 MB → 80.1 KB) while maintaining competitive performance
- TO shows particular promise for large language model fine-tuning, achieving rapid initial convergence while maintaining competitive final validation performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Jointly training optimizer variables and model weights creates a feedback loop that improves gradient approximation quality as optimization progresses.
- **Mechanism**: At each iteration, optimizer variables (At, bt) are updated via gradient descent on the L2 approximation loss l = ½∥gt - At·wt - bt∥², while model weights wt are updated using the approximated gradient. As wt approaches w*, the linear approximation becomes more accurate, reducing approximation error.
- **Core assumption**: The loss landscape is locally well-approximated by a quadratic (strongly convex) or can be locally linearized (non-convex), such that the pseudo-linear form captures essential gradient structure.
- **Evidence anchors**: [abstract] "pseudo-linear TO...matches SGD's convergence rate while effectively reducing variance"; [Section 3.1] "Drawing inspiration from the Taylor expansion of ∇F(w) centered at w*, we have ∇F(w) ≈ ∇²F(w*)(w - w*)"
- **Break condition**: If the loss landscape has high curvature or discontinuous gradients, the linear approximation degrades; Theorem 3.5 shows O(1/log T) convergence for non-convex cases, slower than O(1/T) for strongly convex.

### Mechanism 2
- **Claim**: Variance reduction emerges automatically from the approximation loss objective, eliminating the need for explicit variance reduction mechanisms like SVRG's snapshot gradients.
- **Mechanism**: The approximation loss explicitly minimizes ∥bGt - ∇F(wt)∥². As At and bt are refined, the stochastic approximation bGt converges toward the true gradient, reducing estimator variance. Theorem 3.4 proves both optimality gap and approximation error converge at O(1/t).
- **Core assumption**: Strong convexity of the loss function and bounded iterates/optimizer variables (Propositions 3.2-3.3).
- **Evidence anchors**: [abstract] "reducing variance to zero"; [Section 3.2, Theorem 3.4] "E[∥wt - w*∥²] ≤ M₁/(t+μ) and E[∥bGt - ∇F(wt)∥²] ≤ M₂/(t+μ)"
- **Break condition**: If learning rates αt, βt, γt violate the theorem's constraints (e.g., α sum exceeds 1/D²w), optimizer variables may become unbounded, breaking the convergence guarantee.

### Mechanism 3
- **Claim**: Memory-efficient variants (Diagonal TO, RankOne TO) preserve the core convergence properties while scaling to large models via parameterized structure constraints.
- **Mechanism**: Diagonal TO restricts At to diagonal matrices (element-wise multiplication), reducing parameters from d² + d to 2d. RankOne TO factorizes At = at·ctᵀ, also achieving O(d). The structured gradients are derived analytically, maintaining the approximation loss objective.
- **Core assumption**: The true gradient's structure is sufficiently captured by diagonal or rank-one transformations; this is an approximation of the full linear map.
- **Evidence anchors**: [abstract] "Two memory-efficient variants - Diagonal TO and RankOne TO - reduce O(d²) to O(d)"; [Section 3.3] Algorithm 2 and 3 show the modified update rules with ⊙ (element-wise) and outer product operations.
- **Break condition**: If the optimal gradient transformation requires full-rank interactions between dimensions, Diagonal and RankOne variants underperform Full-TO; empirical results show mixed performance (Table 2 shows some negative ρ for RankOne TO on MNIST).

## Foundational Learning

- **Concept**: Strong convexity and convergence rates
  - **Why needed here**: The O(1/t) convergence guarantee and variance reduction proof rely critically on strong convexity assumptions; understanding when this holds vs. general convex or non-convex cases determines applicability.
  - **Quick check question**: Can you explain why O(1/t) convergence for strongly convex losses is faster than O(1/√t) for general convex, and what property of strongly convex functions enables this?

- **Concept**: Variance in stochastic optimization
  - **Why needed here**: The core innovation is reducing estimator variance to zero; understanding why standard SGD/ADAM variance doesn't vanish helps appreciate the mechanism.
  - **Quick check question**: Why does SGD's gradient estimator variance remain bounded away from zero even as t → ∞, and how does SVRG (or TO) address this differently?

- **Concept**: Matrix factorizations and parameter efficiency
  - **Why needed here**: Diagonal and RankOne variants use structured matrix approximations; understanding the expressiveness vs. parameter tradeoff guides variant selection.
  - **Quick check question**: What is the rank of a matrix At = at·ctᵀ? How many parameters does this require vs. a full d×d matrix, and what expressiveness is sacrificed?

## Architecture Onboarding

- **Component map**: Input: wt, gt → Compute et = gt - At·wt - bt → Update optimizer variables via gradient descent on l → Compute bGt = At·wt + bt → Update wt+1 = wt - γt·bGt

- **Critical path**: The approximation error computation (line 5-6 in Algos 1-3) determines both optimizer update and the final gradient direction; numerical stability here is critical. Initialize A0 = 0, b0 = 0 as recommended in Section 3.1.

- **Design tradeoffs**:
  | Variant | Memory | Expressiveness | When to use |
  |---------|--------|----------------|-------------|
  | Full-TO | O(d²) | Full linear map | Small models (d < 10K), strong convex tasks |
  | Diagonal TO | O(d) | Per-dimension scaling | Large models, simpler loss landscapes |
  | RankOne TO | O(d) | Low-rank interactions | Large models with cross-feature interactions |

- **Failure signatures**:
  - Exploding optimizer variables: Check if ∑αt < 1/D²w (Proposition 3.3); reduce α or add gradient clipping.
  - No convergence improvement over ADAM: May indicate wrong variant for task geometry; try RankOne if features interact.
  - Slower initial convergence: Normal for early iterations when At, bt are poorly estimated; wait until warm-up completes.
  - NaN in optimizer updates: Approximation error et may be large; reduce learning rates α, β or use gradient clipping.

- **First 3 experiments**:
  1. **Strongly convex validation**: Train L2-regularized logistic regression on a small dataset (e.g., MNIST subset, d < 10K features) with Diagonal TO; compare training loss convergence to ADAM across 50 epochs. Expect ρ > 0 per Table 2.
  2. **Variant comparison on same task**: Run all three TO variants plus ADAM on CovType logistic regression; measure both convergence speed and peak memory usage. Verify Table 3 memory reductions hold in your implementation.
  3. **Non-convex stress test**: Fine-tune a small transformer (e.g., DistilBERT, d ~ 768 hidden dim) on a classification task; use Diagonal TO with recommended settings (α ≈ 0.01, β ≈ 1, exponential decay). Monitor if faster initial convergence materializes as shown in Figure 2(c).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical convergence rate for non-convex losses be improved from $O(1/\log T)$ to match standard stochastic rates like $O(1/\sqrt{T})$?
- Basis in paper: [inferred] Theorem 3.5 establishes a convergence rate of $O(1/\log T)$ for non-convex losses, which is significantly slower than the $O(1/T)$ rate achieved for strongly convex losses and slower than standard non-convex SGD analysis.
- Why unresolved: The paper proves this slower rate but does not determine if it is a fundamental limitation of the pseudo-linear approximation or an artifact of the proof technique.
- What evidence would resolve it: A modified theoretical analysis proving a polynomial rate or empirical scaling laws on ResNet/Llama showing iteration complexity consistent with $O(1/\sqrt{T})$.

### Open Question 2
- Question: Are there initialization strategies for the optimizer variables $A_0$ and $b_0$ that outperform the zero-initialization recommended in the text?
- Basis in paper: [inferred] Section 3.1 states that "The initialization of parameters $A_0$ and $b_0$ presents practical challenges" and relies on zero-initialization based on empirical observation.
- Why unresolved: The paper does not explore if warm-starting the optimizer matrix (e.g., approximating second-order information or momentum) could accelerate the early training phase.
- What evidence would resolve it: Comparative experiments on non-convex tasks (e.g., ResNet) benchmarking zero-init against structured initializations (such as identity or diagonal approximations).

### Open Question 3
- Question: How sensitive is the Trainable Optimizer to the specific choice of internal learning rates $\alpha$ and $\beta$?
- Basis in paper: [inferred] Section 4.1 details an extensive hyperparameter search grid for $\alpha$ and $\beta$ (ranging from 0.0 to 1.0), implying significant tuning is required.
- Why unresolved: While computationally efficient in memory, the method introduces two additional hyperparameters; the trade-off between tuning overhead and convergence speed is unclear.
- What evidence would resolve it: An ablation study quantifying the variance in final validation loss across the $\alpha, \beta$ grid to determine if robust default values exist.

## Limitations
- Strong convexity assumptions limit theoretical guarantees; O(1/log T) convergence for non-convex cases is significantly slower
- Memory-efficient variants show mixed performance, with RankOne TO sometimes underperforming (negative ρ values)
- Initialization strategy and learning rate schedules are critical but not extensively explored
- Theoretical bounds require bounded iterates and optimizer variables, but no clear criteria for when these bounds might be violated

## Confidence

**High Confidence**: The core computational framework (Algos 1-3) and memory efficiency claims are straightforward and verifiable. The theoretical analysis for strongly convex cases appears rigorous, and the convergence proofs follow standard techniques.

**Medium Confidence**: The variance reduction mechanism and its empirical manifestation. While Theorem 3.4 proves both approximation error and optimality gap converge at O(1/t), the practical significance of "variance to zero" in stochastic settings needs more validation, especially for non-convex tasks.

**Low Confidence**: The generalizability of TO to arbitrary deep learning architectures and loss landscapes. The paper shows promising results on ResNet and Llama fine-tuning, but the theoretical guarantees don't directly extend to these cases, and the ablation studies are limited.

## Next Checks

1. **Robustness to initialization**: Systematically vary the initialization of At and bt (e.g., random initialization, scaled versions of data statistics) across 10 runs of Full-TO on strongly convex tasks. Measure convergence variance and final performance distribution to assess sensitivity.

2. **Second-order landscape analysis**: For a subset of tasks (e.g., MNIST logistic regression), compute and visualize the empirical Hessian spectrum at different stages of TO optimization. Compare this to ADAM to verify whether TO's performance correlates with local curvature structure as the theory suggests.

3. **Variant selection criteria**: Design a diagnostic test to predict when Diagonal vs. RankOne vs. Full-TO will perform best on a given task. Use metrics like gradient cosine similarity between dimensions, feature correlation matrices, or task-specific domain knowledge to guide variant selection rather than trial-and-error.