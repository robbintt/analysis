---
ver: rpa2
title: 'If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task
  for LLMs'
arxiv_id: '2511.04432'
source_url: https://arxiv.org/abs/2511.04432
tags:
- wang
- zhang
- chen
- answer
- norwegian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well Large Language Models (LLMs) can
  perform temporal reasoning by answering questions as if it were 1940. Using a Norwegian
  trivia book from 1940, models were prompted in both English and Norwegian to answer
  questions while restricting their knowledge to pre-1940 information.
---

# If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs

## Quick Facts
- arXiv ID: 2511.04432
- Source URL: https://arxiv.org/abs/2511.04432
- Authors: Lars Bungum; Charles Yijia Huang; Abeer Kashar
- Reference count: 0
- Primary result: LLM performance on 1940 knowledge tasks scales with model size, with English prompting outperforming Norwegian

## Executive Summary
This study investigates how well Large Language Models (LLMs) can perform temporal reasoning by answering questions as if it were 1940. Using a Norwegian trivia book from 1940, models were prompted in both English and Norwegian to answer questions while restricting their knowledge to pre-1940 information. A strong LLM was used as an automatic grader with manual checks. Results show that performance scales with model size across four LLM families (DeepSeek-R1, Gemma3, Qwen3, Llama3.1), and that English prompting consistently outperformed Norwegian prompting. The largest Norwegian-focused model (NorwAI-Magistral24B) performed competitively with international models. Overall, even the best models achieved mid-eighties accuracy, indicating that temporal reasoning remains challenging despite strong general reasoning capabilities.

## Method Summary
The study used a Norwegian trivia book from 1940 containing 1000 question-answer pairs. Models were prompted to roleplay as a 1940s history quiz expert, answering questions while using only knowledge up to 1940. Both English and Norwegian prompts were tested across four model families at various sizes (1B to 70B parameters). A 120B parameter LLM served as an automatic grader, with manual checks validating the grading process. The task required models to both retrieve historical facts and apply temporal constraints to their reasoning.

## Key Results
- Performance scales consistently with model size across all four evaluated LLM families
- English prompting yielded higher accuracy than Norwegian prompting for all models except Llama3.1
- The 24B parameter Norwegian-focused NorwAI-Magistral model performed competitively with larger international models
- Even the best-performing models achieved only mid-eighties accuracy on the temporal reasoning task

## Why This Works (Mechanism)

### Mechanism 1: Parameter Scaling Enhances Temporal Knowledge Retrieval and Constraint Satisfaction
- Claim: As model parameter count increases, LLMs more reliably retrieve historical facts and adhere to the temporal constraint of "knowledge up to 1940."
- Mechanism: Larger models have greater capacity to store and interconnect vast amounts of training data, including more fine-grained historical facts. Furthermore, scaling laws suggest improved instruction-following capabilities. The task requires a two-part process: retrieve a fact *and* filter it through the 1940 constraint. Larger models appear better at maintaining this context window and applying the constraint during generation.
- Core assumption: The observed performance improvement is due to genuine improvements in reasoning and constraint satisfaction, not merely memorization of the specific 1940 trivia book used in evaluation.
- Evidence anchors:
  - [abstract] "Results show that performance scales with model size across four LLM families..."
  - [section 4] "...the Figure clearly demonstrates a clear trend between model size and performance the TR task."
  - [corpus] Related work (A Study into Investigating Temporal Robustness of LLMs) explores temporal question capabilities but does not validate the 1940 constraint-satisfaction mechanism at scale.
- Break condition: Performance plateaus or degrades at larger parameter sizes, or analysis reveals the test set was contaminated and present in the training data of the largest models.

### Mechanism 2: English Prompting Activates a More Robust Reasoning Substrate
- Claim: Prompting models in English, even for questions answered in Norwegian, yields higher accuracy by leveraging the models' stronger English-language training foundation.
- Mechanism: The majority of training data for the evaluated LLMs is in English, creating a more robust semantic and reasoning structure. When prompted in English, the model activates these stronger internal representations for the core deductive reasoning (temporal filtering) before generating an answer. The entire reasoning chain may be more reliable when initiated in the model's dominant language.
- Core assumption: The benefit stems from a more reliable reasoning process, not just from better tokenization. The model is not simply translating the question to English, answering, and translating back.
- Evidence anchors:
  - [abstract] "Prompting in English consistently gave better results than in Norwegian, an unexpected result."
  - [section 4] "We see that the English prompts are consistently better for all LLM families with the exception of Llama3.1..."
  - [corpus] No corpus papers directly compare prompting language effects on temporal reasoning tasks.
- Break condition: A model family trained predominantly on non-English data shows the reverse effect, or further analysis proves the advantage is solely due to more efficient tokenization in English.

### Mechanism 3: Language-Specific Fine-Tuning Compensates for Scale
- Claim: A model specifically fine-tuned for a target language (Norwegian) can achieve competitive performance on a culturally grounded task, even with a smaller parameter count than general-purpose giants.
- Mechanism: The NorwAI-Magistral24B model's strong performance is likely enhanced by training data with a higher density of high-quality Norwegian text, including relevant cultural and historical information. This specialized knowledge base compensates for the lack of scale. The paper speculates that its "reasoning capabilities" (chain-of-thought trace) may also contribute, but this is not isolated from language-specific training.
- Core assumption: The superior performance of NorwAI-Magistral24B is primarily due to its language-specific optimization, not an uncontrolled variable like its different quantization level (Q8_K_M vs Q4_K_M for others).
- Evidence anchors:
  - [abstract] "The largest Norwegian-focused model (NorwAI-Magistral24B) performed competitively with international models."
  - [section 5] "It is also worth noting that the effect of using warm (on top of pre-trained weights) models had a large impact on this task..."
  - [corpus] Related work (A Collection of Question Answering Datasets for Norwegian) confirms the domain but provides no direct comparison of this mechanism.
- Break condition: Another general-purpose model of similar (24B) size achieves equal or better performance without language-specific training, or ablation shows the Magistral architecture is the sole driver of performance.

## Foundational Learning

- **Concept: Temporal Knowledge Cutoff**
  - Why needed here: This is the core constraint of the entire task. The model must understand it cannot use any information discovered or created after a specific date (e.g., January 1, 1940).
  - Quick check question: If asked "Who is the current monarch of the United Kingdom?", should a model with a 2022 knowledge cutoff answer with Queen Elizabeth II or King Charles III? How does it know to stop?

- **Concept: Instruction Following as Constraint Satisfaction**
  - Why needed here: The model isn't just answering a trivia question; it's following a complex instruction to *roleplay* a 1940-era entity. This requires maintaining the temporal constraint throughout its reasoning process.
  - Quick check question: If you instruct a model to "only use words starting with 'A'," and then ask it to "describe the moon," what kind of output indicates it has successfully satisfied the constraint?

- **Concept: Evaluation via LLM-as-a-Judge**
  - Why needed here: The study's results depend entirely on the reliability of an automated grading system. Understanding the potential biases and failure modes of this evaluator is critical for interpreting the study's conclusions.
  - Quick check question: If a student answers "the 16th President" and the answer key says "Abraham Lincoln," a human grader would likely mark it correct. What are two ways an LLM judge might fail on this task?

## Architecture Onboarding

- **Component map:**
  - The Book -> Subject LLMs (Group A & B) -> Judge LLM (gpt-oss:120B) -> Accuracy Metrics
  - System Prompt -> Question -> Answer -> T/F Classification

- **Critical path:**
  1.  **Inference:** Subject LLM generates an answer based on the system prompt and a question from the dataset.
  2.  **Evaluation:** Judge LLM compares the generated answer to the ground truth answer from the book.
  3.  **Aggregation & Analysis:** Calculate accuracy, analyze scaling trends, and perform qualitative error analysis on consistently failed questions.

- **Design tradeoffs:**
  - **LLM-as-Judge vs. Human Evaluation:** Using an LLM judge trades potential bias/error for speed and cost compared to human graders. The paper mitigates this with manual spot checks.
  - **Quantization:** Using `Q4_K_M` for most models (faster, less memory) but `Q8_K_M` for the key NorwAI model introduces a confounding variable, as lower-bit quantization can degrade model performance.
  - **Prompting Language:** The decision to prompt in both English and Norwegian, while keeping questions in Norwegian, adds experimental complexity but reveals a key insight about the underlying reasoning substrate.

- **Failure signatures:**
  - **Grader Misinterpretation:** The judge may fail on close approximations or answers with extra information (e.g., see `Table 1` for manually identified grading mistakes).
  - **Subject Model Hallucination:** The model gives a confident, plausible, but factually incorrect answer.
  - **Subject Model Constraint Failure:** The model answers correctly for 2025 but not for 1940 (e.g., answering "Russia" instead of "Soviet Union").
  - **Semantic Shift Errors:** The model fails because a term's meaning has changed since 1940 (e.g., "gangsport" or "fyr").

- **First 3 experiments:**
  1.  **Establish Baseline & Scaling Law:** Run the full dataset (English prompts) across a single model family (e.g., Qwen3) at all available parameter sizes. Plot accuracy vs. model size to confirm the scaling trend.
  2.  **Isolate Prompting Language Effect:** Take the top-performing model from Experiment 1. Run the dataset using its Norwegian system prompt. Compare the results directly to the English-prompt run to quantify the language-based performance delta.
  3.  **Ablate Grader Reliability:** Take a random sample of 50 graded responses. Have a human fluent in Norwegian re-grade them. Calculate inter-annotator agreement between the LLM judge and the human to validate the core evaluation mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does model quantization affect performance on Temporal Reframing (TR) tasks compared to full-precision models?
- Basis in paper: [explicit] Section 6.1 (Future Work) explicitly calls for "analyzing the effects of different levels of quantization," noting the current work used only one quantization size per model.
- Why unresolved: The study utilized default quantization (mostly Q4_K_M) to fit models on GPUs, leaving the performance trade-off between compression and temporal reasoning fidelity unknown.
- What evidence would resolve it: A comparative evaluation of the same model families (e.g., DeepSeek-R1, Llama) running at different bit-depths (e.g., FP16, Q8, Q4) on the 1940 TR task.

### Open Question 2
- Question: Does the strong performance of the NorwAI-Magistral model derive primarily from its parameter count or its specific reasoning architecture (Chain-of-Thought traces)?
- Basis in paper: [explicit] Section 6.1 states a desire to "investigate further whether it came as a consequence of increased parameter size or the reasoning capabilities."
- Why unresolved: The NorwAI-Magistral24B performed competitively with larger international models, but the specific contribution of its "reasoning" architecture versus its size remains indistinguishable in the current results.
- What evidence would resolve it: An ablation study comparing the Magistral architecture against a standard dense model of equivalent parameter size (24B) on the same reasoning task.

### Open Question 3
- Question: Can Retrieval-Augmented Generation (RAG) or fine-tuning on historical instruction data effectively solve the Temporal Reframing challenge?
- Basis in paper: [explicit] Section 6.1 proposes experimenting with RAG solutions and creating instruction-data from copyright-expired books to "experiment with the effect on other TR tasks."
- Why unresolved: The current study evaluated static model weights. It is undetermined whether providing external historical context or specialized tuning can overcome the "temporal forgetting" or hallucination issues observed in the error analysis.
- What evidence would resolve it: Performance metrics of models augmented with a repository of pre-1940 text (RAG) or fine-tuned on synthetic historical datasets when attempting the same 1940 TR task.

## Limitations

- Temporal constraint enforcement is unclear - models may be memorizing 1940 answers rather than truly reasoning about temporal constraints
- Evaluation reliability depends on LLM judge with potential systematic biases and cultural blindspots
- Knowledge cutoff contamination cannot be ruled out as models were not audited for training data overlap with the 1940 trivia book

## Confidence

**High Confidence:**
- Larger models consistently achieve better performance on this temporal reasoning task across multiple model families
- English prompting provides a consistent performance advantage over Norwegian prompting
- The task is genuinely challenging, with even top models achieving only mid-eighties accuracy

**Medium Confidence:**
- The scaling trends observed will continue at larger model sizes
- The performance advantage of NorwAI-Magistral24B is primarily due to language-specific fine-tuning rather than other architectural differences
- The English prompting advantage stems from stronger reasoning substrates rather than tokenization efficiency

**Low Confidence:**
- The LLM judge reliably detects all temporal reasoning failures and constraint violations
- Models are genuinely reasoning about temporal constraints rather than retrieving memorized answers
- The performance differences between quantization levels (Q4_K_M vs Q8_K_M) are negligible

## Next Checks

1. **Temporal Constraint Ablation Study:** Run the same models on a modified dataset where questions have answers that changed at multiple points between 1940 and 2025 (not just one cutoff). This would test whether models can apply temporal constraints flexibly rather than memorizing specific 1940 answers.

2. **Cross-Cultural Temporal Transfer:** Evaluate the same models on a temporally constrained question set from a different cultural context (e.g., Japanese or Arabic history). This would help isolate whether the temporal reasoning capability is general or culturally specific to the training data.

3. **Manual Temporal Reasoning Audit:** Conduct a comprehensive human audit of 100+ model responses, specifically looking for temporal reasoning failures that an LLM judge might miss: semantic shifts, incremental answer changes, or cases where the model demonstrates knowledge beyond 1940 while arriving at a correct 1940 answer.