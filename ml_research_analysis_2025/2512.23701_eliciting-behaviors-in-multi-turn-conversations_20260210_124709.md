---
ver: rpa2
title: Eliciting Behaviors in Multi-Turn Conversations
arxiv_id: '2512.23701'
source_url: https://arxiv.org/abs/2512.23701
tags:
- target
- test
- multi-turn
- methods
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for evaluating and eliciting
  complex behaviors in large language models (LLMs) during multi-turn conversations.
  The authors categorize existing behavior elicitation methods into three families
  based on their interaction with the target model: prior knowledge only, offline
  interaction, and online interaction.'
---

# Eliciting Behaviors in Multi-Turn Conversations

## Quick Facts
- arXiv ID: 2512.23701
- Source URL: https://arxiv.org/abs/2512.23701
- Reference count: 30
- Key outcome: Online RL-based elicitation methods achieve 45/19/77% success rates on self-affirmation, inference memory, and jailbreaking tasks with few thousand queries, significantly outperforming static benchmarks.

## Executive Summary
This paper introduces EMBER, a framework for evaluating and eliciting complex behaviors in large language models during multi-turn conversations. The authors categorize existing behavior elicitation methods into prior knowledge only, offline interaction, and online interaction families, then propose a generalized multi-turn formulation using reinforcement learning. They evaluate these methods across three tasks: self-affirmation failure, inference memory violations, and jailbreaking, demonstrating that online methods achieve significantly higher success rates with fewer queries than static approaches.

## Method Summary
The EMBER framework uses a policy model (Qwen3-4B) to generate user-style prompts conditioned on conversation history and test objectives. Training employs GRPO with multi-turn rollouts where the policy generates n turns interleaved with target model responses. The method factorizes generation into high-level strategy then content, using n-gram overlap penalties to prevent repetition and format penalties to enforce structure. Training runs for 1-3 epochs with 32 samples per prefix, temperature=3, top-k=20, and exponential EOS decay to avoid incomplete sequences.

## Key Results
- Online methods achieve 45/19/77% average success rates across self-affirmation, inference memory, and jailbreaking tasks
- Static benchmarks find few or no failure cases in the same tasks
- Factorizing output into strategy and content improves success rate from ~40% to ~80%
- Multi-turn rollouts enable discovery of conversational failure patterns missed by single-turn methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online RL-based elicitation discovers model-specific failure patterns more efficiently than static or offline methods
- Mechanism: The policy samples prompt candidates, receives reward signals based on whether target behaviors were triggered, and updates via GRPO to shift the prompt distribution toward higher-reward regions
- Core assumption: The reward function accurately captures target behavior occurrence and the policy can explore the prompt space efficiently
- Evidence anchors: Abstract shows 45/19/77% success with few thousand queries; GRPO equations in section 4.3; Related paper "Eliciting Language Model Behaviors with Investigator Agents" supports RL-based elicitation

### Mechanism 2
- Claim: Multi-turn rollouts with interleaved policy-target turns enable discovery of conversational failure patterns that single-turn methods miss
- Mechanism: The policy generates n turns interleaved with target model responses, with rewards computed only at conversation end but gradients flowing through all policy tokens
- Core assumption: Target model behavior in later turns depends on earlier context and credit assignment across turns can be learned efficiently
- Evidence anchors: Section 4.3 extends single-turn to multi-turn with sequential dependencies; Figure 5b shows decreasing query efficiency with more turns; STREAM paper addresses multi-turn safety from defense perspective

### Mechanism 3
- Claim: Factorizing generation into high-level strategy then content improves exploration efficiency in large output spaces
- Mechanism: The policy first samples a strategy (e.g., "Challenge the answer"), then conditions content generation on that strategy
- Core assumption: Valid strategies are discrete/fewer than valid full prompts and the policy can learn to map situations to appropriate strategies
- Evidence anchors: Section 4.3 shows explicit factorization formula; Figure 5c demonstrates 40% to 80% improvement; no direct corroboration in neighbors

## Foundational Learning

- Concept: Reinforcement learning with policy gradient (specifically GRPO/PPO-family algorithms)
  - Why needed here: EMBER uses GRPO to optimize the prompt-generation policy; understanding advantage estimation, clipping, and reward normalization is essential
  - Quick check question: Can you explain why GRPO uses group-relative advantages (r − μ)/σ rather than absolute rewards?

- Concept: Language model fine-tuning paradigms (SFT vs. RL)
  - Why needed here: The paper compares offline SFT and online RL; knowing when each applies helps interpret tradeoffs
  - Quick check question: Why might SFT on WildChat generalize across objectives while RL is task-specific?

- Concept: Reward hacking and verifier reliability
  - Why needed here: All online methods depend on accurate rubrics; understanding reward misspecification is critical for debugging
  - Quick check question: If the jailbreaking verifier only checks for target string presence, what failure mode might occur?

## Architecture Onboarding

- Component map:
  Policy model (Qwen3-4B) -> Target model (various) -> Verifier/judge (LLM-based or string-based) -> GRPO trainer with reward normalization

- Critical path:
  1. Initialize policy from pretrained LLM with system prompt specifying test objective
  2. Sample G multi-turn conversations (policy generates, target responds)
  3. Compute rewards via verifier; normalize across G samples
  4. Update policy via clipped objective (Eq. 5); repeat for 1-3 epochs

- Design tradeoffs:
  - Single-turn vs. multi-turn: Multi-turn increases coverage but hurts query efficiency (Figure 5b)
  - Prior knowledge in system prompt: Adding test objectives helps; adding example strategies provides marginal benefit (Figure 5c)
  - Policy model size: Qwen3-4B vs. 8B shows comparable success rates (Table 6); smaller model is more efficient

- Failure signatures:
  - Repetitive turns: Policy generates identical sequences across turns; mitigated by n-gram overlap penalty
  - Reward hacking: Policy finds prompts that trigger verifier without true target behavior; mitigated by stronger verifier
  - Format collapse: Policy ignores strategy/content factorization; mitigated by format penalty

- First 3 experiments:
  1. Reproduce self-affirmation results with Qwen3-4B on Mistral v0.3; verify ~58% success rate matches Table 1
  2. Ablate strategy factorization: Compare "Test Objective" vs. "Test Objective + Strategy" system prompts; expect ~40% → ~80% improvement
  3. Test transferability: Train policy on Llama-3.1-8B, evaluate on Qwen3-8B; expect moderate transfer per Appendix B.3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-turn query efficiency be improved for behavior elicitation methods?
- Basis in paper: Section 5.3 states "How to improve the multi-turn query efficiency is a future direction to explore" after demonstrating that generating more turns decreases both query efficiency and success rate
- Why unresolved: The paper hypothesizes that reward attribution becomes harder with more turns, and initial policies are biased toward direct strategies rather than indirect ones that lead the model astray first
- What evidence would resolve it: Experiments with modified reward attribution methods, curriculum learning that introduces indirect strategies progressively, or hierarchical RL approaches that decompose multi-turn planning

### Open Question 2
- Question: How can the diversity of failure patterns discovered by online elicitation methods be increased without sacrificing success rate?
- Basis in paper: Section 5.4 notes "the systematic property of online methods discussed above seems to be at odds with diversity, as the method tends to converge to a single pattern"
- Why unresolved: The paper shows online methods converge to single high-success patterns while offline SFT produces more diverse samples; training multiple policies or reweighting penalty terms are suggested but not evaluated
- What evidence would resolve it: Ablation studies varying diversity-promoting reward penalties, or systematic comparison of success rate versus failure pattern entropy across training configurations

### Open Question 3
- Question: At what scale of test objectives do offline interaction methods become more query-efficient than online methods?
- Basis in paper: Section 5.2 claims offline efficiency "can potentially be improved with a large amount of test objectives (>100), as the offline interaction cost can be shared" but this is not empirically validated
- Why unresolved: Online methods outperform offline in the experiments, but the amortization argument for offline methods across many objectives remains theoretical since online interaction costs cannot be shared
- What evidence would resolve it: Experiments systematically varying the number of test objectives (10 to 100+) while measuring total queries and per-objective success rates for both method families

## Limitations

- The effectiveness of online methods depends heavily on verifier reliability, creating potential for reward hacking if the rubric has blind spots
- Evaluation is limited to three specific tasks (self-affirmation, inference memory, jailbreaking) which may not represent the full complexity of conversational vulnerabilities
- Multi-turn rollouts suffer from decreasing query efficiency as turn count increases, suggesting fundamental challenges in credit assignment across longer conversations

## Confidence

**High Confidence**: The superiority of online methods over static benchmarks is well-supported by empirical results showing 45/19/77% success rates versus near-zero success for static methods. Implementation details are sufficiently specified to reproduce core findings.

**Medium Confidence**: The claim that factorization into strategy and content significantly improves success rates is supported by Figure 5c, but exact implementation details of format penalty and strategy sampling are underspecified.

**Low Confidence**: The assertion that online methods achieve efficiency gains "with just a few thousand queries" lacks context about computational costs, which could be substantial given iterative interactions with target models.

## Next Checks

1. **Verifier Robustness Test**: Design adversarial prompts that trigger high rewards from the current verifier but don't actually produce target behaviors when evaluated by a stronger judge. Measure the gap between training rewards and evaluation performance to quantify reward hacking potential.

2. **Cross-Model Transferability**: Train policies on one model family (e.g., Mistral) and evaluate on substantially different architectures (e.g., Qwen3). Measure performance degradation to assess how model-specific the learned elicitation strategies are.

3. **Computational Cost Analysis**: Implement the complete training pipeline and measure wall-clock time, GPU memory usage, and API costs for different query budgets. Compare these costs against static benchmark generation to quantify claimed efficiency gains in practical terms.