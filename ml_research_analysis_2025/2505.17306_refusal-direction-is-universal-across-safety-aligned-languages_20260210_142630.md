---
ver: rpa2
title: Refusal Direction is Universal Across Safety-Aligned Languages
arxiv_id: '2505.17306'
source_url: https://arxiv.org/abs/2505.17306
tags:
- refusal
- languages
- language
- across
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the cross-lingual universality of refusal
  directions in large language models (LLMs), showing that refusal vectors extracted
  from one language can effectively modulate refusal behavior across 14 typologically
  diverse languages without additional fine-tuning. Using a newly created multilingual
  dataset (PolyRefuse), the authors demonstrate that refusal directions derived from
  English transfer to other languages with near-perfect effectiveness, and even more
  surprisingly, refusal vectors from any safety-aligned language generalize universally.
---

# Refusal Direction is Universal Across Safety-Aligned Languages

## Quick Facts
- arXiv ID: 2505.17306
- Source URL: https://arxiv.org/abs/2505.17306
- Authors: Xinpeng Wang; Mingyang Wang; Yihong Liu; Hinrich Schütze; Barbara Plank
- Reference count: 40
- One-line primary result: Refusal vectors from one safety-aligned language transfer universally across 14 typologically diverse languages without fine-tuning.

## Executive Summary
This paper investigates cross-lingual universality of refusal directions in large language models, demonstrating that refusal vectors extracted from any safety-aligned language can effectively modulate refusal behavior across all other safety-aligned languages without additional fine-tuning. Using a newly created multilingual dataset (PolyRefuse), the authors show that refusal vectors derived from English transfer to other languages with near-perfect effectiveness, and surprisingly, refusal vectors from any safety-aligned language generalize universally. The analysis reveals that refusal vectors are approximately parallel across languages in the model's activation space, explaining their transferability. However, the separation between harmful and harmless prompts is weaker in non-English languages, contributing to cross-lingual jailbreak vulnerabilities. These findings provide actionable insights for building more robust multilingual safety defenses and deepen the mechanistic understanding of cross-lingual vulnerabilities in LLMs.

## Method Summary
The paper extracts refusal directions using difference-in-means between harmful and harmless prompt activations at specific layer/token positions in the residual stream. For each language, they compute mean activations for harmful vs. harmless prompts, take the difference vector, and validate candidates by measuring refusal score reduction upon ablation (with KL divergence filtering ≤ 0.2). Cross-lingual transfer is tested by ablating the source-language refusal vector from target-language inputs and measuring compliance rate changes via WildGuard classifier. The PolyRefuse dataset contains translated harmful prompts (ADVBENCH, MALICIOUS INSTRUCT, TDC2023) and harmless prompts (ALPACA) across 14 languages with 128/32/572 train/val/test splits per language.

## Key Results
- Refusal vectors extracted from English transfer to 13 other languages with near-perfect effectiveness, reducing compliance rates to near-zero
- Refusal vectors from any safety-aligned language (German, Spanish, French, Italian, Dutch) transfer universally to all other safety-aligned languages
- Non-English languages show weaker separation between harmful and harmless clusters (Silhouette Scores drop from ~0.49 in English to ~0.22-0.32 in others), contributing to cross-lingual jailbreak vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Refusal Direction Universality
The model encodes refusal as a language-agnostic functional feature rather than language-specific surface patterns. Refusal vectors extracted from any safety-aligned language are approximately parallel in activation space, allowing ablation from any source language to remove the shared directional component governing refusal across all languages. This universality fails for safety-misaligned languages like Yoruba where the extracted vector doesn't capture meaningful refusal direction.

### Mechanism 2: Insufficient Harmful-Harmless Separation in Non-English Languages
Cross-lingual jailbreaks succeed because models fail to clearly separate harmful from harmless representations in non-English languages. Even with universal refusal directions, harmful inputs land in intermediate regions where the refusal signal is insufficiently strong. Silhouette Scores quantify this weaker clustering (English at 0.496 vs 0.22-0.32 for others), with jailbroken samples consistently falling between harmful and harmless clusters.

### Mechanism 3: Difference-in-Means Refusal Vector Extraction
A single direction in activation space, computed as the mean difference between harmful and harmless prompt activations, captures sufficient signal to control refusal behavior. At specific layer and token positions, this vector when ablated across all layers removes refusal behavior; when added at a single layer, it enhances refusal. The single-direction approximation assumes refusal is mediated by a low-dimensional subspace.

## Foundational Learning

- **Concept: Residual Stream and Activation Space**
  - Why needed here: The paper's intervention operates directly on Transformer residual stream activations. Without understanding that activations encode semantic features as directions, the ablation/addition operations are opaque.
  - Quick check question: Can you explain why subtracting a projection onto a vector direction from the residual stream removes that feature's influence?

- **Concept: Difference-in-Means as Concept Vector Extraction**
  - Why needed here: The core methodology extracts refusal directions by contrasting activations between harmful and harmless prompt sets. Understanding contrastive methods is essential to interpret and debug the extraction.
  - Quick check question: Given two sets of activations, how would you compute a direction that maximally distinguishes them, and what assumptions does this make about their distributions?

- **Concept: Cross-Lingual Transfer and Language-Agnostic Representations**
  - Why needed here: The paper's central claim rests on refusal being encoded in a language-agnostic manner. Understanding how multilingual models share representations across languages explains why this transfer works.
  - Quick check question: In a multilingual model, why might semantically equivalent sentences in different languages produce similar activations in middle-to-late layers?

## Architecture Onboarding

- **Component map:** Tokenized prompts -> Residual stream activations -> Difference-in-means extraction -> Vector ablation/addition -> WildGuard evaluation

- **Critical path:**
  1. Prepare PolyRefuse dataset: Translate harmful (ADVBENCH, MALICIOUS INSTRUCT, TDC2023) and harmless (ALPACA) prompts to target languages
  2. Extract candidate refusal vectors: Compute difference-in-means at each layer/token position using 128 training samples per class
  3. Validate and select: Ablate each candidate on 32 validation samples; select vector with maximum refusal score reduction (subject to KL divergence < 0.2)
  4. Cross-lingual intervention: Apply ablation to test set (572 samples/language) and measure compliance rate change

- **Design tradeoffs:**
  - Extraction layer selection: Earlier layers capture more language-specific features; later layers capture more abstract/functional features. Empirically finds middle-to-late layers most effective.
  - Single vs. multi-direction: Assumes single direction suffices; corpus evidence suggests orthogonal directions may improve robustness but adds complexity.
  - Token position selection: Final instruction tokens work best; earlier positions may capture prompt content rather than refusal signal.

- **Failure signatures:**
  - High baseline compliance for a language: Indicates safety-misaligned language (e.g., Yoruba at 82.9% in Llama3.1); refusal vector extraction may fail or transfer poorly.
  - Low Silhouette Score: Harmful/harmless clusters overlap significantly; model cannot reliably distinguish harmful content regardless of refusal direction quality.
  - KL divergence spike post-ablation: Ablation is degrading general model capabilities, not just refusal; may indicate vector is not cleanly capturing refusal.

- **First 3 experiments:**
  1. Baseline safety audit: Run all 14 languages through WildGuard without intervention to identify safety-misaligned languages (compliance > 10%). Flag these for careful interpretation.
  2. Within-language extraction validation: Extract refusal vector from language L using L's data, ablate, verify refusal collapses on L's test set. Confirms extraction methodology works per-language.
  3. Cross-lingual transfer test: Extract refusal vector from English, ablate on all 14 languages, plot compliance rate change. Expect near-universal transfer for safety-aligned languages; check for exceptions (e.g., script-divergent languages like Thai/Korean if underrepresented in training).

## Open Questions the Paper Calls Out

- **Can concrete defense strategies be designed to enhance separation of harmful and harmless content in the embedding space, thereby improving multilingual safety?**
  - Basis in paper: Limitations section explicitly states they don't evaluate concrete defense strategies despite identifying the problem of insufficient harmful/harmless separation.
  - Why unresolved: This work identifies the problem but does not propose or evaluate mitigation strategies.
  - What evidence would resolve it: Demonstrating a training or intervention method that increases Silhouette Scores for harmful/harmless clustering in non-English languages, with corresponding reductions in cross-lingual jailbreak compliance rates.

- **Does refusal direction universality extend to languages with extremely limited representation in model pretraining corpora?**
  - Basis in paper: Limitations section states findings may not extend to languages with extremely limited data.
  - Why unresolved: The 14 languages studied may not cover the full spectrum of under-represented languages.
  - What evidence would resolve it: Experiments on a broader set of extremely low-resource languages showing whether refusal vector transferability correlates with pretraining data volume.

- **Why do models fail to establish robust harmful/harmless boundaries in non-English languages despite learning universal refusal directions?**
  - Basis in paper: The paper demonstrates parallel refusal vectors across languages yet shows weaker harmful/harmless clustering in non-English languages without explaining this discrepancy.
  - Why unresolved: The paper establishes this phenomenon empirically but does not investigate the underlying cause.
  - What evidence would resolve it: Analysis correlating harmful/harmless separation quality with factors like safety training data composition per language, or intervention studies showing improved separation from targeted multilingual safety fine-tuning.

## Limitations
- The study focuses on specific 8B-parameter models and their instruction-tuned variants, limiting generalizability to other model families and sizes.
- The single-direction approximation may oversimplify complex safety mechanisms that could require multi-directional approaches for robust control.
- Evaluation focuses primarily on compliance rate changes and clustering metrics without addressing long-term effects on model capability or user experience.

## Confidence
- **High Confidence**: Cross-lingual refusal direction universality for safety-aligned languages (Llama3.1, Qwen2.5, gemma-2 show consistent patterns with quantifiable metrics)
- **Medium Confidence**: Mechanism of approximately parallel refusal vectors explaining transfer (supported by empirical results but relies on simplified geometric interpretation)
- **Medium Confidence**: Cross-lingual jailbreak vulnerability due to weaker harmful-harmless separation (clustering metrics show quantitative differences but causal link to jailbreaks requires further validation)
- **Low Confidence**: Universal applicability to all multilingual models and safety contexts (limited to studied models and prompt types)

## Next Checks
1. **Cross-Model Validation**: Test refusal direction transfer on models outside the studied set (e.g., Mistral, DeepSeek, or smaller 3B-parameter variants) to verify universality claims beyond the specific model family.

2. **Adversarial Robustness Testing**: Evaluate whether attackers can craft prompts that specifically break the assumed parallel structure of refusal directions, or exploit the weaker clustering in non-English languages to create universal jailbreaks.

3. **Longitudinal Capability Impact**: Monitor model performance on non-safety tasks (e.g., MMLU, HELM) before and after refusal direction ablation across all 14 languages to quantify capability trade-offs and ensure no hidden degradation.