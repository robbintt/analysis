---
ver: rpa2
title: 'UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning'
arxiv_id: '2510.20286'
source_url: https://arxiv.org/abs/2510.20286
tags:
- instruction
- reasoning
- grounding
- arxiv
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of GUI grounding, where the
  goal is to map natural language instructions to actionable UI elements in a screenshot.
  Prior work has largely treated instructions as static inputs, overlooking the impact
  of instruction diversity and quality on grounding performance.
---

# UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning

## Quick Facts
- **arXiv ID:** 2510.20286
- **Source URL:** https://arxiv.org/abs/2510.20286
- **Authors:** Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, Steven Hoi
- **Reference count:** 38
- **Primary result:** Introduces Instruction-as-Reasoning paradigm, achieving state-of-the-art GUI grounding with 87.3% on UI-I2E-Bench and 74.1% on AndroidWorld online agent benchmark

## Executive Summary
This paper addresses the challenge of GUI grounding by introducing the Instruction-as-Reasoning paradigm, which treats instructions as dynamic analytical pathways rather than static inputs. The authors identify that 23.3% of instructions in existing datasets contain quality flaws, and demonstrate that leveraging instruction diversity can yield up to 76% relative performance improvements without retraining. Their approach synthesizes diverse, high-quality instructions from four distinct reasoning perspectives (appearance, functionality, location, intent) and uses a two-stage training framework combining supervised fine-tuning and reinforcement learning to instill multi-perspective reasoning capabilities in vision-language models.

The resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results across five grounding benchmarks, demonstrating emergent reasoning capabilities that selectively compose and synthesize novel instruction pathways at inference. The 32B parameter model achieves 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2, while the 7B model achieves 74.1% success rate on the AndroidWorld online agent benchmark, validating the approach's effectiveness in both offline evaluation and real-world deployment.

## Method Summary
The method introduces Instruction-as-Reasoning, a paradigm that transforms natural language instructions into dynamic analytical pathways with distinct reasoning perspectives. The approach employs a two-stage training framework: first, supervised fine-tuning on synthesized diverse instructions to instill multi-perspective reasoning, then reinforcement learning to optimize pathway selection and composition. The synthesis pipeline leverages GPT-4.1 to generate and verify diverse instructions across four perspectives (appearance, functionality, location, intent), addressing the quality flaws found in 23.3% of existing dataset instructions. The models, UI-Ins-7B and UI-Ins-32B, are built on InternVL-2.5 vision encoder and Qwen2.5-VL base models, achieving state-of-the-art performance through emergent reasoning capabilities that compose novel instruction pathways at inference.

## Key Results
- UI-Ins-32B achieves 87.3% on UI-I2E-Bench, outperforming previous state-of-the-art by significant margins
- 76% relative improvement in grounding performance achieved by leveraging instruction diversity without retraining
- UI-Ins-7B demonstrates 74.1% success rate on AndroidWorld online agent benchmark, validating real-world applicability
- Models exhibit emergent reasoning capabilities, composing novel instruction pathways at inference time
- State-of-the-art results across five grounding benchmarks including ScreenSpot-Pro (57.0%) and MMBench-GUI L2 (84.9%)

## Why This Works (Mechanism)
The approach works by treating instructions as dynamic analytical pathways rather than static inputs, allowing the model to leverage diverse reasoning perspectives. By synthesizing high-quality instructions across appearance, functionality, location, and intent dimensions, the model learns to flexibly combine different reasoning strategies. The two-stage training framework first instills these multi-perspective reasoning capabilities through supervised fine-tuning, then optimizes pathway selection through reinforcement learning. This enables the model to adapt its reasoning strategy based on the specific characteristics of each instruction and screenshot pair, rather than relying on a single fixed interpretation.

## Foundational Learning
- **Instruction-as-Reasoning paradigm**: Treating instructions as dynamic analytical pathways rather than static inputs; needed to address instruction quality flaws and diversity limitations; quick check: compare performance on diverse vs. static instruction sets
- **Multi-perspective synthesis**: Generating instructions across appearance, functionality, location, and intent dimensions; needed to capture different reasoning strategies; quick check: ablation study removing individual perspectives
- **Two-stage training**: Combining supervised fine-tuning with reinforcement learning; needed to both learn reasoning patterns and optimize their selection; quick check: compare performance of SFT-only vs. full RL-trained models
- **Vision-language grounding**: Mapping natural language to actionable UI elements; needed for practical GUI interaction applications; quick check: evaluate on diverse UI layouts and instruction types
- **Emergent reasoning**: Model's ability to compose novel instruction pathways; needed for generalization beyond training data; quick check: test on out-of-distribution instructions requiring compositional reasoning
- **Instruction quality analysis**: Identifying and addressing flaws in existing datasets; needed to establish baseline performance limitations; quick check: manual review of instruction quality before and after synthesis

## Architecture Onboarding

**Component Map:**
Vision Encoder (InternVL-2.5) -> Multi-Perspective Instruction Encoder -> Multi-Modal Fusion -> Grounding Head

**Critical Path:**
Input screenshot and instruction → Vision encoder extracts visual features → Instruction encoder processes multi-perspective reasoning → Multi-modal fusion combines visual and textual representations → Grounding head predicts bounding box coordinates

**Design Tradeoffs:**
- Large vision encoder (InternVL-2.5) provides detailed UI element understanding but increases computational cost
- Multi-perspective instruction synthesis requires GPT-4.1 but ensures high-quality training data
- Two-stage training (SFT + RL) balances stable initial learning with optimized reasoning selection
- Choice between 7B and 32B parameter models trades off performance vs. deployment efficiency

**Failure Signatures:**
- Lack of domain-specific knowledge (fails on brand-specific entities)
- Visual ambiguity and hallucination (confuses similar UI elements)
- Lack of layout understanding ability (struggles with complex structural layouts)
- Limited perspective coverage (fails on instructions requiring novel reasoning strategies)

**First Experiments:**
1. Test model performance on synthetic instructions vs. human-verified instructions to assess GPT-4.1 synthesis impact
2. Evaluate ablation study removing individual reasoning perspectives to measure their contribution
3. Test model on adversarially constructed instructions requiring compositional reasoning not seen during training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can external domain-specific knowledge be effectively integrated into the Instruction-as-Reasoning framework to resolve failures in identifying abstract brand entities?
- **Basis in paper:** Section 4.6 (Error Analysis) explicitly identifies "Lack of Domain-Specific Knowledge" (e.g., failing to associate "building block toys" with "MEGA" instead of "Jazwares") as a primary failure mode.
- **Why unresolved:** The current framework relies on the model's parametric knowledge, which is insufficient for specific real-world entity grounding.
- **What evidence would resolve it:** A study integrating a Retrieval-Augmented Generation (RAG) module into the reasoning pipeline, showing improved accuracy on benchmarks containing proper nouns or brand-specific implicit instructions.

### Open Question 2
- **Question:** Can the "emergent" reasoning pathways (compositional or novel perspectives) be quantitatively distinguished from memorized SFT patterns in out-of-distribution scenarios?
- **Basis in paper:** Section 4.5 claims the model exhibits "emergent capabilities" to synthesize novel reasoning perspectives, but the analysis relies primarily on qualitative examples rather than quantitative metrics on unseen data distributions.
- **Why unresolved:** It is unclear if the model is truly generalizing reasoning strategies or simply interpolating between the four fixed perspectives seen during SFT.
- **What evidence would resolve it:** Ablation studies on adversarially constructed instructions that force the model to use compositional strategies (e.g., "State" + "Group Affiliation") not explicitly labeled in the training data.

### Open Question 3
- **Question:** Does the reliance on GPT-4.1 for data synthesis and verification propagate teacher model biases or hallucinations into the UI-Ins student models?
- **Basis in paper:** Section 3.2 states that the data pipeline leverages GPT-4.1 to both generate diverse instructions and verify them. While the paper notes a reduction in flaw rates, it does not analyze if GPT-4.1's specific reasoning style limits the diversity of the synthetic data.
- **Why unresolved:** Using a strong teacher model may inadvertently filter out valid but unconventional human reasoning pathways or introduce subtle, hard-to-detect hallucinations.
- **What evidence would resolve it:** A comparative analysis of model performance when trained on human-verified data vs. GPT-4.1-verified data, specifically examining error rates related to "Visual Ambiguity and Hallucination."

### Open Question 4
- **Question:** How can the architecture be modified to explicitly handle "Lack of layout understanding ability" identified in complex UIs?
- **Basis in paper:** Section 4.6 (Error Analysis) lists "Lack of layout understanding ability" as a distinct failure mode where the model fails to discern the correct clickable area within a structural layout.
- **Why unresolved:** The current vision encoder may lack the inductive bias necessary to parse hierarchical UI structures effectively.
- **What evidence would resolve it:** Experiments incorporating explicit layout features (e.g., DOM tree embeddings or edge detection maps) into the visual encoder to improve accuracy on benchmarks with dense, grid-like interfaces.

## Limitations
- Reliance on synthesized instructions raises questions about domain shift and real-world applicability
- Performance gains may not transfer to truly diverse, real-world scenarios beyond evaluated benchmarks
- The model's ability to handle complex, non-linear multi-perspective combinations is not extensively validated
- Instructions falling outside the four predefined perspectives may not be effectively processed
- Specific reward structure and RL hyperparameter sensitivity are not fully detailed, affecting reproducibility

## Confidence
- **High:** Empirical findings within evaluated benchmarks are well-supported and clearly demonstrated
- **Medium:** Instruction-as-Reasoning paradigm's general applicability is supported but limited by acknowledged constraints
- **Low:** Scalability and robustness to truly diverse, real-world GUI environments remains unproven

## Next Checks
1. Evaluate UI-Ins on a held-out test set of real-world instructions not used in training or synthesis to assess domain generalization and the impact of instruction quality on performance
2. Conduct an ablation study to determine the relative contribution of each of the four perspectives and the effect of instruction diversity on downstream performance, particularly for complex, multi-step instructions
3. Test the model's robustness by introducing adversarial or out-of-distribution instructions that require reasoning beyond the predefined perspectives or involve novel UI elements not seen during training