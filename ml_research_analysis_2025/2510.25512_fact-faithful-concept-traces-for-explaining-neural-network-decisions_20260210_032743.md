---
ver: rpa2
title: 'FaCT: Faithful Concept Traces for Explaining Neural Network Decisions'
arxiv_id: '2510.25512'
source_url: https://arxiv.org/abs/2510.25512
tags:
- concepts
- concept
- fact
- b-cos
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model with inherent concept-based explanations
  that are faithful to the model's decisions. Unlike prior work, FaCT does not make
  restrictive assumptions on concepts, such as being class-specific, small, or aligned
  with human expectations.
---

# FaCT: Faithful Concept Traces for Explaining Neural Network Decisions

## Quick Facts
- arXiv ID: 2510.25512
- Source URL: https://arxiv.org/abs/2510.25512
- Reference count: 40
- Primary result: Model provides inherently concept-based explanations faithful to decisions without restrictive assumptions on concepts

## Executive Summary
FaCT introduces a novel approach to explainable AI that extracts faithful concept-based explanations from neural networks without making restrictive assumptions about concepts. Unlike prior work, FaCT concepts can be class-agnostic, high-dimensional, and need not align with human expectations. The method uses B-cos transforms and bias-free sparse autoencoders to create a structural decomposition where output logits can be exactly attributed to concept contributions. A novel C2-score metric leverages foundation models to evaluate concept consistency without human annotations.

## Method Summary
FaCT inserts a bias-free TopK sparse autoencoder (SAE) at an intermediate layer of a B-cos backbone network. The B-cos transforms ensure the entire network can be expressed as a dynamic linear transform, enabling exact attribution from concept activations back to input pixels. The SAE bottleneck produces sparse concept activations that are used to reconstruct features for the classifier. This architecture ensures that output logits can be strictly decomposed into a linear sum of concept contributions. The method is trained in two stages: first the B-cos backbone is pre-trained, then the SAE is trained on extracted features to minimize reconstruction loss.

## Key Results
- Concept contributions can be faithfully traced from output logits back to input pixels via B-cos transforms
- C2-score shows positive correlation with human concept ratings without requiring human annotations
- User study confirms participants find FaCT's concepts more interpretable than baseline methods
- Performance drops less than 3% on ImageNet compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1: Structural Faithfulness via Bias-Free Sparse Bottlenecks
The architecture uses a bias-free SAE bottleneck that produces sparse concept activations. Because the SAE has no learnable offsets and the classifier operates on the reconstruction, the final logit becomes a strictly additive combination of concept contributions. This structural constraint ensures mathematical faithfulness.

### Mechanism 2: Dynamic Linearity for Tracing (B-cos)
B-cos transforms replace standard layers with ones where weights dynamically align with inputs, allowing the network to be summarized as $y = \tilde{W}(x)x$. This dynamic linearity enables exact attribution from any concept activation back to input pixels, ensuring visualizations "add up" to activation values.

### Mechanism 3: C2-Score for Annotation-Free Consistency
The C2-score evaluates concept consistency by measuring semantic similarity of concept attributions in DINOv2 feature space. It weights foundation model features by attribution maps and computes cosine similarity across images, providing an unsupervised metric that correlates with human judgment.

## Foundational Learning

- **Concept: Dynamic Linearity (B-cos)**
  - Why needed: Standard deep networks are non-linear compositions, making exact decomposition impossible. B-cos allows expressing output as $W_{dynamic} \cdot x$ for a specific input.
  - Quick check: Can you explain why a B-cos layer allows you to write the output $y$ as $W_{dynamic} \cdot x$, whereas a standard ConvNet cannot?

- **Concept: Superposition and Sparse Autoencoders (SAEs)**
  - Why needed: Neural network neurons are often "polysemantic" (responding to multiple unrelated features). SAEs decompose dense vectors into sparse spaces where individual dimensions are more likely to be monosemantic.
  - Quick check: Why does the "TopK" activation function in an SAE favor interpretability over a standard ReLU?

- **Concept: Faithfulness vs. Plausibility**
  - Why needed: This paper strictly optimizes for *faithfulness* (mathematical truth) rather than *plausibility* (human-prettiness). Understanding this distinction is critical for evaluating results.
  - Quick check: If an explanation looks like a dog to a human but the model actually used the texture of the fur alone, is the explanation faithful?

## Architecture Onboarding

- **Component map:** Pre-trained B-cos Backbone -> Bias-Free TopK-SAE Bottleneck -> B-cos Classifier Head
- **Critical path:** Training the SAE bottleneck correctly with zero biases and proper sparsity constraints
- **Design tradeoffs:**
  - Sparsity (TopK) vs Accuracy: Lower TopK increases interpretability but drops ImageNet accuracy; balance needed (e.g., 16-32)
  - Layer Position: Early layers yield simple concepts (colors/edges); later layers yield semantic concepts (objects)
- **Failure signatures:**
  - "Always-on" Latents: Concepts activating on >60% of data, representing data mean
  - Dead Latents: Concepts that never fire due to learning rate/initialization issues
  - Sharp Accuracy Drop: If SAE reconstruction is too sparse, classifier fails
- **First 3 experiments:**
  1. Sanity Check (Eq. 9): Verify concept contributions sum exactly to logits
  2. Concept Deletion (Fig. 7): Remove top concepts and measure logit drop vs random concepts
  3. Visual Trace Inspection: Visualize "Wheel" (Late) and "Yellow" (Early) concepts on School Bus

## Open Questions the Paper Calls Out

### Open Question 1
Can a model without uninterpretable concepts achieve the same performance as FaCT on ImageNet? The paper acknowledges FaCT tolerates some uninterpretable concepts but hasn't tested if these are necessary for performance.

### Open Question 2
Can FaCT be trained end-to-end from scratch with concept interpretability regularization, rather than using the two-stage approach? This could potentially improve concept quality by coupling feature learning with interpretability.

### Open Question 3
What are the limitations of DINOv2 features for evaluating concept consistency via C2-score, and are there better alternatives? The reliance on a single foundation model may not capture all concept semantics.

### Open Question 4
Does FaCT's faithful concept-based explanation enable more effective model steering and concept-editing in domains beyond image classification? Applications to language models and other modalities remain unexplored.

## Limitations

- C2-score correlation with human judgment (0.46 Spearman) is modest, suggesting the metric may not fully capture human concept interpretability
- Architectural constraint of bias-free SAEs may limit bottleneck expressiveness and explain modest performance drops
- Does not address potential shortcuts or spurious correlations that might be captured by extracted concepts

## Confidence

- **High confidence** in mathematical framework for faithful decomposition via B-cos and bias-free SAEs
- **Medium confidence** in practical utility of concepts given user study preference but moderate C2-score correlation
- **Low confidence** in claim that FaCT concepts are "more consistent" than baselines due to reliance on proposed metric

## Next Checks

1. Replicate exact logit decomposition (Eq. 9) on trained FaCT model to verify mathematical faithfulness property
2. Compare C2-score rankings against direct human evaluation of concept consistency on subset of concepts
3. Test FaCT on out-of-distribution data to assess whether concepts capture spurious correlations or generalize beyond training patterns