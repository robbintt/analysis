---
ver: rpa2
title: 'Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach'
arxiv_id: '2601.05269'
source_url: https://arxiv.org/abs/2601.05269
tags:
- manuscripts
- illustrations
- pages
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a scalable deep-learning pipeline to automatically
  identify, extract, and describe illustrations in digitized historical manuscripts.
  The method combines page-level classification (EfficientNet), illustration localization
  (YOLO), and vision-language captioning (LLaVA) to filter text-only pages and focus
  on visual content.
---

# Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach

## Quick Facts
- arXiv ID: 2601.05269
- Source URL: https://arxiv.org/abs/2601.05269
- Reference count: 40
- Primary result: Scalable deep-learning pipeline achieves 95.1% classification accuracy and processes over 5 million Vatican Library pages in days, enabling large-scale manuscript illustration analysis.

## Executive Summary
This paper presents a three-stage deep-learning pipeline to automatically identify, extract, and describe illustrations in digitized historical manuscripts. The system combines EfficientNet classification to filter text-only pages, YOLO object detection to localize illustrations, and LLaVA vision-language captioning for semantic descriptions. Processing speed reaches under 0.06 seconds per page, enabling analysis of over five million Vatican Library pages within days. The method supports keyword-based retrieval and reveals cross-manuscript visual communities through CLIP embedding similarity graphs.

## Method Summary
The pipeline uses a two-stage filtering approach where EfficientNet-B0 first classifies pages as "art" or "no-art" with 95.1% accuracy, then YOLOv11n localizes illustrations with 75.6% mAP@0.5. Surviving pages pass to LLaVA for caption generation. CLIP embeddings create similarity graphs connecting illustrations across manuscripts. The system processes each page in under 0.06 seconds versus 51 seconds for pixel-level segmentation, achieving 850× speedup while maintaining high retrieval effectiveness.

## Key Results
- Classification model achieves 95.1% accuracy, 78.6% precision, and 74.6% recall
- YOLOv11n localizes 350,000 illustrations with 75.6% mAP@0.5 and 0.79 recall
- LLaVA captioning provides semantic descriptions suitable for keyword-based retrieval
- Processing speed enables analysis of over 5 million Vatican Library pages within days
- CLIP-based similarity graphs reveal coherent visual communities across manuscripts

## Why This Works (Mechanism)

### Mechanism 1
A two-stage filtering approach (page classification → object detection) dramatically reduces computational cost compared to pixel-level segmentation while maintaining high retrieval effectiveness. EfficientNet-B0 first classifies pages as "art" or "no-art," discarding >90% of text-only pages. Surviving pages pass to YOLOv11n for bounding-box localization. This reduces per-page processing from ~51 seconds to <0.06 seconds.

### Mechanism 2
Vision-language captioning (LLaVA) enables keyword-based semantic retrieval that would be impossible with visual embeddings alone. LLaVA generates natural-language descriptions for each cropped illustration. Captions are stored alongside images and indexed for text search. Scholars can query "winged horse" or "angel holding a sword" and retrieve matching illustrations without manual cataloging.

### Mechanism 3
CLIP embedding-based similarity graphs reveal cross-manuscript visual communities invisible to page-level analysis. Each cropped illustration is embedded into CLIP's shared vision-language space. Cosine similarity connects each image to its 50 nearest neighbors, forming a graph. Community detection clusters images by shared visual features across manuscripts, time periods, and traditions.

## Foundational Learning

- **Transfer learning with fine-tuning**: Why needed? EfficientNet classifier and YOLO detector are initialized from ImageNet/COCO pre-trained weights and fine-tuned on ~20,000 manuscript pages. Quick check: Can you explain why freezing the backbone and training only the classification head first improves fine-tuning stability?
- **Object detection vs. semantic segmentation**: Why needed? The paper explicitly rejects pixel-level segmentation in favor of bounding-box detection for scalability. Quick check: What is the computational complexity difference between predicting ~10 bounding boxes per image vs. predicting class labels for ~1 million pixels?
- **Vision-language model architectures (CLIP, LLaVA)**: Why needed? CLIP provides embeddings for similarity graphs; LLaVA generates captions. Both rely on contrastive image-text pretraining. Quick check: Why does CLIP enable zero-shot classification and cross-modal retrieval without task-specific training?

## Architecture Onboarding

- **Component map**: [Manuscript Pages (IIIF)] → [EfficientNet-B0 Classifier] → "art" / "no-art" → [YOLOv11n Detector] → cropped illustrations → [LLaVA Captioning] → text descriptions → [Search Database] ← metadata, IIIF URLs + [CLIP Embeddings] → similarity graph (k=50 neighbors)
- **Critical path**: Classification accuracy is the bottleneck—if too many illustrated pages are filtered out, downstream detection and captioning cannot recover them. The paper reports 74.6% recall at the optimal threshold.
- **Design tradeoffs**: Precision vs. recall at classification stage: Lowering threshold increased recall to 83% but raised false positives. YOLO is 850× faster but provides coarser localization. Off-the-shelf vs. fine-tuned VLM: LLaVA was used without fine-tuning.
- **Failure signatures**: Classifier false negatives: Illustrated pages mislabeled "no-art" disappear from pipeline entirely. Detector merged boxes: Multiple illustrations may be detected as a single region. Caption hallucination: LLaVA may describe "a man" instead of recognizing a known religious figure.
- **First 3 experiments**: 1) Reproduce classification on a held-out subset from a different manuscript collection. 2) Ablate the classification stage: run YOLO directly on all pages and compare runtime, detection recall, and false positive rate. 3) Evaluate caption quality systematically using LLaVA, BLIP, and Florence-2 with domain expert ratings.

## Open Questions the Paper Calls Out

- **Domain Transfer Gap**: How robust is the pipeline when applied to digitized manuscript collections from diverse cultural traditions and libraries outside the specific training set used in this study? The authors note that current training was limited to the Vatican Library and Bible of Borso d'Este, and distinct visual conventions in non-Western traditions may trigger false negatives.

- **Captioning Benchmarking**: How does the performance of off-the-shelf vision-language models compare when evaluated using established quantitative metrics versus qualitative human assessment for manuscript illustrations? The authors note their evaluation was limited to qualitative comparison and call for systematic investigation using BLEU, CIDEr metrics.

- **Embedding Domain Gap**: Do visual embeddings derived from modern datasets accurately capture the stylistic and iconographic nuances of historical manuscript art, or do they project modern visual biases onto historical material? The authors explicitly warn that CLIP was trained on modern image datasets rather than manuscript art.

- **Hierarchical Annotation**: Can a hierarchical annotation scheme resolve the high false-positive rate observed in the detection stage? The paper reports moderate precision because the model frequently detected valid "sub-illustrations" that were not present in the ground truth.

## Limitations

- Limited validation of off-the-shelf VLMs (LLaVA, CLIP) generalization to medieval manuscript art and iconographic detail
- Underspecified annotation guidelines for sub-illustration detection affecting ground truth consistency
- Performance metrics based on single manuscript collection (Vatican Library) with uncertain generalizability to other historical periods and writing systems

## Confidence

- **High Confidence**: Classification pipeline effectiveness (95.1% accuracy, 78.6% precision) and computational efficiency gains (850× speedup)
- **Medium Confidence**: YOLO detection performance (75.6% mAP@0.5) validated on internal data but real-world effectiveness uncertain
- **Medium Confidence**: Captioning quality (~75% accuracy on 100 samples) based on qualitative evaluation rather than systematic human benchmarking
- **Low Confidence**: CLIP-based similarity graph insights demonstrated qualitatively but lack systematic evaluation against art-historical ground truth

## Next Checks

1. Apply the complete pipeline to a non-Vatican manuscript collection and measure performance drops in classification, detection, and captioning
2. Remove the classification stage and run YOLO directly on all pages for a 10,000-page subset, comparing total runtime, detection recall, and false positive rate
3. Generate captions for 200 illustrations using LLaVA, BLIP, and Florence-2, have domain experts rate accuracy and usefulness, and compute BLEU/CIDEr against human-written reference captions