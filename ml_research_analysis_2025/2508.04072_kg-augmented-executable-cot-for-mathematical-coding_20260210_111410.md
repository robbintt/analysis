---
ver: rpa2
title: KG-Augmented Executable CoT for Mathematical Coding
arxiv_id: '2508.04072'
source_url: https://arxiv.org/abs/2508.04072
tags:
- code
- reasoning
- mathematical
- kga-ecot
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  models' performance on complex mathematical reasoning tasks, where they often struggle
  with multi-step reasoning and code generation. The proposed KG-Augmented Executable
  Chain-of-Thought (KGA-ECoT) framework integrates structured reasoning, knowledge-enhanced
  retrieval via GraphRAG, and executable code generation to overcome these limitations.
---

# KG-Augmented Executable CoT for Mathematical Coding

## Quick Facts
- arXiv ID: 2508.04077
- Source URL: https://arxiv.org/abs/2508.04077
- Reference count: 9
- This paper proposes KGA-ECoT, a framework that improves LLM mathematical reasoning by integrating structured task graphs, knowledge-enhanced retrieval via GraphRAG, and executable code generation.

## Executive Summary
This paper addresses the challenge of improving large language models' performance on complex mathematical reasoning tasks, where they often struggle with multi-step reasoning and code generation. The proposed KG-Augmented Executable Chain-of-Thought (KGA-ECoT) framework integrates structured reasoning, knowledge-enhanced retrieval via GraphRAG, and executable code generation to overcome these limitations. KGA-ECoT decomposes problems into a Structured Task Graph, leverages hierarchical graph embeddings for precise knowledge retrieval from mathematical libraries, and generates verifiable code to ensure computational accuracy. Evaluations on GSM8K, MATH-500, and SV AMP benchmarks show KGA-ECoT significantly outperforms existing methods, achieving accuracy improvements ranging from 2.36 to 3.59 percentage points across different datasets and model scales. Ablation studies confirm the critical roles of GraphRAG in enhancing code quality and external code execution in ensuring precision, establishing KGA-ECoT as a robust and generalizable framework for complex mathematical reasoning.

## Method Summary
KGA-ECoT is a five-node pipeline that enhances LLM mathematical reasoning through structured task decomposition and executable code generation. The framework builds a directed task graph by analyzing problems, extracting conditions, and modeling dependencies. It uses hierarchical graph embedding to retrieve relevant mathematical functions from a SymPy knowledge graph, generates Python code using retrieved information, executes the code in a Docker sandbox for computational verification, and validates the final answer. The method addresses the limitations of traditional Chain-of-Thought reasoning by providing explicit structure and computational verifiability.

## Key Results
- KGA-ECoT achieves significant accuracy improvements across all datasets and model scales, with gains ranging from 2.36 to 3.59 percentage points
- External code execution provides the largest performance boost, with ablation showing drops of 6-11 percentage points when removed
- GraphRAG enhances code quality, though it showed mixed results with larger models on simpler tasks
- The framework demonstrates strong generalizability across different mathematical reasoning benchmarks (GSM8K, MATH-500, SV AMP)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing mathematical problems into structured task graphs with explicit dependencies improves reasoning coherence and enables systematic code generation.
- Mechanism: The Build Solution node parses problems through five stages (Goal Analysis → Conditions Extraction → Problem Decomposition → Dependency Modeling → Task Sorting), producing a directed task graph where nodes represent subtasks and edges encode execution order. This structure constrains the reasoning space and makes dependencies explicit for downstream code generation.
- Core assumption: LLMs perform better when external structure guides decomposition rather than relying on implicit sequential reasoning.
- Evidence anchors:
  - [abstract] "KGA-ECoT decomposes problems into a Structured Task Graph"
  - [section: Method] Figure 1 shows the full pipeline with explicit dependency modeling and task sorting
  - [corpus] CODEPLAN (Wen et al. 2025) uses similar code-based planning but generates non-executable pseudocode; KGA-ECoT extends this to executable code
- Break condition: If problems cannot be cleanly decomposed into sequential subtasks (e.g., problems requiring iterative refinement or backtracking), the rigid task graph structure may fail.

### Mechanism 2
- Claim: Hierarchical graph embedding that fuses semantic embeddings with graph topology improves retrieval precision for mathematical library functions.
- Mechanism: Instead of using raw node embeddings, the method propagates parent-node embeddings down the hierarchy using: `en = w · xn + (1 − w) · ep`, where `xn` is the bge-m3 semantic embedding and `ep` is the parent embedding. This addresses the distribution mismatch between graph-structure-dependent node embeddings and text-semantics-dependent query embeddings.
- Core assumption: The heterogeneity between node embeddings (topology-driven) and query embeddings (semantics-driven) is a primary bottleneck in GraphRAG retrieval precision.
- Evidence anchors:
  - [section: Related Work] Explicitly cites Gautam et al. (2024) on embedding distribution mismatch: "Lossmismatch = Dist(Enode, Equery)"
  - [section: Hierarchical graph embedding] Describes the weighted fusion formula and propagation from root to leaf
  - [corpus] Weak direct corpus evidence; neighbor papers focus on CoT scaling and code generation rather than embedding alignment
- Break condition: If the knowledge graph structure is flat (lacks clear hierarchy) or if mathematical functions lack meaningful parent-child semantic relationships, the propagation mechanism provides minimal benefit.

### Mechanism 3
- Claim: External code execution in an isolated environment ensures computational verifiability and catches errors that text-based reasoning cannot.
- Mechanism: Generated Python code executes in a Docker container. Successful execution returns numerical results; failures are logged and passed to the Ans Question node for fallback reasoning. This shifts computation from probabilistic token generation to deterministic execution.
- Core assumption: LLM arithmetic and symbolic manipulation are less reliable than Python interpreter execution for mathematical tasks.
- Evidence anchors:
  - [abstract] "generates verifiable code to ensure computational accuracy"
  - [section: Ablation experiment] Removing external execution causes the largest performance drops: DS-7B on GSM8K drops from 86.13% to 79.91%, L3.2-3B on MATH-500 drops from 43.40% to 32.60%
  - [corpus] SEER (neighbor paper) similarly uses execution-guided CoT refinement for code generation
- Break condition: If generated code has syntax errors, uses unavailable libraries, or hits runtime resource limits, the execution fails. The fallback to Ans Question recovery may produce incorrect answers.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: KGA-ECoT builds on CoT but restructures it into a task graph with external code execution. Understanding baseline CoT helps contrast why the structured, executable approach improves on free-form textual reasoning.
  - Quick check question: Can you explain why CoT struggles with multi-step arithmetic where intermediate errors propagate?

- Concept: **Knowledge Graph Construction and GraphRAG**
  - Why needed here: The framework uses a SymPy-based directed acyclic graph with Catalogue Nodes and Callable Nodes. Understanding how entities, relationships, and hierarchical structures are represented is prerequisite to grasping the embedding and retrieval mechanism.
  - Quick check question: What is the difference between vector similarity search (traditional RAG) and graph-traversal-based retrieval (GraphRAG)?

- Concept: **Embedding Space Alignment**
  - Why needed here: The paper explicitly addresses distribution mismatch between node embeddings and query embeddings. Understanding cosine similarity, embedding dimensions, and semantic vs. structural representations is necessary to evaluate the hierarchical fusion approach.
  - Quick check question: Why might a node embedding trained on graph topology fail to match a text-based query embedding semantically?

## Architecture Onboarding

- Component map:
  - Input: Mathematical problem text (natural language)
  - Build Solution: Decomposes into task graph with goal, conditions, subtasks, dependencies
  - GET Query: Hierarchical graph embedding + cosine similarity retrieval from SymPy KG (268 catalog nodes, 3923 callable nodes)
  - Coding: LLM generates Python code using retrieved function signatures/docs
  - Run Code: Docker-isolated execution; returns result or error log
  - Ans Question: Validates output; fallback reasoning if execution fails
  - Output: Final answer with verification

- Critical path:
  1. Problem → Build Solution (task graph quality determines downstream success)
  2. Task graph → GET Query (retrieval precision affects code correctness)
  3. Retrieved knowledge + task graph → Coding (code executability is the main failure point)
  4. Code → Run Code (execution environment must match library versions)
  5. Execution result → Ans Question (final validation and potential recovery)

- Design tradeoffs:
  - **Weight parameter `w` in embedding fusion**: Lower `w` emphasizes local semantics; higher `w` incorporates global context. Paper does not specify optimal values per dataset.
  - **Docker isolation vs. speed**: Sandboxed execution adds latency but prevents malicious or runaway code.
  - **GraphRAG vs. simpler retrieval**: Building and maintaining the SymPy KG requires upfront cost; benefit is most pronounced on MATH-500 (advanced mathematics) rather than GSM8K (elementary arithmetic).

- Failure signatures:
  - **Code execution errors**: Syntax errors, undefined functions, version mismatches (SymPy API changes)
  - **Retrieval misses**: Query embedding fails to match relevant callable nodes; generates code without needed functions
  - **Task graph errors**: Incorrect dependency modeling leads to wrong execution order
  - **Fallback over-reliance**: If Ans Question node frequently recovers from execution failures, the system degrades to text-based CoT

- First 3 experiments:
  1. **Reproduce ablation**: Run KGA-ECoT with GraphRAG disabled on MATH-500 subset; verify reported ~4-5 point drop on advanced problems
  2. **Embedding weight sweep**: Test `w ∈ {0.2, 0.4, 0.6, 0.8}` on retrieval precision; measure code executability rate
  3. **Error taxonomy**: Execute full pipeline on 100 GSM8K problems; classify failure modes (decomposition error, retrieval miss, code error, execution error) to identify highest-leverage improvements

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the GraphRAG module introduce retrieval noise that degrades performance in larger models on simpler reasoning tasks?
  - Basis in paper: The authors note in the ablation study that removing GraphRAG improved performance for DeepSeek-14B on GSM8K and SV AMP, hypothesizing potential "fine-tuning in the knowledge graph construction" or the introduction of "minor noise."
  - Why unresolved: The paper reports the anomaly but does not isolate the specific retrieval errors or embedding misalignments responsible for this performance drop in the larger model.
  - What evidence would resolve it: A qualitative error analysis of the retrieved nodes for the DeepSeek-14B model, identifying instances where retrieval added distracting context versus helpful syntax.

- **Open Question 2**: Can the KGA-ECoT framework be effectively adapted to reasoning domains outside of mathematics (e.g., legal or logistical reasoning) without significant structural re-engineering?
  - Basis in paper: The conclusion claims the framework possesses "high flexibility and generality" and can "tailor knowledge graphs to specific reasoning scenarios," though experiments are restricted to mathematical benchmarks.
  - Why unresolved: The current validation relies entirely on mathematical datasets (GSM8K, MATH-500) and a single knowledge source (SymPy), leaving the generalizability of the hierarchical task graph unproven in other contexts.
  - What evidence would resolve it: Evaluation on non-mathematical reasoning benchmarks using a domain-specific knowledge graph constructed via the proposed hierarchical embedding method.

- **Open Question 3**: How sensitive is the retrieval precision to the specific value of the weighting coefficient $w$ used in the hierarchical graph embedding fusion?
  - Basis in paper: The method section introduces an adjustable weighting coefficient $w$ to balance local semantic features with global context, but the experimental section does not provide a sensitivity analysis for this parameter.
  - Why unresolved: It is unclear if the reported performance depends heavily on tuning $w$ for the specific mathematical domain, or if the method is robust to variations in this weight.
  - What evidence would resolve it: Ablation results showing model performance and retrieval accuracy across a range of values for $w$ (e.g., 0.0 to 1.0).

## Limitations

- **Knowledge Graph Construction Overhead**: Building and maintaining the SymPy knowledge graph requires significant upfront effort and may not generalize easily to other domains
- **Prompt Engineering Sensitivity**: The framework's performance depends heavily on prompt templates for each pipeline node, which are not disclosed in the paper
- **Computational Cost**: Docker-based code execution adds latency and computational overhead compared to pure text-based reasoning approaches

## Confidence

- **Method**: High - The five-node pipeline architecture is clearly described with specific implementation details
- **Reproducibility**: Medium - Key details like prompt templates and exact hyperparameters are missing
- **Generalizability**: Medium - Claims of domain flexibility are not experimentally validated beyond mathematics
- **Novelty**: High - Combines structured task graphs, GraphRAG, and executable code generation in a novel way

## Next Checks

1. **Prompt Template Reconstruction**: Create systematic prompt templates for each pipeline node based on the described functionality and test on a small dataset subset
2. **SymPy KG Construction Validation**: Verify the knowledge graph construction methodology by building a small test graph and checking embedding propagation
3. **Error Mode Classification**: Run the full pipeline on 50-100 problems from GSM8K and MATH-500, classifying each failure by root cause (decomposition, retrieval, code, execution) to identify improvement priorities