---
ver: rpa2
title: 'MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation'
arxiv_id: '2507.10543'
source_url: https://arxiv.org/abs/2507.10543
tags:
- learning
- success
- tasks
- robot
- flowpolicy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MP1 introduces a MeanFlow-based approach for 1-step policy learning
  in robotic manipulation, addressing the slow inference of diffusion models and the
  consistency constraints of flow-based methods. By learning interval-averaged velocities
  via the MeanFlow Identity, MP1 eliminates the need for ODE solvers and consistency
  losses, enabling genuine single-step trajectory generation.
---

# MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation

## Quick Facts
- arXiv ID: 2507.10543
- Source URL: https://arxiv.org/abs/2507.10543
- Reference count: 6
- Primary result: 10.2% higher success rates than DP3 and 7.3% higher than FlowPolicy on Adroit and Meta-World benchmarks, with 6.8 ms inference time

## Executive Summary
MP1 introduces a MeanFlow-based approach for 1-step policy learning in robotic manipulation, addressing the slow inference of diffusion models and the consistency constraints of flow-based methods. By learning interval-averaged velocities via the MeanFlow Identity, MP1 eliminates the need for ODE solvers and consistency losses, enabling genuine single-step trajectory generation. It further incorporates Classifier-Free Guidance for improved controllability and a lightweight Dispersive Loss to enhance generalization in few-shot learning without affecting inference speed. Evaluated on Adroit and Meta-World benchmarks, MP1 achieves 10.2% higher success rates than DP3 and 7.3% higher than FlowPolicy, with 6.8 ms inference time—19× faster than DP3 and nearly 2× faster than FlowPolicy.

## Method Summary
MP1 addresses the fundamental limitations of diffusion and flow-based methods in robotic manipulation by introducing a MeanFlow approach that learns interval-averaged velocities directly from state trajectories. Instead of solving ODEs or enforcing complex consistency constraints, MP1 uses the MeanFlow Identity to map initial and final states to the average velocity over a time interval. This allows for single-step trajectory generation by applying the learned velocity to predict the next state. The method incorporates Classifier-Free Guidance for controllability and a Dispersive Loss for improved few-shot generalization, achieving both speed and performance improvements over existing methods.

## Key Results
- 10.2% higher success rates than DP3 on Adroit and Meta-World benchmarks
- 7.3% higher success rates than FlowPolicy
- 6.8 ms inference time, 19× faster than DP3 and nearly 2× faster than FlowPolicy

## Why This Works (Mechanism)
The core innovation lies in reframing trajectory generation as learning interval-averaged velocities rather than instantaneous velocities. By leveraging the MeanFlow Identity, MP1 bypasses the need for ODE solvers and consistency constraints that plague traditional flow-based methods. The interval-averaged velocity captures the essential dynamics over a finite time horizon, making the model more robust to noise and variations in state transitions. This approach enables genuine single-step inference while maintaining trajectory quality, as the learned velocity naturally incorporates the temporal dependencies needed for successful manipulation tasks.

## Foundational Learning

**MeanFlow Identity**: The mathematical foundation that relates initial and final states through interval-averaged velocities. Why needed: Provides the theoretical basis for bypassing ODE solvers. Quick check: Verify the identity holds for simple linear systems before applying to complex robotics tasks.

**Interval-averaged velocity**: Learning the average velocity over a time interval rather than instantaneous velocity. Why needed: Enables single-step inference while capturing essential dynamics. Quick check: Compare trajectory quality using interval-averaged vs. instantaneous velocity predictions.

**Classifier-Free Guidance**: A technique for balancing conditional and unconditional predictions. Why needed: Improves controllability without requiring separate classifier networks. Quick check: Test different guidance scales to find optimal balance between diversity and task completion.

## Architecture Onboarding

Component map: State input -> MeanFlow Velocity Network -> Interval-averaged velocity -> Next state prediction -> (Classifier-Free Guidance) -> Final trajectory

Critical path: The MeanFlow Velocity Network is the core component that maps state pairs to interval-averaged velocities. This network must be carefully designed to capture the complex dynamics of robotic manipulation while maintaining computational efficiency for single-step inference.

Design tradeoffs: The interval length represents a key tradeoff between trajectory accuracy and computational simplicity. Longer intervals provide more context but may miss fine-grained dynamics, while shorter intervals require more precise velocity predictions.

Failure signatures: Poor performance may manifest as trajectories that fail to reach goal states or exhibit unnatural motion patterns. These failures often indicate issues with the velocity network's ability to capture task-specific dynamics or insufficient training data coverage.

First experiments:
1. Validate MeanFlow Identity on simple linear systems before scaling to complex manipulation tasks
2. Test different interval lengths to find the optimal balance between accuracy and efficiency
3. Compare performance with and without Classifier-Free Guidance to quantify its impact on controllability

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to Adroit and Meta-World benchmarks, with no comparison against more recent trajectory optimization or model-based reinforcement learning methods
- Claims about eliminating consistency losses lack exploration of impact on different interval lengths and sampling noise
- Theoretical justification for Dispersive Loss effectiveness in few-shot settings is not provided

## Confidence
- **High confidence**: Faster inference speed claims (6.8 ms vs. baselines), success rate improvements over DP3 and FlowPolicy
- **Medium confidence**: Generalization claims in few-shot settings, Classifier-Free Guidance effectiveness
- **Low confidence**: Claims about eliminating need for consistency losses in all scenarios, scalability to more complex manipulation tasks

## Next Checks
1. Test MP1 on more diverse robotic manipulation tasks, including those with dynamic obstacles or multi-arm coordination
2. Evaluate the impact of varying interval lengths and noise levels on trajectory accuracy and stability
3. Compare MP1 against state-of-the-art model-based RL and trajectory optimization methods on the same benchmarks