---
ver: rpa2
title: Attention Is Not What You Need
arxiv_id: '2512.19428'
source_url: https://arxiv.org/abs/2512.19428
tags:
- grassmann
- attention
- sequence
- manifold
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper challenges the necessity of explicit self-attention\
  \ for strong sequence modeling, arguing that attention functions as a tensor lifting\
  \ mechanism that, while powerful, is mathematically opaque and difficult to trace.\
  \ As an alternative, it proposes Causal Grassmann, an attention-free architecture\
  \ that models local token pairs as two-dimensional subspaces on a Grassmann manifold,\
  \ encoded via Pl\xFCcker coordinates, and fused back into hidden states through\
  \ gated mixing."
---

# Attention Is Not What You Need

## Quick Facts
- arXiv ID: 2512.19428
- Source URL: https://arxiv.org/abs/2512.19428
- Reference count: 14
- The paper challenges the necessity of explicit self-attention for strong sequence modeling, proposing Causal Grassmann as an attention-free alternative that models token pairs as 2D subspaces on a Grassmann manifold.

## Executive Summary
This paper argues that attention functions as a tensor lifting mechanism that, while powerful, is mathematically opaque and difficult to trace. As an alternative, it proposes Causal Grassmann, an attention-free architecture that models local token pairs as two-dimensional subspaces on a Grassmann manifold via Plücker coordinates, with controlled fusion back into hidden states. Evaluated on Wikitext-2 language modeling and SNLI natural language inference, the model achieves validation perplexities within 10-15% of size-matched Transformers (13-18M parameters) and slightly outperforms a Transformer classification head on SNLI (best validation accuracy 0.8550 vs. 0.8545). The work suggests that geometric evolution mechanisms, rather than attention per se, may be the fundamental requirement for reasoning in neural networks.

## Method Summary
Causal Grassmann replaces traditional attention mechanisms with geometric operations on a Grassmann manifold. For each token pair at offset Δ, the model reduces hidden states to dimension r, computes Plücker coordinates representing the 2D subspace they span, projects these back to the original dimension, and fuses them with the original hidden states via learned gating. This creates a linear-time complexity alternative to quadratic attention, with information propagating through controlled deformations of low-rank subspaces across multi-scale local windows. The architecture maintains causal masking for autoregressive tasks and uses LayerNorm and feed-forward networks similar to standard Transformers.

## Key Results
- Wikitext-2 validation perplexity: Causal Grassmann achieves within 10-15% of Transformer baselines at same parameter count (13-18M)
- SNLI classification: Causal Grassmann head slightly outperforms Transformer head (best validation accuracy 0.8550 vs. 0.8545)
- Complexity analysis: O(Ld²) vs O(L²d_head + Ld²) for attention, with linear scaling in sequence length for fixed rank
- Ablation: Deeper models (12 layers) narrow the perplexity gap compared to shallower ones (6 layers)

## Why This Works (Mechanism)

### Mechanism 1: Plücker Coordinate Encoding of Token Pairs
- Claim: Local token relationships can be encoded as 2D subspaces on a Grassmann manifold rather than pairwise attention weights.
- Mechanism: Hidden states are linearly reduced to z_t ∈ R^r. Pairs (z_t, z_{t+Δ}) define a 2D subspace in R^r, which is embedded via Plücker coordinates p_{ij} = z_{t,i}z_{t+Δ,j} - z_{t,j}z_{t+Δ,i} for all i < j, yielding a fixed-dimensional feature vector.
- Core assumption: The semantic relationship between tokens is adequately captured by the 2D subspace they span, not requiring full pairwise weight matrices.
- Evidence anchors: [abstract] "encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plücker coordinates"; [Section 2.2] Plücker embedding maps each k-dimensional subspace to a point in projective space; [corpus] Weak direct corpus support; related work on tensor decompositions suggests alternative compact representations.
- Break condition: If token relationships require higher-order interactions (3+ tokens simultaneously) or non-linear subspace structures, the 2D encoding becomes insufficient.

### Mechanism 2: Multi-Scale Local Mixing Replaces Global Attention
- Claim: Information can propagate across sequences through controlled local subspace deformations at multiple scales rather than global L×L attention tensors.
- Mechanism: Instead of computing attention between all token pairs, the model uses fixed window offsets W = {Δ_1, ..., Δ_m} (e.g., {1, 2, 4, 8, 12, 16}). Each position only mixes with positions at these offsets, and depth + multi-scale windows propagate information implicitly.
- Core assumption: Long-range dependencies emerge from composition of local geometric operations across layers and scales.
- Evidence anchors: [abstract] "Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows"; [Section 5.1] Gap narrows with depth (12-layer models closer to Transformer than 6-layer); [Section 3.2] Complexity is O(Ld²) vs. O(L²d_head + Ld²) for attention; [corpus] SSMs and state-space models similarly use structured latent states for temporal evolution.
- Break condition: If a task requires true O(1)-hop access to distant tokens (e.g., specific long-range identity matching), local windows may require impractical depth.

### Mechanism 3: Gated Geometric Fusion
- Claim: Learned gating controls how much geometric (Plücker) vs. original hidden state information contributes to the updated representation.
- Mechanism: Concatenate [h_t; g_t], compute gate α_t = σ(W_gate u_t + b_gate), then mix: h̃_t = α_t ⊙ h_t + (1 - α_t) ⊙ g_t. This allows the model to learn when subspace geometry is relevant.
- Core assumption: The optimal contribution of geometric features varies by position and context.
- Evidence anchors: [Section 3.2] Explicit gating formula provided; [Section 5.2] SNLI results show geometric head slightly outperforms Transformer head (0.8550 vs 0.8545); [corpus] No direct corpus comparison for this specific gating mechanism.
- Break condition: If geometric features are uniformly helpful or harmful (not context-dependent), gating adds unnecessary complexity.

## Foundational Learning

- Concept: **Grassmann Manifold Gr(k, n)**
  - Why needed here: Core data structure; must understand it represents k-dimensional subspaces of R^n, not individual vectors.
  - Quick check question: Can you explain why Gr(2, r) has dimension 2(r-2)?

- Concept: **Plücker Embedding and Coordinates**
  - Why needed here: This is the actual computational representation used in the model.
  - Quick check question: Given two vectors u, v ∈ R^4, how many Plücker coordinates does their 2D subspace produce?

- Concept: **Tensor Lifting (as conceptual framing)**
  - Why needed here: The paper reframes attention as tensor lifting; understanding this helps grasp why Grassmann is proposed as an alternative.
  - Quick check question: Why does the paper claim L×L attention matrices resist description by "a small family of explicit invariants"?

## Architecture Onboarding

- Component map: Input tokens → Embeddings + Positional encoding → Linear reduction (d → r) → Multi-scale pairing → Plücker coordinates (r choose 2) → Projection back to d → Gated fusion (h_t, g_t) → LayerNorm → Feed-forward → LayerNorm → Next layer (or output head)

- Critical path:
  1. **Hyperparameter selection**: r (reduced dimension), W (window offsets), number of layers
  2. **Plücker computation correctness**: Must verify wedge product implementation matches paper formula
  3. **Causality enforcement**: Only pair t with t+Δ where Δ > 0 for autoregressive tasks

- Design tradeoffs:
  - **Smaller r**: Lower compute but less expressive subspace encoding (paper uses r=32, d=256)
  - **Larger window set W**: More coverage but higher O(Lmr²) cost
  - **Depth vs. window coverage**: Paper suggests deeper models can compensate for limited local windows

- Failure signatures:
  - Perplexity gap >20% from Transformer baseline at same parameter count → likely need more layers or larger r
  - Training instability with normalization → check Plücker vector normalization (ε parameter)
  - No improvement over baseline on classification → geometric features may not align with task structure

- First 3 experiments:
  1. **Ablation on r**: Test r ∈ {16, 32, 64} on Wikitext-2 to find efficiency/expressiveness balance for your scale.
  2. **Window schedule sensitivity**: Compare W = {1, 2, 4, 8, 12, 16} vs. repeated patterns (as in 12-layer models) to validate depth compensation hypothesis.
  3. **Head comparison on frozen backbone**: Replicate SNLI protocol (DistilBERT + Grassmann head vs. Transformer head) to isolate geometric mixing contribution from representation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit global or long-range invariants of the Grassmann flow (e.g., sequence-level subspace statistics, curvature-like measures) be constructed and injected to improve performance on tasks requiring long-range dependencies?
- Basis in paper: [explicit] Section 6.4 and Conclusion: "Construct explicit global or long-range invariants of the sequence-level Grassmann flow and feed them back as features... 'global invariants + local Grassmann flow' as a promising direction for future work."
- Why unresolved: Current design uses only local windows; long-range dependencies rely implicitly on depth and multi-scale windows, which may be insufficient for tasks requiring true global reasoning.
- What evidence would resolve it: Demonstrating improved performance on long-context benchmarks (e.g., LongBench, document-level QA) after incorporating explicit sequence-level Grassmann invariants.

### Open Question 2
- Question: Does the 10-15% perplexity gap between GrassmannLM and TransformerLM persist, narrow, or widen when scaling to larger models (100M+ parameters) and datasets?
- Basis in paper: [inferred] Results show GrassmannLM consistently trails TransformerLM by 10-15% on Wikitext-2 at 13-18M parameters; Section 5.3 acknowledges current experiments are "proof of concept... not an optimized engineering solution."
- Why unresolved: The authors do not test beyond moderate scales, leaving unclear whether geometric constraints become bottlenecks or advantages at scale.
- What evidence would resolve it: Systematic scaling curves comparing Grassmann and Transformer architectures across parameter counts from 10M to 1B+ on standardized benchmarks.

### Open Question 3
- Question: Can Plücker coordinates or other Grassmann features serve as stable, human-interpretable invariants that correlate with model behavior more reliably than attention weights?
- Basis in paper: [explicit] Conclusion lists "Interpretability studies: Systematically investigate correlations between Plücker coordinates, model behavior, and human-understandable patterns" as future work.
- Why unresolved: The paper claims improved interpretability potential but provides no empirical analysis demonstrating that Grassmann features are actually more interpretable or predictive of model decisions than attention maps.
- What evidence would resolve it: Probing studies showing that specific Plücker coordinate patterns reliably predict linguistic phenomena (e.g., syntactic roles, semantic relations) with higher fidelity than attention head analysis.

## Limitations
- **Empirical scope**: Only validated on Wikitext-2 and SNLI, not tested on harder benchmarks like GLUE, SuperGLUE, or code generation tasks
- **Parameter efficiency claims**: Doesn't analyze whether Grassmann achieves better performance per parameter or training efficiency despite linear complexity
- **Geometric representation adequacy**: Core assumption that 2D subspaces capture sufficient token relationships relies on empirical validation rather than theoretical guarantees

## Confidence
- **High Confidence**: The mathematical framework for Plücker coordinates and Grassmann manifold operations is sound and correctly implemented. The linear complexity analysis versus quadratic attention is accurate for the proposed architecture.
- **Medium Confidence**: The empirical results showing competitive performance on Wikitext-2 and SNLI are reproducible based on the provided specifications. The claim that attention functions as tensor lifting is conceptually coherent and supported by the geometric alternative presented.
- **Low Confidence**: Broader claims about the fundamental nature of reasoning in neural networks and the sufficiency of geometric mechanisms for all sequence modeling tasks extend beyond what the experimental evidence supports. The assertion that "attention is not what you need" is provocative but not conclusively proven across diverse NLP benchmarks.

## Next Checks
1. **Scaling Law Analysis**: Systematically vary r ∈ {16, 32, 64, 128} and window sets W across multiple depths (6, 12, 24 layers) on Wikitext-2 to identify optimal configurations and validate whether deeper Grassmann models consistently close the gap with Transformers.

2. **Cross-Task Generalization**: Evaluate Causal Grassmann on GLUE benchmark tasks (especially RTE, MRPC, QNLI) and a code generation dataset to test whether geometric subspace modeling generalizes beyond language modeling and simple classification.

3. **Comparative Efficiency Study**: Train matched Grassmann and Transformer models with identical parameter budgets on Wikitext-2, measuring not just final perplexity but training throughput (samples/second), convergence speed, and memory usage to quantify practical advantages of linear complexity.