---
ver: rpa2
title: An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures
  of Semantic Transparency for Malay Word Recognition
arxiv_id: '2505.05973'
source_url: https://arxiv.org/abs/2505.05973
tags:
- word
- words
- semantic
- derived
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores semantic transparency in Malay word recognition
  using embedding-based measures. The authors applied t-SNE to visualize 4,226 Malay
  prefixed words in semantic space, revealing distinct clusters by prefix class.
---

# An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition

## Quick Facts
- arXiv ID: 2505.05973
- Source URL: https://arxiv.org/abs/2505.05973
- Reference count: 9
- Primary result: Embedding-based centroid correlation outperformed other measures in predicting Malay lexical decision latencies

## Executive Summary
This study explores semantic transparency in Malay word recognition using embedding-based measures. The authors applied t-SNE to visualize 4,226 Malay prefixed words in semantic space, revealing distinct clusters by prefix class. They computed five measures: LDA classification accuracy (using both embeddings and shift vectors), and three correlation measures (with centroid, FRACSS prediction, and shift vectors). In generalized additive mixed models predicting lexical decision latencies, all measures significantly improved model fit. The centroid correlation measure provided the best prediction (AIC = 12325.08), outperforming both LDA-based measures. This demonstrates that semantic transparency in Malay can be effectively quantified using distributional semantics, with the proximity to a prefix's centroid being the strongest predictor of processing ease.

## Method Summary
The study analyzed 4,226 Malay prefixed words with 300-dimensional FastText embeddings, using 42,934 lexical decision latency observations from 280 participants. Five embedding-based measures were computed: LDA classification accuracy on embeddings and shift vectors, and three correlation measures (centroid, FRACSS prediction, and shift vectors). These measures were evaluated using generalized additive mixed models (GAMMs) predicting inverse reaction times, with model fit assessed via AIC comparison.

## Key Results
- Semantic clustering by prefix class confirmed via t-SNE visualization and LDA classification (93% accuracy)
- Centroid correlation measure provided best prediction of lexical decision latencies (AIC = 12325.08)
- Shift vector correlation showed inhibitory effect on processing speed
- All five measures significantly improved model fit compared to baseline

## Why This Works (Mechanism)

### Mechanism 1: Semantic Clustering via Derivational Prefix
- **Claim:** Complex words with shared derivational prefixes occupy distinct, separable regions in high-dimensional semantic space.
- **Mechanism:** The distributional hypothesis implies that words appearing in similar contexts share meaning. As derivational prefixes systematically alter context and meaning (e.g., turning verbs into nouns), the resulting embeddings cluster geometrically by prefix class.
- **Core assumption:** The static embeddings (FastText) capture sufficient co-occurrence statistics to resolve polysemy and distinct morphological semantics.
- **Evidence anchors:** t-SNE visualization revealed distinct clusters by prefix class; LDA predicted class membership with 93% accuracy using embeddings.

### Mechanism 2: Centroid Proximity Facilitates Recognition
- **Claim:** The correlation between a word's vector and its prefix centroid serves as a proxy for prototypicality, predicting processing ease.
- **Mechanism:** The centroid represents the average vector (prototypical meaning) of a prefix class. Words strongly correlated with this centroid are "canonical" examples of that morphological pattern, requiring less cognitive effort to process via analogical extension.
- **Core assumption:** The mental lexicon organizes morphologically complex words via analogical patterns to a central exemplar rather than strict rule-based decomposition.
- **Evidence anchors:** Centroid correlation measure provided the best prediction (AIC = 12325.08); facilitative effect observed where closer proximity predicted faster recognition.

### Mechanism 3: Shift Vector Magnitude Encodes Semantic Distance
- **Claim:** The geometric displacement (shift vector) between a derived word and its base correlates with semantic opacity and processing cost.
- **Mechanism:** Subtracting the base vector from the derived vector yields a "shift vector." A high correlation between the derived word and this shift vector indicates a large angular distance from the base, implying the meaning has diverged significantly (semantic opacity).
- **Core assumption:** The vector arithmetic (derived - base) linearly captures the semantic contribution of the affixation process.
- **Evidence anchors:** Higher shift correlations predicted slower response times (AIC = 12381.06); vector shifts track semantic change.

## Foundational Learning

- **Concept: Distributional Semantics & Vector Space Models**
  - **Why needed here:** The entire methodology rests on representing word meaning as geometric coordinates (vectors) where distance equals semantic similarity.
  - **Quick check question:** If vector A is "ratu" (queen) and vector B is "raja" (king), would you expect their cosine similarity to be high or low?

- **Concept: Realizational vs. Decompositional Morphology**
  - **Why needed here:** To interpret why the "centroid" (whole-word analogy) outperformed "FRACSS" (compositional prediction). The paper argues against decomposing words into morpheme units.
  - **Quick check question:** Does the model assume the prefix "meN-" exists as an independent mental token, or is it inferred from the relationship between whole words like "makan" and "memakan"?

- **Concept: Generalized Additive Mixed Models (GAMMs)**
  - **Why needed here:** The paper uses GAMMs rather than standard regression to capture non-linear relationships (e.g., the U-shaped curve where words too close to the centroid slow down).
  - **Quick check question:** Why would a linear model fail to capture the effect of semantic similarity if high similarity actually inhibits (slows) recognition?

## Architecture Onboarding

- **Component map:** FastText embeddings -> t-SNE visualization/LDA classification -> 5 feature extractors (LDA-Embed, LDA-Shift, Correlation-Centroid, Correlation-FRACSS, Correlation-Shift) -> GAMMs predicting RT
- **Critical path:** Computing the Correlation with Centroid requires: (1) grouping words by prefix, (2) calculating the mean vector (centroid) for each group, and (3) correlating individual word vectors with that centroid. This feature provided the lowest AIC (best fit).
- **Design tradeoffs:**
  - Classification vs. Correlation: LDA compresses information into binary success metric, while Correlation preserves continuous degree of similarity. Continuous similarity (centroid correlation) proved more predictive of human RT than discrete class membership.
  - Static vs. Contextual: Static FastText embeddings used; contextual embeddings might capture polysemy better but complicate single vector per word analysis.
- **Failure signatures:**
  - Low count prefixes fail to cluster or classify (e.g., pe-, pra- with n=3,4)
  - Non-linearity at extremes: words with extremely high centroid correlation (>0.7) show inhibitory effect
- **First 3 experiments:**
  1. Calculate prefix centroids for a different morphologically rich language (e.g., Turkish) and test if centroid distance predicts lexical decision times
  2. Target words with centroid correlation > 0.75 to verify if inhibitory effect replicates and correlates with error rates in semantic categorization
  3. Add random noise to embeddings of low-frequency words to test if "Shift Vector" measure degrades gracefully compared to "Centroid" measure

## Open Questions the Paper Calls Out

- **Question:** What specific semantic features drive the clustering of Malay prefixed words in embedding space, and can these be extracted and validated linguistically?
- **Basis in paper:** The authors state: "A next step would be to identify a set of features in which these clusters embody. Such a study will shed light on the kind of semantics that could be extracted from word embeddings. We leave more fine-grained analyses for future work."
- **Question:** How do centroids function within the Discriminative Lexicon Model (DLM) for comprehension and production of Malay morphologically complex words?
- **Basis in paper:** The authors note: "On our to-do list is a closer inspection of the potential role of centroids in the DLM." They present preliminary evidence of strong correspondence between centroids and DLM comprehension mappings but do not test this formally.
- **Question:** Would the embedding-based transparency measures generalize to suffixation, circumfixation, and other morphological processes in Malay and related Austronesian languages?
- **Basis in paper:** The study is limited to prefixed words (4,226 words with 10 prefixes). The authors do not address whether the centroid-based approach would work equally well for other affix types or for languages with different morphological profiles.

## Limitations

- Reliance on static FastText embeddings may not fully capture contextual polysemy of Malay prefixes
- Inhibitory effect for words with very high centroid correlation (>0.7) remains unexplained
- Shift vector mechanism depends on existence of clear base forms, which may not hold for all morphologically complex words
- Generalizability to other languages or morphological systems is untested

## Confidence

- **High confidence:** Semantic clustering by prefix class (verified by LDA accuracy and t-SNE visualization)
- **Medium confidence:** Centroid correlation as best predictor (robust AIC improvement, but dependent on static embeddings)
- **Medium confidence:** Shift vector correlation's inhibitory effect (statistically significant but with unclear cognitive interpretation)

## Next Checks

1. Replicate the centroid correlation measure using contextual embeddings (BERT) to assess whether semantic clustering improves with contextualized representations
2. Conduct error analysis on the 10% of words LDA misclassified to determine if semantic transparency judgments align with classification failures
3. Test the model on a morphologically distinct language (e.g., Finnish or Turkish) to evaluate cross-linguistic generalizability of the semantic transparency measures