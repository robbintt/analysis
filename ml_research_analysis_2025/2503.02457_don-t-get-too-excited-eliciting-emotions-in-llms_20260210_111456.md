---
ver: rpa2
title: Don't Get Too Excited -- Eliciting Emotions in LLMs
arxiv_id: '2503.02457'
source_url: https://arxiv.org/abs/2503.02457
tags:
- valence
- arousal
- llms
- emotional
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the ability of large language models to express
  and maintain emotional states during dialogue. Twelve open-weight models were evaluated
  in zero-shot and few-shot configurations, using LLM-based sentiment analysis and
  multi-turn dialogue simulations.
---

# Don't Get Too Excited -- Eliciting Emotions in LLMs

## Quick Facts
- arXiv ID: 2503.02457
- Source URL: https://arxiv.org/abs/2503.02457
- Authors: Gino Franco Fazzi; Julie Skoven Hinge; Stefan Heinrich; Paolo Burelli
- Reference count: 40
- Models vary significantly in ability to express and maintain emotional states during dialogue

## Executive Summary
This study evaluated twelve open-weight LLMs' ability to express and maintain emotional states during multi-turn dialogues using Valence-Arousal dimensional modeling. The researchers found substantial variation in models' affective expressive range, with Llama3, Llama3.2, Mistral, and Gemma2 performing best. Models consistently produced more neutral responses than prompted, particularly for extreme emotional states, and showed convergence toward similar emotional states during conversations. The findings highlight challenges in affect control and suggest the need for improved emotional modeling in language models.

## Method Summary
The researchers evaluated twelve open-weight LLMs using zero-shot and few-shot configurations in a multi-turn dialogue setting. Models were prompted with structured persona descriptions including current emotional state mapped to Valence-Arousal dimensions using SAM descriptors. A 20-turn conversation was simulated between two agents with emotion-matched greetings, and each utterance was scored using an XLM-RoBERTa-based sentiment analysis classifier. The study measured correlation between prompted and generated VA values, examined convergence patterns, and tested whether few-shot examples improved emotional expression.

## Key Results
- Llama3, Llama3.2, Mistral, and Gemma2 showed the highest correlation between prompted and generated VA values
- Models consistently produced more neutral responses than prompted, especially for extreme emotional states
- Multi-turn conversations led to emotional convergence, with some models (like Llama3) showing strong valence convergence while others (like Mistral) maintained divergence

## Why This Works (Mechanism)

### Mechanism 1: Persona-Grounded Affective Conditioning
Structured persona prompts with explicit VA descriptors enable LLMs to adopt and express target emotional states. System prompts inject demographic attributes plus current emotional state described via SAM scale labels, conditioning the model on this context when generating responses. Core assumption: LLMs have sufficient latent representations of emotional language patterns that can be activated through natural language descriptions rather than fine-tuning. Break condition: When extreme VA values are prompted (e.g., very positive + very excited), models fail to maintain intensity, regressing toward neutral responses.

### Mechanism 2: Conversational Emotional Convergence
Multi-turn dialogue between agents causes emotional states to converge, particularly for valence. Each agent's response is conditioned on chat history containing the partner's emotionally-expressive utterances, creating a feedback loop where emotional displays influence subsequent generation, pulling both agents toward intermediate VA values. Core assumption: Models are not rigidly adhering to system prompts when conversational context provides conflicting emotional signals. Break condition: Mistral maintained greater divergence, suggesting architectural or training differences affect susceptibility to convergence pressure.

### Mechanism 3: Regression-to-Neutral Bias
LLMs systematically attenuate extreme emotional prompts toward neutral outputs. Training data distributions and safety/alignment procedures likely penalize extreme outputs, creating a prior that favors moderation. Core assumption: The bias originates from training/alignment rather than limitations in the prompting methodology. Break condition: Few-shot examples may partially overcome bias for some models (mixed results in Table 1), but effect is inconsistent.

## Foundational Learning

- Concept: **Valence-Arousal (VA) Dimensional Emotion Model**
  - Why needed here: The entire evaluation framework relies on mapping emotions to continuous 2D coordinates rather than discrete categories
  - Quick check question: Can you explain why "excited" could combine with either positive or negative valence?

- Concept: **Self-Assessment Manikin (SAM) Scale**
  - Why needed here: Translates continuous VA values (0-1) into human-readable descriptors for prompting
  - Quick check question: What SAM descriptor would you use for valence=0.3, arousal=0.85?

- Concept: **Spearman Correlation for Affective Alignment**
  - Why needed here: Used to quantify how well model outputs match prompted VA targets
  - Quick check question: Why use rank correlation rather than MSE for evaluating VA alignment?

## Architecture Onboarding

- Component map:
  - KDE distribution sampler -> Persona generator -> Prompt constructor -> Dialogue simulator -> VA classifier -> Analysis layer

- Critical path:
  1. Sample VA from KDE -> Map to SAM descriptors -> Construct system prompts for both agents
  2. Initialize dialogue with emotion-matched greeting (Table 4)
  3. Run 20-turn conversation, collecting all utterances
  4. Pass each utterance through VA classifier
  5. Compute per-turn VA trajectories and convergence statistics

- Design tradeoffs:
  - Zero-shot vs Few-shot: Few-shot provides examples but adds complexity and token costs; paper found mixed benefits
  - Automated vs Human evaluation: Automated enables scale (250+ responses/model) but may miss subtle emotional cues
  - Fixed vs Sampled VA: Sampling from KDE distribution increases ecological validity but adds variance

- Failure signatures:
  - **Premature termination**: Agents issue farewells and loop (Appendix A.1) before reaching 20 turns
  - **Extreme VA attenuation**: Outputs cluster around mid-scale regardless of prompt
  - **Emoji-only tail**: Conversations degrade to repetitive emoji exchanges

- First 3 experiments:
  1. Reproduce preliminary experiment with a single model (Llama3 or Mistral), comparing Zero-shot vs Few-shot correlation scores on 50 iterations
  2. Run 10 conversations pairing HV+HA vs LV+HA agents, plot VA trajectories per turn to verify convergence behavior
  3. Test boundary conditions: prompt VA values at extremes (0.0, 0.1, 0.9, 1.0) and measure offset between prompted and output VA to quantify regression-to-neutral

## Open Questions the Paper Calls Out
None

## Limitations

- Model generalization is uncertain; results based on twelve specific open-weight models may not extend to closed models or smaller architectures
- Automated sentiment analysis may miss nuanced emotional expression or cultural context despite enabling large-scale evaluation
- The VA-SAM mapping assumes linear correspondence between numerical values and emotional descriptors, which may not capture complex mixed states

## Confidence

**High confidence**:
- Models demonstrate significant variation in emotional expression capability across VA dimensions
- Extreme VA prompts consistently produce more neutral outputs than requested
- Few-shot examples provide mixed but occasionally substantial improvements

**Medium confidence**:
- Conversational convergence hypothesis is supported but could reflect prompt conditioning rather than true interaction effects
- Regression-to-neutral bias likely originates from training/alignment rather than prompting methodology
- SAM-based prompting enables affective expression, though effectiveness varies by model

**Low confidence**:
- Cross-cultural applicability of VA-SAM framework for model prompting
- Long-term stability of emotional states beyond 20-turn conversations
- Whether observed convergence represents genuine emotional contagion or surface-level linguistic mimicry

## Next Checks

1. **Classifier validation study**: Test XLM-RoBERTa's accuracy specifically on LLM-generated emotional text by comparing automated scores against human annotations for a subset of 100 responses across multiple models

2. **Cross-model ablation**: Run identical VA prompts across models of varying sizes (7B, 13B, 70B parameters) to determine whether performance correlates with model scale or specific architectural features

3. **Extended interaction study**: Conduct 50-turn conversations between emotionally-matched agents to test whether convergence effects strengthen, weaken, or plateau over longer interactions