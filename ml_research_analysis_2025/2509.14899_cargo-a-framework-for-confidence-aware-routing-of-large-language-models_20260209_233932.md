---
ver: rpa2
title: 'CARGO: A Framework for Confidence-Aware Routing of Large Language Models'
arxiv_id: '2509.14899'
source_url: https://arxiv.org/abs/2509.14899
tags:
- llms
- accuracy
- routing
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CARGO, a lightweight, confidence-aware framework
  for routing prompts to the most appropriate large language model (LLM). CARGO uses
  a single embedding-based regressor trained on LLM-judged pairwise comparisons, with
  an optional binary classifier invoked when predictions are uncertain.
---

# CARGO: A Framework for Confidence-Aware Routing of Large Language Models

## Quick Facts
- arXiv ID: 2509.14899
- Source URL: https://arxiv.org/abs/2509.14899
- Reference count: 40
- CARGO achieves 76.4% top-1 routing accuracy and 79.1% win rate against individual expert models.

## Executive Summary
This paper introduces CARGO, a lightweight, confidence-aware framework for routing prompts to the most appropriate large language model (LLM). CARGO uses a single embedding-based regressor trained on LLM-judged pairwise comparisons, with an optional binary classifier invoked when predictions are uncertain. The framework supports both global and category-specific regressors across five task groups: mathematics, coding, reasoning, summarization, and creative writing. Evaluated on four competitive LLMs, CARGO demonstrates that confidence-guided, lightweight routing can achieve expert-level performance with minimal overhead, offering a practical solution for real-world, multi-model LLM deployments.

## Method Summary
CARGO employs a two-stage routing approach where a regressor first scores all candidate LLMs for a given prompt. If the top-2 score gap exceeds a threshold τ, routing proceeds directly to the top-ranked model. If the gap is small, both top-2 models are queried and a binary classifier selects the better response. The framework uses LLM-judged pairwise comparisons to generate training labels without human supervision, and supports category-specific regressors for domain specialization. Training involves collecting prompt responses from all candidate LLMs, generating pairwise comparisons through multiple independent LLM judges, aggregating these into global rankings, and training regressors on prompt embeddings to predict LLM performance scores.

## Key Results
- Top-1 routing accuracy of 76.4% and win rates ranging from 72% to 89% against individual expert models
- Category-specific routing achieves up to 91.9% Top-1-or-2 accuracy in coding tasks
- Optimal threshold τ∈[0.06,0.10] balances accuracy and computational overhead
- Substantial inter-judge agreement (κ=0.62) validates the LLM judging approach

## Why This Works (Mechanism)

### Mechanism 1: Pairwise LLM-Judged Training Signal Generation
Replacing human annotations with multi-judge pairwise comparisons yields training labels that correlate with human preferences. For each prompt, all candidate LLMs generate responses. Multiple independent LLM judges (N=4) compare every response pair, assigning scores {1, 0.5, 0}. Aggregated pairwise scores produce a final ranking. This creates labeled data without human supervision. Core assumption: LLM judges can approximate human evaluation quality, and inter-judge agreement signals reliability.

### Mechanism 2: Confidence-Gapped Two-Stage Routing
Invoking a classifier only when the regressor's top-2 predictions are close improves accuracy while limiting computational overhead. A regressor scores all candidate LLMs. If gap g(p) = ŷ⁽¹⁾(p) - ŷ⁽²⁾(p) ≥ τ, route to top-ranked LLM. If g(p) < τ, query both top-2 LLMs and use a binary classifier to select the better response. Core assumption: Small predicted score gaps indicate genuine uncertainty where a classifier adds value; large gaps indicate confident correct predictions.

### Mechanism 3: Category-Aware Specialization
Training separate regressors per task category captures domain-specific model strengths better than a single global model. A classifier first predicts the prompt category (96% accuracy). Category-specific regressors then score candidate LLMs. This allows the router to learn that Claude excels at coding while DeepSeek excels at reasoning—patterns a global model may dilute. Core assumption: Task categories are predictably correlated with which LLM performs best, and category boundaries are learnable.

## Foundational Learning

- **Concept: Pairwise Preference Aggregation (Bradley-Terry-like scoring)**
  - Why needed here: CARGO's label generation depends on converting pairwise comparisons into global rankings. Without understanding how local preferences aggregate, you cannot debug label quality.
  - Quick check question: Given 4 responses with pairwise scores from 3 judges, can you compute the global ranking and explain why ties occur?

- **Concept: Embedding-based Regression for Ranking**
  - Why needed here: The regressor maps prompt embeddings to predicted LLM scores. Understanding that embeddings capture semantic similarity—and that similar prompts may prefer the same LLM—is essential for interpreting failures.
  - Quick check question: Why might two prompts with high embedding similarity still prefer different LLMs? What features would you add to the regressor input?

- **Concept: Threshold-based Decision Cascades**
  - Why needed here: CARGO's two-stage routing is a cascade where the second stage is conditionally invoked. Understanding latency/accuracy tradeoffs at different thresholds is critical for deployment tuning.
  - Quick check question: If τ=0.10 gives 76.4% accuracy with 55% classifier usage, what accuracy would you expect at τ=0.05? How would you measure the cost-per-accuracy-gain?

## Architecture Onboarding

- **Component map:**
  1. Prompt Ingestion → receives raw prompt text
  2. Embedding Encoder (OpenAI text-embedding-ada-002) → produces 1536-dim vector
  3. Category Classifier (Random Forest) → predicts task category (5 classes)
  4. Score Regressor (Random Forest or Ridge, global or per-category) → predicts normalized score for each candidate LLM
  5. Gap Calculator → computes g(p) = top-1 score - top-2 score
  6. Binary Classifier (MLP) → invoked only if g(p) < τ, selects between top-2 LLMs
  7. LLM Executor → queries selected model(s) via OpenRouter API
  8. Judge Ensemble (offline training only) → 4 LLMs performing pairwise comparisons

- **Critical path:**
  Training: Prompts → LLM responses → Pairwise judging → Score aggregation → Regressor training
  Inference: Prompt → Embedding → Category → Regressor scores → Gap check → (optional) Classifier → LLM call → Response

- **Design tradeoffs:**
  - Global vs. category-specific regressors: Global is simpler (one model) but category-specific captures domain patterns (up to +17% accuracy in coding)
  - Regressor choice: Random Forest has best Top-1 accuracy (58.41%) but MLP classifier is better for binary selection (82.6%)
  - Threshold τ: Lower τ reduces latency (fewer classifier calls) but lowers accuracy; τ∈[0.06,0.10] is empirically optimal
  - Number of judges N: More judges increase label reliability but cost more; N=4 achieved κ=0.62

- **Failure signatures:**
  - Low inter-judge agreement (κ<0.5): Labels may be noisy; regressor fails to learn
  - Self-preference bias >35%: Judge rankings are contaminated; use blind evaluation
  - Classifier invoked >80% of prompts: τ set too high; degenerates to always-query-two-models
  - Category classifier accuracy <85%: Wrong regressor selected; consider global fallback
  - Top-1 accuracy plateaus while Top-2 rises: Regressor learns relative but not absolute ordering; rely on classifier

- **First 3 experiments:**
  1. Reproduce the gap-sweep analysis: Train the Random Forest regressor, then sweep τ from 0.01 to 0.20. Plot coverage accuracy, overall selection accuracy, and classifier invocation rate. Identify the elbow point for your dataset.
  2. Ablate category-specific vs. global routing: Train both variants on the same data. Compare Top-1 accuracy per category. Identify which categories benefit most from specialization and whether a hybrid (global + category-override) improves mean performance.
  3. Test scalability with a new expert: Add a 5th LLM (e.g., Gemini or Llama 3.1) to the pool. Generate responses for 100 held-out prompts, run pairwise judging, and evaluate if the existing regressor generalizes or requires fine-tuning. Measure how many new labeled pairs are needed to recover accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CARGO's routing accuracy and computational overhead scale with large expert pools (e.g., 10, 20, 50+ models)? The regression model's ability to discriminate among many candidates, score gap distributions, and classifier invocation rates with 10+ experts remain uncharacterized.

- **Open Question 2**: Can adaptive or per-category threshold selection outperform fixed global thresholds for routing decisions? The paper uses manual threshold sweeps without exploring automatic threshold adaptation or online learning mechanisms.

- **Open Question 3**: How does judge panel composition and inter-judge agreement affect downstream router performance? The relationship between judge count, judge quality diversity, annotation reliability, and final routing accuracy is not systematically analyzed.

- **Open Question 4**: How does CARGO perform under strict real-time latency constraints in production deployments? The paper evaluates accuracy-cost tradeoffs without measuring total response time including router inference, optional dual-LLM queries, and classifier calls.

## Limitations
- Label generation reliability depends on pairwise LLM judges producing consistent rankings, with self-preference bias introducing systematic noise
- Performance may degrade when routing to models with different architectures, training regimes, or task specializations not seen during training
- While claiming to be "lightweight," invoking the binary classifier on ~50% of prompts effectively doubles LLM queries in those cases, increasing cost

## Confidence
- **High confidence**: The pairwise LLM judging mechanism works as described, supported by human validation showing κ=0.72 agreement between human and automatic rankings
- **Medium confidence**: The optimal threshold τ∈[0.06,0.10] is empirically validated, but the exact relationship between τ, accuracy, and latency cost isn't fully characterized
- **Low confidence**: Claims about "practical solution for real-world deployments" lack cost-benefit analysis and haven't been tested with imbalanced category distributions or novel task types

## Next Checks
1. **Label quality stress test**: Systematically vary the number of LLM judges (N=2, 3, 4, 5) and measure inter-judge agreement, self-preference bias, and downstream regressor accuracy. Determine the minimum judge quality needed for reliable routing.

2. **Cost-accuracy Pareto frontier**: For each τ value, measure (a) routing accuracy, (b) percentage of prompts requiring two LLM calls, (c) total latency and cost. Plot the trade-off curve to identify economically optimal operating points.

3. **Model pool scalability test**: Add a 5th LLM (e.g., Gemini or Llama 3.1) to the routing pool. Measure how many new labeled prompts are needed to retrain the regressor and recover accuracy. Test if the existing regressor can handle the new model without catastrophic forgetting.