---
ver: rpa2
title: 'Zero-Shot Belief: A Hard Problem for LLMs'
arxiv_id: '2502.08777'
source_url: https://arxiv.org/abs/2502.08777
tags:
- event
- author
- source
- events
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates zero-shot large language models (LLMs) on
  the challenging FactBank belief detection task, which requires predicting the factuality
  of events from both authors and nested sources. The authors propose two LLM-based
  approaches: a unified end-to-end system and a hybrid system that combines a fine-tuned
  DeBERTa event tagger with LLM source and belief labeling.'
---

# Zero-Shot Belief: A Hard Problem for LLMs
## Quick Facts
- arXiv ID: 2502.08777
- Source URL: https://arxiv.org/abs/2502.08777
- Reference count: 29
- Evaluates LLMs on zero-shot belief detection, achieving 72.0% F1 on FactBank

## Executive Summary
This paper evaluates zero-shot large language models on the challenging FactBank belief detection task, which requires predicting event factuality from both authors and nested sources. The authors propose two LLM-based approaches: a unified end-to-end system and a hybrid system that combines a fine-tuned DeBERTa event tagger with LLM source and belief labeling. Their hybrid approach achieves new state-of-the-art results on FactBank (72.0% F1 for full predictions, 77.6% F1 for author predictions) and demonstrates strong multilingual generalization on the Italian ModaFact corpus. The work reveals that belief detection remains a difficult task for current LLMs, particularly for nested beliefs and event identification.

## Method Summary
The paper evaluates zero-shot large language models on the FactBank belief detection task, requiring prediction of event factuality from both authors and nested sources. Two approaches are proposed: (1) a unified end-to-end system using a single Chain-of-Thought prompt for complete prediction, and (2) a hybrid system that combines a fine-tuned DeBERTa-large event tagger with LLM-based source and belief labeling. The hybrid approach achieves SOTA results through post-processing with few-shot GPT-4o source normalization. Models tested include multiple open-source, closed-source, and reasoning-based LLMs. The authors also validate multilingual generalization on the Italian ModaFact corpus.

## Key Results
- Hybrid approach achieves 72.0% F1 on FactBank (full predictions), surpassing previous SOTA of 67.5%
- Strong author-level performance: 77.6% F1, while nested beliefs remain challenging at 25.3% F1
- Demonstrates multilingual generalization on ModaFact with competitive results to fine-tuned baselines
- Error analysis reveals LLMs struggle particularly with nested beliefs and event identification

## Why This Works (Mechanism)
The hybrid approach works by separating the challenging event detection task from the more structured source and belief labeling tasks. The fine-tuned DeBERTa model handles the noisy event identification with specialized training, while the LLM leverages its natural language understanding for source attribution and factuality reasoning. The few-shot GPT-4o normalization step corrects systematic source labeling errors, providing crucial post-processing that significantly improves performance.

## Foundational Learning
- **FactBank belief detection**: Requires identifying events, nested sources, and factuality labels; needed because this task tests complex reasoning about nested beliefs
- **Chain-of-Thought prompting**: Step-by-step reasoning approach for LLMs; needed to handle the multi-step nature of belief detection
- **Event detection with token classification**: Binary classification of tokens as events or non-events; needed as the first step in the belief detection pipeline
- **Source normalization**: Post-processing to standardize source names according to dataset guidelines; needed to correct systematic labeling errors
- **Multilingual evaluation**: Testing model performance across languages; needed to validate generalization beyond English
- **Quick check**: Can the model correctly identify whether "The company announced it would acquire..." expresses a belief from the company or a reporter?

## Architecture Onboarding

**Component Map**: DeBERTa event tagger -> LLM source/belief labeling -> GPT-4o source normalization -> F1 evaluation

**Critical Path**: Event identification (DeBERTa) → Source identification (LLM) → Belief labeling (LLM) → Source normalization (GPT-4o) → Evaluation

**Design Tradeoffs**: 
- Unified vs. Hybrid: Single prompt simpler but lower performance; separation allows specialized training for noisy event detection
- Open vs. Closed models: Open models more accessible but generally lower performance; closed models achieve better results but require API access
- Fine-tuning vs. Zero-shot: Fine-tuning DeBERTa improves event detection but adds complexity; pure zero-shot simpler but underperforms

**Failure Signatures**: 
- Low Nest F1 (~25%): Models predict AUTHOR instead of nested sources; fail on pronoun sources ("it")
- Event noun false positives: Over-predicting generic nouns, missing context-dependent event nouns
- Label confusion on future events: Predicting true/ptrue instead of unknown for reported future events

**3 First Experiments**:
1. Fine-tune DeBERTa-large on FactBank training split with specified hyperparameters and evaluate event detection precision/recall
2. Run hybrid pipeline on a small test sample with complete logging of intermediate outputs (events, sources, beliefs before normalization)
3. Compare unified vs. hybrid approaches on a subset of examples to quantify the impact of source normalization

## Open Questions the Paper Calls Out
- Can multilingual optimization of reasoning LLMs (e.g., DeepSeek R1) surpass the fine-tuned mT5-XXL baseline on the Italian ModaFact corpus?
- What specific architectural or prompting modifications are required to significantly improve LLM performance on nested source attribution?
- Can an open-source model replace GPT-4o for the source normalization step without degrading the overall system performance?

## Limitations
- Unclear training data specification for DeBERTa event tagger (which FactBank split was used)
- Incomplete few-shot examples for source normalization and event detection prompts
- Source normalization logic and source-event pairing algorithm not fully specified

## Confidence
- High Confidence: SOTA results on FactBank (72.0% Full F1, 77.6% Author F1) are reproducible with correct data and APIs
- Medium Confidence: Hybrid approach methodology is clear but event tagger training and source normalization introduce uncertainty
- Low Confidence: Complete few-shot examples cannot be fully reconstructed, affecting exact performance numbers

## Next Checks
1. Re-train DeBERTa-large event tagger using exact FactBank training split and hyperparameters, compare precision/recall on validation set
2. Implement full few-shot GPT-4o source normalization pipeline with all examples and document source-event pairing algorithm
3. Create diagnostic test set focusing on nested belief examples with pronoun sources to quantify specific failure modes