---
ver: rpa2
title: 'Hell or High Water: Evaluating Agentic Recovery from External Failures'
arxiv_id: '2508.11027'
source_url: https://arxiv.org/abs/2508.11027
tags:
- function
- tool
- employees
- data
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language model agents struggle to recover from unexpected external
  failures in planning tasks. A benchmark with 830 questions and 4450 functions was
  created by converting text-to-SQL datasets into executable tools.
---

# Hell or High Water: Evaluating Agentic Recovery from External Failures
## Quick Facts
- **arXiv ID:** 2508.11027
- **Source URL:** https://arxiv.org/abs/2508.11027
- **Reference count:** 40
- **Primary result:** Language model agents struggle to recover from unexpected external failures in planning tasks

## Executive Summary
This paper introduces a benchmark to evaluate how language model agents recover from external failures during planning tasks. The authors converted text-to-SQL datasets into executable tools, creating 830 questions and 4450 functions that agents must compose correctly while facing forced external errors. The study reveals that all evaluated models, including GPT-4o, Gemini 2.0, Llama-3.1, Llama-3.3, and Qwen-2.5, experience significant performance drops when encountering external errors, with accuracy decreases ranging from 32.7% to 44.9%. Notably, larger models do not demonstrate better recovery abilities, suggesting that current agent architectures struggle fundamentally with failure recovery in complex tool-use scenarios.

## Method Summary
The authors created a benchmark by converting text-to-SQL datasets into executable tools with 4450 functions across 830 questions. Agents must identify correct function compositions while facing forced external errors injected into the execution environment. The evaluation framework measures agent performance under normal conditions versus error-injected conditions, tracking accuracy drops and recovery behaviors. The study evaluates multiple models including GPT-4o, Gemini 2.0, Llama-3.1, Llama-3.3, and Qwen-2.5 across the same benchmark conditions.

## Key Results
- All evaluated models showed significant performance drops when encountering external errors, with accuracy decreases ranging from 32.7% to 44.9%
- Model size did not correlate with improved recovery ability from external failures
- Most errors stemmed from failures to search for or identify correct tools in large search spaces
- GPT-4o, Gemini 2.0, Llama-3.1, Llama-3.3, and Qwen-2.5 all exhibited similar recovery challenges

## Why This Works (Mechanism)
The paper demonstrates that language model agents struggle with external failure recovery due to fundamental limitations in their planning and search capabilities. When faced with unexpected errors, agents must either identify the correct tool to fix the failure or backtrack and search for alternative solutions in a large function space. The benchmark reveals that current models lack robust mechanisms for error detection, root cause analysis, and adaptive replanning, leading to cascading failures when initial plans encounter obstacles.

## Foundational Learning
**Text-to-SQL conversion to executable tools** - Understanding how natural language queries translate to function calls and tool usage patterns. *Why needed:* Forms the basis for creating realistic planning scenarios. *Quick check:* Can the agent correctly map a simple query to its corresponding function call sequence?

**External error injection mechanisms** - Methods for systematically introducing failures into agent execution environments. *Why needed:* Creates controlled test conditions for evaluating recovery behaviors. *Quick check:* Does the error injection maintain consistency across different test runs?

**Function composition analysis** - Evaluating how agents select and sequence multiple functions to achieve complex goals. *Why needed:* Critical for understanding agent planning capabilities. *Quick check:* Can the agent correctly chain 3-4 functions to complete a multi-step task?

## Architecture Onboarding
**Component map:** Natural language query → Function identification → Tool execution → Error detection → Recovery mechanism → Success/failure determination

**Critical path:** Query parsing → Tool selection → Execution → Error handling → Recovery/replan

**Design tradeoffs:** The benchmark prioritizes realistic planning complexity over controlled failure types, sacrificing some experimental control for ecological validity. The large function space (4450 functions) creates challenging search problems but may obscure specific failure patterns.

**Failure signatures:** Performance degradation when encountering errors, inability to identify correct recovery tools, cascading failures from initial errors, and lack of adaptive replanning strategies.

**First experiments:**
1. Run baseline tests without error injection to establish normal performance levels
2. Introduce single-step errors and measure recovery success rates
3. Compare function search efficiency between models under error conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark construction methodology may not fully capture real-world failure complexity
- Artificial error injection may not reflect stochastic nature of actual system failures
- Evaluation focuses primarily on function composition accuracy, potentially overlooking other recovery behaviors
- Absence of human evaluation for failure recovery quality

## Confidence
**Major claims:**
- Performance degradation under external errors (Medium)
- Model size does not improve recovery (Medium)
- Failure patterns are consistent across model families (Medium)

## Next Checks
1. Test the benchmark with real-world failure scenarios from deployed agent systems to validate the artificial error injection methodology
2. Conduct ablation studies varying error types and frequencies to understand their differential impact on agent performance
3. Evaluate agent behavior under resource constraints to determine if failure recovery patterns change when computational budgets are limited