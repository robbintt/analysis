---
ver: rpa2
title: Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots
arxiv_id: '2512.06193'
source_url: https://arxiv.org/abs/2512.06193
tags:
- risk
- safety
- gauge
- conversational
- harm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GAUGE is a real-time framework that detects hidden conversational
  escalation in AI chatbots by analyzing the LLM's internal probability shifts toward
  affective risk, without relying on external classifiers. It outperforms existing
  safety mechanisms on implicit harm benchmarks, achieving higher AUROC, AUPRC, and
  F1 scores than external classifiers like HateBERT and Llama-Guard-3-8B.
---

# Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots

## Quick Facts
- arXiv ID: 2512.06193
- Source URL: https://arxiv.org/abs/2512.06193
- Authors: Jihyung Park; Saleh Afroogh; David Atkinson; Junfeng Jiao
- Reference count: 22
- Key outcome: GAUGE achieves 6% attack success rate vs 97.3% baseline on adversarial tests

## Executive Summary
GAUGE is a real-time framework that detects hidden conversational escalation in AI chatbots by analyzing the LLM's internal probability shifts toward affective risk, without relying on external classifiers. It outperforms existing safety mechanisms on implicit harm benchmarks, achieving higher AUROC, AUPRC, and F1 scores than external classifiers like HateBERT and Llama-Guard-3-8B. On the MinorBench adversarial test, GAUGE reduces attack success rates from 97% to 6%, demonstrating superior robustness to bypass attempts. The method offers interpretability by projecting logits onto an affective space, enabling monitoring of conversational drift.

## Method Summary
GAUGE uses a two-stage framework to detect conversational escalation. Stage 1 calibrates a risk weight vector λ by analyzing labeled Safe/Harmful dialogues from DiaSafety, updating λ via exponential moving average based on the LLM's token-level probability distributions over an emotion lexicon. Stage 2 applies this calibrated λ to live conversations, computing two metrics: Negative Risk Shift (NRS) measuring directional momentum toward harm, and Absolute Risk Potential (ARP) measuring dwelling in high-risk states. The framework aggregates these metrics across dialogue trajectories to flag implicit harm invisible to surface-level toxicity filters.

## Key Results
- GAUGE-mean achieves 0.6698 AUROC vs 0.5884 for Llama-Guard-3-8B on DiaSafety
- On MinorBench, GAUGE reduces attack success rate from 97.3% to 6%
- GAUGE provides interpretability through affective space projection of conversation trajectories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Internal logit distributions over emotion-associated tokens encode conversational risk signals invisible to surface-level text classifiers.
- **Mechanism:** During autoregressive generation, GAUGE extracts token-level log-probabilities for words from the NRC Emotion Lexicon at each generation step k. These are aggregated into a risk log-probability vector r_k ∈ R^|W|. By tracking how probability mass flows toward affectively charged tokens, GAUGE captures "probabilistic momentum" toward negative outcomes before they manifest as explicit toxicity.
- **Core assumption:** The LLM's internal probability distribution over emotion words reflects meaningful affective steering intent, not just statistical correlation.
- **Evidence anchors:**
  - [abstract]: "GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue."
  - [section 3.1]: "The log-probability of a risk word w_i at generation step k... is computed as the sum of the log-probabilities of its constituent subtokens, conditioned on the current prefix."
  - [corpus]: Related work on emotion-sensitive conversational AI (arxiv:2502.08920) explores affective detection but does not validate the specific logit-probing mechanism GAUGE employs.
- **Break condition:** If emotion lexicon coverage is insufficient for the domain (e.g., slang, emojis), probability signals will be sparse and unreliable.

### Mechanism 2
- **Claim:** A calibrated risk direction vector λ, learned from labeled dialogues, enables detection of harmful affective trajectories via cosine similarity and magnitude metrics.
- **Mechanism:** Stage 1 calibration processes labeled Safe/Harmful dialogues from DiaSafety. For each dialogue, mean feature vector z is computed from the response trajectory. The λ vector is updated via exponential moving average: λ ← (1-β)λ + α·S·ẑ, where S = +1 for harmful, -1 for safe. After calibration, λ represents a directional reference in affective space pointing toward harm.
- **Core assumption:** Harmful and safe dialogues exhibit distinguishable, generalizable patterns in their affective probability trajectories that can be captured by a single direction vector.
- **Evidence anchors:**
  - [section 3.2]: "We derive a reference weight vector λ where positive values indicate a contribution to harm."
  - [table 1]: GAUGE-mean achieves 0.6698 AUROC vs. 0.5884 for Llama-Guard-3-8B, demonstrating the calibrated vector captures signal.
  - [corpus]: No corpus papers validate this specific calibration approach; it remains unique to GAUGE.
- **Break condition:** If training data labels are noisy or the harmful/safe distinction doesn't align with affective trajectory patterns, λ will point in an irrelevant direction.

### Mechanism 3
- **Claim:** Two complementary metrics—Negative Risk Shift (NRS) and Absolute Risk Potential (ARP)—capture both directional momentum and static dwelling in high-risk states.
- **Mechanism:** NRS = cos(λ, z) measures alignment between current response trajectory and the harmful direction (directional momentum). ARP = Σᵢ λᵢ · Z(zᵢ) / Σᵢ λᵢ applies Z-score normalization to measure absolute magnitude of risk, capturing intense affective focus even without directional shift. High NRS indicates active steering toward harm; high ARP indicates sustained dwelling in negative affect.
- **Core assumption:** Conversational escalation manifests either as movement toward negative affect (NRS) or sustained presence in negative affective space (ARP), and these are sufficient signals for implicit harm.
- **Evidence anchors:**
  - [section 3.3.1]: "A high positive NRS indicates the assistant's response trajectory actively aligns with the harmful affective direction defined by λ."
  - [section 3.3.2]: "ARP detects when the conversation is statically dwelling in a high-risk state, capturing intense affective focus even if the directional shift is subtle."
  - [table 2]: On MinorBench, GAUGE achieves 6% ASR vs. 97.3% for Llama-Guard-3-8B, suggesting the dual-metric approach captures adversarial drift.
  - [corpus]: Related work on toxicity mitigation (arxiv:2507.05660) focuses on training data, not inference-time trajectory analysis.
- **Break condition:** If therapeutic validation uses similar affective language as harmful reinforcement, both metrics will produce false positives.

## Foundational Learning

- **Concept: Autoregressive logit extraction**
  - Why needed here: GAUGE operates by intercepting the model's output distribution at each generation step; understanding how logits relate to token probabilities is essential.
  - Quick check question: Given logits [2.0, 1.0, 0.1] for tokens ["sad", "happy", "neutral"], what is P("sad") after softmax?

- **Concept: Cosine similarity in high-dimensional spaces**
  - Why needed here: NRS is computed as cosine similarity between λ and z; interpreting this metric requires understanding what alignment means in affective space.
  - Quick check question: If NRS = 0.85, does this indicate high or low alignment with harmful direction?

- **Concept: Z-score normalization**
  - Why needed here: ARP relies on Z-score normalization to compare risk magnitudes across different conversations and token distributions.
  - Quick check question: If a risk component has mean μ=0.3 and std σ=0.1, what is the Z-score for an observed value of 0.5?

## Architecture Onboarding

- **Component map:**
  Lexicon layer: NRC Emotion Lexicon → pre-tokenized subtoken sequences
  Extraction layer: Per-step log-probability aggregation → risk vectors r_k
  Calibration module (Stage 1): EMA updates on DiaSafety → frozen λ vector
  Detection module (Stage 2): NRS/ARP computation on live trajectories
  Aggregation layer: min/mean/top-k/percentile for dialogue-level scoring

- **Critical path:**
  1. Deploy LLM with modified forward pass that captures logits at each step
  2. Run Stage 1 calibration on labeled dialogue data to produce λ
  3. At inference, for each assistant response: extract trajectory → compute z → calculate NRS and ARP
  4. Apply threshold (τ=0.0 used in paper) or aggregate for dialogue-level decision

- **Design tradeoffs:**
  - Lexicon size vs. coverage: Larger lexicon captures more affective signal but increases O(nk) overhead; paper uses NRC which may miss slang/emojis
  - Aggregation strategy: Mean offers best AUROC (0.6698), but min may be more sensitive to single high-risk tokens
  - Threshold selection: Fixed τ=0.0 works for MinorBench but may need tuning for different deployment contexts

- **Failure signatures:**
  - High false positives on therapeutic content: "I understand why you feel hopeless" triggers high scores (noted in Limitations)
  - Missed harm via out-of-vocabulary terms: Slang, internet speak, or emoji-based harm evades detection
  - Context blindness for multi-turn escalation: GAUGE analyzes single response trajectories; cumulative escalation across turns requires additional architecture

- **First 3 experiments:**
  1. **Reproduce DiaSafety calibration:** Train λ on the DiaSafety train split, verify AUROC falls within reported range (0.62-0.67) on test split to confirm implementation correctness.
  2. **Threshold sensitivity analysis:** Sweep τ from -0.5 to +0.5 on a held-out validation set; plot precision-recall curve to identify optimal operating point for your deployment context.
  3. **Lexicon ablation:** Replace NRC Emotion Lexicon with a domain-specific affect lexicon (e.g., mental health terminology) and measure impact on ARP/NRS distributions for your target use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pragmatic markers be integrated into GAUGE to distinguish therapeutic validation from harmful reinforcement when both contain negative affect words?
- Basis in paper: [explicit] "While GAUGE detects the direction of affect, distinguishing the intent (harmful vs. therapeutic) requires future integration of pragmatic markers."
- Why unresolved: The current framework flags supportive responses like "I understand why you feel hopeless" as high-risk due to negative affect vocabulary, conflating empathic listening with maladaptive reinforcement.
- What evidence would resolve it: A comparative study where GAUGE augmented with discourse-level features (e.g., response positioning, mitigation markers) achieves significantly higher precision on therapeutic dialogue datasets without loss in detecting genuine implicit harm.

### Open Question 2
- Question: How does GAUGE's detection performance degrade when harmful content is expressed through out-of-vocabulary slang, emojis, or emergent internet terminology absent from static emotion lexicons?
- Basis in paper: [explicit] "The method is bounded by the lexicon's static vocabulary. Consequently, it may exhibit reduced sensitivity to harm conveyed through out-of-vocabulary terms, internet slang, or emojis."
- Why unresolved: Adolescent communication increasingly relies on evolving slang and visual symbols, yet GAUGE's reliance on the fixed NRC Emotion Lexicon creates blind spots for novel expressions of distress or manipulation.
- What evidence would resolve it: A benchmark evaluation on a youth-specific dataset containing contemporary slang and emoji-heavy harmful dialogues, comparing GAUGE's recall against dynamic lexicon expansion or embedding-based affect detection methods.

### Open Question 3
- Question: Does the calibrated risk vector λ transfer effectively across different LLM architectures, or does calibration need to be repeated for each target model?
- Basis in paper: [inferred] All experiments use only Llama-3.1-8B-Instruct, and the calibration process (Stage 1) operates on logit distributions that are model-specific. No cross-model transfer experiments are reported.
- Why unresolved: If λ captures model-internal probability dynamics rather than universal affective patterns, deployment across diverse production models may require per-model recalibration, increasing operational costs.
- What evidence would resolve it: A transfer learning experiment where λ calibrated on one model family (e.g., Llama) is evaluated on another (e.g., GPT, Mistral) without recalibration, measuring AUROC degradation.

### Open Question 4
- Question: What is the optimal temporal granularity for NRS and ARP aggregation in multi-turn conversations spanning hours or days rather than single exchanges?
- Basis in paper: [inferred] The paper aggregates token-level scores into dialogue-level metrics but does not address how risk accumulates or decays across temporally extended interactions where escalation may be gradual and intermittent.
- Why unresolved: Real-world parasocial relationships develop over extended periods; a single high-risk response may be less concerning than sustained low-level negative affective momentum across weeks.
- What evidence would resolve it: A longitudinal study with simulated multi-day conversations, evaluating whether exponential decay-weighted aggregation or conversation-history-aware thresholds improve detection of slow-burn escalation patterns.

## Limitations

- Lexicon coverage gaps for slang, emojis, and domain-specific vocabulary limit detection of modern harmful communication
- Therapeutic validation is often flagged as harmful due to shared negative affect vocabulary
- Static calibration vector may not generalize across different LLM architectures or domains

## Confidence

**High Confidence Claims:**
- GAUGE achieves superior AUROC/AUPRC scores compared to external classifiers on the DiaSafety benchmark (0.6698 vs 0.5884 for Llama-Guard-3-8B)
- The dual-metric approach (NRS + ARP) captures different aspects of conversational drift effectively
- GAUGE demonstrates robustness to adversarial attacks on MinorBench (6% ASR vs 97.3% for baseline)

**Medium Confidence Claims:**
- The calibration process produces a generalizable λ vector that captures harmful trajectory patterns
- Logit-based internal probability analysis is more effective than external classifier approaches
- The affective space projection provides meaningful interpretability

**Low Confidence Claims:**
- GAUGE's real-time performance characteristics at scale (computational overhead not reported)
- Generalizability beyond the specific datasets and domain used in evaluation
- Long-term stability of the calibration vector λ across model updates

## Next Checks

1. **Lexicon Coverage Analysis:** Measure the percentage of dialogue tokens that map to the NRC Emotion Lexicon across all evaluation datasets. Identify specific vocabulary gaps (slang, emojis, domain-specific terms) and quantify the impact on detection performance. This would validate whether the lexicon limitation is a theoretical concern or a practical failure point.

2. **Cross-Domain Calibration Transfer:** Train λ on DiaSafety but evaluate on an entirely different domain (e.g., mental health counseling transcripts or customer service conversations). Measure performance degradation to assess whether the calibration captures generalizable harmful patterns or dataset-specific artifacts.

3. **Therapeutic Dialogue False Positive Audit:** Create a curated test set of therapeutic conversations (from counseling transcripts or validated therapy chatbot interactions) and measure GAUGE's false positive rate. This would quantify the practical impact of the therapeutic validation limitation and inform whether threshold tuning or additional pragmatic features are needed for deployment in mental health contexts.