---
ver: rpa2
title: 'MaxGlaViT: A novel lightweight vision transformer-based approach for early
  diagnosis of glaucoma stages from fundus images'
arxiv_id: '2502.17154'
source_url: https://arxiv.org/abs/2502.17154
tags:
- accuracy
- maxvit
- block
- performance
- glaucoma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MaxGlaViT introduces a novel lightweight vision transformer-based
  approach for early diagnosis of glaucoma stages from fundus images. The method addresses
  the challenge of accurately detecting glaucoma at various stages by improving the
  Multi-Axis Vision Transformer (MaxViT) architecture.
---

# MaxGlaViT: A novel lightweight vision transformer-based approach for early diagnosis of glaucoma stages from fundus images

## Quick Facts
- arXiv ID: 2502.17154
- Source URL: https://arxiv.org/abs/2502.17154
- Reference count: 40
- Achieved 92.03% accuracy on HDV1 glaucoma fundus image dataset

## Executive Summary
MaxGlaViT introduces a novel lightweight vision transformer-based approach for early diagnosis of glaucoma stages from fundus images. The method addresses the challenge of accurately detecting glaucoma at various stages by improving the Multi-Axis Vision Transformer (MaxViT) architecture. Key innovations include scaling MaxViT to reduce parameters by 80% while maintaining performance, enhancing the stem block with Efficient Channel Attention (ECA), and replacing Mobile Inverted Bottleneck Convolution (MBConv) blocks with advanced ConvNeXtV2 modules. The model was evaluated on the HDV1 dataset containing fundus images of different glaucoma stages and compared against 40 CNN and 40 ViT models.

## Method Summary
MaxGlaViT is a modified MaxViT architecture specifically designed for glaucoma stage classification from fundus images. The method combines architectural scaling, attention enhancement, and block substitution to create a lightweight yet effective model. The approach begins with a scaled-down MaxViT base, then enhances feature extraction through ECA in the stem and replaces MBConv blocks with ConvNeXtV2 modules that include Global Response Normalization. The model was trained and evaluated on the HDV1 dataset using transfer learning from ImageNet weights, with careful attention to parameter efficiency and generalization capability.

## Key Results
- Achieved state-of-the-art performance with 92.03% accuracy, 92.33% precision, 92.03% recall, 92.13% f1-score, and 87.12% Cohen's kappa score
- Reduced model parameters by 80% (from 31M to 6.2M) while maintaining superior performance
- Outperformed 80 baseline models (40 CNN and 40 ViT architectures) on the HDV1 glaucoma dataset
- Demonstrated superior generalization capabilities for glaucoma stage detection

## Why This Works (Mechanism)

### Mechanism 1
Aggressively scaling down model parameters (reducing blocks and channels) likely improves generalization on small medical datasets by constraining model capacity to match data availability. The authors reduced the standard MaxViT-Tiny (31M parameters) to a "Scaled" version (6.2M parameters). By reducing the depth (blocks) and width (channels), the model is prevented from overfitting the limited training samples (1,542 images), forcing it to learn more robust, generalizable features rather than memorizing noise.

### Mechanism 2
Injecting Efficient Channel Attention (ECA) into the stem (early convolution layers) improves the quality of feature representations passed to the transformer blocks. ECA is added after the initial convolution layers in the stem. Unlike SE or CBAM, ECA avoids dimensionality reduction, capturing cross-channel interactions efficiently. This allows the model to emphasize relevant channels (e.g., structural features of the optic disc) early in the pipeline, providing a cleaner signal for the subsequent MaxViT blocks.

### Mechanism 3
Replacing Mobile Inverted Bottleneck (MBConv) structures with ConvNeXtV2 modules enhances the model's ability to handle feature variance and collapse. The authors hypothesize that MBConv is less effective than modern pure-convolution blocks. ConvNeXtV2 includes a Global Response Normalization (GRN) layer, which increases contrast between channels and prevents feature collapse, potentially making the transformer attention mechanisms more effective.

## Foundational Learning

- **Concept: Hybrid Attention in Vision Transformers (MaxViT)**
  - **Why needed here:** The core architecture uses both local (block) and global (grid) attention. You must understand this to know *where* the ConvNeXtV2 replacement fits (it sits before the attention in the block).
  - **Quick check question:** Does the MaxViT block process the image as a whole sequence (like standard ViT) or does it use a hierarchical grid approach?

- **Concept: Attention Modules (ECA vs. SE/CBAM)**
  - **Why needed here:** The paper selects ECA over CBAM and SE for the stem. You need to understand *why* avoiding dimensionality reduction (a feature of ECA) matters for preserving spatial-channel information in medical imaging.
  - **Quick check question:** Why might "Squeeze" operations in Squeeze-and-Excitation (SE) blocks be detrimental compared to the adaptive 1D convolution used in ECA?

- **Concept: Transfer Learning & Fine-tuning Strategies**
  - **Why needed here:** All models used ImageNet weights. Understanding how to adapt a large pre-trained model to a small dataset (HDV1) via scaling and freezing is critical for reproducing these results.
  - **Quick check question:** When scaling down a model (reducing channels), how do you handle the loading of pre-trained weights from a larger model (e.g., MaxViT-Tiny)?

## Architecture Onboarding

- **Component map:** Input: 224x224 Fundus Image → Stem: Conv2D → BatchNorm → GELU → Conv2D → [ECA Module] → BatchNorm → Stages 1-4: [ConvNeXtV2 Block] → [Block Attention] → [Grid Attention] → Head: Global Average Pool → FC Layer (3 classes)

- **Critical path:** The interaction between the **ECA-enhanced Stem** and the **ConvNeXtV2-enhanced Blocks** is the "secret sauce." The stem provides high-quality initial embeddings, while the ConvNeXtV2 blocks refine these features with GRN before spatial mixing occurs.

- **Design tradeoffs:**
  - **Lightness vs. Accuracy:** The model sacrifices the depth of MaxViT-Tiny (which had lower accuracy) for a shallower, wider (conceptually) or simply more efficient structure.
  - **Complexity:** Adding ConvNeXtV2 and ECA increases the logical complexity of the forward pass compared to a standard off-the-shelf MaxViT, potentially making debugging harder.

- **Failure signatures:**
  - **Overfitting on "Normal":** If the model predicts "Normal" predominantly, the scaling was insufficient or the class imbalance (Normal = 786 vs Early = 289) wasn't handled.
  - **Stem Gradient Issues:** If training diverges immediately, check the implementation of the ECA module; ensure it is not blocking gradient flow.

- **First 3 experiments:**
  1. **Baseline Validation:** Run the "Scaled MaxViT" (no ECA, no ConvNeXtV2) to verify the 87.93% accuracy baseline.
  2. **Stem Ablation:** Add only ECA to the stem of the Scaled MaxViT (keep MBConv) to isolate the +1.08% gain claimed in Table 8.
  3. **Block Ablation:** Take the best Stem config, and swap MBConv for ConvNeXtV2 to isolate the final performance jump to 92.03%.

## Open Questions the Paper Calls Out

None

## Limitations

- **Dataset Size:** HDV1 contains only 1,542 images across 3 glaucoma stages, raising concerns about overfitting despite aggressive scaling. The reported strong performance may not generalize to external datasets without further validation.
- **Architecture Specificity:** The combination of ECA and ConvNeXtV2 has not been validated independently; it's unclear which component contributes more to the performance gain.
- **Class Imbalance:** With Normal class having nearly double the samples of Early and Advanced stages, the model may be biased toward Normal predictions despite the reported balanced metrics.

## Confidence

- **High Confidence:** The general trend of lightweight models performing better on small medical datasets is well-supported by the ablation studies (Scaling MaxViT from 31M to 6.2M parameters).
- **Medium Confidence:** The specific architectural choices (ECA in stem, ConvNeXtV2 blocks) show performance improvements in isolation, but the cumulative effect on the final 92.03% accuracy needs independent verification.
- **Low Confidence:** The comparison against 80 baseline models (40 CNN + 40 ViT) lacks detail on implementation specifics, making it difficult to assess the fairness of these comparisons.

## Next Checks

1. **External Dataset Validation:** Test MaxGlaViT on a completely separate glaucoma fundus image dataset (e.g., RIM-ONE or DRISHTI-GS) to verify generalization claims beyond HDV1.
2. **Ablation Replication:** Independently reproduce the three-stage ablation (Scaled MaxViT → ECA addition → ConvNeXtV2 addition) to verify the incremental performance improvements reported in Tables 8 and 10.
3. **Grad-CAM Visualization:** Generate class activation maps for all three glaucoma stages to verify the model is indeed learning clinically-relevant features (optic disc cupping, nerve fiber layer changes) rather than dataset-specific artifacts.