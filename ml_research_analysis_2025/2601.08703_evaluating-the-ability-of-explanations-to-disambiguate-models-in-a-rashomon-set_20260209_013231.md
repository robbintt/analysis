---
ver: rpa2
title: Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon
  Set
arxiv_id: '2601.08703'
source_url: https://arxiv.org/abs/2601.08703
tags:
- explanations
- explanation
- evaluation
- rashomon
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of evaluating explanations in\
  \ a Rashomon set of models\u2014models with similar performance but different internal\
  \ mechanisms. The authors identify that existing ground-truth and sensitivity-based\
  \ metrics often fail to capture differences between models in a Rashomon set and\
  \ can be misled by adversarial fairwashing attacks."
---

# Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set

## Quick Facts
- arXiv ID: 2601.08703
- Source URL: https://arxiv.org/abs/2601.08703
- Authors: Kaivalya Rawal, Eoin Delaney, Zihao Fu, Sandra Wachter, Chris Russell
- Reference count: 12
- One-line primary result: AXE framework successfully identifies correct explanations with 100% accuracy while traditional metrics fail half the time on adversarial fairwashing attacks

## Executive Summary
This paper addresses the challenge of evaluating explanation quality for models in a Rashomon set—models with similar performance but different internal mechanisms. The authors identify fundamental limitations in existing ground-truth and sensitivity-based metrics that often fail to capture differences between models and can be misled by adversarial attacks. They propose three principles for explanation evaluation: local contextualization, model relativism, and on-manifold evaluation, and introduce AXE, a new framework that evaluates explanations by measuring how well top-n important features predict model outputs using k-NN models. When tested against adversarial fairwashing attacks, AXE successfully identified correct explanations while traditional metrics failed.

## Method Summary
AXE evaluates local feature-importance explanations by training k-NN models to predict the target model's predictions (Y_preds = m(X)) using only the top-n important features identified by each explanation. For each datapoint, a unique k-NN is trained on the filtered feature set, and accuracy is measured against the model's actual output. The final score is the mean accuracy across all datapoints. This approach is ground-truth agnostic and avoids off-manifold behavior by using only existing datapoints. The framework is designed to capture differences between models in a Rashomon set while being robust to adversarial manipulation.

## Key Results
- AXE achieves 100% detection accuracy for correct explanations in adversarial fairwashing scenarios
- Traditional metrics (PGI, PGU) fail 50% of the time when explanations are manipulated to hide discriminatory features
- AXE successfully distinguishes between different models in a Rashomon set using the same explanation, while ground-truth metrics assign identical scores
- The framework demonstrates the importance of model relativism by evaluating explanations relative to each model's unique decision boundary

## Why This Works (Mechanism)

### Mechanism 1: k-NN Predictiveness as Explanation Fidelity Proxy
AXE assumes an explanation is high-quality if its top-n important features can predict model outputs more accurately than unimportant features. For each datapoint, a unique k-NN model is trained using only the top-n features from the explanation, and the ability to recover the original model's prediction serves as a fidelity proxy. This works when the model's decision boundary is locally approximable by k-NN and features have clear importance signals.

### Mechanism 2: Model Relativism via Prediction-Target Alignment
AXE trains k-NN models to predict Y_preds = m(X)—the specific model's outputs—not true labels. This ties explanation quality directly to each model's unique decision surface, ensuring that explanations surface behavioral differences across models in a Rashomon set rather than masking them with ground-truth alignment.

### Mechanism 3: On-Manifold Evaluation Blocks Off-Manifold Manipulation
By restricting k-NN training to existing datapoints X without synthetic perturbations, AXE prevents adversarial models from manipulating off-manifold behavior to fool explainers. This on-manifold approach neutralizes attacks that exploit regions where models differ outside the data manifold.

## Foundational Learning

- Concept: Rashomon Set
  - Why needed here: The entire paper is about disambiguating models with similar performance but different internals. Without this concept, the problem statement is unclear.
  - Quick check question: Can two models with identical accuracy on a test set use completely different features to make predictions?

- Concept: Feature-Importance Explanations (LIME/SHAP)
  - Why needed here: AXE evaluates local feature-importance explanations. You must understand what these explainers produce (signed vectors) and their failure modes.
  - Quick check question: If SHAP assigns importance 0.8 to feature X₁ and -0.2 to X₂, what does this mean for a single prediction?

- Concept: Perturbation-Based Sensitivity Analysis
  - Why needed here: PGI and PGU (the failing baselines) are sensitivity metrics. Understanding why they fail helps clarify why AXE's approach is different.
  - Quick check question: Why might perturbing a feature to measure model sensitivity produce misleading explanations?

## Architecture Onboarding

- Component map: Dataset X, model m, explanation set E → Per-datapoint k-NN trainer → k-NN model Mᵏᵢ → Predict Y_preds = m(X) → Accuracy score → Aggregator → Mean accuracy
- Critical path: 1) Extract top-n features from explanation eᵢ for datapoint xᵢ, 2) Filter X to these features → X_f, 3) Train k-NN on X_f to predict Y_preds, 4) Score accuracy for xᵢ, 5) Average across all datapoints
- Design tradeoffs: n (top-n features) affects precision vs baseline accuracy; k (neighbors) affects locality vs smoothness; distance metric choice matters for non-tabular features
- Failure signatures: All explanations score similarly (check if k-NN is trivially predictable), AXE scores fairwashed explanations highly (data manifold may not cover differing regions), high variance across datapoints (n may be too small)
- First 3 experiments: 1) Replicate Table 2 on COMPAS/German Credit to verify 100% detection, 2) Ablation study on n ∈ {1,3,5,10} and k ∈ {1,3,5,10}, 3) Apply to custom Rashomon set of 2-3 models with different feature reliance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section implicitly raises several areas for future research regarding generalization to other modalities and attack types.

## Limitations
- Limited empirical validation to synthetic adversarial scenarios rather than real-world Rashomon sets
- No systematic exploration of hyperparameter sensitivity for k and n
- Focus exclusively on tabular datasets without addressing image, text, or other modalities
- Only tested against one type of adversarial attack (Slack et al. 2019 fairwashing)

## Confidence
- **High**: AXE framework design and core principles (local contextualization, model relativism, on-manifold evaluation)
- **Medium**: Empirical results on synthetic adversarial data showing AXE's superiority over PGI/PGU
- **Low**: Generalization to real-world Rashomon sets and non-adversarial scenarios

## Next Checks
1. Replicate Table 2 results on COMPAS dataset using n=1, k=5 to verify AXE detects fairwashed explanations
2. Perform ablation study sweeping n ∈ {1, 3, 5, 10} and k ∈ {1, 3, 5, 10} to identify stable hyperparameter configurations
3. Apply AXE to a custom Rashomon set of 2-3 models trained on same data but using different feature subsets, confirming different quality scores for same explanation across models