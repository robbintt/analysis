---
ver: rpa2
title: 'Explanation Multiplicity in SHAP: Characterization and Assessment'
arxiv_id: '2601.12654'
source_url: https://arxiv.org/abs/2601.12654
tags:
- explanation
- multiplicity
- explanations
- shap
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses explanation multiplicity, where repeated SHAP
  explanations for the same model and input yield substantively different feature
  attributions. The authors develop a methodology to disentangle variability arising
  from model training versus explainer stochasticity.
---

# Explanation Multiplicity in SHAP: Characterization and Assessment

## Quick Facts
- arXiv ID: 2601.12654
- Source URL: https://arxiv.org/abs/2601.12654
- Reference count: 40
- SHAP explanations vary substantially across repeated runs even when model and input are fixed.

## Executive Summary
This paper addresses explanation multiplicity in SHAP, where repeated explanations for the same model and input yield substantively different feature attributions. The authors develop a methodology to disentangle variability arising from model training versus explainer stochasticity. They show that explanation stability depends critically on evaluation metrics: ℓ2 distances can suggest stability while top-k Jaccard and RBO metrics reveal substantial rank disagreement. Across multiple datasets and model classes, they find explanation multiplicity is widespread and persists even for high-confidence predictions. To contextualize observed disagreement, they derive randomized baselines using Dirichlet and Mallows models, providing principled reference points. The dominant source of multiplicity varies by dataset size: model-induced in small-data regimes, explainer-induced in large-scale settings. Neural models show higher multiplicity than tree-based models or TabPFN.

## Method Summary
The authors employ a dual-seed protocol to isolate sources of variability: model seed (s_m) controls training randomness while explainer seed (s_e) controls SHAP background sampling. They compute pairwise disagreement using multiple metrics (ℓ2 distance, top-k Jaccard, RBO) across repeated runs. To assess significance, they compare observed disagreement against randomized baselines generated via Dirichlet and Mallows models. Experiments span three tabular datasets (ACS Income, German Credit, Diabetes) with various model classes (DT, RF, XGB, MLP, FT-Transformer, TabPFN) using 5-fold stratified cross-validation.

## Key Results
- Explanation multiplicity is pervasive and persists even for high-confidence predictions.
- Magnitude-based distances can remain near zero while rank-based measures reveal substantial churn in top features.
- In small-data regimes, model-induced variability dominates; in large-scale settings, explainer-induced variability dominates.
- Neural models exhibit higher multiplicity than tree-based models or TabPFN.

## Why This Works (Mechanism)

### Mechanism 1
- SHAP explanations exhibit variability because practical implementations use stochastic background sampling rather than exact Shapley computation. KernelSHAP approximates marginal contributions by sampling from a background dataset D_bg to simulate missing features, introducing stochasticity.
- Core assumption: Background dataset is a representative sample; approximation error is bounded but nonzero.
- Evidence anchors: [abstract] "SHAP explanations can vary substantially across repeated runs"; [Section 3] describes sampling-based estimation; [corpus] prior work documents SHAP sensitivity to background data.

### Mechanism 2
- Magnitude-based metrics (ℓ2 distance) can mask substantial rank instability because they aggregate across all features and are insensitive to permutation. Rank-based metrics explicitly measure disagreement in feature sets or orderings.
- Core assumption: Stakeholders consume explanations as ranked feature lists, not raw attribution vectors.
- Evidence anchors: [abstract] "Magnitude-based distances can remain near zero while rank-based measures reveal substantial churn"; [Section 5.1] defines metrics and notes ℓ2's "illusion of stability."

### Mechanism 3
- Explanation multiplicity persists even for high-confidence predictions because scale effects allow small fluctuations to reorder features with similar importance. High-confidence predictions have smaller average attribution magnitudes, enabling modest absolute fluctuations to swap nearby-ranked features.
- Core assumption: Feature importance distributions often have flat regions or near-ties among mid-ranked features.
- Evidence anchors: [abstract] "Explanation multiplicity persists even for high-confidence predictions"; [Section 6.4] stratifies by prediction confidence; [corpus] prior work links variability to low-confidence predictions.

## Foundational Learning

- **Shapley values and their approximation**: Why needed? The paper's analysis rests on understanding why exact Shapley computation is infeasible and how sampling-based approximations introduce variability. Quick check: Can you explain why computing Shapley values exactly requires 2^(d-1) model evaluations per feature?

- **Rank-based similarity metrics (Jaccard, RBO)**: Why needed? Understanding how different metrics surface or mask multiplicity is central to the paper's contribution. Quick check: Given two ranked lists [A, B, C, D] and [A, C, B, D], what is their top-3 Jaccard similarity?

- **Model multiplicity (Rashomon effect)**: Why needed? The paper explicitly disentangles model-induced from explainer-induced multiplicity; understanding this distinction is prerequisite. Quick check: Why might two models with identical accuracy on a test set produce different feature attributions for the same instance?

## Architecture Onboarding

- **Component map**: Dual-seed protocol (model seed → training randomness, explainer seed → SHAP sampling) → metric hierarchy (ℓ2 → top-k Jaccard → RBO → feature-wise sensitivity) → baseline calibration (Dirichlet for ℓ2, Mallows for rank metrics).

- **Critical path**: 1) Fix dataset, task, model class, explainer family. 2) Run repeated executions varying s_m, s_e independently. 3) Compute pairwise disagreement using multiple metrics. 4) Compare observed disagreement to randomized baselines.

- **Design tradeoffs**: Larger background datasets reduce explainer variance but increase computation (K=50-100); rank-based metrics surface practice-relevant instability but discard magnitude information; baselines require hyperparameter calibration (ρ, κ for Dirichlet; q for Mallows).

- **Failure signatures**: ℓ2 near-zero but Jaccard near-baseline indicates "illusion of stability"; high model-induced multiplicity in small datasets indicates insufficient data for convergence; high explainer-induced multiplicity in large datasets indicates coarse background sampling.

- **First 3 experiments**: 1) Baseline metric comparison: Generate 50 SHAP explanations varying only s_e, compute ℓ2, Jaccard, RBO to verify ℓ2 masks rank churn. 2) Source disentanglement: On German Credit, run dual-seed protocol comparing model-induced vs. explainer-induced variability. 3) Confidence stratification: Bin instances by prediction probability, compute explainer-induced Jaccard distance to verify multiplicity persists in certain bin.

## Open Questions the Paper Calls Out
None

## Limitations
- The characterization relies on SHAP's KernelSHAP implementation; observed variability may differ with exact computation or alternative explainers.
- Background dataset size (K=50-100) is constrained by computation and could affect explainer-induced variability.
- Baseline models (Dirichlet, Mallows) provide statistical reference points but assume specific generative structures that may not fully capture real explanation distributions.

## Confidence

- **High**: Existence of explanation multiplicity across repeated runs; methodology for disentangling model vs. explainer sources; effectiveness of rank-based metrics in revealing instability masked by ℓ2.
- **Medium**: Generalizability of source dominance patterns across all dataset scales; sufficiency of K=50-100 background samples for practical explainer stability.
- **Low**: Absolute thresholds for "excessive" multiplicity without domain-specific context; extrapolation of findings from tabular datasets to other data types.

## Next Checks

1. Reproduce the dual-seed protocol on a held-out tabular dataset to verify source dominance patterns.
2. Test whether increasing background dataset size (K=200-500) reduces explainer-induced variability in large-scale settings.
3. Compare explanation multiplicity across multiple SHAP implementations (KernelSHAP, TreeSHAP, exact for small d) to isolate implementation effects.