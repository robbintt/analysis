---
ver: rpa2
title: 'AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification
  and Correction'
arxiv_id: '2601.22742'
source_url: https://arxiv.org/abs/2601.22742
tags:
- judgment
- legal
- error
- reasoning
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AR-BENCH, a novel benchmark for evaluating
  legal reasoning through the task of appellate review, which involves detecting,
  classifying, and correcting errors in legal judgments. Unlike prior tasks focused
  on judgment prediction or document generation, appellate review assesses models'
  diagnostic reasoning after judgments are issued.
---

# AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction

## Quick Facts
- arXiv ID: 2601.22742
- Source URL: https://arxiv.org/abs/2601.22742
- Reference count: 40
- Key outcome: Introduces AR-BENCH, a benchmark for legal reasoning through appellate review task; evaluates 14 LLMs showing significant limitations in error detection, especially for legal logic and numerical understanding.

## Executive Summary
This paper introduces AR-BENCH, a novel benchmark for evaluating legal reasoning through the task of appellate review, which involves detecting, classifying, and correcting errors in legal judgments. Unlike prior tasks focused on judgment prediction or document generation, appellate review assesses models' diagnostic reasoning after judgments are issued. The authors construct AR-BENCH with 8,700 finely annotated judgments and 34,617 supplementary corpora, covering 20 fine-grained annotations. Comprehensive evaluation of 14 large language models reveals significant limitations in identifying legal application errors, particularly in complex legal logic and numerical understanding. The benchmark provides a structured framework for systematic evaluation and empirical evidence for future improvements in legal AI.

## Method Summary
AR-BENCH is constructed from the JuDGE dataset with 8,700 annotated judgments and 34,617 supplementary documents. The benchmark evaluates models through a three-stage appellate review task: error detection (binary classification), error classification (charge/prison term/fine error), and error correction (generate corrected value). Models are evaluated under five input settings (S1-S5) varying the presence of facts, reasoning, and law articles. The study uses zero-shot evaluation of 14 LLMs including general-domain, reasoning-enhanced, and domain-specific models, with metrics including Macro-F1, ImpScore, and Acc@0.1 for numerical corrections.

## Key Results
- Error detection is most tractable, with reasoning-enhanced models outperforming general-domain models by 11.4% Macro-F1
- Error correction is substantially more challenging, showing clear performance drops across all metrics
- Incorporating legal statutes provides no additional benefit and may degrade performance, with law articles averaging 558 tokens
- Models struggle significantly with numerical reasoning for proportional fines (E6) compared to fixed-amount violations (E5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured task decomposition improves diagnostic reasoning by isolating specific cognitive demands.
- Mechanism: The three-stage design (detection → classification → correction) creates progressive difficulty. Detection requires binary discrimination; classification requires error-type discrimination; correction requires causal understanding and generative repair. This scaffolding exposes capability gaps at each level.
- Core assumption: Models that fail correction but succeed at detection lack causal understanding, not detection ability.
- Evidence anchors:
  - [abstract] "We decompose APPELLATE REVIEW into three sequential subtasks: error detection, error classification, and error correction"
  - [section 4.3] "Error detection appears to be the most tractable task... error correction is substantially more challenging... with a clear performance drop across all metrics"
  - [corpus] LegalReasoner (arXiv:2506.07443) uses similar step-wised verification-correction, supporting decomposition effectiveness.
- Break condition: If models fail detection entirely, later stages become meaningless; hierarchical evaluation collapses.

### Mechanism 2
- Claim: Reasoning-enhanced models outperform on correction because they generate internal Chain-of-Thought before outputs.
- Mechanism: Reasoning-enhanced LLMs (GPT-4o, DeepSeek-V3.2-Thinking) produce extended reasoning paths via RL optimization. This explicit deliberation helps when models must infer correct legal outcomes rather than classify existing errors.
- Core assumption: The benefit comes from explicit reasoning processes, not merely larger scale or better pre-training.
- Evidence anchors:
  - [section 4.1] "Reasoning-enhanced LLMs... optimized through Reinforcement Learning to produce extended reasoning paths"
  - [section 4.3] "Reasoning-enhanced models outperform general-domain models on the error correction task"
  - [corpus] Limited direct comparison in corpus; neighboring papers focus on prediction/generation, not correction specifically.
- Break condition: If reasoning traces are cut off or truncated, performance degrades to general-domain levels.

### Mechanism 3
- Claim: Providing explicit reasoning process improves performance, but raw legal article text adds noise.
- Mechanism: Normal reasoning process (S2) provides structured sentencing factors and logic chains that guide error detection. Raw law articles (S3) are long (mean 557.9 characters vs. 380.9 for facts) and may introduce retrieval confusion or irrelevant context.
- Core assumption: Models can leverage structured reasoning but struggle to map raw statutory text to specific case facts without intermediate interpretation.
- Evidence anchors:
  - [section 4.4] "S2 and S4 consistently outperform S1... incorporating legal statutes provides no additional benefit and may even degrade performance"
  - [section 3.5] "law articles are significantly longer than other judgment content, making effective utilization... a key challenge"
  - [corpus] Weak corpus evidence; neighboring benchmarks (JuDGE, AppealCase) don't explicitly test this mechanism.
- Break condition: If law articles are pre-filtered or summarized, benefit may emerge.

## Foundational Learning

- Concept: **Anomaly detection vs. prediction paradigm**
  - Why needed here: Appellate review identifies errors in finalized judgments (anomaly detection), fundamentally different from predicting outcomes (classification). Conflating these leads to misapplied methods.
  - Quick check question: Given a judgment document, is your task to find what's wrong or predict what should have been decided?

- Concept: **Fine-grained legal error taxonomy**
  - Why needed here: Six distinct error types (E1-E6) require different diagnostic strategies. Charge errors involve element matching; prison term errors involve numerical bounds; fine errors involve proportional calculations.
  - Quick check question: Can you distinguish E5 (fixed-amount violations) from E6 (percentage-based violations) given a case?

- Concept: **Input composition effects on model reasoning**
  - Why needed here: S1-S5 settings demonstrate that what you provide (facts, reasoning, articles, anomalous components) directly affects performance. Engineering must consider information quality, not just quantity.
  - Quick check question: Does adding law articles help your model, or does it create noise? Test S2 vs. S3.

## Architecture Onboarding

- Component map: JuDGE dataset (8,700 judgments) -> Preprocessing (deduplication, filtering) -> Error injection (E1-E6) -> Five input settings (S1-S5) -> Three-stage task (detection → classification → correction) -> Evaluation metrics (Macro-F1, ImpScore, Acc@0.1)
- Critical path: Parse case facts + judgment → Error detection (binary) → Error classification (3 types) → Error correction (generate fix) → Evaluate with task-appropriate metrics
- Design tradeoffs:
  - S2 vs. S4: S2 provides clean reasoning (higher scores) but S4 mirrors real-world error propagation (more realistic but harder)
  - General vs. reasoning-enhanced: Reasoning models excel at correction but cost more latency
  - Single vs. multi-error cases: Dataset restricts to single-defendant cases for tractability; real appellate review handles complexity
- Failure signatures:
  - Numerical reasoning collapse on proportional fines (E6): models struggle with percentage calculations
  - Omission detection failure (E2): models detect explicit errors but miss absence of charges
  - Instruction following degradation: classification task shows lower compliance rates (Table 15), models under-predict errors
- First 3 experiments:
  1. Baseline benchmark run: Evaluate target model on all three subtasks under S4 setting (most realistic), establish MaF1 baselines for each error type.
  2. Input ablation: Compare S1→S2→S3 progression to quantify benefit of reasoning process vs. cost of raw articles for your specific model.
  3. Error-type stratification: Isolate E6 (percentage fines) vs. E5 (fixed fines) to diagnose numerical reasoning capability independently from legal reasoning.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark restricted to single-defendant cases, limiting real-world applicability
- Controlled error injection methodology may not capture naturally occurring errors
- Three-stage decomposition may not fully capture human appellate review complexity

## Confidence
- **High Confidence:** Comparative performance differences between model types; systematic degradation with law articles; performance drop across task stages
- **Medium Confidence:** Reasoning-enhanced models' specific advantage from Chain-of-Thought; appellate review as distinct paradigm from prediction
- **Low Confidence:** Generalizability to multi-defendant cases; specific contribution of supplementary corpus documents

## Next Checks
1. **Error Type Sensitivity Analysis:** Replicate the benchmark but isolate E6 (percentage-based fine errors) from E5 (fixed-amount violations) to independently validate the claim about numerical reasoning limitations on proportional calculations.
2. **Law Article Processing Evaluation:** Test whether pre-processing law articles (summarization, keyword extraction, or semantic filtering) improves performance compared to raw text provision, addressing the apparent noise identified in S3/S5 settings.
3. **Multi-Error Case Extension:** Construct and evaluate a small subset of multi-defendant or multi-error cases to assess whether the observed performance patterns hold when cases contain multiple simultaneous errors, testing the benchmark's scalability beyond single-error scenarios.