---
ver: rpa2
title: 'Patchwork: A Unified Framework for RAG Serving'
arxiv_id: '2505.07833'
source_url: https://arxiv.org/abs/2505.07833
tags:
- patchwork
- arxiv
- components
- pipeline
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Patchwork addresses the challenge of efficiently deploying retrieval-augmented
  generation (RAG) systems, which face performance bottlenecks due to heterogeneous
  components (vector databases, LLMs, augmenters) with diverse scaling behaviors and
  unpredictable execution times. The framework introduces three innovations: a flexible
  Python-based interface for specifying custom RAG pipelines without requiring domain-specific
  languages, a scheduler that optimizes component-level resource allocation and batching
  using a max-flow formulation to maximize throughput, and an online scheduling mechanism
  that dynamically minimizes service-level objective (SLO) violations by predicting
  potential violations and adjusting resource allocation and request prioritization
  accordingly.'
---

# Patchwork: A Unified Framework for RAG Serving

## Quick Facts
- arXiv ID: 2505.07833
- Source URL: https://arxiv.org/abs/2505.07833
- Reference count: 40
- Primary result: Achieves >48% throughput improvement and ~24% reduction in SLO violations for RAG serving

## Executive Summary
Patchwork addresses the challenge of efficiently deploying retrieval-augmented generation (RAG) systems, which face performance bottlenecks due to heterogeneous components (vector databases, LLMs, augmenters) with diverse scaling behaviors and unpredictable execution times. The framework introduces three innovations: a flexible Python-based interface for specifying custom RAG pipelines without requiring domain-specific languages, a scheduler that optimizes component-level resource allocation and batching using a max-flow formulation to maximize throughput, and an online scheduling mechanism that dynamically minimizes service-level objective (SLO) violations by predicting potential violations and adjusting resource allocation and request prioritization accordingly. Experimental evaluation across four distinct RAG implementations shows Patchwork achieves throughput gains exceeding 48% and reduces SLO violations by approximately 24% compared to commercial alternatives.

## Method Summary
Patchwork uses a Python decorator-based system to capture RAG pipeline execution graphs via PEP 523 frame evaluation, then deploys components as distributed services using gRPC. The framework includes an offline scheduler that profiles each component's latency as a function of batch size and resource allocation, solving a max-flow optimization problem via MILP to determine optimal parameters. An online scheduler monitors request progress, estimates SLO violation probability using running averages, and performs request prioritization and admission control to minimize violations. The system was evaluated across four RAG implementations (CRAG, MemoRAG, IRCoT, HippoRAG) using a 3,000-chat subset of LMSYS-Chat-1M and 50,000 C4 documents, deployed on a 4-server cluster with 8 NVIDIA A100 GPUs per server.

## Key Results
- Throughput improvements exceeding 48% compared to commercial baselines across four RAG applications
- SLO violation reduction of approximately 24% through online mitigation strategies
- Resource allocation optimization provides 1.78× throughput improvement beyond batching alone
- Running average prediction for SLO violations performs comparably to XGBoost with lower overhead

## Why This Works (Mechanism)

### Mechanism 1: Component-Level Resource Allocation via Concurrent-Flow Optimization
- **Claim:** Optimizing batch sizes and resource allocation per-component (rather than uniformly) improves throughput in heterogeneous RAG pipelines.
- **Mechanism:** The offline scheduler profiles each component's latency as a piecewise linear function of batch size, then formulates resource allocation as a concurrent-flow maximization problem. The objective is $\max \min_{i \in N} \sum_k \frac{b_{i,k}}{T_i(b_{i,k}, a_{i,k})}$, solved via MILP (Gurobi).
- **Core assumption:** Component latency scales predictably with batch size within identifiable linear regimes.
- **Evidence anchors:**
  - [abstract]: "an offline scheduler that performs component-level resource allocation and batching"
  - [Section 3.3]: Full optimization formulation with constraints (equations 1-6)
  - [Figure 12b]: Ablation shows batching alone provides 9.6× gain; resource allocation adds 1.78×
  - [corpus]: Related systems (SLOs-Serve, HyperFlexis) also use stage-specific allocation, but focus on LLM-only workloads, not heterogeneous RAG components.
- **Break condition:** If latency variance within batch sizes is high (violating piecewise linearity), optimization may misallocate resources.

### Mechanism 2: Runtime SLO Mitigation via Request Prioritization and Admission Control
- **Claim:** Proactively prioritizing at-risk requests and pausing admissions reduces SLO violations compared to reactive dropping.
- **Mechanism:** Online scheduler tracks elapsed time per request, estimates remaining time via running average, flags requests likely to exceed SLO. Flagged requests receive out-of-order scheduling priority; new request admission pauses until no flagged requests remain.
- **Core assumption:** Running average provides sufficient predictive accuracy for SLO violation detection (paper finds it comparable to XGBoost with lower overhead).
- **Evidence anchors:**
  - [abstract]: "dynamically minimizing SLO violations through strategic request prioritization and resource auto-scaling"
  - [Section 3.4]: Full mitigation strategy description
  - [Figure 10]: Running average and XGBoost show similar violation rates
  - [Figure 13]: Up to 24% SLO violation reduction across RAG applications
  - [corpus]: JITServe (neighbor paper) also addresses SLO-aware serving but assumes input-based runtime prediction, which Patchwork explicitly rejects for RAG.
- **Break condition:** Under extreme load, prioritization cannot help—Figure 13 shows 100% violations at highest request rates regardless of mitigation.

### Mechanism 3: Computational Graph Capture via Python Frame Evaluation
- **Claim:** Using PEP 523 frame evaluation API allows automatic graph capture without DSLs or user code modification.
- **Mechanism:** Decorator `@harmonia.make` marks compute nodes; `harmonia.capture()` context manager intercepts Python frames via PEP 523, extracting bytecode and call graph. Orchestrator then deploys nodes as distributed services with gRPC communication.
- **Core assumption:** Users write RAG logic in Python with standard control flow; decorator placement correctly identifies distributed boundaries.
- **Evidence anchors:**
  - [Section 3.2]: "Patchwork can observe arbitrary function calls and extract the corresponding Python bytecode"
  - [Figure 9]: Complete code example showing decorator and capture usage
  - [Table 2]: Shows 5-10 lines needed for execution specification across RAG apps
- **Break condition:** Complex metaprogramming or non-Python extensions may not be captured correctly by frame evaluation.

## Foundational Learning

- **Concept: Max-Flow / Concurrent Flow Optimization**
  - **Why needed here:** The offline scheduler reformulates RAG resource allocation as maximizing concurrent flow through a directed graph where each node is a RAG component.
  - **Quick check question:** Given a pipeline with 3 components and 2 resource types (CPU, GPU), can you formulate the constraint ensuring total allocations don't exceed available resources?

- **Concept: Service Level Objectives (SLOs) in ML Serving**
  - **Why needed here:** Patchwork's online scheduler continuously estimates SLO violation probability and prioritizes at-risk requests.
  - **Quick check question:** If a request has elapsed 5s, estimated remaining time is 3s, and SLO is 10s, would Patchwork flag it? What if SLO were 7s?

- **Concept: RAG Pipeline Architecture**
  - **Why needed here:** Understanding component heterogeneity (retriever: CPU-bound, generator: GPU-bound) is essential to understanding why uniform allocation fails.
  - **Quick check question:** In a pipeline where the retriever scales poorly with batch size but the generator benefits from large batches, which component becomes the bottleneck if batch size increases without resource rebalancing?

## Architecture Onboarding

- **Component map:** Client Library -> Scheduler (Offline + Online) -> Orchestrator
- **Critical path:**
  1. User annotates Python code → Client library captures graph via PEP 523
  2. Offline scheduler profiles each component (4 min for 5 components in experiments)
  3. MILP solved for optimal $(b_{i,k}, a_{i,k})$ per component
  4. Orchestrator deploys components as distributed services with gRPC
  5. Online scheduler receives requests, routes, tracks SLOs, prioritizes/mitigates

- **Design tradeoffs:**
  - **SLO mitigation vs. throughput:** Out-of-order scheduling creates pipeline bubbles; Figure 14 shows up to 16% throughput reduction when mitigation enabled.
  - **Running average vs. XGBoost estimator:** Similar accuracy (Figure 10), but running average chosen for lower overhead and adaptability.
  - **Profile cost vs. deployment longevity:** 4-minute profiling amortizes over long-running inference services.

- **Failure signatures:**
  - **Misidentified bottleneck:** If profiling underestimates latency for a component, resource allocation starves downstream nodes.
  - **Scheduler overload:** At 1024 req/s, scheduler latency ~2.3ms; beyond this, scheduler may bottleneck (Figure 17).
  - **High-load SLO collapse:** When request rate exceeds capacity, mitigation cannot help—all requests violate SLO (Figure 13, high-load regime).

- **First 3 experiments:**
  1. **Reproduce profiling pipeline:** Take one RAG app (e.g., CRAG), run offline profiler, verify piecewise linear latency curves match Figure 5/6 patterns for your hardware.
  2. **Ablate resource allocation:** Deploy with and without component-level allocation, measure throughput difference (expect 1.5-2× based on Figure 12b).
  3. **Stress-test SLO mitigation:** Fix SLO at 2× median latency, vary request rate, plot violation curve—verify 15-24% reduction at medium load, observe collapse at high load.

## Open Questions the Paper Calls Out
None

## Limitations
- The piecewise linear latency assumption may not hold for components with highly variable execution times, potentially invalidating the optimization formulation
- 4-minute profiling overhead is reasonable for long-running services but may be prohibitive for short-lived deployments or rapid experimentation
- The generality of the Python decorator-based graph capture approach across diverse RAG implementations remains untested beyond the four evaluated systems

## Confidence
- **High Confidence:** The core claim that component-level resource allocation improves throughput compared to uniform allocation is well-supported by ablation studies (Figure 12b showing 1.78× improvement) and aligns with established principles in heterogeneous workload scheduling.
- **Medium Confidence:** The SLO violation reduction claims (24% average improvement) are supported by experimental results, but the online scheduler's effectiveness is heavily dependent on accurate latency prediction and may degrade significantly under extreme load conditions where mitigation cannot prevent violations.
- **Low Confidence:** The generality of the Python decorator-based graph capture approach across diverse RAG implementations remains untested beyond the four evaluated systems, and complex metaprogramming patterns may not be captured correctly by the PEP 523 frame evaluation mechanism.

## Next Checks
1. **Latency Profile Validation:** Measure actual latency vs. predicted latency for each RAG component across multiple batch sizes on target hardware to verify the piecewise linear assumption holds in practice.
2. **Extreme Load Stress Test:** Systematically test SLO violation rates at increasing request rates (100, 500, 1000, 1500 req/s) to identify the exact load threshold where mitigation becomes ineffective and compare against baseline systems.
3. **Metaprogramming Edge Cases:** Implement a RAG component using dynamic code generation or complex decorator patterns to verify the frame evaluation capture mechanism correctly identifies all computational boundaries.