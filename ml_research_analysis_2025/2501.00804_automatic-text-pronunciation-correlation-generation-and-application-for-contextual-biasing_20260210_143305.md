---
ver: rpa2
title: Automatic Text Pronunciation Correlation Generation and Application for Contextual
  Biasing
arxiv_id: '2501.00804'
source_url: https://arxiv.org/abs/2501.00804
tags:
- speech
- atpc
- pronunciation
- contextual
- biasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a data-driven method to automatically generate
  pronunciation correlations between text symbols without relying on manually crafted
  pronunciation lexicons. The approach leverages speech-text alignment and speech
  embeddings to compute pronunciation distances between characters, forming an Automatic
  Text Pronunciation Correlation (ATPC) matrix.
---

# Automatic Text Pronunciation Correlation Generation and Application for Contextual Biasing

## Quick Facts
- arXiv ID: 2501.00804
- Source URL: https://arxiv.org/abs/2501.00804
- Reference count: 28
- Key outcome: Data-driven method automatically generates pronunciation correlations between text symbols using speech-text alignment and embeddings, improving Mandarin speech recognition performance by 13.0% in Character Error Rate and 22.5% in Biased Character Error Rate.

## Executive Summary
This paper introduces a novel data-driven approach for generating pronunciation correlations between text symbols without requiring manually crafted pronunciation lexicons. The method leverages speech-text alignment and speech embeddings to compute pronunciation distances between characters, forming an Automatic Text Pronunciation Correlation (ATPC) matrix. The ATPC matrix is then integrated into end-to-end automatic speech recognition systems to improve contextual biasing performance, particularly for languages or dialects lacking artificial pronunciation resources.

The proposed method addresses a significant limitation in current ASR systems that rely heavily on handcrafted pronunciation lexicons, which are time-consuming to create and unavailable for many languages. By automatically learning pronunciation correlations from paired speech-text data, the approach enables effective contextual biasing in scenarios where traditional lexicon-based methods cannot be applied, demonstrating substantial improvements in recognition accuracy for Mandarin speech.

## Method Summary
The method employs a two-stage approach to generate and apply automatic text pronunciation correlations. First, speech-text alignment is performed using an existing ASR model to establish correspondences between audio frames and text characters. From these alignments, speech embeddings are extracted for each character instance. The embeddings are then used to compute pronunciation distances between character pairs, which are organized into an ATPC matrix representing the likelihood that two characters share similar pronunciations.

In the second stage, the ATPC matrix is incorporated into an end-to-end ASR system through a contextual biasing mechanism. During recognition, the system uses the ATPC scores to adjust the likelihood of character sequences based on their pronunciation similarity, enabling more accurate recognition of contextually relevant words or phrases. The method is evaluated on Mandarin speech recognition tasks, demonstrating significant improvements over baseline systems without requiring any manual pronunciation annotation.

## Key Results
- ATPC improves Character Error Rate by 13.0% compared to baseline end-to-end ASR system
- ATPC reduces Biased Character Error Rate by 22.5% in contextual biasing tasks
- The approach demonstrates effectiveness without requiring manually crafted pronunciation lexicons

## Why This Works (Mechanism)
The method works by leveraging the acoustic similarity between pronunciations to infer relationships between written characters. When speech-text alignment reveals that certain characters consistently correspond to similar acoustic patterns, the system can deduce that these characters likely share similar pronunciations. This correlation information is then used to bias the ASR system toward recognizing character sequences that are both contextually relevant and phonetically plausible, even when the exact pronunciations are not explicitly known.

## Foundational Learning
- **Speech-text alignment**: Required to establish correspondence between audio frames and text characters; quick check: verify alignment accuracy using known word boundaries
- **Speech embeddings**: Capture acoustic-phonetic information from aligned speech; quick check: visualize embedding distances for known similar-sounding characters
- **Pronunciation distance computation**: Quantifies similarity between character pronunciations using embedding distances; quick check: validate that similar-sounding characters have lower distance scores
- **Contextual biasing in ASR**: Uses additional information to influence recognition hypotheses; quick check: test biasing effectiveness with known in-domain vocabulary
- **End-to-end ASR architecture**: Integrated system that directly maps speech to text without separate components; quick check: verify baseline performance matches published results
- **Pronunciation lexicon alternatives**: Methods that bypass traditional pronunciation dictionaries; quick check: compare performance against systems using handcrafted lexicons

## Architecture Onboarding

**Component Map:**
Speech-Text Data -> Speech-Text Aligner -> Embedding Extractor -> ATPC Matrix Generator -> Contextual Biasing Module -> E2E-ASR System

**Critical Path:**
Speech-text alignment and embedding extraction are critical for ATPC generation, while the contextual biasing module is critical for applying ATPC in recognition.

**Design Tradeoffs:**
- Accuracy vs. data requirements: More training data improves ATPC quality but increases computational cost
- Generality vs. specificity: ATPC captures general pronunciation patterns but may miss language-specific nuances
- Integration complexity vs. performance gain: Tighter integration with ASR system yields better results but requires architectural modifications

**Failure Signatures:**
- Poor speech-text alignment leading to incorrect pronunciation associations
- Insufficient training data resulting in unreliable pronunciation distance estimates
- Over-reliance on ATPC causing recognition errors for homophones or characters with multiple pronunciations

**3 First Experiments:**
1. Validate speech-text alignment accuracy on a held-out test set with known alignments
2. Generate ATPC matrix for a small character set and manually verify pronunciation similarity patterns
3. Compare baseline ASR performance with and without contextual biasing using ground truth biasing terms

## Open Questions the Paper Calls Out
None

## Limitations
- Method validated only on Mandarin language, limiting generalizability to other languages and writing systems
- Requires substantial paired speech-text data for alignment-based pronunciation learning, making it unsuitable for low-resource scenarios
- Evaluation focused solely on contextual biasing tasks, leaving open questions about utility in general ASR without biasing

## Confidence

**High Confidence:** The core methodology of using speech-text alignment and embeddings to generate pronunciation correlations is technically sound and the reported improvements over the baseline are statistically significant.

**Medium Confidence:** The effectiveness of ATPC as a plug-in module for contextual biasing is demonstrated, but the evaluation is limited to one language and task type.

**Low Confidence:** Claims about ATPC's applicability to languages without pronunciation lexicons are largely theoretical, as validation beyond Mandarin is not provided.

## Next Checks
1. Cross-linguistic Validation: Test ATPC generation and application on languages with different script types (e.g., alphabetic, syllabic, logographic) and phonological structures to assess generalizability.

2. Low-resource Scenario Testing: Evaluate ATPC performance with varying amounts of training data to determine minimum data requirements and robustness to data scarcity.

3. Comparison with Pronunciation Lexicons: For languages where pronunciation lexicons exist, compare ATPC performance against lexicon-based approaches to quantify relative benefits and identify edge cases where each excels.