---
ver: rpa2
title: Benchmarking LLMs for Predictive Applications in the Intensive Care Units
arxiv_id: '2512.20520'
source_url: https://arxiv.org/abs/2512.20520
tags:
- llms
- data
- loss
- clinical
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks Large Language Models (LLMs) like GatorTron,
  Llama, and Mistral against Small Language Models (SLMs) such as BioBERT and Word2Vec
  for predicting shock in ICU patients. Using MIMIC-III data, embeddings from physician
  notes and vitals were extracted and fed into classifiers (Random Forest, Gradient
  Boosting, etc.).
---

# Benchmarking LLMs for Predictive Applications in the Intensive Care Units

## Quick Facts
- **arXiv ID:** 2512.20520
- **Source URL:** https://arxiv.org/abs/2512.20520
- **Reference count:** 24
- **Primary result:** LLMs performed comparably to SLMs in predicting shock in ICU patients, with GatorTron achieving 80.5% weighted recall

## Executive Summary
This study benchmarks Large Language Models (LLMs) against Small Language Models (SLMs) for predicting shock onset in ICU patients using MIMIC-III data. The research compares GatorTron, Llama, and Mistral embeddings against BioBERT and Word2Vec, feeding them into various classifiers including Random Forest and Gradient Boosting. While GatorTron achieved the highest weighted recall of 80.5%, overall performance metrics showed that LLMs performed comparably to SLMs. The study found that fine-tuning with focal loss did not significantly improve results, suggesting that frozen pre-trained embeddings often work as well or better than fine-tuned versions for this clinical prediction task.

## Method Summary
The study extracted embeddings from clinical notes and vitals in MIMIC-III, using physician notes as primary text sources. Text preprocessing included de-identification removal, spelling correction, and drug masking. Embeddings were generated using GatorTron-Base, Llama-3.1-8B, and Mistral-7B, then passed through feature selection (ExtraTreesClassifier selecting top 100 features) and SMOTE oversampling to handle class imbalance. Random Forest, Gradient Boosting, and other classifiers were trained and evaluated using 100 bootstrap iterations. The cohort consisted of 442 patients (355 normal SI, 87 abnormal SI), with Shock Index calculated as Heart Rate divided by Systolic Blood Pressure, and abnormal defined as ≥ 0.7.

## Key Results
- GatorTron achieved the highest weighted recall of 80.5% for shock prediction
- Overall performance metrics showed LLMs performed comparably to SLMs
- Fine-tuning with focal loss addressed class imbalance but did not significantly improve results
- Random Forest consistently outperformed simpler models like Logistic Regression across all embedding types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM embeddings can substitute for SLM embeddings in clinical prediction pipelines without performance degradation, provided the downstream classifier is appropriately selected.
- **Mechanism:** Pre-trained language models encode clinical text into dense vector representations that capture semantic relationships traditional feature engineering cannot. These embeddings are passed to gradient-based or tree-based classifiers that learn decision boundaries for the target outcome.
- **Core assumption:** Pre-trained representations already contain sufficient signal for the prediction task; fine-tuning on limited data may introduce noise rather than improve signal.
- **Evidence anchors:** Abstract states "LLMs performed comparably to SLMs... GatorTron achieved the highest weighted recall of 80.5%, but overall performance metrics were comparable." Section III.A confirms "Large language models... could not perform better than the small language models. They achieved comparable results on the classification task."
- **Break condition:** If the clinical task requires temporal reasoning across patient trajectories rather than snapshot classification, embedding-only approaches may underperform models trained explicitly on sequence prediction objectives.

### Mechanism 2
- **Claim:** Ensemble classifiers outperform simpler linear models when processing high-dimensional embeddings from clinical text.
- **Mechanism:** Embeddings generate hundreds of features. Tree-based ensembles handle high dimensionality and non-linear interactions better than logistic regression, which assumes linear decision boundaries. Feature selection via ExtraTreesClassifier further reduces noise before classification.
- **Core assumption:** Signal in embeddings is distributed across multiple dimensions rather than concentrated in a few linear predictors.
- **Evidence anchors:** Section III.A states "Random Forest had the best accuracy... Ensemble-based classifiers, such as Random Forest and Gradient Boost, consistently outperformed simpler models like Logistic Regression." Section II.E mentions "Feature selection using ExtraTreesClassifier was performed, and the top 100 features were chosen."
- **Break condition:** If embeddings are already well-structured for the task, simpler models may suffice and ensemble complexity adds unnecessary variance.

### Mechanism 3
- **Claim:** Fine-tuning LLMs with focal loss on small clinical cohorts does not reliably improve prediction performance over frozen pre-trained embeddings.
- **Mechanism:** Focal loss down-weights easy examples and focuses learning on hard, minority-class examples. However, with only 442 total patients (355 normal, 87 abnormal SI), fine-tuning introduces optimization instability without sufficient data to recalibrate the model's representations.
- **Core assumption:** Pre-trained model's representations are already near-optimal for the task domain; limited fine-tuning data cannot meaningfully shift them.
- **Evidence anchors:** Abstract states "Fine-tuning with focal loss addressed class imbalance but did not significantly improve results." Section III.B confirms "Non-fine-tuned models consistently delivered superior or comparable performance across all classifiers and metrics." Section V notes "A relatively small dataset restricted the efficacy of advanced fine-tuning strategies."
- **Break condition:** If larger, more balanced cohorts were available, focal loss fine-tuning might show clearer benefits by allowing the model to specialize without overfitting.

## Foundational Learning

- **Concept: Shock Index (SI) = Heart Rate / Systolic Blood Pressure**
  - **Why needed here:** This is the clinical outcome being predicted. SI ≥ 0.7 indicates abnormal physiological decompensation. Understanding this definition is essential to interpret model labels.
  - **Quick check question:** If a patient has HR=90 and SBP=110, what is their Shock Index? Is it normal or abnormal by the study's threshold?

- **Concept: [CLS] Token Pooling for Long Documents**
  - **Why needed here:** The study splits long clinical notes into chunks, extracts [CLS] tokens from each chunk, and averages them. This enables processing documents longer than the model's context window.
  - **Quick check question:** Why would mean pooling of [CLS] tokens across chunks preserve document-level semantics better than using only the first chunk?

- **Concept: SMOTE for Class Imbalance**
  - **Why needed here:** The cohort has ~4:1 imbalance (355 vs 87 patients). SMOTE synthesizes minority-class examples to prevent classifiers from defaulting to majority-class predictions.
  - **Quick check question:** If you apply SMOTE before or after train-test split, which order is correct and why?

## Architecture Onboarding

- **Component map:** MIMIC-III Database → Cohort Curation (17,294 ICU stays → 442 labeled patients) → Text Preprocessing (de-identification removal, spelling correction, drug masking) → Embedding Extraction (GatorTron/Llama/Mistral → 512-4096 token windows) → Feature Selection (ExtraTreesClassifier → top 100 features) → SMOTE Oversampling → Classification (Random Forest, Gradient Boost, XGBoost, AdaBoost, Logistic Regression) → Evaluation (100 bootstrap iterations, weighted metrics)

- **Critical path:** Embedding extraction quality determines downstream performance. If embeddings fail to capture clinically relevant semantics (e.g., medication context, symptom progression), no classifier can recover signal. The study's SHAP analysis confirms that heparin, coumadin, and other drugs drive predictions—embeddings must encode these relationships.

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | LLMs (Llama 8B, Mistral 7B) | Longer context windows (4096 tokens), richer representations | Higher compute, no performance gain vs SLMs |
  | GatorTron (clinical pre-training) | Domain-specific vocabulary | Still comparable to general SLMs on this task |
  | Frozen embeddings vs fine-tuning | Stability, lower compute | No task-specific adaptation |
  | Focal loss vs cross-entropy | Better minority-class focus | Did not improve results in this study |

- **Failure signatures:**
  - Validation loss increases across epochs during fine-tuning → model overfitting to small training set
  - Logistic regression shows high variance (±0.29 accuracy) → linear models unstable on high-dimensional embeddings
  - Mistral embeddings underperform GatorTron/Llama → general-purpose models may lack clinical vocabulary coverage

- **First 3 experiments:**
  1. **Baseline replication:** Extract BioBERT embeddings for the same cohort, train Random Forest with identical preprocessing, verify you achieve comparable metrics (~80% accuracy). This validates your pipeline.
  2. **Ablation on context window:** Compare GatorTron embeddings using 512-token windows vs Llama's 4096-token windows on the same notes. Test whether longer context improves recall for abnormal SI cases.
  3. **Feature importance audit:** Run SHAP analysis on your best model. Verify that clinically plausible features (anticoagulants, vasopressors) drive predictions, not spurious correlations (e.g., note length, common stopwords).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs surpass SLMs in clinical predictive performance if they are pre-trained or fine-tuned specifically on clinical trajectory objectives rather than static NLP tasks?
- **Basis in paper:** The authors conclude that future efforts "should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping."
- **Why unresolved:** Current benchmarks utilize models trained on general text or static clinical concepts, which may not capture the temporal dynamics required for predicting decompensation.
- **What evidence would resolve it:** A study comparing SLMs against LLMs specifically trained on longitudinal patient outcome prediction tasks.

### Open Question 2
- **Question:** Does scaling the fine-tuning dataset size and diversity significantly improve LLM performance for ICU outcome prediction?
- **Basis in paper:** The authors state that a "key limitation of this study is the small dataset size" and that "limited size of the fine-tuning cohort" restricted the efficacy of advanced fine-tuning strategies.
- **Why unresolved:** The study used a small cohort (87 abnormal cases), leaving it unclear if LLMs would separate from SLMs with sufficient data to leverage their parameter count.
- **What evidence would resolve it:** Benchmarking the same models on a significantly larger, multi-center ICU dataset to evaluate the performance gap with increased data volume.

### Open Question 3
- **Question:** Why did non-fine-tuned LLM embeddings often outperform fine-tuned versions in this clinical context?
- **Basis in paper:** Table II and the results section show that non-fine-tuned models "consistently delivered superior or comparable performance," suggesting that standard fine-tuning (focal/cross-entropy) may degrade the generalizable representations of pre-trained LLMs.
- **Why unresolved:** It is uncertain if the observed degradation is due to the specific loss functions used, the small sample size, or the "embedding-classifier" pipeline itself.
- **What evidence would resolve it:** Ablation studies comparing frozen embeddings against various fine-tuning strategies (e.g., LoRA, QLoRA) on larger clinical cohorts.

## Limitations

- **Data Scale Constraint:** The study's relatively small cohort (442 patients total) represents a fundamental limitation that may not reveal potential advantages of LLMs with larger datasets.
- **Implementation Variability:** Critical implementation details like exact masking procedures, specific note types, and ExtraTreesClassifier configuration are undocumented, making exact replication challenging.
- **Temporal Dynamics Omission:** The classification approach treats each patient encounter as a static snapshot, ignoring the temporal progression that leads to shock and potentially underestimating sequence modeling capabilities.

## Confidence

**High Confidence (8-10/10):** The comparative benchmarking results showing comparable performance between LLMs and SLMs, with GatorTron achieving 80.5% weighted recall as the best performer.

**Medium Confidence (5-7/10):** The generalizability of conclusions to other clinical prediction tasks beyond shock index.

**Low Confidence (1-4/10):** The assertion that LLMs are not inherently superior to SLMs for clinical prediction tasks without additional qualifiers.

## Next Checks

1. **Scale Validation:** Replicate the study using a larger MIMIC-III cohort (e.g., 2000+ patients with similar inclusion criteria) to determine if performance patterns persist or if LLMs show advantages with more training data.

2. **Temporal Architecture Test:** Implement a sequence-to-sequence variant where embeddings from multiple time points are fed into recurrent or transformer-based classifiers, comparing performance against the snapshot approach used in this study.

3. **Domain Transfer Experiment:** Train the same LLM models (GatorTron, Llama, Mistral) on a different clinical prediction task from MIMIC-III (e.g., sepsis prediction or mortality prediction) to assess whether the comparable performance finding generalizes beyond shock index prediction.