---
ver: rpa2
title: 'Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints'
arxiv_id: '2510.21181'
source_url: https://arxiv.org/abs/2510.21181
tags:
- causal
- time
- data
- shylock
- relationships
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Shylock addresses causal discovery in multivariate time series,
  particularly in few-shot scenarios where data is limited. It employs group dilated
  convolution with shared kernels to model time-delayed causal relationships while
  reducing parameter count exponentially.
---

# Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints

## Quick Facts
- **arXiv ID:** 2510.21181
- **Source URL:** https://arxiv.org/abs/2510.21181
- **Reference count:** 31
- **Primary result:** Shylock outperforms NOTEARS and TCDF on synthetic and FMRI datasets, achieving superior F1 scores (up to 1.0) and balanced precision-recall in few-shot MTS settings.

## Executive Summary
Shylock addresses causal discovery in multivariate time series, particularly in few-shot scenarios where data is limited. It employs group dilated convolution with shared kernels to model time-delayed causal relationships while reducing parameter count exponentially. A hybrid approach combining local attention-based learning with global DAG constraints ensures acyclic causal graphs and information sharing across sub-networks. Experiments on synthetic and FMRI datasets show Shylock outperforms NOTEARS and TCDF, achieving superior F1 scores (up to 1.0 in some cases) and balanced precision-recall. It effectively handles time delays and avoids overfitting on small datasets, demonstrating robust performance in both few-shot and normal MTS settings.

## Method Summary
Shylock builds n sub-CNNs, one per target variable, using group dilated convolution with shared kernels and attention-based aggregation. Each sub-network processes all n inputs to predict one target variable, learning local attention weights that are combined into a global attention matrix. A DAG constraint via h(W) = tr(Σβ_k A^k) ensures acyclic predictions. The loss combines MSE with DAG penalty and regularization. The method initializes attention matrices with non-diagonal=1, self-attention=0, and thresholds learned attention weights to identify causal edges.

## Key Results
- Shylock achieves F1 scores up to 1.0 on synthetic few-shot data, outperforming TCDF and NOTEARS
- On FMRI datasets, Shylock maintains higher precision and recall than baselines across varying sample sizes
- The method demonstrates exponential parameter reduction through shared kernels while maintaining or improving causal discovery accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Group dilated convolution with shared kernels reduces parameter count exponentially while preserving the ability to model time-delayed causal relationships in few-shot MTS.
- **Mechanism:** Dilated convolutions expand the receptive field exponentially (d_l ≈ T^{l-1}) without increasing kernel size proportionally, allowing the model to capture long-range temporal dependencies with O(LNT) parameters instead of O(n²f). Shared kernels across sub-networks further reduce parameters by reusing weights for local variable associations, which addresses overfitting when sample sizes are smaller than the parameter space.
- **Core assumption:** Time-delayed causal effects can be captured through sparse, localized temporal patterns that do not require unique kernels for each variable pair.
- **Evidence anchors:** [abstract]: "Shylock can reduce the number of parameters exponentially by using group dilated convolution and a sharing kernel, but still learn a better representation of variables with time delay." [section IV-A]: "With a dilation factor of length d_l, the receptive field reaches approximately d_l = T^{l-1}, mitigating long time delays with only O(LNT) parameters."
- **Break condition:** If causal relationships require unique temporal patterns per variable pair that cannot be compressed through weight sharing, performance degrades; or if time delays exceed the maximum receptive field achievable by the dilation schedule.

### Mechanism 2
- **Claim:** Hybrid constraints combining local attention-based learning with global DAG acyclicity enforcement yield more accurate and self-consistent causal graphs than either constraint alone.
- **Mechanism:** Each sub-CNN learns local causal attention weights independently, but these are combined into a global attention matrix that is penalized for cycles via h(W) = tr(Σβ_k A^k). The trace of matrix powers detects paths of length k from any node back to itself; when h(W) = 0, the graph is acyclic. This allows local fitting objectives to receive global structural feedback during optimization.
- **Core assumption:** The true causal structure is acyclic (a DAG), and cyclic predictions represent model artifacts rather than legitimate feedback loops within the observed timeframe.
- **Evidence anchors:** [abstract]: "A hybrid approach combining local attention-based learning with global DAG constraints ensures acyclic causal graphs and information sharing across sub-networks." [section IV-B]: "This function ensures acyclic relationships when h(W) = 0, and its value increases with the presence of loops."
- **Break condition:** If the underlying system contains genuine feedback loops or instantaneous reciprocal causation within the observation window, the DAG constraint will incorrectly prune true edges.

### Mechanism 3
- **Claim:** Thresholded attention matrices provide interpretable causal edge identification with controllable precision-recall tradeoffs.
- **Mechanism:** The attention matrix A is learned end-to-end, where a_{i,j} represents the strength of the causal relationship from variable j to variable i. Post-training, edges are identified where a_{i,j} ≥ threshold. The paper initializes self-attention (a_{k,k}) to α (commonly 0) and off-diagonal elements to 1, allowing the model to learn which cross-variable dependencies are spurious versus genuine.
- **Core assumption:** Attention weights correlate with causal strength, and a single global threshold can separate signal from noise across all variable pairs.
- **Evidence anchors:** [section IV-A]: "a_{i,j} >= threshold represents the i-th time series x_i as the effect and the j-th time series x_j as the cause." [section V-G, Figure 6]: Case study visualization shows bright regions in attention matrix corresponding to ground truth causal edges (0→1, 5→3, 3→1, 1→0).
- **Break condition:** If attention weights are miscalibrated (e.g., uniformly high or low across edges), threshold selection becomes arbitrary and F1 scores degrade; requires calibration data or validation set tuning.

## Foundational Learning

- **Concept: Dilated (Atrous) Convolution**
  - **Why needed here:** Understanding how receptive fields expand without parameter explosion is essential for grasping why Shylock can model long time delays efficiently.
  - **Quick check question:** Given a 1D convolution with kernel size 3 and dilation factors [1, 2, 4] across three layers, what is the total receptive field?

- **Concept: Directed Acyclic Graphs (DAGs) and Cycle Detection**
  - **Why needed here:** The global constraint relies on the mathematical property that tr(A^k) = 0 for all k ≥ 1 if and only if the graph is acyclic; understanding this enables debugging of constraint violations.
  - **Quick check question:** If A is the adjacency matrix of a 3-node graph with edges 1→2 and 2→3, what is tr(A²) and what does it imply about cycles?

- **Concept: Granger Causality (Intuition)**
  - **Why needed here:** Shylock's per-variable sub-network design follows Granger causality principles—predicting each variable from lagged values of all variables—and attention weights identify which predictors matter.
  - **Quick check question:** In a bivariate system, if X_t helps predict Y_{t+1} beyond Y's own history, what does Granger causality conclude?

## Architecture Onboarding

- **Component map:** Input -> Group dilated conv (temporal features) -> Shared kernel (variable aggregation) -> Attention softmax -> Global attention matrix -> DAG penalty computation -> Loss
- **Critical path:** 1. Input → Group dilated conv (temporal features) → Shared kernel (variable aggregation) → Attention softmax; 2. Attention vectors → Assemble into matrix A → Compute DAG penalty h(W); 3. Backpropagation through combined local (MSE) + global (DAG) loss updates both shared and per-network parameters
- **Design tradeoffs:** Shared kernels reduce parameters but may miss variable-specific temporal patterns; paper claims this improves few-shot generalization. Equal weighting of cycle lengths (β_k = 1) simplifies optimization but may over-penalize short cycles vs. long indirect paths. Threshold selection for edge extraction is post-hoc; paper does not specify automatic calibration, implying manual tuning or validation-based selection.
- **Failure signatures:** High SHD with low Recall: Threshold too aggressive; attention weights are weak across all edges. Cyclic output graph despite training: DAG penalty weight α too low or learning rate insufficient for constraint satisfaction. Overfitting on few-shot data (train loss ↓, test loss ↑): Reduce dilation depth or increase kernel sharing; the paper claims Shylock avoids this, but verify with your data. Attention matrix near-uniform: Shared kernel may be collapsing variable distinctions; check gradient flow.
- **First 3 experiments:** 1. Synthetic few-shot validation: Generate data using the paper's spline-interpolation method with n=4–8 variables, sample sizes 40–160, time delays 2–8. Compare SHD and F1 against TCDF and NOTEARS; expect Shylock to maintain >0.5 F1 where TCDF degrades. 2. Ablation on DAG constraint: Train Shylock with α=0 (no DAG penalty) and α>0; count cyclic edges in output graphs. Validate that h(W)→0 correlates with acyclic predictions. 3. Threshold sensitivity analysis: On a held-out validation set, sweep threshold ∈ [0.1, 0.9] and plot Precision-Recall curve. Identify the threshold yielding balanced F1; compare against paper's implicit threshold choice.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does Shylock scale effectively to high-dimensional multivariate time series (e.g., hundreds of variables)?
- **Basis in paper:** [inferred] Experiments were restricted to datasets with very low node counts (n ∈ {4,8} for synthetic and {5,10} for FMRI), while the method relies on an N × N attention matrix and global matrix trace computations which often have high time complexity.
- **Why unresolved:** The computational and memory overhead of maintaining the attention matrix and calculating the global DAG constraint (tr(∑ β^k A^k)) typically grows non-linearly with the number of variables, potentially limiting applicability to large-scale systems.
- **What evidence would resolve it:** Benchmarking results on datasets with significantly larger variable counts (e.g., N > 50 or 100) with reported runtime and memory usage.

### Open Question 2
- **Question:** Can the method accurately detect instantaneous causal relationships in addition to time-delayed ones?
- **Basis in paper:** [inferred] The architecture is heavily optimized for "time-delayed causal effects" using group dilated convolutions with dilation factors ≥ 1, potentially prioritizing lagged dependencies over immediate ones.
- **Why unresolved:** While standard Granger causality and NOTEARS handle instantaneous effects, the specific use of dilated convolutions might introduce a bias or blind spot for relationships occurring at t=0 if the receptive field skips immediate neighbors.
- **What evidence would resolve it:** Experiments on synthetic datasets specifically designed with a high proportion of instantaneous edges to verify if the shared kernels and attention mechanism capture them.

### Open Question 3
- **Question:** Is Shylock robust to non-linear causal mechanisms?
- **Basis in paper:** [inferred] The synthetic data generation method described in Section V-B relies on a linear combination of time-lagged data ("simulation time sequence data of linear target node"), and the error propagation model K=WQ+δ appears linear.
- **Why unresolved:** While the neural network components (CNNs) are universal approximators, validating the method solely on linearly generated synthetic data leaves its performance on complex, non-linear dynamical systems unverified.
- **What evidence would resolve it:** Evaluation on non-linear benchmark datasets (e.g., NetSim with non-linear coupling or chaotic systems like Lorenz).

## Limitations
- Limited scalability to high-dimensional systems due to N×N attention matrix and global DAG constraint computation
- Potential bias against instantaneous causal relationships due to dilated convolution architecture
- Validation only on linearly generated synthetic data, leaving non-linear causal mechanism performance unverified

## Confidence

| Claim | Confidence |
|-------|------------|
| Efficient parameter reduction through dilated convolution and shared kernels | Medium |
| DAG constraint effectiveness via h(W) → 0 | Medium |
| Attention-based edge identification with threshold selection | Low |

## Next Checks
1. **Synthetic ablation study:** Train Shylock with α=0 (no DAG constraint) vs. α>0; count cyclic edges and verify h(W)→0 correlates with acyclic predictions across varying graph densities.
2. **Threshold calibration validation:** On validation data, sweep attention thresholds and plot Precision-Recall curves; identify optimal threshold and assess stability across different synthetic graph structures.
3. **Parameter efficiency verification:** Count actual parameters in Shylock vs. TCDF baseline; verify the claimed exponential reduction holds when accounting for shared kernels and dilation schedule.