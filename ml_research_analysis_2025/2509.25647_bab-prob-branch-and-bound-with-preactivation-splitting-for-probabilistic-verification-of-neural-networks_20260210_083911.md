---
ver: rpa2
title: 'BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic
  Verification of Neural Networks'
arxiv_id: '2509.25647'
source_url: https://arxiv.org/abs/2509.25647
tags:
- linear
- bounds
- equation
- preactivation
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BaB-prob, a novel branch-and-bound framework
  with preactivation splitting for probabilistic verification of neural networks.
  The method iteratively partitions the problem into subproblems by splitting preactivations,
  using linear bound propagation to compute probability bounds for each subproblem.
---

# BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks

## Quick Facts
- arXiv ID: 2509.25647
- Source URL: https://arxiv.org/abs/2509.25647
- Authors: Fangji Wang; Panagiotis Tsiotras
- Reference count: 40
- Primary result: Novel branch-and-bound framework with preactivation splitting consistently outperforms state-of-the-art methods on probabilistic verification benchmarks

## Executive Summary
This paper proposes BaB-prob, a novel branch-and-bound framework with preactivation splitting for probabilistic verification of neural networks. The method iteratively partitions the problem into subproblems by splitting preactivations, using linear bound propagation to compute probability bounds for each subproblem. The approach is proven sound and complete for feedforward-ReLU networks and extends to piecewise-linear activations. Two splitting strategies are introduced: BaB-prob-ordered, which prioritizes preactivations with zero uncertainty, and BaB+BaBSR-prob, which combines BaBSR heuristics with uncertainty levels. Experiments on untrained models, MNIST/CIFAR-10 networks, and VNN-COMP 2025 benchmarks demonstrate that both versions of BaB-prob consistently outperform state-of-the-art methods, especially in medium- to high-dimensional input spaces. The approach achieves high confidence in probabilistic certificates and shows significant improvements in scalability and success rates.

## Method Summary
BaB-prob addresses probabilistic verification of neural networks by iteratively splitting preactivations (network layer inputs before activation functions) into negative and nonnegative cases. This linearization allows tractable probability bounding through linear bound propagation. The method maintains a branch pool of subproblems, each with linear bounds on preactivations and output, and probability bounds computed via cumulative distribution functions of linearly transformed input distributions. Two splitting strategies are proposed: BaB-prob-ordered prioritizes preactivations with zero uncertainty (probability mass between bounds), while BaB+BaBSR-prob combines BaBSR heuristics with uncertainty levels. The algorithm terminates when probability bounds certify the query (≥ η or < η) or timeout occurs.

## Key Results
- Consistently outperforms state-of-the-art methods on VNN-COMP 2025 benchmarks, especially in medium- to high-dimensional input spaces
- Achieves high confidence (>1-10^-4) in probabilistic certificates with N=10^5 Monte Carlo samples
- Requires 5-125x fewer splits than BaB-prob-ordered on CNNs while maintaining or improving performance
- Demonstrates scalability on untrained models with up to 1024-dimensional inputs and achieves 100% success rates on MNIST/CIFAR-10 benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Preactivation Splitting Linearizes ReLU
BaB-prob iteratively partitions Problem 1 into two subproblems by splitting a preactivation into negative and nonnegative cases, thereby rendering the ReLU function linear within each subproblem. Each unstable preactivation y^(k)_j with bounds [ℓ^(k)_j, u^(k)_j] where ℓ < 0 < u is split into two branches: one where y^(k)_j ≥ 0 (ReLU passes input) and one where y^(k)_j < 0 (ReLU outputs zero). Under each constraint, the ReLU behavior becomes deterministic, allowing linear bound propagation to compute tighter bounds. This process terminates when all branches have no unstable preactivations, guaranteeing tight bounds.

### Mechanism 2: Probability Bounds via Linear Transformations
Linear bounds on the network output and preactivations induce conservative probability bounds through cumulative distribution functions of linearly transformed input distributions. Given linear bounds f(x) = a^T x + b, f̄(x) = ā^T x + b̄, and constraints expressed as linear inequalities, the probability bounds p_ℓ and p_u are computed as P(PX + q ≤ 0) and P(P̄X + q̄ ≤ 0) respectively. For Gaussian inputs, this is tractable since linear transformations of Gaussian random variables remain Gaussian. For general distributions, Monte Carlo sampling with N=10^5 samples and Bernstein's inequality provides confidence guarantees.

### Mechanism 3: Uncertainty Level Guides Efficient Splitting
The algorithm prioritizes splits on preactivations with low "uncertainty level" (probability mass between their linear bounds) to accelerate convergence by minimizing bound looseness in child branches. Uncertainty level q(B; y^(k)_j) = P(ȳ^(k)_j(X) ≥ 0, y^(k)_j(X) < 0) measures the probability mass in the ambiguous region between lower and upper linear bounds of a preactivation. Lower uncertainty implies tighter bounds after splitting. BaB-prob-ordered achieves zero uncertainty by splitting only when no earlier-layer unstable preactivations exist, explaining its effectiveness on MLPs. For CNNs with many neurons per layer, BaB+BaBSR-prob combines this with BaBSR heuristics to reduce split count by 5-125x.

## Foundational Learning

- **Concept: Branch-and-Bound with Linear Relaxations**
  - Why needed: BaB-prob's core loop—branching on preactivations and bounding via linear propagation—assumes familiarity with how B&B converts NP-hard problems into finite searches through progressive refinement
  - Quick check question: Given a minimization problem with initial bounds [L, U], after splitting into two subproblems with bounds [L₁, U₁] and [L₂, U₂], what condition on L₁, L₂ allows pruning?

- **Concept: Linear Bound Propagation (LiRPA/CROWN)**
  - Why needed: The paper assumes readers understand how to propagate linear bounds through network layers using precomputed interval bounds, and why this yields conservative (sound but incomplete) relaxations
  - Quick check question: For ReLU with input bounds [ℓ, u] where ℓ < 0 < u, what are the optimal linear upper and lower bounding functions?

- **Concept: Probability of Linear Constraint Satisfaction**
  - Why needed: Computing P(AX + b ≤ 0) for random X requires understanding how linear transformations affect distribution shape; Gaussian invariance under affine transforms is key to tractability
  - Quick check question: If X ~ N(μ, Σ), what is the distribution of a^T X + b, and how would you compute P(a^T X + b > 0)?

## Architecture Onboarding

- **Component map:** ComputeLinearBounds(f, D, C) -> BoundBranchProbability(P, f, f̄, y, ȳ, C) -> MarkPreactivationToSplitOn(B) -> Branch pool B
- **Critical path:** Initialize root branch with unconstrained linear bounds → compute p_ℓ,0, p_u,0 → Loop: aggregate global P_ℓ, P_u; check certification; pop largest-gap branch; split marked preactivation; compute child bounds; insert children → Terminate when P_ℓ ≥ η (TRUE), P_u < η (FALSE), or timeout
- **Design tradeoffs:** BaB-prob-ordered vs BaB+BaBSR-prob: Ordered is simpler and faster on MLPs (zero uncertainty guaranteed); BaBSR-prob scales better on CNNs with many neurons per layer (trades heuristic quality for neuron count); Batch size vs memory: Larger batches improve GPU utilization but increase memory (paper uses 4-8 typically); Monte Carlo samples vs confidence: N=10^5 yields >1-10^-4 confidence in 99.5%+ cases
- **Failure signatures:** Timeouts on deep/wide networks: Split count grows exponentially with unstable preactivations; Low confidence warnings: When empirical bounds barely cross threshold, may need more samples; Memory exhaustion: Branch pool grows with each split until certification
- **First 3 experiments:** 1) Toy soundness/completeness test: Replicate Table 3 on 5-dim MLP and 1×4×4 CNN with known ground truth; 2) Scalability sweep on untrained models: Vary D_i ∈ {16, 256, 1024} for MLPs and (W_i, C_h) ∈ {(16, 16), (32, 32), (48, 64)} for CNNs; 3) Split efficiency comparison: Run both variants on MNIST MLP-256-6 and CIFAR Conv-32-2; log split counts and verify ordered outperforms on MLP while BaBSR-prob outperforms on CNN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the soundness and completeness guarantees of BaB-prob be extended to neural networks with non-piecewise-linear activation functions (e.g., Sigmoid, Tanh)?
- Basis in paper: [explicit] The paper states, "we only prove completeness for ReLU activation function... The proof can be extended to general piecewise-linear activation functions," implying non-PL functions are currently unproven.
- Why unresolved: The current theoretical results rely on the ability to split preactivations to eliminate local non-linearity, a property inherent to ReLU/PL functions but not smooth activations.
- What evidence would resolve it: A formal proof of convergence for smooth activations or an experimental validation demonstrating verification reliability on networks with Sigmoid/Tanh layers.

### Open Question 2
- Question: Why does the BaB-prob-ordered strategy outperform BaB+BaBSR-prob on MLPs, while the reverse is true for CNNs?
- Basis in paper: [explicit] Section 4.1 and 4.2 observe that BaB-prob-ordered shows better scalability on MLPs, whereas BaB+BaBSR-prob scales better on CNNs.
- Why unresolved: The authors note the difference but do not fully analyze the structural reasons why the "uncertainty level" heuristic is more effective for dense layers compared to the BaBSR heuristics combined with uncertainty levels in convolutional layers.
- What evidence would resolve it: An ablation study or theoretical analysis linking the spatial dimensions of CNNs to the effectiveness of specific splitting heuristics.

### Open Question 3
- Question: Can BaB-prob be modified to remain competitive with input-space splitting methods on low-dimensional input problems?
- Basis in paper: [inferred] Table 2 shows that the baseline method PV outperforms BaB-prob on the low-dimensional `acasxu` (5D) and `linearizenn` (4D) benchmarks.
- Why unresolved: While the paper argues preactivation splitting is superior for high-dimensional inputs, the overhead or branching logic in BaB-prob appears to be less efficient than input splitting when the input space is small.
- What evidence would resolve it: A hybrid approach that switches strategies based on input dimension, or an analysis of the computational overhead in BaB-prob for small-scale verification.

## Limitations
- Theoretical guarantees apply only to feedforward-ReLU networks, with extensions to piecewise-linear activations mentioned but not formally proven
- The uncertainty level heuristic for preactivation splitting lacks theoretical guarantees of correlation with actual bound tightness improvement
- For general input distributions beyond Gaussian, the paper relies on Monte Carlo sampling without providing error bounds for finite sample sizes
- The algorithm's scalability remains exponential in the number of unstable preactivations, potentially limiting applicability to very deep or wide networks

## Confidence

- **Soundness and Completeness (High):** Proven for feedforward-ReLU networks with finite termination (Proposition 5, Corollary 1). Theoretical foundations are rigorous and well-established.
- **Superior Performance (High):** Consistently demonstrated across multiple benchmark suites (VNN-COMP 2025, MNIST/CIFAR-10, untrained models) with statistical significance. Success rates and time improvements are substantial and reproducible.
- **Preactivation Splitting Mechanism (Medium):** Sound in theory but practical effectiveness depends on heuristic quality. The zero-uncertainty splitting strategy works well empirically but lacks theoretical guarantees for all network architectures.
- **Scalability Claims (Medium):** Strong empirical evidence for medium-to-high dimensions, but theoretical analysis of complexity growth with network size is limited. Performance on extremely deep/wide networks remains to be fully characterized.

## Next Checks

1. **Complexity Scaling Experiment:** Systematically vary network depth and width (e.g., MLPs with 2-10 hidden layers, widths 32-512; CNNs with 2-6 Conv layers, channels 16-128) and measure split count and runtime growth. Verify whether observed exponential scaling matches theoretical expectations and identify architecture regimes where performance degrades.

2. **Non-Gaussian Input Distribution Test:** Replace Gaussian input with other distributions (e.g., uniform, Laplace) and evaluate whether BaB-prob maintains its performance advantage. Measure Monte Carlo sample requirements for equivalent confidence levels and assess impact on overall verification time.

3. **Uncertainty Level Heuristic Analysis:** Conduct ablation studies where preactivations are split in random order versus uncertainty-guided order. Quantify the actual correlation between uncertainty level and bound tightness improvement across different network architectures and input dimensions. Identify cases where the heuristic fails or provides minimal benefit.