---
ver: rpa2
title: 'MedBLIP: Fine-tuning BLIP for Medical Image Captioning'
arxiv_id: '2505.14726'
source_url: https://arxiv.org/abs/2505.14726
tags:
- blip
- image
- fine-tuning
- medical
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning the BLIP model on the ROCO medical imaging dataset
  significantly improved caption quality for radiology images compared to zero-shot
  baselines. The fine-tuned BLIP outperformed other vision-language models including
  BLIP-2, BLIP-2 Instruct, Gemini 1.5 Flash, and ViT-GPT2 across standard captioning
  metrics such as CIDEr, BERTScore, and SPICE.
---

# MedBLIP: Fine-tuning BLIP for Medical Image Captioning

## Quick Facts
- **arXiv ID**: 2505.14726
- **Source URL**: https://arxiv.org/abs/2505.14726
- **Reference count**: 16
- **Primary result**: Fine-tuning BLIP on ROCO medical imaging dataset significantly improves caption quality over zero-shot baselines and outperforms other VLMs

## Executive Summary
This paper demonstrates that fine-tuning the pre-trained BLIP model on domain-specific medical imaging data substantially improves radiology image captioning performance. The fine-tuned model outperforms zero-shot baselines and other vision-language models including BLIP-2, BLIP-2 Instruct, Gemini 1.5 Flash, and ViT-GPT2 across standard captioning metrics. While full fine-tuning yields the best results, decoder-only fine-tuning achieves competitive performance with 5% less training time. However, qualitative analysis reveals that the model still occasionally hallucinates findings or misidentifies clinical details, highlighting the need for clinical validation before deployment.

## Method Summary
The method involves fine-tuning the BLIP model (ViT-B/16 encoder + BERT decoder) on the ROCO medical imaging dataset using cross-entropy loss with teacher forcing. Three fine-tuning variants were explored: full fine-tuning (both encoder and decoder), decoder-only fine-tuning (encoder frozen), and encoder-only fine-tuning (decoder frozen). Training used learning rate 5×10⁻⁵, AdamW optimizer, gradient accumulation of 4 steps, mixed precision, and early stopping. Evaluation included standard captioning metrics (CIDEr, BERTScore, SPICE) and cosine similarity with Bio-ClinicalBERT embeddings, plus clinical correctness analysis and cross-attention visualization to assess visual grounding.

## Key Results
- Fine-tuned BLIP achieved CIDEr of 0.0917 and SPICE of 0.0409, significantly outperforming the zero-shot baseline (CIDEr: 0.0294, SPICE: 0.0171)
- The model outperformed other VLMs including BLIP-2, BLIP-2 Instruct, Gemini 1.5 Flash, and ViT-GPT2 across all metrics
- Decoder-only fine-tuning achieved competitive results (CosSim: 0.8863, BERTScore F1: 0.5132) with 5% less training time compared to full fine-tuning
- Cross-attention visualization showed improved visual grounding in fine-tuned models, but improved spatial grounding did not guarantee clinical correctness

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning updates model weights to recognize radiology-specific visual patterns and medical terminology underrepresented in natural image pre-training. The vision encoder adapts to medical image statistics while the decoder learns clinical vocabulary and report-style syntax. Core assumption: The pre-trained BLIP model retains sufficient representational plasticity to accommodate domain shift without catastrophic forgetting. Evidence: CIDEr improved from 0.0294 to 0.0917 after fine-tuning.

### Mechanism 2
Decoder-only fine-tuning provides strong efficiency-performance trade-off by freezing the vision encoder while updating only the text decoder. Since the decoder controls vocabulary selection and syntactic patterns, much of the domain adaptation occurs through language-side updates. Core assumption: The pre-trained ViT encoder already captures transferable visual representations that apply sufficiently to radiology images. Evidence: Decoder-only achieves 0.8863 CosSim and 0.5132 BERTScore F1 vs. 0.8943/0.5221 for full fine-tuning.

### Mechanism 3
Cross-attention visualization reveals token-to-image alignment patterns but improved spatial grounding does not guarantee clinical correctness. Decoder cross-attention weights map generated tokens to image regions, showing which visual features influence specific word predictions. Fine-tuning tends to localize attention to anatomically relevant regions. Core assumption: Attention weights meaningfully reflect the visual evidence used for token generation. Evidence: Fine-tuned models exhibit more localized attention over key anatomical regions, but some versions focus on correct regions yet describe unilateral effusions or overstate severity.

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architecture**
  - Why needed here: BLIP combines a ViT encoder with a transformer decoder via cross-attention; understanding this separation is essential for interpreting the ablation study and choosing fine-tuning strategies.
  - Quick check question: In BLIP, which component generates the caption tokens—the encoder, the decoder, or both jointly?

- **Concept: Cross-Attention in Transformers**
  - Why needed here: The attention visualization analysis depends on understanding how decoder tokens query encoder representations to attend to image regions.
  - Quick check question: What does the cross-attention mechanism connect—the decoder's hidden states to which other representations?

- **Concept: Domain Shift in Medical Imaging**
  - Why needed here: The core motivation is that natural image pre-training does not transfer well to radiology; recognizing this gap clarifies why fine-tuning is necessary.
  - Quick check question: Name two characteristics of radiology images that differ from natural images like MS-COCO.

## Architecture Onboarding

- **Component map**: Image (384×384) -> ViT-B/16 Vision Encoder -> BERT-based Text Decoder (with cross-attention) -> Caption tokens
- **Critical path**: 1) Image preprocessing: Resize to 384×384, normalize; 2) Vision encoding: ViT produces image embeddings; 3) Text decoding: BERT decoder generates tokens using cross-attention to image embeddings; 4) Inference: Beam search (beam=4, max_length=128) for caption generation
- **Design tradeoffs**: Full vs. decoder-only fine-tuning: ~5% training time savings vs. highest metric scores; Encoder-only fine-tuning: Poorer performance—language adaptation appears more critical than visual adaptation for this task; Metric selection: CIDEr/SPICE capture n-gram and semantic overlap but do not assess clinical correctness
- **Failure signatures**: Hallucinated findings (e.g., "pleural effusion" when GT shows "reticulonodular shadowing"); Laterality errors (incorrect left/right identification); Generic outputs (non-specific descriptions lacking anatomical precision); Attention-correctness mismatch (localized attention to relevant regions without accurate predictions)
- **First 3 experiments**: 1) Establish zero-shot baseline: Evaluate pre-trained BLIP on ROCO validation split; record CIDEr, BERTScore, SPICE to quantify domain gap; 2) Decoder-only fine-tuning: Freeze ViT encoder, train decoder on ROCO for 1–3 epochs with early stopping; compare metrics and training time to baseline; 3) Full model fine-tuning: Train both encoder and decoder with identical hyperparameters (lr=5e-5, AdamW, mixed precision); assess whether performance gains justify additional compute

## Open Questions the Paper Calls Out

### Open Question 1
How can hallucinated clinical findings in fine-tuned medical VLMs be systematically reduced or detected at inference time? The authors note that fine-tuned BLIP often introduces clinically incorrect details and emphasize the need for clinical validation, but do not propose mitigation strategies beyond qualitative clinical review.

### Open Question 2
Why does improved attention localization in fine-tuned models not guarantee clinically correct caption content? The authors observe that while fine-tuned models attend more precisely, they can still hallucinate findings not present in the ground truth, leaving the representational gap unexplained.

### Open Question 3
What evaluation metrics or protocols better correlate with clinical correctness than standard captioning metrics (CIDEr, BERTScore, SPICE)? The authors state that these metrics do not fully reflect the clinical reliability of generated outputs, but do not propose or validate an automated metric that captures clinical accuracy.

### Open Question 4
Can incorporating structured medical knowledge (e.g., ontologies, clinical guidelines) into BLIP's architecture reduce hallucinations and improve factual grounding? The conclusion states that incorporating structured medical knowledge and safety constraints will be essential for reliable deployment in clinical settings, but the paper does not implement or test any knowledge-enhanced architectures.

## Limitations
- Standard captioning metrics (CIDEr, BERTScore, SPICE) do not directly assess clinical correctness or diagnostic accuracy, potentially masking clinically significant errors
- The qualitative analysis reveals frequent hallucination of findings and laterality errors that metric scores alone would miss
- The attention visualization analysis is descriptive rather than diagnostic, not establishing whether improved visual grounding reduces hallucination rates or improves clinical utility

## Confidence
**High Confidence**: The quantitative performance improvements from fine-tuning BLIP on ROCO are well-supported by metric comparisons (CIDEr from 0.0294 to 0.0917, SPICE from 0.0171 to 0.0409).

**Medium Confidence**: The decoder-only fine-tuning efficiency claims (5% time savings with competitive performance) are plausible but could benefit from more rigorous benchmarking.

**Low Confidence**: The clinical utility and diagnostic reliability of the fine-tuned captions remain uncertain due to the systematic hallucination and laterality errors identified in qualitative analysis.

## Next Checks
1. **Clinical Error Classification Study**: Conduct systematic error analysis categorizing hallucinations, laterality errors, modality misidentifications, and their clinical severity to determine whether fine-tuning reduces clinically significant errors despite improving metric scores.

2. **Generalization Cross-Modality Test**: Evaluate fine-tuned models on radiology images from different datasets or modalities not represented in ROCO (e.g., mammography, nuclear medicine) to assess domain generalization and identify whether encoder adaptation becomes necessary for certain imaging types.

3. **Attention-Correctness Correlation Analysis**: Quantify the relationship between cross-attention localization quality and clinical correctness by annotating attention maps for accuracy and measuring correlation with hallucination rates, establishing whether better visual grounding actually improves clinical reliability.