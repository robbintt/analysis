---
ver: rpa2
title: 'Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?'
arxiv_id: '2508.05464'
source_url: https://arxiv.org/abs/2508.05464
tags:
- evaluation
- capabilities
- benchmarks
- benchmark
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Bench-2-CoP, a systematic framework that\
  \ quantifies the alignment between AI evaluation benchmarks and the EU AI Act\u2019\
  s Code of Practice. Using a validated LLM-as-judge approach, it analyzes 194,955\
  \ questions across six major benchmarks against the Act\u2019s taxonomy of 13 capabilities\
  \ and 9 propensities."
---

# Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?

## Quick Facts
- arXiv ID: 2508.05464
- Source URL: https://arxiv.org/abs/2508.05464
- Reference count: 40
- Primary result: Existing AI benchmarks cover <1% of systemic risk categories required by the EU AI Act, making them insufficient for regulatory compliance.

## Executive Summary
Bench-2-CoP is a systematic framework that quantifies the alignment between AI evaluation benchmarks and the EU AI Act's Code of Practice. Using a validated LLM-as-judge approach, it analyzes 194,955 questions across six major benchmarks against the Act's taxonomy of 13 capabilities and 9 propensities. The results reveal a severe misalignment: current benchmarks overwhelmingly focus on a few behavioral propensities (hallucination 61.6%, bias 17.2%) while neglecting critical capabilities—particularly those related to loss-of-control risks (autonomy, self-replication, evasion of oversight receive zero coverage). This demonstrates that existing public benchmarks are insufficient, on their own, for regulatory compliance.

## Method Summary
The framework uses a validated LLM-as-judge approach where Gemini-2.5-Flash classifies 194,955 questions from six major benchmarks against the EU AI Act Code of Practice taxonomy. The LLM is validated against a human-annotated gold standard (n=597) achieving Cohen's Kappa of 0.75 for capabilities and 0.82 for propensities. Questions are mapped to 13 capabilities (what models can do) and 9 propensities (what models tend to do), then aggregated to reveal coverage patterns and systemic risk pathways through expert mediation.

## Key Results
- 61.6% of regulatory-relevant questions focus on hallucination (P3) alone
- 17.2% focus on bias (P4), with other propensities receiving minimal attention
- Zero coverage for critical loss-of-control capabilities: autonomy (C4), self-replication (C9), evasion of oversight (C8)
- Three systemic risks (cyber offense, CBRN, loss of control) are almost entirely absent from evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large Language Models (LLMs) can function as reliable, scalable classifiers for mapping unstructured benchmark questions to regulatory taxonomies, provided they are validated against a human-annotated gold standard.
- **Mechanism:** The framework uses few-shot prompting to guide a high-performance LLM (Gemini-2.5-Flash) to classify questions against the EU AI Act Code of Practice (CoP) taxonomy. By measuring precision, recall, and Cohen's Kappa against a human-curated dataset (n=597), the system ensures the LLM's judgments meet "substantial" or "almost perfect" agreement thresholds before deployment on the full corpus.
- **Core assumption:** The semantic understanding of the validating LLM is sufficient to capture the nuances of regulatory definitions (capabilities vs. propensities) as well as a human expert.
- **Evidence anchors:** [abstract] "...uses validated LLM-as-judge analysis to map the coverage..."; [Section 3.2] "...Gemini-2.5-Flash demonstrated superior performance... Kappa of 0.75 [for capabilities] and... 0.82 [for propensities]."
- **Break condition:** If the inter-rater reliability (Kappa) drops below 0.6 (substantial agreement) for new taxonomies or different languages, the mechanism fails and requires re-validation or fine-tuning.

### Mechanism 2
- **Claim:** Quantitative coverage analysis exposes structural blind spots in the evaluation ecosystem by revealing concentration biases rather than just performance deficits.
- **Mechanism:** By aggregating 194,955 questions into a unified dataset and normalizing their distribution across 13 capabilities and 9 propensities, the framework highlights "zero coverage" zones. It distinguishes between "incidental" testing (e.g., hallucination measured via wrong answers in MMLU) and "intentional" safety design.
- **Core assumption:** The volume of questions dedicated to a specific risk category serves as a proxy for the evaluation ecosystem's prioritization of that risk.
- **Evidence anchors:** [Section 4.1] "...benchmarks devote 61.6% of their regulatory-relevant questions to 'Tendency to hallucinate'..."; [Section 4.2] "...capabilities central to loss-of-control scenarios... receive zero coverage in the entire benchmark corpus."
- **Break condition:** If benchmarks were to dynamically adapt their question sets based on model responses (adaptive testing), static corpus analysis would become outdated and fail to represent the active evaluation state.

### Mechanism 3
- **Claim:** Systemic risk assessment requires manual expert mediation to construct "risk pathways" that link low-level model attributes to high-level regulatory harms.
- **Mechanism:** Since the EU AI Act defines high-level risks (e.g., "Loss of Control") but does not explicitly map them to technical capabilities (e.g., C8, C9, C10), the study employs a structured expert protocol to define these connections. This allows for the aggregation of scattered metrics into meaningful risk scores.
- **Core assumption:** The specific combinations of capabilities and propensities identified by the expert panel are the necessary precursors for the systemic risks defined by law.
- **Evidence anchors:** [Section 4.5] "...our research team undertook a manual expert mapping... to identify how specific combinations... could plausibly interact..."; [Table 9] Shows the mapping of "Loss of Control" to specific low-level codes like C4 (Autonomy) and C8 (Evade oversight).
- **Break condition:** If regulators issue updated guidance that explicitly formalizes these mappings, the expert-mediated mechanism becomes redundant or potentially conflicting.

## Foundational Learning

- **Concept: EU AI Act Taxonomy (Capabilities vs. Propensities)**
  - **Why needed here:** The core of the Bench-2-CoP framework relies on distinguishing between what a model *can* do (Capabilities, C1-C13) and what it *tends* to do (Propensities, P1-P9). Without this distinction, one cannot interpret the coverage heatmaps.
  - **Quick check question:** Is "Hallucination" a capability or a propensity? (Answer: Propensity P3).

- **Concept: LLM-as-Judge Validation (Cohen's Kappa)**
  - **Why needed here:** The study relies on an LLM to annotate 194,955 questions. Understanding that the LLM was chosen based on a statistical measure of agreement with humans (Kappa) is critical to trusting the results.
  - **Quick check question:** What does a Kappa score of 0.82 signify in this context? (Answer: "Almost perfect" agreement with human annotators).

- **Concept: "Benchmark-Regulation Gap"**
  - **Why needed here:** This is the central problem the paper addresses. It is the misalignment between what industry benchmarks measure (performance/reliability) and what the EU AI Act requires (systemic risk prevention).
  - **Quick check question:** Why are high scores on MMLU insufficient for EU compliance? (Answer: They measure knowledge, not systemic risks like self-replication or oversight evasion).

## Architecture Onboarding

- **Component map:** Data Layer (194,955 questions) -> Validation Module (597 gold samples -> Human Annotation -> LLM Comparison) -> Execution Engine (Gemini-2.5-Flash + Prompt Hydration) -> Analysis Layer (Aggregation -> Normalization -> Risk Pathway Mapping)

- **Critical path:**
  1. **Gold Standard Creation:** If the human annotation is flawed or ambiguous, the validator selection fails.
  2. **Prompt Engineering:** The prompt must strictly enforce JSON output to enable parsing at scale.
  3. **Risk Mapping:** The expert definition of "Risk Pathways" is the lens through which raw data becomes regulatory insight.

- **Design tradeoffs:**
  - **Scale vs. Precision:** The paper uses an LLM instead of humans for the full 194k corpus to ensure feasibility. This accepts a potential ~15-20% classification error rate (inferred from precision/recall) to gain complete corpus coverage.
  - **Breadth vs. Depth:** The study analyzes 6 major benchmarks rather than every existing benchmark to focus on "industry-standard" tools, potentially missing niche safety benchmarks.

- **Failure signatures:**
  - **The "Incidental" Trap:** A high volume of questions tagged for a capability (e.g., Multimodality C11) might be an artifact of text-based descriptions of images rather than genuine multimodal processing (Section 4.2).
  - **JSON Parsing Errors:** If the LLM hallucinates text outside the JSON structure, the pipeline breaks.
  - **Over-reliance on Single Benchmarks:** P4 (Bias) coverage is heavily skewed by the BBQ benchmark; if BBQ has blind spots, the entire ecosystem's assessment of bias is skewed.

- **First 3 experiments:**
  1. **Replicate the Validator Test:** Run the 597 gold-standard questions against a different model (e.g., Llama-3) to see if the "LLM-as-Judge" capability is unique to frontier models.
  2. **"CoP-Bench" Prototype:** Create 10 questions for the "Zero Coverage" category C8 (Evade Oversight) and test if current models can even attempt them, validating the "evaluability" of these risks.
  3. **Stability Check:** Re-run the classification on the MMLU subset with shuffled answer choices to verify if the LLM evaluator is sensitive to answer position or context.

## Open Questions the Paper Calls Out

- **Question:** Can a comprehensive "CoP-Bench" achieve balanced coverage across all 13 capabilities and 9 propensities defined in the EU AI Act Code of Practice?
  - **Basis in paper:** [explicit] Section 5.4 proposes "a single, comprehensive benchmark with balanced focus... designed from the ground up using the EU AI Act's taxonomy... providing balanced, intentional coverage across all categories."
  - **Why unresolved:** The paper identifies extreme imbalance (61.6% on hallucination alone) but does not design or test a new benchmark to address it.
  - **What evidence would resolve it:** Construction and evaluation of a benchmark showing proportional coverage across all CoP categories.

- **Question:** Would expanding the analysis to include confidential external evaluations (e.g., by METR or Apollo) meaningfully reduce the zero-coverage gaps for loss-of-control capabilities (C8, C9, C10)?
  - **Basis in paper:** [inferred] Section 5.2 acknowledges private evaluations exist but were excluded; Section 5.1 notes zero coverage for evasion, self-replication, and AI R&D.
  - **Why unresolved:** The study scope was limited to publicly disclosed benchmarks, leaving the gap between public and private evaluation practices unquantified.
  - **What evidence would resolve it:** Application of Bench-2-CoP framework to private evaluation corpora, or public disclosure of those results.

- **Question:** Do interactive, sandbox-based evaluation environments yield different assessments of autonomous and agentic behaviors compared to static question-answering benchmarks?
  - **Basis in paper:** [explicit] Section 5.4 recommends "interactive, dynamic testing environments—such as sophisticated sandboxes or simulations—that can properly assess emergent and autonomous behaviors."
  - **Why unresolved:** The paper critiques static benchmarks but does not compare them against dynamic methods empirically.
  - **What evidence would resolve it:** Comparative study applying both static benchmarks and sandbox environments to the same model for C4 (Autonomy), C8 (Evasion), and P7 (Power-seeking).

## Limitations
- Expert Panel Composition: Manual risk pathway mapping relies on single research team's judgment without disclosure of expertise or potential conflicts
- Static Analysis Limitation: Framework analyzes snapshot of benchmark questions, not accounting for benchmarks that may evolve over time
- LLM-as-Judge Generalization: Assumes chosen LLM's understanding generalizes across all 194,955 questions without analyzing potential drift

## Confidence
- **High Confidence:** Quantitative coverage analysis (61.6% focus on hallucination, zero coverage for autonomy/evasion) is directly computable from classification results
- **Medium Confidence:** Conclusion that benchmarks are "insufficient for regulatory compliance" follows logically from coverage gap but assumes CoP requirements are comprehensive
- **Medium Confidence:** Expert-mediated risk pathway mapping is methodologically sound but represents single team's interpretation

## Next Checks
1. Replicate risk pathway mapping with second independent expert panel to assess inter-rater reliability
2. Track one studied benchmark (e.g., MMLU) over 6-12 months to see if new question types addressing zero-coverage capabilities emerge
3. Re-run classification pipeline using different high-performing LLM (e.g., Claude-3.5-Sonnet) on stratified subset to verify consistency