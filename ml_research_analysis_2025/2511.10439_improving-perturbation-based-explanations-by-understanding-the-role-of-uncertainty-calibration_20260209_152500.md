---
ver: rpa2
title: Improving Perturbation-based Explanations by Understanding the Role of Uncertainty
  Calibration
arxiv_id: '2511.10439'
source_url: https://arxiv.org/abs/2511.10439
tags:
- calibration
- should
- explanations
- perturbation
- recalx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between uncertainty calibration
  and perturbation-based explanations in machine learning. The authors show that models
  often produce unreliable probability estimates under perturbations commonly used
  in explainability methods, which directly undermines the quality of both global
  and local explanations.
---

# Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration

## Quick Facts
- arXiv ID: 2511.10439
- Source URL: https://arxiv.org/abs/2511.10439
- Authors: Thomas Decker; Volker Tresp; Florian Buettner
- Reference count: 40
- Models produce unreliable probability estimates under explainability perturbations, degrading explanation quality

## Executive Summary
This paper addresses a fundamental problem in explainable AI: perturbation-based explanation methods like SHAP and LIME rely on probability estimates from the base model, but these estimates become systematically unreliable under the very perturbations used to generate explanations. The authors introduce ReCalX, an adaptive calibration method that partitions perturbation levels into bins and learns separate temperature scaling parameters for each, preserving the model's discriminative information while improving calibration. Experiments across tabular and image datasets demonstrate substantial improvements in calibration error (up to 97% relative reduction) and downstream explanation quality, as validated through remove-and-retrain experiments and sensitivity analysis.

## Method Summary
ReCalX adapts temperature scaling to handle different perturbation intensities by partitioning the perturbation level range [0,1] into B equal-width bins. For each bin, a specific temperature parameter T_b is learned by minimizing cross-entropy loss on perturbed validation samples belonging to that bin. During inference, the appropriate temperature is selected based on the current perturbation subset's level. The method is evaluated on tabular datasets (Electricity, Covertype, Credit, Pol) and ImageNet using ResNet50, DenseNet121, ViT, and SigLIP architectures, with 200 validation samples and 10 perturbed instances per perturbation level bin.

## Key Results
- Miscalibration under perturbations directly degrades explanation quality by contaminating probability estimates used for feature importance computation
- ReCalX achieves relative improvements in maximum KL-divergence calibration error ranging from 6.9% to 97% across datasets
- Recalibrated models produce more robust explanations and better identify globally important features, validated through remove-and-retrain experiments
- Adaptive calibration outperforms standard temperature scaling by accounting for non-uniform miscalibration across perturbation intensities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Miscalibration under perturbations directly degrades explanation quality by contaminating the probability estimates used to compute feature importance.
- Mechanism: The paper proves that predictive power decomposes into three terms: baseline bias + mutual information − calibration error. The calibration error term directly subtracts from the idealized predictive power. For local explanations, Theorem 3.4 bounds the squared deviation between actual and ideal explanations by the maximum calibration error across perturbation subsets.
- Core assumption: Perturbation-based explanations aggregate probability outputs across many perturbed inputs; if these probabilities are systematically unreliable, the aggregated importance scores inherit this distortion.
- Evidence anchors:
  - [abstract] "We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality."
  - [section 3] Theorem 3.2 provides the decomposition showing calibration error as a direct subtractive term from predictive power.
- Break condition: If explanations used only rank-order comparisons rather than probability magnitudes, calibration might matter less.

### Mechanism 2
- Claim: Miscalibration varies non-uniformly across perturbation intensities, requiring adaptive rather than global calibration.
- Mechanism: ReCalX partitions perturbation levels into bins and learns separate temperature parameters for each bin. This accounts for the empirical finding that calibration error doesn't increase monotonically with perturbation strength.
- Core assumption: The perturbation level λ(S) = (d−|S|)/d captures the relevant degree of distribution shift that affects calibration.
- Evidence anchors:
  - [section 4] "To account for different perturbation intensities, we partition [0,1] into B equal-width bins and learn a specific temperature for each bin."
  - [section 5, Figure 3] Calibration error results show miscalibration varies significantly across perturbation severity.
- Break condition: If calibration error were uniform across perturbation levels, standard temperature scaling would suffice.

### Mechanism 3
- Claim: Componentwise strictly monotonic calibration maps preserve the mutual information between model predictions and targets, ensuring explanations remain faithful to the original model.
- Mechanism: Proposition 4.2 establishes that deterministic, componentwise strictly monotonic transformations preserve mutual information I(T(f(X)), Y) = I(f(X), Y). Temperature scaling satisfies this property.
- Core assumption: Mutual information I(f(X), Y) represents the relevant information content for explanation purposes.
- Evidence anchors:
  - [section 4] Definition 4.1 and Proposition 4.2 formally define and prove the information-preserving property.
  - [section 4] References [68] showing such calibrators are also accuracy-preserving.
- Break condition: If the explanation method relied on absolute probability values in ways that monotonicity doesn't preserve.

## Foundational Learning

- Concept: **KL-Divergence Calibration Error (CE_KL)**
  - Why needed here: The paper uses this proper scoring rule-based calibration metric, which is directly induced by cross-entropy loss.
  - Quick check question: Why might KL-divergence calibration error be preferred over Expected Calibration Error (ECE) for this analysis?

- Concept: **Mutual Information I(X; Y)**
  - Why needed here: The theoretical analysis positions mutual information as the "idealized predictive power" under perfect calibration.
  - Quick check question: What does I(f(X), Y) = 0 imply about the model's relationship to the target?

- Concept: **Temperature Scaling**
  - Why needed here: ReCalX builds on temperature scaling, a standard post-hoc calibration method.
  - Quick check question: What happens to a model's predictions as T → ∞? As T → 0?

## Architecture Onboarding

- Component map:
Input (x, perturbation subset S) -> Perturbation function π(x, S) -> Base model f -> Perturbation level λ(S) -> Bin selector -> T_b -> Scaled softmax -> Calibrated prediction

- Critical path:
  1. **Validation phase**: Collect perturbed samples across all perturbation levels from held-out validation data. For each bin b, optimize T_b by minimizing cross-entropy on samples with λ(S) in that bin.
  2. **Inference phase**: For each perturbation during explanation computation, compute λ(S), select appropriate T_b, apply temperature scaling before aggregating into explanation.
  3. **Explanation phase**: Use calibrated predictions with standard perturbation-based methods (SHAP, LIME, etc.).

- Design tradeoffs:
  - **Number of bins (B)**: More bins provide finer adaptation but require more validation data per bin.
  - **Validation set size**: Paper uses 200 samples × 10 perturbations per bin = 2000 samples/bin.
  - **Computational overhead**: Inference overhead is minimal, but validation phase requires generating perturbed samples.

- Failure signatures:
  - **Insufficient validation data per bin**: Temperatures become unreliable, potentially increasing calibration error.
  - **Perturbation type mismatch**: If validation uses different perturbation strategies than explanation, learned temperatures may not transfer.
  - **Non-monotonic transformations**: Using calibration methods that aren't componentwise strictly monotonic breaks information preservation.

- First 3 experiments:
  1. **Establish baseline miscalibration**: Measure CE_KL across perturbation levels on your model/dataset without calibration.
  2. **Standard vs. adaptive calibration**: Compare single temperature scaling vs. ReCalX with B=10 bins on validation data.
  3. **Downstream explanation fidelity**: Run remove-and-retrain experiment—rank features by global importance from calibrated vs. uncalibrated explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the calibration principles established for perturbation-based explanations be effectively extended to gradient-based and counterfactual explanation methods?
- Basis in paper: [explicit] The conclusion states the underlying principles "may also have implications for other explanation families, such as gradient-based or counterfactual methods."
- Why unresolved: The theoretical derivations and empirical validation are restricted to perturbation-based techniques.
- What evidence would resolve it: Empirical validation applying perturbation-aware calibration to gradient-based attribution methods.

### Open Question 2
- Question: How does ReCalX perform in complex structured prediction tasks like object detection or semantic segmentation?
- Basis in paper: [inferred] The methodology is demonstrated primarily on classification, leaving higher-dimensional outputs unexplored.
- Why unresolved: Extending monotonicity and temperature scaling to high-dimensional structured outputs requires defining appropriate calibration metrics.
- What evidence would resolve it: Experiments applying adaptive temperature scaling to object detectors or segmenters.

### Open Question 3
- Question: Can a model recalibrated for one specific perturbation strategy (e.g., blurring) produce reliable explanations when a different strategy (e.g., mean imputation) is applied?
- Basis in paper: [inferred] The paper evaluates distinct strategies separately but does not analyze the transferability of the learned calibration parameters.
- Why unresolved: It is unknown if the adaptive temperatures learned for one perturbation set generalize to different perturbation operators.
- What evidence would resolve it: Cross-testing experiments where calibration learned via Strategy A is used to explain with Strategy B.

## Limitations
- Limited scope of perturbation types: primarily evaluates feature removal/blanking and additive Gaussian noise perturbations
- Theoretical assumptions: relies on componentwise strictly monotonic transformations that real-world methods may violate
- Empirical validation restricted to classification tasks, not extending to structured prediction

## Confidence
- **High Confidence**: The empirical demonstration that miscalibration varies non-uniformly across perturbation intensities and degrades explanation quality
- **Medium Confidence**: The theoretical framework connecting calibration error to explanation quality
- **Low Confidence**: The information-theoretic preservation claims for general calibration maps

## Next Checks
1. **Cross-perturbation generalization**: Validate ReCalX on non-additive perturbations (e.g., geometric transformations, adversarial examples) to assess whether the binning approach generalizes beyond evaluated perturbation types.

2. **Calibration stability analysis**: Perform sensitivity analysis on the number of validation samples per bin and the number of bins to quantify how these hyperparameters affect the stability and generalization of learned temperatures.

3. **Alternative calibration methods comparison**: Compare ReCalX against other adaptive calibration approaches (e.g., ensemble methods, quantile-based calibration) to determine whether the temperature scaling approach is optimal.