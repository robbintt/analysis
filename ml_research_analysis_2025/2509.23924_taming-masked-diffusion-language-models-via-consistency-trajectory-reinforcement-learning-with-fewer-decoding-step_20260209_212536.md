---
ver: rpa2
title: Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement
  Learning with Fewer Decoding Step
arxiv_id: '2509.23924'
source_url: https://arxiv.org/abs/2509.23924
tags:
- decoding
- steps
- diffusion
- arxiv
- denoising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses limitations in masked diffusion language
  models (MDLMs), particularly the performance gap between full diffusion-style and
  semi-autoregressive decoding, and training-inference inconsistencies when applying
  reinforcement learning. The authors propose three key contributions: (1) EOS Early
  Rejection (EOSER) decoding, which suppresses early <EOS token generation to avoid
  the "<EOS Trap" in full diffusion decoding; (2) Ascending Step-Size (ASS) decoding
  scheduler, which reduces the number of inference steps from L/2 to log2 L by adapting
  step sizes based on token confidence trends; and (3) Consistency Trajectory Group
  Relative Policy Optimization (CJ-GRPO), which aligns rollout and optimization trajectories
  to reduce skip-step optimization errors.'
---

# Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step

## Quick Facts
- **arXiv ID**: 2509.23924
- **Source URL**: https://arxiv.org/abs/2509.23924
- **Reference count**: 40
- **Primary result**: CJ-GRPO with EOSER and ASS achieves competitive performance with significantly fewer decoding steps (log₂L vs L/2) on math and planning tasks

## Executive Summary
This paper addresses limitations in masked diffusion language models (MDLMs), particularly the performance gap between full diffusion-style and semi-autoregressive decoding, and training-inference inconsistencies when applying reinforcement learning. The authors propose three key contributions: (1) EOS Early Rejection (EOSER) decoding, which suppresses early <EOS> token generation to avoid the "<EOS> Trap" in full diffusion decoding; (2) Ascending Step-Size (ASS) decoding scheduler, which reduces the number of inference steps from L/2 to log₂L by adapting step sizes based on token confidence trends; and (3) Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO), which aligns rollout and optimization trajectories to reduce skip-step optimization errors. Experiments on mathematical and planning tasks using LLaDA-8B-Instruct show that CJ-GRPO with EOSER and ASS achieves competitive performance with significantly fewer decoding steps compared to existing methods.

## Method Summary
The authors address MDLM limitations through three innovations: EOS Early Rejection (EOSER) applies time-varying attenuation to <EOS> token confidence to prevent premature sequence termination; Ascending Step-Size (ASS) scheduler decodes tokens in power-of-2 increments per step, reducing inference steps from L/2 to log₂L; and Consistency Trajectory GRPO (CJ-GRPO) stores intermediate states during rollout and computes trajectory-aware losses to align training and inference. The method is evaluated on GSM8K, MATH500 (math), Sudoku, and Countdown (planning) using LLaDA-8B-Instruct, showing competitive performance with reduced decoding steps.

## Key Results
- EOSER suppresses early <EOS> generation, avoiding the "<EOS> Trap" in full diffusion decoding (Fig. 4 shows delayed spatial/temporal arrival)
- ASS reduces inference steps from L/2 to log₂L (Table 2: 7-8 steps vs 32-128 uniform steps) while maintaining performance
- CJ-GRPO with trajectory consistency achieves 58.20 accuracy on Countdown vs 35.16/45.31 for one-step approximations (Table 3)

## Why This Works (Mechanism)

### Mechanism 1: EOS Early Rejection (EOSER)
- Claim: Suppressing early `<EOS>` token generation improves full diffusion-style decoding performance in MDLMs.
- Mechanism: Applies a time-varying attenuation coefficient γ to `<EOS>` confidence at each denoising step. γ starts low (γmin ∈ [0.4, 0.6] for uniform, 0.01 for ASS) and linearly increases to 1.0, preventing premature sequence termination when token confidence is low.
- Core assumption: The `<EOS> Trap` arises from training data bias (LLaDA replaces `<PAD>` with `<EOS>`), causing disproportionately high `<EOS>` confidence during early low-confidence denoising phases.
- Evidence anchors:
  - [abstract] "...EOS Early Rejection (EOSER) decoding, which suppresses early <EOS> token generation to avoid the '<EOS> Trap' in full diffusion decoding"
  - [Section 3.2] Fig. 1 shows average `<EOS>` confidence significantly exceeds `<non-EOS>` tokens in early steps; EOSER delays `<EOS>` arrival spatially and temporally (Fig. 4)
  - [corpus] Limited direct corroboration; neighbor papers (e.g., "Parallelism and Generation Order in MDLMs") discuss generation order challenges but not EOS-specific mechanisms
- Break condition: If `<EOS>` tokens are not over-represented in training data or model exhibits balanced early-step confidence, attenuation provides no benefit and may delay legitimate termination.

### Mechanism 2: Ascending Step-Size (ASS) Scheduler
- Claim: Adapting token decode counts to confidence trends reduces inference steps from O(L) to O(log₂L) while maintaining performance.
- Mechanism: Decodes 2^s tokens at step s (power-of-2 schedule), with cumulative tokens reaching 2^(s+1)-1. Cautious early decoding (fewer tokens) matches low confidence; aggressive later decoding (more tokens) exploits sharp confidence increases.
- Core assumption: Token confidence follows a predictable low-to-high trajectory during denoising, as observed in Fig. 1(d).
- Evidence anchors:
  - [abstract] "...Ascending Step-Size (ASS) decoding scheduler, which reduces the number of inference steps from L/2 to log₂L by adapting step sizes based on token confidence trends"
  - [Section 3.3] Eq. 6-7 define ASS schedule; Table 2 shows EOSER+ASS achieving competitive performance at 7-8 steps vs. 32-128 uniform steps
  - [corpus] "dUltra" (arXiv:2512.21446) similarly targets fast MDLM sampling via RL, suggesting step reduction is an active research direction
- Break condition: If confidence evolution is task-dependent or irregular (e.g., non-monotonic), fixed power-of-2 scheduling may underfit or overcommit at wrong phases.

### Mechanism 3: Consistency Trajectory GRPO (CJ-GRPO)
- Claim: Aligning rollout and optimization trajectories reduces skip-step optimization errors in MDLM reinforcement learning.
- Mechanism: Stores intermediate states (token confidences πθ and positions pos) at each denoising step in queues. Computes trajectory-aware loss Lθ,s-1 across all steps, ensuring gradient updates reflect the actual parallel decoding path rather than synthetic one-step approximations.
- Core assumption: Non-causal (bidirectional attention) rollout in MDLMs creates distribution shift between masked intermediate states and final clean completions, violating GRPO's causality assumption.
- Evidence anchors:
  - [abstract] "...Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO), which aligns rollout and optimization trajectories to reduce skip-step optimization errors"
  - [Section 3.4] Fig. 3 contrasts AR causality guarantee with MDLM inconsistencies; Table 3 shows consistency trajectory [x0,...,xS] outperforming one-step shortcuts (58.20 vs. 35.16/45.31 on Countdown)
  - [corpus] "MDPO" (arXiv:2508.13148) similarly addresses training-inference divide in MDLMs via preference optimization, corroborating trajectory consistency as a systemic issue
- Break condition: Memory overhead scales with step count S; without ASS reducing steps to log₂L, storage becomes prohibitive for long sequences.

## Foundational Learning

- **Masked Diffusion Language Models (MDLMs)**
  - Why needed here: MDLMs differ fundamentally from autoregressive LLMs—they denoise partially masked sequences via bidirectional attention, enabling parallel decoding but violating causal assumptions embedded in standard RL algorithms.
  - Quick check question: Can you explain why GRPO, designed for sequential token prediction, fails when applied to models that predict all masked positions simultaneously?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the baseline RL algorithm being adapted. It uses group-relative advantages (comparing outputs within a group) rather than absolute rewards, reducing variance. Understanding its original AR formulation clarifies why trajectory consistency matters.
  - Quick check question: How does GRPO's advantage estimation differ from vanilla PPO, and why does non-causal decoding break its rollout-to-optimization mapping?

- **Training-Inference Mismatch in Diffusion Models**
  - Why needed here: The paper's core diagnosis is that MDLMs face unique inconsistencies—training uses clean targets, but inference operates on partially masked states. This motivates both EOSER (inference-time correction) and CJ-GRPO (training-time alignment).
  - Quick check question: If an MDLM is trained to predict clean tokens from 50% masked inputs, what happens when inference starts from 100% masks and decodes in 5 steps?

## Architecture Onboarding

- **Component map**:
  - Base Model (LLaDA-8B-Instruct) -> Denoising Step -> EOSER (attenuates <EOS> logits) -> ASS Scheduler (determines unmask count) -> Sample Tokens -> CJ-GRPO (stores intermediate states for RL training)

- **Critical path**:
  1. Rollout phase: Prompt + masked response → ASS determines unmask count → EOSER modifies logits → sample tokens → enqueue (confidence, position) per step
  2. Reward computation: Verify final completion against ground truth (math) or constraints (planning)
  3. Optimization phase: Dequeue trajectories → compute per-step importance ratios → aggregate trajectory loss → update policy

- **Design tradeoffs**:
  - **Memory vs. Step Count**: CJ-GRPO requires O(S·B) storage for intermediate states. ASS reduces S from O(L) to O(log₂L), making training feasible. Without ASS, long sequences (L=4096) are impractical.
  - **EOSER Strength (γmin)**: Lower γmin better suppresses early termination but risks over-suppression on tasks where early `<EOS>` is valid. Paper uses γmin=0.01 for ASS, 0.4-0.6 for uniform—tune per task.
  - **ASS vs. Semi-AR**: ASS+EOSER enables full parallelism but underperforms on sequential reasoning (math); Semi-AR retains block constraints that align with step-by-step logic. Hybrid approaches may be optimal.

- **Failure signatures**:
  - **Empty/Trivial Outputs**: If γmin too high or ASS decodes too aggressively early, model generates `<EOS>` immediately or produces incoherent short sequences.
  - **Memory OOM during CJ-GRPO**: Step count S too large without ASS; reduce generation length or enforce log₂L scheduling.
  - **Degraded Math Performance**: Full diffusion decoding (even with EOSER) underperforms Semi-AR on GSM8K/MATH500—this is expected per Section 4.4; sequential reasoning benefits from block constraints.

- **First 3 experiments**:
  1. **EOSER Ablation on Planning Task**: Run Countdown with full diffusion decoding, comparing (a) no EOSER, (b) EOSER with γmin=0.5, (c) EOSER with γmin=0.01. Expect (c) > (b) > (a); confirms `<EOS> Trap` mitigation.
  2. **ASS Step Reduction Validation**: For L=256, compare uniform S=128 steps vs. ASS S=8 steps (log₂256=8) on Sudoku. Measure accuracy and wall-clock time. Expect comparable accuracy with ~16× speedup.
  3. **CJ-GRPO Trajectory Consistency Check**: Train three variants on GSM8K—(a) diffu-GRPO (one-step x'S→xS), (b) one-step x0→xS, (c) full consistency trajectory. Compare Table 3 pattern; (c) should match or exceed (a)/(b). Monitor memory to validate O(log₂L) scaling with ASS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can diffusion-style (parallel) and autoregressive (sequential) reasoning be effectively hybridized within MDLMs to improve performance on mathematical tasks that exhibit sequential dependencies?
- Basis in paper: [explicit] The authors observe in Section 4.4 and Appendix B that their full diffusion method underperforms on math tasks compared to Semi-AR, concluding that "future work that hybridizes diffusion-style (parallel) and autoregressive (sequential) reasoning" is needed.
- Why unresolved: The current architectures and decoding strategies force a choice between the parallel processing suitable for planning tasks and the sequential processing required for math, lacking a mechanism to dynamically adapt to the reasoning pattern.
- What evidence would resolve it: A single model architecture or decoding strategy that matches Semi-AR performance on mathematical benchmarks while retaining the parallel decoding speed of EOSER on planning tasks.

### Open Question 2
- Question: Can the attenuation coefficient $\gamma$ in EOS Early Rejection (EOSER) be derived theoretically or adjusted dynamically rather than being set empirically?
- Basis in paper: [inferred] The methodology section (3.2) states that $\gamma_{min}$ is set empirically to specific values (0.4-0.6 for uniform, 0.01 for ASS), suggesting the current heuristic approach may not be optimal or theoretically grounded.
- Why unresolved: The "EOS Trap" severity may vary across different models or contexts; static empirical values might over-suppress or under-suppress termination signals in novel scenarios.
- What evidence would resolve it: An adaptive function for $\gamma$ based on denoising time $t$ or confidence scores that consistently outperforms static values across diverse datasets without manual hyperparameter tuning.

### Open Question 3
- Question: How can the memory overhead of Consistency Trajectory GRPO (CJ-GRPO) be reduced without sacrificing the ability to model long denoising trajectories?
- Basis in paper: [explicit] The Limitations section notes that storing intermediate states for trajectory consistency increases memory overhead proportionally with the number of steps ($S$), creating a bottleneck that the ASS scheduler only partially mitigates.
- Why unresolved: While the Ascending Step-Size (ASS) scheduler reduces space complexity to $O(\log L)$, it potentially degrades performance on tasks requiring finer-grained steps (like math), indicating a need for memory-efficient storage of full trajectories.
- What evidence would resolve it: A gradient checkpointing or compression technique that allows CJ-GRPO to train with standard step counts ($L/2$) on limited GPU memory while maintaining optimization stability.

## Limitations
- **Training-inference mismatch**: The "<EOS> Trap" and trajectory consistency issues are model-specific and may not generalize to all MDLM architectures or training paradigms.
- **Memory overhead**: CJ-GRPO's requirement to store intermediate states creates scalability challenges for long sequences, even with ASS step reduction.
- **Task-specific performance**: Full diffusion decoding with EOSER underperforms Semi-AR on sequential reasoning tasks like mathematics, indicating fundamental architectural limitations.

## Confidence

- **High Confidence**: The empirical demonstration that MDLMs suffer from "<EOS> Trap" during full diffusion decoding (Fig. 1, Fig. 4 showing EOSER effects)
- **Medium Confidence**: The ASS scheduler's ability to reduce inference steps from L/2 to log₂L while maintaining performance (Table 2 results, though hardware benchmarks missing)
- **Medium Confidence**: CJ-GRPO's trajectory consistency advantage over one-step approximations (Table 3 math results, though planning task shows smaller gains)
- **Low Confidence**: The claim that EOSER+ASS+CJ-GRPO represents the optimal configuration across all MDLM tasks (comparative analysis limited to specific benchmarks)

## Next Checks

1. **Task-Specific EOSER Tuning**: Run ablation studies across diverse tasks (code generation, summarization) to determine if fixed γmin=0.01 works universally or requires task-specific calibration. Plot <EOS> confidence trajectories with/without EOSER to verify suppression patterns.

2. **ASS Scheduler Robustness**: Test ASS scheduling on non-power-of-2 sequence lengths and with irregular confidence growth patterns. Compare against alternative step-scheduling approaches (linear, exponential) to isolate the benefit of the specific power-of-2 schedule.

3. **CJ-GRPO Memory Efficiency**: Profile memory usage during training with different sequence lengths (256, 1024, 4096) and group sizes (4, 6, 8). Quantify the actual memory savings from ASS step reduction and determine the practical upper bound for sequence length.