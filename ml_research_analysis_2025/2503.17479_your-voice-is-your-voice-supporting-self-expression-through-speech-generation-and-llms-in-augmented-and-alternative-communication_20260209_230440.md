---
ver: rpa2
title: 'Your voice is your voice: Supporting Self-expression through Speech Generation
  and LLMs in Augmented and Alternative Communication'
arxiv_id: '2503.17479'
source_url: https://arxiv.org/abs/2503.17479
tags:
- users
- communication
- expressivity
- voice
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Speak Ease is an augmentative and alternative communication (AAC)
  system that integrates multimodal input (text, voice, and contextual cues) with
  large language models (LLMs) to enhance user expressivity. The system combines automatic
  speech recognition (ASR), context-aware LLM-based outputs, and personalized text-to-speech
  (TTS) technologies to enable more personalized, natural-sounding, and expressive
  communication.
---

# Your voice is your voice: Supporting Self-expression through Speech Generation and LLMs in Augmented and Alternative Communication

## Quick Facts
- arXiv ID: 2503.17479
- Source URL: https://arxiv.org/abs/2503.17479
- Reference count: 40
- Speak Ease integrates multimodal input with LLMs and personalized TTS to enhance AAC user expressivity through context-aware, identity-preserving communication

## Executive Summary
Speak Ease is an augmentative and alternative communication (AAC) system that combines automatic speech recognition, large language models, and personalized text-to-speech technologies to enhance user expressivity. The system processes multimodal inputs (voice, text, emojis) alongside contextual cues like emotional intent and communication partner to generate personalized, natural-sounding responses. Through feasibility testing and SLP focus groups, Speak Ease demonstrated potential to improve AAC communication by supporting more personalized and contextually relevant interactions while maintaining user identity through voice cloning technology.

## Method Summary
Speak Ease processes user inputs through WhisperX for speech-to-text, contextual tagging for emotional intent and audience, and a custom GPT-4o instance for semantic expansion. The system combines these elements with Eleven Labs personalized TTS using voice banking samples stored in Firebase. The feasibility study involved 6 participants testing system usability, while a focus group of 5 SLPs evaluated the system's potential for enhancing AAC communication expressivity.

## Key Results
- Integration of multimodal input (text, voice, contextual cues) with LLMs enables context-aware, personalized communication
- Personalized TTS using voice cloning maintains user identity and improves perceived naturalness of synthetic speech
- SLP focus group identified potential for enhanced expressivity but noted need for balancing efficiency with expressive features

## Why This Works (Mechanism)

### Mechanism 1: Context-Driven Semantic Expansion
Providing explicit context (communication partner, emotional intent) allows an LLM to expand ambiguous or abbreviated input into socially appropriate, full sentences. The user inputs raw content (e.g., "pizza" or an emoji) alongside selected tags (e.g., "Sad," "Friend"). A custom GPT-4o instance ingests this information to predict pragmatics and generate multiple sentence options that align with the desired tone and audience.

### Mechanism 2: Multimodal Input Redundancy
Offering synchronous voice, touch, and keyboard inputs prevents communication breakdowns caused by fluctuating motor or speech abilities. The system creates a unified input buffer from three sources: ASR (WhisperX) for voice, GUI for emojis/touch, and native keyboard for text. If one modality fails, the user can immediately switch to or supplement with another without restarting the session.

### Mechanism 3: Identity Preservation via Voice Cloning
Personalized Text-to-Speech (TTS) maintains user agency and identity by replicating their unique vocal characteristics rather than using generic synthetic voices. Users pre-record voice samples (voice banking) stored in Firebase. The Eleven Labs TTS engine uses these embeddings to synthesize the LLM-generated text, ensuring the output sounds like the user's "natural" voice rather than a robot.

## Foundational Learning

- **Dysarthria and Disfluency**: Standard ASR models fail on atypical speech patterns. The system uses WhisperX, but engineers must understand that "accuracy" in this context is non-binary; the system must handle "garbled" text by passing it to an LLM for "interpretation" rather than literal transcription.
  - Quick check: How does the system handle an audio input that results in nonsensical text? (Answer: It relies on the LLM to infer meaning/context).

- **Facilitated Communication (FC) and Authorship**: When an LLM rewrites or "expands" user input, the boundary between user intent and AI generation blurs. This is an ethical risk (the "FC" problem).
  - Quick check: If an LLM changes "I want pizza" to "I am starving, let's get pizza," who is the author? How does this affect legal or sensitive communication?

- **Expressivity vs. Efficiency Trade-off**: AAC users typically communicate at 1-25 words per minute vs. 150-200 for speech. Adding layers for "emotion selection" or "voice banking" adds friction.
  - Quick check: In a high-stress, time-critical scenario, which features of Speak Ease would likely be bypassed by the user? (Answer: Emotion tagging/Context selection).

## Architecture Onboarding

- Component map: WhisperX (ASR + VAD) -> Text Buffer -> GPT-4o (Refine & Expand) -> User selects Sentence -> Eleven Labs (Synthesize with User Voice) -> Audio Output
- Critical path: User Input (Voice/Text) -> WhisperX (if voice) -> Text Buffer -> User selects Context/Emotion -> GPT-4o (Refine & Expand) -> User selects Sentence -> Eleven Labs (Synthesize with User Voice) -> Audio Output
- Design tradeoffs:
  - Latency vs. Nuance: Calling GPT-4o and Eleven Labs sequentially introduces significant latency, which risks conversational turn-taking failures.
  - Accessibility vs. Complexity: The "Savvy User" design assumption alienates users with cognitive impairments who may find the interface overwhelming.
  - Filtering vs. Autonomy: The system must decide whether to filter "inappropriate" content generated by the LLM, which restricts user agency.
- Failure signatures:
  - The "Ghost Writer" effect: The LLM hallucinates details not present in the original input.
  - Emotional Flatness: The TTS output sounds "robotic" despite selecting a specific emotion tag.
  - ASR Loop: The system enters a failure loop where ASR misinterprets speech, and the LLM confidently misinterprets the garbled text.
- First 3 experiments:
  1. Latency Profiling: Measure the end-to-end time from "stop speaking" to "audio output" for the full pipeline to verify real-time viability.
  2. Semantic Drift Testing: Input a dataset of ambiguous phrases and measure if the LLM's "refined" outputs alter the core meaning.
  3. Dysarthric ASR Accuracy: Test the WhisperX component specifically against a corpus of dysarthric speech to determine the baseline error rate.

## Open Questions the Paper Calls Out

### Open Question 1
How do end-users with diverse speech impairments (e.g., ALS, Parkinson's) perceive the usability and expressivity of Speak Ease compared to SLP proxies?
- Basis: Section 7 states "it would be beneficial to recruit a diverse group of users with various diagnoses... to test Speak Ease."
- Why unresolved: Current study evaluated solely through SLPs rather than target user population.
- Evidence needed: Results from user study with actual AAC users measuring usability, cognitive load, and satisfaction across different medical conditions.

### Open Question 2
What interface mechanisms can effectively balance the trade-off between communication speed and the cognitive effort required to configure expressivity features?
- Basis: Section 6.3 discusses "Balancing Efficiency and Personal Needs," noting users often forgo expressivity features for speed.
- Why unresolved: Time pressure of real-time conversation often forces users to choose efficiency over expression.
- Evidence needed: Comparative data on typing speed versus message richness in real-time scenarios using automated versus manual expressivity settings.

### Open Question 3
How can LLM-based AAC systems ensure user agency and prevent "facilitated communication" risks where the AI is perceived as the primary author?
- Basis: Section 5.3.5 details concerns about "Facilitated Communication (FC)," and Section 6.4 calls for methods to "ensure AI does not dominate the communication process."
- Why unresolved: SLPs expressed concern that AI-generated suggestions might lead the conversation or obscure user's authentic intent.
- Evidence needed: Studies measuring user perception of ownership and agency when using AI suggestions versus direct input methods.

### Open Question 4
Can emotional expression features in TTS be refined to accurately convey nuanced emotions beyond "happy" or "neutral"?
- Basis: Section 7 notes the need to "enhance the emotional expression feature... to ensure that selected emotions... are accurately conveyed."
- Why unresolved: Participants reported that emotions like "sad" and "angry" did not "come through that much" in voice output.
- Evidence needed: User recognition rates of intended emotional tones in synthesized speech across a wider range of affective states.

## Limitations
- Evaluation scope limited to small feasibility study (N=6) and SLP focus group (N=5) without direct user testing
- Core hypothesis of enhanced expressivity remains untested without quantitative metrics or before/after comparisons
- Real-time performance constraints (ASR accuracy, API latency) were not empirically validated

## Confidence
- High Confidence: Technical integration of WhisperX, GPT-4o, and Eleven Labs APIs functions as described; SLP findings regarding voice personalization are credible
- Medium Confidence: Context-aware semantic expansion and multimodal redundancy claims are theoretically sound but lack empirical validation with actual AAC users
- Low Confidence: Core hypothesis that Speak Ease meaningfully enhances expressivity and communication quality remains untested

## Next Checks
1. **End-to-End Latency Profiling:** Measure complete pipeline duration (speech input to audio output) under realistic conditions to determine if system supports natural conversation flow
2. **Dysarthric ASR Baseline Testing:** Evaluate WhisperX performance specifically on dysarthric speech corpora to quantify ASR error rate the LLM must correct
3. **Semantic Drift Analysis:** Conduct controlled testing where ambiguous user inputs are processed through full pipeline, measuring whether LLM-generated outputs maintain semantic fidelity to original intent