---
ver: rpa2
title: Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods
arxiv_id: '2312.08531'
source_url: https://arxiv.org/abs/2312.08531
tags:
- logt
- convergence
- lemma
- convex
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the last-iterate convergence of stochastic
  gradient methods, addressing a fundamental gap between practical performance and
  theoretical understanding. The authors propose a unified analysis framework based
  on Composite Stochastic Mirror Descent (CSMD) that can handle general domains, composite
  objectives, non-Euclidean norms, and various smoothness and convexity conditions
  simultaneously.
---

# Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods

## Quick Facts
- arXiv ID: 2312.08531
- Source URL: https://arxiv.org/abs/2312.08531
- Authors: Zijian Liu; Zhengyuan Zhou
- Reference count: 40
- Primary result: First high-probability convergence results for last iterates under sub-Gaussian and sub-Weibull noise without restrictive assumptions

## Executive Summary
This paper addresses the fundamental gap between the practical success of stochastic gradient methods and their theoretical understanding of last-iterate convergence. The authors develop a unified analysis framework based on Composite Stochastic Mirror Descent (CSMD) that simultaneously handles general domains, composite objectives, non-Euclidean norms, and various smoothness and convexity conditions. The key innovation is establishing convergence rates for last iterates in high probability under more realistic noise assumptions, including sub-Gaussian and sub-Weibull distributions, without requiring bounded domains or almost surely bounded noise. The framework also extends to heavy-tailed noise scenarios, showing that last iterates converge at near-optimal rates under finite p-th moment assumptions.

## Method Summary
The authors propose a unified analysis framework based on Composite Stochastic Mirror Descent (CSMD) that can handle general domains, composite objectives, non-Euclidean norms, and various smoothness and convexity conditions simultaneously. The framework establishes the first high-probability convergence results for last iterates under sub-Gaussian and sub-Weibull noise without restrictive assumptions like bounded domains or almost surely bounded noise. The analysis proves optimal rates of O(1/√T) for smooth convex and O(1/T) for smooth strongly convex functions in general domains. The unified approach also extends to heavy-tailed noise scenarios, showing that last iterates converge at near-optimal rates under finite p-th moment assumptions.

## Key Results
- First high-probability convergence results for last iterates under sub-Gaussian and sub-Weibull noise without restrictive assumptions
- Optimal convergence rates of O(1/√T) for smooth convex and O(1/T) for smooth strongly convex functions in general domains
- Unified framework extends to heavy-tailed noise scenarios with near-optimal rates under finite p-th moment assumptions
- Last iterate of CSMD achieves (nearly) optimal convergence rates both in expectation and high probability across multiple settings

## Why This Works (Mechanism)
The unified analysis framework leverages the Composite Stochastic Mirror Descent structure to handle the complexities of general domains and various noise distributions simultaneously. By using appropriate mirror maps and exploiting the composite structure of the objective, the framework can establish convergence guarantees without the restrictive assumptions typically required in previous analyses. The key insight is that the mirror descent structure naturally handles the geometry of general domains while the composite objective formulation allows for efficient handling of regularization terms and constraints.

## Foundational Learning
- **Stochastic Mirror Descent**: Optimization algorithm using Bregman divergences for non-Euclidean geometries; needed to handle general domains and non-Euclidean norms
- **Composite Objectives**: Objective functions with regularizers separable from smooth parts; enables handling of constraints and regularization naturally
- **Sub-Gaussian and Sub-Weibull Noise**: Probabilistic models for noise with controlled tails; allows realistic noise modeling without almost sure boundedness
- **Bregman Divergence**: Measure of distance between points in convex analysis; crucial for the geometry-aware updates in mirror descent
- **High-probability Bounds**: Concentration inequalities providing convergence guarantees with explicit probability; stronger than expectation bounds alone

## Architecture Onboarding
- **Component Map**: CSMD -> Mirror Map Selection -> Gradient Estimation -> Update Rule -> Convergence Analysis
- **Critical Path**: Domain specification → Mirror map construction → Noise characterization → Convergence rate derivation
- **Design Tradeoffs**: Generality vs. tightness of bounds, complexity of mirror maps vs. convergence rates, sub-Gaussian assumptions vs. heavy-tailed extensions
- **Failure Signatures**: Convergence degradation when noise deviates from sub-Gaussian/sub-Weibull, performance issues with poorly chosen mirror maps
- **First Experiments**: 1) Compare CSMD last iterate vs. averaged iterate on simple convex problems, 2) Test convergence under different noise distributions, 3) Validate heavy-tailed noise performance against theoretical predictions

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- The unified framework's complexity may limit practical accessibility for practitioners
- Specific requirements for mirror map and domain geometry need closer examination
- Practical implications of theoretical bounds require empirical validation
- Trade-offs between generality and tightness of bounds in specific problem classes

## Confidence
- Last-iterate convergence rates: High - The mathematical proofs appear rigorous and build on established techniques in stochastic optimization
- High-probability guarantees: Medium-High - The results are novel but depend on specific concentration inequalities that may have practical limitations
- Unified framework applicability: Medium - While theoretically comprehensive, the practical utility across all claimed settings needs empirical validation

## Next Checks
1. Empirical validation: Implement and test the CSMD framework on standard benchmark problems to verify that the theoretical convergence rates match practical performance
2. Sensitivity analysis: Examine how the convergence rates degrade under various noise distributions between sub-Gaussian and sub-Weibull regimes
3. Comparison study: Systematically compare the unified framework against specialized analyses for specific problem classes to quantify any trade-offs between generality and tightness of bounds