---
ver: rpa2
title: Clustering and Median Aggregation Improve Differentially Private Inference
arxiv_id: '2506.04566'
source_url: https://arxiv.org/abs/2506.04566
tags:
- data
- privacy
- private
- median
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating high-quality differentially
  private synthetic text by clustering sensitive input data before batching. The authors
  show that uniform random batching leads to heterogeneous batches that degrade synthetic
  text quality, especially for heterogeneous datasets.
---

# Clustering and Median Aggregation Improve Differentially Private Inference

## Quick Facts
- **arXiv ID**: 2506.04566
- **Source URL**: https://arxiv.org/abs/2506.04566
- **Reference count**: 34
- **Key outcome**: Median aggregation with clustered batching produces higher-quality DP synthetic text with tighter privacy guarantees than prior work.

## Executive Summary
This paper addresses the challenge of generating high-quality differentially private synthetic text by clustering sensitive input data before batching. The authors demonstrate that uniform random batching leads to heterogeneous batches that degrade synthetic text quality, especially for datasets with heterogeneous topics. By first clustering similar examples and then batching within clusters, they create more homogeneous inputs that produce better-aligned predictions. Additionally, they introduce a median aggregation method for next-token predictions that exploits reduced local sensitivity when predictions are aligned, allowing for tighter data-dependent and ex-post differential privacy guarantees. Experiments on multiple datasets show that their method produces more representative synthetic data (higher MAUVE scores) and achieves better downstream BERT accuracy than previous state-of-the-art methods, while requiring significantly lower privacy costs.

## Method Summary
The method consists of three key components: (1) clustering private data using k-means centers computed on a public dataset, (2) rebalancing clusters via DP counts to ensure usable batch sizes, and (3) aggregating next-token predictions using median instead of mean. For clustering, the authors compute k-means centers on public DBPedia embeddings and assign private seeds to these centers. They then apply DP count filtering (ε=0.1) to retain the most populated clusters. During generation, they batch seeds within clusters and aggregate next-token logits using median (clipped to [-6,6]) rather than mean, sampling from softmax(median_logits/temperature). This median aggregation exploits the fact that when predictions are aligned, the local sensitivity depends on the median gap, which can be much smaller than the global sensitivity.

## Key Results
- MAUVE scores improve from 0.130 to 0.650 on Yelp100k when using non-private clustering vs random batching
- Median aggregation achieves ex-post privacy costs of 1.41 per batch on AGNews compared to 2.40 for mean-based methods
- BERT accuracy on synthetic data improves from 51% to 60% on AGNews compared to previous DP methods
- The method produces more representative synthetic data while requiring lower privacy budgets than prior approaches

## Why This Works (Mechanism)

### Mechanism 1
Clustering inputs before batching improves synthetic text representativeness by reducing semantic collapse during aggregation. Similar seeds produce similar LLM responses; aggregating homogeneous batches preserves cluster-specific semantics rather than averaging them away. Heterogeneous batches collapse distinct topics into meaningless averages. The core assumption is that the embedding space (Gecko) meaningfully captures semantic similarity relevant to the generation task. Evidence includes non-private clustering improving MAUVE from 0.130 to 0.650 on Yelp100k, demonstrating the signal exists independent of privacy constraints. The break condition is if public cluster centers fail to capture private data distribution structure, rebalancing cannot recover useful clusters.

### Mechanism 2
Median aggregation of logits reduces privacy cost when predictions are aligned, enabling data-dependent ex-post guarantees. Local sensitivity of median depends on the "median gap"—distance between median and adjacent values. When logit vectors are similar (from clustering), the gap is small, reducing how much one seed can shift the median. Clipping forces alignment of top-token logits across samples. The core assumption is that clustering produces sufficiently aligned next-token predictions that median gaps remain small. Evidence includes formal guarantees showing ε(D,X) = max over batches of sum over tokens of γ(Z_{i,t}, x_{i,t}), where γ increases with median gap, and per-batch ε distributions showing median per-batch cost (1.41 on AGNews) substantially below max reported (2.40). The break condition is if predictions diverge (large median gaps), the ex-post ε approaches or exceeds mean-based methods' worst-case guarantee.

### Mechanism 3
Public cluster centers with DP rebalancing outperforms direct DP clustering on high-dimensional embeddings. DP clustering algorithms add noise scaling with dimension; sentence embeddings are high-dimensional, causing failure to find meaningful centers. Using public data (DBPedia) for centers incurs zero privacy cost. Noisy count filtering removes empty/tiny clusters. The core assumption is that public dataset distribution overlaps sufficiently with private data to provide useful cluster structure. Evidence includes DP clustering finding only 9 clusters vs 500 target, while public centers with rebalancing achieves 100 usable clusters, and visual comparisons showing DP clustering creates extreme imbalance while public+rebalancing produces balanced clusters. The break condition is if public and private distributions are disjoint, cluster assignments become arbitrary or highly imbalanced.

## Foundational Learning

- **Local vs. Global Sensitivity**: Why needed here: The median mechanism's advantage depends entirely on local sensitivity (actual data configuration) being lower than global sensitivity (worst case). Understanding this distinction is prerequisite to grasping why data-dependent guarantees are tighter. Quick check: If you add one outlier logit vector to a set of 100 identical vectors, does the median's local sensitivity increase or stay zero?

- **Ex-post Differential Privacy**: Why needed here: The paper's guarantee is output-dependent—you only know ε after seeing the generated text. This differs from standard DP where ε is fixed a priori, affecting how practitioners interpret and report privacy costs. Quick check: Why might an ex-post guarantee be problematic for a data steward who must commit to a privacy budget before releasing synthetic data?

- **Exponential Mechanism via Softmax Sampling**: Why needed here: Algorithm 1 samples tokens from softmax(clipped logits/temperature), which the paper connects to the exponential mechanism. The clipping constant c and temperature τ jointly determine the privacy-utility tradeoff. Quick check: If you increase temperature τ while keeping clipping c fixed, does per-token privacy cost γ increase or decrease?

## Architecture Onboarding

- Component map: Private Dataset → [Embed (Gecko)] → [Cluster Assignment using Public Centers] → [Rebalance via Noisy Counts (ε≈0.1)] → [Batch Assignment] → For each batch: [LLM Logits per seed] → [Clip to [-c,c]] → [Median Aggregation] → [Softmax Sample] → [Append token] → Output: Synthetic Dataset

- Critical path: The clustering → rebalancing → median aggregation pipeline. Errors in clustering compound through aggregation; poor alignment means median gaps stay large, negating the privacy benefit.

- Design tradeoffs:
  - More clusters → better homogeneity but smaller batches → higher variance in median estimation
  - Tighter clipping c → lower sensitivity but more distortion to logit distributions
  - Public data similarity vs. domain specificity: generic public data (Wikipedia/DBPedia) is broadly applicable but may miss domain-specific clusters

- Failure signatures:
  - Many clusters with <100 samples after rebalancing (clustering failed to match distribution)
  - Per-batch ε distribution with long right tail (some batches have unaligned predictions)
  - MAUVE scores barely above baseline despite clustering (public centers irrelevant to private data)

- First 3 experiments:
  1. **Sanity check without privacy**: Run non-private k-means clustering on private embeddings (no DP), batch within clusters, use mean aggregation. Compare MAUVE to random batching. This establishes upper bound on clustering signal.
  2. **Ablate aggregation method**: With clustering fixed (public centers + rebalancing), compare mean vs. median aggregation. Match compute by equalizing batch sizes and output tokens. Report both MAUVE and achieved ex-post ε for median.
  3. **Sensitivity to public data**: Swap DBPedia for a domain-specific public dataset (if available) or artificially limit public data diversity. Measure V-measure against private ground-truth clusters and downstream MAUVE to quantify distribution mismatch penalty.

## Open Questions the Paper Calls Out

- Can inference algorithms be designed to exploit the empirical distribution of per-batch privacy costs to provide tighter overall guarantees than the worst-case maximum? The current analysis reports the maximum privacy cost across all batches (the worst case), even though Appendix B shows many batches incur significantly lower costs (often <50% of the max). An algorithmic modification or accounting method that bounds the total privacy cost based on the aggregate distribution rather than the maximum single batch cost would resolve this.

- How can differentially private clustering be improved for high-dimensional text embeddings to avoid the "curse of dimensionality" without relying on external public data? Existing DP clustering algorithms fail on high-dimensional data (finding <10 clusters when 500 were targeted), forcing the authors to use public data for centers. A DP clustering mechanism that can generate hundreds of balanced, meaningful clusters in high-dimensional embedding spaces without auxiliary public datasets would resolve this.

- Can the benefits of median aggregation be preserved when using black-box model access (API-only) where raw logits are unavailable? The median mechanism relies on clipping and aggregating logit vectors to exploit local sensitivity; it is unclear if a similar reduction in privacy cost can be achieved using only generated tokens or probability vectors. A variation of the aggregation method that operates on top-k probabilities or sampled tokens while maintaining the data-dependent privacy guarantees would resolve this.

## Limitations

- The median aggregation mechanism's privacy guarantees rely critically on prediction alignment, which the authors show empirically but don't formally bound.
- The reliance on public data for clustering centers assumes distributional overlap without quantifying mismatch costs.
- The ex-post nature of the privacy guarantee means practitioners cannot pre-commit to budgets—a significant practical limitation.

## Confidence

- **High Confidence**: Clustering improves MAUVE scores vs random batching (non-private experiments show 5× improvement; private experiments show 1.5-2× improvement)
- **Medium Confidence**: Median aggregation provides tighter privacy than mean under aligned predictions (formal theorem exists, but real-world alignment conditions are only partially characterized)
- **Low Confidence**: Public clustering centers with rebalancing consistently outperforms direct DP clustering (only one comparison shown, high-dimensional embedding clustering is notoriously difficult)

## Next Checks

1. **Sensitivity to Public Data Distribution**: Systematically vary the similarity between public DBPedia and private datasets by limiting DBPedia's topical diversity or using domain-specific public data. Measure the trade-off between distributional mismatch (V-measure drop) and downstream performance degradation.

2. **Per-Batch Privacy Cost Analysis**: For each batch in the AGNews and Yelp experiments, compute the actual median gap across all token positions. Correlate batch-specific median gaps with their achieved ε values to validate the theoretical relationship between alignment and privacy cost.

3. **Scalability Testing**: Reproduce the median aggregation mechanism on larger batch sizes (100-1000 seeds) and higher-dimensional models (7B+ parameters). Measure whether the computational overhead and prediction alignment degrade as model capacity increases, and whether the median gap behavior scales predictably.