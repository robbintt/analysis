---
ver: rpa2
title: Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression
  Profiles
arxiv_id: '2505.02033'
source_url: https://arxiv.org/abs/2505.02033
tags:
- quantum
- data
- microarray
- deep
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a quantum AI model, Deep VQC, to classify
  four brain tumor types and healthy samples using high-dimensional DNA microarray
  gene expression data (54,676 features). The model combines classical preprocessing
  with a variational quantum classifier leveraging amplitude encoding and two-layer
  Hardware Efficient Ansatz circuits.
---

# Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles

## Quick Facts
- arXiv ID: 2505.02033
- Source URL: https://arxiv.org/abs/2505.02033
- Authors: Emine Akpinar; Batuhan Hangun; Murat Oduncuoglu; Oguz Altun; Onder Eyecioglu; Zeynel Yalcin
- Reference count: 21
- Primary result: Deep VQC achieves 0.88 training and 0.79 validation accuracy classifying four brain tumor types from 54,676-gene expression profiles

## Executive Summary
This study introduces Deep VQC, a quantum AI model that classifies four brain tumor types and healthy samples using high-dimensional DNA microarray gene expression data. The model combines classical preprocessing with a variational quantum classifier using amplitude encoding and two-layer Hardware Efficient Ansatz circuits. Deep VQC achieves competitive accuracy (0.85) compared to classical ML models while demonstrating quantum computing's potential to handle high-dimensional biomedical data, with PCA preprocessing improving validation accuracy from 0.79 to 0.86.

## Method Summary
The approach uses DNA microarray data from 130 samples across five classes (ependymoma, glioblastoma, healthy, medulloblastoma, oligodendroglioma) with 54,676 gene expression features. Classical preprocessing includes standard scaling and min-max normalization, with optional PCA retaining 95% variance. The quantum classifier employs amplitude encoding to map features to 15 qubits, followed by sequential dual Hardware Efficient Ansatz circuits with parameterized rotations and entangling gates. The model is trained using cross-entropy loss and gradient descent, measuring Pauli-Z observables to predict class probabilities.

## Key Results
- Deep VQC achieves 0.88 training and 0.79 validation accuracy without PCA
- PCA preprocessing improves validation accuracy to 0.86 while reducing features from 54,676 to 65 dimensions
- Overall accuracy of 0.85 compares competitively to classical models (SVM: 0.95, RF: 0.91)
- Class-wise recall varies significantly (0.25-1.0), indicating class imbalance effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amplitude encoding enables compact representation of high-dimensional gene expression data in quantum states.
- Mechanism: Classical input vectors are mapped to quantum state amplitudes, allowing 2^n features to be encoded using only n qubits.
- Core assumption: The quantum simulator can faithfully represent amplitude-encoded states without significant decoherence or encoding errors.
- Evidence anchors: Abstract states "leveraging amplitude encoding"; methodology section describes encoding process; corpus evidence for this mechanism is weak.
- Break condition: If input data is not properly normalized, amplitude encoding fails; if qubit count is insufficient, data truncation occurs.

### Mechanism 2
- Claim: Sequential dual Hardware Efficient Ansatz circuits capture complex quantum correlations in gene expression patterns.
- Mechanism: Two distinct HEAs operate sequentially with parameterized rotations and entangling operations across 25 layers total.
- Core assumption: Deeper ansatz circuits with sequential processing learn more meaningful representations without hitting barren plateau issues.
- Evidence anchors: Abstract mentions "two-layer Hardware Efficient Ansatz circuits"; methodology describes sequential operation; corpus evidence shows similar shallow circuits are common in NISQ designs.
- Break condition: Excessive circuit depth may cause gradient vanishing; over-parameterization can lead to training instability.

### Mechanism 3
- Claim: PCA preprocessing enhances validation accuracy by reducing noise while preserving class-discriminative variance.
- Mechanism: PCA retains 95% of total variance, reducing 54,676 features to 65 components while maintaining information critical for classification.
- Core assumption: The 5% of discarded variance contains primarily noise rather than class-discriminative information.
- Evidence anchors: Results show accuracy improvement from 0.79 to 0.86 with PCA; methodology section describes 95% variance retention; corpus evidence supports dimensionality reduction benefits.
- Break condition: Aggressive dimensionality reduction risks removing discriminative features; class imbalance may amplify this risk.

## Foundational Learning

- **Variational Quantum Classifier (VQC)**: Why needed: Deep VQC is built on VQC framework; understanding hybrid quantum-classical optimization loop is essential. Quick check: Can you explain how classical gradient descent updates quantum circuit parameters?

- **Amplitude Encoding**: Why needed: The primary data loading mechanism; misuse leads to invalid quantum states or information loss. Quick check: How many qubits are needed to encode a 32-feature vector via amplitude encoding?

- **Microarray "Curse of Dimensionality"**: Why needed: Motivates the entire approach; 130 samples vs. 54,676 features creates severe overfitting risk. Quick check: Why does high dimensionality with few samples challenge classical ML models?

## Architecture Onboarding

- **Component map**: Classical preprocessing → Amplitude encoding onto 15 qubits → HEA-1 (RX/RY rotations + CNOT/Toffoli) → HEA-2 (RY/RZ rotations + CNOT/Toffoli) → Pauli-Z measurement → Cross-entropy optimization

- **Critical path**: Data normalization → Amplitude encoding validation → HEA parameter initialization → Measurement fidelity → Convergence monitoring

- **Design tradeoffs**: 15 qubits balances encoding capacity against NISQ hardware constraints; 25 layers increase expressivity but risk barren plateaus; PCA vs raw trades validation accuracy (+9%) against potential information loss

- **Failure signatures**: Validation accuracy significantly below training accuracy (overfitting: 0.88 vs 0.79 without PCA); cost function plateau without convergence (barren plateau or learning rate issues); class-wise recall variance (0.25-1.0 range indicates class imbalance effects)

- **First 3 experiments**:
  1. Replicate baseline: Train Deep VQC on raw normalized data (54,676 features), verify 0.79 validation accuracy
  2. Ablation study: Test PCA variance thresholds (90%, 95%, 99%) to validate 95% optimality claim
  3. Ansatz comparison: Replace dual HEA with single HEA to measure sequential ansatz contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Deep VQC framework be extended to perform quantum feature selection to identify specific gene biomarkers for brain tumors?
- Basis in paper: The Conclusion states future work will aim to "develop quantum AI approaches capable of identifying key gene features critical for distinguishing brain tumor types."
- Why unresolved: Current study focuses solely on classification accuracy without implementing feature extraction mechanisms.
- What evidence would resolve it: Modified Deep VQC outputting ranked gene features validated against known biological markers.

### Open Question 2
- Question: How does Deep VQC performance degrade on noisy NISQ hardware compared to the ideal simulator?
- Basis in paper: Results rely on noise-free state-vector simulator despite acknowledging NISQ limitations.
- Why unresolved: Robustness to error rates and noise in physical quantum processors remains untested.
- What evidence would resolve it: Execution on physical hardware with accuracy and convergence comparison against simulator results.

### Open Question 3
- Question: Can Deep VQC surpass classical algorithms' accuracy without aggressive dimensionality reduction?
- Basis in paper: Deep VQC achieves 0.85 accuracy versus SVM (0.95) and RF (0.91).
- Why unresolved: Study demonstrates "competitive" performance but not quantum advantage in accuracy.
- What evidence would resolve it: Comparative study where Deep VQC achieves significantly higher accuracy than all classical models.

## Limitations
- Results based on single microarray dataset, limiting generalizability across different gene expression distributions
- 15% validation accuracy gap (0.88 training vs 0.79 validation) suggests overfitting concerns
- Quantum simulator results may not translate to real NISQ hardware due to noise and decoherence effects

## Confidence
- Mechanism 1 (Amplitude Encoding): Medium - Theoretically sound but practical implementation details are sparse
- Mechanism 2 (Dual HEA Architecture): Medium - Sequential design shows promise but lacks ablation study validation  
- Mechanism 3 (PCA Enhancement): High - Clear quantitative improvement documented (9% accuracy gain)
- Comparative Performance: Medium - Classical baselines lack detailed hyperparameter optimization reporting

## Next Checks
1. Cross-dataset validation: Test Deep VQC on at least two additional microarray cancer datasets to verify robustness across different gene expression distributions and sample sizes.

2. Hardware implementation: Deploy the 15-qubit circuit on available NISQ hardware to measure decoherence effects and validate simulator results under realistic noise conditions.

3. Feature importance analysis: Conduct ablation studies removing specific PCA components to quantify contribution of each retained dimension to classification accuracy and identify potential biomarker signatures.