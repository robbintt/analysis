---
ver: rpa2
title: 'BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect
  Sentiment Analysis with Cross-Domain Transfer Learning'
arxiv_id: '2511.23264'
source_url: https://arxiv.org/abs/2511.23264
tags:
- sentiment
- learning
- bangla
- performance
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-aspect sentiment analysis for Bangla
  e-commerce reviews, a challenging task due to limited annotated datasets, morphological
  complexity, code-mixing, and domain shift issues. The proposed BanglaSentNet framework
  integrates LSTM, BiLSTM, GRU, and BanglaBERT models through dynamic weighted ensemble
  learning, incorporating SHAP-based feature attribution and attention visualization
  for transparency.
---

# BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning

## Quick Facts
- **arXiv ID**: 2511.23264
- **Source URL**: https://arxiv.org/abs/2511.23264
- **Reference count**: 40
- **Primary result**: 85% accuracy, 0.88 F1-score on multi-aspect Bangla sentiment classification with explainability and cross-domain transfer learning

## Executive Summary
BanglaSentNet addresses multi-aspect sentiment analysis for Bangla e-commerce reviews, overcoming challenges of limited annotated data, morphological complexity, and domain shift. The framework integrates LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning, achieving 85% accuracy and 0.88 F1-score on 8,755 manually annotated reviews. Cross-domain experiments demonstrate robust generalization with 67-76% zero-shot retention across diverse domains and 90-95% few-shot performance. SHAP-based feature attribution and attention visualization provide transparency, achieving 9.4/10 interpretability score with 87.6% human agreement.

## Method Summary
The framework combines four neural architectures: BanglaBERT (12 layers, 768 hidden, AdamW optimizer, lr=2e-5), BiLSTM (128×2 units, dropout=0.3), LSTM (256 units, gradient clipping=1.0), and GRU (200 units, Adam optimizer, lr=1e-3). Dual embeddings (GloVe 300-dim + FastText + BanglaBERT contextual) feed into recurrent models. Dynamic weighted ensemble learning combines predictions via learned combination optimized on validation data. Explainability integrates SHAP for feature attribution and attention visualization. Training uses 70/15/15 split on 8,755 reviews from five platforms, with cross-domain evaluation on BanglaBook and social media datasets.

## Key Results
- Achieves 85% accuracy and 0.88 F1-score on held-out test set, outperforming standalone models by 3-7%
- Zero-shot cross-domain performance retains 67-76% effectiveness across diverse domains (BanglaBook, social media, general e-commerce, news headlines)
- Few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance
- Explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement

## Why This Works (Mechanism)
The hybrid ensemble architecture leverages complementary strengths: BanglaBERT provides rich contextual representations for morphological complexity, while LSTM/BiLSTM/GRU models capture sequential patterns and local dependencies. Dynamic weighted ensemble learning adapts to input characteristics, optimizing performance across diverse review styles and domains. SHAP-based feature attribution and attention visualization provide transparency by identifying influential tokens and highlighting sentiment-bearing phrases, making predictions interpretable for commercial applications.

## Foundational Learning
- **Bangla morphological complexity**: Required for handling agglutinative word structures and inflectional variations in Bangla text. Quick check: Test model performance on words with multiple morphological variants.
- **Code-mixing handling**: Essential for processing reviews containing English words mixed with Bangla script. Quick check: Evaluate performance on reviews with varying degrees of English word insertion.
- **Cross-domain transfer learning**: Critical for adapting sentiment models across different e-commerce domains with distinct vocabulary and review styles. Quick check: Measure performance degradation when shifting from Daraz to BanglaBook reviews.
- **Ensemble weight optimization**: Needed to dynamically combine heterogeneous model predictions based on validation performance. Quick check: Compare ensemble performance against static averaging of model outputs.
- **Explainability through SHAP**: Required for providing feature attribution and building trust in model predictions for commercial deployment. Quick check: Verify SHAP values correctly identify sentiment-bearing words in sample reviews.
- **Attention visualization**: Important for highlighting which review segments contribute most to sentiment classification. Quick check: Ensure attention weights align with human intuition about sentiment indicators.

## Architecture Onboarding

**Component Map**
Input reviews → Text preprocessing → Dual embeddings (GloVe/FastText/BanglaBERT) → LSTM, BiLSTM, GRU, BanglaBERT models → Weighted ensemble combination → SHAP feature attribution + Attention visualization → Output sentiment predictions

**Critical Path**
Data preprocessing → Model training (individual models) → Ensemble weight optimization → Explainability integration → Cross-domain evaluation

**Design Tradeoffs**
Ensemble complexity vs. performance gain (3-7% improvement justifies added complexity), explainability overhead vs. commercial deployment requirements, cross-domain generalization vs. domain-specific optimization

**Failure Signatures**
Class imbalance causing degraded per-class performance, code-mixing overwhelming language-specific models, ensemble overfitting to validation set, BanglaBERT compatibility issues with TensorFlow 2.4

**Three First Experiments**
1. Train individual models (LSTM, BiLSTM, GRU, BanglaBERT) on the full dataset and measure baseline performance
2. Implement weighted ensemble and optimize weights on validation set, measuring performance improvement over baselines
3. Apply SHAP and attention visualization to sample predictions to verify explainability functionality

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset availability limited to "upon reasonable request" without public repository, restricting independent verification
- Cross-domain evaluation relies on only two validation domains with limited sample sizes (494 and 1,145 reviews)
- Ensemble weight learning algorithm underspecified beyond "gradient descent on validation set"
- Pre-trained embedding sources and training corpus details not disclosed

## Confidence
High confidence: Ensemble architecture with specified hyperparameters and clear performance metrics (85% accuracy, 0.88 F1-score)
Medium confidence: Cross-domain transfer learning results methodologically sound but limited validation scope
Low confidence: Exact dataset composition, ensemble weight optimization details, and pre-trained embedding specifications cannot be independently verified

## Next Checks
1. Request and verify complete annotated dataset from authors, including per-aspect label distributions and code-mixing prevalence across all 8,755 reviews
2. Implement ensemble weight optimization on publicly available Bangla sentiment dataset to validate architecture performance gains
3. Conduct ablation studies removing each model component to quantify individual contributions and verify claimed 3-7% ensemble improvement