---
ver: rpa2
title: How Does the Spatial Distribution of Pre-training Data Affect Geospatial Foundation
  Models?
arxiv_id: '2501.12535'
source_url: https://arxiv.org/abs/2501.12535
tags:
- data
- stratified
- pre-training
- samples
- forest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how the geographic distribution of pre-training
  data affects the performance of Geospatial Foundation Models (GFMs) on downstream
  tasks. While prior work focused on model architecture and pre-training tasks, the
  impact of pre-training data selection has been underexplored.
---

# How Does the Spatial Distribution of Pre-training Data Affect Geospatial Foundation Models?

## Quick Facts
- arXiv ID: 2501.12535
- Source URL: https://arxiv.org/abs/2501.12535
- Authors: Mirali Purohit; Gedeon Muhawenayo; Esther Rolf; Hannah Kerner
- Reference count: 40
- Key outcome: Balanced global sampling strategies consistently outperform clustered or region-specific approaches across both GFMs in few-shot settings, with diminishing differences as finetuning data increases.

## Executive Summary
This study investigates how the geographic distribution of pre-training data affects Geospatial Foundation Models' (GFMs) performance on downstream tasks. The authors evaluate two GFMs (Presto and SatCLIP) pre-trained on five different data compositions: zero pre-training, uniform random sampling, stratified by continent, stratified by biome, and clustered sampling focused on natural forests and world cities. Models were finetuned on continent-specific subsets of two global tasks in few-shot settings (n=100 samples per continent, repeated 50 times). Results show that balanced sampling strategies consistently outperform clustered approaches across both GFMs, with location-based models showing higher sensitivity to geographic bias than pixel-based ones.

## Method Summary
The study compares five pre-training data compositions using two GFMs (Presto and SatCLIP) on two global tasks (CropHarvest and EcoRegions). For each combination, models are finetuned in few-shot settings (n=100 samples per continent) with 50 random seed iterations. Performance is measured using F1 score across continents, comparing balanced strategies (uniform random, stratified continent/biome) against clustered approaches (natural forests, world cities).

## Key Results
- Balanced sampling strategies (uniform random, stratified continent/biome) consistently outperform clustered or region-specific approaches across both GFMs
- Location-encoder models (SatCLIP) show higher sensitivity to geographic bias than pixel-timeseries models (Presto)
- Performance differences between data compositions diminish as finetuning data increases, with balanced pre-training especially beneficial in low-data regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced global sampling strategies produce representations that transfer more effectively across regions than clustered or region-specific sampling.
- Mechanism: Pre-training data with global coverage exposes the model to diverse land cover types, biomes, and geographic features, creating feature representations that capture universally useful patterns rather than overfitting to specific regions. When downstream tasks require generalization to unseen locations, globally-trained features provide better initializations.
- Core assumption: The downstream task benefits from features that generalize across geographic regions, rather than features specialized to specific areas.
- Evidence anchors:
  - [abstract] "balanced and globally representative data compositions often outperform region-specific sampling, highlighting the importance of diversity and global coverage in pre-training data"
  - [section 4, Results] "all balanced data sampling techniques (i.e., UAR, stratified continent/biome) outperform clustered techniques... performance is approximately consistent among balanced techniques, suggesting that among sampling strategies that ensure global representation, there is less difference in downstream task performance"
  - [corpus] Related work (SHRUG-FM) notes that "GFMs often fail to perform reliably in environments underrepresented during pretraining" — consistent with the finding that global coverage mitigates this failure mode.
- Break condition: If downstream tasks are highly specialized to a single region with unique characteristics not represented globally (e.g., polar ice sheet dynamics), balanced global sampling may underperform targeted regional pre-training.

### Mechanism 2
- Claim: Location-encoder architectures exhibit higher sensitivity to geographic bias in pre-training data than pixel-timeseries architectures.
- Mechanism: SatCLIP encodes location directly as input, learning implicit spatial representations that bind features to geographic coordinates. When pre-training data is geographically clustered, the model cannot disentangle location from land cover features, leading to poor generalization. Presto operates on pixel timeseries without explicit location encoding, allowing spectral-temporal patterns to transfer independently of geography.
- Core assumption: Location-encoder models must learn a continuous function over geographic space; clustered sampling creates coverage gaps where this function is undefined or poorly constrained.
- Evidence anchors:
  - [section 4, Results] "SatCLIP, which is a location-encoder model, struggles to generalize when pre-training data lacks global representation. However, Presto is a pixel timeseries model (which does not rely solely on location) and is likely more capable of generalizing on World Cities"
  - [table 1] World Cities performs comparably to balanced strategies on CropHarvest (Presto) but substantially worse on EcoRegions (SatCLIP), particularly in Africa (0.57 vs 0.68 F1) and Asia (0.57 vs 0.68 F1).
  - [corpus] No direct corpus corroboration for architecture-specific geographic bias sensitivity; this appears to be a novel finding in this paper.
- Break condition: If location encoders are augmented with explicit geographic priors (e.g., known climate zones, coordinate embeddings with spatial smoothness constraints), sensitivity to sampling bias may decrease.

### Mechanism 3
- Claim: Pre-training data distribution effects are most pronounced in few-shot regimes and diminish as finetuning data increases.
- Mechanism: In low-data settings, the model relies heavily on representations learned during pre-training — limited finetuning samples cannot substantially reshape features. As finetuning data grows, the model can learn task-specific features directly, reducing dependence on pre-training quality.
- Core assumption: The downstream task has sufficient samples to overcome pre-training deficiencies once past a threshold; this threshold varies by task complexity.
- Evidence anchors:
  - [section 4, Results] "Balanced data compositions outperform clustered data sampling in few-shot scenarios. However, when finetuning is conducted on an increasing amount of data (thousands of samples), the performance differences between various data compositions drop significantly"
  - [figure 2] Lines representing different data compositions converge as finetuning samples increase from ~100 to ~6000.
  - [corpus] InstaGeo notes computational efficiency concerns but does not address finetuning data scaling; no direct corpus evidence on this mechanism.
- Break condition: If finetuning data remains scarce (permanently low-label regimes), pre-training distribution effects persist indefinitely.

## Foundational Learning

- **Concept: Stratified vs. Clustered Sampling**
  - Why needed here: The paper's core intervention is comparing different pre-training data compositions. Understanding how stratification ensures representation across strata (continents, biomes) vs. how clustering creates geographic concentration is essential to interpret results.
  - Quick check question: Given a global dataset, would sampling 100 points uniformly at random produce equal representation across continents? Why or why not?

- **Concept: Location Encoding vs. Pixel-Based Representations**
  - Why needed here: The two GFMs evaluated (SatCLIP, Presto) differ fundamentally in how they incorporate geographic information. Location encoders map coordinates to embeddings; pixel models process spectral values directly.
  - Quick check question: If a location encoder is trained only on European data, what happens when queried on coordinates in South America?

- **Concept: Few-Shot Learning Dynamics**
  - Why needed here: All experiments use n=100 samples per continent. The finding that pre-training effects diminish with more data depends on understanding why few-shot settings amplify pre-training quality.
  - Quick check question: Why might a model with poor pre-training still achieve high accuracy with 10,000 labeled finetuning samples?

## Architecture Onboarding

- **Component map:**
Pre-training Pipeline: Global Data Pool → Sampling Strategy → GFM Pre-training → [UAR | Stratified | Clustered] → [Presto (pixel-timeseries) | SatCLIP (location-enc)] → CropHarvest task / EcoRegions task

- **Critical path:**
  1. Define sampling strategy from {UAR, Stratified-Continent, Stratified-Biome, Natural-Forest, World-Cities}
  2. Sample pre-training data from global pool (6.5M for Presto, 100K for SatCLIP)
  3. Pre-train GFM using official configurations
  4. Create continent-specific downstream subsets
  5. Finetune with n=100 samples, repeat 50× with different seeds
  6. Report mean F1 ± std across iterations

- **Design tradeoffs:**
  - Balanced (UAR, stratified) vs. Clustered: Balanced improves generalization but may underrepresent rare but task-critical environments (e.g., forests for forestry tasks)
  - Stratified-Continent vs. Stratified-Biome: Biomes capture ecological similarity better; continents are administratively convenient but ecologically arbitrary
  - Computation vs. Coverage: Sampling less data with better distribution can outperform sampling more data with bias

- **Failure signatures:**
  - High variance across finetuning seeds (std > 0.03) → insufficient pre-training; model relies on random initialization
  - Strong performance in some continents, near-random in others → geographic bias in pre-training data
  - SatCLIP underperforming baseline → likely clustered pre-training with geographic gaps

- **First 3 experiments:**
  1. **Baseline check:** Finetune both GFMs with zero pre-training (random weights) on all continents with n=100. Establish floor performance; if pre-training helps, all compositions should exceed this.
  2. **Architecture sensitivity test:** Pre-train SatCLIP on Natural Forest and World Cities separately; compare per-continent F1 gaps. Expect larger degradation in non-forest/non-urban continents for Natural Forest/World Cities respectively.
  3. **Scaling sweep:** For one continent (e.g., North America), finetune with n ∈ {100, 500, 1000, 2000, 5000} samples across all compositions. Verify convergence of performance differences as n increases.

## Open Questions the Paper Calls Out
None

## Limitations
- The study evaluates only two GFMs (Presto and SatCLIP), which may limit generalizability to other geospatial foundation models with different architectures.
- All experiments use few-shot settings (n=100 samples per continent); the findings may not extend to scenarios with abundant labeled data where fine-tuning can overcome pre-training biases.
- The geographic and ecological representativeness of the global data pool is assumed but not independently validated—biased pre-training pools could confound results.

## Confidence
- **High**: Balanced sampling strategies (UAR, stratified continent/biome) outperform clustered sampling in few-shot scenarios.
- **Medium**: Location-encoder models (SatCLIP) are more sensitive to geographic bias than pixel-timeseries models (Presto).
- **Medium**: Pre-training data effects diminish with more fine-tuning samples.

## Next Checks
1. **Architecture Generalization Test**: Evaluate a third GFM with a different architecture (e.g., transformer-based) under the same sampling strategies to verify if the observed sensitivity to geographic bias generalizes beyond SatCLIP and Presto.

2. **Bias in Global Data Pool**: Conduct an independent audit of the global data pool used for pre-training to ensure it is not already geographically or ecologically biased, which could confound the interpretation of sampling strategy effects.

3. **Scaling Beyond Few-Shot**: Extend the finetuning sample sweep to tens of thousands of samples and test on tasks with varying complexity to characterize the exact conditions under which pre-training data composition effects disappear.