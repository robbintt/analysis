---
ver: rpa2
title: 'InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems'
arxiv_id: '2507.08235'
source_url: https://arxiv.org/abs/2507.08235
tags:
- causal
- energy
- insightbuild
- building
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InsightBuild, a framework that combines causal
  inference with a fine-tuned LLM to explain energy anomalies in smart buildings.
  It detects causal drivers of anomalies using Granger causality tests and structural
  causal models, then uses an LLM fine-tuned on expert-annotated examples to generate
  natural language explanations.
---

# InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems

## Quick Facts
- arXiv ID: 2507.08235
- Source URL: https://arxiv.org/abs/2507.08235
- Reference count: 12
- Primary result: Combines causal inference with fine-tuned LLM to explain energy anomalies in smart buildings with 84.7% accuracy on Google dataset and 80.0% on Berkeley dataset

## Executive Summary
InsightBuild addresses the challenge of explaining anomalous energy consumption in smart buildings by integrating Granger causality testing with a fine-tuned large language model. The framework first identifies causal drivers of anomalies through statistical testing on time series data, then generates natural language explanations using a specialized LLM trained on expert-annotated examples. Evaluated on two real-world datasets, InsightBuild significantly outperforms both rule-based and deep learning baselines while achieving high expert satisfaction scores.

## Method Summary
The framework processes time series sensor data through preprocessing (imputation, resampling, z-score standardization), then applies sliding window analysis to detect anomalies. For each anomaly, a causal inference module computes pairwise Granger causality tests with p=3 lags, applies structural pruning to remove indirect pathways, and ranks top-3 causes by F-statistic. A fine-tuned LLaMA 2 7B LLM then generates explanations from the ranked causes. The system uses 2,500 expert-annotated cause-explanation pairs for training, with hyperparameters including batch size 8, learning rate 2e-5, and early stopping on validation perplexity.

## Key Results
- Achieves 84.7% explanation accuracy (Acc@1) on Google Smart Buildings dataset
- Achieves 80.0% explanation accuracy (Acc@1) on Berkeley Office Building dataset
- Expert satisfaction scores of 4.2/5.0 (Google) and 4.0/5.0 (Berkeley), significantly higher than baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Lag-Based Causal Prioritization via Granger Testing
Applying pairwise Granger causality tests to sliding windows of telemetry data likely isolates true drivers of energy anomalies better than simple correlation or thresholding. The system fits autoregressive models to test if lagged values of a predictor variable reduce the prediction error of a target beyond the target's own history. Significant F-statistics result in directed edges. Core assumption: causal relationships manifest with time lag rather than instantaneously. Evidence anchors: abstract mentions Granger tests; section 3.2 describes F-test comparison using p=3 lags. Break condition: if building systems react instantaneously or data is non-stationary without differencing.

### Mechanism 2: Structural Pruning of Indirect Pathways
Raw pairwise tests generate false positives due to transitive correlations; structural pruning enforces sparsity to approximate direct causation. The module searches for intermediate paths and removes edges sufficiently explained by indirect paths (F-statistic ratio < 1.5). Core assumption: the underlying system follows a Directed Acyclic Graph structure where indirect effects should be distinguished from direct physical drivers. Evidence anchors: section 3.2 describes SCM criterion; abstract mentions structural pruning. Break condition: if building system has cyclic feedback loops not unrolled by window size.

### Mechanism 3: Grounded Natural Language Generation via Fine-Tuning
Fine-tuning an LLM on aligned (CauseSet, Explanation) pairs forces the model to translate detected causal graphs into text, reducing hallucinations common in vanilla LLMs. Instead of prompting a generic model with raw data, the model is trained on supervised dataset where input is ranked list of causes and output is structured explanation. Core assumption: syntax of causal graph input sufficiently constrains LLM's output space. Evidence anchors: section 5.4 shows "FT-Only" ablation dropped to 65.3% accuracy; abstract mentions fine-tuning on aligned pairs. Break condition: if LLM is overfitted to specific phrasing in training pairs, it may fail to generalize to novel fault combinations.

## Foundational Learning

- **Concept: Granger Causality**
  - Why needed here: This is the mathematical core of first stage. You must understand that Granger causality is about predictive precedence, not physical manipulation. It detects if signal A happens before signal B in statistically significant way.
  - Quick check question: If heating coil turns on and room temperature rises 30 minutes later, does coil Granger-cause temperature rise? (Yes, provided lag window exceeds 30 minutes).

- **Concept: Supervised Fine-Tuning (SFT)**
  - Why needed here: Paper moves beyond prompting to training. Understanding how to structure dataset (Input: Cause Graph → Output: Text) is required to reproduce second stage.
  - Quick check question: Why would generic LLM fail to explain specific HVAC fault without this fine-tuning? (It lacks domain-specific alignment between sensor names and physical consequences).

- **Concept: Sliding Window Analysis**
  - Why needed here: Causal relationships in buildings are transient. Method does not compute global graph; it computes local graph for 24 hours preceding anomaly.
  - Quick check question: How does choice of window size (w=24h vs w=1h) affect detection of slow-building thermal mass effect? (Short window might miss slow causal chains).

## Architecture Onboarding

- **Component map:** Preprocessing → Causal Inference Module (Granger tests + SCM pruning) → LLM Interface (prompt template → fine-tuned LLaMA 2 7B) → Explanation Output

- **Critical path:** The Causal Inference Module is bottleneck. If Granger test returns empty set of causes (due to insufficient data variance in window), LLM has no signal to translate.

- **Design tradeoffs:**
  - Statistical Rigor vs. Sensitivity: Uses p < 0.05 and 1.5x F-stat pruning threshold. Lowering thresholds would detect more potential causes but increase false positives.
  - Latency vs. Accuracy: 7B parameter model requires significant VRAM (approx 16GB+ in bf16) and inference time, making this unsuitable for sub-second real-time control without optimization.

- **Failure signatures:**
  - Empty Explanations: Input prompt generated but LLM returns "No anomaly detected." -> Check: Did preprocessing z-score threshold filter out anomaly?
  - Contradictory Explanations: LLM says "Occupancy dropped" but causal input said `occupancy ↑`. -> Check: This indicates LLM is hallucinating or fine-tuning alignment failed.
  - High Latency: Explanation takes >10 seconds. -> Check: Batch size or GPU memory constraints on LLM inference engine.

- **First 3 experiments:**
  1. **Lag Sensitivity Analysis:** Vary lag order p (e.g., 1 vs 3 vs 5) in Granger test on Berkeley dataset to see if 15-minute resolution data requires different lags than hourly Google data.
  2. **Ablation on Pruning:** Disable structural pruning step and measure drop in Precision@3 to quantify value of removing indirect edges.
  3. **Hallucination Stress Test:** Feed fine-tuned LLM synthetic anomaly with contradictory causes (e.g., `damper_closed` AND `energy_low`) to test if it hallucinates plausible story or flags contradiction.

## Open Questions the Paper Calls Out
- Can the framework support real-time streaming with online causal updates? (Future work explicitly listed)
- Does integrating physics-based domain knowledge improve causal discovery performance? (Proposed as future direction)
- Is the framework transferable to other cyber-physical systems? (Future work includes adapting to data centers and manufacturing plants)

## Limitations
- Current architecture relies on fixed sliding windows and offline fine-tuning on historical data, limiting real-time applicability
- System has only been validated on building energy datasets, leaving efficacy in other operational contexts unproven
- Fine-tuning requires substantial expert-annotated data (2,500 examples), creating data collection burden

## Confidence
- High confidence in Granger causality implementation based on standard statistical methodology
- Medium confidence in structural pruning heuristic due to limited external validation in provided corpus
- High confidence in overall framework efficacy based on significant performance improvements over baselines

## Next Checks
1. Verify Granger causality test implementation with synthetic time series data where ground truth lags are known
2. Test structural pruning algorithm on benchmark causal discovery datasets to validate indirect pathway removal
3. Conduct ablation study removing LLM fine-tuning to quantify contribution of specialized training data