---
ver: rpa2
title: 'MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and
  Completion'
arxiv_id: '2510.13702'
source_url: https://arxiv.org/abs/2510.13702
tags:
- multi-view
- camera
- customization
- feature
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MVCustom, a novel framework for multi-view
  customization that jointly achieves camera pose control and subject customization
  with geometric consistency. Existing methods struggle to preserve identity and perspective
  alignment across views when generating personalized content.
---

# MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion

## Quick Facts
- arXiv ID: 2510.13702
- Source URL: https://arxiv.org/abs/2510.13702
- Reference count: 26
- Primary result: Only method to simultaneously achieve faithful multi-view generation and customization, with significant improvements in camera pose accuracy (0.735±0.101) and multi-view consistency (0.121±0.104) over baselines.

## Executive Summary
This paper introduces MVCustom, a novel framework for multi-view customization that jointly achieves camera pose control and subject customization with geometric consistency. Existing methods struggle to preserve identity and perspective alignment across views when generating personalized content. To address this, the authors propose a diffusion-based approach that combines a video diffusion backbone with dense spatio-temporal attention, enabling temporal coherence to be transferred into multi-view consistency. Two novel inference techniques—depth-aware feature rendering and consistent-aware latent completion—enforce geometric consistency and complete disoccluded regions. Experiments show MVCustom is the only method to simultaneously achieve faithful multi-view generation and customization, with significant improvements in camera pose accuracy (0.735±0.101) and multi-view consistency (0.121±0.104) over baselines. The framework enables robust, controllable, and personalized multi-view content generation.

## Method Summary
MVCustom modifies video diffusion with dense spatio-temporal attention to enable coherent feature propagation across viewpoints. It integrates FeatureNeRF into transformer blocks for subject customization, then enforces geometric consistency during inference through depth-aware feature rendering (using ZoeDepth and mesh rendering) and latent completion for disoccluded regions. The method fine-tunes on WebVid10M and customizes on CO3Dv2, achieving simultaneous multi-view consistency and subject customization through a progressive training approach and specialized inference pipeline.

## Key Results
- Achieves camera pose accuracy of 0.735±0.101, significantly outperforming baselines (CustomDiffusion360: 0.413±0.056, Dream3D: 0.285±0.034)
- Attains multi-view consistency of 0.121±0.104, demonstrating superior geometric alignment across viewpoints
- Only method that simultaneously achieves both faithful multi-view generation and subject customization
- Enables generation of consistent multi-view content while preserving subject identity and following text prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense 3D spatio-temporal attention enables coherent feature propagation across viewpoints, which 1D temporal attention cannot achieve.
- Mechanism: Standard 1D temporal attention operates only at identical spatial positions across frames. The authors progressively expand the spatial attention field during training, allowing pixels at different spatial locations across views to attend to each other. This transfers video temporal coherence into multi-view spatial coherence.
- Core assumption: Viewpoint changes induce spatial displacements that require cross-position attention to model correctly.
- Evidence anchors:
  - [abstract] "dense spatio-temporal attention, enabling temporal coherence to be transferred into multi-view consistency"
  - [section 3.3] "AnimateDiff's 1D temporal attention limits its interactions to identical spatial positions, hindering effective modeling of viewpoint-induced displacements"
  - [corpus] Weak direct corpus support; neighbor papers focus on 3D Gaussian splatting rather than attention mechanisms.
- Break condition: If camera motion involves pure rotation without translation, 1D attention may suffice; dense attention adds overhead without benefit.

### Mechanism 2
- Claim: Rendering features from an anchor frame's depth-derived mesh into other views explicitly enforces geometric consistency for the surrounding scene.
- Mechanism: Given an anchor frame, estimate depth, construct a 3D mesh with features as texture, render this mesh from other target camera poses, and replace masked regions in the denoising U-Net features with rendered features during the first 35 of 50 DDIM steps.
- Core assumption: Monocular depth estimation is accurate enough to construct a useful proxy mesh, and depth scale can be aligned with FeatureNeRF's learned geometry via median depth matching.
- Evidence anchors:
  - [abstract] "depth-aware feature rendering explicitly enforces geometric consistency"
  - [section 3.4] "During the first 35 steps of the 50-step DDIM sampling process, we update each feature map by replacing masked regions with rendered anchor features"
  - [corpus] GSV3D and related papers use depth/Gaussian splatting for geometry, supporting the general approach but not this specific mechanism.
- Break condition: If depth estimation fails on stylized/artistic content, or if the anchor frame lacks coverage of key scene regions, geometric consistency degrades.

### Mechanism 3
- Claim: Stochastic latent perturbation enables natural, diverse completion of disoccluded regions while maintaining temporal coherence.
- Mechanism: For newly visible regions not covered by the anchor mesh (1−M_mask > 0), predict a clean latent x_0, re-noise it via forward diffusion to timestep t, extract the disoccluded region from this perturbed latent, and blend it back. The video backbone's temporal attention ensures coherence across frames.
- Core assumption: The diffusion prior contains sufficient semantic knowledge to inpaint disocclusions meaningfully; stochasticity produces diverse but coherent results.
- Evidence anchors:
  - [abstract] "consistent-aware latent completion ensures accurate perspective alignment"
  - [section 3.4] "we reintroduce noise into x_0 via the forward diffusion process...enforcing spatial coherence across frames through the temporal consistency of the video backbone"
  - [corpus] No direct corpus evidence for this specific latent completion technique.
- Break condition: If disoccluded regions are large (>40% of frame) or require specific semantic content, stochastic completion may produce implausible results.

## Foundational Learning

- Concept: **Neural Radiance Fields (NeRF) and Feature Fields**
  - Why needed here: The multi-view branch uses FeatureNeRF to synthesize pose-aligned feature maps from reference images via epipolar geometry and volume rendering.
  - Quick check question: Can you explain how epipolar geometry constrains which reference pixels contribute to a target ray?

- Concept: **DDPM vs DDIM Sampling**
  - Why needed here: Primary inference uses deterministic DDIM (ODE), while latent completion uses stochastic DDPM forward process (SDE) to introduce diversity.
  - Quick check question: What is the trade-off between deterministic and stochastic sampling in diffusion models?

- Concept: **Temporal Attention in Video Diffusion**
  - Why needed here: Understanding 1D vs 3D attention is critical; the paper modifies AnimateDiff's sparse temporal attention to dense spatio-temporal attention.
  - Quick check question: In 1D temporal attention, which dimensions interact and which are held fixed?

## Architecture Onboarding

- Component map:
  - Main Branch (U-Net with pose-conditioned transformer blocks) -> Multi-view Branch (FeatureNeRF) -> Spatio-temporal Transformer (STT) -> Depth-aware Feature Rendering Module -> Latent Completion Module

- Critical path:
  1. Training: Video backbone (STT) → Customization (FeatureNeRF + textual inversion)
  2. Inference: DDIM sampling → Feature replacement (steps 1-35) → Latent completion (steps τ to T)

- Design tradeoffs:
  - FeatureNeRF placement: 7/16 transformer blocks balances identity preservation vs. text responsiveness (Sec A.2).
  - Anchor frame selection: Single anchor limits coverage; multi-anchor would increase memory/compute.
  - Replacement cutoff (35/50 steps): Earlier preserves geometry but reduces generative flexibility; later increases realism risks geometric drift.

- Failure signatures:
  - **Pose accuracy = 0**: COLMAP reconstruction failed; indicates severe multi-view inconsistency (see CustomDiffusion360 in Table 2).
  - **Static surroundings under camera motion**: Depth-aware feature rendering not activated or depth estimation failed (Fig 5a-i).
  - **Repetitive artifacts in disocclusions**: Latent completion not applied or τ set too late, causing rendered features to bleed inappropriately.

- First 3 experiments:
  1. **Validate STT vs 1D attention**: Run ablation with vertical feature shift insertion (Fig 5b setup); verify spatial flow preservation.
  2. **Calibrate depth scale alignment**: Test median depth grid search (±40% range) on held-out views; measure alignment error between FeatureNeRF-rendered and depth-mesh-rendered objects.
  3. **Probe completion diversity**: Generate multiple seeds for the same prompt/pose; verify disoccluded regions show semantic variation (Fig 7 protocol).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-view customization frameworks be extended to handle substantial variations in object pose, such as transitioning from sitting to standing, while maintaining identity?
- Basis in paper: [explicit] Appendix C explicitly identifies the inability to handle large pose variations as a limitation, noting that the identity is tied to a single canonical pose via FeatureNeRF.
- Why unresolved: The current method binds subject identity to a static radiance field derived from reference images with consistent poses, preventing structural adjustments based on text prompts.
- What evidence would resolve it: A dynamic network or hypernetwork approach that successfully modifies the radiance field's pose to match novel textual descriptions without losing identity fidelity.

### Open Question 2
- Question: Can the MVCustom framework be adapted to DiT-based diffusion architectures despite challenges with frame-level feature consistency?
- Basis in paper: [inferred] Appendix A.1 states the authors utilized a U-Net backbone because DiT models use Conv3D patchification, which merges spatial and temporal dimensions, making accurate frame-level camera pose conditioning difficult.
- Why unresolved: The specific mechanism for pose conditioning (FeatureNeRF) relies on explicit per-frame feature maps, which are obscured in standard DiT architectures.
- What evidence would resolve it: A modified DiT architecture or conditioning mechanism that maintains camera pose accuracy comparable to the U-Net baseline.

### Open Question 3
- Question: How can the trade-off between identity preservation and text alignment sensitivity regarding the number of FeatureNeRF modules be optimized?
- Basis in paper: [inferred] Appendix A.2 discusses a manual trade-off where increasing FeatureNeRF modules improves identity but degrades responsiveness to new text prompts, requiring a fixed compromise (7 out of 16 blocks).
- Why unresolved: The projection layers become biased toward the reference branch as more modules are added, suggesting a conflict in the current conditioning strategy.
- What evidence would resolve it: A method that decouples identity strength from text alignment, removing the need for a manually tuned "sweet spot" for module quantity.

## Limitations
- Dense spatio-temporal attention mechanism lacks detailed implementation specifics, making exact reproduction challenging
- Reliance on ZoeDepth for monocular depth estimation may limit performance on stylized or artistic content
- Single-anchor frame approach for feature rendering may struggle with scenes requiring multiple viewpoints for complete coverage
- Stochastic latent completion mechanism's diversity is difficult to quantify and control, potentially leading to inconsistent results across seeds
- Computational overhead from feature rendering and dense attention is not fully characterized

## Confidence
- **High confidence**: The core contribution of integrating FeatureNeRF with video diffusion for multi-view customization is well-established and empirically validated through quantitative metrics (Pose Accuracy, Multi-view Consistency)
- **Medium confidence**: The effectiveness of depth-aware feature rendering depends heavily on the quality of monocular depth estimation, which is not extensively evaluated across diverse content types
- **Medium confidence**: The latent completion mechanism's ability to produce semantically coherent disocclusions relies on unstated assumptions about the diffusion prior's knowledge of novel regions
- **Low confidence**: The exact implementation details of dense spatio-temporal attention and the grid search for depth alignment are insufficiently specified for exact replication

## Next Checks
1. **Ablation of spatio-temporal attention**: Systematically test the impact of replacing 1D temporal attention with the proposed dense 3D attention by inserting controlled viewpoint shifts and measuring spatial coherence preservation compared to the baseline AnimateDiff.

2. **Depth alignment calibration**: Conduct a controlled experiment varying the median depth scaling factor across the ±40% range on held-out views, measuring the geometric alignment error between FeatureNeRF-rendered and depth-mesh-rendered objects to validate the calibration procedure.

3. **Latent completion diversity analysis**: Generate multiple stochastic samples for identical prompts and poses with significant disocclusions, then perform semantic clustering to quantify the diversity and coherence of completed regions, verifying that the stochastic process produces meaningful variations.