---
ver: rpa2
title: 'Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits
  and Effectiveness'
arxiv_id: '2506.13479'
source_url: https://arxiv.org/abs/2506.13479
tags:
- lora
- loras
- routing
- arrow
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that combining low-rank adapters (LoRAs)
  for zero-shot generalization to novel tasks is largely ineffective, except when
  the target task is already represented in the fine-tuning data. Through theoretical
  analysis and empirical studies on synthetic two-hop reasoning and math word problems,
  the authors show that LoRA merging or routing typically fails to achieve true compositional
  generalization, instead reflecting shallow pattern matching.
---

# Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness

## Quick Facts
- arXiv ID: 2506.13479
- Source URL: https://arxiv.org/abs/2506.13479
- Authors: Mei-Yen Chen; Thi Thu Uyen Hoang; Michael Hahn; M. Saquib Sarfraz
- Reference count: 40
- Primary result: LoRA combination typically fails compositional generalization without explicit bridge training

## Executive Summary
This position paper argues that combining low-rank adapters (LoRAs) for zero-shot generalization to novel tasks is largely ineffective, except when the target task is already represented in the fine-tuning data. Through theoretical analysis and empirical studies on synthetic two-hop reasoning and math word problems, the authors show that LoRA merging or routing typically fails to achieve true compositional generalization, instead reflecting shallow pattern matching. Their results demonstrate that success depends critically on entity familiarity, domain-specific pretraining, and close matches between training and testing prompts.

## Method Summary
The authors test LoRA composition using synthetic two-hop reasoning datasets (F/H/R variants with controlled entity familiarity) and math word problems (GSM-Symbolic progression). They train separate LoRAs on individual reasoning steps using rank-16 adapters targeting MLP layers, then combine them via Uniform averaging or Arrow routing. Bridge variants include CoT-formatted two-hop examples during training. The theoretical analysis proves why simple LoRA addition fails for compositional tasks in single-layer transformers.

## Key Results
- LoRA combination without bridge training achieves <10% accuracy on two-hop reasoning
- Bridge training with CoT-formatted examples increases accuracy to 73.5%-95.1%
- Success requires explicit exposure to the target reasoning pattern in fine-tuning data
- MLP-layer fine-tuning preserves more reasoning capability than attention-layer fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct combination of LoRAs trained on separate subtasks fails to achieve compositional generalization
- Mechanism: LoRA updates target specific low-dimensional subspaces defined by training prompt activations. When combining adapters ∆W_r1→r̃1 and ∆W_r2→r̂2, the model outputs a linear mixture of relevant entities rather than the composed result r2(r1(x)), because the two-hop prompt activation overlaps both one-hop subspaces without computing functional composition
- Core assumption: Transformers approximate key-value storage in MLP layers; LoRA rank is small relative to model width
- Evidence anchors:
  - [abstract] "reusing LoRAs often fails to logically integrate knowledge across disjoint fine-tuning datasets"
  - [Section 3.2, Theorem 2] "Summing these adapters will not result in correct results for composition of the two relations r1, r2" with formal proof showing adapters contribute multiples of iy - ir1(x) and iz - ir2(y) rather than iz - ir2(r1(x))
  - [Section 4.1.1, Figure 1] "with only A→B and B→C adapters (2-combination library), accuracy stays below 10%"
  - [corpus] Related work (K-Merge, HiLoRA) assumes composability but lacks empirical validation of failure modes
- Break condition: When target task is included in training data (3-combination library with oracle expert)

### Mechanism 2
- Claim: Compositional generalization via LoRA combination requires explicit exposure to the target reasoning pattern in fine-tuning data (bridge training)
- Mechanism: Including CoT-formatted examples of two-hop reasoning in both LoRA adapters' training data enables the model to recognize and route to the compositional pattern. The bridge dataset must contain (1) individual subtasks F1, F2 AND (2) CoT-formatted F12 examples, and must be present in both adapters
- Core assumption: Familiarity with reasoning templates transfers across entity pairs; template overlap between train/test is the bridging signal
- Evidence anchors:
  - [abstract] "success depends critically on...close matches between training and testing prompts"
  - [Section 4.1.2, Table 1] Setup 2 (both LoRAs trained with bridge) achieves 73.5%-95.1% vs. <10% without bridge; Setup 3 (bridge without CoT formatting) drops to 1%-11.8%
  - [Section 4.1.2] "the bridge improves two-hop accuracy only when CoT-formatted A→C instances are included during adapter training"
  - [corpus] Weak direct evidence; related papers assume rather than test this mechanism
- Break condition: Entity unfamiliarity (fake names/locations in F dataset) reduces bridge effectiveness; bridge in only one adapter (Setups 4-5) yields <35% accuracy

### Mechanism 3
- Claim: LoRA routing effectiveness depends on alignment between base model pretraining, fine-tuning data format, and target task characteristics
- Mechanism: Domain-specialized models (e.g., Qwen2.5-Math) develop tool-integrated reasoning (TIR) abilities during pretraining. Fine-tuning LoRAs on natural language CoT solutions suppresses these abilities, degrading performance. MLP-layer fine-tuning preserves more capability than attention-layer fine-tuning
- Core assumption: Pretrained capabilities are stored in specific parameter subspaces; LoRA updates can interfere with these subspaces
- Evidence anchors:
  - [abstract] "especially when such knowledge is underrepresented during pretraining"
  - [Section 4.2, Table 14] After routing natural-language LoRAs, Qwen2.5-Math-1.5B-Inst produces 0% code-based solutions (vs. 12% baseline) and 56% calculation errors
  - [Section 4.2, Table 3] MLP-targeted fine-tuning with reusable code improves Qwen2.5-Math-1.5B-Inst from 14% to 20% on GSM-P2
  - [corpus] Assumption: related works do not analyze base model alignment effects
- Break condition: Misaligned fine-tuning data format; routing to attention layers instead of MLP layers

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: The entire paper analyzes LoRA combination mechanics; understanding that LoRA adds learnable low-rank matrices ∆W = AB^T to pretrained weights is essential for grasping why composition fails
  - Quick check question: Can you explain why a rank-16 update has limited expressive power compared to full fine-tuning?

- Concept: **Compositional Generalization**
  - Why needed here: The paper's core question is whether combining LoRAs enables compositional generalization (novel combinations of known elements) or just shallow pattern matching
  - Quick check question: If a model knows A→B and B→C, what would true compositional generalization predict for A→C?

- Concept: **Transformer MLP as Key-Value Storage**
  - Why needed here: The theoretical analysis builds on the mechanistic interpretation that MLP layers store factual associations, explaining why MLP-targeted LoRA updates affect factual recall
  - Quick check question: Why might modifying MLP layers be more effective for factual knowledge than modifying attention layers?

## Architecture Onboarding

- Component map:
  - Base model: Pretrained LLM (Qwen2.5, DeepSeek-R1-Distill variants tested)
  - LoRA adapters: Low-rank matrices trained on specific tasks (rank 16, targeting MLP or attention layers)
  - Routing mechanism: Uniform (simple averaging), Arrow (SVD-based similarity routing), or task-aware methods (CAT requires held-out data)
  - Bridge dataset: Training augmentation containing CoT examples of target compositional pattern

- Critical path:
  1. Define target compositional task requirements upfront
  2. Identify required subskills and verify entity/relation coverage in pretraining
  3. Curate bridge training data with CoT exemplars of target reasoning pattern
  4. Train LoRAs with bridge included in all relevant adapters
  5. Target MLP layers for fine-tuning
  6. Test on held-out templates before deployment

- Design tradeoffs:
  - Data-agnostic routing (Uniform, Arrow) vs. task-aware (CAT): Former is truly zero-shot but fails compositionally; latter requires held-out data
  - MLP vs. attention fine-tuning: MLP preserves more reasoning capability but may be task-dependent
  - Bridge with same vs. different entity pairs: Same entities risk overfitting; different entities risk insufficient transfer

- Failure signatures:
  - Two-hop accuracy <10% indicates missing bridge training
  - Sudden drop in code-based solutions indicates fine-tuning/base model misalignment
  - High calculation errors (>50%) with correct reasoning steps indicates LoRA suppression of computational abilities
  - Performance degrades with model size suggests entity unfamiliarity (F dataset pattern)

- First 3 experiments:
  1. Replicate two-hop reasoning with your domain's entities: Train LoRAs on A→B and B→C separately, test A→C without bridge. Expected: <10% accuracy. This establishes baseline failure mode.
  2. Add bridge training with CoT examples using disjoint entities: Include F12_CoT (or domain equivalent) in both adapters. Expected: 70-90% accuracy if entities are familiar. This validates bridge mechanism.
  3. Test MLP vs. attention fine-tuning on your target task: Compare accuracy when targeting only MLP layers vs. only attention layers. Expected: MLP targeting superior for reasoning tasks. This identifies optimal adapter placement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the theoretical limitations of LoRA composition for single-layer transformers extend to multi-layer architectures in practice?
- Basis in paper: [explicit] The authors state: "One limitation of our theoretical analysis is that (in line with Nichani et al. [2025]) it applies to a single-layer transformer; our experiments test applicability of the conclusions to LLMs across scales."
- Why unresolved: The theoretical proof only covers single-layer models, leaving uncertainty about whether compositional failure is inherent to LoRA or specific to the simplified theoretical setup.
- What evidence would resolve it: Formal analysis of LoRA composition in multi-layer transformers, or systematic experiments showing consistent failure patterns across varying model depths.

### Open Question 2
- Question: Can alternative PEFT methods (LoRI, LoRA Lego, Self-MoE, FLiX, CS-ReFT) achieve genuine compositional generalization where standard LoRA routing fails?
- Basis in paper: [explicit] "Alternative PEFT methods may offer better compositional results... However, positive results for compositional reasoning have not been reported, and our theoretical analysis suggests that it remains challenging."
- Why unresolved: These methods introduce mechanisms like random projections, task-specific masks, or trained routers, but no systematic comparison exists for compositional reasoning tasks.
- What evidence would resolve it: Direct benchmarking of these methods on the synthetic two-hop reasoning and easy-to-hard math tasks used in this paper.

### Open Question 3
- Question: What are the minimal sufficient conditions in fine-tuning data to enable compositional LoRA generalization?
- Basis in paper: [explicit] The bridge experiments show composition requires "CoT-formatted A → C instances" and exposure to "each subtask," but the exact requirements remain unclear. Setup 8 shows "exact task-pair matching is less critical so long as the finetuning set contains examples reflecting the overall reasoning pattern."
- Why unresolved: The paper demonstrates that partial bridging strategies work but does not characterize the boundary conditions or minimal data requirements.
- What evidence would resolve it: Systematic ablations varying the number, diversity, and semantic relatedness of bridge examples to identify the critical threshold for successful composition.

## Limitations

- Theoretical scope limited to single-layer transformers and specific assumptions about MLP behavior
- Synthetic datasets may not capture full complexity of real-world compositional tasks
- Results show strong dependence on base model pretraining and fine-tuning data alignment

## Confidence

- High confidence: The failure of LoRA combination to achieve compositional generalization without bridge training is well-established through both theoretical analysis and empirical results
- Medium confidence: The mechanistic explanation of why composition fails is theoretically sound but relies on specific assumptions about MLP layer behavior
- Low confidence: The generalizability of the math word problem results to other domains requiring compositional reasoning is uncertain

## Next Checks

**Validation Check 1**: Test bridge effectiveness across diverse compositional domains
- Apply the bridge training mechanism to a completely different compositional task (e.g., multi-step reasoning in legal reasoning, scientific inference, or code generation)
- Vary bridge format (CoT vs. step-by-step vs. other reasoning patterns) and measure effectiveness
- Compare performance when bridge uses same entities vs. different entities vs. no bridge

**Validation Check 2**: Characterize the pretraining data requirements for successful LoRA composition
- Systematically vary the overlap between pretraining data and fine-tuning data for the bridge components
- Measure at what point bridge training becomes effective as a function of pretraining coverage
- Test with models having different pretraining corpora (general vs. domain-specific)

**Validation Check 3**: Investigate alternative routing mechanisms for compositional tasks
- Implement and test CAT routing (requiring held-out data) on the same compositional tasks
- Compare Uniform, Arrow, and CAT routing effectiveness across different task types
- Analyze whether routing can recover some compositional ability even with limited bridge data