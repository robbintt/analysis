---
ver: rpa2
title: Efficient Reasoning via Reward Model
arxiv_id: '2511.09158'
source_url: https://arxiv.org/abs/2511.09158
tags:
- reward
- conciseness
- reasoning
- training
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency problem in large reasoning
  models (LRMs) caused by verbose and redundant reasoning paths ("overthinking").
  Prior methods using length penalties often suffer from length collapse and training
  collapse.
---

# Efficient Reasoning via Reward Model

## Quick Facts
- arXiv ID: 2511.09158
- Source URL: https://arxiv.org/abs/2511.09158
- Reference count: 32
- Primary result: 8.1% accuracy improvement and 19.9% token reduction on mathematical benchmarks vs GRPO baseline

## Executive Summary
This paper addresses the inefficiency problem in large reasoning models caused by verbose and redundant reasoning paths ("overthinking"). Prior methods using length penalties often suffer from length collapse and training collapse. The authors propose a two-step framework: first train a conciseness reward model (CRM) that evaluates reasoning path conciseness across multiple dimensions, then use a conciseness reward function (CRF) that explicitly depends on both outcome reward and conciseness score. Extensive experiments on five mathematical benchmarks show 8.1% accuracy improvement and 19.9% reduction in token length compared to original GRPO.

## Method Summary
The method involves training a conciseness reward model (CRM) using a 3B-parameter model fine-tuned on LLM-generated concise/redundant solution pairs with multi-factor scoring (repetition avoidance, step relevance, token efficiency). The CRM is then integrated into GRPO training via a CRF that applies conciseness penalties only when answers are correct, with annealing and difficulty coefficients to stabilize training. The approach uses a conditional reward structure R̂_i = R^o_i · [1 + α · c_i · (s + d_q)] where c_i is the CRM score, s is annealing, and d_q is difficulty-based scaling.

## Key Results
- 8.1% accuracy improvement and 19.9% token reduction on five mathematical benchmarks with Qwen2.5-7B
- CRF mitigates reward hacking compared to additive length penalties (9.8% outcome reward drop when dependency removed)
- Method generalizes across different model backbones (Llama3.1-8B, Mistral-7B) with appropriate hyperparameter tuning
- Theoretical advantages include variance reduction and improved convergence rate in optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CRM provides multi-dimensional conciseness evaluation that avoids failures of simple token-counting penalties.
- Mechanism: A 3B-parameter model is fine-tuned on 65,878 samples labeled by a 72B teacher across three factors: repetition avoidance, step relevance, and token efficiency, producing dense scalar signals (0.1-1.0).
- Core assumption: A 72B teacher model can reliably discriminate concise vs. verbose reasoning paths.
- Evidence anchors: Abstract mentions multi-dimensional evaluation; Section 3.2 describes the four-step pipeline with DeepMath-103K selection and dual-solution generation.

### Mechanism 2
- Claim: Explicit dependency structure in CRF mitigates reward hacking compared to additive formulations.
- Mechanism: Reward is formulated as R̂_i = R^o_i · [1 + α · c_i · (s + d_q)], applying conciseness only when outcome reward = 1.
- Core assumption: Product formulation's conditional gating prevents collapse; variance reduction follows from positive covariance.
- Evidence anchors: Abstract states explicit dependency; Section 4.4 ablation shows 9.8% outcome reward drop when dependency replaced with weighted sum.

### Mechanism 3
- Claim: Annealing and difficulty coefficients jointly stabilize training and adapt length penalties to problem hardness.
- Mechanism: Annealing coefficient s = exp(-step/T) reduces conciseness weight over training; difficulty coefficient d_q = exp(|{i|R^o_i=1}|/G) applies weaker penalties to harder questions.
- Core assumption: Harder questions legitimately require longer solutions; penalizing equally leads to premature truncation.
- Evidence anchors: Section 3.3 notes questions with difficulty 6.5-7 are 15.0% longer; Section 4.4 ablation shows 3.9% and 4.9% performance drops for w/o Ann and w/o Dif.

## Foundational Learning

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: CRF is implemented as a drop-in replacement for the reward function in GRPO. Understanding advantage computation A_i,t = (R_i - μ_R) / (σ_R + ε') is essential for debugging reward shaping.
  - Quick check question: Given a group of 4 responses with rewards [0, 1, 0, 1], what are their advantages?

- Concept: **Reward hacking / specification gaming**
  - Why needed here: The paper diagnoses "length collapse" and "training collapse" as forms of reward hacking induced by additive length penalties.
  - Quick check question: Why would a model trained with R = correctness - 0.1 * length potentially learn to output just "42" for every math problem?

- Concept: **Cold-start problem in RLVR**
  - Why needed here: For Llama3.1-8B and Mistral-7B, direct RL training fails; SFT warmstart is required.
  - Quick check question: If you observe near-zero reward curves in the first 50 steps of GRPO training on a new backbone, what intervention should you try first?

## Architecture Onboarding

- Component map: DeepMath-103K -> Qwen2.5-Math-72B-Instruct (dual-solution generation) -> Qwen2.5-72B-Instruct (scoring) -> Filter -> SFT on Qwen2.5-3B-Instruct -> CRF-GRPO Training

- Critical path:
  1. CRM quality determines reliability of conciseness signals
  2. Hyperparameter α controls accuracy-efficiency tradeoff (α=1.0 for Qwen2.5-7B, α=0.5 for Llama/Mistral)
  3. Difficulty coefficient requires group sampling (G responses per query) during training

- Design tradeoffs:
  - CRM backbone size: 3B chosen for efficiency; larger models may improve scoring quality at inference cost
  - Annealing schedule: exp(-step/T) provides smooth decay; faster decay risks premature convergence
  - Group size G: Larger G improves difficulty estimation but increases compute per step

- Failure signatures:
  - Length collapse: Token count drops rapidly while outcome reward rises → model has learned to memorize answers
  - Training collapse: Both reward and token count crash → reward signal has become too punitive
  - Cold-start failure: Reward curve flat near zero for Llama/Mistral → requires SFT warmstart

- First 3 experiments:
  1. Reproduce CRM training: Sample 1000 examples from DeepMath-103K, run labeling pipeline, train mini-CRM, verify score calibration
  2. CRF ablation on small scale: Train Qwen2.5-7B on MATH-500 subset with (a) vanilla GRPO, (b) additive length penalty, (c) CRF
  3. Cross-backbone sanity check: Apply CRM and α=0.5 to both Llama3.1-8B and Mistral-7B with SFT warmstart

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the CRM trained on mathematical data generalize effectively to non-mathematical reasoning domains such as code generation, logical deduction, or scientific reasoning?
- Basis in paper: Section 3.4 states framework "is adaptable to different human-specified objectives" but evaluation is limited to five mathematical benchmarks.
- Why unresolved: CRM scoring criteria are trained and validated solely on mathematical solutions.
- What evidence would resolve it: Experiments applying trained CRM to benchmarks like HumanEval (code), LogiQA (logical reasoning), or SciBench (scientific reasoning).

### Open Question 2
- Question: Under what conditions does the theoretical assumption that conciseness reward correlates positively with outcome reward (Cov(R⁰_i, c_i) > 0) fail, and how does such failure affect training stability?
- Basis in paper: Proposition 1's variance reduction proof explicitly requires "Cov(R⁰_i, c_i) > 0" but this correlation is not empirically verified.
- Why unresolved: If concise reasoning paths systematically produce incorrect answers for certain problem classes, covariance could become negative.
- What evidence would resolve it: Empirical measurement of Cov(R⁰_i, c_i) across training steps and problem difficulty levels.

### Open Question 3
- Question: How does CRM-based training scale to larger policy models (70B+ parameters), and does the 3B CRM remain sufficiently discriminative for evaluating more capable models' reasoning paths?
- Basis in paper: All experiments use 7B-8B policy models with a 3B CRM; paper does not discuss whether smaller reward model can reliably score reasoning from substantially more capable models.
- Why unresolved: Larger policy models may generate reasoning patterns that exceed CRM's ability to distinguish genuine conciseness from sophisticated multi-step reasoning.
- What evidence would resolve it: Experiments training 70B policy models with current 3B CRM versus scaled-up CRMs.

## Limitations
- CRM generalization across domains: The 3B CRM is trained only on mathematical reasoning data; effectiveness on coding, logical reasoning, or scientific domains is unknown.
- Backbone-specific tuning: The α hyperparameter requires per-backbone calibration (1.0 for Qwen2.5-7B vs 0.5 for Llama/Mistral), suggesting limited transferability.
- Theoretical validation gaps: The claimed variance reduction benefits lack empirical demonstration, and failure conditions (negative covariance) are not tested.

## Confidence

- **High Confidence**: Experimental results showing accuracy improvements (8.1% on Qwen2.5-7B) and token reductions (19.9%) are directly measured and reported. CRM training pipeline and CRF implementation details are sufficiently specified for reproduction.

- **Medium Confidence**: Theoretical advantages of explicit dependency structure (variance reduction, improved convergence) are mathematically sound but lack empirical validation. Difficulty coefficient's effectiveness depends on accurate group accuracy estimation.

- **Low Confidence**: CRM's scoring reliability across diverse reasoning tasks beyond DeepMath-103K is unknown. Method's performance on non-mathematical reasoning tasks has not been tested.

## Next Checks

1. **CRM Quality Validation**: Generate held-out test set of 500 reasoning paths with human annotations for conciseness. Compare CRM scores against human judgments using correlation analysis and compute calibration curves.

2. **Reward Function Stress Test**: Train model with deliberately corrupted CRM scores (random noise added) to verify explicit dependency structure prevents length collapse. Compare against additive formulations under same noise conditions.

3. **Cross-Domain Generalization**: Apply trained CRM and CRF to non-mathematical reasoning benchmark (e.g., HumanEval for coding, or MMLU for general QA). Measure whether accuracy-token length tradeoff generalizes beyond mathematical reasoning.