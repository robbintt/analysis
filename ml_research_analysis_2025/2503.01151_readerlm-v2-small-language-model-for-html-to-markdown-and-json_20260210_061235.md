---
ver: rpa2
title: 'ReaderLM-v2: Small Language Model for HTML to Markdown and JSON'
arxiv_id: '2503.01151'
source_url: https://arxiv.org/abs/2503.01151
tags:
- data
- html
- content
- extraction
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReaderLM-v2 is a 1.5 billion parameter language model designed
  for efficient web content extraction, capable of processing documents up to 512K
  tokens. The model addresses the challenge of converting messy HTML into clean Markdown
  or JSON formats with high accuracy while maintaining significantly lower computational
  requirements than larger models.
---

# ReaderLM-v2: Small Language Model for HTML to Markdown and JSON

## Quick Facts
- arXiv ID: 2503.01151
- Source URL: https://arxiv.org/abs/2503.01151
- Reference count: 11
- A 1.5B parameter model that processes documents up to 512K tokens with 15-20% better accuracy than GPT-4o on web content extraction.

## Executive Summary
ReaderLM-v2 is a 1.5 billion parameter language model specifically designed for efficient web content extraction from HTML to clean Markdown or JSON formats. The model addresses the challenge of converting messy web content into structured formats while maintaining significantly lower computational requirements than larger models. Through a novel three-stage data synthesis pipeline called Draft-Refine-Critique and a comprehensive training strategy, ReaderLM-v2 achieves state-of-the-art performance on carefully curated benchmarks, outperforming GPT-4o by 15-20% while being capable of processing documents up to 512K tokens.

## Method Summary
ReaderLM-v2 employs a four-stage training approach built on the Qwen2.5-1.5B-Instruct base model. The pipeline begins with continuous pre-training using progressive context extension from 32K to 256K tokens with ring-zag attention and RoPE base frequency of 5M. This is followed by supervised fine-tuning with four specialized checkpoints using contrastive loss and linear weight merging, direct preference optimization for reinforcement learning, and self-play iterative tuning. The training data comes from WebMarkdown-1M (1M Common Crawl URLs) and synthetic data generated through the Draft-Refine-Critique pipeline, producing 250K SFT pairs, 100K critique examples, and 150K DPO preference triplets.

## Key Results
- Achieves Rouge-L score of 0.86 for main content extraction, representing a 24.6% improvement over GPT-4o
- Scores 0.72 for instructed extraction while maintaining strong JSON extraction with F1-scores of 0.81-0.84
- Processes documents up to 512K tokens with 15-20% better accuracy than GPT-4o on carefully curated benchmarks

## Why This Works (Mechanism)
The model's success stems from its specialized training pipeline that addresses the unique challenges of web content extraction. The Draft-Refine-Critique data synthesis generates high-quality training pairs by iteratively improving extraction quality through multiple passes. The progressive context extension curriculum allows the model to handle extremely long documents by gradually increasing sequence length during training. The four-stage training strategy combines supervised learning, reinforcement learning from human feedback, and self-improvement cycles to optimize both accuracy and efficiency.

## Foundational Learning
- Ring-zag attention: Why needed - Efficiently handles extremely long sequences by alternating attention patterns; Quick check - Verify alternating attention masks in implementation
- RoPE base frequency tuning: Why needed - Enables proper extrapolation beyond training context lengths; Quick check - Confirm RoPE base set to 5M in configuration
- Contrastive loss in SFT: Why needed - Prevents repetitive token generation and improves diversity; Quick check - Monitor repetition rate on validation set
- Progressive length curriculum: Why needed - Allows smooth transition to 512K context without catastrophic forgetting; Quick check - Verify 40% long sequences in training mix
- Direct preference optimization: Why needed - Refines model preferences through pairwise comparisons; Quick check - Confirm DPO triplet format in dataset
- Self-play iterative tuning: Why needed - Enables model to generate and refine its own training data; Quick check - Track performance after each self-play cycle

## Architecture Onboarding

**Component Map:** Qwen2.5-1.5B-Instruct -> Stage 1 Pre-training -> Stage 2 SFT -> Stage 3 DPO -> Stage 4 Self-play

**Critical Path:** Data synthesis (Draft-Refine-Critique) -> Progressive context extension (Stage 1) -> Contrastive SFT (Stage 2) -> DPO refinement (Stage 3) -> Self-play improvement (Stage 4)

**Design Tradeoffs:** The 1.5B parameter size balances accuracy with computational efficiency, sacrificing some complex schema handling capability for significant speed and memory advantages over larger models.

**Failure Signatures:** Repetitive token generation, context length extrapolation failures, JSON syntax errors in extraction output.

**3 First Experiments:**
1. Test model on progressively longer HTML documents (32K → 128K → 256K → 512K) to verify context handling
2. Evaluate Rouge-L and F1 scores on held-out WebMarkdown test set to confirm benchmark performance
3. Run JSON extraction on schema-guided tasks to identify performance gaps with complex structures

## Open Questions the Paper Calls Out

### Open Question 1
Can the 1.5B parameter model close the performance gap with larger models on complex schema-guided JSON extraction without increasing model size? The authors state the model "still lags behind larger models in this more complex task," but it's unclear if this ceiling is due to model capacity or training pipeline limitations. Ablation studies comparing against larger models on increasingly complex JSON schemas would resolve this.

### Open Question 2
Does extending the self-play iterative tuning beyond a single cycle yield diminishing returns or model collapse? The paper introduces "self-play iterative tuning" but only executes one iteration. The long-term stability and efficacy of the model generating its own refinement data remains unexplored. Performance metrics and stability analysis after 3+ consecutive rounds would provide clarity.

### Open Question 3
To what extent does the skewed language distribution of the training data impact extraction fidelity for low-resource languages? Section 3.1.1 notes the dataset is 82.7% English and Chinese, despite the backbone supporting 29 languages. The model claims multilingual support via Qwen2.5, but evaluation is not explicitly broken down by language diversity. Benchmark results on held-out test sets specifically composed of non-English/Chinese languages would resolve this.

## Limitations
- The synthetic data generation pipeline lacks specific implementation details for the Draft-Refine-Critique process and exact prompt formulations
- Performance in real-world noisy web scraping scenarios may vary from carefully curated benchmark results
- The model still lags behind larger models in complex schema-guided JSON extraction tasks

## Confidence
High confidence in reported results based on systematic methodology, comprehensive evaluation across multiple benchmarks, and use of established metrics. The four-stage training approach is well-justified and technically sound.

## Next Checks
1. Implement and evaluate the Draft-Refine-Critique pipeline with actual prompts to verify synthetic data quality matches reported standards
2. Test model performance on HTML documents with significant noise, malformed tags, or unusual structures not present in training data
3. Benchmark inference speed and memory usage across the full 512K token context to confirm efficiency claims under production conditions