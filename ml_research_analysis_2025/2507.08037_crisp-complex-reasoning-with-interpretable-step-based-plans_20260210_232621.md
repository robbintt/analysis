---
ver: rpa2
title: 'CRISP: Complex Reasoning with Interpretable Step-based Plans'
arxiv_id: '2507.08037'
source_url: https://arxiv.org/abs/2507.08037
tags:
- plan
- plans
- high-level
- reasoning
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CRISP, a multi-domain dataset of high-level
  plans for mathematical reasoning and code generation. The dataset was derived from
  annotated detailed solutions and validated through both intrinsic (LLM-based quality
  judgment) and extrinsic (performance impact) evaluations.
---

# CRISP: Complex Reasoning with Interpretable Step-based Plans

## Quick Facts
- arXiv ID: 2507.08037
- Source URL: https://arxiv.org/abs/2507.08037
- Reference count: 40
- One-line primary result: CRISP is a multi-domain dataset of high-level plans that improve complex reasoning tasks by providing structured, interpretable solution pathways.

## Executive Summary
CRISP introduces a novel dataset of high-level reasoning plans for mathematical and code generation tasks, derived from detailed solutions and validated through both intrinsic (LLM-based quality judgment) and extrinsic (performance impact) evaluations. The dataset was constructed by generating candidate plans using few-shot prompting, then filtering them for conciseness, clarity, coherence, and completeness while ensuring they improve downstream task performance. Experiments demonstrate that fine-tuning small models on CRISP produces higher-quality plans than larger models using few-shot prompting, and these plans significantly outperform Chain-of-Thought reasoning. The dataset also exhibits strong cross-domain transfer, with mathematical training improving code generation planning and vice versa.

## Method Summary
CRISP is constructed from problem-solution pairs in the Magpie-Reasoning-V1-150K dataset, generating high-level plans using few-shot prompting with Mixtral-8x22B-Instruct. Plans undergo intrinsic filtering via LLM-as-judge (Llama-3.1-70B-Instruct) to ensure quality attributes, then extrinsic filtering by measuring downstream solver performance improvement. Fine-tuning uses LoRA (R=32, α=16, dropout=0.05%, lr=1e-5) on Granite-3.1-8B-Instruct for 5 epochs. The final planner-s solver pipeline is evaluated on MBPP, HumanEval, GSM8K, and MATH benchmarks, comparing against Chain-of-Thought baselines.

## Key Results
- Fine-tuned small models on CRISP generate higher-quality plans than large models using few-shot prompting, with LLM judge preferring fine-tuned plans 73.3% of the time
- CRISP plans significantly outperform Chain-of-Thought reasoning, achieving up to 28.1% error reduction on GSM8K
- Cross-domain transfer shows math-trained models perform better on code tasks than code-trained models on math tasks

## Why This Works (Mechanism)

### Mechanism 1
Explicit high-level planning improves downstream task performance more than Chain-of-Thought reasoning for "myopic" tasks (solvable via predefined steps). Decomposing problems into abstract steps reduces reasoning load on the solver by pre-structuring the solution path. Plans omit low-level details, preserving flexibility for execution while enforcing logical flow. Core assumption: Tasks in CRISP are appropriately "myopic" and amenable to fixed-step decomposition; this may not hold for tasks requiring adaptive, feedback-driven planning. Evidence anchors: Table 2 shows up to 28.1% error reduction vs CoT (GSM8K with large solver); related work "Plan-and-Act" and "iCLP" corroborate benefits of separating planning from execution in long-horizon tasks.

### Mechanism 2
Lightweight fine-tuning on CRISP enables small models to generate higher-quality plans than larger models using few-shot prompting. Fine-tuning internalizes planning patterns that emerge inconsistently via prompting, producing plans that are more concise (fewer steps), coherent, and complete. LLM-as-judge preferred fine-tuned small model plans 73.3% of the time over a 70B model. Core assumption: Plan quality criteria (clarity, conciseness, coherence, completeness) measured by LLM-as-judge correlate with downstream utility; the judge (Llama-3.1-70B-Instruct) may have biases despite explicit criteria. Evidence anchors: "the large vanilla model generates, on average, 1.3 more steps than our small fine-tuned model on coding benchmarks"; weak corpus evidence on small vs. large fine-tuning comparison.

### Mechanism 3
Planning capabilities transfer across domains, with math-to-code transfer stronger than code-to-math. Mathematical training develops abstract reasoning patterns (logic, recursion, combinatorics) that underpin algorithmic thinking; code training may focus more on syntax and patterns without deeper abstraction. Core assumption: Transfer asymmetry reflects task structure rather than dataset artifacts; this may depend on domain similarity. Evidence anchors: Table 3 shows math-trained model on MBPP achieves 75.4 vs. code-trained model's 76.1 (small gap), while code-trained model on MATH achieves 71.9 vs. math-trained model's 73.1 (larger gap); "Divide and Conquer" and "Constraints-of-Thought" suggest hierarchical decomposition is a generalizable reasoning skill.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CRISP plans are evaluated against CoT as a baseline; understanding CoT's step-by-step rationale generation is essential to contextualize improvements.
  - Quick check question: Can you explain why CoT may still produce errors even with explicit intermediate steps?

- Concept: Few-shot Prompting
  - Why needed here: The paper's baseline approach uses few-shot prompting to generate plans, contrasted with fine-tuning.
  - Quick check question: What are the limitations of few-shot prompting for tasks requiring consistent structural outputs?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: CRISP fine-tuning uses LoRA for parameter-efficient training; understanding its mechanics is needed to reproduce experiments.
  - Quick check question: How does LoRA reduce computational overhead compared to full fine-tuning?

## Architecture Onboarding

- Component map: Problem+Solution Pairs (Magpie-Reasoning-V1-150K) -> Plan Generation (Mixtral-8x22B-Instruct) -> Intrinsic Filtering (Llama-3.1-70B-Instruct Judge) -> Extrinsic Filtering (Solver Performance) -> Fine-tuning (Granite-3.1-8B-Instruct with LoRA) -> Planner-Solver Pipeline (Granite-3.1-8B-Instruct Solver)

- Critical path: 1) Data preparation: Extract problem + solution pairs from Magpie-Reasoning-V1-150K; 2) Plan generation: Use few-shot prompting to create candidate plans; 3) Intrinsic filtering: LLM judge validates plan quality attributes; 4) Extrinsic filtering: Compare solver performance with vs. without plan; 5) Fine-tuning: Train planner model on filtered plans using LoRA; 6) Evaluation: Run planner-solver pipeline on benchmarks.

- Design tradeoffs: Plan conciseness vs. completeness: Fewer steps may omit critical transitions; more steps may introduce noise. Intrinsic vs. extrinsic filtering: LLM judge is faster but may not capture task utility; extrinsic filtering is costly but grounded. Domain-specific vs. cross-domain training: Single-domain fine-tuning is simpler; cross-domain may yield more robust planners.

- Failure signatures: Plans that are overly verbose or repetitive (fail conciseness). Plans with missing logical steps (fail completeness). Plans that do not improve solver accuracy vs. no-plan baseline (extrinsic rejection). Cross-domain degradation if planning patterns do not transfer.

- First 3 experiments: 1) Reproduce baseline: Run vanilla small model with few-shot plan generation on GSM8K; verify CoT performance matches reported ~84.6% accuracy. 2) Ablate filtering: Train planner on CRISP with only intrinsic filtering (skip extrinsic) and compare solver performance; quantify performance drop. 3) Cross-domain transfer test: Fine-tune planner on math-only CRISP; evaluate on HumanEval (code) and compare to code-trained planner gap (<2% expected per Table 3).

## Open Questions the Paper Calls Out

### Open Question 1
Can the benefits of fine-tuning on CRISP be replicated across a wider range of reasoning domains beyond mathematics and code generation? Basis in paper: The authors state: "Future work may explore expanding CRISP to additional domains and refining planning strategies." Why unresolved: The current work is deliberately limited to two domains; the generalizability of the planning capabilities to other structured reasoning domains (e.g., legal analysis, scientific hypothesis generation) is untested. What evidence would resolve it: Extending the CRISP pipeline to create and validate datasets for new domains and demonstrating performance gains from fine-tuning, similar to those shown for math and code.

### Open Question 2
To what extent does the reliance on LLM-based judgment for validation introduce bias, and can alternative validation methods improve dataset quality? Basis in paper: The validation pipeline depends on Llama-3.1-70B-Instruct as a judge. The paper notes potential bias in LLM evaluations: "which should create a bias toward its own generations," and the intrinsic validation is a key methodological step. Why unresolved: While the authors acknowledge potential bias, they do not systematically evaluate its impact or compare against non-LLM validation methods (e.g., human expert evaluation). What evidence would resolve it: A comparative study measuring the correlation and divergence between LLM-based judgments and human expert evaluations for plan quality.

### Open Question 3
What are the fundamental differences in learned representations that enable strong cross-domain transfer from mathematics to code, but weaker transfer in the reverse direction? Basis in paper: The out-of-domain evaluation reveals an asymmetry: a math-trained model performs better on code tasks than a code-trained model does on math tasks. The authors speculate this is due to mathematical reasoning being more foundational. Why unresolved: The paper provides a high-level hypothesis but does not conduct a mechanistic analysis of the fine-tuned models' representations to explain the asymmetric transfer. What evidence would resolve it: Probing experiments or representational analyses of the fine-tuned models to identify features that facilitate better transfer from math to code.

### Open Question 4
Is the "conciseness" of a high-level plan, as measured by fewer steps, a general predictor of its effectiveness for downstream reasoning tasks? Basis in paper: Analysis of plan quality found that the fine-tuned model generated plans with fewer steps on average than the large vanilla model, which was associated with better performance. The authors suggest "fewer, more concise, and well-structured steps have a greater impact." Why unresolved: This finding is observational and based on a single dataset and model pair; it has not been established as a causal principle. What evidence would resolve it: Controlled experiments where plans are systematically varied in step count and structure to isolate the effect of conciseness on solver performance.

## Limitations

- LLM-based validation may introduce bias and may not align with human preferences for plan quality
- Plan generation prompts and few-shot examples are not fully specified, particularly for the code domain
- The approach focuses on "myopic" tasks amenable to predefined decomposition and may not benefit tasks requiring adaptive, feedback-driven planning

## Confidence

- **High confidence**: CRISP plans significantly outperform Chain-of-Thought reasoning on benchmark tasks (supported by error reduction metrics up to 28.1% on GSM8K). The dataset construction methodology (few-shot generation + LLM filtering + extrinsic validation) is clearly specified and reproducible.
- **Medium confidence**: Fine-tuning small models on CRISP yields higher-quality plans than large models using few-shot prompting (based on LLM judge preference 73.3% of the time, but weak external corpus evidence). Cross-domain transfer claims are supported by performance gaps (<2% for math→code, larger for code→math) but may depend on domain similarity.
- **Low confidence**: The claim that CRISP plans are universally more "concise, clear, coherent, and complete" than few-shot generated plans—this relies entirely on LLM judgment without human validation. The mechanism by which abstract planning reduces reasoning load is theoretically sound but not empirically isolated.

## Next Checks

1. **Human evaluation of plan quality**: Recruit 3-5 NLP researchers to independently rate plan quality (clarity, conciseness, coherence, completeness) on a subset of CRISP samples and compare agreement with LLM judge scores.

2. **Ablation of filtering steps**: Train planners on CRISP subsets with only intrinsic filtering (skip extrinsic) and only extrinsic filtering (skip intrinsic), then measure downstream performance degradation to quantify each filter's contribution.

3. **Transfer to non-symbolic domains**: Test CRISP-trained planners on commonsense reasoning or narrative tasks from datasets like CommonsenseQA or StrategyQA to assess whether planning benefits transfer beyond math/code domains.