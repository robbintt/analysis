---
ver: rpa2
title: Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert
  Parallelism
arxiv_id: '2512.21487'
source_url: https://arxiv.org/abs/2512.21487
tags:
- expert
- attention
- findep
- time
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the inefficiency in MoE inference under disaggregated
  expert parallelism (DEP), where coarse-grained scheduling and limited support for
  shared experts cause GPU idle time. FinDEP addresses this by introducing fine-grained
  task scheduling that partitions attention and expert computations into smaller segments,
  enabling better overlap of computation and communication.
---

# Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism

## Quick Facts
- **arXiv ID:** 2512.21487
- **Source URL:** https://arxiv.org/abs/2512.21487
- **Reference count:** 39
- **Primary result:** FinDEP improves MoE inference throughput by up to 1.61x over PPPipe through fine-grained task scheduling under disaggregated expert parallelism

## Executive Summary
This paper addresses inefficiency in MoE inference under disaggregated expert parallelism (DEP), where coarse-grained scheduling and limited support for shared experts cause GPU idle time. FinDEP introduces fine-grained task scheduling that partitions attention and expert computations into smaller segments, enabling better overlap of computation and communication. The key innovations are: (1) splitting tasks into smaller pieces for fine-grained pipelining, (2) formulating an optimization problem to balance granularity and ordering, and (3) developing an efficient solver to find near-optimal schedules in under a second. Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves inference throughput by up to 1.61x over the state-of-the-art PPPipe method, achieving up to 1.24x speedup even on a 32-GPU system, while enabling real-time adaptation to dynamic workloads.

## Method Summary
FinDEP tackles MoE inference inefficiency by introducing fine-grained task scheduling under disaggregated expert parallelism. The method partitions attention and expert computations into smaller segments, enabling better overlap of computation and communication. The key innovation is a solver that optimizes micro-batch sizes (mₐ), AG pipeline degree (r₁), and EG partitioning (r₂) to maximize throughput while respecting memory constraints. The solver uses linear α-β performance models calibrated offline, iterates over feasible configurations exploiting monotonicity properties, and selects optimal scheduling orders (AASS vs. ASAS) per workload. The entire optimization completes in under a second, enabling real-time adaptation.

## Key Results
- FinDEP improves inference throughput by up to 1.61x over PPPipe
- Achieves up to 1.24x speedup even on 32-GPU systems
- Reduces average GPU idle time by up to 36.6%
- Enables real-time adaptation to dynamic workloads

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partitioning computation and communication into smaller segments enables better overlap, reducing GPU idle time in disaggregated MoE inference.
- **Mechanism:** FinDEP splits attention inputs along the batch dimension (creating r₁ micro-batches) and expert computations along the token dimension (creating r₂ fine-grained segments). This allows A2E/E2A communication to begin earlier and overlap with ongoing computation in both AG and EG.
- **Core assumption:** The overhead of launching additional kernels and communication operations is smaller than the idle time recovered through better overlap.
- **Evidence anchors:**
  - [abstract] "partitioning computation/communication into smaller tasks for fine-grained pipelining"
  - [§4.1] Linear performance models with α (startup overhead) and β (scaling) parameters; the solver explicitly trades off granularity vs. launch overhead.
  - [corpus] HybriMoE (arXiv:2504.05897) shows similar hybrid scheduling benefits, but does not validate FinDEP's specific partitioning strategy.
- **Break condition:** If kernel launch overhead (α) dominates computation time (β × workload) — e.g., very small tensors or extremely slow dispatch — fine-grained partitioning will degrade throughput.

### Mechanism 2
- **Claim:** Two scheduling orders (AASS vs. ASAS) adapt to different workload characteristics, with AASS favoring earlier expert start and ASAS improving AG utilization.
- **Mechanism:** AASS (Attention-All, Shared-All) processes all attention segments first, enabling earlier A2E initiation. ASAS (Attention-Shared-Alternating-Sequential) interleaves attention and shared expert computation, filling AG idle slots. The solver evaluates both and selects the higher-throughput option.
- **Core assumption:** Shared expert computation has no data dependency with sparse expert activation, allowing parallel execution.
- **Evidence anchors:**
  - [§4.2] Figure 4 illustrates comparative advantages; the algorithm independently optimizes both strategies and selects the superior one.
  - [§2.3] "the computation of experts in AE has no data dependency with the shared expert"
  - [corpus] No direct corpus validation; AASS/ASAS comparison is specific to this work.
- **Break condition:** If shared expert computation becomes a bottleneck (e.g., N_shared increases significantly), AASS may starve EG of work while ASAS could thrash AG resources.

### Mechanism 3
- **Claim:** Throughput is monotonically non-decreasing with respect to micro-batch size (mₐ) and AG pipeline degree (r₁), enabling efficient Pareto-frontier search.
- **Mechanism:** Theorems 1–3 prove monotonicity properties that reduce the search space. The solver iterates over mₐ in descending order, computes max r₁ under memory constraints, and skips redundant (mₐ, r₁) pairs. For each pair, convex optimization finds optimal r₂.
- **Core assumption:** Linear α-β performance models accurately predict execution time across partition sizes.
- **Evidence anchors:**
  - [§4.2] Theorems 1–3 with formal proofs; Table 3–4 empirically validate monotonicity.
  - [§5.2] R² values of 0.994–0.999 for model fits on A6000 testbed.
  - [corpus] FSMoE (arXiv:2501.10714) uses similar performance modeling for training but does not address inference scheduling.
- **Break condition:** If actual execution deviates significantly from linear models (e.g., memory bandwidth saturation, thermal throttling), the solver may select suboptimal configurations.

## Foundational Learning

- **Disaggregated Expert Parallelism (DEP):**
  - Why needed here: FinDEP operates on DEP's two-group architecture (AG for attention/shared expert, EG for sparse experts). Understanding bidirectional communication (A2E, E2A) is prerequisite to grasping scheduling constraints.
  - Quick check question: In DEP, why does the expert group remain idle until A2E completes?

- **Pipeline Parallelism with Micro-batches:**
  - Why needed here: PPPipe (the baseline) uses coarse-grained micro-batch pipelining. FinDEP extends this with fine-grained token-level partitioning (r₂).
  - Quick check question: What is the tradeoff between increasing r₁ (more micro-batches) and kernel launch overhead?

- **Linear α-β Performance Models:**
  - Why needed here: The optimization formulation relies on linear models t(x) = α + βx for computation and communication time.
  - Quick check question: What does α represent in the GEMM performance model, and why must it be accounted for when increasing partition granularity?

## Architecture Onboarding

- **Component map:**
  - AG (Attention Group): Attention layers + shared expert(s); fully replicated across ag GPUs; no intra-group communication
  - EG (Expert Group): E sparse experts distributed across eg GPUs; each expert on one GPU; no intra-group communication for activated experts
  - A2E Communication: Routes tokens from AG to target experts in EG via NCCL
  - E2A Communication: Gathers expert outputs back to AG for next layer
  - Solver: Offline model calibration + online Algorithm 1 execution (<1 second)

- **Critical path:**
  For each layer: Attention(mₐ) → SharedExpert(mₐ) ∥ A2E(mₑ) → Expert(mₑ) → E2A(mₑ) → next layer attention. Total latency per layer = max(G(mₐ, mₑ), r₁·F(mₐ, mₑ)) per Eq. 13.

- **Design tradeoffs:**
  - Larger mₐ → higher throughput (monotonic) but constrained by GPU memory
  - Larger r₁ → better overlap but more kernel launches; bounded by memory
  - Larger r₂ → finer expert overlap but launch overhead; convex optimum exists (Theorem 4)
  - AASS vs. ASAS: No universally superior order; workload-dependent

- **Failure signatures:**
  - Throughput plateaus despite increasing r₁/r₂: Launch overhead (α) dominates; check micro-benchmark for elevated α values
  - Solver outputs infeasible r₁=0: Memory constraint violated; reduce batch size or increase GPU count
  - Prediction error >15%: Performance model invalid; recalibrate α, β for current hardware/software stack
  - No speedup over PPPipe on high-bandwidth interconnects: Communication already hidden; computation is the bottleneck (Amdahl's Law, see §5.5 Discussion)

- **First 3 experiments:**
  1. Micro-benchmark calibration: Run GEMM (various M×K×N), attention (varying B, S), and communication (varying payload, ag/eg configs) on target hardware. Fit α, β parameters. Verify R² > 0.99.
  2. Validate monotonicity claims: On a small MoE model (e.g., 2-layer DeepSeek-V2 variant), sweep mₐ and r₁ independently, measuring throughput. Confirm monotonic increase per Tables 3–4.
  3. End-to-end comparison: Deploy FinDEP vs. PPPipe on Testbed C (8× H20) with DeepSeek-V2 and Qwen3-MoE backbones. Measure tokens/s across S ∈ {1024, 2048, 4096, 8192}. Verify 1.02×–1.35× speedup per Table 5; investigate if gains are lower than expected.

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes linear α-β performance models may break down under memory bandwidth saturation or thermal throttling
- Solver assumes static models and does not account for runtime variability or dynamic load imbalance
- Methodology focuses on inference, not training workloads with backward pass asymmetries
- Evaluation limited to Nvidia GPUs; behavior on AMD or CPU-based deployments unknown

## Confidence

- **High confidence:** Throughput improvement claims (1.61× max, 1.24× at 32 GPUs) are well-supported by controlled experiments across four distinct GPU systems and two MoE backbones, with statistically consistent results.
- **Medium confidence:** The solver's sub-second runtime and monotonicity properties are mathematically proven and empirically validated, but rely on accurate model calibration that may not generalize across all hardware/software stacks.
- **Low confidence:** Claims about FinDEP's behavior under extreme memory pressure or on non-Nvidia accelerators are untested; the model's robustness to runtime variability is not quantified.

## Next Checks
1. Model Calibration Robustness: Run FinDEP on a new GPU generation (e.g., H100) without recalibration. Measure prediction error and throughput degradation compared to models calibrated on A100/H20.
2. Stress Test Under Memory Pressure: Construct workloads where GPU memory is 95%+ utilized. Verify that the solver's infeasibility detection (r₁=0) triggers correctly and that alternative configurations are explored.
3. Runtime Variability Analysis: Inject synthetic noise into kernel execution times (±10% variance) and measure solver resilience. Quantify throughput variance and identify failure thresholds.