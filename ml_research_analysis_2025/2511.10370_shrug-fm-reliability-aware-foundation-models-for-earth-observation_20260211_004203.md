---
ver: rpa2
title: 'SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation'
arxiv_id: '2511.10370'
source_url: https://arxiv.org/abs/2511.10370
tags:
- uncertainty
- data
- foundation
- predictions
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHRUG-FM integrates out-of-distribution detection in both raw input
  and embedding spaces with task-specific predictive uncertainty to improve reliability
  of geospatial foundation models. Applied to burn scar segmentation, OOD metrics
  like NCDD correlate with performance drops in underrepresented geographies such
  as low-elevation and high-river areas.
---

# SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation

## Quick Facts
- arXiv ID: 2511.10370
- Source URL: https://arxiv.org/abs/2511.10370
- Reference count: 40
- Primary result: Combines out-of-distribution detection in input and embedding spaces with task-specific predictive uncertainty to improve reliability of geospatial foundation models.

## Executive Summary
SHRUG-FM introduces a reliability-aware framework for geospatial foundation models by integrating out-of-distribution (OOD) detection and predictive uncertainty estimation. Applied to burn scar segmentation, the method demonstrates that OOD metrics like NCDD correlate with performance drops in underrepresented geographies. Uncertainty-based flagging effectively discards low-performing predictions, particularly those with high predictive variance. These complementary signals enable safer deployment by identifying where models are likely to fail, linking prediction errors to specific environmental features underrepresented in pretraining data.

## Method Summary
SHRUG-FM integrates OOD detection and uncertainty estimation for geospatial foundation models. The method uses a frozen ViT-S/16 encoder pretrained on SSL4EO-S12, with K-means clustering on pretraining data to establish reference centroids for both raw input and embedding spaces. It computes NCDD to quantify distributional mismatch and employs Deep Ensembles or MC Dropout to estimate predictive uncertainty. The framework flags unreliable predictions by combining high NCDD scores with high predictive variance, then correlates these flags with HydroATLAS attributes to diagnose systematic blind spots.

## Key Results
- OOD metrics like NCDD correlate with performance drops in underrepresented geographies such as low-elevation and high-river areas
- Uncertainty-based flagging effectively discards low-performing predictions, particularly those with high predictive variance
- Failures are concentrated in specific geographies rather than random, revealing systematic blind spots in pretraining data

## Why This Works (Mechanism)

### Mechanism 1: Distributional Mismatch Detection via Geometric Distance
If a downstream sample lies far from the nearest centroid of the pretraining distribution in the embedding space, it is likely to suffer from performance degradation due to underrepresentation. The framework applies k-means clustering on SSL4EO-S12 to establish reference geometry and computes NCDD for new inputs. A high NCDD suggests the input is not confidently "known" to the foundation model, serving as an early warning signal before task-specific inference.

### Mechanism 2: Task-Specific Uncertainty Filtering
Aggregating predictive variance from an ensemble of decoders allows the system to identify and discard predictions where the model exhibits high internal disagreement. By training multiple decoders on bootstrapped data or using MC Dropout, the system generates a distribution of predictions for a single input. High variance among these predictions indicates the frozen encoder's representation is insufficient for the decoder to resolve a confident output, triggering a "discard" flag.

### Mechanism 3: Semantic Attribution via External Geospatial Layers
Mapping reliability flags to HydroATLAS attributes (e.g., elevation, river area) reveals systematic blind spots in pretraining data rather than random failures. The system overlays OOD scores and uncertainty maps with independent hydro-environmental data to diagnose why the model is failing by analyzing correlation between high-flag areas and specific features.

## Foundational Learning

**Concept: Nearest Centroid Distance Deficit (NCDD)**
- Why needed: This is the core metric used to quantify how "foreign" a satellite image is to the foundation model's experience
- Quick check: Can you explain how NCDD differs from simple Euclidean distance to a centroid? (Hint: It compares the nearest distance vs. distances to *all* other clusters)

**Concept: Epistemic vs. Aleatoric Uncertainty**
- Why needed: The paper uses ensembles to capture epistemic uncertainty (model ignorance) to decide when to flag a prediction
- Quick check: Which type of uncertainty does a Deep Ensemble primarily reduce or quantify, and which is inherent in the data labels?

**Concept: Encoder-Decoder Architecture (Frozen Backbone)**
- Why needed: SHRUG-FM assumes a frozen foundation model (encoder) and only adapts the decoder
- Quick check: Why does the method freeze the encoder weights instead of fine-tuning them for the burn scar task?

## Architecture Onboarding

**Component map:** Sentinel-2 Imagery -> Frozen ViT-S/16 Encoder -> OOD Modules (Input-Space K-means, Embedding-Space K-means) -> CNN Decoder with Uncertainty Head -> Aggregator (NCDD + Variance -> Reliability Flag)

**Critical path:**
1. Establish reference clusters by running K-means on SSL4EO-S12 dataset
2. Pass new image through Encoder to get embeddings
3. Compute NCDD for both raw pixels and embeddings against pretraining clusters
4. Run Ensemble Decoder to get segmentation mask and Predictive Variance
5. Correlate high NCDD/Variance flags with HydroATLAS features to diagnose failure modes

**Design tradeoffs:**
- Ensemble Size: Paper uses 10 members; increasing this improves uncertainty estimation but multiplies inference cost linearly
- Clustering Resolution (k): They use k=15; too few clusters may smooth over critical environmental niches, too many may overfit to noise
- Thresholding: The discard threshold is currently heuristic, balancing "coverage" vs "risk"

**Failure signatures:**
- False Confidence: Low variance and low NCDD, but incorrect prediction
- Silent Failure: High NCDD but low uncertainty
- Conservative Paralysis: High uncertainty on easy samples

**First 3 experiments:**
1. Reproduce the linear relationship between NCDD deciles and F1 score on the burn scar test set
2. Plot the Risk-Coverage curve to quantify error reduction when discarding top X% of uncertain images
3. Train a decoder excluding Southeastern US and verify SHRUG-FM flags this region as high-NCDD/OOD

## Open Questions the Paper Calls Out

**Open Question 1:** Do the reliability signals (OOD scores and predictive uncertainty) consistently correlate with performance drops across diverse downstream tasks and distinct GFM architectures? The current study is limited to burn scar segmentation using SSL4EO-S12 and ViT-S/16 backbone.

**Open Question 2:** Can a learned selective prediction mechanism effectively combine input-space OOD, embedding-space OOD, and predictive uncertainty to abstain from low-confidence predictions better than fixed thresholds? The current framework relies on heuristics and fixed thresholds.

**Open Question 3:** How can the utility of reliability flags be rigorously quantified using risk-coverage metrics to validate their practical deployment value? The paper currently uses descriptive correlations but lacks formal quantification of the trade-off between data discarded and accuracy retained.

## Limitations

- NCDD metric reliability depends on geometric distance in embedding space reflecting semantic shifts; may fail if pretraining data is too diverse or encoder collapses environments
- Method's dependence on frozen encoder limits adaptability to task-specific nuances
- Semantic attribution assumes HydroATLAS features are causal factors, but failures may be driven by sensor noise or atmospheric conditions not captured in static maps

## Confidence

- **High Confidence:** Correlation between NCDD and performance degradation in underrepresented geographies; ensemble-based uncertainty estimation and flagging ability
- **Medium Confidence:** Semantic attribution via HydroATLAS attributes as causal factors for model failure
- **Low Confidence:** Specific decoder architecture details and training hyperparameters not provided

## Next Checks

1. Reproduce the linear relationship between NCDD deciles and F1 score on the burn scar test set to validate the OOD proxy
2. Plot the Risk-Coverage curve to quantify how much "risk" (error) is removed when discarding the top X% of uncertain images
3. Train a decoder on data excluding the Southeastern US and verify if SHRUG-FM correctly flags this region as high-NCDD/OOD during inference