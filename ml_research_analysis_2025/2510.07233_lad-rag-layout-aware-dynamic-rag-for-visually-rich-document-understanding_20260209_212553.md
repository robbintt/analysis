---
ver: rpa2
title: 'LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding'
arxiv_id: '2510.07233'
source_url: https://arxiv.org/abs/2510.07233
tags:
- document
- retrieval
- arxiv
- lad-rag
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LAD-RAG addresses retrieval limitations in visually rich documents
  by constructing a symbolic document graph during ingestion to capture layout and
  cross-page structure, then using an LLM agent to dynamically retrieve evidence at
  inference time. This approach improves retrieval completeness, achieving over 90%
  perfect recall on average without top-k tuning and outperforming baselines by up
  to 20% in recall at comparable noise levels.
---

# LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding

## Quick Facts
- **arXiv ID:** 2510.07233
- **Source URL:** https://arxiv.org/abs/2510.07233
- **Reference count:** 34
- **Primary result:** Achieves over 90% perfect recall without top-k tuning and improves QA accuracy by up to 20% over baselines

## Executive Summary
LAD-RAG addresses retrieval limitations in visually rich documents by constructing a symbolic document graph during ingestion to capture layout and cross-page structure, then using an LLM agent to dynamically retrieve evidence at inference time. This approach improves retrieval completeness, achieving over 90% perfect recall on average without top-k tuning and outperforming baselines by up to 20% in recall at comparable noise levels. It also boosts downstream QA accuracy across multiple benchmarks and models while introducing minimal latency.

## Method Summary
LAD-RAG processes visually rich documents by first using GPT-4o to extract elements (paragraphs, figures, tables) with self-contained summaries while maintaining a running memory of section hierarchy and cross-page references. During ingestion, it constructs a symbolic document graph where nodes represent elements and edges capture layout relationships and cross-page dependencies. At inference, an LLM agent dynamically interacts with both neural (embedding-based) and symbolic (graph-based) indices, iteratively retrieving evidence until sufficient completeness is determined. The system uses Louvain community detection to expand retrieved nodes with their structural context.

## Key Results
- Achieves over 90% perfect recall on average without requiring top-k tuning
- Outperforms baselines by up to 20% in recall while maintaining comparable noise levels
- Improves downstream QA accuracy across multiple models (InternVL2-8B, Pixtral-12B, Phi-3.5-Vision, GPT-4o) and benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Indexing for Layout Preservation
LAD-RAG mitigates structural context loss in standard RAG by maintaining a symbolic graph alongside neural embeddings. During ingestion, the system constructs a document graph where nodes are elements (tables, figures) and edges represent structural relationships (e.g., "figure-caption links," "cross-page continuations"). This allows querying based on layout logic rather than just semantic similarity. The core assumption is that explicit structural metadata captures dependencies that embedding vectors inevitably compress or lose.

### Mechanism 2: Dynamic Query Adaptation via Agentic Loop
Replacing static top-k retrieval with an iterative LLM agent allows retrieval depth to scale with question complexity. An LLM agent (GPT-4o) is equipped with tools to interact with both indices, formulating a plan, executing search/graph tools, observing results, and iterating until it determines sufficient evidence has been gathered. This bypasses the fixed cutoff of standard retrievers. The core assumption is that the agent possesses sufficient reasoning capability to judge evidence completeness and avoid infinite loops.

### Mechanism 3: Contextual Expansion via Community Detection
The system uses Louvain community detection on the document graph to group retrieved nodes into structurally coherent clusters. When a specific node is retrieved, a `Contextualize` tool expands the result to include its structural community, ensuring that if a piece of evidence (e.g., a chart) is found, its related context (e.g., the page it's on, the section it belongs to) is also included. The core assumption is that relevant evidence tends to cluster structurally within the document graph.

## Foundational Learning

- **Concept:** Symbolic vs. Neural Indices
  - **Why needed here:** LAD-RAG relies on the distinct complementarity of these two storage types. Neural indices handle "fuzzy" semantic matching while symbolic indices handle "exact" structural logic.
  - **Quick check question:** Can a neural embedding distinguish between "the table on page 5" and "the table on page 6" if they contain identical data?

- **Concept:** Agentic Tool Use (Function Calling)
  - **Why needed here:** The inference engine is an active agent, not a passive pipeline. You need to understand how LLMs select and execute tools (search, filter, contextualize) based on a plan.
  - **Quick check question:** What prevents an LLM agent from entering an infinite loop of retrieval?

- **Concept:** Graph Community Detection
  - **Why needed here:** The `Contextualize` tool depends on partitioning the graph into communities. Understanding modularity helps in tuning why certain nodes are grouped together as "context."
  - **Quick check question:** How does connecting nodes by "cross-page dependencies" change the community structure compared to simple page-level segmentation?

## Architecture Onboarding

- **Component map:** PDF/Image Input -> Large Vision-Language Model (GPT-4o) -> Extracts Elements (Nodes) + Updates Running Memory -> Constructs Graph (Edges) -> Stores in Symbolic Graph DB + Neural Vector Index. User Query -> Agent Controller -> Selects Tool (SemanticSearch / GraphQuery / Contextualize) -> Executes -> Evaluates Completeness -> Output Evidence.

- **Critical path:** 1) Running Memory Update: The system's ability to track "section hierarchy" and "active entities" across pages during ingestion is the linchpin for cross-page edge construction. 2) Termination Condition: The agent's ability to decide when to stop retrieving is critical for balancing recall vs. latency.

- **Design tradeoffs:** LAD-RAG prioritizes "Perfect Recall" (finding all evidence) over precision, often increasing the number of retrieved pages compared to minimal baselines but reducing noise compared to high top-k baselines. Dynamic agent loops introduce latency (2-5 LLM calls per query) but eliminate the need for manual top-k tuning.

- **Failure signatures:** Fragmented Evidence: If LVLM ingestion fails to identify cross-page continuations, the graph remains disconnected, and `Contextualize` will fail to retrieve the full object. Loop Exhaustion: If the agent cannot find sufficient evidence, it may loop until hitting the max step count (set to 20), increasing latency without improving results.

- **First 3 experiments:** 1) Ablation of Contextualize: Run retrieval with and without the `Contextualize` tool on multi-page questions to measure the specific contribution of community expansion (expected delta: ~4% recall). 2) Static vs. Dynamic k: Compare fixed top-k=10 retrieval against the dynamic agent to validate if the agent effectively varies retrieval depth based on query complexity. 3) Ingestion Memory Depth: Test retrieval performance on documents with deep nesting (e.g., legal contracts) to see if the "running memory" effectively captures deep section hierarchies.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does replacing the unified LVLM (GPT-4o) with specialized, modular tools during the ingestion phase affect structural fidelity on documents with degraded or noisy layouts? The paper notes that while they use a unified model for simplicity, "models can still struggle with noisy inputs... Future work could explore modular alternatives."

- **Open Question 2:** To what extent can the structural relationships captured in the symbolic document graph be utilized during the generative phase to enhance multi-hop reasoning, beyond their current use for retrieval? The authors explicitly limit their scope: "The scope of this paper is therefore limited to retrieval improvements, not generative reasoning or answer synthesis."

- **Open Question 3:** How robust is the LLM agent's dynamic termination condition against premature stopping when scaled down to smaller, open-source models? The inference relies on an agent determining "sufficient evidence," a complex reasoning task that may degrade with smaller models or noisier queries.

## Limitations

- The approach may degrade for documents with non-linear layouts or those lacking clear structural hierarchies
- Reliance on a single LLM (GPT-4o) for both ingestion and inference creates potential bottlenecks
- Scalability analysis is limited to documents under 100 pages, with latency measurements that may not hold for industrial-scale deployments

## Confidence

- **High Confidence**: The retrieval performance claims (90%+ perfect recall, 20% improvement over baselines) are well-supported by quantitative results across four benchmarks with consistent methodology.
- **Medium Confidence**: The architectural claims about hybrid indexing are supported by ablation studies, but the paper doesn't explore edge cases where symbolic graphs might introduce noise.
- **Low Confidence**: The scalability analysis is limited to documents under 100 pages, and the latency measurements (2-5 LLM calls) may not hold for industrial-scale deployments.

## Next Checks

1. **Ablation on Document Structure**: Test LAD-RAG on documents with varying layout complexity (legal contracts vs. slide decks) to measure performance degradation when structural hierarchies are absent or ambiguous.

2. **Cross-LLM Validation**: Implement the agent using different LLM models (Claude, Gemini) to assess whether the retrieval performance is dependent on GPT-4o's specific reasoning capabilities or generalizes across models.

3. **Scale Testing**: Evaluate the system on documents exceeding 100 pages with complex cross-page dependencies to verify that the running memory mechanism and community detection scale effectively beyond the tested benchmarks.