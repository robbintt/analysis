---
ver: rpa2
title: Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems
arxiv_id: '2506.07517'
source_url: https://arxiv.org/abs/2506.07517
tags:
- variables
- data
- exogenous
- learning
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selection bias in recommender
  systems caused by correlated latent exogenous variables. The authors propose a novel
  likelihood-based approach that models the data generation process with latent exogenous
  variables under normality assumptions and develops a Monte Carlo algorithm to estimate
  the likelihood function.
---

# Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems

## Quick Facts
- arXiv ID: 2506.07517
- Source URL: https://arxiv.org/abs/2506.07517
- Reference count: 40
- This paper proposes a likelihood-based method for recommender systems that models correlated latent exogenous variables to address selection bias.

## Executive Summary
This paper addresses a critical limitation in debiasing recommender systems: the assumption that latent exogenous variables affecting both user observation behavior and preferences are independent. By relaxing this assumption and modeling these variables as correlated, the method captures hidden dependencies that traditional debiasing approaches miss. The authors develop a Monte Carlo algorithm to estimate the likelihood function, enabling effective learning under the correlated structure. Extensive experiments on synthetic and three real-world datasets demonstrate that this approach significantly outperforms existing debiasing methods, particularly when correlations between latent variables are strong.

## Method Summary
The method models the data generation process using structural causal models where observation behavior ($O$) and preferences ($R$) are influenced by correlated latent exogenous variables ($U_O$ and $U_R$). Under normality assumptions, the authors derive a likelihood function that captures the joint probability of observing both outcomes. Since analytical solutions are intractable for binary outcomes, they employ Monte Carlo sampling to approximate the likelihood. The model uses alternating optimization to handle the non-differentiability introduced by indicator functions, optimizing prediction and propensity parameters separately while sharing the correlation parameter $\rho$. The approach can be combined with standard debiasing losses through a weighted multi-task objective.

## Key Results
- The proposed method significantly outperforms baseline debiasing methods (IPS, DR, and advanced techniques) when correlation between latent variables is strong.
- Achieves better recommendation accuracy across AUC, Recall, and NDCG metrics compared to existing approaches.
- Maintains stable performance across different correlation values, demonstrating robustness to varying data conditions.
- Demonstrates effectiveness on both synthetic data (with controlled correlation) and three real-world datasets (Coat, Yahoo! R3, KuaiRec).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relaxing the independence assumption between exogenous variables allows the model to capture hidden correlations between user observation behavior ($O$) and preference ($R$).
- Mechanism: The method introduces a bivariate normal distribution for latent exogenous variables $U_O$ and $U_R$ with a learnable covariance parameter $\rho$. By jointly modeling the structural equations $O = f_O(X, U_O)$ and $R = f_R(X, U_R)$ with this dependency, the system corrects for bias that standard IPS or DR methods attribute to noise.
- Core assumption: The correlation between latent exogenous variables follows a Gaussian distribution (Assumption 1).
- Evidence anchors:
  - [Page 3, Section 4.1]: Defines the structural causal model with correlated $U_O$ and $U_R$.
  - [Page 2, Figure 1]: Distinguishes this from traditional unmeasured confounding (where a latent variable causes both, rather than the noise terms being correlated).
  - [corpus]: Weak direct support; most related work (e.g., generative models for bias) focuses on latent confounders rather than correlated noise structures.
- Break condition: If the true data generation process involves non-Gaussian dependencies or independence, the parameter $\rho$ may become unidentifiable or mislead the correction.

### Mechanism 2
- Claim: Numerical likelihood estimation enables learning where analytical solutions are intractable due to binary outcome thresholds.
- Mechanism: Since the likelihood for binary preferences involves integrals without closed-form solutions, the method employs a Monte Carlo (MC) estimator. It samples noise terms $\epsilon \sim N(0,1)$ to approximate the expectation of the joint probability $f(z > 0, y > 0 | x)$.
- Core assumption: The Monte Carlo sample size $L$ is sufficient to approximate the true expectation without excessive variance.
- Evidence anchors:
  - [Page 4, Section 4.3]: Derives the MC estimator $MC^r_{u,i}$ using samples $\epsilon_l$.
  - [Page 8, Figure 3]: Shows performance stability increases with sample size, validating the approximation.
  - [corpus]: General alignment with MC methods in complex systems, though specific application to RS debiasing noise is novel here.
- Break condition: If the sample size $L$ is too small, gradient variance destabilizes training; if too large, computational cost outweighs accuracy gains.

### Mechanism 3
- Claim: Alternating optimization based on the symmetry of endogenous variables resolves non-differentiability issues in the loss function.
- Mechanism: The indicator function $I(\cdot)$ in the MC estimator blocks gradient flow. The authors exploit the symmetry between the equations for $z$ and $y$ to decompose the objective into two differentiable parts ($L^R_{MLE}$ and $L^D_{MLE}$), optimizing prediction parameters ($\theta_r$) and propensity parameters ($\theta_o$) iteratively.
- Core assumption: The structural equations are symmetric enough that gradients from one path can effectively update the shared parameter $\rho$.
- Evidence anchors:
  - [Page 4, Section 4.4]: Explicitly derives the symmetric formulation and the alternating optimization strategy.
  - [Page 5, Algorithm 1]: Defines the iterative loop for updating $\theta_r$ and $\theta_o$.
  - [corpus]: Not explicitly covered in neighbors; this is a specific architectural workaround for the proposed estimator.
- Break condition: If the symmetry assumption does not hold (e.g., vastly different functional forms for $g_o$ and $g_r$), the alternating updates may fail to converge to a joint optimum.

## Foundational Learning

- Concept: **Structural Causal Models (SCM) & Exogenous Variables**
  - Why needed here: The paper fundamentally alters the standard SCM assumption that exogenous "noise" variables ($U$) are independent. Understanding the difference between an unobserved *confounder* (a hidden cause) and *correlated exogenous variables* (correlated noise) is required to grasp the problem formulation.
  - Quick check question: In Figure 1, how does the relationship between $U_O$ and $U_R$ in the proposed model differ from the traditional unmeasured confounding model?

- Concept: **Selection Bias & Missing Not At Random (MNAR)**
  - Why needed here: The mechanism relies on the fact that user observations ($O$) are not random but correlated with preferences ($R$). Standard loss functions fail because they assume missing data is random (MAR).
  - Quick check question: Why does the "Naive" estimator (Eq. 2) fail when user interaction depends on their latent preference?

- Concept: **Monte Carlo Integration**
  - Why needed here: The method estimates a complex likelihood (probability of binary outcome + observation) by averaging samples from a distribution rather than calculating it analytically.
  - Quick check question: How does increasing the sample size $L$ in Eq. 12 affect the variance of the gradient estimate during training?

## Architecture Onboarding

- Component map:
  - Inputs: User-Item features $x_{u,i}$
  - Backbone: Neural Collaborative Filtering (NCF) producing embeddings
  - Heads: Two separate MLPs outputting $g_o(x; \theta_o)$ (propensity score) and $g_r(x; \theta_r)$ (predicted preference)
  - Correlation Module: A scalar parameter $\rho$ (and derived $\sqrt{1-\rho^2}$) shared across the batch
  - Sampler: A Monte Carlo loop generating $L$ noise samples per batch item

- Critical path:
  1. Batch user-item pairs include observed ($O=1$) and unobserved ($O=0$) interactions
  2. For observed pairs, sample $\epsilon_L$ times
  3. Compute $MC^r_{u,i}$ (Est. likelihood of $R=1, O=1$)
  4. Alternate: Step A - Fix $\theta_o$, update $\theta_r$ and $\rho$ using $MC^r$. Step B - Fix $\theta_r$, update $\theta_o$ and $\rho$ using symmetric formulation
  5. Combine with standard Debiasing Loss (e.g., DR) weighted by $\alpha$

- Design tradeoffs:
  - **MC Sample Size ($L$)**: Higher $L$ reduces variance but linearly increases memory/compute per batch. Paper suggests [100, 500] for small datasets and [10, 60] for larger ones
  - **Loss Weight ($\alpha$)**: Balances the proposed MLE objective with standard debiasing losses. High correlation data requires higher $\alpha$
  - **Differentiability**: The architecture strictly requires the symmetric alternating optimization because standard backprop cannot pass through the indicator function $I(\cdot)$ in a single pass

- Failure signatures:
  - **Gradient Explosion/Vanishing**: If trying to optimize $\theta_o$ and $\theta_r$ simultaneously without the alternating strategy, gradients may die for samples far from the decision boundary
  - **Unstable $\rho$**: If $|\rho|$ is small (weak correlation), the estimate may fluctuate wildly (Page 6, Section 5.3)
  - **Computational Bottleneck**: Excessive MC samples on large datasets will cause OOM errors; requires tuning $L$ relative to GPU memory

- First 3 experiments:
  1. **Sanity Check (Synthetic)**: Generate data with known $\rho$ (e.g., 0.8). Train the model and verify if the learned $\hat{\rho}$ converges to the ground truth (Replicate Figure 2)
  2. **Ablation on Symmetry**: Attempt to train the model by optimizing $\theta_o$ and $\theta_r$ jointly without the alternating decomposition to verify convergence failure or instability
  3. **Hyperparameter Sensitivity**: Run on a real-world dataset (e.g., Coat) sweeping $\alpha$ to find the "elbow" where the likelihood loss dominates the debiasing loss without overfitting (Replicate Figure 4)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational costs of Monte Carlo sampling be reduced to enable scalability for large-scale industrial recommender systems?
- Basis in paper: [explicit] "One of the potential limitations is that the Monte Carlo sampling process, though effective, introduces computational costs that may hinder scalability for large-scale industrial systems."
- Why unresolved: The proposed method relies on Monte Carlo integration to estimate likelihood functions without closed-form solutions, which requires repeated sampling that scales poorly with dataset size.
- What evidence would resolve it: Development of alternative estimation techniques (e.g., variational inference, analytical approximations) that achieve comparable debiasing performance with lower computational complexity, validated on datasets with millions of users and items.

### Open Question 2
- Question: Can the normality assumption on correlated latent exogenous variables be relaxed while maintaining identifiability and estimation accuracy?
- Basis in paper: [explicit] The method models "latent exogenous variables under mild normality assumptions" assuming $(U_O, U_R) \sim N(0, \Sigma)$.
- Why unresolved: Real-world user behavior noise may not follow Gaussian distributions; the paper does not explore alternative distributions or distribution-free approaches.
- What evidence would resolve it: Theoretical analysis establishing identifiability conditions under non-Gaussian distributions, or empirical demonstrations that alternative distributions (e.g., mixture models, heavy-tailed distributions) improve performance when the normality assumption is violated.

### Open Question 3
- Question: How can the correlation parameter ρ be more reliably estimated when the true correlation between exogenous variables is weak?
- Basis in paper: [explicit] "When |ρ| is small, the variance of the estimated ρ is larger... This may suggest that when the magnitude of ρ is small, the correlation information contained in the observed data is less, which may damage model performance."
- Why unresolved: Weak correlations provide limited signal in observational data, leading to high estimation variance that propagates to prediction errors.
- What evidence would resolve it: Methods incorporating prior knowledge, regularization strategies, or semi-supervised approaches that stabilize ρ estimation when correlation is weak, with improved MSE/AUC metrics on synthetic data where |ρ| < 0.3.

## Limitations

- The normality assumption for correlated latent exogenous variables may not hold in real-world data, potentially limiting generalizability.
- The alternating optimization strategy may converge to suboptimal solutions if the symmetry assumption between structural equations is violated.
- Computational overhead from Monte Carlo sampling increases linearly with sample size, limiting scalability to extremely large datasets.

## Confidence

- **High Confidence**: The method's effectiveness in reducing selection bias when exogenous variables are correlated, as demonstrated by consistent improvements in AUC, Recall, and NDCG across multiple datasets.
- **Medium Confidence**: The proposed likelihood-based approach is theoretically sound under the stated assumptions, but real-world applicability depends on whether the correlation structure matches the model's assumptions.
- **Low Confidence**: The claim that this method is "plug-and-play" with any recommender backbone, as performance may vary significantly depending on the base model architecture and data characteristics.

## Next Checks

1. Test the method on datasets with known non-Gaussian correlation structures to evaluate robustness beyond the bivariate normal assumption.
2. Compare convergence and final performance against a joint optimization baseline (where θ_o and θ_r are updated simultaneously) to quantify the impact of the alternating strategy.
3. Conduct a systematic ablation study on the MC sample size L to identify the point of diminishing returns in terms of variance reduction versus computational cost.