---
ver: rpa2
title: Architecture for a Trustworthy Quantum Chatbot
arxiv_id: '2503.04875'
source_url: https://arxiv.org/abs/2503.04875
tags:
- quantum
- software
- code
- qiskit
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: C4Q 2.0 is a chatbot designed to provide trustworthy and accurate
  support for quantum computing. It addresses the challenge of unreliable responses
  from generic chatbots in specialized fields by integrating specialized LLMs for
  classification and question answering with a deterministic logical engine for quantum
  computations.
---

# Architecture for a Trustworthy Quantum Chatbot

## Quick Facts
- arXiv ID: 2503.04875
- Source URL: https://arxiv.org/abs/2503.04875
- Reference count: 40
- Primary result: C4Q 2.0 achieves near-perfect accuracy in quantum computing query classification and maintains high backend reliability through hybrid LLM and deterministic execution.

## Executive Summary
C4Q 2.0 is a chatbot designed to provide trustworthy and accurate support for quantum computing. It addresses the challenge of unreliable responses from generic chatbots in specialized fields by integrating specialized LLMs for classification and question answering with a deterministic logical engine for quantum computations. Key features include ready-to-run Qiskit code for gate definitions and circuit operations, solutions to classical programming problems using quantum methods, and a feedback mechanism for iterative improvement. C4Q 2.0 achieves near-perfect accuracy in classification and maintains high backend reliability, with all unit tests passing. Comparative evaluations show that C4Q 2.0 outperforms other chatbots in correctness and maintainability, particularly with newer versions of Qiskit. Its modular architecture ensures transparency, traceability, and adaptability, making it a reliable tool for quantum software engineering and education.

## Method Summary
C4Q 2.0 uses a hybrid architecture combining fine-tuned BERT models for classification (5 categories) and QA (9 question types) with a deterministic logical engine based on Qiskit for quantum computations. The system routes user queries through a classification LLM to determine intent, extracts parameters using a QA LLM, and delegates actual mathematical operations to a deterministic Logical Engine. Code generation uses pre-verified Qiskit templates filled with extracted parameters, with user confirmation for parameter verification. The architecture employs React frontend, Django API backend, PostgreSQL database, and Qiskit-based logical engine. Training took approximately 9 hours per model on MacBook Pro M2 Pro with 32GB RAM.

## Key Results
- Achieved 1.00 classification accuracy for quantum computing queries across five categories
- Maintained ~97.9% correctness across Qiskit versions <1.0.0 and >1.0.0, outperforming generic chatbots
- All 189 unit tests for backend reliability passed
- QA LLM Exact Match and F1 scores approximately 88.3%

## Why This Works (Mechanism)

### Mechanism 1: Separation of Probabilistic Parsing and Deterministic Execution
The system routes user queries through a fine-tuned BERT classifier to determine intent, then extracts parameters using a QA LLM but delegates actual mathematical operations to a deterministic Logical Engine. This ensures quantum state vectors and gate operations are computed mathematically rather than predicted textually, reducing hallucination rates common in generic LLMs.

### Mechanism 2: Template-Based Code Generation
Instead of token-by-token code generation, the system fills placeholders in pre-verified Qiskit code snippets with parameters extracted from user queries. This approach reduces syntax and logic errors and provides robustness against library version changes, as developers can revise templates rather than retrain models.

### Mechanism 3: Human-in-the-Loop Parameter Verification
Explicit user confirmation of extracted parameters mitigates the error rates of the QA extraction model. Users verify extracted parameters before the Logical Engine executes, filtering extraction errors (specifically the 30% error rate observed in phase shift extraction) before they affect computation.

## Foundational Learning

- **Concept: Hybrid Architecture (Neuro-Symbolic AI)**
  - Why needed here: To understand why C4Q does not rely solely on an LLM. You must grasp that LLMs provide the interface (parsing intent), while symbolic systems (Qiskit/Templates) provide the truth (logic).
  - Quick check question: Does the C4Q Logical Engine use an LLM to calculate the result of a quantum gate application?

- **Concept: Variational Quantum Algorithms (VQE/QAOA)**
  - Why needed here: C4Q solves optimization problems (TSP/Knapsack) by converting them to QUBO forms and running these specific algorithms. Understanding them is required to interpret the generated code.
  - Quick check question: When solving the Knapsack Problem, does C4Q use a brute-force search or a quantum optimizer like QAOA?

- **Concept: BERT Fine-tuning for Classification/QA**
  - Why needed here: The paper relies on fine-tuning BERT rather than prompting GPT-4. This distinction is crucial for understanding the low latency and high accuracy of the classification module compared to generic chatbots.
  - Quick check question: What mechanism does the Classification LLM use to distinguish between a request to "define" a gate vs "draw" a gate?

## Architecture Onboarding

- **Component map:** User Query -> API -> Classification LLM (Route) -> QA LLM (Extract Params) -> User Confirmation -> Logical Engine (Fill Template/Compute) -> Response

- **Critical path:** User Query -> API -> **Classification LLM** (Route) -> **QA LLM** (Extract Params) -> **User Confirmation** -> **Logical Engine** (Fill Template/Compute) -> Response

- **Design tradeoffs:**
  - Maintainability vs. Flexibility: Template system is robust against library updates (high maintainability) but fails on requests outside predefined scope (low flexibility)
  - Accuracy vs. Latency: Confirmation step adds a turn to conversation but is necessary to counter ~30% error rate in complex parameter extraction

- **Failure signatures:**
  - Parameter Confusion: System extracts "initial state" instead of "phase shift" (known QA LLM vulnerability)
  - Code Execution Error: Generated code fails if execution environment's Qiskit version doesn't match template structure
  - Classification Dead-end: Ambiguous queries might force classifier to pick wrong bucket, leading to irrelevant template generation

- **First 3 experiments:**
  1. Test the Feedback Loop: Submit Knapsack problem with deliberate parameter ambiguity to verify if QA LLM extracts correctly and if confirmation UI allows correction
  2. Version Regression Test: Execute generated Qiskit code from C4Q on local environment running Qiskit 1.0+ vs <1.0 to validate template claim regarding version stability
  3. Phase Shift Stress Test: Query system 10 times for "phase shift" applications to observe QA LLM's specific failure mode and mitigation workflow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of the QA LLM be improved for extracting complex parameters like phase shifts and Knapsack Problem values?
- Basis in paper: [explicit] "In the future, we aim to focus on improving the QA LLM's performance, particularly in accurately extracting phase shift parameters and interpreting KP-related parameters."
- Why unresolved: Current evaluation reveals failure rate of nearly 30% for phase shift extraction and 20% for KP item selection, often due to model confusing context
- What evidence would resolve it: Modified fine-tuning approach or architectural change that reduces parameter extraction error rates to below 10%

### Open Question 2
- Question: Does C4Q 2.0 effectively support educational outcomes and meet practical needs of quantum computing learners?
- Basis in paper: [explicit] "We plan to collaborate with quantum computing educators to conduct user studies, ensuring that C4Q 2.0 meets pedagogical needs and supports quantum education more effectively."
- Why unresolved: Current evaluation focuses on backend reliability, classification accuracy, and code correctness, but lacks empirical data on user learning success or usability
- What evidence would resolve it: Results from user studies with students and educators demonstrating improved learning metrics or high usability scores

### Open Question 3
- Question: Can template-based architecture be extended to handle general quantum programming queries without sacrificing trustworthiness?
- Basis in paper: [inferred] Paper highlights system relies on "Templated Qiskit Code Generation" and currently supports only "predefined set" of gates and specific problems (TSP, KP)
- Why unresolved: While templates ensure correctness, they limit chatbot's scope; paper doesn't address how to handle arbitrary or complex circuit constructions outside templates
- What evidence would resolve it: Mechanism that allows safe, on-the-fly code generation for user-defined circuits while maintaining high correctness standards of template approach

## Limitations
- System's ability to handle out-of-scope queries remains unknown, as paper doesn't explore scenarios where user queries fall outside template scope
- Reported performance metrics (classification accuracy, QA F1 scores) lack independent verification or comparison to established baselines
- Feedback mechanism's effectiveness is asserted but not empirically demonstrated in terms of actual improvement over time

## Confidence
- **High Confidence:** Hybrid architecture (separating classification, extraction, and deterministic computation) is well-specified and template-based code generation approach is technically sound for known problems
- **Medium Confidence:** Claimed performance metrics are internally consistent but lack independent verification or comparison to baseline models
- **Low Confidence:** System's ability to handle out-of-scope queries and actual impact of feedback mechanism on improving accuracy over time are not empirically demonstrated

## Next Checks
1. Cross-Validation Study: Test C4Q 2.0 against baseline system using GPT-4 for all tasks (classification, extraction, and code generation) on same quantum computing queries to quantify performance benefit of hybrid architecture

2. Template Coverage Analysis: Systematically generate 50 quantum computing queries across diverse topics (including novel algorithms not in five categories) to measure percentage that fall outside template coverage and evaluate fallback behavior

3. Longitudinal Feedback Impact: Deploy C4Q 2.0 with active feedback collection for 30 days, then measure if user feedback correlates with measurable improvements in extraction accuracy or classification precision compared to initial reported metrics