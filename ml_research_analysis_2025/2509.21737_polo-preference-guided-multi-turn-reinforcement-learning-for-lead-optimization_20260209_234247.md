---
ver: rpa2
title: 'POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization'
arxiv_id: '2509.21737'
source_url: https://arxiv.org/abs/2509.21737
tags:
- optimization
- learning
- polo
- molecule
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces POLO, a multi-turn reinforcement learning
  framework that transforms large language models into sample-efficient molecular
  optimization agents for lead optimization in drug discovery. POLO employs Preference-Guided
  Policy Optimization (PGPO), which extracts learning signals at two levels: trajectory-level
  reinforcement learning reinforces successful optimization strategies, while turn-level
  preference learning ranks intermediate molecules to provide dense comparative feedback.'
---

# POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization

## Quick Facts
- **arXiv ID:** 2509.21737
- **Source URL:** https://arxiv.org/abs/2509.21737
- **Reference count:** 40
- **Primary result:** POLO achieves 84% success rate on single-property tasks and 50% on multi-property tasks using only 500 oracle evaluations

## Executive Summary
This paper introduces POLO, a multi-turn reinforcement learning framework that transforms large language models into sample-efficient molecular optimization agents for lead optimization in drug discovery. POLO employs Preference-Guided Policy Optimization (PGPO), which extracts learning signals at two levels: trajectory-level reinforcement learning reinforces successful optimization strategies, while turn-level preference learning ranks intermediate molecules to provide dense comparative feedback. This dual-level approach enables POLO to learn from every molecular evaluation, maximizing sample efficiency. Experiments demonstrate that POLO achieves 84% average success rate on single-property tasks (2.3× better than baselines) and 50% on multi-property tasks using only 500 oracle evaluations, establishing a new state-of-the-art in sample-efficient molecular optimization.

## Method Summary
POLO trains LLMs for molecular optimization through a two-stage process: first fine-tuning on MolOptIns (500K high-similarity modification pairs), then applying PGPO training that combines trajectory-level PPO with turn-level preference learning. The framework uses a multi-turn MDP formulation where states encode complete optimization histories, enabling strategic planning. A critical two-stage trajectory filtering mechanism (top 50% by variance, then top 75% by reward) prevents training collapse. During inference, POLO employs an evolutionary strategy with elite pool maintenance, temperature scheduling, and oracle caching to maximize sample efficiency.

## Key Results
- POLO achieves 84% average success rate on single-property tasks, outperforming baselines by 2.3×
- On dual-property optimization, POLO reaches 50.3% average success rate with 500 oracle evaluations
- For triple-property tasks, POLO achieves 13.0% success rate, demonstrating the challenge of high-dimensional optimization
- POLO maintains high similarity (0.73-0.78) while improving target properties significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-level learning (trajectory + preference) extracts substantially more training signal per oracle call than single-level RL.
- Mechanism: Trajectory-level PPO reinforces successful optimization strategies via cumulative rewards R(τ). Turn-level preference learning ranks all molecules within each trajectory, generating O(T²) pairwise comparisons that provide dense credit assignment without additional oracle calls. The preference objective uses Lambda weights to prioritize pairs with large reward differences that are currently misranked.
- Core assumption: Intermediate molecular evaluations within a trajectory contain valid preference signal; ranking quality correlates with optimization progress.
- Evidence anchors:
  - [abstract] "PGPO...extracts learning signals at two complementary levels: trajectory-level optimization reinforces successful strategies, while turn-level preference learning provides dense comparative feedback"
  - [section 2.4] "While standard RL extracts only O(N) learning signals from N trajectories...our approach generates up to O(NT²) pairwise comparisons from the same data"
  - [corpus] TSPO paper addresses multi-turn RL with sparse rewards but uses different credit assignment; SEISMO uses trajectory-aware agents for molecular optimization but lacks the preference ranking mechanism
- Break condition: If trajectories are too short (T<3), preference signal degrades quadratically; if reward variance within trajectories is low, preference pairs become uninformative.

### Mechanism 2
- Claim: Multi-turn MDP formulation enables strategic planning that single-turn methods cannot achieve.
- Mechanism: State s_t encodes complete conversational history including all prior molecules and oracle scores. This allows the policy π_θ(a_t|s_t) to condition on accumulated experience, learning which modification patterns succeed rather than treating each optimization independently. The rollback mechanism reverts to best-scoring molecule after failures.
- Core assumption: LLMs can effectively use in-context history to improve subsequent decisions; the context window can encode meaningful optimization trajectories.
- Evidence anchors:
  - [abstract] "POLO enables LLMs to learn from complete optimization trajectories rather than isolated steps"
  - [section 2.3] "This multi-turn structure enables us to extract learning signals from every intermediate evaluation, not just the final outcome"
  - [corpus] LUMINA studies multi-turn agent limitations; MURPHY applies multi-turn GRPO to code generation with self-correction
- Break condition: If context length exceeds model capacity, earlier trajectory information is lost; if the LLM cannot reliably parse and reason over molecular history, strategic planning fails.

### Mechanism 3
- Claim: Two-stage trajectory filtering prevents training collapse into repetitive, meaningless generation.
- Mechanism: Stage 1 selects the 50% of lead-molecule groups with highest reward variance (diverse optimization challenges). Stage 2 retains top 75% of trajectories by cumulative reward within each group. This filters low-quality trajectories that would destabilize training while preserving sufficient data for preference learning.
- Core assumption: High-variance groups provide richer learning signal; low-quality trajectories actively harm training rather than being neutral.
- Evidence anchors:
  - [section B.1] "Without filtering, the model exhibits unstable training dynamics: the average response length suddenly explodes after approximately 35 training steps, indicating the model has collapsed into generating repetitive, meaningless tokens"
  - [section B.3] "With our two-stage filtering (blue line), training exhibits stable convergence with response length around 800-1000 tokens"
  - [corpus] No direct corpus evidence for this specific filtering mechanism in multi-turn RL
- Break condition: If filtering is too aggressive (<30% retention), insufficient training data; if too permissive (>90%), instability risk increases.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: Trajectory-level optimization uses PPO with clipped importance sampling to update policy π_θ stably. Understanding the clip ratio ε, advantage estimates Â_t, and importance ratio ρ_t is essential for debugging training.
  - Quick check question: Can you explain why PPO clips the importance ratio rather than using unclipped policy gradients?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Turn-level preference learning adapts DPO to multi-turn trajectories. The preference objective uses log-ratio log(π_θ/π_ref) to parameterize preferences without explicit reward modeling. The reference policy π_ref is initialized from SFT on MolOptIns.
  - Quick check question: How does DPO avoid training a separate reward model, and what role does the reference policy play?

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: Lead optimization is formalized as a multi-turn MDP M=⟨S,A,P,R⟩ where state includes conversation history, action is structured output (reasoning + SMILES), and rewards balance property improvement with similarity constraints.
  - Quick check question: In POLO's MDP, what information is encoded in state s_t and how does it differ from single-turn formulations?

## Architecture Onboarding

- Component map:
  Lead Molecule → Molecular Environment (oracle evaluation, reward computation)
                        ↓
  LLM Agent (Qwen2.5-1.5B + LoRA) ← Conversation History (molecules, scores)
                        ↓
  Structured Output (reasoning + SMILES) → Reward Signal
                        ↓
  PGPO Training Loop (PPO trajectory loss + DPO preference loss)
                        ↓
  Trajectory Filtering (variance + quality thresholds)
                        ↓
  Policy Update → Repeat
  Inference adds evolutionary refinement: trained agent → N rollouts per parent → elite pool maintenance.

- Critical path:
  1. SFT initialization on MolOptIns (500K examples) establishes π_ref and initial π_θ
  2. Trajectory collection with multi-turn rollouts (T=5 turns, N=16 rollouts per lead)
  3. Two-stage filtering (50% groups, 75% trajectories) produces D_filtered
  4. Dual-level optimization: J_PGPO(θ) = J_traj(θ) + λ_pref · J_pref(θ)
  5. Inference: evolutionary strategy with elite pool, temperature scheduling, oracle caching

- Design tradeoffs:
  - Trajectory length T=5: Longer trajectories provide more preference pairs but increase context load; ablation shows diminishing returns beyond 5-7 turns
  - Rollouts per parent N=32: More rollouts increase diversity but limit evolutionary generations under fixed budget; excessive parallelization degrades performance
  - Preference weight λ_pref=0.3: Higher values (0.5-0.7) can overshadow trajectory exploration; lower values (0.1) underutilize preference signal
  - Filtering thresholds (50%/75%): More aggressive filtering improves stability but reduces data; optimal balance prevents model collapse

- Failure signatures:
  - Response length explosion (>2000 tokens) indicates model collapse; immediately check trajectory filtering
  - Success rate plateau with single-turn methods suggests insufficient multi-turn context utilization
  - High similarity but low success rate indicates over-conservative exploration (reward shaping may be too punitive)
  - Preference loss not decreasing suggests uninformative pairs; check intra-trajectory reward variance

- First 3 experiments:
  1. Validate SFT quality: Before PGPO training, evaluate the fine-tuned model on held-out molecules to confirm it generates valid, high-similarity modifications (target: >90% validity, >0.6 similarity). This establishes the baseline chemical prior.
  2. Ablate trajectory filtering: Train with and without the two-stage filter on a small molecule set (32 leads). Monitor response length and validation performance. Expect collapse around step 35 without filtering.
  3. Sweep preference weight: Train with λ_pref ∈ {0, 0.1, 0.3, 0.5, 0.7} on QED task with 64 leads. Plot validation improvement vs. training step. Expect optimal performance at 0.3-0.5, plateau at 0, slight degradation at 0.7.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the POLO framework generalize effectively to scientific discovery domains beyond molecular optimization?
- Basis in paper: [explicit] Conclusion states POLO "provides a blueprint for applying language models to other scientific discovery tasks where iterative refinement and sample efficiency are paramount."
- Why unresolved: No experiments or discussion of applications outside drug discovery. The chemical priors from MolOptIns are domain-specific, and it's unclear whether the dual-level learning mechanism transfers to non-molecular domains.
- What evidence would resolve it: Benchmarking POLO on iterative optimization tasks in other scientific domains (e.g., materials design, protein engineering) with similar sample efficiency gains.

### Open Question 2
- Question: How can success rates on high-dimensional multi-objective optimization tasks be improved beyond the current 13% on triple-property tasks?
- Basis in paper: [inferred] Table 2 shows dramatic performance degradation from dual-objective (50.3% average) to triple-objective tasks (13.0%), indicating fundamental scalability challenges.
- Why unresolved: The paper attributes difficulty to "competing properties" but offers no architectural or algorithmic solutions for higher-dimensional objective spaces.
- What evidence would resolve it: Modified training objectives, reward shaping strategies, or curriculum approaches that demonstrate improved triple-objective performance.

### Open Question 3
- Question: What are the scaling laws for POLO with respect to model size, and where do returns diminish?
- Basis in paper: [inferred] Appendix L shows 1.5B→3B scaling improves QED success from 71.5% to 84.0%, but only tests one larger model size without establishing scaling trends.
- Why unresolved: Limited experimentation leaves unclear whether benefits continue linearly, plateau, or require architectural modifications at larger scales.
- What evidence would resolve it: Systematic evaluation across multiple model scales (1.5B, 3B, 7B, 13B+) showing performance scaling curves and identifying optimal model size for molecular optimization.

### Open Question 4
- Question: How well do computationally optimized molecules transfer to wet-lab experimental validation?
- Basis in paper: [inferred] All experiments use computational oracles (QED, DRD2, JNK3 predictors) rather than actual synthesis and biological testing.
- Why unresolved: Computational proxies may not capture real-world synthesizability constraints, off-target effects, or bioavailability issues that emerge experimentally.
- What evidence would resolve it: Selecting top POLO-optimized candidates and testing synthesis success rates and biological activity in actual wet-lab experiments.

## Limitations

- **Oracle Implementation Dependency**: POLO's performance critically depends on the accuracy and availability of black-box property oracles, creating a significant reproducibility barrier.
- **Trajectory Filtering Sensitivity**: The two-stage filtering (50% variance, 75% reward) prevents training collapse but introduces hyperparameter sensitivity with no ablation studies showing robustness.
- **Context Length Constraints**: Multi-turn reasoning requires accumulating conversation history in the LLM context, with performance degradation when context window constraints force truncation of early trajectory information.

## Confidence

- **High Confidence**: Single-property optimization results (84% SR with 500 oracle calls) are well-supported by ablation studies and comparison baselines. The trajectory-level PPO and turn-level preference learning mechanisms are clearly specified and mathematically grounded.
- **Medium Confidence**: Multi-property optimization performance (50% SR) and inference-time evolutionary strategy results show promising trends but have less rigorous validation.
- **Low Confidence**: Claims about POLO's superiority in sample efficiency relative to non-LLM methods are difficult to verify due to oracle implementation differences and lack of direct comparisons with established baselines.

## Next Checks

1. **Oracle Independence Test**: Evaluate POLO's policy using multiple oracle implementations (e.g., different bioactivity prediction models) to verify performance consistency. If success rates vary >15% across oracles, the learned policy may be overfit to specific oracle behaviors rather than genuine molecular optimization principles.

2. **Filtering Robustness Sweep**: Systematically vary the two-stage filtering thresholds (30%-70% variance, 50%-90% reward) across multiple molecular tasks. Measure training stability (response length variance) and final success rates. Identify threshold ranges where performance degrades gracefully versus catastrophically.

3. **Context Length Sensitivity Analysis**: Train POLO with progressively longer trajectory lengths (T=3, 5, 7, 9) and measure performance degradation. Plot success rate vs. context length to identify the point where performance plateaus or declines. This determines whether POLO's multi-turn advantage is limited by practical context constraints.