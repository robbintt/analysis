---
ver: rpa2
title: 'CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual
  Discourse Relation Classification'
arxiv_id: '2509.16903'
source_url: https://arxiv.org/abs/2509.16903
tags:
- discourse
- task
- language
- pages
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses discourse relation classification across 16
  languages and 6 frameworks using a unified set of 17 labels. The approach benchmarks
  multilingual transformers, prompt-based LLMs, and introduces HiDAC, a hierarchical
  dual-adapter contrastive learning model.
---

# CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification

## Quick Facts
- **arXiv ID:** 2509.16903
- **Source URL:** https://arxiv.org/abs/2509.16903
- **Reference count:** 22
- **Primary result:** Hierarchical dual-adapter contrastive learning (HiDAC) achieves 67.5% accuracy with 16.7% fewer parameters than full fine-tuning

## Executive Summary
This work addresses discourse relation classification across 16 languages and 6 frameworks using a unified set of 17 labels. The approach benchmarks multilingual transformers, prompt-based LLMs, and introduces HiDAC, a hierarchical dual-adapter contrastive learning model. HiDAC combines intermediate-layer contrastive learning with upper-layer cross-entropy classification, achieving 67.5% accuracy with fewer parameters than full fine-tuning. Progressive unfreezing of 75% of encoder layers matched full fine-tuning performance. Prompt-based models significantly underperformed, highlighting the challenge of cross-framework generalization. The results demonstrate that adapter-based methods can reduce training cost without sacrificing accuracy.

## Method Summary
The study introduces HiDAC, a hierarchical dual-adapter contrastive learning model for discourse relation classification. The approach combines intermediate-layer contrastive learning with upper-layer cross-entropy classification. Key technical innovations include: (1) progressive unfreezing strategy where 75% of encoder layers achieve performance matching full fine-tuning, (2) contrastive learning at intermediate layers to capture semantic similarities across frameworks, and (3) adapter-based fine-tuning that reduces parameters by 16.7% compared to full fine-tuning. The unified label set approach maps 6 different discourse frameworks to 17 shared categories, enabling cross-framework generalization.

## Key Results
- HiDAC achieves 67.5% accuracy across 16 languages and 6 discourse frameworks
- Adapter-based approach uses 16.7% fewer parameters than full fine-tuning
- Progressive unfreezing of 75% of encoder layers matches full fine-tuning performance
- Prompt-based models significantly underperform compared to adapter-based methods

## Why This Works (Mechanism)
The hierarchical dual-adapter architecture works by leveraging contrastive learning at intermediate layers to capture semantic similarities across different discourse frameworks, while the upper layers perform task-specific classification. This dual-stage approach allows the model to learn framework-agnostic representations before specializing for the unified label set. The adapter-based parameter efficiency stems from freezing most transformer layers while only fine-tuning small adapter modules, reducing computational overhead without sacrificing accuracy. The progressive unfreezing strategy enables gradual adaptation, starting with broader semantic learning before fine-tuning deeper layers for task-specific features.

## Foundational Learning
- **Discourse relation classification:** Why needed: Core task of identifying semantic relationships between text segments across languages and frameworks. Quick check: Understanding PDTB, RST, and other discourse frameworks and their annotation schemes.
- **Adapter-based fine-tuning:** Why needed: Parameter-efficient alternative to full fine-tuning for adapting large language models to new tasks. Quick check: Familiarity with adapter architectures and their placement in transformer models.
- **Contrastive learning:** Why needed: Enables learning semantic similarities across different frameworks without explicit alignment. Quick check: Understanding contrastive loss functions and their application in representation learning.
- **Progressive unfreezing:** Why needed: Strategic layer-by-layer adaptation strategy for efficient fine-tuning. Quick check: Knowledge of layer-wise learning rates and their impact on model convergence.
- **Cross-framework generalization:** Why needed: Essential for handling multiple discourse annotation schemes simultaneously. Quick check: Understanding the challenges of mapping between different discourse frameworks.
- **Multilingual transformer models:** Why needed: Foundation architecture for handling multiple languages in discourse analysis. Quick check: Familiarity with mBERT, XLM-R, and their cross-lingual capabilities.

## Architecture Onboarding

**Component Map:**
Encoder layers (frozen) -> Adapter modules (trainable) -> Contrastive learning (intermediate layers) -> Cross-entropy classification (upper layers)

**Critical Path:**
Input text → Multilingual encoder → Adapter modules → Contrastive representation learning → Classification head → Discourse relation labels

**Design Tradeoffs:**
- Adapter vs. full fine-tuning: 16.7% parameter reduction vs. potential accuracy gains
- Contrastive vs. supervised-only learning: Better framework generalization vs. increased complexity
- Progressive unfreezing percentage: 75% optimal balance vs. computational cost
- Unified label set: 17 labels for cross-framework compatibility vs. potential loss of framework-specific nuance

**Failure Signatures:**
- Low contrastive learning performance indicates poor framework alignment
- Accuracy drops with progressive unfreezing suggest layer importance misestimation
- High variance across languages indicates multilingual model limitations
- Poor performance on certain discourse relations reveals label set mapping issues

**3 First Experiments to Run:**
1. Compare adapter-based vs. full fine-tuning performance on a single language-framework pair
2. Test contrastive learning effectiveness by evaluating representation similarity across frameworks
3. Evaluate progressive unfreezing strategy by varying the percentage of unfrozen layers

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- 67.5% accuracy improvement of 1.3 points over fine-tuning may have limited practical significance without specified downstream applications
- Benchmark includes only 16 languages and 6 frameworks, potentially limiting generalizability
- Performance gap between adapter-based and prompt-based methods suggests remaining challenges in true framework-agnostic classification
- Scalability of progressive unfreezing beyond 75% threshold remains unexplored

## Confidence

**High confidence** in adapter-based efficiency gains: The 16.7% parameter reduction with maintained accuracy is directly measurable and reproducible

**Medium confidence** in cross-framework generalization: While the unified label set approach shows promise, the performance gap between adapter-based and prompt-based methods suggests remaining challenges in truly framework-agnostic classification

**Low confidence** in scalability claims: The progressive unfreezing strategy's effectiveness beyond the tested 75% threshold remains unexplored

## Next Checks
1. **Zero-shot framework transfer test:** Evaluate HiDAC's performance on entirely unseen discourse frameworks to validate true cross-framework generalization claims
2. **Long-tail label analysis:** Examine performance disparities across the 17 unified labels to identify whether certain discourse relations remain particularly challenging
3. **Computational overhead measurement:** Quantify inference-time differences between adapter-based and full fine-tuning approaches across varying batch sizes and hardware configurations