---
ver: rpa2
title: A Trustworthy By Design Classification Model for Building Energy Retrofit Decision
  Support
arxiv_id: '2504.06055'
source_url: https://arxiv.org/abs/2504.06055
tags:
- energy
- data
- building
- feature
- retrofit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a trustworthy-by-design machine learning
  framework for recommending energy efficiency retrofit measures in residential buildings.
  The approach uses multi-label classification to suggest combinations of retrofit
  actions, requiring only minimal user-accessible inputs such as basic building characteristics
  and energy performance data.
---

# A Trustworthy By Design Classification Model for Building Energy Retrofit Decision Support

## Quick Facts
- **arXiv ID:** 2504.06055
- **Source URL:** https://arxiv.org/abs/2504.06055
- **Reference count:** 40
- **Primary result:** ML framework recommending retrofit measures using multi-label classification, achieving up to 53% recall improvement with synthetic data augmentation.

## Executive Summary
This study presents a trustworthy-by-design machine learning framework for recommending energy efficiency retrofit measures in residential buildings. The approach uses multi-label classification to suggest combinations of retrofit actions based on minimal user-accessible inputs such as building characteristics and energy performance data. The framework incorporates synthetic data generation via CTGAN to address class imbalance and employs SHAP for model interpretability and feature engineering. Evaluated on UK and Latvian datasets, the model demonstrates robustness and transparency for sustainable energy transitions while addressing key challenges in retrofit decision support.

## Method Summary
The framework uses a Multi-Layer Perceptron (MLP) with sigmoid output activations and Binary Cross-Entropy loss for multi-label classification of four retrofit categories. Data preprocessing includes min-max scaling and label encoding, with synthetic data generation via CTGAN to address class imbalance. SHAP analysis guides feature engineering by identifying and removing spurious features that may cause overfitting. The model is trained using PyTorch Lightning with Optuna hyperparameter optimization and evaluated using multi-label metrics including accuracy, precision, recall, and F1-score.

## Key Results
- Multi-label classification framework achieved 53% improvement in recall for retrofit measure detection
- CTGAN synthetic data generation significantly improved recall for minority classes, particularly in the Latvian dataset
- SHAP-based feature selection revealed and mitigated overfitting to location-specific features
- Framework demonstrated robustness across UK EPC and Latvian datasets with different data characteristics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-label classification enables simultaneous recommendation of coherent retrofit combinations
- **Mechanism:** MLP with sigmoid outputs predicts probabilities for four retrofit categories simultaneously, capturing dependencies between measures
- **Core assumption:** Historical data contains implicit patterns of effective retrofit combinations
- **Evidence anchors:** Abstract mentions multi-label classifier for retrofit combinations; section 3.1 justifies multi-label approach; related work focuses on single-target prediction
- **Break condition:** If retrofit measures are statistically independent or only single measures exist in dataset

### Mechanism 2
- **Claim:** CTGAN synthetic data improves recall for underrepresented retrofit measures
- **Mechanism:** CTGAN generates synthetic samples conditioned on minority classes, pushing decision boundary away from majority class
- **Core assumption:** Generative model captures joint distribution sufficiently well to augment minority classes without significant noise
- **Evidence anchors:** Abstract highlights CTGAN for class imbalance; section 5.4 shows recall improvement from 41.5% to 63.9%; general support for data augmentation in energy domains
- **Break condition:** If original dataset too small to train stable generator or synthetic data diverges from physical reality

### Mechanism 3
- **Claim:** SHAP-based feature selection removes spurious correlations and enhances generalization
- **Mechanism:** SHAP quantifies feature contributions, enabling removal of features causing shortcut learning (e.g., location overfitting)
- **Core assumption:** High SHAP values for non-physical features indicate overfitting risks
- **Evidence anchors:** Abstract mentions SHAP for interpretability; section 5.2 shows location features contribute unexpectedly and their removal doesn't affect predictive capacity; Explainable AI typically used for post-hoc analysis
- **Break condition:** If domain knowledge is incorrect or critical confounding variables are removed

## Foundational Learning

- **Concept: Multi-label Classification (vs. Multi-class)**
  - **Why needed here:** Retrofit requires recommending a set of measures that often co-occur
  - **Quick check question:** Does the output layer use a single `softmax` activation (multi-class) or multiple `sigmoid` activations (multi-label)?

- **Concept: Class Imbalance & Evaluation Metrics**
  - **Why needed here:** Retrofit datasets have few examples of deep renovation compared to minor updates
  - **Quick check question:** If a model predicts "No Retrofit" for 100% of cases in a dataset where only 5% of buildings are retrofitted, what is the accuracy? (Answer: 95%, but Recall is 0%.)

- **Concept: Shapley Values (SHAP)**
  - **Why needed here:** Mechanism for trustworthy AI that attributes predictions to specific features
  - **Quick check question:** If a feature "Zip Code" has high SHAP value for a specific prediction but low global importance, does that suggest robust or brittle model behavior?

## Architecture Onboarding

- **Component map:** Input Layer -> Preprocessing -> Feature Engineering -> Data Augmentation -> Core Model -> Explainability Layer
- **Critical path:** Train Preliminary Model -> Analyze SHAP Global Importance -> Identify & Remove Spurious Features -> Augment Data with CTGAN -> Retrain Final Model
- **Design tradeoffs:**
  - High-level umbrella categories (4 classes) vs. specific measures (improves robustness but reduces granularity)
  - Real vs. Synthetic data (preserves authenticity vs. improves minority class detection)
  - MLPs vs. complex architectures (easier deployment and explanation vs. potential performance gains)
- **Failure signatures:**
  - High Accuracy, Low Recall: Model predicting only majority class
  - Dominance of Location Features: SHAP shows "City ID" as top predictor
  - Synthetic Drift: Column Pair Trends Score drops below 60%
- **First 3 experiments:**
  1. Train baseline MLP on raw UK EPC data with all features, measure F1-score
  2. Apply SHAP analysis, remove bottom 20% of features by importance, retrain and compare F1-score
  3. On Latvian dataset, compare Recall for "DHW Upgrades" class between raw data and CTGAN-augmented models

## Open Questions the Paper Calls Out

- **Open Question 1:** Can transfer learning and lifelong learning strategies successfully extend the framework to new geographical regions while retaining previously acquired knowledge?
- **Open Question 2:** How can the trade-off between domain-specific feature engineering and model generalization be resolved?
- **Open Question 3:** To what extent do alternative data generation techniques or model architectures improve performance compared to the current CTGAN-MLP combination?

## Limitations

- Evaluation focuses primarily on UK and Latvian datasets with limited testing on truly unseen building stocks
- CTGAN augmentation effectiveness lacks extensive validation of synthetic data quality beyond basic metrics
- SHAP-based feature engineering applied in somewhat black-box manner without explicit sensitivity analysis of different pruning thresholds

## Confidence

- **High confidence:** Multi-label classification framework design and appropriateness for retrofit recommendation
- **Medium confidence:** Synthetic data generation effectiveness (supported by recall improvements but limited synthetic data quality validation)
- **Medium confidence:** SHAP-based feature selection impact (supported by overfitting detection but limited exploration of alternative approaches)

## Next Checks

1. Test model generalization on a geographically and architecturally distinct building dataset not used in training
2. Conduct ablation studies comparing CTGAN augmentation with alternative approaches (SMOTE, simple oversampling)
3. Perform cross-validation with different train/test splits to assess stability of the 53% recall improvement claim