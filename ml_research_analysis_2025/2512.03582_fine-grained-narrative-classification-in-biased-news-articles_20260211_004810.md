---
ver: rpa2
title: Fine-grained Narrative Classification in Biased News Articles
arxiv_id: '2512.03582'
source_url: https://arxiv.org/abs/2512.03582
tags:
- narrative
- classification
- persuasive
- fine-grained
- articles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for fine-grained narrative
  classification in biased news articles. The authors developed INDI-PROP, a dataset
  of 1,266 Indian news articles annotated for ideological bias (pro-government, pro-opposition,
  neutral), event-specific narratives, and persuasive techniques.
---

# Fine-grained Narrative Classification in Biased News Articles

## Quick Facts
- arXiv ID: 2512.03582
- Source URL: https://arxiv.org/abs/2512.03582
- Reference count: 0
- Key outcome: Novel FANTA and TPTC frameworks achieve 0.728 micro-F1 on persuasive techniques through hierarchical reasoning in biased news analysis

## Executive Summary
This paper introduces a novel framework for fine-grained narrative classification in biased news articles. The authors developed INDI-PROP, a dataset of 1,266 Indian news articles annotated for ideological bias (pro-government, pro-opposition, neutral), event-specific narratives, and persuasive techniques. They propose FANTA for bias and narrative classification, leveraging entity-relation dynamics and context framing, and TPTC for persuasive technique identification, which decomposes the task into coarse and fine-grained levels. Experiments show substantial improvements over fine-tuned PLMs, with TPTC achieving 0.728 micro-F1 and 0.784 weighted-F1 on persuasive techniques, demonstrating that hierarchical reasoning enhances model performance in propaganda analysis.

## Method Summary
The authors propose two inference-only frameworks using GPT-4o-mini for fine-grained narrative classification in biased news articles. FANTA employs multi-hop reasoning through sequential stages: named entity recognition, co-reference resolution, entity-relation extraction, and context framing for bias and narrative classification. TPTC uses a two-stage coarse-to-fine approach for persuasive technique identification, first mapping text to 7 coarse-grained categories (G1-G7) before mapping to 20 fine-grained techniques. The INDI-PROP dataset contains 1,266 articles from 5 Indian news outlets, annotated for bias, event-specific narratives across CAA and Farmers' Protest, and 20 persuasive techniques.

## Key Results
- TPTC achieves 0.728 micro-F1 and 0.784 weighted-F1 on persuasive techniques
- FANTA achieves 0.770-0.780 weighted-F1 on bias classification
- Model shows high sensitivity to political keywords, sometimes over-interpreting factual reporting as ideologically biased
- Error analysis reveals main source of error is confusion between neutral and biased articles

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical multi-hop reasoning improves bias and narrative classification over single-step inference. FANTA decomposes classification into two stages—Information Extraction (NER, co-reference, entity-relation pairs) and Context Framing (positioning actors/institutions within discourse). This layered approach treats bias as an "interpretive conclusion rather than a direct classification task." Core assumption: bias is rarely explicit in isolated statements but encoded in tone, emphasis, and moral perspective across entity interactions.

### Mechanism 2
Two-stage coarse-to-fine decomposition substantially improves persuasive technique identification. TPTC first maps text to 7 coarse-grained persuasive categories, then maps each category to its constituent fine-grained techniques. This limits the search space and provides conceptual scaffolding before fine-level classification. Core assumption: persuasion operates as a multi-layered communicative phenomenon where coarse intent is easier to identify than specific techniques.

### Mechanism 3
Event-specific narrative taxonomies anchored to ideological polarity enable interpretable storyline classification. Narrative labels are explicitly tied to ideological side, communicative intent, and discursive focus. The bias label from the first stage constrains the narrative search space to pro-government or pro-opposition branches. Core assumption: narratives function as "cognitive and emotional scaffolds" that organize isolated persuasive techniques into coherent ideological storylines.

## Foundational Learning

- **Concept: Multi-hop Chain-of-Thought Prompting**
  - Why needed: Both FANTA and TPTC rely on decomposing complex classification into sequential reasoning steps
  - Quick check: Can you explain why asking an LLM to "first identify entities, then analyze framing" might outperform asking it to "classify bias" directly?

- **Concept: Propaganda Technique Taxonomies (e.g., SemEval shared tasks)**
  - Why needed: TPTC's coarse-grained categories are derived from 20 fine-grained techniques established in prior work
  - Quick check: What is the difference between "Loaded Language" and "Name Calling/Labeling" as persuasive techniques?

- **Concept: Named Entity Recognition and Coreference Resolution**
  - Why needed: FANTA's first stage requires extracting entities and resolving references across articles
  - Quick check: Given "The PM said he would veto the bill," what entities and relations would FANTA extract?

## Architecture Onboarding

- **Component map:** Raw article → NER + Coreference + Entity-Relation Extraction → Context Framing → Bias Classification → (if biased) Narrative Classification using event-specific taxonomy
- **Critical path:** Bias classification accuracy determines narrative classification validity—errors propagate if neutral articles are misclassified as biased (or vice versa)
- **Design tradeoffs:** Concise vs. Full FANTA (computational cost vs. reasoning depth), LLM vs. PLM baselines (precision vs. recall), event-specific vs. generalizable taxonomies (interpretability vs. transfer)
- **Failure signatures:** Over-prediction of bias, partially correct narratives, low macro-F1 on persuasion techniques
- **First 3 experiments:** 1) Ablate context framing stage to isolate contribution, 2) Cross-event generalization (train on CAA, test on Farmers' Protest), 3) Coarse category confusion analysis

## Open Questions the Paper Calls Out

### Open Question 1
Can FANTA and TPTC generalize to diverse socio-political events beyond CAA and the Farmers' protest without extensive re-annotation? The authors state the current focus is restricted to two specific events and that future work must address this by "expanding event coverage."

### Open Question 2
How does the integration of multimodal evidence, such as images or article metadata, impact the accuracy of narrative classification? The authors list the reliance on "textual cues" and the lack of "multimodal evidence" as a primary limitation.

### Open Question 3
To what extent can the proposed frameworks maintain performance when applied to multilingual or code-mixed Indian media content? The paper notes that experiments are "limited to English news sources" and explicitly suggests "incorporating multilinguality" as a necessary extension.

## Limitations

- Event-specific narrative taxonomies require hand-crafted design per event, limiting transfer to new domains
- Model over-interprets political keywords as ideological bias, struggling to distinguish neutral reporting
- Limited to English news sources without multimodal evidence integration

## Confidence

- FANTA effectiveness: High (systematic improvements over PLM baselines)
- TPTC two-stage approach: Medium-High (strong micro-F1 but concerning macro-F1 imbalance)
- Generalizability claims: Low (extensive re-annotation needed for new events)

## Next Checks

1. **Cross-domain transfer test**: Evaluate FANTA on propaganda datasets from different political contexts (e.g., US election coverage) to assess whether the hierarchical reasoning approach generalizes beyond Indian political discourse.

2. **Error propagation analysis**: Systematically measure how classification errors in the first stage (bias/neutral detection) affect downstream narrative and technique identification, quantifying the cascading impact of initial misclassifications.

3. **Annotation reliability verification**: Re-annotate a random 10% sample of INDI-PROP articles using the published guidelines to establish inter-annotator agreement rates and validate the claimed 0.77-0.82 agreement scores.