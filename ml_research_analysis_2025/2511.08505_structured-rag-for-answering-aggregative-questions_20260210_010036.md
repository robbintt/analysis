---
ver: rpa2
title: Structured RAG for Answering Aggregative Questions
arxiv_id: '2511.08505'
source_url: https://arxiv.org/abs/2511.08505
tags:
- answer
- documents
- questions
- queries
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: S-RAG addresses the challenge of answering aggregative questions
  over unstructured corpora, which require reasoning over information distributed
  across many documents. Traditional RAG systems struggle with these queries due to
  retrieval completeness, context size limits, and long-range contextualization issues.
---

# Structured RAG for Answering Aggregative Questions

## Quick Facts
- arXiv ID: 2511.08505
- Source URL: https://arxiv.org/abs/2511.08505
- Reference count: 11
- Primary result: S-RAG-GoldSchema achieves 0.845 answer recall and 0.899 answer comparison on HOTELS, outperforming vector-based RAG systems by large margins.

## Executive Summary
Structured RAG (S-RAG) addresses the challenge of answering aggregative questions over unstructured corpora by converting documents into structured records stored in a SQL database. During ingestion, it predicts a schema from example documents and generates structured records; during inference, it translates natural language queries into SQL queries over this structured representation. The method shows substantial improvements over baselines on two new aggregative QA datasets (HOTELS and WORLD CUP) and a public benchmark (FinanceBench).

## Method Summary
S-RAG transforms unstructured document corpora into structured SQL representations to enable precise aggregative queries. The ingestion phase involves iterative schema prediction from sample documents and questions, followed by record extraction and type validation for each document. During inference, the system translates natural language questions into SQL queries using the schema and column statistics, executes them against the database, and formats results as natural language answers. A hybrid mode combines SQL retrieval with vector-based RAG for cases where the schema is insufficient.

## Key Results
- S-RAG-GoldSchema achieves 0.845 answer recall and 0.899 answer comparison on HOTELS dataset
- S-RAG-InferredSchema achieves 0.690 answer recall and 0.823 answer comparison on HOTELS dataset
- S-RAG-Hybrid achieves 0.740 answer recall and 0.813 answer comparison on FinanceBench aggregative subset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured representation eliminates the retrieval completeness problem by enabling exact-set operations over all documents.
- Mechanism: By converting each document into a schema-conformant record stored in a SQL database, S-RAG transforms an approximate retrieval problem into an exact query problem. Aggregations (COUNT, AVG, MAX) and filters operate over the full corpus deterministically, rather than over a truncated retrieval set.
- Core assumption: Documents in the corpus represent instances of a common entity type with shared, extractable attributes.
- Evidence anchors: [abstract] "At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation."
- Break condition: If documents are heterogeneous (multiple schemas) or attributes are not consistently extractable, the single-table model fails.

### Mechanism 2
- Claim: Schema-driven standardization enables cross-document comparison by normalizing lexical variations at ingestion time.
- Mechanism: During record prediction, the LLM receives attribute descriptions and example values, forcing consistent lexicalization (e.g., "1M" vs "1,000,000" vs "1B") across documents. This preprocessing shifts the burden of normalization from inference to ingestion.
- Core assumption: The schema prediction step captures query-relevant attributes with sufficiently precise descriptions.
- Evidence anchors: [section 3.2.2] "Because the same descriptions and examples are shared across the prediction of different records, this process enables cross-document standardization."
- Break condition: Vague or missing attribute descriptions lead to inconsistent value formats, breaking SQL filters and aggregations.

### Mechanism 3
- Claim: Text-to-SQL translation bypasses embedding limitations for precise filter and aggregation semantics.
- Mechanism: SQL expresses exact constraints (e.g., "greater than 1000 employees", "before 2020") that dense embeddings struggle to capture via similarity search. The LLM generates SQL using schema metadata and column statistics, then the database executes it deterministically.
- Core assumption: The LLM can reliably translate natural language questions into correct SQL given the schema and statistics.
- Evidence anchors: [section 2] "Embedders limitation: sparse and dense embedders are likely to struggle to capture the full semantic meaning of filters."
- Break condition: Complex natural language queries with ambiguous or multi-step reasoning may produce incorrect SQL; no fallback correction loop is described.

## Foundational Learning

- Concept: SQL aggregation and filtering semantics
  - Why needed here: Understanding how GROUP BY, WHERE, and aggregate functions (AVG, COUNT, MAX) work is essential to grasp why S-RAG's approach solves aggregative queries that vector similarity cannot.
  - Quick check question: Can you write a SQL query that computes the average rating of hotels with more than 100 reviews?

- Concept: Schema design and normalization
  - Why needed here: The quality of the predicted schema directly determines what questions can be answered. Understanding tradeoffs between granularity and coverage is critical.
  - Quick check question: Given a corpus of product reviews, what attributes would you include in a schema to support queries about average ratings by category?

- Concept: Text-to-SQL / semantic parsing
  - Why needed here: The inference phase relies on LLM-based NL-to-SQL translation. Understanding failure modes (schema linking, ambiguous references) helps diagnose inference errors.
  - Quick check question: What ambiguities might arise when translating "top-performing companies last year" into SQL without prior schema context?

## Architecture Onboarding

- Component map: Schema Predictor -> Record Predictor -> SQL Database -> Query Translator -> Answer Generator
- Critical path: Schema quality → Record standardization → SQL translation accuracy. Errors propagate forward; a missing or poorly described attribute in the schema cannot be recovered later.
- Design tradeoffs:
  - Single-schema assumption vs. multi-schema corpora (current design assumes homogeneous documents)
  - Ingestion compute cost vs. inference speed (upfront extraction cost enables fast SQL queries)
  - Gold schema (manual, high quality) vs. inferred schema (automated, may miss attributes)
- Failure signatures:
  - Low recall on aggregative queries → likely missing attributes in schema
  - Incorrect filter results → likely inconsistent value standardization (e.g., unit mismatches)
  - SQL execution errors → likely type validation gaps or ambiguous attribute descriptions
- First 3 experiments:
  1. Ablate schema quality: Compare GoldSchema vs. InferredSchema on a held-out query set to quantify schema prediction impact.
  2. Test hybrid mode: Run HYBRID-S-RAG on FinanceBench full set to measure gains on mixed aggregative/non-aggregative workloads.
  3. Stress-test standardization: Introduce controlled lexical variations (e.g., "1M" vs "1,000,000") in a synthetic corpus and measure SQL filter accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the S-RAG framework be extended to support heterogeneous corpora where documents adhere to multiple distinct schemas rather than a single unified schema?
- Basis in paper: [explicit] The Conclusion explicitly identifies this as a limitation: "our approach is limited to corpora that can be represented by a single schema, whereas in the real world a corpus may contain documents derived from multiple schemas."
- Why unresolved: The current methodology assumes a single entity type per corpus (e.g., all documents are "Hotels"), relying on a universal attribute set for SQL generation.
- What evidence would resolve it: A modified S-RAG implementation evaluated on a multi-domain dataset that successfully maps documents to different schemas and executes cross-schema queries.

### Open Question 2
- Question: How can the pipeline be adapted to handle complex data types, such as nested objects or lists, which were excluded in this work?
- Basis in paper: [explicit] The Conclusion encourages "future research on corpora that incorporate more complex structures," and Footnote 3 notes that list and nested attributes were excluded "for simplicity at inference time."
- Why unresolved: The current system relies on a flat schema structure to simplify text-to-SQL translation, limiting its applicability to documents with deep hierarchical data.
- What evidence would resolve it: An evaluation on a dataset containing complex nested attributes (e.g., detailed financial tables) where S-RAG maintains high answer recall without manual flattening.

### Open Question 3
- Question: What techniques can improve the automated schema prediction phase to eliminate standardization errors, such as unit inconsistencies, observed in the InferredSchema experiments?
- Basis in paper: [explicit] The Conclusion highlights schema prediction as an "important direction for future research," while the Results section attributes performance drops to "incomplete descriptions which lead to standardization issues."
- Why unresolved: The LLM-based schema prediction currently lacks a robust verification mechanism to ensure that attribute descriptions enforce consistent units (e.g., distinguishing 1M from 1B) across all documents.
- What evidence would resolve it: A refined ingestion method that significantly closes the performance gap between S-RAG-InferredSchema and S-RAG-GoldSchema on the FinanceBench dataset.

### Open Question 4
- Question: What is the optimal strategy for dynamically selecting between the standard SQL inference mode and the Hybrid mode to maximize accuracy?
- Basis in paper: [inferred] The paper introduces a Hybrid Inference Mode (HYBRID-S-RAG) to handle cases where the schema is insufficient but does not define a rigorous method for detecting these cases a priori.
- Why unresolved: The system currently lacks a defined mechanism to automatically detect when a schema is "incomplete" for a given query to trigger the fallback to vector-based RAG.
- What evidence would resolve it: An ablation study analyzing a decision mechanism for switching modes, showing improved efficiency or accuracy over using a single mode for all queries.

## Limitations
- Limited to homogeneous corpora with single unified schema
- Does not handle complex nested data structures or multi-step reasoning
- No automatic mechanism for detecting when to use hybrid mode
- Evaluation limited to synthetic datasets and filtered subset of FinanceBench

## Confidence
- Medium: The method is well-justified for homogeneous document corpora with recurring structure, and empirical gains on curated datasets are substantial. However, the assumptions about schema homogeneity and consistent value extraction are strong, and the lack of ablation studies on schema quality or standardization limits the evidence base.

## Next Checks
1. Test S-RAG on a heterogeneous corpus (multiple entity types) to measure schema prediction failure rate and accuracy degradation.
2. Introduce controlled lexical variations (e.g., "1M", "1,000,000", "1B") in a synthetic dataset and measure the impact on SQL filter accuracy and answer recall.
3. Conduct a human evaluation of SQL translation correctness and answer usefulness on a subset of queries, comparing against vector-based RAG baselines.