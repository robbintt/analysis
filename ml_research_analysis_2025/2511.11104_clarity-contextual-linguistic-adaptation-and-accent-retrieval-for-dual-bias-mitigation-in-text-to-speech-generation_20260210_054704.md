---
ver: rpa2
title: 'CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias
  Mitigation in Text-to-Speech Generation'
arxiv_id: '2511.11104'
source_url: https://arxiv.org/abs/2511.11104
tags:
- accent
- text
- speech
- accents
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLARITY introduces a dual-signal framework to jointly mitigate
  accent and linguistic bias in instruction-guided TTS. It uses LLM-guided text adaptation
  to localize input to the target dialect and retrieval-augmented accent prompting
  (RAAP) to supply accent-consistent speech prompts.
---

# CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation

## Quick Facts
- arXiv ID: 2511.11104
- Source URL: https://arxiv.org/abs/2511.11104
- Reference count: 0
- Primary result: CLARITY achieves up to 63.36% accent accuracy, reduces FDR, and maintains high NISQA quality (4.31) across 12 English accents

## Executive Summary
CLARITY addresses accent and linguistic bias in instruction-guided TTS by introducing a dual-signal framework that combines LLM-guided text adaptation with retrieval-augmented accent prompting (RAAP). The system parses user instructions into structured metadata, adapts text to match target dialects, and retrieves accent-consistent audio prompts to condition the TTS backbone. Evaluated across twelve English accents, CLARITY demonstrates significant improvements in accent accuracy (63.36% vs 46% baseline), reduces bias toward dominant accents (US/CA), and maintains high perceptual quality. Human evaluations confirm reduced bias and stronger accent alignment, with consistent gender matching. The framework is backbone-agnostic and effectively balances accent fidelity with dialect-aware synthesis.

## Method Summary
CLARITY operates through a five-stage pipeline: instruction parsing using Gemini-2.5-flash-lite to extract metadata, LLM-guided text adaptation via GPT-4o-mini or LLaMA-3.1 to localize input to target dialects, judge selection using GPT-5-mini to score adaptation candidates, RAAP retrieval using ECAPA-TDNN for accent scoring and TF-IDF for text similarity to select prompts from curated AESRC/SEAME datasets, and synthesis with CosyVoice2 using the adapted text and retrieved prompt. The framework fine-tunes ECAPA-TDNN for 12-class accent classification and evaluates using accent accuracy, NISQA quality, FDR, and human MOS across 3,600 curated implicit instructions spanning 12 English accents.

## Key Results
- Accent accuracy reaches 63.36% with RAAP+text adaptation vs 46% baseline (ablation shows RAAP drives 17% gain)
- Fairness Discrepancy Rate reduced, with false positive rate for US/CA accents dropping significantly (Fig 7)
- NISQA quality maintained at 4.31, with human evaluations confirming reduced bias and superior accent consistency (Fig 5)
- Gender matching accuracy ~100% in human tests, demonstrating successful metadata parsing

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Accent Prompting (RAAP)
- **Claim:** Supplying accent-consistent audio prompts counteracts system's tendency to default to dominant phonetic patterns (accent bias)
- **Mechanism:** ECAPA-TDNN scores candidate prompt speeches based on accent confidence C(s_i, m); filtering curated pool P using target metadata and selecting high-confidence samples conditions TTS backbone with strong acoustic prior of target accent
- **Core assumption:** Backbone TTS model capable of zero-shot voice cloning and will prioritize prompt acoustics over training distribution bias
- **Evidence anchors:** Abstract states RAAP supplies accent-consistent speech prompts; section 3.2 shows RAAP ablation drops accuracy from ~63% to ~46%; Fig 7 shows reduced false positive rate for US/CA accents
- **Break condition:** If TTS backbone ignores prompt acoustics in favor of strong text-to-speech alignment priors, accent retrieval will fail to manifest in output

### Mechanism 2: LLM-Guided Dialect Localization
- **Claim:** Localizing input text to match target dialect (linguistic adaptation) improves accent fidelity by aligning "what" (text) with "how" (accent)
- **Mechanism:** LLMs perform style transfer to inject dialect-specific lexical and cultural cues into standard text; formalized as maximizing judgment score J_LLM; aligning textual input distribution D_u with target dialect D_t reduces "dual bias"
- **Core assumption:** TTS model's phoneme generation conditioned not just on prompt audio but also on linguistic context of text; standard text acts as "silent distractor" for minority accents
- **Evidence anchors:** Abstract states localized text improves accent accuracy; section 3.2 shows GPT-adapted text achieved best objective accuracy (63.36%); human evaluations confirmed superior accent consistency
- **Break condition:** If text adaptation introduces hallucinations or errors that confuse TTS front-end (phoneme conversion), quality will degrade despite improved accent matching

### Mechanism 3: Metadata-Driven Alignment
- **Claim:** Structured parsing of free-form instructions prerequisite for accurate dual-signal optimization
- **Mechanism:** LLM parses vague user instructions u into structured metadata schema m (accent, gender, age); metadata acts as query key for both text adapter and audio retriever
- **Core assumption:** User's intent can be deterministically mapped to fixed set of accent classes and demographic attributes
- **Evidence anchors:** Section 2 defines parsing function m = f_parse(u; θ_LLM); section 3.2 shows RAAP achieves ~100% accuracy for gender/accent retrieval when given structured metadata
- **Break condition:** If instruction parser fails to resolve ambiguities (e.g., "local" implies multiple possible accents), subsequent retrieval and adaptation steps will optimize for wrong target

## Foundational Learning

- **Concept:** Zero-Shot TTS (Speaker Prompting)
  - **Why needed here:** CLARITY is backbone-agnostic but relies on backbone's ability to perform "in-context learning" using reference audio signal to clone accent
  - **Quick check question:** Can the model generate speech in voice it has never explicitly trained on, given only short audio clip?

- **Concept:** ECAPA-TDNN (Speaker Verification)
  - **Why needed here:** This architecture used not for verification but as scoring function C(s_i, m) to quantify how strongly retrieved audio sample exhibits target accent
  - **Quick check question:** How does model separate speaker identity from accent characteristics in embedding space? (Assumption: paper finetunes it for accent classification)

- **Concept:** LLM-as-a-Judge
  - **Why needed here:** Framework relies on LLM (GPT-5) to score quality of text adaptation J_LLM to select best candidate text
  - **Quick check question:** Does judge's preference correlate with human perception of dialect authenticity?

## Architecture Onboarding

- **Component map:** Gemini-2.5-flash-lite (User Text + Instruction → JSON Metadata) -> GPT-4o-mini / LLaMA-3.1 (Standard Text + Metadata → Localized Text) -> GPT-5-mini (Standard + Adapted Text → Selection Score) -> ECAPA-TDNN + TF-IDF (Metadata + Text → Prompt Audio) -> CosyVoice2 (Selected Text + Prompt Audio → Output Speech)

- **Critical path:** Retrieval (RAAP) step is primary driver of accent accuracy (Table 1 shows largest jump from ~46% to ~63%); text adaptation provides marginal but consistent boost

- **Design tradeoffs:**
  - Standard vs. Adapted Text for Retrieval: Paper found using standard text for similarity search cos(φ(z_i), φ(x)) yielded slightly better results than adapted text (Table 5), likely because retrieval pool transcripts are standard
  - LLM Selection: GPT-adapted text outperformed LLaMA, but LLaMA was competitive (62.47% vs 63.36%)

- **Failure signatures:**
  - US/CA Bias: If system defaults to American accents, check if ECAPA-TDNN confidence threshold is too low or if prompt speech is effectively silent/neutral
  - Low NISQA: If quality drops below 4.0, text adaptation may have introduced phonetically difficult nonsense or retrieval prompt is low quality

- **First 3 experiments:**
  1. RAAP Ablation: Run TTS backbone with silent vs. random vs. RAAP-selected prompts on 12 accents to establish baseline bias reduction (Target: Replicate Fig 4/7)
  2. Text Adaptation Fidelity: Compare GPT-adapted text vs. Standard text using same retrieved prompt to isolate contribution of linguistic bias mitigation (Target: ~1-2% ACC gain)
  3. Retrieval Similarity Test: Compare TF-IDF vs. Semantic Embedding similarity for aligning prompt transcripts with user text to verify paper's choice of TF-IDF (Reference: Section 2, Eq. 6)

## Open Questions the Paper Calls Out

- **Question:** Can CLARITY framework effectively generalize to multilingual settings and low-resource accents outside twelve English variants tested?
  - **Basis in paper:** [explicit] Conclusion explicitly identifies "future work focusing on multilingual adaptation" as primary direction
  - **Why unresolved:** Current evaluation restricted to twelve specific English accents; LLM-based adaptation may not handle code-switching or dialectal nuances in languages with fewer training resources
  - **What evidence would resolve it:** Successful application of framework to non-English dialects (e.g., regional Spanish or Arabic varieties) with comparable objective accent accuracy and subjective naturalness scores

- **Question:** How can "LLM-as-Judge" selection mechanism be refined to minimize self-preference bias or evaluation mismatches?
  - **Basis in paper:** [explicit] Footnote 11 notes that GPT-5 scores "may not be the optimum" and suggests "Using multiple LLM-as-Judge models... we leave this for future work"
  - **Why unresolved:** Ablation study showed Max selection (choosing between GPT and LLaMA based on judge scores) underperformed GPT-only approach, indicating current judge is biased or misaligned with perceptual quality
  - **What evidence would resolve it:** Ablation study utilizing diverse panel of judges or human evaluations to select adapted text, resulting in higher accuracy than single-model baseline

- **Question:** Does reliance on TF-IDF for text similarity in Retrieval-Augmented Accent Prompting (RAAP) limit semantic matching capabilities?
  - **Basis in paper:** [inferred] Equation 6 explicitly defines text similarity using TF-IDF vectors, lexical matching method, rather than modern dense semantic embeddings
  - **Why unresolved:** TF-IDF fails to capture semantic paraphrasing; if user text is semantically similar but lexically distinct from prompt transcript, RAAP might retrieve sub-optimal prompts
  - **What evidence would resolve it:** Comparative experiments substituting TF-IDF with sentence-transformer embeddings in Equation 6, demonstrating improved correlation with semantic relevance or higher NISQA scores

## Limitations

- LLM-based parsing and adaptation introduce cascading failure risks when handling ambiguous or culturally-specific instructions that don't map cleanly to fixed accent-gender-age schema
- Text adaptation quality depends heavily on judge's preferences (GPT-5-mini), which may not perfectly align with human perception of dialect authenticity across all 12 accents
- Backbone-agnostic claim assumes sufficient zero-shot voice cloning capability, which may not hold across different TTS architectures

## Confidence

- **Accent accuracy improvement (63.36% vs ~46% baseline):** High confidence - ablation studies clearly demonstrate RAAP's contribution with substantial effect size across multiple accents
- **Dual-bias mitigation effectiveness:** Medium confidence - objective metrics show improvement and human evaluations confirm reduced bias, but performance on truly novel or ambiguous instructions remains untested
- **LLM-guided text adaptation contribution:** Low-Medium confidence - 1-2% absolute improvement is modest compared to RAAP's impact, and reliance on judge scores rather than direct human evaluation introduces uncertainty

## Next Checks

1. **Instruction parsing robustness test:** Systematically evaluate metadata parser on diverse set of culturally-specific and ambiguous instructions (e.g., "local", "regional", "working class") to measure failure rates and cascading effects on downstream accent accuracy (Target: <10% parsing ambiguity rate)

2. **Minority accent stress test:** Evaluate CLARITY on 6 lowest-frequency accents (CN, JP, KR, PT, RU, ES) under adversarial conditions where instruction clarity is deliberately reduced; measure whether framework maintains dual-bias mitigation advantage or reverts to US/CA defaults (Target: <15% US/CA false positives for these accents)

3. **Backbone generalization validation:** Test CLARITY with two additional TTS backbones beyond CosyVoice2 (e.g., VITS, DiffSpeech) to verify zero-shot voice cloning assumption; compare accent accuracy retention rates across backbones to quantify true "backbone-agnostic" claim (Target: >50% accent accuracy retention with any modern zero-shot TTS model)