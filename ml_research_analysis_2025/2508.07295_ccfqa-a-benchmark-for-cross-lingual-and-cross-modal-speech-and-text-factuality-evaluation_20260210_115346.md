---
ver: rpa2
title: 'CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality
  Evaluation'
arxiv_id: '2508.07295'
source_url: https://arxiv.org/abs/2508.07295
tags:
- speech
- benchmark
- language
- ccfqa
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CCFQA, a novel benchmark designed to evaluate
  the factuality of multimodal large language models (MLLMs) in cross-lingual and
  cross-modal scenarios. Unlike existing benchmarks that focus on single modalities
  or primarily English, CCFQA features parallel speech-text factual question-answer
  pairs across 8 languages, enabling systematic assessment of factual knowledge consistency
  across languages and input modalities.
---

# CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation

## Quick Facts
- arXiv ID: 2508.07295
- Source URL: https://arxiv.org/abs/2508.07295
- Reference count: 9
- Current MLLMs show significant performance degradation in cross-lingual and cross-modal factuality evaluation

## Executive Summary
This paper introduces CCFQA, a novel benchmark designed to evaluate the factuality of multimodal large language models (MLLMs) in cross-lingual and cross-modal scenarios. Unlike existing benchmarks that focus on single modalities or primarily English, CCFQA features parallel speech-text factual question-answer pairs across 8 languages, enabling systematic assessment of factual knowledge consistency across languages and input modalities. The authors demonstrate that current MLLMs, including GPT-4o-mini and Qwen2.5-Omni-7B, exhibit significant performance degradation in cross-lingual and cross-modal settings, highlighting the challenge of maintaining factual consistency. To address this, the paper proposes a few-shot transfer learning strategy that leverages English as a bridge language to effectively transfer factual QA capabilities to multilingual spoken question answering tasks.

## Method Summary
The benchmark consists of 6,000 parallel speech-text factual QA pairs across 8 languages, constructed through automatic translation of an English seed set with rigorous filtering to ensure quality and factual consistency. The evaluation framework tests four task types: Question Answering (QA), Cross-Lingual QA (XQA), Spoken QA (SQA), and Cross-Lingual Spoken QA (XSQA). The proposed solution involves a sequential curriculum learning strategy (ASR → SRT → SQA) and a few-shot transfer learning approach that uses English as a bridge language, achieving competitive performance with just 5-shot training per target language.

## Key Results
- Current MLLMs (GPT-4o-mini, Qwen2.5-Omni-7B) show significant performance degradation in cross-lingual and cross-modal settings
- Few-shot transfer learning with English bridge achieves competitive performance with GPT-4o-mini-Audio using only 5-shot training
- Cross-modal consistency scores drop significantly (e.g., GPT-4o-mini: 62.7% cross-modal vs. 96.6% cross-lingual)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot transfer learning using English as a bridge language improves cross-lingual spoken question answering performance.
- **Mechanism:** The approach leverages LLMs' superior factual reasoning in English by (1) pre-training the model on English SQA tasks with supervised fine-tuning, then (2) applying only 5-shot examples per target language. The learned task structure and factual capabilities transfer across languages through shared model parameters, reducing dependency on large-scale annotated data in each target language.
- **Core assumption:** Factual knowledge representations in the LLM are partially language-agnostic after English training, and minimal target-language examples are sufficient for cross-lingual adaptation.
- **Evidence anchors:**
  - [abstract] "we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training"
  - [Page 5, Few-Shot Transfer Learning section] "By leveraging English as a bridge, the learned knowledge is effectively transferred to other languages (e.g., Japanese or Cantonese), greatly reducing the reliance on large-scale annotated data in each target language"
  - [corpus] Related work CCHall (arxiv:2505.19108) confirms cross-lingual/cross-modal hallucination is underexplored; no corpus evidence directly validates English-as-bridge mechanism.
- **Break condition:** The mechanism likely fails when (1) target languages have significantly different morphological structure from English, (2) factual knowledge is culture-specific rather than universal, or (3) speech characteristics vary drastically from training distribution.

### Mechanism 2
- **Claim:** Sequential curriculum learning (ASR → SRT → SQA) enables effective speech-to-text alignment for question answering.
- **Mechanism:** The model progressively learns increasingly complex speech-to-text mappings. Stage 1 (ASR) establishes basic acoustic-to-phoneme alignment. Stage 2 (SRT) adds cross-lingual mapping capability. Stage 3 (SQA) adds reasoning on top of recognition. Explicit task instructions (e.g., `<|qa|>`, `<|eng|><|fra|>`) separate these capabilities within a unified architecture.
- **Core assumption:** Speech understanding capabilities are compositional and can be built incrementally; explicit task tokens effectively condition the model for each task type.
- **Evidence anchors:**
  - [Page 5, Pretraining section] "we adopt a sequential curriculum learning strategy that trains the model on three tasks in order: (1) Automatic Speech Recognition (ASR), (2) Speech Recognition and Translation (SRT), and (3) Spoken Question Answering (SQA)"
  - [Page 5, Table 4] Shows explicit instruction design separating ASR, SRT, and SQA tasks with language tokens
  - [corpus] MCAT (arxiv:2512.01512) supports curriculum approaches for scaling speech translation, though not identical methodology.
- **Break condition:** Curriculum learning benefits degrade if (1) earlier-stage tasks are insufficiently mastered before progressing, (2) task boundaries are ambiguous causing interference, or (3) the adapter capacity (80 queries) is insufficient for all three tasks.

### Mechanism 3
- **Claim:** Cross-modal and cross-lingual evaluation reveals factual inconsistency that monolingual/text-only benchmarks miss.
- **Mechanism:** Parallel speech-text pairs across 8 languages enable controlled comparison. By presenting identical factual questions in different modalities/languages, the benchmark measures whether the model's internal knowledge representation is consistent. Performance degradation (observed across all tested MLLMs) indicates modality-specific or language-specific biases in factual recall.
- **Core assumption:** A robust MLLM should produce equivalent answers for identical factual questions regardless of input modality or language; inconsistency indicates underlying representational fragmentation.
- **Evidence anchors:**
  - [Page 1, Figure 1] Demonstrates GPT-series models producing inconsistent answers (e.g., "Wrangell–St. Elias" in English vs. "Death Valley National Park" in Chinese for the same question)
  - [Page 6, Table 7] Shows cross-modal consistency scores dropping significantly (e.g., GPT-4o-mini: 62.7% cross-modal vs. 96.6% cross-lingual)
  - [corpus] CCHall (arxiv:2505.19108) independently identifies cross-lingual/cross-modal hallucination gaps; CLAIM (arxiv:2506.11073) confirms multilingual object hallucination in vision-language models.
- **Break condition:** The evaluation framework may produce false positives if (1) translation quality varies across languages introducing ambiguity, (2) speech recording quality affects ASR accuracy (WER ranges from 3.2% to 18.2% per Table 9), or (3) multiple valid answers exist for a question.

## Foundational Learning

- **Concept: Cross-lingual Transfer Learning**
  - Why needed here: Understanding how knowledge learned in one language transfers to others is critical for interpreting why 5-shot English training improves multilingual SQA.
  - Quick check question: If a model learns factual QA in English, what conditions determine whether this knowledge transfers to Japanese speech input?

- **Concept: Multimodal Alignment in Neural Networks**
  - Why needed here: The speech adapter (Q-Former + MLP) must align audio representations with text-based LLM embeddings; understanding this alignment is essential for debugging cross-modal consistency failures.
  - Quick check question: What role does the Q-Former play in converting variable-length speech features to fixed-dimensional representations the LLM can process?

- **Concept: Curriculum Learning**
  - Why needed here: The sequential ASR→SRT→SQA training strategy assumes capabilities build progressively; evaluating this assumption is key for model debugging.
  - Quick check question: Why might training ASR and SQA simultaneously fail when sequential training succeeds?

## Architecture Onboarding

- **Component map:**
  Speech Input → [Whisper Encoder (frozen, 635M)] → Audio Features → [Q-Former + MLP (80.5M, trainable)] → [GemmaX2-9B LLM (9.2B) + LoRA Adapter (8.9M)] → Text Output (answer in target language)
  Total trainable parameters: ~89.4M (adapter + LoRA)

- **Critical path:**
  1. Data preparation: Ensure parallel speech-text QA pairs with ASR quality check (WER threshold enforced)
  2. Pretraining: ASR task on FLEURS → SRT task on FLEURS → English SQA (3K synthetic samples)
  3. Few-shot transfer: 5 examples per target language
  4. Evaluation: Run all 4 task types (QA, XQA, SQA, XSQA) with LLM-judge scoring

- **Design tradeoffs:**
  - Frozen speech encoder vs. full fine-tuning: Frozen encoder limits acoustic adaptation but reduces compute; trade-off is WER sensitivity (Table 9 shows 3.2%-18.2% WER across languages)
  - 5-shot vs. more examples: Minimal data efficiency vs. potential underfitting on low-resource languages like Cantonese
  - English-centric bridge: Strong factual recall in English vs. introducing Anglo-centric bias (acknowledged in Limitations)

- **Failure signatures:**
  - High F1 but low LLM accuracy: Model produces fluent, plausible-sounding but factually incorrect answers (hallucination signature)
  - Low F1 but high accuracy: Model knows facts but fails to follow instruction format
  - Sharp XSQA performance drop vs. XQA: Speech adapter failing to preserve semantic content across languages
  - Cantonese-specific degradation (Table 6 shows lowest scores): Low-resource language underrepresentation

- **First 3 experiments:**
  1. **Baseline validation:** Run Qwen2-Audio on CCFQA subset (100 samples/language) to reproduce reported SQA/XSQA gaps; verify your evaluation pipeline matches paper's LLM-judge methodology.
  2. **Ablation on shot count:** Train LLM-SQA with 1-shot, 3-shot, 5-shot, and 10-shot for French and Cantonese; plot performance curve to identify diminishing returns point.
  3. **Cross-modal consistency probe:** For 50 English questions, compare model responses in text vs. speech modalities; compute consistency rate to quantify modality gap independent of benchmark evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending the CCFQA benchmark to include visual modalities reveal further inconsistencies in factuality that are not apparent in speech and text inputs?
- Basis in paper: [explicit] The authors state in the Limitations section that "Future work could consider extending it to additional modalities (e.g., vision)."
- Why unresolved: The current evaluation is restricted to audio and text, leaving the interaction of factual consistency with visual data unexplored.
- What evidence would resolve it: Evaluation of MLLMs on a tri-modal (image-speech-text) extension of the dataset.

### Open Question 2
- Question: How can the English-centric bias inherent in the pivot-language transfer strategy be mitigated without sacrificing the performance gains demonstrated by the 5-shot learning approach?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that the method "introduces a language bias centered around English."
- Why unresolved: The paper demonstrates the efficacy of the strategy but does not propose a method to neutralize the linguistic skew caused by using English as the central bridge.
- What evidence would resolve it: Performance metrics on culturally specific facts that differ between English and the target language, comparing the current strategy with a de-biased approach.

### Open Question 3
- Question: What specific architectural modifications to special language tokens are required to resolve the performance degradation observed in cross-lingual spoken QA tasks?
- Basis in paper: [inferred] The authors note in the "Language Tags for MLLMs" section that current token designs (e.g., in Phi-4) struggle with XSQA, causing "a sharp drop in performance."
- Why unresolved: The paper identifies token design as a critical failure point but does not experimentally validate a solution for this architectural flaw.
- What evidence would resolve it: Comparative performance of models with varying special token configurations on the XSQA benchmark.

## Limitations
- The benchmark relies on LLM-judge scoring which introduces potential subjectivity and calibration issues
- The benchmark construction process may introduce selection bias toward questions that are inherently harder for current MLLMs
- The 5-shot transfer learning approach has only been validated on a limited set of languages, raising questions about scalability to truly low-resource languages

## Confidence

- **High confidence**: The observation that current MLLMs exhibit significant performance degradation in cross-lingual and cross-modal settings (supported by multiple experiments showing consistent drops across GPT-4o-mini and Qwen2.5-Omni-7B)
- **Medium confidence**: The effectiveness of the few-shot transfer learning strategy (based on limited ablation studies with 5-shot training, but promising results against GPT-4o-mini-Audio)
- **Medium confidence**: The benchmark's ability to reveal factual inconsistency (though methodology is sound, the LLM-judge scoring introduces uncertainty)

## Next Checks

1. **LLM-judge reliability test**: Run the same CCFQA evaluation with three different LLM judges (GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro) and measure inter-judge agreement rates. Calculate Cohen's kappa to quantify scorer consistency and identify questions with high disagreement.

2. **Cross-lingual generalization stress test**: Apply the 5-shot transfer learning approach to a truly low-resource language not in the original 8 (e.g., Swahili or Zulu) and measure performance degradation. Compare against a zero-shot baseline to quantify the minimum effective shot count for different language families.

3. **Modality-specific error analysis**: For the top-10% worst-performing cross-modal questions, manually analyze whether failures stem from (a) ASR errors, (b) semantic degradation in the speech adapter, or (c) LLM hallucination. Use the WER data from Table 9 to correlate transcription quality with answer accuracy.