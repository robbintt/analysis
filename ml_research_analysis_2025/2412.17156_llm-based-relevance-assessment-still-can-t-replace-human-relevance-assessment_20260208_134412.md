---
ver: rpa2
title: LLM-based relevance assessment still can't replace human relevance assessment
arxiv_id: '2412.17156'
source_url: https://arxiv.org/abs/2412.17156
tags:
- relevance
- llm-based
- evaluation
- systems
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper critically challenges claims that LLM-based relevance\
  \ assessments can fully replace human judgments in information retrieval evaluation.\
  \ Through empirical analysis of TREC 2024 RAG track data, the authors demonstrate\
  \ several critical limitations: LLM-based assessments show degraded correlation\
  \ among top-performing systems (Kendall's \u03C4 dropping from 0.85 to 0.56 for\
  \ top 20 systems), can be systematically manipulated by participants (one deliberately\
  \ crafted system ranked 5th under LLM evaluation but 28th under human evaluation),\
  \ and create circularity when used both for re-ranking and evaluation (Kendall's\
  \ \u03C4 dropping to -0.40 among top 5 systems)."
---

# LLM-based relevance assessment still can't replace human relevance assessment

## Quick Facts
- arXiv ID: 2412.17156
- Source URL: https://arxiv.org/abs/2412.17156
- Authors: Charles L. A. Clarke; Laura Dietz
- Reference count: 13
- Primary result: LLM-based relevance assessments cannot replace human judgments due to correlation degradation, manipulation vulnerability, and circularity issues.

## Executive Summary
This paper critically challenges claims that LLM-based relevance assessments can fully replace human judgments in information retrieval evaluation. Through empirical analysis of TREC 2024 RAG track data, the authors demonstrate several critical limitations: LLM-based assessments show degraded correlation among top-performing systems (Kendall's τ dropping from 0.85 to 0.56 for top 20 systems), can be systematically manipulated by participants (one deliberately crafted system ranked 5th under LLM evaluation but 28th under human evaluation), and create circularity when used both for re-ranking and evaluation (Kendall's τ dropping to -0.40 among top 5 systems). The authors argue that LLM-based assessments represent a form of re-ranking rather than true evaluation, are susceptible to bias favoring LLM-generated content, and cannot serve as gold standards since they lack human-grounded task context.

## Method Summary
The authors analyzed TREC 2024 RAG track data comparing LLM-based (Umbrela) and human relevance assessments across 75 systems and 301 queries. They computed Kendall's τ correlation between manual and automatic NDCG rankings at various cutoff points, submitted adversarial runs using LLM-based re-ranking to test manipulation vulnerability, and simulated circularity by re-ranking all submitted runs with Umbrela labels. The analysis focused on how correlation degrades when focusing on top-performing systems rather than the full leaderboard, and how LLM-based re-ranking can artificially inflate scores when the same LLM is used for evaluation.

## Key Results
- LLM-human agreement degrades substantially at the performance frontier (Kendall's τ drops from 0.85 to 0.56 for top 20 systems)
- One deliberately crafted system ranked 5th under LLM evaluation but 28th under human evaluation
- Circularity creates negative correlation among top systems (Kendall's τ drops to -0.40 for top 5 systems)
- Twelve systems exceeded 0.95 NDCG under LLM evaluation but only 0.68-0.72 under human evaluation

## Why This Works (Mechanism)

### Mechanism 1: Correlation Degradation at Performance Frontier
LLM-human agreement degrades substantially when focusing on top-performing systems rather than the full leaderboard. Easy distinctions among poorly-performing systems inflate overall Kendall's τ scores. When low-performing systems are excluded, the metric must discriminate among systems that are genuinely competitive—revealing LLM evaluators' inability to capture fine-grained human relevance distinctions.

### Mechanism 2: Evaluation Manipulation Via LLM-Aligned Re-ranking
LLM-based evaluation can be systematically exploited by using the same or similar LLM as a re-ranker, achieving artificially inflated scores. Since LLM-based relevance assessment is computationally equivalent to LLM-based re-ranking (both predict query-passage affinity), participants who apply LLM-based re-ranking effectively optimize for the evaluation metric directly.

### Mechanism 3: Circularity-Induced Score Inflation
When LLMs serve as both system component (re-ranker) and evaluation metric, the resulting evaluation becomes invalid and negatively correlated with human judgment at the top. Re-ranking with an LLM creates outputs that the same LLM will rate highly. This inflates scores and inverts rankings.

## Foundational Learning

- Concept: Kendall's τ (rank correlation coefficient)
  - Why needed here: The paper's central evidence uses τ to quantify agreement between human and LLM evaluations; understanding that τ = 1 means perfect rank agreement, τ = 0 means random ordering, and τ = -1 means inverted ranking is essential.
  - Quick check question: If τ drops from 0.85 to 0.56 when restricting to top systems, what fraction of system pairs are now ranked in opposite order by the two evaluators?

- Concept: NDCG (Normalized Discounted Cumulative Gain)
  - Why needed here: All reported system performance scores use NDCG; understanding that it measures ranked retrieval quality with position discounting is necessary to interpret the score inflation findings.
  - Quick check question: Why would NDCG scores approaching 0.95 under LLM evaluation but only 0.72 under human evaluation indicate a problem with the evaluation rather than genuine performance?

- Concept: Test collection reusability
  - Why needed here: The paper's core concern is that LLM-evaluated collections cannot serve as valid benchmarks for future research; understanding TREC-style evaluation goals clarifies why top-system discrimination matters.
  - Quick check question: If a test collection is built using LLM judgments, what happens when future systems incorporate similar LLM components?

## Architecture Onboarding

- Component map:
Query → Retrieval System → Candidate Passages
                           ↓
              [Optional: LLM Re-ranker] → Re-ranked Passages
                           ↓
              Evaluation Pipeline
              ├── LLM-based Assessor (e.g., Umbrela)
              └── Human Assessor (gold standard)
                           ↓
                  Aggregated Metrics (NDCG, τ)

- Critical path: Human assessment remains the only validated path for gold-standard evaluation. LLM assessment is permissible only for preliminary screening or when human judgment is impractical—and must be validated against human labels on a sample.

- Design tradeoffs:
  - Speed/cost vs. validity: LLM evaluation is fast and cheap but susceptible to manipulation and circularity; human evaluation is expensive but grounded.
  - Prompt transparency vs. security: Publishing evaluation prompts enables reproducibility but also enables gaming; keeping prompts secret reduces gaming but undermines scientific transparency.
  - Hybrid approaches: Using LLMs to filter candidates for human judgment may offer partial benefits, though the paper notes "LLM assistance does not appear to increase correlation with fully manual assessments."

- Failure signatures:
  - τ degradation at top of leaderboard (signal: high overall τ but low τ@20, τ@15)
  - Score clustering near ceiling under LLM evaluation (many systems >0.95 NDCG)
  - Negative τ between LLM and human evaluation among top systems (rank inversion)
  - Systems using known LLM re-rankers substantially outperforming non-LLM baselines under LLM evaluation only

- First 3 experiments:
  1. Reproduce correlation analysis on your own retrieval benchmark: compute τ between human and LLM judgments overall, then restrict to top-k systems and observe degradation.
  2. Adversarial re-ranking test: take an existing retrieval system, apply the same LLM used for evaluation as a final-stage re-ranker, and measure score inflation.
  3. Human validation on high-LLM-score outputs: sample passages rated highly by LLM evaluators and have human assessors judge them.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can safeguards or debiasing techniques be developed to prevent circularity and manipulation in LLM-based evaluation while maintaining correlation with human judgments?
- Basis in paper: The authors state that "LLM-based evaluation pipelines without safeguards against feedback loops" create risks, and warn that systems can be "strategically subverted" but propose no solutions.
- Why unresolved: The paper demonstrates the problem exists but does not investigate defensive countermeasures.
- What evidence would resolve it: A study showing that specific safeguards successfully prevent score inflation while preserving valid correlation.

### Open Question 2
- Question: At what rate will LLM-human assessment correlation degrade over time as more retrieval systems incorporate LLM components?
- Basis in paper: The authors predict: "we predict that this correlation will degrade as developers incorporate LLM-based evaluation components into their systems in more refined ways."
- Why unresolved: This is a longitudinal prediction based on Goodhart's law, not yet empirically validated across multiple evaluation cycles.
- What evidence would resolve it: Longitudinal analysis comparing LLM-human correlation across successive TREC tracks.

### Open Question 3
- Question: Can the "narcissism" bias (LLM judges favoring LLM-generated content) be quantified and corrected?
- Basis in paper: The paper cites Balog et al. showing "a clear and substantial bias in favor of LLM-based rankers" and references LLM "narcissism" as a theoretical challenge.
- Why unresolved: The bias is documented but no correction method is proposed or tested.
- What evidence would resolve it: Experiments showing that debiased LLM judges achieve higher correlation with human judgments.

## Limitations
- Results are specific to the RAG task and TREC 2024 dataset
- Umbrela's specific implementation details are not fully public
- Analysis focuses on ranking correlations rather than absolute relevance quality judgments

## Confidence
- **High confidence**: Core empirical findings (correlation degradation, manipulation vulnerability, circularity effects) are directly measured from TREC 2024 RAG track data
- **Medium confidence**: Claim that LLM-based assessments cannot replace human judgment requires generalization to other domains and evaluation settings

## Next Checks
1. Test whether correlation degradation at the performance frontier occurs on non-RAG retrieval tasks using the same LLM evaluators
2. Implement the adversarial re-ranking experiment on a different dataset with multiple LLM evaluators
3. Conduct human validation studies on LLM-high-scoring outputs across multiple domains to quantify the narcissism bias