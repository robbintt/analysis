---
ver: rpa2
title: LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student
  Perceptions than Educator Feedback
arxiv_id: '2601.15280'
source_url: https://arxiv.org/abs/2601.15280
tags:
- feedback
- learning
- multimodal
- were
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated an AI-facilitated multimodal feedback system
  that integrates structured text, visual slide references, and streaming audio narration
  for learning activities. In a controlled online experiment (n=197 students), the
  AI system produced learning gains equivalent to original educator feedback while
  significantly improving perceived clarity, specificity, conciseness, motivation,
  satisfaction, and reducing cognitive load.
---

# LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student Perceptions than Educator Feedback

## Quick Facts
- **arXiv ID**: 2601.15280
- **Source URL**: https://arxiv.org/abs/2601.15280
- **Reference count**: 40
- **Primary result**: AI multimodal feedback achieved equivalent learning gains to educator feedback while significantly improving student perceptions of clarity, specificity, conciseness, motivation, satisfaction, and reducing cognitive load.

## Executive Summary
This study compared an AI-facilitated multimodal feedback system against original educator feedback in a controlled online experiment with 197 students. The AI system integrated structured text, visual slide references, and streaming audio narration for learning activities. While learning gains were equivalent between conditions, the AI feedback significantly outperformed educator feedback on perceived clarity, specificity, conciseness, motivation, satisfaction, and reduced cognitive load. Process log analysis revealed that human feedback led to more submissions for multiple-choice questions, while AI feedback encouraged more revisions on open-ended questions. The findings demonstrate that real-time, context-aware multimodal AI feedback can effectively support learning at scale while reducing instructor workload and enhancing student experience.

## Method Summary
The study employed a within-subjects design where participants received AI multimodal feedback for some practice questions and original educator feedback for others. The AI system used RAG-based retrieval of course-specific slides and structured text generation with optional streaming audio narration. Learning materials covered "multimedia principle in e-learning design" with 13 practice questions (8 MCQs, 5 OEQs). The system used gpt-5-2025-08-07 with reasoning_effort=low to generate JSON-formatted feedback incorporating retrieved slide references. Feedback was delivered through an embedded iframe interface with left panel showing feedback plus slide reference and audio button, and right panel containing answer input. Participants completed pre/post tests and rated feedback on various perception measures.

## Key Results
- AI and educator feedback produced equivalent learning gains on pre/post tests
- AI feedback significantly outperformed educator feedback on perceived clarity, specificity, conciseness, motivation, satisfaction, and reduced cognitive load
- Correctness, trust, and acceptance ratings were comparable between AI and human feedback
- For MCQs, human feedback led to more submissions; for OEQs, AI feedback encouraged more revisions
- Learners found AI feedback easier to understand and more actionable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coordinated multimodal presentation (text + visual + audio) improves perceived feedback quality while maintaining or reducing cognitive load.
- Mechanism: Text provides primary corrective information; retrieved slide page offers visual grounding; optional audio narration presents complementary explanation via separate processing channel. Aligned modalities allow learners to choose preferred channel without split-attention costs.
- Core assumption: Learners will engage optional modalities selectively rather than consuming all channels simultaneously.
- Evidence anchors:
  - [abstract] "integrates structured text, visual slide references, and streaming audio narration... significantly outperforming it on perceived clarity, specificity, conciseness, motivation, satisfaction, and reducing cognitive load"
  - [section 3.3.2] "narration does not duplicate the text verbatim; instead, it provides complementary explanations... dual-channel presentation is informed by Mayer's multimedia learning theory and Paivio's dual-coding theory"
  - [corpus] Weak direct evidence on multimodal feedback specifically; related work SlideItRight focuses on slide retrieval rather than combined audio narration.
- Break condition: If learners feel compelled to process all modalities exhaustively, or if audio narrates text verbatim, extraneous load may increase and reverse the effect.

### Mechanism 2
- Claim: RAG-based retrieval of course-specific slides increases perceived relevance and actionability of feedback.
- Mechanism: Pre-computed slide embeddings (visual + textual) are matched to questions via cosine similarity; top-matching slide page is displayed alongside feedback. This grounds AI-generated text in materials learners already associate with the course, reducing gap between generic explanation and contextualized guidance.
- Core assumption: The slide deck contains information relevant to specific practice questions.
- Evidence anchors:
  - [abstract] "retrieved most relevant slide page references"
  - [section 3.5] "For each question, the system retrieves the top 3 most relevant slide pages by cosine similarity to the question vector... the best-match slide page is presented in the resource area"
  - [corpus] SlideItRight (Zhao et al. 2025) demonstrates slide retrieval for open-ended feedback but does not isolate its effect from other modalities.
- Break condition: If slide deck is sparse, outdated, or misaligned with practice questions, retrieved slides may be irrelevant or misleading, reducing trust.

### Mechanism 3
- Claim: Structured, progressively disclosed text reduces perceived cognitive effort compared to dense prose.
- Mechanism: Feedback is generated as JSON object with distinct tags (`<statement>`, `<explanation>`, `<advice>`) and inline tooltips for secondary context. Visual cues (color-coded correctness indicators) and on-demand detail reduce need to parse long paragraphs simultaneously.
- Core assumption: Learners will interact with tooltips rather than ignore them or feel overwhelmed by their presence.
- Evidence anchors:
  - [section 3.4] "Inline `<term>` tags for context-aware keyword tooltips that are not directly related to the corrective information. These appear on demand to keep the main feedback concise and to reduce the cognitive load."
  - [section 6.3] "Participants also reported significantly lower mental effort when using the AI feedback... contrasts with prior work in which learners experienced increased cognitive load when receiving AI feedback, as they had difficulty efficiently locating essential information"
  - [corpus] No direct corpus evidence on tooltip-style progressive disclosure in AI feedback.
- Break condition: If tooltip content is essential rather than supplementary, learners who skip it may miss key information; if too many terms are flagged, interface becomes cluttered.

## Foundational Learning

- Concept: **Cognitive Load Theory (extraneous vs. germane load)**
  - Why needed here: Paper explicitly frames multimodal design as tradeoff between leveraging dual channels and avoiding split-attention/redundancy. Understanding which load type is being targeted is essential to interpret "lower mental effort" result.
  - Quick check question: When authors report reduced cognitive load, are they claiming reduced extraneous load, reduced germane load, or an undifferentiated measure?

- Concept: **Multimedia Learning Principles (dual coding, contiguity, signaling)**
  - Why needed here: Design rationale invokes Mayer's principles to justify text + visual + audio coordination. These principles predict when multimodal feedback helps vs. harms.
  - Quick check question: If audio narration repeated text feedback word-for-word, which multimedia principle would be violated, and what would be predicted effect?

- Concept: **Retrieval-Augmented Generation (RAG) basics**
  - Why needed here: System uses pre-computed embeddings and cosine similarity to retrieve relevant slides. Understanding RAG helps diagnose retrieval failures and plan improvements.
  - Quick check question: If slide page is visually rich but textually sparse, how might embedding strategy affect its retrievability for text-heavy question?

## Architecture Onboarding

- Component map: Question text + images, learner answer, prompt template, knowledge base (slide embeddings) -> Cosine similarity search over pre-computed slide vectors -> Top-3 candidates -> LLM (gpt-5 with reasoning_effort=low) produces JSON feedback -> JSON parsed into HTML with visual cues and tooltip markup -> Formatted text + retrieved slide image + streaming audio narration -> Embedded iframe in host platform

- Critical path:
  - Pre-processing (offline): Embed all slide pages (vision + text) and store vectors; pre-generate MCQ feedback for each option
  - Runtime (per submission): Retrieve top-3 slides -> construct LLM prompt with question, answer, slide descriptors -> generate JSON -> parse and render -> stream audio asynchronously
  - Latency budget: MCQ median ~0.3s (pre-generated); OEQ median ~6.2s (real-time generation). Monitor for spikes >10s

- Design tradeoffs:
  - reasoning_effort=low trades depth for speed; may reduce nuance in complex explanations
  - Pre-generation for MCQs improves latency but requires upfront compute and storage
  - Audio is optional and streaming; learners who skip audio may miss complementary explanations
  - Tooltip design assumes secondary context is skippable; essential caveats should be in main text

- Failure signatures:
  - Retrieved slide is unrelated to question -> learner confusion, reduced trust
  - JSON parsing errors (malformed output) -> blank or broken feedback panel
  - Audio narration latency >5s -> learners abandon audio channel
  - Tooltip terms over-highlighted -> visual clutter, increased (not reduced) cognitive load
  - Correctness score mismatch with actual answer quality -> learner frustration or misplaced trust

- First 3 experiments:
  1. **Ablate the slide retrieval**: Compare full system vs. text-only feedback (no slides, no audio) on perceived relevance and learning gains to isolate contribution of RAG-based visuals
  2. **Vary tooltip density**: Randomly assign learners to low, medium, or high tooltip frequency; measure cognitive load, time-on-task, and subjective clarity to find optimal level
  3. **Audio content manipulation**: Compare complementary narration (current design) vs. verbatim text narration vs. no audio; measure engagement, comprehension, and preference to validate dual-channel assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does AI multimodal feedback sustain equivalent learning outcomes and engagement over extended time periods (e.g., full semester courses) and improve knowledge retention on delayed post-tests?
- Basis in paper: [explicit] The authors state "this study cannot address long-term effects or performance on delayed post-tests. Further experiments are needed in real classroom settings... to evaluate lasting learning outcomes."
- Why unresolved: The controlled one-hour crowdsourcing session limits understanding of sustained engagement, knowledge decay, and transfer effects beyond immediate post-tests.
- What evidence would resolve it: Longitudinal classroom studies with delayed post-tests (weeks/months later) comparing AI multimodal vs. educator feedback on retention and transfer tasks.

### Open Question 2
- Question: What are the individual and interactive effects of specific feedback modalities (structured text, slide retrieval, audio narration) on learning outcomes and cognitive load?
- Basis in paper: [explicit] The authors acknowledge "the observed differences... may be attributable to multiple confounded factors, such as the presentation style of slides, different feedback source (AI vs. human)... future work should employ orthogonal manipulations of feedback design factors to disentangle their individual and interactive effects."
- Why unresolved: The study compared a bundled AI multimodal system against baseline, but did not isolate which modalities drove improvements in clarity, specificity, and reduced cognitive load.
- What evidence would resolve it: Factorial experiments independently varying text structure, visual references, and audio narration to measure each component's contribution to learning and perception outcomes.

### Open Question 3
- Question: Does AI multimodal feedback reduce extraneous cognitive load while preserving or increasing beneficial generative cognitive load during learning?
- Basis in paper: [explicit] The authors state "we were only able to measure overall perceived load, which does not allow us to disentangle different types of cognitive load... Future work should employ more fine-grained measures to better capture how AI feedback influences different cognitive processes."
- Why unresolved: Lower perceived mental effort could indicate either productive scaffolding (reduced extraneous load) or potentially shallow processing (reduced generative load).
- What evidence would resolve it: Studies using differentiated cognitive load instruments (e.g., Paas scale adaptations) or dual-task paradigms to separately measure extraneous, intrinsic, and germane load during AI feedback interaction.

## Limitations

- Equivalence of learning gains based on pre/post test scores without reported effect sizes or statistical power calculations
- Controlled lab setting with single topic and participant pool limits generalizability
- Claim that learners "selectively" engage optional modalities not directly measured; log data only tracks submission and revision counts
- Design assumes tooltip content is supplementary, but no data confirms whether learners actually skip or read tooltips
- RAG retrieval mechanism's precision and recall not reported, so relevance of retrieved slides is assumed rather than verified

## Confidence

- **High confidence**: The finding that AI feedback is perceived as clearer, more specific, and less cognitively demanding than educator feedback is supported by statistically significant survey results and aligns with multimodal design rationale
- **Medium confidence**: The claim of equivalent learning gains rests on pre/post test comparisons but lacks effect size reporting and external validation
- **Low confidence**: The assertion that learners engage modalities selectively is based on design intent, not empirical evidence of actual usage

## Next Checks

1. **Measure actual modality engagement**: Log detailed interaction data (clicks, audio play duration, tooltip expansions) to confirm learners selectively engage rather than exhaustively process all modalities
2. **Test retrieval relevance**: Conduct a blind relevance judgment study where human raters score retrieved slides for each question to quantify RAG precision and identify failure cases
3. **Replicate in diverse contexts**: Run the same study with different topics, learner populations, and feedback types (e.g., code, essays) to test generalizability of equivalence and preference findings