---
ver: rpa2
title: 'TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific
  Web Discourse Detection'
arxiv_id: '2508.08265'
source_url: https://arxiv.org/abs/2508.08265
tags:
- scientific
- debate
- category
- tweet
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel council debate method for scientific
  web discourse detection in social media. The approach simulates structured academic
  discussions among multiple large language models (LLMs) to identify tweets containing
  scientific claims, references to scientific studies, or mentions of scientific entities.
---

# TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection

## Quick Facts
- arXiv ID: 2508.08265
- Source URL: https://arxiv.org/abs/2508.08265
- Reference count: 29
- Primary result: Council debate method achieves F1=0.7805 for scientific study reference detection (1st place) in CheckThat! 2025 Task 4a

## Executive Summary
This paper introduces a novel council debate framework using multiple large language models to detect scientific discourse in social media. The method simulates structured academic discussions where specialized LLM agents deliberate and vote on whether tweets contain scientific claims, references to studies, or scientific entities. Three debating strategies are explored: single debate (two LLMs argue, one judges), team debate (collaborative teams), and council debate (expert models deliberate under chairperson moderation). The council debate approach achieves the highest performance on the development set and ranks first in detecting scientific study references in the official evaluation, while placing 8th and 9th for scientific claims and entities respectively.

## Method Summary
The approach employs three debating strategies to classify tweets into three scientific discourse categories. Single debate uses two LLMs to argue opposite positions while a third acts as judge. Team debate organizes LLMs into collaborative teams within each side of the argument. Council debate involves multiple specialized LLM agents deliberating together under chairperson moderation. Each debate concludes with a majority vote to determine the final classification. The method leverages the collective reasoning capabilities of multiple models to improve detection accuracy compared to single-model approaches.

## Key Results
- Council debate achieves F1=0.7805 for scientific study reference detection (1st place in official evaluation)
- System ranks 8th for scientific claims detection (F1=0.7273) and 9th for scientific entities detection (F1=0.7766)
- Performance on development set exceeds test set results, particularly for claims and entities categories
- LLMs demonstrate superior performance in identifying scientific study references compared to claims or entities

## Why This Works (Mechanism)
The council debate framework works by leveraging multiple LLM agents with specialized expertise to simulate academic discourse. Through structured deliberation and argumentation, the models can cross-examine each other's reasoning and reach consensus on complex scientific discourse classification. The chairperson moderation ensures focused discussion and prevents off-topic arguments. This multi-agent approach compensates for individual model limitations by combining diverse perspectives and reasoning patterns, particularly effective for identifying scientific study references which require nuanced understanding of academic citations and research context.

## Foundational Learning

**Multi-agent deliberation systems**: Why needed - To combine diverse reasoning capabilities of different LLMs for complex classification tasks. Quick check - Verify if debate transcripts show meaningful argumentation rather than agreement without discussion.

**Academic discourse simulation**: Why needed - To mimic how researchers evaluate scientific claims and references. Quick check - Assess if debate transcripts contain discipline-specific terminology and citation patterns.

**Majority voting mechanisms**: Why needed - To aggregate individual model judgments into final classification decisions. Quick check - Calculate variance in individual model predictions to determine if voting adds value.

**Prompt engineering for debate**: Why needed - To structure LLM interactions and guide them toward productive deliberation. Quick check - Compare debate quality with and without specific instruction prompts.

## Architecture Onboarding

**Component map**: Tweet input -> Preprocessing -> Individual LLM analysis -> Debate initiation -> Argumentation phase -> Chairperson moderation -> Voting phase -> Classification output

**Critical path**: Tweet preprocessing -> LLM specialization assignment -> Debate execution -> Majority voting -> Final classification

**Design tradeoffs**: Multiple LLM usage provides robustness but increases computational cost; structured debate ensures quality but adds latency; specialized agents improve accuracy but require careful role assignment.

**Failure signatures**: Debate deadlock (no majority reached), off-topic arguments, model disagreement without resolution, excessive computational overhead, overfitting to training data distribution.

**First experiments**: 1) Compare single LLM vs. council debate performance on development set. 2) Run ablation study with different numbers of debate participants. 3) Test debate effectiveness across different scientific disciplines.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the council debate framework generalize to other text classification tasks beyond scientific discourse detection?
- Basis in paper: The Conclusion states, "In future work, we plan to extend our debating framework to other classification tasks."
- Why unresolved: The study only validated the method on the specific CheckThat! 2025 Task 4a dataset (scientific discourse in tweets).
- What evidence would resolve it: Application of the same council debate algorithms to standard NLP benchmarks (e.g., sentiment analysis, topic classification) comparing results against single-model baselines.

### Open Question 2
- Question: To what extent does prompt design influence the "consensus" reached by the council members versus the inherent reasoning of the models?
- Basis in paper: The Conclusion notes the authors aim to "investigate the impact of prompt design" in future work.
- Why unresolved: The study utilized fixed prompts (Appendix A) but did not ablate or vary them to see if the debate dynamics were prompt-dependent.
- What evidence would resolve it: An ablation study running the council debate with varied prompt styles (e.g., zero-shot vs. few-shot, different persona instructions) to measure variance in F1 scores.

### Open Question 3
- Question: What features of "scientific claims" make the debating approach significantly less effective (8th place) compared to "scientific study references" (1st place)?
- Basis in paper: The paper reports a large performance gap between categories, noting LLMs are "less effective at detecting scientific claims" but perform well on references, without explaining the cause.
- Why unresolved: The results section highlights the discrepancy but offers no error analysis regarding why the debate logic failed for scientific claims.
- What evidence would resolve it: A qualitative error analysis of the debate transcripts for false negatives in Category 1 to identify if the claim definitions were ambiguous to the debating agents.

### Open Question 4
- Question: Did the heuristic dependency between Category 2 and Category 3 negatively impact the independent classification accuracy of Category 3?
- Basis in paper: The authors used a heuristic where a positive prediction for Category 2 automatically triggered a positive label for Category 3, yet the system ranked 9th for Category 3.
- Why unresolved: It is unclear if the low rank for Category 3 was due to model inability or noise introduced by automatically propagating Category 2 predictions.
- What evidence would resolve it: An ablation study comparing the current heuristic approach against an independent council debate for Category 3.

## Limitations
- Performance drop from development to test sets suggests potential overfitting to training data distribution
- Computational overhead of running multiple LLM debates per tweet raises scalability concerns
- Limited analysis of why debate framework performs poorly on scientific claims compared to study references

## Confidence
High confidence in council debate's effectiveness for scientific study reference detection (F1=0.7805, 1st place). Medium confidence in performance for scientific claims (F1=0.7273, 8th place) and scientific entities (F1=0.7766, 9th place). Medium confidence in observation that LLMs perform better at detecting scientific study references than claims or entities, as the paper provides limited analysis of this pattern.

## Next Checks
1. Test the council debate method on out-of-domain scientific discourse datasets to assess generalization capabilities beyond social media contexts
2. Conduct ablation studies comparing single LLM detection against the multi-agent debate approach to quantify the marginal benefit of the debating framework
3. Evaluate the method's performance across different scientific disciplines to determine if certain fields benefit more from the debate-based approach than others