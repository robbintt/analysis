---
ver: rpa2
title: Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large
  Language Models
arxiv_id: '2508.01908'
source_url: https://arxiv.org/abs/2508.01908
tags:
- replay
- learning
- reptile
- continual
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates continual pre-training of large language
  models using experience replay and gradient alignment to address catastrophic forgetting
  across multiple languages. The authors evaluate Llama-family models (99M to 6B parameters)
  pre-trained sequentially on English, French, German, Arabic, and Japanese (100B
  tokens each) using meta-experience replay, which combines replay with Reptile-based
  meta-updates.
---

# Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models

## Quick Facts
- **arXiv ID:** 2508.01908
- **Source URL:** https://arxiv.org/abs/2508.01908
- **Reference count:** 40
- **One-line primary result:** Experience replay with gradient alignment significantly reduces catastrophic forgetting in continual LLM pre-training across multiple languages while improving downstream task performance.

## Executive Summary
This paper investigates continual pre-training of large language models using experience replay and gradient alignment to address catastrophic forgetting across multiple languages. The authors evaluate Llama-family models (99M to 6B parameters) pre-trained sequentially on English, French, German, Arabic, and Japanese (100B tokens each) using meta-experience replay, which combines replay with Reptile-based meta-updates. Results show that both replay (especially 50% replay rate) and gradient alignment significantly reduce forgetting while improving downstream task performance compared to sequential training, with the combination yielding the best results. Scaling analysis reveals that replay with gradient alignment provides stable and efficient continual learning across model sizes.

## Method Summary
The study employs Meta-Experience Replay (MER), combining experience replay with Reptile-based gradient alignment for continual pre-training. Replay buffers store past examples on disk, with mixed batches containing 25% or 50% replay samples. Reptile interpolation occurs every 500 batches with a meta-learning rate of 0.1. The approach uses AdamW optimizer with cosine learning rate decay and 357-step warmup. Models range from 99M to 6B parameters and are trained sequentially on English, French, German, Arabic, and Japanese datasets (100B tokens each). The evaluation measures retained validation loss across all tasks and downstream task performance on HellaSwag, PiQA, and PubMedQA benchmarks.

## Key Results
- Both replay (25-50% rate) and gradient alignment significantly reduce catastrophic forgetting compared to sequential training
- Combining replay with Reptile provides the best performance, achieving 67.5 average score on downstream tasks versus 67.33 for joint training
- 25% replay rate is more compute-efficient than model scaling for smaller models, while larger models benefit more from scaling
- Reptile adds negligible computational overhead (three times model size in FLOPs every 500 batches)

## Why This Works (Mechanism)

### Mechanism 1: Experience Replay as Stationary Distribution Optimization
Interleaving replay examples with incoming data mitigates catastrophic forgetting by converting non-stationary learning into optimization over an approximately stationary distribution. A fraction α of each batch is drawn from a disk-backed buffer storing past examples, ensuring parameters remain in regions that perform well on historical data. The buffer sufficiently covers the distribution of past tasks through reservoir sampling.

### Mechanism 2: Reptile-Based Gradient Alignment as Regularized Meta-Learning
Periodic interpolation with past parameters promotes gradient alignment across batches, reducing interference and potentially enabling backward transfer. Every k steps, parameters are updated via interpolation: θ_t ← θ_{t-k} + ε(θ_t - θ_{t-k}). This approximately optimizes a regularized objective that encourages positive gradient dot products between batches.

### Mechanism 3: Compute-Optimal Allocation Between Replay Rate and Model Scale
Given fixed FLOPs, low replay rates (25%) are more efficient than model scaling, but high replay rates (50%) are less efficient than scaling. 25% replay adds 1.33× FLOPs per token while 50% replay adds 2× FLOPs. Scaling analysis shows diminishing returns for replay beyond 25%, while model scaling maintains power-law improvements.

## Foundational Learning

- **Stability-Plasticity Dilemma**
  - **Why needed here:** The core problem CPT addresses; replay explicitly trades compute for stability while preserving plasticity on new data
  - **Quick check question:** Can you explain why increasing batch replay ratio α reduces plasticity even as it improves stability?

- **Catastrophic Forgetting**
  - **Why needed here:** The failure mode being mitigated; understanding that forgetting increases with training steps on new distributions
  - **Quick check question:** Why does the paper distinguish "continual pre-training" (100B tokens) from fine-tuning (smaller datasets, fewer steps) in terms of forgetting risk?

- **First-Order Meta-Learning (Reptile)**
  - **Why needed here:** Reptile's theoretical connection to gradient alignment explains why this cheap operation works
  - **Quick check question:** What does the interpolation step θ_t ← θ_{t-k} + ε(θ_t - θ_{t-k}) achieve compared to standard gradient descent?

## Architecture Onboarding

- **Component map:** Data Loader -> Replay Buffer (disk-backed) -> Experience Replay (Algorithm 1) -> AdamW Update -> Reservoir Sampling -> Reptile Interpolation (every 500 steps)

- **Critical path:**
  1. Initialize replay buffer on disk with metadata tracking
  2. Training loop: receive batch → combine with buffer samples → AdamW update → reservoir sampling to buffer → every k batches: Reptile interpolation
  3. Checkpoint buffer metadata for restart capability

- **Design tradeoffs:**
  - α=0.25 vs α=0.50: 1.33× vs 2× FLOPs; paper shows 0.25 is compute-optimal for stability
  - k=500, ε=0.1: Large k for stronger regularization; small ε for stability—tuned but not extensively ablated
  - Disk vs RAM buffer: Infinite capacity but I/O latency mitigated via prefetch queue

- **Failure signatures:**
  - Validation loss rising on earlier tasks → insufficient α or buffer not covering distribution
  - New task loss not decreasing → α too high (plasticity loss)
  - Training instability after Reptile step → ε too large or k too small
  - Slow training despite replay → prefetch queue empty (I/O bottleneck)

- **First 3 experiments:**
  1. **Baseline calibration:** Run 560M sequential (α=0, no Reptile) on DCLM→OSCAR-Fr→OSCAR-De; measure forgetting score and final validation loss
  2. **Replay-only ablation:** Same setup with α=0.25; expect ~2.4 retained loss (Table 2)
  3. **Full MER integration:** Add Reptile (k=500, ε=0.1); expect further reduction to ~2.35 retained loss

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the synergy between gradient alignment and replay increase super-linearly with model scale, offering greater relative gains in 70B+ parameter models compared to the 6B models tested?
- **Basis in paper:** The authors state that their data provides "preliminary evidence of the very exciting possibility that improvements at small scales related to meta-learning can lead to even bigger improvements than those experienced by vanilla learning with large scales of compute."
- **Why unresolved:** The scaling analysis was capped at 6B parameters, leaving the behavior of MER at state-of-the-art scales unknown.

### Open Question 2
- **Question:** Does the reduction in validation loss from MER correlate with the preservation of specific factual knowledge, or does it primarily stabilize linguistic fluency?
- **Basis in paper:** The authors note that "a deeper evaluation such as directly measuring knowledge changes, or testing multilingual QA throughout learning would provide greater insight about the evolution of factual knowledge, which is largely opaque based on our experiments."
- **Why unresolved:** The study relied on aggregate loss metrics and general downstream benchmarks rather than targeted knowledge probes.

### Open Question 3
- **Question:** How does the compute-efficiency of MER degrade when applied to significantly longer sequences of tasks or environments where domains are revisited intermittently?
- **Basis in paper:** The authors identify a limitation: "Our study is largely limited to three tasks... Real-world continual learning for LLMs might involve many more stages, possibly with revisiting domains... further validation is needed."
- **Why unresolved:** The experiments only covered 3-task and 5-task sequences, which may not capture the accumulation of errors or buffer inefficiencies in longer training streams.

## Limitations

- Claims about compute-efficiency trade-offs rely on extrapolation from limited scale range (99M-6B parameters) where inverse power law fits may not hold
- Experimental scope constrained to 5 sequential tasks, raising questions about generalizability to longer task sequences
- Study focuses exclusively on language-specific pre-training without examining cross-task interference scenarios

## Confidence

- **High Confidence:** The core mechanism of experience replay reducing forgetting (supported by multiple ablation studies and consistent validation loss improvements across model sizes)
- **Medium Confidence:** The compute-efficiency claim that 25% replay is optimal (based on inverse power law fits that may not extrapolate reliably)
- **Medium Confidence:** The gradient alignment benefits (limited ablation on k and ε hyperparameters, no comparison to alternative regularization methods)
- **Low Confidence:** The claim that gradient alignment enables backward transfer (minimal evidence provided beyond correlation with downstream task performance)

## Next Checks

1. **Scale Extrapolation Validation:** Train a 13B parameter model using the 25% replay configuration to test whether inverse power law predictions hold at larger scales, measuring both validation loss retention and compute efficiency relative to baseline.

2. **Task Sequence Length Test:** Extend the sequential training to 10 languages (maintaining 100B tokens each) to evaluate whether the observed forgetting mitigation remains effective as task sequence length increases, measuring forgetting score degradation per additional task.

3. **Gradient Alignment Ablation:** Systematically vary k (100, 500, 1000) and ε (0.01, 0.1, 0.5) parameters in the Reptile implementation while holding replay rate constant at 25% to identify the sensitivity of gradient alignment benefits to hyperparameter tuning.