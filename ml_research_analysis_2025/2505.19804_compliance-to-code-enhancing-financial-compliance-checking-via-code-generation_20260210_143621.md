---
ver: rpa2
title: 'Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation'
arxiv_id: '2505.19804'
source_url: https://arxiv.org/abs/2505.19804
tags:
- compliance
- code
- regulatory
- reasoning
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Compliance-to-Code, a large-scale Chinese
  dataset for financial regulatory compliance. It provides structured annotations
  of regulatory clauses into machine-readable compliance units (CUs) with Python code
  mappings.
---

# Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation

## Quick Facts
- **arXiv ID:** 2505.19804
- **Source URL:** https://arxiv.org/abs/2505.19804
- **Reference count:** 13
- **Key outcome:** FinCheck pipeline achieves 78.1% Pass@1 rate using structured regulation parsing and code generation

## Executive Summary
This paper introduces Compliance-to-Code, a large-scale Chinese dataset for financial regulatory compliance, along with FinCheck, an automated pipeline that transforms regulatory text into executable Python code for compliance checking. The system decomposes regulations into structured Compliance Units (CUs) with four logical elements (Subject, Condition, Constraint, Context) and generates deterministic Python functions to verify compliance against financial data. Experiments show GLM-4-9B-0414 excels at regulation structuring while DeepSeek-R1-0528 performs best in code generation, establishing a new benchmark for LLM-driven financial compliance automation.

## Method Summary
The FinCheck pipeline processes financial regulations through a structured approach: first parsing natural language clauses into Compliance Units with explicit schema, then generating Python code with Chain-of-Thought reasoning, and finally executing this code against company financial data to produce compliance reports. The system uses supervised fine-tuning on Qwen3-8B for structure prediction and leverages reasoning-augmented generation for code synthesis. The approach emphasizes modularity and auditability, decomposing complex regulations into manageable logical components that can be systematically verified through execution rather than probabilistic generation.

## Key Results
- FinCheck achieves 78.1% Pass@1 rate on code generation tasks using reasoning-augmented generation
- GLM-4-9B-0414 outperforms other models on regulation structuring with F1 scores of 0.88
- The pipeline demonstrates practical utility through end-to-end verification on real financial data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing regulatory text into structured Compliance Units (CUs) reduces semantic ambiguity before code generation
- **Mechanism:** The model first parses natural language into a fixed schema (Subject, Condition, Constraint, Contextual Info), explicitly separating *who* must act from *when* and *how*. This constrains the search space for the subsequent code generator, preventing the conflation of distinct logical requirements
- **Core assumption:** The source text can be cleanly partitioned into these four slots without losing critical nuance or cross-sentence dependencies
- **Evidence anchors:** [Abstract] "Each clause is modularly structured with four logical elements-subject, condition, constraint, and contextual information-along with regulation relations."

### Mechanism 2
- **Claim:** Translating structured logic into executable Python code enables deterministic, reproducible compliance verification
- **Mechanism:** Instead of generating a probabilistic text judgment ("This is likely compliant"), the system generates a Python function accepting a DataFrame. Compliance is determined by executing this code against real data, yielding a binary result that can be unit-tested
- **Core assumption:** The generated code is syntactically correct, logically sound, and operates on a predefined data schema (columns) available at runtime
- **Evidence anchors:** [Abstract] "...deterministic Python code mappings... to facilitate automated auditing."

### Mechanism 3
- **Claim:** Reasoning-Augmented Generation (explicit Chain-of-Thought) significantly boosts functional correctness in code synthesis
- **Mechanism:** By forcing the model to articulate the calculation steps (e.g., "Step 1: compute total distribution") in natural language comments or reasoning blocks *before* writing syntax, the model effectively plans the logic, reducing immediate logical errors
- **Core assumption:** The generated reasoning path is grounded in the regulation and guides the code generation phase effectively
- **Evidence anchors:** [Section 6.2] "Incorporating explicit reasoning (Structure-Reasoning-to-Code) further elevates Pass@1 to 78.1%."

## Foundational Learning

- **Concept:** **Compliance Unit (CU) Schema**
  - **Why needed here:** The core data structure of the system. You cannot interpret the dataset or debug the pipeline without understanding the decomposition of a regulation into `Subject` (Who), `Condition` (When), `Constraint` (What), and `Context`
  - **Quick check question:** In the clause "A company shall not issue bonuses if net profit is negative," which part is the *Constraint* and which is the *Condition*?

- **Concept:** **Pandas DataFrame Operations**
  - **Why needed here:** The generated code outputs are not standalone scripts but functions designed to process tabular financial data (DataFrames)
  - **Quick check question:** If a generated function checks `df['Net Profit'] < 0`, does this operate on a single row or the whole column vector?

- **Concept:** **Catastrophic Forgetting in SFT**
  - **Why needed here:** The paper highlights a trade-off where extended fine-tuning improves style (CodeBLEU) but degrades reasoning accuracy (Pass@1)
  - **Quick check question:** Why might a model generate syntactically perfect code that fails to solve the specific logic problem after 9 epochs of training?

## Architecture Onboarding

- **Component map:** Raw Regulation -> Structure Predictor -> CU JSON -> Code Generator -> Python Function -> Executor -> Compliance Report
- **Critical path:** The **R2S -> S2C (Regulation-to-Structure-to-Code)** chain. If the Structure Predictor misclassifies a "Condition" as a "Constraint," the Code Generator will create a function with inverted logic (e.g., `if True: pass` instead of `if True: raise Error`)
- **Design tradeoffs:** SFT on Qwen3-8B improves structure extraction (F1 0.63 -> 0.71 equivalent performance vs larger models), but for code generation, explicit prompting with reasoning (CoT) often outperforms fine-tuning on reasoning metrics (Pass@1) unless carefully regularized
- **Failure signatures:** Semantic Drift: Code uses columns like `hypothetical_post_min_high_split_eps` which don't exist in the input schema (Figure 1)
- **First 3 experiments:** 
  1. Unit Test the Golden Dataset: Load the 307 Python modules from the dataset and run them against the provided sample DataFrames
  2. Ablate the Structure: Run the Code Generator using *raw text* vs. *Gold Structured CUs*. Measure the drop in Pass@1
  3. Probe Numerical Reasoning: Specifically evaluate the "Difficult" subset (58 tasks) to see if the model fails on calculation logic (math) or condition mapping (logic)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the FinCheck pipeline be effectively adapted for English-language regulations or non-financial legal domains without significant loss of accuracy?
- Basis in paper: [explicit] "While our approach shows strong results on Chinese financial regulations, extending to other domains and languages may require additional adaptation."
- Why unresolved: The current dataset and experiments are exclusively tailored to Chinese financial statutes
- What evidence would resolve it: Benchmarks of the FinCheck pipeline on English datasets (e.g., CODE-ACCORD) demonstrating comparable Pass@1 scores

### Open Question 2
- Question: Can advanced graph-based methods or external knowledge bases improve the modeling of complex legal relationships across multiple documents?
- Basis in paper: [explicit] "Accurately modeling and reasoning about complex legal relationships across documents is still challenging; more advanced graph-based methods... may help address this."
- Why unresolved: Current inter-unit relations handle limited dependencies; multi-hop cross-document logic remains a bottleneck
- What evidence would resolve it: Performance improvements on clauses requiring cross-document reasoning when augmented with knowledge graphs

### Open Question 3
- Question: How can supervised fine-tuning (SFT) be optimized to prevent the degradation of logical reasoning capabilities while improving stylistic code alignment?
- Basis in paper: [inferred] Section 6.3 notes that increasing SFT epochs improves CodeBLEU but decreases Pass@1, suggesting a trade-off caused by catastrophic forgetting
- Why unresolved: The paper identifies the tension but only conjectures the cause, leaving regularization strategies unexplored
- What evidence would resolve it: Experiments utilizing regularization techniques (e.g., elastic weight consolidation) that stabilize or improve Pass@1 scores over extended training

## Limitations

- The modular pipeline approach suffers from error propagation, where mistakes in structure prediction (45% of cases) directly impact code generation quality
- The system assumes perfect access to structured financial data and doesn't address handling missing data or data quality issues common in real-world compliance
- Performance on complex multi-hop reasoning tasks remains limited, with Pass@1 dropping to ~24-34% on "Difficult" tasks

## Confidence

**High Confidence:**
- The FinCheck dataset and benchmark are novel contributions to financial compliance automation
- GLM-4-9B-0414 demonstrates superior performance on regulation structuring tasks
- DeepSeek-R1-0528 excels in code generation tasks

**Medium Confidence:**
- Reasoning-Augmented Generation significantly improves code correctness
- The modular pipeline approach is more auditable than end-to-end alternatives
- The 78.1% Pass@1 rate represents practical usability

**Low Confidence:**
- The system's performance on unseen, complex regulatory scenarios
- Long-term stability of fine-tuned models (catastrophic forgetting concerns)
- Integration with existing compliance workflows and human oversight requirements

## Next Checks

1. **End-to-End Pipeline Testing with Real Financial Data:** Execute the complete FinCheck pipeline (Structure Predictor → Code Generator → Executor → Report Generator) on actual company financial statements from Wind API. Measure not just code correctness but also runtime performance, error handling, and report quality when processing real-world data with missing values and inconsistencies.

2. **Cross-Domain Transferability Assessment:** Test the FinCheck system on regulatory compliance tasks from other financial domains (e.g., banking regulations, securities law) that were not represented in the original 307 clauses. This would validate whether the model learned domain-general compliance patterns or simply memorized the specific clauses in the training set.

3. **Human Expert Validation Study:** Conduct a blinded study where compliance officers evaluate the system's compliance reports against their own manual analysis of the same regulations and company data. Measure inter-rater reliability, identify systematic errors, and assess whether the system's explanations and reasoning are sufficient for regulatory audit purposes.