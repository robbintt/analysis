---
ver: rpa2
title: 'Generating Natural-Language Surgical Feedback: From Structured Representation
  to Domain-Grounded Evaluation'
arxiv_id: '2511.15159'
source_url: https://arxiv.org/abs/2511.15159
tags:
- feedback
- surgical
- video
- action
- tissue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a structure-aware pipeline for generating natural-language
  surgical feedback grounded in clinically meaningful representations. The approach
  first induces an Instrument-Action-Tissue (IAT) ontology from real trainer-to-trainee
  feedback transcripts, then trains a multimodal video-to-IAT predictor that fuses
  video frames, fine-grained temporal instrument motion, and clinical context.
---

# Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation

## Quick Facts
- arXiv ID: 2511.15159
- Source URL: https://arxiv.org/abs/2511.15159
- Authors: Firdavs Nasriddinov, Rafal Kocielnik, Anima Anandkumar, Andrew J. Hung
- Reference count: 40
- Key outcome: IAT-grounded feedback generation improves clinician fidelity scores from 2.17 to 2.44 (12.4% gain)

## Executive Summary
This paper presents a structure-aware pipeline for generating natural-language surgical feedback grounded in clinically meaningful representations. The approach first induces an Instrument-Action-Tissue (IAT) ontology from real trainer-to-trainee feedback transcripts, then trains a multimodal video-to-IAT predictor that fuses video frames, fine-grained temporal instrument motion, and clinical context. These IAT triplets condition GPT-4o to produce trainer-style feedback. Evaluation shows that adding temporal tracking and context improves IAT recognition AUCs (Instrument: 0.67→0.74, Action: 0.60→0.63, Tissue: 0.74→0.79), and IAT-conditioned feedback reaches a 1–5 clinician-aligned fidelity score of 2.44 versus 2.17 for video-only generation (+12.4%), with the share of admissible outputs (score≥3) doubling from 21% to 42%.

## Method Summary
The method consists of three main stages: ontology induction, multimodal IAT prediction, and conditional text generation. First, a semi-automated pipeline extracts IAT triplets from training transcripts by identifying instruments, clustering action and tissue spans, and consolidating synonyms. Second, a multimodal predictor fuses video frames, instrument kinematic tracks, and clinical context (phase, anatomy) to predict IAT triplets for each frame. Third, GPT-4o generates feedback conditioned on these IAT triplets. The pipeline was evaluated on 1,000 cholecystectomy videos from the da Vinci surgical system, with human clinician assessment of generated feedback quality.

## Key Results
- Multimodal IAT recognition improves AUCs: Instrument (0.67→0.74), Action (0.60→0.63), Tissue (0.74→0.79)
- IAT-grounded feedback reaches mean fidelity score of 2.44/5 vs 2.17 for video-only (12.4% improvement)
- Share of admissible outputs (score≥3) doubles from 21% to 42% with IAT grounding
- IAT-conditioned generation produces more detailed and clinically aligned feedback

## Why This Works (Mechanism)
The structured IAT representation bridges the gap between raw surgical video and clinically meaningful feedback. By explicitly modeling instruments, actions, and tissues, the system captures the semantic content that trainers actually reference when providing feedback. The multimodal fusion of visual, kinematic, and contextual signals enables more accurate IAT recognition than any single modality alone. GPT-4o's strong language generation capabilities are then directed toward clinically relevant content through IAT conditioning, rather than generating generic or irrelevant feedback.

## Foundational Learning
- **IAT Ontology Construction**: Creating a structured vocabulary of surgical elements (instruments, actions, tissues) is essential for grounding feedback in clinically meaningful concepts. Quick check: Verify that extracted IAT triplets cover the full range of feedback in the training corpus.
- **Multimodal Feature Fusion**: Combining visual, temporal, and contextual features improves recognition accuracy by capturing complementary information. Quick check: Compare single-modality vs multimodal IAT prediction performance.
- **Conditional Text Generation**: Conditioning language models on structured representations guides generation toward clinically relevant content. Quick check: Assess whether IAT-conditioned feedback contains more instrument-specific and action-specific language.

## Architecture Onboarding

**Component Map**: Video frames, instrument kinematics, clinical context -> Multimodal IAT predictor -> GPT-4o (IAT-conditioned) -> Natural-language feedback

**Critical Path**: The sequence from multimodal IAT prediction through GPT-4o conditioning represents the core innovation. IAT triplets must be accurately predicted to guide meaningful feedback generation.

**Design Tradeoffs**: Using GPT-4o enables high-quality language generation but requires significant computational resources and API costs. The structured IAT approach sacrifices some flexibility compared to end-to-end generation but gains clinical interpretability and control.

**Failure Signatures**: Poor IAT recognition leads to irrelevant or generic feedback. Missing instruments or misidentifying tissues results in feedback that doesn't match what's actually happening in the video. Over-reliance on context without visual confirmation can produce hallucinated feedback.

**3 First Experiments**:
1. Ablation study removing each input modality (video, kinematics, context) to quantify individual contributions
2. Comparison of IAT-conditioned vs prompt-only GPT-4o generation on the same video inputs
3. Human evaluation of IAT recognition accuracy across different surgical phases and instrument types

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on small expert panel (n=7) and single procedure (cholecystectomy), limiting generalizability
- IAT ontology induction from 1,000 transcripts may miss rare but clinically important feedback patterns
- Multimodal input fusion showed marginal improvements (AUC gains of 0.02-0.05 per component)

## Confidence

**High Confidence**: IAT ontology construction methodology and multimodal feature extraction pipeline are technically sound and reproducible

**Medium Confidence**: Reported AUC improvements and fidelity score gains are likely valid within the evaluated domain but may not generalize

**Medium Confidence**: Clinician evaluation methodology is appropriate but sample size limits statistical power

## Next Checks
1. Test the pipeline on multi-procedure datasets (e.g., hernia repair, appendectomy) to assess domain transfer
2. Conduct longitudinal studies with training cohorts to measure actual learning outcomes from AI-generated feedback
3. Implement ablation studies isolating individual IAT components to quantify their marginal contributions to generation quality