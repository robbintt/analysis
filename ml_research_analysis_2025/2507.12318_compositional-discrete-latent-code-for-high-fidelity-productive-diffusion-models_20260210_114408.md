---
ver: rpa2
title: Compositional Discrete Latent Code for High Fidelity, Productive Diffusion
  Models
arxiv_id: '2507.12318'
source_url: https://arxiv.org/abs/2507.12318
tags:
- diffusion
- arxiv
- image
- generation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that the effectiveness of diffusion models comes
  largely from input conditioning, not the model architecture itself. The authors
  identify a core limitation: continuous representations used to condition diffusion
  models are difficult to sample and lack compositionality, making them unsuitable
  for generating novel out-of-distribution images.'
---

# Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models

## Quick Facts
- arXiv ID: 2507.12318
- Source URL: https://arxiv.org/abs/2507.12318
- Reference count: 38
- Key outcome: Demonstrates that conditioning diffusion models with discrete latent codes achieves state-of-the-art FID for unconditional generation and enables compositional generation of novel images by recombining features from different images

## Executive Summary
This paper identifies a fundamental limitation of diffusion models: while they achieve remarkable image generation quality, this success stems primarily from powerful input conditioning rather than architectural innovations. The authors argue that continuous representations used to condition diffusion models are difficult to sample and lack compositionality, making them unsuitable for generating novel out-of-distribution images. To address this, they propose Discrete Latent Codes (DLCs), which are sequences of discrete tokens derived from Simplicial Embeddings trained with self-supervised learning. DLCs are easy to sample and, crucially, are compositional—enabling novel image generation by recombining features from different images. The authors train diffusion models on ImageNet conditioned on DLCs, achieving state-of-the-art FID for unconditional generation. They also demonstrate that DLCs enable high-quality, diverse compositional generation and enable text-to-image generation via fine-tuning a text diffusion model to produce DLCs.

## Method Summary
The proposed method uses discrete latent codes as conditioning for diffusion models, where each image is represented as a sequence of discrete tokens derived from Simplicial Embeddings trained with self-supervised learning. The key insight is that these discrete representations are both easy to sample and compositional, allowing for novel image generation by recombining features from different images. The approach involves training a diffusion model on ImageNet conditioned on these DLCs, achieving state-of-the-art FID for unconditional generation. For text-to-image generation, the authors fine-tune a text diffusion model (LLADA) to produce DLCs, which are then fed into the image generator. This modular approach leverages large-scale pretrained language models without requiring joint training from scratch.

## Key Results
- Achieves state-of-the-art FID for unconditional image generation on ImageNet using diffusion models conditioned on discrete latent codes
- Demonstrates compositional generation capabilities, producing novel images by recombining features from different source images
- Enables text-to-image generation through fine-tuning a text diffusion model to produce DLCs, without requiring joint training of the full pipeline

## Why This Works (Mechanism)
The effectiveness of diffusion models comes largely from powerful input conditioning rather than architectural innovations. Continuous representations used to condition diffusion models are difficult to sample and lack compositionality, limiting their ability to generate novel out-of-distribution images. Discrete latent codes address these limitations by providing representations that are both easy to sample and compositional, enabling the recombination of features from different images to create novel outputs. The Simplicial Embeddings used to derive these codes are trained with self-supervised learning, capturing rich semantic information while maintaining the discrete structure necessary for efficient sampling and composition.

## Foundational Learning
- Simplicial Embeddings: A self-supervised learning approach for creating discrete representations that capture semantic information while maintaining compositionality. Why needed: To generate discrete latent codes that are both semantically meaningful and easy to sample. Quick check: Verify that the embeddings preserve semantic relationships when mapped to discrete tokens.
- Discrete Diffusion Models: Diffusion models that operate on discrete sequences rather than continuous representations. Why needed: To enable efficient sampling and composition of latent codes while maintaining the generative capabilities of diffusion models. Quick check: Ensure the discrete diffusion process converges and produces coherent outputs.
- Compositional Generation: The ability to generate novel images by recombining features from different source images. Why needed: To overcome the limitation of traditional diffusion models that struggle to generate truly novel out-of-distribution images. Quick check: Measure the novelty of generated images compared to training data using established metrics.

## Architecture Onboarding

Component map: Simplicial Embeddings -> Discrete Latent Codes -> Diffusion Model -> Generated Images

Critical path: Image input → Simplicial Embedding extraction → Discrete tokenization → Diffusion model conditioning → Image generation

Design tradeoffs: The paper explores different dimensional configurations (32×4096 vs 512×256 tokens) to balance representation capacity with computational efficiency. Longer sequences improve performance but increase training cost.

Failure signatures: Poor semantic preservation in the discrete tokenization step would manifest as generated images that lack coherence or fail to capture meaningful features from the input. Insufficient compositionality would result in generated images that appear as simple interpolations rather than novel combinations.

First experiments:
1. Verify that Simplicial Embeddings preserve semantic relationships when mapped to discrete tokens
2. Test the discrete diffusion model's ability to reconstruct training images from their DLC representations
3. Evaluate the compositionality of DLCs by recombining tokens from different images and measuring the novelty of the results

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the text-to-DLC-to-image pipeline maintain high fidelity and coherence when scaling from 9M image-text pairs to the full magnitude of datasets like LAION-5B, and how does the compute efficiency compare to training monolithic text-to-image models from scratch?
- Basis in paper: Section 6 states the text-to-image results are a "proof-of-concept" trained on a "random subset of 9M image-text pairs," which is "orders of magnitude less" than standard models.
- Why unresolved: The authors demonstrate feasibility on a small scale but do not verify if the modular pipeline sustains its advantages at internet scale.
- What evidence would resolve it: A comparison of FID/CLIP score scaling curves against compute budget for the DLC pipeline versus end-to-end baselines on the full LAION dataset.

### Open Question 2
- Question: Can a single discrete diffusion model jointly model text and image DLCs in a shared sequence, enabling bidirectional generation or interleaved text-image content without the cascaded Markov chain?
- Basis in paper: The Conclusion suggests that "large scale pretrained diffusion language models offer a vision for a unified text-image generation interface," implying the current separate pipeline is a step toward integration.
- Why unresolved: The current method separates text-to-DLC generation (LLADA) and image generation (DiT), leaving the potential for a single unified model unexplored.
- What evidence would resolve it: Training a unified discrete diffusion model on mixed text/DLC tokens and evaluating its ability to perform text-to-image, image-to-text, and in-filling tasks simultaneously.

### Open Question 3
- Question: What are the theoretical scaling limits of DLC representation, specifically regarding the trade-off between sequence length and vocabulary size for a fixed dimensionality budget?
- Basis in paper: Section 5.1 investigates fixed-dimensional configurations ($32 \times 4096$ vs $512 \times 256$) and finds longer sequences improve performance but increase training cost (Figure 5), leaving the optimal scaling trajectory undefined.
- Why unresolved: The paper empirically tests three configurations but does not determine if increasing token count indefinitely continues to yield fidelity gains or if vocabulary capacity eventually becomes the bottleneck.
- What evidence would resolve it: A systematic scaling law analysis plotting generation FID against varying sequence lengths and vocabulary sizes to identify the point of diminishing returns.

## Limitations
- The compositional generation capabilities are demonstrated but the evaluation could be more rigorous to establish that this represents a meaningful advance over existing approaches
- While the authors claim that DLCs enable text-to-image generation without joint training, the quality and diversity of these generated images relative to state-of-the-art text-to-image models remain unclear from the provided evidence
- The extent to which DLCs genuinely enable "novel out-of-distribution image generation" through compositionality is not thoroughly validated with controlled experiments that isolate this effect from other factors

## Confidence
- High confidence: The technical approach of using discrete latent codes for diffusion model conditioning is sound and the implementation details are well-described
- Medium confidence: The claim that DLCs are easier to sample than continuous representations is supported by the methodology but would benefit from more direct empirical validation
- Medium confidence: The compositional generation capabilities are demonstrated but the evaluation could be more rigorous to establish that this represents a meaningful advance over existing approaches

## Next Checks
1. Conduct ablation studies comparing DLC-based diffusion models against continuous conditioning approaches while controlling for other variables like model size and training data
2. Evaluate the diversity and novelty of compositional generation using established metrics like LPIPS and KID in addition to FID, with systematic comparisons to baseline methods
3. Perform human perceptual studies to assess whether generated images from DLC-based text-to-image generation are perceived as higher quality or more semantically coherent than those from comparable models