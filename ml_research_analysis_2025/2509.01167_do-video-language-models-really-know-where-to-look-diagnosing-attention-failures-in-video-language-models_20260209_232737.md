---
ver: rpa2
title: Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures
  in Video Language Models
arxiv_id: '2509.01167'
source_url: https://arxiv.org/abs/2509.01167
tags:
- video
- confidence
- question
- answer
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether vision-language encoders like CLIP\
  \ and SigLIP effectively identify informative video frames for video MLLMs. Through\
  \ experiments on three benchmarks (EgoSchema, MVBench, NExTQA), it reveals that\
  \ these encoders struggle with interrogative and ambiguous prompts, show little\
  \ correlation between confidence scores and model performance, and that sampled\
  \ frames have minimal impact on predictions\u2014models can drop up to 50% of frames\
  \ with little accuracy loss."
---

# Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models

## Quick Facts
- arXiv ID: 2509.01167
- Source URL: https://arxiv.org/abs/2509.01167
- Reference count: 25
- Primary result: Vision-language encoders (CLIP, SigLIP) struggle to identify informative video frames, with confidence scores poorly correlating with MLLM accuracy and models maintaining performance even after dropping 50% of frames.

## Executive Summary
This paper investigates whether vision-language encoders like CLIP and SigLIP effectively identify informative video frames for video MLLMs. Through experiments on three benchmarks (EgoSchema, MVBench, NExTQA), it reveals that these encoders struggle with interrogative and ambiguous prompts, show little correlation between confidence scores and model performance, and that sampled frames have minimal impact on predictions—models can drop up to 50% of frames with little accuracy loss. Oracle experiments show significant performance gaps, indicating room for improvement. Using CRIS, a variant of CLIP for referring image segmentation, improves keyframe sampling performance, but substantial gaps remain.

## Method Summary
The paper evaluates vision encoder effectiveness for video MLLM frame selection using three benchmarks. No training is performed—only inference. Three frame selection strategies are tested: max/min confidence sampling, question vs answer-based selection, and sliding window inference to compute oracle performance. Video MLLMs include SmolVLM2, LLaVA-OV, and VideoLLaMA3 models. Vision encoders include SigLIP, SigLIP2, and CRIS variants. The primary metric is accuracy on video QA tasks, with Spearman correlation between encoder confidence and model correctness as a secondary measure.

## Key Results
- Vision encoders show minimal correlation between confidence scores and MLLM accuracy
- Models maintain good performance even after removing up to 50% of frames
- Answer-based frame selection outperforms question-based selection on EgoSchema
- CRIS improves keyframe sampling but substantial gaps remain
- Oracle performance reveals significant potential for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP/SigLIP-based frame selection underperforms because these encoders are not robust to interrogative or ambiguous language.
- Mechanism: Encoders are pre-trained on declarative image-text pairs. When the prompt is a question rather than a declarative sentence, the text encoder produces embeddings that do not align well with relevant visual content, leading to suboptimal keyframe rankings.
- Core assumption: The primary failure mode for keyframe selection is a linguistic mismatch (question vs. statement) rather than a purely visual one.
- Evidence anchors:
  - "...popular vision encoders critically suffer from their limited capability to identify where the MLLM should look inside the video..."
  - "...all models demonstrate notable performance improvements when using answer-based frame selection compared to question-based on EgoSchema."
  - Corpus papers focus on efficient keyframe selection but do not explicitly test the linguistic robustness of the underlying encoders as a primary failure mode.
- Break condition: The claim is weakened for shorter videos where relevant information is dense (e.g., MVBench, NExTQA), as performance differences between question-based and answer-based sampling were found to be minimal.

### Mechanism 2
- Claim: Vision encoder confidence scores are not reliable indicators for frame importance.
- Mechanism: A high CLIP/SigLIP probability for a given frame does not correlate with the MLLM's likelihood of correctly answering the query based on that frame. The encoder's "confidence" captures text-image similarity on a pre-training distribution, not the utility of the visual information for the MLLM's downstream reasoning task.
- Core assumption: A keyframe's utility is defined by its contribution to the MLLM's final answer, not just its surface-level similarity to the query text.
- Evidence anchors:
  - "Encoder confidence scores show minimal correlation with actual model accuracy, suggesting confidence-based sampling provides limited guidance."
  - "...confidence distributions for correct and incorrect predictions are indistinguishable."
  - Corpus paper 'MLLMs Know Where to Look' suggests that MLLMs can perceive small details, implying their internal attention mechanism is distinct from the external vision encoder's confidence.
- Break condition: The correlation may not be zero for all model/encoder pairs; one model (VideoLLaMA3-7B) showed an inverse relationship, suggesting complex, architecture-dependent interactions.

### Mechanism 3
- Claim: Current video MLLMs and benchmarks suffer from "static appearance bias" and do not require precise temporal grounding.
- Mechanism: Many video QA questions can be answered by a single frame or are solvable without video at all (language priors). This makes robust keyframe selection appear less critical than it is for true temporal understanding, as models can maintain performance even with a high frame-drop rate.
- Core assumption: The lack of sensitivity to frame selection is a symptom of both model limitations in temporal reasoning and benchmark design flaws (questions not requiring deep video understanding).
- Evidence anchors:
  - "...models can maintain good performance even after removing up to 50% of frames, with substantial gaps remaining between uniform and oracle performance."
  - "Our results reveal the limitations of current video understanding models and benchmarks, indicating that current video MLLMs may struggle to fully leverage video context..."
  - Multiple corpus papers aim to select "informative" frames, implicitly assuming that such frames are critical for performance, a premise challenged by this paper's findings.
- Break condition: This finding applies primarily to the evaluated benchmarks. On benchmarks designed to require precise temporal reasoning, the ability to drop frames without performance loss would likely diminish.

## Foundational Learning

- **Concept: Vision-Language Pre-training (CLIP/SigLIP)**
  - Why needed here: This is the foundational technology whose limitations are being diagnosed. One must understand that these models learn a shared embedding space for images and *declarative* text, making them brittle when text input shifts to interrogative forms.
  - Quick check question: How would the embedding for the text "A cat is on the ground" differ from "What is on the ground?" in a CLIP model, and why does this matter for frame retrieval?

- **Concept: Keyframe Sampling Strategies**
  - Why needed here: This is the component being evaluated. Understanding different strategies (uniform, confidence-based, oracle) is essential to interpret the performance gaps and the nature of the "attention failure."
  - Quick check question: Explain why the "oracle" performance represents an upper bound and what the large gap between uniform sampling and oracle performance implies about the potential for future improvements in frame selection.

- **Concept: Spearman Rank Correlation**
  - Why needed here: This statistical metric is used as primary evidence for the lack of relationship between encoder confidence and model accuracy. Understanding it is necessary to evaluate the strength of the paper's empirical claims.
  - Quick check question: In the context of this paper, what would a Spearman rank correlation of 0.0 between encoder confidence and model accuracy mean for a confidence-based sampling algorithm?

## Architecture Onboarding

- **Component map**: Vision Encoder (SigLIP/CLIP) -> Frame Sampler -> Video MLLM (e.g., LLaVA-OV, VideoLLaMA3) -> Final answer accuracy
- **Critical path**: The accuracy of the final Video MLLM response is the ultimate metric. The experiments trace the dependency from the Vision Encoder's frame scores → the Frame Sampler's selection → the Video MLLM's input → the final answer accuracy.
- **Design tradeoffs**:
  - Uniform vs. Confidence-based Sampling: Uniform sampling is computationally cheap and simple but may miss key moments in long videos. Confidence-based sampling is more sophisticated but, as shown, can be misled by linguistic nuances.
  - Question-based vs. Answer-based Prompting: Using the answer for frame selection improves performance but requires access to the ground truth or a generated answer, making it infeasible for standard inference.
- **Failure signatures**:
  - A keyframe sampler that consistently provides frames leading to incorrect answers.
  - A confidence score distribution that does not differentiate between windows that produce correct and incorrect answers.
  - Minimal performance degradation when dropping up to 50% of frames from a video, indicating redundancy in the visual input.
- **First 3 experiments**:
  1. **Ablation on Prompt Type**: Compare frame selection using (a) the raw question, (b) the ground-truth answer, and (c) a declarative reformulation of the question to quantify the effect of linguistic ambiguity.
  2. **Confidence Correlation Study**: For a given dataset, compute Spearman correlation between the vision encoder's max confidence score for a sliding window and the binary correctness of the MLLM's prediction on that window, replicating the result that ρ ≈ 0.
  3. **Redundancy Analysis**: Incrementally drop frames (10%, 20%, ... 90%) from the input and plot the MLLM's accuracy to empirically determine the "information density" of the video and validate the static appearance bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do some video MLLMs exhibit an inverse relationship where accuracy improves when selecting frames with minimum rather than maximum encoder confidence?
- Basis in paper: Page 3 notes VideoLLaMA3-7B shows an inverse relationship, stating "This phenomenon needs further investigation in future work to understand this unexpected behavior."
- Why unresolved: The authors identify the empirical anomaly but do not provide a theoretical or ablation-based explanation for why lower relevance scores would correlate with better reasoning in specific model families.
- What evidence would resolve it: An ablation study analyzing the feature space of the specific encoder or the reasoning patterns of the MLLM on low-confidence frames to determine if "distractor" frames force better reasoning or if the encoder is fundamentally misaligned with the MLLM's requirements.

### Open Question 2
- Question: Can agent-based systems utilizing sophisticated question analysis effectively predict question difficulty to dynamically determine optimal video sampling strategies?
- Basis in paper: Appendix B.2 discusses failed attempts to predict difficulty using GPT-4o and concludes: "This insight suggests promising future work in developing more sophisticated question analysis approaches, such as agent-based systems..."
- Why unresolved: Initial experiments using standard LLM inference showed minimal correlation with actual performance, leaving the potential for specialized, fine-tuned, or agentic analysis methods unexplored.
- What evidence would resolve it: A comparative study validating a dynamic sampling agent that successfully correlates predicted question difficulty with the required frame density, outperforming uniform sampling.

### Open Question 3
- Question: What specific vision encoder architectures are required to close the substantial performance gap between current keyframe sampling and the oracle upper bound?
- Basis in paper: The authors observe a large gap between uniform and oracle performance (Table 3) and, regarding their mitigation with CRIS, state: "Still, substantial room for improvement remains, which we leave for future work."
- Why unresolved: While CRIS offers improvements over SigLIP, current encoders still fail to consistently identify the most crucial visual evidence, leaving significant accuracy gains (e.g., +17.8% on EgoSchema) on the table.
- What evidence would resolve it: The proposal and validation of a new keyframe selector or vision encoder that statistically significantly narrows the gap between automated selection accuracy and the oracle sliding-window accuracy.

## Limitations

- Benchmark Coverage: Findings primarily demonstrated on EgoSchema, MVBench, and NExTQA; generalizability to benchmarks requiring precise temporal reasoning remains unclear.
- Linguistic Ambiguity Scope: The claim that encoders are not robust to interrogative language is supported but the exact boundary of this brittleness is not mapped.
- Architecture Dependence: The inverse relationship in VideoLLaMA3 suggests complex model-specific interactions without mechanistic explanation.

## Confidence

- **High Confidence**: The empirical finding that CLIP/SigLIP encoders show minimal correlation between their confidence scores and MLLM accuracy is robustly supported by the Spearman correlation analysis across all three datasets.
- **Medium Confidence**: The claim that current video MLLMs suffer from static appearance bias is supported by frame-dropping experiments but its generality is limited by benchmark selection.
- **Low Confidence**: The proposed mechanism that vision encoders fail on interrogative prompts because they are trained on declarative captions is plausible but not rigorously tested.

## Next Checks

1. **Temporal Grounding Stress Test**: Evaluate the frame selection methods on a benchmark specifically designed to require precise temporal localization (e.g., EgoVQA, NExTVideo). If the large gap between uniform and oracle performance disappears, it would confirm that the current limitations are benchmark-dependent.

2. **Linguistic Robustness Analysis**: Design a controlled experiment where the only difference between prompts is the presence of interrogative vs. declarative syntax (e.g., "Two people are wrestling" vs. "What are two people doing?"). Measure the impact on frame selection quality to isolate the effect of linguistic form.

3. **Encoder-Architecture Interaction Study**: Systematically vary the vision encoder (CLIP, SigLIP, CRIS) and MLLM (VideoLLaMA3, LLaVA-OV) combinations to map the space of confidence-accuracy correlations. This would determine whether the inverse relationship observed in VideoLLaMA3 is a general phenomenon or an isolated case.