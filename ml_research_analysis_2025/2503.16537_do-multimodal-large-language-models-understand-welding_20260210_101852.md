---
ver: rpa2
title: Do Multimodal Large Language Models Understand Welding?
arxiv_id: '2503.16537'
source_url: https://arxiv.org/abs/2503.16537
tags:
- images
- performance
- context
- weld
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of multimodal large language
  models (MLLMs) in assessing weld quality across three contexts: RV & Marine, Aeronautical,
  and Farming. Using a dataset of real-world and online weld images annotated by domain
  experts, the researchers tested GPT-4o and LLaVA-1.6 models in both zero-shot and
  WeldPrompt (Chain-of-Thought with in-context learning) settings.'
---

# Do Multimodal Large Language Models Understand Welding?

## Quick Facts
- arXiv ID: 2503.16537
- Source URL: https://arxiv.org/abs/2503.16537
- Authors: Grigorii Khvatskii; Yong Suk Lee; Corey Angst; Maria Gibbs; Robert Landers; Nitesh V. Chawla
- Reference count: 40
- Primary result: GPT-4o outperformed LLaVA-1.6 on weld quality assessment; both models performed better on online images than real-world ones, suggesting reliance on memorization.

## Executive Summary
This study evaluates multimodal large language models (MLLMs) for assessing weld quality across three industry contexts: RV & Marine, Aeronautical, and Farming. Using a dataset of real-world and online weld images annotated by domain experts, the researchers tested GPT-4o and LLaVA-1.6 models in both zero-shot and WeldPrompt (Chain-of-Thought with in-context learning) settings. Results showed that both models performed better on online images than real-world ones, indicating reliance on memorization rather than reasoning. WeldPrompt improved recall in some contexts but showed inconsistent performance overall. GPT-4o outperformed LLaVA-1.6 across all metrics.

## Method Summary
The study used two datasets: Real-World (collected from welders and training centers) and Online (73 images after removing 15 annotated ones, sourced from Google searches). Domain experts labeled each weld image as acceptable or unacceptable for three contexts. Two MLLMs were evaluated: GPT-4o and LLaVA-1.6-Mistral-7B. The evaluation used zero-shot prompting and WeldPrompt (a Chain-of-Thought approach with in-context learning using 5 retrieved similar examples). Metrics included precision, recall, F1-score, and ROC-AUC, computed per context and averaged using micro, macro, weighted, and sample approaches.

## Key Results
- GPT-4o significantly outperformed LLaVA-1.6 across all metrics and contexts
- Both models performed better on online images than real-world ones, suggesting memorization rather than reasoning
- WeldPrompt improved recall in some contexts but showed inconsistent performance, with trade-offs between precision and recall
- Model performance varied dramatically across contexts due to class imbalance and safety standards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs achieve higher performance on previously-seen image distributions through pattern retrieval rather than domain reasoning.
- Mechanism: When input images resemble training data distribution (online weld images), models activate memorized visual-textual associations. Novel real-world images lack these cached patterns, forcing the model into weaker generalization pathways.
- Core assumption: Online weld images were present in pre-training corpora; real-world shop-floor images were not.
- Evidence anchors:
  - [abstract] "both models perform better on online images, likely due to prior exposure or memorization"
  - [section 4.1.1] "suggests that the model relies heavily on memorizing and retrieving its training data... instead of using reasoning"
  - [corpus] Related welding ML papers (e.g., Out of Distribution Detection for Arc Welding) emphasize distribution shift as a key failure mode, but do not directly validate memorization claims for MLLMs.
- Break condition: If real-world images were actually present in pre-training data, or if performance gaps disappear with controlled dataset matching, the memorization hypothesis weakens.

### Mechanism 2
- Claim: Chain-of-Thought with in-context learning (WeldPrompt) can improve recall by providing task-relevant exemplars, but introduces precision trade-offs.
- Mechanism: Retrieving k similar images with correct reasoning traces primes the model's output distribution toward context-aligned responses. However, exemplar selection via embedding similarity may surface superficially similar but contextually inappropriate examples.
- Core assumption: Embedding similarity (vit-base-patch16-224) correlates with weld assessment relevance; correct zero-shot examples exist in the reference pool.
- Evidence anchors:
  - [abstract] "WeldPrompt improves model recall in certain contexts but exhibits inconsistent performance across others"
  - [section 3.2.2] "we then used cosine similarity to identify the five closest images in the pre-processed reference set"
  - [corpus] No direct corpus validation for this specific prompting mechanism in welding; weak external corroboration.
- Break condition: If recall improvements vanish with random exemplar selection (vs. embedding-based retrieval), the mechanism relies on retrieval quality, not CoT itself.

### Mechanism 3
- Claim: Domain-specific class imbalance and context stringency interact with model conservatism, producing context-dependent precision-recall trade-offs.
- Mechanism: High-standards contexts (Aeronautical) have fewer acceptable examples, biasing models toward rejection (high precision, low recall). Lower-standards contexts (Farming) show the inverse.
- Core assumption: Expert labels reflect ground truth; model conservatism is a response to training distribution and prompt framing.
- Evidence anchors:
  - [section 4.1.1] "GPT-4o model was generally conservative, rejecting welds deemed acceptable by the expert" in Aeronautical
  - [table 1] Aeronautical IMB ratio: 0.145 (Real-World) vs. Farming: 2.412 (Online)
  - [corpus] Physic-HM paper notes process-logic blindness in multimodal anomaly detection, but does not directly address class imbalance effects.
- Break condition: If balanced datasets produce uniform precision-recall across contexts, imbalance is the primary driver.

## Foundational Learning

- Concept: **Zero-shot vs. Few-shot In-Context Learning**
  - Why needed here: The paper contrasts zero-shot classification (no exemplars) with WeldPrompt (k=5 similar exemplars). Understanding how exemplars shift output distributions is essential for interpreting the results.
  - Quick check question: Given a classification task, can you explain why adding correct examples to the prompt might improve recall but harm precision?

- Concept: **Embedding-based Retrieval (Vision Encoders)**
  - Why needed here: WeldPrompt uses vit-base-patch16-224 embeddings and cosine similarity to retrieve exemplars. Misalignment between embedding similarity and task relevance is a hypothesized failure mode.
  - Quick check question: If two weld images have high cosine similarity in a general vision embedding space, does that guarantee they share the same acceptability label in a specific domain context?

- Concept: **Precision-Recall Trade-off in Imbalanced Domains**
  - Why needed here: The three welding contexts have vastly different class imbalance ratios. Metrics shift dramatically depending on model conservatism.
  - Quick check question: In a domain with 85% negative examples, what happens to precision if the model defaults to predicting "unacceptable"?

## Architecture Onboarding

- Component map: Input Layer -> Vision Encoder -> Retrieval Module -> Prompt Constructor -> MLLM Backbone -> Output Parser
- Critical path: Image → Embedding → Retrieval → Prompt Assembly → MLLM → Binary Response → Aggregation
- Design tradeoffs:
  - GPT-4o: Higher performance, but proprietary and costly; 128K context window underutilized
  - LLaVA-1.6: Open-weight, deployable on edge; 32K context; significantly lower accuracy
  - WeldPrompt: May improve recall at precision cost; depends on quality of retrieved exemplars
- Failure signatures:
  - Near-zero recall with high precision: Model defaulting to "unacceptable" (class imbalance + conservatism)
  - WeldPrompt underperforms zero-shot: Retrieved exemplars may be misleading or embedding space misaligned
  - Large gap between Online vs. Real-World performance: Distribution shift; potential memorization
- First 3 experiments:
  1. **Baseline Replication**: Run zero-shot evaluation on both datasets with GPT-4o; confirm Online > Real-World gap per Table 2.
  2. **Ablation on Retrieval**: Replace embedding-based retrieval with random exemplar selection; measure impact on recall/precision.
  3. **Class Balance Sensitivity**: Subsample Aeronautical and Farming contexts to equalize positive/negative ratios; re-run zero-shot to isolate imbalance effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific fine-tuning enable smaller MLLMs to outperform large general-purpose models (e.g., GPT-4o) in specialized industrial inspection tasks?
- Basis in paper: [explicit] The authors note that while smaller models historically showed potential, LLaVA-1.6 significantly underperformed compared to GPT-4o, suggesting "further testing is required when preparing small models for production deployments."
- Why unresolved: The study only evaluated a general-purpose smaller model (LLaVA-1.6) against a general-purpose large model (GPT-4o) without applying domain-specific fine-tuning to the smaller model.
- What evidence would resolve it: A comparative study where smaller architectures are fine-tuned on welding-specific datasets and evaluated against general-purpose SOTA models on real-world welding images.

### Open Question 2
- Question: How can prompting strategies be optimized to consistently balance the trade-off between precision and recall across different safety-critical contexts?
- Basis in paper: [explicit] The paper highlights that while WeldPrompt improved recall in some instances, it "exhibits inconsistent performance across others" and "trade-offs between precision and recall remain a key consideration."
- Why unresolved: The introduced WeldPrompt strategy showed instability, improving F1 scores in Farming contexts but degrading performance in Aeronautical contexts on real-world data.
- What evidence would resolve it: Identification of a prompting mechanism that yields statistically significant improvements in both precision and recall simultaneously across all three domains (RV & Marine, Aeronautical, Farming).

### Open Question 3
- Question: How can the reasoning outputs of MLLMs be analyzed and refined to align with expert human logic in explainable AI (XAI) frameworks for Industry 5.0?
- Basis in paper: [explicit] The authors state that their analysis of model reasoning "did not yield significant findings" and that "further investigation into this aspect will be vital for advancing XAI research."
- Why unresolved: The study focused primarily on binary classification performance metrics rather than deeply evaluating the semantic validity or expert-alignment of the model's chain-of-thought explanations.
- What evidence would resolve it: A qualitative and quantitative framework for scoring the "expert-alignability" of MLLM reasoning chains, showing a correlation between reasoning quality and classification accuracy.

## Limitations

- Memorization vs. reasoning: The study suggests models rely on memorization for online images, but lacks controlled experiments to definitively prove this mechanism
- Inconsistent WeldPrompt performance: The prompting strategy showed unstable results across contexts, with unclear causes (exemplar quality vs. context sensitivity)
- Class imbalance confounding: LLaVA-1.6's poor performance may be primarily due to class imbalance effects rather than true model capability limitations

## Confidence

- High: GPT-4o outperforms LLaVA-1.6 across all metrics (directly observed in Table 2)
- Medium: Models rely on memorization for online images (supported by distribution shift and context differences, but no direct memorization test)
- Medium: WeldPrompt improves recall at precision cost (observed in Table 2, but mechanism unclear due to inconsistent context effects)
- Low: Class imbalance alone explains context-dependent performance (correlation present but causation not established)

## Next Checks

1. Conduct distribution matching experiment: curate real-world and online datasets with identical visual characteristics (lighting, backgrounds, weld types) and re-evaluate to isolate memorization vs. reasoning effects
2. Implement exemplar quality control: replace embedding-based retrieval with random exemplar selection in WeldPrompt; if performance drops, retrieval quality drives results, not CoT mechanism
3. Perform class balance ablation: subsample each context to 50/50 positive-negative ratio and re-run zero-shot evaluation to determine if imbalance drives observed precision-recall trade-offs