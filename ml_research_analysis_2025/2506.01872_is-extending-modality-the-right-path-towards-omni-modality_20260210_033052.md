---
ver: rpa2
title: Is Extending Modality The Right Path Towards Omni-Modality?
arxiv_id: '2506.01872'
source_url: https://arxiv.org/abs/2506.01872
tags:
- modality
- fine-tuning
- language
- merging
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates modality extension as a path to omni-modality
  in large language models. The authors examine three questions: whether modality
  extension compromises language abilities, if model merging can integrate modality-specific
  models, and if omni-modality fine-tuning improves knowledge sharing.'
---

# Is Extending Modality The Right Path Towards Omni-Modality?

## Quick Facts
- arXiv ID: 2506.01872
- Source URL: https://arxiv.org/abs/2506.01872
- Reference count: 40
- Primary result: Modality extension degrades reasoning capabilities but enhances knowledge through visual modalities, with weighted model merging offering partial preservation of language skills.

## Executive Summary
This paper investigates modality extension as a path to omni-modality in large language models. The authors examine three questions: whether modality extension compromises language abilities, if model merging can integrate modality-specific models, and if omni-modality fine-tuning improves knowledge sharing. Through extensive experiments, they find that modality fine-tuning degrades reasoning and instruction-following while enhancing knowledge through visual modalities. Weighted model merging better preserves language capabilities than standard averaging, but still underperforms specialized models. Omni-modality fine-tuning is less efficient than modality-specific fine-tuning, requiring more data for similar performance. The study concludes that modality fine-tuning introduces trade-offs and that current approaches fall short of achieving true omni-modality, suggesting the need for more refined strategies.

## Method Summary
The study evaluates three modality-extended models (Qwen2-VL-7B-Instruct, LLaVA-Video-7B-Qwen2, LLaVA-OneVision-Qwen2-7B-SI) fine-tuned on 1.4T tokens of multi-modal data. Weighted model merging is implemented by computing parameter shift magnitudes and applying softmax normalization to determine merge weights. Omni-modality fine-tuning is tested using a 3:2:1 mix of text, image, and video data. Models are evaluated on knowledge benchmarks (MMLU-Pro), reasoning tasks (MATH, GPQA, HumanEval+), instruction following (IFEval), and multi-modal tasks (MMMU, Video-MME).

## Key Results
- Visual modality fine-tuning improves MMLU-Pro by ~5% while degrading MATH by 10.2% and HumanEval+ Pass@1 by 16.6%
- Weighted model merging with parameter shift-based weights better preserves language capabilities than averaging, but still underperforms specialized models
- Omni-modality fine-tuning requires 3× more data than modality-specific fine-tuning while underperforming on multimodal benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual modality fine-tuning injects new parametric knowledge while degrading reasoning and instruction-following.
- Mechanism: Parameter shift magnitude correlates with training data scale—visual modalities (1.4T+ tokens) cause larger shifts that enrich knowledge representations but overwrite reasoning-optimized weights.
- Core assumption: Knowledge and reasoning capabilities occupy overlapping parameter regions, creating unavoidable trade-offs during fine-tuning.
- Evidence anchors:
  - "modality fine-tuning degrades reasoning and instruction-following while enhancing knowledge through visual modalities"
  - "Qwen2-VL-7B-Instruct...improves MMLU-Pro performance by approximately 5%...still exhibits substantial performance drops: 3.0% on GPQA, 10.2% on MATH"
- Break condition: If reasoning degradation scales non-linearly with model size (observed: 72B models show reduced degradation), larger models may absorb modality shifts without capability loss.

### Mechanism 2
- Claim: Weighted model merging preserves language capabilities by prioritizing parameters with larger modality-specific shifts.
- Mechanism: The paper computes Δ_avg = avg|θ_ori - θ_mft| for each parameter tensor, then applies softmax to create importance weights.
- Core assumption: Parameter shift magnitude indicates importance for modality-specific tasks, and interpolation between specialized models can preserve multi-modal capabilities.
- Evidence anchors:
  - "Weighted model merging better preserves language capabilities than standard averaging"
  - "Qwen2-VL-7B-Instruct, which undergoes the most extensive modality fine-tuning, exhibits the largest parameter shift...10 times larger than those of LLaV A-Video-7B-Qwen2"
- Break condition: If attention head analysis reveals modality-specific heads (not found: masking any head drops performance), fine-grained head-level merging may outperform parameter-level approaches.

### Mechanism 3
- Claim: Omni-modality fine-tuning is less data-efficient than sequential modality-specific fine-tuning.
- Mechanism: Joint training across modalities requires learning cross-modal representations simultaneously, increasing sample complexity.
- Core assumption: Modalities have distinct statistical properties that interfere during joint optimization, requiring more samples to achieve equilibrium.
- Evidence anchors:
  - "Omni-modality fine-tuning is less efficient than modality-specific fine-tuning, requiring more data for similar performance"
  - "LLaVA-Next requires only one-third of the training data used by NextGPT yet significantly outperforms it"
- Break condition: If progressive curriculum learning (adding modalities sequentially with warm-up) reduces interference, efficiency gap may narrow.

## Foundational Learning

- Concept: **Modality Extension Architecture**
  - Why needed here: Understanding that MLLMs = Base LLM + Modality Encoders + Modality Projectors is prerequisite to analyzing parameter shifts.
  - Quick check question: Can you explain why freezing only the LLM (vs. full fine-tuning) might preserve reasoning but limit modality integration?

- Concept: **Parameter Shift and Catastrophic Forgetting**
  - Why needed here: The trade-off between knowledge injection and reasoning degradation relies on understanding how fine-tuning overwrites existing weights.
  - Quick check question: Why would visual modality fine-tuning affect math reasoning (MATH benchmark) despite no mathematical visual content?

- Concept: **Model Merging (Weight Averaging)**
  - Why needed here: Section 5.3's weighted merging formula (θ_merge = α_0θ_0 + (1-α_0)Σα_iθ_i) requires understanding task-agnostic merging and why simple averaging fails.
  - Quick check question: Why does the paper use parameter shift magnitude (Δ_avg) rather than validation performance to determine merge weights?

## Architecture Onboarding

- Component map:
  Base LLM (Qwen2-7B-Instruct) → Modality Encoders (Image/Video/Audio) → Modality Projectors (align modalities to text embedding space) → Fine-tuned MLLM (e.g., Qwen2-VL-7B-Instruct) → [Merge Option] Combine multiple MLLMs → Weighted Merged Model

- Critical path:
  1. Start with strong base LLM (evaluated on MMLU, IFEval, GPQA, MATH, HumanEval+)
  2. Fine-tune on modality-specific data (image: 8.5M samples, video: 1.6M additional)
  3. Evaluate degradation on core abilities (expect 5-10% reasoning drop)
  4. If merging: compute Δ_avg for each MLLM, apply weighted merge with α_0 ≈ 0.2-0.4 for base LLM
  5. Re-evaluate on both textual (MMLU-Pro, IFEval) and multimodal (MMMU, Video-MME) benchmarks

- Design tradeoffs:
  - **Knowledge vs. Reasoning**: Visual fine-tuning gains +5% MMLU-Pro but costs -10% MATH
  - **Merge weight α_0**: Higher values preserve reasoning (GPQA -33% vs. -15%) but limit multimodal gains
  - **Omni-modal vs. Sequential**: Omni-modal requires 3× data but underperforms specialized models by 10-15%
  - **Model size**: 72B models show reduced reasoning degradation (-0.5% GPQA vs -3.0% for 7B)

- Failure signatures:
  - IFEval score drops >15%: instruction-following weights overwritten (seen in LLaVA-OneVision: 58.2%→28.6%)
  - HumanEval+ Pass@1 = 0%: coding capability collapse (LLaVA-OneVision-Qwen2-7B-SI)
  - MMLU improvement <1% with >1T tokens: modality not injecting knowledge (audio modality pattern)
  - Merge performs worse than individual models: α_0 too low or conflicting modality specializations

- First 3 experiments:
  1. **Baseline degradation test**: Fine-tune Qwen2-7B-Instruct on image-only data (8.5M samples), evaluate delta on MMLU-Pro (+5%), MATH (-10%), IFEval (-10%) to quantify trade-offs.
  2. **Weight sweep for merging**: Merge Qwen2-VL + LLaVA-Video with α_0 ∈ {0.1, 0.3, 0.5}, measure GPQA preservation and MMMU performance to find optimal balance point.
  3. **Data efficiency comparison**: Train omni-modal model on 4.5M mixed-modality data vs. sequential image→video fine-tuning (1.3M + 1M), compare VizWiz and MSVD-QA performance to validate 3× efficiency claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reasoning degradation during modality fine-tuning be mitigated while maintaining multimodal gains?
- Basis in paper: The authors identify that "future efforts to retain the base LLM's capabilities should focus on addressing reasoning degradation during modality fine-tuning" and note that reasoning consistently declines across all models tested.
- Why unresolved: Model merging partially mitigates but does not fully preserve reasoning; no mechanism is proposed to specifically protect reasoning parameters during modality extension.
- What evidence would resolve it: A fine-tuning method or architecture modification that maintains GPQA, MATH, and HumanEval performance within 1-2% of the base LLM while achieving comparable multimodal performance.

### Open Question 2
- Question: What training paradigms could make omni-modality fine-tuning as efficient and effective as modality-specific fine-tuning?
- Basis in paper: The paper states that "omni-modality fine-tuning proves inefficient compared to modality-specialized models, requiring more training data while offering limited improvements" and that it "remains an area requiring more refined design."
- Why unresolved: Current joint training across modalities underperforms compared to specialized models even with more data; the optimal data mixing ratios, curriculum strategies, or architectural changes remain unknown.
- What evidence would resolve it: An omni-modality training approach that achieves parity with specialized models (e.g., LLaVA-Next for images) on VQAv2 and MSVD-QA benchmarks using equal or less training data.

### Open Question 3
- Question: Why does small-step fine-tuning succeed for merged language models but fail for merged omni-modal models?
- Basis in paper: The authors note "although small-step fine-tuning works on merged language models, it fails in omni-modal models" without further investigation into the underlying cause.
- Why unresolved: The mechanism for why multi-modal merged models respond differently to small-step fine-tuning remains unexplored; it is unclear whether this is due to parameter interference, modality competition, or architectural factors.
- What evidence would resolve it: Ablation studies identifying which components (attention heads, projector weights, LLM layers) cause small-step fine-tuning to degrade performance in omni-modal merged models, paired with a modified approach that achieves gains similar to those seen in language-only merged models.

## Limitations

- The study's results are constrained by specific base models and fine-tuning protocols that may not generalize to other LLM architectures
- The 1.4T token fine-tuning scale may not capture effects at larger scales where reasoning degradation appears reduced
- The weighted merging approach relies on parameter shift magnitude as a proxy for task importance without validating this assumption through ablation studies

## Confidence

*High Confidence*
- Visual fine-tuning degrades reasoning capabilities (MATH -10.2%, HumanEval+ -16.6%)
- Weighted merging preserves language capabilities better than averaging
- Omni-modal fine-tuning requires more data than sequential modality-specific tuning

*Medium Confidence*
- Parameter shift magnitude accurately reflects task importance for merging weights
- 3× data efficiency gap between omni-modal and sequential approaches
- Trade-offs are unavoidable rather than improvable with better optimization

## Next Checks

1. Conduct head-level analysis to determine if modality-specific attention heads exist, potentially enabling more precise merging than parameter-level approaches
2. Test progressive curriculum learning where modalities are added sequentially with warm-up periods to reduce interference and improve omni-modal efficiency
3. Scale experiments to 70B+ parameter models to verify if reasoning degradation diminishes with model size as suggested by reduced degradation patterns