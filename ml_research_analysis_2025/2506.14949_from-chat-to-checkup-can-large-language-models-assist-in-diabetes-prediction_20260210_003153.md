---
ver: rpa2
title: 'From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?'
arxiv_id: '2506.14949'
source_url: https://arxiv.org/abs/2506.14949
tags:
- llms
- diabetes
- accuracy
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) for diabetes prediction using structured numerical data from the Pima Indian
  Diabetes Database (PIDD). Six LLMs, including four open-source and two proprietary
  models, were tested using zero-shot, one-shot, and three-shot prompting strategies
  and compared against traditional ML models.
---

# From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?
arXiv ID: 2506.14949
Source URL: https://arxiv.org/abs/2506.14949
Reference count: 26
Primary result: Proprietary LLMs (GPT-4o, Gemma-2-27B) achieved highest accuracy (74.35%) but underperformed traditional ML (75.97%) on diabetes prediction from structured data.

## Quick Facts
- Evaluates LLMs for diabetes prediction using structured numerical data from Pima Indian Diabetes Database
- Tests six LLMs (4 open-source, 2 proprietary) using zero-shot, one-shot, and three-shot prompting
- Compares against traditional ML models (Random Forest, Logistic Regression, SVM)
- Proprietary models outperform open-source; Gemma-2-27B achieves highest accuracy at 74.35%
- LLM performance remains below traditional ML methods despite no task-specific training

## Executive Summary
This study evaluates whether large language models can effectively predict diabetes using structured numerical data from the Pima Indian Diabetes Database without task-specific training. The researchers tested six LLMs using various prompting strategies and compared their performance against traditional machine learning models. While proprietary models like GPT-4o and Gemma-2-27B achieved the highest accuracy (74.35%), they still underperformed traditional ML methods (best: Random Forest at 75.97%), highlighting both the potential and current limitations of LLMs for medical classification tasks.

## Method Summary
The study used the Pima Indian Diabetes Database (768 instances, 8 numeric features + 1 binary target) to evaluate six LLMs using zero-shot, one-shot, and three-shot prompting strategies. For ML baselines, data was split 80/20 with z-score normalization. LLMs received tabular data converted to natural language prompts requiring binary output. Proprietary models (GPT-4o, Gemini Flash 2.0) were accessed via API, while open-source models (Gemma-2-27B, Mistral-7B, Llama-3.1-8B, Llama-3.2-2B) ran through Ollama. Performance was measured using accuracy, precision, recall, and F1-score, with LLM metrics averaged over three independent runs with shuffled test sets.

## Key Results
- Proprietary LLMs (GPT-4o, Gemini Flash 2.0) outperformed open-source models across all prompting strategies
- Gemma-2-27B achieved highest accuracy at 74.35% in three-shot setting, showing 10.2% improvement over one-shot
- Traditional ML methods (Random Forest at 75.97%) still outperformed all LLMs despite requiring task-specific training
- Gemini Flash 2.0 achieved 0.73 accuracy but only 0.47 F1-score, indicating majority class bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting improves LLM classification performance on structured medical data through in-context learning
- Mechanism: Providing labeled examples in the prompt creates a local demonstration distribution, allowing the model to infer the classification boundary without weight updates
- Core assumption: Pre-trained representations contain transferable patterns that can be activated by semantically similar demonstrations
- Evidence anchors:
  - Performance varied across prompting strategies
  - Three-shot prompting consistently led to performance improvements
  - Few-shot in-context learning remains underexplored for structured clinical data

### Mechanism 2
- Claim: Larger parameter count and proprietary training data correlate with better structured-data classification
- Mechanism: Models leverage scale and broader pre-training corpora to encode numerical relationships and medical priors
- Core assumption: Scale and data diversity translate into stronger implicit representations of tabular reasoning
- Evidence anchors:
  - Proprietary LLMs perform better than open-source ones
  - Llama-3.2-2B (smallest model) achieved only 0.35-0.37 accuracy across all prompting strategies
  - No direct external validation of scale effects on PIDD

### Mechanism 3
- Claim: Class imbalance in the target dataset can cause high-accuracy, low-F1 behavior
- Mechanism: Models systematically overpredict the majority class to maximize raw accuracy
- Core assumption: Models optimize for prompt-specified objectives without explicit calibration for class balance
- Evidence anchors:
  - Gemini Flash 2.0's high accuracy but low F1-score reflects tendency to overpredict majority class
  - Dataset is approximately 65.1% non-diabetic and 34.9% diabetic
  - Corpus papers do not analyze LLM calibration under class imbalance

## Foundational Learning

- Concept: **In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The entire experimental design relies on zero/one/three-shot prompting; understanding how demonstrations shape model behavior is critical for interpreting results
  - Quick check question: If you provide a model with 3 labeled examples where 2 are class 0 and 1 is class 1, what implicit class prior might the model infer?

- Concept: **Classification Metrics for Imbalanced Data (F1 vs Accuracy)**
  - Why needed here: The paper highlights cases where accuracy and F1 diverge; selecting the wrong metric can mask systematic failure modes
  - Quick check question: A model predicts "non-diabetic" for all 768 PIDD samples. What is its accuracy? What is its F1-score for the diabetic class?

- Concept: **Structured Data Serialization for LLMs**
  - Why needed here: LLMs accept text, not dataframes; prompt format directly affects model performance
  - Quick check question: How might reordering features or changing descriptors affect model predictions?

## Architecture Onboarding

- Component map: PIDD -> Serialization Layer -> Prompt Layer -> Model Layer -> Evaluation Layer
- Critical path:
  1. Load PIDD and serialize each test row into a natural language prompt
  2. For few-shot settings, prepend 1 or 3 labeled examples from training distribution
  3. Call LLM inference API; parse binary response (0 or 1)
  4. Compute metrics; repeat 3× with independently shuffled test sets; report mean

- Design tradeoffs:
  - Prompt vs Fine-tuning: Prompting requires no training data but underperforms specialized models (74.35% vs 98% in prior DL work)
  - Proprietary vs Open-Source: Proprietary models outperform open-source but introduce cost, latency, and reproducibility concerns
  - Accuracy vs F1: High accuracy with low F1 indicates majority-class bias; F1 is more informative for clinical utility

- Failure signatures:
  - High accuracy, low F1 → Model overpredicting majority class
  - No improvement from few-shot → Demonstrations may be out-of-distribution or model lacks in-context capacity
  - Non-binary outputs → Prompt parsing failure

- First 3 experiments:
  1. Test whether reordering features or adding unit context changes Gemma-2-27B three-shot accuracy
  2. Extend from three-shot to five/ten-shot on Gemma-2-27B; check if F1 continues improving or plateaus
  3. Explicitly instruct models to consider class balance or provide equal-class demonstrations; measure impact on Gemini Flash 2.0's F1-score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid approaches that integrate Large Language Models with traditional classifiers outperform standalone models in diabetes prediction?
- Basis in paper: The "Future Works" section explicitly encourages investigating "hybrid approaches that combine LLMs with conventional classifiers" to bridge the performance gap with traditional machine learning
- Why unresolved: The current study only evaluated LLMs and traditional ML models independently; it did not test integrated or ensemble architectures
- What evidence would resolve it: Empirical results from a hybrid model showing superior accuracy or F1-scores compared to standalone Random Forest (75.97%) or Gemma-2-27B (74.35%) baselines

### Open Question 2
- Question: Does domain-specific fine-tuning significantly close the performance gap between LLMs and specialized Deep Learning models on structured medical data?
- Basis in paper: The authors state in "Future Works" that research should "explore fine-tuning LLMs on structured datasets" to enhance applicability, noting that current prompt-based performance (approx. 74%) lags behind literature benchmarks for Deep Learning (approx. 98%)
- Why unresolved: This study relied exclusively on zero-shot and few-shot prompting without updating model weights
- What evidence would resolve it: A comparative study showing that fine-tuned versions of Llama or Gemma on PIDD can match or exceed the 90%+ accuracy reported in prior Deep Learning literature

### Open Question 3
- Question: Which prompt engineering strategies are required to stabilize LLM performance across different model architectures in medical classification?
- Basis in paper: The abstract notes "performance variation across prompting strategies" and explicitly encourages "future work on prompt engineering"
- Why unresolved: Results demonstrated that moving from one-shot to three-shot prompting improved Gemma-2-27B but failed to yield consistent gains for Llama or Mistral models
- What evidence would resolve it: Identification of specific prompting techniques that yield consistent, positive marginal gains across all tested open-source models

## Limitations

- Experimental design leaves several critical parameters unspecified, particularly exact few-shot examples used and test set sampling strategy
- Class imbalance effects on LLM calibration are demonstrated but not systematically addressed through model-specific calibration techniques
- Comparison between proprietary and open-source models conflates model architecture differences with training data quality and scale

## Confidence

- **High Confidence**: The observed relationship between few-shot prompting and performance improvement is well-supported by experimental results and aligns with established in-context learning literature
- **Medium Confidence**: The superiority of proprietary models over open-source ones is demonstrated within this study's specific context but requires external validation
- **Low Confidence**: The interpretation of Gemini Flash 2.0's high accuracy but low F1-score as evidence of majority class bias is plausible but not definitively proven

## Next Checks

1. **Prompt Format Ablation**: Systematically vary feature ordering and descriptor phrasing in prompts to determine whether observed performance differences are robust to prompt format changes
2. **Shot Count Scaling**: Extend few-shot experiments beyond three examples to five and ten shots for Gemma-2-27B to determine whether performance plateaus or continues improving
3. **Class-Balanced Prompting**: Explicitly instruct models to consider class balance or provide equal-class demonstrations in few-shot settings to test whether this mitigates the observed high-accuracy/low-F1 phenomenon