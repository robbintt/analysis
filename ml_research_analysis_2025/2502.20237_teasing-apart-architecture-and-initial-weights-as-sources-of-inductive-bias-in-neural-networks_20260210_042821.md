---
ver: rpa2
title: Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias
  in Neural Networks
arxiv_id: '2502.20237'
source_url: https://arxiv.org/abs/2502.20237
tags:
- learning
- data
- neural
- image
- inductive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the relative contributions of neural network
  architecture and initial weights as sources of inductive bias in meta-learning.
  The authors conduct a large-scale experiment training 430 models across four architectures
  (MLP, CNN, LSTM, Transformer) on three tasks: concept learning, modular arithmetic,
  and few-shot Omniglot classification.'
---

# Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks

## Quick Facts
- **arXiv ID**: 2502.20237
- **Source URL**: https://arxiv.org/abs/2502.20237
- **Reference count**: 7
- **Primary result**: Meta-learning can substantially reduce or eliminate performance differences across architectures and data representations for in-distribution tasks

## Executive Summary
This study investigates whether neural network architecture or initial weights serve as the primary source of inductive bias in meta-learning. Through a large-scale experiment training 430 models across four architectures (MLP, CNN, LSTM, Transformer) on three distinct tasks, the authors demonstrate that meta-learned initial weights can override architectural biases for in-distribution tasks. While randomly initialized models show significant performance variation across architectures, meta-learned models achieve similar high accuracy regardless of architecture choice. However, when faced with out-of-distribution tasks like modular arithmetic with unseen moduli, all architectures fail catastrophically regardless of their inductive bias.

## Method Summary
The authors conduct a comprehensive experiment training 430 models across four architectures (MLP, CNN, LSTM, Transformer) on three tasks: concept learning, modular arithmetic, and few-shot Omniglot classification. They systematically compare performance between meta-learned initial weights and randomly initialized weights. The concept learning task involves learning Boolean functions from positive and negative examples, modular arithmetic tests generalization to unseen moduli, and Omniglot provides few-shot image classification. Performance is measured across different data representations to isolate the effects of architectural versus weight-based inductive bias.

## Key Results
- Meta-learned models achieve similar high accuracy (0.82-0.96) across all architectures in concept learning, while randomly initialized models show much greater variation (0.50-0.71)
- Architectures performing well without meta-learning tend to meta-train more effectively
- All architectures fail catastrophically on out-of-distribution modular arithmetic tasks regardless of inductive bias
- Meta-learning substantially reduces or eliminates performance differences across architectures and data representations for in-distribution tasks

## Why This Works (Mechanism)
Meta-learning works by exposing models to multiple related tasks during training, allowing them to learn task-agnostic patterns and initialization parameters that facilitate rapid adaptation. The meta-learned initial weights encode task structure and representation preferences that can compensate for architectural limitations. This mechanism enables models to learn effective representations and update rules that are more important than the specific architectural choices for in-distribution tasks. The meta-training process effectively creates a shared initialization that works across different architectures by focusing on task-relevant features rather than architecture-specific biases.

## Foundational Learning
- **Inductive bias**: The set of assumptions that allows a learning algorithm to prioritize one hypothesis over another. Why needed: Essential for understanding how models generalize beyond training data. Quick check: Can be tested by examining performance on out-of-distribution examples.
- **Meta-learning**: Learning to learn, where models acquire the ability to quickly adapt to new tasks. Why needed: Central to understanding how initial weights can encode task structure. Quick check: Measured by few-shot learning performance on novel tasks.
- **Weight initialization**: The starting point for optimization that can significantly impact convergence and final performance. Why needed: Key variable being manipulated to separate from architectural effects. Quick check: Compare training curves from different initializations.
- **Architectural bias**: The inherent preferences and limitations built into a neural network's structure. Why needed: The primary factor being compared against weight-based bias. Quick check: Analyze feature representations across different architectures.
- **Distribution shift**: Changes in data distribution between training and test conditions. Why needed: Critical for understanding generalization limits. Quick check: Evaluate performance when test data differs from training distribution.

## Architecture Onboarding

**Component Map**: Data -> Model (Architecture + Initial Weights) -> Loss -> Gradient Updates -> Performance

**Critical Path**: Architecture Selection -> Weight Initialization -> Task Exposure -> Performance Evaluation -> Generalization Testing

**Design Tradeoffs**: 
- Architecture complexity vs. generalization capability
- Task diversity vs. meta-learning efficiency
- In-distribution performance vs. out-of-distribution robustness
- Computational cost vs. experimental scope

**Failure Signatures**:
- Large performance gaps between meta-learned and random initializations
- Architecture-specific failure modes persisting after meta-learning
- Catastrophic performance drops on out-of-distribution tasks
- Inconsistent generalization across different data representations

**First Experiments**:
1. Compare single-task learning vs. meta-learning performance across architectures
2. Test transfer learning from meta-trained weights to novel but related tasks
3. Evaluate the impact of task distribution diversity on meta-learning effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize beyond the three studied tasks (concept learning, modular arithmetic, few-shot classification)
- Modular arithmetic task failure may be task-specific rather than revealing fundamental limitations
- Meta-learning setup assumes access to many similar tasks, which may not reflect practical constraints
- Focus on supervised learning tasks without exploring reinforcement learning or unsupervised settings

## Confidence
- **High**: Meta-learning reduces in-distribution performance gaps across architectures (supported by consistent results across multiple architectures and tasks)
- **Medium**: Meta-learning can override architectural biases (depends on task distribution assumptions)
- **Low**: Conclusions about out-of-distribution robustness (based on narrow modular arithmetic task slice)

## Next Checks
1. Test the same methodology on reinforcement learning tasks where exploration and temporal credit assignment might amplify architectural differences
2. Evaluate performance when meta-training tasks have heterogeneous distributions to test robustness to task diversity
3. Investigate whether observed meta-learning benefits persist when reducing the number of meta-training tasks to reflect more realistic sample efficiency constraints