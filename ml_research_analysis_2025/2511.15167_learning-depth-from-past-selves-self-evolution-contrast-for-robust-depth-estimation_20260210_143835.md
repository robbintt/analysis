---
ver: rpa2
title: 'Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth
  Estimation'
arxiv_id: '2511.15167'
source_url: https://arxiv.org/abs/2511.15167
tags:
- depth
- contrastive
- estimation
- learning
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of robust depth estimation under
  adverse weather conditions in self-supervised monocular depth estimation. It proposes
  a novel self-evolution contrastive learning framework called SEC-Depth that leverages
  intermediate training parameters (latency models) to construct a self-evolution
  contrastive scheme.
---

# Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation

## Quick Facts
- arXiv ID: 2511.15167
- Source URL: https://arxiv.org/abs/2511.15167
- Reference count: 12
- Key outcome: Achieves up to 13.33% decrease in AbsRel errors compared to MonoViT baseline on adverse weather conditions

## Executive Summary
This paper addresses the challenge of robust depth estimation under adverse weather conditions in self-supervised monocular depth estimation. The authors propose SEC-Depth, a self-evolution contrastive learning framework that leverages historical model parameters (latency models) as negative samples to improve robustness. The method introduces a dynamic update strategy for these latency models and a self-evolution contrastive loss that treats outputs from historical latency models as negative samples, adaptively adjusting learning objectives while implicitly sensing weather degradation severity.

## Method Summary
SEC-Depth builds upon standard self-supervised monocular depth estimation by introducing a self-evolution contrastive learning framework. The method maintains a queue of past model states (latency models) updated via exponential moving average (EMA). During training, it constructs a triplet where the current model's prediction on a clean image serves as the anchor, its prediction on an augmented (adverse) image serves as the positive, and the latency model's prediction on the augmented image serves as the negative. The loss explicitly pushes the current representation away from the historical one while maintaining standard photometric consistency. The method also discretizes depth maps into probability distributions over fixed intervals using a Gaussian kernel to prevent overfitting to local pixel noise or weather artifacts.

## Key Results
- Achieves up to 13.33% decrease in AbsRel errors compared to MonoViT baseline
- Outperforms state-of-the-art methods on multiple benchmark datasets
- Demonstrates strong zero-shot generalization capabilities across six different adverse weather conditions
- Integrates seamlessly with diverse baseline models without requiring architectural modifications or dataset priors

## Why This Works (Mechanism)

### Mechanism 1
Using historical model parameters (latency models) as negative samples improves robustness to adverse weather by forcing the current model to diverge from suboptimal states inherent in the training trajectory. The method maintains a queue of past model states updated via EMA. During training, it constructs a triplet where the current model's prediction on a clean image is the anchor, its prediction on an augmented (adverse) image is the positive, and the latency model's prediction on the augmented image is the negative. The loss explicitly pushes the current representation away from the historical one.

### Mechanism 2
Discretizing depth maps into probability distributions (Interval-Based Depth) prevents the contrastive loss from overfitting to local pixel noise or weather artifacts. Instead of minimizing pixel-wise depth differences, the method converts depth maps into probability distributions over fixed intervals (bins) using a Gaussian kernel. It minimizes the Jensen-Shannon (JS) divergence between these distributions.

### Mechanism 3
Dynamic queue updating based on sample variance ensures the contrastive learning signal remains informative and prevents "collapse" where negatives become too similar to positives. The queue of latency models is not updated on a fixed schedule alone but is also updated proactively if the variance of depth differences between the anchor and negative samples falls below the variance between anchor and positive samples.

## Foundational Learning

- **Concept: Self-Supervised Photometric Consistency**
  - **Why needed here:** This is the base training objective. Without it, the depth network has no signal to learn geometry.
  - **Quick check question:** Can you explain how a warped image $\tilde{I}'$ is synthesized from depth $D$ and why SSIM is often preferred over simple L1 loss for this?

- **Concept: Contrastive Learning (Triplet Loss)**
  - **Why needed here:** SEC-Depth frames robustness as a comparison problem.
  - **Quick check question:** What happens to the embedding space if the negative sample is actually *too* hard (i.e., indistinguishable from the anchor)?

- **Concept: Exponential Moving Average (EMA)**
  - **Why needed here:** The "latency models" are not frozen snapshots but EMA-smoothed versions of the training weights.
  - **Quick check question:** How does a high momentum value ($\omega=0.99$ vs $\omega=0.9$) affect the latency of the negative model in tracking the current model's improvements?

## Architecture Onboarding

- **Component map:** DepthNet ($F_t$) -> DepthNet (Latency Models) -> Bin Distribution Layer
- **Critical path:**
  1. Standard Pass: Input clean image $I$ → DepthNet → $D_P$ (Positive)
  2. Augmented Pass: Input augmented image $I_{aug}$ → DepthNet → $D_A$ (Anchor)
  3. Negative Pass: Input $I_{aug}$ → Latency Models → $\{D_{N1}...D_{Nk}\}$ (Negatives)
  4. Loss Fusion: Compute Photometric Loss + SECL

- **Design tradeoffs:**
  - Queue Size ($j$): Paper uses $j=3$. Larger queues offer more diverse negatives but increase memory/compute.
  - Bin Count ($N$): Paper uses $N=32$. Higher $N$ captures fine depth detail but increases distribution comparison cost.
  - Update Step ($S$): Paper uses $S=5$. Frequent updates ($S=1$) maximize robustness training but significantly extend training time.

- **Failure signatures:**
  - Loss Plateau: If SECL drops to near zero too quickly, check if the latency models have collapsed (are identical to the current model).
  - NaN/Instability: If margin $\alpha_2$ is too large or the weight $w$ scales up too fast, the contrastive pressure may destabilize the photometric convergence.

- **First 3 experiments:**
  1. Baseline Integration: Train MonoViT on KITTI without SEC-Depth, then with SEC-Depth. Compare AbsRel on WeatherKITTI validation set.
  2. Ablation on Negative Step ($S$): Run training with $S=1, 5, 10$. Plot training time vs. AbsRel to find efficiency sweet spot.
  3. Visual Distribution Check: Visualize the "Binning Layer" output. Does the probability distribution $P$ for a rainy image match the clean image better after training?

## Open Questions the Paper Calls Out

### Open Question 1
Can the latency model framework be extended to multi-frame or temporal self-supervised depth estimation methods? The introduction acknowledges video sequences as a source for self-supervision, but the method is validated exclusively on monocular baselines (MonoViT, PlaneDepth).

### Open Question 2
Is the exponential decay strategy for the margin parameter ($\alpha_1$) robust to variations in optimizer schedules and convergence speeds? The ablation study notes a "significant decline in performance" when using a linear decay, suggesting the method is sensitive to the alignment between margin decay and model convergence.

### Open Question 3
Can the framework learn robust representations using only real-world adverse weather data, rather than synthetic augmentations? The training process relies on synthetic weather augmentation ($I_{aug}$) derived from clear images to generate negative samples.

## Limitations
- Relies heavily on synthetic adverse weather data and controlled evaluation settings
- Performance gains on real-world adverse weather data remain to be validated
- Core assumptions about using historical model states as effective negative samples lack extensive ablation studies on alternative designs

## Confidence

- **High confidence**: The improvement in quantitative metrics (AbsRel, RMSE) on WeatherKITTI and generalization to six adverse weather conditions is well-documented and reproducible.
- **Medium confidence**: The effectiveness of the self-evolution contrastive mechanism relies on specific hyperparameter choices that may not generalize across different datasets or baseline architectures without tuning.
- **Low confidence**: The paper does not provide extensive analysis of failure cases or performance degradation in scenarios with complex depth distributions or extreme weather conditions beyond the tested augmentations.

## Next Checks

1. **Real-world Adverse Weather Validation**: Evaluate SEC-Depth on datasets with naturally occurring adverse weather (e.g., Foggy Zurich, RainCityscapes) to verify that the zero-shot generalization claims hold beyond synthetic augmentation.

2. **Ablation on Historical Model Design**: Systematically vary the number of latency models (1, 3, 5, 10), EMA momentum values, and update frequencies to determine the sensitivity of the method to these design choices and identify optimal configurations.

3. **Failure Mode Analysis**: Intentionally test SEC-Depth on scenes with extreme depth complexity (e.g., crowded urban scenes, scenes with many reflective surfaces) to characterize where the method fails and whether the binning strategy becomes a bottleneck.