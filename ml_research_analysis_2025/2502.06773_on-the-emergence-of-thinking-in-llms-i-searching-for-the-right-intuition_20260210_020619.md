---
ver: rpa2
title: 'On the Emergence of Thinking in LLMs I: Searching for the Right Intuition'
arxiv_id: '2502.06773'
source_url: https://arxiv.org/abs/2502.06773
tags:
- step
- reasoning
- arxiv
- reward
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a post-training framework called Reinforcement
  Learning via Self-Play (RLSP) to enable search and reasoning behaviors in LLMs.
  The core idea is to decouple exploration and correctness signals during reinforcement
  learning training, encouraging diverse reasoning behaviors while ensuring correctness.
---

# On the Emergence of Thinking in LLMs I: Searching for the Right Intuition

## Quick Facts
- arXiv ID: 2502.06773
- Source URL: https://arxiv.org/abs/2502.06773
- Reference count: 40
- Primary result: RLSP framework boosts Llama-3.1-8B-Instruct accuracy by 23% on MATH-500 and Qwen2.5-32B-Instruct by 10% on AIME 2024

## Executive Summary
This paper introduces Reinforcement Learning via Self-Play (RLSP), a post-training framework designed to enable emergent reasoning behaviors in large language models. The core innovation is decoupling exploration and correctness signals during reinforcement learning, encouraging diverse reasoning behaviors while ensuring accuracy. Through empirical evaluation, RLSP demonstrates significant performance gains on mathematical reasoning tasks and reveals emergent behaviors like backtracking and self-verification, even with simple length-based exploration rewards.

## Method Summary
RLSP operates through three stages: supervised fine-tuning on reasoning demonstrations, reinforcement learning training with a dual reward combining outcome verification and exploration incentives, and iterative refinement. The exploration reward encourages longer reasoning traces, which the paper shows triggers emergent behaviors like backtracking and self-correction. The framework is evaluated on Llama-3.1-8B-Instruct and Qwen2.5-32B-Instruct models using MATH-500 and AIME 2024 benchmarks, with implementation details provided through the OpenRLHF framework.

## Key Results
- Llama-3.1-8B-Instruct accuracy improves from 50.6% to 73.4% on MATH-500 with RLSP
- Qwen2.5-32B-Instruct achieves 83.2% on AIME 2024, a 10% improvement over base model
- Emergent behaviors (backtracking, verification, self-correction) appear even with simple length-based exploration rewards
- Performance gains are consistent across different model families and sizes, suggesting scalability

## Why This Works (Mechanism)

### Mechanism 1
Simple length-based exploration rewards can trigger emergent reasoning behaviors (backtracking, verification, self-correction) even without explicit supervision for these behaviors. The length reward counteracts the PPO KL penalty that naturally penalizes longer responses, incentivizing extended CoT trajectories that implicitly contain search behaviors, which are then reinforced when they lead to correct outcomes.

### Mechanism 2
Decoupling exploration and correctness signals with weighted combination (α=0.8) prevents reward hacking while maintaining learning signal. The exploration reward provides dense early training signal when outcome rewards are sparse, while the α-weighting ensures the correctness signal dominates, pushing the model toward valid solutions.

### Mechanism 3
Longer CoT chains increase computational expressiveness, and RLSP exploits this by incentivizing trajectory exploration over rationales. The framework trains models to search over "trajectories over rationales," treating reasoning as meta-search rather than single-path inference.

## Foundational Learning

- **Proximal Policy Optimization (PPO) with KL Penalty**: Understanding why the KL penalty naturally discourages long reasoning chains is critical for grasping why the exploration reward is necessary.
  - Quick check: Why would a KL penalty term naturally discourage the model from generating long reasoning chains?

- **Exploration vs. Exploitation in Reward Shaping**: The dual-reward structure separates exploration (dense, process-focused) from exploitation (sparse, outcome-focused), fundamental to understanding why α-weighting matters.
  - Quick check: What happens if α=1.0 (pure outcome reward) vs. α=0.5 (equal weighting)?

- **Emergent Behavior in LLMs**: Understanding what emergence means (and what it doesn't) is essential for interpreting results.
  - Quick check: How does the paper define "emergent," and what evidence would contradict this claim?

## Architecture Onboarding

- **Component map**: Base Model → [Optional: SFT on reasoning traces] → PPO Training Loop → Reward Function (Outcome Verifier + Exploration Reward)

- **Critical path**: Outcome verifier must be reliable, α-weighting must be tuned, exploration reward must be designed to resist gaming

- **Design tradeoffs**: Length vs. judge-based exploration reward, SFT vs. pure RL, model family dependency on pure RL effectiveness

- **Failure signatures**: Response length increases without accuracy gains, no emergent behaviors despite length increase, performance degrades on simpler problems

- **First 3 experiments**:
  1. Baseline ablation: Train Llama-3.1-8B-Instruct with pure outcome reward (α=1.0) on MATH-500 subset to verify no emergent behaviors appear
  2. Exploration reward comparison: Compare length-only vs. judge-based exploration rewards on fixed 500-problem subset, measuring both accuracy and qualitative emergence
  3. Cross-domain transfer: Train on math problems and evaluate on held-out coding problems to test whether emergent search behaviors are general reasoning patterns or domain-specific heuristics

## Open Questions the Paper Calls Out

- Can pure RL with only outcome rewards induce thinking behaviors at large scales, or are exploration rewards strictly necessary?
- Can RLSP lead to reasoning strategies that surpass human heuristics or exhibit "Move 37"-style unexpected novelty?
- How does context length specifically influence the depth and efficacy of reasoning in RLSP-trained models?

## Limitations

- Emergence of search behaviors depends heavily on pretraining data composition, which is not characterized
- Length-based exploration reward may not generalize to domains where CoT is less natural
- Outcome verifier's robustness to answer format variations is not discussed
- The claim that emergent behaviors are due to simple rewards rather than pretraining patterns lacks direct evidence

## Confidence

- High confidence: RLSP improves accuracy on math problems when outcome verification is reliable (MATH-500 +23%, AIME +10%)
- Medium confidence: Length-based exploration rewards trigger emergent reasoning behaviors across model families, but the exact mechanism (pretraining vs. RL dynamics) is unclear
- Low confidence: The theoretical claim about CoT length and computational expressiveness directly applies to RLSP dynamics

## Next Checks

1. Ablation study: Train Llama-3.1-8B-Instruct with pure outcome reward (α=1.0) on MATH-500 subset to verify no emergent behaviors appear
2. Exploration reward comparison: Compare length-only vs. judge-based exploration rewards on fixed 500-problem subset, measuring both accuracy and qualitative emergence through manual sample inspection
3. Cross-domain transfer: Train on math problems and evaluate on held-out coding problems to test whether emergent search behaviors are general reasoning patterns or domain-specific heuristics