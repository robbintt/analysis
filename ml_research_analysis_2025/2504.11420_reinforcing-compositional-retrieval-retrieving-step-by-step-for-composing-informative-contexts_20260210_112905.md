---
ver: rpa2
title: 'Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing
  Informative Contexts'
arxiv_id: '2504.11420'
source_url: https://arxiv.org/abs/2504.11420
tags:
- retrieval
- examples
- training
- find
- program
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses compositional retrieval in retrieval-augmented
  LLM frameworks, where multiple interdependent examples must be selected to optimize
  collective utility rather than treating each candidate independently. The proposed
  method formulates this as a Markov Decision Process with a tri-encoder architecture
  that models retrieval as a sequence of conditional probabilities, allowing each
  selection to condition on previously retrieved examples.
---

# Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts

## Quick Facts
- **arXiv ID**: 2504.11420
- **Source URL**: https://arxiv.org/abs/2504.11420
- **Reference count**: 31
- **Primary result**: Sequential retriever with tri-encoder architecture and reinforcement learning improves compositional generalization accuracy on semantic parsing tasks by explicitly modeling inter-example dependencies.

## Executive Summary
This paper addresses compositional retrieval in retrieval-augmented LLM frameworks, where multiple interdependent examples must be selected to optimize collective utility rather than treating each candidate independently. The proposed method formulates this as a Markov Decision Process with a tri-encoder architecture that models retrieval as a sequence of conditional probabilities, allowing each selection to condition on previously retrieved examples. The retriever is trained in two stages: efficient supervised fine-tuning using coverage-based data construction, followed by reinforcement learning with a reward grounded in structural correspondence of generated programs. Experiments on compositional generalization semantic parsing tasks show consistent improvements over both top-k and sequential retrieval baselines, demonstrating the effectiveness of explicitly modeling inter-example dependencies and the importance of task-specific reward signals in refining retrieval policies.

## Method Summary
The method formulates compositional retrieval as a Markov Decision Process where each retrieval step conditions on previously selected examples. A tri-encoder architecture separates query, context, and candidate encoding to preserve signal strength while accumulating context representations. Training proceeds in two stages: supervised fine-tuning constructs coverage-maximizing sequences using greedy selection of candidates that maximize local structure coverage from program parse trees, trained with InfoNCE loss; reinforcement learning then refines the policy using grouped advantage optimization (GRPO) with a reward based on Jaccard similarity of structural program representations. The approach scales linearly with retrieval steps while maintaining computational efficiency.

## Key Results
- RCR with RL outperforms sequential retrieval baselines by 3-5% accuracy on compositional generalization splits
- Tri-encoder architecture provides 2-3% gains over single concatenated encoder variants
- Coverage-based SFT initialization significantly accelerates RL convergence compared to random initialization
- Structural reward formulation produces cleaner gradients than binary correctness rewards during RL training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing retrieval probability into sequential conditional probabilities enables modeling inter-example dependencies that top-k methods cannot capture.
- **Mechanism**: Rather than computing P(Z|x) ≈ ∏P(zi|x) independently, the method computes P(Z|x) = P(z1|x) · ∏P(zi|x, z1,...,zi-1). Each selection step conditions on the partial retrieval sequence, allowing the retriever to prefer candidates that complement rather than duplicate already-selected examples.
- **Core assumption**: The utility of a retrieved set is non-decomposable—collective utility depends on compositional coverage, not just individual relevance.
- **Evidence anchors**: [abstract] "decomposing the probability of retrieving a set of elements into a sequence of conditional probabilities and allowing each retrieval step to be conditioned on previously selected examples"; [section 2.1] Eq. 3 formalizes the conditional probability decomposition.
- **Break condition**: If retrieved elements are truly independent (no compositional structure needed), sequential conditioning adds complexity without benefit.

### Mechanism 2
- **Claim**: The tri-encoder architecture preserves query signal strength while accumulating context representations across retrieval steps.
- **Mechanism**: Three separate encoders—query encoder Eq, context encoder Ez, and candidate encoder Ec—allow computing selection logits as Ec(cj)⊤(Eq(x) + λ∑Ez(zi)). The weighted sum accumulates context without concatenating text, which would dilute the original query signal and promote repetitive selection.
- **Core assumption**: Directly encoding concatenated [query, retrieved_examples] weakens the query representation and causes the retriever to select examples similar to already-retrieved ones rather than complementary ones.
- **Evidence anchors**: [section 2.2] "we avoid using a single encoder to process the concatenated input [x, z1, · · · , zt−1], which may weaken the signal input x"; [table 1] "se2+ tri-encoder" outperforms baseline "se2" across splits, isolating architecture contribution.
- **Break condition**: If query and context can be effectively fused through cross-attention without signal loss, the tri-encoder's separate encoding becomes unnecessary overhead.

### Mechanism 3
- **Claim**: Two-stage training (coverage-based SFT → GRPO with structural reward) initializes a strong policy then aligns it to downstream LLM preferences.
- **Mechanism**: SFT constructs training sequences by greedily selecting candidates maximizing local structure coverage (Algorithm 1), creating interpretable supervision without LLM scoring. RL then refines using GRPO with reward = Jaccard similarity between generated and gold program local structures, directly optimizing for the LLM's compositional needs.
- **Core assumption**: Coverage-based SFT provides a sufficient initialization that RL can refine; the structural reward correlates with downstream task success.
- **Evidence anchors**: [section 3.1] Algorithm 1 details greedy coverage maximization for SFT data; [section 3.2] Eq. 6 defines the structural reward; Eq. 8-9 define GRPO objective.
- **Break condition**: If structural coverage does not correlate with downstream correctness for the target task, the reward signal becomes noisy and RL may degrade the SFT policy.

## Foundational Learning

- **Markov Decision Processes (MDPs)**
  - Why needed here: The paper formulates retrieval as an MDP with states (query + partial sequence), actions (candidate selection), and policies (tri-encoder distribution). Understanding state transitions and policy gradients is essential.
  - Quick check question: Given state st = [x, z1, z2] and action selecting z3, what is the next state?

- **Contrastive Learning / InfoNCE Loss**
  - Why needed here: SFT trains the tri-encoder using InfoNCE loss (Eq. 7) with in-batch and hard negatives. Understanding negative sampling strategies is critical for effective training.
  - Quick check question: In InfoNCE loss, what happens if all negatives are easy (dissimilar to the positive)?

- **Program Local Structures**
  - Why needed here: The reward and SFT data construction both depend on extracting local structures (subgraphs with parent-child and sibling relations) from program parse trees.
  - Quick check question: For a program tree, what distinguishes a valid "local structure" from an arbitrary subgraph?

## Architecture Onboarding

- **Component map**:
  - Eq (Query Encoder): BERT-base, encodes input x once per query
  - Ez (Context Encoder): BERT-base, encodes each selected example zi during retrieval
  - Ec (Candidate Encoder): BERT-base, pre-encodes entire candidate pool C
  - Policy head: Computes softmax(Ec(cj)⊤(Eq(x) + λ∑Ez(zi)) / τ) for selection
  - LLM: GPT-3.5-turbo-instruct for inference, generates programs given retrieved context

- **Critical path**:
  1. Pre-encode candidate pool with Ec
  2. SFT: Construct coverage-maximizing sequences → train with InfoNCE
  3. RL: Sample retrieval groups → generate programs → compute structural reward → GRPO update
  4. Inference: Greedy sequential selection (k=4 steps)

- **Design tradeoffs**:
  - λ controls context accumulation strength (paper uses 0.1); higher values may over-condition on retrieved examples
  - Group size in GRPO affects advantage estimation stability (paper finds 32-40 optimal)
  - k=4 retrieval steps balance coverage gains vs. computational cost and learning stability

- **Failure signatures**:
  - Retrieving semantically similar but structurally redundant examples → suggests SFT coverage objective not working or λ too low
  - RL degrading SFT performance → suggests reward signal too noisy or KL penalty β too low
  - Poor generalization on TMCD splits → suggests insufficient structural diversity in training data

- **First 3 experiments**:
  1. **Sanity check**: Implement SFT-only retriever with k=2, verify it outperforms BM25 top-k on a single GeoQuery split. If not, debug coverage computation.
  2. **Architecture isolation**: Compare tri-encoder vs. single concatenated encoder using identical training data. Expect tri-encoder to show gains on Template splits.
  3. **RL reward ablation**: Train with binary correctness reward vs. structural coverage reward (Table 8). Verify structural reward provides cleaner gradient signal early in training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can compositional retrieval mechanisms be designed to maintain stability over longer retrieval sequences without degradation from compounding errors?
- Basis in paper: [explicit] The limitations section states: "as retrieval steps increase, computational costs grow, and learning stability decreases due to compounding errors in sequential selection."
- Why unresolved: The paper restricts experiments to k=4 steps to balance efficiency and effectiveness, leaving the scalability challenge unaddressed.
- What evidence would resolve it: Experiments showing stable or improved performance when scaling to 8, 16, or more retrieval steps, with analysis of error propagation dynamics.

### Open Question 2
- Question: Can the tri-encoder retriever be adapted to dynamically incorporate new candidate entries after training without full retraining?
- Basis in paper: [explicit] The limitations section states: "similar to generative retrieval, our retriever cannot dynamically incorporate new entries after training."
- Why unresolved: The current architecture encodes the candidate pool at training time, with no mechanism for incremental updates.
- What evidence would resolve it: A modified architecture or fine-tuning approach that allows adding new candidates post-training while maintaining retrieval quality comparable to full retraining.

### Open Question 3
- Question: How well does the compositional retrieval approach generalize to non-programmatic tasks such as multi-hop QA or reasoning tasks?
- Basis in paper: [inferred] The paper evaluates exclusively on compositional generalization semantic parsing benchmarks (GeoQuery, COVR-10), stating this is an "ideal initial scenario." Generalization to other domains is unstated.
- Why unresolved: The reward design and data construction rely heavily on program structure coverage metrics specific to semantic parsing.
- What evidence would resolve it: Experimental results on diverse compositional tasks (multi-hop QA, chain-of-thought reasoning) using task-appropriate reward formulations.

### Open Question 4
- Question: How sensitive is the RL training stability to the choice of inference LLM, particularly when the retriever must align to different model preferences?
- Basis in paper: [inferred] The method uses gpt-3.5-turbo-instruct for all experiments, and RL training explicitly aligns the retriever to "the LLM's preferences," but cross-LLM compatibility is not analyzed.
- Why unresolved: Different LLMs may have distinct in-context learning behaviors, potentially requiring retriever retraining.
- What evidence would resolve it: Transfer experiments showing whether a retriever trained with one LLM performs well when paired with different inference models, or analysis of alignment gap across models.

## Limitations

- Sequential conditioning and RL training become unstable as retrieval steps increase beyond 4, with compounding errors degrading performance
- The retriever cannot dynamically incorporate new candidates after training without full retraining, limiting practical deployment
- Structural coverage reward assumes correlation with downstream correctness, which may not hold for tasks with non-local dependencies or semantic mismatches

## Confidence

- **High confidence**: The sequential conditioning mechanism (Mechanism 1) is well-supported by the mathematical formulation and clear experimental ablation showing performance gains over independent selection.
- **Medium confidence**: The tri-encoder architecture's contribution (Mechanism 2) is supported by controlled ablation but lacks direct corpus comparisons to alternative context fusion methods.
- **Medium confidence**: The two-stage training approach (Mechanism 3) shows empirical success through ablation studies, but the assumption that structural coverage correlates with downstream performance remains untested on diverse compositional tasks.

## Next Checks

1. **Reward signal validation**: On GeoQuery, compute correlation between structural Jaccard reward and actual program correctness across validation samples. If correlation < 0.7, investigate alternative reward formulations.
2. **Architecture necessity test**: Implement a variant using cross-attention to fuse query and retrieved contexts, training with identical SFT data. Compare performance on Template splits to determine if tri-encoder provides marginal or substantial gains.
3. **Coverage vs. correctness analysis**: For failed cases on TMCD splits, examine whether retrieved examples have high structural coverage but low semantic relevance to the query, indicating the reward may over-optimize for structure at the expense of meaning.