---
ver: rpa2
title: LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit
arxiv_id: '2504.11497'
source_url: https://arxiv.org/abs/2504.11497
tags:
- circuit
- design
- performance
- sizing
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM-based AI agent to automate the transistor
  sizing process in Analog and Mixed-Signal (AMS) circuit design. By integrating large
  language models with Ngspice simulations and data analysis functions through prompt
  engineering and function calling, the agent successfully optimized multiple circuits.
---

# LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit

## Quick Facts
- arXiv ID: 2504.11497
- Source URL: https://arxiv.org/abs/2504.11497
- Authors: Chang Liu; Emmanuel A. Olowe; Danial Chitnis
- Reference count: 18
- LLM-based AI agent successfully optimizes transistor sizing for AMS circuits, achieving target performance metrics in up to 60% of trials on a 20-transistor opamp.

## Executive Summary
This paper introduces an LLM-based AI agent that automates the transistor sizing process in Analog and Mixed-Signal (AMS) circuit design. The agent uses a ReAct loop, combining large language models with Ngspice simulations and data analysis functions through prompt engineering and function calling. Tested across seven basic circuits and a 20-transistor operational amplifier, Claude 3.5 Sonnet achieved the highest success rate and fewest iterations. The results demonstrate the potential of LLMs to improve efficiency and reduce manual effort in AMS circuit sizing.

## Method Summary
The method employs a ReAct loop where the agent cycles through reasoning (analyzing simulation results vs. targets) and acting (modifying netlist parameters) using Claude 3.5 Sonnet with Chain-of-Thought prompting and function calling. The system integrates with Ngspice for simulation and custom analysis functions to extract performance metrics. The agent was evaluated on seven basic circuits and a 20-transistor opamp using PTM 180nm models, with a maximum of 25 iterations per optimization.

## Key Results
- Claude 3.5 Sonnet achieved the highest success rate and fewest iterations among tested LLMs for basic circuit optimization
- The agent successfully reached target performance metrics for the 20-transistor opamp in up to 60% of trials across three different requirement groups
- The approach demonstrates potential to reduce manual effort and improve efficiency in AMS circuit sizing compared to traditional iterative manual methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ReAct (Reasoning and Acting) loop enables the agent to perform iterative, automated optimization of circuit parameters.
- Mechanism: The agent decomposes the task into a cycle of Reasoning (analyzing simulation results vs. targets) and Acting (modifying netlist parameters). This cycle continues until performance metrics are met or an iteration limit is reached, automating the manual "simulate-analyze-adjust" workflow of a human designer.
- Core assumption: The LLM possesses sufficient embedded knowledge of circuit design principles to propose parameter changes that will positively impact the observed performance metrics.
- Evidence anchors:
  - [Page 2, Section II.A]: "In the optimization process, action, observation, and comparison create a Reasoning and Acting (ReAct) loop... where the agent cycles through reasoning (analyzing) and taking action based on its insights."
  - [Page 2, Fig. 2]: Shows the "Action, observation and comparison" forming the "ReAct optimization loop."
  - [corpus]: Related work (e.g., AnaFlow, arxiv:2511.03697) supports the use of agentic workflows for reasoning-driven circuit sizing.
- Break condition: The optimization fails if the LLM's suggested parameter changes do not converge on a solution within the iteration limit (e.g., 20 or 25 iterations), or if it "oscillates between solutions" as noted in the discussion.

### Mechanism 2
- Claim: Chain-of-Thought (CoT) prompting and in-context learning enable the LLM to handle complex, multi-objective trade-offs inherent in AMS design.
- Mechanism: The system prompt provides a structured context including the circuit type, target metrics, and the history of previous and current results. The CoT instruction forces the model to generate intermediate reasoning steps (e.g., "Increase the gain: To achieve a gain above..."). This explicit reasoning helps the model navigate dependencies, such as trading power consumption for bandwidth.
- Core assumption: Providing a history of past results helps the model infer the causal relationship between parameter changes and performance shifts, preventing it from repeating unsuccessful adjustments.
- Evidence anchors:
  - [Page 2, Section II.B]: "The CoT instructions guide the LLMs to think step by step based on their observation to prompt them to generate new, potentially optimal design points."
  - [Page 3, Section III.B]: "Despite fluctuations... the agent succeeds... proving the effectiveness of in-context learning and our prompt strategy."
  - [corpus]: Corpus evidence for this specific CoT mechanism is weak, though TopoSizing (arxiv:2509.14169) also emphasizes "reasoning" in sizing.
- Break condition: The reasoning chain becomes flawed or the LLM fails to correctly interpret the multi-metric trade-offs, leading to improvements in one area (e.g., gain) that degrade another critical metric (e.g., output voltage range) beyond acceptable limits.

### Mechanism 3
- Claim: Function calling integrates external, specialized tools (Ngspice) for simulation and analysis, grounding LLM decisions in quantitative data.
- Mechanism: The agent uses a set of pre-defined functions (e.g., `run_ngspice`, `ac_simulation`, `phase_margin`) which the LLM calls via structured outputs. The agent's backend executes these functions with the modified netlist and returns the numerical results to the LLM as an observation. This bridges the gap between the LLM's qualitative knowledge and the quantitative reality of circuit physics.
- Core assumption: The LLM can reliably select the correct sequence of analysis functions and correctly interpret the structured numerical data returned by those functions.
- Evidence anchors:
  - [Page 2, Section II.C]: "We utilized CoT to guide the LLM thinking step by step to choose required functions... taking into account the entire optimization flow."
  - [Page 4, Section V]: "By employing function-calling techniques, the LLMs are integrated with the external simulator Ngspice and data analysis functions."
  - [corpus]: This is a common pattern; AMS-IO-Agent (arxiv:2512.21613) also uses a framework connecting an LLM with industrial-level design tasks.
- Break condition: The mechanism fails if a function call errors out, returns malformed data, or if the LLM misinterprets the numerical results (e.g., confusing units or metrics), breaking the observation step of the ReAct loop.

## Foundational Learning

- Concept: **SPICE Netlists and Circuit Sizing**
  - Why needed here: The agent's primary output is a modified SPICE netlist. Understanding that transistor sizing involves changing Width (W) and Length (L) parameters to meet performance targets (gain, bandwidth, power) is fundamental.
  - Quick check question: In a SPICE netlist, which parameters for a MOSFET (e.g., M1) would the agent most likely modify to optimize the circuit's performance for a given topology?

- Concept: **ReAct (Reasoning and Acting) Pattern**
  - Why needed here: The entire agent architecture is built on a ReAct loop. Understanding this cycle (Thought → Action → Observation) is key to debugging why the agent might be stuck in a loop or failing to converge.
  - Quick check question: If an agent observes that its last parameter change increased power consumption beyond the target, what is the next logical step in the ReAct cycle?

- Concept: **Prompt Engineering (CoT & In-Context Learning)**
  - Why needed here: The agent's performance is heavily dependent on how the problem is framed in the prompt. Knowing that the prompt includes previous results as context helps explain how the agent avoids repeating mistakes.
  - Quick check question: Why is the "previous results" field in the prompt template critical for the agent's ability to optimize a circuit over multiple iterations?

## Architecture Onboarding

- Component map: User Interface/Input -> Agent Core (LLM) -> Prompt Constructor -> Tool Layer (Function Calling) -> Simulation & Analysis Backend -> Output Parser
- Critical path: The **Tool Layer** is the most critical component to verify. The agent fails if `run_ngspice` cannot be called correctly, if the netlist is malformed for the simulator, or if the analysis functions cannot parse the simulator's raw output. A failure here breaks the observation loop entirely.
- Design tradeoffs:
  - **LLM Choice:** The paper shows Claude 3.5 Sonnet had higher success but more variability on complex circuits, while GPT-4o failed completely on one circuit type. This suggests a tradeoff between reliability and capability.
  - **Iteration Limit:** Setting a limit (e.g., 25 iterations) trades the potential for finding a solution against API costs and time. The paper shows some solutions were found in as few as 11-13 iterations, while others hit the limit.
  - **Context History:** Including more previous iterations in the prompt gives the LLM more data for in-context learning but consumes the context window, potentially leading to confusion or truncation.
- Failure signatures:
  1. **Oscillation:** The agent suggests the same or alternating parameter sets, failing to converge. This is noted in the paper's discussion.
  2. **Metric Trade-off Failure:** Improving one metric (e.g., gain) causes another critical metric (e.g., phase margin) to fail, and the agent cannot find a balance within the iteration limit.
  3. **Tool Errors:** The agent generates a function call with incorrect arguments or a malformed netlist, causing the simulation to fail and return an error instead of performance data.
- First 3 experiments:
  1. **Reproduce Single-Transistor Optimization:** Run the agent on one of the seven basic circuits (e.g., a common-source amplifier) with a single target metric (e.g., gain ≥ 20dB) to verify the full ReAct loop and tool execution.
  2. **Ablate Context History:** Modify the prompt constructor to *not* include the "previous results" for the same basic circuit. Compare the iteration count and success rate to quantify the value of in-context learning.
  3. **Stress Test with Conflicting Targets:** Give the agent a circuit and two conflicting targets (e.g., minimize power AND maximize bandwidth) to observe its reasoning process for handling trade-offs and identify if it gets stuck in an oscillation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed AI agent effectively balance the trade-offs between Power, Performance, and Area (PPA) when layout constraints and transistor area are explicitly included in the optimization objectives?
- Basis in paper: [explicit] The paper states that "the overall area of transistors is excluded in this study" and suggests "future work should integrate area and layout considerations for a holistic PPA trade-off."
- Why unresolved: The current study focuses solely on performance metrics (gain, bandwidth, etc.) and power, ignoring the physical silicon area, which is a critical constraint in real-world AMS design.
- What evidence would resolve it: Successful optimization runs where the agent converges on a netlist that meets specific area budgets while simultaneously satisfying the existing performance targets.

### Open Question 2
- Question: Does the integration of true Monte Carlo simulations with accurate device models improve the agent's ability to ensure design robustness against process variations?
- Basis in paper: [explicit] The authors note "high variations" in their manual variation tests and conclude that "A true Monte Carlo simulations with accurate device models are needed for the agent to ensure the design robustness."
- Why unresolved: The current variation tests used simple Gaussian distributions on parameters, which may not accurately reflect actual manufacturing variations or correlate specific device mismatches.
- What evidence would resolve it: Demonstrating that the agent can optimize circuits to maintain target specifications across process corners or a sufficiently large Monte Carlo sample set.

### Open Question 3
- Question: Can a specific feedback mechanism be implemented to prevent the LLM from oscillating between solutions and ensure convergence when the iteration limit is extended?
- Basis in paper: [explicit] The authors note that "LLMs may oscillate between solutions" and that "A feedback mechanism should be introduced to prompt diverse solutions and improve reliability."
- Why unresolved: The current ReAct loop sometimes fluctuates or degrades performance metrics while optimizing others, lacking a mechanism to "remember" or steer away from previously failed states.
- What evidence would resolve it: A modification to the agent architecture that detects repetitive or oscillating parameter updates and successfully forces a strategic pivot to explore new regions of the design space.

## Limitations

- The paper does not disclose the exact prompt templates, initial netlist files, or detailed function signatures, making faithful reproduction difficult.
- The agent can get stuck oscillating between solutions, and the paper acknowledges the need for a feedback mechanism to prompt diverse solutions—this is not yet implemented.
- While successful on seven basic circuits and one opamp, results may not generalize to more complex topologies or different technology nodes without significant prompt engineering.

## Confidence

- **High Confidence**: The core ReAct loop mechanism (Reasoning + Acting + Observation) is well-supported by the text and standard in agentic workflows. The integration of function calling with Ngspice is technically sound.
- **Medium Confidence**: The claim that CoT and in-context learning enable effective multi-objective trade-offs is supported by experimental results but lacks detailed ablation studies. The success on the 20-transistor opamp is promising but limited to three requirement groups.
- **Low Confidence**: The claim that Claude 3.5 Sonnet is "the best performer" is based on a single comparison (GPT-4o failed completely on one circuit). Without broader LLM benchmarking, this conclusion is not robust.

## Next Checks

1. **Prompt Template Validation**: Reconstruct the exact system prompt and CoT instructions from the paper's description and test whether the agent can reproduce the results on at least one basic circuit.
2. **Ablation of Context History**: Run the agent with and without the "previous results" field in the prompt to quantify the impact of in-context learning on iteration count and success rate.
3. **Conflict Resolution Test**: Design a test case with mutually exclusive targets (e.g., minimum power AND maximum bandwidth) to observe whether the agent oscillates or can reason through trade-offs.