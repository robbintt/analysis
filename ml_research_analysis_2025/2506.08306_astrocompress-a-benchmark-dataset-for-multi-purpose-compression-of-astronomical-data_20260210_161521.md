---
ver: rpa2
title: 'AstroCompress: A benchmark dataset for multi-purpose compression of astronomical
  data'
arxiv_id: '2506.08306'
source_url: https://arxiv.org/abs/2506.08306
tags:
- data
- compression
- image
- neural
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AstroCompress, a comprehensive benchmark
  dataset for lossless compression of astronomical imagery. The dataset comprises
  five distinct datasets spanning space-based (Hubble, JWST) and ground-based (Keck,
  SDSS) telescopes, totaling ~320 GB of 16-bit unsigned integer data in 2D, 3D, and
  4D formats.
---

# AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data

## Quick Facts
- **arXiv ID**: 2506.08306
- **Source URL**: https://arxiv.org/abs/2506.08306
- **Reference count**: 40
- **Primary result**: Neural compression methods consistently outperform classical approaches on lossless astronomical imagery, with noise levels strongly influencing compressibility

## Executive Summary
This paper introduces AstroCompress, a comprehensive benchmark dataset for lossless compression of astronomical imagery spanning space-based (Hubble, JWST) and ground-based (Keck, SDSS) telescopes. The dataset comprises five distinct datasets totaling ~320 GB of 16-bit unsigned integer data in 2D, 3D, and 4D formats. The authors benchmark seven compression methods (three neural: IDF, L3C, PixelCNN++; four classical: JPEG-XL, JPEG-LS, JPEG-2000, RICE) and find that neural methods consistently outperform classical approaches, with JPEG-XL establishing the new state-of-the-art among non-neural methods. VDM estimates suggest significant room for improvement. The analysis reveals that noise levels strongly influence compressibility, with higher noise reducing compression ratios, and demonstrates that neural models trained on diverse datasets generalize better across astronomical data types.

## Method Summary
The study evaluates lossless compression on five astronomical datasets (Hubble, JWST, Keck, SDSS, LCO) totaling ~320 GB of 16-bit unsigned integer data. Neural methods are trained on random 32×32 patches and evaluated by tiling full images into patches with padding. Two strategies handle 16-bit support: splitting into MSB/LSB as 2-channel 8-bit input or direct 16-bit input requiring model scaling. Classical methods process full images. Compression ratios are measured as 16 / negative log-likelihood, with VDM providing theoretical upper bounds. Training uses standard hyperparameters (IDF: LR 1e-3, decay 0.999; L3C: LR 1e-4, decay 0.9; PixelCNN++: LR 2e-4).

## Key Results
- Neural compression methods consistently outperform classical approaches across all five astronomical datasets
- JPEG-XL (max) establishes the new state-of-the-art among non-neural methods
- VDM estimates suggest significant room for improvement in practical compression methods
- Noise levels strongly influence compressibility, with higher noise reducing compression ratios
- Neural models trained on diverse datasets (Keck) generalize better across astronomical data types than models trained on target domains

## Why This Works (Mechanism)

### Mechanism 1: Noise-Dominated Bitrate Allocation
Compression ratio is inversely correlated with image noise levels because background pixels (98%+ of image area) follow approximately Gaussian noise distributions whose entropy scales with log₂(σ). Background pixel noise is effectively incompressible random information; longer exposure times accumulate more noise bits, reducing compression potential. Core assumption: background noise is approximately i.i.d. Gaussian; this holds well for ground-based optical data but less so for space-based infrared. Break condition: For saturated pixels approaching 16-bit max, entropy decreases as values cluster near ceiling.

### Mechanism 2: Cross-Dimensional Redundancy Exploitation
Temporal and spectral correlations in multi-frame data can improve compression, but standard neural codecs designed for natural images underperform classical methods at exploiting these cross-frame correlations. JWST's "up-the-ramp" sampling creates temporally correlated 3D cubes where pixel values monotonically increase; SDSS provides same sky region across 5 wavelength filters and multiple epochs. Classical codecs use reversible color transforms that effectively decorrelate channels. Break condition: When temporal gaps are large (SDSS images taken days apart), atmospheric variation destroys temporal correlation.

### Mechanism 3: Diversity-Induced Generalization
Models trained on heterogeneous datasets with varied filters, exposure times, and noise conditions can generalize better—even outperforming models trained on the target domain. The Keck dataset, with its diverse observing conditions, acts as data augmentation that prevents overfitting to specific noise patterns. Core assumption: underlying signal structure (point sources, extended galaxies) is invariant across instruments; only noise characteristics differ. Break condition: Cross-domain transfer fails when detector physics differs fundamentally (optical CCD vs. infrared HgCdTe).

## Foundational Learning

- **Lossless Compression via Probability Modeling**
  - Why needed here: All methods evaluated operate by estimating pixel probability distributions; understanding Shannon's source coding theorem explains why VDM's likelihood estimates indicate "room for improvement"
  - Quick check question: Can you explain why a perfect probability model p(x) achieves bitrate = -log₂ p(x) bits per symbol?

- **Astronomical Detector Physics**
  - Why needed here: Understanding why 16-bit depth matters, what "up-the-ramp sampling" means for JWST, and why cosmic rays and charge transfer inefficiency create compression challenges specific to space-based imaging
  - Quick check question: Why would a pixel's value monotonically increase during a JWST integration but not during a ground-based CCD exposure?

- **Neural Generative Model Taxonomy**
  - Why needed here: The paper benchmarks three distinct neural approaches (flows→IDF, VAE→L3C, autoregressive→PixelCNN++) with different computational tradeoffs; VDM represents theoretical upper bound via diffusion
  - Quick check question: Which neural architecture would you choose if encoding speed was critical but decoding could be slow?

## Architecture Onboarding

- **Component map**: HuggingFace datasets API -> uint16 numpy arrays -> 32×32 patch extraction -> 16-bit handling (MSB/LSB split or direct) -> Model families (Flow-based IDF, VAE-hierarchical L3C, Autoregressive PixelCNN++, Diffusion VDM) -> Bits-per-pixel evaluation

- **Critical path**: 1. Select dataset split (train/test spatially non-overlapping per WCS verification) 2. Choose 16-bit handling strategy (MSB/LSB split generally performs similarly or better) 3. Train on 32×32 patches with appropriate data augmentation (horizontal flip only) 4. Evaluate on full images via patch-wise likelihood aggregation

- **Design tradeoffs**: Patch size vs. context: 32×32 patches limit compression ratio but enable GPU memory efficiency; Diversity vs. specificity: Training on combined datasets improves generalization but may underperform on any single domain compared to specialized models; Encoding speed vs. compression: JPEG-XL (max) achieves best classical performance but takes ~90 seconds per Hubble image

- **Failure signatures**: Neural models trained on single instruments fail catastrophically on others (Table 2: Hubble-trained model achieves only 0.67 ratio on LCO); RGB-pretrained models struggle with space-based data due to different noise characteristics; Temporal compression fails when frame gaps exceed atmospheric correlation timescales

- **First 3 experiments**: 1. **Baseline reproduction**: Run JPEG-XL (default and max effort) and IDF on SDSS-2D test split; verify compression ratios within 5% of reported values (3.14 and 2.91 respectively) 2. **16-bit handling ablation**: Train PixelCNN++ on Keck using both direct 16-bit and MSB/LSB 2-channel strategies; expect ~10% difference favoring split approach 3. **Generalization probe**: Train IDF on Keck only, evaluate on all five single-frame datasets; verify that SDSS-2D performance (target: ~3.02) exceeds SDSS-trained model performance (target: ~2.91)

## Open Questions the Paper Calls Out

### Open Question 1
How can neural lossy compression algorithms be designed to selectively preserve scientifically valuable data (such as source pixels) while aggressively discarding background noise? The paper restricts its scope to lossless compression, and lossy methods require defining fidelity metrics relevant to specific scientific analyses rather than just visual quality. A benchmark of lossy neural codecs quantifying trade-off between compression ratio and preservation of scientific measurements would resolve this.

### Open Question 2
Can neural network architectures be specifically optimized to exploit cross-wavelength and temporal correlations in astronomical data cubes more effectively than current classical codecs? Standard neural compression models adapted from natural image processing (2D) do not effectively capture the unique 3D and 4D structures present in time-series and multi-wavelength astronomy data. The development of a neural architecture that consistently outperforms JPEG-XL on the SDSS-3D and JWST-2D-Res datasets would resolve this.

### Open Question 3
Can the theoretical performance upper bounds indicated by Variational Diffusion Models (VDM) be translated into practical, computationally efficient codecs suitable for transmission from space-based observatories? The high computational cost of diffusion steps currently prevents these models from being deployed on hardware with strict power and latency constraints. A study demonstrating a distilled or accelerated diffusion-based codec that retains high compression ratios while operating within feasible encoding times for satellite downlinks would resolve this.

## Limitations
- Study focuses exclusively on 16-bit unsigned integer data, potentially missing compression opportunities in floating-point scientific data formats common in radio astronomy
- Patch-based evaluation (32×32) may underestimate the true compression potential of neural methods that could benefit from larger spatial contexts
- Does not address lossy compression trade-offs that might be acceptable for certain scientific applications where perfect reconstruction is not required

## Confidence
- **High Confidence**: Neural compression methods consistently outperform classical approaches across all tested astronomical datasets; inverse correlation between noise levels and compression ratios is robustly demonstrated
- **Medium Confidence**: Cross-dimensional redundancy exploitation provides meaningful compression gains for temporally correlated data; diversity-induced generalization claim showing Keck-trained models outperforming SDSS-trained models on SDSS data is supported but requires further validation
- **Low Confidence**: VDM estimates suggesting significant room for improvement represent theoretical bounds that may not be achievable with current neural architectures; claim that RGB-pretrained models perform 15-25% worse on space-based data lacks extensive empirical validation

## Next Checks
1. **Cross-Format Generalization**: Evaluate the same compression methods on floating-point radio astronomy data (e.g., from ALMA or VLA) to assess performance beyond 16-bit integer formats
2. **Context Window Scaling**: Re-run key neural compression benchmarks using larger patch sizes (64×64, 128×128) to quantify the impact of spatial context on compression ratios and determine if current patch-based evaluation underestimates neural method performance
3. **Instrument Transfer Robustness**: Conduct systematic ablation studies training models on pairs of instruments (e.g., Keck+Hubble vs. Keck+JWST) and evaluating on held-out third instruments to quantify which instrument combinations provide optimal cross-domain generalization