---
ver: rpa2
title: Skywork-R1V3 Technical Report
arxiv_id: '2507.06167'
source_url: https://arxiv.org/abs/2507.06167
tags:
- reasoning
- learning
- wang
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skywork-R1V3 introduces an RL-enhanced vision-language model that
  achieves 76.0% accuracy on the MMMU benchmark, matching entry-level human performance.
  Its core innovation is transferring reasoning capabilities from text-only LLMs to
  visual tasks using an elaborate post-training RL framework without additional pretraining.
---

# Skywork-R1V3 Technical Report

## Quick Facts
- arXiv ID: 2507.06167
- Source URL: https://arxiv.org/abs/2507.06167
- Reference count: 18
- Key outcome: 76.0% accuracy on MMMU benchmark, matching entry-level human performance

## Executive Summary
Skywork-R1V3 is a vision-language model that achieves state-of-the-art performance on the MMMU benchmark through reinforcement learning. The model introduces an RL-enhanced framework that activates and enhances reasoning capabilities without additional pretraining, achieving 76.0% accuracy. Key innovations include using critical token entropy for checkpoint selection and maintaining connector module trainability during RL to ensure robust cross-modal alignment.

## Method Summary
Skywork-R1V3 employs a three-stage post-training pipeline: initial cold-start SFT from R1V2 reasoning traces, followed by RL with GRPO optimization on 15K math problems, and concluding with connector-only tuning on multi-domain data. The model uses a connector module for cross-modal alignment and introduces critical token entropy as an indicator for selecting high-quality RL checkpoints. Training leverages a hybrid format-accuracy reward model and curriculum learning approaches to balance reasoning strength with broad knowledge.

## Key Results
- Achieves 76.0% accuracy on MMMU benchmark, matching entry-level human performance
- RL improves out-of-domain MMMU performance from 68.1% to 74.5%, while SFT degrades it to 65.4%
- Connector-only tuning post-RL improves performance from 74.5% to 76.0%
- Outperforms larger open-source VLMs and approaches proprietary models on general and domain-specific tasks

## Why This Works (Mechanism)

### Mechanism 1: Reinforcement Learning Activates Generalizable Reasoning
RL optimization over accuracy rewards encourages the model to discover and reinforce reasoning pathways that produce correct answers across problem types. The policy gradient updates reward correct outcomes regardless of specific solution patterns, allowing the model to develop flexible problem-solving strategies rather than memorizing fixed sequences.

### Mechanism 2: Critical Token Entropy Predicts Reasoning Quality
High entropy at reasoning-initiation tokens ("Wait...", "Alternatively...") correlates with genuine reasoning capability. Models that genuinely deliberate maintain uncertainty at decision branches, sampling from a broader action space. Models merely mimicking style produce deterministic, low-entropy outputs at these critical junctures.

### Mechanism 3: Connector Module Enables Cross-Modal Gradient Flow
The connector translates visual representations into the LLM's embedding space. During RL, when the policy explores reasoning paths that leverage visual information, gradients must flow through the connector to align visual features with newly developed reasoning patterns. A frozen connector blocks this adaptation, causing the language model to receive uninformative or misaligned visual signals.

## Foundational Learning

- **Proximal Policy Optimization (PPO) and GRPO**: Understanding clipped surrogate objectives, advantage estimation, and KL penalties is prerequisite to following Section 3.3's algorithm descriptions.
  - Quick check: Can you explain why GRPO normalizes rewards within groups of sampled traces rather than using raw binary rewards directly?

- **Chain-of-Thought Reasoning Transfer**: Familiarity with CoT prompting and how it manifests in LLMs is assumed.
  - Quick check: What distinguishes "genuine" CoT reasoning from surface-level pattern imitation in language models?

- **Vision-Language Model Architecture**: Understanding of typical VLM components (visual encoder, connector/projector, LLM backbone) and their information flow is assumed.
  - Quick check: In a standard VLM, which component maps visual features to the language model's embedding dimension, and why might it need task-specific tuning?

## Architecture Onboarding

- **Component map**: Visual Encoder -> Connector (MLP projection) -> Language Backbone (InternVL-38B) -> Reward Model (Qwen2.5-Instruct-32B verifier)

- **Critical path**:
  1. Cold-Start SFT (12K filtered samples from R1V2 reasoning traces) → establishes "think-before-answer" style
  2. RL with GRPO (15K math problems, no reasoning labels) → activates generalizable reasoning; connector must be trainable
  3. Connector-Only Tuning (10K multi-domain data) → rebalances knowledge distribution without disrupting reasoning

- **Design tradeoffs**:
  - Visual encoder trainability: Trainable encoder provides additional gains but increases compute
  - Learning rate: 1e-5 accelerates early gains but risks late-stage instability
  - Curriculum learning: Progressively harder data causes distribution shift and harms generalization
  - Clip-Higher: Intended to boost exploration but caused severe instability in this context

- **Failure signatures**:
  - Connector frozen during RL → reward collapse, repetitive/degenerate outputs
  - High learning rate (1e-5) in late RL stages → entropy surge, policy destabilization
  - SFT-only training → high in-domain performance, poor out-of-domain transfer
  - Hallucinated "can't see the image" statements in CoT → ~25% accuracy drop on affected samples

- **First 3 experiments**:
  1. Connector ablation during RL: Train with frozen vs. trainable connector on small math dataset
  2. Entropy-based checkpoint selection: Log entropy at critical tokens during RL, plot against validation accuracy
  3. Post-RL tuning component study: Compare connector-only, connector+LLM, and full model tuning on multi-domain data

## Open Questions the Paper Calls Out

### Open Question 1
Does the connector module's pivotal role in RL-based cross-modal alignment generalize to different connector architectures (e.g., Q-Former vs. MLP) or vision-language model designs?

### Open Question 2
What are the root causes of the "can't see the image" hallucination in the Chain-of-Thought, and how can it be systematically mitigated?

### Open Question 3
How can the conflict between skills learned on high-difficulty problems and core reasoning paths for medium-difficulty problems be resolved to enable effective curriculum learning?

## Limitations
- Training corpus (15K math problems) is relatively small compared to pretraining datasets
- Entropy-based checkpoint selection lacks external validation and may be sensitive to tokenization
- Connector module's importance may not generalize to architectures with different cross-modal integration approaches

## Confidence
- **High Confidence**: Empirical demonstration that RL improves out-of-domain generalization while SFT degrades it; connector ablation results showing training collapse when frozen; post-RL connector-only tuning improvements
- **Medium Confidence**: Entropy correlation with reasoning quality as checkpoint selection method; claim that RL activates latent reasoning rather than creating it from scratch
- **Low Confidence**: Generalizability of critical token entropy as universal reasoning indicator; scalability claims to larger datasets; assertion that findings apply broadly to VLMs beyond tested architecture

## Next Checks
1. **Architecture Transfer Test**: Apply RL framework to different VLM architecture (e.g., Qwen2.5-VL) to verify connector trainability is universally critical
2. **Base Model Dependency Analysis**: Repeat training from base LLM with weaker reasoning capabilities to test whether RL can truly activate latent reasoning
3. **Entropy Metric Validation**: Conduct study correlating human-labeled "genuine reasoning" vs. "pattern imitation" with entropy measurements at critical tokens