---
ver: rpa2
title: 'MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance'
arxiv_id: '2510.00499'
source_url: https://arxiv.org/abs/2510.00499
tags:
- speech
- text
- layer
- arxiv
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOSS-Speech, a true speech-to-speech large
  language model that directly generates speech without intermediate text. It employs
  a modality-based layer-splitting architecture to fuse speech and text representations
  in early layers and specialize in later layers, along with a frozen pretraining
  strategy to preserve text capabilities.
---

# MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance

## Quick Facts
- arXiv ID: 2510.00499
- Source URL: https://arxiv.org/abs/2510.00499
- Authors: Xingjian Zhao, Zhe Xu, Qinyuan Cheng, Zhaoye Fei, Luozhijie Jin, Yang Wang, Hanfu Chen, Yaozhou Jiang, Qinghui Gao, Ke Chen, Ruixiao Li, Mingshu Chen, Ruiming Wang, Wenbo Zhang, Yiyang Zhang, Donghua Yu, Yang Gao, Xiaogui Yang, Yitian Gong, Yuanfan Xu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu
- Reference count: 35
- Primary result: Achieves state-of-the-art performance on spoken question answering benchmarks while generating speech directly without intermediate text

## Executive Summary
MOSS-Speech introduces a true speech-to-speech large language model that directly generates speech outputs without relying on intermediate text representations. The model employs a novel modality-based layer-splitting architecture that fuses speech and text representations in early layers while specializing in later layers, enabling cross-modal transfer while preserving modality-specific generation capabilities. Through a two-stage training approach with frozen pretraining and a semantic-focused ASR tokenizer, the model achieves competitive performance on both speech understanding and generation tasks while maintaining strong text comprehension capabilities.

## Method Summary
The approach initializes from Qwen3-8B and implements a layer-splitting architecture where layers 0-31 are shared for cross-modal fusion while layers 32-35 are modality-specific (text vs. speech branches). Training proceeds in two stages: first freezing the text backbone while training speech components, then jointly fine-tuning with mixed text and speech data. A custom ASR-trained tokenizer converts speech to discrete tokens optimized for semantic content rather than acoustic reconstruction. The model is trained on a large-scale multilingual corpus (~4M hours) with synthetic fine-tuning data generated from text QA pairs, enabling speech-to-speech and speech-to-text generation capabilities.

## Key Results
- Achieves state-of-the-art performance on spoken question answering benchmarks
- Maintains text comprehension comparable to original Qwen3-8B backbone (67.19 vs 76.60 MMLU)
- Generates speech quality comparable to text-guided models (UTMOS scores competitive with baselines)
- Successfully bridges speech and text modalities without performance degradation in either domain

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layer-split architecture enables cross-modal transfer while preserving modality-specific generation.
- **Mechanism:** Early layers (0-31) perform joint speech-text fusion; final 4 layers specialize per modality. Similarity analysis shows alignment peaks in middle layers then declines—splitting at this inflection captures shared representations before divergence.
- **Core assumption:** Speech-text alignment follows a consistent layer-wise pattern across model depths that can be exploited architecturally.
- **Evidence anchors:** [abstract] "modality-based layer-splitting architecture to fuse speech and text representations in early layers and specialize in later layers"; [Section 2.1] Figure 2 shows similarity increases through layer ~10, fluctuates layers 11-25, decreases in final 3-4 layers; [corpus] Limited direct corpus support for layer-splitting specifically; related work on modular pipelines suggests architectural separation affects interaction quality.
- **Break condition:** If similarity patterns vary significantly across languages or speech types, fixed split point may be suboptimal.

### Mechanism 2
- **Claim:** Frozen pre-training preserves text backbone knowledge while initializing speech alignment.
- **Mechanism:** Stage 1 freezes all Qwen-3-8B parameters; trains only speech embeddings, speech-specific layers, and speech LM head. This anchors speech representations to stable text hidden states before joint adaptation.
- **Core assumption:** Pre-trained text representations form a sufficiently rich target for speech alignment without modification.
- **Evidence anchors:** [abstract] "frozen pretraining strategy to preserve text capabilities"; [Section 3.1.2] Stage 1 trains "only the newly introduced speech-related components"; [Section 5] Ablation shows Frozen Pretrain (FP) achieves 66.50 MMLU vs. 62.11 for No Frozen (NF).
- **Break condition:** If speech modality requires fundamentally different representations than text provides, frozen alignment may limit speech quality.

### Mechanism 3
- **Claim:** ASR-objective tokenizer preserves semantics better than reconstruction-only codecs for LLM learning.
- **Mechanism:** Encoder trained with ASR loss (not just reconstruction) produces tokens optimized for semantic content. Single-codebook design reduces sequence length vs. multi-codebook alternatives.
- **Core assumption:** Semantic coherence matters more than acoustic fidelity for speech language modeling tasks.
- **Evidence anchors:** [Section 2.2] "we adopt automatic speech recognition (ASR) as the sole training objective for our tokenizer encoder"; [Table 3] ASR-based codec achieves 10.80% WER vs. 14.45% for reconstruction-focused Mimi-8; [corpus] Related work on spoken QA (VoxRAG, prosody papers) emphasizes semantic content preservation but doesn't directly validate ASR-tokenizer approach.
- **Break condition:** Tasks requiring fine acoustic detail (speaker verification, emotion detection) may suffer from semantic-focused compression.

## Foundational Learning

- **Concept: Speech Tokenization (Discrete Units)**
  - Why needed here: Understanding how continuous audio becomes discrete tokens is essential for grasping the entire architecture.
  - Quick check question: Can you explain why single-codebook tokens simplify autoregressive generation compared to multi-codebook approaches?

- **Concept: Cross-Modal Alignment in Transformers**
  - Why needed here: The layer-splitting decision rests on understanding how different modalities align in shared representation space.
  - Quick check question: What does it mean for speech and text representations to be "aligned" at a given layer?

- **Concept: Frozen vs. Fine-tuning Strategies**
  - Why needed here: The two-stage training depends on understanding when to freeze vs. unfreeze parameters.
  - Quick check question: Why might training all parameters from the start cause text capability degradation?

## Architecture Onboarding

- **Component map:**
  - Speech Encoder (causal, streaming): Audio → discrete tokens (12.5 Hz, 175 bps)
  - Transformer Backbone (36 layers, from Qwen-3-8B): Layers 0-31 shared, 32-35 split
  - Text Head: Final 4 layers → text tokens
  - Speech Head: Parallel 4 layers → speech tokens
  - Speech Decoder (flow-matching): Speech tokens → waveform

- **Critical path:** Audio input → Encoder → Speech tokens → Shared backbone (0-31) → Speech branch (32-35) → Decoder → Audio output

- **Design tradeoffs:**
  - Lower frame rate (12.5 Hz vs. 25 Hz) reduces sequence length but may lose temporal detail
  - Streaming encoder adds latency constraints vs. chunk-based alternatives
  - Single-codebook simplifies modeling vs. multi-codebook acoustic richness

- **Failure signatures:**
  - Text benchmarks (MMLU/CMMLU) dropping significantly indicates overfitting to speech
  - High WER on tokenizer reconstruction suggests semantic information loss
  - Similarity scores not peaking in expected layer range suggests alignment failure

- **First 3 experiments:**
  1. Validate layer-split point: Run similarity analysis (Section C methodology) on your data to confirm layer ~32 split is appropriate for your backbone.
  2. Ablate frozen pre-training: Compare FP-Full vs. NF (no frozen) on your target benchmarks to quantify preservation benefit.
  3. Tokenizer quality check: Fine-tune a small LM on ASR task using your tokenizer; compare WER against baselines (Table 3) to validate semantic preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the persistent performance gap between speech-to-text (S→T) and speech-to-speech (S→S) generation, and how can it be fully closed?
- Basis in paper: [explicit] Table 6 shows S→T (77.33) substantially outperforms S→S (63.67) on LlamaQA, with similar gaps on other benchmarks. The authors state they "narrow the gap" but do not eliminate it.
- Why unresolved: The paper demonstrates improvement but does not investigate the root causes of residual degradation when generating speech output versus text output.
- What evidence would resolve it: Ablation studies isolating tokenizer quality, speech decoding fidelity, and cross-modal alignment errors; analysis of error types specific to S→S generation.

### Open Question 2
- Question: How can text capability degradation be further mitigated when extending LLMs to speech, beyond the 10-point MMLU drop observed?
- Basis in paper: [explicit] Table 7 shows MMLU drops from 76.60 (Qwen3-8B baseline) to 67.19 (MOSS-Speech), despite frozen pretraining. The authors frame preservation as a goal but acknowledge measurable degradation.
- Why unresolved: Even with frozen pretraining and layer splitting, text performance does not fully recover, and the trade-off between speech acquisition and text retention is not fully characterized.
- What evidence would resolve it: Systematic comparison of alternative preservation techniques (e.g., rehearsal, elastic weight consolidation); analysis of which text capabilities degrade most and why.

### Open Question 3
- Question: Is the empirically chosen layer-split point (32nd of 36 layers) optimal, or should the split depth adapt dynamically based on task or modality?
- Basis in paper: [inferred] Section 2.1 selects layer 32 based on similarity analysis showing divergence in final layers, but the choice is heuristic and not systematically ablated across different split depths.
- Why unresolved: The analysis is conducted on a fixed architecture; whether the optimal split generalizes across model sizes, datasets, or task types remains unknown.
- What evidence would resolve it: Ablation experiments varying the split point across multiple layers; evaluation of task-specific or learned split strategies.

### Open Question 4
- Question: To what extent does the model preserve and appropriately respond to paralinguistic cues (emotion, prosody, non-vocal sounds) in input speech?
- Basis in paper: [explicit] The introduction motivates true speech-to-speech models by their potential to preserve "paralinguistic cues such as prosody, emphasis, and emotion," but evaluation focuses on QA accuracy and speech quality, not cue preservation or response.
- Why unresolved: No benchmark or quantitative analysis is provided for paralinguistic understanding or expressive generation beyond intelligibility and speaker similarity.
- What evidence would resolve it: Evaluation on paralinguistic benchmarks; qualitative and quantitative analysis of emotion/prosody transfer from input to output speech.

## Limitations

- The empirically chosen layer-split point (layer 32) may not generalize across different model scales or speech types
- Performance gap persists between speech-to-text and speech-to-speech generation despite architectural improvements
- Reliance on synthetic data for fine-tuning raises questions about real-world deployment performance
- Streaming encoder design at 12.5 Hz may compromise temporal resolution compared to chunk-based approaches

## Confidence

**High Confidence Claims:**
- The layer-splitting architecture successfully enables cross-modal transfer while preserving modality-specific generation (supported by MMLU/CMMLU retention and strong speech QA performance)
- Frozen pre-training effectively preserves text capabilities during speech adaptation (confirmed by ablation showing 4.4-point MMLU improvement over non-frozen baseline)
- The ASR-objective tokenizer produces semantically coherent tokens with competitive WER (demonstrated 3.65% absolute WER improvement over reconstruction-based alternative)

**Medium Confidence Claims:**
- Speech generation quality matches text-guided models (based on UTMOS scores but lacks direct comparison on identical samples)
- The model generalizes to real spoken questions (validated on LLaMA-QA and WebQA but using synthetic fine-tuning data)
- Layer 32 is optimal split point (inferred from similarity analysis but not systematically ablated)

## Next Checks

1. **Layer Split Sensitivity Analysis**: Systematically vary the split point (layers 28, 30, 32, 34) and measure impact on both text retention (MMLU/CMMLU) and speech performance (StoryCloze, QA). This would reveal whether layer 32 is truly optimal or if the architecture is robust to split location.

2. **Real Data Fine-tuning Transfer**: Fine-tune the pre-trained model on a small set of real spoken question-answer pairs (rather than synthetic) and measure performance degradation. This directly tests whether synthetic fine-tuning data adequately prepares the model for real-world deployment.

3. **Acoustic Detail Preservation Test**: Evaluate the model on tasks requiring fine acoustic information (speaker identification, emotion classification, or fine-grained prosody analysis) to quantify what semantic compression costs in terms of acoustic fidelity. Compare against reconstruction-focused tokenization approaches on these benchmarks.