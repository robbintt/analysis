---
ver: rpa2
title: 'ReDSM5: A Reddit Dataset for DSM-5 Depression Detection'
arxiv_id: '2508.03399'
source_url: https://arxiv.org/abs/2508.03399
tags:
- depression
- symptoms
- dsm-5
- symptom
- posts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ReDSM5, a new dataset of 1484 Reddit posts
  annotated at the sentence level by a licensed psychologist for the nine DSM-5 depression
  symptoms, along with clinical rationales. Unlike prior datasets that label entire
  posts as depressed or not, ReDSM5 supports symptom-specific and explainable depression
  detection aligned with clinical practice.
---

# ReDSM5: A Reddit Dataset for DSM-5 Depression Detection

## Quick Facts
- **arXiv ID**: 2508.03399
- **Source URL**: https://arxiv.org/abs/2508.03399
- **Reference count**: 24
- **Primary result**: New dataset of 1484 Reddit posts annotated for DSM-5 depression symptoms with clinical rationales

## Executive Summary
ReDSM5 introduces a novel dataset containing 1484 Reddit posts annotated at the sentence level by a licensed psychologist for the nine DSM-5 depression symptoms, including clinical rationales for each annotation. Unlike previous datasets that label entire posts as depressed or not, ReDSM5 enables symptom-specific and explainable depression detection aligned with clinical practice. The authors provide exploratory linguistic analysis revealing symptom-specific patterns in emotion, grammar, and entity use, and establish baseline performance for both symptom classification and explanation generation using fine-tuned language models.

## Method Summary
The authors collected Reddit posts related to depression and had a licensed psychologist annotate each sentence for the presence of DSM-5 depression symptoms, providing clinical rationales for each annotation decision. The dataset includes detailed symptom-level labels rather than binary post-level classifications, enabling more granular analysis of depression manifestations. Baseline experiments were conducted using fine-tuned language models for both symptom classification (achieving F1 scores up to 0.54 micro, 0.49 macro) and explanation generation (scoring 0.62 on a 0-1 scale using deepseek-r1 as judge).

## Key Results
- Fine-tuned LLM achieves symptom classification F1 scores of 0.54 (micro) and 0.49 (macro)
- Explanation generation quality scores 0.62 on 0-1 scale using deepseek-r1 judge
- Linguistic analysis reveals distinct patterns across different DSM-5 symptoms in emotion, grammar, and entity usage

## Why This Works (Mechanism)
The dataset's clinical grounding through licensed psychologist annotation ensures alignment with diagnostic standards, while the sentence-level granularity captures the heterogeneous nature of depression expression across different symptoms. The inclusion of clinical rationales enables model interpretability and supports explainable AI approaches essential for mental health applications where understanding model decisions is critical for clinician trust and patient safety.

## Foundational Learning
1. **DSM-5 Depression Criteria** - The nine symptom categories (depressed mood, anhedonia, weight changes, sleep disturbances, psychomotor changes, fatigue, worthlessness, concentration problems, suicidal ideation) form the diagnostic framework; needed for understanding symptom boundaries and clinical relevance; quick check: verify all nine symptoms are represented in the dataset
2. **Clinical Annotation Practices** - Single psychologist annotation with rationales represents expert clinical judgment; needed for establishing diagnostic validity; quick check: review annotation guidelines and rationale examples for consistency
3. **Reddit Depression Discourse** - Self-disclosure patterns and community norms on mental health subreddits influence expression styles; needed for contextualizing language patterns; quick check: examine sample posts to understand typical depression discussion formats

## Architecture Onboarding
**Component Map**: Reddit posts -> Sentence segmentation -> DSM-5 symptom annotation -> Clinical rationale generation -> Fine-tuned LLM training -> Performance evaluation
**Critical Path**: Data collection → Expert annotation → Model training → Evaluation
**Design Tradeoffs**: Sentence-level vs. post-level annotation (increased granularity vs. annotation burden), single vs. multiple annotators (clinical expertise vs. inter-rater reliability), Reddit vs. clinical data (natural language vs. diagnostic validity)
**Failure Signatures**: Low inter-annotator agreement on ambiguous symptoms, model confusion between similar symptoms (fatigue vs. psychomotor changes), poor explanation quality indicating superficial symptom detection
**First 3 Experiments**: 1) Inter-rater reliability study on 100 posts with multiple psychologists, 2) Cross-platform validation using depression posts from Twitter/X, 3) Clinical impact assessment comparing model predictions to therapist assessments

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (n=1484 posts) may limit generalizability and model robustness
- Single-annotator approach raises concerns about inter-rater reliability and annotation subjectivity
- Reddit user population may not represent broader depression demographics, introducing selection bias

## Confidence
- **High confidence** in clinical grounding and novelty of sentence-level DSM-5 symptom annotation
- **Medium confidence** in baseline model performance metrics, which show moderate results requiring improvement
- **Low confidence** in generalizability beyond Reddit population and sufficiency of current evaluation metrics for clinical deployment

## Next Checks
1. Conduct inter-rater reliability assessment by having multiple licensed psychologists independently annotate a subset of posts to establish consistency and identify potential annotation ambiguities
2. Perform external validation by testing the dataset and models on depression-related content from other social media platforms or clinical settings to assess generalizability beyond Reddit users
3. Implement and evaluate clinically-informed performance metrics beyond standard F1 scores, including precision-recall tradeoffs at different severity thresholds and assessment of false positive/negative rates in clinically meaningful categories