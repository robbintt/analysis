---
ver: rpa2
title: 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post
  Training'
arxiv_id: '2509.25758'
source_url: https://arxiv.org/abs/2509.25758
tags:
- heads
- reasoning
- attention
- think
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study uses circuit analysis to reveal how post-training methods\
  \ create specialized attention heads that support complex reasoning in language\
  \ models. Different training paradigms\u2014distillation, supervised fine-tuning\
  \ (SFT), and group relative policy optimization (GRPO)\u2014induce distinct patterns\
  \ of emergent heads."
---

# Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training

## Quick Facts
- arXiv ID: 2509.25758
- Source URL: https://arxiv.org/abs/2509.25758
- Authors: Yein Park; Minbyul Jeong; Jaewoo Kang
- Reference count: 40
- Key outcome: Different post-training methods induce distinct emergent attention head patterns, with trade-offs between reasoning sophistication and reliability.

## Executive Summary
This study uses circuit analysis to reveal how post-training methods create specialized attention heads that support complex reasoning in language models. Different training paradigms—distillation, supervised fine-tuning (SFT), and group relative policy optimization (GRPO)—induce distinct patterns of emergent heads. Distillation and SFT steadily add large numbers of reasoning heads, particularly in middle-to-late layers, while GRPO dynamically activates and prunes heads in response to reward signals, leading to sparse but high-impact updates. Controllable "think on/off" models do not have dedicated reasoning heads; instead, disabling explicit reasoning triggers a broader but less efficient set of compensatory heads. Ablation experiments confirm that emergent heads are causally responsible for improved performance but also introduce trade-offs, such as over-thinking on simple tasks.

## Method Summary
The paper employs Edge Attribution Patching with Integrated Gradients (EAP-IG) to identify attention heads contributing to reasoning. Circuits are constructed by comparing clean and corrupted activations, then validated through ablation and scaling experiments. Three post-training methods are analyzed: SFT with 4e-5 learning rate for 5 epochs, distillation from DeepSeek-R1-1.5B, and GRPO with 1e-6-5e-6 learning rates for 3 epochs. Checkpoints are saved every 100 steps. Models tested include Qwen2.5-Math-1.5B-Instruct and Qwen3-8B-Instruct across math reasoning benchmarks (AIME, AMC, MATH-500) and general QA (TriviaQA).

## Key Results
- SFT and distillation induce cumulative addition of stable reasoning heads, especially in middle-to-late layers
- GRPO operates as a dynamic search process, activating and pruning heads based on reward signal fluctuations
- Controllable "think on/off" models lack dedicated reasoning heads; think-off mode activates broader, less efficient compensatory heads
- Emergent heads are causally responsible for improved reasoning performance but introduce trade-offs like over-thinking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distillation and SFT induce cumulative addition of stable reasoning heads in mid-to-late layers
- Mechanism: Direct supervision forces dense parameter updates that install new computational pathways rather than reusing existing ones
- Core assumption: Teacher model's reasoning traces encode specialized computational patterns requiring dedicated heads
- Evidence anchors: Abstract states "Distillation and SFT foster a cumulative addition of stable reasoning heads"; Section 4.2 shows SFT consistently activates additional heads in middle-to-late layers
- Break condition: If ablation of emergent heads does not degrade reasoning performance, or if scaling up these heads fails to produce reasoning-like behavior

### Mechanism 2
- Claim: GRPO operates as dynamic search, iteratively activating and pruning heads based on reward signals
- Mechanism: RL explore-exploit dynamics manifest at circuit level: new head activation represents exploration, retention/pruning represents exploitation
- Core assumption: Reward signals provide meaningful feedback on utility of specific attention head contributions
- Evidence anchors: Abstract describes GRPO as "dynamic search mode"; Section 5 shows head activation fluctuations correlate with accuracy reward curves
- Break condition: If head emergence patterns do not correlate with reward curves across multiple runs, or if final GRPO heads overlap substantially with SFT/distillation heads

### Mechanism 3
- Claim: Controllable "think on/off" models lack dedicated thinking heads; think-off activates broader compensatory set
- Mechanism: Think-on mode represents efficient reasoning pathway sharing components with general instruction-following; think-off compensates with diffuse circuits
- Core assumption: Think-on represents learned efficiency gate, not separate module
- Evidence anchors: Abstract states "Controllable 'think on/off' models do not have dedicated thinking heads"; Section 6 shows circuits share most components between think-on and think-off modes
- Break condition: If ablating think-off heads does not improve low-sample efficiency or if think-on circuits show unique heads absent in think-off

## Foundational Learning

- Concept: **Circuit Analysis via Edge Attribution Patching (EAP-IG)**
  - Why needed here: Traces which attention heads causally contribute to reasoning by attributing output changes to specific edges
  - Quick check question: Can you explain how integrated gradients along a path from corrupted to clean activation yield an edge importance score?

- Concept: **Attention Head Ablation and Scaling**
  - Why needed here: Causal validation depends on surgically disabling or amplifying specific heads to observe performance effects
  - Quick check question: What would you conclude if ablating an emergent reasoning head improved performance on simple tasks but degraded it on complex ones?

- Concept: **Pass@k vs. Success@k Metrics**
  - Why needed here: Separates latent capability (pass@k) from generation efficiency (success@k), critical for interpreting think on/off tradeoffs
  - Quick check question: Why might a model with higher pass@k have lower success@k, and what does this imply about its attention head configuration?

## Architecture Onboarding

- Component map: Transformer DAG (Nodes = attention heads, MLPs, embeddings, logits; Edges = residual stream contributions) -> Circuit Subgraph (sparse nodes/edges selected via EAP-IG scores) -> Emergent Heads (active in post-trained but absent in baseline circuits) -> Think On/Off Gate (token-based control switching between efficient and compensatory modes)

- Critical path: 1. Baseline circuit mapping -> 2. Post-training circuit mapping -> 3. Emergent head identification -> 4. Ablation/scaling validation -> 5. Performance tradeoff analysis

- Design tradeoffs:
  - SFT/Distillation: High head count, stable pathways, potential for overthinking on simple tasks
  - GRPO: Low head count, dynamic adaptation, risk of eroding basic numeracy if reward over-optimizes complex reasoning
  - Think On/Off: Efficiency vs. robustness—compact circuits for efficiency, broad activation for coverage

- Failure signatures:
  - Overthinking: Excessive reasoning chain length on simple tasks, linked to overactive mid-to-late layer heads
  - Compensation collapse: Think-off mode underperforms at low k but recovers at high k due to exploratory breadth
  - Reward misalignment: GRPO heads overfit to dataset style, degrading out-of-distribution generalization

- First 3 experiments:
  1. Replicate emergent head identification: Train Qwen2.5-Math-1.5B with SFT on OpenR1-Math-220k, construct circuits at 100-step checkpoints, compare head counts to baseline
  2. Ablation causality test: Zero out top 10 emergent heads in a distilled model, measure pass@1 drop on AIME and AMC; verify degradation is specific to reasoning benchmarks
  3. Think-off compensation probe: For Qwen3-8B, ablate think-off compensatory heads, compare success@k vs. pass@k curves to confirm efficiency-coverage tradeoff

## Open Questions the Paper Calls Out

- Can training objectives be explicitly designed to encourage targeted head activation and prevent uncontrolled head growth during post-training?
- How can reward shaping jointly optimize both reasoning plan quality and calculation reliability to mitigate over-thinking failure modes?
- Do emergent reasoning head patterns generalize across model families beyond Qwen, or are they architecture-specific?
- What mechanistic role do compensatory heads in "think off" mode play, and can they be selectively suppressed without harming robust exploration?

## Limitations
- Methodological assumption that EAP-IG attribution accurately identifies causally relevant heads may conflate correlation with causation
- Study focuses on relatively small models (1.5B-8B parameters), raising questions about scalability to frontier models
- Circuit analysis methodology has not been validated across diverse model families or tasks beyond mathematical reasoning

## Confidence
- **High Confidence**: Empirical observation that different post-training methods produce distinct head emergence patterns
- **Medium Confidence**: Characterization of GRPO as "dynamic search process" with exploration-exploitation dynamics
- **Low Confidence**: Claim that think on/off models lack dedicated reasoning heads and instead use compensatory circuits

## Next Checks
1. Apply circuit analysis methodology to non-mathematical reasoning tasks to test generalization beyond mathematical domains
2. Replicate key findings using different circuit analysis methods (path-specific attribution, activation patching) to confirm EAP-IG validity
3. Train same post-training methods on larger model variants (34B-70B parameters) to determine whether emergent head patterns scale predictably or exhibit phase transitions