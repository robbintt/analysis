---
ver: rpa2
title: Learning with the $p$-adics
arxiv_id: '2512.22692'
source_url: https://arxiv.org/abs/2512.22692
tags:
- p-adic
- which
- have
- learning
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the potential of p-adic numbers as an alternative
  to real numbers for machine learning. The key contributions are: p-adic classification:
  The paper develops linear and nonlinear binary classifiers in the p-adic domain,
  showing that some problems which cannot be solved by real linear classifiers (such
  as XOR and parity problems) are easy for p-adic linear classifiers, and vice versa.'
---

# Learning with the $p$-adics

## Quick Facts
- arXiv ID: 2512.22692
- Source URL: https://arxiv.org/abs/2512.22692
- Authors: André F. T. Martins
- Reference count: 14
- Primary result: Proposes p-adic numbers as an alternative to real numbers for machine learning, showing they can solve problems like XOR and parity that real linear classifiers cannot

## Executive Summary
This paper explores the potential of p-adic numbers as an alternative to real numbers for machine learning. The author develops fundamental machine learning components—classifiers, regressors, and representations—in the p-adic domain, demonstrating that p-adic classifiers can solve problems like XOR and parity that are challenging for real-valued linear classifiers. The hierarchical structure of p-adic numbers is leveraged for representation learning, with applications to semantic networks. The paper establishes theoretical foundations for p-adic machine learning while identifying key open problems regarding computational efficiency and practical implementation.

## Method Summary
The paper develops p-adic machine learning by adapting standard ML components to the p-adic domain. For classification, it defines linear and nonlinear binary classifiers using p-adic norms and shows they can solve problems intractable for real linear classifiers. For regression, it formulates linear regression in the p-adic domain, proving optimal solutions pass through d+1 training points. For representations, it leverages the hierarchical structure of p-adic numbers to create compact semantic networks. The theoretical framework is built on p-adic arithmetic and ultrametric properties, with propositions establishing classifier capabilities and limitations.

## Key Results
- p-adic linear classifiers can solve XOR and parity problems that real linear classifiers cannot
- Optimal p-adic regressors pass through exactly d+1 training points, unlike real-valued regression
- Simple Quillian semantic networks can be represented as compact p-adic linear networks, which is impossible with real numbers
- The hierarchical structure of p-adic numbers enables efficient representation learning for certain problem classes

## Why This Works (Mechanism)
p-adic numbers possess unique properties that make them suitable for certain machine learning tasks. The key mechanism is their ultrametric distance structure, which creates a hierarchical organization of data points. This hierarchy allows p-adic classifiers to naturally partition data into nested regions, making problems like parity and counting easier to solve than with real-valued classifiers. The discrete nature of p-adic norms also enables compact representations for certain semantic structures that would require larger networks in the real domain.

## Foundational Learning
- **p-adic arithmetic**: Operations in Qp using p-adic norms - needed for implementing ML algorithms in the p-adic domain
- **Ultrametric spaces**: Distance functions satisfying the strong triangle inequality - needed to understand p-adic classifier behavior
- **Discrete norms**: p-adic norm takes discrete values rather than continuous - needed to understand classifier decision boundaries
- **Hierarchical clustering**: p-adic balls form a tree-like structure - needed for understanding representation capabilities
- **Polynomial discriminant functions**: Nonlinear decision boundaries in p-adic space - needed for extending beyond linear classifiers

## Architecture Onboarding

### Component Map
p-adic ML pipeline: Data preparation -> p-adic transformation -> Linear/Nonlinear classifier/regression -> Decision rule -> Output

### Critical Path
The critical path is the transformation from real-valued data to p-adic space and the subsequent classification/regression decision. The choice of prime p and the p-adic representation of input features directly impacts model performance.

### Design Tradeoffs
- **Single vs multiple primes**: Using multiple primes (adelic framework) could improve expressiveness but increases complexity
- **Linear vs nonlinear**: Linear p-adic classifiers are computationally efficient but limited; nonlinear extensions increase expressiveness but complicate learning
- **Discrete vs continuous**: p-adic norms are discrete, enabling compact representations but potentially causing ties in multi-class scenarios

### Failure Signatures
- **Tie-breaking failures**: Multi-class classification may produce ties due to discrete p-adic norms
- **Optimization failures**: Gradient-based methods may not work due to the lack of ordering in Qp
- **Representation failures**: Not all problems benefit from p-adic representations; some may require more complex structures

### First Experiments
1. Implement XOR classification to verify p-adic linear classifiers solve this problem efficiently
2. Test parity problem classification with varying input dimensions
3. Implement and benchmark p-adic regression on synthetic datasets with known noise characteristics

## Open Questions the Paper Calls Out
**Open Question 1**: Can efficient (non-exponential) gradient-based or Newton-style optimization algorithms be developed for learning p-adic classifiers and regressors?
The paper notes current algorithms take exponential runtime with respect to the number of features d. Classical gradient-based optimization doesn't transfer directly because Qp is not an ordered field.

**Open Question 2**: What are suitable non-linear activation functions and learning criteria for multi-layer p-adic neural networks?
The paper only develops linear p-adic predictors and shows multi-layer networks could solve count-thresholding problems, but specific non-linearities and training methods remain unspecified.

**Open Question 3**: How can adelic predictors that combine information from multiple primes (or all primes) be constructed and learned?
The adelic framework is only sketched without concrete learning algorithms or empirical validation for combining multiple p-adic classifiers.

**Open Question 4**: How can native p-adic multi-class classification handle tie-breaking in ultrametric Voronoi cells?
The paper describes the structure of permissible ties but proposes no resolution strategy for the discrete nature of p-adic norms causing frequent ties.

## Limitations
- Current learning algorithms have exponential runtime complexity with respect to feature dimensions
- The paper lacks comprehensive empirical validation on real-world datasets
- Practical implementation details and computational efficiency claims need verification
- Multi-layer p-adic networks and their learning algorithms remain undeveloped

## Confidence
- **High confidence**: Theoretical foundations of p-adic arithmetic and their ultrametric properties
- **Medium confidence**: Propositions regarding classifier capabilities for specific problem classes (XOR, parity, congruence problems)
- **Low confidence**: Practical implementation details and computational efficiency claims

## Next Checks
1. Benchmark p-adic classifiers against standard approaches on diverse datasets (MNIST, CIFAR-10) to quantify computational efficiency and accuracy trade-offs
2. Implement and test the proposed regression algorithm on real-world datasets with known noise characteristics
3. Develop and evaluate multi-layer p-adic networks on complex tasks requiring hierarchical feature learning