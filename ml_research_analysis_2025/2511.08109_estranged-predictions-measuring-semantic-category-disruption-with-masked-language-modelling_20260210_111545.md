---
ver: rpa2
title: 'Estranged Predictions: Measuring Semantic Category Disruption with Masked
  Language Modelling'
arxiv_id: '2511.08109'
source_url: https://arxiv.org/abs/2511.08109
tags:
- fiction
- semantic
- masked
- science
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses masked language modelling (MLM) to quantify conceptual
  permeability across the ontological categories human, animal, and machine in science
  fiction versus general fiction. Sentences containing target terms were masked and
  fed to RoBERTa, which generated top-5 lexical predictions.
---

# Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling

## Quick Facts
- arXiv ID: 2511.08109
- Source URL: https://arxiv.org/abs/2511.08109
- Reference count: 40
- Primary result: Science fiction exhibits significantly higher conceptual permeability than general fiction, especially for machine terms

## Executive Summary
This paper uses masked language modeling to quantify how science fiction disrupts ontological categories compared to general fiction. By masking terms like "human," "animal," and "machine" in sentences and analyzing what lexical predictions RoBERTa generates, the study measures conceptual stability across genres. Results show science fiction creates more permeable boundaries between categories, with machine terms being particularly destabilized—often predicted as human-related words in SF contexts. The framework provides a computational method for detecting genre-conditioned ontological assumptions in literary texts.

## Method Summary
The methodology extracts sentences containing target ontological terms from two corpora (Gollancz SF Masterworks and NovelTM general fiction), masks each target term, and uses roberta-base to generate top-5 predictions. A Gemini LLM classifies each prediction into ontological categories (Human, Animal, Machine, or Others). Three metrics are computed: retention rate (same-category predictions), replacement rate (cross-category predictions), and entropy (predictive uncertainty). The study compares these metrics across categories and genres using permutation tests and two-way ANOVA.

## Key Results
- Science fiction shows significantly higher conceptual permeability than general fiction across all ontological categories
- Machine terms in SF exhibit the highest entropy and lowest retention, frequently predicted as human-related words
- Directional asymmetry: machine-to-human substitutions far exceed human-to-machine across both genres
- Entropy analysis confirms machine referents in SF elicit the broadest semantic dispersion

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masked language modeling with RoBERTa reveals genre-specific conceptual permeability by quantifying substitution probabilities for ontological category terms within sentential contexts
- **Mechanism:** Masking target terms and extracting top-k predictions surfaces latent semantic expectations. When "human-like" substitutions occur for masked "machine" tokens, this indicates the linguistic environment has made the category boundary permeable. Comparing substitution patterns across corpora quantifies how different genres destabilize categorical expectations
- **Core assumption:** RoBERTa's pre-training on general English encodes normative semantic associations and animacy hierarchies
- **Evidence anchors:** Abstract mentions "using RoBERTa to generate lexical substitutes"; section 4.4.2 defines replacement rate as cross-category predictions
- **Break condition:** If RoBERTa's pre-training data already contains substantial science fiction, baseline expectations would incorporate "estranged" patterns

### Mechanism 2
- **Claim:** Gemini classifies predicted tokens into ontological categories when provided sentential context, enabling systematic quantification of cross-category substitution rates
- **Mechanism:** Gemini receives the masked sentence plus each predicted token, assigning semantic categories based on contextual meaning rather than lexical form alone. This converts unstructured predictions into analyzable categorical data
- **Core assumption:** Gemini's classification judgments align with human reader interpretations of ontological category membership
- **Evidence anchors:** Section 4.3.2 describes quality control ensuring 98% coverage; Gemini's post-classification fusion consolidates semantically adjacent labels
- **Break condition:** If Gemini exhibits systematic classification drift (e.g., over-classifying as "Human"), replacement rates would be artificially inflated

### Mechanism 3
- **Claim:** Entropy over top-k predictions quantifies predictive uncertainty, serving as a proxy for conceptual stability of masked terms
- **Mechanism:** High entropy indicates the model distributes probability mass evenly across multiple candidates, suggesting the context doesn't strongly constrain the masked term's semantic field
- **Core assumption:** Entropy reflects semantic dispersion rather than noise, and the top-5 window captures meaningful variation
- **Evidence anchors:** Section 4.4.3 defines high entropy as indicating looser context; section 5.1 shows masked machine tokens in SF exhibiting highest mean entropy
- **Break condition:** If top-5 predictions include many low-probability tokens due to model artifacts, entropy would conflate noise with meaningful dispersion

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - **Why needed here:** The entire pipeline depends on understanding how MLMs generate context-conditioned predictions for masked tokens, and why prediction distributions reveal semantic expectations
  - **Quick check question:** Given "The [MASK] barked loudly," what factors determine whether a model predicts "dog" vs. "man" vs. "machine"?

- **Concept: Distributional Semantics**
  - **Why needed here:** The theoretical framework assumes meaning emerges from contextual associations. Understanding this clarifies why substitution patterns reveal conceptual boundaries rather than dictionary definitions
  - **Quick check question:** If "machine" appears in sentence structures typically associated with "human" agents, what does distributional semantics predict about the term's acquired meaning?

- **Concept: Animacy Hierarchies in Language**
  - **Why needed here:** The paper interprets results through Silverstein's animacy hierarchy (human > animal > machine). Understanding this explains why "human" terms show higher retention and why machine-to-human substitutions predominate
  - **Quick check question:** In subject position, why do languages grammatically privilege human referents over inanimate ones, and how might this affect MLM predictions?

## Architecture Onboarding

- **Component map:** Sentence Extraction Module -> Masking Layer -> RoBERTa-base MLM -> Gemini Classification Pipeline -> Metrics Computation -> Statistical Testing
- **Critical path:** Sentence extraction → Masking → RoBERTa prediction → Gemini classification → Metric aggregation → Statistical comparison across corpora
- **Design tradeoffs:**
  - Equal-weight vs. probability-weighted aggregation: Method 1 (equal weight) amplifies marginal cross-category signals; Method 2 preserves probability structure but may suppress weak permeability signals
  - Top-k window size: k=5 balances coverage with noise; smaller k increases precision but misses low-probability substitutions
  - LLM classifier dependency: Using Gemini avoids manual annotation but introduces model-specific classification biases
- **Failure signatures:**
  - Low classification coverage (<98%): Indicates Gemini API failures or prompt formatting issues; check retry logs
  - Near-identical retention rates across categories: Suggests masking pipeline not functioning or RoBERTa returning degenerate predictions
  - Negative entropy values: Mathematically impossible; indicates probability extraction error
- **First 3 experiments:**
  1. Validation run on small sample (n=100 sentences): Verify masking, prediction extraction, and classification work end-to-end before full corpus processing
  2. Ablation on classification method: Compare Gemini classifications against human annotations on 200 random predictions to estimate classification error rate
  3. Sensitivity analysis on k: Run pipeline with k=3, k=5, k=10 to assess whether entropy and replacement rate findings are robust to prediction window size

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the framework's detection of conceptual permeability change when applied to alternative ontological binaries or thematic categories?
  - **Basis in paper:** The conclusion states that the lexical extraction step "can be readily adapted to explore alternative conceptual binaries"
  - **Why unresolved:** The current study limited its scope to the specific triad of human, animal, and machine to validate the methodology against established science fiction theory
  - **What evidence would resolve it:** Applying the identical pipeline to other conceptual oppositions (e.g., natural/artificial, self/other) and analyzing the resulting retention and replacement rates

- **Open Question 2:** To what extent do the ontological classifications generated by the Gemini model align with human expert categorization?
  - **Basis in paper:** The methodology relies entirely on Gemini for classifying predicted tokens into semantic categories, yet the paper notes that models can naturalize hierarchies
  - **Why unresolved:** While quality control checks ensured coverage, the paper does not validate whether Gemini's "Other-Hybrid" or "Other-Machine" distinctions map accurately onto literary-theoretical definitions
  - **What evidence would resolve it:** A validation study comparing Gemini's classifications against human annotators for a subset of predictions to calculate inter-rater agreement scores

- **Open Question 3:** Would a masked language model fine-tuned specifically on science fiction text exhibit lower entropy and higher retention for machine terms?
  - **Basis in paper:** The study uses roberta-base as an instrument of "normativity" to detect estrangement, defined as deviation from standard English
  - **Why unresolved:** It is unclear if the observed high entropy for "machine" in SF is an intrinsic property of the genre's language or a result of comparing a specialized genre against a general-purpose model
  - **What evidence would resolve it:** Fine-tuning a RoBERTa model on the Gollancz SF corpus and re-running the masking task to see if the model "normalizes" the category disruptions

## Limitations
- The study relies on pre-trained language model assumptions that may not hold if RoBERTa's training corpus already contains substantial science fiction content
- Gemini classification pipeline effectiveness depends entirely on prompt design and fusion rules that are not fully specified
- The methodology cannot distinguish between genuine semantic shifts and classification artifacts

## Confidence
- **High confidence**: The methodological framework for measuring conceptual permeability via masked language modeling is sound and the entropy findings for machine terms in science fiction are robust
- **Medium confidence**: The observed genre differences in retention and replacement rates depend on the validity of Gemini's classification accuracy
- **Medium confidence**: The directional asymmetry findings align with theoretical expectations but require validation that Gemini isn't systematically biased toward human classifications

## Next Checks
1. Conduct human annotation validation: Have three independent annotators classify 500 randomly selected RoBERTa predictions from both genres to estimate Gemini's classification accuracy and identify systematic biases
2. Test RoBERTa's genre exposure: Analyze RoBERTa's training corpus composition to determine if science fiction content was present and quantify its potential impact on baseline semantic expectations
3. Verify corpus composition: Confirm the exact publication date distributions and genre labeling methodology for both the Gollancz SF Masterworks and NovelTM samples to ensure the genre comparison is valid