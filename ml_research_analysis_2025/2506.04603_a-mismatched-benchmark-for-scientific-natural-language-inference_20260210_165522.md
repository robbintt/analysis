---
ver: rpa2
title: A MISMATCHED Benchmark for Scientific Natural Language Inference
arxiv_id: '2506.04603'
source_url: https://arxiv.org/abs/2506.04603
tags:
- sentence
- mismatched
- pairs
- scientific
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MISMATCHED, a benchmark for scientific Natural
  Language Inference (NLI) that addresses the lack of diversity in existing datasets
  by covering non-computer science domains like psychology, engineering, and public
  health. The benchmark contains 2,700 human-annotated sentence pairs and is designed
  as an out-of-domain testbed to evaluate model robustness.
---

# A MISMATCHED Benchmark for Scientific Natural Language Inference

## Quick Facts
- arXiv ID: 2506.04603
- Source URL: https://arxiv.org/abs/2506.04603
- Authors: Firoz Shaik; Mobashir Sadat; Nikita Gautam; Doina Caragea; Cornelia Caragea
- Reference count: 21
- Primary result: MISMATCHED benchmark achieves best SLM performance of 78.17% Macro F1 with SciBERT

## Executive Summary
This paper introduces MISMATCHED, a benchmark for scientific Natural Language Inference (NLI) that addresses the lack of diversity in existing datasets by covering non-computer science domains like psychology, engineering, and public health. The benchmark contains 2,700 human-annotated sentence pairs and is designed as an out-of-domain testbed to evaluate model robustness. The authors establish strong baselines using both small language models (SLMs) and large language models (LLMs), with the best SLM (SCIBERT) achieving a Macro F1 of 78.17%, indicating substantial room for improvement. They also demonstrate that incorporating implicit relations—sentence pairs without explicit linking phrases—into model training can improve performance on scientific NLI tasks.

## Method Summary
The MISMATCHED benchmark was constructed by collecting scientific articles from non-computer science domains (psychology, engineering, public health), extracting sentence pairs using distant supervision with linking phrases, and then having human annotators classify semantic relationships. The benchmark includes 2,700 sentence pairs annotated across four classes: ENTAILMENT, REASONING, CONTRASTING, and NEUTRAL. Baseline models were trained using fine-tuning approaches for SLMs (BERT, SciBERT, RoBERTa, XLNet) and few-shot prompting for LLMs (Llama-2/3, Mistral, Phi-3, GPT-4o, Gemini). Models were evaluated on both in-domain and out-of-domain settings to measure robustness.

## Key Results
- SCIBERT achieves 78.17% Macro F1 on MISMATCHED, outperforming generic pre-trained models
- Fine-tuning on combined CS training data (MSCINLI+) improves performance over individual datasets
- Incorporating implicit discourse relations improves performance by 1.5% for SCIBERT
- REASONING class is consistently the most challenging (F1: 44-75%)
- LLMs perform significantly worse than SLMs (57-63% vs 78%) despite no fine-tuning requirement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training on scientific text improves OOD generalization on non-CS scientific domains
- Mechanism: SciBERT, pre-trained on 1.14M scientific papers, outperforms both BERT and RoBERTa on MISMATCHED
- Core assumption: Linguistic structures in CS research papers transfer to non-CS domains
- Evidence anchors: Section 4.2 shows SciBERT outperforms RoBERTa; Table 4 shows 78.17% vs 78.11% overall

### Mechanism 2
- Claim: Training data diversity (combining multiple CS subdomains) improves OOD robustness
- Mechanism: Models fine-tuned on MSCINLI+ (228K pairs) consistently outperform those trained on individual datasets
- Core assumption: Diversity across CS subdomains generalizes to non-CS domains
- Evidence anchors: Section 4.2 shows consistent improvements; Table 5 shows SciBERT improving from 76.27% to 78.17%

### Mechanism 3
- Claim: Incorporating implicit discourse relations during training improves NLI performance
- Mechanism: Adding implicit pairs (2:1 ratio) exposes models to natural scientific writing without explicit connectives
- Core assumption: Implicit relations contain valid semantic relations that can be reliably pseudo-labeled
- Evidence anchors: Section 5 shows SCIBERT improves by 1.5%; Table 9 shows 79.66% vs 78.17%

## Foundational Learning

- Concept: Natural Language Inference (NLI) as 4-way classification (ENTAILMENT, REASONING, CONTRASTING, NEUTRAL)
  - Why needed here: Scientific NLI adds REASONING class, which is hardest (F1: 75.09% vs 80.94% for CONTRASTING)
  - Quick check question: Can you distinguish "Sentence A causes Sentence B" (REASONING) from "Sentence A is restated in Sentence B" (ENTAILMENT)?

- Concept: Out-of-domain (OOD) evaluation via mismatched testbeds
  - Why needed here: MISMATCHED contains no training data—it measures robustness, not memorization
  - Quick check question: If your model achieves 90% on the test set but was trained on papers from the same venue, is that ID or OOD evaluation?

- Concept: Distant supervision via linking phrases
  - Why needed here: Training data is automatically constructed by extracting adjacent sentences where the second begins with connectives
  - Quick check question: Why might a model trained only on explicit relations struggle with sentences lacking connectives?

## Architecture Onboarding

- Component map: Paper collection from WoS/PubMed -> Distant supervision extraction via linking phrase patterns -> Human annotation via COGITO (3 annotators, Fleiss κ=0.72) -> Paper-level split (dev 300 / test 2400)

- Critical path: 1) Fine-tune SciBERT on MSCINLI+ (4 hours on RTX A5000, 5 epochs, early stopping on MISMATCHED dev) 2) Evaluate on MISMATCHED test (paper-level split prevents leakage) 3) Optional: Add implicit relations (pseudo-label with confidence >0.6, then two-stage fine-tuning with lr=2e-6 on second stage)

- Design tradeoffs: SLM vs LLM: SLMs achieve 78% F1 with fine-tuning; LLMs achieve only 57-63% even with few-shot. Zero-shot vs few-shot: 12-shot achieves best LLM performance; 16-shot degrades.

- Failure signatures: REASONING class consistently lowest (F1: 44-75%)—confused with ENTAILMENT and CONTRASTING; RoBERTa shows 2-3% ID→OOD drop; LLM zero-shot near random (28-55% F1)

- First 3 experiments: 1) Baseline: Fine-tune SciBERT on MSCINLI+, evaluate on MISMATCHED. Target: ~78% Macro F1. 2) Ablation: Train on SciNLI only vs MSciNLI only vs MSCINLI+. Measure per-domain F1. 3) Implicit relations: Extract implicit pairs, pseudo-label with threshold 0.6, two-stage fine-tune. Target: 1-2% gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific domain adaptation methods effectively bridge the performance gap between CS-based training data and non-CS evaluation sets?
- Basis in paper: The authors state, "In our future work, we will develop domain adaptation methods for scientific NLI to improve the performance on the MISMATCHED set."
- Why unresolved: Current models rely on training data from computer science domains, leading to measurable performance drop on out-of-domain MISMATCHED set.
- What evidence would resolve it: A domain adaptation technique that significantly raises Macro F1 on MISMATCHED closer to in-domain baselines.

### Open Question 2
- Question: How do implicit scientific relations manifest in other non-CS disciplines, such as Physics or Chemistry?
- Basis in paper: The "Limitations" section notes, "There are numerous scientific domains and disciplines (e.g., Physics, Chemistry, etc.) that are not covered by our dataset."
- Why unresolved: Current benchmark is restricted to Psychology, Engineering, and Public Health, leaving generalizability to physical sciences unknown.
- What evidence would resolve it: Construction of evaluation sets for Physics and Chemistry showing similar or distinct error profiles.

### Open Question 3
- Question: What specific linguistic or semantic indicators can models learn to better distinguish "Reasoning" relations from "Entailment"?
- Basis in paper: The paper presents a confusion matrix showing "Reasoning" class is most challenging, often confused with "Entailment," yet offers no solution for this distinction.
- Why unresolved: While paper identifies reasoning is difficult, it does not investigate underlying causes of confusion or propose mitigation methods.
- What evidence would resolve it: An ablation study or model architecture that improves F1 score for "Reasoning" class specifically.

## Limitations

- LLM evaluation methodology is limited—prompts were not optimized for scientific domain, likely underestimating true capability
- Implicit relation pseudo-labeling may introduce bias since teacher model is fine-tuned on same training data that benefits from implicit pairs
- 0.6 confidence threshold for selecting implicit pairs is arbitrary without systematic sensitivity analysis

## Confidence

**High Confidence**: SciBERT outperforms generic pre-trained models; combined training (mSciNLI+) improves performance; human annotation quality is sufficient

**Medium Confidence**: Incorporating implicit relations improves performance; OOD evaluation reveals robustness; models struggle most with REASONING class

**Low Confidence**: LLM performance estimates are accurate; 2:1 ratio of implicit to explicit pairs is optimal; 12-shot exemplars are ideal number

## Next Checks

1. **Prompt Optimization Study**: Systematically evaluate different prompt templates and exemplar selections for each LLM to establish baseline performance

2. **Teacher Model Bias Analysis**: Compare implicit relation pseudo-labels from fine-tuned teacher model against human annotations on subset of implicit pairs

3. **Domain Transfer Robustness**: Test model performance on MISMATCHED when training data is restricted to individual CS subdomains to isolate contribution of domain diversity