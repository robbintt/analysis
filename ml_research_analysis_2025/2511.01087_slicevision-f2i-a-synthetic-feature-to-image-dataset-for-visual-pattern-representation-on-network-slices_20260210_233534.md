---
ver: rpa2
title: 'SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation
  on Network Slices'
arxiv_id: '2511.01087'
source_url: https://arxiv.org/abs/2511.01087
tags:
- network
- dataset
- visual
- slicevision-f2i
- pattern
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents SliceVision-F2I, a synthetic dataset for visual\
  \ representation of network slice KPIs, addressing the challenge of accurate slice\
  \ classification in noisy 5G/6G environments. The dataset transforms multivariate\
  \ KPI vectors into 16\xD716 RGB images using four encoding methods\u2014physically\
  \ guided, Perlin noise, neural wallpapering, and fractal branching\u2014with 30,000\
  \ samples per method."
---

# SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices

## Quick Facts
- **arXiv ID**: 2511.01087
- **Source URL**: https://arxiv.org/abs/2511.01087
- **Reference count**: 8
- **Primary result**: Visual pattern-based CNNs achieve perfect classification accuracy on synthetic network slice KPIs, significantly outperforming traditional ML methods, especially for minority URLLC class.

## Executive Summary
This paper presents SliceVision-F2I, a synthetic dataset for visual representation of network slice KPIs, addressing the challenge of accurate slice classification in noisy 5G/6G environments. The dataset transforms multivariate KPI vectors into 16×16 RGB images using four encoding methods—physically guided, Perlin noise, neural wallpapering, and fractal branching—with 30,000 samples per method. Each sample includes a raw KPI vector and its corresponding image, simulating realistic noise and missing data. The authors demonstrate that visual pattern-based CNNs achieve perfect classification accuracy, significantly outperforming traditional ML methods on raw KPIs, particularly for the minority URLLC class. This establishes visual abstraction as a robust paradigm for network analytics, enabling real-time processing and interpretability. The dataset is publicly available and supports applications in anomaly detection, classification, and benchmarking of vision-based ML techniques for network management.

## Method Summary
The dataset uses a KPI simulator to generate synthetic 10-dimensional KPI vectors (delay, jitter, loss, throughput, retransmissions, discard, RSSI, SNR, CPU/memory) for three network slice types (eMBB, URLLC, mIoT) based on 3GPP TR 28.801 specifications. The simulator incorporates Gaussian noise (σ=0.1–0.3), 5% MAR missing data, 2% cross-slice contamination, and Weibull outliers. Four encoding methods transform these KPI vectors into 16×16 RGB images: physically-guided mappings (domain-inspired), Perlin noise patterns (procedural), neural wallpapering (symmetric patterns), and fractal branching (recursive structures). The dataset provides 30,000 samples per encoding method with class distribution 20%/10%/70% (eMBB/URLLC/mIoT), including both raw KPI CSV files and image NPY files.

## Key Results
- Visual pattern-based CNNs achieve perfect classification accuracy (F1=1.00) on encoded images, significantly outperforming traditional ML methods on raw KPIs
- All CNN-based approaches handle class imbalance better than traditional methods, with URLLC F1 improving from 0.64-0.70 to 1.00
- Mathematically-generated patterns (Perlin noise, wallpaper, fractal) consistently outperform physically-inspired mappings (94.6% vs 99.85-100% accuracy)
- The 16×16 resolution balances classification accuracy with real-time processing requirements for network analytics

## Why This Works (Mechanism)

### Mechanism 1
Visual encoding of multivariate KPIs enables CNNs to capture discriminative patterns that traditional ML methods miss when operating on raw numerical vectors. The four encoding methods transform 10-dimensional KPI vectors into spatially-structured 16×16×3 tensors where inter-KPI relationships become geometric and textural patterns. CNNs then exploit spatial convolution to learn hierarchical features that capture these latent relationships. The visual encoding functions preserve or amplify class-discriminative information while filtering noise that confuses traditional classifiers. Break condition: If encoding functions become non-injective (different KPI vectors map to identical images), discriminative information is lost and performance degrades to or below raw KPI baselines.

### Mechanism 2
Structurally-generated patterns (Perlin, wallpaper, fractal) outperform physically-inspired mappings because mathematical regularities create more consistent class-specific visual signatures. Procedural generation methods with well-defined mathematical properties (octaves, persistence, symmetry groups, recursion depth) produce patterns where slice-type KPI distributions map to consistent spatial structures. Physically-guided mappings rely on domain-inspired heuristics that may not optimize for discriminability. Break condition: If KPI distributions shift beyond training range, mathematical patterns may produce out-of-distribution artifacts that degrade generalization.

### Mechanism 3
Visual encoding provides robustness to class imbalance by creating more distinctive representations for minority classes (URLLC). Traditional ML on raw KPIs struggles with URLLC (10% prevalence) because feature overlap in 10D space creates poor decision boundaries. Visual encoding amplifies URLLC's distinctive characteristics (low delay, low jitter) into high-frequency, high-contrast patterns that CNNs can discriminate even with limited samples. Break condition: If minority class KPIs have high variance or overlap significantly with majority classes, visual encoding may not create sufficient discriminability.

## Foundational Learning

- **Network Slicing KPIs (3GPP TR 28.801)**: Understanding how delay, jitter, packet loss, throughput differ across eMBB/URLLC/mIoT is essential for interpreting why visual patterns separate classes. Quick check: Given eMBB prioritizes throughput while URLLC prioritizes latency, which KPI values would you expect to be lowest for URLLC slices?

- **Procedural Pattern Generation (Perlin Noise, L-systems)**: The encoding methods use procedural generation techniques with controllable parameters; understanding frequency, octaves, persistence, and recursion depth explains why patterns are slice-discriminative. Quick check: If delay controls Perlin noise frequency (fr = 10(1 + δ̃ + ȷ̃)), would high-delay slices produce coarser or finer textures?

- **CNN Inductive Biases for Spatial Patterns**: CNNs assume spatial locality and translation equivariance; the success of this approach depends on whether encoding methods produce patterns where these biases are useful. Quick check: If a KPI encoding placed all discriminative information in a single pixel, would a CNN with 3×3 kernels be effective?

## Architecture Onboarding

- **Component map**: KPI Simulator (Eq. 2-4) → Encoding Layer (4 parallel methods) → CNN Classifier → Evaluation Framework (stratified 80-20 split, F1 per class)

- **Critical path**: 1. Validate KPI simulation produces realistic slice distributions (check Table 3 parameters) 2. Verify encoding functions correctly map KPIs to images (inspect sample outputs) 3. Train CNN on encoded images → monitor URLLC F1 specifically 4. Compare against raw-KPI ML baselines (Table 4)

- **Design tradeoffs**: Resolution vs. latency: 16×16 chosen for real-time; higher resolution may improve marginal accuracy but increases inference cost. Encoding complexity vs. interpretability: Physically-guided most interpretable but lowest accuracy; fractal/wallpaper highest accuracy but less physically grounded. Synthetic vs. real data: Controlled noise enables reproducibility but requires validation on operational traces before deployment.

- **Failure signatures**: URLLC F1 < 0.75 with any encoding → check class imbalance handling, encoding parameter ranges. Traditional ML matching CNN performance → encoding may not be injective or CNN architecture is underfitting. Large gap between train/test accuracy → overfitting to synthetic noise patterns, insufficient noise diversity.

- **First 3 experiments**: 1. Reproduce baseline comparison: Train Random Forest on raw KPIs vs. simple CNN on wallpaper-encoded images; verify URLLC F1 improvement is >30%. 2. Encoding ablation: Train identical CNN architecture on each of the 4 encoding methods; confirm fractal/wallpaper ≥ Perlin ≥ physically-guided. 3. Noise robustness test: Incrementally increase Gaussian noise σ from 0.1 to 0.5 and plot accuracy degradation for best CNN vs. best traditional ML; expect CNN to degrade more gracefully.

## Open Questions the Paper Calls Out

- **Domain transfer**: How do visual pattern-based classifiers perform on real-world network traces compared to the synthetic SliceVision-F2I dataset? The paper states validation with actual network traces is still necessary for practical deployment. This remains unresolved because all experiments used synthetic data generated from statistical distributions derived from 3GPP specifications; no real network measurements were evaluated.

- **Hybrid approaches**: Can hybrid approaches combining traditional ML with pattern-based techniques improve performance or computational efficiency over pure CNN methods? The paper suggests future research should look into hybrid approaches that combine traditional machine learning with pattern-based techniques. This remains unresolved because only standalone CNN and traditional ML methods were compared; no hybrid architectures were explored.

- **Encoding performance gap**: Why do physically-guided patterns achieve lower accuracy (94.6%) compared to mathematically-generated patterns (99.85-100%)? The paper notes this consistent ~5% accuracy gap but does not analyze whether the gap stems from encoding granularity, channel interaction terms, or noise sensitivity. This remains unresolved because no ablation studies isolating individual KPI-to-channel mappings or noise robustness tests per encoding method were conducted.

## Limitations
- All experiments use synthetic data generated from statistical distributions; no real-world network traces were evaluated for domain transfer validation
- Perfect classification accuracy (1.0000 F1) achieved by wallpaper and fractal methods appears suspiciously high given stated 2% contamination, suggesting potential overfitting to synthetic noise patterns
- The paper doesn't address temporal dynamics, bursty traffic patterns, or evolving class distributions that occur in real network slice deployments

## Confidence

- **Visual encoding improves slice classification**: Medium confidence (synthetic validation only)
- **Procedural patterns outperform physical mappings**: Low confidence (insufficient ablation analysis)
- **Visual methods handle class imbalance**: Medium confidence (static imbalance, no temporal dynamics)
- **Dataset enables real-time processing**: High confidence (resolution vs. latency tradeoff clearly specified)

## Next Checks

1. **Domain transfer test**: Evaluate the best-performing encoding (wallpaper/fractal) on real operational KPI traces from multiple 5G operators. Measure performance degradation relative to synthetic data and identify which noise characteristics in real data break the visual encoding assumptions.

2. **Parameter sensitivity analysis**: Systematically vary key encoding parameters (Perlin octaves, wallpaper symmetry groups, fractal recursion depth) across their full ranges. Map performance changes to specific pattern characteristics to understand which mathematical properties are essential for discriminability.

3. **Temporal drift robustness**: Simulate KPI distribution shifts over time (e.g., 5% monthly drift in delay/jitter distributions) and measure classification accuracy decay. Compare visual encoding methods against traditional ML under continuous distribution shift to validate real-time deployment claims.