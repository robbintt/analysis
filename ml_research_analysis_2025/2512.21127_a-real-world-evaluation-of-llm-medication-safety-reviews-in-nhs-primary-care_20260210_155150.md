---
ver: rpa2
title: A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care
arxiv_id: '2512.21127'
source_url: https://arxiv.org/abs/2512.21127
tags:
- system
- patient
- intervention
- clinical
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated an LLM-based medication safety review system
  using real NHS primary care EHR data from 277 patients. The system achieved 100%
  sensitivity in detecting clinically significant issues but correctly identified
  all issues and proposed appropriate interventions in only 46.9% of patients.
---

# A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care

## Quick Facts
- arXiv ID: 2512.21127
- Source URL: https://arxiv.org/abs/2512.21127
- Reference count: 40
- Primary result: LLM achieved 100% sensitivity in detecting medication safety issues but correctly identified all issues and proposed appropriate interventions in only 46.9% of patients

## Executive Summary
This study evaluated an LLM-based medication safety review system using real NHS primary care EHR data from 277 patients. The system achieved perfect sensitivity in detecting clinically significant medication safety issues but only correctly identified all issues and proposed appropriate interventions in 46.9% of patients. Failure analysis revealed that contextual reasoning errors (overconfidence in uncertainty, misapplying guidelines without patient context, misunderstanding healthcare delivery, factual errors, and process blindness) outnumbered factual errors 6:1. Performance declined with patient complexity across age, medication count, and comorbidity burden. These findings highlight that while LLMs can reliably detect medication safety concerns, the dominant failure modes involve contextual reasoning rather than knowledge gaps, suggesting that addressing these issues requires better calibration of uncertainty and improved contextual understanding rather than simply expanding medical knowledge bases.

## Method Summary
The study used gpt-oss-120b (120B parameters) with Inspect evaluation framework to analyze structured EHR data from 277 patients (selected from 2.1M adults in NHS Cheshire and Merseyside). Patient profiles containing demographics, SNOMED-coded diagnoses, medications (dm+d codes), lab results, hospital episodes, and observations were converted to chronologically ordered markdown. The system performed single-pass inference without external knowledge sources, producing structured JSON outputs. Evaluation used hierarchical 3-level scoring by a single clinician reviewer: Level 1 (issue detection sensitivity/specificity), Level 2 (issue correctness), Level 3 (intervention appropriateness), with composite score combining F1 for issues and intervention appropriateness.

## Key Results
- 100% sensitivity [95% CI 98.2–100] in detecting clinically significant medication safety issues
- 46.9% [95% CI 41.1–52.8] of patients had all issues correctly identified and appropriate interventions proposed
- 6:1 ratio of contextual reasoning failures to factual errors, with five identified failure patterns dominating: overconfidence in uncertainty (n=51), protocol vs patient gap (n=49), protocol vs practice gap (n=30), factual errors (n=25), and process blindness (n=23)

## Why This Works (Mechanism)

### Mechanism 1: Detection-Action Decomposition with Hierarchical Degradation
- Claim: LLMs can achieve high sensitivity for issue detection while failing at correct issue identification and intervention appropriateness.
- Mechanism: The system separates signal detection (recognizing something is wrong) from clinical reasoning (determining what specifically is wrong and what to do). Detection relies on pattern recognition over medical knowledge; appropriate action requires contextual integration of patient-specific factors, healthcare delivery knowledge, and sequencing logic.
- Core assumption: Detection and action capabilities scale differently with patient complexity and task structure.
- Evidence anchors:
  - [abstract]: "sensitivity 100% [95% CI 98.2–100], specificity 83.1%... yet correctly identified all issues and interventions in only 46.9% [95% CI 41.1–52.8] of patients"
  - [section 3.1]: Level 1 achieved 100% sensitivity; Level 2 correct issue identification in 58.7%; Level 3 appropriate interventions in 58.7% of passing cases
  - [corpus]: RxSafeBench similarly found LLM medication safety issues in simulated consultations, supporting that detection-action gaps generalize
- Break condition: If detection sensitivity dropped below clinician-level thresholds, the hierarchical framework would fail to reveal the knowledge-application gap

### Mechanism 2: Contextual Reasoning Failure Dominance Over Knowledge Gaps
- Claim: The dominant failure mode is contextual reasoning (86% of failures), not factual knowledge gaps (14%), contradicting assumptions that medical AI fails primarily from missing knowledge.
- Mechanism: The LLM possesses sufficient pharmacological and guideline knowledge but fails to apply it appropriately to individual patient contexts. Five failure patterns: overconfidence in uncertainty (n=51), protocol vs patient gap (n=49), protocol vs practice gap (n=30), factual errors (n=25), process blindness (n=23).
- Core assumption: Medical training data contains knowledge but lacks implicit healthcare delivery context and patient-specific reasoning patterns.
- Evidence anchors:
  - [abstract]: "failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge"
  - [section 3.2]: "contextual reasoning failures outnumbered factual errors 6:1"
  - [corpus]: Kim et al. (cited in paper) found 64-72% of medical hallucinations stem from reasoning failures rather than knowledge gaps
- Break condition: If RAG or medical fine-tuning significantly improved performance, knowledge gaps would be implicated as primary cause

### Mechanism 3: Patient Complexity as Unified Performance Predictor
- Claim: Performance degrades with patient complexity as a single latent construct, not as independent effects of age, medications, or comorbidities.
- Mechanism: Age, medication count, and QoF register count are highly intercorrelated (r=0.56-0.67). Multiple regression showed no variable retained independent significance when controlling for others (R²=0.10), suggesting complexity manifests through interconnected factors that jointly tax contextual reasoning.
- Core assumption: Complex patients require more contextual integration, exposing reasoning limitations invisible in simpler cases.
- Evidence anchors:
  - [section 3.3]: "multiple regression revealed no variable retained a statistically significant independent effect... The three variables collectively explained only 10% of variance"
  - [figure 3]: Correlation matrix shows high intercorrelation among complexity measures
  - [corpus]: Weak corpus evidence for this specific mechanism in medication safety
- Break condition: If complexity measures showed independent effects, targeted interventions could focus on specific complexity dimensions

## Foundational Learning

- Concept: Hierarchical evaluation frameworks for clinical AI
  - Why needed here: Binary classification metrics (sensitivity/specificity) mask where in the clinical reasoning pipeline failures occur; this paper uses 3-level evaluation to expose degradation patterns
  - Quick check question: Can you explain why 100% sensitivity with 46.9% fully correct outputs doesn't contradict each other?

- Concept: Prescribing Safety Indicators (PINCER methodology)
  - Why needed here: Used for case sampling and ground truth; 30.1% clinician-indicator disagreement shows deterministic criteria miss contextual judgment
  - Quick check question: Why might a clinician disagree that a patient meeting a prescribing safety indicator actually needs intervention?

- Concept: Contextual reasoning vs. knowledge retrieval in LLMs
  - Why needed here: 6:1 ratio of contextual to factual errors reframes improvement strategies from knowledge augmentation to reasoning enhancement
  - Quick check question: If you could only fix one failure category, which would most improve overall system performance?

## Architecture Onboarding

- Component map: EHR preprocessing pipeline -> patient profile generation -> LLM inference -> structured output -> clinician evaluation -> hierarchical scoring
- Critical path: EHR data quality → patient profile completeness → LLM inference → clinician review → failure mode classification. The 23 excluded cases for data quality issues represent early pipeline failures.
- Design tradeoffs:
  - Single-pass inference vs. agentic workflows: Paper deliberately chose single-pass to measure inherent model capabilities; agentic architectures might address information-gathering failures
  - Single-expert review vs. multiple blinded reviewers: Enabled 277-case analysis but introduces anchoring bias (Section 3.4.1 estimates 7.9% inflation)
  - Structured-only data vs. including free-text notes: Lacks rich context that might address uncertainty; limits process blindness failures
- Failure signatures:
  - Overconfidence in uncertainty: Recommends immediate action when information gathering needed (e.g., stopping verapamil without understanding indication)
  - Protocol vs patient gap: Applies guidelines rigidly to palliative/frail patients where benefit-risk differs
  - Protocol vs practice gap: Misinterprets duplicate prescriptions as errors when intentional dose combinations
  - Hallucinations: Confidently misidentifies drug compositions (Monomil XL as clopidogrel, calcium channel blocker, or opioid across cases)
  - Process blindness: Recommends abrupt discontinuation without tapering or without ensuring alternative management
- First 3 experiments:
  1. Quantify anchoring bias: Run blinded clinician evaluation on subset (System outputs hidden during initial assessment) to measure actual vs. observed agreement rates
  2. Test RAG augmentation with BNF/dm+d: Specifically target the 14% factual error category; if performance doesn't improve, confirms contextual reasoning is primary barrier
  3. Implement uncertainty-based deferral: Add mechanism for system to request additional information when confidence below threshold; measure reduction in overconfidence failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agentic architectures with tool-calling capabilities reduce contextual reasoning failures by enabling LLMs to request additional information before making recommendations?
- Basis in paper: [explicit] The paper states "medication safety systems require the option to request additional information—for example through tool calling—rather than being constrained to generate recommendations from incomplete data" and notes that "single-pass inference precluded information-gathering, protocol lookups, and self-consistency checks that agentic workflows might have enabled."
- Why unresolved: This study deliberately used simple single-pass inference to measure inherent model capabilities; agentic approaches were not evaluated.
- What evidence would resolve it: A follow-up evaluation comparing single-pass vs. agentic architectures on the same patient cohort, measuring whether information-gathering capabilities reduce the 51 "overconfidence in uncertainty" failures.

### Open Question 2
- Question: Would LLM-based medication review systems reduce medication-related harm and improve patient outcomes in prospective clinical deployment?
- Basis in paper: [explicit] The abstract concludes that "this work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations" and the conclusion calls for "full clinical trials."
- Why unresolved: This was a retrospective evaluation focused on failure mode characterisation; no patient outcomes or prospective safety data were collected.
- What evidence would resolve it: A randomised controlled trial measuring medication-related adverse events, hospitalisations, and patient-reported outcomes in practices using LLM-assisted review versus standard care.

### Open Question 3
- Question: Does incorporating free-text clinical notes and richer patient context (goals of care, prognosis) reduce protocol-vs-patient reasoning failures?
- Basis in paper: [explicit] Section 3.2.2 states: "Addressing this requires richer patient context, including goals of care and prognosis. Since the patient profile included only structured data, this rich patient context was not available for this evaluation."
- Why unresolved: The EHR export contained only coded SNOMED data; no free-text clinical notes were available.
- What evidence would resolve it: A comparative evaluation using the same hierarchical framework on patient profiles with and without free-text notes, measuring reduction in "protocol vs patient gap" failures (n=49 in this study).

### Open Question 4
- Question: What is the magnitude and direction of anchoring bias in clinician evaluation of LLM outputs, and how does it affect reported accuracy metrics?
- Basis in paper: [inferred] The paper acknowledges the clinician reviewed cases while viewing System outputs rather than independently, and found that "observed clinician-agreement accuracy (95.7%) exceeded this [model self-consistency] ceiling by 7.9%," suggesting anchoring may inflate agreement.
- Why unresolved: Study design prioritised sample size and failure characterisation over blinded evaluation; no independent baseline clinician assessments were collected.
- What evidence would resolve it: A subset re-evaluation with clinicians blinded to LLM outputs, compared against the non-blinded assessments to quantify the anchoring effect.

## Limitations

- Single-blind clinician review design introduces anchoring bias, with Section 3.4.1 estimating 7.9% inflation in failure rates due to reviewer knowledge of system outputs
- Absence of free-text clinical notes from EHRs limits contextual reasoning capabilities and may artificially inflate process blindness failures
- Single LLM model (gpt-oss-120b) without exploration of different model families, sizes, or fine-tuning strategies

## Confidence

- **High confidence**: Detection-action decomposition mechanism (100% sensitivity with 46.9% full correctness) is directly observable from hierarchical evaluation results; patient complexity as unified construct (10% variance explained, no independent predictors) is statistically supported by the regression analysis.
- **Medium confidence**: Contextual reasoning failure dominance (6:1 ratio) is well-documented in failure analysis, but the absence of free-text notes means some contextual gaps may be artificial; the five specific failure patterns are identified but their relative frequencies could shift with different evaluation contexts.
- **Low confidence**: The generalizability of failure patterns to other healthcare settings, LLM architectures, or clinical domains remains untested; the automated scorer's reliability compared to multiple human reviewers is estimated but not empirically validated.

## Next Checks

1. **Blind reviewer validation**: Conduct a subset of evaluations with truly blinded clinician reviewers (system outputs hidden during initial assessment) to quantify actual anchoring bias and refine failure rate estimates.

2. **RAG augmentation experiment**: Implement retrieval-augmented generation with BNF/dm+d and NICE guidelines specifically targeting the 14% factual error category; if performance doesn't improve, this confirms contextual reasoning as the primary barrier.

3. **Free-text incorporation test**: Re-run the evaluation pipeline including clinical notes from a subset of patients to determine whether process blindness failures decrease with richer contextual information.