---
ver: rpa2
title: 'BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution'
arxiv_id: '2505.15308'
source_url: https://arxiv.org/abs/2505.15308
tags:
- image
- backdoor
- images
- poisoned
- badsr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the security of image super-resolution (SR)
  models by proposing BadSR, a stealthy backdoor attack method. The key idea is to
  approximate clean HR images to predefined target images in the feature space while
  maintaining visual similarity, addressing the limitation of previous backdoor attacks
  that ignored the stealthiness of poisoned HR images.
---

# BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution

## Quick Facts
- arXiv ID: 2505.15308
- Source URL: https://arxiv.org/abs/2505.15308
- Reference count: 40
- Primary result: Achieves high attack success rates across various SR models and datasets while maintaining high stealthiness, with poisoned HR images showing minimal visual difference from original HR images

## Executive Summary
This paper introduces BadSR, a stealthy backdoor attack method for image super-resolution (SR) models. The key innovation is approximating clean HR images to predefined target images in the feature space while maintaining visual similarity, addressing the stealthiness limitation of previous backdoor attacks. BadSR incorporates poisoned HR images with existing triggers and introduces an adversarially optimized trigger and a genetic algorithm-based poisoned sample selection method. Experimental results show BadSR achieves high attack success rates across various SR models and datasets, effectively impacting downstream tasks like image classification and object detection, while maintaining high stealthiness with minimal visual difference between poisoned and original HR images.

## Method Summary
BadSR is a three-stage approach that generates stealthy poisoned samples for SR backdoor attacks. First, it creates poisoned HR images by optimizing them to match target image features in a substitute model's feature space while constraining pixel-space deviation via L2 norm bounds. Second, it generates adversarial triggers through pixel-level perturbation optimization that maximizes reconstruction loss while maintaining perceptual similarity to clean images. Third, it selects the most effective poisoned samples using a genetic algorithm that ranks candidates by their backdoor gradient magnitudes. The method maintains model functionality on clean inputs while achieving high attack success rates when triggers are present.

## Key Results
- Achieves Attack Success Rates exceeding 85% across multiple SR models (EDSR, RCAN, ESRGAN, SwinIR, LIIF)
- Maintains high stealthiness with poisoned HR images showing SSIM > 0.65 and PSNR > 28 dB compared to originals
- Effectively impacts downstream tasks including image classification (ResNet-50) and object detection (Faster R-CNN)
- Demonstrates effectiveness across multiple datasets including DIV2K, Set5, Set14, BSD100, Urban100, and DIV2K100

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Poisoned HR images can be made visually indistinguishable from original HR images while encoding target image features, enabling stealthy label poisoning in SR tasks.
- **Mechanism:** The method optimizes poisoned HR images ($y_p$) to minimize feature-space distance to a target image ($y_t^*$) using a substitute model's feature extractor $f_\phi(\cdot)$, while constraining pixel-space deviation via $\ell_2$-norm bound $\epsilon$. This creates images that train the SR model to associate triggered inputs with target features without obvious visual tampering.
- **Core assumption:** The feature space of the substitute model transfers sufficiently to the target SR model—i.e., features that align in $f_\phi$ will be learned similarly by the victim model.
- **Evidence anchors:**
  - [abstract] "The key idea of BadSR is to approximate the clean HR image and the pre-defined target image in the feature space while ensuring that modifications to the clean HR image remain within a constrained range."
  - [Section IV-B, Eq. 13] Formally defines: $\min_{y_p} \|f_\phi(y_p) - f_\phi(y_t^*)\|_2^2$ s.t. $\|y_p - y\|_2 \leq \epsilon$
  - [corpus] Related work "FFCBA" similarly uses feature-based optimization for clean-label attacks, suggesting feature-space alignment is a viable attack strategy across domains.
- **Break condition:** If the substitute model's features diverge significantly from the victim model's internal representations, the poisoned HR images may fail to induce the target mapping.

### Mechanism 2
- **Claim:** Pixel-level adversarial perturbation triggers maximize attack effectiveness while preserving model functionality on clean inputs.
- **Mechanism:** Unlike global semantic triggers, BadSR optimizes per-pixel perturbations $\delta$ to maximize reconstruction loss $L_{adv} = \|f_\theta(x + \delta) - y\|_2$ of a substitute SR model, subject to perceptual similarity constraints (LPIPS) and dynamic $L_2$ penalties. This creates triggers that strongly activate backdoor behavior without degrading clean-image performance.
- **Core assumption:** The substitute SR model $f_\theta$ provides a reasonable proxy for how the victim model will respond to perturbations.
- **Evidence anchors:**
  - [Section IV-C] "we design an adversarial perturbation-based trigger that maintains the model's normal functionality while achieving strong attack effectiveness."
  - [Table II] BadSR achieves the highest PSNR/SSIM on clean images compared to other trigger types (BadNet, Blend, WaNet, etc.), confirming minimal functional degradation.
  - [corpus] Weak direct evidence—corpus papers focus on classification/other domains; no SR-specific trigger optimization comparisons available.
- **Break condition:** If the victim model architecture differs substantially from the substitute (e.g., Transformer vs. CNN), trigger transferability may degrade.

### Mechanism 3
- **Claim:** Genetic algorithm selection of high-gradient poisoned samples increases attack success rate at lower poisoning rates.
- **Mechanism:** Samples are ranked by "backdoor gradient" norm $\|g_p\|_2 = \|\partial L_{bkd}/\partial \theta\|_2$, measuring each sample's influence on backdoor loss. A genetic algorithm optimizes subset selection to maximize cumulative gradient impact while minimizing subset size via regularization $\lambda|S_i|$.
- **Core assumption:** Gradient magnitude during training correlates with long-term backdoor implantation strength.
- **Evidence anchors:**
  - [Section IV-D, Eq. 23-27] Formalizes backdoor gradient and GA fitness function.
  - [Table V] Ablation shows "effective poisoning" improves ASR from 80.10% to 85.73% at 10% poisoning rate.
  - [corpus] No direct corpus validation of gradient-based sample selection for backdoors; mechanism is paper-specific.
- **Break condition:** If gradient computation is unstable or the victim model uses non-standard optimization (e.g., second-order methods), gradient-based selection may not correlate with attack success.

## Foundational Learning

- **Concept:** Feature-space vs. pixel-space optimization
  - **Why needed here:** BadSR's core innovation relies on minimizing distance in a neural network's feature space while constraining pixel-space changes. Without understanding this distinction, the dual-objective optimization (Eq. 11-12) will be opaque.
  - **Quick check question:** Given two images that are visually similar (high SSIM), could they have very different features in a VGG network? Why or why not?

- **Concept:** Data poisoning backdoor attacks
  - **Why needed here:** The entire threat model assumes the attacker can modify training data but not the model training process. Understanding the poisoning pipeline (Eq. 1-10) is prerequisite to grasping why stealthy labels matter.
  - **Quick check question:** In a standard classification backdoor, what is the "label" that gets poisoned? In SR, what is the analogous "label"?

- **Concept:** Genetic algorithms for subset selection
  - **Why needed here:** The effective poisoning component uses GA to select which samples to poison. Understanding fitness functions, crossover, and mutation operators is necessary to modify or debug this component.
  - **Quick check question:** Why might a genetic algorithm outperform random sampling for selecting influential training samples? What is the fitness function optimizing?

## Architecture Onboarding

- **Component map:**
  Original Dataset (LR, HR pairs) -> [Poisoned HR Generator] -> Poisoned HR images (y_p)
                    ↓                               ↓
              Feature extractor (RRDBNet)    Substitute SR model (RRDBNet)
                    ↓                               ↓
                    + L2 constraint optimization    + LPIPS + dynamic penalty
                    ↓                               ↓
              [Trigger Generator] -> Adversarial triggers (δ)
                    ↓
              [Effective Poisoning Selector] -> Backdoor gradient computation
                    ↓                               + Genetic algorithm
                    ↓                               ↓
              Final poisoned subset -> Final poisoned subset
                    ↓
  Poisoned Dataset -> Victim SR Model Training -> Backdoored Model

- **Critical path:**
  1. Feature extractor selection (must be differentiable and representative of target models)
  2. Perturbation budget $\epsilon$ tuning (too small = weak attack; too large = detectable)
  3. Trigger optimization convergence (300 iterations per LR image)
  4. Poisoning rate selection (10% chosen as trade-off; see Fig. 10)

- **Design tradeoffs:**
  - **Stealthiness vs. Effectiveness:** Lower $\epsilon$ (HR perturbation budget) improves SSIM but reduces ASR (Table IV: $\epsilon=0.05$ → ASR 85.73%, SSIM 0.6895; $\epsilon=0.2$ → ASR 88.93%, SSIM 0.3407)
  - **Poisoning rate vs. Detection risk:** Higher rates increase ASR but make poisoned samples more discoverable
  - **Trigger visibility vs. Transferability:** More aggressive triggers may transfer better but fail stealthiness checks

- **Failure signatures:**
  - Low ASR with high SSIM → $\epsilon$ too small, increase perturbation budget
  - High ASR but low clean-image PSNR → Trigger too aggressive, increase LPIPS weight $\lambda_1$
  - Random ASR across models → Substitute model poorly matched; try different architecture
  - Defense methods (bit depth reduction, JPEG) dropping ASR → Trigger relies on high-frequency components; consider low-frequency trigger designs

- **First 3 experiments:**
  1. **Validate poisoned HR stealthiness:** Generate poisoned HR images with $\epsilon=0.05$, compute SSIM/PSNR against originals. Target: SSIM > 0.65, PSNR > 28 dB (per Table III).
  2. **Establish baseline ASR:** Train EDSR on DIV2K with 10% poisoned samples using BadSR triggers and poisoned HR. Target: ASR > 85% on DIV2K validation set.
  3. **Test trigger transferability:** Apply triggers optimized on RRDBNet to SwinIR (Transformer architecture). Measure ASR gap to assess substitute model sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the semantic complexity or frequency content of the predefined target image affect the required perturbation budget and the convergence of the poisoned HR image optimization?
- **Basis in paper:** [inferred] The methodology (Section IV-B) relies on minimizing the feature distance to a target image $y^*_t$ while constraining perturbations, but does not analyze how specific properties of $y^*_t$ influence this optimization trade-off.
- **Why unresolved:** The paper evaluates effectiveness using standard datasets but does not ablate the characteristics of the target images themselves (e.g., high-frequency textures vs. smooth gradients).
- **What evidence would resolve it:** An ablation study measuring the Attack Success Rate (ASR) and SSIM when using target images of varying spatial frequencies or structural complexity.

### Open Question 2
- **Question:** To what extent does the choice of the substitute model architecture impact the transferability and effectiveness of BadSR when attacking structurally diverse SR models?
- **Basis in paper:** [inferred] The method relies on a specific substitute model (RRDBNet) for generating both poisoned HR images and triggers (Section V-A), assuming this generalizes to other architectures like SwinIR or LIIF.
- **Why unresolved:** While the paper demonstrates success across various target models, it does not analyze the sensitivity of the attack success rate relative to the architectural gap between the substitute and the target.
- **What evidence would resolve it:** Experiments comparing attack performance when using different substitute models (e.g., a CNN vs. a Transformer) to attack a fixed target model.

### Open Question 3
- **Question:** Can the BadSR method be effectively adapted for Video Super-Resolution (VSR) to maintain temporal consistency across frames without compromising stealthiness?
- **Basis in paper:** [inferred] The paper focuses exclusively on Single Image Super-Resolution (SISR) (Section V-A), leaving the temporal domain unexplored.
- **Why unresolved:** Applying independent pixel-level perturbations to video frames would likely introduce flickering artifacts, breaking the "visual stealthiness" constraint defined in the threat model.
- **What evidence would resolve it:** A modified BadSR framework applied to a VSR dataset, evaluated using temporal consistency metrics (e.g., temporal SSIM) alongside standard SR metrics.

## Limitations
- No analysis of defense robustness beyond standard preprocessing methods
- Limited ablation on poisoning rate sensitivity beyond 5-15% range
- No investigation of transferability between different SR model families
- Target image selection methodology not specified

## Confidence
**High confidence:** The core mechanism of feature-space alignment for stealthy poisoning is well-supported by formal equations and experimental results showing minimal visual degradation (SSIM > 0.65) while achieving high ASR (>85%). The dual optimization framework (feature space + pixel space) is technically sound and the results across multiple SR architectures demonstrate transferability.

**Medium confidence:** The genetic algorithm selection method shows measurable improvement in ASR (80.10% → 85.73% at 10% poisoning), but the lack of detailed hyperparameter specifications (population size, mutation rates, regularization λ) creates uncertainty about reproducibility. The gradient-based selection assumes linear correlation between gradient magnitude and backdoor strength, which may not hold for all SR model architectures.

**Low confidence:** The substitute model assumption for trigger generation lacks extensive validation. While the paper shows BadSR works across different victim models (EDSR, RCAN, SwinIR), there's no systematic study of substitute-victim architectural mismatch. The effectiveness of adversarial triggers on Transformer-based SR models (SwinIR) is demonstrated but not deeply analyzed.

## Next Checks
1. **Substitute model sensitivity:** Systematically vary the substitute model architecture (CNN vs Transformer) and measure trigger transferability to quantify the impact of architectural mismatch on attack effectiveness.

2. **Poisoning rate optimization:** Conduct fine-grained analysis of poisoning rates (1%, 3%, 5%, 10%, 15%, 20%) to identify the minimum effective poisoning rate while maintaining high stealthiness, and measure detection probability at each rate.

3. **Cross-dataset generalization:** Evaluate BadSR on SR models trained on different datasets (Flickr2K, DIV8K) to assess whether the attack's effectiveness depends on dataset-specific features or generalizes across diverse training distributions.