---
ver: rpa2
title: 'ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for
  Complex Long-Horizon Tasks'
arxiv_id: '2510.14621'
source_url: https://arxiv.org/abs/2510.14621
tags:
- arxiv
- tasks
- evaluation
- mobile
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ColorBench, a graph-structured benchmarking
  framework for evaluating mobile agents on complex long-horizon tasks. The key innovation
  is modeling mobile screen states as nodes and action transitions as edges to create
  a static simulation of dynamic mobile environments.
---

# ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks

## Quick Facts
- arXiv ID: 2510.14621
- Source URL: https://arxiv.org/abs/2510.14621
- Authors: Yuanyi Song; Heyuan Huang; Qiqiang Lin; Yin Zhao; Xiangmou Qu; Jun Wang; Xingyu Lou; Weiwen Liu; Zhuosheng Zhang; Jun Wang; Yong Yu; Weinan Zhang; Zhaoxiang Wang
- Reference count: 20
- Key outcome: Graph-structured mobile agent benchmark with 175 tasks (average 13+ steps) that models screen states as nodes and action transitions as edges, enabling stable simulation of dynamic mobile environments while supporting multiple valid solutions and error recovery paths.

## Executive Summary
ColorBench introduces a novel graph-structured framework for benchmarking mobile agents on complex long-horizon tasks. By modeling mobile screen states as nodes and action transitions as edges, it creates a static simulation of dynamic mobile environments that addresses limitations of existing offline static benchmarks (single predefined paths) and online dynamic testing (instability and non-reproducibility). The framework contains 175 tasks (74 single-app, 101 cross-app) with an average of over 13 steps per task, each supporting multiple valid solutions and error paths for quasi-dynamic interaction.

The benchmark was evaluated across various models including closed-source (GPT-4o, Qwen-VL Max, GLM-4.5V) and open-source models (GUI-OWL, Qwen2.5-VL, UI-TARS). Results show that GLM-4.5V achieved the highest completion rate (51.54%) among closed-source models, while GUI-OWL-32B-RL achieved the best performance overall (SR: 40.54%, CR: 53.72%). The evaluation revealed significant limitations in existing models' ability to handle complex long-horizon tasks, particularly in memory, reflection, and multi-step planning capabilities.

## Method Summary
ColorBench models mobile environments as directed graphs where screen states are nodes and action transitions are edges, creating a strongly connected graph that supports multiple valid solution paths and error recovery through backtracking. The framework uses BFS for shallow pages and DFS for complex tasks to collect trajectories, then merges them into a unified graph using VLM-based semantic and action-transition filtering with manual verification. Each task includes at least two correct paths and several typical error paths, enabling evaluation of multi-solution navigation and reflective backtracking capabilities. The benchmark supports atomic-level capability analysis across 15 atomic task types, milestone-based completion rate tracking, and success rate calculation for complete task execution.

## Key Results
- GLM-4.5V achieved highest completion rate (51.54%) among closed-source models
- GUI-OWL-32B-RL achieved best overall performance (SR: 40.54%, CR: 53.72%)
- Models struggled with cross-app tasks requiring memory retention and reflection
- Multi-agent modules (planning/reflection/memory) showed inconsistent benefits, sometimes decreasing performance
- Atomic capability analysis revealed specific weaknesses in search, share, navigate, and copy operations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The graph structure enables stable simulation of dynamic mobile environments by capturing finite interaction states.
- **Mechanism:** Screen states are abstracted as nodes $V=\{N_1, N_2, ...\}$ and action transitions as edges $E=\{(N_i, N_j, a), ...\}$. This creates a strongly connected directed graph that models all reachable states from the HOME node, allowing agents to traverse paths deterministically without real-time UI variability.
- **Core assumption:** The diversity of real-device interactions can be sufficiently captured by a finite set of screen states and transitions, without requiring infinite environmental modeling.
- **Evidence anchors:**
  - [abstract] "By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors."
  - [section 3.1] "The graph abstracts the finite states of real mobile environment into a strongly connected directed graph G = (V, E)."
  - [corpus] MobileUse (arXiv:2507.16853) addresses hierarchical reflection for mobile operations, supporting the need for structured state management in dynamic environments.
- **Break condition:** If application interfaces change dynamically in ways that cannot be predicted or collected during graph construction (e.g., personalized content that fundamentally alters available actions), the graph may not cover relevant state transitions.

### Mechanism 2
- **Claim:** Multi-path support with reflective backtracking enables more realistic assessment of agent capabilities compared to single-path evaluation.
- **Mechanism:** Each task includes at least two correct paths and several typical error paths. When an agent makes an error, the graph's strongly connected structure allows backtracking to previous nodes rather than terminating evaluation. This captures collaborative intelligence (reflection, self-correction) that single-path benchmarks miss.
- **Core assumption:** Error recovery is a meaningful capability to evaluate, and agents that can self-correct should be scored higher than those that fail permanently on first error.
- **Evidence anchors:**
  - [abstract] "Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction."
  - [section 3.2] "It is designed to encompass not only the optimal path but also sub-optimal ones and, importantly, recovery paths where the agent corrects its errors."
  - [corpus] Mobile-Agent-E (arXiv:2501.11733) emphasizes self-evolving capabilities for complex tasks, aligning with the need for reflection mechanisms.
- **Break condition:** If backtracking paths are not properly connected in the graph structure, agents may enter unrecoverable states despite the theoretical support for reflection.

### Mechanism 3
- **Claim:** Atomic task capability decomposition enables fine-grained diagnosis of failure modes in complex long-horizon tasks.
- **Mechanism:** Complex tasks are decomposed into subtasks with milestones. Completion Rate (CR) measures milestone achievement, while Atomic Capability (AC) scores assess performance on 15 atomic task types (e.g., search, share, navigate, copy). This identifies which specific capabilities fail even when overall task completion is low.
- **Core assumption:** Failures in complex tasks can be attributed to specific atomic capability weaknesses rather than solely to planning or reasoning deficits.
- **Evidence anchors:**
  - [abstract] "It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis."
  - [section 5.1.1] "By extracting common characteristics across all milestone points, we categorize them into 15 atomic task capabilities."
  - [corpus] Atomic-to-Compositional Generalization (arXiv:2506.08972) explicitly addresses the gap between atomic task performance and compositional generalization, supporting this decomposition approach.
- **Break condition:** If atomic task capabilities are highly interdependent in ways the benchmark doesn't capture, diagnosing individual capability failures may misattribute the root cause.

## Foundational Learning

- **Concept: Directed graphs and strongly connected components**
  - **Why needed here:** ColorBench models mobile environments as graphs; understanding node-edge relationships is essential for interpreting benchmark structure and agent navigation paths.
  - **Quick check question:** Can you explain why a strongly connected graph is necessary for supporting backtracking and multiple solution paths?

- **Concept: Offline static vs. online dynamic evaluation paradigms**
  - **Why needed here:** ColorBench explicitly positions itself as bridging these paradigms; understanding their tradeoffs (stability vs. realism) clarifies the benchmark's design motivation.
  - **Quick check question:** What are the three main limitations of offline static evaluation identified in the paper, and how does the graph structure address each?

- **Concept: Multimodal LLM grounding and UI understanding**
  - **Why needed here:** The evaluated agents are MLLMs that must translate visual UI understanding into action coordinates; grounding accuracy directly impacts benchmark performance.
  - **Quick check question:** Why did the paper use Qwen2.5-VL-7b as a grounding model for closed-source models like GPT-4o?

## Architecture Onboarding

- **Component map:** BFS trajectory collection -> DFS trajectory collection -> Graph construction with VLM filtering -> Manual verification -> Bounding box annotation -> Evaluation with milestone tracking

- **Critical path:**
  1. Define task set with partially ambiguous instructions enabling multiple paths
  2. Collect trajectories via BFS (6300 screenshots) + DFS (1343 screenshots)
  3. Merge into unified graph using VLM-based semantic and action-transition filtering
  4. Annotate bounding boxes using multi-VLM pipeline
  5. Manual quality control â†’ final graph (1989 curated screenshots)
  6. Deploy evaluation with milestone-based AC calculation

- **Design tradeoffs:**
  - Finite states vs. infinite real-world variability: Graph captures most probable states but may miss rare edge cases
  - Automated construction vs. manual quality: 3-step VLM pipeline with human verification balances scale and accuracy
  - Multiple correct paths vs. evaluation complexity: More paths increase realism but complicate success criteria definition
  - Stable testing vs. temporal dynamics: Static screenshots lose real-time content changes (ads, time displays) but gain reproducibility

- **Failure signatures:**
  - Low SR with high CR: Agent reaches milestones but fails final step (check atomic capabilities for specific weakness)
  - High single-app SR, low cross-app SR: Navigation or app-switching capability deficit
  - Recurring identical errors without backtracking: Reflection module not functioning or graph lacking recovery edges
  - Random node returns causing inconsistent results: Multiple images per node simulating randomness; set fixed random seed (2025) for reproducibility

- **First 3 experiments:**
  1. Baseline establishment: Run GUI-OWL-32B and Qwen2.5-VL-32B on 20-task subset with and without multi-agent modules (plan/reflection/memory) to measure module contribution deltas
  2. Graph coverage validation: Select 10 ColorBench tasks executable on real devices; compare failure reasons between graph and real-device evaluation to verify graph captures primary failure modes
  3. Atomic capability profiling: Evaluate target model on full benchmark, generate AC score breakdown across 15 capabilities; identify lowest-scoring capabilities and cross-reference with task failure logs to confirm diagnostic validity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-agent module integration be optimized to prevent erroneous coupling and error accumulation in long-horizon tasks?
- **Basis in paper:** [Explicit] Section 5.5 states that "overly complex module combinations can cause erroneous coupling and error accumulation," leading to situations where tasks become unresolvable despite adding sophisticated modules.
- **Why unresolved:** The ablation study shows that adding planning or reflection modules sometimes decreases Success Rate (SR) for specific models (e.g., Qwen2.5-VL-32B), indicating that current integration strategies are unstable.
- **What evidence would resolve it:** A proposed integration framework that provides theoretical or empirical guarantees of monotonic performance improvement when adding collaborative modules.

### Open Question 2
- **Question:** What architectures or training methods can effectively instill active memorization and self-correction capabilities in mobile agents?
- **Basis in paper:** [Explicit] Section 5.4 identifies "vague memory of essential historical information" and a "lack of effective reflection on recurring erroneous actions" as primary causes of failure in complex tasks.
- **Why unresolved:** The authors observe that most evaluated models fail to retain critical context (e.g., a paper title) across cross-app transitions, causing infinite loops, and merely list this as a required improvement.
- **What evidence would resolve it:** A model that successfully maintains state across cross-app transitions in ColorBench, specifically solving tasks requiring information retention (e.g., the Xiaohongshu-to-Arxiv example).

### Open Question 3
- **Question:** How can specialized GUI foundation models overcome overfitting to match the generalization capabilities of large general-purpose models?
- **Basis in paper:** [Explicit] Section 5.3 notes that specialized foundation models (e.g., UI-TARS, OS-Atlas) often underperform general models (e.g., Qwen2.5-VL) because fine-tuning can "easily lead to overfitting, reducing their ability to generalize."
- **Why unresolved:** There is a trade-off between acquiring operational knowledge (often via specialization) and maintaining reasoning capabilities, which the paper identifies but does not resolve.
- **What evidence would resolve it:** A specialized model that consistently outperforms generalist baselines (like GLM-4.5V) on both the held-out test set and novel, complex cross-app tasks.

## Limitations

- Static graph structure cannot capture temporal dynamics like real-time notifications or changing content
- Manual verification steps introduce scalability constraints and potential human bias
- 175 tasks represent a finite subset of possible mobile interactions, potentially missing edge cases
- Temporal elements such as ads, time displays, and personalized content cannot be fully modeled

## Confidence

- **High Confidence:** The graph-structured approach effectively bridges offline and online evaluation paradigms, providing stable and reproducible testing conditions. Well-supported by experimental results showing consistent performance metrics across multiple model evaluations and ablation studies.
- **Medium Confidence:** The multi-path support with reflective backtracking genuinely improves capability assessment over single-path benchmarks. While the mechanism is sound, the extent to which real agents utilize these recovery paths versus getting permanently stuck remains partially empirical.
- **Medium Confidence:** The atomic capability decomposition provides meaningful diagnostic insights. The categorization into 15 atomic task types is logical, but the granularity may not perfectly align with how actual failures propagate through complex task sequences.

## Next Checks

1. **Temporal Dynamics Validation:** Select 10 ColorBench tasks and execute them on real devices, comparing failure modes and success rates with the static graph evaluation. Document cases where temporal elements (notifications, time-dependent content) create divergences.

2. **Graph Coverage Analysis:** Systematically sample 50 random mobile interaction sequences from real device logs and map them to the ColorBench graph structure. Calculate coverage percentage and identify common interaction patterns missing from the benchmark.

3. **Recovery Path Utilization Study:** For a subset of 30 tasks, instrument agent evaluations to log not just success/failure but the specific paths taken, including backtracking instances. Analyze whether agents actually utilize the multi-path structure for error recovery or consistently fail on first error regardless of available recovery edges.