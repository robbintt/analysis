---
ver: rpa2
title: 'From Static Structures to Ensembles: Studying and Harnessing Protein Structure
  Tokenization'
arxiv_id: '2511.10056'
source_url: https://arxiv.org/abs/2511.10056
tags:
- structure
- protein
- sequence
- tokens
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the properties of discrete protein structure
  representations and demonstrates how to harness them for modeling protein dynamics.
  The authors first show that using pre-trained ESM3 sequence embeddings significantly
  improves structure prediction accuracy over ProGen2 embeddings in a GPT-based model,
  indicating a semantic gap between sequence and structure modalities.
---

# From Static Structures to Ensembles: Studying and Harnessing Protein Structure Tokenization

## Quick Facts
- arXiv ID: 2511.10056
- Source URL: https://arxiv.org/abs/2511.10056
- Reference count: 40
- Key result: A training-free synonym-swap method generates protein conformational ensembles achieving 0.84 RMSF correlation with MD simulations

## Executive Summary
This paper investigates discrete protein structure representations and demonstrates how to harness them for modeling protein dynamics. The authors show that using pre-trained ESM3 sequence embeddings significantly improves structure prediction accuracy over ProGen2 embeddings, revealing a semantic gap between sequence and structure modalities. Analyzing the ESM3 VQ-VAE structural codebook reveals significant semantic redundancy, where distinct tokens often decode to nearly identical structures. Exploiting this redundancy, the authors introduce a "synonym swap" strategy that generates conformational ensembles by perturbing predicted structures with structurally equivalent tokens. This training-free, near-instantaneous method achieves a median RMSF correlation of 0.84 with MD simulations, demonstrating its effectiveness for capturing protein flexibility.

## Method Summary
The authors use a GPT-based model with a ProGen2 backbone, adapting input sequences through ESM3 embeddings and projecting them to align with structural tokens from a VQ-VAE codebook. They exploit semantic redundancy in the codebook by identifying "synonym" tokens that decode to similar structures, then generate conformational ensembles through random token swaps within synonym clusters. The method requires no training beyond initial structure prediction and operates near-instantaneously compared to traditional MD simulations.

## Key Results
- ProGen2 embeddings fail to generate coherent structures, while ESM3 embeddings achieve strong predictive performance
- VQ-VAE codebook contains significant semantic redundancy with multiple distinct tokens mapping to nearly identical local geometries
- Synonym-swap ensemble generation achieves median RMSF correlation of 0.