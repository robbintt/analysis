---
ver: rpa2
title: 'From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI
  Personality Detection'
arxiv_id: '2601.18582'
source_url: https://arxiv.org/abs/2601.18582
tags:
- personality
- reward
- detection
- reasoning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of personality detection using
  large language models (LLMs), focusing on the Myers-Briggs Type Indicator (MBTI)
  framework. Existing methods rely on classification-based approaches that treat personality
  dimensions independently, failing to capture the complex interdependencies between
  traits.
---

# From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection

## Quick Facts
- arXiv ID: 2601.18582
- Source URL: https://arxiv.org/abs/2601.18582
- Authors: Yuan Cao; Feixiang Liu; Xinyue Wang; Yihan Zhu; Hui Xu; Zheng Wang; Qiang Qiu
- Reference count: 11
- One-line primary result: Novel ranking-based LLM paradigm achieves state-of-the-art MBTI personality detection with 80.57% Macro-F1 on Kaggle and 66.10% Macro-F1 on PANDORA datasets.

## Executive Summary
This paper addresses the challenge of personality detection using large language models (LLMs), focusing on the Myers-Briggs Type Indicator (MBTI) framework. Existing methods rely on classification-based approaches that treat personality dimensions independently, failing to capture the complex interdependencies between traits. The authors propose a novel ranking-based paradigm that treats personality detection as a comparative ranking task rather than categorical classification. Their method, PerDet-R1, employs a two-stage training process: supervised fine-tuning (SFT) to establish ranking capabilities using advanced LLM reasoning traces, followed by reinforcement learning with Group Relative Policy Optimization (GRPO) using a ranking-based reward function. Experimental results demonstrate state-of-the-art performance across multiple personality detection benchmarks, achieving 80.57% Macro-F1 on Kaggle and 66.10% Macro-F1 on PANDORA datasets, with significant improvements over existing classification-based approaches. The ranking-based approach better captures the continuous nature of personality traits and their interdependencies.

## Method Summary
The authors propose PerDet-R1, a ranking-based paradigm for MBTI personality detection that treats the task as list-wise ranking rather than four independent binary classifications. The method employs a two-stage training process: first, supervised fine-tuning (SFT) distills reasoning capabilities from a teacher model (Qwen-plus) into a student model (Qwen2.5-7B-Instruct) using 1,000 filtered samples where the ground-truth label appears in the Top-K predictions. Second, reinforcement learning with Group Relative Policy Optimization (GRPO) optimizes ranking performance using a dual-component reward function combining Normalized Discounted Cumulative Gain (NDCG) and dimension similarity metrics. This approach captures the continuous nature of personality traits and their interdependencies, addressing the limitations of existing classification-based methods that treat MBTI dimensions independently.

## Key Results
- Achieves 80.57% Macro-F1 on Kaggle MBTI dataset and 66.10% Macro-F1 on PANDORA dataset, outperforming existing classification-based approaches
- The dual-component reward function (NDCG + dimension similarity) prevents reward collapse and provides stable training signals during reinforcement learning
- The two-stage SFT+RL training pipeline shows significant performance improvement over either stage alone, with SFT providing essential task understanding and formatting capabilities
- Ranking-based approach captures personality trait interdependencies better than classification methods, particularly for ambiguous or "blurred boundary" cases

## Why This Works (Mechanism)

### Mechanism 1: Ranking Paradigm Shift
- Claim: Treating personality detection as a list-wise ranking task, rather than four independent binary classifications, enables the model to capture dimensional interdependencies and improve overall prediction accuracy.
- Mechanism: The model is trained via Supervised Fine-Tuning (SFT) to generate a Top-K list of MBTI types with an associated reasoning trace. Reinforcement Learning (RL), specifically Group Relative Policy Optimization (GRPO), then optimizes the ranking quality using a combined reward. This forces the model to consider all four MBTI dimensions as a unified entity during inference, rather than as separate outputs.
- Core assumption: Personality traits exist on a continuous spectrum with complex interdependencies (e.g., an INTJ's "Ni" vs. an ENTJ's "Te" dominant function) that are better represented by a preference ranking than a single hard label.
- Evidence anchors:
  - [abstract] "...we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm."
  - [section] "Prevailing approaches... fundamentally limit their efficacy by treating MBTI prediction as four independent binary classification tasks... resulting in... dimensional decoupling that ignores psychometrically established interactions."
  - [corpus] Weak/Missing. Corpus neighbors focus on LLM prediction and persona, but do not explicitly validate the "ranking vs. classification" paradigm for MBTI.
- Break condition: If personality traits were truly orthogonal and independent, a multi-label or multi-output classification approach would be equally or more effective. A breakdown in performance on ambiguous or "blurred boundary" cases (e.g., xNTP) would suggest the ranking assumption is flawed.

### Mechanism 2: Dual-Component Reward Function
- Claim: A reward function combining Normalized Discounted Cumulative Gain (NDCG) with a Dimension Similarity (DS) score provides stable, dense reward signals that mitigate reward hacking and guide the model toward correct, well-ranked outputs.
- Mechanism: The NDCG component rewards placing the ground-truth type in a higher position within the Top-K list, weighted by a similarity score based on shared MBTI letters. The DS component rewards matching letters in the first predicted answer. This synergy provides continuous feedback on both ranking position and partial correctness, preventing the reward collapse observed when using either component alone.
- Core assumption: Reward hacking, where the model exploits a sparse reward signal to maximize score without learning the intended task, is a significant risk in RL-based LLM fine-tuning. A dense, well-shaped reward is necessary for stable convergence.
- Evidence anchors:
  - [abstract] "Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings."
  - [section] "As observed from the red line in Figure 4, the NDCG reward collapses abruptly during training without the dimension similarity reward."
  - [corpus] Weak/Missing. Corpus neighbors discuss LLMs in affective domains but do not provide evidence on the specific architecture of reward functions for ranking tasks.
- Break condition: The model would exhibit reward hacking, finding output patterns that maximize the reward score without meaningful improvement in prediction quality. Training curves would show instability or collapse of the reward signal.

### Mechanism 3: Two-Stage SFT + RL Training
- Claim: A sequential training pipeline, beginning with Supervised Fine-Tuning (SFT) to establish reasoning capabilities and output formatting, followed by Reinforcement Learning (RL) to refine ranking performance, provides a more robust initialization and leads to superior final performance than either stage alone.
- Mechanism: Stage 1 (SFT) uses a small dataset of 1,000 high-quality samples (filtered by rejection sampling) from a teacher model to distill reasoning traces and enforce a standardized output format. Stage 2 (RL) initializes the policy with this SFT model and uses the GRPO algorithm with the dual-component reward to further optimize the model's generalization and ranking ability.
- Core assumption: The SFT stage provides a necessary "cold start," giving the model a basic understanding of the task and output format. Directly applying RL without this initialization would be inefficient or fail to converge.
- Evidence anchors:
  - [abstract] "First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities... Subsequently, we introduce Group Relative Policy Optimization..."
  - [section] "As shown by the blue line in Figure 4, without the first-stage SFT, the performance improvement during later RL training becomes weak."
  - [corpus] Weak/Missing. While corpus papers like Fin-R1 use SFT+RL, they do not provide direct comparative evidence for the personality detection task.
- Break condition: The RL stage fails to improve upon the SFT baseline, or training dynamics are highly unstable. A breakdown in the transfer of reasoning skills from the teacher model during SFT would also impair the entire pipeline.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the core RL algorithm used in the second training stage. It estimates advantages based on group-level reward statistics, making it suitable for tasks without a value model.
  - Quick check question: Can you explain how GRPO calculates the advantage \(A_i\) for a given response within a group of responses?

- Concept: **Normalized Discounted Cumulative Gain (NDCG)**
  - Why needed here: NDCG is a critical component of the reward function. It measures the quality of a ranking by accounting for the position of relevant items, giving higher weight to correct answers ranked at the top.
  - Quick check question: In the context of this paper's reward function, what two pieces of information does the NDCG@k score primarily capture about the model's output?

- Concept: **Rejection Sampling**
  - Why needed here: This technique is used during SFT data construction to ensure high quality. It filters out generated samples where the ground-truth label is not present in the model's Top-K predictions.
  - Quick check question: According to the paper, what is the primary criterion for "rejecting" a sample during the SFT data construction process?

## Architecture Onboarding

- Component map: Input (social media posts) -> Stage 1 (SFT with teacher model + rejection sampling) -> Stage 2 (GRPO with dual reward) -> Output (Top-K MBTI ranking)
- Critical path: SFT Data Generation (teacher model + filtering) -> Stage 1 SFT -> Stage 2 GRPO Training -> Inference. The quality of the SFT data is paramount; poor reasoning traces will propagate to the final model.
- Design tradeoffs:
  - **Ranking vs. Classification**: Choosing a ranking paradigm better captures trait interdependencies but requires more complex data construction and reward modeling compared to standard classification.
  - **Dual Reward vs. Single Reward**: Using both NDCG and DS provides stable, dense rewards but adds complexity. Removing either component leads to training instability or reward collapse.
  - **Teacher Model Distillation**: Using a powerful model like Qwen-plus for SFT data provides high-quality training signals but introduces dependency on the teacher's capabilities.
- Failure signatures:
  - **Reward Collapse**: NDCG reward drops abruptly during RL training, indicating the model is exploiting the reward function without learning the task. Typically caused by a missing or flawed reward component.
  - **Format Drift**: The model fails to generate the required `<think>` and `<answer>` tags, breaking reward calculation. Indicates a failure in the SFT stage to enforce formatting.
  - **Stagnant Performance**: The RL stage fails to improve upon the SFT baseline, suggesting the reward signal may be too sparse or the advantage estimation is not effective.
- First 3 experiments:
  1. **Baseline Verification**: Replicate the SFT-only baseline (w/o GRPO) and the RL-only baseline (w/o SFT) on a held-out validation set to confirm the performance gap shown in Table 2.
  2. **Reward Ablation**: Train the full pipeline with only the NDCG reward and then with only the DS reward to reproduce the "reward collapse" phenomenon and confirm the necessity of both components.
  3. **Data Quality Check**: Manually inspect a sample of the generated SFT data to verify the quality and coherence of the distilled reasoning traces before starting Stage 1 training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-modal signals (profile avatars, facial expressions) be integrated into the ranking-based personality detection framework to improve performance beyond text-only approaches?
- Basis in paper: [explicit] The conclusion states: "In future work, we will explore how to achieve higher-performance personality trait detection by utilizing multi-modal data such as profile avatars and facial expressions."
- Why unresolved: The current framework processes only textual social media posts; multi-modal integration requires new architectural components, fusion strategies, and training paradigms not yet developed.
- What evidence would resolve it: Comparative experiments on datasets containing both text and images, measuring whether visual modalities yield statistically significant improvements in Macro-F1 and NDCG scores.

### Open Question 2
- Question: How does rejection sampling during SFT affect generalization to difficult cases where the teacher model's initial predictions exclude the ground truth?
- Basis in paper: [inferred] The paper filters out "samples where the ground truth does not appear in the model's Top-K predictions," retaining only 1,000 samples, potentially creating selection bias toward already-solvable cases.
- Why unresolved: The paper does not evaluate performance on "hard" samples or analyze whether the model fails precisely when personality signals are most ambiguous.
- What evidence would resolve it: Evaluation on held-out difficult samples excluded by rejection sampling, compared against baseline models trained without filtering on the full data distribution.

### Open Question 3
- Question: What mechanisms explain the mutual dependency between NDCG reward and dimension similarity reward during GRPO training?
- Basis in paper: [inferred] The ablation study shows "the NDCG reward collapses abruptly during training without the dimension similarity reward" and vice versa, but the underlying cause remains unexplained.
- Why unresolved: Understanding this interdependency is critical for designing robust rule-based reward functions for subjective ranking tasks where reward hacking risks are elevated.
- What evidence would resolve it: Analysis of reward landscape dynamics, visualization of policy gradient contributions from each component, and experiments with alternative reward formulations that decouple position-sensitive and correctness signals.

### Open Question 4
- Question: Does the validity ceiling of self-reported MBTI labels limit achievable performance, and how would models perform against professionally assessed personality ground truth?
- Basis in paper: [inferred] The Kaggle and PANDORA datasets use self-reported labels from social media, which may contain noise from social desirability bias or user misunderstanding of their own traits.
- Why unresolved: If self-reported labels substantially diverge from true personality, the 80.57% Macro-F1 may approach the theoretical maximum, making further gains difficult.
- What evidence would resolve it: Experiments correlating model predictions with validated psychometric assessments, or comparison on datasets labeled through psychologist-administered instruments.

## Limitations

- The ranking paradigm's superiority over classification is theoretically motivated but lacks direct empirical validation against a classification baseline using identical reward structures and data quality
- The SFT data construction process depends heavily on the teacher model's reasoning trace quality, but the prompt template and selection criteria are not fully specified, making reproducibility difficult
- The paper does not include ablation studies isolating the contribution of the ranking formulation from other innovations like reward function design or data quality improvements

## Confidence

- **High Confidence**: The experimental results showing state-of-the-art performance on both Kaggle and PANDORA datasets; the documented training instability when using single-component rewards instead of the dual reward function.
- **Medium Confidence**: The claim that treating personality detection as ranking captures trait interdependencies better than classification; the assertion that the two-stage SFT+RL pipeline is necessary for stable training.
- **Low Confidence**: The specific mechanisms by which the ranking paradigm improves predictions beyond what could be achieved with better reward shaping or data quality; the generalizability of results to other personality frameworks beyond MBTI.

## Next Checks

1. **Ranking vs Classification Isolation Test**: Implement a classification baseline using the same SFT data and reward function structure (treating each MBTI dimension as independent binary classification) and compare performance directly against the ranking approach to isolate the contribution of the paradigm shift.

2. **Teacher Model Sensitivity Analysis**: Vary the quality threshold for SFT data selection and test different teacher models to quantify how much of the performance gain depends on the initial reasoning trace quality versus the training methodology itself.

3. **Cross-Personality Framework Transfer**: Apply the PerDet-R1 methodology to a different personality framework (such as Big Five/OCEAN) to test whether the ranking paradigm generalizes beyond MBTI's four-dimensional structure.