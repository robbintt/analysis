---
ver: rpa2
title: 'good4cir: Generating Detailed Synthetic Captions for Composed Image Retrieval'
arxiv_id: '2503.17871'
source_url: https://arxiv.org/abs/2503.17871
tags:
- image
- retrieval
- dataset
- datasets
- cirr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces good4cir, a pipeline for generating synthetic
  annotations for composed image retrieval (CIR) datasets using vision-language models.
  The approach addresses the challenge of limited, low-quality, or costly manual annotations
  in existing CIR datasets by breaking down the annotation process into three focused
  stages: extracting object-level descriptions from query images, generating consistent
  descriptions for target images, and synthesizing natural language modifications
  between images.'
---

# good4cir: Generating Detailed Synthetic Captions for Composed Image Retrieval

## Quick Facts
- arXiv ID: 2503.17871
- Source URL: https://arxiv.org/abs/2503.17871
- Reference count: 40
- Primary result: A three-stage VLM pipeline generates synthetic CIR annotations that improve retrieval accuracy, particularly for fine-grained modifications.

## Executive Summary
This paper introduces good4cir, a pipeline for generating synthetic annotations for composed image retrieval (CIR) datasets using vision-language models. The approach addresses the challenge of limited, low-quality, or costly manual annotations in existing CIR datasets by breaking down the annotation process into three focused stages: extracting object-level descriptions from query images, generating consistent descriptions for target images, and synthesizing natural language modifications between images. The method reduces hallucination, improves modification diversity, and ensures object-level consistency. Applied to enhance existing datasets (CIRRR) and create new ones (Hotel-CIR), the pipeline improves retrieval accuracy, particularly for fine-grained modifications. Results show that models trained on good4cir-generated datasets achieve higher recall@K and mAP@K scores on benchmarks like CIRR and CIRCO compared to models trained on original datasets.

## Method Summary
The good4cir pipeline uses a three-stage VLM approach to generate synthetic CIR annotations. Stage 1 extracts object-level descriptions from query images as JSON descriptors. Stage 2 generates target image descriptions by comparing against Stage 1 output, ensuring consistency and identifying differences. Stage 3 synthesizes 3-8 text modification captions describing differences between images. A fourth deterministic stage permutes captions and filters combinations exceeding 77 tokens. The method is applied to create CIRRR (enhanced CIRR) and Hotel-CIR (new hotel dataset). For model training, a CLIP ViT-B backbone with 4-layer cross-attention fusion and contrastive loss is used.

## Key Results
- good4cir reduces VLM hallucination by ~50% compared to single-stage prompting
- CIRRR improves CIRR model performance: R@1 increases from 29.9 to 34.1
- Hotel-CIR improves fine-grained retrieval: R@1 reaches 62.4 on hotel-specific test set
- Combined training (CIRR + CIRRR + Hotel-CIR) achieves best generalization: mAP@5 on CIRCO increases from 2.54 to 4.64

## Why This Works (Mechanism)

### Mechanism 1
Breaking CIR annotation into focused sub-tasks reduces VLM hallucination compared to end-to-end prompting. The three-stage pipeline isolates object detection, comparison, and difference synthesis, constraining each stage to a specific cognitive load and reducing context complexity.

### Mechanism 2
Passing structured object descriptors between stages enforces vocabulary consistency and focuses modification text on real differences. Stage 2 receives Stage 1's JSON descriptor list and is explicitly instructed to reuse identical descriptors for matching objects and generate new descriptors only for changed objects.

### Mechanism 3
Caption permutation generates diverse compound modifications that improve model generalization. Stage 4 combines 1-3 individual captions into compound sentences with syntactic variation, training models to handle varying query complexity.

## Foundational Learning

- **Composed Image Retrieval (CIR)**: Why needed: The entire pipeline generates training data for CIR models. Quick check: Given an image of a red chair and text "make it blue," what should the retrieved image contain?
- **Vision-Language Model (VLM) Hallucination**: Why needed: The pipeline's primary design goal is mitigating VLM hallucination. Quick check: Why might a VLM describe a "wooden table" in an image that only contains a metal desk?
- **Contrastive Learning for Retrieval**: Why needed: Section 5 describes training with a contrastive loss where (query_image, modification_text, target_image) triplets form positive pairs. Quick check: In a batch of N triplets, how many negative samples does each positive pair implicitly compare against?

## Architecture Onboarding

- **Component map**: Stage 1 VLM call → Stage 2 VLM call → Stage 3 VLM call → Stage 4 (deterministic) → Training data → Model training
- **Critical path**: Stage 1 → Stage 2 → Stage 3 → Stage 4 → Training data → Model training. Each stage depends on the previous stage's output.
- **Design tradeoffs**: Cost vs. quality (GPT-4o costs ~$200 for CIRRR); granularity vs. abstraction (object-centric focus limits abstract modifications); diversity vs. coherence (permutation may create awkward phrasing).
- **Failure signatures**: Highly similar image pairs → hallucinated differences in Stage 3; dense object scenes → counting errors in descriptors; cross-domain training → domain mismatch degrades performance.
- **First 3 experiments**:
  1. Reproduce Stage 3.5 comparison: Run single-stage vs. three-stage prompting on 50 image pairs; manually annotate hallucination rate.
  2. Ablate Stage 4: Train model on individual captions only vs. with permutations; measure Recall@K delta on CIRCO.
  3. Domain transfer test: Train on Hotel-CIR only, evaluate on CIRR test set to quantify cross-domain generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can a post-processing step effectively increase caption stylistic diversity while maintaining semantic accuracy? The current pipeline relies solely on prompt engineering to achieve diversity, which proves insufficient.

### Open Question 2
Can open-source vision-language models match GPT-4o's performance in generating high-quality CIR annotations through fine-tuning or improved prompting strategies? The paper found GPT-4o significantly superior but did not quantify the performance gap.

### Open Question 3
How can the pipeline be extended to capture abstract, scene-level, or conceptual differences beyond object-level modifications? The current three-stage architecture is fundamentally designed around object extraction and comparison.

## Limitations
- Pipeline cannot effectively capture abstract or holistic style modifications (e.g., "make the room feel more modern")
- Caption permutation may generate semantically redundant or contradictory combinations without quality filtering
- Dependence on expensive closed-source VLMs (GPT-4o) raises scalability concerns

## Confidence
**High confidence**: The core finding that three-stage prompting reduces hallucination compared to single-stage prompting is well-supported by qualitative examples and quantitative performance improvements across multiple benchmarks.

**Medium confidence**: The claim that caption permutation specifically drives generalization improvements is plausible but not directly validated through ablation studies.

**Low confidence**: The open-sourcing timeline and exact implementation details remain uncertain, as code and datasets will be available in the camera-ready version but are not currently accessible.

## Next Checks
1. **Ablation study on permutation strategy**: Train models with only individual captions (no permutations) and compare Recall@K performance to models trained with permuted captions to quantify the specific contribution of caption diversity.

2. **Cross-domain generalization analysis**: Train on Hotel-CIR only and evaluate on CIRR test set (and vice versa) to precisely measure the domain mismatch penalty and identify which types of modifications transfer poorly.

3. **Hallucination rate quantification**: Implement systematic annotation of hallucination rates in Stage 3 outputs across 100 image pairs, comparing the three-stage pipeline against a single-stage baseline to provide quantitative evidence of hallucination reduction.