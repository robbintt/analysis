---
ver: rpa2
title: Unexplored flaws in multiple-choice VQA evaluations
arxiv_id: '2511.22341'
source_url: https://arxiv.org/abs/2511.22341
tags:
- option
- prompt
- format
- accuracy
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multiple-choice visual question answering benchmarks are highly
  sensitive to minor variations in prompt formatting, such as changes to option ID
  sets, delimiters, or separators. This sensitivity significantly impacts model performance
  and benchmarking outcomes, with accuracy differences of up to 59 percentage points
  and model rankings shifting from second-to-last to first.
---

# Unexplored flaws in multiple-choice VQA evaluations

## Quick Facts
- arXiv ID: 2511.22341
- Source URL: https://arxiv.org/abs/2511.22341
- Reference count: 40
- Multiple-choice VQA benchmarks are highly sensitive to minor variations in prompt formatting, impacting model performance and rankings significantly.

## Executive Summary
This study reveals that multiple-choice Visual Question Answering benchmarks are surprisingly sensitive to minor variations in prompt formatting, such as changes to option ID sets, delimiters, or separators. Across seven Multimodal Large Language Models (MLLMs) evaluated on five datasets with 48 prompt format variations, accuracy differences reached up to 59 percentage points and model rankings shifted dramatically. Even high-confidence predictions remain vulnerable to these biases, and existing bias mitigation methods fail to address them. The findings demonstrate that format-induced biases can significantly distort benchmarking outcomes and model comparisons, independent of visual reasoning capability.

## Method Summary
The study evaluated seven MLLMs across five datasets using 48 prompt format variations, systematically varying option IDs (uppercase, lowercase, numbers, Roman numerals), delimiters (dot, colon, bracket, double brackets), and separators (line break, comma, semicolon). Circular evaluation was employed to eliminate position bias, and both accuracy and coverage metrics were measured. Linear mixed models were used to statistically analyze the impact of format changes while controlling for random effects between model-dataset pairs. The analysis included both standard generation accuracy and the Cloze Prompt Length Normalized method.

## Key Results
- Accuracy differences of up to 59 percentage points between different prompt formats
- Model rankings shifted from second-to-last to first place depending on format
- Coverage failures occurred for specific formats (e.g., numerical IDs with LLaVA-1.5 dropped to 0%)
- Existing bias mitigation methods (PriDe, PIA, CP-LN) failed to address format-induced biases

## Why This Works (Mechanism)

### Mechanism 1: Tokenization and Prior Association Bias
MLLM performance fluctuates because specific option IDs (e.g., "A" vs. "1") and delimiters trigger different prior probabilities or token associations in the LLM backbone, independent of visual context. The model assigns probability mass to tokens based on their frequency and context in pre-training data. Changing "A." to "A)" or "1" alters the token sequence and attention context. The paper indicates that uppercase IDs (+6pp) and numbers (-10pp) produce statistically significant deviations.

### Mechanism 2: Instruction-Following Collapse (Coverage Failure)
Complex or non-standard formatting tokens (e.g., double brackets, Roman numerals) cause the model to fail to generate any valid option ID, resulting in "out-of-scheme" answers. The instruction-following capability of MLLMs is brittle. When the prompt structure exceeds the complexity seen during alignment, the model generates conversational filler or incorrect tokens rather than the constrained option ID. This forces accuracy to 0% not because of visual inability, but because of generation constraints.

### Mechanism 3: Contextual Displacement of Visual Reasoning
Prompt formatting biases persist even when the model claims high confidence in the correct answer, suggesting that formatting noise can displace the weight of visual evidence. Even if the model "knows" the answer (high logit for the correct concept), the introduction of a specific format shifts the probability distribution such that a wrong option wins. The paper notes two patterns: uniform degradation (format confuses the model generally) or top-confidence degradation (format specifically destabilizes high-certainty predictions).

## Foundational Learning

- **Concept: Position vs. Format Bias**
  - Why needed here: The paper isolates *format* bias from *order* bias (which is well-known). You must distinguish between "The model prefers the first option" (position) and "The model prefers the letter A" (format) to diagnose errors correctly.
  - Quick check question: If you shuffle the options and the model consistently chooses "B" regardless of content, is this position or format bias? (Answer: Format/ID bias).

- **Concept: Coverage as a Metric**
  - Why needed here: Standard accuracy metrics can be misleading. If a model answers "I think it is Paris" instead of "A", accuracy is 0%, but the reasoning might be correct. "Coverage" measures if the model followed the formatting instructions at all.
  - Quick check question: A model scores 50% accuracy with 100% coverage vs. 50% accuracy with 60% coverage. Which is better? (Answer: The former; the latter is failing to follow instructions nearly half the time).

- **Concept: Linear Mixed Models (LMM) for Bias Analysis**
  - Why needed here: The paper uses LMMs to statistically prove that format changes cause performance shifts, separating them from random noise or dataset difficulty.
  - Quick check question: Why use an LMM instead of a simple average? (Answer: To account for random effects like "Model-Dataset" pairs while isolating fixed effects like "Option ID style").

## Architecture Onboarding

- **Component map:** Image + Prompt -> Vision Encoder + Projector -> LLM Backbone -> Logits for Option IDs
- **Critical path:** The **Prompt Template Constructor**. This is where the flaw resides. The mapping of `Option Text` -> `Option ID` (with delimiters/separators) currently happens arbitrarily, injecting noise into the LLM's context window.
- **Design tradeoffs:**
  - **Generation vs. Perplexity:** Generation (asking the model to say "A") suffers from coverage issues. Perplexity (CP-LN) is robust to format but computationally expensive and sometimes less accurate.
  - **Standardization vs. Robustness:** Standardizing on one format (e.g., "A.") might inflate scores artificially if the model was trained on that format. Using multiple formats reveals true robustness but lowers reported SOTA scores.
- **Failure signatures:**
  - **Zero-Coverage Drop:** Accuracy suddenly drops to ~0% or random baseline when switching from "A." to "1." (indicates instruction-following failure).
  - **Rank Reversal:** Model A beats Model B on Format 1, but loses on Format 2 by 40+ points (indicates overfitting to prompt styles).
  - **Confidence Mismatch:** The model assigns high probability (confidence) to an option, but the output generation fails to match the requested format (e.g., outputs "(A)" when asked for "A").
- **First 3 experiments:**
  1. **Format Sensitivity Sweep:** Run your validation set using the 4 ID types (Upper, Lower, Number, Roman) and 4 delimiters (Dot, Colon, Bracket, Double Bracket). Plot accuracy and coverage heatmaps as seen in Figure 4/5.
  2. **Circular Evaluation Baseline:** Verify if the observed sensitivity is due to *order* (shuffling options) or *format* (changing IDs) by running both variations. The paper uses circular evaluation to eliminate order bias; do the same to isolate format effects.
  3. **CP-LN vs. Vanilla Comparison:** Compare standard generation accuracy against the Cloze Prompt (Length Normalized) method (Appendix B.3) to see if your model's low performance is due to poor reasoning (visual) or poor instruction following (formatting).

## Open Questions the Paper Calls Out

- **Open Question 1:** How can targeted mitigation methods be developed to effectively address prompt format-induced biases in multiple-choice VQA? (The study demonstrates that existing bias mitigation strategies fail to correct for the newly identified sensitivities to option IDs and delimiters.)

- **Open Question 2:** Does training MLLMs with diverse prompt formats successfully enhance robustness against prompt sensitivity in evaluation? (The paper recommends this approach but doesn't conduct experiments on training or fine-tuning models with the proposed data augmentation strategy.)

- **Open Question 3:** What additional, unexplored prompt formatting biases exist in multiple-choice VQA evaluations? (The analysis "does not cover all possible prompt format variations, and additional biases may exist.")

- **Open Question 4:** What are the underlying mechanisms causing the complex interaction between prompt format variations and option position bias? (While the paper quantifies the bias, it does not offer an interpretability analysis of why specific formats trigger specific positional preferences.)

## Limitations
- The analysis focuses on seven specific MLLMs across five datasets, limiting generalizability to other architectures or tasks
- Open-ended format performance comparison is not systematically quantified
- The trade-off between evaluation complexity and bias reduction is not fully explored

## Confidence
- **High Confidence:** The core claim that MLLMs are highly sensitive to minor prompt formatting variations is well-supported by statistical analysis and empirical validation across 48 prompt permutations
- **Medium Confidence:** The proposed mechanisms (tokenization bias, instruction-following collapse, contextual displacement) are plausible but not fully proven through ablation studies
- **Low Confidence:** The effectiveness of proposed mitigation strategies (disclosure, diverse training formats, open-ended evaluation) is assumed rather than empirically tested

## Next Checks
1. **Mechanism Isolation Experiment:** Design an ablation study to separate tokenization bias from instruction-following failure by testing whether models that fail on numerical IDs would succeed with explicit instructions for numerical responses.

2. **Cross-Task Generalization:** Apply the 48 prompt format variations to non-VQA tasks like visual reasoning, image captioning, or document understanding to confirm this is a fundamental MLLM limitation rather than VQA-specific.

3. **Bias Mitigation Validation:** Implement the three recommendations (disclosure, diverse training, open-ended evaluation) and measure their effectiveness by training a model on uniformly sampled prompt formats and comparing its sensitivity to the baseline.