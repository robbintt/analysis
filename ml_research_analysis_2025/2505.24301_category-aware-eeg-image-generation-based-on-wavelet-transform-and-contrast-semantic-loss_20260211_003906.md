---
ver: rpa2
title: Category-aware EEG image generation based on wavelet transform and contrast
  semantic loss
arxiv_id: '2505.24301'
source_url: https://arxiv.org/abs/2505.24301
tags:
- features
- images
- image
- visual
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reconstructing visual stimuli
  from EEG signals, a crucial step in brain-computer interfaces. The authors propose
  a novel framework that integrates Discrete Wavelet Transform (DWT) and a gated attention
  mechanism within a transformer-based EEG encoder to extract meaningful features
  from noisy time-series EEG data.
---

# Category-aware EEG image generation based on wavelet transform and contrast semantic loss

## Quick Facts
- **arXiv ID:** 2505.24301
- **Source URL:** https://arxiv.org/abs/2505.24301
- **Reference count:** 6
- **Primary result:** 43% single-subject classification accuracy on THINGS-EEG, outperforming state-of-the-art methods

## Executive Summary
This paper proposes a novel framework for reconstructing visual stimuli from EEG signals using a transformer-based encoder that integrates Discrete Wavelet Transform (DWT) and a gated attention mechanism. The model employs a dual-loss mechanism combining CLIP loss, MSE loss, and category-aware clustering loss to enhance feature alignment and classification accuracy. Experimental results on the THINGS-EEG dataset demonstrate state-of-the-art performance with a maximum single-subject accuracy of 43%, along with improved semantic alignment in reconstructed images validated through a novel semantic-based score metric.

## Method Summary
The approach uses a transformer-based EEG encoder that first applies channel-wise 1D Haar wavelet decomposition to separate approximation and detail coefficients, then fuses the reconstructed signal with original features. Local and global features are extracted in parallel through convolutional and transformer branches, combined via a sigmoid-gated attention mechanism. The model is trained with a tri-loss combination (CLIP alignment, MSE, category-aware clustering) to align EEG features with image/text embeddings and improve semantic separation. Generated images are produced using a pre-trained diffusion model (SDXL-Turbo) and evaluated through both classification accuracy and a semantic-based score using ConvNext and WordNet similarity.

## Key Results
- Achieves maximum single-subject classification accuracy of 43% on THINGS-EEG dataset
- Ablation study shows DWT contributes ~6% accuracy improvement (27.35% to 33.4%)
- Category-aware clustering loss provides modest but measurable improvement (~0.6% accuracy)
- Semantic-based score demonstrates improved semantic alignment in reconstructed images

## Why This Works (Mechanism)

### Mechanism 1: Wavelet-Domain Denoising and Feature Preservation
The DWT block applies channel-wise 1D decomposition using the Haar wavelet, separating approximation coefficients (low-frequency trends) from detail coefficients (high-frequency transients). An inverse DWT reconstructs the signal, which is fused with original temporal features via learned convolution. This captures slow neural dynamics and abrupt signal changes simultaneously while suppressing noise through selective reconstruction.

### Mechanism 2: Gated Attention for Adaptive Local-Global Feature Balancing
The gated attention mechanism selectively balances local temporal patterns and global dependencies by combining convolutional local features with transformer-based global features using adaptive weights. This addresses functional heterogeneity of EEG channels across brain regions by learning to balance local and global feature contributions per channel.

### Mechanism 3: Category-aware Clustering Loss for Semantic Separation
The category-aware clustering loss improves semantic alignment by clustering same-category EEG features and separating different categories in high-dimensional space. The contrastive formulation requires no absolute labels—only relative similarity within each batch—enabling the model to learn discriminative features even when training and testing categories differ.

## Foundational Learning

- **Discrete Wavelet Transform (1D, single-level):** Understanding how DWT separates approximation (low-freq) and detail (high-freq) coefficients is essential for debugging the DWT block and interpreting feature fusion. *Quick check:* Given a 100-sample EEG epoch, what are the dimensions of cA1 and cD1 after single-level Haar decomposition? (Answer: 50 each)

- **Contrastive Learning without Hard Labels:** The category-aware loss constructs positive/negative pairs dynamically per batch; understanding this distinguishes it from standard supervised classification. *Quick check:* How does the margin M in the clustering loss affect false positive/negative rates in feature clustering?

- **CLIP-based Multimodal Alignment:** The model aligns EEG features with CLIP image/text embeddings; understanding CLIP's joint embedding space is critical for interpreting alignment loss. *Quick check:* If L_align is removed, why does accuracy collapse to near-random (Table 3 shows 0.8% mean)? What does this imply about the learned EEG representation?

## Architecture Onboarding

- **Component map:** Input EEG (64 channels × 100 timepoints) → DWT Block → Local/Global feature extraction → Gated Attention fusion → CLIP alignment + MSE + Clustering loss → Diffusion model → Generated image → ConvNext classification → Semantic score

- **Critical path:** 1) EEG preprocessing: 1000Hz → bandpass 0.1-100Hz → baseline correction → downsample to 100Hz → trial extraction [-200ms, 800ms]; 2) DWT decomposition per channel → fusion with original features; 3) Parallel local/global feature extraction → gated combination; 4) CLIP alignment loss + MSE loss + category-aware clustering loss; 5) Inference: EEG → encoder → diffusion model → generated image → ConvNext classification → semantic score

- **Design tradeoffs:** Haar wavelet chosen for time-localization over frequency resolution; deeper wavelet decomposition could capture finer frequency bands but increases computation. Pre-trained diffusion model (SDXL-Turbo) avoids costly training but limits generation to SDXL's prior; categories outside its training distribution may fail. Category-aware loss enables zero-shot but provides modest accuracy gain (~0.6% mean improvement per ablation).

- **Failure signatures:** L_align removal causes total collapse (~0.8% accuracy); CLIP alignment is non-optional. Removing DWT drops mean accuracy from 33.4% to 27.35%, indicating frequency features are critical. Specific categories (e.g., "bator4") fail generation due to ambiguous labels lacking semantic grounding or diffusion model unfamiliarity. Subject variability: sub-05 achieves only 26.0% top-1 vs. sub-08's 43.0%, suggesting inter-subject feature distribution shift.

- **First 3 experiments:** 1) Baseline encoder ablation: Train with L_align + L_MSE only (no L_C, no DWT) to establish lower bound; expect ~27-30% accuracy per ablation table. 2) DWT-only validation: Replace gated attention with standard transformer; isolate DWT contribution. If accuracy stays near 33%, DWT is dominant; if it drops, attention is critical. 3) Per-category semantic score analysis: Generate images for all test categories, compute semantic scores, and correlate with (a) training sample count per category, (b) label clarity (WordNet path depth). Identify systematic failure modes (e.g., abstract vs. concrete objects).

## Open Questions the Paper Calls Out

### Open Question 1
Can the integration of interpretable deep learning models or traditional EEG analysis methods enhance the extraction of robust features and the interpretability of the reconstruction process? The current model relies on complex deep learning components which may act as "black boxes," obscuring the specific neural correlates of the extracted semantic features.

### Open Question 2
To what extent does developing customized pre-training strategies for diffusion models improve the generation of images for categories currently lacking visual representations? The current reliance on general pre-trained weights (SDXL-Turbo) limits the model when specific test categories are absent or underrepresented in the diffusion model's original training distribution.

### Open Question 3
Can more sophisticated architectures or loss functions further bridge the gap between noisy EEG signals and semantic representations in the latent space? While the proposed dual-loss mechanism improves alignment, the semantic scores remain moderate (mean 0.383), and classification accuracy varies significantly across subjects (26% to 43%), indicating room for improvement in feature consistency.

## Limitations

- Substantial subject variability (26.0% to 43.0% accuracy) suggests learned representations may not generalize uniformly across individuals
- Reliance on pre-trained diffusion model (SDXL-Turbo) introduces potential bottlenecks for categories outside its training distribution
- Category-aware clustering loss provides only modest gains (~0.6% accuracy), raising questions about complexity justification

## Confidence

- **High confidence:** The core framework architecture (DWT + gated attention + CLIP alignment) is well-specified and quantitative results are directly supported by the experimental section
- **Medium confidence:** The semantic-based score metric is internally consistent but relies on proxy measures (WordNet similarity) rather than direct human evaluation of image quality
- **Low confidence:** The claim of "potential zero-shot discrimination capabilities" from category-aware clustering is supported by ablation but not fully demonstrated in cross-category generalization experiments

## Next Checks

1. **Subject generalization test:** Train on 6 subjects, test on held-out subject to measure cross-subject performance and validate claims about inter-subject variability

2. **Human evaluation of semantic alignment:** Conduct AMT-style perceptual study comparing generated images to ground truth stimuli, measuring human-rated semantic similarity

3. **Category-specific failure analysis:** Systematically analyze categories with lowest semantic scores and accuracy to identify whether failures stem from label ambiguity, insufficient training samples, or diffusion model limitations