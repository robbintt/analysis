---
ver: rpa2
title: Continual Learning Beyond Experience Rehearsal and Full Model Surrogates
arxiv_id: '2505.21942'
source_url: https://arxiv.org/abs/2505.21942
tags:
- learning
- sparc
- tasks
- task
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPARC addresses continual learning by eliminating the need for
  experience rehearsal and full model surrogates through parameter-efficient depth-wise
  separable convolutions and task-specific working memories. It achieves superior
  performance on Seq-TinyImageNet with only 6% of the parameters required by full-model
  surrogates, while matching rehearsal-based methods on various CL benchmarks.
---

# Continual Learning Beyond Experience Rehearsal and Full Model Surrogates

## Quick Facts
- arXiv ID: 2505.21942
- Source URL: https://arxiv.org/abs/2505.21942
- Authors: Prashant Bhat; Laurens Niesten; Elahe Arani; Bahram Zonooz
- Reference count: 40
- One-line primary result: SPARC achieves competitive continual learning performance without experience rehearsal or full model surrogates using depth-wise separable convolutions and task-specific working memories

## Executive Summary
SPARC presents a novel continual learning approach that eliminates the need for experience rehearsal and full model surrogates through parameter-efficient depth-wise separable convolutions and task-specific working memories. The method achieves superior performance on Seq-TinyImageNet with only 6% of the parameters required by full-model surrogates while matching rehearsal-based methods on various CL benchmarks. By combining task-specific working memories with task-agnostic semantic memory for cross-task knowledge consolidation, SPARC addresses catastrophic forgetting while maintaining parameter efficiency across different task sequences.

## Method Summary
SPARC uses depth-wise separable convolutions to isolate task-specific parameters (working memory) while sharing semantic knowledge across tasks through exponential moving average updates of point-wise filters. The architecture grows linearly with tasks but remains parameter-efficient compared to full-model surrogates. Training involves optimizing only current task parameters on cross-entropy loss, followed by consolidation of shared filters and weight re-normalization to mitigate task-recency bias. The approach works for both Class-IL and Task-IL settings, with inference requiring forward passes through all task sub-networks in Class-IL scenarios.

## Key Results
- Achieves superior performance on Seq-TinyImageNet with only 6% of parameters compared to full-model surrogates
- Matches rehearsal-based methods on various continual learning benchmarks without requiring experience replay
- Demonstrates scalability across different task sequences (Seq-CIFAR100, Seq-TinyImageNet, Seq-MiniImageNet) with competitive results in both Class-IL and Task-IL settings

## Why This Works (Mechanism)

### Mechanism 1: Parameter Isolation with Depth-wise Separable Convolutions
- **Claim:** Parameter isolation using DSCs mitigates catastrophic forgetting by eliminating parameter overlap between tasks while maintaining parameter efficiency.
- **Mechanism:** SPARC replaces standard convolutions with DSCs, isolating depth-wise filters (spatial features) as task-specific working memories while partially sharing point-wise filters (channel mixing). This reduces parameters by a factor of h×w compared to standard convolutions.
- **Core assumption:** Spatial features are highly task-specific and prone to interference, whereas cross-channel correlations contain generalizable semantic information.
- **Evidence anchors:** Abstract states elimination of experience rehearsal through DSCs; section 3.1 describes DSC parameter efficiency and implicit regularization.
- **Break condition:** If tasks require significant spatial reuse (fine-grained distinctions), freezing depth-wise filters may restrict plasticity and lead to underfitting on later tasks.

### Mechanism 2: Cross-Task Knowledge Consolidation via EMA
- **Claim:** Consolidating cross-task knowledge via EMA of shared point-wise filters allows positive forward transfer without explicit data replay.
- **Mechanism:** SPARC divides point-wise filters into task-specific and task-agnostic sets. After training task t, shared filters are updated as EMA of weights from previous task, mimicking slow-learning semantic memory.
- **Core assumption:** A shared feature subspace exists across tasks that can be captured by linear combinations without destabilizing earlier tasks.
- **Evidence anchors:** Abstract mentions semantic memory for cross-task consolidation; section 3.2 describes EMA-based knowledge consolidation.
- **Break condition:** If task sequence is non-stationary (domain shift), EMA may fail to adapt fast enough, causing negative transfer.

### Mechanism 3: Weight Re-normalization for Class-IL Bias Mitigation
- **Claim:** Weight re-normalization in the classifier layer mitigates task-recency bias inherent in parameter isolation methods during Class-IL.
- **Mechanism:** SPARC calculates IQR of activations for each task and re-scales classifier weights/biases to equalize magnitudes, preventing models from favoring tasks with higher weight norms.
- **Core assumption:** Task bias is primarily a function of output logit magnitude rather than feature embedding quality.
- **Evidence anchors:** Abstract mentions weight re-normalization mitigates task-specific biases; section 3.3 describes sequential weight magnitude issues in Class-IL.
- **Break condition:** If activation distributions are heavily skewed or contain extreme outliers, IQR-based normalization may fail to center weights effectively.

## Foundational Learning

- **Concept:** Depth-wise Separable Convolutions (DSCs)
  - **Why needed here:** SPARC relies on DSCs for structural basis of parameter isolation; understanding factorization into depth-wise and point-wise operations is essential.
  - **Quick check question:** Can you calculate the parameter reduction ratio when replacing a 3×3 standard conv with a DSC for 64 input and 128 output channels?

- **Concept:** Class-Incremental Learning (Class-IL) vs. Task-Incremental Learning (Task-IL)
  - **Why needed here:** Weight re-normalization is critical only for Class-IL where task identity is unknown at inference; understanding the difference is essential for diagnosing task-specific bias.
  - **Quick check question:** In Class-IL, must the model determine which task the sample belongs to before classifying the class, or does it do both simultaneously?

- **Concept:** Complementary Learning Systems (CLS) Theory
  - **Why needed here:** SPARC explicitly grounds its working memory (fast-learning) and semantic memory (slow-learning) in this biological theory; understanding fast vs. slow learning dynamic explains EMA updates.
  - **Quick check question:** Which component of SPARC corresponds to the "fast-learning" hippocampus, and which to the "slow-learning" neocortex?

## Architecture Onboarding

- **Component map:** Backbone (ResNet-18 with DSCs) -> Working Memory (task-specific depth-wise + half point-wise + task-specific BN + classifier head) -> Semantic Memory (shared point-wise filters via EMA) -> Controller (weight re-normalization)

- **Critical path:**
  1. **Task Init:** Instantiate new task-specific sub-network (Depth-wise + Half Point-wise)
  2. **Training:** Optimize only current task's parameters on Dt using Cross-Entropy
  3. **Consolidation:** Update Semantic Memory (shared Point-wise weights) using EMA of current weights
  4. **Normalization:** Record max activations on Dt, calculate scaling factor η, re-normalize classifier weights Wt
  5. **Freeze:** Lock current task parameters

- **Design tradeoffs:**
  - **Linear Growth:** Unlike fixed-capacity methods, SPARC grows linearly with tasks but remains cheaper than PNNs due to DSCs
  - **Inference Cost:** Class-IL requires forward passes through all sub-networks (concatenating outputs), increasing latency with task count

- **Failure signatures:**
  - **High Task-IL / Low Class-IL gap:** Class-IL performance disproportionately low suggests weight re-normalization failing to align logit magnitudes
  - **Capacity Saturation:** If semantic memory dominates early tasks and performance drops on later tasks, EMA momentum α may be too high
  - **Overfitting on Buffer:** If implemented with buffer (accidentally), working memories may overfit

- **First 3 experiments:**
  1. **Ablation on Isolation:** Run Seq-CIFAR100 with standard convolutions vs. DSCs to verify parameter efficiency claim and performance delta
  2. **Re-normalization Stress Test:** Evaluate Class-IL accuracy with and without weight re-normalization to visualize task-recency bias
  3. **Scalability Limit:** Run 20-task sequence to plot parameter growth curve and confirm bounds of 6% claim relative to baselines

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SPARC's working and semantic memory framework be effectively adapted to Vision Transformers (ViTs)?
  - **Basis in paper:** Authors state SPARC is optimized for CNN architectures and list extending framework to ViTs as primary future research goal
  - **Why unresolved:** Architecture relies on depth-wise separable convolutions for parameter efficiency, not directly transferable to self-attention layers
  - **What evidence would resolve it:** Successful implementation using adapter modules or LoRA within ViT maintaining comparable parameter efficiency and Class-IL performance

- **Open Question 2:** Does dynamic resource allocation based on task difficulty improve SPARC's efficiency compared to current static allocation strategy?
  - **Basis in paper:** Authors note static allocation ignores task difficulty, potentially leading to insufficient capacity or over-parameterization, suggesting dynamic allocation as future work
  - **Why unresolved:** Current model allocates fixed filters per task regardless of complexity, potentially wasting resources on easy tasks or underperforming on hard ones
  - **What evidence would resolve it:** Modified SPARC adjusting sub-network width/depth based on task complexity metrics, demonstrating higher accuracy or fewer total parameters

- **Open Question 3:** How can SPARC be modified to operate in task-free or online continual learning scenarios where explicit task boundaries are unavailable?
  - **Basis in paper:** Authors list known task boundary assumption as limitation, stating this information is not always available in real-world settings
  - **Why unresolved:** Current architecture relies on distinct task boundaries to instantiate new sub-networks and switch/consolidate semantic memories
  - **What evidence would resolve it:** Integration of automatic task boundary detection mechanism or soft-allocation strategy allowing SPARC to function without oracle task identifiers

## Limitations

- Critical implementation details remain underspecified including optimizer choice (SGD vs. Adam), exact data augmentation strategy, and semantic memory update schedule
- Performance claims may be misleading if benchmarks use larger buffers than stated, as paper claims to eliminate rehearsal entirely
- Long-term scalability beyond 20 tasks is not demonstrated, leaving questions about parameter growth and performance degradation in extended sequences
- Comparison to rehearsal-based methods may be unfair if those methods use significantly more computational resources or memory

## Confidence

- **High Confidence** (Core Mechanism): Depth-wise separable convolution-based parameter isolation approach is well-defined and theoretically sound with explicit mathematical framework
- **Medium Confidence** (Empirical Claims): Performance results on Seq-TinyImageNet and benchmark comparisons are reported but lack full methodological transparency and variance measures
- **Low Confidence** (Reproducibility): Critical hyperparameters including optimizer type, weight decay, and data augmentation pipeline are not specified with apparent contradictions in semantic memory update rules

## Next Checks

1. **Parameter Efficiency Verification**: Implement ablation comparing standard convolutions vs. depth-wise separable convolutions on Seq-CIFAR100 to measure actual parameter reduction ratio and validate 6% claim relative to full-model surrogates

2. **Weight Re-normalization Impact**: Run controlled experiments on Class-IL benchmarks with and without weight re-normalization step to quantify task-recency bias reduction and verify κ=5 hyperparameter effectiveness

3. **Scalability Stress Test**: Extend task sequence to 20+ tasks and measure both parameter growth (to confirm linear scaling) and performance degradation over time, particularly focusing on when semantic memory's EMA updates become insufficient for maintaining plasticity