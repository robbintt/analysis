---
ver: rpa2
title: Learning Robust Negation Text Representations
arxiv_id: '2507.12782'
source_url: https://arxiv.org/abs/2507.12782
tags:
- negation
- text
- data
- hedging
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving negation and hedging
  understanding in text encoders and large language models (LLMs). The core method
  involves creating a contrastive learning dataset, HedgeTriple, using diverse negation
  and hedging patterns distilled from LLMs, grounded in linguistic taxonomies.
---

# Learning Robust Negation Text Representations

## Quick Facts
- arXiv ID: 2507.12782
- Source URL: https://arxiv.org/abs/2507.12782
- Reference count: 31
- Primary result: Contrastive fine-tuning on taxonomy-grounded negation/hedging data improves negation understanding benchmarks while preserving general capabilities

## Executive Summary
This work addresses the challenge of improving negation and hedging understanding in text encoders and large language models (LLMs). The core method involves creating a contrastive learning dataset, HedgeTriple, using diverse negation and hedging patterns distilled from LLMs, grounded in linguistic taxonomies. Finetuning a BERT-based model on this dataset significantly improves negation understanding on benchmarks like NevIR and ExcluIR while maintaining general capabilities, with optimal performance achieved at around 150K training samples. The approach also benefits LLMs, enhancing their negation understanding at the cost of minor arithmetic reasoning degradation. Results demonstrate that diversity in data patterns is more impactful than quantity, and combining both negation and hedging data yields the best performance.

## Method Summary
The method generates a contrastive learning dataset (HedgeTriple) by prompting an LLM with taxonomy-structured instructions to create negated and hedged variants of sentence anchors from existing corpora. The dataset contains 31K anchors with 4 negation and 2 hedging variants each, filtered by Levenshtein distance to ensure minimal pairs. A BERT-based model is fine-tuned using Multiple Negative Ranking Loss (MNRL) on these triples, treating hedged variants as positives and negated variants as hard negatives. The approach is evaluated on negation understanding benchmarks (NevIR, ExcluIR, CANNOT, M3-Counterfactual) and general capability benchmarks (MTEB), with ablation studies confirming the importance of pattern diversity over quantity.

## Key Results
- Contrastive fine-tuning on HedgeTriple improves NevIR RR from 8.10 to 40.56 while maintaining MTEB performance
- Pattern diversity in negation/hedging data is more important than data quantity for effective learning
- Hedging data inclusion helps retain general capabilities while improving negation understanding
- LLM fine-tuning shows similar negation improvements but with minor arithmetic reasoning degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit hard negatives with high lexical overlap force the model to learn semantic rather than surface-level distinctions.
- Mechanism: The HedgeTriple dataset provides anchor–negation pairs that are topically similar but semantically contradictory. Contrastive learning (MNRL) pushes these apart in embedding space, requiring the encoder to attend to negation cues rather than relying on shared vocabulary.
- Core assumption: Negated sentences retain enough surface similarity to serve as effective hard negatives (empirically validated by post-hoc analysis of Levenshtein distance thresholds).
- Evidence anchors:
  - [abstract] "We adopt a standard contrastive learning strategy to finetune a strong BERT-based model, and observe large improvement in negation understanding capabilities."
  - [section] Section 3.2: "We explicitly provide hard negative samples which have high lexical overlap but contradictory meaning."
  - [corpus] Limited direct corpus support; related work on negation-aware training (TNG-CLIP) similarly uses synthetic negatives but for vision-language models.
- Break condition: If anchor–negation pairs drift too far in surface form (e.g., >10 words difference), the contrastive signal weakens; the filtering step in Section 3.1.3 addresses this.

### Mechanism 2
- Claim: Linguistic taxonomy-grounded prompts yield higher pattern diversity, which transfers better than larger-scale but pattern-repetitive data.
- Mechanism: The generation process instructs an LLM using explicit categories from the Pullum & Huddleston negation taxonomy (verbal, absolute, affixal, lexical) and the Crompton hedging taxonomy (single-word, multi-word cues). This produces variation in negation placement and hedging construction, preventing the model from overfitting to a single transformation type.
- Core assumption: LLMs can follow taxonomy-structured instructions reliably; manual prompt iteration was used to achieve this (Section 7 acknowledges this limitation).
- Evidence anchors:
  - [section] Section 3.1.2: "We adopt the taxonomy of negation from Pullum and Huddleston (2002). It provides linguistic definitions along with a sample of typical constructions."
  - [section] Section 5.1: "diversity in negation and hedging patterns plays a bigger role than quantity."
  - [corpus] Neighbor paper "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers" similarly argues taxonomy-driven approaches improve negation handling.
- Break condition: If the LLM collapses to a few default patterns despite prompt constraints (noted in hedging generation, requiring explicit cue injection), diversity degrades.

### Mechanism 3
- Claim: Including hedged variants as positives mitigates catastrophic forgetting of general capabilities while still separating negation semantics.
- Mechanism: The triple structure (anchor, hedged-positive, negated-negative) creates a semantic gradient: hedged statements are semantically closer to the original assertion than negated ones. This provides a smoother learning signal and preserves performance on tasks requiring uncertainty or qualified statements.
- Core assumption: Hedged statements are meaningfully more similar to affirmatives than negations; this is explicitly stated as the design motivation.
- Evidence anchors:
  - [section] Section 3.1: "we assume that a hedged variant is more similar in meaning to the original than the negated text."
  - [section] Section 5.1 (ablation): "finetuning on hedging data is beneficial in retaining general capabilities."
  - [corpus] No direct corpus evidence on hedging as a regularizer; this is a relatively novel finding in this paper.
- Break condition: If hedging cues are too weak or rare in the target domain, the regularization benefit diminishes; hedging-only training underperforms on negation benchmarks.

## Foundational Learning

- Concept: **Contrastive Learning (Multiple Negative Ranking Loss)**
  - Why needed here: The entire training pipeline is built on MNRL; understanding how in-batch negatives and explicit hard negatives interact is essential for debugging embedding quality.
  - Quick check question: Can you explain why MNRL with explicit hard negatives differs from standard contrastive learning with only in-batch sampling?

- Concept: **Negation and Hedging as Modality Markers**
  - Why needed here: The method treats these as distinct semantic operations; knowing how they differ (negation flips truth value, hedging reduces certainty) clarifies why triple construction works.
  - Quick check question: Given "The treatment is effective," would "The treatment might be effective" or "The treatment is not effective" be closer in embedding space after training?

- Concept: **Catastrophic Forgetting in Fine-tuning**
  - Why needed here: The paper explicitly navigates a negation–general performance tradeoff; understanding this helps interpret ablation results and choose training set sizes.
  - Quick check question: What would you expect to happen if you trained on 200K negation-only samples without hedging data?

## Architecture Onboarding

- Component map:
  - Anchor selection from existing corpora (SNLI, Multi-NLI, etc.) -> Taxonomy-guided LLM generation of negated/hedged variants -> Levenshtein filtering -> MPNet backbone + MNRL loss with (anchor, hedged-positive, negated-negative) -> Evaluation on negation and general benchmarks

- Critical path:
  1. Anchor selection from existing corpora (SNLI, Multi-NLI, sentence-compression, Simple Wikipedia, COCO Captions)
  2. Taxonomy-guided LLM generation of negated/hedged variants
  3. Filtering for minimal pairs (Levenshtein distance < 60)
  4. Contrastive fine-tuning with explicit negatives
  5. Evaluation on both negation and general benchmarks

- Design tradeoffs:
  - **Data size vs. general capability**: More negation samples improve negation benchmarks but degrade general performance (optimal ~150K)
  - **Diversity vs. quantity**: Taxonomy-driven diversity outperforms larger-scale but repetitive synthetic data (HedgeMPNet outperforms NegMPNet with 80K samples and Jina with 50K)
  - **LLM adaptation cost**: Fine-tuning Llama-3-8B improves negation but degrades arithmetic reasoning (GSM8K drops from 75.36 to 67.10)

- Failure signatures:
  - **Low diversity in hedging**: LLM defaults to a few common cues; solution is explicit cue injection from curated lists
  - **Excessive lexical drift**: Generated pairs that omit key content from anchors; addressed by Levenshtein filtering
  - **Arithmetic regression in LLM adaptation**: Observed inverted calculations and loss of quantitative commonsense; no mitigation proposed in this work

- First 3 experiments:
  1. **Baseline replication**: Fine-tune MPNet on HedgeTriple and reproduce the NevIR score improvement (target: ~40 RR vs. 8.10 baseline).
  2. **Ablation on data composition**: Train with negation-only, hedging-only, and combined data; verify that combined data yields best negation–general balance.
  3. **Taxonomy coverage analysis**: Audit generated samples to confirm all negation types (verbal, absolute, affixal, lexical) appear with sufficient frequency; if skewed, adjust prompt weighting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the taxonomy-based synthesis strategy be effectively adapted to improve negation robustness in multilingual text encoders?
- Basis in paper: [explicit] In the Limitations section, the authors state: "As a starting point, we focused exclusively on English, but the same strategy can be readily adapted to other languages."
- Why unresolved: The linguistic taxonomies for negation and hedging (Pullum & Huddleston; Crompton) used to prompt the LLM are English-specific, and it is unconfirmed if the synthetic data generation method transfers effectively to languages with different negation syntax.
- What evidence would resolve it: Replicating the fine-tuning process on a multilingual encoder (e.g., mBERT) using translated or language-specific taxonomies and evaluating on non-English negation benchmarks.

### Open Question 2
- Question: Does incorporating explicit reasoning steps into the fine-tuning instructions enhance LLM negation understanding beyond the current pairwise setup?
- Basis in paper: [explicit] In the Limitations section regarding finetuning strategies, the authors suggest: "finetuning using more diverse instructions with a reasoning step would likely unlock more sophisticated negation reasoning abilities."
- Why unresolved: The current study converts triples into simple "Yes/No" pairs for LLM fine-tuning, which may limit the model's ability to learn complex logical relationships compared to a chain-of-thought approach.
- What evidence would resolve it: A comparative experiment fine-tuning LLMs on HedgeTriple converted into a reasoning-intensive format (e.g., CoT) vs. the current pairwise format, measured on complex negation benchmarks.

### Open Question 3
- Question: What is the mechanistic cause of the performance degradation in arithmetic reasoning (specifically "inverted calculations") resulting from negation-focused fine-tuning?
- Basis in paper: [inferred] The paper notes a performance drop on GSM8K and identifies specific error types like "inverted calculation" (swapping operands) in Appendix C, suggesting a conflict between negation sensitivity and mathematical logic that is not explained.
- Why unresolved: The authors observe the correlation (negation robustness negatively impacting arithmetic) and the symptoms, but the paper does not investigate why learning to process "not" interferes with the positional or logical processing required for subtraction or division.
- What evidence would resolve it: A mechanistic analysis (e.g., probing attention heads) comparing how mathematical operands are represented before and after negation fine-tuning to identify interference patterns.

## Limitations

- The method is currently limited to English language negation and hedging patterns, with uncertain transferability to other languages
- Fine-tuning on negation data causes unintended degradation in arithmetic reasoning capabilities in LLMs
- Critical training hyperparameters (learning rate, batch size, optimization schedule) are not specified in the paper

## Confidence

- **High confidence**: The core mechanism of using contrastive learning with hard negatives to improve negation understanding is well-supported. The taxonomy-grounded generation approach is clearly described and its benefits over pattern-repetitive data are demonstrated.
- **Medium confidence**: The optimal training set size (~150K samples) and the relative importance of diversity over quantity are empirically observed but may depend on the specific model architecture and training regime used.
- **Low confidence**: The claim that hedging data prevents catastrophic forgetting lacks strong empirical backing beyond the ablation studies presented. The mechanism by which hedging acts as a regularizer remains somewhat speculative.

## Next Checks

1. **Cross-linguistic transferability test**: Generate a small HedgeTriple-like dataset for a morphologically rich language (e.g., Finnish or Turkish) and evaluate whether the same contrastive approach transfers, particularly for negation patterns that differ structurally from English.

2. **Arithmetic capability preservation**: Design a controlled experiment that explicitly measures quantitative reasoning before and after negation fine-tuning, isolating whether the degradation is specific to arithmetic or affects broader numerical commonsense.

3. **Zero-shot negation generalization**: Test whether models trained on HedgeTriple can correctly handle novel negation patterns (e.g., "neither...nor" constructions or complex embedded negations) that were not present in the training data, to validate true understanding versus pattern memorization.