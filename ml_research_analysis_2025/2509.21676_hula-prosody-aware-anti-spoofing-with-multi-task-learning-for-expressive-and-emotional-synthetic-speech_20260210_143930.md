---
ver: rpa2
title: 'HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for Expressive
  and Emotional Synthetic Speech'
arxiv_id: '2509.21676'
source_url: https://arxiv.org/abs/2509.21676
tags:
- speech
- hula
- learning
- detection
- asvspoof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting expressive and
  emotional synthetic speech, which current anti-spoofing systems struggle with due
  to their lack of prosody-aware modeling. The proposed HuLA framework introduces
  a two-stage prosody-aware multi-task learning approach that combines self-supervised
  learning (SSL) representations with explicit prosodic supervision.
---

# HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for Expressive and Emotional Synthetic Speech

## Quick Facts
- **arXiv ID**: 2509.21676
- **Source URL**: https://arxiv.org/abs/2509.21676
- **Authors**: Aurosweta Mahapatra; Ismail Rasim Ulgen; Berrak Sisman
- **Reference count**: 40
- **Primary result**: HuLA achieves 17.34% EER on ASVspoof 2024 Track 1, significantly outperforming strong baselines on expressive and emotional synthetic speech detection.

## Executive Summary
Current anti-spoofing systems struggle with expressive and emotional synthetic speech due to their inability to capture prosodic nuances. The proposed HuLA framework addresses this limitation through a two-stage prosody-aware multi-task learning approach that combines self-supervised learning representations with explicit prosodic supervision. By first learning natural prosodic variation from real speech before exposure to synthetic speech, HuLA develops a strong understanding of prosodic patterns that enables it to detect the subtle mismatches present in expressive and emotional synthetic speech. The method consistently outperforms established baselines across multiple challenging datasets including ASVspoof 2024, EmoFake, and cross-lingual scenarios.

## Method Summary
HuLA employs a two-stage training paradigm using XLS-R 300M as the backbone. In Stage 1, the model is fine-tuned exclusively on real speech (LibriSpeech 100h) with auxiliary tasks of F0 prediction and voiced/unvoiced classification to learn natural prosodic patterns. In Stage 2, the model is jointly optimized for spoof detection and prosody tasks on both real and synthetic data (ASVspoof 2019). The architecture uses frame-level F0 values extracted via DIO algorithm as regression targets and binary V-UV classification as auxiliary supervision. Speaker-wise F0 normalization is applied to stabilize training across different speakers. The model uses weighted loss combining spoof detection, F0 prediction, and V-UV classification, with RawBoost augmentation applied during training.

## Key Results
- HuLA achieves 17.34% EER on ASVspoof 2024 Track 1, outperforming strong baselines
- The model demonstrates excellent performance on emotional speech with 3.01% EER on EmoFake dataset
- Cross-lingual robustness is demonstrated with 13.51% EER on Spanish HABLA and 32.50% EER on Mandarin ADD 2022 Track 1
- HuLA consistently outperforms AASIST and SSL-SLS baselines across all evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
Learning natural prosodic variation from real speech before exposure to synthetic speech improves detection of expressive fakes. The two-stage training paradigm enables the model to internalize natural prosodic patterns in Stage 1, then detect mismatches between learned natural prosody and imperfect synthetic reproduction in Stage 2. This works because synthetic speech systems imperfectly reproduce F0 contours and voiced/unvoiced structure, creating detectable deviations from natural prosody.

### Mechanism 2
Explicit frame-level prosodic supervision combined with SSL embeddings provides complementary detection signals. Frame-level F0 values serve as regression targets while binary V-UV classification helps the model learn temporal prosodic structure. These auxiliary tasks force the SSL backbone to encode prosodic information that might otherwise be underemphasized in generic speech representations.

### Mechanism 3
Speaker-wise F0 normalization stabilizes training and improves cross-speaker generalization. F0 values are normalized using speaker-specific mean and standard deviation computed only on voiced frames, reducing inter-speaker variability while preserving within-speaker prosodic patterns relevant to authenticity detection.

## Foundational Learning

- **Concept: Fundamental Frequency (F0) and Prosody**
  - Why needed here: F0 is the acoustic correlate of pitch and central to prosody. The entire method relies on F0 prediction as an auxiliary task. Without understanding F0 contours, you cannot interpret why synthetic speech deviates from natural patterns
  - Quick check question: Given a speech frame, can you explain why F0 = 0 indicates an unvoiced segment and what this means for prosody modeling?

- **Concept: Multi-Task Learning (MTL) with Auxiliary Tasks**
  - Why needed here: The core architecture uses weighted loss combining spoof detection (L_CLS), F0 prediction (MSE), and V-UV classification (BCE). Understanding gradient interactions between tasks is essential for debugging and tuning loss weights (α=0.4, β=0.2)
  - Quick check question: If increasing α (F0 weight) improves prosody accuracy but degrades spoof detection, what tradeoff does this represent?

- **Concept: Self-Supervised Learning (SSL) for Speech (XLS-R/wav2vec 2.0)**
  - Why needed here: XLS-R 300M provides the backbone representations. Understanding how SSL models encode speech (convolutional encoder + transformer layers) and what fine-tuning changes is necessary for architectural decisions
  - Quick check question: Why does the paper use a weighted sum of all transformer layers in Stage 2 instead of only the final layer, and what does this suggest about layer specialization?

## Architecture Onboarding

- **Component map**:
  XLS-R 300M backbone (convolutional encoder → transformer layers) → Pro-MTL module (Linear→GRU→two heads) → spoof classifier (weighted layer sum) → output

- **Critical path**:
  1. Load pre-trained XLS-R 300M
  2. **Stage 1**: Train on real speech only (LibriSpeech 100h) with Pro-MTL module using L = MSE(F0) + 0.3×BCE(V-UV). Save checkpoint
  3. **Stage 2**: Initialize from Stage 1, add spoof classifier, train on ASVspoof 2019 with RawBoost augmentation using L = L_CLS + 0.4×MSE(F0) + 0.2×BCE(V-UV)
  4. **Inference**: Load Stage 2 model, discard Pro-MTL heads, use only spoof classifier

- **Design tradeoffs**:
  - Two-stage vs. single-stage: Two-stage generalizes better to out-of-domain (17.34% vs. 23.12% on ASVspoof 2024) but sacrifices in-domain accuracy (0.48% vs. 0.80% EER on ASVspoof 2019)
  - Pro-MTL module architecture: GRU chosen for temporal modeling with 256 hidden size; larger capacity may overfit
  - Loss weights (α=0.4, β=0.2): Empirically set; higher prosody weights may improve expressive detection but hurt neutral speech performance

- **Failure signatures**:
  - Low F0 prediction accuracy → backbone not learning prosodic features; check DIO extraction quality, normalization, or learning rates
  - Good in-domain, poor OOD performance → model overfitting to dataset artifacts; verify Stage 1 training on diverse real speech
  - High EER on emotional speech specifically → prosody module may not capture expressive variation; consider expanding Stage 1 data
  - Cross-lingual degradation → Stage 1 English-only prosody may not transfer; consider multilingual Stage 1 data

- **First 3 experiments**:
  1. **Baseline ablation**: Train single-stage HuLA on ASVspoof 2019, evaluate on ASVspoof 2019, 2021, 2024, and EmoFake to isolate contribution of prosody-aware MTL
  2. **Loss weight sensitivity**: Systematically vary α and β to determine optimal configuration for expressive vs. neutral speech detection
  3. **Stage 1 data composition**: Train Stage 1 on LibriSpeech vs. LibriSpeech + emotional speech to test whether diverse prosodic exposure enhances generalization

## Open Questions the Paper Calls Out

The paper explicitly states that future work will extend HuLA by incorporating richer prosodic and paralinguistic features such as jitter, shimmer, and speaking rate, which were not implemented in the current framework.

## Limitations

- The two-stage training paradigm, while improving OOD generalization, significantly reduces in-domain accuracy compared to single-stage approaches
- Speaker-wise F0 normalization may not generalize equally well across languages with substantially different prosodic systems, as evidenced by performance degradation on Mandarin
- The paper does not explore whether alternative architectures or single-stage training with stronger regularization could achieve similar results without the complexity of two-stage training

## Confidence

- **High confidence**: The core experimental results showing HuLA's superior performance on ASVspoof 2024 Track 1 (17.34% EER) and EmoFake (3.01% EER) are well-supported by reported metrics and ablation studies
- **Medium confidence**: The mechanism explaining why two-stage training improves OOD generalization is plausible but not conclusively proven
- **Medium confidence**: The architectural claim that explicit prosodic supervision is necessary is supported by ablation but not rigorously tested against alternatives

## Next Checks

1. **Cross-lingual Stage 1 validation**: Train Stage 1 on multilingual speech (Common Voice) and evaluate whether this improves cross-lingual OOD performance, particularly on Mandarin and Spanish test sets

2. **Single-stage with regularization comparison**: Implement a single-stage HuLA variant with stronger L2 regularization and compare against two-stage approach on ASVspoof 2024 and EmoFake to determine if simpler regularization achieves similar generalization

3. **Loss weight sensitivity on expressive speech**: Systematically vary α and β specifically on EmoFake and Mixed Emotions datasets to determine if heavier prosody weighting improves detection of emotional speech at the cost of neutral speech performance