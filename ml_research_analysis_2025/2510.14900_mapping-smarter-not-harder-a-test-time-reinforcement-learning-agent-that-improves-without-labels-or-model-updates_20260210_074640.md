---
ver: rpa2
title: 'Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That
  Improves Without Labels or Model Updates'
arxiv_id: '2510.14900'
source_url: https://arxiv.org/abs/2510.14900
tags:
- schema
- mapping
- confidence
- evidence
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a test-time reinforcement learning agent
  for schema mapping that self-improves without labeled data or model updates. The
  agent identifies ambiguous field mappings, formulates targeted web-search queries,
  and uses confidence-based rewards to iteratively refine its mappings.
---

# Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates

## Quick Facts
- arXiv ID: 2510.14900
- Source URL: https://arxiv.org/abs/2510.14900
- Authors: Wen-Kwang Tsao; Yao-Ching Yu; Chien-Ming Huang
- Reference count: 12
- This paper introduces a test-time reinforcement learning agent for schema mapping that self-improves without labeled data or model updates.

## Executive Summary
This paper presents a novel approach to schema mapping that enables an LLM to improve its performance at test time without labeled data or model updates. The system uses confidence-based proxy rewards derived from prediction consistency across multiple inferences, combined with targeted web search for evidence collection. When tested on converting Microsoft Defender for Endpoint logs to a common schema, the approach achieved 93.94% accuracy after 100 iterations, significantly outperforming both LLM-only (56.4%) and RAG-based baselines (72.73%). The method demonstrates how confidence scores can serve as effective proxy rewards in scenarios where ground truth labels are unavailable.

## Method Summary
The approach employs a test-time reinforcement learning agent that improves schema mapping through confidence-based evidence accumulation. For each field, the LLM generates n=3 predictions using prompt variants, and confidence is calculated as the consistency across these predictions. When conflicts are detected (disagreement across predictions), the agent formulates targeted web search queries to gather external evidence. Evidence is added to context only if it improves confidence. The learning occurs through context enrichment rather than weight updates, with the policy π(a|s) being the LLM's mapping decision given current evidence. The system runs for up to 100 iterations, with reward r_t = C_{t+1} - C_t guiding evidence acceptance.

## Key Results
- Improved accuracy from 56.4% (LLM-only) to 93.94% over 100 iterations using GPT-4o
- Reduced low-confidence mappings requiring expert review by 85%
- Demonstrated effective self-improvement in industrial settings without ground truth labels

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Based Proxy Rewards
Claim: Prediction consistency across multiple inferences can serve as a trainable reward signal when ground truth labels are unavailable.
Mechanism: For each source field, the model generates n=3 predictions using prompt variants; confidence C = count(most_frequent_prediction) / adjusted_total. The reward r_t = C_{t+1} − C_t guides whether newly collected evidence should be retained in context. Evidence is added only if confidence improves (Algorithm 1, lines 9–11).
Core assumption: Prediction consistency correlates with correctness.
Evidence anchors: [abstract] "By treating confidence scores as a proxy for correctness, the system demonstrated effective self-improvement in scenarios where ground truth labels are unavailable."

### Mechanism 2: Targeted External Evidence Collection
Claim: Web-search-derived evidence can resolve semantically ambiguous field mappings that static knowledge bases cannot.
Mechanism: When conflict detection identifies disagreement across n=3 prompt-variant predictions for a field, the agent formulates a targeted search query (e.g., Microsoft Defender field definitions), retrieves evidence, and evaluates whether adding it to context improves confidence.
Core assumption: External web sources contain authoritative, domain-specific definitions that resolve ambiguity.
Evidence anchors: [abstract] "The agent identified ambiguous mappings, generated targeted web search queries to gather evidence, and applied confidence-based rewards to iteratively refine its decisions."

### Mechanism 3: Verbal, Memory-Based Policy Improvement
Claim: An LLM's mapping policy can improve at test time without weight updates by accumulating useful evidence in context.
Mechanism: The policy π(a|s) is the LLM's mapping decision given current evidence context. Learning occurs via context update: evidence that increases confidence is preserved; unhelpful evidence is discarded.
Core assumption: Context enrichment with relevant evidence causally improves mapping decisions.
Evidence anchors: [abstract] "A reinforcement learning agent improved schema mapping accuracy from 72.73% to 93.94% over 100 iterations without labeled data or model updates."

## Foundational Learning

- Concept: Self-Consistency (Wang et al., 2022)
  - Why needed here: Forms the theoretical basis for confidence calculation—consistency across multiple reasoning paths indicates reliability.
  - Quick check question: If you run the same prompt 3 times and get 3 different answers, should confidence be high or low?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Baseline 2 (72.73%) uses RAG with internal KB; the RL agent extends this by dynamically retrieving external evidence when internal KB is insufficient.
  - Quick check question: What happens when your internal KB doesn't contain documentation for a newly seen vendor schema?

- Concept: Verbal Reinforcement Learning (Reflexion)
  - Why needed here: This is the learning paradigm—policy improvement via linguistic feedback stored in memory, not weight updates.
  - Quick check question: How does an agent improve its policy if it cannot modify model parameters?

## Architecture Onboarding

- Component map: System Prompt -> User Prompt -> LLM -> Conflict Detector -> Search Prompt -> Evidence Collector -> Confidence Evaluator -> Context Manager
- Critical path: 1. User Prompt → LLM generates n=3 mapping predictions per field. 2. Conflict Detector identifies fields with disagreement across predictions. 3. For each conflict: Search Prompt → Query → Evidence Collector retrieves external info. 4. Confidence Evaluator compares new vs. prior confidence for the field. 5. If reward (confidence delta) > 0: Context Manager adds evidence to E. 6. Repeat for α=100 iterations or until convergence.
- Design tradeoffs: n=3 vs n=10 inferences (3 is cost-efficient with strong accuracy; 10 improves calibration but increases compute ~3.3x); Evidence scope (internal KB only vs. external web search); Iteration limit (more iterations enable more evidence accumulation but with diminishing returns).
- Failure signatures: Overconfidence regime (Confidence → 1.0 while accuracy plateaus); Evidence rejection cascade (19% of iterations reject new evidence); Context pollution (Low-quality or contradictory evidence degrades performance); Sparse documentation (In domains without web-accessible schema definitions).
- First 3 experiments: 1. Reproduce baseline comparison (LLM-only, RAG with internal KB, RL agent for 10 iterations) on small schema subset. 2. Ablate confidence threshold (modify reward condition to require larger confidence deltas). 3. Calibrate overconfidence (compare n=3 vs n=10 inference settings on same schema).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to handle complex mapping cardinalities (1-to-N, N-to-M)?
- Basis in paper: [explicit] The authors state in the Limitations section that "Extension to more complex mapping cardinalities... remains future work."
- Why unresolved: The current confidence metric relies on the frequency of a single prediction, which fails to account for scenarios where a source field correctly maps to multiple targets.
- What evidence would resolve it: A modified reward function that successfully optimizes for precision/recall on multi-target mapping tasks without manual intervention.

### Open Question 2
- Question: Does the approach generalize to domains outside of cybersecurity, such as healthcare or finance?
- Basis in paper: [explicit] The Limitations section notes that "validation in other domains (healthcare, finance, etc.) would strengthen the generalizability claims."
- Why unresolved: Schema semantics and documentation availability differ by industry; it is unclear if confidence serves as a reliable proxy for correctness outside of cybersecurity logs.
- What evidence would resolve it: Experimental results replicating the 72% to 94% improvement trajectory on standard healthcare or financial schema matching benchmarks.

### Open Question 3
- Question: Can ensemble-based or self-assessed confidence metrics mitigate the "overconfidence gap"?
- Basis in paper: [explicit] The Conclusion suggests future work could use "multiple models" or "directly prompting LLMs to provide explicit self-assessed confidence scores."
- Why unresolved: The current method often shows confidence (1.0) exceeding accuracy (~94%), indicating poor calibration that makes the system's stopping criterion less reliable.
- What evidence would resolve it: A comparative study demonstrating that alternative metrics maintain a tighter correlation with accuracy throughout the iteration process.

### Open Question 4
- Question: How robust is the agent when external evidence is low-quality or adversarial?
- Basis in paper: [inferred] The Limitations section notes "Evidence Quality Dependency" and Ethical Considerations warns that web evidence "could occasionally include malicious or adversarial content."
- Why unresolved: While the paper demonstrates success with a search tool, it does not quantify performance degradation when retrieved documents provide conflicting or incorrect definitions.
- What evidence would resolve it: An ablation study measuring accuracy retention when the external search tool is configured to return noisy or misleading results.

## Limitations
- The confidence-based reward signal may not generalize beyond well-documented domains with authoritative web sources
- Single model evaluation (GPT-4o) limits generalizability claims to other LLM architectures
- The 85% reduction in low-confidence mappings doesn't address whether remaining ambiguous mappings can be resolved at all

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core mechanism of using confidence consistency as proxy reward signal | High |
| 85% reduction in low-confidence mappings | Medium |
| Industrial applicability claim | Medium |

## Next Checks
1. **Domain transferability test**: Apply the RL agent to schema mapping tasks from vendors with minimal public documentation to measure degradation in accuracy and evidence quality.
2. **Multi-LLM validation**: Run the complete 100-iteration pipeline with different LLM backends (Claude, Llama, Gemini) to assess whether confidence-based rewards generalize across model architectures.
3. **Evidence quality audit**: Sample and manually evaluate the retrieved evidence tuples to quantify the precision of search-derived information and identify systematic failure patterns in evidence collection.