---
ver: rpa2
title: 'The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge
  Entanglement-Behavior Framework'
arxiv_id: '2510.25732'
source_url: https://arxiv.org/abs/2510.25732
tags:
- knowledge
- unlearning
- entanglement
- factual
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SKEB (Stimulus-Knowledge Entanglement-Behavior
  Framework) to evaluate unlearning robustness in LLMs by investigating how persuasive
  prompt framing can recover supposedly forgotten factual knowledge. Drawing from
  ACT-R and Hebbian theories, SKEB models knowledge entanglement through domain graphs
  and tests how rhetorical strategies (emotional, logical, authority) interact with
  semantic connectivity to bypass unlearning.
---

# The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework

## Quick Facts
- arXiv ID: 2510.25732
- Source URL: https://arxiv.org/abs/2510.25732
- Authors: Aakriti Shah; Thai Le
- Reference count: 25
- Key outcome: SKEB framework shows persuasive prompt framing (especially authority) can recover supposedly forgotten factual knowledge, with entanglement metrics predicting unlearning vulnerability.

## Executive Summary
This paper introduces the SKEB (Stimulus-Knowledge Entanglement-Behavior) framework to evaluate unlearning robustness in LLMs by investigating how persuasive prompt framing can recover supposedly forgotten factual knowledge. Drawing from ACT-R and Hebbian theories, SKEB models knowledge entanglement through domain graphs and tests how rhetorical strategies interact with semantic connectivity to bypass unlearning. Results demonstrate that authority framing substantially enhances factual knowledge recall (14.8% baseline vs 24.5% with authority framing), with effectiveness inversely correlated to model size. The framework develops nine graph-based entanglement metrics, with distance-weighted influence (M9) showing the strongest predictive power (r=0.77) for factual recall.

## Method Summary
The SKEB framework constructs domain graphs from the Harry Potter corpus (1,296 entities, 35,922 edges weighted by chapter co-occurrence), then generates 300 base prompts and 3 persuasive variants each (emotional, logical, authority) using GPT-4. Four unlearned models (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B) are evaluated, with responses classified by an ensemble of judge models (gpt-4o-mini, gpt-4.1-mini, gpt-5-nano) into factual/non-factual/hallucination categories. Nine entanglement metrics M1-M9 are computed for each prompt's induced subgraph, with logistic regression using M9, M4, and M3 predicting response behavior. Unlearning uses WHP gradient ascent with specified hyperparameters (batch_size=1, lr=1e-4, 3 epochs, 4 GPUs).

## Key Results
- Authority framing substantially enhances factual knowledge recall (14.8% baseline vs 24.5% with authority framing)
- Effectiveness inversely correlated to model size (128% recovery in 2.7B vs 15% in 13B models)
- Distance-weighted influence (M9) shows strongest predictive power for factual recall (r=0.77)
- Predictive model explains 78% of variance in unlearning robustness
- Different architectures show distinct correlation patterns after unlearning

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Entanglement Enables Recovery via Spreading Activation
Unlearning suppresses but does not erase knowledge; densely connected concepts resist removal because activation can spread through alternative pathways. Domain graphs model co-occurring entities as weighted edges, with prompts activating subgraphs and activation spreading proportionally to edge weights (M9: distance-weighted influence). Higher entanglement = more retrieval pathways.

### Mechanism 2: Rhetorical Framing Modulates Activation Thresholds
Persuasive framing (authority, emotion, logic) systematically alters which semantic pathways are engaged. Authority prompts activate more densely connected subgraphs (normalized entanglement: 98.57 vs. 29.99 baseline), creating more spreading activation routes to suppressed knowledge.

### Mechanism 3: Model Size Inversely Correlates with Recovery Vulnerability
Larger models (13B) resist persuasive recovery better than smaller models (2.7B), showing 15% vs. 128% factual recall gains under authority framing. Assumption—larger models develop higher activation thresholds or better recognize manipulative framing, treating reframed queries as categorically different.

## Foundational Learning

- **Spreading Activation (ACT-R / Hebbian Theory)**: Understanding that activation decays with distance explains why M9 predicts recall. Quick check: If node A activates node B with weight 0.8 and B activates C with weight 0.5, what is A's indirect influence on C?

- **Graph-Based Semantic Representation**: The framework models knowledge as domain graphs (V, E, d); entanglement metrics quantify structural properties like density, redundancy, and centrality. Quick check: What does high subgraph density (M6) imply about retrieval robustness?

- **Persuasion Theory (Authority, Emotion, Logic)**: Framing conditions determine what knowledge WILL be activated; understanding Cialdini's principles explains why authority framing outperforms emotion. Quick check: Why might authority framing increase factual recall but also hallucination risk?

## Architecture Onboarding

- **Component map**: Domain Graph Constructor -> Prompt Transformer -> Entanglement Calculator -> Unlearning Processor -> Behavior Evaluator -> Predictive Model

- **Critical path**: Prompt → Entity extraction → Induced subgraph → M9 calculation → Model response → Judge evaluation → Correlation with entanglement. M9 is the strongest predictor; errors in graph construction or entity extraction cascade to all downstream metrics.

- **Design tradeoffs**: Co-occurrence graphs are computationally tractable but may not reflect true internal representations; judge ensemble (GPT-based) introduces dependency on external models; WHP unlearning is one method—other approaches may show different robustness profiles.

- **Failure signatures**: Low inter-judge agreement (<90%) indicates ambiguous evaluation criteria; correlation collapse after unlearning (e.g., M2: 0.837 → -0.017) suggests genuine pathway disruption vs. suppression; high hallucination with high entanglement indicates over-activation of semantically distant associations.

- **First 3 experiments**: (1) Replicate M9 correlation on different domain to test generalizability; (2) Ablate framing types to test authority advantage when controlling for prompt length/complexity; (3) Cross-architecture transfer: train predictive model on LLaMA-2-7B, test on LLaMA-3.1-8B.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do SKEB's entanglement metrics and persuasion vulnerability findings generalize to sensitive real-world domains (PII, medical records, copyrighted material)? Basis: "Whether our findings generalize to more sensitive domains (PII, harmful content, copyrighted material) remains an open research direction."

- **Open Question 2**: Does the inverse relationship between model size and persuasion vulnerability persist, reverse, or saturate at larger scales (70B+ parameters)? Basis: "The inverse size-vulnerability relationship we observe might reverse at much larger scales or saturate at some threshold."

- **Open Question 3**: Can architectural innovations (modular memory systems, causal isolation) achieve robust unlearning that resists persuasive framing attacks? Basis: "Achieving truly robust unlearning may require architectural innovations (modular memory systems, causal isolation of knowledge components) rather than just scaling existing designs."

- **Open Question 4**: Do different unlearning methods (influence functions, model editing) produce distinct robustness profiles against entanglement-based attacks? Basis: "Other unlearning approaches (influence functions, model editing) might show different robustness profiles."

## Limitations

- Mechanistic link between co-occurrence graphs and internal knowledge representations remains correlational rather than causal
- Generalization beyond Harry Potter domain is untested; framework's effectiveness across diverse knowledge domains remains an open question
- Judge model reliability depends on external models (gpt-4o-mini, gpt-4.1-mini, gpt-5-nano) that may introduce unexplained variance

## Confidence

- **High**: Framework's ability to predict unlearning vulnerability from entanglement metrics (M9 correlation r=0.77), inverse relationship between model size and recovery effectiveness (r=-0.89), comparative effectiveness of authority framing (14.8% vs 24.5% baseline)
- **Medium**: Causal interpretation that unlearning suppresses rather than erases knowledge, assumption that co-occurrence graphs accurately represent internal connectivity, transferability of entanglement-behavior mappings across architectures
- **Low**: Precise mechanisms by which rhetorical framing alters activation patterns, long-term stability of unlearning under repeated persuasive attacks, framework's effectiveness on non-fictional knowledge domains

## Next Checks

1. **Cross-domain validation**: Apply SKEB to factual domains (scientific facts, historical events) to test whether entanglement metrics maintain predictive power outside fictional narratives

2. **Human judge validation**: Re-run a subset of evaluations with human judges to verify that judge model classifications align with human assessment of factual recall vs. hallucination

3. **Alternative unlearning method comparison**: Test whether SKEB predicts recovery vulnerability across different unlearning approaches (influence functions, model editing) to assess method generalizability