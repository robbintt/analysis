---
ver: rpa2
title: 'ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant
  Simulation'
arxiv_id: '2509.21730'
source_url: https://arxiv.org/abs/2509.21730
tags:
- user
- recommendations
- recommendation
- assistant
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProPerSim, a simulation-based task and benchmark
  designed to develop AI assistants that are both proactive and personalized. The
  core method involves training an assistant, ProPerAssistant, using retrieval-augmented
  generation and preference alignment via Direct Preference Optimization, based on
  feedback from a user agent with diverse personas in a simulated home environment.
---

# ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation

## Quick Facts
- arXiv ID: 2509.21730
- Source URL: https://arxiv.org/abs/2509.21730
- Reference count: 40
- Primary result: Simulation-based training improves AI assistant personalization scores from 2.2 to 3.3/4 across 32 diverse personas

## Executive Summary
This paper introduces ProPerSim, a simulation-based framework for developing proactive and personalized AI assistants without requiring real human interaction data. The approach trains an assistant, ProPerAssistant, using feedback from simulated user agents with rich personas defined by Big Five personality traits and additional attributes. Through 14 days of simulated interaction across 32 personas, the assistant learns to adapt recommendations to individual preferences via retrieval-augmented generation and Direct Preference Optimization. The method demonstrates significant improvement in personalization scores while outperforming baseline approaches.

## Method Summary
ProPerSim simulates a home environment where a user agent with a rich persona (defined by Big Five personality traits plus age, background, interests, lifestyle, daily plan requirements, and long-term goals) interacts with an AI assistant. The assistant uses retrieval-augmented generation with compressed interaction history and retrieved similar past experiences to make proactive recommendations. Feedback from the user agent creates preference pairs that train the assistant via Direct Preference Optimization. The simulation runs for 14 days per persona with 2.5-minute timesteps, generating approximately 10 days of interaction per persona on A100 GPUs.

## Key Results
- ProPerAssistant improves personalization scores from 2.2 to 3.3 out of 4 over 14 simulated days
- The approach outperforms No Memory, AR Memory, and ARS Memory baselines
- High-openness personas show lower improvement rates due to the current rubric not rewarding novelty
- Complex personas with conditional preferences (e.g., specific timing requirements) score significantly lower than simple personas

## Why This Works (Mechanism)

### Mechanism 1: Simulation-Based Persona-Driven Feedback Loop
User agents with rich personas generate realistic behavioral signals that enable preference learning without human data collection. Each user agent is defined by Big Five personality traits plus six attributes, guiding action generation and evaluation through personalized rubrics. Feedback creates training pairs for preference optimization. Core assumption: LLM-based agents with detailed personas produce preference signals that transfer to real human preferences. Evidence: Human evaluators rated agent action naturalness at 8.25/10 and persona alignment at 8.02/10.

### Mechanism 2: Explicit Preference Training via Direct Preference Optimization
Training with explicit preference pairs outperforms in-context reward signals alone. For each user action, the assistant generates n=2 candidate recommendations, which the user agent scores, creating (chosen, rejected) pairs stored in a replay buffer. DPO training samples 200 pairs per run, updating model weights to increase likelihood of preferred responses. Core assumption: Preference pairs from simulation provide sufficient gradient signal for meaningful behavioral change.

### Mechanism 3: Retrieval-Augmented State for Contextual Decision-Making
Combining compressed interaction history with retrieved similar past experiences improves personalization quality. Internal state contains structured summaries (recent 10 minutes detailed, older interactions compressed into 4 hourly + 3 four-hour blocks) and top-5 similar past pairs retrieved via OpenAI embeddings. Core assumption: Similar past situations provide actionable guidance for current decisions.

## Foundational Learning

- Concept: **Big Five Personality Traits as Behavioral Priors**
  - Why needed here: The entire simulation framework depends on personality traits driving consistent behavior and evaluation patterns. Without understanding how Extraversion, Agreeableness, Openness, Conscientiousness, and Neuroticism map to preferences, you cannot debug persona behavior.
  - Quick check question: Given a persona with high Openness and low Conscientiousness, would they prefer frequent novel suggestions or structured, predictable interactions?

- Concept: **Direct Preference Optimization (DPO) vs. RLHF**
  - Why needed here: The paper claims DPO enables efficient preference learning without complex RL pipelines. Understanding the difference helps evaluate whether this choice is appropriate for your use case.
  - Quick check question: DPO requires paired preference data. If your feedback is scalar ratings rather than pairwise comparisons, what preprocessing step is required?

- Concept: **Replay Buffer for Training Stability**
  - Why needed here: The paper samples 200 examples from accumulated buffer per training run, inspired by RL. This prevents catastrophic forgetting and overfitting to recent experiences.
  - Quick check question: If training after only 50 interactions, why might sampling 200 examples from the buffer cause issues?

## Architecture Onboarding

- Component map:
  User Agent (LLM + Persona + Rubric) -> Simulation Environment (Smallville-style home) -> ProPerAssistant (LLaMA 3.3 70B 4-bit + QLoRA) -> Internal State (Structured memory + Retrieval) -> DPO Training Loop

- Critical path:
  1. Persona definition -> Rubric personalization
  2. Action generation -> Recommendation decision
  3. Evaluation -> Preference pair creation
  4. Buffer accumulation -> DPO training
  5. Model update -> Next iteration

- Design tradeoffs:
  - **n=2 candidates**: Low compute cost, limited preference signal. Increasing n improves training diversity but adds API costs linearly.
  - **T=2.5 min timesteps**: Finer granularity approaches real-time but increases simulation length. Current: ~10 days/persona on A100.
  - **200-sample buffer sampling**: Balances stability vs. forgetting. Too small -> overfitting; too large -> slow adaptation.

- Failure signatures:
  - Score plateaus early (<Day 3): Check if preference pairs have sufficient distinction (chosen vs. rejected scores differ).
  - High frequency persists (>15 recs/hour): DPO may not be learning "No Recommendation" as valid action; check reward signal for null actions.
  - Persona-specific collapse (some personas fail, others succeed): Inspect rubric complexity; complex preferences (multi-dimensional timing, context-sensitive content) underperform.

- First 3 experiments:
  1. **Baseline ablation**: Run No Memory, AR Memory, ARS Memory, and ProPerAssistant on same 4 personas for 5 days. Verify paper's claim that explicit DPO training outperforms in-context reward signals.
  2. **Persona complexity analysis**: Identify highest/lowest scoring personas from your run. Compare their rubric structure (simple vs. complex timing/content rules) to validate the paper's hypothesis about preference complexity.
  3. **Timestep sensitivity**: Run T=5min vs T=2.5min on 2 personas. Measure whether finer granularity improves Timing scores or just adds compute without benefit.

## Open Questions the Paper Calls Out

### Open Question 1
Would incorporating an explicit "Diversity/Novelty" objective into the evaluation rubric improve adaptation for high-openness personas? The current method reinforced already well-rated recommendations rather than novelty, causing low-openness personas to benefit disproportionately more than high-openness ones. A follow-up experiment comparing high-openness persona scores between the standard rubric and a modified rubric with diversity weighting.

### Open Question 2
To what extent does the preference alignment learned from LLM-based user agents transfer to real human users? The paper relies entirely on a simulated environment using LLMs to generate user actions and evaluations, motivated by privacy and cost challenges of collecting real human data. While human evaluators validated the realism of the agents' behavior, the efficacy of training on agent feedback for real-world deployment remains a hypothesis. Deployment of ProPerAssistant with human participants to compare live satisfaction ratings with the simulation's predicted scores.

### Open Question 3
Can the framework be enhanced to better handle personas with highly conditional or multidimensional preferences? The analysis of "Simple vs. Complex" preferences notes the worst-performing persona had nuanced demands (e.g., specific 6–9 AM analytical content vs. 9 PM introspective content) that posed a greater challenge for consistent personalization. The current architecture achieved significantly lower scores for these complex personas, suggesting the memory or reasoning mechanisms are insufficient for strict context-dependency. Experiments using augmented memory modules or hierarchical planning targeting the identified "worst-case" personas to observe score convergence.

## Limitations
- Simulation-based training may not accurately reflect real human preferences despite high agent realism ratings
- Complex personas with conditional or multidimensional preferences show significantly lower improvement rates
- The method requires substantial computational resources (A100 GPUs) for full-scale simulation

## Confidence

- **High confidence**: The simulation methodology and technical implementation details are clearly specified and reproducible. The score improvement trajectory (2.2 → 3.3/4) is directly measured within the simulation.
- **Medium confidence**: The relative performance of ProPerAssistant compared to baselines is credible, but the absolute score improvements may be specific to the simulation design.
- **Low confidence**: The claim that learned preferences transfer to real human users is speculative without external validation.

## Next Checks

1. **Human-in-the-loop validation**: Test ProPerAssistant with 10-20 real users representing diverse personas to measure actual preference alignment vs. simulation predictions.

2. **Cross-environment transfer**: Deploy the same trained assistant in a different simulated environment (e.g., office vs. home) to test generalization of learned preferences.

3. **Rubric sensitivity analysis**: Systematically vary rubric complexity and attribute weights across personas to identify which preference dimensions are most reliably learned through simulation.