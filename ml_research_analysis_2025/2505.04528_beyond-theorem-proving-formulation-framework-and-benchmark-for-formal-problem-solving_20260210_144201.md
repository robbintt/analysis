---
ver: rpa2
title: 'Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving'
arxiv_id: '2505.04528'
source_url: https://arxiv.org/abs/2505.04528
tags:
- answer
- proof
- problem
- formal
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of a rigorous formulation and evaluation
  framework for formal problem-solving in AI. The authors propose Formal Problem-Solving
  (FPS), a framework that treats problem-solving as a deterministic Markov decision
  process within formal theorem proving environments like Lean 4.
---

# Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving

## Quick Facts
- arXiv ID: 2505.04528
- Source URL: https://arxiv.org/abs/2505.04528
- Reference count: 40
- Primary result: FPS framework with RPE evaluation achieves 23.77% solving rate on FormalMath500 vs 47.55% proving rate; D-FPS shows higher human-alignment with near-zero incorrect submissions

## Executive Summary
This paper addresses the gap between theorem proving and problem-solving in AI by introducing Formal Problem-Solving (FPS), a framework that treats problem-solving as a deterministic Markov decision process within formal theorem proving environments. The authors construct three benchmarks (FormalMath500, MiniF2F-Solving, PutnamBench-Solving) and introduce Restricted Propositional Equivalence (RPE) as a symbolic evaluation method. Experiments show that while proving rates are significantly higher than solving rates, D-FPS improves human-alignment by reducing incorrect submissions despite lower solving rates.

## Method Summary
The FPS framework formulates problem-solving as a deterministic MDP where models construct proofs to find answers represented as metavariables. The process enforces constructive proofs by coupling metavariables with proof goals, ensuring process-level verification. For find-all problems, D-FPS separates solving and verification into forward (deriving answer from givens) and backward (verifying answer implies problem condition) reasoning. RPE evaluates answers by testing propositional equivalence under restricted proof automation (rfl, norm_num, ring_nf, rw_search, aesop), achieving 100% precision and 97.18% recall on human-annotated data.

## Key Results
- Solving rates: 23.77% (FormalMath500), 27.47% (MiniF2F-Solving), 0.31% (PutnamBench-Solving)
- Proving rates: 47.55%, 53.60%, 1.54% respectively (roughly double solving rates)
- D-FPS shows significantly lower incorrect submissions (NE-Submitted) compared to FPS
- RPE achieves 100% precision, 97.18% recall, and 0.9732 Cohen's kappa on human-annotated test set

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FPS enables process-verified problem-solving by coupling metavariables with proof goals, ensuring the answer is derived constructively rather than guessed.
- **Mechanism:** The framework initializes a proof state with a metavariable hole (?a, ?w) representing the unknown answer, then couples it with the main goal. Tactics must fill this hole while proving the goal, which Lean's kernel tracks. Upon completion, the answer term is extracted from metavariable assignments. This guarantees P(â) holds for any extracted answer â.
- **Core assumption:** The proof assistant's kernel correctly tracks metavariable dependencies and doesn't allow non-constructive shortcuts (e.g., axiom of choice applied retroactively).
- **Evidence anchors:** Abstract states "FPS ensures process-level verification by coupling metavariables and enforcing constructive proofs"; Section 3.2 describes splitting queriable a as metavariable ?w by apply Exists.intro and coupling with main goal.
- **Break condition:** If the proof assistant allows axioms that bypass constructive requirements (Lean does assume axiom of choice), shortcuts become possible.

### Mechanism 2
- **Claim:** D-FPS improves human-alignment by enforcing deductive forward reasoning before backward verification, reducing incorrect submissions.
- **Mechanism:** D-FPS splits the proof goal into two sub-goals: forward (h.mp) and backward (h.mpr). The forward goal must derive the answer proposition A from hypotheses via deductive steps. The backward goal verifies that A implies the original problem condition. This structure forces the model to reason from givens to answer, matching human problem-solving patterns.
- **Core assumption:** Human preference correlates with deductive, forward-chaining reasoning rather than mixed forward-backward or guess-and-check patterns.
- **Evidence anchors:** Abstract mentions "D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment"; Section 5.2, Table 1 shows D-FPS has "significantly lower NE-Submitted than FPS" (e.g., 0.00% vs 19.38% on FormalMath500 for ICL vs proof search).
- **Break condition:** If the model cannot express answers propositionally (D-FPS requires A : Prop), the framework fails; Theorem 3.7 claims expressiveness matches FPS for find-all problems, but find-one problems remain underexplored.

### Mechanism 3
- **Claim:** RPE (Restricted Propositional Equivalence) provides human-aligned answer evaluation by testing equivalence under limited proof automation.
- **Mechanism:** Rather than checking exact string match or full propositional equality (too permissive), RPE tests whether â = ā can be proven using only a restricted tactic set: rfl (definitional equality), norm_num, ring_nf, rw_search, and aesop. This captures "closeness" to ground truth without accepting trivially equivalent but differently expressed answers.
- **Core assumption:** The chosen tactic set approximates human notions of "obviously equivalent" without being so permissive as to accept tautologically correct but unsimplified answers.
- **Evidence anchors:** Abstract states "RPE achieves 100% precision, 97.18% recall, and 0.9732 Cohen's kappa on a human-annotated test set"; Section 4.1 defines RPE holding if and only if ∀(vi : Ti), ∧φi, â = ā can be proven by restricted proof automation T.
- **Break condition:** RPE correctly rejects unsimplified forms like "a = √180/2" vs ground truth "a = 3√5" (Appendix G.2), but fails on numerical approximations (0.4667 vs 7/15).

## Foundational Learning

- **Concept:** Dependent Type Theory / Curry-Howard Isomorphism
  - **Why needed here:** FPS relies on the correspondence between propositions and types; a problem's answer type Ta and proof are unified in the proof term.
  - **Quick check question:** Can you explain why "∃(a : Ta), ∧ψi" is a type that must be inhabited?

- **Concept:** Metavariables in Interactive Theorem Provers
  - **Why needed here:** FPS extracts answers from metavariable assignments (?w := â) after proof completion.
  - **Quick check question:** What happens if a metavariable is never assigned but the proof closes?

- **Concept:** Constructive vs Classical Logic
  - **Why needed here:** D-FPS enforces constructive derivation; non-constructive axioms could bypass answer construction.
  - **Quick check question:** Why does "∃x, P(x)" require exhibiting a witness in constructive logic but not in classical logic?

## Architecture Onboarding

- **Component map:**
  - Problem Formalization: Informal problem → (V, a, Φ, Ψ) tuple → Lean 4 statement
  - FPS Pipeline: Initialize → Apply Exists.intro → Solve goals → Extract answer from metavariable
  - D-FPS Pipeline: Initialize → Split goals (constructor) → Forward reasoning → Backward proving → Extract
  - RPE Evaluator: Formalize equivalence statement → Run restricted tactics → Accept/reject

- **Critical path:**
  1. Problem must be correctly formalized (incorrect formalization = wrong answer accepted)
  2. Metavariable coupling must be preserved (premature assignment = unsound extraction)
  3. RPE tactic set must be calibrated (too strict = false negatives; too permissive = false positives)

- **Design tradeoffs:**
  - FPS vs D-FPS: FPS is more flexible (higher solving rates: 23.77% vs 15.50%) but D-FPS produces fewer incorrect submissions (0.00% vs 19.38% NE-Submitted)
  - RPE precision vs recall: Current config achieves 100% precision, 97.18% recall—tuned conservatively for evaluation reliability

- **Failure signatures:**
  - High "Solution" error rate (~70% on simpler benchmarks): Model derives correct answer but formal proof fails type-check
  - High "Answer" error rate on PutnamBench (55-65%): Model cannot solve the math itself
  - FPS shortcuts: Answer term directly constructed from problem predicate (e.g., "a = {x | P x}" for "find all x where P")

- **First 3 experiments:**
  1. **Sanity check:** Run FPS proof search on MiniF2F-Solving with InternLM2.5-StepProver; verify extracted answers match RPE-accepted solutions (target: ~27% solve rate per Table 1)
  2. **D-FPS validation:** Compare NE-Submitted rates between FPS (Hybrid CoT) and D-FPS (Hybrid CoT) on FormalMath500; expect D-FPS near 0% per paper claims
  3. **RPE calibration:** On xVerify subset, test RPE with relaxed tactic sets (add simp, omega) to measure precision/recall tradeoff; baseline is 100%/97.18%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the significant performance gap between formal proving (proving the ground truth) and formal solving (deriving the answer) be effectively bridged?
- Basis in paper: Section 5.2 notes that solving rates are roughly half of proving rates, stating, "This huge gap shows the difficulty of FPS and calls for more exploration."
- Why unresolved: Current models struggle with the specific requirements of problem-solving, such as continuously handling coupled metavariables and constructive derivations.
- What evidence would resolve it: Development of supervised fine-tuning (SFT) strategies or specialized architectures that raise solving rates to near parity with proving rates on the constructed benchmarks.

### Open Question 2
- Question: Can inference-scaling techniques applied to D-FPS reliably produce sound and human-aligned answers without ground-truth supervision?
- Basis in paper: Section 5.2 observes that D-FPS's near-zero incorrect submission rate "depicts a promising picture of unsupervised problem-solving" using inference scaling.
- Why unresolved: The theoretical promise exists, but current D-FPS solving rates are low; it remains unproven if scaling alone is sufficient for robust unsupervised performance.
- What evidence would resolve it: Experiments demonstrating that search-based inference scaling on D-FPS can achieve high accuracy on difficult benchmarks without reliance on pre-provided answers.

### Open Question 3
- Question: How can the FPS framework be extended to effectively benchmark and evaluate "find-one" problems, such as counterexample construction?
- Basis in paper: Appendix L states that "find-one problems... remain underexplored. This calls for benchmarks for non-trivial find-one problems."
- Why unresolved: The current work focuses on "find-all" problems, and the RPE metric is designed for equivalence checking rather than validating single instances from multiple candidates.
- What evidence would resolve it: The creation of a specific benchmark for "find-one" formal problems and the adaptation of the evaluation framework to support them.

## Limitations
- FPS and D-FPS solving rates remain low (max 27.47%) compared to proving rates, especially for difficult benchmarks like PutnamBench-Solving (0.31%)
- Framework's dependence on formal theorem proving environments may limit scalability to broader mathematical domains
- Find-one problems are underexplored, with current framework better suited for find-all problems only

## Confidence
**High Confidence:**
- Metavariable coupling in FPS ensures process-level verification
- D-FPS reduces incorrect submissions through deductive reasoning
- RPE achieves reported precision and recall metrics on human-annotated test set

**Medium Confidence:**
- D-FPS expressiveness claim matching FPS for find-all problems
- RPE restricted tactic set calibration as optimal for human-aligned evaluation
- FPS/D-FPS scalability to problems beyond current benchmark scope

**Low Confidence:**
- Long-term viability of formal theorem proving environments for general mathematical problem-solving
- Framework's ability to handle increasingly complex mathematical domains

## Next Checks
1. **Expressiveness Validation**: Test D-FPS on 50-100 additional find-all problems spanning algebra, geometry, and number theory to verify Theorem 3.7's claim; measure solving rates and answer correctness across problem types.

2. **RPE Calibration Study**: Systematically vary the restricted tactic set (add/remove tactics like simp, omega, linarith) on 100 FormalMath500 problems; measure precision-recall tradeoff to determine if current configuration is optimal.

3. **Generalization Assessment**: Apply FPS to 50-100 problems from non-competition mathematical domains (e.g., proof-based undergraduate mathematics) to evaluate framework's handling of problems beyond competition-style questions; assess if 23-27% solving rate range holds or degrades.