---
ver: rpa2
title: 'From Embeddings to Equations: Genetic-Programming Surrogates for Interpretable
  Transformer Classification'
arxiv_id: '2509.21341'
source_url: https://arxiv.org/abs/2509.21341
tags:
- plus
- minus
- times
- divide
- d500
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to produce interpretable symbolic
  surrogates for transformer-based classification models by leveraging frozen embeddings
  and a view-based genetic programming approach. The method extracts frozen embeddings
  from transformer encoders (ModernBERT for text, DINOv2 for images, and SigLIP for
  image-text pairs), partitions them into disjoint, information-preserving views via
  semantic-preserving feature partitioning, and trains a cooperative multi-population
  genetic program to learn additive, closed-form logit programs.
---

# From Embeddings to Equations: Genetic-Programming Surrogates for Interpretable Transformer Classification

## Quick Facts
- **arXiv ID:** 2509.21341
- **Source URL:** https://arxiv.org/abs/2509.21341
- **Reference count:** 40
- **Primary result:** Symbolic surrogates for transformer classifiers achieve strong F1 (up to ~0.99) while remaining highly parsimonious and interpretable.

## Executive Summary
This paper presents a method to produce interpretable symbolic surrogates for transformer-based classification models by leveraging frozen embeddings and a view-based genetic programming approach. The method extracts frozen embeddings from transformer encoders (ModernBERT for text, DINOv2 for images, and SigLIP for image-text pairs), partitions them into disjoint, information-preserving views via semantic-preserving feature partitioning, and trains a cooperative multi-population genetic program to learn additive, closed-form logit programs. A one-standard-error rule on validation F1 with a complexity tie-break selects a canonical surrogate, which is then calibrated via temperature scaling. Across five benchmarks (SST2G, 20NG, MNIST, CIFAR10, MSC17), the surrogates achieve strong predictive performance (up to F1 ≈0.99 on MNIST, CIFAR10, and MSC17; ≈0.95 on SST2G) while remaining highly parsimonious. The approach also yields detailed interpretability analyses, including dimension usage, overlap statistics, contribution-based importance, and global effect profiles via PDP and ALE, demonstrating that compact symbolic programs can effectively summarize transformer decision logic.

## Method Summary
The method extracts frozen embeddings from pre-trained transformers, standardizes them using training statistics, and partitions the dimensions into disjoint views using Semantic-Preserving Feature Partitioning (SPFP). A cooperative multi-population genetic programming (MEGP) algorithm then evolves additive logit programs across these views, with one population per view. The best-performing models are selected via a one-standard-error rule that balances validation F1 against complexity. Temperature scaling calibrates the final symbolic surrogates. The approach emphasizes interpretability through compact symbolic expressions while maintaining competitive predictive performance.

## Key Results
- Symbolic surrogates achieve F1 scores up to 0.99 on MNIST, CIFAR10, and MSC17 benchmarks.
- Heavy-tailed dimension usage emerges, with a few "pivot" dimensions dominating predictions across logits.
- Canonical surrogates maintain strong calibration (low ECE) after temperature scaling.
- Interpretable dimension importance and global effect profiles are extracted via PDP and ALE.

## Why This Works (Mechanism)

### Mechanism 1: Disjoint View Decomposition
- **Claim:** Partitioning high-dimensional embeddings into disjoint, semantic-preserving views reduces search complexity and improves the convergence of symbolic regression compared to searching the full space directly.
- **Mechanism:** Semantic-Preserving Feature Partitioning (SPFP) groups embedding dimensions into disjoint views based on relevance-redundancy criteria. This forces the subsequent Genetic Programming (GP) step to evolve simpler, smaller expression trees per view rather than one monolithic, intractable tree over hundreds of dimensions.
- **Core assumption:** The transformer's embedding geometry permits a factorization where disjoint subsets of dimensions retain sufficient class-relevant information without requiring dense cross-view interactions in the symbolic program.
- **Evidence anchors:**
  - [Section III-C] describes SPFP creating disjoint, information-preserving views to reduce redundancy.
  - [Section IV-A] notes that SPFP partitions (e.g., 4 views for SST2G) drive the complexity profile, preventing explosion.
  - [Corpus] Related work in "Decomposable Neuro Symbolic Regression" supports the general efficacy of decomposition strategies for handling complexity in symbolic tasks.
- **Break condition:** If SPFP assigns critical interacting dimensions to strictly disjoint sets, the additive model may fail to capture necessary feature interactions (e.g., XOR-like logic split across views), causing a performance ceiling.

### Mechanism 2: Cooperative Multi-Population Evolution (MEGP)
- **Claim:** Evolving cooperative populations (one per view) that aggregate additively allows the system to approximate complex transformer decision boundaries while maintaining parsimony.
- **Mechanism:** Multi-Population Ensemble GP (MEGP) instantiates $V$ populations. Individuals are assembled into "teams" for fitness evaluation (classification cross-entropy). This cooperative coevolution encourages complementary programs where each view captures a specific slice of the decision logic, summed to produce logits.
- **Core assumption:** The transformer's logit function can be approximated by a sum of low-order arithmetic expressions over partitioned dimensions.
- **Evidence anchors:**
  - [Abstract] states the method learns "additive, closed-form logit programs" via cooperative multi-population GP.
  - [Section III-D] details the fitness evaluation via assembling teams to produce additive logits $z_c(x) = \sum f^{(v)}_c$.
  - [Corpus] Evidence is weak for this specific coevolutionary mechanism on embeddings; corpus papers focus on Bayesian SR or general GP scalability (Kozax), lacking direct validation of this specific cooperative scheme.
- **Break condition:** If the encoder's internal logic relies on multiplicative attention heads that cannot be linearly decomposed into additive view contributions, the surrogate may require excessively deep trees to compensate, defeating the interpretability goal.

### Mechanism 3: Heavy-Tailed Dimension Reuse
- **Claim:** The surrogate's efficacy relies on the emergence of a small subset of "pivot" dimensions that drive the majority of the prediction, allowing the rest to be ignored.
- **Mechanism:** The GP search naturally prunes irrelevant dimensions due to parsimony pressure. The resulting canonical models show a heavy-tailed usage distribution: a few dimensions are reused across many logits (high global importance), while most are sparse or unused.
- **Core assumption:** The frozen embeddings (ModernBERT/DINOv2) encode semantics in a sufficiently disentangled manner that a few axes correlate strongly with class logits.
- **Evidence anchors:**
  - [Section IV-C] and [Figure 5] demonstrate heavy-tailed frequency counts, where a handful of dimensions dominate usage.
  - [Section IV-D] identifies "dominant" dimensions (e.g., d112 in 20NG, d721 in CIFAR10) with near-perfect monotonic effects.
  - [Corpus] Related symbolic regression papers (e.g., "Discovering Symbolic Differential Equations") emphasize enforcing sparsity to uncover governing laws, aligning with this usage pattern.
- **Break condition:** If the embedding space is highly anisotropic or entangled (where signal is spread uniformly), the symbolic model may struggle to find sparse anchors, leading to bloated, unreadable equations.

## Foundational Learning

- **Concept:** **Semantic-Preserving Feature Partitioning (SPFP)**
  - **Why needed here:** You cannot effectively run standard GP on 768+ dimensions. You must understand how SPFP reduces the search space by clustering features based on redundancy and relevance to the label before the evolutionary step.
  - **Quick check question:** Given a 768-dim embedding, does SPFP produce random subsets or clusters based on mutual information/redundancy?

- **Concept:** **Parsimony Pressure in Genetic Programming**
  - **Why needed here:** The paper explicitly trades accuracy for complexity (1-SE rule). Understanding how multi-objective optimization (error vs. tree size) works is crucial to interpreting why the final equations are "compact" rather than perfect.
  - **Quick check question:** If two models have identical F1 scores, which one does the canonical selection process choose: the deeper one or the one with fewer nodes?

- **Concept:** **Temperature Scaling for Calibration**
  - **Why needed here:** Symbolic surrogates often output uncalibrated logits. Understanding post-hoc calibration (Platt scaling) is necessary to bridge the gap between the GP's raw output and valid probabilities (ECE/Brier scores).
  - **Quick check question:** Does fitting the temperature parameter $T$ on the test set improve F1 score or only the reliability (log-loss/ECE)?

## Architecture Onboarding

- **Component map:**
  Frozen Encoders -> SPFP -> MEGP -> Selection (1-SE) -> Calibration -> Canonical Surrogate

- **Critical path:** The SPFP partitioning is the structural backbone. If the partition is poor (e.g., critical features isolated in a small view that gets pruned or stalled), the MEGP cannot recover the signal because populations cannot access dimensions outside their assigned view.

- **Design tradeoffs:**
  - **Accuracy vs. Interpretability:** The "1-SE rule" explicitly sacrifices a potential ~1% F1 gain for a massive reduction in symbolic complexity (nodes/depth).
  - **Global vs. Local:** This architecture learns a global surrogate (one equation for the whole dataset) rather than local explanations (like LIME), offering consistency but potentially lower fidelity on outliers.

- **Failure signatures:**
  - **Bloated Complexity:** If validation F1 is low and complexity is high, the SPFP likely failed to separate signal from noise, forcing GP to build "hacky" deep trees.
  - **Stagnation:** If MEGP stalls early with poor fitness, the "additive" assumption may be violated for the dataset (requires interaction between views).
  - **Overconfidence:** High F1 but high ECE (Expected Calibration Error) indicates the GP constants are extreme; requires strict temperature scaling.

- **First 3 experiments:**
  1. **Sanity Check (Random Partition):** Replace SPFP with a random partition of equivalent size. If performance drops significantly, SPFP is the causal mechanism for efficiency.
  2. **Complexity Ablation:** Remove the parsimony pressure/complexity penalty. Observe if F1 improves marginally while complexity explodes (validating the trade-off).
  3. **View Overlap Analysis:** Check the UpSet plots (dimension overlap). If logits share almost no dimensions in a multi-class problem, inspect if classes are learned independently or if the partitioning is too restrictive.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the surrogate's influential embedding dimensions be mapped back to raw inputs (tokens or image patches) in a way that remains faithful to the symbolic program?
  - **Basis in paper:** [explicit] Conclusion states "Surrogates explain decisions in the embedding space rather than raw inputs; mapping influential dimensions back to tokens or image regions requires additional attribution."
  - **Why unresolved:** The pipeline operates on frozen embeddings; no grounding mechanism connects dimension d687's importance to specific input features.
  - **What evidence would resolve it:** Development and evaluation of attribution methods that link surrogate dimensions to tokens/patches while preserving consistency with the symbolic logit expressions.

- **Open Question 2:** Would coupling SPFP view construction and MEGP training (e.g., via differentiable or bilevel optimization) yield better accuracy–parsimony trade-offs than the current decoupled approach?
  - **Basis in paper:** [explicit] Conclusion identifies this as future work: "couple SPFP and MEGP through differentiable or bilevel view learning."
  - **Why unresolved:** SPFP views are fixed before MEGP evolution begins; no feedback loop allows view boundaries to adapt based on symbolic search progress.
  - **What evidence would resolve it:** Comparison experiments where view partitioning is jointly optimized with GP, measuring F1 and complexity against the decoupled baseline.

- **Open Question 3:** How robust are the canonical surrogates to distribution shift and do they generalize when embeddings come from out-of-distribution inputs?
  - **Basis in paper:** [explicit] Conclusion lists "distribution-shift stress tests" and "causally robust surrogates" as future work.
  - **Why unresolved:** All experiments use held-out test splits from the same data distribution; no evaluation under covariate or label shift is reported.
  - **What evidence would resolve it:** Evaluating canonical surrogates on transformed inputs (e.g., adversarial, domain-shifted) and reporting degradation in F1, calibration, and surrogate fidelity.

## Limitations

- The exact algorithmic details of SPFP partitioning are not fully specified, relying on a separate paper [25] for implementation details.
- The cooperative team assembly mechanism in MEGP is cited as "under review" [26], creating uncertainty in faithful reproduction.
- The study focuses on frozen embeddings without analyzing the impact of fine-tuning encoders or dynamic embedding updates.
- The generalizability to highly non-linear transformer architectures (e.g., attention-heavy models) remains untested.

## Confidence

- **High Confidence:** Empirical performance metrics (F1, AUC, ECE) across five benchmarks are directly measured and reported.
- **Medium Confidence:** The three core mechanisms (SPFP, MEGP, dimension sparsity) are logically coherent but rely on cited but not fully detailed algorithms.
- **Low Confidence:** Claims about the scalability of this approach to larger models or more complex datasets are not empirically validated.

## Next Checks

1. **SPFP Robustness:** Implement multiple feature partitioning strategies (random, mRMR, SPFP) and measure the performance gap to isolate the causal effect of SPFP.
2. **Interaction Necessity:** Introduce a synthetic dataset with known feature interactions (e.g., XOR patterns) to test if the additive view assumption breaks down.
3. **Calibration Impact:** Systematically vary the temperature scaling procedure (e.g., Platt scaling, isotonic regression) to quantify its effect on ECE and overall reliability.