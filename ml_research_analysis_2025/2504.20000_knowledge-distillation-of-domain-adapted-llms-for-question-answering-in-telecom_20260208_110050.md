---
ver: rpa2
title: Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom
arxiv_id: '2504.20000'
source_url: https://arxiv.org/abs/2504.20000
tags:
- teacher
- metrics
- vocabulary
- performance
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of domain adaptation of large language
  models (LLMs) for telecom-specific question-answering (QA) tasks using knowledge
  distillation (KD). The core method involves systematically studying the impact of
  supervised fine-tuning (SFT) of teacher and/or student models prior to KD, considering
  different vocabulary matches (same vs.
---

# Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom

## Quick Facts
- arXiv ID: 2504.20000
- Source URL: https://arxiv.org/abs/2504.20000
- Reference count: 27
- Primary result: SFT of teacher and student models improves distilled model performance across all metrics, with SFT of teacher being particularly impactful when using models with the same vocabulary.

## Executive Summary
This paper addresses domain adaptation of large language models (LLMs) for telecom-specific question-answering (QA) tasks using knowledge distillation (KD). The core contribution is a systematic study of how supervised fine-tuning (SFT) of teacher and/or student models prior to KD affects distilled performance, considering different vocabulary matches and two KD algorithms (vanilla KD and Dual Space KD). The study uses the TeleQuAD dataset and evaluates performance using 14 diverse metrics across N-gram, embedding, and LLM-based categories.

## Method Summary
The paper systematically studies the impact of SFT on teacher and/or student models prior to KD, considering two vocabulary scenarios (same vs. different) and two KD algorithms (vanilla KD and Dual Space KD). SFT is performed using LoRA (rank=256, alpha=8) on the TeleQuAD training split. For KD, the study compares vanilla KD (KL divergence on token distributions) with Dual Space KD (Cross-Model Attention for embedding-space alignment). Performance is evaluated on 14 metrics across three categories: N-gram (BLEU variants, ROUGE-L), embedding (Cosine similarity, BERTScore), and Oracle-LLM (RAGAS metrics).

## Key Results
- SFT of teacher improves distilled model performance when both models have same vocabulary, irrespective of algorithm and metrics
- When SFT is not feasible, using models with different vocabulary and DSKD algorithm performs better
- Overall, SFT of both teacher and student results in better performance across all metrics, although statistical significance depends on the vocabulary of the teacher models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFT of teacher model prior to KD improves distilled student performance when teacher and student share the same vocabulary
- Core assumption: Domain-adapted teacher provides higher-quality supervision signals through token distribution matching
- Evidence anchors: Abstract result, Section 3.2.1 statistical analysis showing NH rejection in 13/14 metrics for H^S_SFT, corpus support from Delta KD and Warmup-Distill papers

### Mechanism 2
- Claim: DSKD outperforms vanilla KD when teacher and student have different vocabularies and SFT is not feasible
- Core assumption: Semantic embedding alignment can compensate for tokenization mismatches
- Evidence anchors: Abstract result, Section 1.2.2 description of Cross-Model Attention, Section 3.3.1 showing NH rejection in 10/14 metrics for H^B_Alg, limited corpus evidence

### Mechanism 3
- Claim: SFT of both teacher and student yields best distilled performance across configurations
- Core assumption: Benefits of teacher and student SFT are cumulative and not redundant
- Evidence anchors: Abstract result, Section 3.1 inference about SFT improving performance, indirect support from TinyR1-32B-Preview and RLKD papers

## Foundational Learning

- **KL Divergence for Knowledge Distillation**
  - Why needed here: Vanilla KD relies on minimizing KL divergence between teacher and student output distributions
  - Quick check question: Can you explain why KL divergence requires aligned probability distributions over the same vocabulary?

- **LoRA (Low-Rank Adaptation) for Parameter-Efficient Fine-Tuning**
  - Why needed here: The paper uses LoRA for SFT (rank=256, alpha=8)
  - Quick check question: How does LoRA reduce trainable parameters while preserving model capacity, and what is the role of the rank and alpha hyperparameters?

- **Multi-Faceted LLM Evaluation Metrics**
  - Why needed here: The paper evaluates performance using 14 metrics across three categories
  - Quick check question: Why might N-gram metrics like BLEU be insufficient for evaluating open-ended QA, and how do embedding-based or LLM-based metrics address this gap?

## Architecture Onboarding

- **Component map**: Teacher Models (Llama-7b or Mistral-7b) -> SFT Module (LoRA) -> KD Algorithms (Vanilla KD or DSKD) -> Student Model (TinyLlama-1.1b) -> Evaluation Suite (14 metrics)

- **Critical path**: Select teacher-student pair -> Apply SFT to teacher and/or student -> Run KD (vanilla or DSKD) -> Evaluate distilled model on test set -> Perform statistical hypothesis testing

- **Design tradeoffs**:
  - Same vocabulary + vanilla KD: Simpler, more efficient, but requires teacher SFT
  - Different vocabulary + DSKD: More flexible, better when SFT is infeasible, but adds complexity
  - SFT investment: Dual SFT yields best results but doubles training cost

- **Failure signatures**: Statistically insignificant improvements despite SFT, performance degradation with (TSFT, SB) vs (TB, SSFT) for different vocabulary, large variance across metric groups

- **First 3 experiments**:
  1. Baseline: KD with base (untrained) Llama teacher and TinyLlama student using vanilla KD
  2. Teacher SFT impact: SFT the Llama teacher, then run vanilla KD with base TinyLlama student
  3. Vocabulary mismatch recovery: Run DSKD with base Mistral teacher and base TinyLlama student

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the observed KD trends hold for tasks beyond QA, such as code generation or agent-based systems?
  - Basis: Authors explicitly mention extending to other tasks in conclusion
  - Resolution: Replicate experimental setup on code generation benchmarks and compare statistical significance

- **Open Question 2**: How does the size gap between teacher and student models affect the efficacy of dual-space KD versus vanilla KD?
  - Basis: Authors note current teachers are relatively small and suggest evaluating larger models
  - Resolution: Conduct experiments using significantly larger teacher models (e.g., Llama-70B)

- **Open Question 3**: Would increasing training dataset size yield statistically significant improvements for cross-vocabulary distillation?
  - Basis: Authors suspect limited training data caused non-significant results for Mistral-TinyLlama pair
  - Resolution: Re-run cross-vocabulary experiments with larger, scaled-up telecom dataset

## Limitations

- **Hyperparameter Omission**: KD training parameters (learning rate, epochs, batch size, temperature, alpha weighting) are completely omitted
- **Evaluation Oracle Ambiguity**: Specific Oracle-LLM model used for RAGAs metrics is not mentioned
- **DSKD Implementation Gaps**: Specific implementation details like projection dimensions are absent

## Confidence

- **High Confidence**: Claims about SFT of teacher improving performance for same-vocabulary pairs (supported by direct statistical analysis)
- **Medium Confidence**: Claims about DSKD superiority for different-vocabulary pairs when SFT is infeasible (weaker evidence, limited corpus validation)
- **Low Confidence**: Claims about cumulative benefits of dual SFT (statistical significance depends on vocabulary, non-significant results observed)

## Next Checks

1. **KD Hyperparameter Sensitivity Analysis**: Systematically vary KD training parameters while keeping SFT configurations fixed to determine if performance differences are algorithm-driven

2. **Oracle-LLM Consistency Study**: Re-run RAGAs evaluations using different Oracle-LLM models to quantify metric variance and establish robustness bounds

3. **Vocabulary Mismatch Stress Test**: Create controlled experiments varying the degree of vocabulary mismatch to identify the exact threshold where DSKD becomes necessary and validate the paper's claims about vocabulary differences