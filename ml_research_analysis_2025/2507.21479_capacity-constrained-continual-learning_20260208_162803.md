---
ver: rpa2
title: Capacity-Constrained Continual Learning
arxiv_id: '2507.21479'
source_url: https://arxiv.org/abs/2507.21479
tags:
- capacity
- optimal
- problem
- have
- capacity-constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies capacity-constrained continual learning using\
  \ a Linear-Quadratic-Gaussian (LQG) sequential prediction problem. The authors define\
  \ capacity as the mutual information between an agent\u2019s internal state and\
  \ its observation history, bounded by B bits."
---

# Capacity-Constrained Continual Learning

## Quick Facts
- **arXiv ID:** 2507.21479
- **Source URL:** https://arxiv.org/abs/2507.21479
- **Reference count:** 27
- **Primary result:** Theoretical framework for optimal capacity allocation in linear-Gaussian continual learning under mutual information constraints

## Executive Summary
This paper develops a theoretical framework for understanding how agents should allocate limited computational resources in continual learning settings. The authors study a Linear-Quadratic-Gaussian (LQG) sequential prediction problem where capacity is defined as the mutual information between an agent's internal state and its observation history, bounded by B bits. They derive an optimal solution showing that capacity-constrained agents take the form of linear transformations of the posterior mean with additive Gaussian noise. For steady-state conditions, they characterize how capacity should be optimally allocated across subsystems in diagonal systems, demonstrating through experiments that subsystems with longer mixing times, higher process noise, or better observation SNR receive proportionally more capacity.

## Method Summary
The paper analyzes capacity-constrained continual learning through a Linear-Quadratic-Gaussian framework where an agent predicts latent states θt from observations Yt. The capacity constraint is formalized as I(St; Ht) ≤ B bits, where St is the agent's internal state and Ht is the observation history. The solution decomposes into two stages: (1) computing the optimal posterior mean θ̄t and covariance Mt via Kalman filtering, and (2) applying rate-distortion optimal compression that adds Gaussian noise to produce the final prediction θ̂t. For steady-state diagonal systems, optimal capacity allocation follows a water-filling solution that assigns more bits to subsystems with higher marginal prediction error reduction.

## Key Results
- Capacity-constrained agents in LQG settings are linear Gaussian, taking the form of linear transformations of posterior means with additive Gaussian noise
- For diagonal systems, optimal capacity allocation follows a water-filling solution prioritizing subsystems with longer mixing times, higher process noise, or better observation SNR
- Experimental results show subsystems with mixing time ai=0.99 receive ~3× more capacity than ai=0.9 at moderate budgets
- The optimal asymptotic cost scales exponentially with capacity allocation per dimension

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Capacity Constraint
- **Claim:** Bounding mutual information I(St; Ht) ≤ B bits provides a tractable approximation of hard memory constraints while enabling analytical optimization.
- **Mechanism:** Mutual information measures average retained information from observation history, transforming the constraint into a continuous optimization problem using rate-distortion theory.
- **Core assumption:** The mutual information constraint approximates hard memory constraints well at scale.
- **Evidence anchors:** Abstract states mutual information bounds observation history; Section 3.2 explains it as relaxation of hard memory constraint.
- **Break condition:** When B is very small or when the agent's state distribution is highly non-Gaussian.

### Mechanism 2: Linear Gaussian Agent Structure
- **Claim:** Under sufficient conditions, optimal capacity-constrained agents are linear Gaussian—predictions are linear transformations of the posterior mean with additive Gaussian noise.
- **Mechanism:** Solution decomposes into Kalman filtering for posterior mean, then rate-distortion compression adding Gaussian noise.
- **Core assumption:** System is linear-Gaussian (LQG), and Theorem 3 conditions (Ft invertible, covariance inequality satisfied) hold.
- **Evidence anchors:** Abstract describes linear transformation with additive Gaussian noise; Section 5 proves linear Gaussian optimality.
- **Break condition:** When Theorem 3's sufficient conditions fail (e.g., Ft singular, inequality violated), optimality guarantees don't hold.

### Mechanism 3: Optimal Capacity Allocation via Marginal Benefit
- **Claim:** Capacity should be allocated to subsystems based on their marginal reduction in prediction error per bit.
- **Mechanism:** For diagonal systems, water-filling-like solution assigns more capacity to dimensions where exp(-2Bi)λi reduction is largest.
- **Core assumption:** System is diagonal or block-diagonal, and steady-state conditions apply.
- **Evidence anchors:** Section 7.1 derives water-filling equation; Figures 1-6 show experimental confirmation of allocation patterns.
- **Break condition:** For coupled systems, allocation problem doesn't decompose cleanly; Theorem 6's conditions may fail.

## Foundational Learning

- **Concept: Mutual Information I(X; Y)**
  - **Why needed here:** Core constraint I(St; Ht) ≤ B bounds retained information; understanding MI is essential for the theoretical framework.
  - **Quick check question:** If St is a deterministic function of Ht, what is I(St; Ht)? (Answer: H(St), the entropy of St)

- **Concept: Kalman Filtering**
  - **Why needed here:** Unconstrained optimal solution is Kalman filtering; capacity-constrained solution builds on posterior mean θ̄t and covariance Mt.
  - **Quick check question:** What does the Kalman gain K_t balance? (Answer: Relative uncertainty in prediction vs. observation)

- **Concept: Rate-Distortion Theory (Gaussian Case)**
  - **Why needed here:** Paper uses Gaussian distortion-rate function D(B, Σ) to find optimal compression; Appendix B derives this explicitly.
  - **Quick check question:** For scalar Gaussian with variance σ² and rate B bits, what's minimum mean-squared error? (Answer: σ²·exp(-2B))

## Architecture Onboarding

- **Component map:** Environment -> Kalman Filter -> Rate-Distortion Compressor -> Capacity Allocator
- **Critical path:**
  1. Compute steady-state covariance Σ (solve Lyapunov equation 20) and posterior covariance M (solve Riccati equation 21)
  2. Compute eigenvalues λi and eigenvectors U of Σ - M
  3. Solve water-filling equation Σ[log(2λi/η)]⁺ = 2B for η
  4. Compute Bi = (1/2)[log(2λi/η)]⁺ for each dimension
  5. Construct Ft = U·diag(1-exp(-2Bi))·U⊤ and Ψt = U·diag([1-exp(-2Bi)]exp(-2Bi)λi)·U⊤

- **Design tradeoffs:**
  - **Capacity vs. accuracy:** Exponential decay exp(-2Bi)λi means diminishing returns; first ~5 bits per dimension capture most signal
  - **Incremental vs. batch updates:** Theorem 3's conditions ensure incremental updates match batch performance; when violated, must track full history
  - **Subsystem coupling:** Block-diagonal systems allow some coupling benefits but require checking Theorem 6 conditions

- **Failure signatures:**
  - **Grey region in Figure 7:** When total capacity is small relative to system dimension, Theorem 6 conditions fail—optimal allocation unknown
  - **Non-invertible Ft:** If Bi = 0 for some dimensions, Ft becomes singular; must handle degenerate case via U1 subset
  - **Non-steady-state transients:** During initial timesteps, Cov[θ̄t] hasn't converged; must use time-varying Ft, Ψt

- **First 3 experiments:**
  1. **Scalar validation:** Implement scalar case (d=m=1), verify optimal cost = M + exp(-2B)[Σ-M] matches simulation across B ∈ {0.1, 1, 5, 10} bits
  2. **Diagonal allocation:** Replicate Case 1 (different mixing times), confirm capacity allocation ratios match Figure 1; measure prediction MSE vs. theoretical optimum
  3. **Condition violation test:** Construct block-diagonal system where Theorem 6 conditions fail at low capacity; characterize performance gap between linear Gaussian agent and unconstrained optimal

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are the sufficient conditions derived in Theorem 3 also necessary for an optimal solution to be a linear Gaussian agent?
- **Basis in paper:** [explicit] The paper states, "whether or not the conditions in Theorem 3 are also necessary is still an open problem, and we leave it to future work."
- **Why unresolved:** Authors proved sufficiency but not necessity; no proof that no other optimal incremental solutions exist when conditions fail.
- **What evidence would resolve it:** Formal mathematical proof of necessity or counter-example showing optimal linear Gaussian agent exists that violates Condition (14).

### Open Question 2
- **Question:** How can this framework be extended to derive solutions for capacity-constrained control and reinforcement learning problems?
- **Basis in paper:** [explicit] Concluding remarks identify "interesting future direction is to derive solutions for capacity-constrained control/decision problems, such as capacity-constrained reinforcement learning."
- **Why unresolved:** Paper solves passive prediction problem, not settings where agent's actions influence environment's state transitions.
- **What evidence would resolve it:** Derivation of optimal agent structures and capacity allocation strategies for capacity-constrained LQG control problem or MDP.

### Open Question 3
- **Question:** What is the optimal capacity allocation strategy when total capacity is small and sufficient conditions of Theorem 6 fail?
- **Basis in paper:** [explicit] In experimental analysis (Section 7.3, Case 4), authors note "the sufficient conditions of Theorem 6 do not hold. Thus, we do not know what the optimal capacity allocation is in such cases."
- **Why unresolved:** Theoretical framework relies on linear Gaussian agent being optimal (which requires sufficient conditions); when these break down, structure is undefined.
- **What evidence would resolve it:** Characterization of optimal agent structure (e.g., non-linear or non-Gaussian) and corresponding allocation scheme for low-capacity regime.

### Open Question 4
- **Question:** How does the optimal agent design change if the agent does not know the true system model parameters a priori?
- **Basis in paper:** [inferred] Problem formulation states "we will assume that the agent knows the true model of the system," establishing this as limiting assumption.
- **Why unresolved:** Continual learning typically involves learning system dynamics alongside making predictions; impact of model uncertainty unexplored.
- **What evidence would resolve it:** Analysis of capacity-constrained problem where agent must simultaneously estimate model parameters (A, C, Σ) and latent state θt.

## Limitations

- **Sufficient conditions uncertainty:** The sufficient conditions (Theorem 3) for linear Gaussian optimality under incremental updates are analytically derived but may not capture all practical scenarios.
- **Mutual information relaxation:** The capacity constraint as mutual information is a relaxation that may not accurately reflect hard memory budgets in extreme regimes (very low B or non-Gaussian state distributions).
- **Coupled system complexity:** For coupled systems beyond block-diagonal, the optimal capacity allocation remains an open question without general solution methods.

## Confidence

- **High Confidence:** The information-theoretic capacity constraint (I(St; Ht) ≤ B) and its relaxation interpretation — supported by standard information theory and consistent with established literature.
- **Medium Confidence:** Linear Gaussian agent structure — theoretically derived for LQG systems but sufficient conditions for incremental updates require careful verification in practice.
- **Medium Confidence:** Optimal capacity allocation for diagonal systems — water-filling solution is mathematically sound but real-world coupling effects may alter allocations.

## Next Checks

1. **Break Condition Testing:** Construct a non-diagonal system where Theorem 6's sufficient conditions fail, and empirically measure the performance gap between the linear Gaussian solution and unconstrained optimal.
2. **Relaxation Fidelity Analysis:** Compare the mutual information constraint's predictions against hard memory budget simulations in extreme capacity regimes (B → 0).
3. **Incremental vs. Batch Validation:** For block-diagonal systems, implement both incremental update and full-history tracking approaches, measuring when performance diverges under varying capacity levels.