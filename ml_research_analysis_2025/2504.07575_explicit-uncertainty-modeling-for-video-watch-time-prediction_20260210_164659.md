---
ver: rpa2
title: Explicit Uncertainty Modeling for Video Watch Time Prediction
arxiv_id: '2504.07575'
source_url: https://arxiv.org/abs/2504.07575
tags:
- prediction
- watch-time
- confidence
- time
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes EXUM (Explicit Uncertainty Model), a framework
  for watch-time prediction in video recommendation systems. It addresses the challenge
  of stochastic user watch-time behavior by introducing a confidence prediction module
  that explicitly models the uncertainty of the predicted watch-time distribution.
---

# Explicit Uncertainty Modeling for Video Watch Time Prediction

## Quick Facts
- **arXiv ID:** 2504.07575
- **Source URL:** https://arxiv.org/abs/2504.07575
- **Reference count:** 25
- **Primary result:** Proposed EXUM framework with explicit uncertainty modeling improves video watch-time prediction accuracy and achieves 0.31% increase in online A/B testing

## Executive Summary
This paper addresses the challenge of stochastic user watch-time behavior in video recommendation systems by proposing EXUM (Explicit Uncertainty Model). The framework introduces a confidence prediction module that explicitly models the uncertainty of predicted watch-time distributions. By wrapping around existing distribution modeling techniques (quantile prediction and ordinal regression), EXUM enables joint optimization through adversarial learning. The method was deployed in a large-scale industrial video recommendation platform, achieving significant improvements in both offline metrics (up to 16.84% in MAE) and online A/B testing (0.31% increase in user watch time).

## Method Summary
EXUM encapsulates existing watch-time prediction backbones (D2Q for quantile prediction and CREAD for ordinal regression) with an additional confidence branch. During training, the framework uses a confidence score to create a mixed output between the predicted value and ground truth, with gradients scaled by confidence. An adversarial loss forces confidence scores away from extremes. The architecture uses a shared-bottom MLP feeding into both prediction and confidence heads, allowing the confidence branch to learn error patterns from the same features used for prediction. At inference, only the prediction branch is used, making the approach cost-effective for deployment.

## Key Results
- **Online performance:** 0.31% increase in users' video watch time in large-scale A/B testing
- **Offline accuracy:** Up to 16.84% improvement in Mean Absolute Error over state-of-the-art baselines
- **Ranking performance:** Up to 2.37% improvement in XAUC metric on public datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A confidence branch acts as a dynamic gradient gate, suppressing learning signals for high-uncertainty samples while reinforcing them for predictable ones.
- **Mechanism:** The framework introduces a confidence score $c$. During loss calculation, the gradient of the prediction error w.r.t the model weights is scaled by $c$. If the model is uncertain ($c \to 0$), the gradient approaches zero, effectively ignoring noisy or out-of-distribution samples rather than forcing the model to fit them.
- **Core assumption:** The assumption is that hard-to-predict (stochastic) watch times should be treated as noise to be filtered during training, rather than signal to be overfit.
- **Evidence anchors:**
  - [section 3.1] (Equations 6 and 9 show the gradient is proportional to confidence $c$).
  - [corpus] (Corpus papers like "Multi-Granularity Distribution Modeling" corroborate the difficulty of complex watch-time distributions, validating the need for noise handling).

### Mechanism 2
- **Claim:** Adversarial regularization prevents the model from collapsing into a "short-cut" where it simply memorizes the ground truth during training.
- **Mechanism:** Without constraint, the loss minimization prefers setting confidence $c \to 0$ (selecting ground truth) to achieve zero error instantly. The authors add an adversarial loss $-\log c$ to force $c \to 1$. The equilibrium point between the prediction loss (pushing $c$ down) and adversarial loss (pushing $c$ up) determines the effective uncertainty modeling.
- **Core assumption:** There exists a balance point where the model is confident enough to learn but uncertain enough to handle outliers.
- **Evidence anchors:**
  - [section 3.2] (Theoretical derivation of gradient degradation $\partial L / \partial c \ge 0$).
  - [section 3.3] (Equations 12 & 13 define the joint objective).

### Mechanism 3
- **Claim:** Shared-bottom feature representation allows the uncertainty head to correlate confidence directly with the input features that drive prediction error.
- **Mechanism:** The architecture uses a "Shared Bottom" MLP feeding into both the watch-time predictor and the confidence predictor. This allows the confidence head to learn which feature embeddings result in high-variance predictions, effectively learning "error surfaces" of the main model.
- **Core assumption:** The uncertainty of the prediction is derivable from the same user/video/context features used for the prediction itself.
- **Evidence anchors:**
  - [section 3.1] (Figure 3 illustrates the Shared Bottom architecture).
  - [abstract] (Mentions the framework encapsulates existing techniques with an additional branch).

## Foundational Learning

- **Concept:** **Quantile Regression vs. Ordinal Regression**
  - **Why needed here:** EXUM wraps around these specific backbones (D2Q for Quantile, CREAD for Ordinal). You must understand that D2Q predicts a percentile value while CREAD predicts the probability of crossing discrete time buckets.
  - **Quick check question:** Does the target model output a single probability $p \in [0,1]$ (Quantile) or a vector of probabilities across time windows (Ordinal)?

- **Concept:** **The Uncertainty Paradox**
  - **Why needed here:** This is the core problem statement. Understanding that increasing model capacity to catch "out-of-distribution" samples (long tail) typically hurts accuracy on "easy" samples.
  - **Quick check question:** Why can't we simply maximize the variance of the predicted distribution to cover all possible watch times?

- **Concept:** **Adversarial Learning Objectives**
  - **Why needed here:** The training process is not pure minimization; it is a tug-of-war between fitting the data (Prediction Loss) and enforcing confidence (Adversarial Loss).
  - **Quick check question:** In this context, does the "adversary" generate fake data, or does it manipulate the loss landscape of the existing data? (Answer: It manipulates the loss landscape via $\lambda$).

## Architecture Onboarding

- **Component map:**
  Input Layer -> Shared Bottom MLP [128, 64, 32] -> Tower A (Prediction) and Tower B (Confidence) -> Fusion Node

- **Critical path:**
  1. Check backbone (D2Q or CREAD) implementation.
  2. Implement the Fusion Node: $p' = c \cdot p + (1-c) \cdot y$.
  3. Calculate Prediction Loss using $p'$ (not $p$).
  4. Add Adversarial Loss $L_c = -\sum \log c$ with weight $\lambda$.
  5. **Crucial:** During inference, detach the Confidence Tower; use only Tower A ($p$) for serving.

- **Design tradeoffs:**
  - **Inference Cost:** Zero added latency at serving time if the confidence head is stripped (as suggested in [section 4.1]).
  - **Training Stability:** Requires careful tuning of $\lambda$. The paper notes Ordinal Regression (CREAD) is sensitive to small $\lambda$, while Quantile (D2Q) tolerates larger $\lambda$.

- **Failure signatures:**
  - **Confidence Collapse:** $c$ trends to 0 for all samples. Cause: $\lambda$ is too small.
  - **Over-confidence:** $c$ trends to 1, but MAE/XAUC improves insignificantly. Cause: $\lambda$ is too large, forcing the model to ignore the safety valve of using ground truth.

- **First 3 experiments:**
  1. **Lambda Sweep:** Run offline training with $\lambda \in \{0.01, 0.1, 1.0, 10.0\}$ on a validation set to find the intersection point where MAE is minimized (refer to Figure 6).
  2. **Backbone Ablation:** Verify EXUM works on both D2Q and CREAD backbones to ensure the gradient modulation logic holds for both regression types.
  3. **Confidence Distribution Analysis:** Plot the distribution of $c$ during training steps. Ensure it stabilizes (variance reduces) rather than collapsing to a single point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a theoretically optimal functional form exist to map uncertainty to prediction error for arbitrary watch-time distributions?
- Basis in paper: [explicit] The conclusion states, "it is still an open question whether there exists an optimal way to express the uncertainty of the watch-time prediction model under arbitrary distribution."
- Why unresolved: The current EXUM framework utilizes a linear ensemble selector ($p' = cp + (1-c)y$), but the analysis in Section 4.5 suggests the relationship between confidence and error is not necessarily linear.
- What evidence would resolve it: Deriving or empirically identifying a non-linear uncertainty mapping function that yields lower MAE/XAUC than the current linear confidence branch across diverse datasets.

### Open Question 2
- Question: Can the adversarial loss weight ($\lambda$) be determined adaptively or theoretically rather than via empirical grid search?
- Basis in paper: [inferred] Section 4.4.2 demonstrates that the optimal $\lambda$ varies drastically between backbones (0.1 for CREAD vs. 16.0 for D2Q), and Section 3.3 notes only that "a careful design should be used" without providing a calculation method.
- Why unresolved: The paper treats $\lambda$ as a hyperparameter requiring a search over $\{0.001, \dots, 32.0\}$, creating a deployment burden as the optimal value is sensitive to the backbone architecture.
- What evidence would resolve it: An adaptive scheduling algorithm for $\lambda$ or a theoretical bound based on the initial error distribution that converges to optimal performance without manual tuning.

### Open Question 3
- Question: What specific data characteristics determine whether a Quantile Prediction (QP) or Ordinal Regression (OR) backbone is superior for EXUM?
- Basis in paper: [inferred] Section 4.4.2 notes that "neither QP nor OR is currently a universally superior solution" and that performance "depends on the data characteristics," but leaves the specific causal attributes (e.g., skewness, noise type) unidentified.
- Why unresolved: The paper shows contradictory results where D2Q (QP) wins on WeChat but CREAD (OR) wins on KuaiRand, but the analysis does not isolate which statistical properties of the datasets drive this preference.
- What evidence would resolve it: Systematic experiments correlating dataset statistics (e.g., kurtosis, duration variance) with the performance delta between D2Q-EXUM and CREAD-EXUM.

## Limitations
- The model's reliance on shared feature representations may not capture complex, task-specific error patterns that could be better modeled with task-specific encoders
- The adversarial confidence regularization requires careful hyperparameter tuning (λ) and may not generalize well across different video domains or user behaviors
- The paper's discussion on the "uncertainty paradox" and the choice of specific backbones relies more on internal logic than external literature substantiation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| The core mechanism of confidence-weighted gradient scaling and its impact on training stability | High |
| The effectiveness of the adversarial confidence regularization | Medium |
| The discussion on the "uncertainty paradox" and the choice of specific backbones | Low |

## Next Checks

1. **Feature Ablation Study:** Systematically remove or modify user, video, and context features to determine their impact on the confidence head's ability to predict uncertainty, validating the assumption that uncertainty is derivable from the same features used for prediction.

2. **Cross-Domain Evaluation:** Deploy EXUM on a different video recommendation dataset (e.g., MovieLens with watch-time extensions) to test the generalizability of the λ tuning and the overall framework's robustness to different user behaviors and video types.

3. **Confidence Head Capacity Analysis:** Experiment with varying the capacity of the confidence head (e.g., [8], [32], [64]) to determine if a larger or smaller network improves the model's ability to distinguish signal from noise, especially in cases of sparse feature spaces.