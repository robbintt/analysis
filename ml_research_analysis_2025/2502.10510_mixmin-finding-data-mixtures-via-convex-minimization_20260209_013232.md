---
ver: rpa2
title: 'MixMin: Finding Data Mixtures via Convex Minimization'
arxiv_id: '2502.10510'
source_url: https://arxiv.org/abs/2502.10510
tags:
- mixmin
- data
- mixing
- loss
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of finding optimal data mixtures
  for training machine learning models, particularly for large language models and
  chemistry tasks. The authors formalize this as a bi-level optimization problem and
  show that it becomes convex as model classes become more expressive.
---

# MixMin: Finding Data Mixtures via Convex Minimization

## Quick Facts
- arXiv ID: 2502.10510
- Source URL: https://arxiv.org/abs/2502.10510
- Reference count: 40
- Primary result: MixMin achieves 1-5% relative improvement in negative log likelihood for language modeling tasks using less than 0.2% additional compute

## Executive Summary
MixMin addresses the problem of finding optimal data mixtures for training machine learning models, particularly for large language models and chemistry tasks. The authors formalize this as a bi-level optimization problem and show that it becomes convex as model classes become more expressive. They propose a gradient-based approach that uses cheap proxy models to approximately solve the convex objective. The method consistently improves over baselines like random search and RegMix, achieving 1-5% relative improvement in negative log likelihood for language modeling tasks using less than 0.2% additional compute. Notably, MixMin mixtures found for smaller models also improved training of larger models, suggesting scale-invariance.

## Method Summary
MixMin transforms the bi-level data mixing optimization into a single convex minimization problem by leveraging properties of Bayes optimal models for cross-entropy and MSE losses. The method trains cheap proxy models on each source distribution independently, computes their predictions on target samples, and optimizes mixture weights via entropic descent on the simplex. Once optimal weights are found, the final model is retrained on the reweighted mixture of sources. The approach requires pre-computing proxy predictions on target samples, creating memory constraints but enabling efficient optimization independent of model size.

## Key Results
- MixMin achieves 1-5% relative improvement in negative log likelihood for language modeling tasks using less than 0.2% additional compute
- For chemistry tasks, MixMin improved average precision scores by 0.03-0.15
- MixMin mixtures found for smaller models (160M params) also improved training of larger models (410M params), demonstrating scale-invariance

## Why This Works (Mechanism)

### Mechanism 1: Convex Reduction of Bi-Level Data Mixing
The bi-level data mixing objective reduces to a single convex minimization as model classes become more expressive. When the hypothesis space contains models close to Bayes optimal for all mixtures, the optimal mixture weights minimize a simple convex objective: L(Σλp fp(x), y) over the target distribution. This transforms an intractable nested optimization into gradient-based optimization on the simplex.

### Mechanism 2: Linear Mixture Property for CE/MSE Losses
For cross-entropy or MSE losses without covariate shift, the Bayes optimal model for a mixture is the linear mixture of per-source Bayes optimal models. Under unconditional CE or conditional CE/MSE with p(x) constant across sources, the Bayes optimal fλ for mixture dpλ equals Σλp fp(x). This allows the convex objective to be expressed as a linear combination of predictions.

### Mechanism 3: Robustness to Weak Proxy Models
MixMin produces useful mixture weights even when proxy models are far from Bayes optimal. The method trains cheap proxy models on each source, computes their predictions on target samples, and optimizes λ via entropic descent. Empirically, weights found with 1% of training compute transfer well to larger models, even when ensembling the weak proxies underperforms retraining.

## Foundational Learning

- **Bi-level Optimization**
  - Why needed here: Data mixing is naturally framed as finding λ whose risk minimizer optimizes downstream loss—a nested optimization that is generally intractable without structural assumptions.
  - Quick check question: Can you articulate why the inner minimization (finding fλ) makes the outer objective non-convex in standard settings?

- **Bayes Optimal Models**
  - Why needed here: The convex reduction hinges on properties of Bayes optimal predictors for CE/MSE; understanding why fλ = Σλp fp(x) holds is essential for knowing when MixMin applies.
  - Quick check question: For cross-entropy loss, what is the Bayes optimal predictor f*(x) in terms of the data distribution?

- **Entropic/Mirror Descent on the Simplex**
  - Why needed here: MixMin optimizes λ ∈ ΔP (probability simplex) using entropic descent, a variant of mirror descent suited to simplex constraints.
  - Quick check question: Why is entropic descent preferred over projected gradient descent for probability simplex constraints?

## Architecture Onboarding

- **Component map:** Proxy models -> Target sample cache -> Convex optimizer -> Retraining step
- **Critical path:** 1) Train |P| proxy models on individual sources (embarrassingly parallel) 2) Pre-compute all proxy predictions on target samples Dt 3) Run entropic descent (O(|P||Dt|) per iteration, independent of model size) 4) Apply λ* to sample/reweight source data for final training
- **Design tradeoffs:**
  - Number of sources |P|: More sources increase proxy training cost linearly but improve potential mixtures; MixMin scales better than grid/random search (exponential in |P|)
  - Proxy compute budget: 1–5% of final training compute is often sufficient; increasing yields diminishing returns
  - Target sample size |Dt|: Larger Dt improves gradient estimates but increases precomputation; 10k–100k samples typical
- **Failure signatures:**
  - Covariate shift across sources: Convex reduction invalid; consider domain-adapted objectives or remove covariate shift via preprocessing
  - Unexpressive final model class: Excess error bound degrades; verify model capacity is sufficient for sources
  - Proxy models with no signal: Mixture weights unstable across random seeds; increase proxy compute or check data quality
- **First 3 experiments:**
  1. End-to-end smoke test: Apply MixMin to 3–5 sources with a small proxy model (e.g., 1% of target training compute) on a held-out target validation set; verify λ* improves over natural/balanced baselines.
  2. Proxy compute ablation: Vary proxy training from 0.5% to 10% of target compute; plot final model performance vs. proxy compute to identify stability point.
  3. Scale transfer check: Find λ* using small proxy models (e.g., 160M params), then train a larger model (e.g., 410M) on that mixture; measure if improvements transfer.

## Open Questions the Paper Calls Out

### Open Question 1
Can the theoretical excess error of MixMin be bounded to rigorously explain its robustness to weak proxy models? The authors state "we hope future work investigates and generalizes these phenomenon" regarding why MixMin performs well even when proxy models are far from Bayes optimal. A theoretical proof establishing a bound on the data mixing error relative to the approximation error of the proxy models would resolve this.

### Open Question 2
How can the MixMin framework be extended to handle covariate shift or loss functions other than cross-entropy and MSE? The Limitations section notes the reduction is specific to CE/MSE and requires no covariate shift, excluding tasks like image classification. A modified optimization objective or algorithm that converges to optimal mixtures even when input distributions differ across sources would resolve this.

### Open Question 3
How does MixMin perform when composed with upstream data filtering methods? Section 2 states "We leave studying such compositions to future work," referring to the combination of data mixing and data filtering. Empirical results comparing the performance of MixMin on raw sources versus sources pre-processed via various filtering criteria would resolve this.

## Limitations
- The method assumes no covariate shift across source distributions, which rarely holds in practice
- The theoretical guarantees require very expressive model classes that may not be available
- Several implementation details are underspecified including exact optimizer hyperparameters and data filtering thresholds

## Confidence

**High confidence**: The convex reduction mechanism for CE/MSE losses under no covariate shift; The scale-invariance result (MixMin mixtures found for small models improve larger models)

**Medium confidence**: The overall method effectiveness; The robustness to weak proxy models

**Low confidence**: The general convex reduction claim; The chemistry domain results

## Next Checks
1. **Covariate shift sensitivity test**: Apply MixMin to a controlled multi-domain dataset where you artificially vary the input distribution across sources. Compare performance to a domain-adapted version that conditions on source or uses separate input distributions in the convex objective.

2. **Model expressiveness ablation**: Systematically vary the capacity of both proxy and final models (e.g., 0.1B to 10B parameters) and measure the relationship between model size and improvement from MixMin. Plot excess error bounds against empirical performance gaps to validate the ε-approximation assumption.

3. **Transfer across tasks validation**: Take a MixMin mixture found for one target task (e.g., PIQA) and evaluate the same mixture on related tasks (e.g., ARC-Easy, SciQ). Measure correlation between task similarity and transfer performance to understand when scale-invariance generalizes across task boundaries.