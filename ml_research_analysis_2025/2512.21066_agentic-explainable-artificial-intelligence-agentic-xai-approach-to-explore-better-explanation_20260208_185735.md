---
ver: rpa2
title: Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore
  Better Explanation
arxiv_id: '2512.21066'
source_url: https://arxiv.org/abs/2512.21066
tags:
- rounds
- agentic
- refinement
- round
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an "agentic XAI" framework combining SHAP-based
  explainability with multimodal LLM-driven iterative refinement to generate progressively
  enhanced agricultural recommendations. Tested on rice yield data from 26 Japanese
  fields, the system initially provided SHAP-based explanations and iteratively refined
  recommendations across 11 rounds (Rounds 0-10).
---

# Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation

## Quick Facts
- arXiv ID: 2512.21066
- Source URL: https://arxiv.org/abs/2512.21066
- Reference count: 30
- Key outcome: Agentic XAI framework combining SHAP-based explainability with iterative LLM refinement achieves 30-33% quality improvement in agricultural recommendations, peaking at Rounds 3-4 before declining from excessive iteration

## Executive Summary
This study introduces an "agentic XAI" framework that combines SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced agricultural recommendations. Tested on rice yield data from 26 Japanese fields, the system initially provides SHAP-based explanations and iteratively refines recommendations across 11 rounds. Both human crop scientists and LLMs evaluated recommendations across seven metrics, revealing quality improvements with average score increases of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed substantial quality deterioration, indicating a bias-variance trade-off where early rounds lacked explanation depth while excessive iteration introduced verbosity and ungrounded abstraction.

## Method Summary
The framework uses Random Forest with SHAP TreeExplainer to generate initial beeswarm plots showing feature importance for rice yield prediction. An LLM (Claude Sonnet 4) then iteratively refines recommendations by identifying analytical gaps, generating Python code to extract quantitative evidence, executing code on the dataset, and synthesizing enhanced recommendations. This process repeats across 11 rounds (Round 0 baseline plus Rounds 1-10). Recommendations are evaluated by 12 crop scientists and 14 LLMs across seven quality metrics using randomized, blind presentation. Statistical analysis includes ANOVA for significance testing and GAM with derivative-based peak detection to identify optimal refinement rounds.

## Key Results
- Quality peaks at Rounds 3-4 with 30-33% average score improvement from baseline
- Excessive refinement (Rounds 5-10) causes substantial quality deterioration due to verbosity and ungrounded abstraction
- Both human experts and LLMs independently identify similar quality peaks, suggesting evaluator convergence
- Conciseness metric shows early peak followed by consistent decline, while Cost Consideration improves despite lacking economic data
- Inverted U-shaped trajectory confirms bias-variance trade-off in explanation depth

## Why This Works (Mechanism)

### Mechanism 1: Iterative Refinement with Grounded Evidence
The MLLM reviews previous outputs, identifies analytical gaps, generates executable Python code to extract quantitative evidence (correlations, interaction effects, statistical summaries), then synthesizes this grounded evidence into enhanced recommendations while preserving context across rounds. Core assumption: The LLM can reliably identify which analytical gaps matter for practical recommendations and generate valid statistical code. Evidence anchors: Quality peaks at Rounds 3-4 with 30-33% improvement; ANOVA indicates significant differences across rounds (p < 0.001); GAM detects inverted U-shaped trajectory.

### Mechanism 2: Bias-Variance Trade-off in Explanation Depth
Early rounds provide SHAP-based interpretations without supplementary quantitative support—lacking actionable specificity. Later rounds accumulate analyses that may exceed the grounding capacity of available data, leading to ungrounded extrapolation (e.g., economic reasoning without economic parameters). Core assumption: The trade-off pattern generalizes to other domains; optimal stopping point is context-dependent but detectable. Evidence anchors: Excessive refinement shows substantial drop in recommendation quality; Conciseness shows early peak followed by decline; Cost Consideration improves without grounding data.

### Mechanism 3: Hybrid Human-LLM Evaluation Convergence
Randomized presentation, blind evaluation, and isolated conversation contexts reduce position and verbosity biases. Multiple LLM evaluators from different providers mitigate individual model biases, while human experts ensure domain credibility. Core assumption: Convergence between evaluator groups reflects genuine quality detection rather than correlated artifacts. Evidence anchors: Both evaluator groups confirm quality peaks at Rounds 3-4; recent studies show LLM evaluators achieve 75-85% agreement with human expert judgments.

## Foundational Learning

- **SHAP (SHapley Additive exPlanations)**: Feature attribution method that explains individual predictions by quantifying each feature's contribution. Needed here as the initial evidence base that the agentic system iteratively refines. Quick check: Given a SHAP beeswarm plot showing "Days to heading" with high positive SHAP values for late heading dates, what yield prediction change would you infer?

- **Agentic AI with tool use**: Autonomous agents that generate and execute code to accomplish tasks. Needed here as the core mechanism enabling the LLM to create supplementary analyses. Quick check: What risks arise when an LLM agent generates code that modifies or filters the original dataset without external validation?

- **Bias-variance trade-off**: Fundamental ML concept describing the tension between underfitting (high bias) and overfitting (high variance). Needed here to understand why explanation quality follows an inverted U-shaped curve. Quick check: In the explanation context, what would "high variance" manifest as in the generated recommendations?

## Architecture Onboarding

- **Component map**: XAI Analysis Module (Random Forest + SHAP) → MLLM Refinement Loop (gap analysis → code generation → execution → synthesis) → Evaluation Layer (12 humans + 14 LLMs) → Context Manager (preserves conversation history)

- **Critical path**: SHAP plot → Round 0 baseline interpretation → Gap analysis + code generation → Execution + PDF compilation → Synthesis → Repeat until stopping criterion → Evaluation

- **Design tradeoffs**: Early stopping (Rounds 3-4) optimizes practical utility vs. later rounds increase sophistication but degrade actionability; static dataset limits productive refinement vs. external knowledge integration could extend cycles but introduces complexity; human experts ensure credibility but don't scale vs. LLM evaluators scale but may share biases

- **Failure signatures**: Over-refinement (Conciseness declining + Cost Consideration improving without economic data = ungrounded expansion); context drift (later recommendations referencing analyses not in original data = hallucination); evaluator divergence (human and LLM scores separating = domain-knowledge gap)

- **First 3 experiments**: 1) Stopping criterion calibration: Test framework with varying stopping points (2, 4, 6, 8 rounds) to verify Round 3-4 peak replication; 2) Grounding data ablation: Provide progressively reduced initial information to quantify information scope effects; 3) Evaluator validation: Compare human expert scores against individual LLM evaluators to identify optimal configurations

## Open Questions the Paper Calls Out

1. **Domain generalization**: Does the bias-variance trade-off and optimal early stopping window (Rounds 3-4) generalize across diverse agricultural systems and non-agricultural domains? The study tested only rice yield data from 26 fields in Fukushima, Japan, requiring cross-domain validation.

2. **Adaptive stopping mechanisms**: What adaptive mechanisms can detect real-time quality degradation and trigger optimal early stopping? The study used post-hoc evaluation to identify peak quality, but no mechanism exists for the system to self-detect degradation.

3. **End-user evaluation**: How do end-users (farmers) evaluate recommendation quality compared to domain experts and LLM evaluators? The study evaluated recommendations using crop scientists and LLMs, but not actual farmers who would implement them.

4. **External knowledge integration**: Can integration of external knowledge sources (RAG, MCP) extend the productive refinement window beyond Rounds 3-4? The quality degradation in later rounds was attributed to ungrounded reasoning when lacking relevant data, but this hypothesis was not tested with augmented data sources.

## Limitations

- **Dataset specificity**: Results based on rice yield data from 26 Japanese fields may not generalize to other crops or geographical regions
- **Static information scope**: Quality degradation in later rounds attributed to limited grounding data, but external knowledge integration was not tested
- **Post-hoc stopping criterion**: Optimal stopping point identified through post-evaluation rather than real-time quality monitoring

## Confidence

- **High confidence**: Inverted U-shaped quality trajectory (ANOVA/GAM results), SHAP-based XAI methodology, and statistical analysis framework are well-documented and reproducible
- **Medium confidence**: Agentic refinement mechanism relies on undocumented prompt engineering; evaluator convergence depends on untested prompt configurations
- **Low confidence**: Generalization of bias-variance trade-off pattern to other domains and scalability of LLM evaluators to replace human experts in all contexts

## Next Checks

1. **Stopping criterion calibration**: Test framework on held-out data with stopping points at 2, 4, 6, and 8 rounds to verify Round 3-4 peak replication and whether Conciseness/Contextual Relevance metrics provide reliable early-warning signals

2. **Grounding data ablation**: Systematically reduce initial information provided to the agent (SHAP only → SHAP + statistics → SHAP + domain literature) to quantify how information scope affects peak quality and degradation rate

3. **Evaluator validation**: Compare human expert scores against individual LLM evaluators to identify which models and prompt configurations maximize agreement, testing whether removing highest-divergence LLMs improves ensemble reliability