---
ver: rpa2
title: 'Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based
  Continual Learning'
arxiv_id: '2507.09118'
source_url: https://arxiv.org/abs/2507.09118
tags:
- modality
- learning
- clip
- continual
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MG-CLIP, a method for CLIP-based continual
  learning that preserves and compensates for the modality gap. The modality gap reflects
  pre-trained knowledge and expands during fine-tuning, causing forgetting.
---

# Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning

## Quick Facts
- arXiv ID: 2507.09118
- Source URL: https://arxiv.org/abs/2507.09118
- Authors: Linlan Huang; Xusheng Cao; Haori Lu; Yifan Meng; Fei Yang; Xialei Liu
- Reference count: 40
- One-line primary result: MG-CLIP outperforms existing CLIP-based continual learning methods by preserving the modality gap through early stopping and compensating with an intra-modal classifier, without requiring additional data.

## Executive Summary
This paper addresses the challenge of catastrophic forgetting in continual learning with vision-language models like CLIP. The authors identify that standard fine-tuning expands the modality gap (the angular separation between image and text feature cones), which degrades both old task performance and zero-shot generalization. MG-CLIP solves this by monitoring and limiting gap expansion during training while introducing an auxiliary visual classifier to recover plasticity. Experiments show superior performance on multiple benchmarks compared to both standard replay and non-replay methods.

## Method Summary
MG-CLIP uses LoRA-based adaptation on a frozen CLIP backbone with two key innovations: (1) Modality gap preservation through early stopping when the negative cosine similarity between image-text pairs deviates by more than 10% from the initial task value, and (2) Gap compensation via an auxiliary cosine classifier operating directly on visual features, whose output is fused with the CLIP text classifier during inference. The method estimates a global training epoch count from the first task and applies it to all subsequent tasks.

## Key Results
- Outperforms standard replay-based methods on CIFAR-100 and ImageNet benchmarks without requiring additional data storage
- Better preserves zero-shot capabilities on held-out datasets compared to baselines
- Demonstrates that modality gap preservation directly correlates with reduced catastrophic forgetting
- Shows the visual classifier effectively compensates for the limited plasticity caused by gap preservation

## Why This Works (Mechanism)

### Mechanism 1: Modality Gap Preservation via Early Stopping
Standard cross-entropy loss expands the modality gap by pushing image and text features apart, causing forgetting. MG-CLIP monitors mean negative cosine similarity during training and halts when the relative deviation exceeds a threshold (e.g., 10%), preventing over-distortion of the pre-trained geometry.

### Mechanism 2: Plasticity Recovery via Intra-Modal Compensation
While preserving the gap aids stability, it limits plasticity since the text classifier operates in a lower-rank subspace. MG-CLIP trains a separate cosine classifier on image features and fuses its output with the text classifier, leveraging discriminative dimensions orthogonal to the text feature space.

### Mechanism 3: LoRA-based Efficient Adaptation
Low-Rank Adaptation updates only low-rank matrices in attention blocks, limiting feature space deformation and implicitly supporting gap preservation while providing sufficient expressiveness for class-incremental learning.

## Foundational Learning

- **Concept: Modality Gap in Vision-Language Models**
  - Why needed: Central to understanding why standard fine-tuning causes forgetting in CLIP-based continual learning
  - Quick check: If you measure average cosine similarity between random image-text pairs in pre-trained CLIP, would you expect it to be 0, positive, or negative, and why does this matter for forgetting?

- **Concept: Class-Incremental Learning (CIL)**
  - Why needed: The paper addresses CIL's specific constraints: learning new classes sequentially without task identifiers at inference time
  - Quick check: How does CIL differ from Task-Incremental Learning regarding the requirement for the model to distinguish between task-specific heads?

- **Concept: Zero-Shot Generalization**
  - Why needed: A primary success metric is maintaining CLIP's ability to classify classes never seen during training
  - Quick check: Why might standard replay methods actually harm the zero-shot capabilities of a pre-trained model compared to a non-replay method?

## Architecture Onboarding

- **Component map:** Shared Backbone (ViT-B/16 CLIP Image Encoder with LoRA) -> Text Encoder (CLIP Text Encoder) -> Text Classifier -> Intra-Modal Classifier (visual space) -> Gap Monitor

- **Critical path:**
  1. Load CLIP with LoRA adapters
  2. Calculate initial negative similarity (neg_0) on Task 1 data
  3. Train with epoch-by-epoch monitoring; stop when relative change in neg exceeds 10%
  4. Fix epoch count for all subsequent tasks
  5. Train visual classifier for 3 epochs on frozen features after each task
  6. Fuse text and visual classifier outputs during inference with weight β=4

- **Design tradeoffs:**
  - Threshold α: Lower preserves stability but may underfit; higher improves new task accuracy but risks forgetting
  - Fusion weight β: Controls reliance on visual-only vs cross-modal logic
  - LoRA Rank: Higher increases capacity but increases risk of distorting pre-trained space

- **Failure signatures:**
  - Runaway Gap: Negative similarity drops rapidly while accuracy fluctuates (fix: reduce α or learning rate)
  - Stagnation: Poor new task accuracy, neg doesn't change (fix: increase LoRA rank or relax α)
  - Zero-Shot Collapse: Performance on held-out datasets drops significantly (fix: verify early stopping logic)

- **First 3 experiments:**
  1. Baseline Gap Analysis: Train on CIFAR-100 using standard CE, plot neg similarity and accuracy per task
  2. Hyperparameter Sweep (α): Run MG-CLIP with varying α (5%, 10%, 20%) on 5-task split
  3. Zero-Shot Retention Test: After CIFAR-100 training, evaluate on Oxford Pets without fine-tuning

## Open Questions the Paper Calls Out

- Can MG-CLIP be effectively integrated with explicit distillation strategies or specialized loss functions? (Basis: Authors note lack of specially designed loss functions and plan to explore integration with other continual learning methods)
- How does the method perform with fine-tuning strategies other than LoRA? (Basis: Paper uses LoRA simply and notes different implementations are left for future work)
- Is the training duration estimated from the first task robust for non-uniform task sequences? (Basis: Method estimates global epoch count from first task, assuming similar optimization needs)

## Limitations
- Optimal definition and measurement of the modality gap remains unclear - the paper uses mean negative cosine similarity but this may not capture all relevant aspects
- The empirical early stopping threshold (α = 0.10) appears tuned for specific CLIP backbone and datasets, raising generalizability concerns
- LoRA-based adaptation's sufficiency in constraining feature space deformation without compromising learning capacity is primarily empirical

## Confidence
- **High Confidence:** Core mechanism of modality gap preservation through early stopping is well-supported by empirical evidence
- **Medium Confidence:** Claim that modality gap reflects pre-trained knowledge preservation is plausible but needs more direct causation evidence
- **Low Confidence:** Assertion that LoRA sufficiently constrains feature space deformation is primarily empirical without theoretical justification

## Next Checks
1. Apply MG-CLIP to alternative vision-language models (e.g., OpenCLIP, BLIP) to verify generalizability beyond the specific CLIP variant
2. Systematically vary the modality gap measurement metric to determine robustness of reported benefits to gap quantification method
3. Conduct controlled study examining how different LoRA rank values affect both modality gap dynamics and learning capacity, establishing theoretical framework for rank selection