---
ver: rpa2
title: 'Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native
  Software Engineering'
arxiv_id: '2506.17937'
source_url: https://arxiv.org/abs/2506.17937
tags:
- software
- reuse
- code
- development
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AI-assisted generative software reuse is rapidly replacing traditional
  software reuse practices, leading to a new form of cargo cult development where
  developers trust AI-generated code without fully understanding it. This paper identifies
  key challenges including code quality evaluation, copyright concerns, and the need
  for systematic practices.
---

# Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering

## Quick Facts
- arXiv ID: 2506.17937
- Source URL: https://arxiv.org/abs/2506.17937
- Authors: Tommi Mikkonen; Antero Taivalsaari
- Reference count: 30
- AI-assisted generative software reuse is rapidly replacing traditional software reuse practices, leading to cargo cult development where developers trust AI-generated code without fully understanding it.

## Executive Summary
This position paper identifies the emergence of AI native software development as a new form of opportunistic reuse that operates through AI assistants like ChatGPT and Copilot. The authors argue this practice has evolved into a modern cargo cult, where developers accept AI-generated code without understanding its internal logic or appropriateness. While acknowledging the productivity benefits, the paper raises critical concerns about code quality evaluation, copyright issues, and the lack of systematic practices. The authors propose an initial research agenda to transform these opportunistic practices into a reliable engineering discipline.

## Method Summary
This is a conceptual position paper rather than an empirical study. The authors draw on observations from software development projects over the past 1½ years and reference 30 prior works. No experimental methodology, algorithms, or reproducible procedures are provided. The paper proposes a research agenda with open questions but lacks quantitative metrics or measurable outcomes.

## Key Results
- AI-assisted code generation is becoming the dominant form of software reuse, replacing traditional practices like searching GitHub or Stack Overflow
- Current AI native development operates as a form of cargo cult programming where developers trust generated code without understanding it
- Critical challenges include code quality evaluation, copyright concerns, and the need to move from opportunistic to systematic practices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-assisted code generation accelerates development by synthesizing patterns from vast training datasets into functional artifacts.
- Mechanism: Large language models trained on code repositories extract syntactic and semantic patterns, then generate contextually relevant code based on natural language prompts.
- Core assumption: Training data quality and diversity correlate with generation accuracy for the target domain.
- Evidence anchors: [abstract] "AI-assisted approaches in which developers place their trust on code that has been generated by artificial intelligence"; [section 3] "AI native software development relies on patterns learned from vast, diverse datasets"; [corpus] SENAI paper confirms LLMs produce functionally correct snippets but may overlook SE principles.

### Mechanism 2
- Claim: Trust in AI-generated code operates as a form of delegation to an opaque authority, creating cargo cult dynamics.
- Mechanism: Developers bypass traditional verification by treating AI as an expert oracle, shifting from "verify before trust" to "trust unless obviously wrong."
- Core assumption: Developers retain sufficient expertise to detect subtle errors in generated code.
- Evidence anchors: [section 1] "Generated code can in many cases be nearly perfect... However, occasionally AI will hallucinate results that will look deceivingly convincing"; [section 4] "developers using AI-generated code must be skillful enough to distinguish and evaluate the quality of the generated artifacts."

### Mechanism 3
- Claim: Generative reuse implicitly reuses training data fragments, creating indirect licensing and copyright exposure.
- Mechanism: Generated code is statistical recombination of training examples, and similarity to specific sources may trigger license obligations.
- Core assumption: Generated code that is "close enough" to copyrighted sources can create legal liability.
- Evidence anchors: [section 4] "developers are ultimately reusing fragments of the training datasets... generated code might not be 100% identical with known sources, it might still be close enough to trigger copyright and license issues"; mentions tools like Black Duck SCA.

## Foundational Learning

- Concept: **Cargo Cult Programming**
  - Why needed here: The paper's central thesis frames AI-assisted reuse as a new variant of cargo cult—using code without understanding its internal logic or appropriateness.
  - Quick check question: Can you explain why a code snippet works, or only that it appears to work?

- Concept: **Software Reuse Taxonomy (Ad Hoc → Opportunistic → Systematic → Institutionalized)**
  - Why needed here: The paper positions AI native development as an evolution of opportunistic reuse, and proposes moving it toward systematic practice.
  - Quick check question: What distinguishes opportunistic reuse (trawling GitHub) from institutionalized reuse (managed component libraries)?

- Concept: **Code Review at Scale**
  - Why needed here: The paper warns that reviewing AI-generated code becomes unmanageable at scale—traditional review practices may not transfer.
  - Quick check question: How would you audit 10,000 lines of generated code for correctness, security, and license compliance?

## Architecture Onboarding

- Component map:
  - Training Data Layer: Public code repositories (GitHub, NPM, PyPI), forums (Stack Overflow), documentation
  - Generation Layer: AI assistants (ChatGPT, Gemini, Copilot) that produce code artifacts
  - Evaluation Layer: Developer review, automated testing, license scanning tools (e.g., Black Duck SCA)
  - Integration Layer: Existing codebase, CI/CD pipelines, production systems
  - Governance Layer: Standards, copyright tracking, maintenance documentation

- Critical path:
  1. Prompt formulation → 2. Code generation → 3. Quality/validity assessment → 4. License/copyright check → 5. Integration with existing architecture → 6. Documentation for maintainability

- Design tradeoffs:
  - Velocity vs. Verifiability: Faster generation creates review bottlenecks
  - Abstraction vs. Control: Prompts trade precise specification for convenience
  - Trust vs. Transparency: AI "oracle" model obscures provenance and reasoning

- Failure signatures:
  - Hallucinated APIs or functions that don't exist but look plausible
  - Generated code that compiles but violates architectural boundaries
  - Monolithic outputs lacking subsystem structure
  - License contamination from training data proximity

- First 3 experiments:
  1. **Calibration test**: Generate code for a known problem, measure error rate and hallucination frequency against manual implementation
  2. **Provenance audit**: Run license scanning tools on AI-generated code to detect indirect violations
  3. **Boundary test**: Compare maintainability of generated code vs. human-written code for a subsystem refactor—assess modularity, documentation, and onboarding time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative AI-assisted reuse be transformed into a systematic method, or are the notions of systematic engineering and generative AI fundamentally in conflict?
- Basis in paper: The authors explicitly ask if it is possible to define systematic practices for generative AI-assisted reuse, or if the concepts are mutually exclusive.
- Why unresolved: Current usage is opportunistic ("try and see"), and the stochastic nature of AI models clashes with the predictability required by systematic engineering disciplines.
- Evidence: The establishment of a formal framework that successfully standardizes AI generation workflows, ensuring reproducible and verifiable outputs.

### Open Question 2
- Question: How can developers effectively evaluate the quality and security of AI-generated artifacts when they lack natural subsystem boundaries or recognizable human development patterns?
- Basis in paper: The paper questions how developers can evaluate quality if generated code does not follow typical development patterns or standard structural organization.
- Why unresolved: Traditional code review relies on recognizing design patterns and modular boundaries; AI may produce valid but unstructured or monolithic code that resists standard review techniques.
- Evidence: The development of new evaluation metrics or automated tooling capable of assessing semantic correctness independent of structural familiarity.

### Open Question 3
- Question: What are the realistic limits of "prompt engineering" in generating verifiable, production-ready end-to-end systems?
- Basis in paper: The authors ask, "What are the realistic limits of 'prompt engineering' in generating real life systems?" and question if AI can generate complete end-to-end solutions.
- Why unresolved: Generative AI currently "hallucinates" convincing but bogus code; the complexity of generating entire functional systems without human oversight introduces unacceptably high risks for production environments.
- Evidence: Empirical benchmarks measuring the functional correctness and security of fully AI-generated applications against human-written counterparts.

## Limitations
- The paper is conceptual rather than empirical, lacking specific data, metrics, or reproducible methods
- Authors' observations from "software development projects over the past 1½ years" are not documented with specific evidence
- No operationalized definitions or measurement approaches for key concepts like "cargo cult behavior"
- The paper lacks empirical validation of its central claims about AI-generated code quality and copyright issues

## Confidence
**High confidence**: The observation that AI-assisted code generation is becoming prevalent in software development is well-supported by industry trends and the widespread adoption of tools like GitHub Copilot and ChatGPT.

**Medium confidence**: The characterization of current AI-assisted reuse as "cargo cult development" is conceptually sound but lacks empirical validation.

**Low confidence**: The assertion that generated code is frequently "nearly perfect" contradicts known issues with hallucination and the documented variability in LLM code generation quality.

## Next Checks
1. **Empirical validation of cargo cult behavior**: Design and deploy a developer survey measuring actual understanding of AI-generated code they've accepted, complemented by practical code comprehension tests to validate self-reported understanding.

2. **Systematic quality comparison**: Conduct controlled experiments comparing AI-generated versus human-written code across standardized programming tasks, measuring correctness, security vulnerabilities, maintainability, and documentation completeness.

3. **License contamination detection**: Perform comprehensive copyright scanning on diverse samples of AI-generated code using tools like Black Duck SCA, establishing similarity thresholds and manual review processes to validate automated detection of potential license violations.