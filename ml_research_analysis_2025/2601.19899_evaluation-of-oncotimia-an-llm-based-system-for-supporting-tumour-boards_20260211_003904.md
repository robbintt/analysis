---
ver: rpa2
title: 'Evaluation of Oncotimia: An LLM based system for supporting tumour boards'
arxiv_id: '2601.19899'
source_url: https://arxiv.org/abs/2601.19899
tags:
- clinical
- data
- tumour
- cancer
- lung
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates ONCOTIMIA, an LLM-based system for automating
  lung cancer tumour board form completion. The system integrates retrieval-augmented
  generation (RAG) with hybrid relational-vector storage to convert unstructured clinical
  narratives into structured, standardised records.
---

# Evaluation of Oncotimia: An LLM based system for supporting tumour boards

## Quick Facts
- arXiv ID: 2601.19899
- Source URL: https://arxiv.org/abs/2601.19899
- Reference count: 5
- Primary result: LLM-assisted autocompletion achieves 80% accuracy with 20-21 second latency for lung cancer tumour board form completion

## Executive Summary
ONCOTIMIA is an LLM-based system that automates lung cancer tumour board form completion using retrieval-augmented generation (RAG) with hybrid relational-vector storage. The system transforms unstructured clinical narratives into structured, standardised tumour board records by combining semantic embeddings for unstructured text with structured data storage. Six LLMs deployed via AWS Bedrock were evaluated on ten synthetic Spanish clinical cases, achieving 80% correct field completion with clinically acceptable end-to-end latencies. Larger models demonstrated superior accuracy without prohibitive delays, indicating that LLM-assisted autocompletion is technically feasible and operationally viable in oncology workflows.

## Method Summary
The system integrates a multi-layer data lake with hybrid relational and vector storage, where structured clinical variables are stored in PostgreSQL and unstructured text is encoded as semantic embeddings via the Nomic model in Qdrant. An ETL pipeline processes raw documents through landing, staging, and refined layers before retrieval queries extract relevant fragments for LLM form completion. Six LLMs accessed through AWS Bedrock (including GPT-OSS-120b and Pixtral-large) were evaluated on ten synthetic Spanish lung cancer clinical cases generated via Qwen3-14b and validated by oncologists. The form schema consists of seven conditional blocks with rule-driven logic that activates specific sections based on responses to trigger questions.

## Key Results
- Best-performing configuration achieved 80% correct field completion with 20-21 second mean latency
- Larger models (GPT-OSS-120b, Pixtral-large) showed higher accuracy without prohibitive delays
- System demonstrates technical feasibility for automating tumour board documentation workflows

## Why This Works (Mechanism)

### Mechanism 1
Hybrid storage combining relational and vector databases enables accurate field extraction from unstructured clinical narratives. Structured clinical variables are stored in PostgreSQL while unstructured text is encoded as semantic embeddings via the Nomic model in Qdrant. At inference time, queries retrieve relevant fragments from vector storage, which are injected into prompts alongside form schemas for the LLM to complete.

### Mechanism 2
Rule-driven adaptive form logic reduces irrelevant field generation and improves output coherence. Block 1 serves as a central node collecting core variables; responses conditionally activate Blocks 2-7 (e.g., prior malignancy history activates Block 2, treatment refusal triggers Block 3). This constrains the LLM to generate only clinically relevant sections.

### Mechanism 3
Model scale improves accuracy without proportional latency increase due to serving optimizations. Larger models achieve ~80% accuracy with 20-21 second latency, while GPT-OSS-20b shows 72% accuracy with 54-second latency. AWS Bedrock's serving infrastructure appears optimized for larger models.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture pattern; understanding how retrieved context is injected into prompts is essential for debugging field extraction failures.
  - Quick check question: Can you explain how a query retrieves document fragments and how those fragments are incorporated into the LLM prompt?

- Concept: **Vector embeddings and similarity search**
  - Why needed here: The Nomic embedding model and Qdrant storage underpin the retrieval mechanism; quality issues here cascade to all downstream tasks.
  - Quick check question: What would happen if clinical terminology has poor semantic representation in the embedding space?

- Concept: **ETL pipelines for healthcare data**
  - Why needed here: The three-layer data lake with sequential ETL processes is the foundation for data quality; understanding this is critical for tracing data lineage.
  - Quick check question: Where would you look if a form field is consistently populated with incorrect values?

## Architecture Onboarding

- Component map: Data ingestion layer → Landing layer (raw documents) → Staging layer (cleaned/tokenized) → Refined layer (PostgreSQL + Qdrant) → Backend microservices → LLM abstraction layer → AWS Bedrock (6 models) → Reverse proxy handles traffic, security, logging

- Critical path: Document upload → ETL chain → Embedding generation → Vector storage → Form request → Query construction → Vector retrieval → Prompt assembly → LLM inference → Form population

- Design tradeoffs: Hybrid storage adds complexity vs. pure vector store, but enables explicit structured queries alongside semantic retrieval; synthetic data evaluation ensures privacy but may not capture real-world document variability; model interchangeability via abstraction layer adds overhead but enables A/B testing and fallback

- Failure signatures: Low accuracy on specific field types → Check embedding quality for related terminology, review retrieval results; high latency variance → Investigate AWS Bedrock throttling or prompt length growth; incorrect block activation → Review conditional logic rules and source document clarity for trigger fields

- First 3 experiments: 1) Replicate retrieval pipeline: Ingest one synthetic case, query vector store for each form block, manually inspect retrieved fragments for relevance; 2) Latency profiling: Run each of the 6 models on identical inputs, measure per-component timing (retrieval vs. generation); 3) Error categorization: For fields with incorrect completion, classify errors as retrieval-failure, extraction-failure, or schema-mismatch

## Open Questions the Paper Calls Out

### Open Question 1
Does ONCOTIMIA reduce actual documentation time and improve user acceptance in real tumour board settings, compared to current manual workflows? Future work will include formal assessment of time savings and user acceptance in real tumour board settings.

### Open Question 2
What is the error profile across clinical categories (e.g., staging vs. biomarkers vs. treatment history), and do certain fields exhibit systematically higher failure rates? Future work will include fine-grained error analysis by clinical category.

### Open Question 3
Can safety guarantees, traceability, and explainability be strengthened to reliably handle hallucinations or incomplete source documentation? Further research is needed to strengthen safety guarantees, traceability and explainability, especially in the presence of model hallucinations or incomplete source documentation.

### Open Question 4
Does ONCOTIMIA generalize to real patient data and other cancer types beyond synthetic lung cancer cases? The evaluation used only 10 synthetic Spanish lung cancer cases; external validity to real clinical data and other tumour types is unstated.

## Limitations

- Synthetic evaluation dataset may not capture real-world document variability, ambiguity, and complexity
- Ground-truth labeling methodology is not fully specified, with unclear extent of oncologist validation
- Clinical workflow integration and user acceptance remain untested in operational tumour board settings

## Confidence

- **High confidence**: Hybrid storage architecture feasibility and RAG-based field extraction mechanism
- **Medium confidence**: Model performance results (80% accuracy, 20-21s latency) based on synthetic evaluation
- **Low confidence**: Operational viability in clinical settings without real-world validation studies

## Next Checks

1. Evaluate the system on 50-100 actual clinical narratives from real tumour boards, comparing performance to synthetic case results and documenting specific failure modes unique to real documents.

2. Deploy the system in a live oncology department for one month, measuring not just accuracy and latency but also user satisfaction, documentation time reduction, and any impact on tumour board efficiency.

3. Systematically categorize errors by document source (pathology reports, imaging reports, clinical notes) to identify which document types pose the greatest challenges for the RAG pipeline and LLM extraction.