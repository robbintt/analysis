---
ver: rpa2
title: Interpreting Emergent Planning in Model-Free Reinforcement Learning
arxiv_id: '2504.01871'
source_url: https://arxiv.org/abs/2504.01871
tags:
- agent
- plan
- plans
- planning
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first mechanistic evidence that a model-free
  reinforcement learning agent can learn to plan. Using concept-based interpretability,
  the authors show that a Deep Repeated ConvLSTM (DRC) agent trained on Sokoban internally
  represents planning-relevant concepts and uses them to form and evaluate plans.
---

# Interpreting Emergent Planning in Model-Free Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2504.01871
- **Source URL:** https://arxiv.org/abs/2504.01871
- **Reference count:** 40
- **Key outcome:** First mechanistic evidence that a model-free RL agent can learn to plan, showing DRC agent internally represents planning-relevant concepts and uses them to formulate and evaluate plans in Sokoban.

## Executive Summary
This paper provides the first mechanistic evidence that a model-free reinforcement learning agent can learn to plan. Using concept-based interpretability, the authors show that a Deep Repeated ConvLSTM (DRC) agent trained on Sokoban internally represents planning-relevant concepts and uses them to form and evaluate plans. The agent learns to represent concepts predicting long-term effects of actions (Agent Approach Direction and Box Push Direction) and iteratively refines plans when given extra computation. Crucially, intervening on these concept representations can steer the agent's behavior, demonstrating their causal role in planning. The analysis reveals the agent uses a parallelized bidirectional search algorithm - searching forward from boxes and backward from targets - which differs from standard planning approaches in reinforcement learning. These findings advance understanding of emergent planning capabilities in model-free agents, which is particularly relevant given recent observations of planning-like behaviors in large language models trained with reinforcement learning.

## Method Summary
The authors train a DRC(3,3) agent on Sokoban using IMPALA and then apply concept-based interpretability techniques to investigate whether the agent has learned to plan. They use linear probing to decode planning-relevant concepts (Agent Approach Direction and Box Push Direction) from the ConvLSTM cell states, finding that 1x1 probes can predict these concepts with high accuracy. They visualize how plans form iteratively over the agent's internal computational steps, revealing a bidirectional search mechanism. Finally, they demonstrate causal steering by manually adding concept vectors to the cell state, which causes the agent to execute specific (sometimes suboptimal) plans, proving these representations drive behavior.

## Key Results
- DRC agent solves 97.3% of Sokoban test levels and benefits from extra "thinking steps"
- Linear probes decode Agent Approach Direction and Box Push Direction from ConvLSTM cell states with macro F1 scores around 0.6 (vs 0.2 baseline)
- Visualization shows plans form iteratively through bidirectional search (forward from boxes, backward from targets)
- Interventions on concept representations can steer agent behavior to execute specific plans
- 1x1 probes outperform 3x3 probes, indicating spatially-localized linear representations

## Why This Works (Mechanism)

### Mechanism 1: Linear Spatial Concept Representation
The DRC agent represents planning-relevant information as linear, spatially-localized features within its ConvLSTM cell states. The agent maintains a cell state where spatial dimensions correspond directly to the Sokoban grid, with planning concepts encoded linearly at specific locations relevant to those boxes, allowing simple linear probes to decode the plan with high accuracy.

### Mechanism 2: Parallelized Bidirectional Search
The agent generates plans by extending representations bidirectionally—forward from boxes and backward from targets—rather than using strictly sequential forward search. During internal computational ticks, the agent iteratively extends path channels or concept activations, simultaneously searching forward from current box positions and backward from goal targets until these representations meet.

### Mechanism 3: Causal Steering via Representation Engineering
The internal representations of plans are causally responsible for action selection. The agent's policy head consumes the cell state containing these plan representations, and by manually adding the vector representation of a specific plan to the cell state at a specific layer, the agent can be "steered" to execute that plan, proving the representation drives the policy.

## Foundational Learning

- **Concept: Linear Probing**
  - **Why needed here:** To verify the existence of hypothesized "concepts" inside the black-box neural network and distinguish planning from complex reflex.
  - **Quick check question:** Can you train a logistic regression classifier to predict "future box direction" from the agent's hidden state better than from raw pixels?

- **Concept: ConvLSTM (Convolutional LSTM)**
  - **Why needed here:** The DRC architecture relies on this hybrid that preserves spatial structure while performing temporal recurrence, making spatial planning possible inside a vector space.
  - **Quick check question:** Does the hidden state of the network retain the 2D grid shape of the Sokoban board, or is it flattened into a 1D vector?

- **Concept: Representation Engineering / Activation Steering**
  - **Why needed here:** To prove causality - not just seeing that the plan is there, but knowing if we can edit the plan by editing the brain.
  - **Quick check question:** If I add a vector representing "Wall" to a specific location in the hidden state, does the agent behave as if a wall exists there?

## Architecture Onboarding

- **Component map:** Symbolic Observation (8x8x7) -> Conv layer -> 8x8x32 Encoding -> 3-layer ConvLSTM stack (DRC(3,3)) -> Cell State ($g_t^d$) -> Policy Head (Logits) + Value Head

- **Critical path:**
  1. Observation encoded
  2. **Internal Ticks (Planning):** Information flows up/down the 3-layer stack over 3 ticks (where bidirectional search likely occurs)
  3. Final Tick Cell State is read by the Policy Head to select an action

- **Design tradeoffs:**
  - 1x1 vs 3x3 Probes: 1x1 proves strict spatial localization, while 3x3 might capture distributed features
  - Layer selection: Interventions work best at different layers for Agent paths vs. Box paths, suggesting functional specialization

- **Failure signatures:**
  - Correlation without Causation: Probes work (High F1), but interventions fail - network tracks plan but uses different execution path
  - Off-Distribution Drift: High scaling factors push activations outside training manifold, causing erratic behavior

- **First 3 experiments:**
  1. Train 1x1 Linear Probes: Train probes to predict $C_B$ (Box Push Direction) on final layer's cell state. Verify Macro F1 > 0.6.
  2. Visualize Iterative Refinement: Run agent on "Cutoff" level with 5 thinking steps. Visualize decoded plan at each tick to confirm coherent path formation.
  3. Implement "Box-Shortcut" Intervention: In custom level, inject "Push Right" vector into box's cell state location. Confirm agent pushes box Right even if Left is optimal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this interpretability methodology confirm emergent planning in diverse architectures (e.g., Transformers) and non-spatial environments?
- **Basis in paper:** Section 9 explicitly calls for extending investigation to "other RL agents, and other environments."
- **Why unresolved:** The methodology relies on spatial correspondence between cell states and Sokoban grid, which may not exist in generic agents lacking spatially-structured hidden states.
- **What evidence would resolve it:** Successful application of probing and intervention protocol to agents with unstructured hidden states or non-spatial domains.

### Open Question 2
- **Question:** Which specific training factors (architectural recurrence vs. environment dynamics) are causally necessary for emergence of planning?
- **Basis in paper:** Section 9 notes it would be "helpful to better understand the role of different training factors, e.g., model architecture, environment dynamics."
- **Why unresolved:** Paper demonstrates planning emerges in DRC agent but doesn't isolate whether ConvLSTM structure or Sokoban nature was primary catalyst.
- **What evidence would resolve it:** Ablation studies measuring emergence of planning-relevant concepts across agents with varied recurrence depths or training environments.

### Open Question 3
- **Question:** Does reliance on linear probes fail to capture planning mechanisms represented non-linearly in activation space?
- **Basis in paper:** Section 2.4 restricts analysis to linearly represented concepts, while Appendix E.7 admits this "assumption may be argued to be overly-restrictive."
- **Why unresolved:** If agent plans using complex non-linear feature interactions, linear probes would fail to decode relevant concepts, potentially resulting in false negatives regarding planning.
- **What evidence would resolve it:** Application of non-linear probes to trained agent to identify any planning-relevant concepts missed by linear analysis.

## Limitations

- Scalability concerns: Unclear whether linear spatial representation and bidirectional search mechanisms generalize to larger, more complex environments with continuous actions or partial observability
- Methodology dependence: Analysis depends heavily on availability of symbolic observations and ability to track ground-truth plans through future trajectories
- Task specificity: Results may be specific to Sokoban's spatial structure and deterministic rules, limiting generalizability

## Confidence

**High Confidence (Likelihood >80%):**
- DRC agent learns to plan and represents planning-relevant concepts in cell states
- Linear probes successfully decode Agent Approach Direction and Box Push Direction
- Interventions on representations can causally steer agent behavior

**Medium Confidence (Likelihood 60-80%):**
- Specific mechanism is parallelized bidirectional search rather than alternative planning algorithms
- Planning representations are spatially localized rather than distributed

**Low Confidence (Likelihood <60%)**
- Planning mechanism generalizes to other model-free RL agents or architectures
- Same interpretability approach scales to more complex environments or language models

## Next Checks

1. **Ablation of Spatial Structure:** Train variant DRC agent without spatial convolutions (using fully connected layers) and test whether planning concepts can still be linearly decoded from cell state to test if spatial organization is essential.

2. **Intervention Robustness Testing:** Systematically vary intervention scaling factor α and measure point at which steering behavior breaks down, comparing to activation norms in training distribution to determine if interventions push representations outside learned manifold.

3. **Cross-Domain Planning Detection:** Apply same probing and intervention methodology to model-free agent trained on different planning task (such as 2D maze navigation or simple grid-world puzzle) to test if planning mechanism is task-specific or more general emergent capability.