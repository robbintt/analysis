---
ver: rpa2
title: Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social
  Media Recommendation
arxiv_id: '2511.14768'
source_url: https://arxiv.org/abs/2511.14768
tags:
- emotional
- user
- engagement
- content
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ESMR, a hybrid recommendation framework that
  combines LightGBM for engagement and reinforcement learning (RL) with causally informed
  rewards to personalize social media content based on user emotional states. ESMR
  predicts emotions using a TabTransformer model and applies causal discovery (DirectLiNGAM)
  to identify behavioral drivers of emotional shifts.
---

# Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social Media Recommendation

## Quick Facts
- **arXiv ID**: 2511.14768
- **Source URL**: https://arxiv.org/abs/2511.14768
- **Reference count**: 40
- **Primary result**: Hybrid LightGBM+RL framework reduces negative emotion days from 5.22 to 2.14 and achieves 97.7% happy emotion prevalence in 30-day simulated traces

## Executive Summary
This paper introduces ESMR, a hybrid recommendation system that combines LightGBM for engagement prediction with reinforcement learning (RL) for adaptive, emotion-aware content personalization. The framework leverages causal discovery to identify behavioral drivers of emotional shifts and activates RL only when users exhibit prolonged negative emotions, balancing engagement with emotional recovery. Evaluated on simulated 30-day user traces, ESMR significantly reduces negative emotion days, increases emotional wellbeing, and cuts emotional volatility, all while maintaining strong engagement retention with only a 2.25% reduction in engagement reward versus baseline. Ablation studies confirm the complementary roles of emotion and engagement shaping in the system's success.

## Method Summary
ESMR is a hybrid recommendation framework that combines LightGBM for engagement prediction with reinforcement learning (RL) for adaptive, emotion-aware content personalization. The system predicts user emotions using a TabTransformer model and applies causal discovery (DirectLiNGAM) to identify behavioral drivers of emotional shifts. RL is only activated when users exhibit prolonged negative emotions, optimizing for both engagement and emotional recovery. The model is evaluated on 30-day simulated user traces, showing significant reductions in negative emotion days and emotional volatility while maintaining strong engagement retention.

## Key Results
- Reduces negative emotion days from 5.22 to 2.14 in 30-day simulated traces
- Increases happy emotion prevalence to 97.7% and cuts emotional volatility by 37.8%
- Achieves strong engagement retention with only 2.25% reduction in engagement reward versus baseline

## Why This Works (Mechanism)
The hybrid approach leverages LightGBM's strengths in engagement prediction and RL's adaptability for emotional regulation, activating RL only when needed to optimize both engagement and emotional wellbeing. Causal discovery identifies behavioral drivers of emotional shifts, enabling targeted interventions. This targeted activation of RL ensures efficient resource use and prevents over-intervention, while the balance between engagement and emotional recovery is maintained through carefully designed reward structures.

## Foundational Learning
- **LightGBM for engagement prediction**: Efficient gradient boosting for tabular engagement data, providing strong baseline predictions.
- **TabTransformer for emotion prediction**: Effective for tabular emotion features, capturing non-linear relationships.
- **DirectLiNGAM for causal discovery**: Identifies linear causal relationships between behaviors and emotions, enabling targeted interventions.
- **Reinforcement learning for adaptive personalization**: Optimizes long-term user outcomes by balancing engagement and emotional recovery.
- **Simulation-based evaluation**: Allows controlled, reproducible testing of recommendation strategies over extended periods.

## Architecture Onboarding

**Component Map**: User Behavior Data -> TabTransformer (Emotion Prediction) -> DirectLiNGAM (Causal Discovery) -> LightGBM (Engagement) <-> RL Agent (Adaptive Recommendation)

**Critical Path**: Emotion Prediction -> Causal Discovery -> RL Activation (if prolonged negative emotion) -> Personalized Recommendation

**Design Tradeoffs**: Balance between engagement and emotional wellbeing; activation of RL only after prolonged negative emotions to avoid over-intervention.

**Failure Signatures**: Over-intervention leading to reduced engagement; failure to detect prolonged negative emotions; inaccurate emotion predictions affecting RL activation.

**First Experiments**:
1. Validate emotion prediction accuracy on held-out data
2. Test causal discovery on synthetic emotion-behavior datasets
3. Simulate RL activation thresholds to optimize emotional recovery

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation-based evaluation may not fully capture real-world user behavior and emotional dynamics
- 30-day simulation window may be too short to assess long-term psychological impacts
- RL reward design involves subjective weightings that could significantly impact outcomes

## Confidence
- **High confidence**: Engagement metrics and emotional volatility reduction
- **Medium confidence**: Long-term emotional impact predictions
- **Medium confidence**: Causal relationships identified

## Next Checks
1. Conduct A/B testing with real users over 90+ days to validate simulation findings and assess real-world effectiveness
2. Implement cross-validation with alternative emotion prediction models (e.g., transformer-based architectures) to test robustness of emotional state classification
3. Perform sensitivity analysis on RL reward weightings and activation thresholds to identify optimal balance points between engagement and emotional wellbeing