---
ver: rpa2
title: Improving MLLM Historical Record Extraction with Test-Time Image
arxiv_id: '2509.09722'
source_url: https://arxiv.org/abs/2509.09722
tags:
- confidence
- ensemble
- accuracy
- image
- consensus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel ensemble framework for improving
  text extraction from noisy historical documents using a single MLLM (Gemini 2.0
  Flash). The approach generates multiple augmented versions of each document image
  through test-time augmentations (blur, padding, grid warp, etc.) and uses a Needleman-Wunsch-style
  aligner to produce both a consensus transcription and an agreement-based confidence
  score.
---

# Improving MLLM Historical Record Extraction with Test-Time Image

## Quick Facts
- **arXiv ID**: 2509.09722
- **Source URL**: https://arxiv.org/abs/2509.09722
- **Reference count**: 40
- **Primary result**: Ensemble framework improves field transcription accuracy by over 4 percentage points on 622 Pennsylvania death records

## Executive Summary
This paper introduces a novel ensemble framework for improving text extraction from noisy historical documents using a single MLLM (Gemini 2.0 Flash). The approach generates multiple augmented versions of each document image through test-time augmentations (blur, padding, grid warp, etc.) and uses a Needleman-Wunsch-style aligner to produce both a consensus transcription and an agreement-based confidence score. The ensemble method demonstrates over 4 percentage points improvement in field transcription accuracy relative to single-shot baseline predictions.

## Method Summary
The proposed method employs test-time augmentations to generate multiple perturbed versions of input document images, which are then processed by a single MLLM (Gemini 2.0 Flash) to produce diverse predictions. These predictions are aligned using a Needleman-Wunsch-style sequence alignment algorithm, enabling the generation of consensus transcriptions and agreement-based confidence scores. The approach introduces a new dataset of 622 Pennsylvania death records for evaluation and demonstrates that different augmentation types (padding, blurring, grid warp) have varying effectiveness for accuracy improvement versus confidence estimation.

## Key Results
- Ensemble method improves field transcription accuracy by over 4 percentage points compared to single-shot baseline
- Padding and blurring augmentations are most effective for accuracy gains
- Grid-warp perturbations are best for confidence estimation
- Simple, scalable approach deployable to other document collections

## Why This Works (Mechanism)
The ensemble framework leverages test-time augmentation to create diverse input representations of the same document, which helps overcome the limitations of single MLLM predictions on noisy historical documents. By generating multiple augmented variants and aligning their outputs, the system can identify and correct errors that might occur in any single prediction. The agreement-based confidence scores provide a quantitative measure of prediction reliability, which is crucial for practical deployment in archival digitization workflows.

## Foundational Learning
- **Test-time augmentation**: Creating multiple perturbed versions of input during inference to improve robustness and accuracy
  - *Why needed*: Historical documents have varying degradation patterns that a single model pass may not handle effectively
  - *Quick check*: Verify that augmented images still contain readable text content after transformation

- **Needleman-Wunsch alignment**: Dynamic programming algorithm for optimal sequence alignment
  - *Why needed*: Aligns predictions from different augmented versions to identify consensus and discrepancies
  - *Quick check*: Confirm alignment produces reasonable character mappings between predictions

- **Agreement-based confidence scoring**: Using prediction consensus as a proxy for reliability
  - *Why needed*: Provides quantitative measure of transcription confidence without human annotation
  - *Quick check*: Correlate agreement scores with actual transcription accuracy on validation set

## Architecture Onboarding
- **Component map**: Image Augmentation -> MLLM Inference -> Needleman-Wunsch Alignment -> Consensus Generation -> Confidence Scoring
- **Critical path**: Document image → Augmentation pipeline → MLLM (Gemini 2.0 Flash) → Alignment algorithm → Final transcription
- **Design tradeoffs**: Single MLLM vs. ensemble of models (computational efficiency vs. potential accuracy gains), alignment complexity vs. consensus quality
- **Failure signatures**: Low agreement scores may indicate ambiguous text regions, poor augmentation choices, or MLLM limitations on specific document types
- **First experiments**: 1) Test augmentation effectiveness on small document subset, 2) Validate alignment quality on synthetic perturbed pairs, 3) Compare confidence scores against human expert annotations

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness heavily dependent on augmentation choice and alignment strategy
- Evaluation limited to single dataset (622 Pennsylvania death records)
- Confidence scores not validated against human expert annotations
- Optimal augmentation set may vary significantly across document types

## Confidence
- **High Confidence**: Ensemble approach demonstrably improves transcription accuracy over single-shot predictions
- **Medium Confidence**: Specific augmentation effectiveness ranking may not generalize across document collections
- **Low Confidence**: Reliability of agreement-based confidence scores for practical deployment

## Next Checks
1. Cross-dataset validation: Evaluate the ensemble framework on at least three additional historical document collections with varying degradation patterns, layouts, and languages to assess generalizability of augmentation effectiveness and accuracy gains.

2. Confidence calibration study: Compare agreement-based confidence scores against human expert annotations on a subset of documents to determine if higher agreement correlates with higher transcription accuracy and to establish appropriate confidence thresholds for practical use.

3. Ablation analysis on alignment strategy: Test alternative alignment approaches (e.g., dynamic programming variants, neural aligners) and different consensus voting schemes to determine if the Needleman-Wunsch-based method is optimal or if simpler approaches yield comparable results with reduced computational overhead.