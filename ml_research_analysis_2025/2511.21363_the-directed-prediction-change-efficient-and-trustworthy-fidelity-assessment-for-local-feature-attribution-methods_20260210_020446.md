---
ver: rpa2
title: The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment
  for Local Feature Attribution Methods
arxiv_id: '2511.21363'
source_url: https://arxiv.org/abs/2511.21363
tags:
- methods
- local
- perturbation
- data
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel metric for evaluating the fidelity
  of local feature attribution methods. The core idea is to modify the existing Prediction
  Change (PC) metric by incorporating the direction of both perturbation and attribution,
  resulting in the Directed Prediction Change (DPC) metric.
---

# The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods

## Quick Facts
- **arXiv ID**: 2511.21363
- **Source URL**: https://arxiv.org/abs/2511.21363
- **Reference count**: 39
- **Primary result**: Introduces Directed Prediction Change (DPC) metric that achieves 10x speedup over baseline while providing deterministic evaluation of local feature attribution methods

## Executive Summary
This paper addresses the challenge of evaluating local feature attribution (FA) methods by introducing the Directed Prediction Change (DPC) metric. DPC builds upon the existing Prediction Change (PC) metric by incorporating the direction of both perturbations and attributions, enabling effective evaluation of local FA methods. The key innovation is that DPC achieves an almost tenfold speedup compared to PC while eliminating randomness, resulting in a deterministic and trustworthy evaluation procedure. Across 4,744 distinct explanations, DPC demonstrates that combining it with PC enables comprehensive evaluation of both baseline-oriented and local feature attribution methods with reproducible outcomes.

## Method Summary
The DPC metric modifies the existing PC metric by incorporating directional information into the evaluation process. While PC measures prediction change after feature perturbation without considering attribution direction, DPC tracks both the direction of feature attribution and the direction of perturbation. The method replaces features with a zero (mean) baseline in a stepwise manner, calculating prediction changes that are weighted by the alignment between attribution and perturbation directions. This directional approach allows DPC to evaluate local FA methods more effectively while maintaining computational efficiency. The implementation uses a fixed number of perturbation steps (20) and operates deterministically without randomness, unlike some existing evaluation approaches.

## Key Results
- DPC achieves an almost tenfold speedup compared to existing Prediction Change (PC) metric
- DPC eliminates randomness, providing deterministic and reproducible evaluation outcomes
- DPC enables holistic evaluation of both baseline-oriented and local feature attribution methods when used alongside PC
- Validated across 4,744 distinct explanations demonstrating effectiveness and computational efficiency

## Why This Works (Mechanism)
The paper doesn't provide explicit mechanism details for why DPC works, but the directional incorporation of attribution and perturbation information allows for more precise measurement of attribution fidelity by considering the alignment between feature importance and their actual impact on model predictions.

## Foundational Learning

**Feature Attribution Methods**: Techniques that assign importance scores to input features explaining model predictions. *Why needed*: DPC is specifically designed to evaluate these methods' fidelity. *Quick check*: Can you explain the difference between local and baseline-oriented FA methods?

**Prediction Change (PC) Metric**: Baseline metric that evaluates attribution methods by measuring prediction change after feature perturbation. *Why needed*: DPC builds directly upon PC as its foundation. *Quick check*: How does PC differ from DPC in handling attribution directions?

**Fidelity Assessment**: Evaluation of how well explanation methods capture true model behavior. *Why needed*: Core purpose of DPC metric development. *Quick check*: What makes a fidelity metric trustworthy and efficient?

**Perturbation Direction**: The direction (increase/decrease) of feature modification during evaluation. *Why needed*: DPC explicitly incorporates this to improve evaluation accuracy. *Quick check*: Why might ignoring perturbation direction lead to misleading evaluations?

**Deterministic Evaluation**: Methods that produce identical results across repeated runs. *Why needed*: DPC claims to eliminate randomness for reproducibility. *Quick check*: How does randomness typically affect FA method evaluation?

## Architecture Onboarding

**Component Map**: Input Features -> Feature Attribution Methods -> DPC Evaluation -> Prediction Change Scores -> Fidelity Assessment

**Critical Path**: Feature perturbation sequence → Direction alignment calculation → Prediction change measurement → DPC score aggregation

**Design Tradeoffs**: Fixed perturbation steps (20) provide consistency but may limit adaptability; zero baseline replacement is simple but may create out-of-distribution issues for nonlinear models

**Failure Signatures**: Conflicting perturbation directions may indicate suboptimal step sizes; out-of-distribution effects from baseline replacement may reduce expressiveness for complex models

**Three First Experiments**:
1. Compare DPC scores between local FA methods and baseline-oriented methods on the same dataset
2. Measure convergence behavior by varying perturbation step counts from 5 to 50
3. Evaluate DPC robustness on adversarial examples versus clean data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does varying the number of perturbation steps influence the stability and precision of the Directed Prediction Change (DPC) metric?
- Basis in paper: [explicit] The authors explicitly state in the "Future Work" section that "future work could study the effect of the number of perturbation steps on DPC."
- Why unresolved: The current implementation uses a fixed number of steps (20) without a systematic analysis of how this hyperparameter impacts the metric's robustness or convergence.
- What evidence would resolve it: A sensitivity analysis measuring the variance and convergence of DPC scores across a wide range of perturbation step counts on both tabular and image data.

### Open Question 2
- Question: Can using local perturbations instead of baseline replacement in the DPC framework effectively mitigate out-of-distribution effects in highly nonlinear models?
- Basis in paper: [explicit] The "Future Work" section suggests it "would further be valuable to examine local perturbations for DPC instead of baseline replacement to counter out-of-distribution effects."
- Why unresolved: The current method replaces features with a zero (mean) baseline, which may push data out of the model's training distribution, hampering expressiveness for nonlinear models.
- What evidence would resolve it: Comparative experiments evaluating DPC scores using in-distribution local perturbations (e.g., noise injection) versus the standard baseline replacement method on complex data types.

### Open Question 3
- Question: Do adaptive perturbation step sizes offer superior evaluation performance compared to the fixed step sizes currently utilized in the DPC calculation?
- Basis in paper: [explicit] The authors list exploring "adaptive choices for the perturbation step size" as a specific direction for future work.
- Why unresolved: The current fixed step size may be suboptimal for different datasets or feature types, potentially contributing to the "conflicting perturbation directions" noted in the Limitations section.
- What evidence would resolve it: Implementing an adaptive step-size algorithm within the DPC framework and comparing its fidelity correlation and ranking consistency against the fixed-size baseline.

## Limitations
- Performance metrics and computational complexity analysis are not provided despite claims of 10x speedup
- Claims of effectiveness across 4,744 explanations lack details about dataset and model diversity
- Does not clearly specify evaluation gaps or provide systematic comparison with alternative fidelity metrics beyond PC

## Confidence
- **High confidence**: The core methodological contribution of incorporating directional information into the PC metric is well-defined and technically sound
- **Medium confidence**: The claimed performance improvements (speedup and determinism) are supported by the conceptual framework but lack empirical validation details
- **Low confidence**: The generalizability claims across diverse explanations and the completeness of evaluation relative to existing metrics are not sufficiently substantiated

## Next Checks
1. Provide detailed runtime benchmarks comparing DPC against PC and other state-of-the-art fidelity metrics across multiple computational environments and hardware configurations

2. Conduct ablation studies to quantify the specific contribution of directional components to evaluation accuracy, separating the effects of determinism from directional information

3. Test DPC's performance on out-of-distribution data and adversarial examples to evaluate robustness claims, particularly in scenarios where feature attribution methods may fail or produce misleading explanations