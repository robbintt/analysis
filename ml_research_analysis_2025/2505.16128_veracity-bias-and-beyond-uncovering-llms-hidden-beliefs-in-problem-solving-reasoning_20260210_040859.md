---
ver: rpa2
title: 'Veracity Bias and Beyond: Uncovering LLMs'' Hidden Beliefs in Problem-Solving
  Reasoning'
arxiv_id: '2505.16128'
source_url: https://arxiv.org/abs/2505.16128
tags:
- bias
- llms
- groups
- evaluation
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study reveals that large language models (LLMs) exhibit systematic
  biases linking solution correctness to demographic groups, beyond explicit social
  context provocations. Through experiments on five prevalent LLMs across mathematics,
  coding, commonsense reasoning, and writing tasks, the research identifies two forms
  of veracity bias: Attribution Bias, where models disproportionately attribute correct
  solutions to certain demographic groups, and Evaluation Bias, where identical solutions
  receive different assessments based on perceived authorship.'
---

# Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning

## Quick Facts
- arXiv ID: 2505.16128
- Source URL: https://arxiv.org/abs/2505.16128
- Reference count: 40
- Primary result: LLMs exhibit systematic biases linking solution correctness to demographic groups, showing Attribution Bias and Evaluation Bias across multiple task domains

## Executive Summary
This study reveals that large language models systematically exhibit veracity bias, where they disproportionately attribute correct solutions to certain demographic groups and evaluate identical solutions differently based on perceived authorship. Through experiments across five prevalent LLMs in mathematics, coding, commonsense reasoning, and writing tasks, researchers identified two distinct forms of bias: Attribution Bias, where models assign correct solutions to preferred demographic groups, and Evaluation Bias, where identical solutions receive different assessments based on authorship. The findings show pervasive biases, with African-American groups consistently receiving fewer correct attributions in math and coding, while Asian authors are least preferred in writing evaluation. The research raises significant concerns about deploying LLMs in educational and evaluation settings.

## Method Summary
The researchers conducted controlled experiments across five prevalent LLMs using carefully constructed prompts that varied demographic information while keeping solution content constant. They tested four distinct task domains: mathematics problems, coding challenges, commonsense reasoning questions, and writing evaluation tasks. For each task, they measured two types of veracity bias - attribution bias (whether models correctly attributed solutions to demographic groups) and evaluation bias (whether identical solutions received different quality assessments based on perceived authorship). The study also included an analysis of color stereotyping in visualization code, where models automatically assigned stereotypical colors to racial groups when generating code for data visualizations.

## Key Results
- African-American groups consistently received fewer correct attributions and more incorrect ones in mathematics and coding tasks across multiple LLMs
- Asian authors were least preferred in writing evaluation tasks, receiving systematically lower quality assessments for identical solutions
- Models automatically assigned stereotypical colors to racial groups in visualization code, demonstrating implicit bias beyond explicit task prompts
- Both Attribution Bias and Evaluation Bias were observed across all tested task domains, indicating pervasive veracity bias in LLMs

## Why This Works (Mechanism)
The mechanisms underlying veracity bias in LLMs appear to stem from learned associations between demographic information and solution quality embedded during training. These biases emerge automatically during model reasoning processes, suggesting they are deeply integrated into the models' internal representations rather than surface-level phenomena. The automatic nature of these biases indicates they are not merely responses to explicit prompts but reflect learned patterns about the relationship between demographic groups and problem-solving abilities.

## Foundational Learning
- **Attribution Bias**: The tendency to disproportionately attribute correct solutions to certain demographic groups. Why needed: Forms the core mechanism of veracity bias that this study investigates. Quick check: Does the model consistently assign correct solutions to specific demographic groups across multiple tasks?
- **Evaluation Bias**: The phenomenon where identical solutions receive different assessments based on perceived authorship. Why needed: Complements attribution bias by showing how biases affect quality evaluation, not just solution assignment. Quick check: Do models rate the same solution differently when presented with different demographic attributions?
- **Large Language Model Architecture**: Transformer-based models trained on diverse web data. Why needed: Understanding the model architecture helps explain how biases can become embedded in reasoning processes. Quick check: Are the observed biases consistent across different model architectures and training approaches?
- **Problem-Solving Reasoning**: The cognitive processes models use to solve mathematical, coding, commonsense, and writing tasks. Why needed: Provides context for understanding how biases manifest in different reasoning domains. Quick check: Do biases appear consistently across all reasoning domains or only in specific types?
- **Demographic Prompting**: The technique of varying demographic information in prompts while keeping solution content constant. Why needed: The experimental methodology used to isolate and measure veracity bias. Quick check: Does changing only demographic information produce different model responses?
- **Color Stereotyping**: The automatic assignment of stereotypical colors to racial groups in visualization tasks. Why needed: Demonstrates implicit bias extending beyond explicit problem-solving tasks. Quick check: Do models consistently use stereotypical colors when generating visualization code for different demographic groups?

## Architecture Onboarding
Component map: Demographic Prompt -> LLM Reasoning -> Solution Attribution/Evaluation -> Bias Manifestation
Critical path: Input prompt containing demographic information flows through model reasoning to produce biased attributions or evaluations
Design tradeoffs: The study uses controlled prompts to isolate biases but may oversimplify real-world demographic representation
Failure signatures: Consistent patterns of correct solutions attributed to preferred groups, identical solutions receiving different quality assessments, automatic stereotyping in visualization tasks
3 first experiments:
1. Test multiple demographic variations within the same task to identify specific patterns of preference
2. Compare model responses across different reasoning domains to assess domain-specific biases
3. Analyze the relationship between demographic information placement in prompts and bias strength

## Open Questions the Paper Calls Out
None

## Limitations
- The sample of five LLMs may not represent the full diversity of model architectures and training approaches
- Demographic group prompts, while carefully constructed, may not capture all nuances of real-world demographic representation
- The study focuses on binary attributions (correct/incorrect) which may oversimplify complex reasoning processes
- Writing task results show high variability between models that warrant further investigation

## Confidence
High confidence in: the existence of systematic veracity bias across multiple LLMs and task types; the distinction between attribution and evaluation bias mechanisms; and the pervasiveness of these biases across mathematics, coding, commonsense reasoning, and writing domains.

Medium confidence in: the specific magnitude of bias effects across different demographic groups; the interpretation of model behavior as reflecting "beliefs" rather than learned associations; and the generalizability of findings to all LLMs and cultural contexts.

Low confidence in: the precise mechanisms by which these biases emerge during training; the extent to which these biases reflect real-world stereotypes versus artifacts of training data distribution; and the implications for specific deployment scenarios without further contextual analysis.

## Next Checks
1. Conduct experiments with larger and more diverse model samples including different architectural approaches (transformers vs other architectures) and training paradigms to assess generalizability of findings.
2. Design cross-cultural studies using demographic categories and prompts relevant to non-Western contexts to evaluate the universality of identified biases.
3. Implement ablation studies varying training data composition and fine-tuning procedures to isolate the sources and mechanisms of veracity bias emergence.