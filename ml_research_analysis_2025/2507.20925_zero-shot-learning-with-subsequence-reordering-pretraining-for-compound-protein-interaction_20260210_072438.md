---
ver: rpa2
title: Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein
  Interaction
arxiv_id: '2507.20925'
source_url: https://arxiv.org/abs/2507.20925
tags:
- protein
- learning
- methods
- performance
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses zero-shot compound-protein interaction (CPI)
  prediction, which is critical for drug discovery but challenging due to novel proteins
  and vast chemical space. Existing methods fail to capture complex subsequence dependencies
  within proteins and rely heavily on large, specialized datasets.
---

# Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction

## Quick Facts
- arXiv ID: 2507.20925
- Source URL: https://arxiv.org/abs/2507.20925
- Authors: Hongzhi Zhang; Zhonglie Liu; Kun Meng; Jiameng Chen; Jia Wu; Bo Du; Di Lin; Yan Che; Wenbin Hu
- Reference count: 40
- Primary result: PSRP-CPI achieves up to 13.98% AUROC and 36.36% AUPRC gains in zero-shot CPI prediction

## Executive Summary
This paper addresses the critical challenge of zero-shot compound-protein interaction (CPI) prediction in drug discovery, where novel proteins and vast chemical space create significant obstacles. Existing CPI methods struggle to capture complex subsequence dependencies within proteins and depend heavily on large, specialized datasets. The authors propose PSRP-CPI, a novel pretraining approach using subsequence reordering to explicitly model dependencies between protein subsequences. This method demonstrates substantial improvements over baseline approaches, particularly in zero-shot scenarios where both compounds and proteins are unseen during training.

## Method Summary
PSRP-CPI introduces a subsequence reordering pretraining strategy that explicitly captures dependencies between protein subsequences by shuffling their order during training. The method also incorporates length-variable protein augmentation to enhance performance on small datasets. By pretraining on protein sequence data before fine-tuning on CPI tasks, the model learns robust protein representations that transfer effectively to zero-shot scenarios. The approach is integrated with existing CPI methods and evaluated across four benchmark datasets, showing consistent improvements in prediction accuracy, particularly for unseen proteins and compounds.

## Key Results
- Achieves up to 13.98% AUROC and 36.36% AUPRC improvements in zero-shot CPI prediction
- Demonstrates state-of-the-art performance on the BindingDB dataset
- Shows significant advantages in data-scarce settings compared to existing pretraining models
- Particularly effective in Unseen-Both scenarios where both compounds and proteins are novel

## Why This Works (Mechanism)
The subsequence reordering pretraining explicitly forces the model to learn robust representations of protein subsequences by disrupting their natural order during training. This process enhances the model's ability to capture intrinsic dependencies between subsequences that are crucial for understanding protein function and binding properties. The length-variable augmentation strategy further improves generalization by exposing the model to proteins of varying lengths, preventing overfitting to specific sequence patterns. Together, these mechanisms enable the model to develop more flexible and transferable protein representations that perform well even when encountering entirely novel protein-compound pairs.

## Foundational Learning

**Compound-Protein Interaction Prediction**: Predicting whether a compound will bind to a specific protein target. *Why needed*: Core task in drug discovery for identifying potential drug candidates. *Quick check*: Model must predict binary interaction outcomes from protein and compound inputs.

**Zero-Shot Learning**: Making predictions for samples from classes not seen during training. *Why needed*: Essential for drug discovery where novel proteins and compounds constantly emerge. *Quick check*: Model must perform well on entirely unseen protein-compound pairs.

**Subsequence Dependencies**: Relationships between different segments of protein sequences. *Why needed*: Proteins' binding properties depend on interactions between multiple sequence regions. *Quick check*: Model must capture long-range dependencies within protein sequences.

**Pretraining Strategies**: Using unsupervised or self-supervised learning on large datasets before task-specific training. *Why needed*: Leverages abundant unlabeled protein data to improve downstream task performance. *Quick check*: Pretraining should improve performance on target task with less labeled data.

**Protein Augmentation**: Techniques to artificially expand protein sequence datasets. *Why needed*: Addresses data scarcity in specialized protein families or binding scenarios. *Quick check*: Augmentation should improve model robustness without introducing artifacts.

## Architecture Onboarding

Component map: Protein sequences → Subsequence Reordering → Embedding Layer → Transformer Encoder → Attention Mechanism → Prediction Head

Critical path: The subsequence reordering pretraining stage is essential, as it directly shapes the learned protein representations that drive downstream performance. The embedding and transformer layers form the core feature extraction pipeline.

Design tradeoffs: The method trades increased pretraining computational cost for improved zero-shot generalization. Length-variable augmentation adds training complexity but improves performance on small datasets.

Failure signatures: Poor performance on highly dissimilar protein families suggests limitations in the learned representations' generalizability. Failure to improve on large, diverse datasets may indicate diminishing returns from pretraining.

First experiments:
1. Compare baseline CPI performance with and without PSRP-CPI pretraining on standard datasets
2. Evaluate zero-shot performance on protein families structurally dissimilar to training data
3. Test sensitivity to subsequence length and reordering patterns during pretraining

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on benchmark datasets rather than real-world drug discovery scenarios
- Reliance on specific protein subsequence patterns may limit generalization to all protein families
- Computational overhead from pretraining and reordering operations may impact practical deployment

## Confidence
High confidence: Demonstrated improvements in zero-shot CPI prediction performance are well-supported by experimental results across multiple benchmark datasets.

Medium confidence: Generalizability to entirely new protein families and scalability to industrial applications requires further validation.

Low confidence: Claims of superiority over all existing pretraining approaches are limited by specific experimental conditions.

## Next Checks
1. Evaluate PSRP-CPI on real-world drug discovery datasets with known clinical outcomes to assess practical applicability
2. Conduct ablation studies isolating subsequence reordering contribution to quantify pretraining impact
3. Test performance on protein families with limited structural homology to determine representation generalizability