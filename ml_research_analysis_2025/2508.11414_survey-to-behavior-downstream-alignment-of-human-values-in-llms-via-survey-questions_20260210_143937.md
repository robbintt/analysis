---
ver: rpa2
title: 'Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey
  Questions'
arxiv_id: '2508.11414'
source_url: https://arxiv.org/abs/2508.11414
tags:
- value
- have
- values
- moral
- survey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates whether LLMs\u2019 internal value preferences\
  \ can be manipulated by fine-tuning them on value survey questions. The approach\
  \ uses scalar ratings of value-related descriptions as the sole tuning signal, without\
  \ requiring curated positive/negative examples or complex preference datasets."
---

# Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions

## Quick Facts
- arXiv ID: 2508.11414
- Source URL: https://arxiv.org/abs/2508.11414
- Reference count: 39
- This work demonstrates that fine-tuning LLMs on scalar value survey ratings can manipulate internal value preferences and generalize to downstream moral judgment tasks.

## Executive Summary
This paper investigates whether LLMs' internal value preferences can be manipulated through fine-tuning on value survey questions. The approach uses scalar ratings of value-related descriptions as the sole tuning signal, without requiring curated positive/negative examples or complex preference datasets. Experiments show that this method consistently reduces ratings for targeted values in the value survey (average drop 1.87-2.24 points) and generalizes to out-of-domain moral judgment tasks. On the AITA dataset, models showed directional shifts in moral judgments for values like Benevolence_dependability and Universalism_concern, with Qwen3 8B achieving the highest average probability gain (11.4%). The method also influenced behavior in text-based adventure games, reducing power-seeking and disutility behaviors when finetuned on appropriate value profiles. The results demonstrate that simple fine-tuning on a small, scientifically validated psychometric dataset can effectively align LLMs to human values in a way that generalizes to real-world scenarios.

## Method Summary
The method uses supervised fine-tuning with LoRA to adjust model weights to produce lower scalar ratings for targeted value descriptions. Value descriptions from the Schwartz Value Theory taxonomy are converted to 16K training samples via paraphrased templates with scalar ratings (1-6). For fine-tuning, target value ratings are set to 1 while others remain at baseline. The approach is evaluated both in-domain (held-out value descriptions) and out-of-domain (AITA moral judgment tasks and MACHIAVELLI text-based games). The key innovation is using simple scalar ratings from validated psychometric instruments as the sole alignment signal, avoiding the need for complex preference datasets or hand-crafted examples.

## Key Results
- Fine-tuning consistently reduced ratings for targeted values in held-out survey questions (average drop 1.87-2.24 points across three models)
- Behavioral shifts generalized to AITA moral judgment tasks with directional probability gains for values like Benevolence_dependability (7.5-11.4%) and Universalism_concern (7.5-9.6%)
- In MACHIAVELLI text games, value profiles reduced power-seeking (20.5% reduction for Qwen3 8B) and disutility behaviors (47.4% reduction for Qwen3 8B)
- Model responsiveness varied by baseline tendencies, with Qwen3 8B showing largest effects due to higher initial power-seeking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on scalar survey ratings modifies internal value representations that generalize to unseen behavioral contexts
- Mechanism: Supervised fine-tuning with LoRA adjusts model weights to produce lower scalar ratings for targeted value descriptions. This creates a consistent bias in the model's value representation that persists across different prompt formulations and transfers to moral judgment tasks where those values are implicated.
- Core assumption: Value descriptions in surveys activate similar representational circuits as real-world moral scenarios involving those values
- Evidence anchors:
  - [abstract] "We demonstrate that our simple approach can not only change the model's answers to in-domain survey questions, but also produces substantial shifts (value alignment) in implicit downstream task behavior"
  - [Page 5] "the intervention consistently reduces the ratings of the targeted value descriptions on unseen prompts. On average, the target value rating drops by 2.03 points for LLaMA3.1 8B, 2.24 points for Qwen3 8B, and 1.87 points for Falcon3 7B"
  - [corpus] Weak direct corpus support for this specific mechanism; neighbor papers focus on cultural alignment rather than survey-to-behavior transfer
- Break condition: Training descriptions must semantically align with evaluation scenarios; narrow/superficial descriptions fail to generalize (see Conformity_interpersonal failure mode on Page 6)

### Mechanism 2
- Claim: Transfer success depends on semantic coverage overlap between training descriptions and evaluation contexts
- Mechanism: Values with training descriptions that capture core relational dynamics (e.g., Benevolence_dependability emphasizing "keeping promises," "being there for others") align with evaluation scenarios involving those same dynamics, enabling behavioral transfer. Values with narrow training descriptions (surface politeness) fail when evaluation requires broader relational reasoning.
- Core assumption: The model's value representation is distributed across semantically related concepts rather than stored as abstract labels
- Evidence anchors:
  - [Page 5-6] "Benevolence_Dependability exhibited a clear and consistent shift across all three models, largely due to the strong alignment between the fine-tuning data and the evaluation setting"
  - [Page 6] "The value descriptions used during training focused narrowly on surface-level expressions of politeness... However, during labeling, GPT-4o generalized Conformity_interpersonal much more broadly to cover interpersonal relationships and social dynamics"
  - [corpus] No direct corpus evidence on description-evaluation alignment
- Break condition: Misalignment between training description specificity and evaluation scenario complexity causes transfer failure

### Mechanism 3
- Claim: Model responsiveness to value manipulation varies with baseline behavioral tendencies
- Mechanism: Models with higher baseline power-seeking/disutility (Qwen3 8B) have larger behavioral "headroom" for reduction, making value-based interventions more effective. Models with already-low baseline behaviors (Falcon3 7B) show minimal changes as they operate near floor levels on target metrics.
- Core assumption: Behavioral metrics have bounded ranges, and intervention effects scale with distance from bounds
- Evidence anchors:
  - [Page 7] "Qwen3 8B baseline promotes substantially more power-seeking, moral violations, and disutility than the other models... the power minimization profile substantially reduces power-seeking behaviors by 20.5% and disutility by 47.4%"
  - [Page 7] "Falcon3 7B shows the smallest responsiveness to value finetuning, with negligible or insignificant behavioral changes across all tested profiles"
  - [corpus] No corpus evidence on baseline-effect size relationships
- Break condition: Models near behavioral bounds (floor or ceiling) show diminished intervention effects

## Foundational Learning

- Concept: **Schwartz Value Theory and value taxonomies**
  - Why needed here: The entire method builds on mapping human values to a structured taxonomy (20 values with sub-values) and using validated psychometric instruments. Understanding that values are organized in compatible/conflicting circular structures helps predict intervention side effects.
  - Quick check question: Can you explain why adjusting "Power_dominance" might conflict with "Universalism_concern" based on Schwartz's circular model?

- Concept: **Low-Rank Adaptation (LoRA) mechanics**
  - Why needed here: All experiments use LoRA fine-tuning. Understanding that LoRA adds trainable low-rank decomposition matrices rather than modifying full weights explains why the approach is computationally lightweight and preserves base capabilities.
  - Quick check question: If LoRA rank=128 and α=512 are used, what effective learning rate scaling does this imply, and why might early stopping (patience=2) matter for preserving non-target values?

- Concept: **Logit-based evaluation for constrained outputs**
  - Why needed here: AITA evaluation extracts normalized probabilities from logits for three token options (NTA, YTA, Neutral) rather than using open-ended generation. This provides cleaner measurement of directional probability shifts.
  - Quick check question: Why is logit extraction preferable to generation for measuring small probability changes in moral judgment tasks?

## Architecture Onboarding

- Component map:
  - Value Survey Dataset -> Fine-tuning Layer -> Baseline Model -> In-Domain Eval -> Out-of-Domain Eval 1 -> Out-of-Domain Eval 2

- Critical path:
  1. Establish baseline ratings: Generate all description-template combinations, collect scalar outputs via majority vote
  2. Construct intervention dataset: Set target value ratings to 1, keep others at baseline
  3. Fine-tune with LoRA (10 epochs, early stopping patience=2, lr=1e-4)
  4. Evaluate in-domain generalization (unseen descriptions) and out-of-domain transfer (AITA, MACHIAVELLI)

- Design tradeoffs:
  - Single-value vs. multi-value intervention: Single-value enables cleaner causal attribution; multi-value profiles better approximate real alignment goals but introduce interaction effects
  - Template diversity vs. description coverage: 16K samples from templates but only 594 unique descriptions—template variation may not capture scenario diversity needed for robust transfer
  - Down-rating vs. up-rating: Paper only tests reducing value commitment; up-rating may have non-symmetric effects

- Failure signatures:
  - Low target drop + high variance: Model overfitting to template patterns rather than learning value representation (monitor via Table 1 metrics)
  - High target drop + negative probability gain: Semantic mismatch between training descriptions and evaluation scenarios (see Conformity_interpersonal, Security_personal in Falcon3)
  - Inconsistent cross-model results: Indicates value-specific susceptibility varies by pre-training (compare Qwen3 responsiveness vs. Falcon3 resistance)

- First 3 experiments:
  1. Baseline establishment run: For your chosen model, generate baseline ratings for all 594 descriptions using all templates, verify inter-template consistency before any fine-tuning
  2. Single-value pilot: Select Benevolence_dependability (high transfer success in paper), fine-tune with rating=1, evaluate on held-out descriptions AND 50 manually reviewed AITA posts to validate directional shift
  3. Failure mode replication: Attempt Conformity_interpersonal manipulation to confirm that narrow training descriptions produce poor transfer, then iterate by expanding description coverage to test if transfer improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does scaling the volume and diversity of value descriptions beyond the limited set used in this study improve the robustness of the alignment signal?
- Basis in paper: [explicit] The authors state in the limitations that the small coverage of value descriptions "is unlikely to capture the full range of real-world scenarios associated with each value, which may reduce the robustness of the tuning signal."
- Why unresolved: The current method relies on a sparse set of descriptions (seven per sub-value), potentially limiting the model's ability to generalize the value preference to complex, novel situations.
- What evidence would resolve it: Experiments utilizing significantly larger and more diverse datasets of value descriptions, compared against the current baseline for stability and variance on held-out tasks.

### Open Question 2
- Question: Do these value alignment findings generalize to populations and cultural contexts significantly different from the Western, liberal demographics of the Reddit AITA dataset?
- Basis in paper: [explicit] The authors note that their data "may contain a sample bias" and explicitly ask, "It also remains to be seen whether our findings generalize beyond the Reddit corpus... Experiments with significantly different underlying populations may lead to different results."
- Why unresolved: The current evaluation relies on a dataset biased toward a specific demographic, leaving the cross-cultural stability of the intervention untested.
- What evidence would resolve it: Evaluating models fine-tuned with this method on moral judgment datasets derived from non-Western or culturally distinct populations.

### Open Question 3
- Question: How can the semantic gap between training definitions and evaluation contexts be bridged to ensure consistent behavioral shifts for ambiguous values?
- Basis in paper: [inferred] The authors observed that for Conformity_interpersonal, the training data focused on "surface-level expressions of politeness," while evaluation required "deeper moral judgments about navigating complex relationships," leading to a failure to generalize.
- Why unresolved: The paper identifies this mismatch as a cause of failure but does not determine if refining the semantic scope of training data can resolve the transfer gap.
- What evidence would resolve it: Ablation studies where training descriptions are broadened to match the complexity of the evaluation scenarios, specifically measuring improvements in probability gain for previously resistant values.

## Limitations
- Semantic alignment between training descriptions and evaluation contexts is critical but not systematically predictable
- Evaluation relies on GPT-4o annotations which may not perfectly align with ground truth moral judgments
- Only demonstrates value reduction (rating=1), not enhancement, leaving bidirectional capability untested
- Limited model and value diversity in experiments restricts generalizability claims

## Confidence
- **High Confidence**: The claim that LoRA fine-tuning on scalar survey ratings can reduce targeted value ratings in held-out survey questions
- **Medium Confidence**: The claim that these value manipulations generalize to downstream moral judgment tasks
- **Low Confidence**: The claim that this method provides a scalable approach to LLM value alignment

## Next Checks
1. **Description-Evaluation Alignment Study**: For each value that failed to transfer (like Conformity_interpersonal), conduct a qualitative analysis comparing training descriptions against evaluation scenarios. Document specific semantic gaps and test whether expanding training descriptions to cover the evaluation context improves transfer.

2. **Bidirectional Value Manipulation Test**: Replicate the entire experimental pipeline for value enhancement (rating=6 instead of 1) on Benevolence_dependability and Power_dominance. Compare whether the mechanism works symmetrically for increasing versus decreasing value commitment, and measure any unintended side effects on other values.

3. **Cross-Dataset Generalization Validation**: Apply the most successful value manipulation (Benevolence_dependability reduction) to a held-out moral reasoning dataset not mentioned in the paper (e.g., ETHICS or Moral Stories). Measure whether the behavioral shifts observed in AITA and MACHIAVELLI replicate in a third, independently constructed evaluation context.