---
ver: rpa2
title: The Impact of Off-Policy Training Data on Probe Generalisation
arxiv_id: '2511.17408'
source_url: https://arxiv.org/abs/2511.17408
tags:
- on-policy
- probes
- behaviour
- domain
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers evaluated how different synthetic data generation strategies
  affect probe generalisation across eight LLM behaviours. Testing linear and attention
  probes on multiple models, they found that response strategy significantly impacts
  performance, with effects varying by behaviour.
---

# The Impact of Off-Policy Training Data on Probe Generalisation

## Quick Facts
- arXiv ID: 2511.17408
- Source URL: https://arxiv.org/abs/2511.17408
- Reference count: 40
- Researchers evaluated how different synthetic data generation strategies affect probe generalisation across eight LLM behaviours

## Executive Summary
This paper systematically evaluates how off-policy training data affects probe generalization across eight LLM behaviors. The authors find that response strategy significantly impacts probe performance, with text-ambiguous behaviors (those defined by response intent rather than surface properties) showing the largest generalization failures. They discover that successful generalization to incentivized data strongly predicts generalization to natural data, enabling failure prediction without on-policy test data. Additionally, they find that domain shifts cause greater performance degradation than response-strategy shifts, suggesting that same-domain off-policy data can yield more reliable probes than different-domain on-policy data when natural examples are unavailable.

## Method Summary
The authors evaluated probe generalization using four response strategies (on-policy natural, incentivized, prompted, off-policy) across eight LLM behaviors in two domains each. They trained linear and attention probes on activation data from Llama-3.2-3B-Instruct, Ministral-8B, and Gemma-27B models. Probes were trained on either same-domain or different-domain training sets, then evaluated on on-policy natural test data. The authors also tested whether incentivized test data could serve as a proxy for natural data when unavailable, and analyzed whether mixing multiple training domains could mitigate distribution shift effects.

## Key Results
- Response strategy significantly impacts probe performance, with text-ambiguous behaviors showing largest generalization failures
- Successful generalization to incentivized data strongly predicts generalization to natural data (R=0.793)
- Domain shifts cause greater performance degradation than response-strategy shifts
- Text-ambiguous behaviors (deception, sandbagging, sycophancy) show systematically worse generalization than text-evident behaviors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probe generalization success depends on whether target behaviors are "text-evident" (surface-observable properties) versus "text-ambiguous" (require inferring underlying intent)
- Core assumption: The distinction between text-evident and text-ambiguous behaviors explains variance in generalization
- Evidence anchors:
  - [abstract]: "largest generalisation failures occurred for behaviours defined by response 'intent' rather than surface-level text properties"
  - [section 6]: "The 'easy four' (refusal, lists, metaphors, science) are text-evident... The 'hard four' (sycophancy, deferral-to-authority, deception, sandbagging) are text-ambiguous"
  - [corpus]: Related work on deception probes (Goldowsky-Dill et al.) also finds domain sensitivity

### Mechanism 2
- Claim: Domain shift degrades probe performance more than response-strategy shift (off-policy vs on-policy)
- Core assumption: Domain-specific features dominate over response-generation features in learned representations
- Evidence anchors:
  - [abstract]: "shifts in training data domain cause greater performance degradation than shifts in response strategy"
  - [section 5.1, Figure 3]: Shows domain shift coefficient of -0.272 (p<0.001) vs. smaller, inconsistent response strategy effects

### Mechanism 3
- Claim: Generalization to incentivized data predicts generalization to natural data, enabling failure prediction without natural test sets
- Core assumption: The correlation (R=0.793) holds across behaviors and models
- Evidence anchors:
  - [abstract]: "successful generalisation to incentivised data strongly predicts generalisation to natural data"
  - [section 5.2, Figure 4]: Pearson coefficient of 0.793 across Llama, Ministral, and Gemma models

## Foundational Learning

- Concept: **On-policy vs. off-policy data generation**
  - Why needed here: The paper's core comparison requires understanding whether training data comes from the target model's native distribution (on-policy) or an external source (off-policy)
  - Quick check question: Given a prompt x, if you sample response y from model M vs. feeding externally-written y through M's activations, which is on-policy?

- Concept: **Linear probing of activations**
  - Why needed here: The paper tests whether simple linear classifiers on hidden states can detect behaviors—and when they fail
  - Quick check question: What does it mean if a probe achieves 0.95 AUROC on training-distribution data but 0.55 on natural data?

- Concept: **Distribution shift types**
  - Why needed here: The paper distinguishes between response-strategy shift (prompted/incentivized/off-policy) and domain shift (task type)—different shifts have different magnitudes of impact
  - Quick check question: If you train on math questions but deploy on creative writing, what type of shift is this?

## Architecture Onboarding

- Component map: Data generation pipeline -> Activation extraction -> Probe training -> Evaluation
- Critical path: 1) Select target behavior and dataset domain; 2) Generate responses using each strategy; 3) Extract activations from target layer; 4) Train probes with balanced samples; 5) Evaluate on natural test data
- Design tradeoffs:
  - Same-domain off-policy vs. different-domain on-policy: Paper recommends former when natural data unavailable
  - Linear vs. attention probes: Attention probes achieve higher absolute scores but similar generalization patterns
  - Training data scale: Diminishing returns past ~1000 samples
- Failure signatures:
  - High training AUROC + low natural test AUROC → distribution shift problem
  - Low incentivized test AUROC with off-policy training → predicted natural generalization failure
  - Text-ambiguous behavior + off-policy training → expect degradation
- First 3 experiments:
  1. Replicate the correlation analysis (Figure 4) on a new behavior not in the original eight
  2. Test mixed-domain training (2+ domains) to see if domain-shift penalty reduces
  3. Compare layer selection: The paper uses layer 12 for Llama-3B across most behaviors

## Open Questions the Paper Calls Out

1. **Do the observed generalization failures persist in frontier-scale models (100B–1T parameters), particularly those capable of reasoning in private languages?**
   - Basis: The authors explicitly state they mostly used relatively small models and cite evidence that SOTA LLMs might reason in their own language
   - Why unresolved: The study was constrained to models like Llama-3.2-3B and Gemma-27B
   - What evidence would resolve it: Replicating experiments on 100B+ parameter models and analyzing the correlation between reasoning coherence and probe generalization

2. **Are non-linear "deep" probes or black-box monitors more robust to the response strategy shifts that degrade linear and attention probes?**
   - Basis: The authors note they only tested linear and attention probes, suggesting future work should evaluate "deep probes" with many layers and non-linearities
   - Why unresolved: The complexity of the probe architecture was isolated to two types
   - What evidence would resolve it: Training multi-layer perceptron (MLP) probes or black-box classifiers on the same off-policy datasets

3. **Can specific training interventions, such as invariant risk minimization or domain adaptation, mitigate the performance degradation caused by domain shifts?**
   - Basis: The authors identify the need for methods to handle distribution shifts and explicitly suggest "techniques from domain adaptation or invariant risk minimization"
   - Why unresolved: The paper establishes the problem but does not implement or test algorithmic solutions
   - What evidence would resolve it: Applying domain-adversarial training or IRM objectives during probe training

## Limitations
- Experiments mostly used relatively small models (Llama-3.2-3B, Ministral-8B, Gemma-27B)
- Only tested linear and attention probes, not deeper non-linear architectures
- Did not implement or test algorithmic solutions to mitigate distribution shift

## Confidence

Our confidence in the core empirical findings is **High** for text-evident behaviors and **Medium** for text-ambiguous behaviors. The correlation between incentivized and natural generalization (R=0.793) appears robust across models but lacks direct mechanistic validation. Domain shift appears consistently larger than response-strategy shift in our experiments.

## Next Checks

1. Test the incentivised-natural correlation on behaviors outside the original eight to verify generalizability of the proxy prediction mechanism.

2. Evaluate probe performance using mixed-domain training (3+ domains) to determine if domain shift penalties are due to limited diversity rather than fundamental domain specificity.

3. Compare probe performance across different model architectures (e.g., decoder-only vs. decoder-encoder) to assess whether the text-ambiguity mechanism generalizes beyond Llama-3.2-3B-Instruct.