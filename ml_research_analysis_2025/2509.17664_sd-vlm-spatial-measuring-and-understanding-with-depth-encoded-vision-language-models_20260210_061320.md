---
ver: rpa2
title: 'SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language
  Models'
arxiv_id: '2509.17664'
source_url: https://arxiv.org/abs/2509.17664
tags:
- depth
- spatial
- height
- image
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SD-VLM improves 3D spatial understanding in vision-language models
  by introducing precise spatial annotations and depth positional encoding. The Massive
  Spatial Measuring and Understanding (MSMU) dataset provides 700K question-answer
  pairs with 2.5M numerical annotations derived from 3D scenes, enabling accurate
  spatial reasoning.
---

# SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models

## Quick Facts
- **arXiv ID:** 2509.17664
- **Source URL:** https://arxiv.org/abs/2509.17664
- **Reference count:** 40
- **Primary result:** SD-VLM achieves 56.31% success rate on MSMU-Bench, outperforming GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively.

## Executive Summary
SD-VLM introduces a novel approach to enhance 3D spatial understanding in vision-language models by integrating depth information through depth positional encoding (DPE) and training on a specialized dataset with precise spatial annotations. The Massive Spatial Measuring and Understanding (MSMU) dataset contains 700K question-answer pairs with 2.5M numerical annotations derived from 3D scenes, enabling accurate spatial reasoning tasks like object size measurement and distance calculation. By adding depth-based sinusoidal embeddings to image features and training on MSMU, SD-VLM achieves state-of-the-art performance on spatial reasoning benchmarks, demonstrating strong generalization across indoor and outdoor scenes.

## Method Summary
SD-VLM enhances vision-language models with 3D spatial understanding through two key innovations: the MSMU dataset and Depth Positional Encoding (DPE). The MSMU dataset provides 700K question-answer pairs with 2.5M precise numerical annotations for spatial reasoning tasks, derived from 3D scenes in ScanNet and ScanNet++. DPE integrates depth maps by pooling them to match visual feature dimensions, applying sinusoidal encoding, and adding the resulting embeddings directly to image features. The model is trained via LoRA fine-tuning on MSMU for one epoch using 8 V100 GPUs, with depth maps normalized to a range of [0, 100] to ensure stability.

## Key Results
- SD-VLM achieves 56.31% success rate on MSMU-Bench, outperforming GPT-4o by 26.91% and Intern-VL3-78B by 25.56%
- The model generalizes well to other spatial reasoning tasks beyond the MSMU dataset
- Performance drops slightly (55.35%) when evaluated on estimated depth rather than ground truth depth
- Training on MSMU without mixing general instruction data causes catastrophic forgetting of general VQA capability

## Why This Works (Mechanism)
SD-VLM improves 3D spatial reasoning by providing models with explicit depth information through DPE and training on datasets with precise spatial annotations. The additive depth embeddings create a bridge between 2D visual perception and 3D spatial understanding, allowing the model to reason about absolute distances and object sizes rather than just relative positions. The MSMU dataset's quantitative annotations train the model to make precise measurements rather than relying on statistical size priors, while the depth information enables proper geometric reasoning about the 3D scene structure.

## Foundational Learning
- **Depth positional encoding:** Adding depth-based sinusoidal embeddings to image features to integrate 3D information into 2D vision models. Needed for bridging 2D perception and 3D understanding; check by verifying feature dimension alignment.
- **Monocular depth estimation:** Using Depth-Anything-V2 to generate depth maps from single images when ground truth depth is unavailable. Needed for practical deployment; check by comparing estimated vs. ground truth depth quality.
- **LoRA fine-tuning:** Low-rank adaptation technique for efficient model modification without full fine-tuning. Needed for computational efficiency; check by monitoring training stability and performance.
- **Sinusoidal position encoding:** Using periodic functions to encode positional information in a way that generalizes to unseen positions. Needed for depth embedding; check by visualizing depth feature patterns.
- **Adaptive pooling:** Resizing depth maps to match visual feature dimensions through mean pooling. Needed for feature alignment; check by verifying matching grid resolutions.
- **Catastrophic forgetting:** The tendency of models to lose previously learned capabilities when trained on new tasks. Needed to understand training requirements; check by evaluating on general VQA benchmarks during training.

## Architecture Onboarding

**Component Map:** Image features -> Depth positional encoding -> Combined features -> LLM projector -> LLM

**Critical Path:** Depth map pooling → Sinusoidal encoding → Feature addition → LoRA fine-tuning → MSMU training

**Design Tradeoffs:** Additive DPE vs. cross-attention mechanisms (simpler but potentially less expressive); training on estimated vs. ground truth depth (practicality vs. performance); LoRA vs. full fine-tuning (efficiency vs. capacity).

**Failure Signatures:** Dimension mismatch between pooled depth and image features; depth scale instability causing training divergence; catastrophic forgetting of general VQA capabilities.

**First Experiments:**
1. Verify DPE implementation by checking that depth embeddings have correct dimensions and are properly added to image features
2. Test depth normalization sensitivity by training with different α values (50, 100, 200) and comparing performance
3. Evaluate catastrophic forgetting by testing on general VQA benchmarks before and after MSMU training

## Open Questions the Paper Calls Out
- Does DPE scale effectively with larger VLMs (70B+), or is the additive encoding bottlenecked compared to cross-attention methods?
- To what extent does MSMU enable learning of implicit camera intrinsics versus memorizing statistical size priors?
- How robust is DPE to monocular depth estimation failures on transparent or reflective surfaces common in domestic scenes?

## Limitations
- Performance advantage diminishes when evaluated on estimated depth (55.35%) rather than ground truth depth (56.31%)
- Lack of explicit LoRA hyperparameters (rank, dropout) in implementation details
- Ambiguity in depth pooling resolution for LLaVA-1.5's dynamic resolution capability

## Confidence
- **High confidence:** Core DPE mechanism and MSMU dataset construction are well-specified and reproducible
- **Medium confidence:** LoRA fine-tuning procedure and training hyperparameters are mostly specified
- **Medium confidence:** Quantitative improvements over baselines are valid for MSMU-Bench dataset
- **Low confidence:** Qualitative evaluation using GPT-4 scoring lacks precision and may introduce variability

## Next Checks
1. Reproduce DPE integration by implementing the depth positional encoding module with exact pooling dimensions matching dynamic ViT patch grids
2. Evaluate depth normalization sensitivity by testing model performance across different depth normalization ranges (α values)
3. Test generalization by evaluating the trained model on LRR-Bench and other spatial reasoning benchmarks beyond MSMU