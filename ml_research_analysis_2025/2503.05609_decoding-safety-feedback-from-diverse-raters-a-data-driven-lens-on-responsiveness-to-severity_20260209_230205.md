---
ver: rpa2
title: 'Decoding Safety Feedback from Diverse Raters: A Data-driven Lens on Responsiveness
  to Severity'
arxiv_id: '2503.05609'
source_url: https://arxiv.org/abs/2503.05609
tags:
- raters
- rater
- severity
- metrics
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel data-driven approach for analyzing
  ordinal safety ratings in pluralistic settings, addressing the challenge of interpreting
  nuanced differences in safety feedback from diverse populations. The authors define
  non-parametric responsiveness metrics that quantify how raters convey distinctions
  and variations in the severity of safety violations.
---

# Decoding Safety Feedback from Diverse Raters: A Data-driven Lens on Responsiveness to Severity

## Quick Facts
- arXiv ID: 2503.05609
- Source URL: https://arxiv.org/abs/2503.05609
- Reference count: 29
- This paper introduces a novel data-driven approach for analyzing ordinal safety ratings in pluralistic settings, addressing the challenge of interpreting nuanced differences in safety feedback from diverse populations.

## Executive Summary
This paper introduces a novel data-driven approach for analyzing ordinal safety ratings in pluralistic settings, addressing the challenge of interpreting nuanced differences in safety feedback from diverse populations. The authors define non-parametric responsiveness metrics that quantify how raters convey distinctions and variations in the severity of safety violations. They apply these metrics across two publicly available datasets of pluralistic safety feedback, demonstrating their utility in extracting nuanced insights crucial for aligning AI systems reliably in multi-cultural contexts.

## Method Summary
The method defines two non-parametric metrics—Monotonic Precision Area (MPA) and Weighted Recall Area (WRA)—that decompose responsiveness to severity into stochastic ordering and discrimination properties. MPA measures whether higher Likert scores correspond to higher probabilities of true severity, while WRA measures the discriminative power of each score. Both metrics are computed from precision and recall at each ordinal score threshold, combined via harmonic mean. The approach applies these metrics to analyze safety feedback from diverse demographic groups using both guideline-based references (trained raters) and crowd-based references (majority consensus), enabling nuanced comparison of how different populations perceive safety violations.

## Key Results
- Demonstrated that traditional metrics (Kendall's τ, AUROC) fail to distinguish between different types of rater biases (conservative scoring vs. downward-shift) that MPA and WRA successfully differentiate
- Found significant variations in responsiveness across demographic groups, with certain groups showing systematically different severity perceptions compared to majority populations
- Showed that MPA and WRA provide actionable insights for rater selection and dataset curation in pluralistic alignment settings

## Why This Works (Mechanism)

### Mechanism 1: Severity-to-Score Mapping via Rater-Specific Perception Functions
- Claim: Each rater's ordinal scores reflect an underlying perception function applied to true item severity.
- Mechanism: True severity V_i is transformed by rater-specific function f_j to produce perceived severity V^j_i = f_j(V_i), which then maps to observable scores S^j_i via response function M^j_k. This separation allows analyzing how different raters use the same ordinal scale differently.
- Core assumption: True severity V is unidimensional and monotonically influences every rater's judgment (though the transformation f_j is rater-specific).
- Evidence anchors:
  - [abstract]: "We define non-parametric responsiveness metrics that quantify how raters convey broader distinctions and granular variations in the severity of safety violations."
  - [section 3.1]: "Let V_i be the true underlying severity of item i. Let V^j_i be the true severity of item i as perceived by rater j. Then, V^j_i = f_j(V_i) where f_j is the rater-specific perception function."
  - [corpus]: Related work "Whose View of Safety?" confirms diverse human experiences lead to misaligned systems, supporting the need for rater-specific perception modeling.
- Break condition: If severity is fundamentally multidimensional and cannot be projected to a unidimensional latent trait, the monotonicity assumptions fail.

### Mechanism 2: Two-Property Responsiveness Decomposition via MPA and WRA
- Claim: Responsiveness to severity comprises two distinct properties—stochastic ordering and discrimination—captured by separate metrics.
- Mechanism: **Monotonic Precision Area (MPA)** quantifies whether P(U=1|S=s1) ≥ P(U=1|S=s2) when s1 > s2, computed by penalizing violations of monotonicity at each score. **Weighted Recall Area (WRA)** quantifies discrimination via P(S=s|U=1) × P(S<s|U=0), representing concordance probability at each score. The harmonic mean combines both.
- Core assumption: An observable binary reference U can serve as a valid proxy for unobservable true severity V, either from trained rater guidelines (V_g) or crowd consensus (V_c).
- Evidence anchors:
  - [abstract]: "They define non-parametric responsiveness metrics that quantify how raters convey broader distinctions and granular variations."
  - [section 4.2]: Equations 3 and 4 formalize Y_so(s) and Y_d(s); section notes "we take their harmonic mean to combine them into one metric."
  - [corpus]: Related work on Human Label Variation (HLV) reframes rating variation as signal rather than noise, supporting this decomposition.
- Break condition: If the binary reference U is highly noisy or systematically biased against certain groups, both metrics will reflect reference artifacts rather than true responsiveness.

### Mechanism 3: Metric-Guided Rater Selection and Optimization Dataset Curation
- Claim: Responsiveness metrics enable informed rater selection and weighted data curation for alignment optimization.
- Mechanism: For guideline-driven alignment, select raters with high MPA+WRA from each demographic group, then curate items at varying severity levels. For direct crowd alignment, identify groups with low metrics to either: (a) inspect high-vs-low severity items pairwise, or (b) identify score ranges with monotonicity violations and weight items appropriately in reward model training.
- Core assumption: Responsiveness patterns observed on evaluation data generalize to new items and persist over time.
- Evidence anchors:
  - [abstract]: "We show that our approach can inform rater selection and feedback interpretation by capturing nuanced viewpoints across different demographic groups."
  - [section 6]: "select raters who exhibit the highest responsiveness to severity from each and every group... curate optimization datasets with items at varying levels of severity."
  - [corpus]: "Towards Low-Resource Alignment" emphasizes aligning to diverse perspectives, but corpus lacks direct validation of metric-guided selection improving downstream outcomes.
- Break condition: If rater behavior shifts across domains or over time, selection based on historical responsiveness may not generalize.

## Foundational Learning

- Concept: **First-order stochastic dominance**
  - Why needed here: The MPA metric assumes that higher scores should stochastically dominate lower scores in terms of true severity probability.
  - Quick check question: Given two raters where Rater A's score distribution dominates Rater B's for all severity thresholds, which has higher MPA?

- Concept: **Precision and Recall at ordinal score thresholds**
  - Why needed here: MPA and WRA are computed directly from precision(s) and recall(s) at each Likert score, not standard binary classification thresholds.
  - Quick check question: Why does the paper compute precision/recall exactly at S=s rather than S≥s?

- Concept: **Response biases in Likert scales (extreme response, central tendency)**
  - Why needed here: These biases (extreme responses, central tendency, polarization) motivate the need for responsiveness metrics that disentangle scale usage from actual severity response.
  - Quick check question: A rater uses only scores 0 and 4 on a 0-4 scale. Would this affect MPA, WRA, or both?

## Architecture Onboarding

- Component map:
  - **Input layer**: Crowd rater Likert scores S ∈ {0,...,K}, binary reference U (from trained raters or crowd binarization)
  - **Per-score computation**: For each score k, compute count, P(U=1|S=k), P(S=k|U=1), P(S<k|U=0)
  - **MPA module**: Cumulative penalized precision differences via max-accumulated running precision
  - **WRA module**: Weighted recall product at each score, integrated via AUC
  - **Aggregation**: Harmonic mean of MPA and WRA; optionally macro-averaged across binarization thresholds
  - **Output**: Per-rater or per-group responsiveness scores (MPA, WRA, HM)

- Critical path:
  1. Establish binary reference U (trained raters → guideline-based; crowd exclusion → crowd-based with boundaries {1,2,...,K})
  2. For each rater/group, compute per-score precision and recall components
  3. Compute MPA via cumulative penalized precision differences (Equation 3)
  4. Compute WRA via weighted recall product integration (Equation 4)
  5. Combine via harmonic mean; bootstrap for confidence intervals

- Design tradeoffs:
  - **Guideline-based vs. crowd-based reference**: Guideline-based (V_g) captures expert consensus but may miss marginalized group perspectives; crowd-based (V_c) captures collective judgment but may systematically differ from minority viewpoints (as observed with Black trisections in Figure 2)
  - **Per-score vs. cumulative thresholds**: Computing precision/recall exactly at S=s (not S≥s) preserves granularity but may be noisier for sparse score usage
  - **Macro vs. micro averaging across boundaries**: Macro-averaging across binarization thresholds increases robustness but may obscure threshold-specific patterns

- Failure signatures:
  - **MPA near 0 with moderate WRA**: Rater uses scale but not monotonically (e.g., extreme response bias, reversed ordering)
  - **WRA near 0 with moderate MPA**: Rater orders correctly but fails to discriminate (e.g., central tendency bias clustering all items at middle scores)
  - **Both metrics near 0**: Rater's V^j is fundamentally misaligned with reference V, or reference U is unreliable
  - **High variance in HM across bootstrap trials**: Insufficient data per rater/group

- First 3 experiments:
  1. **Baseline calibration on synthetic data**: Generate simulated raters with known scoring patterns (normal, downward-shift, conservative per Appendix D). Verify MPA drops for conservative scoring while WRA drops for downward-shift, and traditional metrics (Kendall's τ, AUROC) fail to distinguish these cases.
  2. **Demographic group comparison with both reference types**: Compute MPA/WRA for all trisectional groups using guideline-based reference (trained raters) and crowd-based reference. Identify groups where rankings differ significantly between reference types, indicating guideline-blind vs. crowd-divergent perceptions.
  3. **Per-violation-type responsiveness decomposition**: For each violation type (bias, sexual, violent), compute MPA/WRA for top-level demographic groups. Cross-reference low-metric groups with qualitative inspection of high-score vs. low-score items to validate that low metrics reflect legitimate pluralistic viewpoints rather than noise.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach critically relies on the quality of binary reference U, which may systematically bias against minority groups when using crowd-based references
- Assumes severity is unidimensional, potentially oversimplifying complex safety violations that span multiple harm dimensions
- Metrics capture only ordinal relationships and cannot detect systematic scaling biases where raters consistently over/underestimate severity magnitudes

## Confidence
- **High confidence** in the mathematical formulation and non-parametric computation of MPA and WRA metrics
- **Medium confidence** in the practical utility for rater selection and dataset curation
- **Low confidence** in the assumption that binary references provide valid ground truth for severity across demographic groups

## Next Checks
1. **Synthetic data stress test**: Generate simulated raters with controlled biases (extreme response, central tendency, reversed ordering) and verify that MPA/WRA correctly identify each bias pattern while traditional metrics (Kendall's τ, AUROC) fail to distinguish them.

2. **Cross-dataset consistency**: Apply the same responsiveness metrics to additional pluralistic safety datasets (e.g., toxicity, bias, privacy violations) to test whether demographic group patterns observed in the DIVE dataset generalize across different safety domains and rating contexts.

3. **Downstream alignment validation**: Train reward models using datasets curated via metric-guided rater selection versus random selection, then evaluate whether the former produces more equitable safety behavior across demographic groups in held-out test cases.