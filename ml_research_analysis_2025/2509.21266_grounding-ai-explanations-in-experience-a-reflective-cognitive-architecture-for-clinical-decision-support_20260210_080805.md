---
ver: rpa2
title: 'Grounding AI Explanations in Experience: A Reflective Cognitive Architecture
  for Clinical Decision Support'
arxiv_id: '2509.21266'
source_url: https://arxiv.org/abs/2509.21266
tags:
- rules
- data
- prediction
- disease
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing explainable disease
  prediction models that balance high accuracy with clinically meaningful explanations.
  It proposes the Reflective Cognitive Architecture (RCA), a novel framework that
  uses Large Language Models (LLMs) to learn from data through iterative rule refinement
  and distribution-aware checks.
---

# Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support

## Quick Facts
- arXiv ID: 2509.21266
- Source URL: https://arxiv.org/abs/2509.21266
- Reference count: 40
- Achieves state-of-the-art predictive performance with up to 40% relative improvement over baselines while generating clinically meaningful explanations

## Executive Summary
This paper addresses the challenge of developing explainable disease prediction models that balance high accuracy with clinically meaningful explanations. The authors propose the Reflective Cognitive Architecture (RCA), a novel framework that uses Large Language Models (LLMs) to learn from data through iterative rule refinement and distribution-aware checks. RCA treats prediction errors as learning opportunities and grounds its reasoning in global statistical evidence. Evaluated on three datasets including a proprietary Catheter-Related Thrombosis (CRT) cohort and 22 baselines, RCA achieved state-of-the-art predictive performance with up to 40% relative improvement over baselines. More importantly, it generated explanations that were clear, logical, evidence-based, and balanced, maintaining efficacy even at scale.

## Method Summary
RCA employs three specialized LLMs working in concert: a Prediction LLM that generates predictions using current rules, a Reflection LLM that updates the rule base from error batches, and a Check LLM that validates rules against global statistics. The system converts structured clinical features into natural language narratives, then iteratively refines rules through error analysis. Rules are validated against dataset statistics to prevent overfitting to local anomalies. The architecture was evaluated on CRT, Diabetes, and Heart Disease datasets using both predictive metrics and human-centric explanation quality measures.

## Key Results
- Achieved state-of-the-art predictive performance with up to 40% relative improvement over baselines
- Generated explanations rated as clear, logical, evidence-based, and balanced by clinical experts
- Maintained efficacy at scale with minimal degradation across noise conditions
- RCA+Qwen2.5-72B outperformed traditional machine learning baselines (Catboost, XGBoost) and LLM-based tools (LLM+Tools, LLM+Code)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating prediction errors as learning opportunities builds sound logical argumentation through iterative rule refinement
- Mechanism: A reflection LLM (M_ref) aggregates misclassified samples into short-term experience batches, then distills them into abstract rules that update a long-term rule base (R_k), converting specific failures into generalizable principles
- Core assumption: Prediction errors contain systematic signal about reasoning deficiencies rather than random noise
- Evidence anchors:
  - [abstract] "iterative rule refinement mechanism that improves its logic from prediction errors"
  - [section 3.2] "This mechanism converts the short-term experience of specific errors into the long-term memory of abstract, generalizable rules"
  - [corpus] Weak direct corpus support; neighbor papers focus on reflective learning (fCrit) and dialogue-based reasoning but not error-driven rule extraction
- Break condition: If error batches contain predominantly noisy labels or contradictory cases, rules may become unstable or overfit to outliers

### Mechanism 2
- Claim: Grounding rules in global statistical distribution improves evidence-based reasoning and robustness to noise
- Mechanism: A checking LLM (M_chk) validates proposed rules against D_train (global distribution summary including means, quantiles, frequencies), removing overly specific rules and flagging outliers
- Core assumption: Global statistics provide a valid "sanity check" that prevents overfitting to local anomalies
- Evidence anchors:
  - [abstract] "distribution-aware rules check mechanism that bases its reasoning in the dataset's global statistics"
  - [section 4.3] Robustness experiments show "minimal degradation across all noise conditions" compared to baselines
  - [corpus] Weak; no neighbor papers explicitly test distribution-grounded validation loops
- Break condition: If training distribution diverges significantly from test distribution, global statistics may mislead rather than ground reasoning

### Mechanism 3
- Claim: Direct narrative engagement with data enables deeper understanding than tool-mediated or code-based analysis
- Mechanism: Structured clinical features are converted to natural language narratives (e.g., "Granulocyte-to-lymphocyte ratio is 4.88, D-dimer is 3.16"), which LLMs process directly without API or code interpreter abstraction layers
- Core assumption: Instance-level textual engagement fosters granular pattern recognition that summarized tool outputs cannot replicate
- Evidence anchors:
  - [section 3.1] "This makes the data directly accessible to the LLM"
  - [section 4.2] LLM+Tools and LLM+Code baselines underperform, "never develop a granular, instance-level 'feel' for the data"
  - [corpus] Indirect support from "Learning Through Dialogue" showing linguistic engagement impacts learning dynamics
- Break condition: If narratives become too verbose or ambiguous, LLM attention may dilute signal; scaling to high-dimensional features (>50) may hit context limits

## Foundational Learning

- Concept: In-context learning with long context windows
  - Why needed here: RCA passes entire rule bases (R_k), distribution summaries (D_train), and error batches (S_error) as context to LLMs; understanding how context length affects performance is critical
  - Quick check question: Can you explain why providing distribution statistics in-context might improve zero-shot reasoning on tabular data?

- Concept: Rule-based expert systems and knowledge representation
  - Why needed here: The rule base R_k functions as explicit symbolic knowledge; understanding trade-offs between expressiveness and interpretability helps diagnose rule quality issues
  - Quick check question: What makes a rule "overly specific" versus "generalizable" in a clinical prediction context?

- Concept: Statistical outliers and robustness in medical data
  - Why needed here: Medical datasets contain noise, missing values, and data entry errors; M_chk explicitly handles outlier detection to prevent rule corruption
  - Quick check question: How would you distinguish a true clinical edge case from a data entry error using only distributional statistics?

## Architecture Onboarding

- Component map: Data Narrative Layer -> M_pred (with R_k, D_train) -> Prediction -> Errors collected -> M_ref generates rules -> M_chk validates rules -> R_{k+1} updated -> repeat for k epochs -> Final R_f used for inference

- Critical path: Data → Narrative → M_pred (with R_k, D_train) → Prediction → Errors collected → M_ref generates rules → M_chk validates rules → R_{k+1} updated → repeat for k epochs → Final R_f used for inference

- Design tradeoffs:
  - Error batch capacity T (set to 25): Larger batches provide more signal but increase token costs and may dilute specific patterns
  - Epoch count (15-25): More epochs allow deeper rule refinement but risk overfitting to training noise
  - Rule base size: Must balance comprehensiveness with context window limits

- Failure signatures:
  - Circular rules: Rules that reference each other without grounding (M_chk should catch)
  - Threshold drift: Learned thresholds diverge from clinical norms (check against medical literature)
  - High variance in explanation scores: Indicates unstable rule base, may need reduced T or early stopping
  - Accuracy plateaus early: May indicate insufficient error diversity or need for larger T

- First 3 experiments:
  1. Ablation with w/o distribution: Run RCA+Qwen2.5 on CRT dataset without D_train provided to M_pred and M_chk; expect MCC drop (per Table 1: CRT MCC 0.17 → -0.07)
  2. Robustness stress test: Inject 10% random missing values into CRT test set; compare RCA vs. best baseline (Catboost) degradation rates
  3. Scalability probe: Run on Cardiovascular Disease dataset (70k samples); measure per-sample latency and rule base growth rate across epochs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the improvement in explanation quality provided by RCA translate to statistically significant improvements in diagnostic accuracy or reduced cognitive load for clinicians in a live, operational workflow?
- Basis in paper: [explicit] The authors acknowledge that their human-centric validation was limited to a "formal human-centric study" with only "3 expert clinicians" and "50 sample explanations" (Section 4.1).
- Why unresolved: The current evaluation is an offline, controlled experiment. It does not demonstrate efficacy in the complex, time-constrained environment of actual clinical practice.
- What evidence would resolve it: A randomized controlled trial (RCT) where clinicians use RCA versus standard tools for real patient cases, measuring diagnostic errors and decision time.

### Open Question 2
- Question: How does the reliance on a static global distribution ($D_{train}$) affect the model's resilience to concept drift or shifts in patient population demographics over time?
- Basis in paper: [inferred] The "Distribution-aware Rules Check" (Section 3.3) grounds reasoning in the fixed $D_{train}$. While the paper tests robustness to noise (Section 4.3), it does not address how the system adapts if the statistical properties of incoming patients diverge from the training set.
- Why unresolved: The mechanism assumes the training distribution remains representative, which is often not true in dynamic hospital environments where protocols or patient profiles change.
- What evidence would resolve it: Experiments evaluating performance when the test data distribution is intentionally skewed or shifted relative to the provided $D_{train}$ context.

### Open Question 3
- Question: Can the Data Narrative mechanism be effectively extended to incorporate unstructured multimodal data, such as medical imaging or free-text clinical notes?
- Basis in paper: [explicit] The method is designed specifically for "structured clinical features" formatted as "tabular data" (Section 3.1).
- Why unresolved: The "Data Narrative" step explicitly converts structured vectors to text; the architecture lacks a defined mechanism for ingesting or reasoning over non-tabular inputs like X-rays or doctor's notes.
- What evidence would resolve it: Modifying the input pipeline to include image or text embeddings and evaluating if the rule refinement logic generalizes to these modalities.

### Open Question 4
- Question: What is the computational and financial cost overhead of the iterative, multi-LLM reflective process compared to traditional machine learning baselines?
- Basis in paper: [inferred] The framework requires multiple LLM calls ($M_{pred}, M_{ref}, M_{chk}$) per iteration (Section 3.2, 3.3), yet the comparison focuses solely on predictive accuracy (Section 4.2) without discussing latency or API costs.
- Why unresolved: While RCA outperforms Catboost in accuracy, the computational resources required to run 15-25 epochs of LLM reflection may render it impractical for low-resource or high-frequency settings.
- What evidence would resolve it: A comparative analysis of inference latency, total token usage, and wall-clock training time against all baselines.

## Limitations

- Core claims about reflective rule refinement lack direct corpus validation
- Private CRT dataset prevents exact replication of key results
- Automated explanation scoring mechanism lacks full prompt specification
- Performance improvement calculation lacks baseline specification
- Computational cost overhead not addressed

## Confidence

- **High confidence**: Predictive accuracy improvements on Diabetes and Heart Disease datasets (public data, clear baselines)
- **Medium confidence**: Mechanism 1 (error-driven rule refinement) - theoretically sound but no direct corpus validation
- **Medium confidence**: Mechanism 2 (distribution-grounded validation) - supported by robustness experiments but no external validation
- **Low confidence**: Explanation quality improvements - dependent on automated scoring system with incomplete specification

## Next Checks

1. **Ablation with Distribution Grounding:** Run RCA+Qwen2.5 on the public CRT dataset (or closest available) with and without D_train context; measure MCC change to verify distribution grounding impact (target: CRT MCC drop from 0.17 to -0.07)

2. **Cross-Dataset Robustness:** Test RCA on 2-3 additional tabular medical datasets with varying noise levels (10-30% missing/inconsistent values); compare degradation rates against Catboost and other strong baselines

3. **Rule Base Analysis:** After RCA training on Heart Disease dataset, manually audit 50 randomly sampled rules for clinical validity, specificity thresholds, and circular references; compare against medical literature standards