---
ver: rpa2
title: 'Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought
  Tokens in LLMs'
arxiv_id: '2511.02690'
source_url: https://arxiv.org/abs/2511.02690
tags:
- training
- cost
- performance
- response
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses training agents under strict trajectory-level
  constraints, such as limited token budgets or safety requirements, where conventional
  constrained RL methods fall short. It proposes a curriculum learning strategy that
  gradually tightens constraints during training, enabling smoother adaptation to
  deployment conditions.
---

# Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs

## Quick Facts
- arXiv ID: 2511.02690
- Source URL: https://arxiv.org/abs/2511.02690
- Reference count: 40
- Key outcome: Training agents under strict trajectory-level constraints via curriculum learning achieves up to 7.5× inference speedup while preserving accuracy

## Executive Summary
This work addresses training agents under strict trajectory-level constraints, such as limited token budgets or safety requirements, where conventional constrained RL methods fall short. It proposes a curriculum learning strategy that gradually tightens constraints during training, enabling smoother adaptation to deployment conditions. The method adaptively adjusts the permissible cost budget based on agent performance, avoiding the need for per-context evaluations. Theoretical analysis in a binary-tree MDP shows sample complexity improvement from O(2^H) to O(H^3). Empirically, the approach outperforms baselines across RL environments and two LLM models on math reasoning tasks, compressing chain-of-thought tokens while preserving accuracy and achieving up to 7.5× inference speedup on consumer hardware.

## Method Summary
The method trains agents under strict trajectory-level constraints by progressively tightening cost budgets during training. For each task, a teacher component maintains a rolling buffer of K rollouts and uses binary search to find the smallest cost budget α_t that satisfies performance constraints (at least β fraction of rollouts succeed). The student component trains a non-contextual policy using standard RL algorithms (REINFORCE/RLOO) with rewards only granted when trajectories satisfy the current cost constraint. The curriculum starts with relaxed constraints and tightens them as the agent improves, avoiding the sparse-reward problem of imposing target constraints from the outset. The approach is validated on RL benchmarks (BINARYTREE, PUDDLEGRID) and LLM math reasoning tasks (SVAMP, GSM8K) using models QWEN2.5-MATH-1.5B and METAMATH-LLEMMA-7B.

## Key Results
- Achieves up to 7.5× inference speedup on consumer hardware while preserving accuracy on math reasoning tasks
- Theoretical sample complexity improvement from O(2^H) to O(H^3) in binary-tree MDPs
- Outperforms TARGET, UNCONSTRAINED, and SOFT-RL baselines across RL environments and LLM tasks
- Successfully compresses chain-of-thought tokens to 10-35% of original length without accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Cost Budget Selection via Constrained Optimization
The teacher solves an optimization problem to select training-time cost budget α_t that minimizes distance to deployment target α* while maintaining performance above threshold β. Binary search efficiently finds the smallest α satisfying the performance constraint, starting from α* and expanding upward if needed. This creates a curriculum that begins easier and tightens as the agent improves.

### Mechanism 2: Progressive Constraint Tightening as Implicit Difficulty Shaping
Gradually reducing the permissible cost budget from relaxed to target values transforms an intractable sparse-reward problem into a sequence of learnable intermediate tasks. The curriculum starts with α_1 >> α*, where approximately 50% of rollouts yield positive reward, and tightens as training progresses.

### Mechanism 3: Avoiding Contextual Policy Overhead via Non-Contextual Design
Unlike contextual RL approaches that would require the policy π(s, x, a) to learn across all (prompt, budget) pairs, CURLTRAC trains a non-contextual policy that experiences varying α_t during training but operates under fixed α* at deployment. A rolling buffer stores K rollouts per task, and α_x is updated using only previously collected rollouts—no additional rollouts needed.

## Foundational Learning

- **Concept: Trajectory-level vs. Expectation-based Constraints**
  - Why needed here: The paper enforces strict constraints via indicator functions (reward=0 if constraint violated), not soft expectation bounds. This distinction explains why conventional CRL methods fail here.
  - Quick check question: Can you explain why E[cost] ≤ α is fundamentally different from requiring every trajectory to satisfy cost ≤ α?

- **Concept: Curriculum Learning in RL**
  - Why needed here: The core technique requires understanding how task difficulty scheduling affects sample complexity and policy convergence.
  - Quick check question: If you start with α = max_cost and decrease by fixed 10% each episode, what failure mode might occur vs. adaptive decrease?

- **Concept: Sample Complexity Analysis in MDPs**
  - Why needed here: The theoretical contribution relies on comparing O(2^H) vs. O(H³) complexity to justify the curriculum approach.
  - Quick check question: Why does a random policy in a binary tree of depth H require ~2^H samples to reach the leftmost leaf?

## Architecture Onboarding

- **Component map:** Environment -> Teacher (maintains buffer, computes α_t) -> Student (executes K rollouts, updates policy) -> Buffer (stores trajectories)

- **Critical path:**
  1. Sample task x_t and its deployment constraint α*_xt
  2. Teacher retrieves per-task buffer, computes current α_t via binary search over [α*_xt, α_max]
  3. Student executes K rollouts under constraint α_t, receives reward only if cost ≤ α_t
  4. Student updates policy π_{t+1} using collected rollouts
  5. Teacher stores new rollouts in buffer, recomputes α_x for future steps

- **Design tradeoffs:**
  - Buffer size K: Larger K → more stable α estimates, but slower adaptation. Paper uses K=10–15.
  - Threshold β: Controls aggressiveness of curriculum. β=0.5 balances exploration and exploitation. Lower β tightens faster but risks sparse rewards.
  - Per-task vs. global α: Per-task handles heterogeneous constraints but requires warmup; global converges faster initially but plateaus earlier.

- **Failure signatures:**
  - Training reward stuck at 0: α_t too tight → increase β or check constraint feasibility
  - Constraint satisfaction at training but not test: Buffer contamination or α_t not converging to α*_xt
  - Per-task α never decreases: Task too difficult even at α_max, or K too small for reliable estimates
  - Rapid performance collapse: α_t decreasing too fast → increase β or add α decrease rate limit

- **First 3 experiments:**
  1. Replicate BINARYTREE (depth H=12, tabular policy) with β∈{0.1, 0.5, 0.9} to validate curriculum sensitivity and match Figure 16.
  2. Run PUDDLEGRID-SINGLE with K∈{5, 10, 20} to characterize buffer size effects on convergence speed and stability.
  3. Fine-tune QWEN2.5-MATH-1.5B on SVAMP subset (1000 samples) with target=10% original length; verify token reduction without accuracy drop matches Figure 4 trends.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Matryoshka-style learning be integrated with this curriculum strategy to enable single models that dynamically adapt to unknown or varying test-time token budgets?
- **Open Question 2:** Does the curriculum-based compression approach generalize to other inference bottlenecks, such as reducing the number of rollouts in best-of-N sampling or compressing input context?
- **Open Question 3:** What changes occur in a model's internal representations and attention patterns after curriculum-based compression, and do these correlate with improved reasoning efficiency?

## Limitations

- The adaptive curriculum assumes relaxed constraints (α >> α*) are always feasible, but may fail for extreme deployment constraints
- Theoretical sample complexity improvement is proven only for binary-tree MDPs, with generalization claims lacking formal proof
- Maintains one cost budget per task/prompt without exploring whether multiple budgets or distributions might be more effective

## Confidence

**High Confidence (8-10/10)**:
- Adaptive curriculum learning via binary search is technically sound and well-described
- Empirical results on RL benchmarks are reproducible
- Theoretical analysis for binary-tree MDPs is mathematically rigorous

**Medium Confidence (4-7/10)**:
- LLM performance improvements depend on specific implementation details
- Generalization from binary-tree MDPs to complex LLM environments is plausible but unproven
- Claims about avoiding contextual policy overhead lack direct empirical comparison

**Low Confidence (1-3/10)**:
- Claims about avoiding contextual policy overhead vs. PROCURL-TARGET are based on architectural arguments
- Efficiency of binary search lacks computational complexity analysis
- No discussion of sensitivity to hyperparameters beyond values used

## Next Checks

1. Replicate the BINARYTREE experiment with varying β values (0.1, 0.5, 0.9) to validate curriculum sensitivity and confirm theoretical sample complexity improvement matches Figure 16.

2. Implement a stress test where the target constraint α* is set to an infeasible value (e.g., requiring zero tokens) to observe whether the adaptive curriculum fails gracefully or produces misleading results.

3. Compare CURLTRAC against a contextual RL baseline (e.g., PROCURL-TARGET) on LLM math reasoning tasks to directly validate the claim that non-contextual policies are more practical for LLM deployment scenarios.