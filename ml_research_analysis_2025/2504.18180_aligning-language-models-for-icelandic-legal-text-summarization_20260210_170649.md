---
ver: rpa2
title: Aligning Language Models for Icelandic Legal Text Summarization
arxiv_id: '2504.18180'
source_url: https://arxiv.org/abs/2504.18180
tags:
- legal
- training
- language
- summaries
- icelandic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates whether preference-based training methods,
  such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference
  Optimization (DPO), can enhance the ability of language models to generate Icelandic
  legal text summaries. Three models were trained: GPT-SW3 (1.3B), Llama2 (7B), and
  a Llama2 variant pre-trained on Icelandic legal text (Ice-Llama2).'
---

# Aligning Language Models for Icelandic Legal Text Summarization

## Quick Facts
- arXiv ID: 2504.18180
- Source URL: https://arxiv.org/abs/2504.18180
- Reference count: 12
- Primary result: Preference training improves legal accuracy but not Icelandic language quality in summarization

## Executive Summary
This study evaluates whether preference-based training methods can enhance Icelandic language models for legal text summarization. Three models were trained: GPT-SW3 (1.3B), Llama2 (7B), and Ice-Llama2 (7B pre-trained on Icelandic legal text). Results show that Direct Preference Optimization (DPO) improves legal accuracy over standard fine-tuning, while Reinforcement Learning from Human Feedback (RLHF) suffers from instability and does not improve performance. Neither method significantly enhances Icelandic language quality. The findings suggest preference training can improve domain-specific accuracy but requires careful implementation and further development for practical legal applications.

## Method Summary
The research employed a three-phase training approach: (1) domain pre-training on Icelandic Supreme Court rulings, (2) instruction fine-tuning on rulings with lawyer-written summaries, and (3) preference training using either DPO or RLHF. Models were evaluated using ROUGE metrics, perplexity scores, and human expert assessments of Icelandic language quality and legal accuracy. Pairwise preference datasets were constructed from top ROUGE-performing outputs, and DPO was applied for 2 epochs while RLHF experiments faced stability challenges requiring reward normalization.

## Key Results
- DPO improved legal accuracy in generated summaries over standard fine-tuning
- RLHF led to training instability and did not improve performance
- Neither preference method significantly enhanced Icelandic language quality
- Automated metrics (ROUGE) and human evaluations showed discrepancies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO improves domain-specific legal accuracy when applied after supervised fine-tuning, but risks overfitting on smaller datasets.
- Mechanism: DPO transforms RL reward maximization into a classification task by penalizing outputs resembling rejected data, increasing likelihood of preferred responses without training a separate reward model.
- Core assumption: Pairwise comparison dataset accurately reflects domain expert preferences; ROUGE-based selection captures quality signals.
- Evidence anchors:
  - [abstract] "Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage."
  - [Section 4.2.1] "GPT-SW3-1.3B achieved its best improvements after 2 epochs of DPO training, following 5 epochs of fine-tuning. However, performance plateaued after 3 epochs, and in some cases began to degrade, likely due to overfitting."
- Break condition: Overfitting occurs when DPO epochs exceed optimal (observed at 3+ epochs for GPT-SW3).

### Mechanism 2
- Claim: RLHF can yield better human-evaluated performance than DPO but suffers from training instability and higher computational cost.
- Mechanism: RLHF trains a reward model to classify outputs and assign scalar rewards, then uses PPO to optimize the policy with KL-divergence constraints for stability.
- Core assumption: Reward model accurately captures human preferences; policy updates remain within stable bounds.
- Evidence anchors:
  - [Section 4.2.2] "Early efforts to use the reward model to train a policy using PPO resulted in highly unstable training. The policy quickly learned to exploit the reward model by generating sequences of empty lines, random characters, or incomplete word endings, leading to a spike in KL divergence and inflated rewards."
  - [Section 6] "The RLHF model outperformed both DPO variations in evaluations by human experts."
- Break condition: Reward model exploitation or KL divergence spike.

### Mechanism 3
- Claim: Language-specific pre-training establishes a stronger foundation for target-language generation than preference training alone.
- Mechanism: Continued pre-training on domain-specific corpora reduces perplexity on in-domain text, producing higher-quality output in the target language regardless of subsequent preference training.
- Core assumption: Perplexity reduction correlates with downstream generation quality for the target domain.
- Evidence anchors:
  - [Section 4] "After the phase one training process... both Llama2 variants achieved lower perplexity scores than GPT-SW3."
  - [Section 5.2] "The discrepancy in the quality of Icelandic text between Llama2-7B and GPT-SW3-1.3B highlights the importance of language-specific pre-training."
- Break condition: Domain-specific pre-training data quality is poor.

## Foundational Learning

- Concept: Preference Optimization vs. Reward Modeling
  - Why needed here: DPO and RLHF are fundamentally different approaches. DPO directly optimizes policy from preferences; RLHF learns a reward function first. Understanding this distinction explains why DPO is more stable but potentially less flexible.
  - Quick check question: Can you explain why DPO avoids training a separate reward model and what tradeoff this introduces?

- Concept: Perplexity and ROUGE Limitations
  - Why needed here: The paper shows perplexity and ROUGE scores do not correlate with human expert preferences. Automated metrics can be misleading for domain-specific tasks.
  - Quick check question: If a model achieves the highest ROUGE score but experts rank it lowest, what might be happening?

- Concept: KL Divergence in Policy Optimization
  - Why needed here: RLHF instability manifests as KL divergence spikes when the policy drifts too far from the reference model. Understanding this helps diagnose and prevent reward hacking.
  - Quick check question: Why does PPO constrain policy updates, and what happens when those constraints fail?

## Architecture Onboarding

- Component map: R dataset -> Phase 1 pre-training -> RS dataset -> Phase 2 fine-tuning -> Preference dataset -> DPO/RLHF -> Human evaluation
- Critical path: Pre-train on domain corpus first -> Fine-tune with instruction pairs -> Generate preference pairs from best checkpoint -> Apply DPO for stability or RLHF for flexibility
- Design tradeoffs:
  - DPO vs. RLHF: DPO is simpler, faster, more stable; RLHF is more flexible (not limited to pairwise), potentially better human-evaluated results, but computationally expensive and unstable
  - Larger model vs. language-specific pre-training: Llama2-7B achieved better ROUGE; GPT-SW3-1.3B achieved better Icelandic quality
  - Automated vs. human evaluation: ROUGE is cheap but misleading; human evaluation is expensive but necessary for domain alignment
- Failure signatures:
  - DPO overfitting: ROUGE improves but human ranking degrades; model occasionally produces excellent summaries but frequently fails (high variance)
  - RLHF instability: Policy generates empty lines, random characters, or truncated words; KL divergence spikes; rewards inflate without quality improvement
  - Metric-human mismatch: Top ROUGE model ranked last by experts
- First 3 experiments:
  1. Establish baseline: Fine-tune base model on RS dataset only, evaluate with ROUGE and perplexity
  2. DPO sweep with early stopping: Train DPO for 1, 2, 3 epochs with different fine-tuning durations
  3. RLHF stability test: Train reward model for 1 epoch, normalize rewards, run PPO with aggressive KL penalty

## Open Questions the Paper Calls Out

- Would integrating human feedback earlier in the training pipeline produce better alignment with expert preferences than using ROUGE-based selection? The authors note that the gap between ROUGE scores and expert preferences suggests earlier integration could be beneficial, but cost prevented testing this alternative.

- Can Retrieval-Augmented Generation (RAG) improve factual accuracy in Icelandic legal summarization by reducing hallucinations and contradictory statements? The authors explicitly propose RAG could help increase factual accuracy, as qualitative analysis revealed factual errors including contradictory statements.

- Would expanding training data to include lower court rulings and summaries, beyond Supreme Court cases, significantly improve model performance? The study was limited to ~2,600 rows from Supreme Court rulings, compared to ~120,000 rows in comparable studies.

- Do the findings generalize to other low-resource languages and legal domains beyond Icelandic? The study tested only Icelandic with a small expert cohort, and limitations may affect generalization to other domains and languages.

## Limitations

- Small-scale human evaluation (18 legal experts, ~6 responses per model) introduces uncertainty about preference rankings generalizability
- RLHF implementation experienced severe instability issues, limiting meaningful comparison with DPO
- Language quality improvements from preference training were minimal, suggesting domain alignment and language quality may require distinct approaches
- Pairwise preference dataset construction using ROUGE-based selection may not fully capture nuanced legal accuracy preferences

## Confidence

**High Confidence:** DPO improves legal accuracy over standard fine-tuning is well-supported by both automated metrics and human evaluation. Larger models achieve better ROUGE scores while language-specific pre-training produces better Icelandic text quality.

**Medium Confidence:** RLHF yields better human-evaluated performance than DPO is based on limited human evaluation data. Neither method significantly improves Icelandic language quality is supported by expert scores but lacks statistical power.

**Low Confidence:** DPO is preferable to RLHF for practical deployment relies heavily on RLHF's training instability. Optimal DPO training duration (2-3 epochs) may not generalize across different model sizes.

## Next Checks

1. **Expanded Human Evaluation:** Conduct human evaluation with at least 50 legal experts rating all model outputs to ensure statistical significance for preference comparisons.

2. **Robust Preference Dataset Construction:** Implement multiple preference dataset construction methods (ROUGE-based, entropy-based, and direct expert selection) to isolate whether preference signal quality or training method drives improvements.

3. **Cross-Domain Transfer Test:** Apply the best-performing preference training approach (DPO) to a different Icelandic domain (e.g., news articles or technical documentation) to determine whether legal accuracy improvements transfer to other text types.