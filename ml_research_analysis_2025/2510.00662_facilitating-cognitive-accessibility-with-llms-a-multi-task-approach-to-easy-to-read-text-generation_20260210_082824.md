---
ver: rpa2
title: 'Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read
  Text Generation'
arxiv_id: '2510.00662'
source_url: https://arxiv.org/abs/2510.00662
tags:
- text
- etr-fr
- dataset
- information
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating cognitively accessible
  text for individuals with disabilities by introducing ETR-fr, the first high-quality,
  paragraph-aligned dataset compliant with European Easy-to-Read (ETR) guidelines.
  The authors propose a multi-task learning (MTL) approach that combines text summarization,
  simplification, and ETR generation, evaluated using two strategies: multi-task retrieval-augmented
  generation (RAG) for in-context learning and MTL-LoRA for parameter-efficient fine-tuning.'
---

# Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation

## Quick Facts
- arXiv ID: 2510.00662
- Source URL: https://arxiv.org/abs/2510.00662
- Reference count: 40
- Introduces ETR-fr, first high-quality, paragraph-aligned dataset for Easy-to-Read French text generation

## Executive Summary
This paper addresses the challenge of generating cognitively accessible text for individuals with disabilities by introducing ETR-fr, the first high-quality, paragraph-aligned dataset compliant with European Easy-to-Read (ETR) guidelines. The authors propose a multi-task learning (MTL) approach that combines text summarization, simplification, and ETR generation, evaluated using two strategies: multi-task retrieval-augmented generation (RAG) for in-context learning and MTL-LoRA for parameter-efficient fine-tuning. Experiments on Mistral-7B and LLaMA-3-8B show that MTL-LoRA consistently outperforms single-task baselines in in-domain settings, achieving the highest SRB score of 39.60 and compression ratio of 56.11%, while RAG-based approaches demonstrate better generalization on out-of-domain political texts. Human evaluation confirms the benefits of multi-task learning across ETR criteria and text quality dimensions, though illustration generation remains challenging.

## Method Summary
The authors propose two complementary approaches for Easy-to-Read text generation. First, a multi-task RAG system uses jina-embeddings-v3 to retrieve relevant examples from auxiliary datasets (OrangeSum for summarization, WikiLarge FR for simplification) during inference, with L2 distance ranking and variable shot counts. Second, an MTL-LoRA approach fine-tunes LLaMA-3-8B or Mistral-7B with LoRA adapters (r=128, α=128) on all three tasks simultaneously, using task-weighted loss and hyperparameters selected via grid search. Both approaches are evaluated on the newly created ETR-fr dataset (523 paragraph-aligned pairs) and an out-of-domain political subset (ETR-fr-politic), with metrics including SRB composite score, compression ratio, and human evaluation against 38 ETR guideline criteria.

## Key Results
- MTL-LoRA consistently outperforms single-task baselines in in-domain settings, achieving SRB score of 39.60 and compression ratio of 56.11%
- RAG-based approaches demonstrate better generalization on out-of-domain political texts despite lower in-domain performance
- Human evaluation confirms multi-task learning benefits across ETR criteria and text quality dimensions
- Negative compression ratios in zero-shot settings (-309.24% for Mistral-7B) indicate prompt misalignment issues

## Why This Works (Mechanism)
The multi-task approach works by leveraging complementary relationships between summarization (extracting key information), simplification (reducing complexity), and ETR generation (cognitive accessibility). By training on multiple related tasks, the model learns shared representations that transfer across tasks, improving generalization. The RAG component provides in-context examples that guide generation toward ETR-compliant outputs, while the LoRA-based fine-tuning allows efficient adaptation to all three tasks simultaneously. This combined approach addresses the challenge that ETR guidelines require both simplification and cognitive accessibility considerations that go beyond standard text simplification.

## Foundational Learning
- Easy-to-Read (ETR) guidelines - European standards for cognitive accessibility requiring simplified language, clear structure, and visual support; needed to understand evaluation criteria and dataset construction
- Multi-task learning (MTL) - Joint training on related tasks to improve generalization; needed to understand how summarization and simplification tasks benefit ETR generation
- LoRA (Low-Rank Adaptation) - Parameter-efficient fine-tuning technique that inserts low-rank matrices into attention layers; needed to understand the MTL fine-tuning approach
- Retrieval-augmented generation (RAG) - Using retrieved examples for in-context learning; needed to understand the prompt-based multi-task approach
- SRB composite score - Harmonic mean of SARI, ROUGE-L, and BERTScore-F1; needed to evaluate text quality and simplification effectiveness
- KMRE readability metric - Measures lexical difficulty; needed to assess cognitive accessibility of generated text

## Architecture Onboarding
- **Component map**: Source text -> Task selector -> (RAG retrieval OR LoRA fine-tuned model) -> ETR output
- **Critical path**: Input encoding -> Task-specific processing (summarization/simplification/ETR) -> Text generation -> Post-processing
- **Design tradeoffs**: Multi-task learning improves generalization but requires careful task mixing; RAG provides better OOD performance but depends on quality retrieval; LoRA enables efficient fine-tuning but needs hyperparameter optimization
- **Failure signatures**: Negative compression ratios indicate prompt misalignment; degradation on OOD data suggests overfitting to training domain; hallucinations risk factual inconsistency
- **First experiments to run**: 1) Validate dataset loading and splits with stratified sampling; 2) Test zero-shot generation to establish baseline; 3) Run RAG retrieval with k=2-3 examples to verify context quality

## Open Questions the Paper Calls Out
1. **Does improved performance translate to comprehension gains?** The practical utility of outputs for users with intellectual disabilities remains untested since evaluation relied on NLP researchers and linguists rather than the neurodivergent population.

2. **How to model cognitive load explicitly?** Current training objectives maximize text quality and simplicity scores, but these are static linguistic features that do not dynamically account for the processing effort required by users with cognitive impairments.

3. **What constraints minimize hallucinations?** The paper identifies susceptibility to hallucinations as a key limitation, warning that this is particularly risky for audiences who may interpret outputs literally or depend on high textual reliability.

4. **Does guideline adherence erase nuance?** The study warns that rigid adherence may exclude culturally specific or individualized accessibility strategies, potentially erasing nuance, flattening perspective, or reinforcing harmful stereotypes.

## Limitations
- Dataset accessibility pending publisher agreement, making full validation impossible until release
- Human evaluation involved only 4 evaluators per output, which may not capture the full diversity of cognitive accessibility needs
- Out-of-domain testing limited to single political domain with only 33 pairs, which may not represent broader OOD scenarios

## Confidence
- **High confidence**: Experimental methodology and technical implementation of LoRA-based multi-task learning framework
- **Medium confidence**: Dataset quality and alignment claims (cannot be independently verified)
- **Medium confidence**: Generalization results (limited to single political domain with 33 pairs)

## Next Checks
1. Validate dataset release timeline and accessibility by monitoring the GitHub repository for ETR-fr distribution and confirming publisher agreement status with authors.

2. Replicate the multi-task RAG generalization experiment using additional out-of-domain test sets beyond political texts (e.g., scientific, legal, or technical domains) to assess the robustness of the retrieval-based approach across different text genres.

3. Conduct a sensitivity analysis of the MTL-LoRA approach by varying the task mixing ratio (τ) and LoRA rank (r) parameters systematically to determine optimal configurations for different resource constraints and application scenarios.