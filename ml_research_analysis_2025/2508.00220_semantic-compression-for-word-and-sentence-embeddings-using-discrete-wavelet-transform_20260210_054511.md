---
ver: rpa2
title: Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet
  Transform
arxiv_id: '2508.00220'
source_url: https://arxiv.org/abs/2508.00220
tags:
- embeddings
- coefficients
- embedding
- tasks
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Discrete Wavelet Transform (DWT) to compress
  word and sentence embeddings while preserving semantic information. DWT analyzes
  embedding vectors by decomposing them into approximation (low-frequency) and detail
  (high-frequency) coefficients, capturing both overall structure and fine-grained
  features.
---

# Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform

## Quick Facts
- arXiv ID: 2508.00220
- Source URL: https://arxiv.org/abs/2508.00220
- Reference count: 19
- Primary result: DWT reduces embedding dimensionality by 50-93% with minimal performance loss on semantic similarity tasks and often improves downstream task accuracy

## Executive Summary
This paper applies Discrete Wavelet Transform (DWT) to compress word and sentence embeddings while preserving semantic information. DWT analyzes embedding vectors by decomposing them into approximation (low-frequency) and detail (high-frequency) coefficients, capturing both overall structure and fine-grained features. Experiments show DWT reduces embedding dimensionality by 50-93% with minimal performance loss on semantic similarity tasks and often improves downstream task accuracy. For example, DWT embeddings compress BERT and GPT embeddings by 50-75% while maintaining comparable or better performance across STS benchmarks and classification tasks. The approximation coefficients, in particular, retain dense semantic information and outperform baselines in several cases. This demonstrates that DWT is an effective, model-agnostic method for compressing embeddings by exploiting redundancy and correlation, potentially improving efficiency in NLP applications.

## Method Summary
The method applies Discrete Wavelet Transform to decompose embedding vectors into approximation (cA) and detail (cD) coefficients using convolution with Mother Wavelets followed by downsampling. The approach supports recursive application to cA coefficients for higher compression ratios. The paper tests multiple coefficient selections (cA, cD, cAA, cDA, and combinations) across different embedding types including GloVe, FastText, BERT, GPT, SBERT, and RoBERTa. Evaluation uses Spearman's correlation for semantic similarity tasks and classification accuracy for downstream tasks, comparing against baselines including original embeddings and PCA compression.

## Key Results
- DWT achieves 50-75% dimensionality reduction while maintaining comparable or better performance across STS benchmarks and classification tasks
- Approximation coefficients (cA) retain dense semantic information and often outperform baseline embeddings
- Compression up to 93% is possible with minimal performance loss, though higher compression levels show task-dependent degradation
- DWT outperforms DCT and PCA baselines in several semantic similarity and downstream classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Domain Separation of Semantic Information
DWT decomposes embedding vectors into approximation coefficients (low-frequency, slowly-varying components) and detail coefficients (high-frequency, rapid variations), each capturing different semantic properties. The approximation coefficients capture correlated, redundant features across neighboring dimensions while detail coefficients capture abrupt changes and nuanced distinctions. This works because embedding dimensions exhibit local correlation—neighboring dimensions carry related information. Evidence shows cA captures relational words while cD captures nuanced associations, with 50-75% compression maintaining performance.

### Mechanism 2: Multi-Resolution Recursive Compression
Recursive application of DWT to approximation coefficients achieves 75–93% dimensionality reduction while retaining task-relevant semantics. After Level-1 decomposition (cA₁, cD₁), DWT is applied to cA₁ to produce Level-2 coefficients (cAA₂, cAD₂), and recursively further. Each level halves the dimension. Higher-level approximation coefficients condense increasingly abstract semantic representations, with cAAA (128 dim) and cAAAA (64 dim) outperforming baseline on MRPC (+1.5%) and MR (+0.2%).

### Mechanism 3: Task-Dependent Coefficient Selection
Different tasks benefit from different coefficient selections—approximation (cA), detail (cD), or combined coefficients—suggesting embeddings encode multiple semantic aspects. The paper empirically selects among cA, cD, cAA, cDA, and combinations based on task/embedding type. SBERT benefits more from cA (relational semantics); RoBERTa benefits more from cD (distinctive features). Combined coefficients recover information lost at Level-1, with cD outperforming on SST5 (+4.5%) and TREC (+2%).

## Foundational Learning

- **Concept: Discrete Wavelet Transform (DWT) basics**
  - **Why needed here:** DWT is the core transformation. Understanding filter banks, Mother Wavelets, and the distinction between approximation vs. detail coefficients is essential to interpret results.
  - **Quick check question:** Given a 300-dimensional embedding, what are the dimensions of Level-1 cA, cD, and Level-2 cAA coefficients? (Answer: 150, 150, 75)

- **Concept: Embedding semantics and evaluation**
  - **Why needed here:** The paper evaluates on semantic similarity (STS benchmarks, SimLex, WS353, MEN) and downstream classification. Understanding cosine similarity, Spearman correlation, and what these benchmarks measure is necessary to assess compression tradeoffs.
  - **Quick check question:** If DWT-compressed embeddings achieve 98% of baseline Spearman correlation on STS-B with 50% fewer dimensions, what does this imply about the original embedding's redundancy?

- **Concept: Dimensionality reduction vs. compression**
  - **Why needed here:** DWT differs from PCA, DCT, and quantization. PCA learns eigenvectors from data; DCT uses global frequency bases; DWT uses localized, multi-resolution analysis. Understanding these differences clarifies when DWT is preferable.
  - **Quick check question:** Why might DWT preserve semantics better than PCA for embeddings with locally correlated dimensions?

## Architecture Onboarding

- **Component map:** Embedding vector -> DWT transform (cA, cD) -> Recursive levels (optional) -> Coefficient selection (cA, cD, cAA, cDA, combinations) -> Compressed embedding output

- **Critical path:** Select base embedding model based on task -> Choose Mother Wavelet family (Coiflets generally perform well) -> Determine compression level (L=1 for 50%, L=2 for 75%, L=3-4 for higher) -> Select coefficient type empirically: start with cA for relational tasks, cD for distinctive-feature tasks -> Evaluate on task-specific benchmark

- **Design tradeoffs:** Higher levels (L≥3) risk degradation on fine-grained tasks (TREC -7.4%). cA favors relational semantics; cD favors nuances. Combined coefficients recover info but reduce compression ratio. Mother Wavelet selection remains empirical. Larger models show more redundancy and compress better.

- **Failure signatures:** Performance drops >5% indicate over-compression (L≥3) or wrong coefficient type. DCT baseline outperforming suggests task requires global frequency info. Inconsistent results across tasks indicate embedding-specific coefficient preference requiring per-task tuning. No improvement over baseline suggests embedding dimensions lack local correlation.

- **First 3 experiments:** 1) Apply Level-1 DWT (cA and cD) to SBERT-Base on STS-B, compare Spearman correlation to baseline and DCT-50%. 2) Apply 4-level DWT to RoBERTa-Large on MRPC and TREC, plot accuracy vs. dimension. 3) Compare cA vs cD vs cA+cDA vs cD+cAD on SST5 and MR tasks using RoBERTa-Base.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the optimal Mother Wavelet family and scaling factor be systematically determined for a specific embedding model or NLP task?
- **Basis in paper:** Section 8 (Limitations) states the authors "defer the investigation into the selection of the best MW to future work," noting that the current paper serves only as a proof of concept using standard families like Coiflets and Symlets.
- **Why unresolved:** The paper empirically tests a subset of wavelets but lacks a theoretical framework or heuristic to predict which MW shape or scale best aligns with the internal structure of a given embedding type.
- **What evidence would resolve it:** A comparative study establishing correlations between specific MW properties (e.g., vanishing moments, compactness) and embedding characteristics, potentially leading to an automated selection algorithm.

### Open Question 2
- **Question:** What theoretical guidelines can predict whether approximation (cA), detail (cD), or combined coefficients will yield superior performance for a given downstream task?
- **Basis in paper:** Section 8 (Limitations) explicitly mentions that "a detailed study and analysis of coefficient selection fall beyond the scope of this paper and are planned for future work."
- **Why unresolved:** The results show inconsistent behavior where approximation coefficients excel in some tasks while detail coefficients excel in others, making a priori selection impossible without empirical testing.
- **What evidence would resolve it:** A formal analysis mapping the energy distribution of approximation vs. detail coefficients to semantic properties required by specific task types.

### Open Question 3
- **Question:** Does DWT compression preserve the semantic nuances required for generative tasks, such as text generation or summarization, as effectively as it does for discriminative tasks?
- **Basis in paper:** The paper evaluates GPT embeddings but restricts the evaluation to discriminative tasks like semantic similarity and classification, leaving the impact on generation unexplored.
- **Why unresolved:** While DWT effectively retains information for classification by filtering high-frequency "noise," it is unknown if this filtering removes the subtle stylistic or lexical diversity necessary for high-quality text generation.
- **What evidence would resolve it:** Experiments applying DWT-compressed embeddings as input to generative pipelines and evaluating the output using metrics like BLEU, ROUGE, or semantic faithfulness.

## Limitations
- Theoretical justification for why embedding dimensions exhibit local correlation assumed by DWT remains unproven
- Optimal coefficient selection (cA vs cD vs combinations) and Mother Wavelet choice remain empirical, requiring task-specific tuning
- Corpus support for DWT in NLP is weak—most applications are in signal/image domains, and related work remains preliminary

## Confidence

- **High Confidence**: DWT achieves 50-75% dimensionality reduction with minimal performance loss on semantic similarity tasks (STS benchmarks). This is directly measured and reproducible.
- **Medium Confidence**: DWT improves downstream classification accuracy on several benchmarks (MRPC, SST5, TREC) with 75-93% compression. Performance gains are task-dependent and require empirical coefficient selection.
- **Low Confidence**: Claims about semantic concentration in approximation coefficients are inferred from performance, not directly measured. The theoretical basis for embedding dimension correlation remains unproven.

## Next Checks

1. **Correlation Analysis**: Measure Pearson correlation coefficients between adjacent dimensions in BERT, SBERT, and RoBERTa embeddings. Quantify the proportion of dimensions with |correlation| > 0.1 to validate DWT's core assumption of local correlation.

2. **Coefficient Ablation Study**: For each task (STS, MRPC, SST5, TREC), systematically test all coefficient combinations (cA, cD, cAA, cDA, cA+cDA, cD+cAD) across 2-3 Mother Wavelets (db2, sym2, coif1). Identify patterns in coefficient-task mapping to move beyond empirical selection.

3. **Computational Overhead Benchmark**: Measure wall-clock time for DWT compression vs. baseline inference for SBERT-Large on STS-B. Compare against PCA and quantization baselines to quantify practical efficiency gains beyond dimensionality reduction.