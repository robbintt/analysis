---
ver: rpa2
title: Balancing Accuracy and Novelty with Sub-Item Popularity
arxiv_id: '2508.05198'
source_url: https://arxiv.org/abs/2508.05198
tags:
- popularity
- novelty
- personalised
- recommendation
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited novelty in music recommendation
  when using personalized popularity scores (PPS), which tend to reinforce known content.
  The authors propose extending the RecJPQ framework to model personalized popularity
  at the sub-item level, enabling finer-grained personalization by capturing shared
  repetition patterns across sub-embeddings.
---

# Balancing Accuracy and Novelty with Sub-Item Popularity

## Quick Facts
- **arXiv ID:** 2508.05198
- **Source URL:** https://arxiv.org/abs/2508.05198
- **Reference count:** 40
- **Primary result:** sPPS improves novelty@K without sacrificing accuracy, outperforming PPS in personalized novelty metrics

## Executive Summary
This paper addresses the problem of limited novelty in music recommendation when using personalized popularity scores (PPS), which tend to reinforce known content. The authors propose extending the RecJPQ framework to model personalized popularity at the sub-item level, enabling finer-grained personalization by capturing shared repetition patterns across sub-embeddings. They integrate both item-level (PPS) and sub-ID-level (sPPS) popularity signals into a weighted scoring function. Experiments on Yandex and Last.fm-1K datasets show that sPPS consistently achieves higher personalized novelty without sacrificing accuracy, outperforming PPS in novelty@K metrics. The sub-ID approach offers explicit control over the accuracy–novelty trade-off, helping systems surface unexpected yet relevant items.

## Method Summary
The method extends RecJPQ-based BERT4Rec with two popularity signals: item-level PPS and sub-ID-level sPPS. RecJPQ decomposes item embeddings into m=32 sub-embeddings using SVD-based quantization, assigning each item a tuple of sub-IDs. Users' histories are tracked at both item and sub-ID levels, with counts log-transformed and Z-score normalized. Final scores combine model logits with PPS and sPPS using tunable weights α and β (γ=1-α-β). The framework captures latent attribute preferences across different items, enabling generalization beyond exact repetitions.

## Key Results
- On Last.fm at novelty≥12, sPPS improves NDCG@40 from 0.2749 to 0.3016 (+9.7%)
- sPPS consistently achieves higher personalized novelty without sacrificing accuracy across both Yandex and Last.fm datasets
- The method provides explicit control over accuracy–novelty trade-off through adjustable α and β weights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sub-ID decomposition via SVD-based quantization groups items with shared latent characteristics, enabling cross-item generalization that item-level approaches cannot achieve.
- Mechanism: RecJPQ encodes each item as an m-tuple of sub-identifiers derived from SVD decomposition of the sequence-item interaction matrix. Items sharing sub-IDs naturally cluster around latent attributes (e.g., genre, artist), allowing the model to recognize preference patterns across different items.
- Core assumption: Sub-IDs correspond to semantically meaningful item attributes that drive user preferences, not arbitrary mathematical artifacts.
- Evidence anchors:
  - [abstract] "capture shared repetition patterns across sub-embeddings—latent structures not accessible through item-level popularity alone"
  - [section 2.3] "RecJPQ naturally groups items that share similar latent characteristics (e.g., musical genre or artist)"
- Break condition: If sub-ID assignments are random or don't reflect meaningful item attributes, cross-item generalization fails and sPPS provides no benefit over PPS.

### Mechanism 2
- Claim: Counting sub-ID occurrences in user history reveals implicit preferences for underlying attributes even when users never repeat exact items.
- Mechanism: For each user u and sub-ID position j, compute c_j^u[k] = count of sub-ID k in their history. Apply log transformation with smoothing epsilon, sum across m positions, then Z-score normalize. This captures repeated exposure to latent attributes (e.g., electronic genre in position 3) across different tracks.
- Core assumption: Repeated sub-ID exposure signals preference, not mere availability or platform bias.
- Evidence anchors:
  - [abstract] "modelling sub-id popularity yields a fine-grained personalisation signal that captures user repetition patterns, reveals structural similarities between items"
- Break condition: If user histories are too short or sub-ID distributions are uniform, counts provide weak preference signals.

### Mechanism 3
- Claim: Linear combination of three signals (model logits, item-PPS, sub-ID-PPS) with scalar weights enables explicit, tunable control over the accuracy-novelty trade-off at inference time.
- Mechanism: logits_final_i = γ·logits_rec_i + α·PPS_std_i + β·sPPS_std_i, where γ = 1 - α - β ensures convex combination. Item-level PPS drives memorization, sub-ID sPPS enables generalization to novel items sharing preferred attributes, base model provides learned representations.
- Core assumption: The three signals are sufficiently independent and complementary for linear combination to be meaningful.
- Evidence anchors:
  - [abstract] "enabling explicit control over the trade-off between accuracy and personalised novelty"
  - [section 4, results] "at a fixed NDCG@40 of approximately 0.32, the PPS-only model yields a novelty score of about 10, whereas the sub-ID model achieves roughly 12, a 20% relative improvement"
- Break condition: If PPS and sPPS are highly correlated (sub-IDs dominated by item-specific codes), the trade-off control collapses to a single dimension.

## Foundational Learning

- Concept: Product Quantisation (PQ) and Joint Product Quantisation (JPQ)
  - Why needed here: RecJPQ extends PQ/JPQ for recommendation. Understanding how d-dimensional embeddings split into m segments, each mapped to V centroids in a codebook, is essential for grasping sub-ID generation.
  - Quick check question: Given d=256 dimensions and m=32 splits, how many dimensions per sub-ID, and how many unique sub-ID combinations exist with V centroids?

- Concept: Transformer-based sequential recommendation (SASRec, BERT4Rec)
  - Why needed here: The base model is BERT4Rec integrated with RecJPQ. Transformers excel at sequential patterns but struggle with exact repetition—hence the original PPS motivation.
  - Quick check question: Why do standard Transformer recommenders underperform on repetitive music sequences compared to PPS-augmented versions?

- Concept: Personalized novelty metric (Novelty@K)
  - Why needed here: The paper optimizes for this specific metric. It uses -log₂(p(i|u)) where p(i|u) is the user's historical interaction frequency with item i, penalizing familiar items.
  - Quick check question: How does personalized novelty differ from global novelty based on catalog popularity, and what does a novelty score of 12 indicate vs. 10?

## Architecture Onboarding

- Component map: RecJPQ encoder -> BERT4Rec -> PPS calculator -> sPPS calculator -> Score combiner
- Critical path:
  1. Offline: Run SVD on sequence-item matrix, assign sub-ID codes [z₁, z₂, ..., zₘ] to each item
  2. Training: Train RecJPQ-BERT4Rec with fixed α=0.4, β=0.4 (as paper did)
  3. Inference: For each candidate item, compute sPPS from user's sub-ID history, combine with logits and PPS using tunable α, β
- Design tradeoffs:
  - m=32 splits, d=256: Finer granularity (higher m) captures more nuanced patterns but increases codebook memory and count sparsity
  - Training vs. inference tuning: Paper fixes α, β at training but varies at inference—allows deployment-time A/B testing without retraining, but may be suboptimal compared to training with target parameters
  - Novelty threshold filtering: Higher thresholds improve novelty metrics but reduce candidate pool, potentially hurting real-world engagement
- Failure signatures:
  - sPPS variance near zero: Sub-IDs too uniformly distributed—check codebook initialization and SVD quality
  - Novelty collapses despite high β: PPS still dominating—verify γ calculation and Z-score normalization isn't inflating PPS
  - sPPS outperforms base model logits: Sequential model may be undertrained; check if RecJPQ reconstruction degrades embedding quality
- First 3 experiments:
  1. Replicate PPS-only baseline (β=0, vary α 0.0→0.9) on Yandex subset to verify accuracy-novelty trade-off curve matches Figure 2
  2. Test sPPS-only (α=0, vary β 0.0→0.9) to confirm higher novelty at comparable NDCG, validating sub-ID signal independence
  3. Run combined configuration (β=0.9, vary α) to verify the green curve behavior—NDCG gains with less novelty loss than PPS-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does varying α and β popularity weights during training time (rather than only at inference) yield better accuracy–novelty trade-offs or faster convergence?
- Basis in paper: Future work section states: "we plan to further explore the interplay between item-level and sub-ID-level popularity signals by varying α and β parameters directly at training time."
- Why unresolved: Current experiments fix α=0.4, β=0.4 at training and only vary them at inference, leaving training-time dynamics unexplored.
- What evidence would resolve it: Experiments comparing models trained with different α/β schedules against inference-time-only tuning, reporting both accuracy and novelty metrics.

### Open Question 2
- Question: Does sPPS generalize beyond music recommendation to domains with different repetition patterns (e.g., e-commerce, video streaming)?
- Basis in paper: The paper evaluates only on Yandex and Last.fm-1K music datasets. Music has uniquely high repetition patterns that may not transfer.
- Why unresolved: The sPPS method is motivated by music-specific "repetitive listening" behavior; whether sub-ID popularity captures useful latent structure in non-music domains remains untested.
- What evidence would resolve it: Experiments on non-music sequential datasets (e.g., MovieLens, Amazon) comparing PPS vs. sPPS performance.

### Open Question 3
- Question: How sensitive is sPPS to RecJPQ hyperparameters (codebook size V, number of sub-IDs m, embedding dimension d)?
- Basis in paper: The paper adopts d=256, m=32 after evaluating "several architectural settings" but provides no systematic analysis of how sub-ID granularity affects the accuracy–novelty trade-off.
- Why unresolved: Sub-ID granularity determines how latent patterns are captured; different configurations may yield substantially different novelty gains.
- What evidence would resolve it: Ablation studies varying m and V systematically while measuring both NDCG and novelty@K across multiple thresholds.

## Limitations
- Codebook size V and exact SVD procedure for RecJPQ sub-ID assignment are unspecified, critical for reproducibility
- Smoothing constant ε for PPS/sPPS is only described as "small," affecting score distributions
- BERT4Rec hyperparameters are not provided, impacting base model performance
- Limited dataset diversity (music-only) raises questions about generalizability to other domains

## Confidence
- **High**: sPPS improves novelty@K without sacrificing accuracy (supported by multiple experiment configurations)
- **Medium**: Linear combination of PPS and sPPS enables explicit trade-off control (mechanism plausible but optimal weights not established)
- **Medium**: Sub-ID generalization works as claimed (mechanism described but empirical evidence for semantic clustering is indirect)

## Next Checks
1. Implement RecJPQ sub-ID generation with varying codebook sizes (V=256, 512, 1024) to determine optimal granularity and verify whether semantic clustering emerges
2. Conduct ablation study isolating sub-ID signal by testing sPPS-only (α=0) across different sequence lengths to confirm preference pattern detection
3. Test the weighted combination mechanism on a non-music sequential dataset (e.g., e-commerce) to evaluate domain generalizability of the accuracy-novelty trade-off control