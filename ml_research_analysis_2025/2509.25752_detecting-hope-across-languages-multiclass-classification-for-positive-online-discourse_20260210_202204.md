---
ver: rpa2
title: 'Detecting Hope Across Languages: Multiclass Classification for Positive Online
  Discourse'
arxiv_id: '2509.25752'
source_url: https://arxiv.org/abs/2509.25752
tags:
- hope
- speech
- across
- detection
- xlm-roberta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the detection of hopeful speech in multilingual
  social media, specifically in English, Spanish, and Urdu, by categorizing it into
  three classes: Generalized Hope, Realistic Hope, and Unrealistic Hope. The approach
  leverages transformer-based models, particularly XLM-RoBERTa, combined with active
  learning strategies for iterative model refinement.'
---

# Detecting Hope Across Languages: Multiclass Classification for Positive Online Discourse

## Quick Facts
- arXiv ID: 2509.25752
- Source URL: https://arxiv.org/abs/2509.25752
- Reference count: 11
- Primary result: XLM-RoBERTa with active learning achieves 0.80 macro F1 on multilingual hope speech detection

## Executive Summary
This study addresses the detection of hopeful speech in multilingual social media across English, Spanish, and Urdu by categorizing it into three classes: Generalized Hope, Realistic Hope, and Unrealistic Hope. The approach leverages transformer-based models, particularly XLM-RoBERTa, combined with active learning strategies for iterative model refinement. Evaluated on the PolyHope dataset, the proposed method achieved superior performance across all languages, with XLM-RoBERTa significantly outperforming baseline models such as Logistic Regression in terms of macro F1 scores.

## Method Summary
The method employs XLM-RoBERTa with a sigmoid multilabel classification head, trained using weighted binary cross-entropy loss to handle severe class imbalance. Active learning with entropy-based uncertainty sampling iteratively refines the model by adding the most uncertain samples to the training pool. The approach includes preprocessing (lowercasing, removing special characters, mentions, links, and digits) and is evaluated against a TF-IDF-based Logistic Regression baseline using macro F1 as the primary metric.

## Key Results
- XLM-RoBERTa achieved 0.80 macro F1 on the development set, outperforming the Logistic Regression baseline (0.72 F1)
- Active learning strategies improved minority class performance, particularly for Realistic Hope and Unrealistic Hope
- The model demonstrated effective cross-lingual transfer across typologically diverse languages (English, Spanish, German, Urdu)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** XLM-RoBERTa's pretrained cross-lingual representations enable superior multiclass hope detection across typologically diverse languages.
- **Mechanism:** The model's 100-language pretraining corpus learns language-agnostic semantic patterns that transfer to hope speech categories, capturing nuanced distinctions between Generalized, Realistic, and Unrealistic Hope that surface-level lexical features miss.
- **Core assumption:** Hope speech exhibits consistent semantic markers across languages that transfer learning can exploit.
- **Evidence anchors:** [abstract]: "XLM-RoBERTa significantly outperforming baseline models such as Logistic Regression in terms of macro F1 scores"; [section 3.5.2]: "It often performs better in non-English languages due to its extensive multilingual pretraining corpus"

### Mechanism 2
- **Claim:** Uncertainty-based active learning accelerates convergence on minority hope categories by focusing annotation effort on decision boundary samples.
- **Mechanism:** Entropy-based sampling (U(x) = −Σ pk log pk) identifies samples where the model's probability distribution is most uncertain, iteratively adding these to the training pool to refine decision boundaries for underrepresented classes.
- **Core assumption:** Uncertain samples provide higher information gain than random sampling for improving minority class performance.
- **Evidence anchors:** [abstract]: "combined with active learning strategies for iterative model refinement"; [section 3.4.4]: "ensures that the model focuses on difficult and ambiguous samples, which accelerates convergence and improves generalization"

### Mechanism 3
- **Claim:** Class-weighted loss counters severe label imbalance, preventing the model from collapsing to the dominant "Not Hope" class.
- **Mechanism:** Weighted binary cross-entropy assigns higher loss penalties to minority classes (Realistic Hope, Unrealistic Hope), forcing the optimizer to prioritize their correct classification despite lower representation.
- **Core assumption:** Class frequency inversely correlates with classification difficulty; minority classes need proportional upweighting.
- **Evidence anchors:** [section 3.1]: "The datasets are highly imbalanced, necessitating weighted learning strategies"; [section 3.4.3]: Weighted loss formula explicitly applied

## Foundational Learning

- **Concept: Transformer attention and contextual embeddings**
  - Why needed here: Understanding why XLM-RoBERTa outperforms TF-IDF baselines requires grasping how self-attention captures hope's context-dependent semantics (e.g., "I hope" can express Generalized or Unrealistic Hope depending on surrounding context).
  - Quick check question: Can you explain why a fixed embedding for "hope" would fail to distinguish "I hope this works" from "I hope to become a billionaire by tomorrow"?

- **Concept: Entropy-based uncertainty sampling**
  - Why needed here: The active learning loop relies on entropy as the uncertainty metric; practitioners must understand when entropy fails (e.g., out-of-distribution samples that appear confidently wrong).
  - Quick check question: If a model outputs probabilities [0.33, 0.34, 0.33] vs. [0.98, 0.01, 0.01], which sample has higher uncertainty and why might this be misleading?

- **Concept: Class-weighted loss functions**
  - Why needed here: With "Not Hope" at 5,383 samples vs. "Realistic Hope" at 1,113 in Spanish training data, understanding inverse frequency weighting is critical to avoiding collapsed predictions.
  - Quick check question: If you weight by inverse frequency, what happens when a class has only 10 samples—does this create a new problem?

## Architecture Onboarding

- **Component map:** Raw social media text -> Preprocessing (lowercase, remove special characters, mentions, links, digits) -> XLM-R tokenization -> XLM-RoBERTa base encoder -> Sigmoid multilabel head -> Weighted BCE loss -> Epoch end -> Uncertainty sampling -> Add batch to labeled pool -> Repeat -> Evaluation with macro F1

- **Critical path:**
  1. Verify class distribution per language (Table 1 shows severe imbalance)
  2. Compute class weights from training frequencies
  3. Initialize XLM-R with multilabel classification head
  4. Train with weighted loss for N epochs
  5. After each epoch: compute uncertainty on unlabeled pool, select top-k, add to training
  6. Evaluate on dev set with macro F1; compare to LR baseline

- **Design tradeoffs:**
  - XLM-R vs. language-specific BERT: XLM-R simplifies deployment (single model) but may underperform monolingual models for high-resource languages
  - Active learning iterations vs. computational cost: More iterations improve minority class recall but require repeated inference on unlabeled pool
  - Macro vs. micro F1: Paper optimizes macro F1 (equal class weight), which may over-index on rare classes vs. real-world utility

- **Failure signatures:**
  - Model predicts "Not Hope" for >90% of samples: Class weighting insufficient, increase weight or use focal loss
  - Large gap between dev and test performance: Overfitting to dev set distribution; needs regularization or early stopping
  - Urdu significantly underperforms other languages: Check tokenizer coverage for Urdu script; consider transliteration or vocabulary expansion
  - Active learning degrades performance: Uncertainty metric capturing noise; add confidence threshold or diversity sampling

- **First 3 experiments:**
  1. **Baseline replication:** Train LR with TF-IDF and XLM-R without active learning on English only; verify reported gap (0.72 vs. 0.80 F1 on dev)
  2. **Ablation study:** Run XLM-R with and without class weighting; quantify impact on minority class recall (Realistic Hope, Unrealistic Hope)
  3. **Language-specific analysis:** Evaluate per-class F1 for Urdu; if Realistic/Unrealistic Hope <0.50, investigate whether issue is data scarcity, tokenizer gaps, or cultural annotation differences

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the integration of multimodal features (e.g., images, emojis, video) significantly improve the detection accuracy of nuanced hope categories compared to the text-only XLM-RoBERTa approach? The authors explicitly state in the Limitations section that "the exclusive focus on text overlooks multimodal expressions of hope" and suggest expanding to "multimodal analysis."

- **Open Question 2:** To what extent do culturally specific idioms and contextual slang in low-resource languages (like Urdu) degrade the performance of cross-lingual models compared to high-resource languages? Section 6 notes that "Cultural and contextual subtleties in languages like Urdu may still challenge model accuracy."

- **Open Question 3:** How robust is the proposed active learning strategy in maintaining performance when the class imbalance ratio shifts dramatically from the training data to real-world distributions? The paper mentions that datasets "exhibit class imbalance" and may not "reflect the diversity of real-world discourse," yet claims active learning improves generalization.

## Limitations

- The paper lacks direct ablation evidence isolating the contribution of active learning from the model architecture change
- Implementation details for active learning (initial pool size, batch size, iterations) are underspecified
- Cross-lingual transfer effectiveness is assumed but not rigorously validated for culturally idiomatic hope expressions

## Confidence

**High Confidence:**
- XLM-RoBERTa outperforms Logistic Regression baseline on macro F1 scores across all four languages
- Cross-lingual pretraining enables effective hope speech detection across typologically diverse languages
- Class-weighted loss prevents model collapse to dominant class in severely imbalanced datasets

**Medium Confidence:**
- Active learning accelerates convergence on minority hope categories through uncertainty sampling
- Weighted loss functions improve minority class classification without causing overfitting
- Transformer attention mechanisms capture context-dependent hope speech semantics better than TF-IDF

**Low Confidence:**
- Uncertainty-based active learning provides higher information gain than random sampling for minority classes
- Entropy-based uncertainty sampling reliably identifies informative samples vs. noise
- Class weighting by inverse frequency optimally balances minority class performance

## Next Checks

1. **Ablation Study on Active Learning:** Run XLM-RoBERTa with and without uncertainty-based active learning using identical hyperparameters. Track per-class F1 scores and overall macro F1 across iterations to quantify active learning's specific contribution to minority class performance.

2. **Per-Class Performance Analysis:** Compute detailed per-class F1, precision, and recall for all four categories across all languages. Focus on Realistic Hope and Unrealistic Hope performance to verify class weighting effectiveness and identify potential overfitting patterns.

3. **Uncertainty Metric Validation:** Implement confidence threshold filtering (e.g., only add samples where max class probability <0.8) to the active learning loop. Compare minority class performance with and without thresholding to test whether entropy captures semantic uncertainty vs. distributional outliers.