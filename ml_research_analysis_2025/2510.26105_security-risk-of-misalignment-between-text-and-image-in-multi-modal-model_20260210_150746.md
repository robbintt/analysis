---
ver: rpa2
title: Security Risk of Misalignment between Text and Image in Multi-modal Model
arxiv_id: '2510.26105'
source_url: https://arxiv.org/abs/2510.26105
tags:
- image
- content
- diffusion
- prema
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a misalignment between text and image modalities
  in multi-modal diffusion models, where input images can significantly influence
  generated content regardless of the prompt. To exploit this vulnerability, the authors
  propose Prompt-Restricted Multi-modal Attack (PReMA), a method that manipulates
  generated content by modifying only the input image while keeping the prompt fixed.
---

# Security Risk of Misalignment between Text and Image in Multi-modal Model

## Quick Facts
- **arXiv ID:** 2510.26105
- **Source URL:** https://arxiv.org/abs/2510.26105
- **Reference count:** 40
- **Primary result:** Proposed PReMA method achieves up to 80% attack success rate in generating NSFW content via image-only manipulation in diffusion models

## Executive Summary
This paper identifies a critical misalignment between text and image modalities in multi-modal diffusion models, where input images can significantly influence generated content regardless of the prompt. The authors propose Prompt-Restricted Multi-modal Attack (PReMA), which manipulates generated content by modifying only the input image while keeping the prompt fixed. PReMA achieves high attack success rates (up to 80% on some models) in generating Not-Safe-For-Work content for image inpainting and style transfer tasks across multiple diffusion models, demonstrating effectiveness even with benign prompts and strong transferability across different prompts.

## Method Summary
PReMA is a gradient-based adversarial attack that optimizes the input image to generate specific content while keeping the text prompt fixed. The method uses Adam optimizer to perturb input image pixels over 300 iterations, minimizing the L2 distance between generated content and target images in latent space. The attack incorporates safety checker circumvention by minimizing cosine similarity between generated image features and NSFW concept embeddings. The optimization occurs in sigmoid-transformed space to maintain valid pixel ranges, and the forward diffusion process uses 5 steps for efficiency during attack generation.

## Key Results
- PReMA achieves up to 80% attack success rate on Stable Diffusion v1.5 inpainting with benign prompts
- The attack successfully generates NSFW content across multiple diffusion models including Stable Diffusion and Kandinsky
- Safety checker circumvention rates improve from ~12% to ~74% when incorporating the safety loss term
- The attack demonstrates strong transferability across different benign prompts while maintaining the same NSFW output

## Why This Works (Mechanism)

### Mechanism 1
Modifying the input image latent can override textual guidance in the diffusion process. PReMA optimizes the input image using gradient descent to minimize L2 distance between generated content and target images. The core assumption is that the denoising process is sensitive to initial latent state such that gradient updates significantly alter output trajectory.

### Mechanism 2
The cross-attention mechanism is vulnerable to "image-side" manipulation. The paper theoretically shows that in cross-attention layers, the image-derived query interacts with text-derived keys/values. By solving simplified attention equations, the paper demonstrates that finding specific input latents allows generating target content even with fixed text conditions.

### Mechanism 3
Safety checkers relying on embedding distance can be bypassed by minimizing this distance during optimization. PReMA incorporates a secondary loss term that minimizes cosine similarity between generated image features and NSFW concept embeddings used by safety checkers, pushing adversarial images away from detection thresholds.

## Foundational Learning

- **Concept:** Latent Diffusion Models (LDMs)
  - **Why needed here:** PReMA operates in latent space ($z$), not pixel space. Understanding encoder $E$ and decoder $D$ is required to grasp where attack injects noise.
  - **Quick check question:** Does optimization loop modify pixel values of input image directly, or latents after encoding?

- **Concept:** Cross-Attention Mechanics
  - **Why needed here:** Core vulnerability is defined as misalignment in cross-attention layers ($Q$ vs $K$).
  - **Quick check question:** In this paper's context, does "Query" ($Q$) come from text prompt or image latent?

- **Concept:** Adversarial Optimization
  - **Why needed here:** Attack is structured optimization problem using Adam (gradients), not random noise.
  - **Quick check question:** What is target variable $x_{tar}$ serving in loss function equation $J$?

## Architecture Onboarding

- **Component map:** Input Image $x$ $\to$ Encoder $\to$ Latent $z$ $\to$ [U-Net Denoiser + Text Encoder] $\to$ Decoder $\to$ Output Image
- **Critical path:** Calculation of $\nabla_x J$ (Equation 2). If gradient cannot flow from target NSFW concept back through diffusion denoising steps to input image pixels, attack fails.
- **Design tradeoffs:**
  - **Iterations vs. Quality:** Increasing iterations $T$ improves visual quality but increases computational cost linearly
  - **Transferability vs. White-box:** Attack transfers well within Stable Diffusion architectures but poorly to Kandinsky due to structural differences
- **Failure signatures:**
  - **Low Transferability:** Adversarial images on KDv2.1 perform poorly on SDv1.5 (10-20% ASR vs 50-80% intra-family)
  - **Safety Filter Rejection:** Without $L_{sc}$ loss term, safety checker rejects majority of generated images
- **First 3 experiments:**
  1. Run PReMA on SDv1.5 Inpainting with benign prompt and masked NSFW target to verify ~80% ASR
  2. Replicate Table 5 by running attack with and without safety checker loss term to measure circumvention rate
  3. Generate adversarial image using Prompt A and test if it still generates NSFW content when swapped with Prompt B

## Open Questions the Paper Calls Out

### Open Question 1
How can cross-architecture transferability of adversarial examples be improved to enhance real-world applicability? The authors state effectiveness is "constrained by poor transferability across different model architectures," hindering real-world use. Current transferability is inconsistent and dependent on architectural similarity.

### Open Question 2
Can effective black-box adversarial perturbation techniques be developed for multi-modal diffusion models? The Limitations section explicitly lists "developing black-box adversarial perturbation techniques" as future work. Current PReMA relies on white-box access (gradient-based optimization).

### Open Question 3
How does difficulty of manipulation scale with length of target output sequence? Section 6 notes "difficulty of manipulation escalates for PReMA as length of target output sequence grows." Paper does not characterize how target complexity impacts optimization process.

## Limitations

- The exact NSFW target images and prompts used to generate them are not provided, critical for reproducibility
- Attack demonstrates high success rates within same architecture family but poor transferability to Kandinsky models (10-20% ASR)
- Safety checker circumvention relies on gradient-based optimization of differentiable feature embeddings, which may not generalize to all detection systems

## Confidence

- **High confidence:** Core mechanism of gradient-based optimization on input images to influence generated content
- **Medium confidence:** Cross-attention vulnerability theoretically derived but limited empirical validation
- **Medium confidence:** Safety checker circumvention shows strong controlled results but may not generalize to all systems

## Next Checks

1. Reproduce white-box baseline: Run PReMA on SDv1.5 Inpainting with benign prompt and masked NSFW target to verify claimed ~80% Attack Success Rate, documenting specific target images and prompts
2. Test robustness to stronger safety filters: Apply attack to safety checkers using non-differentiable logic or robust statistics to assess whether gradient-based evasion fails
3. Evaluate on diverse source images: Test attack across broader and more diverse set of source images to determine if success rates vary significantly with input characteristics