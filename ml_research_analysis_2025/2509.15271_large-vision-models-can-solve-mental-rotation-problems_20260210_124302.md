---
ver: rpa2
title: Large Vision Models Can Solve Mental Rotation Problems
arxiv_id: '2509.15271'
source_url: https://arxiv.org/abs/2509.15271
tags:
- rotation
- layer
- vision
- layers
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether modern Vision Transformers (ViTs)\
  \ can implicitly encode 3D viewpoint and structural information needed for mental\
  \ rotation tasks. The authors evaluate four ViT-based models (Google\u2019s ViT,\
  \ CLIP, DINOv2, DINOv3) across seven datasets involving 3D block structures, text,\
  \ and photo-realistic objects, measuring their ability to distinguish rotated objects\
  \ from mirrored ones."
---

# Large Vision Models Can Solve Mental Rotation Problems

## Quick Facts
- **arXiv ID**: 2509.15271
- **Source URL**: https://arxiv.org/abs/2509.15271
- **Reference count**: 0
- **Primary result**: Self-supervised Vision Transformers can implicitly encode 3D viewpoint and structural information needed for mental rotation tasks, with intermediate layers showing stronger geometric sensitivity than final layers.

## Executive Summary
This paper investigates whether modern Vision Transformers can implicitly encode 3D viewpoint and structural information needed for mental rotation tasks. The authors evaluate four ViT-based models (Google's ViT, CLIP, DINOv2, DINOv3) across seven datasets involving 3D block structures, text, and photo-realistic objects, measuring their ability to distinguish rotated objects from mirrored ones. By probing layer-wise embeddings, they find that self-supervised models outperform supervised ones, with intermediate layers showing stronger geometric sensitivity than final layers. Performance degrades with increasing rotation complexity and occlusion, mirroring human reaction times. Notably, only DINOv3 (in its largest size) succeeds on fully unconstrained rotations.

## Method Summary
The study uses pre-trained Vision Transformers and extracts patch-averaged embeddings (not CLS tokens) from each layer. These embeddings are fed into a Siamese MLP probe that classifies whether image pairs show the same object under rotation or a mirrored counterpart. The probe consists of shared MLPs (R^d → R^128, batch norm, ReLU, ℓ2-normalize) with elementwise absolute difference fed to a logistic head. Training uses AdamW with 10^-3 learning rate, batch size 256, 15-epoch warmup plus cosine annealing, max 200 epochs, and early stopping with 50-epoch patience. Standardize embeddings using training split statistics. Seven datasets are tested: Shepard-Metzler (±0° and Free rotation), Text (Normal/Random/Pseudo), and Photo-Realistic (90° and 30° elevation).

## Key Results
- Self-supervised ViTs (CLIP, DINOv2, DINOv3) outperform supervised ViTs on mental rotation tasks
- Intermediate transformer layers (typically 40-70% depth) outperform final layers for geometric reasoning
- Only DINOv3 Huge succeeds on fully unconstrained ("Free") mental rotation tasks
- Performance degrades with increasing rotation complexity and occlusion, mirroring human reaction times

## Why This Works (Mechanism)

### Mechanism 1: Training Objective Determines Pose Preservation
Self-supervised ViTs preserve geometric structure better than supervised ViTs because their training objectives do not enforce class-level invariance. Supervised ViTs optimize for categorical discrimination, encouraging invariance to viewpoint and pose, while self-supervised models use contrastive or teacher-student objectives that preserve visual cues needed for alignment without collapsing pose information.

### Mechanism 2: Intermediate Layers Encode Geometric Equivariance Before Semantic Abstraction
Mental rotation relies on intermediate transformer layers where rotation-angle structure is explicitly organized in embedding space. ViTs exhibit progressive layer specialization—early layers capture low-level primitives, intermediate layers encode geometric/relational structure, and final layers abstract toward semantics. Pose information peaks mid-network and degrades toward the output.

### Mechanism 3: Scale and Training Recipe Enable Unconstrained Rotations
Only the largest self-supervised ViT (DINOv3 Huge) can solve fully unconstrained ("Free") mental rotation, implying capacity and training data scale are prerequisite for generalizing to novel pose configurations. DINOv3's "Gram anchoring" stabilizes dense features during large-scale training, preventing collapse while maintaining patch-level geometric fidelity.

## Foundational Learning

- **Concept: Equivariance vs. Invariance**
  - Why needed here: Mental rotation requires equivariance (pose must be represented), not invariance (pose discarded). The paper explicitly frames this as the core representational requirement.
  - Quick check question: Can you explain why rotation-invariant features fail to distinguish a rotated object from its mirror image?

- **Concept: Self-Supervised Learning Paradigms (Contrastive, Teacher-Student)**
  - Why needed here: The performance gap between supervised and self-supervised ViTs hinges on their training objectives; understanding CLIP's contrastive alignment and DINO's teacher-student framework is essential.
  - Quick check question: How does a teacher-student framework encourage augmentation-invariant representations without explicit labels?

- **Concept: Vision Transformer Layer Structure (Patch Tokens, CLS Token)**
  - Why needed here: The paper extracts patch-averaged embeddings per layer and shows CLS tokens perform worse. Understanding ViT tokenization is prerequisite to replicating the pipeline.
  - Quick check question: Why might spatially averaged patch embeddings preserve more geometric information than the CLS token?

## Architecture Onboarding

- **Component map**: Image pairs → ViT encoder → Layer-wise patch-averaged embeddings → Siamese MLP probe → Binary classification (rotation vs. mirror)

- **Critical path**: Select model family and size → Extract embeddings from layers 8-18 → Use patch-averaged embeddings, not CLS tokens → Train Siamese probe with stratified 10-fold cross-validation

- **Design tradeoffs**: Supervised vs. self-supervised (pose preservation), Layer selection (earlier for simple, deeper intermediate for complex), MAE (fails entirely), Patch-averaged vs. CLS token (patch-averaged significantly better)

- **Failure signatures**: Final-layer embeddings yield near-chance accuracy, CLS token embeddings underperform patch-averaged, MAE-style models fail across all layers, Supervised ViT shows weak performance even at optimal layers

- **First 3 experiments**: 1) Replicate Figure 3 for DINOv2 Base: probe each layer on Shepard-Metzler ±0° and Free; verify intermediate-layer peak. 2) Ablate embedding type: Compare patch-averaged vs. CLS token vs. pooled output on Photo-Realistic 90°; quantify accuracy drop. 3) Difficulty scaling: Replicate Figure 4 by varying rotation angle constraints (±0°, ±10°, ±20°, Free) and plot accuracy vs. layer; confirm degradation mirrors human reaction-time patterns.

## Open Questions the Paper Calls Out

### Open Question 1
How can vision transformer architectures be modified to preserve geometric structure and pose information into the final embedding layers, rather than discarding it in favor of semantic invariance? The conclusion states that "pose information is strongest in intermediate layers but often lost in final embeddings" and explicitly suggests "opportunities for improved architectures."

### Open Question 2
Why do reconstruction-based self-supervised methods (e.g., MAE) fail to capture the equivariant geometric structure required for mental rotation compared to contrastive methods? The authors note that MAE "failed to solve the mental rotation problem at any layer," contrasting sharply with the success of teacher-student frameworks (DINO) and contrastive models (CLIP), but provide no causal explanation.

### Open Question 3
Does the capacity to solve unconstrained ("Free") mental rotation tasks emerge primarily from model scale (parameters/data) or from specific training dynamics like DINOv3's "Gram anchoring"? It is unclear if the success is due to the 7B parameter scale, the 1.7B image dataset, or the specific stabilization provided by the Gram anchoring loss unique to DINOv3.

## Limitations

- The synthetic nature of Shepard-Metzler blocks and text-based stimuli may not fully capture real-world mental rotation complexity
- The study relies on a relatively simple Siamese MLP probe that might underestimate the true geometric encoding present in the embeddings
- The finding that only DINOv3 Huge solves unconstrained rotations raises questions about whether this reflects architectural superiority or merely scale effects

## Confidence

- **High Confidence**: Self-supervised ViTs preserve geometric structure better than supervised ViTs; Intermediate transformer layers outperform final layers for mental rotation; Performance degrades with increasing rotation complexity and occlusion
- **Medium Confidence**: DINOv3's Gram anchoring specifically enables unconstrained rotation solving; The layer-wise emergence pattern (plateau followed by decline) is universal across models; MAE-style models fundamentally cannot capture geometric equivariance
- **Low Confidence**: The exact computational mechanism by which Gram anchoring stabilizes dense features; Whether the observed layer patterns reflect architectural constraints or optimization dynamics; The relationship between human reaction-time patterns and model accuracy degradation

## Next Checks

1. Test whether the layer-wise performance pattern holds when using alternative probe architectures (e.g., attention-based probing, contrastive probing) to verify that the Siamese MLP isn't limiting detection of geometric structure.

2. Train a smaller DINOv3 variant with Gram anchoring enabled to determine whether scale or the anchoring mechanism is primarily responsible for unconstrained rotation performance.

3. Evaluate the best-performing models on mental rotation tasks using natural image datasets (e.g., rotated objects in COCO or ImageNet) to assess real-world applicability beyond synthetic and rendered stimuli.