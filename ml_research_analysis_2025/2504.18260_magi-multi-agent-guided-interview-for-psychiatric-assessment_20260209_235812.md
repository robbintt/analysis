---
ver: rpa2
title: 'MAGI: Multi-Agent Guided Interview for Psychiatric Assessment'
arxiv_id: '2504.18260'
source_url: https://arxiv.org/abs/2504.18260
tags:
- symptom
- psycot
- anxiety
- mark
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAGI introduces a multi-agent framework that transforms the MINI
  structured psychiatric interview into automated workflows, addressing the challenge
  of aligning LLM-based mental health assessment with clinical diagnostic protocols.
  The system uses four specialized agents to manage interview navigation, generate
  adaptive questions, validate symptom responses, and produce transparent DSM-5 compliant
  diagnoses through Psychometric Chain-of-Thought reasoning.
---

# MAGI: Multi-Agent Guided Interview for Psychiatric Assessment

## Quick Facts
- arXiv ID: 2504.18260
- Source URL: https://arxiv.org/abs/2504.18260
- Reference count: 40
- Key outcome: 32% improvement in diagnostic agreement with expert clinicians compared to single-agent LLM baselines

## Executive Summary
MAGI introduces a multi-agent framework that transforms the MINI structured psychiatric interview into automated workflows, addressing the challenge of aligning LLM-based mental health assessment with clinical diagnostic protocols. The system uses four specialized agents to manage interview navigation, generate adaptive questions, validate symptom responses, and produce transparent DSM-5 compliant diagnoses through Psychometric Chain-of-Thought reasoning. Evaluation on 1,002 real-world cases showed MAGI achieved 32% improvement in diagnostic agreement with expert clinicians compared to single-agent LLM baselines.

## Method Summary
MAGI employs four specialized agents: Navigation (MINI tree traversal), Question (probe/explain/empathy), Judgment (response validation with forced-choice after ~5 ambiguous turns), and Diagnosis (PsyCoT: symptom anchoring, syndromal validation, evidence binding). The system was evaluated on 1,002 interview sessions using simulated participants constructed via LLMs, with dual-expert annotations by licensed psychologists (ICC 0.78–0.87, κ > 0.85). The framework supports multiple LLMs including GPT-4o, Claude-3.5-sonnet, GLM-Zero, and DeepSeek-R1.

## Key Results
- 32% improvement in diagnostic agreement with expert clinicians compared to single-agent LLM baselines
- Cohen's kappa values reached 0.839-0.942 for suicide risk detection and 0.615-0.616 for depression assessment
- Highest dialogue quality scores (Relevance, Accuracy, Completeness, Guidance on 5-point Likert) among all baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating navigation control from language generation reduces protocol violations in structured clinical interviews.
- Mechanism: The navigation agent enforces MINI's branching logic as a finite-state controller, compelling the dialogue to remain on diagnostic nodes until criteria are satisfied. This prevents the question agent's LLM from taking "creative detours" that could skip required symptom assessments.
- Core assumption: LLMs without external state management will deviate from structured protocols when encountering ambiguous or emotionally complex responses.
- Evidence anchors: Abstract states "navigation agent adhering to the MINI's branching structure"; section 3.1 explains separation prevents "hallucinations"; MoodAngels paper similarly uses multi-agent separation.

### Mechanism 2
- Claim: Psychometric Chain-of-Thought (PsyCoT) improves diagnostic accuracy by forcing explicit mapping from utterances to DSM-5 criteria through intermediate validation steps.
- Mechanism: PsyCoT decomposes diagnosis into three phases: symptom anchoring (verifying temporal thresholds and exclusionary conditions), syndromal validation (confirming symptom counts and core symptom inclusion), and evidence binding (creating auditable links between dialogue segments and diagnostic codes).
- Core assumption: LLMs default to heuristic pattern matching rather than systematic criterion-by-criterion evaluation when diagnosing from free text.
- Evidence anchors: Abstract mentions "PsyCoT traces that explicitly map symptoms to clinical criteria"; Table 2 shows PsyCoT improved suicide risk κ from 0.259–0.427 to 0.839–0.942; MentalSeek-Dx paper corroborates structured reasoning benefits.

### Mechanism 3
- Claim: Adaptive strategy switching (probing → explaining → empathy) maintains participant engagement without sacrificing diagnostic completeness.
- Mechanism: The question agent monitors contextual signals (confusion markers, distress cues, topic shifts) and dynamically selects conversational strategies. Empathetic responses stabilize engagement; explanation clarifies ambiguous constructs; probing extracts required evidence.
- Core assumption: Rigid question templates cause disengagement and incomplete symptom collection, especially when participants are emotionally distressed.
- Evidence anchors: Section 3.2 describes strategy activation based on contextual inconsistencies and emotional distress; Table 1 shows MAGI achieved highest Guidance (3.460) and Completeness (3.497) scores; WiseMind paper supports balancing "instrumental and humanistic benefits."

## Foundational Learning

- Concept: MINI (Mini International Neuropsychiatric Interview) structured diagnostic protocol
  - Why needed here: MAGI's architecture directly operationalizes MINI's decision-tree logic. Without understanding branching criteria and skip patterns, you cannot debug navigation agent behavior.
  - Quick check question: Given a participant denies depressed mood (node A1a), which subsequent nodes should the system skip versus probe?

- Concept: DSM-5 diagnostic criteria (thresholds, core symptoms, exclusion rules)
  - Why needed here: PsyCoT's symptom anchoring phase requires encoding these criteria into validation logic. Misunderstanding "≥5 symptoms over 2 weeks including core symptom" will cause false negatives.
  - Quick check question: For Major Depressive Episode, what two symptoms are "core" and must be present for diagnosis?

- Concept: Multi-agent coordination patterns (shared state, message passing, forced-choice constraints)
  - Why needed here: The four agents interoperate through constrained data flows. Understanding how judgment outputs feed navigation state transitions is essential for debugging interview stalls.
  - Quick check question: When the judgment agent returns "ambiguous," what action should the navigation agent trigger in the question agent?

## Architecture Onboarding

- Component map: Participant Input → Question Agent (utterance generation) → Judgment Agent (criteria validation) → Navigation Agent (state transition) → (loop until node complete) → Diagnosis Agent (PsyCoT synthesis)

- Critical path: Judgment agent's forced-choice mechanism (triggered after 5 unproductive turns) → this is the fallback that guarantees diagnostic progression when participants resist direct answers

- Design tradeoffs:
  - Strict protocol adherence (navigation) vs. conversational naturalness (question agent)
  - Empathy generation vs. evidence collection efficiency
  - Paper acknowledges accuracy tradeoff: "judgment agent occasionally prioritizes participant comfort over forcing definitive answers"

- Failure signatures:
  - Infinite loop: Participant gives consistently ambiguous responses; judgment never satisfies node criteria; navigation never transitions
  - Premature termination: Navigation agent skips critical nodes due to misinterpreted judgment outputs
  - Disconnected reasoning: PsyCoT produces diagnosis without traceable evidence links (audit path broken)

- First 3 experiments:
  1. Replicate the judgment agent's decision thresholds: Test whether "I wake up 3-4 times every night" correctly maps to insomnia criterion across multiple LLMs
  2. Stress-test navigation state transitions: Simulate participants who refuse to answer suicide screening questions—verify forced-choice mechanism activates appropriately
  3. Validate PsyCoT audit trails: For 10 sampled diagnoses, manually trace whether each cited symptom in PsyCoT output has corresponding dialogue evidence

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on simulated participants generated by LLMs rather than actual clinical interviews, which may not capture full complexity of human responses including cultural factors and non-verbal cues
- The system's forced-choice mechanism after ~5 ambiguous turns could potentially override genuine participant uncertainty
- While κ values are high for suicide risk (0.839-0.942), depression assessment shows more modest agreement (0.615-0.616), suggesting differential performance across diagnostic domains

## Confidence

- **High confidence**: Multi-agent separation preventing protocol deviations, PsyCoT's explicit evidence mapping mechanism, and the 32% improvement over single-agent baselines
- **Medium confidence**: The forced-choice mechanism's clinical appropriateness and whether simulated participants accurately represent real-world interview dynamics
- **Low confidence**: Long-term clinical deployment safety and whether high κ values translate to improved patient outcomes

## Next Checks
1. Validate forced-choice appropriateness: Test whether the system's forced-choice mechanism after 5 ambiguous turns respects participant autonomy by comparing with clinician judgments on when to override participant uncertainty
2. Real-world participant testing: Deploy MAGI with actual clinical populations to assess whether κ values hold when participants provide emotionally complex, culturally diverse, or non-verbal responses
3. Audit trail completeness verification: For 50 randomly selected diagnoses, verify that every DSM-5 criterion mapping in PsyCoT can be traced to specific dialogue segments without gaps or hallucinations