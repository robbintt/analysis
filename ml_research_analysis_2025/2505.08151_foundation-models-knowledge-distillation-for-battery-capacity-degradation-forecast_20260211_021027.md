---
ver: rpa2
title: Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast
arxiv_id: '2505.08151'
source_url: https://arxiv.org/abs/2505.08151
tags:
- capacity
- battery
- degradation
- distillation
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a foundation model-based approach for battery
  capacity degradation forecasting, addressing the challenge of generalizing across
  battery scales and operating conditions. The authors fine-tune the Timer time-series
  foundation model using Low-Rank Adaptation (LoRA) on open-source small-cell datasets
  to create Battery-Timer, then apply knowledge distillation to compress this model
  into lightweight expert forecasters suitable for embedded deployment.
---

# Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast

## Quick Facts
- **arXiv ID**: 2505.08151
- **Source URL**: https://arxiv.org/abs/2505.08151
- **Reference count**: 33
- **Key outcome**: Foundation model-based approach for battery capacity degradation forecasting with zero-shot transfer from small cells to large energy-storage systems

## Executive Summary
This paper introduces a foundation model-based approach for battery capacity degradation forecasting, addressing the challenge of generalizing across battery scales and operating conditions. The authors fine-tune the Timer time-series foundation model using Low-Rank Adaptation (LoRA) on open-source small-cell datasets to create Battery-Timer, then apply knowledge distillation to compress this model into lightweight expert forecasters suitable for embedded deployment. Battery-Timer consistently outperforms specialized expert models in zero-shot transfer from small cells to large energy-storage systems. Knowledge distillation improves cross-protocol generalization while substantially reducing computational overhead, demonstrating a practical path to deployable cross-scale degradation forecasting through combining foundation models with targeted distillation.

## Method Summary
The method combines three key components: (1) fine-tuning the Timer TSFM with LoRA on small-cell datasets to create Battery-Timer while preserving pre-trained temporal priors, (2) incorporating a trend penalty to enforce monotonic degradation consistent with battery physics, and (3) applying response-based knowledge distillation to transfer learned patterns to lightweight student models for embedded deployment. The approach uses 220,153 cycles from four open-source datasets to train the teacher model, then distills knowledge to students tested on an independent large-cell dataset. The framework addresses the challenge of zero-shot cross-scale generalization while maintaining physical constraints on degradation trajectories.

## Key Results
- Battery-Timer consistently outperforms specialized expert models in zero-shot transfer from small cells to large energy-storage systems
- Knowledge distillation improves cross-protocol generalization, with distilled students outperforming vanilla supervised models on CC protocols despite being distilled on CCCV
- The approach substantially reduces computational overhead while maintaining forecasting accuracy, demonstrating practical deployment potential

## Why This Works (Mechanism)

### Mechanism 1: Subspace Alignment via Low-Rank Adaptation (LoRA)
Fine-tuning with LoRA aligns generic time-series priors to battery degradation dynamics while preserving the pre-trained knowledge, preventing catastrophic forgetting. Instead of updating all Transformer weights, LoRA injects trainable low-rank matrices into the attention projection layers, restricting optimization to a low-dimensional subspace that adapts the foundation model's vocabulary to battery capacity trajectories without overwriting temporal reasoning learned during pre-training.

### Mechanism 2: Dark Knowledge Transfer via Soft Labels
Knowledge distillation transfers "dark knowledge" (inter-class similarities or output smoothness) from the Teacher to the Student, improving the Student's robustness to distribution shift more effectively than hard-label training. The student model minimizes a hybrid loss combining MSE and KL-Divergence, where soft targets contain information about the relative difficulty of future timesteps and the uncertainty of the trajectory, teaching the student not just what the value is, but how the temporal structure evolves.

### Mechanism 3: Physics-Informed Monotonicity Constraint
Explicitly penalizing upward capacity drift acts as a regularization mechanism that aligns forecasts with the physical reality of battery degradation. The loss function is augmented with a trend penalty that discourages predictions where end capacity is higher than start capacity, biasing the model toward monotonic decay and preventing the generation of physically impossible "self-healing" trajectories caused by noise in training data.

## Foundational Learning

- **Time-Series Foundation Models (TSFMs)**: Large pre-trained Transformers on generic numeric time-series to learn universal temporal priors before seeing any battery data. Quick check: How does Timer's input tokenization differ from standard LLMs like GPT-4? (Answer: Timer tokenizes segments of numeric vectors rather than discrete words.)

- **Zero-Shot Generalization**: Predicting capacity for large battery packs using a model fine-tuned only on small cells, without further training on the target domain. Quick check: Does "Zero-Shot" mean predicting without any data, or without any re-training on the target domain? (Answer: Without re-training; the model uses the specific battery's history as context.)

- **Response-Based Knowledge Distillation**: Learning from the teacher's soft predictions rather than just ground truth points. Quick check: Why use a temperature parameter in the distillation loss? (Answer: To soften the probability distribution, revealing more "dark knowledge" about the uncertainty of minor classes/points.)

## Architecture Onboarding

- **Component map**: Raw charge-discharge files -> Cycle-indexed capacity sequences (L=96, H=96) -> Timer (Decoder-only Transformer) + LoRA Adapters -> Teacher predictions -> Student models (PaiFilter/DLinear/etc.) -> Lightweight forecasters for BMS

- **Critical path**: 1) Pre-processing: Aggregating raw charge-discharge files into cycle-indexed capacity sequences with 96-step sliding windows; 2) Adapter Training: Freeze Timer, train LoRA weights on open-source data with Trend Penalty; 3) Distillation: Train Student models on SJTUIE dataset using predictions from fine-tuned Timer as soft targets

- **Design tradeoffs**: Teacher Capacity vs. Efficiency (84M Timer is accurate but too heavy for BMS); Trend Penalty (high λ ensures monotonicity but increases RMSE - optimal at 0.02); LoRA Rank (increasing beyond 8 shows minor improvements but increases trainable parameters)

- **Failure signatures**: Horizon Collapse (predictions beyond 96 steps rapidly degrade into linear/constant mean value - do not use rolling autoregressive inference for long horizons); Architecture Mismatch (LightTS showed performance drops after distillation, implying some architectures struggle to mimic Teacher's output distribution)

- **First 3 experiments**: 1) LoRA Injection Ablation: Test injecting LoRA into q-proj only vs. q,k,v-proj to verify attention adaptation effectiveness; 2) Cross-Protocol Robustness: Distill student on CCCV data and test on CC data - verify if distilled student beats vanilla student trained directly on CC; 3) LOBO (Leave-One-Battery-Out): Train Teacher without WZU dataset and test on SJTUIE to verify contribution of diverse pre-training data vs. specific chemistry matching

## Open Questions the Paper Calls Out
1. Can endowing time-series foundation models with an explicit termination signal and LLM-like autoregressive interface prevent the error accumulation and over-smoothing observed in long-horizon rolling forecasts?

2. Why does knowledge distillation fail to improve certain lightweight architectures like LightTS, and can this architectural incompatibility be predicted?

3. How can exogenous operational variables (e.g., temperature, C-rate) be integrated into the TSFM framework without breaking the zero-shot cross-scale transfer capability?

## Limitations
- Distillation hyperparameter selection (temperature T, soft loss weight α) is not specified in the paper, which could significantly impact student model performance
- Cross-protocol generalization claims rely on a single distillation run on CCCV data tested on CC data, with limited ablation across different source-target protocol combinations
- The paper assumes monotonic degradation applies universally, potentially overlooking chemistries with significant recovery effects or rest periods that could trigger capacity increases

## Confidence
- **High Confidence**: Foundation model + LoRA fine-tuning approach is well-supported by literature on TSFM adaptation and catastrophic forgetting prevention
- **Medium Confidence**: Knowledge distillation mechanism is theoretically sound and shows promising cross-protocol results, but lack of specified hyperparameters and limited protocol ablation studies reduces confidence in generalizability
- **Low Confidence**: Claim that Battery-Timer consistently outperforms specialized expert models in zero-shot transfer is based on a single large-scale test case without broader validation across multiple target datasets

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary distillation temperature T (e.g., 1.0, 2.0, 5.0) and soft loss weight α (e.g., 0.3, 0.5, 0.7) to determine their impact on cross-protocol generalization and identify optimal values for different target protocols

2. **Cross-Protocol Ablation Study**: Extend the current CCCV→CC experiment to include multiple protocol pairs (CC→CCCV, CV→CC, etc.) and evaluate whether distilled students consistently outperform students trained directly on target protocol data across all combinations

3. **Zero-Shot Transfer Robustness**: Test the zero-shot generalization capability on additional unseen battery datasets with different chemistries, form factors, and operating conditions beyond the SJTUIE dataset to validate the claimed universal applicability of the foundation model approach