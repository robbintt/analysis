---
ver: rpa2
title: The Effect of Architecture During Continual Learning
arxiv_id: '2601.19766'
source_url: https://arxiv.org/abs/2601.19766
tags:
- architecture
- learning
- task
- tasks
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  by jointly optimizing neural network architecture and weights. The authors introduce
  a Sobolev space framework to model the coupling between architecture and weights,
  proving that weight updates alone are insufficient to mitigate forgetting under
  distribution shifts.
---

# The Effect of Architecture During Continual Learning

## Quick Facts
- arXiv ID: 2601.19766
- Source URL: https://arxiv.org/abs/2601.19766
- Authors: Allyson Hahn; Krishnan Raghavan
- Reference count: 39
- Primary result: Joint optimization of architecture and weights achieves up to two orders of magnitude improvement in continual learning performance over static architectures

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by jointly optimizing neural network architecture and weights rather than treating architecture as fixed. The authors introduce a Sobolev space framework to model the coupling between architecture and weights, proving mathematically that weight updates alone are insufficient to mitigate forgetting under distribution shifts. They formulate continual learning as a bilevel optimization problem where the upper level searches for optimal architecture and the lower level computes optimal weights via dynamic programming. The approach uses a derivative-free direct search for architecture optimization and a low-rank transfer mechanism to map knowledge across architectures of different dimensions.

## Method Summary
The method employs a bilevel optimization framework where architecture (ψ) is optimized in the upper level using neighborhood directional direct search (NDDS), while weights (w) are optimized in the lower level using standard continual learning techniques with experience replay. When architecture changes are triggered, a low-rank transfer mechanism V=AwB^T maps knowledge from the old architecture to the new one by training projection matrices A and B while freezing the original weights. The system uses Hamiltonian gradient descent with a replay buffer containing 200K samples (10% recent, 80% older, 10% random) and triggers architecture search when the loss increases by a threshold of 10%. The approach is validated across regression, classification, and graph neural network tasks.

## Key Results
- Up to two orders of magnitude improvement in performance compared to static architecture approaches
- Significant reduction in catastrophic forgetting across sequential tasks
- Enhanced robustness to noise, particularly when tasks introduce significant distribution shifts
- Superior performance in scenarios where task distributions differ substantially

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weight-only optimization is insufficient to mitigate catastrophic forgetting when task distributions shift significantly.
- **Mechanism:** The paper models the interaction between weights and architecture in a Sobolev space W^{k,p}. It proves that for a continual learning solution to exist, the forgetting loss J must satisfy absolute continuity. If tasks are dissimilar (measured by the symmetric difference of data distributions), weight updates Δw alone cannot bound the variation in J (Theorem 8, Lemma 10). Changing the architecture ψ introduces an additional control term (architecture gradients) that can restore the boundedness of the total variance (Theorem 12).
- **Core assumption:** The loss function ℓ is bounded and Lipschitz continuous, and tasks are sampled from a measurable space where the symmetric difference μ(X^{(t)} △ X^{(t+1)}) represents distribution shift.
- **Evidence anchors:**
  - [abstract]: "...proving that weight updates alone are insufficient to mitigate catastrophic forgetting under distribution shifts."
  - [section 4]: Theorem 12 states that if Σ(J(τ) - J(τ+1)) ≥ ε for fixed ψ, choosing a new ψ(t) can force the sum < ε.
  - [corpus]: Related work supports the difficulty of weight-only updates; "Mechanistic Analysis of Catastrophic Forgetting..." (arXiv:2601.18699) notes that fine-tuning interferes with capabilities.
- **Break condition:** If the data distribution shift between tasks is negligible (symmetric difference ≈ 0), the necessity for architecture change diminishes, and the benefit over standard weight updates may vanish.

### Mechanism 2
- **Claim:** A low-rank transfer mechanism (V = AWB^T) allows knowledge transfer between architectures of mismatched dimensions without random re-initialization.
- **Mechanism:** When architecture search identifies a new layer size, the old weight matrix W (r × s) cannot directly populate the new weight matrix V (a × b). The method inserts trainable projection matrices A (a × r) and B (b × s) to compute V. Crucially, A and B are trained while W is frozen to minimize the reconstruction loss on the current task before full training resumes (Algorithm 2, Step 5). This preserves the "knowledge" encoded in W while projecting it into a higher-dimensional space.
- **Core assumption:** The knowledge required for previous tasks is linearly compressible or projectable via low-rank matrices such that the frozen W remains a valid latent representation.
- **Evidence anchors:**
  - [abstract]: "...develop a low-rank transfer mechanism to map knowledge across architectures of mismatched dimensions."
  - [section 5.2]: Eq. (13) defines the update w(t+Δt) = A*(t)w(t)B*(t)^T.
  - [corpus]: "Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation" (arXiv:2505.11998) supports the efficacy of low-rank methods in CL contexts.
- **Break condition:** If the rank of the transformation required to map old knowledge to the new capacity exceeds the capacity of the A and B matrices, or if the optimal architecture change is structural (e.g., changing activation functions, non-linear topology) rather than dimensional, this specific projection mechanism fails.

### Mechanism 3
- **Claim:** A derivative-free direct search (NDDS) efficiently approximates the optimal architecture by treating architecture as a discrete control variable.
- **Mechanism:** Since architecture ψ is discrete (e.g., number of neurons), gradients w.r.t ψ do not exist in the classical sense. The NDDS algorithm polls "neighboring" architectures (e.g., ±k neurons) and selects the direction that minimizes the expected loss. This mimics a gradient descent in the architecture space Ψ to solve the upper-level bilevel optimization problem.
- **Core assumption:** The optimal architecture lies in the neighborhood of the current architecture (local search sufficiency) and that evaluating a limited number of poll points is computationally feasible.
- **Evidence anchors:**
  - [section 5.1]: "We call the search method used a neighborhood directional direct-search (NDDS)... mimic the weak derivatives available to us..."
  - [section 7.3.2]: Condition C3 isolates this search mechanism.
  - [corpus]: Corpus signals regarding architecture search are weak for this specific NDDS method, but "SCALE: Upscaled Continual Learning..." (arXiv:2511.03270) discusses structured upscaling.
- **Break condition:** If the optimal architecture is vastly different from the initialization (requiring global search), NDDS will get stuck in local minima.

## Foundational Learning

- **Concept: Sobolev Spaces & Weak Derivatives**
  - **Why needed here:** The paper formalizes neural networks not just as parameterized functions, but as elements in a Sobolev space W^{k,p}. This is critical for deriving the theoretical guarantee that weights alone are insufficient (because it allows the authors to define the "forgetting loss" over a function space and use calculus of variations to prove divergence).
  - **Quick check question:** Can you explain why the authors use the Sobolev norm instead of just the Euclidean norm for weights to measure the loss?

- **Concept: Set Symmetric Difference (A △ B)**
  - **Why needed here:** This is the mathematical proxy for "distribution shift." The proofs of existence for the CL solution (Lemma 6, Theorem 8) rely on the assumption that the measure of the symmetric difference between consecutive tasks' data defines the "dissimilarity" that drives forgetting.
  - **Quick check question:** If the symmetric difference between two datasets is zero, what does Theorem 8 imply about the need for architecture changes?

- **Concept: Bilevel Optimization**
  - **Why needed here:** The method separates the learning into two loops: the upper level searches for ψ (architecture) and the lower level searches for w (weights). Understanding this hierarchy is necessary to distinguish between the NDDS step (outer loop) and the standard training/transfer step (inner loop).
  - **Quick check question:** In the bilevel formulation, which variable (ψ or w) is considered the "control" at the upper level?

## Architecture Onboarding

- **Component map:** Current weights w^(t) -> NDDS Architecture Search -> New Architecture ψ* -> Low-Rank Transfer (V=AwB^T) -> Updated weights w^(t+1) -> Standard CL Training

- **Critical path:** The "Transfer" step (Step 2) is the most critical. If A and B are trained for too few epochs, the projection does not smooth the loss landscape, leading to spikes in forgetting (as seen in the MNIST Experiment 2 analysis).

- **Design tradeoffs:**
  - **C3 (Search only) vs. C4 (Search + Transfer):** C3 randomly reinitializes weights upon architecture change. C4 transfers them. C3 is faster per iteration but destroys past knowledge; C4 is computationally heavier due to the A/B training phase but maintains performance.
  - **Search Step Size:** A large step in NDDS may skip over optimal architectures; a small step increases compute time.

- **Failure signatures:**
  - **Sudden Accuracy Drop:** If A/B matrices are not trained sufficiently (insufficient epochs in Step 5 of Algorithm 2), the model experiences a "sharp change" in loss, effectively re-initializing the network behavior. See Section 7.5.4 "Analysis of Reduced Accuracy."
  - **Stagnation:** If the architecture search (NDDS) threshold is set too low or the neighborhood is too small, the architecture may fail to expand capacity to meet the needs of difficult tasks (e.g., MNIST rotations > 90 degrees).

- **First 3 experiments:**
  1. **Regression Baseline (Sine Waves):** Replicate the 2-task experiment (Figure 8) to verify the "33% improvement" claim. Compare Condition C1 (Static) vs. C4 (AWB Full) to ensure the pipeline reduces the Hamiltonian loss.
  2. **Noise Robustness:** Run the "Noisy Sine" experiment (Figure 10) to test the mechanism on dissimilar tasks (μ(A △ B) ≥ δ). Verify that C4 maintains a lower gradient norm and higher BWT (Backward Transfer) than C1/C2.
  3. **Architecture Scaling (MNIST):** Replicate the MNIST experiment with attention to the A/B training epoch count. Test the "Failure Signature" by running C4 with 10 vs. 150 epochs for A/B training to observe the accuracy drop on difficult rotations (Figure 16).

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the AWB transfer mechanism scale to large language models with billions of parameters, where full architecture search and weight transfer become computationally prohibitive?
- **Open Question 2:** What is the principled method for determining the optimal number of A/B training epochs before architecture transitions?
- **Open Question 3:** Can the low-rank transfer mechanism (V=AwB^T) be extended to architectural changes beyond neuron count, such as adding/removing layers, changing activation functions, or modifying connectivity patterns?
- **Open Question 4:** Under what conditions does the NDDS architecture search converge to a globally optimal architecture versus a local optimum?

## Limitations

- Theoretical framework assumes distribution shifts are the primary driver of catastrophic forgetting, which may not capture all forgetting mechanisms in practice.
- Low-rank transfer mechanism assumes linear compressibility of knowledge, potentially limiting effectiveness for non-linear architectural changes.
- NDDS architecture search uses a neighborhood-based approach that may converge to local optima when optimal architectures differ substantially from initial configurations.

## Confidence

- **High Confidence:** The empirical superiority of joint architecture-weight optimization over static approaches (up to 2 orders of magnitude improvement) is well-supported by experimental results across multiple domains.
- **Medium Confidence:** The theoretical proofs showing weight-only optimization is insufficient under distribution shifts are mathematically sound but depend on specific assumptions about loss functions and task dissimilarity measures.
- **Medium Confidence:** The AWB transfer mechanism's effectiveness relies on the assumption that knowledge is linearly projectable via low-rank matrices, which is supported by related work but not extensively validated within this paper.

## Next Checks

1. **Noise Sensitivity Analysis:** Systematically vary noise levels in the noisy sine experiment to determine the threshold where architecture changes become necessary versus when weight-only optimization suffices.

2. **Architecture Transfer Capacity:** Test the low-rank transfer mechanism with architectures having fundamentally different structures (e.g., CNN to FNN, or different activation functions) to identify when the linear projection assumption breaks down.

3. **NDDS Search Efficiency:** Implement and compare the NDDS architecture search against alternative approaches (random search, genetic algorithms) on the same tasks to quantify whether the neighborhood-based approach achieves similar or better architectures with reduced computational cost.