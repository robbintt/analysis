---
ver: rpa2
title: Directional Ensemble Aggregation for Actor-Critics
arxiv_id: '2507.23501'
source_url: https://arxiv.org/abs/2507.23501
tags:
- learning
- ensemble
- critic
- aggregation
- disagreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the overestimation bias problem in actor-critic\
  \ methods for continuous control by proposing Directional Ensemble Aggregation (DEA).\
  \ DEA introduces two fully learnable directional parameters - one for the critic\
  \ (\u03BA\u0304) and one for the actor (\u03BA) - that are updated using ensemble\
  \ disagreement-weighted Bellman errors."
---

# Directional Ensemble Aggregation for Actor-Critics

## Quick Facts
- arXiv ID: 2507.23501
- Source URL: https://arxiv.org/abs/2507.23501
- Reference count: 40
- Primary result: DEA outperforms static ensemble aggregation baselines across MuJoCo environments and learning regimes

## Executive Summary
Directional Ensemble Aggregation (DEA) addresses overestimation bias in actor-critic methods by introducing fully learnable directional parameters κ̄ and κ for critic and actor aggregation respectively. These parameters are updated using ensemble disagreement-weighted Bellman errors, enabling adaptive conservatism and exploration. Experiments across MuJoCo environments show DEA achieves top average ranks in final return, IQM, and AULC metrics while outperforming static baselines like SAC and REDQ across both interactive and sample-efficient learning regimes.

## Method Summary
DEA modifies SAC by adding two scalar parameters (κ̄ for critic, κ for actor) that weight ensemble disagreement in aggregation. The method computes pairwise absolute Q-value differences as disagreement signals, then updates κ̄ and κ via sign-based gradients of disagreement-weighted Bellman errors. This creates directional updates where each sample contributes equally regardless of error magnitude. The critic target uses yκ̄ = r + γ[Q̄κ̄(s′,a′) − α log π(a′|s′)] with Q̄κ̄ = mean(ensemble) + κ̄·δ̄, while the actor uses Q̃κ = mean(ensemble) + κ·δ. Parameters are constrained to (-1, 1) via tangent transformation.

## Key Results
- DEA achieves top average rank across final return, IQM, and AULC metrics in MuJoCo experiments
- κ̄ trajectories remain predominantly negative while κ increases over training, confirming decoupled dynamics
- Disagreement signals decrease as training progresses, supporting phase-appropriate conservatism adjustments
- DEA outperforms static ensemble baselines (SAC and REDQ) across both interactive (1M steps) and sample-efficient (300K steps) regimes

## Why This Works (Mechanism)

### Mechanism 1: Directional Update via Sign-Based Gradients
- Claim: Sign-only gradients from disagreement-weighted absolute Bellman errors stabilize aggregation parameter learning by equalizing sample contributions regardless of error magnitude
- Core assumption: Sign information alone contains sufficient signal for learning appropriate conservatism levels; magnitude variations are treated as noise
- Evidence anchors: Section 5 describes gradient computation explicitly; directional nature gives DEA its name
- Break condition: If Bellman error signs become uncorrelated with actual overestimation direction, the directional signal may become unreliable

### Mechanism 2: Decoupled Critic-Actor Aggregation with Opposing Tendencies
- Claim: Separate κ̄ and κ parameters enable simultaneous conservative value estimation and exploratory policy optimization
- Core assumption: Critic and actor benefit from different levels of optimism/conservatism at the same training stage, and ensemble disagreement reliably indicates when optimism becomes safe
- Evidence anchors: Figure 1 shows κ̄ remaining negative while κ increases; entropy term creates asymmetric pressure
- Break condition: If κ̄ and κ evolve in same direction, decoupling benefit is lost

### Mechanism 3: Ensemble Disagreement as Training-Phase Indicator
- Claim: Pairwise absolute Q-value differences serve as proxy for epistemic uncertainty that naturally decreases as training progresses
- Core assumption: Q-value disagreement correlates with meaningful epistemic uncertainty rather than just ensemble diversity
- Evidence anchors: Figures 6, 7 show disagreement decreasing relative to Q-value scale; plays key role in regulating balance between conservatism and exploration
- Break condition: If disagreement remains persistently high or collapses prematurely without value accuracy improving, phase-adaptation mechanism fails

## Foundational Learning

- Concept: Overestimation bias in Q-learning
  - Why needed here: DEA's core motivation is mitigating overestimation that accumulates through actor exploitation
  - Quick check question: Can you explain why taking the minimum over an ensemble reduces overestimation but may cause underestimation?

- Concept: Soft actor-critic framework (entropy-regularized RL)
  - Why needed here: DEA is built on SAC and uses its entropy term in target computation
  - Quick check question: What role does the temperature parameter α play in balancing reward maximization vs. exploration?

- Concept: Update-to-data (UTD) ratio and learning regimes
  - Why needed here: DEA is evaluated across interactive (low UTD) and sample-efficient (high UTD) regimes
  - Quick check question: Why does increasing UTD ratio amplify overestimation bias and training instability?

## Architecture Onboarding

- Component map:
  - Critic ensemble (N Q-networks with target networks) -> DEA parameters (κ̄, κ) -> Directional targets (Q̄κ̄, Q̃κ) -> Shared Bellman error updates

- Critical path:
  1. Sample mini-batch from replay buffer
  2. Compute ensemble disagreement δ̄(s′,a′) and δ(s,a)
  3. Construct directional targets Q̄κ̄ and Q̃κ using current κ̄, κ
  4. Update critics toward shared target yκ̄
  5. Update κ̄ via sign-gradient of disagreement-weighted error
  6. Update κ via sign-gradient of disagreement-weighted error
  7. Update actor to maximize Q̃κ − α log π
  8. Soft update target networks

- Design tradeoffs:
  - Initialization sensitivity: κ̄ initialization affects early training dynamics more in high-UTD regimes
  - Ensemble size requirements: Small ensembles may not provide meaningful disagreement signals
  - Tangent transformation: Maps κ, κ̄ to (-1, 1) for unconstrained optimization stability
  - Additional hyperparameters: Learning rates ηκ̄ and ηκ for aggregation parameters add tuning complexity

- Failure signatures:
  - κ and κ̄ both converging to similar values (loss of decoupling benefit)
  - κ̄ becoming positive early (insufficient critic conservatism)
  - Disagreement remaining high throughout training (ensemble not converging)
  - Performance collapse when switching learning regimes without adjusting ensemble size

- First 3 experiments:
  1. Replicate Figure 1 in a single MuJoCo environment: Track κ̄ and κ trajectories to verify expected dynamics
  2. Ablate directional vs. magnitude-based updates: Replace sign-based gradients with standard MSE gradients
  3. Cross-regime transfer test: Train DEA with interactive-regime hyperparameters and evaluate in sample-efficient setting

## Open Questions the Paper Calls Out

- **Question**: Can DEA's learnable directional aggregation mechanism be extended to offline reinforcement learning settings?
  - Basis: "Our work focuses exclusively on online RL; extending these ideas to the offline setting is an exciting direction for future work..."
  - Why unresolved: Offline RL faces fundamentally different challenges where the policy cannot explore
  - What evidence would resolve it: Empirical evaluation on D4RL showing improved policy quality under distributional shift

- **Question**: How does DEA perform in sparse-reward environments?
  - Basis: "We deliberately focused on dense-reward environments... we expect these benefits to be even more pronounced in sparse-reward environments..."
  - Why unresolved: Adaptive exploration dynamics may differ when rewards are sparse
  - What evidence would resolve it: Experiments on sparse-reward continuous control tasks comparing DEA against SAC/REDQ

- **Question**: What theoretical convergence guarantees can be established for DEA?
  - Basis: "These benefits are not yet supported by theoretical analysis. A key challenge is that DEA's learnable aggregation rules depend on ensemble disagreement..."
  - Why unresolved: Sign-based gradient updates create non-standard optimization landscape
  - What evidence would resolve it: Formal proof of convergence under standard assumptions

## Limitations

- Effectiveness may diminish when ensembles are too small to provide meaningful disagreement signals
- Initialization sensitivity affects early training dynamics more in high-UTD regimes
- Additional hyperparameters (learning rates for κ̄ and κ) add tuning complexity

## Confidence

- **High confidence**: Final performance improvements (average rank, IQM, AULC metrics)
- **Medium confidence**: Directional updates via sign gradients mechanism
- **Medium confidence**: Decoupled critic-actor aggregation dynamics
- **Medium confidence**: Disagreement as training-phase indicator
- **Medium confidence**: Regime robustness claims

## Next Checks

1. **Sign vs. magnitude gradient ablation**: Implement DEA variants using standard MSE gradients instead of sign-based directional updates for κ̄ and κ; compare stability and performance across learning regimes to isolate the directional mechanism's contribution

2. **Disagreement signal validation**: Create controlled environments with varying levels of ensemble diversity while keeping epistemic uncertainty constant; measure whether DEA's parameters track true uncertainty or ensemble artifacts

3. **Cross-regime initialization study**: Systematically vary κ̄ and κ initialization values in each regime, measuring sensitivity and identifying ranges where DEA fails to learn appropriate directional parameters; compare against SAC baseline sensitivity