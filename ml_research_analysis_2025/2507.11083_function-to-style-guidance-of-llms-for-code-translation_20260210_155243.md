---
ver: rpa2
title: Function-to-Style Guidance of LLMs for Code Translation
arxiv_id: '2507.11083'
source_url: https://arxiv.org/abs/2507.11083
tags:
- code
- translation
- source
- llms
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the dual challenge of correctness and readability
  in LLM-generated code translations by introducing a function-to-style guidance framework
  (F2STrans). The method employs a two-stage approach: first, functional learning
  uses high-quality, function-consistent source-target code pairs mined from online
  programming platforms to ensure translation correctness; second, style learning
  leverages both positive and negative translation examples to preserve the source
  code''s stylistic features, enhancing readability.'
---

# Function-to-Style Guidance of LLMs for Code Translation

## Quick Facts
- arXiv ID: 2507.11083
- Source URL: https://arxiv.org/abs/2507.11083
- Reference count: 36
- Small model (Qwen 1.5B) outperforms GPT-4 and prompt-enhanced Qwen 32B across 20 translation scenarios

## Executive Summary
This paper addresses the dual challenge of correctness and readability in LLM-generated code translations by introducing a function-to-style guidance framework (F2STrans). The method employs a two-stage approach: first, functional learning uses high-quality, function-consistent source-target code pairs mined from online programming platforms to ensure translation correctness; second, style learning leverages both positive and negative translation examples to preserve the source code's stylistic features, enhancing readability. A new benchmark was constructed with up-to-date source code, extensive test cases, and manually annotated ground-truth translations to rigorously evaluate functional and stylistic consistency. Experiments on both the new benchmark and existing datasets demonstrate that F2STrans significantly improves code translation performance across various LLM types and scales.

## Method Summary
The F2STrans framework employs a two-stage approach to code translation. First, functional learning mines high-quality, function-consistent source-target code pairs from online platforms like Codeforces, filtering them through differential testing to ensure semantic equivalence. This stage trains a base model on pairs where input-output behavior is identical. Second, style learning uses both positive examples (generated by a strong teacher model with style prompts) and negative examples (generated by the intermediate functional model) to refine the model's stylistic fidelity. The method employs a list-wise contrastive loss to maximize the probability of style-positive translations while suppressing style-poor ones, preserving the source code's stylistic features while maintaining functional correctness.

## Key Results
- F2STrans outperforms both larger models (GPT-4) and prompt-enhanced variants (Qwen 32B) on average across 20 diverse translation scenarios
- A small model (Qwen 1.5B) trained with F2STrans achieves superior performance compared to significantly larger models
- The framework demonstrates consistent improvements across 5 programming languages (C, C++, Go, Java, Python) and various translation pairs
- Ablation studies confirm the effectiveness of both functional filtering and style consensus selection components

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Consistency Filtering
The framework employs differential testing to filter training pairs, ensuring strict semantic equivalence by rejecting code pairs that produce divergent outputs despite solving the same problem. This approach reduces semantic hallucination by conditioning the model on strict I/O alignment rather than loose topic similarity.

### Mechanism 2: Style Consensus Selection
The system generates multiple translation candidates and selects the one with highest stylistic similarity (CSSim) to peer candidates. This noise-filtering mechanism assumes consensus style represents optimal target, improving stylistic fidelity without requiring human annotation.

### Mechanism 3: Contrastive Style Refinement
The second stage utilizes list-wise loss that maximizes probability of "style-positive" translations while suppressing "style-poor" translations. This contrastive signal sharpens the model's boundary between functional equivalence and stylistic identity.

## Foundational Learning

- **Concept: Differential Testing**
  - **Why needed here:** Standard code similarity doesn't guarantee functional equality; execution against test inputs verifies semantic parity
  - **Quick check question:** If two code snippets produce same output for 10 test cases but use different algorithms, are they "functionally consistent" according to this paper's strict definition? (Answer: Only if they pass exhaustive differential testing; otherwise, they might be rejected)

- **Concept: Contrastive Learning (List-wise)**
  - **Why needed here:** Style learning teaches "which code is better" not just "how to write code"
  - **Quick check question:** Why does the paper use list-wise loss rather than simple supervised fine-tuning on positive pairs only? (Answer: To explicitly suppress generation of style-poor code, which standard supervised learning might ignore)

- **Concept: Style Quantification (CSSim)**
  - **Why needed here:** Operationalizes subjective "readability" via measurable metrics (variable names, API calls, structure)
  - **Quick check question:** If model translates C++ `vector` to Python `list` but changes loop variable from `i` to `element`, which CSSim component is most penalized? (Answer: Variable naming distance)

## Architecture Onboarding

- **Component map:** Data Miner -> Functional Filter -> Functional Learner ($M_{fun}$) -> Style Generator -> Style Learner ($M_{sty}$)
- **Critical path:** Quality of Functional Filter is bottleneck; if differential testing is incomplete, $M_{fun}$ learns faulty logic that propagates to negative example generation
- **Design tradeoffs:** Relies on strong teacher model (Qwen 32B) for style generation, creating ceiling for student performance; computationally expensive compared to prompt engineering
- **Failure signatures:** Functional regression (beautiful code with wrong answers); compilation spikes (syntax errors suggesting differential testing filter failed)
- **First 3 experiments:** 1) Pipeline validation: run "Relevance-driven Selection" on 100 pairs and manually verify LLM Judge scores correlate with functional consistency; 2) Style ablation: train $M_{fun}$ and compare against $M_{sty}$ on held-out set, checking for new functional errors; 3) Negative sampling test: generate negatives randomly vs. paper's selection method to confirm "hard negatives" are required

## Open Questions the Paper Calls Out
- Can a direct metric be developed to measure stylistic consistency between different programming languages? (Current method relies on selecting best target based on similarity to other generated targets rather than direct quantification)
- Does training on competitive programming data generalize to production-level software migration? (Codebase structure and dependencies differ significantly from algorithmic problems)
- Is reliance on strong teacher model (Qwen 32B) necessary for effective style learning? (Student cannot exceed teacher's stylistic capabilities)

## Limitations
- Differential testing verification is limited by quality and coverage of test suites; edge cases may lead to runtime failures
- Style consensus mechanism's effectiveness is bounded by stylistic quality of teacher model (Qwen 32B)
- Computational expense of two-stage process with massive dataset generation and filtering

## Confidence
- **High Confidence:** Dual-objective framework (functional correctness + stylistic preservation) is well-founded and addresses documented gap
- **Medium Confidence:** CSSim metric may not fully capture human notions of readability; consensus assumption could be domain-dependent
- **Medium Confidence:** Claim that 1.5B model outperforms GPT-4 requires scrutiny of evaluation methodology and test set composition

## Next Checks
1. **Test Coverage Validation:** Manually audit 50 code pairs from functional learning stage to verify differential testing correctly identifies functionally consistent pairs, focusing on edge cases and algorithmic diversity
2. **Style Metric Grounding:** Conduct human evaluation study comparing CSSim scores against human readability assessments on subset of translations to validate metric captures meaningful stylistic improvements
3. **Generalization Stress Test:** Evaluate F2STrans on code snippets from programming domains not represented in Codeforces training data (e.g., embedded systems, scientific computing) to assess whether learned style consistency transfers beyond training distribution