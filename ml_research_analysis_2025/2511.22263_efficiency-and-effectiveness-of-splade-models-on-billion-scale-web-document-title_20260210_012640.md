---
ver: rpa2
title: Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document
  Title
arxiv_id: '2511.22263'
source_url: https://arxiv.org/abs/2511.22263
tags:
- retrieval
- query
- splade
- pruning
- efficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares BM25, SPLADE, and Expanded-SPLADE models on
  billion-scale web document retrieval tasks, evaluating both effectiveness and efficiency.
  The study demonstrates that SPLADE models achieve superior retrieval performance
  over BM25, particularly for complex queries, but at significantly higher computational
  costs.
---

# Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document Title

## Quick Facts
- arXiv ID: 2511.22263
- Source URL: https://arxiv.org/abs/2511.22263
- Reference count: 29
- Primary result: SPLADE models achieve superior retrieval performance over BM25 for complex queries but require significant computational optimization through pruning strategies to be practical at scale

## Executive Summary
This paper presents a comprehensive evaluation of SPLADE models compared to traditional BM25 on billion-scale web document retrieval tasks, focusing on both effectiveness and efficiency. The authors demonstrate that while SPLADE models significantly outperform BM25 in retrieval quality, particularly for complex queries, they incur substantially higher computational costs. Through systematic experimentation with various pruning strategies, the study identifies Expanded-SPLADE with thresholding as the optimal balance, achieving up to 85% latency reduction while maintaining over 94% of original performance. The research provides practical insights for deploying sparse retrieval models in large-scale search systems, highlighting the critical trade-off between retrieval quality and computational efficiency.

## Method Summary
The study compares BM25, SPLADE, and Expanded-SPLADE models on a billion-scale web corpus using the TREC 2019 Deep Learning Track dataset. The evaluation examines both retrieval effectiveness (NDCG@10) and efficiency (query latency) across different query types. To address the computational overhead of SPLADE models, the authors implement three pruning strategies: document-centric static pruning, top-k query term selection, and boolean queries with term thresholds. The experiments systematically measure the impact of these strategies on both effectiveness and efficiency, with particular attention to query length effects on latency. The analysis provides quantitative comparisons of retrieval performance and computational costs across all model variants and pruning configurations.

## Key Results
- SPLADE models achieve significantly higher NDCG@10 scores than BM25, especially for complex queries
- Short queries incur up to 6.27x higher latency with SPLADE compared to BM25
- Expanded-SPLADE with thresholding achieves 85% latency reduction while maintaining 94% of original retrieval performance
- Document-centric static pruning and boolean query thresholds provide effective efficiency gains with minimal effectiveness loss

## Why This Works (Mechanism)
SPLADE models leverage learned sparse representations that capture semantic relationships between queries and documents more effectively than traditional bag-of-words approaches like BM25. The expansion component in Expanded-SPLADE further enhances retrieval quality by incorporating related terms, improving recall for complex queries. The pruning strategies work by reducing the search space through document filtering, query term selection, and boolean constraints, which decreases computational overhead without substantially impacting the quality of retrieved results. The effectiveness of these strategies depends on maintaining sufficient query-document relevance signals while eliminating redundant or less informative terms and documents from the retrieval process.

## Foundational Learning
- Sparse vs dense retrieval: Understanding the trade-offs between sparse representations (efficient but potentially less semantically rich) and dense representations (more semantic but computationally expensive) is crucial for selecting appropriate retrieval architectures for different scale requirements
- Query expansion techniques: Learning how query expansion improves recall and handles vocabulary mismatches helps in understanding why Expanded-SPLADE outperforms basic SPLADE models
- Pruning strategies in information retrieval: Familiarity with various pruning approaches (static, dynamic, term-based) is essential for implementing efficient retrieval systems at scale

## Architecture Onboarding

**Component Map**: Query Input -> Preprocessing -> SPLADE Model -> Document Scoring -> Pruning Strategy -> Ranked Results

**Critical Path**: The critical path involves query encoding through the SPLADE model, document scoring across the entire corpus, and result ranking. Pruning strategies interrupt this path by reducing the document set or query term space before scoring.

**Design Tradeoffs**: The primary tradeoff is between retrieval effectiveness and computational efficiency. More aggressive pruning reduces latency but risks losing relevant documents, while less aggressive pruning preserves effectiveness at the cost of higher computational overhead.

**Failure Signatures**: Ineffective pruning may result in either minimal latency improvement with significant effectiveness loss, or substantial latency reduction with catastrophic effectiveness degradation. Query length sensitivity indicates potential scaling issues for real-world deployments.

**First 3 Experiments**:
1. Compare BM25 vs basic SPLADE on a subset of queries to establish baseline effectiveness differences
2. Apply document-centric static pruning to SPLADE and measure latency vs NDCG@10 trade-off
3. Implement boolean query thresholding and evaluate its impact on both short and long queries

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based on a single web corpus (TREC 2019 Deep Learning Track), limiting generalizability to other domains
- Study focuses on specific pruning strategies without exploring alternative optimization techniques like learned dynamic pruning
- Computational measurements are hardware-specific and may not translate directly to other infrastructure setups

## Confidence

| Claim | Confidence |
|-------|------------|
| SPLADE models achieve higher effectiveness than BM25 | High |
| Efficiency measurements are hardware-dependent | Medium |
| Pruning strategy effectiveness generalizes across datasets | Medium |

## Next Checks

1. Test the pruning strategies on multiple diverse datasets to assess generalizability across different domains and document collections
2. Implement and compare alternative pruning techniques such as learned dynamic pruning or query-specific optimization methods
3. Evaluate the long-term effectiveness impact of pruning by measuring retrieval quality across multiple retrieval cycles or user sessions