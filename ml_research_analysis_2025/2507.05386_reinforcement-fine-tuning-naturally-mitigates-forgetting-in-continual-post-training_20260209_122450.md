---
ver: rpa2
title: Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training
arxiv_id: '2507.05386'
source_url: https://arxiv.org/abs/2507.05386
tags:
- learning
- arxiv
- tasks
- continual
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting in continual post-training
  (CPT) of multimodal large language models, comparing supervised fine-tuning (SFT)
  and reinforcement fine-tuning (RFT). The authors find that while SFT suffers from
  severe performance degradation on previously learned tasks, RFT naturally preserves
  task-specific knowledge and even enhances general capabilities.
---

# Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training

## Quick Facts
- arXiv ID: 2507.05386
- Source URL: https://arxiv.org/abs/2507.05386
- Authors: Song Lai; Haohan Zhao; Rong Feng; Changyi Ma; Wenzhuo Liu; Hongbo Zhao; Xi Lin; Dong Yi; Qingfu Zhang; Hongbin Liu; Gaofeng Meng; Fei Zhu
- Reference count: 32
- Primary result: Reinforcement fine-tuning naturally mitigates catastrophic forgetting in continual post-training, preserving task-specific knowledge while enhancing general capabilities

## Executive Summary
This paper investigates catastrophic forgetting in continual post-training of multimodal large language models, comparing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). The authors find that while SFT suffers from severe performance degradation on previously learned tasks, RFT naturally preserves task-specific knowledge and even enhances general capabilities. Theoretical analysis reveals that RFT's gradient updates are naturally scaled by reward variance, creating an implicit regularization that protects previously acquired knowledge. The authors also propose RIF-RFT, a rollout-based instance filtering algorithm that improves training efficiency without compromising RFT's forgetting mitigation properties.

## Method Summary
The study compares SFT and RFT for continual post-training on a sequence of 7 multimodal tasks using Qwen2.5-VL-7B-Instruct as the base model. SFT employs full-parameter fine-tuning with learning rate 1e-5 and batch size 24, while RFT uses GRPO with learning rate 1e-6, rollout batch size 512, and group size 8. The reward function combines accuracy (90%) and format compliance (10%). RIF-RFT is introduced as an optional instance filtering mechanism. Evaluation metrics include average accuracy across all tasks and a forgetting measure (FM) that quantifies performance degradation on previous tasks.

## Key Results
- RFT consistently outperforms SFT in mitigating catastrophic forgetting, with forgetting measures (FM) showing 8-10% improvement
- SFT causes severe performance drops on general benchmarks (MMLU-Pro: -16.9%), while RFT improves MMMU scores by 2.1%
- RIF-RFT maintains comparable performance to full GRPO while reducing computational cost by filtering 58% of samples
- Theoretical analysis shows RFT's gradient updates are naturally scaled by reward variance, providing implicit regularization

## Why This Works (Mechanism)

### Mechanism 1: Reward Variance as Implicit Regularization
RFT naturally mitigates catastrophic forgetting because the expected magnitude of its parameter updates is scaled by the variance of the reward signal, dampening updates on uncertain samples. The paper derives that the "forgetting risk" of an RFT gradient update is approximately Var[r] · R(g_SFT). When a model is uncertain about a task, reward variance is high, implicitly regularizing the update magnitude in parameter subspaces sensitive to prior knowledge. The covariance between the squared advantage and the gradient's impact on prior tasks is assumed to be small.

### Mechanism 2: Avoidance of Distributional Collapse
RFT preserves general capabilities better than SFT because it optimizes for reward rather than strict token-matching likelihood. SFT forces the model to maximize the likelihood of a specific response distribution, effectively "memorizing" the downstream task and shifting the model manifold away from general knowledge. RFT allows the model to retain high-probability general responses as long as they achieve positive rewards.

### Mechanism 3: Rollout-based Instance Filtering (RIF-RFT)
Training efficiency can be improved by filtering "incompetent samples" without compromising the anti-forgetting properties of RFT. Samples for which the current policy consistently generates zero-reward outputs provide no effective policy gradient (advantage estimates collapse to noise). Pruning them focuses compute on informative samples.

## Foundational Learning

**Concept: Policy Gradient & Advantage Function**
- Why needed here: The core theoretical argument relies on how RFT estimates gradients using advantage functions
- Quick check question: If a model gets a reward of 1.0 for every sample in a batch, what is the variance, and what happens to the RFT gradient update? (Answer: Variance is 0; effective gradient update vanishes/dampens)

**Concept: Fisher Information Matrix (FIM)**
- Why needed here: The paper defines "forgetting risk" using the FIM to measure parameter sensitivity
- Quick check question: What does a high value in the diagonal of the Fisher Information Matrix indicate about a specific model parameter regarding previous tasks?

**Concept: Catastrophic Forgetting in SFT**
- Why needed here: The baseline problem that RFT addresses
- Quick check question: Why does maximizing log-likelihood on Task B data necessarily degrade performance on Task A in a fixed-capacity model?

## Architecture Onboarding

**Component map:** Prompt Input → Group Sampling (n=8) → Reward Calculation → Variance Estimation → Policy Update (scaled by variance)

**Critical path:** The core training loop processes samples in groups, computes rewards, estimates variance, and updates the policy with gradients scaled by reward variance.

**Design tradeoffs:**
- GRPO vs. ReMax/RLOO: Paper shows GRPO generally balances task retention and general capability best
- KL Penalty (β): Essential for training stability but less critical for knowledge retention than the implicit variance regularization

**Failure signatures:**
- "Incompetent Samples": High compute waste on samples yielding zero reward; addressed by RIF-RFT
- Instability w/o KL: Training process without KL penalty exhibits significant instability (requires multiple restarts)

**First 3 experiments:**
1. SFT vs. RFT Baseline: Run CPT on a 3-task sequence using standard SFT vs. GRPO, plot forgetting measure (FM)
2. Variance Ablation: Implement "forced-variance" GRPO with constant scaling to validate Proposition 5.2
3. Generalization Check: Evaluate post-CPT model on MMMU benchmark to verify SFT drops <45% while RFT maintains >50%

## Open Questions the Paper Calls Out

**Open Question 1:** Can combining RFT with explicit continual learning strategies (e.g., experience replay or regularization-based methods) yield further improvements over standalone RFT?

**Open Question 2:** How does RFT's forgetting mitigation scale to significantly longer task sequences (e.g., 20+ tasks) and diverse task domains beyond vision-language?

**Open Question 3:** Under what conditions does the theoretical approximation error E in Proposition 5.2 become non-negligible, and can it be bounded more tightly?

**Open Question 4:** Could dynamic or adaptive instance filtering in RIF-RFT, which periodically re-evaluates "incompetent samples" during training, recover additional learnable instances without sacrificing efficiency?

## Limitations
- Findings based on 7 multimodal tasks with specific characteristics may not generalize to tasks with different reward structures
- KL penalty de-emphasis conclusion drawn from single model architecture and reward formulation
- Theoretical framework relies on assumptions about covariance that aren't empirically validated across diverse scenarios

## Confidence

**High Confidence:**
- RFT consistently outperforms SFT in mitigating catastrophic forgetting across tested task sequence
- Observed performance degradation in SFT (up to 16.9% on MMLU-Pro) is statistically significant
- RIF-RFT maintains comparable performance to full GRPO while reducing computational cost

**Medium Confidence:**
- Variance-based implicit regularization mechanism is primary driver of forgetting mitigation
- RFT's preservation of general capabilities is directly attributable to reward-based optimization
- Anti-forgetting properties hold across different base model architectures and task types

**Low Confidence:**
- Theoretical framework fully explains empirical observations across all scenarios
- RIF-RFT's filtering criteria don't create systematic blind spots in learned policy
- Observed effects would persist with alternative reward formulations or evaluation metrics

## Next Checks

1. **Reward Structure Sensitivity Analysis:** Implement GRPO variant with normalized variance instead of batch reward variance, compare forgetting measures across SFT, standard GRPO, and variance-normalized GRPO.

2. **Cross-Architecture Generalization:** Apply CPT pipeline to different multimodal architecture (e.g., LLaVA-Next or Chameleon), evaluate whether RFT maintains forgetting mitigation advantage.

3. **Hard Sample Retention Test:** Modify RIF-RFT to maintain small percentage (5-10%) of zero-reward samples identified as "hard" based on input complexity metrics, compare final performance on challenging test instances.