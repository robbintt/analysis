---
ver: rpa2
title: Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X
  Resource Allocation
arxiv_id: '2501.13552'
source_url: https://arxiv.org/abs/2501.13552
tags:
- feature
- state
- network
- performance
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an explainable AI (XAI)-based framework for
  feature selection and model reduction in deep reinforcement learning (DRL) agents
  applied to vehicle-to-everything (V2X) communications. The method employs SHAP-based
  feature importance rankings to systematically identify and remove non-influential
  state features, thereby simplifying the model without significant performance loss.
---

# Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation

## Quick Facts
- arXiv ID: 2501.13552
- Source URL: https://arxiv.org/abs/2501.13552
- Reference count: 40
- Primary result: Achieves 97% of original sum-rate performance while reducing optimal state features by 28%, average training time by 11%, and trainable weight parameters by 46% in a network with eight vehicular pairs.

## Executive Summary
This paper introduces an explainable AI (XAI)-based framework for feature selection and model reduction in deep reinforcement learning (DRL) agents applied to vehicle-to-everything (V2X) communications. The method employs SHAP-based feature importance rankings to systematically identify and remove non-influential state features, thereby simplifying the model without significant performance loss. Applied to a multi-agent DRL setting for joint sub-band assignment and power allocation in cellular V2X communications, the approach achieves 97% of the original sum-rate performance while reducing optimal state features by 28%, average training time by 11%, and trainable weight parameters by 46% in a network with eight vehicular pairs. The framework provides a model-agnostic, post-hoc explainability pipeline that enhances understanding of DRL agent inferences and enables efficient model simplification.

## Method Summary
The methodology combines multi-agent deep reinforcement learning (MADRL) with explainable AI for feature selection. The approach uses Deep-SHAP to compute feature importance scores from trained DRL agents, then iteratively masks low-importance features to identify a minimal sufficient state representation. The framework consists of centralized training (at the base station) with decentralized execution (at vehicular agents), using a finite blocklength regime for ultra-reliable low-latency communications (URLLC). The XAI pipeline analyzes the trained model's state-action value function to rank input features, enabling systematic reduction of state space dimensionality without retraining the original model from scratch.

## Key Results
- Achieved 97% of original sum-rate performance with 28% reduction in optimal state features
- Reduced average training time by 11% and trainable weight parameters by 46% in 8-agent scenario
- Maintained URLLC reliability constraints (decoding error ≤ εmax,k) after feature reduction

## Why This Works (Mechanism)

### Mechanism 1: Attribution via Backpropagated Shapley Values
The Deep-SHAP method identifies non-influential state features by approximating the marginal contribution of each input neuron to the output Q-values. The framework utilizes a background dataset (XBG) to establish a reference distribution, linearizes the deep neural network (DNN) components, and backpropagates contribution scores (Shapley values) from the output layer to the input features. This mechanism assumes that the linear approximation of non-linearities within the DNN provides a sufficiently accurate proxy for feature importance.

### Mechanism 2: Iterative Feature Masking (Pruning)
Systematically masking the lowest-ranked features allows for the identification of a minimal sufficient state representation. Algorithm 2 iteratively masks the p least important features (replacing values with column variance) and evaluates performance degradation against a precision threshold (Δ). If degradation is within tolerance, features are permanently removed. This mechanism assumes that "global" importance rankings hold "locally" for specific decision boundaries.

### Mechanism 3: Computational Complexity Reduction via Input Dimensionality
Reducing the state space dimensionality directly lowers the number of trainable parameters and accelerates convergence. The DNN architecture size is defined by the input dimension (|x|). By reducing |x| (from 80 to 58 features in the 8-pair scenario), the input layer and subsequent hidden layers (H1, H2, H3) contain fewer neurons and connections. This mechanism assumes that the environment dynamics remain sufficiently stable such that the reduced feature set captures the necessary information for the Markov Decision Process (MDP).

## Foundational Learning

- **Concept: Shapley Values (Game Theory)**
  - Why needed here: To understand how credit assignment is distributed among input features. You must grasp that SHAP values represent the average marginal contribution of a feature value across all possible coalitions.
  - Quick check question: Why does the paper use a "background dataset" (XBG) rather than a single reference value for the Shapley calculation?

- **Concept: Deep Q-Networks (DQN) & Replay Buffers**
  - Why needed here: The XAI method analyzes a trained model. Understanding the distinction between the training phase (generating XBG) and the execution/testing phase (generating hold-out X) is critical for data segregation.
  - Quick check question: In Algorithm 1, why are experiences stored in XBG during training, while the hold-out dataset X is generated during distributed execution?

- **Concept: Finite Blocklength Regime (URLLC)**
  - Why needed here: The paper optimizes for reliability (error probability ε) rather than standard Shannon capacity. Understanding Eq. (3) is necessary to interpret the reward function r(t) used to evaluate "Average Network Performance" (α).
  - Quick check question: Why does the objective function penalize the decoding error probability εk(t)[n] rather than maximizing the raw data rate?

## Architecture Onboarding

- **Component map:** Agents (K V2V transmitters with local DQNs) -> Central Trainer (BS with global weights and Replay Buffer D) -> Explainer (Python SHAP library with DeepExplainer) -> Selector (Iterative loop Algorithm 2)

- **Critical path:**
  1. Train Original-MADRL until convergence
  2. Generate background data (XBG) and hold-out dataset (X)
  3. Compute SHAP matrices Sk and derive global importance vector Φ̄
  4. Execute Algorithm 2 to determine the optimal feature subset based on threshold Δ
  5. Retrain the Simplified-MADRL model using only the selected features

- **Design tradeoffs:**
  - Threshold Δ: Setting Δ too high (>2% of α) leads to aggressive pruning and performance collapse. Setting it too low yields minimal computational gains.
  - K-means Summary: Using < 1% of XBG speeds up SHAP calculation but risks unstable importance estimates.

- **Failure signatures:**
  - Noisy Explanations: High variance in SHAP values across the hold-out set indicates the model has not converged or XBG is insufficient.
  - Reliability Crash: The simplified model maintains sum-rate but violates εmax constraints (Network Availability drops) because safety-critical features were pruned.
  - Divergence during Retraining: Simplified model fails to converge, suggesting the pruned state space violates the Markov property.

- **First 3 experiments:**
  1. Baseline Sanity Check: Run the Original-MADRL for K=4 and K=8. Verify convergence against Fig. 5/6 to ensure the reward weights (λ1, λ2, λ3) are correctly tuned.
  2. SHAP Stability Test: Calculate SHAP values using 100%, 10%, and 1% of XBG. Compare the resulting feature rankings to justify the 1% K-means summary approximation used in the paper.
  3. Threshold Sensitivity: Execute Algorithm 2 with Δ = [1, 2, 5, 10]. Plot the "Average Network Performance" vs. "Retained Features" curve to replicate Fig. 4 and confirm the "elbow" point.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can XAI be applied to digital twin systems to simplify complex virtual representations while maintaining synchronization between the twin and the actual network?
  - Basis: The conclusion identifies applying XAI to digital twin systems to interpret model actions using environment semantics as a "promising research direction."
  - Why unresolved: The current framework validates feature selection on physical layer resource allocation but does not address the latency or complexity constraints specific to maintaining digital twin synchronization.
  - What evidence would resolve it: A framework demonstrating that XAI-based simplification reduces digital twin update latency without desynchronizing from the real-world network state.

- **Open Question 2:** Can the proposed SHAP-based feature selection methodology be generalized to other wireless resource allocation problems, such as massive MIMO or beamforming?
  - Basis: The authors state they "plan to adapt the proposed XAI-based methodology for other wireless resource allocation problems" in the future.
  - Why unresolved: The current validation is limited to joint sub-band assignment and power allocation in V2X; performance on problems with significantly different constraint structures is unknown.
  - What evidence would resolve it: Successful application of the two-stage XAI pipeline to a distinct optimization problem (e.g., beamforming) achieving similar parameter reduction rates without performance loss.

- **Open Question 3:** Does the K-means summarization technique used to approximate SHAP values retain sufficient fidelity for feature selection in much larger networks?
  - Basis: The paper identifies the "considerable computational complexity" of Deep-SHAP as a "major obstacle" and relies on a 1% K-means summary to make calculations tractable.
  - Why unresolved: While effective for 8 vehicular pairs, the approximation error of such aggressive summarization in high-dimensional state spaces could lead to incorrect feature pruning.
  - What evidence would resolve it: A sensitivity analysis comparing feature rankings generated by full SHAP calculations versus K-means approximations across varying network densities.

## Limitations

- The approach assumes stationary environment dynamics - rapid changes in vehicular density or interference patterns could invalidate the reduced feature set.
- The reliance on K-means summarization for SHAP computation introduces approximation error, though results suggest 1% sampling suffices.
- The method's effectiveness depends heavily on the choice of Δ threshold, requiring problem-specific tuning.

## Confidence

- **High**: The 28% reduction in state features while maintaining 97% performance is well-supported by empirical results (Table III, Fig. 4). The computational complexity reduction (11% training time, 46% parameters) follows directly from reduced input dimensionality.
- **Medium**: The assumption that SHAP-based importance rankings generalize from training to execution phases holds in this V2X context but requires validation in domains with different feature distributions or non-stationary environments.
- **Low**: The paper doesn't address potential catastrophic failure modes when the simplified model encounters previously masked feature combinations in rare but critical scenarios.

## Next Checks

1. **Non-stationary Test**: Apply the framework to a V2X environment with varying vehicular density (e.g., rush hour vs. low traffic) and measure performance degradation of the simplified model compared to original.
2. **Feature Interaction Analysis**: Design synthetic test cases where two individually low-importance features combine to create critical signals, then evaluate whether the masking approach correctly preserves or identifies these interactions.
3. **Cross-Environment Transfer**: Train the MADRL agent in one V2X scenario (e.g., highway) and apply the same feature selection to a different scenario (e.g., urban intersection) to test robustness of the importance rankings across environments.