---
ver: rpa2
title: Learning Treatment Policies From Multimodal Electronic Health Records
arxiv_id: '2507.20993'
source_url: https://arxiv.org/abs/2507.20993
tags:
- treatment
- causal
- data
- policy
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of learning treatment policies
  from multimodal electronic health records (EHRs) that combine tabular data with
  clinical text. The key problem is that existing causal policy learning methods assume
  tabular covariates satisfying strong unconfoundedness assumptions, which are often
  violated in multimodal settings where confounding factors are distributed across
  modalities.
---

# Learning Treatment Policies From Multimodal Electronic Health Records

## Quick Facts
- **arXiv ID**: 2507.20993
- **Source URL**: https://arxiv.org/abs/2507.20993
- **Reference count**: 40
- **Key outcome**: The proposed causal policy learning method for multimodal EHR data either matches or exceeds risk-based policies depending on how baseline risk aligns with treatment benefit, and produces substantially different treatment assignments compared to risk-based approaches on real-world data.

## Executive Summary
This study addresses the challenge of learning treatment policies from multimodal electronic health records that combine tabular data with clinical text. The key problem is that existing causal policy learning methods assume tabular covariates satisfying strong unconfoundedness assumptions, which are often violated in multimodal settings where confounding factors are distributed across modalities. The authors propose extending causal policy learning to multimodal data by estimating treatment effects from pre-trained representations while using expert-provided annotations during training to supervise effect estimation. At inference, the model relies only on multimodal representations without requiring annotations. The method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets.

## Method Summary
The method learns treatment policies by estimating heterogeneous treatment effects from multimodal representations of EHR data. During training, expert annotations of confounders are used to construct a Doubly Robust (DR) pseudo-outcome that serves as an unbiased supervision signal. A two-stage process is employed: Stage 1 fits nuisance models (propensity and response surfaces) on annotated confounders to construct the pseudo-outcome, and Stage 2 trains a neural network to predict this pseudo-outcome using only multimodal representations. At inference, the model maps raw multimodal data (text embeddings + tabular) directly to treatment effect estimates, eliminating the need for costly real-time expert annotation.

## Key Results
- The causal policy learning approach either matches or exceeds risk-based policies depending on how baseline risk aligns with treatment benefit in each dataset
- Produces substantially different treatment assignments compared to risk-based approaches on real-world data, demonstrating the practical importance of choosing appropriate policy learning methods for multimodal clinical data
- Achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The proposed method enables causal policy learning from imperfect representations by targeting "coarsened" treatment effects rather than exact individual effects.
- **Mechanism:** Instead of requiring the representation $\phi$ to capture all confounders perfectly (identifying $\tau_x(x)$), the model estimates $\tau_\phi(\phi)$, which is the expected treatment effect conditional on the representation. This averages treatment effects over patient subgroups consistent with the embedding. As long as the "coarsening bias" (error from information loss) is smaller than the "margin" between the effects of different patients, the ranking order for treatment prioritization is preserved.
- **Core assumption:** The multimodal representations $\phi$ preserve enough information about the confounders such that the ordering of treatment effects is not inverted for patients near the decision boundary.
- **Evidence anchors:** [Section 4.1] Defines coarsened effects $\tau_\phi(\phi)$ and discusses conditions where coarsening bias $\delta_i$ relative to margins $\gamma_i$ preserves optimal policy ordering.

### Mechanism 2
- **Claim:** The architecture decouples confounding adjustment from inference by using expert annotations to construct a "pseudo-outcome" that serves as a noisy but unbiased supervision signal.
- **Mechanism:** During training, the method uses the Doubly Robust (DR) learner. It fits nuisance models (propensity and response surfaces) on expert-annotated confounders $X$ to construct a pseudo-outcome $\Delta_x$. This pseudo-outcome is an unbiased estimate of the true treatment effect. A second-stage model is then trained to predict this pseudo-outcome using only the multimodal representations $\phi$.
- **Core assumption:** The expert-provided annotations $X$ are sufficient to satisfy the unconfoundedness assumption ($Y(t) \perp T | X$) during training.
- **Evidence anchors:** [Page 6] "Stage 1... constructs a pseudo-outcome... [which] depends on estimates of the propensity score... fitted on the annotated data."

### Mechanism 3
- **Claim:** The system achieves practical deployment feasibility by forcing the effect estimator to rely solely on representations at inference time, eliminating the need for costly real-time expert annotation.
- **Mechanism:** The model acts as a "student" learning from the "teacher" (the DR pseudo-outcome derived from annotations). Once trained, the student model maps raw multimodal data (text embeddings + tabular) directly to a treatment effect estimate. This allows the system to score new patients instantly without manual review.
- **Core assumption:** The mapping from multimodal representation to treatment effect is learnable and generalizes to unseen patients.
- **Evidence anchors:** [Abstract] "At inference, the model relies only on the multimodal representations without requiring annotations."

## Foundational Learning

- **Concept: Unconfoundedness (Ignorability)**
  - **Why needed here:** This is the core causal bottleneck. The paper argues that standard EHR data is *confounded* because text contains hidden factors. Understanding this explains *why* the method needs expert annotations during trainingâ€”to satisfy the conditional independence $Y(t) \perp T | X$.
  - **Quick check question:** Can you explain why the authors claim risk-based models (predicting $Y$) might misallocate treatment compared to causal models (predicting $Y(1)-Y(0)$)? (Hint: Check the correlation between baseline risk and treatment benefit in the results).

- **Concept: Doubly Robust (DR) Estimation**
  - **Why needed here:** The method uses a DR learner to generate the training signal (pseudo-outcome). You must understand that DR estimation combines outcome modeling and propensity modeling, remaining unbiased if *either* is correct, but not necessarily both.
  - **Quick check question:** In Stage 1, the method constructs a pseudo-outcome. Does this require the representation $\phi$ or the annotations $X$? Why?

- **Concept: Representation Learning (Encoders)**
  - **Why needed here:** The "coarsened" mechanism relies on pre-trained text encoders (specifically ModernBERT) to compress clinical notes into vectors. The quality of the final policy is strictly bounded by how well these vectors preserve clinical confounders.
  - **Quick check question:** Table 1 shows prediction metrics for confounders from text. Why is this "recoverability" test a critical sanity check for the entire method's validity?

## Architecture Onboarding

- **Component map:** Tabular Variables + Clinical Text -> ModernBERT Encoder -> Multimodal Representations $\phi$ -> Stage 2 Effect Model -> Treatment Effect Estimates -> Policy Ranking
- **Critical path:** The flow of the **pseudo-outcome**. If the annotations $X$ do not capture the confounders, the pseudo-outcome is biased. If the representation $\phi$ cannot predict the pseudo-outcome (high Stage 2 loss), the coarsening bias is too high.
- **Design tradeoffs:**
  - **Annotation vs. Scalability:** The method trades off high upfront annotation cost for low inference cost.
  - **Precision vs. Robustness:** By using "coarsened" effects, the system sacrifices precision in estimating exact individual effects (CATE) for robustness against the information loss inherent in text embeddings.
- **Failure signatures:**
  - **Confounder Recoverability Failure:** If AUROC for predicting confounders from text (Table 1) is $\approx 0.5$, the text representation contains no signal, and the policy will fail.
  - **Agreement with Risk Model:** If the causal policy perfectly matches the risk-based policy, the causal mechanism may not be learning distinct benefit signals, or the dataset simply has aligned risk/benefit (like SynSum).
  - **Stage 2 Regression Loss:** High validation loss in Stage 2 implies the representation $\phi$ is insufficient to approximate the effect defined by the annotations.
- **First 3 experiments:**
  1. **Recoverability Check:** Replicate Table 1. Train simple classifiers to predict known confounders (e.g., "fever", "cough") from the text embeddings alone. If F1 scores are high, proceed.
  2. **Stage 2 Ablation:** Train the final effect model using *only* tabular data vs. *only* text vs. *both*. Compare the Precision in Estimation of Heterogeneous Effect (PEHE) to quantify the value added by multimodality.
  3. **Risk-Benefit Correlation Analysis:** Plot the relationship between baseline risk ($\mu_0$) and treatment effect ($\tau_x$) on the training data. Verify if the causal policy actually differs from the risk policy (as seen in MIMIC-Syn vs. SynSum).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the method perform when annotations are collected from human experts or derived from auxiliary models rather than ground-truth confounders?
- **Basis in paper:** [explicit] "Future work should investigate how the method performs when using annotations collected from human experts or derived from auxiliary models."
- **Why unresolved:** Experiments used simulated annotations based on known data generating processes (symptoms, diagnoses, vital signs), representing an idealized setting.
- **What evidence would resolve it:** Empirical evaluation on datasets where confounders are labeled by clinicians or predicted by imperfect auxiliary models.

### Open Question 2
- **Question:** Can the preservation of treatment effect ordering be verified in real-world data where ground truth is unavailable?
- **Basis in paper:** [inferred] The paper states that verifying whether coarsened effects preserve ordering "cannot be tested on real-world data, as it depends on the unknown true treatment effects."
- **Why unresolved:** There is currently no practical diagnostic to detect if the coarsening bias $\delta_i$ has exceeded the margin $\gamma_i$ in real applications.
- **What evidence would resolve it:** The development of a proxy metric for ordering preservation that does not rely on counterfactuals.

### Open Question 3
- **Question:** Does fine-tuning the text encoder improve the recovery of confounding information compared to using frozen representations?
- **Basis in paper:** [inferred] The method relies on frozen ModernBERT embeddings; the paper notes that if the encoder "does not fully preserve this information," unconfoundedness is violated.
- **Why unresolved:** The study does not analyze if the representation space could be optimized to minimize coarsening bias during training.
- **What evidence would resolve it:** A comparison of PEHE (Precision in Estimation of Heterogeneous Effect) and policy value between frozen and fine-tuned encoder architectures.

## Limitations

- **Data access and reproducibility**: The study relies on anonymized but complex clinical datasets (MIMIC-III, SynSum) with specific preprocessing steps for ICU windowing and clinical text segmentation. Without the provided code, reproducing the exact experimental setup would require significant effort to replicate these data preparation pipelines.
- **Confounder recoverability assumption**: The method's effectiveness fundamentally depends on clinical text containing sufficient information about confounding factors. While the paper demonstrates this for specific confounders in Table 1, the generalizability to all potential confounders in real-world clinical settings remains uncertain.
- **Generalizability of causal effect estimates**: The method assumes that the relationship between representations and treatment effects learned on annotated training data will generalize to unannotated inference data. Distribution shifts in clinical text style, documentation practices, or patient populations between training and deployment could compromise this mapping.

## Confidence

- **High confidence**: The theoretical framework for coarsened treatment effect estimation is sound and builds on established causal inference principles. The empirical demonstration that causal policies differ from risk-based approaches on real-world data (MIMIC-Real) is convincing.
- **Medium confidence**: The method's practical utility depends heavily on the recoverability of confounders from clinical text, which is demonstrated but not exhaustively validated across all relevant clinical domains. The performance gains over risk-based methods are dataset-dependent and may not generalize uniformly.
- **Low confidence**: The specific implementation details (hyperparameter tuning, exact preprocessing steps) are not fully specified in the text, making exact replication challenging. The sensitivity of results to ModernBERT's pretraining domain and architecture choices is not explored.

## Next Checks

1. **Confounder recovery validation**: Replicate the analysis from Table 1 by training simple classifiers to predict key confounders (e.g., fever, cough, age) from the ModernBERT embeddings alone. This sanity check should achieve AUROC > 0.7 for at least 80% of confounders to justify proceeding with full policy evaluation.

2. **Distribution shift analysis**: Train the effect model on MIMIC-Real data from one time period and evaluate on data from a different period. Compare policy performance to assess robustness to temporal shifts in clinical documentation practices.

3. **Annotation efficiency experiment**: Systematically vary the amount of expert annotation available during training (10%, 30%, 50%, 100%) and measure the corresponding degradation in policy value. This would quantify the practical trade-off between annotation cost and policy performance.