---
ver: rpa2
title: Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian
arxiv_id: '2511.22069'
source_url: https://arxiv.org/abs/2511.22069
tags:
- have
- lemma
- therefore
- preprint
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence behavior of gradient descent
  on the score matching objective for learning a single Gaussian distribution using
  an over-parameterized student model. The authors study multiple noise regimes and
  initialization strategies, revealing rich and contrasting convergence behaviors.
---

# Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian

## Quick Facts
- arXiv ID: 2511.22069
- Source URL: https://arxiv.org/abs/2511.22069
- Authors: Yiran Zhang; Weihang Xu; Mo Zhou; Maryam Fazel; Simon Shaolei Du
- Reference count: 40
- Primary result: The paper analyzes convergence of gradient descent on score matching objective for learning a single Gaussian with over-parameterized student model, revealing rich dynamics across noise regimes.

## Executive Summary
This paper studies gradient descent convergence for over-parameterized score matching when learning a single Gaussian distribution. The authors analyze three distinct regimes based on noise scale and initialization strategy, revealing surprising dynamics: for large noise, all parameters converge to ground truth at O(1/√τ) rate; for small noise with random initialization, exactly one parameter converges while others diverge yet loss still converges at O(1/τ); and with exponentially small initialization, all parameters converge. These results highlight the importance of noise scale in ensuring stable convergence and demonstrate that over-parameterization can lead to counterintuitive dynamics where loss converges without parameter recovery.

## Method Summary
The paper analyzes gradient descent on the population score matching objective for learning a single Gaussian N(μ*, I_d) using an over-parameterized student model with n≫1 parameters. The student network outputs s_t(x) = Σᵢ wᵢ,t(x)μᵢ,t - x where weights wᵢ,t(x) are normalized exponentials based on distances to μᵢ,t. The analysis considers three regimes: large noise (t > log n + log M + 2) with standard initialization, small noise with exponentially small initialization (∥μᵢ⁽⁰⁾∥ ≤ exp(-poly(M₀))), and small noise with random Gaussian initialization far from ground truth. The theoretical guarantees show O(1/√τ) convergence for large noise, full parameter convergence for exponentially small initialization, and winner-takes-all dynamics with loss convergence for random initialization.

## Key Results
- Large noise scales (t > log n + log M + 2) ensure global convergence at O(1/√τ) rate to ground truth parameters
- Small noise scales can create stationary points with nonzero loss, preventing convergence from arbitrary initialization
- With exponentially small initialization, all n parameters converge to ground truth
- With random Gaussian initialization far from ground truth, exactly one parameter converges while others diverge, yet loss still converges at O(1/τ) rate

## Why This Works (Mechanism)

### Mechanism 1: Large Noise Simplifies Gradient Dynamics
- **Claim**: When noise scale t exceeds log(n) + log(M) + 2, gradient descent achieves global convergence at O(1/√τ) rate.
- **Mechanism**: Large noise exponentially dampens all parameter magnitudes (μᵢ,t = μ̃ᵢexp(-t)), reducing parameter spread so ∥μᵢ∥ ≤ 1/(3n). This suppresses cubic gradient terms that complicate dynamics, making the dominant gradient term identical to gradient EM, which has known convergence guarantees.
- **Core assumption**: Ground truth is a single Gaussian; step size η ≤ O(1/n⁴d²).
- **Evidence anchors**:
  - [abstract]: "large noise scales ensure global convergence similar to gradient EM"
  - [Section 2, Theorem 2.1]: Convergence rate O(n³d²/√ητ) with explicit condition t > log n + log M + 2
  - [corpus]: Related work on "Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures" shows similar convergence structure
- **Break condition**: If t is too small or step size η too large, cubic terms dominate and dynamics become unpredictable.

### Mechanism 2: Exponentially Small Initialization Enforces Parameter Coherence
- **Claim**: When all parameters initialize exponentially close to zero (∥μᵢ⁽⁰⁾∥ ≤ exp(-poly(M₀))), all n parameters converge to ground truth.
- **Mechanism**: Parameters remain clustered around a geometric center A(τ) that decays predictably as A(τ+1) = (1 - η/n)A(τ). The key insight is tracking deviation from this reference point—gradients for all parameters are nearly identical (∥∇μᵢL - A/n∥ ≤ 6KBA/n), preventing any parameter from escaping the cluster.
- **Core assumption**: Initialization must satisfy ∥μᵢ⁽⁰⁾∥ ≤ 1/(108nd·exp(10⁶ndM₀³)); Theorem 3.4 shows this exponential bound is necessary, not just sufficient.
- **Evidence anchors**:
  - [abstract]: "when the parameters are initialized to be exponentially small, gradient descent ensures convergence of all parameters"
  - [Section 3.2, Theorem 3.2]: Explicit bound and proof via tracking geometric center
  - [corpus]: No direct corpus evidence for this specific initialization regime
- **Break condition**: Theorem 3.4 provides counterexample: with initialization exp(-M₀/100), one parameter converges while others diverge—exponential smallness is strict.

### Mechanism 3: Winner-Takes-All Dynamics Under Random Initialization
- **Claim**: With random Gaussian initialization far from ground truth, exactly one parameter converges while others diverge to infinity, yet loss still converges at O(1/τ) rate.
- **Mechanism**: Two-stage dynamics—Stage 1: The parameter closest to ground truth decreases fastest, reaching ∥μ₁∥ ≤ 1/M₀³ while others remain above M₀/2. Stage 2: Once μ₁ is near ground truth, the loss gradient pushes other parameters away; minimum distance min_{i≠i₀} ∥μᵢ∥ increases monotonically.
- **Core assumption**: M₀ = ∥μ - μ*∥ ≥ 10⁹√d·n¹⁰; one parameter must have strictly smaller initial distance (gap of at least M₀^{-1/3}).
- **Evidence anchors**:
  - [abstract]: "with high probability, only one parameter converges while the others diverge, yet the loss still converges to zero"
  - [Section 3.3, Theorem 3.5]: Two-stage analysis with O(M₀²/η) warmup time
  - [corpus]: Limited corpus evidence for this divergent-yet-convergent-loss phenomenon
- **Break condition**: If initial spread is too small or no parameter has clear distance advantage, dynamics may enter unstudied intermediate regime.

## Foundational Learning

- **Concept: Score Matching Objective**
  - **Why needed here**: The paper's loss function L_t(s_t) = E[∥s_t(X_t) - ∇_x ln q_t(X_t)∥²] is the foundation; understanding that we're learning gradients of log-density, not the density directly, is essential.
  - **Quick check question**: If the ground truth score is μ*_t - x for a Gaussian N(μ*_t, I), what form should the student network output take?

- **Concept: Ornstein-Uhlenbeck (OU) Process**
  - **Why needed here**: The forward diffusion follows dX_t = -X_t dt + √2 dW_t, transforming the data distribution. The relationship μ*_t = μ* exp(-t) shows how noise scales dampen parameters.
  - **Quick check question**: At noise scale t=0, what is μ*_t? At t→∞, what does μ*_t approach?

- **Concept: Over-parameterization in Mixture Models**
  - **Why needed here**: Using n≫1 parameters to learn a single Gaussian component creates rich dynamics where some parameters may diverge while loss converges—counterintuitive without understanding that the loss only cares about weighted output, not individual parameters.
  - **Quick check question**: If all parameters diverge to infinity but maintain the correct weighted average v(x) = Σᵢ wᵢ(x)μᵢ ≈ μ*, will the loss still be zero?

## Architecture Onboarding

- **Component map**:
  - Student network -> s_t(x) = Σᵢ wᵢ,t(x)μᵢ,t - x
  - Weights -> wᵢ,t(x) = exp(-∥x-μᵢ,t∥²/2) / Σⱼ exp(-∥x-μⱼ,t∥²/2)
  - Loss function -> L_t = E_{X_t~q_t}[∥s_t(X_t) - ∇ ln q_t(X_t)∥²]
  - Gradient update -> μᵢ^{(τ+1)} = μᵢ^{(τ)} - η∇_{μᵢ}L

- **Critical path**:
  1. Choose noise scale t first—if large (t > log n + log M + 2), use standard step size
  2. For small t, initialization choice determines regime: exponentially small → all converge; random far → single converges
  3. Monitor not just loss but parameter norms—diverging parameters are acceptable in regime 3

- **Design tradeoffs**:
  - **Large vs small noise**: Large noise guarantees convergence but may reduce model expressiveness; small noise enables richer dynamics but requires careful initialization
  - **Full parameter recovery vs loss minimization**: Theorem 3.4 shows these are different goals—loss can converge without parameter recovery
  - **Step size bounds**: Must satisfy η < 1 for stability; η ≤ O(1/n⁴d²) for theoretical guarantees

- **Failure signatures**:
  - Loss plateau at nonzero value → likely hit stationary point (Theorem 3.1), need different initialization or larger noise
  - All parameters growing without bound → step size too large, reduce η
  - Single parameter near ground truth, others at infinity → expected behavior under random initialization, not a failure

- **First 3 experiments**:
  1. **Verify large-noise convergence**: Set n=5, d=3, t=10, η=0.01; initialize with M=4, all parameters at (4,0,0)+small noise; confirm all parameters converge to ground truth per Figure 1(a)
  2. **Test random initialization behavior**: Same setup but t=0 (small noise), random Gaussian initialization centered at M=6; verify only one parameter converges while others diverge per Figure 1(b)
  3. **Check stationary point existence**: Initialize with μ₁=s·e₁, μ₂=-s·e₁, others at 0; vary s and observe loss landscape; confirm nonzero stationary point exists per Theorem 3.1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the convergence dynamics for multi-component Gaussian mixture ground truths (m ≥ 2) in the over-parameterized score matching setting?
- Basis in paper: [explicit] "Extending the theory to more complex ground-truth distributions, such as multi-component Gaussian mixtures, remains a challenging and important direction for future work."
- Why unresolved: The analysis heavily exploits the structure of a single Gaussian teacher (m=1), including specific gradient forms and symmetry arguments that may not extend to multiple components.
- What evidence would resolve it: Convergence guarantees or counterexamples for over-parameterized student models learning 2+ component Gaussian mixtures under score matching.

### Open Question 2
- Question: How do the fixed-time gradient dynamics analyzed in this paper combine under the time-averaged loss used in practical diffusion training?
- Basis in paper: [explicit] "Understanding how these fixed-time components interact under a full t-averaging scheme is an interesting and nontrivial next step."
- Why unresolved: The analysis fixes a single noise scale t, while practical training aggregates gradients across multiple t values, potentially introducing interactions between regimes.
- What evidence would resolve it: Convergence analysis for gradient descent on the integrated score matching objective with multiple noise scales.

### Open Question 3
- Question: What happens for initializations in the intermediate regime between exponentially small and random far-from-ground-truth?
- Basis in paper: [explicit] "Understanding this intermediate regime remains an interesting open question."
- Why unresolved: The paper characterizes two extreme cases (all parameters converge vs. one converges with others diverging), but initializations between these remain unanalyzed.
- What evidence would resolve it: Characterization of parameter convergence behavior for initializations with moderate (polynomial) distances from zero.

### Open Question 4
- Question: Do stochastic gradient methods and adaptive optimizers (e.g., Adam) exhibit different convergence behavior than vanilla gradient descent in this setting?
- Basis in paper: [explicit] "Developing a theoretical framework that captures the behavior of these optimizers in the over-parameterized regime is an important open problem."
- Why unresolved: The analysis assumes population gradients with fixed step size GD, while practice uses SGD with adaptive learning rates and momentum.
- What evidence would resolve it: Convergence guarantees or empirical studies comparing SGD/Adam behavior to GD predictions in over-parameterized score matching.

## Limitations

- Stationary point analysis only characterizes existence but not practical frequency or avoidance strategies for nonzero stationary points in small noise regimes
- Winner-takes-all dynamics requires strict initial distance gaps and may be sensitive to noise and step size perturbations
- Exponential initialization necessity is proven via adversarial construction rather than characterizing all possible initialization schemes

## Confidence

- Large noise convergence: High - follows directly from gradient EM convergence guarantees
- Exponential initialization convergence: Medium - rigorous proof but requires strict bounds
- Winner-takes-all dynamics: Medium - proof assumes strict distance gaps that may not hold in practice
- Stationary point characterization: Medium - existence proven but practical implications unclear

## Next Checks

1. **Stationary point exploration**: Implement a grid search over initialization distances to empirically map the stationary point landscape. Vary the parameter spread parameter s in the configuration μ₁=s·e₁, μ₂=-s·e₁, others at 0, and measure loss convergence rates for s ∈ [1, 100]. This would validate whether stationary points at intermediate s values create practical training barriers.

2. **Robustness to initialization noise**: Test the winner-takes-all dynamics under noisy initializations. Start with n=10 parameters, initialize 9 at μ=5·e₁ and 1 at μ=4.9·e₁, then add Gaussian noise σ=0.1. Track how the convergence outcome (which parameter wins) varies with noise scale and whether multiple parameters can converge simultaneously under certain noise levels.

3. **Loss landscape visualization**: For d=1, n=3, plot the loss landscape L_t(μ₁,μ₂,μ₃) for small noise (t=0) and large noise (t=5). Identify and characterize the stationary points predicted by theory, then verify that gradient descent trajectories starting from random initializations follow predicted convergence patterns.