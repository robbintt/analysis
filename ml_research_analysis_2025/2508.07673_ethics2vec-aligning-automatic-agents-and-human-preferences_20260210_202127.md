---
ver: rpa2
title: 'Ethics2vec: aligning automatic agents and human preferences'
arxiv_id: '2508.07673'
source_url: https://arxiv.org/abs/2508.07673
tags:
- agent
- human
- automatic
- which
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying and comparing the
  ethical values embedded in automatic agents' decision-making strategies, which are
  often opaque to human users. The core idea is to extend the Anything2Vec approach
  to map an agent's decision-making or control law strategy to a multivariate vector
  representation that can be compared against human values.
---

# Ethics2vec: aligning automatic agents and human preferences

## Quick Facts
- arXiv ID: 2508.07673
- Source URL: https://arxiv.org/abs/2508.07673
- Authors: Gianluca Bontempi
- Reference count: 1
- One-line primary result: Extends Anything2Vec to quantify and compare the ethical values embedded in automatic agents' decision-making strategies using multivariate vector representations.

## Executive Summary
This paper introduces Ethics2Vec, a method for quantifying and comparing the ethical values embedded in automatic agents' decision-making strategies. The core idea is to map an agent's control law or decision-making strategy to a multivariate vector representation that can be compared against human values. For binary agents, this involves reconstructing a two-dimensional vector from the derivatives of false positive and false negative rates at the optimal threshold. For continuous control agents, it defines a vector of derivatives of risk probabilities with respect to control actions. The resulting Ethics2Vec representation enables assessment of alignment between an agent's ethical trade-offs and human preferences.

## Method Summary
Ethics2Vec extends the Anything2Vec approach to map an agent's decision-making or control law strategy to a multivariate vector representation. For binary agents, the method reconstructs a two-dimensional vector from the derivatives of false positive and false negative rates at the optimal threshold. For continuous control agents, it defines a vector of derivatives of risk probabilities with respect to control actions. The method allows quantifying how an agent trades between different ethical criteria and enables assessment of alignment with human preferences through vector comparison.

## Key Results
- Successfully recovers relative ethical weights from simulated binary agents using derivatives of false positive and false negative rates
- Distinguishes between aggressive and careful driving strategies in continuous control simulations
- Demonstrates the method's ability to quantify ethical trade-offs between accident risk and punctuality in self-driving car scenarios

## Why This Works (Mechanism)
The method works by leveraging the Anything2Vec framework to create vector representations of agent behavior. By computing derivatives of risk functions with respect to control actions or decision thresholds, it captures the sensitivity of an agent's ethical trade-offs. These derivatives form a vector that mathematically encodes how the agent balances different ethical criteria, enabling quantitative comparison with human preferences.

## Foundational Learning
- Vector embeddings: Represent complex decision strategies as mathematical vectors; needed for quantitative comparison of agent behaviors
- Partial derivatives: Measure sensitivity of risk functions to control actions; quick check: verify smoothness of risk functions
- Multi-criteria decision making: Framework for balancing multiple ethical objectives; needed to model real-world trade-offs
- ROC curve analysis: Tool for binary decision evaluation; needed to extract ethical weights from binary agents
- Risk function modeling: Define and estimate probabilities of ethical outcomes; quick check: validate risk probability estimates

## Architecture Onboarding
**Component Map:** Human values -> Risk criteria (r_i) -> Agent control law -> Ethics2Vec vector -> Alignment metric

**Critical Path:** The method requires defining human-relevant risk criteria, estimating the agent's control law, computing derivatives of risk functions, and comparing the resulting vector to human preferences.

**Design Tradeoffs:** The approach trades computational simplicity for the assumption of differentiable, well-behaved risk functions. It prioritizes mathematical tractability over handling discontinuous or highly non-linear agent behaviors.

**Failure Signatures:** If risk functions are non-differentiable or poorly estimated, the Ethics2Vec representation will be inaccurate. If the human-defined criteria set is incomplete compared to the agent's actual objectives, the alignment metric will be misleading.

**Three First Experiments:**
1. Apply Ethics2Vec to a binary classification agent with known ROC characteristics to verify recovery of theoretical ethical weights
2. Test the method on a continuous control agent with analytically defined risk functions to validate derivative calculations
3. Compare Ethics2Vec representations of two agents with clearly different control strategies (e.g., aggressive vs. conservative) to confirm distinguishability

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes smooth, well-behaved objective functions and relies on first-order derivatives, which may not hold for complex, real-world agents with nonlinear or discontinuous decision boundaries
- Experimental validation is limited to simple, simulated environments, raising concerns about robustness when applied to agents with intricate, multi-objective control laws
- The Ethics2Vec representation is a static snapshot that does not account for how an agent's ethical trade-offs might shift under varying environmental conditions or over time

## Confidence
**High** confidence in the mathematical derivation of the Ethics2Vec representation, as the approach is grounded in established vector embedding techniques and differential calculus.
**Medium** confidence in the method's practical applicability and robustness, due to the lack of empirical testing on complex, real-world agents and the unaddressed limitations regarding non-differentiable strategies and dynamic environments.
**Low** confidence in the interpretability and utility of the Ethics2Vec representation for human users, as the paper does not provide evidence or mechanisms for translating these abstract vectors into actionable ethical insights.

## Next Checks
1. Apply Ethics2Vec to a diverse set of real-world autonomous agents (e.g., different self-driving car models, robotic systems) and evaluate whether the recovered ethical vectors accurately reflect known differences in their control strategies and reported behavior
2. Test the method's robustness by introducing non-differentiable decision boundaries or abrupt changes in agent behavior, and assess whether the Ethics2Vec representation remains meaningful or breaks down
3. Conduct a user study where participants are presented with Ethics2Vec vectors and asked to interpret the ethical trade-offs encoded; measure whether human users can reliably understand and compare the ethical stances of different agents based solely on these vector representations