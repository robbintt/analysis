---
ver: rpa2
title: Advancing Reference-free Evaluation of Video Captions with Factual Analysis
arxiv_id: '2509.16538'
source_url: https://arxiv.org/abs/2509.16538
tags:
- caption
- video
- captions
- evaluation
- vc-inspector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces VC-Inspector, a reference-free video caption
  evaluator that focuses on factual accuracy. The approach addresses the challenge
  of evaluating video captions without human-annotated references by training a lightweight
  multimodal model to assess caption quality based on factual correctness.
---

# Advancing Reference-free Evaluation of Video Captions with Factual Analysis

## Quick Facts
- arXiv ID: 2509.16538
- Source URL: https://arxiv.org/abs/2509.16538
- Reference count: 18
- Primary result: VC-Inspector achieves 37.99 Kendall's tau correlation with human judgments on VATEX-Eval, outperforming reference-free baselines

## Executive Summary
This paper introduces VC-Inspector, a reference-free video caption evaluator that focuses on factual accuracy by training a lightweight multimodal model to assess caption quality based on factual correctness. The approach addresses the challenge of evaluating video captions without human-annotated references by using large language models to create synthetic captions with controllable factual errors. VC-Inspector outperforms existing reference-free methods and rivals reference-based metrics on the VATEX-Eval dataset, achieving Kendall's tau of 37.99 and Spearman's rho of 42.45. It generalizes across video and image caption benchmarks, offering interpretable evaluations with factual error explanations.

## Method Summary
VC-Inspector generates synthetic training data by systematically replacing objects and actions in ground truth captions with semantically-related alternatives using Llama-3.3-70B-Instruct, creating captions with known, graded factual errors. The model is trained to output both a quality score (1-5) and natural language explanation identifying incorrect elements. Qwen2.5-VL-3B/7B is fine-tuned with LoRA (rank=32, alpha=32) while freezing the vision encoder, using a prompt format that produces structured score+explanation outputs. The deterministic scoring scheme provides consistent quality assessment without requiring reference captions.

## Key Results
- Achieves 37.99 Kendall's tau and 42.45 Spearman's rho on VATEX-Eval, outperforming EMScore (22.88 τ) and CLIPScore (23.27 τ)
- Outperforms reference-based metrics (SPICE 23.29 τ, CIDEr 26.16 τ) in reference-free setting
- Generalizes to image captioning benchmarks (Flickr8K-Expert) with τ of 16.81-17.94
- Explanation training improves factual grounding (τ from 34.29 to 37.99) with moderate human agreement (Krippendorff's α=0.46)

## Why This Works (Mechanism)

### Mechanism 1
Controllable synthetic data generation enables scalable training of factual evaluators. An LLM extracts objects and actions from ground truth captions, then systematically replaces random subsets with semantically-related alternatives. The number of replacements directly determines the quality score via `1 - |R|/(|O|+|A|)`, creating captions with known, graded factual errors. This works because same-category replacements create semantically meaningful errors that generalize to real captioning mistakes.

### Mechanism 2
Joint score-and-explanation training improves factual grounding over scalar-only supervision. The model is trained to output both a quality score (1-5) AND a natural language explanation identifying which objects/actions are incorrect. This auxiliary supervision forces the model to learn explicit error detection rather than just pattern matching, acting as a regularizer that improves score prediction accuracy.

### Mechanism 3
Factual grounding outperforms embedding-based semantic alignment for caption evaluation. Rather than relying on CLIP-style embeddings that measure general semantic similarity, VC-Inspector explicitly learns to identify mismatches between video content and caption assertions. The deterministic scoring scheme provides a consistent scale that embedding-based methods lack, better detecting hallucinations and factual errors.

## Foundational Learning

- **Reference-free vs. reference-based evaluation**: Needed because ground truth captions aren't always available for "in-the-wild" videos. Quick check: Can you explain why BLEU/CIDEr require reference captions while CLIPScore doesn't?

- **Instruction tuning for multimodal models**: VC-Inspector fine-tunes Qwen2.5-VL using a specific prompt format to produce structured score+explanation outputs. Quick check: What is the difference between pretraining, fine-tuning, and instruction tuning for vision-language models?

- **Correlation metrics for evaluation meta-assessment**: The paper measures success via Kendall's τb and Spearman's ρ correlation with human judgments. Quick check: Why use rank correlation (Kendall/Spearman) instead of Pearson correlation for evaluating caption quality metrics?

## Architecture Onboarding

- **Component map**: Llama-3.3-70B-Instruct → ActivityNet captions → 44K synthetic pairs → Qwen2.5-VL-3B/7B with LoRA → Video (32 frames, 224×224) + caption text → Score (1-5) + Explanation

- **Critical path**: 1. Extract objects/actions from GT caption → 2. Randomly sample K objects + L actions → 3. Generate alternatives → 4. Create pseudo-caption → 5. Compute deterministic score → 6. Format explanation → 7. Instruction-tune Qwen2.5-VL

- **Design tradeoffs**: 44K subset vs. 218K balanced data for computational constraints; discretized scores (1-5) vs. continuous for LLM compatibility; objects/actions only vs. full factual elements to limit scope but cover major error types

- **Failure signatures**: Generic explanations without specific error identification indicate overfitting; correlation drops on YouCook2-FG-Eval suggest domain gaps; inference >0.5s/video indicates serving issues

- **First 3 experiments**: 1. Reproduce synthetic data pipeline on 100 ActivityNet captions; 2. Ablate explanation training to compare τb on VATEX-Eval; 3. Test cross-domain generalization on Flickr8K-Expert

## Open Questions the Paper Calls Out

- How can the factual analysis framework be expanded to explicitly model attributes, spatial relationships, and fine-grained temporal ordering?
- To what extent does training on synthetic, perturbed captions transfer to the detection of naturally occurring hallucinations in diverse real-world video domains?
- Does the deterministic linear penalty for factual errors align with human perception of error severity?
- Can VC-Inspector be effectively utilized as a reward model to improve video captioning models via Reinforcement Learning?

## Limitations

- Scope limited to object/action errors, cannot handle attributes, spatial relations, or temporal ordering
- Synthetic data may not fully capture real-world caption errors beyond controlled object/action swaps
- Moderate human agreement (α=0.46) on explanations suggests evaluation consistency challenges

## Confidence

- Reference-free factual accuracy mechanism: High
- Synthetic data generation scalability: Medium
- Explanation training benefits: Medium
- Cross-domain generalization: Medium

## Next Checks

1. Test VC-Inspector on caption errors involving attributes and temporal ordering to identify failure modes
2. Compare synthetic vs. human-written captions with controlled errors to validate generation pipeline representativeness
3. Evaluate model performance across varying video types (action-heavy vs. object-focused) to identify domain sensitivity