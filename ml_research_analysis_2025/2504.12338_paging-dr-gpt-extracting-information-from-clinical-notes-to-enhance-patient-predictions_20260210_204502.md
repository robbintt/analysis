---
ver: rpa2
title: 'Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient
  Predictions'
arxiv_id: '2504.12338'
source_url: https://arxiv.org/abs/2504.12338
tags:
- mortality
- patient
- patients
- risk
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using GPT-4o-mini to extract information from
  unstructured clinical notes for mortality prediction in cardiac ICU patients. By
  asking GPT three questions about risk of death, readmission, and overall health,
  the researchers created features for logistic regression models.
---

# Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions

## Quick Facts
- **arXiv ID**: 2504.12338
- **Source URL**: https://arxiv.org/abs/2504.12338
- **Reference count**: 40
- **Primary result**: GPT-4o-mini extracted features from discharge notes improved mortality prediction by 5.1 percentage points in AUC (0.874 vs 0.823) when combined with EMR data

## Executive Summary
This study demonstrates that GPT-4o-mini can effectively extract clinically relevant information from unstructured discharge notes to enhance mortality prediction in cardiac ICU patients. The researchers used three targeted questions about risk of death, readmission, and overall health to generate features for logistic regression models. When combined with standard EMR data, the GPT-extracted features achieved a 5.1 percentage point increase in AUC and a 29.9% improvement in positive predictive value for the highest-risk patients. The approach shows that LLMs can complement traditional EMR-based models by capturing information typically lost in unstructured clinical documentation.

## Method Summary
The study used GPT-4o-mini to extract features from unstructured discharge notes for 8,800 cardiac ICU patients from UCSF Medical Center (2014-2019). Three questions were posed to GPT about each patient's risk of death, risk of readmission, and overall health, with responses used to create categorical features. A manual review of 500 patient notes established gold-standard labels for model training. Logistic regression models were trained using EMR data alone, GPT features alone, and combined EMR+GPT features, with performance evaluated using AUC, precision, and positive predictive value.

## Key Results
- GPT-only models outperformed traditional EMR-based models (AUC: 0.821 vs 0.823)
- Combined EMR+GPT models achieved the best performance with 5.1 percentage point AUC increase (0.874 vs 0.823)
- 29.9% improvement in positive predictive value for the highest-risk patients (top 10% risk scores)
- GPT successfully extracted clinically relevant information about patient risk not captured in structured EMR data

## Why This Works (Mechanism)
The approach works because GPT-4o-mini can understand and synthesize complex clinical narratives that are difficult to capture in structured EMR fields. Discharge notes contain rich contextual information about patient conditions, treatment responses, and provider assessments that influence mortality risk but are not systematically recorded in structured formats. By asking targeted questions, the researchers converted unstructured text into standardized categorical features that could be combined with traditional EMR data for improved risk prediction.

## Foundational Learning
- **Clinical note structure**: Understanding discharge note formats and content organization is essential for designing effective extraction prompts
- **Risk prediction modeling**: Knowledge of logistic regression and performance metrics (AUC, precision, PPV) is needed to evaluate model improvements
- **Prompt engineering**: Careful question design is crucial for obtaining consistent, clinically relevant responses from LLMs
- **Gold-standard labeling**: Manual review processes are necessary to validate LLM outputs against expert clinical judgment
- **Multimodal feature integration**: Combining structured EMR data with unstructured text features requires careful data preprocessing and model architecture
- **Performance evaluation**: Understanding statistical significance and clinical relevance of prediction improvements is critical for real-world application

## Architecture Onboarding
**Component Map**: EMR Data -> Feature Extraction -> GPT-4o-mini API -> Categorical Features -> Logistic Regression Model
**Critical Path**: Structured EMR features + GPT-extracted categorical features → Logistic Regression → Risk Score → Mortality Prediction
**Design Tradeoffs**: GPT-4o-mini offers cost efficiency over larger models but may miss nuanced clinical details; multiple API calls per patient increase computational costs but enable detailed feature extraction
**Failure Signatures**: Inconsistent GPT responses to similar clinical narratives, missing critical information in discharge notes, or integration issues between structured and unstructured data sources
**First 3 Experiments**: 1) Test GPT consistency across identical clinical scenarios with minor wording variations, 2) Compare feature extraction performance across different medical specialties, 3) Evaluate cost-effectiveness of GPT-4o-mini vs. larger models for clinical note processing

## Open Questions the Paper Calls Out
None

## Limitations
- Single-center design at UCSF limits generalizability to other healthcare systems
- Restricted to cardiac ICU patients and 30-day mortality outcomes
- Manual review of only 5.7% of notes may not capture documentation heterogeneity
- Scalability and cost-effectiveness concerns for routine clinical implementation
- Single time point analysis doesn't account for temporal changes in patient condition

## Confidence
- **High confidence**: Core finding that LLM-extracted features improve mortality prediction when combined with EMR data
- **Medium confidence**: Absolute performance gains reported, given single-center design and specific patient population
- **Low confidence**: Scalability and cost-effectiveness for routine clinical implementation without further economic analysis

## Next Checks
1. External validation across multiple healthcare systems with different documentation practices and patient demographics
2. Prospective evaluation of model performance in real-time clinical settings to measure impact on care decisions and outcomes
3. Detailed cost-effectiveness analysis comparing computational and API costs against potential clinical benefits and alternative approaches