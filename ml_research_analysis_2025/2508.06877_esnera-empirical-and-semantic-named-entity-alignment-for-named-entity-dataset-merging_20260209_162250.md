---
ver: rpa2
title: 'ESNERA: Empirical and semantic named entity alignment for named entity dataset
  merging'
arxiv_id: '2508.06877'
source_url: https://arxiv.org/abs/2508.06877
tags:
- label
- similarity
- merging
- labels
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of merging multiple Named Entity
  Recognition (NER) datasets with inconsistent label schemas, which hinders the development
  of robust NER models. The proposed ESNERA method combines empirical similarity (measuring
  entity overlap through model predictions) and semantic similarity (capturing label
  semantics via contextual embeddings) to automatically align and merge labels across
  datasets.
---

# ESNERA: Empirical and semantic named entity alignment for named entity dataset merging

## Quick Facts
- arXiv ID: 2508.06877
- Source URL: https://arxiv.org/abs/2508.06877
- Reference count: 40
- Primary result: ESNERA merges 15 labels across Chinese NER datasets with comparable micro-F1 scores (0.79) to manual merging

## Executive Summary
This paper addresses the challenge of merging multiple Named Entity Recognition (NER) datasets with inconsistent label schemas, which hinders the development of robust NER models. The proposed ESNERA method combines empirical similarity (measuring entity overlap through model predictions) and semantic similarity (capturing label semantics via contextual embeddings) to automatically align and merge labels across datasets. Using a greedy pairwise merging strategy and grid search optimization, ESNERA maximizes merged label coverage while maintaining NER performance within 2% of baseline models. Experiments demonstrate that ESNERA successfully merges 15 labels across three Chinese NER datasets (OntoNotes, CLUENER, BosonNER) with comparable micro-F1 scores (0.79) to manual merging, outperforming independent training. Additionally, ESNERA effectively adapts to low-resource domains, improving financial NER performance by 3%.

## Method Summary
ESNERA addresses the challenge of merging heterogeneous NER datasets by computing two types of label similarity: empirical (based on model prediction overlap) and semantic (based on contextual embeddings). The method uses a greedy pairwise merging strategy that selects datasets to merge based on unidirectional empirical similarity sums, then applies grid search optimization over parameters λ (balancing empirical vs semantic weights) and τ (merge threshold) to maximize label coverage while maintaining F1 performance within 2% of baseline. The approach includes a label augmentation step using pseudo-labeling to handle missing entity types. The final model is trained on the unified corpus with the aligned label space.

## Key Results
- Successfully merged 15 labels across three Chinese NER datasets (OntoNotes, CLUENER, BosonNER)
- Achieved micro-F1 score of 0.79, comparable to manual merging and only 1% below baseline models
- Outperformed independent training of separate datasets
- Improved financial NER performance by 3% in low-resource domain adaptation
- Ablation study confirmed complementary roles of empirical and semantic similarities

## Why This Works (Mechanism)

### Mechanism 1: Empirical Similarity Captures Annotation Practice Alignment
Measuring entity prediction overlap reveals whether labels in different datasets encode similar annotation behaviors, independent of label names. Train a NER model on source dataset Ds, predict on target dataset Dt's training set, then compute Sempirical(Ls, Lt) = |Npred(Ls) ∩ Ntrue(Lt)| / |Npred(Ls)|. High scores indicate that when the source-trained model predicts Ls, those entities are actually labeled Lt in the target schema. Core assumption: Annotation consistency within each dataset is high enough that model predictions generalize to annotation patterns. Evidence anchors: [abstract] "combines empirical similarity (measuring entity overlap through model predictions)"; [section 3.1.1] Formula (1) and the address→GPE example (70/100 = 0.7); [corpus] Related work on pseudo-labeling (Arazo et al.) confirms this is a recognized but noise-prone approach. Break condition: Low annotation quality in either dataset produces unreliable empirical signals; very small datasets yield unstable predictions.

### Mechanism 2: Semantic Similarity Captures Contextual Meaning Proximity
Contextual embeddings reveal whether entities under different labels occupy similar semantic regions, complementing empirical overlap. Extract BERT embeddings for all entities under each label, apply centralization (subtract mean) and normalization (L2=1), compute mean vector per label, then Ssemantic(Ls, Lt) = Vs · Vt. Core assumption: Embedding space clusters entities by semantic role sufficiently well for label-level aggregation to be meaningful. Evidence anchors: [abstract] "semantic similarity (capturing label semantics via contextual embeddings)"; [section 3.1.2] Steps 1-3 with formulas (2)-(7); Figure 5 shows semantic clustering (e.g., person name↔name = 0.93); [corpus] Sentence-BERT work (Reimers & Gurevich) supports embedding-based similarity; no direct corpus evidence for this specific aggregation method. Break condition: Domain shift between datasets causes embedding distributions to diverge; rare entity types have insufficient samples for stable mean vectors.

### Mechanism 3: Greedy Pairwise Merging with Grid Search Optimization
Sequential pairwise alignment with parameter optimization maximizes label coverage while bounding performance degradation. (1) Compute unidirectional empirical similarity sums for all dataset pairs; (2) Select highest-sum pair as initial merge; (3) Iteratively merge next highest-similarity dataset with intermediate result; (4) Grid search over λ∈[0.3-0.7] and τ∈[0.1-0.9] to maximize merged labels subject to F1 drop <2%. Core assumption: Local greedy choices approximate global optimum; the 2% F1 tolerance is a meaningful quality threshold. Evidence anchors: [abstract] "greedy pairwise merging strategy and grid search optimization"; [section 3.2.2] Four-step greedy algorithm; section 3.3.2 grid search procedure; [section 4.4] Table 4 shows 15 labels merged at λ=0.3-0.4, τ=0.4 with Micro-F1=0.79 (1% drop from baseline); [corpus] No direct corpus comparison for this specific greedy approach. Break condition: Exponential path explosion for >5 datasets may require pruning heuristics; adversarial label schemas could trap greedy selection in suboptimal paths.

## Foundational Learning

- Concept: Pseudo-labeling and cross-dataset transfer
  - Why needed here: Empirical similarity depends on training a model on one dataset and predicting on another; understanding prediction noise and confirmation bias is critical for interpreting Sempirical scores.
  - Quick check question: If a source-trained model achieves 60% precision on target labels, what does that imply about annotation alignment?

- Concept: Contextual embeddings and semantic space geometry
  - Why needed here: Semantic similarity uses BERT embeddings; centralization and normalization affect how cosine similarity behaves—without this, offset and scale differences can dominate comparisons.
  - Quick check question: Why subtract the mean embedding before computing cosine similarity between label vectors?

- Concept: Set-theoretic label relations (equivalence, subset, overlap, disjoint)
  - Why needed here: ESNERA doesn't explicitly classify relations but uses similarity scores to approximate them; understanding these categories helps interpret why some merges succeed (PERSON↔name) and others fail (PRODUCT drops from 0.58 to 0.34).
  - Quick check question: When Sempirical is high but Ssemantic is low, what type of relation might exist?

## Architecture Onboarding

- Component map:
Input: Multiple NER datasets (Ds, Dt, ...) with heterogeneous label schemas
  ↓
[Label Similarity Computation]
  ├── Empirical Similarity Module
  │     Train model on Ds → Predict on Dt → Compute overlap ratios
  ├── Semantic Similarity Module
  │     Extract BERT embeddings → Centralize & normalize → Cosine similarity
  └── Merged Similarity: Smerge = (1-λ)·Ssemantic + λ·Sempirical
  ↓
[Merging Path Selection]
  Greedy selection based on unidirectional empirical similarity sums
  ↓
[Grid Search Optimization]
  Iterate over λ, τ → Merge labels where Smerge > τ → Evaluate F1 → Select max merged labels with F1 drop <2%
  ↓
[Label Augmentation] (if needed)
  Pseudo-label missing entity types in target using source-trained model
  ↓
Output: Unified NER corpus with aligned label space

- Critical path: The grid search (section 3.3.2) is the bottleneck—it requires training and evaluating a NER model for each (λ, τ) combination. With 5 λ values × 9 τ values = 45 configurations per merge stage, and two stages (CLUENER→BosonNER→OntoNotes), this is ~90 model training runs.

- Design tradeoffs:
  - λ=0 (semantic-only) vs λ=1 (empirical-only): Ablation (Table 8) shows both reduce merged labels from 15 to 13, with F1 stable. Semantic-only misses company→company name; empirical-only misses movie→product name.
  - τ threshold: Lower τ merges more labels but risks semantic drift (e.g., PRODUCT F1 dropped to 0.34 due to fine-grained merges). Higher τ is conservative but may under-merge.
  - Greedy vs exhaustive path search: Greedy scales linearly; exhaustive is O(n!) for n datasets.

- Failure signatures:
  - PRODUCT F1 = 0.34 (vs baseline 0.58): Fine-grained labels (book, movie, game) merged into coarse PRODUCT introduced semantic drift (section 4.4, Table 5).
  - LANGUAGE, LAW F1 drops >0.09: Very low support counts (8, 17) make these labels sensitive to merge noise.
  - NUM F1 dropped 0.06 on FinReportNER: Source datasets lacked amount-type annotations, limiting pseudo-label quality (section 4.5, Table 7).

- First 3 experiments:
1. Reproduce empirical similarity matrix: Train BERT-CRF on CLUENER, predict on BosonNER training set, compute Sempirical for all label pairs. Compare heatmap to Figure 4a. Deviation indicates data or implementation issues.
2. Single-stage merge with fixed parameters: Set λ=0.4, τ=0.4, merge CLUENER→BosonNER. Train on merged corpus, evaluate on BosonNER test set. Expect F1 within 2% of BosonNER baseline.
3. Ablation check: Run merge with λ=0 (semantic-only) and λ=1 (empirical-only). Verify that merged label counts drop as reported in Table 8. If not, check embedding extraction or pseudo-labeling pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hierarchical label modeling resolve the performance degradation observed when merging fine-grained entity types into coarse-grained categories?
- Basis in paper: [explicit] The conclusion notes that "poor performance of some labels indicates challenges" and suggests future research explore "hierarchical label modeling to determine more specific label relations."
- Why unresolved: The current method treats label relations (equivalence, subset, etc.) via similarity thresholds, but specific mappings (e.g., merging `book`, `movie`, `game` into `PRODUCT`) caused a significant F1 drop (0.58 to 0.34), suggesting the flat merging strategy is insufficient for complex hierarchies.
- What evidence would resolve it: Experiments utilizing a taxonomy-aware loss function or hierarchical classification layer that preserves the distinction between fine-grained source labels and coarse-grained target labels.

### Open Question 2
- Question: How does ESNERA perform in multilingual or cross-lingual NER scenarios where embedding spaces and annotation norms differ significantly?
- Basis in paper: [explicit] The conclusion identifies "extending this method to multilingual scenarios and verifying its applicability in cross-language NER tasks" as an important future direction.
- Why unresolved: The current study is restricted to Chinese datasets using Chinese-BERT-wwm-ext. It is unclear if semantic similarity calculations remain robust when aligning labels across languages with distinct morphological structures or lacking direct translation equivalents.
- What evidence would resolve it: Successful application of the ESNERA framework to merge datasets from distinct linguistic families (e.g., aligning English CoNLL-2003 with Chinese OntoNotes) without significant loss in alignment accuracy.

### Open Question 3
- Question: Does the unidirectional greedy merging strategy result in suboptimal alignments compared to a globally optimized merging path?
- Basis in paper: [inferred] The methodology states that the greedy strategy is used "to circumvent the combinatorial explosion resulting from a one-time global merge," implying a trade-off between computational efficiency and alignment optimality.
- Why unresolved: By prioritizing the highest pairwise similarity iteratively, the algorithm might make early merging decisions that limit the quality of subsequent integrations, a limitation not tested against global optimization methods in the experiments.
- What evidence would resolve it: A comparative study evaluating the final merged corpus quality when using the greedy approach versus an exhaustive search or graph-based optimization algorithm on a small-scale dataset.

### Open Question 4
- Question: Do the optimal interpolation parameters (λ) and merging thresholds (τ) generalize effectively to domains outside of news and finance without requiring a new grid search?
- Basis in paper: [inferred] Section 4.5 suggests that matching hyperparameters between the main experiment and the financial domain indicate "cross-dataset generalization capabilities," but this is based on a limited sample of two related tasks.
- Why unresolved: The assertion that "grid search can be skipped in other scenarios" relies on a single transfer learning test (FinReportNER). It remains unverified if these static parameters are robust for domains with vastly different entity distributions, such as biomedical or legal texts.
- What evidence would resolve it: Testing the fixed optimal parameters (λ=0.4, τ=0.4) on a diverse out-of-domain dataset (e.g., biomedical NER) to see if performance degrades compared to a grid-searched baseline.

## Limitations

- The empirical similarity computation relies on noisy pseudo-labeling when source and target datasets have significant domain differences
- The greedy merging strategy may not find globally optimal label alignments for more than three datasets
- The 2% F1 tolerance threshold appears somewhat arbitrary and may not generalize across different NER domains or languages
- Performance on extremely low-resource entity types (like LANGUAGE and LAW with <20 samples) remains questionable

## Confidence

- High confidence: The semantic similarity mechanism and its implementation (BERT embedding extraction, centralization, and normalization steps)
- Medium confidence: The empirical similarity computation and greedy merging algorithm, given limited direct comparison to alternatives
- Medium confidence: The 2% F1 threshold and its appropriateness for quality control
- Low confidence: The method's generalization to datasets with significantly different annotation guidelines or to non-Chinese languages

## Next Checks

1. Test ESNERA on a controlled synthetic dataset where the true label alignment is known, to verify whether the method correctly identifies equivalent labels and rejects dissimilar ones
2. Implement an exhaustive (rather than greedy) search for merging paths on a small set of datasets to quantify the optimality gap of the greedy approach
3. Conduct sensitivity analysis on the λ and τ parameters by testing intermediate values beyond the reported grid search range to check for local optima