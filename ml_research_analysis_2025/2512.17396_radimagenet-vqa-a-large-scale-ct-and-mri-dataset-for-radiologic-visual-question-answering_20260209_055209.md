---
ver: rpa2
title: 'RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question
  Answering'
arxiv_id: '2512.17396'
source_url: https://arxiv.org/abs/2512.17396
tags:
- pathology
- radimagenet-vqa
- medical
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces RadImageNet-VQA, a large-scale dataset for\
  \ training and evaluating radiologic visual question answering on CT and MRI images.\
  \ The dataset includes 750K images and 7.5M question-answer samples across three\
  \ tasks\u2014abnormality detection, anatomy recognition, and pathology identification\u2014\
  spanning eight anatomical regions and 97 pathology categories."
---

# RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering

## Quick Facts
- arXiv ID: 2512.17396
- Source URL: https://arxiv.org/abs/2512.17396
- Reference count: 23
- Primary result: Large-scale CT/MRI VQA dataset with 750K images and 7.5M QA samples; existing VLMs perform well on anatomy and basic abnormality detection but struggle with fine-grained pathology identification, especially in open-ended settings.

## Executive Summary
RadImageNet-VQA introduces a large-scale benchmark for radiologic visual question answering on CT and MRI images. The dataset addresses a critical gap in medical VQA by providing structured question-answer pairs for abnormality detection, anatomy recognition, and pathology identification across eight anatomical regions. The paper demonstrates that while general-purpose VLMs can learn basic radiologic tasks, pathology identification remains a significant challenge, particularly for open-ended questions. Text-only analysis confirms the dataset minimizes shortcutting, requiring genuine image-grounded reasoning for accurate answers.

## Method Summary
The dataset is generated from RadImageNet using template-based VQA generation with anatomy-constrained distractors to suppress linguistic shortcuts. Three task types are supported: abnormality detection (yes/no questions), anatomy recognition (multiple-choice), and pathology identification (both multiple-choice and open-ended). The benchmark evaluation uses a 1,000-image, 9,000-QA pair test set with stratified coverage across anatomical regions and pathology categories. Models are evaluated using exact match for closed-ended questions, LLM-as-a-judge (Mistral-Large 2.1) for open-ended questions, and rule-based parsing for multiple-choice options.

## Key Results
- General-purpose VLMs achieve high accuracy on anatomy recognition (90%+) and basic abnormality detection (70-80%) but struggle with pathology identification (20-30% for open-ended)
- Fine-tuning on RadImageNet-VQA yields substantial improvements across all tasks (+20% average), though pathology identification remains the primary bottleneck
- Medical-pretrained vision encoders (MedSigLIP) provide no advantage over general-purpose encoders (SigLIP) for CT/MRI VQA tasks
- Text-only accuracy collapses to near-random levels (2-10% for open-ended, ~25% for multiple-choice), confirming the dataset minimizes linguistic shortcuts

## Why This Works (Mechanism)

### Mechanism 1: Template-based VQA generation with anatomy-constrained distractors suppresses linguistic shortcuts and forces image-grounded reasoning
- **Core assumption:** Models cannot reliably infer correct answers from question phrasing or option distributions alone when distractors are semantically plausible within the same anatomical context
- **Evidence anchors:** Text-only analysis reveals accuracy collapses to near-random without image inputs; distractors are restricted to clinically plausible diseases from the same region
- **Break condition:** If distractor pools become too large or include semantically implausible options, models may exploit option elimination heuristics rather than visual reasoning

### Mechanism 2: Two-phase training (visual alignment → instruction tuning) transfers general VLM capabilities to radiologic tasks, but medical-pretrained vision encoders provide no advantage over general-purpose ones for CT/MRI VQA
- **Core assumption:** Radiologic VQA capability depends more on task-specific instruction tuning than on domain-specialized visual representations, at least for current medical encoder training regimes
- **Evidence anchors:** Standard SigLIP outperforms medically pre-trained MedSigLIP for VQA with CT and MRI; Lingshu-7B and Qwen2.5-VL-7B converge to nearly identical performance
- **Break condition:** If medical encoders were pretrained on large-scale, CT/MRI-only corpora with matched pathology labels, transfer gains might emerge

### Mechanism 3: Text-only ablation reveals residual shortcut behavior patterns (conservative defaults vs. distributional guessing), distinguishing dataset quality from model strategy artifacts
- **Core assumption:** A high-quality dataset should force text-only accuracy to random baseline across both normal and abnormal subsets; systematic deviations indicate either dataset bias or model strategy artifacts
- **Evidence anchors:** Medical VLMs default to "no pathology seen" without images—high accuracy on normal cases, near-zero on abnormal; general-purpose models spread predictions uniformly
- **Break condition:** If questions explicitly name rare pathologies with unique semantic cues, text-only models could achieve above-random accuracy via label priors even with perfect distractor design

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architecture**
  - **Why needed here:** The paper evaluates and fine-tunes VLMs that combine a vision encoder (SigLIP) with an LLM via a projection adapter. Understanding this separation is essential for interpreting why encoder choice and training phases affect results.
  - **Quick check question:** If you freeze the LLM and only train the vision encoder and projector, what capability is being adapted and what is preserved?

- **Concept: Linguistic Shortcuts in VQA**
  - **Why needed here:** The paper's central claim is that RadImageNet-VQA minimizes shortcuts present in prior datasets. Recognizing shortcut patterns (label-frequency bias, option elimination, anatomy-to-pathology priors) is critical for interpreting benchmark results and designing better datasets.
  - **Quick check question:** Why would a model achieve 30% accuracy on a yes/no question without seeing the image, and what does that imply about the dataset?

- **Concept: Instruction Tuning vs. Visual Alignment**
  - **Why needed here:** The two-phase training paradigm separates visual feature adaptation from task-specific response learning. Misunderstanding this leads to incorrect conclusions about what data or training choices drive improvements.
  - **Quick check question:** In which phase would adding more diverse VQA samples (vs. image-caption pairs) most improve pathology identification performance?

## Architecture Onboarding

- **Component map:** DICOM → 2D slice extraction → RadImageNet metadata → Template engine → QA templates + caption templates → Distractor sampling → RadImageNet-VQA corpus → Vision encoder (SigLIP/MedSigLIP) → Projection adapter → LLM → Benchmark evaluation

- **Critical path:** 
  1. Verify RadImageNet license and access (expert-curated, requires permission per acknowledgments)
  2. Run template generation pipeline on CT/MRI training split → validate distractor constraints
  3. Execute two-phase training: alignment (1 epoch, LLM frozen) → instruction tuning (2 epochs, full model)
  4. Run text-only ablation on benchmark subset to confirm shortcut suppression
  5. Compare SigLIP vs. MedSigLIP encoder initialization with identical training recipe

- **Design tradeoffs:**
  - **Scale vs. annotation quality:** RadImageNet provides expert labels but uses template-generated (not human-authored) questions—scalable but may lack linguistic nuance
  - **Distractor plausibility vs. difficulty:** More plausible distractors increase task difficulty but risk frustrating valid visual reasoning if semantic boundaries are ambiguous
  - **Alternating vs. mixed sampling:** Alternating reduces inter-dataset interference under small-batch constraints but may slow convergence on diverse corpora

- **Failure signatures:**
  - Text-only accuracy significantly above random (>15% on open-ended, >30% on MC) indicates residual shortcuts
  - Medical VLM collapsing on abnormal cases in text-only mode suggests over-conservative safety alignment or pathology-prior bias
  - Fine-tuned pathology accuracy <50% on MC despite >90% anatomy accuracy indicates visual feature limitations, not training methodology

- **First 3 experiments:**
  1. **Baseline shortcut audit:** Run text-only evaluation on VQA-RAD, SLAKE, and RadImageNet-VQA benchmark with 2–3 VLMs to replicate shortcut suppression finding
  2. **Encoder ablation:** Fine-tune LLaVA-OneVision with SigLIP vs. MedSigLIP using identical data and hyperparameters; compare per-task deltas
  3. **Distractor sensitivity:** Vary "no pathology seen" frequency in MC options (0%, 30%, 100%) and measure accuracy impact on medical vs. general-purpose VLMs to confirm bias hypothesis

## Open Questions the Paper Calls Out

### Open Question 1: Why do medically pretrained vision encoders (e.g., MedSigLIP) provide no advantage over general-purpose encoders for CT/MRI VQA tasks?
- **Basis in paper:** Section 4.3.3 states standard SigLIP outperforms medically pre-trained MedSigLIP for VQA with CT and MRI
- **Why unresolved:** The paper observes this counterintuitive result but only speculates on the cause; no controlled experiments isolate whether the issue is modality mismatch, pretraining data composition, or encoder architecture
- **What evidence would resolve it:** Ablation studies comparing vision encoders pretrained exclusively on CT/MRI versus mixed medical sources, and analysis of feature alignment between encoder representations and radiologic visual features

### Open Question 2: What architectural or training innovations are needed to breakthrough the pathology identification bottleneck?
- **Basis in paper:** Section 4.1 and 4.3.2 identify pathology identification as the "primary bottleneck" and "major unsolved problem"
- **Why unresolved:** The paper demonstrates the problem's persistence but does not investigate whether the limitation stems from visual feature resolution, class imbalance, insufficient disease-specific supervision, or fundamental model capacity
- **What evidence would resolve it:** Systematic experiments varying model scale, adding disease-specific visual tokens, incorporating hierarchical pathology taxonomies, or introducing contrastive learning on hard negative pathology pairs

### Open Question 3: Does safety alignment in proprietary models (GPT-5, Gemini) suppress abnormality detection, causing below-random performance?
- **Basis in paper:** Section 4.1 states aggressive safety alignment may suppress the model's willingness to identify abnormalities
- **Why unresolved:** This is stated as intuition/hypothesis without empirical validation; the paper does not test whether prompting strategies or alignment adjustments can recover performance
- **What evidence would resolve it:** Controlled experiments with varying safety configurations, analysis of refusal patterns in model outputs, and comparison with unaligned or weakly-aligned model variants on the same abnormality detection task

## Limitations
- Pathology identification remains the primary bottleneck, with open-ended accuracy below 30% even after fine-tuning
- Medical-pretrained vision encoders provide no advantage over general-purpose ones for CT/MRI tasks
- Proprietary models with aggressive safety alignment may suppress abnormality detection performance

## Confidence
- Dataset construction methodology: High
- Template-based generation with anatomy-constrained distractors: High
- Two-phase training approach: High
- Medical encoder ablation results: High
- Text-only shortcut analysis: High
- Pathology identification bottleneck characterization: High
- Safety alignment impact on abnormality detection: Medium (intuition-based)

## Next Checks
1. Verify RadImageNet-VQA dataset access and benchmark split structure (1,000 images, 9,000 QA pairs)
2. Run text-only ablation on benchmark subset with 2-3 VLMs to confirm shortcut suppression (should drop to ~25% MC, <10% open-ended)
3. Compare zero-shot performance on benchmark with InternVL3.5-8B or Qwen2.5-VL-7B-Instruct (expect anatomy MC ~90%+, pathology open-ended <20%)