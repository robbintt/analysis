---
ver: rpa2
title: Generative Dataset Distillation Based on Self-knowledge Distillation
arxiv_id: '2501.04202'
source_url: https://arxiv.org/abs/2501.04202
tags:
- dataset
- distillation
- logits
- synthetic
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generative dataset distillation method that
  improves upon existing techniques by integrating self-knowledge distillation and
  logits standardization. The method trains a GAN to generate synthetic datasets and
  aligns them with the original dataset by matching the distributions of prediction
  logits after standardization.
---

# Generative Dataset Distillation Based on Self-knowledge Distillation

## Quick Facts
- arXiv ID: 2501.04202
- Source URL: https://arxiv.org/abs/2501.04202
- Reference count: 0
- Improves dataset distillation accuracy by 1-3% through logits standardization and model pool randomization

## Executive Summary
This paper presents a generative dataset distillation method that improves upon existing techniques by integrating self-knowledge distillation and logits standardization. The method trains a GAN to generate synthetic datasets and aligns them with the original dataset by matching the distributions of prediction logits after standardization. This standardization ensures consistency in logits range, reducing variability and improving the accuracy of alignment. Experiments on MNIST, FashionMNIST, and CIFAR-10 datasets show that the proposed method outperforms state-of-the-art methods, with notable improvements in distillation performance and cross-architecture generalization. The standardization step particularly enhances model accuracy and stability across different neural network architectures.

## Method Summary
The method trains a conditional GAN generator to produce synthetic images, then refines it through a distillation phase that aligns synthetic and original data distributions. A model pool containing ConvNet3, ResNet10, and ResNet18 is used to extract logits from both original and synthetic batches. The logits are standardized using Z(x; τ) = (x - mean(x)) / std(x) × τ with τ=2, then probability distributions are computed via softmax. A self-knowledge distillation loss based on KL divergence between these distributions is combined with the GAN loss to update the generator. The λ_SKD hyperparameter is set to 0.001 for MNIST and 0.01 for FashionMNIST and CIFAR-10.

## Key Results
- Achieves 0.9% accuracy improvement on CIFAR-10 IPC=10 through logits standardization
- Shows consistent 67-70% cross-architecture accuracy across ConvNet3, ResNet18, AlexNet, and VGG11
- Outperforms state-of-the-art methods on MNIST, FashionMNIST, and CIFAR-10 datasets
- Demonstrates improved model accuracy and stability across different neural network architectures

## Why This Works (Mechanism)

### Mechanism 1: Logits Standardization Before Distribution Matching
Standardizing logits to a consistent range before softmax distribution matching reduces alignment bias and improves accuracy. The method applies Z(x; τ) = (x - mean(x)) / std(x) × τ to both original and synthetic logits, normalizing scale differences before computing probability distributions. This prevents cases where similar logits ranges cause incorrect matching despite different semantic meanings.

### Mechanism 2: Distribution Matching via Self-Knowledge Distillation Loss
Using KL divergence on probability distributions captures inter-class relationships better than point-wise MSE on raw logits. Instead of L2 distance on logits vectors, the method computes L_SKD = Σ_k d(x^O)(k) log(d(x^O)(k) / d(x^S)(k)), where d represents softmax probabilities after standardization. This preserves the relative confidence structure across all classes.

### Mechanism 3: Model Pool Randomization for Cross-Architecture Generalization
Randomly selecting models from a diverse pool during distillation forces the generator to produce architecture-agnostic synthetic data. Each training iteration randomly samples from {ConvNet3, ResNet10, ResNet18}, so the generator cannot overfit to any single architecture's feature preferences. The synthetic data must satisfy distribution matching across varied inductive biases.

## Foundational Learning

- **Knowledge Distillation Fundamentals**
  - Why needed here: The core loss function assumes familiarity with why soft labels transfer more information than hard labels.
  - Quick check question: Why does a soft label distribution [0.7, 0.2, 0.1] contain more useful information for training than a hard label [1, 0, 0]?

- **GAN Training Dynamics**
  - Why needed here: The generator must first be pre-trained adversarially before distillation refinement begins.
  - Quick check question: What happens to synthetic data quality if the discriminator collapses early in GAN training?

- **Temperature Scaling in Softmax**
  - Why needed here: The τ parameter in standardization directly affects distribution smoothness and matching sensitivity.
  - Quick check question: If τ is set too high (e.g., τ=10), what happens to the entropy of the resulting probability distribution?

## Architecture Onboarding

- **Component map:**
  Input: Noise z + Label y → Concatenate → Generator G → Synthetic Image
                                                         ↓
  Original Image ────┬───→ Random Model Selection ──→ Logits → Standardize
                     │                                         ↓
  Synthetic Image ───┘                           Softmax → KL Divergence Loss
                                                           ↓
                                        Combined with GAN Loss → Update G

- **Critical path:**
  1. Pre-train conditional GAN (generator G + discriminator D) to convergence using standard adversarial loss
  2. Begin distillation phase: freeze discriminator, keep generator trainable
  3. Each iteration: sample model from pool → forward pass original and synthetic batches → standardize logits → compute L_SKD → update generator with L_total = L_CGAN + λ_SKD × τ² × L_SKD

- **Design tradeoffs:**
  - λ_SKD weight (0.001 for MNIST, 0.01 for FashionMNIST/CIFAR-10): Higher values prioritize distribution matching but may reduce visual fidelity
  - Temperature τ (set to 2): Controls distribution smoothness; higher values blur class boundaries
  - IPC (images per class): Lower IPC increases compression but requires more precise distillation
  - Pool diversity vs. compute: More architectures improve generalization but slow training

- **Failure signatures:**
  - Generator mode collapse: All synthetic images for a class become nearly identical
  - Architecture-specific overfitting: High accuracy on pool models, significant drop on held-out architectures
  - Numerical overflow: Very small τ causes standardized logits to explode before softmax
  - Distribution mismatch without convergence: L_SKD plateaus high, indicating synthetic data never captures original structure

- **First 3 experiments:**
  1. **Standardization ablation**: Compare "Ours" vs. "Ours (No Stand.)" on CIFAR-10 IPC=10 to isolate the standardization contribution (paper shows ~0.9% improvement)
  2. **Temperature sweep**: Test τ ∈ {1.0, 1.5, 2.0, 3.0, 5.0} on FashionMNIST to validate the τ=2 choice and observe sensitivity
  3. **Cross-architecture holdout**: Train using only {ConvNet3, ResNet10} pool, evaluate on ResNet18 and AlexNet to verify that performance gains hold for unseen architectures

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. The limitations section below addresses key unresolved issues.

## Limitations
- Method validation limited to simple, low-resolution datasets (MNIST, FashionMNIST, CIFAR-10)
- Unclear if cross-architecture generalization holds for non-CNN architectures like Vision Transformers
- Method requires manual tuning of temperature parameter τ and weight parameter λ_SKD

## Confidence
- **Medium** in logits standardization mechanism - ablation shows improvement but theoretical justification underdeveloped
- **Medium** in model pool randomization mechanism - cross-architecture results compelling but diversity hypothesis not directly tested
- **Low** in overall methodology reproducibility - critical implementation details missing (model pool status, optimizer specs, batching strategy)

## Next Checks
1. **Standardization ablation with alternative normalization**: Compare logits standardization against batch normalization and layer normalization in the distillation loss to isolate whether the specific standardization approach provides unique benefits.
2. **Model pool architectural similarity test**: Replace ResNet18 in the pool with another ResNet variant (e.g., ResNet34) to test whether performance drops when pool diversity decreases, directly validating the diversity hypothesis.
3. **Teacher model quality dependency**: Train the distillation process with pool models at different quality levels (80% vs. 95% accuracy) to quantify how sensitive the method is to teacher model quality, revealing whether it truly captures structural relationships or simply memorizes good teacher outputs.