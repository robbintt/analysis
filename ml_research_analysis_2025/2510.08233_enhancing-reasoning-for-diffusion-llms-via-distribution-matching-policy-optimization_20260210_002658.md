---
ver: rpa2
title: Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization
arxiv_id: '2510.08233'
source_url: https://arxiv.org/abs/2510.08233
tags:
- arxiv
- preprint
- diffusion
- policy
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distribution Matching Policy Optimization
  (DMPO), a novel reinforcement learning framework specifically designed for diffusion
  large language models (dLLMs). The key challenge addressed is adapting RL algorithms
  for dLLMs, which differ from autoregressive models in their bidirectional generation
  and forward denoising process.
---

# Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization

## Quick Facts
- arXiv ID: 2510.08233
- Source URL: https://arxiv.org/abs/2510.08233
- Reference count: 40
- Primary result: DMPO achieves up to 42.9% accuracy improvement over previous dLLM RL methods on reasoning benchmarks

## Executive Summary
This paper introduces Distribution Matching Policy Optimization (DMPO), a novel reinforcement learning framework specifically designed for diffusion large language models (dLLMs). The key innovation addresses the challenge of adapting RL algorithms for dLLMs, which differ from autoregressive models in their bidirectional generation and forward denoising process. DMPO shifts from reward maximization to distribution matching, using importance sampling and weighted denoising cross-entropy (WDCE) loss to match the policy distribution to the optimal, reward-tilted one. A critical implementation challenge with small batch sizes is addressed through weight baseline subtraction techniques.

## Method Summary
DMPO introduces a distribution matching approach for reinforcement learning with diffusion LLMs. Unlike traditional RL methods that maximize rewards directly, DMPO uses weighted denoising cross-entropy (WDCE) loss to match the policy distribution to the optimal reward-tilted distribution. The method employs importance sampling to compute the loss and addresses the challenge of small batch sizes through weight baseline subtraction. This framework is specifically designed to handle the unique characteristics of dLLMs, including their bidirectional generation capability and forward denoising process, making it distinct from RL approaches developed for autoregressive models.

## Key Results
- DMPO achieves up to 42.9% accuracy improvement over the best previous dLLM RL method
- DMPO shows 55.8% improvement over the base model without requiring supervised fine-tuning
- The method demonstrates consistent performance gains across four reasoning benchmarks: GSM8K, MATH500, Countdown, and Sudoku

## Why This Works (Mechanism)
DMPO works by reframing the RL objective from reward maximization to distribution matching. Instead of directly optimizing for rewards, it matches the current policy distribution to the optimal policy distribution that would be induced by the reward function. This is achieved through weighted denoising cross-entropy loss with importance sampling weights derived from the reward function. The distribution matching approach is particularly well-suited for dLLMs because it naturally handles the bidirectional generation process and the forward denoising dynamics that characterize these models.

## Foundational Learning

**Diffusion Language Models**: These models generate text through a forward denoising process rather than autoregressive sampling. Understanding their unique generation dynamics is crucial because traditional RL approaches designed for autoregressive models don't directly apply.

**Importance Sampling**: A technique for estimating expectations under a target distribution using samples from a different distribution. In DMPO, it's used to compute the weighted loss by reweighting samples according to their importance under the optimal policy.

**Weighted Denoising Cross-Entropy (WDCE)**: An extension of standard cross-entropy loss that incorporates sample weights. The "why needed" is to incorporate reward information into the training objective while maintaining the denoising framework. "Quick check": Verify that weights are properly normalized and that the loss remains well-behaved when rewards vary widely.

**Distribution Matching**: The core principle of DMPO where the goal is to match the current policy distribution to the optimal distribution rather than directly maximizing rewards. This is particularly effective for dLLMs because it aligns with their probabilistic generation framework.

## Architecture Onboarding

**Component Map**: Diffusion LLM (input) -> Forward Denoising Process -> WDCE Loss with Importance Weights -> Distribution Matching Update -> Improved Policy

**Critical Path**: The critical path involves computing importance weights from rewards, applying these weights in the WDCE loss calculation, and updating the model parameters through gradient descent on the weighted loss.

**Design Tradeoffs**: The main tradeoff is between the theoretical elegance of distribution matching and the practical challenges of implementing it with small batch sizes. The weight baseline subtraction technique addresses this but adds implementation complexity.

**Failure Signatures**: Potential failures include unstable training due to high-variance importance weights, poor performance when rewards are sparse or uninformative, and breakdown of the weight baseline subtraction method with very small batch sizes.

**First Experiments**: 1) Validate that importance weights properly reflect reward structure by visualizing weight distributions. 2) Test the stability of training with different batch sizes and baseline subtraction schemes. 3) Compare distribution matching performance against direct reward maximization on a simple reasoning task.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness is primarily demonstrated on reasoning tasks, limiting confidence in generalizability to other domains
- The weight baseline subtraction technique for small batch sizes may not scale well to production scenarios with larger batches
- The distribution matching approach requires careful tuning of weighting schemes, with limited exploration of hyperparameter sensitivity

## Confidence
- DMPO's superiority over previous dLLM RL methods: High confidence based on benchmark results
- Generalizability beyond reasoning tasks: Medium confidence due to limited test domain diversity
- No supervised fine-tuning requirement: Medium confidence as base model quality likely influences effectiveness
- Computational efficiency gains for small batch sizes: Medium confidence pending larger-scale validation

## Next Checks
1. Test DMPO on non-reasoning tasks (e.g., creative writing, code generation) to assess generalizability beyond the current benchmark suite
2. Scale experiments with larger batch sizes to validate the weight baseline subtraction technique and assess computational efficiency at production scales
3. Conduct ablation studies removing the distribution matching component to quantify the specific contribution of this innovation versus other aspects of the RL training pipeline