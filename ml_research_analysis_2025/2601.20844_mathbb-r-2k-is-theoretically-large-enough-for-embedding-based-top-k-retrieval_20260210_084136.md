---
ver: rpa2
title: $\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$
  Retrieval
arxiv_id: '2601.20844'
source_url: https://arxiv.org/abs/2601.20844
tags:
- dimension
- minimal
- self
- setting
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the minimal dimension required to embed subset\
  \ memberships into vector spaces for top-k retrieval, denoted as Minimal Embeddable\
  \ Dimension (MED). The authors derive tight bounds for MED theoretically and support\
  \ them empirically for \u21132 metric, inner product, and cosine similarity."
---

# $\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval

## Quick Facts
- arXiv ID: 2601.20844
- Source URL: https://arxiv.org/abs/2601.20844
- Authors: Zihao Wang; Hang Yin; Lihui Liu; Hanghang Tong; Yangqiu Song; Ginny Wong; Simon See
- Reference count: 40
- Primary result: Minimal Embeddable Dimension (MED) is $\Theta(k)$ and independent of universe size $m$, contradicting prior work suggesting polynomial growth.

## Executive Summary
This paper establishes that embedding-based top-$k$ retrieval is theoretically feasible in surprisingly low dimensions, specifically $\Theta(k)$ for general query embeddings and $O(k^2 \log m)$ when queries are constrained to centroids. The key insight is that geometric constraints, not fundamental limitations of vector spaces, determine the dimensionality requirements. The authors prove that for any universe size $m$, a $2k$-dimensional space suffices for perfect retrieval, challenging the conventional wisdom that embedding dimensionality must grow with the number of items to embed.

## Method Summary
The authors define Minimal Embeddable Dimension (MED) as the smallest dimension where every subset of size at most $k$ can be perfectly retrieved. They prove tight bounds through geometric constructions (cyclic polytopes for $O(k)$ upper bound) and probabilistic methods (Gaussian random vectors for $O(k^2 \log m)$ upper bound under centroid constraints). The method involves binary search over dimensions to find minimal $d$ achieving zero violations, where violations occur when non-answer elements score higher than answer elements for any subset query. Experiments validate the theoretical bounds through numerical simulations across different metric spaces (inner product, cosine, $\ell_2$).

## Key Results
- MED(m,k) = $\Theta(k)$ for all three scoring functions, independent of universe size $m$
- MED-C = $O(k^2 \log m)$ for centroid-constrained queries, showing logarithmic growth with $m$
- Numerical simulations confirm theoretical bounds with practical convergence to zero-violation configurations
- The results demonstrate that embedding limitations stem from learnability challenges rather than geometric constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A 2k-dimensional vector space is theoretically sufficient to achieve perfect top-k retrieval for any universe size m.
- Mechanism: Cyclic polytopes in $\mathbb{R}^{2k}$ are ⌊k⌋-neighborly, meaning any k vertices form a face that can be linearly separated from remaining vertices. This geometric property guarantees existence of hyperplanes separating any subset S (|S|≤k) from its complement.
- Core assumption: Infinite numerical precision is available; floating-point constraints are not modeled.
- Evidence anchors:
  - [abstract] "MED(m,k) = Θ(k), meaning perfect top-k retrieval is theoretically possible in a low-dimensional space regardless of the number of elements"
  - [Section 3.1] Theorem 3.2: "k−1 ≤ MED(m, k; F_linear) ≤ 2k" with upper bound via cyclic polytope construction
  - [corpus] Weak direct support; corpus papers focus on different embedding properties
- Break condition: Fixed-precision floating-point representations may violate separation margins when m is extremely large relative to k.

### Mechanism 2
- Claim: Constraining query embeddings to be centroids of their answer sets increases dimension requirements to $O(k^2 \log m)$.
- Mechanism: When query vectors w_q = (1/|S|)∑_{x∈S} x are fixed as centroids, the degrees of freedom reduce from optimizing (m choose k) subset embeddings to only m element embeddings. Probabilistic construction with Gaussian random vectors shows positive probability of k-centroid shattering when n > C·k²·log m.
- Core assumption: Element embeddings can be sampled from isotropic Gaussian N(0, I_n/n) without structural constraints from downstream tasks.
- Evidence anchors:
  - [abstract] "MED-C = O(k² log m)... logarithmic dependency between the MED and the number of elements to embed"
  - [Section 4.1] Theorem 4.1-4.3: Probabilistic proofs using concentration of inner products and norms
  - [corpus] No directly comparable centroid constraints in corpus papers
- Break condition: If element embeddings must satisfy additional semantic constraints (e.g., similarity preservation), random construction may fail.

### Mechanism 3
- Claim: Retrieval failure stems from learnability challenges, not fundamental geometric limitations.
- Mechanism: The paper shows geometric spaces can encode all subset memberships (MED=Θ(k)), but constructing query embeddings that achieve separation requires learning (m choose k) functionals. Neural networks can theoretically approximate these functions, but practical training may not converge.
- Core assumption: Universal approximation theorems translate to practical learnability with finite data and gradient descent.
- Evidence anchors:
  - [abstract] "embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints"
  - [Section 3.4] "the real problem is how to build a function to predict them [query embeddings]"
  - [Section 4.3] Free embedding optimization with more degrees of freedom yields weaker empirical results than centroid setting—optimization landscape matters more than capacity
- Break condition: If training data or model capacity is severely limited, even geometrically sufficient dimensions may yield poor retrieval.

## Foundational Learning

- Concept: **VC Dimension and k-Shattering**
  - Why needed here: The paper defines MED through k-shattering, which extends VC-dimension concepts to subset retrieval rather than binary classification.
  - Quick check question: Given a functional family F in ℝ^n, what is the largest set size m that can be m-shattered? (Answer: VCD(n; F))

- Concept: **Neighborly Polytopes**
  - Why needed here: Cyclic polytopes provide the explicit construction proving MED ≤ 2k. Understanding why ⌊d/2⌋-neighborliness guarantees separability is essential.
  - Quick check question: In ℝ^{2k}, what is the maximum k such that every k-vertex subset of a cyclic polytope forms a face? (Answer: k)

- Concept: **Probabilistic Method for Existence Proofs**
  - Why needed here: MED-C upper bounds use random Gaussian constructions with union bounds to show positive probability of valid configurations.
  - Quick check question: Why does showing P(event) > 0 prove existence without providing an explicit construction? (Answer: Probability space is non-empty)

## Architecture Onboarding

- Component map: Element encoder -> Scoring function -> Top-k selection -> Query encoder
- Critical path: Query encoding → score computation → ranking. The query encoder is the learnability bottleneck; element embeddings can be placed on cyclic polytope vertices if unconstrained.
- Design tradeoffs:
  - Higher dimensions (d ≫ 2k): Easier optimization landscape, but increased storage/compute costs
  - Centroid vs learned queries: Centroids reduce parameters but require $O(k^2 \log m)$ dimensions; learned queries need only $O(k)$ dimensions but harder to train
  - Free element embeddings vs fixed: Free embeddings enable Θ(k) dimensions but (m choose k) optimization targets; constrained embeddings reduce targets but increase dimension needs
- Failure signatures:
  - Retrieval accuracy plateaus despite increasing model capacity → likely learnability issue, not dimension
  - Accuracy degrades as m grows while k fixed with d constant → dimension may be insufficient for learned queries (should not occur with centroid if d = $O(k^2 \log m)$)
  - Optimization fails to find zero-violation configuration in simulation → numerical precision or learning rate issues
- First 3 experiments:
  1. Validate MED bounds: Construct cyclic polytope embeddings for m=100 elements in $\mathbb{R}^{2k}$ with k=5; verify all (m choose k) subsets are linearly separable by checking margin existence.
  2. Compare optimization regimes: Run centroid-setting simulation (optimize only m embeddings) vs free-embedding optimization for m=50, k=2 across dimensions d=5,10,20; measure violations at convergence.
  3. Learnability probe: Train a small MLP to predict query embeddings for known subsets; measure correlation between training loss and retrieval accuracy to diagnose learnability vs geometry bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the $\Theta(k)$ minimal embeddable dimension bound hold for non-Euclidean spaces, such as hyperbolic or Wasserstein spaces?
- **Basis in paper:** [explicit] Section 6 states future work should include "more discussion of advanced embedding spaces, such as hyperbolic and Wasserstein spaces."
- **Why unresolved:** The current theoretical proofs rely on geometric constructions specific to $\mathbb{R}^d$ (e.g., cyclic polytopes, $\ell_2$ balls), which do not directly translate to curved or probability metric spaces.
- **What evidence would resolve it:** Deriving tight bounds for MED in hyperbolic space or constructing counter-examples showing the dimension dependency changes.

### Open Question 2
- **Question:** How does finite numerical precision (e.g., float32 or float4) affect the Minimal Embeddable Dimension compared to the theoretical real-number bounds?
- **Basis in paper:** [explicit] Section 6 suggests "a more realistic setting of fixed-point float numbers," and Section 3.4 notes that limited numerical precision challenges the observability of the $\Theta(k)$ bound.
- **Why unresolved:** The theoretical existence proofs rely on infinite precision real numbers, whereas real-world systems suffer from quantization noise and finite floating-point capacity.
- **What evidence would resolve it:** Empirical simulations of the cyclic polytope construction under fixed floating-point constraints or a theoretical analysis of MED under quantization noise.

### Open Question 3
- **Question:** How does a power-law distribution of answer set cardinalities impact the required dimensionality for perfect retrieval?
- **Basis in paper:** [explicit] Section 6 calls for investigating "situations where the cardinality of answer sets follows a power-law distribution."
- **Why unresolved:** The current analysis assumes a uniform cap $k$ on answer set size, which does not reflect the skewed distribution of relevance often seen in real-world retrieval tasks.
- **What evidence would resolve it:** Extending the MED definition to variable set sizes and deriving bounds based on expected distribution statistics rather than a hard maximum.

## Limitations
- Theoretical proofs assume infinite numerical precision, while practical implementations face floating-point constraints
- Probabilistic construction relies on random Gaussian embeddings that may not reflect structured semantic data
- Learnability claims are based on existence proofs rather than practical training dynamics and convergence rates
- Centroid constraints may not reflect practical retrieval scenarios where queries need more flexible representations

## Confidence
- **High Confidence**: Θ(k) bounds for MED with free element embeddings and learned query embeddings (Section 3.1-3.2). The cyclic polytope construction and neighborliness properties are well-established geometric results with rigorous proofs.
- **Medium Confidence**: O(k² log m) bounds for MED-C with centroid queries (Section 4.1-4.3). The probabilistic method proofs are mathematically sound but rely on idealized random embeddings that may not generalize to structured data.
- **Low Confidence**: Practical learnability claims separating geometric constraints from optimization challenges (Section 3.4, 4.3). While the theoretical framework is sound, empirical validation on real datasets and with practical training regimes is limited.

## Next Checks
1. **Precision Sensitivity Analysis**: Test the cyclic polytope construction under varying numerical precision (32-bit, 64-bit, arbitrary precision) for large m (e.g., m=1000) to identify precision thresholds where separation margins become unreliable.

2. **Structured Data Validation**: Replace random Gaussian embeddings with embeddings derived from real semantic data (e.g., sentence embeddings from a pre-trained model) to verify whether the O(k) and O(k² log m) bounds hold under structured distributions.

3. **Learnability Benchmark**: Implement a practical training pipeline for learned query embeddings on synthetic data generated from the theoretical constructions, measuring the gap between theoretical zero-violation solutions and achievable performance under gradient descent with finite data.