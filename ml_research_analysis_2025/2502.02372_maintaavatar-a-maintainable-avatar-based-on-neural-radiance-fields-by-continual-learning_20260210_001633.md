---
ver: rpa2
title: 'MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual
  Learning'
arxiv_id: '2502.02372'
source_url: https://arxiv.org/abs/2502.02372
tags:
- pose
- human
- learning
- neural
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MaintaAvatar, the first work to address the
  problem of creating a maintainable virtual avatar that can continually learn new
  poses and appearances while preserving the ability to render past appearances. The
  key challenge is catastrophic forgetting, where learning new appearances and poses
  degrades the rendering quality of past appearances.
---

# MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning

## Quick Facts
- arXiv ID: 2502.02372
- Source URL: https://arxiv.org/abs/2502.02372
- Reference count: 16
- Primary result: Achieves 29.495 PSNR and 0.9663 SSIM on ZJU-MoCap, outperforming baselines by 1.26 dB

## Executive Summary
This paper introduces MaintaAvatar, the first work addressing continual learning for human avatars in Neural Radiance Fields. The key challenge is catastrophic forgetting, where learning new poses and appearances degrades rendering quality of past appearances. The proposed solution combines a Global-Local Joint Storage Module to separate appearance representations and prevent color bleeding, with a Pose Distillation Module to preserve pose information across tasks. The method achieves superior performance on ZJU-MoCap and THuman2.0 datasets while requiring only limited data collection.

## Method Summary
MaintaAvatar extends deformable NeRF with SMPL-based inverse skinning to learn human avatars continually. The model introduces a Global-Local Joint Storage Module that separately stores global appearance variations (geometry and color embeddings) and local spatial details (Tri-plane features) to prevent color bleeding between tasks. A Pose Distillation Module preserves pose accuracy by distilling outputs from a pose correction network. The system uses generative replay, freezing previous models to generate pseudo-ground-truth patches for training on new data. The architecture includes an 8-layer MLP for color/density prediction and a 4-layer MLP for pose residual correction, with two-phase training that delays pose distillation to avoid blurring.

## Key Results
- Achieves 29.495 PSNR and 0.9663 SSIM on ZJU-MoCap, outperforming PersonNeRFCL (28.605 PSNR, 0.9632 SSIM) by 1.26 dB
- Maintains 22.32 PSNR on THuman2.0 dataset with limited training data
- Demonstrates effective prevention of catastrophic forgetting through quantitative improvements over baselines including CLNeRF and MEIL-NeRF
- Shows quick fine-tuning capability with only 5 images per new appearance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating global and local appearance representations reduces color bleeding between sequentially learned appearances.
- Mechanism: The Global-Local Joint Storage Module stores coarse, whole-body appearance variations in compact global embeddings while encoding fine-grained spatial details in Tri-plane-based local embeddings. This factorization prevents interference because global embeddings capture task-level identity while Tri-planes isolate pixel-level details per appearance.
- Core assumption: Appearance changes involve both global shifts and local variations that can be disentangled spatially.
- Evidence anchors: [abstract] "introduces a Global-Local Joint Storage Module to model global and local appearance variations" [Section 3.2] "global embeddings can represent the overall changes in human appearance, while local embeddings is used to represent the fine-grained variations"

### Mechanism 2
- Claim: Distilling pose correction outputs from frozen past models preserves accurate body pose rendering across tasks.
- Mechanism: The Pose Distillation Module computes L2 loss between the current model's pose residual MLP output and the frozen previous model's output on the same pose input. This forces the new model to retain the learned pose correction function, preventing the MLP from overfitting to new poses only.
- Core assumption: Pose correction is a learned function that generalizes across appearances and can be transferred via output distillation.
- Evidence anchors: [abstract] "a Pose Distillation Module to preserve pose information from past tasks" [Section 3.3] "M LPp overfits to learning new poses... maintaining the accuracy of poses from past tasks during continual learning is a challenge"

### Mechanism 3
- Claim: Generative replay with frozen past models enables training on new data while preserving old rendering capability.
- Mechanism: At task T, the model copies and freezes Θ_{T-1}. Given saved camera/pose parameters from past tasks, Θ_{T-1} generates pseudo-ground-truth patches that supervise Θ_T on past appearances. The joint loss balances current and replay supervision.
- Core assumption: The frozen model's renderings are sufficiently high-quality to serve as supervision targets.
- Evidence anchors: [abstract] "requires only limited data collection to quickly fine-tune while avoiding catastrophic forgetting" [Section 3.1] Steps 1-4 describe the replay pipeline explicitly

## Foundational Learning

- Concept: **Catastrophic Forgetting in Neural Networks**
  - Why needed here: The core problem is that standard fine-tuning erases past knowledge; understanding this motivates the replay strategy.
  - Quick check question: Can you explain why updating a neural network on new data degrades performance on old data?

- Concept: **SMPL Model and Linear Blend Skinning (LBS)**
  - Why needed here: The deformation field relies on inverse LBS to map observed poses to a canonical T-pose space.
  - Quick check question: How does inverse skinning transform a point from observation space to canonical space?

- Concept: **Neural Radiance Fields (NeRF) Rendering Pipeline**
  - Why needed here: The base architecture extends NeRF with pose-conditioned deformations; you must understand ray sampling, volume rendering, and MLP prediction of color/density.
  - Quick check question: Given a camera ray, how does NeRF compute the final pixel color?

## Architecture Onboarding

- Component map:
  - Pose input → MLP_p (residual) → Inverse LBS → Canonical point → Tri-plane query (ℓt) → Concatenate embeddings → MLP_o → Color/density → Volume rendering

- Critical path: Pose input → MLP_p (residual) → Inverse LBS → Canonical point → Tri-plane query (ℓt) → Concatenate embeddings → MLP_o → Color/density → Volume rendering

- Design tradeoffs:
  - Few-shot training (5 images) vs. geometric fidelity: Limited views constrain surface accuracy
  - Replay patch size (64×64) vs. memory: Larger patches improve supervision but increase storage
  - Two-phase training: Delaying pose distillation (first 10k/70k iterations) prevents blurring but extends training

- Failure signatures:
  - Color bleeding: Likely Global-Local module failure; check Tri-plane gradient flow
  - Incorrect poses on old appearances: Pose distillation not activated or λ_β too low
  - Blurry renderings: Pose distillation activated too early; verify t₀ setting

- First 3 experiments:
  1. **Baseline replay**: Implement copy-freeze-replay without Global-Local or Pose Distillation; confirm catastrophic forgetting occurs (expect PSNR ~25.8 as MEIL-NeRF baseline on ZJU-MoCap).
  2. **Ablate Global-Local**: Add only global embeddings; visualize color bleeding between appearances (e.g., green→yellow contamination as in Figure 6).
  3. **Full model validation**: Enable both modules; verify PSNR approaches Joint training (29.5 on ZJU-MoCap) and check pose consistency across tasks.

## Open Questions the Paper Calls Out

- Question: How can the model be adapted to maintain rendering quality when processing significant geometric changes in clothing, such as switching from tight-fitting to loose-fitting outfits?
  - Basis in paper: [explicit] The authors explicitly list this as a limitation, stating, "Our method shows performance drops with significant clothing shape changes."
  - Why unresolved: The current Global-Local Joint Storage Module effectively models color/texture variations but appears insufficient for handling drastic alterations in silhouette or geometry.
  - What evidence would resolve it: Quantitative results (PSNR/SSIM) on a dataset specifically designed with high-variance clothing geometries (e.g., coats, dresses) showing comparable performance to tight-clothing tasks.

- Question: Can the Pose Distillation Module be enhanced to support robust generalization to complex, unseen poses when only a few training samples are available?
  - Basis in paper: [explicit] The paper notes that the method "struggles with pose generalization in complex poses due to limited exposure in the few-shot dataset."
  - Why unresolved: The reliance on few-shot data limits the model's exposure to the full range of human articulation, leading to artifacts in challenging poses not well-represented in the sparse training set.
  - What evidence would resolve it: Successful synthesis of high-fidelity novel views for "complex" poses (e.g., extreme athleticism or yoga) that were excluded from the training distribution.

- Question: How does the memory footprint and inference speed of the Global-Local Joint Storage Module scale as the number of sequential tasks increases significantly?
  - Basis in paper: [inferred] While the paper demonstrates results on 4 tasks, it does not analyze the storage overhead of accumulating Tri-planes and embeddings over long periods.
  - Why unresolved: A "maintainable" avatar in a real-world scenario implies indefinite updates; the storage cost of adding new local embeddings and Tri-planes for every future outfit is unstated.
  - What evidence would resolve it: A complexity analysis detailing GPU memory growth and rendering latency as the number of learned appearances scales from 4 to 50 or 100.

## Limitations
- Performance degradation occurs with significant clothing shape changes, as the method struggles with geometric variations beyond color/texture
- Limited pose generalization in complex poses due to sparse training data, leading to artifacts in challenging articulations
- Memory and computational overhead increase with the number of sequential tasks, though scalability analysis is not provided

## Confidence
- **High confidence**: The catastrophic forgetting problem formulation and replay-based solution are well-established in continual learning literature. The PSNR/SSIM improvements over baselines (29.495 vs 28.605 PSNR on ZJU-MoCap) are directly measurable.
- **Medium confidence**: The Global-Local separation mechanism is logically sound, but the effectiveness depends critically on the unspecified Tri-plane generator implementation. The pose distillation approach is plausible but lacks direct corpus precedent.
- **Low confidence**: Without the Tri-plane generator specification, we cannot verify whether the claimed 1.26 dB PSNR improvement over PersonNeRFCL is achievable.

## Next Checks
1. **Implement baseline replay without Global-Local or Pose Distillation**: Train a deformable NeRF with SMPL-based inverse skinning and generative replay only. Confirm catastrophic forgetting occurs (expect PSNR ~25.8 as MEIL-NeRF baseline on ZJU-MoCap) to validate that the problem exists and replay is necessary.

2. **Ablate Global-Local module only**: Add global embeddings but remove Tri-planes, using only global color/geometry embeddings for all appearance rendering. Visualize color bleeding between sequentially learned appearances (expect green→yellow contamination as shown in Figure 6) to confirm the necessity of local Tri-plane separation.

3. **Full model validation with pose consistency check**: Enable both Global-Local and Pose Distillation modules. After training on multiple tasks, render held-out views of past appearances and compare body poses to ground truth. Verify that pose distillation maintains accuracy while avoiding blur (pose consistency should match Joint training while PSNR approaches 29.5 on ZJU-MoCap).