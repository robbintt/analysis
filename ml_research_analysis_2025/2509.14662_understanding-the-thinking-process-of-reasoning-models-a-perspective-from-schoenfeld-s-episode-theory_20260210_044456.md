---
ver: rpa2
title: 'Understanding the Thinking Process of Reasoning Models: A Perspective from
  Schoenfeld''s Episode Theory'
arxiv_id: '2509.14662'
source_url: https://arxiv.org/abs/2509.14662
tags:
- reasoning
- episode
- verify
- explore
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work applies Schoenfeld\u2019s Episode Theory\u2014a cognitive\
  \ framework for human mathematical problem-solving\u2014to analyze reasoning traces\
  \ generated by Large Reasoning Models (LRMs). By annotating thousands of sentences\
  \ and paragraphs from model-generated solutions with seven cognitive labels (Read,\
  \ Analyze, Plan, Implement, Explore, Verify, Monitor), the authors establish the\
  \ first publicly available benchmark for fine-grained machine reasoning analysis."
---

# Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory

## Quick Facts
- arXiv ID: 2509.14662
- Source URL: https://arxiv.org/abs/2509.14662
- Reference count: 17
- Authors: Ming Li; Nan Zhang; Chenrui Fan; Hong Jiao; Yanbin Fu; Sydney Peters; Qingshu Xu; Robert Lissitz; Tianyi Zhou
- Primary result: First publicly available benchmark for fine-grained machine reasoning analysis using Schoenfeld's Episode Theory

## Executive Summary
This work applies Schoenfeld's Episode Theory—a cognitive framework for human mathematical problem-solving—to analyze reasoning traces generated by Large Reasoning Models (LRMs). By annotating thousands of sentences and paragraphs from model-generated solutions with seven cognitive labels (Read, Analyze, Plan, Implement, Explore, Verify, Monitor), the authors establish the first publicly available benchmark for fine-grained machine reasoning analysis. Using hierarchical paragraph- and sentence-level annotations on SAT math problems, the study reveals distinct transition patterns in LRM reasoning that align with human cognitive processes. Experiments show that LLMs can automate this annotation with moderate accuracy, though human annotation remains superior. The framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.

## Method Summary
The study collects reasoning traces from DeepSeek-R1 on 1,385 SAT Math problems and manually annotates 38 responses using a hierarchical labeling system. Annotators label paragraphs with 3 categories (General, Explore, Verify) and sentences with 7 cognitive episode labels (Read, Analyze, Plan, Implement, Explore, Verify, Monitor). The annotation process uses detailed guidebooks and achieves consensus through training. For automation, the paper evaluates LLM-based approaches (zero-shot, guidebook-guided, example-guided) and training-based methods (BERT/RoBERTa fine-tuning, SVM/MLP/KNN on embeddings). The best model (GPT-4.1 with examples+guidebook) achieves 0.805 sentence accuracy and 0.764 Cohen's κ on a 30% test split.

## Key Results
- LRM reasoning traces exhibit episodic structure that aligns with Schoenfeld's Episode Theory, with distinct transition patterns mirroring human problem-solving
- Hierarchical annotation resolves granularity conflicts, allowing precise labeling of nested cognitive behaviors
- LLM automation with guidebooks achieves 0.805 sentence-level accuracy, though confusion persists between Analyze-Verify and Implement-Verify categories
- High self-loop probabilities in transition matrices indicate models tend to stay within the same cognitive episode, similar to human behavior

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Episode Alignment
LRM reasoning traces exhibit episodic structure that can be mapped to human cognitive categories. LRMs trained with RL or SFT on reasoning traces produce outputs that naturally segment into problem-solving phases—reading, analyzing, planning, implementing, exploring, verifying, and monitoring—which align with Schoenfeld's Episode Theory originally developed from human think-aloud protocols.

### Mechanism 2: Hierarchical Annotation Resolves Granularity Conflicts
A two-level annotation hierarchy (paragraph-level and sentence-level) handles nested cognitive behaviors better than single-level schemes. Paragraph-level labels capture broad behavioral phases while sentence-level labels capture fine-grained actions within those phases, resolving cases like "planning during verification."

### Mechanism 3: Guidebook-Guided LLMs Enable Scalable Automation
Providing detailed annotation guidebooks as context substantially improves LLM performance on cognitive episode classification, though human annotation remains superior. Guidebooks operationalize abstract cognitive categories into concrete decision rules and examples, reducing ambiguity.

## Foundational Learning

- **Schoenfeld's Episode Theory**: Framework viewing problem-solving as a sequence of cognitive "episodes" (Read, Analyze, Explore, Plan, Implement, Verify) with metacognitive control (Monitor) governing transitions. Why needed: This is the core theoretical framework for analyzing LRM reasoning traces.
  - Quick check: If a model says "Wait, let me reconsider" and then tries a different formula, which two episode labels are involved?

- **Chain-of-Thought (CoT) Reasoning in LRMs**: LRMs differ from standard LLMs by producing explicit intermediate reasoning steps through RL training (DeepSeek-R1) or SFT. Why needed: The externalized reasoning is what gets annotated.
  - Quick check: What distinguishes an LRM's output from a standard instruction-following LLM's output on a math problem?

- **Think-Aloud Protocol Analysis**: The annotation methodology adapts techniques from human cognitive research. Why needed: Understanding how researchers code verbal protocols helps interpret guidebooks and annotation decisions.
  - Quick check: Why might a sentence like "Let me double-check" be labeled Verify rather than Monitor?

## Architecture Onboarding

- **Component map**: Raw LRM reasoning traces -> Hierarchical labeling system -> LLM-based classifiers or trained models -> Transition matrix construction and analysis
- **Critical path**: Collect LRM outputs on SAT math problems -> Train human annotators using guidebooks -> Produce gold-standard annotations -> Evaluate automated annotation methods -> Analyze transition dynamics and confusion patterns
- **Design tradeoffs**: Humans provide higher accuracy; automation provides scalability. Current best automation (GPT-4.1 + guidebook) achieves ~80% accuracy—sufficient for large-scale pattern analysis but not gold-standard quality. Finer granularity introduces more ambiguity but hierarchy mitigates this.
- **Failure signatures**: High Analyze-Verify confusion; High Implement-Verify confusion; Monitor under-detection; Context dependency failures
- **First 3 experiments**:
  1. Validate annotation reliability: Have two annotators label 50 responses independently. Calculate Cohen's κ. Target: κ > 0.7.
  2. Establish automation baseline: Test GPT-4.1 with guidebook-only prompting on 30% held-out test set.
  3. Probe confusion categories: Extract all sentences where ground truth is Verify but model predicts Analyze or Implement. Manually inspect for systematic patterns.

## Open Questions the Paper Calls Out

### Open Question 1
Do the episode transition dynamics and high self-loop probabilities observed in SAT-level reasoning persist when models attempt significantly harder mathematical tasks like Olympiad problems? The current data does not contain high-difficulty items, so it is unknown if models simply increase Implementation steps or enter fundamentally different cognitive loops under extreme complexity.

### Open Question 2
How can the automated annotation accuracy be improved, particularly to disambiguate the high-confusion pairs like Analyze-Verify and Implement-Verify? Current zero-shot and training-based methods show specific failure modes in distinguishing conceptually adjacent states, preventing automation of the labor-intensive annotation process.

### Open Question 3
Are the observed "human-aligned" transition patterns specific to DeepSeek-R1 or consistent across different Large Reasoning Model families? The study exclusively analyzes traces from DeepSeek-R1, leaving the universality of these cognitive structures across other LRMs unstated.

## Limitations

- Domain specificity: Framework validated only on SAT Math problems; generalization to other domains (code, logic puzzles) is untested
- Reliability quantification: Paper does not report Cohen's κ for final annotation, only noting annotators were trained until "consensus is reached"
- Automation accuracy: Best automated methods achieve ~80% accuracy but struggle with specific confusion pairs (Analyze-Verify, Implement-Verify)

## Confidence

- **High confidence**: Theoretical alignment between LRM reasoning traces and Schoenfeld's Episode Theory is well-supported by qualitative and quantitative validation
- **Medium confidence**: Hierarchical annotation methodology effectively resolves granularity conflicts, primarily supported by internal methodology
- **Medium confidence**: LLM automation with guidebooks achieves moderate accuracy (0.805 sentence-level), but systematic confusion suggests fundamental limitations

## Next Checks

1. **Cross-domain validation**: Apply the framework to reasoning traces from code generation or logical puzzles to test domain generalization beyond mathematical problem-solving
2. **Inter-annotator reliability quantification**: Measure Cohen's κ for the final annotation to provide concrete reliability metrics beyond qualitative consensus
3. **Confusion category disambiguation**: Conduct targeted experiments to improve classification of the most confused pairs through enhanced guidebook examples or context-aware models