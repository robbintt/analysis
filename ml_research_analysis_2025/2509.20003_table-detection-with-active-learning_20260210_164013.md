---
ver: rpa2
title: Table Detection with Active Learning
arxiv_id: '2509.20003'
source_url: https://arxiv.org/abs/2509.20003
tags:
- learning
- active
- detection
- table
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient data annotation
  for table detection in document images, a task that requires extensive labeled data.
  The authors propose an active learning framework that selects the most informative
  samples for annotation, reducing annotation costs while maintaining high performance.
---

# Table Detection with Active Learning

## Quick Facts
- arXiv ID: 2509.20003
- Source URL: https://arxiv.org/abs/2509.20003
- Authors: Somraj Gautam; Nachiketa Purohit; Gaurav Harit
- Reference count: 40
- One-line primary result: Active learning framework reduces annotation costs by up to 75% while maintaining high mAP for table detection

## Executive Summary
This paper addresses the challenge of efficient data annotation for table detection in document images, a task that requires extensive labeled data. The authors propose an active learning framework that selects the most informative samples for annotation, reducing annotation costs while maintaining high performance. Their method combines uncertainty-based and diversity-based strategies to identify hard examples, including cases with overlapping tables, ambiguous predictions, and multi-table configurations. Experiments on two benchmark datasets (TableBank-LaTeX and TableBank-Word) using state-of-the-art architectures (YOLOv9 and CascadeTabNet) demonstrate that their active learning approach significantly outperforms random sampling.

## Method Summary
The framework employs four sampling strategies: Prediction Uncertainty with confidence binning, Mask Ambiguity (MA), Bounding-Box Ambiguity (BBA), and Table Count (TC). Tested on YOLOv9 and CascadeTabNet using TableBank-LaTeX and TableBank-Word datasets. Uncertainty threshold = 0.95. IoU threshold for BBA: 0.006 (Word), 0.004 (LaTeX). Budget evaluated from 2k to 10k samples.

## Key Results
- Active learning approach achieves higher mAP scores within the same annotation budget
- Up to 75% reduction in annotation effort in some cases
- YOLOv9 + Table Count strategy achieves 89.2% mAP at 2k samples vs 78.5% for random
- CascadeTabNet benefits most from Mask/Bounding-Box Ambiguity strategies

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Binned Uncertainty Sampling
The framework computes a confidence score for detected tables and segregates samples into bins. It applies a sampling rate inversely proportional to confidence level, forcing the training loop to focus on boundary cases where the model is hesitant rather than correctly classified "easy" examples.

### Mechanism 2: Spatial Ambiguity as a Proxy for Hardness
The system uses Bounding-Box Ambiguity (BBA) counting overlapping detections and Mask Ambiguity (MA) measuring divergence between segmentation mask and bounding box. High scores on these metrics trigger selection, indicating structural inconsistencies in predictions.

### Mechanism 3: Table-Count Density Prior
The framework assigns higher sampling probability to images containing multiple tables. This ensures the model is exposed to complex layouts and multi-object dependencies early in the annotation budget, improving performance particularly for single-stage detectors.

## Foundational Learning

- **Active Learning (Pool-Based)**: The iterative loop: Train → Infer on Unlabeled → Score/Select → Annotate → Retrain. Critical for understanding the "budget" and "sampling strategy" concepts.
- **Object Detection Confidence Scores (IoU + Probability)**: Detection confidence is typically P(class) × IoU. Understanding how YOLO (Sigmoid) vs. CascadeTabNet (Softmax) calculate this is critical for debugging the sampling strategy.
- **Intersection over Union (IoU)**: Used not just for evaluation but as a fundamental component of the Bounding-Box Ambiguity score, acting as a collision detector for overlapping tables.

## Architecture Onboarding

- **Component map**: Initial Pool → Inference Engine → Scoring Module → Selector → Human-in-the-Loop
- **Critical path**: The scoring module is the novel component. If the scores (BBA, MA) do not correlate with detection difficulty, the system collapses to random sampling.
- **Design tradeoffs**: YOLOv9 benefits most from Table Count strategy (learning diversity), while CascadeTabNet benefits most from Mask/Bounding-Box Ambiguity (refining structural confusion).
- **Failure signatures**: Stagnation (mAP curve flattens early) likely caused by selecting "outliers" rather than "hard" examples; Worse than Random indicates biased selection criteria.
- **First 3 experiments**: 1) Baseline Calibration with Random Sampling, 2) Ablation on Strategy (Confidence-only, Ambiguity-only, Hybrid), 3) Architecture Sensitivity testing best strategy on both single-stage and multi-stage detectors.

## Open Questions the Paper Calls Out

Can the proposed active learning framework effectively reduce annotation costs for the more complex task of table structure recognition (identifying rows, columns, and cells)? The authors plan to extend this work to table structure recognition where labeling costs are significantly higher.

## Limitations

- Effectiveness of BBA and MA metrics may degrade if dataset lacks complex multi-table layouts
- Optimal sampling strategy appears architecture-dependent, requiring careful tuning per model choice
- Confidence-based selection assumes well-calibrated models; overconfident incorrect predictions could mislead the system

## Confidence

- **High Confidence**: Core premise that active learning reduces annotation costs while maintaining performance
- **Medium Confidence**: Specific mechanisms (BBA, MA) show promise but general applicability beyond table detection remains unproven
- **Medium Confidence**: "75% reduction in annotation effort" is context-specific to TableBank datasets and YOLOv9/CascadeTabNet architectures

## Next Checks

1. Test the framework on a dataset with minimal table overlap to verify BBA metric degrades gracefully
2. Evaluate performance when initialized with a poorly calibrated model to assess robustness to confidence score reliability
3. Apply the strategy to a different detection task (e.g., medical image annotation) to test generalizability beyond document tables