---
ver: rpa2
title: 'LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR'
arxiv_id: '2509.18722'
source_url: https://arxiv.org/abs/2509.18722
tags:
- speech
- far-field
- corpus
- lotusdis
- thai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LOTUSDIS, the first publicly available Thai\
  \ far-field conversational speech corpus designed for robust distant ASR. The dataset\
  \ includes 114 hours of unscripted, multi-party dialogue recorded simultaneously\
  \ across nine single-channel microphones at varying distances (0.12\u201310 m),\
  \ without relying on microphone arrays."
---

# LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR

## Quick Facts
- arXiv ID: 2509.18722
- Source URL: https://arxiv.org/abs/2509.18722
- Reference count: 0
- This paper introduces LOTUSDIS, the first publicly available Thai far-field conversational speech corpus designed for robust distant ASR.

## Executive Summary
This paper introduces LOTUSDIS, the first publicly available Thai far-field conversational speech corpus designed for robust distant ASR. The dataset includes 114 hours of unscripted, multi-party dialogue recorded simultaneously across nine single-channel microphones at varying distances (0.12–10 m), without relying on microphone arrays. A reproducible baseline was established by fine-tuning Whisper models on LOTUSDIS, with evaluations covering zero-shot performance, data augmentation, and front-end processing. Results show that off-the-shelf models suffer severe degradation with distance, while fine-tuning reduces overall WER from 64.3% to 38.3% and far-field WER from 81.6% to 49.5%. These findings highlight the importance of distance-diverse training data and demonstrate LOTUSDIS as a valuable resource for developing robust ASR systems in real-world, far-field environments.

## Method Summary
The study fine-tuned a Thai-adapted Whisper large-v3 model (Pathumma-whisper-th-large-v3) on the LOTUSDIS corpus, which contains 114 hours of multi-party Thai conversations recorded across nine single-channel microphones at distances from 0.12m to 10m. Training was performed for 5 epochs on an NVIDIA H200 GPU using the corpus's 88-hour training split. The model was evaluated on the 13.3-hour test split, with performance measured separately for near-field (≤1m) and far-field (2-10m) conditions, as well as for overlapping versus single-speaker segments. Additional experiments tested the impact of data augmentation (RIR convolution + noise mixing) and front-end processing (WPE dereverberation, MMSE-LSA spectral subtraction) on model robustness.

## Key Results
- Fine-tuning on LOTUSDIS reduced overall WER from 64.3% to 38.3% and far-field WER from 81.6% to 49.5%
- Single-microphone training caused severe overfitting, with condenser-only models achieving 19.26% WER on their own device but 97.95% on unseen far-field channels
- Data augmentation partially compensated for single-mic training limitations, reducing far-field WER from 79.5% to 65.4%
- Front-end processing (WPE, MMSE-LSA) consistently degraded fine-tuned model performance, increasing near-field WER from 21.59% to 35.92%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on distance-diverse single-channel data substantially improves far-field ASR robustness.
- Mechanism: Exposure to varying reverberation, SNR, and device colorations during fine-tuning allows the model to learn invariants across acoustic conditions, reducing the mismatch between pre-training distributions and real-world far-field speech.
- Core assumption: The acoustic diversity in training data generalizes to unseen far-field conditions with similar characteristics.
- Evidence anchors:
  - [abstract] "Fine-tuning on LOTUSDIS dramatically improved robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and far-field WER from 81.6 to 49.5"
  - [section 4.3] "Pathumma-whisper-th-large-v3 showed even stronger gains, with the overall WER reduced from 64.3% in the zero-shot condition to 38.33% after fine-tuning"
  - [corpus] No direct corpus evidence on generalization mechanisms beyond WER improvements; assumes transfer to similar acoustic conditions.
- Break condition: If target deployment environments have fundamentally different room acoustics (e.g., much larger spaces, different noise profiles), gains may not fully transfer.

### Mechanism 2
- Claim: Single-microphone training causes severe overfitting to device-specific acoustics, undermining cross-device generalization.
- Mechanism: Models learn microphone-specific spectral patterns and noise signatures as predictive features rather than robust acoustic-phonetic mappings, causing catastrophic degradation when those patterns are absent.
- Core assumption: Device-specific artifacts are learned as discriminative features during training.
- Evidence anchors:
  - [abstract] Not directly addressed
  - [section 4.5] "A model trained exclusively on the near-field condenser microphone achieved a WER of 19.26% on that device... However, this came at the cost of extreme degradation on unseen far-field channels, with BT3m performance collapsing to 97.95%"
  - [corpus] Corpus shows 31.5% overlap ratio and varied microphone types; assumes similar overlap distributions across mics.
- Break condition: If the deployment microphone exactly matches training conditions, single-mic training may actually outperform multi-mic training.

### Mechanism 3
- Claim: Traditional front-end processing (dereverberation, spectral subtraction) applied uniformly can harm fine-tuned model performance.
- Mechanism: Preprocessing introduces artifacts or removes informative signal components that the fine-tuned model has learned to use, particularly affecting near-field channels where such processing is unnecessary.
- Core assumption: The model has learned to extract useful information from raw reverberant signals during fine-tuning.
- Evidence anchors:
  - [abstract] Not directly addressed
  - [section 4.4] "Applying WPE consistently degraded performance, increasing near-field WER from 21.59% to 35.92% and far-field WER from 49.54% to 56.12%"
  - [corpus] Weak corpus evidence on why this occurs; the mechanism is inferred from performance patterns.
- Break condition: If using zero-shot models without fine-tuning on far-field data, front-end processing may still provide benefits.

## Foundational Learning

- **Far-field vs. Near-field Acoustics**
  - Why needed here: Understanding how distance affects SNR, reverberation (RT60), and direct-to-reverberant ratio is essential for interpreting WER degradation patterns across microphone positions.
  - Quick check question: Can you explain why a 10m distance microphone would show "pronounced high-frequency roll-off, lower SNR, and stronger reverberation" compared to a 0.12m lavalier?

- **Transfer Learning and Domain Adaptation**
  - Why needed here: The paper's approach relies on fine-tuning pre-trained Whisper models; understanding what transfers and what doesn't is critical for effective adaptation.
  - Quick check question: Why would a model pre-trained on diverse data still fail on Thai far-field speech without fine-tuning?

- **Data Augmentation for Speech (SpecAugment, RIR convolution)**
  - Why needed here: Section 4.5 shows augmentation can partially compensate for single-mic training limitations by simulating acoustic diversity.
  - Quick check question: How would convolving near-field speech with room impulse responses (RIRs) simulate far-field conditions?

## Architecture Onboarding

- **Component map:**
  - Recording setup: 9 single-channel microphones (3 lavaliers at 0.12-0.15m, 3 condensers at ≤0.5m, JBL at 2m, BT3m at 3m, BT10m at 10m)
  - Corpus: 114 hours total (88h train, 12.8h dev, 13.3h test), 90 sessions, 86 speakers
  - Baseline model: Pathumma-whisper-th-large-v3 (Thai-fine-tuned Whisper large-v3)
  - Optional front-end: WPE dereverberation, MMSE-LSA spectral subtraction (both showed negative results)
  - Augmentation pipeline: RIR convolution + noise mixing for single-mic scenarios

- **Critical path:**
  1. Start with Thai-fine-tuned Whisper variant (not vanilla Whisper)
  2. Fine-tune on LOTUSDIS train split using all 5 microphone types (5 epochs on H200)
  3. Evaluate on test split with per-microphone and per-condition (overlap vs. single-speaker) breakdown
  4. If only single-mic data available, apply RIR-based augmentation before fine-tuning

- **Design tradeoffs:**
  - **Multi-mic vs. single-mic training:** Multi-mic provides robustness (49.5% vs. 79.5% far-field WER) but requires diverse recording setup
  - **With vs. without front-end processing:** Fine-tuning alone (49.5% WER) outperforms fine-tuning + WPE (56.1% WER) or MMSE-LSA (54.6% WER)
  - **Augmentation complexity:** Simple RIR + noise mixing provides meaningful gains (79.5% → 65.4% far-field WER) without multi-mic infrastructure

- **Failure signatures:**
  - **Zero-shot on far-field:** WER >100% on distant microphones indicates severe domain mismatch
  - **Single-mic training on unseen devices:** WER >90% suggests device-specific overfitting
  - **Uniform front-end processing:** Near-field WER increase of 15-25% indicates unnecessary preprocessing
  - **Overlap conditions:** Consistent 10-20% WER increase in overlapped speech (Fig. 2) indicates need for separation or diarization

- **First 3 experiments:**
  1. **Reproduce baseline:** Fine-tune Pathumma-whisper-th-large-v3 on LOTUSDIS train split, verify ~38% overall WER and ~49% far-field WER on test split
  2. **Ablate microphone types:** Train on near-field only (lavalier + condenser), evaluate on far-field (JBL, BT3m, BT10m) to confirm overfitting pattern
  3. **Test augmentation strategy:** Train single-mic (condenser) with RIR augmentation, measure gap vs. all-mic baseline on far-field channels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive front-end processing strategies that selectively apply dereverberation based on estimated channel characteristics outperform the uniform application strategies tested?
- **Basis in paper:** [explicit] Section 4.4 states that "a uniform front-end strategy is suboptimal" and suggests "applying front-end processing selectively to far-field channels or adopting adaptive strategies."
- **Why unresolved:** The baseline experiments only applied WPE and MMSE-LSA uniformly, which degraded performance; the efficacy of conditional application remains untested.
- **What evidence would resolve it:** Evaluation of a system that dynamically disables enhancement for near-field inputs, demonstrating lower overall WERs than the Table 3(C) baselines.

### Open Question 2
- **Question:** Does integrating explicit speech separation modules specifically reduce the high substitution error rates observed in Thai tone-bearing syllables during overlapping speech?
- **Basis in paper:** [explicit] Section 4.6 reports "substitution errors were more frequent in Thai's tone-bearing syllables" under overlap and highlights the "need for advances in both speech separation and linguistic modeling."
- **Why unresolved:** The current Whisper baseline was evaluated without a dedicated separation front-end, leaving the interaction between separation, tonal accuracy, and overlap unaddressed.
- **What evidence would resolve it:** An ablation study showing that a separation-to-ASR cascade significantly reduces the overlap-specific WER spike and specifically lowers tonal substitution errors.

### Open Question 3
- **Question:** Can sophisticated neural augmentation methods fully close the generalization gap between single-microphone training and the diverse multi-microphone baseline?
- **Basis in paper:** [inferred] Table 3 shows that while "lightweight" augmentation helped single-mic models, a large performance gap remained (e.g., 65.39% vs 49.54% far-field WER) compared to the all-mic baseline.
- **Why unresolved:** The authors demonstrated improvements using simple RIR augmentation but did not explore if advanced techniques could match the robustness provided by diverse real recording conditions.
- **What evidence would resolve it:** A single-mic training regime using neural acoustic simulation achieving far-field WERs statistically indistinguishable from the all-microphone baseline.

## Limitations

- Generalization bounds to environments with different room acoustics (RT60, noise profiles) remain untested beyond the 3.2m × 2.4m × 2.4m recording space.
- Single-mic fine-tuning with augmentation shows partial compensation but with substantial performance gaps (79.5% → 65.4% far-field WER) compared to multi-mic training (49.5% WER).
- The mechanism behind front-end processing degradation is not fully characterized—whether it's due to over-processing, feature destruction, or model-architecture incompatibility.

## Confidence

- **High confidence**: The documented WER improvements from fine-tuning on LOTUSDIS (38.3% overall, 49.5% far-field) are reproducible given the dataset and baseline model specification.
- **Medium confidence**: The claim that multi-mic training is necessary for robustness, as single-mic training shows severe overfitting to device-specific acoustics.
- **Low confidence**: The assertion that augmentation fully compensates for lack of multi-mic training, given the remaining 15-20% WER gap.

## Next Checks

1. Evaluate LOTUSDIS-trained models on external Thai far-field datasets with different room characteristics to test acoustic generalization.
2. Conduct ablation studies comparing different augmentation strategies (RIR-only, noise-only, combined) to identify optimal single-mic training approaches.
3. Test whether domain-specific front-end processing pipelines (instead of uniform WPE/MMSE-LSA) can improve fine-tuned model performance without degradation.