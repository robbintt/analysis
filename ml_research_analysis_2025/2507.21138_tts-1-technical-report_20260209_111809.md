---
ver: rpa2
title: TTS-1 Technical Report
arxiv_id: '2507.21138'
source_url: https://arxiv.org/abs/2507.21138
tags:
- audio
- speech
- training
- arxiv
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Inworld TTS-1, a set of two large-scale text-to-speech
  models based on Transformer architectures. TTS-1-Max contains 8.8B parameters for
  highest quality and expressiveness, while TTS-1 contains 1.6B parameters optimized
  for real-time and on-device applications.
---

# TTS-1 Technical Report

## Quick Facts
- arXiv ID: 2507.21138
- Source URL: https://arxiv.org/abs/2507.21138
- Authors: Oleg Atamanenko; Anna Chalova; Joseph Coombes; Nikki Cope; Phillip Dang; Zhifeng Deng; Jimmy Du; Michael Ermolenko; Feifan Fan; Yufei Feng; Cheryl Fichter; Pavel Filimonov; Louis Fischer; Kylan Gibbs; Valeria Gusarova; Pavel Karpik; Andreas Assad Kottner; Ian Lee; Oliver Louie; Jasmine Mai; Mikhail Mamontov; Suri Mao; Nurullah Morshed; Igor Poletaev; Florin Radu; Dmytro Semernia; Evgenii Shingarev; Vikram Sivaraja; Peter Skirko; Rinat Takhautdinov; Robert Villahermosa; Jean Wang
- Reference count: 40
- Primary result: Introduces two large-scale Transformer-based TTS models (TTS-1-Max: 8.8B parameters, TTS-1: 1.6B parameters) achieving state-of-the-art performance with sequential training (Pre-training → SFT → RL) and 48 kHz audio codec with super-resolution

## Executive Summary
This paper introduces Inworld TTS-1, a set of two large-scale text-to-speech models based on Transformer architectures. TTS-1-Max contains 8.8B parameters for highest quality and expressiveness, while TTS-1 contains 1.6B parameters optimized for real-time and on-device applications. Both models use a sequential training methodology including pre-training on 1M hours of raw audio mixed with text data, supervised fine-tuning on 200k hours of high-quality audio-text pairs, and reinforcement learning alignment using a reward function combining WER, speaker similarity, and perceptual quality scores. A novel 48 kHz audio codec with a super-resolution module enables high-resolution speech synthesis, and textual audio markups provide fine-grained control over speaking styles and non-verbal vocalizations. The models support 11 languages and achieve state-of-the-art performance, with TTS-1-Max showing superior WER and speaker similarity scores compared to both the smaller model and commercial alternatives in internal evaluations. The system also features a streaming inference pipeline with optimized low-latency decoding.

## Method Summary
The TTS-1 models employ a sequential training pipeline: pre-training on 1M hours of mixed audio-text data using a SpeechLM architecture (LLaMA-3.2-1B or LLaMA-3.1-8B backbone), supervised fine-tuning on 200k hours of high-quality audio-text pairs, and reinforcement learning alignment using GRPO with a composite reward function. The system uses a custom 48 kHz audio codec (X-codec2) with a super-resolution module for efficient high-resolution synthesis, and supports 11 languages with textual audio markups for style control. The models achieve state-of-the-art performance through this multi-stage approach that combines autoregressive token prediction with perceptual quality optimization.

## Key Results
- TTS-1-Max (8.8B parameters) achieves superior WER and speaker similarity scores compared to both the smaller TTS-1 model and commercial alternatives in internal evaluations
- The 48 kHz audio codec with super-resolution module achieves the highest DNSMOS score (4.195) compared to 16/24 kHz versions
- RL alignment with GRPO reduces hallucinations and improves perceptual quality beyond what supervised learning achieves alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequential training (Pre-training → SFT → RL) is hypothesized to be critical for robust speech synthesis quality and instruction following.
- **Mechanism:** Large-scale pre-training establishes a strong joint distribution of text and audio tokens. SFT then concentrates this distribution into high-quality, responsive behaviors. Finally, RL alignment optimizes for perceptual metrics (like speaker similarity and noise reduction) that pure likelihood training often misses.
- **Core assumption:** The model requires a broad foundational understanding of audio patterns before it can be effectively refined for specific quality metrics.
- **Evidence anchors:** [abstract] demonstrates sequential process; [section 3.4] shows pre-training improves WER by ~15%; related reports (Qwen3-TTS, CosyVoice 2) adopt similar multi-stage training.

### Mechanism 2
- **Claim:** A single-codebook codec with a super-resolution decoder enables efficient 48 kHz generation without the computational cost of RVQ.
- **Mechanism:** The X-codec2 encoder compresses audio into a single stream of discrete tokens (50/sec). The decoder uses strided transposed convolutions to expand the temporal resolution of predicted acoustic features before converting them to waveforms, effectively "hallucinating" high-frequency details necessary for 48 kHz output.
- **Core assumption:** Acoustic features predicted at a lower temporal resolution contain sufficient information to reconstruct high-fidelity audio via convolutional upsampling.
- **Evidence anchors:** [section 2.1] describes super-resolution module; [section 3.2] shows 48 kHz decoder achieving highest DNSMOS (4.195); [corpus] evidence on this specific mechanism is weak.

### Mechanism 3
- **Claim:** Reinforcement Learning with GRPO steers the model away from hallucinations and improves perceptual quality beyond what supervised learning achieves.
- **Mechanism:** GRPO generates multiple outputs for a prompt, ranks them using a composite reward function (WER, Speaker Similarity, DNSMOS), and updates the policy to favor outputs with higher relative advantages. This directly penalizes artifacts like "clicks or pops" that standard cross-entropy loss might ignore.
- **Core assumption:** The reward models (Whisper for WER, WavLM for similarity) are accurate proxies for human auditory preference.
- **Evidence anchors:** [abstract] mentions RL alignment; [section 3.5] shows training curves with combined reward model; [corpus] corroborated by general trends in LLM alignment.

## Foundational Learning

- **Concept: Autoregressive Token Prediction (SpeechLM)**
  - **Why needed here:** This system frames speech synthesis as a language modeling task. You must understand that the model predicts the next *audio token* (not a waveform sample) conditioned on text and reference audio tokens.
  - **Quick check question:** How does the model handle the vocabulary expansion from 128k text tokens to 193k tokens to accommodate audio codes?

- **Concept: Neural Audio Codecs (Specifically X-codec2)**
  - **Why needed here:** The quality of the TTS is bottlenecked by the codec. Understanding that this codec uses a *single* codebook (unlike EnCodec which uses RVQ) is vital for debugging tokenization issues.
  - **Quick check question:** Why does the paper claim a single codebook structure is more efficient for streaming inference than residual vector quantization?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** The RL stage is a key differentiator. You need to distinguish GRPO from standard PPO—specifically how it uses group statistics (mean of rewards) rather than a separate value function (critic) to estimate advantages.
  - **Quick check question:** Why does the reward pipeline require decoding the audio tokens back to waveform before calculating the reward?

## Architecture Onboarding

- **Component map:** Audio Encoder (X-codec2) + Text Tokenizer → Discrete Tokens → SpeechLM → Audio Decoder with Super-Resolution → Waveform

- **Critical path:** The "Prompt" construction is the most critical data structure. It concatenates `[Text Tokens] + [Reference Audio Tokens]`. The SpeechLM must autoregressively generate `[Target Audio Tokens]` which are then decoded.

- **Design tradeoffs:**
  - **Latency vs. Quality:** TTS-1 (1.6B) is optimized for speed (DDP training, smaller batch), while TTS-1-Max (8.8B) requires FSDP/DeepSpeed and yields lower WER but slower inference.
  - **Streaming vs. Stability:** Streaming requires "concatenation at non-voicing regions" to avoid artifacts, which introduces slight latency overhead to find these silence pockets.
  - **Volume Stability:** The decoder adds an RMS loudness loss term; removing this causes noticeable volume drops during streaming concatenation.

- **Failure signatures:**
  - **Hallucinations:** The model generates non-verbal sounds (clicks/pops) not present in text. *Fix:* Increase RL alignment weight or check GRPO reward weights.
  - **Style Bleeding:** Reference audio prosody overrides the prompt instructions. *Fix:* Ensure the SpeechLM is not caching prompt audio tokens too aggressively in the inference engine.
  - **Volume Fluctuation:** Audio clips sound quieter/louder across chunks. *Fix:* Verify the context window of the decoder includes the previous chunk to stabilize volume.

- **First 3 experiments:**
  1. **Codec Isolation Test:** Feed ground-truth audio through the encoder and decode it back (bypassing the SpeechLM) to establish the upper bound of audio quality (DNSMOS ≈ 4.19).
  2. **Ablate Pre-training:** Fine-tune the SFT stage from a base LLaMA model vs. the audio-pretrained checkpoint to verify the 15% WER degradation on your specific data.
  3. **Reward Sensitivity:** Run inference with the RL-aligned model using extreme temperatures (T < 0.5 vs T > 1.0) to observe the trade-off between speaker similarity (high at low T) and expressiveness (high at high T).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can evaluation methodologies be refined to accurately capture perceptual quality in dynamic scenarios, specifically accounting for prosodic and emotional variations?
- **Basis in paper:** [explicit] The Conclusion states that current metrics like speaker similarity fluctuate with emotionally expressive speech, failing to fully capture perceptual quality.
- **Why unresolved:** Automated metrics (WER, SIM) do not bridge the gap between quantitative scores and human perception in dynamic, real-world contexts.
- **What evidence would resolve it:** A new evaluation protocol or metric that demonstrates high correlation with human preference scores specifically for varied prosodic and emotional content.

### Open Question 2
- **Question:** Why does the inclusion of text-based instruction-following data during supervised fine-tuning degrade synthesis quality despite training loss indicating no issues?
- **Basis in paper:** [inferred] Section 3.4 notes that mixing text instructions led to synthesis failures (hallucinations/unreliable speech) "despite observing no adverse impact on the training loss."
- **Why unresolved:** The mechanism causing the disconnect between the optimization objective (loss) and the generative failure in multimodal settings is unidentified.
- **What evidence would resolve it:** A training strategy or loss function modification that allows mixed text-audio instruction tuning without degrading speech synthesis reliability.

### Open Question 3
- **Question:** How can cross-lingual style generalization be improved to match the fidelity of English speech synthesis when using audio markup tags?
- **Basis in paper:** [inferred] Section 3.6 reports that the model demonstrates "emergent capability for style generalization to non-English languages, although with reduced fidelity."
- **Why unresolved:** The style control dataset was composed entirely of English audio, creating a domain gap for the other 10 supported languages.
- **What evidence would resolve it:** Quantitative benchmarks showing equitable style transfer accuracy and control across all 11 supported languages.

## Limitations

- Architectural details of the super-resolution module are not fully specified, potentially impacting reproducibility
- GRPO implementation specifics (KL divergence coefficient, clip range, optimizer settings) are not provided in tables
- Evaluation relies heavily on internal benchmarks and comparisons to commercial APIs without independent verification

## Confidence

**High Confidence:**
- Sequential training methodology (Pre-training → SFT → RL) improves speech quality and instruction following based on internal ablation studies
- 48 kHz audio codec with super-resolution module achieves higher DNSMOS scores than 16/24 kHz versions
- RL alignment with GRPO reduces hallucinations compared to supervised learning alone

**Medium Confidence:**
- TTS-1-Max outperforms commercial alternatives in WER and speaker similarity (based on internal comparisons)
- The 1.6B parameter model achieves real-time performance without significant quality degradation
- Streaming inference pipeline maintains quality through chunk concatenation at non-voicing regions

**Low Confidence:**
- Claimed state-of-the-art performance across all 11 supported languages without per-language fine-tuning
- Specific language-dependent CPS thresholds used for data filtering are not disclosed
- Exact impact of vocabulary expansion (128k → 193k tokens) on model performance

## Next Checks

1. **Codec Quality Verification:** Implement the X-codec2 super-resolution module independently and measure DNSMOS on the same validation set to verify the claimed 4.195 score. Compare against a baseline using standard upsampling techniques.

2. **Training Ablation Replication:** Replicate the pre-training ablation study (Figure 5) by fine-tuning the SFT stage from both a base LLaMA model and the audio-pretrained checkpoint using the same data and hyperparameters to verify the 15% WER improvement.

3. **Reward Function Impact Analysis:** Conduct a controlled experiment varying the GRPO reward weights (α, β, γ) systematically to determine their individual contributions to WER, speaker similarity, and perceptual quality improvements, validating the authors' equal weighting choice.