---
ver: rpa2
title: 'Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement
  Learning'
arxiv_id: '2601.13942'
source_url: https://arxiv.org/abs/2601.13942
tags:
- search
- visual
- image
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of knowledge-intensive visual
  question answering where Large Multimodal Models struggle due to static parametric
  knowledge and inability to handle long-tail or evolving visual information. The
  authors propose Glance-or-Gaze (GoG), a framework that shifts from passive image
  perception to active visual planning by introducing a Selective Gaze mechanism that
  dynamically chooses between global context (glance) and fine-grained region analysis
  (gaze) before retrieval.
---

# Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.13942
- **Source URL**: https://arxiv.org/abs/2601.13942
- **Reference count**: 23
- **Primary result**: SOTA on six knowledge-intensive visual QA benchmarks, improving by 5-20 points over strong baselines

## Executive Summary
This paper addresses the challenge of knowledge-intensive visual question answering where Large Multimodal Models struggle due to static parametric knowledge and inability to handle long-tail or evolving visual information. The authors propose Glance-or-Gaze (GoG), a framework that shifts from passive image perception to active visual planning by introducing a Selective Gaze mechanism that dynamically chooses between global context (glance) and fine-grained region analysis (gaze) before retrieval. The framework employs a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning to instill active selection and cross-modal reflection, followed by Complexity-Adaptive Reinforcement Learning to enhance planning capabilities for complex queries. Experiments across six benchmarks demonstrate state-of-the-art performance with significant gains ranging from 5 to 20 points over strong baselines, validating the effectiveness of the adaptive GoG planning paradigm.

## Method Summary
The Glance-or-Gaze framework implements a two-stage training pipeline. First, Reflective GoG Behavior Alignment uses supervised fine-tuning on 5,750 GoG-Instruct samples to teach basic tool invocation patterns and the Glance→Gaze decision structure. Second, Complexity-Adaptive Reinforcement Learning employs GRPO to optimize over sampled trajectories, rewarding correct final answers and format compliance, with harder samples (Level 2) providing denser learning signal. The Selective Gaze mechanism uses Grounding DINO to propose candidate bounding boxes, then evaluates their relevance before deciding which (if any) to search, filtering irrelevant information before retrieval. The model employs LoRA adapters (rank=8) for efficient parameter updates during both stages.

## Key Results
- SOTA performance on six knowledge-intensive visual QA benchmarks
- 5-20 point improvements over strong baselines
- Ablation shows removing Selective Gaze causes -1.52 to -1.83 average score drops
- Complexity-adaptive RL on Level 2 data yields +3.49 to +5.94 average gains over Level 1 data

## Why This Works (Mechanism)

### Mechanism 1: Selective Gaze Pre-Filters Visual Noise Before Retrieval
The model first proposes candidate bounding boxes via visual grounding, then evaluates their relevance before deciding which (if any) to search. This forces a deliberate "observe" step rather than passive "see." If grounding consistently misses target regions, the gaze mechanism has nothing to select and degrades to whole-image behavior.

### Mechanism 2: Two-Stage Training Shifts Behavior From Single-Step to Iterative Search
SFT establishes basic tool invocation patterns; RL then reshapes the policy toward multi-step verification. SFT provides demonstration trajectories with explicit "Glance → Decision → Gaze" structure. GRPO-based RL then optimizes over groups of sampled trajectories, rewarding correct final answers and format compliance. Harder samples (Level 2) provide denser learning signal.

### Mechanism 3: Hard-Example Stratification Provides Stronger Policy Gradient Signal
Training RL on samples where SFT achieves <50% pass rate yields better generalization than training on easier samples. Level 2 samples sit at the decision boundary or beyond, producing higher-variance rewards within GRPO groups, which sharpens the advantage estimates and policy updates.

## Foundational Learning

- **Concept**: Group Relative Policy Optimization (GRPO)
  - Why needed here: The RL stage uses GRPO to optimize search policies by comparing grouped trajectory rewards rather than single samples, reducing variance.
  - Quick check question: Can you explain why normalizing rewards within a group stabilizes training compared to absolute reward baselines?

- **Concept**: Visual Grounding (Object Localization)
  - Why needed here: The gaze mechanism depends on Grounding DINO to propose candidate regions before selective search.
  - Quick check question: Given an image and a text query like "the license plate on the car," what does a grounding model output?

- **Concept**: Tool-Augmented LMM Training
  - Why needed here: The model must learn to emit structured tokens (`<img_search>`, `<text_search>`, `<answer>`) that trigger external tools, which requires supervised then reinforced behavior shaping.
  - Quick check question: What is the difference between prompt-based tool use and training-tool-integrated tool use?

## Architecture Onboarding

- **Component map**: (Image, Question) → LMM backbone → SFT training → LoRA adapters → RL optimization → Tools (Grounding DINO, SerpAPI, Jina Reader, Qwen3-32B) → LLM-as-Judge evaluation

- **Critical path**:
  1. SFT training must converge on basic tool syntax and the Glance→Gaze pattern before RL.
  2. RL rollout sampling (N=5 per prompt) must execute tools in real-time; search API reliability is a bottleneck.
  3. Reward computation depends on judge model availability and latency.

- **Design tradeoffs**:
  - LoRA (r=8) vs. full fine-tuning: Saves memory but may limit capacity for complex policy changes.
  - Level 2 vs. Level 1 data: Harder data improves performance but may slow convergence.
  - Parallel tool calls (4 threads): Faster exploration but increases API rate-limit risk.

- **Failure signatures**:
  - Model outputs malformed tool tokens → format reward drops to 0 → RL collapses to KL penalty.
  - Grounding returns overlapping/irrelevant boxes → Selective Gaze has no valid candidates → falls back to whole-image search.
  - Search API timeout (>5% failure rate noted in paper) → incomplete context → answer quality degrades.

- **First 3 experiments**:
  1. Sanity check: Run GoG-SFT on 100 held-out InfoSeek samples; verify tool token emission and correct XML formatting.
  2. Ablation: Train without Selective Gaze (w/o SG) on same data; compare crop selection accuracy and answer correctness to confirm mechanism contribution.
  3. Hardness sweep: Train three RL variants on Level 1 only, Level 2 only, and mixed; plot average score vs. pass-rate threshold to validate complexity-adaptive claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does GoG's Selective Gaze mechanism generalize to multilingual or cross-lingual visual question answering tasks?
- **Basis in paper**: [explicit] The limitations section states: "our experiments primarily focus on English-language benchmarks, and the generalization of GoG to multilingual or cross-lingual visual question answering remains unexplored."
- **Why unresolved**: The current evaluation exclusively uses English-language benchmarks, and the grounding tool (Grounding DINO) and search pipeline may have different effectiveness across languages with varying tokenization and web content availability.
- **What evidence would resolve it**: Experiments on multilingual VQA benchmarks (e.g., X-VQA, multilingual InfoSeek variants) showing performance gaps and whether the Selective Gaze mechanism transfers across languages.

### Open Question 2
- **Question**: Can the framework's robustness be improved to handle the 1-5% infrastructure failure rate without degrading answer quality?
- **Basis in paper**: [explicit] The authors acknowledge: "we observe an occasional failure rate of approximately 1–5% due to network instability, API timeouts, or malformed webpage content. Such failures may cause incomplete information retrieval and degrade answer quality."
- **Why unresolved**: The current implementation lacks explicit failure recovery mechanisms or fallback strategies when search tools fail mid-trajectory.
- **What evidence would resolve it**: Ablation studies comparing current behavior against variants with retry mechanisms, graceful degradation to parametric knowledge, or cached responses when tools fail.

### Open Question 3
- **Question**: What is the computational cost and latency overhead of the iterative RL-learned search behavior compared to single-pass approaches?
- **Basis in paper**: [inferred] The paper shows RL training increases "Mix Search" behavior from ~20-29% to ~74-77%, indicating more multi-step searches. However, no analysis of inference time, API call counts, or computational cost per query is provided.
- **Why unresolved**: The tradeoff between improved accuracy and increased latency/cost from iterative reflection and multiple search calls remains unquantified.
- **What evidence would resolve it**: Per-query analysis of average search iterations, total API calls, end-to-end latency, and cost comparisons between SFT-only and RL-trained models across difficulty levels.

### Open Question 4
- **Question**: How does GoG perform when the grounding tool (Grounding DINO) fails to detect relevant visual entities?
- **Basis in paper**: [inferred] The Selective Gaze mechanism depends on Grounding DINO returning bounding boxes before the model selects regions. If grounding fails on small, occluded, or unusual entities, the entire gaze pipeline may break down.
- **Why unresolved**: No analysis examines cases where grounding produces no results or incorrect bounding boxes, and whether the model can recover through whole-image search fallbacks.
- **What evidence would resolve it**: Error analysis on samples where Grounding DINO produces no detections or incorrect regions, with comparison to a baseline that bypasses grounding.

## Limitations
- Heavy dependence on visual grounding tool quality (Grounding DINO)
- 1-5% infrastructure failure rate from search APIs affecting consistency
- Potential overfitting to InfoSeek benchmark distribution

## Confidence
- **High confidence**: The mechanism of Selective Gaze reducing visual noise before retrieval
- **Medium confidence**: The two-stage training approach effectiveness
- **Medium confidence**: Complexity-adaptive RL benefits
- **Low confidence**: Claims about generalizability beyond the InfoSeek benchmark

## Next Checks
1. **Cross-dataset generalization test**: Evaluate GoG on non-InfoSeek benchmarks (e.g., OK-VQA, A-OKVQA) to verify that the Glance-or-Gaze planning strategy transfers beyond the training distribution.

2. **Grounding failure analysis**: Systematically test the model when Grounding DINO produces incorrect or missing bounding boxes (e.g., by using a controlled subset of images where ground truth regions are known) to quantify the actual impact of grounding failures on overall performance.

3. **Alternative RL algorithm comparison**: Replace GRPO with standard PPO or direct policy optimization and compare performance to determine whether the specific GRPO formulation is critical to the observed gains or if simpler RL approaches would suffice.