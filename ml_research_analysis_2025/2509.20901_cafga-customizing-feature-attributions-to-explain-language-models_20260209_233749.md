---
ver: rpa2
title: 'CafGa: Customizing Feature Attributions to Explain Language Models'
arxiv_id: '2509.20901'
source_url: https://arxiv.org/abs/2509.20901
tags:
- explanations
- cafga
- users
- user
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CafGa, an interactive system for generating
  and evaluating feature attribution explanations at customizable granularities for
  language models. CafGa allows users to define prediction tasks and customize text
  segmentation interactively, supporting default options like word, sentence, or paragraph-level
  divisions while enabling further refinement through user interactions.
---

# CafGa: Customizing Feature Attributions to Explain Language Models

## Quick Facts
- arXiv ID: 2509.20901
- Source URL: https://arxiv.org/abs/2509.20901
- Authors: Alan Boyle; Furui Cheng; Vilém Zouhar; Mennatallah El-Assady
- Reference count: 13
- Key outcome: Interactive system for customizable feature attribution explanations in language models, showing 64% user preference over automated baselines

## Executive Summary
CafGa is an interactive system that enables users to generate and evaluate feature attribution explanations for language models at customizable granularities. Unlike traditional word-level attributions, CafGa allows users to define semantic text segments through interactive grouping, then computes Shapley-based feature attributions on these customized units. The system uses KernelSHAP to estimate feature importance and provides perturbation-based fidelity metrics (deletion/insertion curves) to help users validate their segmentations. A user study with ten participants and four experts found that CafGa is easy to use for experienced users and produces explanations preferred over automated baselines in 64% of comparisons, particularly for complex reasoning tasks.

## Method Summary
CafGa implements a human-in-the-loop approach to feature attribution where users define prediction tasks (prompt template, input text, target answer, evaluator type) and customize text segmentation into semantic units. The system then generates perturbed samples by removing subsets of these user-defined groups, queries the LLM multiple times per sample, converts responses to boolean outcomes via the evaluator, and estimates Shapley values through weighted linear regression. Explanations are visualized as heatmaps with color-blind friendly palettes, and deletion/insertion curves provide fidelity feedback. The system is implemented as a TypeScript/React frontend with FastAPI backend, supporting both API-based and local model usage.

## Key Results
- Human-customized explanations were preferred over automated baselines (PartitionSHAP and MExGen) in 64% of comparisons
- HotpotQA multi-hop reasoning tasks showed 75% preference for human-generated explanations
- Interactive segmentation improved fidelity scores from 0.77 to 0.96 in a worked example
- System demonstrated ease of use for experienced users while novices required more guidance

## Why This Works (Mechanism)

### Mechanism 1: Human-Guided Semantic Segmentation Improves Attribution Interpretability
User-customized text segmentation captures semantic units that fixed word-level methods miss, producing explanations better aligned with both human intuition and model reasoning. Users interactively group words into meaningful spans, and the system computes attributions at this customized granularity via KernelSHAP, treating each user-defined group as an atomic feature for perturbation and Shapley value estimation. This approach assumes human semantic grouping better reflects meaningful information units influencing LLM decisions than automatic tokenization or n-gram partitioning.

### Mechanism 2: Perturbation-based Shapley Value Approximation Quantifies Feature Contributions
The system uses sampling-based perturbation of grouped input features, followed by weighted linear regression, to approximate Shapley values quantifying each group's marginal contribution to predictions. For n feature groups, perturbed samples are generated by removing subsets of groups, each perturbed input is sent to the LLM multiple times, responses are converted to boolean outcomes via user-defined evaluators, and probability estimates are computed as the proportion of true outcomes. This method assumes perturbation preserves enough context for meaningful outputs and that boolean evaluators capture relevant aspects of predictions.

### Mechanism 3: Perturbation Curves Provide Fidelity Feedback on Explanation Quality
Deletion and insertion curves plot prediction change as features are removed or added in attribution order, providing objective fidelity signals that help users validate whether their segmentation captures model-relevant information. Deletion starts with all features present and iteratively removes highest-attributed features; insertion starts with none and adds them. The x-axis plots percentage of words perturbed (not groups) to avoid bias toward large groupings; the y-axis shows prediction difference from original. Area Over the Perturbation Curve (AOPC) serves as the fidelity score, with larger deletion areas and smaller insertion areas indicating better fidelity.

## Foundational Learning

- **Concept: Shapley Values and KernelSHAP**
  - Why needed here: CafGa's attribution engine is built on KernelSHAP, which approximates Shapley values. Understanding marginal contribution, efficiency, and sampling tradeoffs is essential for interpreting results and setting parameters.
  - Quick check question: Given three feature groups A, B, C, can you explain why the Shapley value for A averages its marginal contribution across all possible orderings of {A, B, C}?

- **Concept: Perturbation-based Explanation Methods**
  - Why needed here: The entire attribution pipeline relies on removing feature groups and observing prediction changes. Understanding how perturbation relates to importance—and its limitations—is critical for using CafGa effectively.
  - Quick check question: Why might deleting a word like "not" from "The service was not good" cause a larger prediction shift than deleting "was"?

- **Concept: Fidelity vs. Plausibility Tradeoffs in Explainability**
  - Why needed here: Users may prefer plausible explanations that do not faithfully reflect model reasoning. CafGa uses fidelity metrics to mitigate this, but the tradeoff remains.
  - Quick check question: If an explanation is highly intuitive to a user but has low deletion fidelity, what might that indicate about the model's actual reasoning process?

## Architecture Onboarding

- **Component map:** Frontend (TypeScript + React + Vite) -> Backend (FastAPI + Uvicorn) -> Attribution engine (Python + SHAP) -> Model interface (OpenAI API) -> Evaluator module (Boolean operators + NLI model)

- **Critical path:** 1) User defines task (prompt template + input text + target answer + evaluator type) 2) User selects/creates segmentation (preset or interactive custom grouping) 3) Backend generates perturbation samples based on groups 4) LLM queried multiple times per sample, responses converted to boolean via evaluator 5) KernelSHAP estimates Shapley values via weighted linear regression 6) Frontend renders attribution heatmap 7) Backend computes deletion/insertion curves, frontend displays AOPC fidelity scores 8) User iterates (refine segmentation, re-run, compare fidelity)

- **Design tradeoffs:** Speed vs. accuracy (interactive mode limits samples), granularity vs. interpretability (fewer groups = faster but may obscure reasoning), model-agnostic vs. model-specific (broad applicability vs. inability to leverage internal activations)

- **Failure signatures:** High latency/timeout (too many custom groups exceeds sampling budget), low fidelity scores (segmentation doesn't align with model usage), confusing evaluators (non-experts struggled with evaluator logic), circular validation (high fidelity may not guarantee true faithfulness)

- **First 3 experiments:** 1) Replicate Section 3 worked example (restaurant recommendation task, compare fidelity 0.77 → ~0.96) 2) Granularity sweep (word/sentence/paragraph on 200-word review, record time, AOPC, interpretability) 3) Evaluator sensitivity test (compare "Contains" vs. "Entails" on binary classification task, observe attribution pattern changes)

## Open Questions the Paper Calls Out

- **Guidance mechanisms for speed-accuracy tradeoff**: Providing users with appropriate guidance in balancing this trade-off and thereby determining a suitable number of groups remains an important direction for future investigation.

- **Alternative fidelity metrics**: These metrics introduce a degree of circularity as both the explanation and the evaluation are based on perturbation. Their applicability also becomes questionable when features represent groups rather than atomic units, and further research is needed to develop more reliable fidelity metrics.

- **Real-world practical use**: Evaluating the practical use of CafGa in real-world, non-laboratory settings would provide valuable insights and offer a more accurate assessment of its usability and effectiveness.

## Limitations
- Perturbation-based evaluation introduces potential circularity as both explanation and fidelity metrics rely on the same removal mechanism
- Manual segmentation process requires significant user effort and may not scale well for analyzing many instances or complex tasks
- System's reliance on external LLM APIs introduces variability in response consistency and potential cost barriers

## Confidence
- **High confidence**: Core technical implementation (KernelSHAP sampling, perturbation-based attribution, fidelity metrics) follows established methods with clear algorithmic description
- **Medium confidence**: User preference results are promising but limited by small sample size (10 participants) and potential selection bias in task choices
- **Medium confidence**: Claim that semantic segmentation improves interpretability is supported by qualitative feedback but lacks systematic analysis of when and why human grouping outperforms automated methods

## Next Checks
1. **Scalability test**: Apply CafGa to a larger dataset (minimum 100 instances) across diverse task types to assess whether the 64% preference rate holds and identify task characteristics that benefit most from customization

2. **Expert validation study**: Have NLP experts independently evaluate the faithfulness of human-generated vs. automated explanations without knowing the source, measuring agreement with model internals when accessible

3. **Ablation analysis**: Systematically compare attribution quality when using different sampling budgets (e.g., 10x, 100x, 1000x samples) and evaluator types to quantify the tradeoff between computational cost and attribution accuracy