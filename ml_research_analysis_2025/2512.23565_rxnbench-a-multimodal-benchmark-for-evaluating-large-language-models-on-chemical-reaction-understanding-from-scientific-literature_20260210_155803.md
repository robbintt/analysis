---
ver: rpa2
title: 'RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical
  Reaction Understanding from Scientific Literature'
arxiv_id: '2512.23565'
source_url: https://arxiv.org/abs/2512.23565
tags:
- open
- chemical
- reasoning
- reaction
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RxnBench evaluates multimodal large language models on chemical
  reaction understanding from scientific PDFs through two tasks: Single-Figure QA
  (SF-QA) with 1,525 questions from 305 reaction schemes testing visual perception
  and mechanistic reasoning, and Full-Document QA (FD-QA) with 540 questions from
  108 articles requiring cross-modal synthesis of text, schemes, and tables. Models
  achieve high accuracy on SF-QA (up to 96.23%) but struggle with FD-QA where no model
  exceeds 50% accuracy, revealing critical gaps in deep chemical logic and precise
  structural recognition despite strong performance on explicit text extraction.'
---

# RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature

## Quick Facts
- arXiv ID: 2512.23565
- Source URL: https://arxiv.org/abs/2512.23565
- Reference count: 36
- Primary result: MLLMs achieve up to 96.23% on single-figure QA but struggle with full-document QA (max 50% accuracy), highlighting gaps in chemical reasoning despite strong visual perception

## Executive Summary
RxnBench introduces a multimodal benchmark evaluating large language models on chemical reaction understanding from scientific literature. The benchmark features two tasks: Single-Figure QA (SF-QA) with 1,525 questions from 305 reaction schemes testing visual perception and mechanistic reasoning, and Full-Document QA (FD-QA) with 540 questions from 108 articles requiring cross-modal synthesis of text, schemes, and tables. Models show high accuracy on SF-QA (up to 96.23%) but struggle significantly with FD-QA where no model exceeds 50% accuracy. The benchmark reveals critical gaps in deep chemical logic and precise structural recognition despite strong performance on explicit text extraction.

## Method Summary
RxnBench evaluates MLLMs on chemical reaction understanding through two zero-shot multiple-choice tasks. SF-QA uses cropped reaction scheme images with 4-choice questions testing visual perception and mechanistic reasoning. FD-QA uses full PDF articles rendered at 144 dpi as image sequences with variable-response 5-choice questions requiring cross-modal synthesis. Both tasks use GPT-4o-based response mapping for free-text outputs to option alignment. The benchmark includes bilingual (English/Chinese) questions sourced from 8 chemistry journals published within 5 years. Exact-match scoring applies with missing/extraneous selections counted as errors.

## Key Results
- SF-QA accuracy reaches 96.23% (Gemini-3-Flash) while FD-QA struggles with max 50% accuracy
- Inference-time reasoning (Chain-of-Thought) improves performance significantly, particularly for complex reasoning tasks
- Structure recognition remains universally challenging with accuracy drops of 15-25 points across all models
- Task breakdown shows Fact Extraction at 37.2% of SF-QA, Mechanism & Process at 19.4%, Reagent Roles at 16.5%, Comparative Analysis at 14.2%, Structure Recognition at 8.8%, and Global Understanding at 3.9%

## Why This Works (Mechanism)

### Mechanism 1
Inference-time reasoning (Chain-of-Thought) improves performance on complex chemical reasoning tasks by generating internal chains of thought before outputting answers. This enables multi-step reasoning for chemical transformations that require tracking reactants→intermediates→products across dynamic processes, bridging perception with deep chemical logic.

### Mechanism 2
Performance scales with document complexity—single-figure extraction is near-solved for top models, but full-document synthesis exposes critical gaps in cross-modal integration. SF-QA isolates visual perception within cropped reaction schemes, while FD-QA requires navigating multi-page PDFs with spatially separated text, schemes, and tables, demanding reference resolution across modalities.

### Mechanism 3
Structure recognition remains a universal bottleneck that reasoning improvements alone cannot resolve. Chemical structures require pixel-level graph parsing to capture exact connectivity, stereochemistry, and functional group identity. General-purpose visual encoders lack the domain-specific inductive biases for this precision task.

## Foundational Learning

- **SMILES / Extended-SMILES notation**: Needed for structure recognition tasks requiring molecular string representations; quick check: Given "CC(C)C(=O)O", identify functional group and count carbon atoms?
- **Stereochemistry (wedge/dash bonds)**: Critical for distinguishing atomic positioning; quick check: What does solid vs. dashed wedge indicate about atomic positioning?
- **Reaction scheme components**: Essential for distinguishing species roles; quick check: How do you distinguish catalyst from reactant based on reaction scheme?

## Architecture Onboarding

- **Component map**: PDF Input → Uni-Parser → [Scheme/Table/Text Extraction] → Visual Encoder → Multimodal LLM → Inference Module → Structured Output Parser
- **Critical path**: Visual encoding of molecular graphs → Cross-modal reference resolution → Multi-step reasoning chain
- **Design tradeoffs**: Proprietary vs. Open-weight (Gemini-3-Flash: 96.23% vs Qwen3-VL-Think: 91.77%), Instruct vs. Think (5-7% gain, 2-3x latency), Single-figure vs. Full-document evaluation
- **Failure signatures**: Hallucinated structures, cross-modal misalignment, stereochemistry inversion, context overflow
- **First 3 experiments**: 1) Baseline SF-QA evaluation on Qwen3-VL-8B-Instruct (~75% expected), 2) Think vs. Instruct ablation on Mechanism questions (94.59% vs ~89.86% delta), 3) Structure recognition stress test (expect 15-25 point drops)

## Open Questions the Paper Calls Out

- **Can domain-specific visual encoders resolve structure recognition bottlenecks?**: Current MLLMs rely on general-purpose vision encoders that struggle with fine-grained stereochemical details; specialized chemical visual encoders achieving significantly higher accuracy on Structure Recognition would resolve this.

- **Do active agentic workflows improve FD-QA performance?**: Current models struggle to synthesize information across long contexts; agentic architectures autonomously navigating, querying, and verifying literature data would demonstrate improvement.

- **Does integration of external chemical tools mitigate hallucinations?**: Current models generate chemically incorrect structures; tool-augmented MLLMs showing reduction in chemically invalid structures and increased exact-match scores would resolve this.

## Limitations

- Visual encoder limitations may not be the primary bottleneck for structure recognition due to uncontrolled variables in the GPT-4o mapping pipeline
- Inference-time reasoning benefits vary significantly across task types, suggesting task-dependent rather than universal applicability
- FD-QA evaluation methodology has ambiguities in context length handling and error analysis distinguishing visual vs. reasoning failures

## Confidence

- **Low**: Visual encoder limitations as primary bottleneck - strong evidence but evaluation doesn't isolate visual encoding from mapping failures
- **Medium**: Inference-time reasoning benefits - consistent improvements but effect size varies across task types
- **Medium**: FD-QA evaluation methodology - claims critical gaps but has evaluation setup ambiguities

## Next Checks

1. **Ablation Study on Visual Encoding**: Evaluate structure recognition tasks using cropped molecular images to isolate whether performance gains stem from improved visual encoding versus enhanced reasoning.

2. **Mapping Pipeline Robustness Test**: Implement and evaluate alternative response mapping strategies to quantify how much of the reported accuracy drop in structure recognition stems from the mapping process versus actual model capabilities.

3. **Cross-Modal Resolution Analysis**: Conduct detailed error analysis on FD-QA questions requiring reference resolution across text, schemes, and tables to measure the frequency and impact of cross-modal misalignment errors.