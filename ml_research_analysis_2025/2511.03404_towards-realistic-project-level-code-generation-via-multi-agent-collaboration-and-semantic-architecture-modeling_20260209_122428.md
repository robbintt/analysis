---
ver: rpa2
title: Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration
  and Semantic Architecture Modeling
arxiv_id: '2511.03404'
source_url: https://arxiv.org/abs/2511.03404
tags:
- code
- generation
- test
- project-level
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating complete software
  projects from user requirements, which is more complex than function-level code
  generation. It introduces CodeProjectEval, a new dataset built from 18 real-world
  repositories with 12.7 files and 2,388.6 lines of code per task, supplemented with
  documentation and executable test cases.
---

# Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling

## Quick Facts
- arXiv ID: 2511.03404
- Source URL: https://arxiv.org/abs/2511.03404
- Authors: Qianhui Zhao; Li Zhang; Fang Liu; Junhang Cheng; Chengru Wu; Junchen Ai; Qiaoyuanhe Meng; Lichen Zhang; Xiaoli Lian; Shubin Song; Yuanping Guo
- Reference count: 29
- Key outcome: CodeProjectEval dataset and ProjectGen framework achieve 310 test cases passed (10x improvement over baselines) and 52/124 on DevBench (57% improvement).

## Executive Summary
This paper tackles the challenge of generating complete software projects from user requirements, moving beyond function-level code generation. The authors introduce CodeProjectEval, a new dataset of 18 real-world repositories with 12.7 files and 2,388.6 lines of code per task, and ProjectGen, a multi-agent framework that uses structured architectural planning and iterative refinement. By decomposing the task into architecture design, skeleton generation, and code filling stages with judge feedback, ProjectGen significantly outperforms existing approaches on both datasets.

## Method Summary
ProjectGen is a multi-agent framework that generates complete software projects through three sequential stages: Architecture Design, Skeleton Generation, and Code Filling, with iterative refinement guided by judge feedback. The core innovation is the Semantic Software Architecture Tree (SSAT), a structured JSON representation of the project's modules, files, classes, and functions. Each stage uses a specialized agent (ArchAgent, SkeletonAgent, CodeAgent) to generate artifacts, which are validated by corresponding judge agents (JudgeA, JudgeS, JudgeC) using predefined thresholds. The framework incorporates memory-based context management to retrieve past error-feedback pairs for more efficient iterative refinement.

## Key Results
- On CodeProjectEval: ProjectGen passes 310 test cases (roughly 10x improvement over baselines)
- On DevBench: ProjectGen passes 52/124 test cases (57% improvement over baselines)
- The framework shows consistent superiority across all evaluation metrics including weighted average of passed tests and SketchBLEU

## Why This Works (Mechanism)

### Mechanism 1: Structured Intermediate Representation (SSAT)
- Claim: Decomposing requirements into a strict Semantic Software Architecture Tree (SSAT) reduces the semantic gap between natural language and code by constraining the solution space before implementation begins.
- Mechanism: The framework mandates an "Architecture Design" phase where an LLM must generate a JSON-like tree structure defining modules, files, classes, and function signatures with parameters. This forces explicit planning of dependencies and interfaces, preventing "hallucinated" imports or missing files during later coding stages.
- Core assumption: The LLM possesses sufficient reasoning capability to accurately infer file structure and function signatures from requirements alone, without seeing the implementation details.
- Evidence anchors:
  - [abstract]: "...we introduce the Semantic Software Architecture Tree (SSAT), a structured and semantically rich representation that effectively bridges user requirements and source code implementation."
  - [section 4.1]: "SSAT adopts a tree-structured, machine-parsable format that explicitly encodes modules, files, and functions... enabling LLMs to interpret architectural intent and progressively generate implementation-level artifacts."
  - [corpus]: The broader literature (e.g., MRG-Bench) highlights the critical need for repository-level context, but the specific causal link to a tree-structured intermediate representation is unique to this paper's evaluation.
- Break condition: If the requirements are ambiguous or the project requires dynamic architectural changes during coding, a static pre-generated SSAT may over-constrain the solution, leading to incoherent code.

### Mechanism 2: Generator-Judge Iteration with Execution Feedback
- Claim: Introducing a "Judge" agent to evaluate outputs against check tests at each stage prevents the cascading propagation of early errors.
- Mechanism: Instead of a single pass, the system uses a loop: Generator → Judge → Feedback → Generator. Crucially, the "Code Filling" stage uses executable "check tests" (distinct from final unit tests). If code fails, the error log is fed back to the Code Agent for targeted revision, ensuring basic functional correctness before final evaluation.
- Core assumption: The "Judge" agent (LLM-as-judge) can reliably diagnose the *cause* of a failure (e.g., import error vs. logic error) and provide actionable feedback, and the check tests provide sufficient coverage to catch regressions.
- Evidence anchors:
  - [abstract]: "ProjectGen... incorporates iteration-based refinement guided by judge feedback..."
  - [section 4.3]: "JudgeC first verifies whether the generated code can successfully pass the predefined check tests... [then] analyzes the error logs... and provides the names of the code files that require modifications."
  - [corpus]: The effectiveness of iterative refinement is supported by related work in stepwise refinement (e.g., SR-Eval), which finds that static, single-turn generation fails to mirror real-world development.
- Break condition: If the error is structural (e.g., a fundamental flaw in the SSAT discovered only during code filling), the code-level judge may struggle to propose the necessary architectural backtracking, resulting in repetitive failed attempts.

### Mechanism 3: Semantic Memory Retrieval
- Claim: Maintaining a "memory" of previous corrections and retrieving semantically similar past errors improves the efficiency and success rate of iterative refinement.
- Mechanism: The system records the (Feedback, Code Diff, Test Status Change) tuple for every iteration. When a new error occurs, it uses BM25 to retrieve the top-k most relevant past experiences from the agent's memory to inform the current fix, simulating "learning" within the session.
- Core assumption: The vector space of error messages correlates with the solution space; i.e., similar-looking error logs require similar fixes, even across different files or contexts.
- Evidence anchors:
  - [section 4.3.3]: "This approach allows CodeAgent to leverage historical experience to perform targeted refinements while ensuring functional correctness."
  - [abstract]: "...memory-based context management."
  - [corpus]: Corpus support for specific memory retrieval mechanisms in code generation is weak or missing; the paper anchors this claim primarily in its internal experimental results.
- Break condition: If the context window required to store the "memory" exceeds the model's limits, or if the retrieval mechanism surfaces misleading historical fixes for novel errors, performance may degrade.

## Foundational Learning

- Concept: **Abstract Syntax Trees (AST) vs. Semantic Trees**
  - Why needed here: The SSAT is essentially a semantic AST defined before code exists. Understanding that code has a hierarchical structure (Module → File → Class → Function) is prerequisite to understanding how SSAT guides the LLM.
  - Quick check question: Can you distinguish between a syntactic error (e.g., missing parenthesis) and a semantic/structural error (e.g., missing file import) in a Python repository?

- Concept: **Cascading Errors in Code Generation**
  - Why needed here: The paper explicitly targets the "propagation" of errors. A developer needs to understand how a missing function definition in file A causes an ImportError in file B, which is why the "Skeleton" generation phase precedes "Code Filling."
  - Quick check question: If an LLM generates `main.py` importing `utils.helper` but forgets to generate `utils.py`, at which stage of ProjectGen would this be caught?

- Concept: **LLM-as-a-Judge**
  - Why needed here: The system relies on LLMs not just to write code, but to *evaluate* it (JudgeA, JudgeS, JudgeC). You must accept that an LLM can provide a scalar score or pass/fail signal based on a rubric, rather than deterministic logic.
  - Quick check question: What are the risks of using an LLM to evaluate code correctness versus using a deterministic unit test suite?

## Architecture Onboarding

- Component map:
  - **Inputs**: PRD (Requirements), UML (Structure), Architecture Design (Dir Tree).
  - **Phase 1 (Arch)**: ArchAgent generates SSAT → JudgeA validates requirement coverage.
  - **Phase 2 (Skeleton)**: SkeletonAgent generates code with `pass` bodies → JudgeS validates compilation/interface matching.
  - **Phase 3 (Code)**: CodeAgent fills logic in topological order → JudgeC runs check tests → (Iterates until pass or max retries).
  - **Data**: Memory stores (Feedback, Diff) tuples for retrieval.

- Critical path: The **SSAT Generation** is the single point of failure. If the architectural tree is incorrect (e.g., missing a module), the downstream Skeleton and Code agents will hallucinate or fail, as they are strictly bound to the SSAT structure.

- Design tradeoffs:
  - **Correctness vs. Cost**: The iterative Judge loop significantly increases token usage and latency (up to 10 iterations in experiments) but is required for passing complex test cases.
  - **Determinism vs. Flexibility**: Strict adherence to SSAT ensures files match the design but may prevent the agent from "discovering" a better architecture during coding.

- Failure signatures:
  - **ImportError loops**: On large projects (CodeProjectEval), agents often fix one import but break another, cycling without resolving global dependency issues.
  - **Stubbed Code**: The CodeAgent may leave `NotImplementedError` or `pass` blocks if the SSAT description is vague or context length is exhausted.
  - **Test Regression**: A fix for one check test causes another previously passing test to fail (implied by the need for memory to track test status changes).

- First 3 experiments:
  1. **Sanity Check (DevBench)**: Run ProjectGen on a small task (e.g., "Hybrid_Images"). Verify that the SSAT is generated correctly and the iterative loop terminates.
  2. **Ablation (No Iteration)**: Run ProjectGen with `max_iterations=1` on the same task. Compare the pass rate to understand the marginal utility of the Judge/Feedback loop.
  3. **Stress Test (Scaling)**: Run on a CodeProjectEval task with >2,000 lines of code. Monitor for `ImportError` frequency and verify if the "Memory" component successfully retrieves relevant past diffs to fix them.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does interactive human modification of the Semantic Software Architecture Tree (SSAT) impact the fidelity and functional correctness of the generated software?
- Basis in paper: [explicit] Section 8 states, "we aim to explore interactive modification of software architecture representations, enabling human developers to directly guide and adjust generated architectures in real time."
- Why unresolved: The current framework operates in a fully automated loop without mechanisms for intermediate human intervention or correction of the architectural design.
- What evidence would resolve it: A comparative study measuring the pass rate of generated code when humans are allowed to edit the SSAT versus the current automated approach.

### Open Question 2
- Question: Can the generated implementation be leveraged to iteratively refine the initial architectural representation (SSAT) to improve consistency?
- Basis in paper: [explicit] Section 8 suggests that "generated code could be leveraged to iteratively update and refine the architectural representation... which may improve the overall coherence."
- Why unresolved: The current pipeline is unidirectional (Architecture → Code); the feedback loop from code back to architecture design has not been implemented or tested.
- What evidence would resolve it: Experiments demonstrating that updating the SSAT based on skeleton/code generation feedback reduces dependency errors (e.g., ImportErrors) in the final build.

### Open Question 3
- Question: Can ProjectGen maintain its performance advantages when applied to statically typed or non-Python programming languages?
- Basis in paper: [inferred] Section 7.3 notes that while the framework is language-agnostic, all experiments were conducted on Python, limiting generalizability to other ecosystems.
- Why unresolved: The specific prompts, SSAT structure, and error handling have been optimized for Python syntax and idioms.
- What evidence would resolve it: Evaluation results on an extended version of CodeProjectEval containing Java or C++ repositories showing similar pass rates relative to baselines.

### Open Question 4
- Question: How can the framework be enhanced to generate code with a scale (lines of code) comparable to ground-truth implementations for complex projects?
- Basis in paper: [inferred] Section 7.1.1 observes that for larger-scale datasets, "the gap in project size... becomes increasingly pronounced," with generated code often significantly shorter than ground truth.
- Why unresolved: The authors note that "effectively guiding the LLM to produce code of larger or more appropriate scale" remains a notable challenge.
- What evidence would resolve it: Metrics showing a reduction in the LOC gap between generated projects and ground truth without a loss in functional correctness.

## Limitations

- The framework's performance degrades on very large projects due to import dependency resolution issues, with generated code often significantly shorter than ground truth implementations.
- The memory retrieval mechanism's effectiveness is primarily validated internally, with weak corpus support for the assumption that similar error messages require similar fixes.
- The reliance on different LLMs for generation (DeepSeek-V3) and evaluation (GPT-4o) may introduce inconsistencies in evaluation standards and language understanding.

## Confidence

- **High Confidence**: The overall framework design and the necessity of intermediate representation (SSAT) for complex project generation is well-founded, supported by the significant improvement over baselines.
- **Medium Confidence**: The iterative refinement mechanism with judge feedback is sound in principle, but the effectiveness of the memory-based context management component needs more rigorous validation.
- **Low Confidence**: The scalability claims for very large projects (>2,000 lines) are based on limited experimental data, and the paper acknowledges issues with import dependency resolution that weren't fully resolved.

## Next Checks

1. **Ablation Study on Memory Component**: Run ProjectGen with and without the memory retrieval mechanism on a subset of CodeProjectEval tasks to quantify its marginal contribution to success rate and iteration count.
2. **Stress Test on Architectural Ambiguity**: Design PRDs with intentional ambiguities or missing requirements and measure how often the SSAT generation fails or produces incorrect architectures.
3. **Cross-LLM Consistency Check**: Replace the GPT-4o Judge agents with DeepSeek-V3 across all stages and measure the variance in pass rates to assess the impact of using different LLMs for generation vs. evaluation.