---
ver: rpa2
title: 'UniSE: A Unified Framework for Decoder-only Autoregressive LM-based Speech
  Enhancement'
arxiv_id: '2510.20441'
source_url: https://arxiv.org/abs/2510.20441
tags:
- speech
- arxiv
- inproc
- unise
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes UniSE, a unified decoder-only autoregressive
  language model (LM) framework for speech enhancement (SE) that handles speech restoration
  (SR), target speaker extraction (TSE), and speech separation (SS) tasks. The model
  uses continuous features extracted by a frozen WavLM with an adapter as conditions,
  and generates discrete tokens of the target speech via autoregressive modeling.
---

# UniSE: A Unified Framework for Decoder-only Autoregressive LM-based Speech Enhancement

## Quick Facts
- **arXiv ID:** 2510.20441
- **Source URL:** https://arxiv.org/abs/2510.20441
- **Reference count:** 0
- **Primary result:** UniSE achieves competitive performance on SR, TSE, and SS tasks using a unified decoder-only autoregressive LM with task token conditioning.

## Executive Summary
UniSE proposes a unified decoder-only autoregressive language model framework for speech enhancement that handles three distinct tasks: speech restoration, target speaker extraction, and speech separation. The model uses frozen WavLM features with a linear adapter, discrete token representations via BiCodec, and task-specific learnable tokens to switch between operational modes. Experiments show competitive performance against advanced baselines across multiple benchmarks, demonstrating the effectiveness of decoder-only AR LM in unifying SE tasks.

## Method Summary
UniSE uses a frozen WavLM-base-plus model with averaged transformer layers and a linear adapter to extract acoustic features. These features condition a 63M parameter LLaMA decoder-only backbone to generate discrete tokens via autoregressive modeling. BiCodec encodes target speech into 32 global tokens (speaker timbre) and semantic tokens at 50 tokens/second (speech content). Three learnable task tokens (T_SR, T_TSE, T_rTSE) define operational modes for speech restoration, target speaker extraction, and reverse target speaker extraction respectively.

## Key Results
- UniSE achieves competitive DNSMOS and NISQA scores on 2020 DNS Challenge and 2025 URGENT Challenge for speech restoration
- The model performs close to state-of-the-art on Libri2Mix for target speaker extraction and Libri2Mix/WSJ0-2mix for speech separation
- Multi-task training doesn't deteriorate individual task performance, demonstrating effective task unification
- The 63M parameter model outperforms larger LLaSE-G1 (~1B parameters), suggesting efficient feature extraction compensates for smaller LM backbone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task tokens enable a single decoder-only LM to switch between distinct speech enhancement operational modes without architectural changes.
- **Mechanism:** Learnable task-specific tokens (T_SR, T_TSE, T_rTSE) serve as conditional prefixes that prime the autoregressive model for different input-output mappings.
- **Core assumption:** The decoder-only architecture can learn sufficiently distinct conditional distributions from task token conditioning alone.
- **Evidence anchors:**
  - [section 2.3] "We define three operational modes... correspond to three distinct learnable task-specific tokens"
  - [section 3.2] "UniSE-TSE exclusively trained in the TSE mode achieves close performance... incorporating more tasks won't deteriorate the performance of individual task"

### Mechanism 2
- **Claim:** Disentangled discrete token representation (global + semantic) enables high-fidelity speech reconstruction while remaining tractable for autoregressive modeling.
- **Mechanism:** BiCodec encodes target speech into two streams: (1) global tokens (32 fixed-length) capturing speaker characteristics/timbre, and (2) semantic tokens (50 tokens/second) representing speech content.
- **Core assumption:** The disentanglement quality is sufficient such that global tokens preserve speaker identity and semantic tokens preserve linguistic content.
- **Evidence anchors:**
  - [section 2.2] "global feature E_g with a fixed length of 32 tokens... semantic feature E_s with a flexible length of 50 tokens per second"
  - [section 3.2] Table 6 shows X-codec2 with larger codebook size causes performance decay

### Mechanism 3
- **Claim:** Frozen WavLM with trainable adapter preserves robust speech representations while enabling task-specific adaptation to the LM's representational space.
- **Mechanism:** Averaging features across all WavLM transformer layers provides both acoustic and semantic information. The linear adapter projects these to the LM's hidden dimension.
- **Core assumption:** The frozen WavLM features contain sufficient information for all three SE tasks, and a linear adapter provides adequate transformation capacity.
- **Evidence anchors:**
  - [section 2.1] "We average the features from all transformer layers in WavLM to obtain sufficient acoustic and semantic information simultaneously"
  - [section 3.2] 63M parameter model outperforms LLaSE-G1 (~1B parameters)

## Foundational Learning

- **Concept:** Autoregressive Language Modeling (next-token prediction)
  - **Why needed here:** The entire UniSE framework generates discrete speech tokens via sequential prediction.
  - **Quick check question:** Given input sequence [T_SR, D, E_d, G, E_g, S, E_s], what is the model computing at each generation step?

- **Concept:** Neural Audio Codecs and Speech Tokenization
  - **Why needed here:** UniSE converts continuous speech to discrete tokens (BiCodec) and back.
  - **Quick check question:** Why might single-layer quantization be preferable to RVQ for autoregressive modeling, and what's the tradeoff?

- **Concept:** Conditional Generation with Prefix/Prompt Tuning
  - **Why needed here:** Task tokens function as learned prompts that condition generation.
  - **Quick check question:** How does the input sequence format differ between SR mode and TSE mode, and what information does each prefix component provide?

## Architecture Onboarding

- **Component map:** Audio -> WavLM (frozen) -> Linear Adapter -> Decoder-only LLaMA -> BiCodec Decoder -> Output Audio

- **Critical path:**
  1. WavLM feature extraction quality (frozen, so data diversity during pretraining matters)
  2. Adapter mapping to LM space (only trainable component before LM)
  3. AR generation of semantic tokens (variable length, content-critical)
  4. BiCodec reconstruction (disentanglement quality limits ceiling)

- **Design tradeoffs:**
  - Single-layer quantization vs. RVQ: Simpler AR modeling but potentially lower reconstruction fidelity
  - Frozen WavLM vs. end-to-end: Parameter efficiency and stability vs. potential for task-specific feature optimization
  - Multi-task unification vs. specialized models: Convenience and parameter sharing vs. potential per-task performance ceiling

- **Failure signatures:**
  - Speaker identity drift: Global tokens may not sufficiently condition timbre
  - Content hallucination: Semantic token generation may diverge from actual speech content
  - Mode confusion: Task tokens fail to induce distinct behaviors
  - Codec artifacts: Reconstruction quality caps performance

- **First 3 experiments:**
  1. Ablate task tokens: Replace learnable T_SR/T_TSE/T_rTSE with fixed embeddings or remove them entirely
  2. Codec substitution: Swap BiCodec for an alternative (e.g., EnCodec, DAC) to isolate codec contribution
  3. Mode interference test: Train three separate single-task models and compare against unified UniSE

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the UniSE framework be effectively extended to handle more general audio processing tasks beyond the specific speech enhancement sub-tasks currently supported?
- **Open Question 2:** How does the choice of neural audio codec, specifically regarding codebook size and quantization strategy, impact the performance ceiling of the autoregressive Language Model?
- **Open Question 3:** Can the proposed inference strategy for Speech Separation be scaled effectively to scenarios involving more than two speakers?

## Limitations
- The multi-task unification claim may be dataset-specific and doesn't fully test generalization to unseen degradation types
- The SS mode relies on a cascading approach that may not scale well to more complex separation scenarios
- The decoder-only LM backbone is relatively small (63M parameters) compared to other LLaMA-based SE models

## Confidence
**High Confidence:** The core mechanism of using frozen WavLM features with a linear adapter is well-established in the literature
**Medium Confidence:** The task token conditioning mechanism appears to work as described based on ablation results
**Low Confidence:** The generalization claim across diverse degradation types is weakly supported due to potential domain gaps

## Next Checks
1. Train three separate UniSE models (SR-only, TSE-only, SS-only) with identical architectures but without task token switching, then compare their individual task performance against the unified model
2. Replace BiCodec with an alternative speech tokenization method (e.g., EnCodec or a simple Mel-spectrogram discretization) while keeping all other components constant
3. Evaluate UniSE on datasets with degradation types not seen during training (e.g., different codecs, room acoustics, or non-stationary noise) and measure performance degradation