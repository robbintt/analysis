---
ver: rpa2
title: 'DUET: Dual Model Co-Training for Entire Space CTR Prediction'
arxiv_id: '2510.24369'
source_url: https://arxiv.org/abs/2510.24369
tags:
- conference
- duet
- candidate
- prediction
- kuaishou
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUET addresses the dual challenges of limited expressiveness and
  severe Sample Selection Bias (SSB) in large-scale recommender system pre-ranking.
  It proposes a set-wise modeling framework that captures inter-item dependencies
  in real-time under strict latency constraints, combined with a dual model co-training
  mechanism that extends supervision to unexposed samples via mutual pseudo-label
  refinement.
---

# DUET: Dual Model Co-Training for Entire Space CTR Prediction

## Quick Facts
- **arXiv ID:** 2510.24369
- **Source URL:** https://arxiv.org/abs/2510.24369
- **Reference count:** 40
- **Primary result:** Improves prediction accuracy across the entire candidate space and mitigates SSB, achieving up to 17.31% relative improvement in AUC and significant business gains.

## Executive Summary
DUET addresses the dual challenges of limited expressiveness and severe Sample Selection Bias (SSB) in large-scale recommender system pre-ranking. It proposes a set-wise modeling framework that captures inter-item dependencies in real-time under strict latency constraints, combined with a dual model co-training mechanism that extends supervision to unexposed samples via mutual pseudo-label refinement. This approach improves prediction accuracy across the entire candidate space and mitigates SSB. Extensive offline experiments and online A/B tests show DUET outperforms state-of-the-art baselines by up to 17.31% relative improvement in AUC and delivers significant business gains such as +0.195% total watch time and +0.173% novel cluster exposure. The system is now fully deployed in Kuaishou and Kuaishou Lite, serving hundreds of millions of users.

## Method Summary
DUET is a CTR prediction framework for recommender system pre-ranking that addresses two key challenges: capturing inter-item dependencies (set-wise modeling) and mitigating Sample Selection Bias (SSB) from training on exposed items. It uses Linear Attention to efficiently model user-candidate and candidate-candidate interactions within a candidate set, and employs dual model co-training with mutual pseudo-labeling to extend supervision to unexposed samples. The system trains two identical models (M_A and M_B) with independent initialization, where each model's predictions on unexposed samples serve as soft pseudo-labels for the other. A KL divergence regularization term balances view diversity with prediction consensus. The approach achieves significant improvements in AUC and business metrics while maintaining real-time latency constraints.

## Key Results
- Achieves up to 17.31% relative improvement in AUC over state-of-the-art baselines
- Delivers +0.195% total watch time and +0.173% novel cluster exposure in online A/B tests
- Fully deployed in Kuaishou and Kuaishou Lite, serving hundreds of millions of users
- Shows consistent improvements across multiple datasets (RecFlow and industrial stream)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Set-wise modeling captures inter-item dependencies (synergistic and suppressive relationships) that point-wise models miss, improving list coherence and diversity.
- **Mechanism:** Instead of scoring each item independently, DUET processes the entire candidate set in one forward pass using Linear Attention for two streams: (1) user-candidate interaction (cross-attention between candidates and user history) and (2) intra-candidate self-attention. This enables items to "see" each other during scoring. Degree normalization (inspired by GCN) prevents numerical explosion from removing softmax.
- **Core assumption:** Item interactions within a candidate set meaningfully affect user click behavior beyond individual relevance scores.
- **Evidence anchors:**
  - [abstract] "set-wise modeling framework that captures inter-item dependencies in real-time under strict latency constraints"
  - [section 3.1] Equations 3-4 define LinearAttn for F_c-s and F_c-c; Equation 7 adds degree normalization D^-1
  - [corpus] HoMer (arXiv:2510.11100) addresses sequential and set-wise contexts for CTR, suggesting this is an active research direction, though direct validation of DUET's specific linear attention approach is not provided in corpus
- **Break condition:** If candidate set size varies dramatically across requests without batching/padding strategies, or if attention degree normalization values approach zero (cold-start items with low similarity to history), gradient flow may destabilize.

### Mechanism 2
- **Claim:** Dual model co-training with mutual pseudo-labeling extends supervision to unexposed samples while reducing confirmation bias inherent in single-model pseudo-labeling.
- **Mechanism:** Two models (M_A, M_B) with identical architecture but independent initialization train simultaneously. For unexposed samples, each model's predictions serve as soft pseudo-labels for the other's loss. This creates an error-correction loop: if one model's pseudo-labels drift, the other can counteract. Only exposed samples use ground-truth labels.
- **Core assumption:** Independent initialization creates sufficiently diverse "views" that errors are uncorrelated enough for mutual correction.
- **Evidence anchors:**
  - [abstract] "dual model co-training mechanism that extends supervision to unexposed samples via mutual pseudo-label refinement"
  - [section 3.2-3.3] Equations 8-11 define pseudo-label exchange and conditional BCE loss; ablation (Table 3) shows removing co-training causes -5.17% RelaImpr (largest degradation)
  - [corpus] UKD (arXiv:2205.10249) uses knowledge distillation for unclicked samples but relies on single-teacher pseudo-labels, which DUET explicitly critiques as noise-prone
- **Break condition:** If both models converge to similar representations too quickly (view collapse), pseudo-labels become self-reinforcing errors. Assumption: random initialization provides enough initial diversity.

### Mechanism 3
- **Claim:** Consistency regularization via symmetric KL divergence balances view diversity (needed for error correction) with prediction consensus (needed for stable training).
- **Mechanism:** A KL divergence term (Equation 9) penalizes distributional disagreement between M_A and M_B predictions. This soft constraint prevents excessive divergence while preserving enough difference for mutual correction. Hyperparameter λ controls the trade-off.
- **Core assumption:** An optimal λ exists that maintains beneficial diversity without training instability.
- **Evidence anchors:**
  - [section 3.2] Equation 9 defines L_con as symmetric KL; section 4.4 shows unimodal λ sensitivity (Figure 4)
  - [section 4.3] Ablation shows removing KL causes -1.22% to -2.56% RelaImpr
  - [corpus] No direct validation in corpus; this is a standard semi-supervised learning technique applied to recommendation
- **Break condition:** λ too small → models diverge, producing inconsistent pseudo-labels that degrade co-training; λ too large → models homogenize, collapsing to self-training dynamics without error correction benefit.

## Foundational Learning

- **Concept: Sample Selection Bias (SSB) in cascaded recommenders**
  - **Why needed here:** DUET's entire motivation rests on SSB being severe at pre-ranking due to training on exposed items (tiny fraction) but inference on full candidate space.
  - **Quick check question:** Can you explain why SSB is worse at pre-ranking than at ranking stage? (Answer: Pre-ranking operates on larger candidate pools with simpler models, creating larger train-inference distribution gaps.)

- **Concept: Linear Attention complexity reduction**
  - **Why needed here:** Standard attention is O(mn) which is prohibitive for m~1000s candidates and n~100s-1000s history items; Linear Attention achieves O(m+n) via kernel trick.
  - **Quick check question:** How does Linear Attention rearrange computation to avoid the quadratic term? (Answer: φ(Q)(φ(K)^T V) → φ(Q)(φ(K)^T V) leverages associativity to compute KV first.)

- **Concept: Co-training vs. Self-training in semi-supervised learning**
  - **Why needed here:** DUET distinguishes itself from prior pseudo-labeling methods by using two models for mutual correction rather than a single model teaching itself.
  - **Quick check question:** What is confirmation bias in self-training, and how does co-training mitigate it? (Answer: Self-training amplifies initial errors; co-training relies on view diversity to catch each other's mistakes.)

## Architecture Onboarding

- **Component map:** Embedding Layer -> User-Candidate Linear Attention -> Intra-Candidate Linear Attention -> Fusion Layer -> MLP Head -> Co-Training Wrapper -> Loss Computation

- **Critical path:**
  1. Log processing collects exposed samples with ground-truth labels
  2. Unexposed candidates from same requests are pooled (no labels)
  3. Both models train on mixed batches: exposed uses real labels, unexposed uses peer pseudo-labels
  4. After training cycle, only one model deploys (best performer on validation) to avoid 2× serving cost

- **Design tradeoffs:**
  - **Set-wise vs. Point-wise:** Set-wise captures interactions but requires processing all m candidates together; batching/padding strategies needed for variable set sizes
  - **Dual models:** 2× training cost, but only 1× inference cost (deploy single model)
  - **Pseudo-label noise:** Soft labels preserve uncertainty but may propagate errors if both models agree on wrong predictions; KL regularization mitigates but doesn't eliminate

- **Failure signatures:**
  - **Training divergence:** Check degree normalization values (D diagonal) approaching zero or infinity; monitor KL divergence between models—if it grows unbounded, λ may be too small
  - **View collapse:** If model agreement >95% early in training, initialization may be too similar; consider stronger regularization or different seeds
  - **Latency violations:** Profile Linear Attention implementation; ensure φ(K)^T V is computed efficiently (avoid materializing full attention matrix)

- **First 3 experiments:**
  1. **Sanity check:** Train single model (no co-training) with set-wise architecture; compare AUC to two-tower baseline to isolate set-wise contribution
  2. **Co-training ablation:** Run full DUET vs. single-model pseudo-labeling (self-training); measure pseudo-label quality (correlation with held-out ground truth on unexposed samples if available)
  3. **λ sensitivity sweep:** Grid search λ ∈ {0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5} on validation set; plot RelaImpr to verify unimodal curve and identify optimal λ for your data distribution

## Open Questions the Paper Calls Out

- **Question:** Can the dual-model architecture be replaced by a more parameter-efficient mechanism (e.g., a single model with auxiliary branches) to halve the training memory overhead while maintaining the error-correction benefits?
  - **Basis in paper:** [inferred] The methodology relies on maintaining "two independent models... $M_A$ and $M_B$" with distinct parameters to provide diverse views, which inherently doubles the training resource requirements compared to single-model baselines.
  - **Why unresolved:** The paper validates the effectiveness of the dual-model approach but does not explore compression techniques or shared-parameter architectures that might reduce the industrial cost of training two full models.
  - **What evidence would resolve it:** Experiments comparing the current dual-model setup against parameter-sharing variants (e.g., using a single backbone with two output heads) to measure the trade-off between training efficiency and pseudo-label quality.

- **Question:** How does the performance of the co-training mechanism degrade in scenarios where the unexposed candidate set distribution differs drastically from the exposed training distribution?
  - **Basis in paper:** [inferred] The paper assumes unexposed data provides "rich signals" for pseudo-labeling, but also notes that pre-ranking models must generalize to a "much larger candidate space with distinct distributions," which risks introducing systematic noise into the mutual pseudo-labels.
  - **Why unresolved:** The offline experiments utilize "streaming data" which reflects the system's natural distribution, but the paper does not specifically stress-test the co-training loop against extreme distributional shifts or "out-of-domain" unexposed items.
  - **What evidence would resolve it:** Ablation studies on synthetic datasets where the distribution gap between exposed and unexposed sets is controlled and widened, observing the point where pseudo-label confirmation bias overwhelms the error-correction loop.

- **Question:** Does an adaptive or curriculum-based regularization coefficient ($\lambda$) yield more stable convergence than the fixed coefficient used in current experiments?
  - **Basis in paper:** [inferred] The analysis of hyperparameter $\lambda$ reveals a "unimodal relationship" where performance drops sharply if the value is too small (excessive divergence) or too large (homogenization), suggesting a fixed value may be suboptimal throughout the different phases of training.
  - **Why unresolved:** The paper identifies the sensitivity of $\lambda$ but treats it as a static hyperparameter to be tuned, leaving the potential for dynamic adjustment strategies unexplored.
  - **What evidence would resolve it:** Comparative experiments employing a scheduled $\lambda$ (e.g., starting low to encourage diversity and increasing later to enforce consensus) to see if it improves final AUC or training speed.

## Limitations
- Exact kernel function φ(·) for Linear Attention is unspecified, potentially affecting numerical stability
- Training batch composition (ratio of exposed to unexposed samples) not detailed, impacting co-training dynamics
- Degree normalization values approaching zero for cold-start items could cause gradient instability

## Confidence
- **High:** Set-wise modeling improves coherence/diversity by capturing item interactions; dual co-training extends supervision to unexposed samples; observed business impact (+0.195% total watch time, +0.173% novel cluster exposure)
- **Medium:** Linear Attention O(m+n) complexity claim (no direct latency measurements provided); KL regularization effectively balances view diversity and consensus (only shown via unimodal λ sensitivity, not independent validation)
- **Low:** Claim that co-training mitigates SSB better than single-model pseudo-labeling (no ablation against self-training on industrial data; RecFlow results may not generalize)

## Next Checks
1. Implement sanity check: single model (no co-training) with set-wise architecture vs. two-tower baseline to isolate set-wise contribution
2. Run co-training ablation: full DUET vs. single-model pseudo-labeling (self-training) on industrial data to measure pseudo-label quality and SSB mitigation
3. Perform λ sensitivity sweep on validation set to verify unimodal curve and identify optimal λ for your data distribution