---
ver: rpa2
title: 'MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection'
arxiv_id: '2512.04492'
source_url: https://arxiv.org/abs/2512.04492
tags:
- stance
- knowledge
- expert
- atheism
- msme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MSME tackles real-world zero-shot stance detection by introducing\
  \ a three-stage multi-expert framework: Knowledge Preparation retrieves and refines\
  \ background knowledge; Expert Reasoning employs three specialized modules\u2014\
  Knowledge Expert for factual reasoning, Label Expert for clarifying stance-label\
  \ mappings, and Pragmatic Expert for detecting rhetorical cues; Decision Aggregation\
  \ combines all analyses into a final stance. Experiments on three datasets (SEM16,\
  \ P-Stance, Weibo-SD) show state-of-the-art performance, with Macro-F1 improvements\
  \ up to +7.9 over baselines, especially on complex targets and rhetorically rich\
  \ texts."
---

# MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection

## Quick Facts
- **arXiv ID:** 2512.04492
- **Source URL:** https://arxiv.org/abs/2512.04492
- **Reference count:** 28
- **Primary result:** State-of-the-art zero-shot stance detection on three datasets, with Macro-F1 improvements up to +7.9 over baselines, especially on complex targets and rhetorically rich texts.

## Executive Summary
MSME introduces a three-stage multi-expert framework for zero-shot stance detection, addressing real-world challenges of complex targets, implicit intent (e.g., irony), and knowledge gaps. The framework retrieves and refines background knowledge, employs three specialized experts (Knowledge, Label, Pragmatic), and aggregates their analyses via a Meta-Judge. Experiments on SEM16, P-Stance, and Weibo-SD datasets show consistent improvements over baselines, with ablation studies confirming the necessity of each expert component.

## Method Summary
MSME is a multi-stage prompting framework requiring no model training. It begins with Knowledge Preparation: external knowledge is retrieved via SerpAPI, chunked (300 tokens, 30 overlap), embedded with BGE, and filtered to top-3 relevant chunks. Explicit Stance Labels (ESL) are generated via few-shot prompting. In Expert Reasoning, three parallel LLM calls analyze the text: Knowledge Expert distills refined knowledge and reasons from a factual perspective; Label Expert expands coarse labels into fine-grained sub-labels and reasons accordingly; Pragmatic Expert detects rhetorical cues and infers underlying intent. Finally, Decision Aggregation uses a Meta-Judge LLM to synthesize all expert outputs into a final stance prediction.

## Key Results
- MSME achieves state-of-the-art performance on three datasets (SEM16, P-Stance, Weibo-SD)
- Macro-F1 improvements up to +7.9 over baselines, with largest gains on complex targets and rhetorically rich texts
- Ablation studies confirm each expert's necessity: Label Expert crucial for ambiguous target-label mappings, especially in Chinese data
- MSME + Noise drops only 0.4-1.0 F1 vs. No KE + Noise dropping 4.6-7.8, demonstrating robustness

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Filtering Reduces Noise
Filtering raw retrieved knowledge through relevance-based selection reduces noise and improves downstream reasoning accuracy. The Knowledge Expert selects knowledge chunks from K_raw that are most relevant to the target-text pair, yielding K_fine. For each retained chunk, it generates a conclusion via chain-of-thought reasoning. This refined, filtered knowledge is then shared with all downstream experts. Core assumption: Relevant background knowledge can be identified via embedding similarity and target-text relevance, and irrelevant knowledge degrades stance reasoning. Evidence: MSME + Noise drops only 0.4-1.0 F1 vs. No KE + Noise dropping 4.6-7.8.

### Mechanism 2: Fine-grained Label Decomposition Clarifies Ambiguity
Decomposing coarse stance labels into fine-grained sub-labels clarifies target-label mappings, especially for compound or ambiguous targets. The Label Expert takes Explicit Stance Labels (ESL) and subdivides Favor/Against into specific sub-labels (FSL) reflecting different degrees or motivations. The expert then reasons which fine-grained label best fits the text, reducing semantic ambiguity. Core assumption: Ambiguity in stance detection often stems from coarse labels that conflate multiple nuanced positions; finer granularity reduces this. Evidence: On SEM16's compound target "Climate Change," MSME achieves +9.4 F1 over COLA.

### Mechanism 3: Explicit Rhetorical Detection Mitigates Misinterpretation
Explicitly detecting rhetorical devices (irony, sarcasm, hyperbole) and inferring actual intent mitigates systematic misclassification of non-literal expressions. The Pragmatic Expert analyzes the text for rhetorical patterns R. If present, it extracts R and infers the underlying intent I, then maps that intent to a stance label. If absent, it proceeds with literal interpretation. Core assumption: LLMs default to literal interpretation; explicit prompting to detect rhetoric and infer intent corrects this bias. Evidence: On texts identified as containing rhetoric, Pragmatic Expert alone improves F1 by +4.3 to +8.1 over "No Expert" baseline.

## Foundational Learning

- **Concept: Zero-shot stance detection**
  - **Why needed here:** The entire MSME framework operates without target-specific training data; understanding zero-shot paradigms clarifies why external knowledge and explicit reasoning chains are necessary.
  - **Quick check question:** Can you explain why zero-shot stance detection requires different mechanisms than in-target supervised stance detection?

- **Concept: Chain-of-thought reasoning in LLMs**
  - **Why needed here:** The Knowledge Expert uses a CoT-like process (k_fine → conclusion); understanding CoT helps grasp how explicit intermediate steps improve reasoning quality.
  - **Quick check question:** How does prompting an LLM to show its reasoning steps differ from prompting for direct classification?

- **Concept: Multi-agent / multi-expert coordination**
  - **Why needed here:** MSME uses three specialized experts and a Meta-Judge; understanding role-based decomposition and aggregation is essential for debugging and extending the framework.
  - **Quick check question:** What are the tradeoffs between having multiple specialized agents vs. one general-purpose agent?

## Architecture Onboarding

- **Component map:**
  SerpAPI retrieval -> Chunking (300 tokens, 30 overlap) -> BGE embedding -> Dedup (threshold 0.85) -> Top-3 chunks = K_raw -> Generate ESL -> KE (outputs K_fine + reasoning + stance) -> Shared K_fine to LE and PE -> LE (outputs FSL + analysis + stance) -> PE (outputs rhetoric + intent + stance) -> Meta-Judge (synthesizes all outputs -> final stance)

- **Critical path:**
  1. K_raw quality directly limits KE's filtering effectiveness.
  2. KE's K_fine is shared with LE and PE—KE failure propagates.
  3. Meta-Judge aggregation quality depends on structured, comparable outputs from all experts.

- **Design tradeoffs:**
  - Three-stage decomposition vs. single-prompt end-to-end: The paper shows "Integrated" (single prompt) drops F1 by 2.9-5.3. Decomposition improves handling of multiple factors but increases latency and API cost.
  - Explicit label refinement vs. using raw labels: Fine-grained labels help compound targets but add complexity for simple targets.
  - Parallel experts vs. sequential: Current design runs LE and PE after KE produces K_fine, creating a mild dependency.

- **Failure signatures:**
  - Noisy/irrelevant K_raw + missing KE: Large F1 drops (4.6-7.8) per noise injection experiment.
  - Compound target + missing LE: Weibo-SD performance drops most when LE removed (-4.5 F1 with GPT-3.5).
  - High-rhetoric texts + missing PE: Experiment 3 shows PE adds +4.3 to +8.1 F1 on rhetorical subsets.

- **First 3 experiments:**
  1. Validate retrieval pipeline: On a held-out target, manually inspect top-3 chunks from K_raw. Check relevance and redundancy removal. If >50% irrelevant, tune chunk size or similarity threshold.
  2. Ablate each expert individually: Run MSME on a 100-sample subset of SEM16 or Weibo-SD, removing one expert at a time. Confirm that F1 declines match paper-reported ranges (KE: -2.9 to -3.8; LE: -1.6 to -4.5; PE: -1.2 to -2.4).
  3. Stress-test Meta-Judge: Compare MSME's Meta-Judge vs. simple majority voting (Vote) on the same subset. Expect MSME to outperform by ~0.6-1.7 F1; if not, inspect whether expert outputs are contradictory or poorly structured.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of the Knowledge Expert is heavily dependent on the quality and relevance of retrieved external knowledge; in domains where background information is sparse or the target is already well-known to LLMs, this component may provide diminishing returns.
- The fine-grained label decomposition mechanism lacks direct empirical validation against alternative label-handling strategies, making it unclear whether the specific FSL structure is optimal or necessary.
- The rhetorical detection component shows positive impact in experiments but the paper does not provide detailed analysis of false positives/negatives in rhetoric classification, raising questions about robustness across different domains and languages.

## Confidence
- **High confidence:** The overall framework architecture and the reported performance improvements over baselines (F1 gains of 2.9-7.9 points) are well-supported by the ablation studies and controlled experiments presented.
- **Medium confidence:** The specific mechanisms for each expert (knowledge filtering, label refinement, rhetoric detection) are logically sound and show experimental support, but lack direct comparison against alternative implementations of the same concepts.
- **Low confidence:** The generalizability of the framework to domains with different rhetoric patterns, knowledge availability, or stance-label structures is not thoroughly explored.

## Next Checks
1. **Knowledge retrieval validation:** Manually audit the top-3 knowledge chunks retrieved for 20 randomly selected targets to measure relevance rate and identify systematic retrieval failures.
2. **Expert-specific ablation with domain variation:** Run MSME with individual experts removed on datasets from different domains (e.g., health misinformation, product reviews) to test if the importance ranking of experts holds across contexts.
3. **Rhetoric detection precision analysis:** Create a small test set of texts with known rhetorical devices and measure the Pragmatic Expert's precision and recall, comparing against human annotation to establish baseline performance.