---
ver: rpa2
title: A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic
  Styles
arxiv_id: '2502.15856'
source_url: https://arxiv.org/abs/2502.15856
tags:
- artistic
- generative
- style
- images
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper critically assesses modern generative models\u2019\
  \ ability to replicate artistic styles. A new dataset, AI-pastiche, was created\
  \ using 73 prompts across 12 generative models, producing 953 AI-generated artworks\
  \ imitating historical painting styles."
---

# A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic Styles

## Quick Facts
- arXiv ID: 2502.15856
- Source URL: https://arxiv.org/abs/2502.15856
- Reference count: 40
- Only about 30% of AI-generated images were misclassified as human-made, with best models achieving nearly 50% authenticity

## Executive Summary
This paper evaluates modern generative models' ability to replicate historical artistic styles through two lenses: perceptual authenticity (whether outputs appear human-made) and prompt adherence (how well outputs match instructions). The authors created AI-pastiche, a dataset of 953 AI-generated artworks imitating 73 historical painting styles across 12 generative models. User surveys revealed that while models perform well for recent artistic periods, they struggle with older styles due to hyperrealism bias, and fail to accurately render complex subjects like human figures and multi-element scenes.

## Method Summary
The authors created 73 standardized text prompts covering painting styles from the 15th to 20th century, then generated images using 12 pre-trained models (DALL-E3, StableDiffusion variants, Flux, Midjourney, etc.). They manually curated the highest-quality outputs to create the AI-pastiche dataset of 953 images. Two separate user surveys evaluated authenticity (600+ public participants classifying AI vs human art) and prompt adherence (expert evaluators rating style/content accuracy on a 3-point scale). The study analyzed results by period, style, and subject matter to identify systematic strengths and weaknesses.

## Key Results
- Only ~30% of AI-generated images were misclassified as human-made, with the best model achieving nearly 50% authenticity
- Hyperrealism emerged as a primary obstacle, making outputs visually striking but historically inconsistent
- Generative models struggled with complex scenarios, human depictions, and maintaining stylistic accuracy for older artistic periods
- Models perform particularly well for 19th-20th century styles (Impressionism, Cubism) but have difficulty with Renaissance and Baroque precision requirements

## Why This Works (Mechanism)

### Mechanism 1: Hyperrealism-Dominance Trade-off
- **Claim:** Generative models optimized for photorealism produce outputs that are visually striking but stylistically inconsistent with pre-photographic artistic periods.
- **Mechanism:** Diffusion models trained on mixed datasets develop priors toward sharp textures, pore-level detail, and artificial lighting that override stylistic conditioning when generating historical painting styles.
- **Core assumption:** Model training corpora contain disproportionate representation of photographic references relative to historical artworks.
- **Evidence anchors:**
  - [abstract] "Hyperrealism emerged as a primary obstacle, as models often over-emphasize detail, making outputs visually striking but historically inconsistent."
  - [Section 7.2.1] "This excessive sharpness and detail can create a fundamental mismatch between the expected stylistic conventions and the generated output."
  - [corpus] LouvreSAE paper addresses similar entanglement, noting "existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering" which "can be computationally expensive and may still entangle style with subject matter."
- **Break condition:** If models were trained on period-balanced datasets with explicit style supervision during pre-training, hyperrealism bias would diminish.

### Mechanism 2: Period-Dependent Style Fidelity Gradient
- **Claim:** Style replication accuracy decreases with historical distance from contemporary visual conventions.
- **Mechanism:** Training data distribution reflects modern image prevalence; models develop stronger priors for 19th-20th century styles (Impressionism, Cubism) that share visual vocabulary with contemporary photography than for Renaissance or Baroque precision requirements.
- **Core assumption:** The relationship between training data temporal distribution and generation fidelity is approximately monotonic.
- **Evidence anchors:**
  - [Section 6.1, Table 6] XX century achieves 0.33 misclassification ratio versus 0.22 for XVIII century; Renaissance at 0.24 versus Impressionism at 0.38.
  - [Section 6.1] "generative models perform particularly well in mimicking art of the last century, and (some styles) of the XIX century. They clearly seem to be in much more trouble in producing convincing artifacts of previous periods."
  - [corpus] Weak corpus support—no directly comparable period-fidelity studies found in neighbors.
- **Break condition:** Explicit temporal conditioning or period-specific fine-tuning would flatten this gradient.

### Mechanism 3: Subject-Complexity Failure Amplification
- **Claim:** Human depictions and complex multi-element scenes disproportionately degrade perceived authenticity compared to landscapes or still lifes.
- **Mechanism:** Anatomical precision requires spatial coherence across multiple body parts; errors in one region (hands, facial features) trigger gestalt rejection even when other elements are accurate. Complex scenes multiply opportunities for anachronisms or distortions.
- **Core assumption:** Human visual processing has evolved specialized detectors for anatomical inconsistencies that trigger faster rejection than landscape anomalies.
- **Evidence anchors:**
  - [Section 6.1.2, Figure 7] Negative weights for "crowd" (-0.15), "person" (-0.12), "portrait" (-0.10) versus positive for landscape elements.
  - [Section 7.1.1] "Correctly rendering hands and fingers remains one of the major challenges in generative image synthesis."
  - [corpus] Parametric Shadow Control paper notes portrait generation lacks "intuitive shadow control" and struggles across "diverse styles," consistent with subject-specific difficulty.
- **Break condition:** Specialized anatomical modules or human-centric architectural components would reduce this amplification effect.

## Foundational Learning

- **Diffusion Model Denoising Process:**
  - Why needed here: Understanding that fixed denoising steps create rigid computational budgets regardless of scene complexity (Section 7.1.2 criticizes this limitation).
  - Quick check question: Can you explain why a 50-step diffusion process allocates identical computation to a simple still life versus a complex battle scene?

- **Text Encoder-Image Decoder Alignment:**
  - Why needed here: The paper evaluates prompt adherence separately from authenticity, requiring understanding of how CLIP/T5 encodings condition latent diffusion (Table 1 references SD3.5 using CLIP and T5).
  - Quick check question: How would a misalignment between text encoder semantics and diffusion decoder priors manifest as prompt adherence failures?

- **Perceptual Evaluation Methodology:**
  - Why needed here: The study uses human surveys rather than automated metrics, with specific design choices (randomization, cultural background assessment) that affect validity.
  - Quick check question: Why did the authors choose three-class relative ranking (Good/Medium/Low) for prompt adherence rather than absolute scoring?

## Architecture Onboarding

- **Component map:**
  Prompt Design (73 standardized prompts) -> Multi-Model Generation (12 diffusion models) -> Dataset Construction (953 images + metadata) -> ┌─────────────────┬────────────────────┐ -> │ Authenticity    │ Prompt Adherence   │ -> │ Survey          │ Evaluation         │ -> │ (600+ public)   │ (5706 entries)     │ -> └─────────────────┴────────────────────┘ -> Analysis: period/style/subject correlations

- **Critical path:** Prompt standardization -> model selection -> blind survey design -> statistical analysis. Prompt structure (style + period + subject + tone) determines downstream evaluation validity.

- **Design tradeoffs:**
  - Authenticity vs. adherence: SD1.5 and Omnigen sacrifice strict adherence for aesthetic quality; DALL-E3 and Leonardo Phoenix prioritize instruction following.
  - Survey scale vs. depth: Public authenticity survey (600 participants, binary classification) vs. expert adherence evaluation (smaller pool, three-class ranking).
  - Dataset balance vs. scope: Current imbalance toward XIX-XX century (62.5% combined) versus comprehensive historical coverage.

- **Failure signatures:**
  - **Hyperrealism flag:** Skin pores visible in Renaissance portraits, excessive glossiness in still lifes.
  - **Anachronism flag:** Modern objects (sneakers, vehicles, iron fences) in period-specific prompts.
  - **Anatomical distortion flag:** Incorrect finger counts, merged limbs in crowd scenes.
  - **Atmospheric inconsistency flag:** Dawn/sunset lighting failures; clouds rendered photographically rather than stylistically.

- **First 3 experiments:**
  1. **Baseline authenticity test:** Generate 20 images per model using 5 Renaissance prompts; run blind classification with 50 participants to establish per-model baseline.
  2. **Period complexity probe:** Compare misclassification rates across 5 periods using identical subject matter (e.g., portrait) to isolate temporal gradient effects.
  3. **Adherence-authenticity correlation:** For your highest-adherence model, plot authenticity score against prompt constraint level (minimal vs. highly detailed prompts) to characterize the trade-off curve.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do Large Language Models (LLMs) possess an "aesthetic sense," and can they articulate the principles guiding their artistic preferences?
- **Basis in paper:** [explicit] The introduction states this work is part of a project to "assess whether Large Language Models possess an aesthetic sense and, if so, to identify the aesthetic principles that guide their preferences."
- **Why unresolved:** The current paper focuses on creating the dataset and evaluating generative models; the separate task of using this dataset to evaluate LLMs as judges is listed as future work.
- **What evidence would resolve it:** A study using the AI-pastiche dataset to correlate LLM aesthetic evaluations with human expert judgments, demonstrating consistency independent of memorization.

### Open Question 2
- **Question:** Can embedding-based models like CLIP accurately capture nuanced artistic adherence to replace human evaluation?
- **Basis in paper:** [explicit] The authors note that while they considered automated evaluation, the ability of embeddings to capture subtle attributes remains uncertain, stating, "we plan to address CLIP... in our future research."
- **Why unresolved:** The adherence survey was conducted manually by the research team because current automated metrics may fail to capture complex stylistic instructions.
- **What evidence would resolve it:** A high statistical correlation between CLIP-score metrics and human subjective ratings on the "Adherence to Prompt Instructions" survey data.

### Open Question 3
- **Question:** Can dynamic inference methods, such as confidence-based step adjustments, mitigate the "rigid inference time" issue in complex artistic compositions?
- **Basis in paper:** [explicit] The conclusion identifies fixed computational budgets as a limitation and suggests "future improvements may involve confidence-based step adjustments."
- **Why unresolved:** Current diffusion models operate on fixed denoising steps, often resulting in underdeveloped details in complex scenes or overprocessing in simple ones.
- **What evidence would resolve it:** A modified model architecture that adaptively extends generation steps for complex prompts, resulting in statistically higher authenticity scores compared to fixed-step baselines.

## Limitations
- Only 2 of 73 prompts were provided verbatim, creating significant uncertainty about how prompt phrasing affects style transfer quality across models
- Dataset shows strong imbalance toward XIX-XX century styles (62.5% combined), potentially overstating modern-era model capabilities while underrepresenting older period challenges
- Inference settings (sampling steps, guidance scale, seed behavior) were not specified for any model, yet these parameters critically influence both authenticity and adherence outcomes

## Confidence
- **High confidence:** The hyperrealism trade-off mechanism is well-supported by multiple evidence anchors showing consistent patterns across models and survey results.
- **Medium confidence:** The period-dependent fidelity gradient shows clear statistical trends but lacks comprehensive temporal coverage in the dataset.
- **Low confidence:** Subject-complexity amplification effects, while statistically significant in survey data, rely on untested assumptions about human visual processing evolution.

## Next Checks
1. **Prompt sensitivity test:** Generate identical subject matter (portrait, landscape) across all 12 periods using standardized prompt templates to isolate temporal effects from prompt variability.
2. **Anatomical module comparison:** Compare human figure generation quality between models with and without specialized anatomical conditioning layers to quantify the amplification effect.
3. **Training corpus analysis:** Analyze model training datasets for temporal and style distribution to validate the hypothesis that disproportionate photographic content drives hyperrealism bias.