---
ver: rpa2
title: 'Personality Vector: Modulating Personality of Large Language Models by Model
  Merging'
arxiv_id: '2509.19727'
source_url: https://arxiv.org/abs/2509.19727
tags:
- personality
- merging
- vectors
- trait
- traits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose personality vector merging, a method that modulates
  personality traits in large language models (LLMs) without additional training.
  They construct personality vectors by subtracting the weights of a pre-trained model
  from those of a personality-fine-tuned model and merge them into other models to
  induce desired personality traits.
---

# Personality Vector: Modulating Personality of Large Language Models by Model Merging

## Quick Facts
- **arXiv ID**: 2509.19727
- **Source URL**: https://arxiv.org/abs/2509.19727
- **Reference count**: 28
- **Primary result**: Personality vectors enable continuous, multi-trait personality modulation in LLMs through weight-space arithmetic, achieving Pearson correlations >0.9 for single traits and >0.6 for multi-trait composition with DaRE.

## Executive Summary
Personality Vector proposes a method to modulate personality traits in large language models through model merging without additional training. The approach constructs personality vectors by subtracting weights of pre-trained models from personality-fine-tuned models, then merges these vectors into target models to induce desired traits. Experiments demonstrate strong alignment between intended personality modulation and observed linguistic and behavioral outcomes, with Pearson correlations exceeding 0.9 for single-trait control and 0.6 for multi-trait composition after applying DaRE. The method supports continuous control over trait intensity through scaling coefficients and transfers across diverse domains including role-playing agents, multilingual models, and vision-language models.

## Method Summary
The method extracts personality vectors by subtracting base model weights from personality-fine-tuned weights, then merges these vectors into target models using task arithmetic or TIES-Merging with optional DaRE sparsification. Fine-tuning is performed on the Big5-Chat dataset (10K samples per personality condition) using SFT with specific hyperparameters (lr=5e-6 for Llama/1e-5 for Qwen, cosine scheduler, 3 epochs). The personality vector ϕ_p = θ_p - θ_pretrained is merged via θ' = θ_base + αϕ_p, where α controls trait intensity. For multi-trait composition, DaRE (drop_rate=0.5) and TIES-Merging (trim_rate=0.7) are applied to mitigate parameter interference.

## Key Results
- Single-trait personality modulation achieved Pearson correlations >0.9 between scaling coefficient and BFI scores
- Multi-trait composition improved from ρ≈0.58 to ρ≈0.65 using Task arithmetic + DaRE sparsification
- Personality vectors successfully transferred to role-playing agents (Extraversion score increased from 2.4 to 3.9) and vision-language models with trait-specific visual interpretation differences

## Why This Works (Mechanism)

### Mechanism 1
Personality traits can be linearly transferred between models through weight-space arithmetic operations. Fine-tuning on personality-specific dialogue data creates a "personality vector" that encodes trait-specific parameter updates. Merging this vector into target models via θ_new = θ_base + αϕ_p modulates personality expression proportional to α. The core assumption is that personality-related knowledge resides in a learnable subspace orthogonal to general language capabilities during fine-tuning. Evidence shows layer-wise analysis reveals trait-specific features encoded in deeper layers, while GSM8K reasoning shows preserved general capabilities.

### Mechanism 2
Scaling coefficient α provides continuous, extrapolative control over trait intensity beyond the original fine-tuned state. The personality vector represents a direction in weight space where linear scaling (α ∈ [0.1, 2.0]) moves along this direction. α > 1.0 exaggerates traits beyond the fine-tuned model while α < 1.0 produces intermediate expression. Core assumption is that personality traits occupy a roughly linear subspace where interpolation and extrapolation correspond meaningfully to behavioral intensity. Evidence shows strong positive correlation (>0.9) between α and BFI scores, with linguistic features becoming more pronounced at higher scaling values.

### Mechanism 3
Multi-trait composition suffers from parameter interference mitigated by sparsification techniques like DaRE. Personality vectors share overlapping parameter space due to common dialogue objectives, causing destructive interference when merged directly. DaRE randomly drops and rescales parameters, preserving salient trait-specific features while reducing overlap. Core assumption is that trait-specific parameters can be approximately separated from shared conversational parameters through random sparsification. Evidence shows Task arithmetic alone achieves ρ≈0.58, but Task arithmetic + DaRE improves to ρ≈0.65, with high cosine similarity across personality vectors indicating parameter overlap.

## Foundational Learning

- **Task Vectors (Ilharco et al., 2022)**: Personality vectors apply task vector methodology to behavioral attributes rather than task performance. Quick check: Can you explain why subtracting base weights from fine-tuned weights isolates task-specific knowledge rather than just removing shared parameters?

- **Big Five Personality Framework (OCEAN)**: The entire method assumes personality can be decomposed into five quasi-independent dimensions with high/low polarity. Quick check: Why might composing all five traits simultaneously be harder than modulating a single trait?

- **Parameter Interference in Model Merging**: Understanding why multiple vectors conflict explains why DaRE/TIES-Merging are necessary for multi-trait composition. Quick check: If two task vectors have cosine similarity of 0.5, what happens when you add them directly versus applying a sparsification method?

## Architecture Onboarding

- **Component map**: SFT fine-tuning -> Personality vector extraction -> Merging module (Task arithmetic/TIES-Merging + DaRE) -> Evaluation (BFI questionnaire + LIWC linguistic analysis)

- **Critical path**: Fine-tuning quality directly determines vector quality. If the fine-tuned model doesn't exhibit strong trait expression (BFI score separation), the extracted vector will be weak.

- **Design tradeoffs**:
  - Single-trait: High control (ρ > 0.9), simple, but limited expressiveness
  - Multi-trait without sparsification: Fast but poor control (ρ ≈ 0.58)
  - Multi-trait with DaRE: Better control (ρ ≈ 0.65) but requires hyperparameter tuning
  - Negative scaling: Reverses traits but risks degrading conversational coherence

- **Failure signatures**:
  - Low BFI correlation despite merging → Vector extraction error or insufficient fine-tuning
  - Aggressive/unethical outputs → Excessive scaling (α > 1.5) on Low Agreeableness or High Neuroticism
  - Instruction-following degradation → Total merged scale > 2.0 in multi-trait setting
  - Cross-lingual failure → Target model shares insufficient backbone with source

- **First 3 experiments**:
  1. Fine-tune Llama-3.1-8B-Instruct on High Extraversion, extract vector, merge at α ∈ {0.5, 1.0, 1.5, 2.0}, verify BFI correlation > 0.85
  2. Merge all five high-trait vectors with α = 0.4 each, compare Task arithmetic vs. Task arithmetic + DaRE on correlation
  3. Apply personality vector to a code assistant model, verify trait expression without destroying domain performance

## Open Questions the Paper Calls Out

### Open Question 1
Can orthogonal personality vectors be constructed to eliminate parameter interference when merging multiple traits simultaneously? The paper suggests future research could explore constructing orthogonal personality vectors or designing merging algorithms specifically optimized for reducing interference. High cosine similarity (>0.3) between personality vectors indicates shared parameter space, and even DaRE only partially mitigates interference, achieving ρ≈0.62 for multi-trait composition versus ρ>0.9 for single-trait scaling.

### Open Question 2
How can merging hyperparameters (scaling coefficients, DaRE drop rates, TIES trim rates) be automatically optimized per trait for precise personality control? Future work may benefit from automated methods for tuning merging parameters to further enhance the precision and flexibility of personality control. The study fixed scaling coefficients across traits and manually tuned DaRE/TIES parameters, with independent per-trait coefficient tuning not explored.

### Open Question 3
Do personality vectors encode trait representations directly or serve as steering signals that amplify latent personality features already present in base models? Layer-wise analysis shows cosine similarity declines at deeper layers and remains high between prompted and fine-tuned models, suggesting pretrained LLMs already possess latent representations of the Big Five personality dimensions. The analysis is correlational and does not disentangle whether vectors inject new information or reweight existing representations.

## Limitations
- Personality vector transferability critically depends on architectural similarity between source and target models
- Cross-modal transfer to vision-language models lacks comprehensive quantitative validation of personality expression strength
- Long-term stability of personality modulation under continued training or fine-tuning remains unexplored

## Confidence
- **High Confidence**: Single-trait personality modulation shows strong empirical validation with Pearson correlations consistently above 0.9, robust linguistic feature alignment via LIWC-22, and preserved general capabilities
- **Medium Confidence**: Multi-trait composition with DaRE achieves moderate correlation (ρ ≈ 0.65) but shows parameter interference remains problematic, with sparsification helping but not fully solving the issue
- **Low Confidence**: Cross-modal transfer to vision-language models demonstrates qualitative differences in visual interpretation but lacks comprehensive quantitative validation of personality expression strength and behavioral correlation metrics

## Next Checks
1. **Cross-Architectural Transfer Test**: Apply personality vectors extracted from Llama-3.1-8B to significantly different architectures (e.g., Mistral-7B or Mixtral-8x7B) and measure BFI correlation decay to quantify architectural dependency limits

2. **Long-term Stability Assessment**: Fine-tune a personality-modulated model on a different task (e.g., medical QA) and measure whether personality traits persist or degrade, establishing whether vectors remain stable under continued training

3. **Multi-modal Behavioral Validation**: For vision-language models, develop a comprehensive evaluation protocol that measures personality expression through both visual interpretation tasks and linguistic outputs, establishing quantitative correlation metrics comparable to the text-only benchmarks