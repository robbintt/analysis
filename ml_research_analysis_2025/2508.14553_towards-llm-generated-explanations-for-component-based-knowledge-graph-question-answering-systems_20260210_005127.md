---
ver: rpa2
title: Towards LLM-generated explanations for Component-based Knowledge Graph Question
  Answering Systems
arxiv_id: '2508.14553'
source_url: https://arxiv.org/abs/2508.14553
tags:
- data
- explanations
- component
- explanation
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach for generating explanations for
  component-based Question Answering (QA) systems, focusing on the verbalization of
  SPARQL queries and RDF triples as input/output data flows. The authors implemented
  both template-based and LLM-based (GPT-3.5 and GPT-4) methods for explanation generation.
---

# Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems

## Quick Facts
- arXiv ID: 2508.14553
- Source URL: https://arxiv.org/abs/2508.14553
- Reference count: 1
- Primary result: LLM-generated explanations achieved average ratings of 3.7-3.8 compared to 3.5-3.7 for templates in expert evaluation

## Executive Summary
This paper introduces an approach for generating explanations for component-based Question Answering systems using Large Language Models (LLMs). The method focuses on verbalizing SPARQL queries and RDF triples as input/output data flows within the QA pipeline. The authors compare template-based and LLM-based (GPT-3.5 and GPT-4) approaches for explanation generation, finding that LLM-generated explanations outperform template-based methods in expert evaluations.

## Method Summary
The approach implements both template-based and LLM-based methods for generating explanations of QA component behavior. The system verbalizes SPARQL queries and RDF triples to create human-readable explanations of data flows between components. GPT-3.5 and GPT-4 models are evaluated against template-based approaches, with explanations assessed through expert evaluation and quantitative correlation analysis of output data explanations.

## Key Results
- LLM-generated explanations achieved higher expert ratings (3.7-3.8) compared to template-based approaches (3.5-3.7)
- Correlation coefficients for output data explanations ranged from 0.015 to 0.595 across different data type combinations
- LLMs demonstrated superior performance in generating human-readable explanations of QA component behavior

## Why This Works (Mechanism)
The mechanism leverages LLMs' natural language generation capabilities to transform technical query and data representations into accessible explanations. By using pre-trained language models, the approach can generate contextually appropriate verbalizations that capture both the technical content and user-friendly presentation of complex SPARQL queries and RDF triples.

## Foundational Learning
- SPARQL query language: Why needed - to understand the target format being verbalized; Quick check - can you identify SELECT vs CONSTRUCT queries
- RDF triples: Why needed - fundamental data structure in knowledge graphs; Quick check - can you parse subject-predicate-object triples
- Component-based QA architecture: Why needed - to understand system flow and data dependencies; Quick check - can you trace data flow between components
- LLM prompt engineering: Why needed - critical for generating quality explanations; Quick check - can you formulate effective few-shot prompts

## Architecture Onboarding
- Component map: Question parser -> Query generator -> KG access -> Answer generator -> Explanation generator
- Critical path: Natural language question → SPARQL query → RDF triples → Explanation
- Design tradeoffs: Template precision vs. LLM flexibility in explanation generation
- Failure signatures: Misinterpretations of complex SPARQL patterns, loss of technical accuracy in verbalization
- First experiments: 1) Basic SPARQL to text conversion, 2) Template-based explanation generation, 3) LLM explanation quality comparison

## Open Questions the Paper Calls Out
None

## Limitations
- Expert evaluation sample size (n=7) may limit statistical robustness
- Performance variability across different data type combinations (correlation 0.015-0.595)
- Evaluation focused on technical experts rather than end-users

## Confidence
- LLM superiority over templates: Medium confidence
- General applicability across data types: Low confidence
- Expert evaluation representativeness: Medium confidence

## Next Checks
1. Conduct a larger-scale user study with diverse participant backgrounds (technical and non-technical) to assess real-world usability and comprehension of the generated explanations
2. Implement cross-dataset validation to test the approach's performance on different knowledge graphs and query types beyond the current scope
3. Perform ablation studies to isolate the contribution of different explanation components (SPARQL verbalization, RDF triple formatting, component labels) to overall explanation quality and user understanding