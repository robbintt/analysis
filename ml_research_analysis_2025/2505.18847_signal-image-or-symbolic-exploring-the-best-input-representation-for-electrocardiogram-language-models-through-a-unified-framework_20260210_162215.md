---
ver: rpa2
title: 'Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language
  Models Through a Unified Framework'
arxiv_id: '2505.18847'
source_url: https://arxiv.org/abs/2505.18847
tags:
- signal
- end-to-end
- https
- training
- xsig
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first comprehensive benchmark comparing\
  \ three ECG input representations\u2014raw time-series signals, rendered images,\
  \ and discretized symbolic sequences\u2014for Electrocardiogram-Language Models\
  \ (ELMs). The authors systematically evaluate each modality across six public datasets\
  \ and five text generation metrics under controlled training settings."
---

# Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework

## Quick Facts
- arXiv ID: 2505.18847
- Source URL: https://arxiv.org/abs/2505.18847
- Reference count: 30
- Primary result: Symbolic ECG tokenization (ECG-Byte) achieves highest performance across six public datasets, with robustness to signal perturbations and superior scaling with ECG duration.

## Executive Summary
This paper presents the first comprehensive benchmark comparing three ECG input representations—raw time-series signals, rendered images, and discretized symbolic sequences—for Electrocardiogram-Language Models (ELMs). The authors systematically evaluate each modality across six public datasets and five text generation metrics under controlled training settings. Symbolic representations achieve the greatest number of statistically significant performance wins over both signal and image inputs, with ECG-Byte's End-to-End training approach consistently outperforming alternatives. The study further demonstrates that symbolic input maintains robustness to signal perturbations and shows superior scaling with ECG duration. These findings provide clear empirical guidance for selecting input representations when developing next-generation ELMs, with symbolic tokenization emerging as the optimal modality for autoregressive ECG analysis.

## Method Summary
The study benchmarks three ECG input representations (Signal, Image, Symbolic) across six public datasets using a unified framework with Llama 3.2 1B LLM. All models use LoRA fine-tuning and are evaluated on text generation tasks with metrics including BLEU-4, ROUGE-L, METEOR, BertScore, and Accuracy. The symbolic approach (ECG-Byte) uses byte-pair encoding on discretized ECG signals, while image representations use rendered plots processed by CLIP/ViT encoders, and signal representations use pretrained ECG encoders. The study systematically ablates token budgets (512 vs 1024) and ECG segment lengths to understand scaling properties and robustness to signal perturbations.

## Key Results
- ECG-Byte (symbolic, End-to-End) achieves highest BLEU-4 scores across six datasets, with statistically significant wins over both signal and image inputs
- Symbolic representations show superior scaling with ECG duration, improving from 22.87 to 25.09 BLEU-4 as segment length increases from 1250 to 2500 samples
- Symbolic input demonstrates better robustness to signal perturbations, showing +0.09 BLEU-4 improvement versus -0.10 degradation for image-based CLIP
- At token budget T=512, symbolic methods suffer from truncation and perform worse than encoder-based approaches, but excel at T≥1024

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Signal Compression via BPE Preserves Temporal Structure
Symbolic ECG representations outperform signal and image modalities because byte-pair encoding (BPE) preserves temporal sequence information while reducing dimensionality to a manageable token count. ECG-Byte discretizes continuous signal amplitudes into symbols (a-z), then applies BPE to create compressed token sequences. Unlike encoder-based methods that collapse entire ECGs into single latent vectors (via `<signal>` placeholder), symbolic tokens maintain sequence length proportional to signal complexity, enabling the LLM's attention mechanism to operate on temporal patterns directly.

### Mechanism 2: Avoiding Encoder Information Bottleneck
End-to-End training with symbolic input avoids the compression bottleneck inherent in 2-Stage Scratch and End-to-End LLaVA approaches. Encoder-based methods project Xsig or Ximg to a single d-dimensional latent vector z, which represents the entire ECG. This single-token representation must encode all morphological features. Symbolic representation instead produces m tokens (where m scales with signal complexity), distributing information across the sequence.

### Mechanism 3: Robustness Through Quantization
Symbolic representations exhibit greater robustness to signal perturbations because discretization acts as implicit denoising. ECG-Byte's quantization maps continuous values to a fixed symbol vocabulary (26 symbols), rounding small amplitude variations to the same token. When Gaussian noise or baseline wander is added, many perturbations fall within quantization bins and do not change token identity.

## Foundational Learning

- **Concept: Autoregressive Language Modeling (Next-Token Prediction)**
  - Why needed here: All ELM variants use the same loss function L(θ) = −Σ log pθ(yt|y<t), training the model to predict assistant responses conditioned on ECG + query. Understanding this objective is essential for interpreting why tokenization strategy matters.
  - Quick check question: Given input sequence [ECG tokens, "What is the rhythm?", <response>], which tokens contribute to the loss?

- **Concept: Byte-Pair Encoding (BPE) for Compression**
  - Why needed here: ECG-Byte's performance depends on BPE's ability to compress repetitive ECG patterns into fewer tokens while preserving discriminative information. The paper uses 5000 merges on 300k ECGs.
  - Quick check question: If an ECG signal produces raw symbol sequence of length 30,000, what determines the final token sequence length after BPE?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: All experiments use LoRA (rank=16, α=32) rather than full fine-tuning. This constrains model capacity and may influence representation comparisons.
  - Quick check question: With LoRA applied to attention layers only, what fraction of total parameters are trainable?

## Architecture Onboarding

- **Component map:**
  Xsig (raw signal) → [Transformation] → XID (tokens) / Ximg (image) / Xsig* (stacked) → [Encoder or Tokenizer] → [Projection/Concatenation] → [LLM Backbone: Llama 3.2 1B / Gemma 2B / Qwen 2.5] → [Generated text response S]

- **Critical path:**
  1. Preprocess ECGs consistently: 250Hz, 0.5-100Hz bandpass, db6 wavelet denoising
  2. Train ECG-Byte tokenizer on 300k unsegmented ECGs with 5000 BPE merges
  3. Fine-tune LLM with LoRA on Mapping datasets (PULSE ECG-Instruct, ECG-Chat)
  4. Evaluate on held-out test sets using BLEU-4, ROUGE-L, METEOR, BertScore, Accuracy

- **Design tradeoffs:**
  | Constraint | Recommended Approach | Rationale |
  |------------|---------------------|-----------|
  | T≥1024 tokens | End-to-End ECG-Byte | Best performance, scales with ECG length |
  | T=512 tokens | End-to-End LLaVA | Avoids ECG token truncation |
  | No encoder training budget | End-to-End ECG-Byte | No separate encoder pretraining needed |
  | Image-only data available | End-to-End LLaVA with CLIP | Can process Ximg directly |

- **Failure signatures:**
  - ECG-Byte at T=512: BLEU-4 crashes to 14.05 due to aggressive ECG token truncation
  - LLaVA-style with perturbed images: CLIP(Ximg) shows -0.10 BLEU-4 degradation
  - Across all modalities on perturbations: "average degradation is minor" suggests models underutilize ECG signal

- **First 3 experiments:**
  1. Reproduce the T=1024 vs. T=512 ablation on a single dataset to validate tokenization pipeline
  2. Test perturbation sensitivity with controlled noise levels to verify quantization-as-denoising hypothesis
  3. Profile inference latency for each modality at fixed T=1024 to quantify End-to-End ECG-Byte speedup

## Open Questions the Paper Calls Out

- **Open Question 1:** How can future architectures and training strategies be modified to compel ELMs to leverage ECG-level information more effectively rather than relying primarily on textual priors? The current study identifies this limitation but does not propose architectural changes to address it.

- **Open Question 2:** Can symbolic ECG tokenization methods be adapted to maintain robust performance under strict sequence length constraints (e.g., T=512) where they currently suffer from aggressive truncation? The study benchmarks existing methods but doesn't investigate compression techniques for low-resource settings.

- **Open Question 3:** To what extent is the superior performance of symbolic representations attributable to the specific tokenization normalization strategy (instance normalization) versus the discrete representation itself? The study uses instance normalization as a deviation from original ECG-Byte but doesn't isolate this factor's contribution.

- **Open Question 4:** How does the choice of specific pre-trained encoders interact with input representations to influence performance, independent of the training paradigm? The study couples representations with specific encoders, confounding representation utility with encoder utility.

## Limitations
- Symbolic ECG-Byte performance is contingent on having sufficient token budget (T≥1024), with significant degradation at T=512
- Models show only modest degradation under signal perturbation, suggesting limited utilization of ECG signal information regardless of representation
- Study uses fixed model sizes (1B parameters) and doesn't explore how representation advantages scale with model capacity
- The superiority of symbolic representations may be influenced by specific preprocessing choices (instance normalization) not systematically isolated

## Confidence

**High Confidence:**
- ECG-Byte's End-to-End training outperforms encoder-based approaches when token budget is sufficient (T≥1024)
- Symbolic representations show superior scaling with ECG duration
- Symbolic input maintains better robustness to signal perturbations

**Medium Confidence:**
- The information bottleneck hypothesis for encoder-based method limitations
- Quantization as denoising mechanism for symbolic robustness

**Low Confidence:**
- The claim that autoregressive language modeling objectives transfer equivalently to symbolic ECG tokens as they do to natural language tokens

## Next Checks
1. Systematically evaluate ELM performance across a broader range of token budgets (256, 512, 768, 1024, 1536) to precisely characterize the threshold where symbolic advantages emerge and quantify performance trade-offs.

2. Design controlled experiments varying perturbation amplitude relative to quantization bin width to empirically validate whether symbolic robustness stems from the quantization mechanism.

3. Implement attention visualization and feature importance analysis across all three input representations to determine whether ELMs actually utilize different information from each modality or if observed performance differences reflect architectural artifacts.