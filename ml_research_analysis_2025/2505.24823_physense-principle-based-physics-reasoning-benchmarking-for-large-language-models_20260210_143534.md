---
ver: rpa2
title: 'PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language
  Models'
arxiv_id: '2505.24823'
source_url: https://arxiv.org/abs/2505.24823
tags:
- reasoning
- llms
- problem
- physics
- hint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhySense, a benchmark designed to evaluate
  how well large language models (LLMs) can apply fundamental physical principles
  to solve physics problems. Unlike previous benchmarks, PhySense focuses on problems
  where core principles like symmetry or dimensional analysis provide efficient, expert-like
  solutions, yet remain challenging for LLMs without principle-first reasoning.
---

# PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models

## Quick Facts
- arXiv ID: 2505.24823
- Source URL: https://arxiv.org/abs/2505.24823
- Reference count: 40
- Primary result: Current LLMs show low token efficiency and fail to apply core physics principles efficiently, even when explicitly hinted.

## Executive Summary
PhySense introduces a benchmark to evaluate how well large language models can apply fundamental physical principles to solve physics problems. Unlike previous benchmarks, PhySense focuses on problems where core principles like symmetry or dimensional analysis provide efficient, expert-like solutions, yet remain challenging for LLMs without principle-first reasoning. The dataset contains 380 problems across various physics domains and three prompt strategies: zero-shot, hint, and no-computation. Evaluation across seven state-of-the-art LLMs reveals that while reasoning models outperform non-reasoning ones, all models fall short of expert-level performance and show low token efficiency compared to humans. Hints and no-computation prompts provide only marginal improvement, highlighting a fundamental gap in LLMs' grasp of principle-based reasoning. The findings emphasize the need for models to integrate deeper conceptual understanding rather than relying on brute-force computation.

## Method Summary
The PhySense benchmark evaluates seven state-of-the-art LLMs (GPT-o4-mini-high, Claude 3.7 Thinking, Gemini 2.5 Pro, DeepSeek R1, GPT-4.1, Claude 3.7 Sonnet, DeepSeek V3) on 380 curated physics problems across 19 problem models. Problems are designed to be solvable via principle-based reasoning (symmetry, dimensional analysis, conservation laws, topology) rather than brute-force computation. Three prompt strategies are tested: zero-shot, hint (principle suggestion), and no-computation (suppress calculation). Evaluation measures accuracy (5% tolerance for numerical, exact match for multiple choice) and token usage. Expert solutions demonstrate principle-first approaches that achieve human-level efficiency with ~100 tokens versus ~10,000 tokens for reasoning models.

## Key Results
- Reasoning models outperform non-reasoning models but all fall short of expert-level performance
- All models show low token efficiency, using ~10,000 tokens versus ~100 for human experts
- Hints and no-computation prompts provide only marginal improvement in accuracy
- Models frequently default to brute-force computation when symmetry-based solutions exist

## Why This Works (Mechanism)

### Mechanism 1: Principle-First Reasoning Shortcuts
Problems designed for principle-based solutions expose a reasoning efficiency gap where LLMs default to brute-force computation rather than conceptual shortcuts. The benchmark constrains problems to those solvable via symmetry, dimensional analysis, conservation laws, or topology. These principles reduce search space and computational complexity. LLMs fail to recognize or apply these shortcuts, instead attempting lengthy derivations. Expert physicists encode knowledge as reusable principles that override procedural computation; LLMs lack this hierarchical organization.

### Mechanism 2: Hint Prompting Fails Due to Misapplication, Not Recognition
Providing explicit hints about applicable principles yields marginal accuracy gains because LLMs misapply principles rather than failing to recognize them. "Hint" and "No-Computation" prompts signal which principle category to use, but the model's internal representation of that principle is incomplete or incorrectly parameterized. The model knows "use symmetry" but fails to identify all relevant symmetries (e.g., misses time-reversal in Example 3). The bottleneck is principled reasoning competence, not retrieval or recognition.

### Mechanism 3: Token Efficiency as a Proxy for Conceptual Compression
Token usage measures how compressed a model's reasoning is; high token counts indicate lack of conceptual abstraction. Reasoning models generate ~10,000+ tokens per problem (vs. ~1,000 for non-reasoning, ~100 for humans). This suggests models cannot compress physical intuition into abstract rules. They compensate with lengthy chain-of-thought, often diverging into unnecessary calculations. Human experts' concise solutions reflect compressed representations; token count inversely correlates with principle integration depth.

## Foundational Learning

- **Symmetry arguments in physics** (reflection, rotation, time-reversal, charge conjugation)
  - Why needed here: 10 of 19 problem models rely on symmetry; Example 1 and Example 2 require identifying reflection symmetry to avoid solving Kirchhoff equations
  - Quick check question: Given a uniformly charged square plate, which points have Ez = 0 by symmetry alone?

- **Dimensional analysis and Π-theorem**
  - Why needed here: Problems like "DimLS" and "WrdH" test whether models can deduce functional forms (e.g., sin(x) arguments must be dimensionless) without computation
  - Quick check question: A pendulum's period T depends on length L and gravity g. Use dimensional analysis to derive T ∝ √(L/g)

- **Topological ground state degeneracy**
  - Why needed here: "GSDeq" and "GSDGen" problems require understanding that boundary conditions (periodic vs. antiperiodic) determine degeneracy, not bulk properties
  - Quick check question: Why does a torus geometry support different ground state counts than a sphere for a topological phase?

## Architecture Onboarding

- **Component map**: PhySense dataset (380 problems) -> Prompting module (zero-shot, hint, no-computation) -> Model interface (7 LLMs via API) -> Evaluation harness (accuracy + token usage logging)

- **Critical path**: 1) Load problem from dataset → construct prompt with selected strategy 2) Call model API → capture completion tokens and final answer 3) Extract boxed answer → compare to ground truth 4) Log accuracy and token count per problem, model, prompt type

- **Design tradeoffs**:
  - Problem novelty vs. coverage: All problems are new to prevent memorization, but limits domain breadth (no astrophysics, optics)
  - Text-only vs. multimodal: Current version excludes diagrams; future work should add figures
  - Token count vs. reasoning quality: Token efficiency is a proxy, not a direct measure of principle understanding

- **Failure signatures**:
  - Over-thinking: Model generates long computation chains when symmetry suffices (see Example 1, Gemini-2.5 Pro integrates 2D field instead of using reflection)
  - Hallucinated symmetries: Model assumes translational invariance where none exists (Example 3, GPT-4.1 incorrectly claims translation symmetry)
  - Principle invocation without comprehension: Model names correct principle but misapplies it (hinted models still fail 2DEFL honeycomb lattice problem)

- **First 3 experiments**:
  1. Baseline replication: Run all 7 models on full 380-problem set with zero-shot prompting; confirm token and accuracy distributions match paper Tables 1-2
  2. Ablation on hint specificity: Replace generic hints ("Consider symmetry") with precise hints ("Use reflection symmetry about x=y diagonal"); measure delta in accuracy and token reduction
  3. Fine-tuning intervention: Supervised fine-tune a smaller model (e.g., Qwen-7B) on PhySense training split with expert solutions emphasizing principle-first reasoning; evaluate whether token efficiency improves without accuracy drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning or reinforcement learning effectively train LLMs to prioritize principle-first reasoning over brute-force calculation?
- Basis in paper: The Conclusion states: "For future directions, it will be important to try improving LLM's principle-based reasoning via supervised fine tuning or reinforcement learning."
- Why unresolved: The study only evaluates existing SOTA models without modifying their training pipelines to encourage conceptual efficiency
- What evidence would resolve it: Demonstrated improvements in token efficiency and accuracy on PhySense after training models on datasets of expert principle-based reasoning traces

### Open Question 2
- Question: How does the inclusion of multi-modal inputs (e.g., diagrams, graphs) impact the capability of LLMs to apply physical principles?
- Basis in paper: The Limitations section notes the "current iteration is exclusively text-based, omitting multi-modal reasoning," and suggests future work should "incorporate multi-modal problems"
- Why unresolved: The benchmark currently restricts evaluation to textual problem descriptions, leaving visual reasoning untested
- What evidence would resolve it: Evaluation results from an extended version of PhySense containing visual elements applied to vision-language models

### Open Question 3
- Question: What evaluation metrics beyond accuracy and token usage can effectively capture the semantic quality and "interpretable" nature of physics reasoning?
- Basis in paper: The Limitations section calls for the development of "more nuanced evaluation metrics for reasoning quality"
- Why unresolved: Current metrics (accuracy and token count) do not distinguish between a correct answer derived via messy calculation versus elegant principle application
- What evidence would resolve it: A new evaluation metric that correlates strongly with human expert assessments of reasoning validity and conceptual soundness

## Limitations

- Dataset accessibility: The PhySense benchmark dataset is not publicly released with the paper, preventing immediate replication
- API-based evaluation: Reliance on commercial model APIs with undocumented default hyperparameters makes exact reproduction difficult
- Narrow domain focus: Benchmark excludes astrophysics, optics, and other physics domains, limiting generalizability
- Token efficiency assumptions: Human-expert comparison assumes equivalent problem difficulty and solution completeness without rigorous validation

## Confidence

- **High Confidence**: Reasoning models outperform non-reasoning models, all models fall short of expert-level performance, and token efficiency remains low compared to humans
- **Medium Confidence**: Hints and no-computation prompts provide only marginal improvement, and the core bottleneck is principled reasoning competence rather than recognition or retrieval
- **Low Confidence**: The assertion that LLMs "know when to invoke physical principles" but "misapply" them rather than failing to recognize them

## Next Checks

1. **Dataset Access and Baseline Replication**: Obtain the PhySense dataset and reproduce the baseline results across all seven models with zero-shot prompting to verify the reported accuracy and token usage distributions

2. **Hint Specificity Ablation**: Systematically test whether replacing generic principle hints with precise, problem-specific hints improves accuracy and token efficiency, isolating whether the issue is principle recognition or misapplication

3. **Fine-Tuning Intervention Study**: Fine-tune a smaller model on the PhySense training split using expert solutions that emphasize principle-first reasoning, then evaluate whether token efficiency approaches human levels without sacrificing accuracy, testing the compression hypothesis directly