---
ver: rpa2
title: 'Compositional Translation: A Novel LLM-based Approach for Low-resource Machine
  Translation'
arxiv_id: '2503.04554'
source_url: https://arxiv.org/abs/2503.04554
tags:
- translation
- comptra
- bm25
- sentence
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach called compositional translation
  for improving low-resource machine translation with large language models. The method
  decomposes a sentence into simpler phrases, translates each phrase using retrieved
  in-context examples, and then recombines the translated phrases to generate the
  final translation.
---

# Compositional Translation: A Novel LLM-based Approach for Low-resource Machine Translation

## Quick Facts
- **arXiv ID**: 2503.04554
- **Source URL**: https://arxiv.org/abs/2503.04554
- **Reference count**: 40
- **Primary result**: Compositional translation consistently outperforms strong baselines for low-resource language pairs by decomposing sentences into phrases, translating with retrieved examples, and recombining.

## Executive Summary
This paper introduces compositional translation, a novel approach for low-resource machine translation that leverages the compositionality property of translation and the effectiveness of LLMs at handling short phrases. The method decomposes a sentence into simpler phrases, translates each phrase using retrieved in-context examples, and then recombines the translated phrases to generate the final translation. Experiments on multiple low-resource language pairs from various benchmarks show that compositional translation consistently outperforms strong baselines including similarity-based few-shot translation and existing strategies like chain-of-thought prompting, translating step-by-step, and translate-estimate-and-refine.

## Method Summary
The Compositional Translation (CompTra) pipeline operates in three stages: (1) Decomposition: An LLM splits the source sentence into minimal propositions using a MinWikiSplit-style prompt; (2) Translation: For each phrase, the system retrieves top-5 examples via BM25 from a selection pool and translates few-shot, filtering results with FastText language identification; (3) Recombination: The LLM translates the original sentence using the self-generated phrase-translation pairs as in-context examples. The method is particularly effective for low-resource languages and works well even with smaller language models, though it requires multiple sequential LLM calls making it slower than standard few-shot approaches.

## Key Results
- CompTra consistently outperforms standard few-shot translation and strong baselines (CoT, T-E-R, TST) across multiple low-resource language pairs
- BM25 retrieval outperforms dense retrieval (SONAR) for short phrase queries in low-resource settings
- The method is robust to domain mismatch between the selection pool and test data
- CompTra shows limited effectiveness for high-resource language pairs where standard few-shot methods already perform well

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing complex sentences into shorter, independent phrases reduces cognitive load on LLMs, allowing more accurate translation.
- **Mechanism**: The "Divide" prompt instructs the LLM to break a source sentence into "minimal propositions," hypothesizing that LLMs have better latent capabilities for translating short, simple strings in low-resource languages than complex ones.
- **Core assumption**: LLMs struggle with low-resource translation primarily due to complex sentence structure rather than just vocabulary gaps.
- **Evidence anchors**: Abstract states "shorter phrases should be intrinsically easier to translate"; Table 15 shows phrases consistently achieve lower (better) MetricX-QE scores than full sentences.
- **Break condition**: If decomposition creates fragments that lack context (e.g., just keywords), performance degrades significantly.

### Mechanism 2
- **Claim**: Retrieving in-context examples for short phrases yields higher-quality demonstrations than retrieving examples for full sentences.
- **Mechanism**: A short phrase has a sparser, more specific semantic footprint, allowing the retriever (BM25 or SONAR) to find "tighter" matches in the selection pool than a long, complex sentence query would.
- **Core assumption**: The selection pool is dense enough to contain matches for the sub-phrases.
- **Evidence anchors**: Abstract notes it's "easier to match with relevant examples... especially beneficial in low-resource scenarios"; corpus neighbors suggest RAG and data generation are key trends for low-resource MT.
- **Break condition**: If the selection pool is extremely sparse or domain-mismatched, the retrieval step may return irrelevant noise.

### Mechanism 3
- **Claim**: Using self-generated phrase-translation pairs as in-context examples for the final translation leverages the "compositionality" of translation.
- **Mechanism**: The final "Merge" step prompts the LLM to translate the full sentence, but populates the context window with the specific phrase pairs generated in previous steps, acting as a local, highly relevant translation memory.
- **Core assumption**: The LLM can successfully synthesize distinct phrase translations into a grammatically correct whole.
- **Evidence anchors**: Paper states "leverages the compositionality property of translation"; "LLM is prompted to translate the initial sentence with the help of the self-generated phrase-translation pairs."
- **Break condition**: If the LLM hallucinates or produces incorrect language outputs for the phrases, the final step propagates these errors.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here**: The entire CompTra pipeline relies on the model performing tasks (decomposing, translating) via prompts without weight updates.
  - **Quick check question**: Can you explain how "k-shot" prompting differs from fine-tuning?

- **Concept: Compositionality in Machine Translation**
  - **Why needed here**: The method assumes that a sentence's translation can be constructed from the translation of its parts (phrases), and that these parts can be processed independently.
  - **Quick check question**: How does the meaning of "The cat sat on the mat" relate to the meanings of "The cat" and "sat on the mat"?

- **Concept: Sparse vs. Dense Retrieval (BM25 vs. SONAR)**
  - **Why needed here**: The system uses a retriever to find examples for phrases. Understanding why BM25 (lexical) might work better than SONAR (dense) for short queries is key to reproducing results.
  - **Quick check question**: Why might a keyword-based search (BM25) outperform a semantic vector search for very short, specific phrases?

## Architecture Onboarding

- **Component map**: Retriever -> LLM (Worker) -> Filters -> LLM (Worker) -> LLM (Worker)
- **Critical path**: Input: Source sentence (English) → Decomposition: LLM splits sentence → List of Phrases → Translation: For each phrase → Retrieve top-k examples → LLM translates phrase → Filter by language ID → Recombination: Construct prompt with Source Sentence + All (Phrase, Translation) pairs → LLM outputs final translation
- **Design tradeoffs**:
  - Latency vs. Quality: CompTra requires 3 distinct LLM calls and multiple retrieval steps, making it significantly slower than standard few-shot MT
  - Retrieval Method: BM25 is faster and cheaper than dense embeddings (SONAR) and performed better in ablations
- **Failure signatures**:
  - Over-generation: In low-resource settings, LLMs may repeat n-grams; the paper applies a post-processing rule to stop after 8 repeated bigrams
  - Hallucination: The LLM might translate a phrase into the wrong language; requires explicit filtering step
  - Bad Decomposition: If the "Divide" prompt results in sentence fragments rather than full propositions, the retrieval step fails to find matches
- **First 3 experiments**:
  1. Baseline Comparison: Run 5-shot BM25 vs. CompTra on a single low-resource language (e.g., Amharic) using a mid-sized model (8B-70B) to verify the lift reported in Table 1
  2. Ablate the Decomposition: Replace the "Divide" prompt with a "Paraphrase" or "Word extraction" prompt to confirm that "meaningful phrases" are necessary
  3. Retriever Swap: Switch BM25 for SONAR to observe the performance drop described in Table 7

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Compositional Translation be effectively adapted for high-resource language pairs where it currently underperforms?
- Basis in paper: Table 13 shows CompTra fails to outperform few-shot BM25 on five high-resource languages (French, German, Japanese, Portuguese, Spanish). The authors note self-generated in-context demonstrations fail to contribute meaningfully to the MT task and, in some cases, even hinder performance.
- Why unresolved: The paper focuses exclusively on low-resource languages and does not propose modifications to make the approach beneficial for HRLs where base LLM performance is already strong.
- What evidence would resolve it: Experiments with modified decomposition/recombination strategies, or thresholds for when to apply CompTra vs. standard few-shot.

### Open Question 2
- Question: What decomposition strategy is optimal for different language families, and why does the native decomposition outperform structural alternatives?
- Basis in paper: Table 8 shows significant performance variation across decomposition strategies with "Words" and "Structure" performing worst because "phrases obtained... are not full independent sentences, making them difficult to translate."
- Why unresolved: The paper demonstrates that decomposition quality matters but does not systematically analyze which linguistic or structural features predict optimal decomposition.
- What evidence would resolve it: Controlled experiments correlating phrase grammaticality, independence, and semantic coverage with translation quality across diverse languages.

### Open Question 3
- Question: How does CompTra perform on non-English-centric translation directions (X→English or X→Y)?
- Basis in paper: The paper evaluates only English→X and briefly French→X directions. The authors note translating from French "is more difficult than from English" with narrowed performance gaps, suggesting source language affects method efficacy.
- Why unresolved: LLM capabilities differ across languages, and retrieval quality for non-English source languages remains unexplored.
- What evidence would resolve it: Experiments on diverse source-target language pairs including X→English and X→Y directions.

### Open Question 4
- Question: What are the computational costs and latency implications of CompTra's three-step pipeline compared to direct few-shot MT?
- Basis in paper: The method requires three sequential LLM calls plus multiple retrievals. The authors acknowledge DecoMT's sequential nature "makes it slow and suboptimal" but do not analyze CompTra's own latency.
- Why unresolved: Practical deployment requires understanding time and resource overhead, especially for real-time translation applications.
- What evidence would resolve it: Systematic latency measurements and token consumption analysis across model sizes and sentence lengths.

## Limitations
- Method's effectiveness heavily depends on the quality of the decomposition prompt and availability of a sufficiently relevant selection pool
- Performance gains are primarily demonstrated for low-resource languages, with limited effectiveness for high-resource language pairs
- The three-stage pipeline (decompose, translate phrases, recombine) introduces significant computational overhead compared to direct few-shot approaches

## Confidence
- **High Confidence**: Core claim that compositional translation outperforms standard few-shot translation and established strategies is well-supported by extensive experiments across multiple benchmarks and language pairs
- **Medium Confidence**: Claim that BM25 consistently outperforms dense retrieval (SONAR) for short phrases is supported by ablation results but may be dataset-dependent
- **Low Confidence**: Paper doesn't provide detailed analysis of failure cases or quantify frequency of phrase translation errors requiring filtering

## Next Checks
1. Replicate the decomposition ablation: Replace the MinWikiSplit-based "Divide" prompt with alternative decomposition strategies to verify that meaningful phrase decomposition is crucial for performance gains
2. Stress test retrieval robustness: Systematically evaluate CompTra's performance when the selection pool size is reduced or when using cross-domain examples to quantify method's sensitivity to retrieval quality
3. Measure end-to-end latency: Conduct timing experiments to measure the actual computational overhead of the three-stage pipeline and compare with standard few-shot translation approaches across different model sizes