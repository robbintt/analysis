---
ver: rpa2
title: 'Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding
  Korean'
arxiv_id: '2510.24150'
source_url: https://arxiv.org/abs/2510.24150
tags:
- fact
- story
- hint
- deduced
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Ko-MuSR is the first benchmark for evaluating multistep soft reasoning\
  \ in long Korean narratives, minimizing data contamination by following MuSR\u2019\
  s synthesis pipeline and human verification. Evaluations of four LLMs\u2014two multilingual\
  \ and two Korean-specialized\u2014show multilingual models outperform Korean-specialized\
  \ ones in Korean reasoning tasks, suggesting cross-lingual transfer of reasoning\
  \ ability."
---

# Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean

## Quick Facts
- **arXiv ID:** 2510.24150
- **Source URL:** https://arxiv.org/abs/2510.24150
- **Reference count:** 40
- **Primary result:** Multilingual models outperform Korean-specialized models on Korean narrative reasoning tasks, indicating cross-lingual transfer of reasoning ability

## Executive Summary
Ko-MuSR is the first benchmark for evaluating multistep soft reasoning in long Korean narratives, minimizing data contamination by following MuSR's synthesis pipeline and human verification. Evaluations of four LLMs—two multilingual and two Korean-specialized—show multilingual models outperform Korean-specialized ones in Korean reasoning tasks, suggesting cross-lingual transfer of reasoning ability. Carefully designed prompting strategies combining few-shot examples, reasoning traces, and hints significantly boost accuracy, approaching human-level performance. However, smaller language models (SLMs) benefit less from detailed prompting, with some even experiencing accuracy drops, indicating prompting methods should be tailored to model size. Ko-MuSR provides a foundation for advancing Korean NLP through systematic evaluation of long-context reasoning and prompting strategies.

## Method Summary
Ko-MuSR evaluates multistep soft reasoning on Korean narratives using a synthetic generation pipeline that creates Murder Mysteries, Object Placements, and Team Allocations tasks. The benchmark uses GPT-4o/o1 to generate narratives, reasoning trees, and questions, followed by human verification for logical consistency. Evaluation employs `lm-evaluation-harness` and `vLLM` with a specific prompting strategy: 3-shot examples with Chain-of-Thought reasoning traces and task-specific hints. The primary metric is accuracy, reported as the average of two runs across multiple model families including multilingual and Korean-specialized LLMs.

## Key Results
- Multilingual models (Qwen3 32B, LLaMA 4 Scout) outperform Korean-specialized models on Ko-MuSR tasks, demonstrating cross-lingual reasoning transfer
- 3-shot/CoT/hint prompting achieves nearly optimal performance across tasks, with hints providing the most significant gains
- SLMs (Qwen3 1.7B) benefit less from detailed prompting and experience accuracy drops in some tasks, suggesting capacity constraints on in-context learning

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Transfer of Reasoning Schemas
Multilingual models leverage abstract reasoning schemas learned during massive pretraining on English data and apply them to Korean inputs via multilingual alignment in the embedding space. This suggests that "reasoning" is partially decoupled from "language proficiency," allowing cross-lingual generalization of reasoning ability from high-resource to target languages.

### Mechanism 2: Prompting as Task Decomposition Scaffolding
Detailed prompting strategies (few-shot examples + Chain-of-Thought + Hints) significantly improve performance by forcing the model to decompose complex narrative logic into manageable steps. The hint component and CoT examples constrain the model's output space, shifting it from pattern matching to explicit step-by-step derivation that aligns with the logical structure of the dataset.

### Mechanism 3: Capacity Constraints on In-Context Learning
Smaller Language Models benefit less from detailed prompting and may experience accuracy drops because their limited parameter capacity is overwhelmed by instruction-following overhead. In SLMs, attention heads and parameter space are fully utilized for basic language modeling, and adding complex instructions introduces "noise" that prevents focus on core narrative logic.

## Foundational Learning

- **Concept: Soft Reasoning vs. Formal Logic**
  - Why needed: Ko-MuSR evaluates narrative consistency and human-like deduction rather than formal logic, so understanding this difference is crucial for diagnosing model failures
  - Quick check: If a model fails a murder mystery question, is it because it didn't understand the Korean grammar, or because it failed to apply the abstract concept of "opportunity"?

- **Concept: Cross-Lingual Alignment**
  - Why needed: The paper shows reasoning transfers from English to Korean, so understanding how multilingual models align embeddings across languages is crucial for explaining this transfer
  - Quick check: Why might a Korean-specialized model underperform a multilingual model even on Korean inputs? (Answer: The multilingual model may have superior reasoning capabilities transferred from English)

- **Concept: Prompt Engineering Overhead**
  - Why needed: The finding that SLMs struggle with complex prompts challenges the "more prompts = better performance" heuristic
  - Quick check: For a 2B parameter model, should you add 3-shot examples with detailed hints? (Answer: Proceed with caution; evidence suggests this might actually degrade performance)

## Architecture Onboarding

- **Component map:** GPT-4o/o1 (Narrative -> Reasoning Tree -> Questions) -> Human Verification (logical consistency check) -> lm-evaluation-harness/vLLM (model inference)
- **Critical path:** The synthesis pipeline is the bottleneck; generating logically consistent narratives with reasoning trees is complex, and failure here leads to unanswerable questions and noise in the benchmark
- **Design tradeoffs:** Synthetic vs. Natural Data (synthetic avoids contamination but may lack naturalistic complexity); Multilingual vs. Specialized (specialized offers lower latency but lower reasoning accuracy)
- **Failure signatures:** "Prompt Collapse" in SLMs (accuracy drop when moving from 0-shot to 3-shot prompting); False Positives in Specialized Models (high linguistic fluency but low logical consistency)
- **First 3 experiments:** 1) Baseline Verification: Run Zero-Shot evaluation on both LLMs and SLMs to establish reasoning gaps; 2) Ablation Study: Systematically remove Hints, then CoT, then Few-Shot examples to quantify contributions; 3) Cross-Contamination Check: Train a small model on the dataset to ensure it's learnable and valid

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data bias may lack the naturalistic complexity and edge cases present in real human-written Korean stories, potentially inflating model performance
- Parameter threshold ambiguity: the exact capacity threshold where prompting effectiveness breaks down is not rigorously established
- Cultural nuance omission: the benchmark focuses on structural logic rather than cultural reasoning, limiting generalizability to culturally-specific tasks

## Confidence

- **High Confidence:** Multilingual models outperforming Korean-specialized models is well-supported by empirical results across multiple model families and tasks
- **Medium Confidence:** The cross-lingual alignment mechanism is plausible but not directly tested; capacity constraints hypothesis is supported but lacks systematic ablation
- **Low Confidence:** The exact parameter threshold where prompting effectiveness breaks down is not rigorously established

## Next Checks

1. **Parameter Threshold Validation:** Systematically evaluate a spectrum of model sizes (1B, 3B, 7B, 14B, 32B) on Ko-MuSR using identical prompting strategies to identify the precise capacity threshold where performance gains from detailed prompts reverse.

2. **Natural Data Contamination Test:** Apply Ko-MuSR evaluation to a corpus of naturally written Korean narratives to assess whether synthetic benchmark performance correlates with real-world reasoning capability.

3. **Cultural Reasoning Stress Test:** Design a minimal variant of Ko-MuSR that incorporates culturally specific Korean reasoning scenarios to test whether multilingual models maintain their advantage when cultural nuance becomes central to the reasoning task.