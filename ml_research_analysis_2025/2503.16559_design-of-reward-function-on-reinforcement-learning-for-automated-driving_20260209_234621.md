---
ver: rpa2
title: Design of Reward Function on Reinforcement Learning for Automated Driving
arxiv_id: '2503.16559'
source_url: https://arxiv.org/abs/2503.16559
tags:
- reward
- function
- driving
- learning
- vehicle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a general design scheme of process-oriented
  reward function for automated driving. The scheme calculates evaluation values between
  0 and 1 for all evaluation items, and determines the reward based on the product
  of those values.
---

# Design of Reward Function on Reinforcement Learning for Automated Driving

## Quick Facts
- arXiv ID: 2503.16559
- Source URL: https://arxiv.org/abs/2503.16559
- Reference count: 9
- One-line primary result: Proposed multiplicative reward composition enables automated driving agent to achieve human-acceptable behavior in both circuit racing and highway cruising scenarios

## Executive Summary
This paper proposes a general design scheme for process-oriented reward functions in reinforcement learning for automated driving. The method calculates evaluation values between 0 and 1 for all driving criteria, then determines rewards based on the product of these values. The approach is demonstrated on simulated circuit driving and highway cruising, showing appropriate driving positions, lane changing behavior, and safe following distances. The multiplicative composition creates dense gradients that guide the policy toward optimizing all criteria simultaneously rather than exploiting sparse goal-based rewards.

## Method Summary
The method uses Asynchronous Advantage Actor-Critic (A3C) with a 4-layer fully-connected neural network (500×500 hidden layers) to learn driving policies. The reward function is constructed by evaluating driving metrics through six template functions that map values to [0,1] ranges, then multiplying these evaluations together. The system includes a low-level controller that converts actor outputs (acceleration in local coordinates) to steering and pedal commands using a 2-wheel model and PID controller. The approach is validated on TORCS racing simulator with custom patches to extract nearby vehicle data, training on 10 courses and testing on held-out scenarios.

## Key Results
- Circuit driving: Agent learns to travel on inside of corners and decelerate appropriately for sharp curves while maintaining target velocity of 250 km/h
- Highway cruising: Agent successfully changes lanes to avoid slower vehicles, decelerates appropriately when approaching front vehicles, and accelerates to prevent rear vehicles from catching up
- Reward structure: Multiplicative composition of normalized evaluation values (0-1) enables dense process-oriented feedback compared to sparse rewards

## Why This Works (Mechanism)

### Mechanism 1: Multiplicative Reward Composition for Process-Oriented Dense Feedback
Multiplying normalized evaluation values across all driving criteria produces dense, process-aware gradients that guide policy toward human-acceptable driving behavior. Each evaluation function $e_k(f)$ maps a driving metric to [0,1], and the temporary reward $r_{tmp}(s,a,s') = \prod_{k=0}^{n-1} e_k(f_k(s,a,s'))$ creates a dense signal where any single poor evaluation proportionally reduces the reward. This ensures the agent optimizes all criteria simultaneously rather than exploiting sparse goal-based rewards. Core assumption: Driving quality can be decomposed into independent, commensurable evaluation items where multiplication meaningfully captures their joint satisfaction.

### Mechanism 2: Terminal State Value Inflation to Prevent Goal Avoidance
Scaling terminal rewards by $1/(1-\gamma)$ ensures the agent prefers reaching the goal over infinite horizon reward accumulation. Without correction, an agent receiving reward 0.9 per step with $\gamma=0.99$ computes $Q = 0.9/(1-0.99) = 90$, which exceeds a terminal reward of 1.0. Setting terminal reward to $r_{tmp}/(1-\gamma)$ makes goal-reaching the dominant strategy. Core assumption: The discount factor $\gamma$ is known and stable, and the task has well-defined terminal states.

### Mechanism 3: Modular Evaluation Function Templates for Task Adaptation
Predefined evaluation function templates (target-value, threshold-based, dual-target) enable rapid reward function construction across diverse driving scenarios. Six template types handle common driving constraints: maintaining target values, tolerating one-sided deviations, and balancing competing objectives. Designers select and parameterize templates per evaluation item without deriving custom functions. Core assumption: Driving constraints can be categorized into these six functional forms with appropriate parameter tuning.

## Foundational Learning

- **Actor-Critic Architecture**: A3C requires understanding how policy ($\pi$) and value ($V$) functions interact during training. Quick check: Can you explain why the advantage function $A(s,a) = \sum \gamma^i r_{t+i} + \gamma^k V(s_{t+k}) - V(s_t)$ reduces variance compared to raw returns?

- **Reward Shaping vs. Sparse Rewards**: The paper's core contribution is a process-oriented (dense) reward design that contrasts with +1/-1 sparse rewards common in game-playing RL. Quick check: Given a driving task, would you use sparse collision penalties or continuous distance-based risk evaluation? Why?

- **Discount Factor ($\gamma$) and Value Estimation**: Understanding $\gamma$ is essential for the terminal state correction mechanism (equation 6) and interpreting the infinite-horizon value calculation. Quick check: If $\gamma=0.99$ and you receive reward 0.8 per step forever, what is the total discounted return?

## Architecture Onboarding

- **Component map**: State → Actor network (4-layer MLP) → Controller module → Environment → State' + Reward module evaluation → A3C gradient update
- **Critical path**: Vehicle state inputs flow through actor network to produce acceleration commands, which the controller converts to steering/throttle using target point projection and PID control, then environment returns next state and reward for policy updates
- **Design tradeoffs**: Multiplicative vs. additive reward (multiplication ensures joint optimization but risks gradient vanishing); template generality vs. specificity (six templates cover common cases but may not fit temporal constraints); exploration variance offset (0.04) balances exploration vs. convergence speed
- **Failure signatures**: Policy avoids goal (terminal reward scaling not applied); excessive conservatism (evaluation function tolerances too tight); unstable lane changes (lane preference reward too high); slow convergence (multiplicative reward produces small gradients)
- **First 3 experiments**: 1) Implement evaluation functions $e_0$-$e_3$ for circuit driving and verify agent learns to maintain target velocity and stay near track center; 2) Remove terminal scaling factor and compare goal-reaching behavior to confirm necessity; 3) Add risk evaluation function to circuit policy and test on highway with slower leading vehicle to verify lane-change behavior

## Open Questions the Paper Calls Out

### Open Question 1
How can hand-crafted safety rewards be effectively integrated with rewards generated via Inverse Reinforcement Learning (IRL) for passenger comfort? The authors state they plan to "develop the scheme combining the hand-crafted reward and the generated reward by IRL." This remains unresolved as the current method relies solely on manually designed process-oriented rewards, and the mechanism for merging these with learned rewards is undefined.

### Open Question 2
Does the introduction of risk-sensitive reinforcement learning improve safety performance compared to the standard A3C method used in this study? The authors mention that "risk sensitive reinforcement learning... will be introduced for more safety" as future work. This is unresolved because the current study uses A3C, which maximizes expected return but does not explicitly account for risk variability or worst-case scenarios.

### Open Question 3
How can the evaluation function parameters be automatically tuned to eliminate unstable motion during maneuvers like lane changes? The authors observed "unstable motion" in the passing lane and hypothesized that "smoother driving might be obtained by enhancing the evaluation function parameters." This remains unresolved as the current manual parameter adjustment led to suboptimal behavior (wandering) in specific scenarios, indicating a need for a more robust tuning methodology.

## Limitations
- Multiplicative reward composition assumes evaluation items are commensurable and independent, but lacks sensitivity analysis for correlation effects or numerical underflow conditions
- Template-based evaluation function system provides useful modularity but is limited to six predefined functional forms, potentially missing complex temporal or conditional constraints
- Terminal state value correction mechanism is mathematically sound but assumes known, stable discount factor and well-defined terminal states without empirical validation

## Confidence

**High**: Terminal state value correction mechanism (equation 6) - mathematically rigorous with clear derivation and numerical demonstration
**Medium**: Multiplicative reward composition - theoretically justified but lacks sensitivity analysis for correlation effects
**Medium**: Template-based evaluation functions - practical approach with demonstrated applications but limited functional coverage

## Next Checks

1. **Correlation sensitivity analysis**: Test multiplicative reward composition with evaluation items of varying correlation strengths to quantify gradient instability and identify safe operating ranges for parameter tuning
2. **Terminal state behavior ablation**: Remove the $1/(1-\gamma)$ scaling factor and measure goal-avoidance behavior in both circuit and highway tasks to empirically validate the correction necessity
3. **Template extensibility test**: Attempt to implement a temporal priority constraint (e.g., "maintain safe distance, but pass within 10 seconds if possible") using existing templates to identify functional gaps requiring custom solutions