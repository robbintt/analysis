---
ver: rpa2
title: 'DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal
  LoRA Adaptation'
arxiv_id: '2509.21433'
source_url: https://arxiv.org/abs/2509.21433
tags:
- erasure
- concepts
- concept
- scope
- dyme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-concept erasure in
  diffusion models, where the goal is to selectively suppress multiple target concepts
  without degrading the generation of non-target content. The core issue is that existing
  static erasure methods, which fine-tune a single checkpoint to remove all target
  concepts, struggle to scale and suffer from interference between concept updates.
---

# DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation

## Quick Facts
- arXiv ID: 2509.21433
- Source URL: https://arxiv.org/abs/2509.21433
- Reference count: 40
- Key outcome: Dynamic LoRA-based framework that achieves >90% harmonic accuracy for multi-concept erasure while preserving non-target fidelity

## Executive Summary
This paper addresses the challenge of multi-concept erasure in diffusion models, where the goal is to selectively suppress multiple target concepts without degrading the generation of non-target content. The core issue is that existing static erasure methods, which fine-tune a single checkpoint to remove all target concepts, struggle to scale and suffer from interference between concept updates. To overcome this, the authors propose DyME, a dynamic erasure framework that trains lightweight, concept-specific LoRA adapters and composes them on-demand at inference. To mitigate interference among LoRA adapters, DyME introduces bi-level orthogonality constraints at both the feature and parameter levels. The paper also introduces ErasureBench-H, a new hierarchical benchmark with a brand-series-character structure, enabling principled evaluation of scalable, multi-concept erasure. Experiments on CIFAR-100, Imagenette, and ErasureBench-H demonstrate that DyME consistently outperforms state-of-the-art baselines, achieving over 90% harmonic accuracy even as the erasure scope grows, while maintaining high fidelity for non-target content.

## Method Summary
DyME addresses multi-concept erasure by training one lightweight LoRA adapter per concept, injected only into cross-attention Value (W_v) and Output (W_o) projections of a frozen diffusion backbone. At inference, only adapters corresponding to the prompt's erasure subset are activated and their classifier-free guidance predictions are averaged. To prevent crosstalk between adapters, DyME enforces bi-level orthogonality: input-aware (penalizing alignment of LoRA-induced representation shifts for prompt-specific inputs) and input-agnostic (enforcing a symmetric parameter constraint guaranteeing orthogonal shifts for all inputs). The model is trained jointly with a reconstruction loss and the two orthogonality terms. The framework scales to large erasure scopes without degradation in non-target fidelity.

## Key Results
- DyME achieves >90% harmonic accuracy on CIFAR-100 with 20-concept erasure scope, outperforming static baselines (ESD, AC, FMN, MACE, SPM, SalUn) by 20-30 percentage points
- Bi-level orthogonality consistently improves multi-concept erasure performance, with input-aware constraints contributing more than input-agnostic in tested settings
- DyME maintains high non-target fidelity (Acc_UP) while effectively suppressing target concepts (Acc_EE), even as erasure subset size grows to 5 concepts
- On the hierarchical ErasureBench-H (brand–series–character), DyME shows the largest margin over baselines when erasure scope expands

## Why This Works (Mechanism)

### Mechanism 1: Dynamic LoRA Composition at Inference
- Claim: Decoupling training scope from inference subset preserves non-target fidelity while maintaining erasure effectiveness.
- Mechanism: Instead of fine-tuning a single checkpoint to erase all concepts in C_scope, DyME trains lightweight concept-specific LoRA adapters. At inference, only adapters corresponding to the prompt's erasure subset C_subset are activated and composed via classifier-free guidance averaging.
- Core assumption: Concept erasure objectives are approximately additive when adapters operate in non-interfering subspaces; erasure requests vary per generation.
- Evidence anchors:
  - [abstract]: "DyME, an on-demand erasure framework that trains lightweight, concept-specific LoRA adapters and dynamically composes only those needed at inference."
  - [Section 4.2]: "At inference, DyME identifies the erasure subset C_subset for each prompt p. Only the corresponding LoRA modules are activated, and their classifier-free guidance predictions are computed and averaged to yield a unified denoising direction."
  - [corpus]: Neighbor paper "Sculpting Memory" addresses multi-concept forgetting with dynamic masking, suggesting the broader relevance of dynamic/conditional erasure strategies, though via different mechanisms.
- Break condition: If many concepts are requested simultaneously without orthogonal constraints, adapter crosstalk dominates and harmonic accuracy collapses (see DyME w/o ortho ablation, Table 1).

### Mechanism 2: Input-Aware Orthogonality Constraint
- Claim: Penalizing alignment between LoRA-induced representation shifts in cross-attention disentangles adapters for prompts seen during training.
- Mechanism: For each concept pair (c_i, c_j), compute the LoRA-induced output shift ΔO^(i), ΔO^(j) in cross-attention (Eq. 1). The orthogonality score OS(i,j) = 1 - ⟨ΔO^(i), ΔO^(j)⟩_F / (‖ΔO^(i)‖_F ‖ΔO^(j)‖_F) (Eq. 2) is maximized via L_aware_ortho (Eq. 3), encouraging orthogonal update directions in feature space.
- Core assumption: Cross-attention projections (q, k, v, o) are the primary site of concept-specific signal; disentangled shifts reduce prompt-specific interference.
- Evidence anchors:
  - [Section 4.1]: "For each pair of concepts, we compute the change induced by their respective LoRA modules and penalize alignment between these shifts."
  - [Table 4]: Ablation shows removing input-aware orthogonality (Config 2) drops harmonic accuracy from 82.01% (DyME) to 67.91%.
  - [corpus]: No direct corpus precedent for representation-space orthogonality in erasure; neighbor "Orthogonal Adaptation" (Po et al., 2024, cited in paper) enforces parameter-space orthogonality but abstracts from cross-attention.
- Break condition: If training prompts do not cover the semantic diversity of test prompts, input-aware constraints alone are insufficient (necessitating input-agnostic constraints).

### Mechanism 3: Input-Agnostic Orthogonality Constraint
- Claim: Enforcing a symmetric parameter-space condition on LoRA weight products guarantees orthogonal representation shifts for all inputs, providing distribution-robust disentanglement.
- Mechanism: Theorem 1 establishes that under fixed Q/K projections and LoRA on V/O only, the condition (M^(i))^T M^(j) + (M^(j))^T M^(i) = 0 (where M^(i) = W_o^(0)ΔW_v^(i) + ΔW_o^(i)W_v^(0) + ΔW_o^(i)ΔW_v^(i)) is sufficient for ⟨ΔO^(i), ΔO^(j)⟩_F = 0 for all X, z. This is enforced via L_agnostic_ortho (Eq. 4).
- Core assumption: Q/K projections can remain frozen without significantly limiting erasure capacity; LoRA updates on V/O capture concept-specific signal.
- Evidence anchors:
  - [Section 4.1 & Theorem 1]: Formal proof that the parameter constraint implies universal output orthogonality.
  - [Table 4]: Ablation shows removing input-agnostic orthogonality (Config 3) drops harmonic accuracy from 82.01% to 79.64%, a smaller but consistent contribution.
  - [corpus]: Neighbor "UnGuide" uses LoRA for concept forgetting but does not impose orthogonality constraints; corpus evidence for this specific mechanism is weak outside this paper.
- Break condition: If LoRA is applied to Q/K as well (violating Theorem 1 assumptions), the sufficiency guarantee no longer holds, though empirical gains may persist.

## Foundational Learning

- Concept: Cross-attention in diffusion models
  - Why needed here: DyME targets cross-attention projections (Q, K, V, O) as the primary site for LoRA injection and orthogonality constraints. Understanding how text embeddings condition visual latents via Q/K/V/O is essential to grasp where and why crosstalk occurs.
  - Quick check question: In a cross-attention block, which projections consume text embeddings vs. visual latents as input?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: DyME trains one LoRA adapter per concept. You must understand LoRA's factorization (W = W_0 + BA), rank, scale (α/r), and injection points to implement and debug the framework.
  - Quick check question: If a LoRA adapter has rank r=8 and scale α=8, what is the effective scaling of the low-rank update relative to the base weight?

- Concept: Classifier-Free Guidance (CFG)
  - Why needed here: DyME composes multiple LoRA adapters at inference by averaging their CFG predictions. You need to understand CFG (ε_θ(z_t, t, p) vs. ε_θ(z_t, t, ∅)) to see how adapter-specific guidance vectors are combined.
  - Quick check question: How does increasing CFG scale trade off between prompt adherence and sample diversity?

## Architecture Onboarding

- Component map:
  Pre-trained diffusion backbone (Stable Diffusion v1.4) with frozen base weights -> Per-concept LoRA adapters (injected into cross-attention V and O projections) -> Orthogonality modules (compute pairwise OS(i,j) for input-aware loss and M^(i)^T M^(j) + M^(j)^T M^(i) for input-agnostic loss) -> Training loop (optimizes all adapters with L = L_rec + λ_1 L_aware_ortho + λ_2 L_agnostic_ortho) -> Inference composition (identifies C_subset from prompt, activates corresponding adapters, composes via CFG averaging)

- Critical path:
  1. Define erasure scope C_scope and neutral substitute targets (e.g., "background").
  2. Attach one LoRA adapter per concept to V/O projections in cross-attention layers.
  3. Train all adapters jointly with reconstruction loss (push target concept toward substitute) and bi-level orthogonality losses.
  4. At inference, parse prompt to identify C_subset, activate only relevant adapters, compose via CFG averaging, generate image.

- Design tradeoffs:
  - Dynamic vs. static erasure: Dynamic enables per-generation control but requires orthogonality to avoid crosstalk; static is simpler but scales poorly (Figure 1).
  - V/O-only LoRA vs. full Q/K/V/O: Restricting to V/O enables Theorem 1's guarantee but may limit erasure capacity; Q/K LoRA would require new theoretical analysis.
  - Orthogonality strength (λ_1, λ_2): Too high → underfits erasure objective; too low → crosstalk. Table 4 suggests input-aware contributes more than input-agnostic in the tested setting.
  - LoRA rank/scale: Paper uses r=8, α=8; higher rank increases capacity but also interference risk.

- Failure signatures:
  - Static baselines (ESD, AC, FMN, MACE, SPM, SalUn) degrade sharply as erasure scope expands (Figure 1, left): Acc_UP drops or Acc_EE rises due to gradient conflicts.
  - DyME w/o orthogonality (Table 1, Table 4): Harmonic accuracy collapses when multiple adapters are composed (e.g., 5-concept conjunction: 31.59% vs. 40.12% for full DyME).
  - High concept scope (brand-level, Table 3): All methods degrade, but DyME maintains the largest margin; extremely broad scopes still cause non-trivial Acc_EE even for DyME.

- First 3 experiments:
  1. Single-concept erasure validation: Train DyME on a 5-concept scope (CIFAR-100 subset), evaluate per-concept Acc_EE and Acc_UP with erasure subset size 1. Verify harmonic accuracy > 80% and minimal deviation from Table 6 baseline.
  2. Ablation on orthogonality constraints: Remove input-aware, input-agnostic, or both orthogonality terms. Confirm that bi-level orthogonality provides the largest gain (Table 4 pattern).
  3. Multi-concept conjunction scaling: Fix erasure scope at 5 concepts, test per-generation erasure subset sizes N ∈ {2, 3, 4, 5} with conjunction prompts. Verify DyME harmonic accuracy remains > 40% at N=5 (Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DyME scale when the erasure scope grows to hundreds or thousands of concepts during training, given that each concept requires its own LoRA adapter?
- Basis in paper: [explicit] The authors train "lightweight, concept-specific LoRA adapters" but only evaluate with erasure scopes up to 20 concepts (CIFAR-100) and ~300 concepts (ErasureBench-H), with the latter tested only hierarchically.
- Why unresolved: Training time and memory grow linearly with erasure scope size. The paper does not analyze training scalability or memory overhead for very large erasure scopes.
- What evidence would resolve it: Experiments reporting training time, memory usage, and performance when scaling erasure scope to 500+ concepts in a flat (non-hierarchical) setting.

### Open Question 2
- Question: Is DyME robust to adversarial prompt engineering or synonym substitution designed to bypass erasure?
- Basis in paper: [inferred] The evaluation uses canonical prompts ("a photo of the {target concepts}"), and the authors acknowledge that "input-aware constraints... may leave residual overlap in unobserved or biased input distributions."
- Why unresolved: The paper does not evaluate against adversarial attacks, paraphrased prompts, or concept aliasing that could evade erasure while still generating target content.
- What evidence would resolve it: Adversarial robustness tests using paraphrased prompts, synonym substitution, or prompt engineering attacks targeting erased concepts.

### Open Question 3
- Question: Does the assumption of fixed query (W_q) and key (W_k) projections limit erasure effectiveness compared to full cross-attention adaptation?
- Basis in paper: [explicit] Theorem 1 explicitly assumes "LoRA adaptation is restricted to the value and output projections W_v, W_o, with the query and key projections fixed."
- Why unresolved: This restriction simplifies the orthogonality proof but may constrain the model's capacity to fully suppress concepts represented in query/key projections.
- What evidence would resolve it: Ablation comparing DyME with fixed vs. adapted W_q/W_k on erasure effectiveness and composition stability metrics.

## Limitations
- The orthogonality regularization weights (λ₁, λ₂) are not reported, making exact reproduction difficult
- The reconstruction loss L_rec is vaguely defined as "distance between generated image and neutral substitute" without implementation details
- The proposed ErasureBench-H benchmark is not yet released, limiting direct comparison
- The input-agnostic orthogonality proof assumes LoRA is applied only to V/O projections, so guarantees may not hold if extended to Q/K

## Confidence
- High: Harmonic accuracy consistently > 90% for moderate erasure scopes, outperforming all baselines (Fig. 1, Tables 1, 3)
- Medium: Bi-level orthogonality improves multi-concept erasure over single-level or no orthogonality, supported by ablation (Table 4) and theory (Theorem 1)
- Low: Input-agnostic orthogonality constraint provides the claimed universal disentanglement; proof assumes fixed Q/K, which may not hold in practice

## Next Checks
1. Implement ablation removing each orthogonality term to confirm their distinct contributions (replicate Table 4 pattern)
2. Scale erasure scope beyond 5 concepts (e.g., CIFAR-100 full dataset) to test scalability limits and confirm no catastrophic interference
3. Extend LoRA to Q/K projections and verify whether Theorem 1's sufficiency condition still correlates with performance gains