---
ver: rpa2
title: 'HiPS: Hierarchical PDF Segmentation of Textbooks'
arxiv_id: '2509.00909'
source_url: https://arxiv.org/abs/2509.00909
tags:
- segmentation
- text
- section
- headings
- hierarchy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study tackles hierarchical segmentation of deeply nested
  legal textbooks in PDF format, a task where existing tools struggle with hierarchy
  depth and structure recovery. It introduces two approaches: TOC-Based PageParser,
  which uses Table of Contents metadata for structured extraction, and LLM-Refined
  PageParser, which combines OCR-based candidate selection with large language model
  refinement.'
---

# HiPS: Hierarchical PDF Segmentation of Textbooks

## Quick Facts
- arXiv ID: 2509.00909
- Source URL: https://arxiv.org/abs/2509.00909
- Reference count: 31
- Key outcome: TOC-based methods excel with high-quality metadata, while LLM-based refinement reduces false positives in deep hierarchy segmentation

## Executive Summary
This study tackles hierarchical segmentation of deeply nested legal textbooks in PDF format, where existing tools struggle with hierarchy depth and structure recovery. It introduces two approaches: TOC-Based PageParser, which uses Table of Contents metadata for structured extraction, and LLM-Refined PageParser, which combines OCR-based candidate selection with large language model refinement. Evaluated on a manually curated dataset of 49 English legal textbooks with up to seven hierarchy levels, the methods improve upon baseline tools in detecting section titles, assigning hierarchy levels, and defining boundaries. TOC-based methods excel with high-quality metadata, while LLM-based refinement reduces false positives. The work highlights the ongoing challenges of deep hierarchy segmentation and provides open-source code and data for replication.

## Method Summary
The paper introduces two hierarchical segmentation approaches for deeply nested legal textbooks. The TOC-Based PageParser extracts outline entries from PDF metadata and matches them to text content using exact, substring, or fuzzy matching (fuzzywuzzy partial ratio ≥80). The LLM-Refined PageParser combines spatial whitespace detection with OCR candidate selection, then uses LLMs to filter and classify headings. Both methods output hierarchical trees evaluated via Zhang-Shasha tree edit distance, with precision/recall measured using edit distance tolerance ≤2. The system processes 49 legal textbooks from the Open Research Library, with ground truth consisting of 9,812 manually curated headings.

## Key Results
- TOC-Based PageParser outperforms other methods in tolerant Precision (P_ED) when metadata quality is high
- LLM-Refined PageParser substantially reduces false positives by combining spatial context with semantic classification
- Tree Edit Distance evaluation reveals structural misalignments better than flat text metrics, especially at deeper hierarchy levels

## Why This Works (Mechanism)

### Mechanism 1: Metadata-Anchored Hierarchy Reconstruction
Leveraging existing Table of Contents (TOC) metadata provides superior hierarchy recovery compared to layout-only inference, provided the metadata is complete. The TOC-Based PageParser extracts outline entries (titles, levels, page numbers) directly from the PDF's internal structure, then anchors these entries to the full text using a three-stage matching strategy (exact → substring → fuzzy) to resolve OCR artifacts and formatting inconsistencies.

### Mechanism 2: Spatial-Semantic Candidate Filtering
Combining visual whitespace detection with semantic classification reduces false positives better than font-based heuristics alone. The LLM-Refined PageParser first uses Tesseract OCR to identify candidate lines based on spatial separation (empty lines preceding/following text). These candidates, along with contextual text, are then filtered by an LLM to distinguish true headings from body text or noise.

### Mechanism 3: Hierarchical Boundary Assignment via Tree Edit Distance
Global hierarchy consistency is better evaluated via tree structure comparison than local token matching. The system constructs a hierarchical tree and uses the Zhang-Shasha algorithm to compute the edit distance between the predicted tree and the ground truth tree, penalizing structural misalignments more than minor text differences.

## Foundational Learning

- **Poppler/pdftohtml XML Structure**: Required to understand the intermediate XML representation used for font metrics and positioning. Quick check: Can you identify a scenario where `pdftohtml` would split a single visual heading into multiple XML nodes?

- **Zhang-Shasha Algorithm (Tree Edit Distance)**: Essential for understanding hierarchy preservation evaluation. Quick check: If a parser detects all correct headings but indents one level incorrectly, will the F1 score or Tree Edit Distance better reflect this failure?

- **OCR Preprocessing & Whitespace Heuristics**: Critical for understanding the LLM-Refined path's candidate selection. Quick check: Why does the paper suggest using empty lines (spatial separation) as a signal rather than just font size?

## Architecture Onboarding

- **Component map**: Input (Legal Textbook PDFs) → Preprocessor (pdftohtml XML OR Tesseract OCR) → Extractor (TOC-Based Parser OR LLM-Refined Parser) → Assembler (Segment_Text Algorithm 1) → Evaluator (P_ED, R_ED, Tree Edit Distance)

- **Critical path**: The Matching Logic in Algorithm 1, where candidates are converted into actual document segments. If the fuzzy matching threshold (80%) is too loose, false positives spike; if too tight, recall drops on OCR errors.

- **Design tradeoffs**: TOC-Based is computationally cheaper and precise for well-tagged files, while LLM-Based is expensive but handles untagged/deeply nested files better. XML is faster but misses visual context, while OCR captures layout but introduces character errors.

- **Failure signatures**: Metadata Mismatch shows high edit distance on Level 4-5, suggesting incomplete TOC metadata. LLM Hallucination might assign inconsistent hierarchy levels to similar headers without proper prompt engineering.

- **First 3 experiments**:
  1. Run TOC-Based Parser on a textbook with verified outline metadata to confirm successful heading-to-text matching.
  2. Compare LLM-Refined pipeline using XML-only vs XML+OCR to validate spatial context claims.
  3. Select a Level 7 hierarchy file to compare Tree Edit Distance of TOC-Based vs XML-OCR-GPT4 methods.

## Open Questions the Paper Calls Out

- **Can hierarchical segmentation methods maintain accuracy on PDFs that lack Table of Contents metadata?**: The study only evaluated documents with existing TOC metadata, leaving unexplored how LLM-Refined or other approaches perform without this structural cue.

- **How effectively can extracted hierarchical segmentations support downstream knowledge graph construction?**: While the paper produces structured output suitable for knowledge extraction, no downstream task evaluation was conducted to validate utility.

- **Would publisher-aware fine-tuning or adaptation significantly improve segmentation performance?**: Figure 6 color-codes results by publisher group, noting that certain publisher styles exhibit individual strengths, suggesting publisher-specific formatting patterns affect accuracy.

- **Can existing multimodal approaches designed for shallow hierarchies be adapted to handle deep (5+ level) textbook structures?**: The paper's experiments show even state-of-the-art tools struggle with depth, but did not extend multimodal methods to deep hierarchies.

## Limitations

- Dependency on PDF metadata quality severely limits TOC-Based method applicability when outline metadata is incomplete or missing at deeper levels
- LLM-Refined approach introduces significant computational overhead through API calls and requires careful prompt engineering
- Evaluation dataset remains relatively small (49 books) and domain-specific to legal textbooks, limiting generalizability

## Confidence

- **High Confidence**: TOC-Based PageParser's superior performance when metadata is complete (P_ED results in Section 5.1)
- **Medium Confidence**: LLM-Refined pipeline's ability to reduce false positives through spatial-semantic filtering
- **Low Confidence**: Generalization of findings to non-legal deeply nested documents beyond the specific dataset

## Next Checks

1. **Metadata Dependency Test**: Systematically evaluate TOC-Based PageParser across the dataset to quantify exactly how performance degrades as outline metadata completeness decreases at each hierarchy level.

2. **Cross-Domain Transfer**: Apply the best-performing methods to a different deeply nested document corpus (e.g., technical standards, medical guidelines) to assess generalizability beyond legal textbooks.

3. **LLM Prompt Sensitivity Analysis**: Conduct controlled experiments varying the LLM prompts and parameters to determine the minimum effective configuration for maintaining performance while reducing computational costs.