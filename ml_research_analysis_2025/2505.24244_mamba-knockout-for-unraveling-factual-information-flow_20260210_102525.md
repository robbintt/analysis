---
ver: rpa2
title: Mamba Knockout for Unraveling Factual Information Flow
arxiv_id: '2505.24244'
source_url: https://arxiv.org/abs/2505.24244
tags:
- token
- information
- knockout
- figure
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how factual information flows through\
  \ Mamba-based language models by adapting Transformer interpretability techniques,\
  \ specifically attention knockout. The authors examine Mamba-1 and Mamba-2 models,\
  \ revealing that while these models share some information flow patterns with Transformers\u2014\
  such as reliance on subject tokens in late-intermediate layers\u2014they also display\
  \ distinct behaviors like Mamba-1's dependence on the final token and lack of first-position\
  \ bias."
---

# Mamba Knockout for Unraveling Factual Information Flow

## Quick Facts
- **arXiv ID:** 2505.24244
- **Source URL:** https://arxiv.org/abs/2505.24244
- **Authors:** Nir Endy; Idan Daniel Groswald; Yuval Ran-Milo; Yonatan Slutzky; Itay Tshuva; Raja Giryes
- **Reference count:** 26
- **Primary result:** Adapts Transformer interpretability techniques to Mamba-based language models, revealing shared and distinct information flow patterns, particularly the critical role of subject tokens in late-intermediate layers and the importance of context-dependent features.

## Executive Summary
This paper investigates how factual information flows through Mamba-based language models by adapting Transformer interpretability techniques, specifically attention knockout. The authors examine Mamba-1 and Mamba-2 models, revealing that while these models share some information flow patterns with Transformers—such as reliance on subject tokens in late-intermediate layers—they also display distinct behaviors like Mamba-1's dependence on the final token and lack of first-position bias. By leveraging Mamba's factorized structure, the authors introduce a novel feature knockout mechanism that distinguishes between context-dependent and context-independent features, finding that context-dependent features play a critical role in inter-token information exchange. The work extends interpretability methods beyond Transformers and provides a unified framework for understanding factual information processing in both attention-based and state-space model architectures.

## Method Summary
The authors adapt attention knockout methodology from Transformers to Mamba architectures by exploiting theoretical connections between SSMs and attention mechanisms. For Mamba-1, they use kernel representation to compute implicit attention matrices (Mᵢⱼ = Qᵢ · Hᵢⱼ · Kⱼ), while for Mamba-2 they leverage the SSD framework where the SSM layer can be expressed as L ◦ (XMX⊤)X. They introduce a novel feature knockout technique that classifies Mamba's H independent features based on their decay characteristics (‖Ā‖₁), distinguishing context-dependent features (slow decay, top 1/3 by norm) from context-independent features (fast decay, bottom 1/3). The analysis uses the COUNTERFACT dataset, specifically a filtered subset of 672 triplets where Mamba-1, Mamba-2, and GPT-2 all correctly predicted the target token. Interventions are applied using a sliding window of 9 layers centered at different relative depths, and effects are measured by the relative change in correct-token prediction probability.

## Key Results
- Subject tokens play a critical role in factual recall at late-intermediate layers (~70% depth) across Mamba-1, Mamba-2, and Transformer architectures
- Context-dependent features (top 1/3 by ‖Ā‖₁) are primarily responsible for token-to-token information exchange, while context-independent features enrich individual tokens
- Mamba-1 shows distinct behavior including dependence on the final token and lack of first-position bias, unlike Transformers
- Knocking out the last token's self-attention in final layers causes a sharp increase in correct-token probability in Mamba-1 and Falcon-Mamba, suggesting potential inhibitory effects of self-attention

## Why This Works (Mechanism)

### Mechanism 1: Hidden/Implicit Attention Equivalence
- Claim: Mamba SSMs can be interpreted through an attention-like framework, enabling fine-grained token-to-token knockout interventions previously restricted to Transformers.
- Mechanism: For Mamba-1, the kernel representation expresses token relationships as Mᵢⱼ = Qᵢ · Hᵢⱼ · Kⱼ (Ali et al., 2024). Setting Mᵢⱼ = 0 blocks information flow from token j to token i. For Mamba-2, the SSD framework (Dao and Gu, 2024) shows the SSM layer can be expressed as L ◦ (XMX⊤)X, where L ◦ (XMX⊤) functions as an attention matrix—knockout zeros specific entries.
- Core assumption: The attention interpretation captures the primary token-to-token information channels in SSMs, even though SSMs also include token-independent operations (gating, convolution).
- Evidence anchors:
  - [abstract]: "We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms"
  - [section 3.3/3.4]: Mathematical formulations for hidden attention (Mamba-1: Mᵢⱼ = Qᵢ · Hᵢⱼ · Kⱼ) and implicit linear attention (Mamba-2: L ◦ (XMX⊤)X)
  - [corpus]: Weak direct support; related work "Visual Attention Exploration in Vision-Based Mamba Models" (FMR 0.60) discusses attention-like behavior in Mamba but for vision tasks
- Break condition: If token-independent operations (gating, convolution) dominate factual information flow, or if the continuous state representation carries information not captured by the discrete attention approximation.

### Mechanism 2: Feature Decay-Based Functional Classification
- Claim: Features in Mamba can be classified by their decay characteristics (‖Ā‖₁), and context-dependent features (slow decay) primarily drive inter-token information exchange while context-independent features (fast decay) enrich individual tokens.
- Mechanism: The state transition accumulates as ∏A(t) = Ā^(∑Δ(t)). Features with Ā ≈ 1 retain long-range context; features with Ā ≈ 0 forget quickly. The paper classifies top 1/3 ‖Ā‖₁ as context-dependent, bottom 1/3 as context-independent. Knocking out context-dependent features from subject to last token produces effects similar to full knockout.
- Core assumption: The norm ‖Ā‖₁ is a sufficient proxy for a feature's role in information transfer, and the 1/3 division threshold meaningfully separates functional categories.
- Evidence anchors:
  - [abstract]: "context-dependent features are primarily responsible for token-to-token information exchange"
  - [section 3.5]: Formal definition: "context-dependent features as those with the largest one-third of ‖Ā‖₁ values"
  - [section 4.4]: Figure 6 shows knockout of context-dependent features mirrors full knockout; context-independent knockout has minimal impact
  - [corpus]: No direct corpus support for this specific mechanism
- Break condition: If features exhibit non-monotonic or task-dependent decay patterns, or if the 1/3 threshold masks important sub-categories within each group.

### Mechanism 3: Universal Subject-Token Criticality at Late-Intermediate Layers
- Claim: Across SSM-based and Transformer-based architectures, subject tokens (e.g., "Beats Music" in "Beats Music is owned by ___") serve as critical information carriers, with essential subject-to-last-token transfer occurring in late-intermediate layers (~70% depth).
- Mechanism: Knocking out attention from subject tokens to the final token in a window centered at late-intermediate layers causes consistent, pronounced drops in correct-token probability. This pattern persists across Mamba-1, Mamba-2, GPT-2, Llama, and Mistral.
- Core assumption: The knockout intervention reveals causally necessary information pathways rather than inducing artifact failures; the COUNTERFACT subset (672 triplets where all models predict correctly) generalizes to broader factual recall.
- Evidence anchors:
  - [abstract]: "subject tokens playing a critical role in late-intermediate layers"
  - [section 4.2.1]: "inhibiting the final token's attention to subject tokens in late-intermediate layers consistently causes a notable drop in correct-token probability, mirroring patterns observed in Transformer-based models"
  - [section 4.3]: Consistent pattern across model sizes (130M to 2.8B) and architectures (Figures 3-5, 13)
  - [corpus]: "Promote, Suppress, Iterate" studies factual queries but focuses on one-to-many recall, not the layer-wise flow mechanism
- Break condition: If the late-intermediate pattern is specific to attribute-prediction tasks (subject-relation-attribute) rather than general factual recall, or if window size artifacts drive the observed depth localization.

## Foundational Learning

- Concept: **Selective State-Space Models (SSMs)**
  - Why needed here: Mamba's core operation is a selective SSM where matrices A(t), B(t), C(t) depend on input. Understanding x(t+1) = A(t)x(t) + B(t)u(t), y(t) = C(t)x(t) is essential for interpreting feature decay and attention equivalence.
  - Quick check question: Given A(t) = Ā^Δ(t), if Ā = 0.1 versus Ā = 0.9, which feature retains more historical context after 10 timesteps?

- Concept: **Attention Knockout Methodology (Geva et al., 2023)**
  - Why needed here: This is the interpretability technique being extended. It zeroes attention weights between specific token pairs at specific layers to identify critical information pathways.
  - Quick check question: If knocking out attention from token r to token c at layer l causes a 40% drop in correct-token probability, what does this suggest about the information flow?

- Concept: **Linear Attention and SSD Duality**
  - Why needed here: The theoretical bridge relies on understanding that Mamba-2 is equivalent to a subclass of linear attention Transformers. The formulation L ◦ (XMX⊤)X makes this explicit.
  - Quick check question: How does masked linear attention differ from softmax attention, and why does the lower-triangular mask L matter for autoregressive generation?

## Architecture Onboarding

- Component map:
  - **Mamba SSM layers**: Each layer processes H independent features, each with its own Ā (decay) parameter
  - **Hidden attention matrix**: Implicit structure Mᵢⱼ capturing token-to-token influence (computed on-demand for knockout)
  - **Token positions**: Recurrent state propagates left-to-right; information flows from earlier to later positions
  - **Feature classification**: Context-dependent (top 1/3 ‖Ā‖₁) vs context-independent (bottom 1/3)

- Critical path:
  Input → [Early layers: relation token processing in Transformers, diffuse in Mamba] → [Late-intermediate layers (~70% depth): subject-to-last-token transfer via context-dependent features] → [Final layers: prediction from last token]

- Design tradeoffs:
  - **Knockout window size**: Window=9 blocks 37.5% of layers in a 24-layer model. Smaller windows (1-5) give better resolution but may miss distributed effects; larger windows (12-15) smooth results but obscure layer-specific patterns.
  - **Feature threshold**: 1/3 division is heuristic. Alternative: analyze continuous ‖Ā‖₁ distribution.
  - **Dataset filtering**: Using only correctly-predicted examples ensures clean causal signal but excludes failure modes.

- Failure signatures:
  - **Mamba-1 last-token surge**: Knocking out last-to-last attention in final layers *increases* correct-token probability to ~1 (Figure 2)—unexpected, possibly indicating self-attention suppression improves focus
  - **Transformer first-token bias**: GPT-2, Llama, Mistral show strong first-position dependence; Mamba models do not
  - **Small model oversensitivity**: 130M models show exaggerated knockout effects—use smaller windows (3-5) for better resolution

- First 3 experiments:
  1. **Replicate subject-to-last knockout across window sizes**: Run with WS ∈ {1, 3, 5, 9, 12, 15} on Mamba-1 2.8B to verify late-intermediate peak location is stable
  2. **Feature knockout ablation**: Compare all-features vs context-dependent-only vs context-independent-only knockout from subject tokens; quantify effect size ratio
  3. **Cross-architecture comparison on same data**: Run subject knockout on GPT-2, Llama-3, Mistral, Mamba-1, Mamba-2 using the identical 672-triplet subset; plot layer-wise effect curves to confirm universal vs architecture-specific patterns

## Open Questions the Paper Calls Out

- **Question:** Why does blocking the information flow from the last token to itself in the final layers cause a sharp increase in correct-token probability in Mamba-1 and Falcon-Mamba?
  - **Basis in paper:** [explicit] The authors observe a "marked increase in the correct-token probability" and state they "defer a deeper exploration of its implications to future work."
  - **Why unresolved:** The authors characterize this result as "surprising" and currently lack a mechanistic explanation for why inhibiting self-attention improves factual recall accuracy.
  - **What evidence would resolve it:** An analysis of the residual stream or specific attention heads during this intervention to determine if the last token normally carries inhibitory noise that is removed by the knockout.

- **Question:** Do distinct architectures like Transformers and Mamba converge to similar factual information flow patterns because of shared inductive biases?
  - **Basis in paper:** [explicit] The limitations section states the methods "do not shed light on why both Transformers and Mamba models converge to these similar patterns" and calls for explaining the "inductive-biases."
  - **Why unresolved:** The paper establishes the empirical parallel regarding subject token importance but does not investigate the theoretical drivers behind this convergence.
  - **What evidence would resolve it:** Theoretical analysis linking the optimization landscapes of SSMs and Attention, or experiments tracking the emergence of these patterns during pre-training on identical data.

- **Question:** Does selectively fine-tuning only context-dependent features enable more efficient training in SSM-based architectures?
  - **Basis in paper:** [explicit] The authors suggest "future work could potentially investigate whether selectively fine-tuning context-dependent features alone enables more efficient training."
  - **Why unresolved:** While the paper identifies that these features facilitate token-to-token exchange, it does not test their utility as isolated targets for parameter updates.
  - **What evidence would resolve it:** Comparative fine-tuning experiments measuring convergence speed and performance retention when updating only context-dependent features versus all parameters.

## Limitations

- The theoretical connection between Mamba's SSM operations and attention mechanisms may not fully capture the complexity of factual information flow, particularly the role of token-independent operations like gating and convolution.
- The fixed 1/3 threshold for classifying features as context-dependent versus context-independent assumes a clean separation that may not exist in practice, potentially masking important sub-categories or continuous relationships.
- The findings based on the COUNTERFACT dataset's filtered subset of 672 correctly-predicted examples may not generalize to broader factual recall tasks or failure modes.

## Confidence

- **High Confidence** in the empirical observation that subject tokens play a critical role in factual recall at late-intermediate layers across architectures. The cross-model consistency (Mamba-1, Mamba-2, GPT-2, Llama, Mistral) using a common dataset subset provides strong evidence for this pattern.
- **Medium Confidence** in the mechanism explaining why this occurs through context-dependent features with slow decay. While the theoretical framework and experimental evidence are compelling, the fixed threshold for feature classification and the assumption that L₁ norm decay is the primary determinant of functional role remain open questions.
- **Medium Confidence** in the attention interpretation of Mamba's SSM operations. The mathematical equivalence between kernel representation (Mamba-1) and SSD framework (Mamba-2) with attention is theoretically sound, but the practical fidelity of this interpretation for capturing all relevant information flow pathways requires further validation.

## Next Checks

1. **Cross-task generalization test**: Apply the subject-token knockout methodology to non-attribute-prediction factual recall tasks (e.g., question answering, multi-hop reasoning) to determine if the late-intermediate layer pattern is task-specific or universal across factual recall.

2. **Feature decay continuum analysis**: Instead of the binary 1/3 classification, analyze the continuous relationship between feature decay rate (‖Ā‖₁) and knockout impact across the full spectrum of features to identify potential sub-clusters or non-linear relationships.

3. **Token-independent operation ablation**: Design interventions that specifically target Mamba's gating and convolution operations (beyond the attention interpretation) to quantify their contribution to factual information flow and determine if the attention framework captures the complete picture.