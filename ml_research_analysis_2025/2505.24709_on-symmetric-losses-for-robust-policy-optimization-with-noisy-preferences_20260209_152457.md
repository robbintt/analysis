---
ver: rpa2
title: On Symmetric Losses for Robust Policy Optimization with Noisy Preferences
arxiv_id: '2505.24709'
source_url: https://arxiv.org/abs/2505.24709
tags:
- reward
- preference
- policy
- noise
- symmetric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a robust framework for policy optimization
  from noisy human preferences. The key idea is to view reward modeling as binary
  classification, enabling the use of symmetric losses known for their robustness
  to label noise.
---

# On Symmetric Losses for Robust Policy Optimization with Noisy Preferences

## Quick Facts
- arXiv ID: 2505.24709
- Source URL: https://arxiv.org/abs/2505.24709
- Authors: Soichiro Nishimori; Yu-Jie Zhang; Thanawat Lodkaew; Masashi Sugiyama
- Reference count: 40
- Key outcome: Symmetric losses (ramp, sigmoid) significantly outperform standard approaches under high noise levels, achieving higher reward accuracy and win rates in policy evaluation

## Executive Summary
This paper proposes a principled framework for robust policy optimization under noisy human preferences by viewing reward modeling as a binary classification problem. The key insight is that symmetric loss functions, known for their robustness to symmetric label noise in classification, can preserve action rankings even under asymmetric noise when combined with antisymmetric scoring functions. This rank preservation is shown to be sufficient for policy improvement. The authors introduce Symmetric Preference Optimization (SymPO), a novel offline preference optimization method based on this insight, and demonstrate its effectiveness through experiments on synthetic MNIST preference data and real-world language model alignment tasks.

## Method Summary
The method reframes reward modeling from preference data as binary classification, where each preference pair (a1, a2) with label y ∈ {-1, +1} is treated as a classification example. The scoring function g(a1, a2) = r(a1) - r(a2) is optimized to minimize a symmetric loss function ℓ(y * g(a1, a2)). The authors prove that asymmetric noise in preferences is mathematically equivalent to symmetric noise under this formulation, making symmetric losses robust to the asymmetric noise typically found in human preference data. SymPO applies this framework as a drop-in modification to existing offline preference optimization methods, replacing standard losses with symmetric alternatives like ramp and sigmoid losses.

## Key Results
- Symmetric losses (ramp and sigmoid) significantly outperform standard approaches (logistic, hinge) under high noise levels (>40%) in both reward accuracy and win rates
- Theoretical proof that symmetric losses preserve action rankings under asymmetric noise, which is sufficient for policy improvement
- Empirical validation on synthetic MNIST preference data and real-world language model alignment tasks with Anthropic HH and UltraFeedback datasets
- Win rate improvements of up to 15% over baselines when noise rates exceed 30%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward modeling from human preference data can be reformulated as a binary classification task, enabling the application of classification theory and robust loss functions to policy optimization.
- Mechanism: A preference pair (a1, a2) with a label y ∈ {-1, +1} is treated as a binary classification example. The method optimizes a scoring function g(a1, a2) = r(a1) - r(a2), where r is the reward function, to minimize a loss function ℓ(y * g(a1, a2)). Minimizing this classification risk implicitly learns the reward function.
- Core assumption: The underlying preference model (e.g., Bradley-Terry) can be expressed such that the probability of preference is a function of the difference in rewards between actions.
- Evidence anchors:
  - [abstract] "We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem."
  - [section 2.2] "...the reward modeling problem can be cast as a binary classification task... In this study, we constrain g(a1, a2) to have the form of g(a1, a2) = r(a1) - r(a2)..."
  - [corpus] Corpus evidence is weak or missing on this specific reframing, validating it as a primary contribution.

### Mechanism 2
- Claim: Asymmetric noise in preference labels is mathematically equivalent to symmetric noise in the risk minimization objective, provided the scoring function is antisymmetric.
- Mechanism: Preference data has a unique structure where swapping actions flips the label. Because the scoring function is antisymmetric (g(a1, a2) = -g(a2, a2)), a "random flip" of action pairs allows asymmetric noise rates (εp, εn) to be transformed into a single symmetric noise rate. Thus, methods robust to symmetric noise are inherently robust to asymmetric noise.
- Core assumption: The scoring function g(a1, a2) must be antisymmetric, which is guaranteed by the formulation g(a1, a2) = r(a1) - r(a2).
- Evidence anchors:
  - [abstract] "...asymmetric and symmetric noise are equivalent in reward modeling..."
  - [section 3.1] "Lemma 1 (Equivalence of asymmetric and symmetric noise (Informal))... if the scoring function g is symmetric... then flipping the actions a1 and a2 does not change the value of the scoring function."
  - [corpus] Corpus evidence is weak or missing, reinforcing this theoretical equivalence as a key contribution.

### Mechanism 3
- Claim: Using symmetric loss functions for the classification objective provides theoretical robustness to label noise and guarantees a rank-preserving reward function, which is sufficient for policy improvement.
- Mechanism: Symmetric losses (ℓ(z) + ℓ(-z) = K), such as ramp and sigmoid losses, are known from classification theory to be robust to symmetric label noise. The paper proves that minimizing such a loss yields a reward function that is rank-preserving with respect to the true underlying reward. This rank preservation is shown to be a sufficient condition for improving the policy over a reference.
- Core assumption: The loss function must be classification-calibrated.
- Evidence anchors:
  - [abstract] "...symmetric losses preserve action rankings even under asymmetric noise, which is sufficient for policy improvement."
  - [section 4.2] "Theorem 2 (A classification-calibrated loss induces a rank-preserving reward)... the minimizer of the ℓ-risk... is rank-preserving concerning the true reward."
  - [corpus] Corpus evidence is weak or missing on this specific guarantee, confirming its novelty.

## Foundational Learning

- Concept: Binary Classification with Noisy Labels
  - Why needed here: The core innovation is reframing RL reward modeling as a noisy label classification problem and applying robust loss theory
  - Quick check question: How does a symmetric loss differ from a standard logistic loss in its robustness to label noise?

- Concept: Reward Modeling & Bradley-Terry Model
  - Why needed here: This is the standard method for learning a reward function from preferences, which this paper enhances
  - Quick check question: Under the Bradley-Terry model, what determines the probability of preferring one action over another?

- Concept: Offline Preference Optimization (DPO)
  - Why needed here: This is the policy optimization paradigm to which the proposed SymPO method is applied as a drop-in modification
  - Quick check question: How does Direct Preference Optimization (DPO) avoid the need for a separate reward model?

## Architecture Onboarding

- Component map: Input (noisy preference pairs) -> Model (policy with implicit reward) -> Objective (SymPO loss with symmetric function) -> Optimizer (standard policy optimization)

- Critical path: The most critical element is the selection and implementation of the symmetric loss function. The entire theoretical guarantee of robustness depends on using a loss that satisfies the symmetric condition and is classification-calibrated.

- Design tradeoffs:
  - Rank Preservation vs. Exact Reward: Symmetric losses are non-convex and do not recover exact reward magnitudes. They guarantee correct ranking only, which the paper proves is sufficient for policy improvement
  - Theory vs. Practice: Theoretical guarantees are asymptotic and assume instance-independent noise. Real-world data may violate these assumptions
  - Simplicity vs. Hyperparameters: Unlike noise-aware methods (e.g., rDPO), SymPO does not require the noise rate as a hyperparameter, making it simpler to deploy

- Failure signatures:
  - Degradation under high noise: Using standard losses (e.g., DPO's logistic loss) under high noise (>40%) will result in poor policy improvement
  - Incorrect Loss: Using a loss that is not symmetric or classification-calibrated will void the robustness and rank-preservation guarantees
  - Optimization Difficulty: Non-convex symmetric losses (e.g., ramp loss) may be more challenging to optimize than convex losses

- First 3 experiments:
  1. Loss Function Ablation: On a synthetic dataset (e.g., MNIST Preference) with injected noise, compare symmetric (ramp, sigmoid) vs. non-symmetric (logistic, hinge) losses. Plot test reward accuracy vs. noise rate to validate robustness
  2. Noise Equivalence Test: Train models on datasets with asymmetric and its theoretically equivalent symmetric noise. Compare empirical risk and learned rankings to validate the core Lemma 1
  3. End-to-End Alignment: Apply SymPO to a real-world LLM alignment task (e.g., Anthropic HH dataset). Inject synthetic label noise and compare win rates against baselines (DPO, rDPO) to demonstrate practical robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical guarantee for robust policy improvement be extended from the asymptotic regime to the finite-sample case?
- Basis in paper: [explicit] The authors state in the Limitations section: "Our theoretical guarantee for policy improvement holds in the asymptotic regime and does not fully extend to the finite-sample case."
- Why unresolved: The current theoretical analysis relies on infinite-data limits to prove that the risk minimizer is rank-preserving, which is a condition that does not strictly hold with a finite number of training samples
- What evidence would resolve it: A finite-sample error bound that quantifies the convergence rate of the SymPO policy towards the optimal policy under a fixed dataset size n

### Open Question 2
- Question: How can the SymPO framework be adapted to handle instance-dependent noise rather than assuming a uniform noise rate?
- Basis in paper: [explicit] The paper notes: "Our analysis assumes instance-independent noise, whereas real-world preference data often exhibit instance-dependent noise... extending our framework to accommodate instance-dependent noise is an important direction."
- Why unresolved: The current noise model (Eq. 7) assumes fixed error rates εp and εn across all action pairs, failing to account for the reality that annotators are more likely to err on ambiguous or similar pairs
- What evidence would resolve it: A modified theoretical framework proving rank preservation where the noise rates ε are functions of the input instances, validated by experiments on datasets with simulated instance-dependent noise

### Open Question 3
- Question: Is it possible to construct a symmetric loss function that also possesses Class Probability Estimation (CPE) properties to recover exact reward magnitudes?
- Basis in paper: [explicit] In Section 4.3, the authors state: "One may still inquire whether the exact reward values can be recovered... all symmetric losses are non-CPE, as shown in [7]."
- Why unresolved: While symmetric losses are robust to noise, their inability to perform CPE means they preserve action rankings but lose the magnitude information of the reward, which may be necessary for certain fine-grained control tasks
- What evidence would resolve it: The theoretical derivation of a loss function that satisfies both the symmetric condition (ℓ(z) + ℓ(-z) = K) and the CPE condition, or a proof demonstrating that such a combination is mathematically impossible

## Limitations

- Theoretical guarantees assume instance-independent noise and hold asymptotically, which may not capture real-world preference data characteristics
- Rank preservation guarantee is sufficient for policy improvement but does not recover exact reward magnitudes needed for calibrated reward functions
- Empirical evaluation relies on model-based win rate assessment rather than human verification, and the noise injection procedure may not fully represent complex real-world human preference noise

## Confidence

**High Confidence**: The theoretical framework connecting symmetric losses to robust policy optimization is sound and well-supported by classification theory. The equivalence between asymmetric and symmetric noise under antisymmetric scoring functions is mathematically rigorous.

**Medium Confidence**: The empirical demonstrations show consistent improvements under synthetic noise, but the real-world applicability depends on noise characteristics not fully characterized in the paper. The choice of symmetric losses (ramp and sigmoid) appears effective, but the sensitivity to other symmetric loss functions remains unexplored.

**Low Confidence**: The claim that rank preservation alone is sufficient for policy improvement in all practical scenarios is plausible but not exhaustively validated. The paper's ablation studies are limited, and the impact of different noise patterns on long-term policy performance is unclear.

## Next Checks

1. **Human Evaluation Validation**: Conduct human preference evaluations on the final policies trained with SymPO under high noise conditions to verify that the GPT-4o-mini win rate evaluation correlates with human judgments, particularly for cases where the noise rate exceeds 30%.

2. **Robustness to Different Noise Patterns**: Test SymPO's performance when the noise is instance-dependent (e.g., correlated with action difficulty or domain shift) rather than the instance-independent noise assumed in the theory, using both synthetic and real-world preference datasets.

3. **Ablation on Symmetric Loss Functions**: Systematically evaluate the performance of other symmetric loss functions (e.g., exponential loss, squared loss) under varying noise conditions to determine whether ramp and sigmoid are uniquely effective or represent a broader class of viable options.