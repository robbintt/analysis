---
ver: rpa2
title: 'LATTS: Locally Adaptive Test-Time Scaling'
arxiv_id: '2509.20368'
source_url: https://arxiv.org/abs/2509.20368
tags:
- latexit
- sha1
- base64
- latts
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LATTS, a method that adaptively allocates
  computation to different reasoning steps in language models using verifier feedback.
  Unlike prior work that treats problems uniformly, LATTS uses acceptance-rejection
  sampling to dynamically adjust effort based on step-level difficulty, allowing more
  trials for harder steps and fallback strategies (stop, max, backtrack, restart)
  when needed.
---

# LATTS: Locally Adaptive Test-Time Scaling

## Quick Facts
- **arXiv ID**: 2509.20368
- **Source URL**: https://arxiv.org/abs/2509.20368
- **Reference count**: 40
- **Primary result**: Adaptive computation allocation achieves superior accuracy-compute tradeoffs on MATH500 and AIME problems

## Executive Summary
LATTS (Locally Adaptive Test-Time Scaling) introduces a novel approach to optimizing language model reasoning by dynamically allocating computation based on step-level difficulty. Unlike traditional methods that treat problems uniformly, LATTS uses acceptance-rejection sampling with verifier feedback to determine when additional reasoning effort is warranted. The system can employ various fallback strategies including stopping, backtracking, or restarting when encountering particularly challenging steps. Empirical results demonstrate significant efficiency gains, achieving comparable performance to baselines while using up to 50× fewer tokens.

## Method Summary
LATTS operates by treating each reasoning step as a separate decision point for computational allocation. Using a verifier model to assess intermediate reasoning quality, the system determines whether to allocate additional computational resources to a given step. The core mechanism employs acceptance-rejection sampling to decide when to continue investing in a particular reasoning path versus when to employ fallback strategies. These strategies include stopping computation entirely for easy steps, taking maximum likelihood predictions when appropriate, backtracking to reconsider previous decisions, or restarting from scratch. This granular approach allows LATTS to concentrate computational resources where they're most needed, adapting to the varying difficulty levels within individual problems.

## Key Results
- Achieves superior accuracy-compute tradeoffs compared to beam search and BoN baselines
- Demonstrates up to 50× reduction in token usage while maintaining competitive performance
- Excels particularly on challenging problems like AIME, showing robustness to complex reasoning requirements
- Works effectively with general-purpose LLM verifiers, broadening practical applicability

## Why This Works (Mechanism)
LATTS works by recognizing that not all reasoning steps require equal computational investment. The method uses verifier feedback to identify steps that are likely to benefit from additional reasoning attempts. By employing acceptance-rejection sampling, LATTS can dynamically allocate more trials to harder steps while avoiding wasted computation on steps that are already on the right track. The fallback strategies provide robustness when initial reasoning attempts fail, allowing the system to either restart, backtrack, or take the most likely prediction when appropriate. This targeted approach ensures computational resources are concentrated where they yield the highest returns in terms of accuracy improvements.

## Foundational Learning

**Acceptance-Rejection Sampling**: A Monte Carlo method for generating samples from a distribution by rejecting samples that don't meet certain criteria. Needed to determine when additional computational resources should be allocated to a reasoning step. Quick check: Verify the acceptance probability is properly calibrated to the verifier's confidence scores.

**Test-Time Scaling**: The practice of allocating additional computational resources during inference to improve model performance. Needed as the broader framework within which LATTS operates. Quick check: Ensure scaling benefits plateau appropriately with increasing computation.

**Verifier Feedback Integration**: Using a separate model to assess the quality of intermediate reasoning steps. Needed to make informed decisions about computational allocation. Quick check: Validate verifier accuracy across different reasoning patterns and difficulty levels.

## Architecture Onboarding

**Component Map**: Problem -> Step Decomposition -> LATTS Allocator -> Verifier -> Fallback Strategy Selector -> Final Answer

**Critical Path**: The most computationally intensive path runs through the LATTS Allocator and Verifier for each reasoning step, as multiple forward passes may be required to evaluate different reasoning attempts before selecting the appropriate fallback strategy.

**Design Tradeoffs**: LATTS trades increased per-step computation (multiple verifier calls) for reduced total computation across the entire problem. This creates tension between latency requirements and accuracy gains, particularly for problems with many steps requiring intensive allocation.

**Failure Signatures**: Poor verifier quality leads to misallocation of resources (over-investing in easy steps, under-investing in hard ones). Uniform difficulty distribution across steps reduces LATTS' advantages. Excessive backtracking or restarting can indicate poor initial problem decomposition.

**First Experiments**:
1. Run LATTS on a simple arithmetic problem with known difficulty distribution to verify adaptive allocation behavior
2. Compare LATTS with beam search on a benchmark problem to measure token efficiency gains
3. Test LATTS with a deliberately weakened verifier to assess sensitivity to verifier quality

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance heavily dependent on verifier model quality, with poor verifiers leading to ineffective resource allocation
- Multiple forward passes per step may create latency issues for time-sensitive applications
- Limited benefits for problems with uniformly distributed difficulty across steps

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Effectiveness on MATH500 and AIME problems | High |
| Theoretical grounding of acceptance-rejection approach | High |
| General applicability to different problem domains | Medium |
| Computational efficiency (50× token reduction) | Medium |

## Next Checks
1. Test LATTS on diverse problem domains beyond mathematics, particularly in code generation and natural language inference, to validate cross-domain effectiveness claims.

2. Conduct ablation studies to quantify the individual contributions of the different fallback strategies (stop, max, backtrack, restart) to overall performance.

3. Evaluate the sensitivity of LATTS to verifier quality by testing with verifiers of varying accuracy levels and measuring the impact on final task performance.