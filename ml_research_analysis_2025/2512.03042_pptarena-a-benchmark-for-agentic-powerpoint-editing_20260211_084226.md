---
ver: rpa2
title: 'PPTArena: A Benchmark for Agentic PowerPoint Editing'
arxiv_id: '2512.03042'
source_url: https://arxiv.org/abs/2512.03042
tags:
- editing
- slide
- slides
- pptarena
- pptpilot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PPTArena introduces a benchmark for PowerPoint editing that measures
  reliable modifications to real slides under natural-language instructions. Unlike
  image-based or generation-focused approaches, it provides ground-truth decks, structured
  rubrics, and dual VLM judges for instruction fidelity and visual quality.
---

# PPTArena: A Benchmark for Agentic PowerPoint Editing

## Quick Facts
- arXiv ID: 2512.03042
- Source URL: https://arxiv.org/abs/2512.03042
- Reference count: 40
- Primary result: PPTPilot improves over strong proprietary agents and frontier VLM systems by more than 10 percentage points on compound, layout-sensitive, and cross-slide edits

## Executive Summary
PPTArena introduces a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. Unlike image-based or generation-focused approaches, it provides ground-truth decks, structured rubrics, and dual VLM judges for instruction fidelity and visual quality. PPTPilot, a structure-aware agent, decomposes edits into semantic operations and routes between high-level APIs and deterministic XML patching with iterative verification. On the benchmark, PPTPilot improves over strong proprietary agents and frontier VLM systems by more than 10 percentage points on compound, layout-sensitive, and cross-slide edits, while maintaining competitive performance on simpler cases. Despite these gains, all evaluated agents still struggle on long-horizon, document-scale tasks, indicating that robust PPT editing remains an open challenge.

## Method Summary
PPTArena is a benchmark for PowerPoint editing that provides 100 real-world editing cases across 2,125 slides and 800+ targeted edits. The benchmark uses dual VLM-as-Judge evaluation scoring 0–5 on Instruction Following (IF) and Visual Quality (VQ), with IF judge using structured JSON/XML diffs and VQ judge using rendered slide screenshots. PPTPilot is a dual-path agent that routes edits between high-level programmatic tools (python-pptx) and deterministic XML operations (OOXML patching) based on instruction analysis and presentation structure. The agent employs an iterative plan-edit-verify loop with up to three refinement iterations and uses GPT-5 as primary VLM executor and judge.

## Key Results
- PPTPilot achieves 2.84 IF and 3.21 VQ scores, improving over strong baselines by more than 10 percentage points on compound, layout-sensitive, and cross-slide edits
- The hybrid execution routing architecture provides significant benefits for complex edits requiring precise layout control
- All evaluated agents, including PPTPilot, underperform on long-horizon, document-scale tasks spanning 20+ slides
- Dual-judge evaluation protocol provides more reliable and interpretable assessment than single-judge approaches

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Execution Routing
- Claim: Routing edits between high-level APIs and deterministic XML patching improves controllability and reliability compared to single-modality approaches
- Mechanism: A "Skill Router" VLM analyzes the instruction and presentation structure, directing content-centric or repetitive tasks (e.g., translation, bulk updates) to a programmatic python-pptx path for efficiency, while routing fine-grained structural or layout modifications to a direct Office Open XML (OOXML) editing path for precision
- Core assumption: VLMs can accurately classify edit tasks and presentation structures to make optimal routing decisions, and the python-pptx library and OOXML schema are complete enough for the required operations
- Evidence anchors: [abstract] "...routes between high-level programmatic tools and deterministic XML operations for precise control..."; [Page 6, Section 4] "...PPTPilot's core design is a dual-path architecture..."

### Mechanism 2: Iterative Plan-Edit-Verify Loop
- Claim: An iterative loop of planning, editing, and verifying outputs against task constraints improves robustness on complex, long-horizon edits
- Mechanism: After an initial edit, a verifier model (a VLM) assesses the rendered output against the original instructions and ground-truth constraints. It provides feedback on failures, which the agent uses to refine its edits in subsequent iterations, typically up to three
- Core assumption: The verifier VLM can accurately detect subtle semantic and visual errors, and the agent can effectively interpret and act on the feedback
- Evidence anchors: [abstract] "...and verifies outputs through an iterative plan-edit-check loop against task-specific constraints."; [Page 7, Table 4] "Hybrid + Loop (3×): The full system with up to three refinement iterations..."

### Mechanism 3: Dual-Judge Evaluation Protocol
- Claim: Separately evaluating instruction following (IF) and visual quality (VQ) using specialized VLMs with distinct inputs provides a more reliable and interpretable assessment than a single judge
- Mechanism: The benchmark uses two VLM judges. The IF judge receives structured JSON/XML diffs to focus on semantic content correctness. The VQ judge receives rendered slide screenshots to focus on aesthetics and layout
- Core assumption: VLMs are capable judges when provided with focused, modality-appropriate contexts and detailed, per-sample style-target rubrics
- Evidence anchors: [Page 4, Section 3.3] "We instead measure an agent's performance on two fundamental axes: Instruction Following and Visual Quality..."; [Page 5, Section 3.3] "...we employ a dual-judge architecture that separately conducts the evaluation for instruction following and visual quality..."

## Foundational Learning

- Concept: Office Open XML (OOXML) Structure
  - Why needed here: The pptx file format is a zipped archive of XML files (e.g., slide1.xml, theme.xml). Understanding this structure is essential for the XML patching path and for interpreting the structured diffs used by the Instruction Following judge
  - Quick check question: What XML file within a .pptx archive would you need to modify to change a slide's master layout or theme colors?

- Concept: Python-pptx Library
  - Why needed here: This is the primary tool for the programmatic editing path. Understanding its capabilities and limitations is crucial for knowing which edits can be automated efficiently versus those requiring XML patching
  - Quick check question: Using python-pptx, how would you iterate through all shapes on a specific slide to find a text box with a certain name or content?

- Concept: VLM-as-Judge Limitations
  - Why needed here: The entire evaluation pipeline relies on VLMs. Understanding their potential biases (e.g., favoring certain layouts, misinterpreting text-image relationships) and context limitations is critical for designing robust evaluation protocols and interpreting results
  - Quick check question: Besides pixel similarity, what are two potential failure modes of a VLM judge when evaluating a slide's visual quality?

## Architecture Onboarding

- Component map: Skill Router (VLM) -> Programmatic Path (python-pptx) or XML Path (OOXML patching) -> Verifier (VLM) -> Refinement Loop (up to 3 iterations) -> Dual-Judge Evaluation (IF Judge, VQ Judge)

- Critical path: Receive instruction and source PPT -> Generate PPT JSON summary and screenshots -> Skill Router decides path (Prog/XML) and target slides -> Execute edit (generate code or patches) -> Verify edited PPT; if issues and loops remain, go to step 3 with feedback -> Final PPT is scored by PPTArena's dual-judge pipeline

- Design tradeoffs:
  - Complexity vs. Reliability: The hybrid system is more complex to build and maintain than a single-path agent but offers greater control and reliability
  - Cost vs. Accuracy: The iterative loop and dual-judge evaluation increase inference cost but improve performance and evaluation fidelity
  - Generalizability vs. Specialization: The current design is specialized for PPT. Generalizing to other Office formats (Word, Excel) would require new tool integrations and routing logic

- Failure signatures:
  - Routing Errors: Agent uses XML for a bulk text update (slow, error-prone) or python-pptx for a master slide edit (impossible/limited)
  - Verifier Hallucination: Loop fails to converge because verifier gives incorrect feedback
  - Judge Bias: Benchmark scores do not match human intuition; IF judge misses semantic errors or VQ judge is overly critical of minor style variations
  - Long-Horizon Drift: On multi-slide tasks, early edits create inconsistencies that propagate

- First 3 experiments:
  1. Ablate the Skill Router: Force all edits through the XML path, then all through the python-pptx path. Compare scores to the hybrid version to quantify the routing benefit
  2. Vary Refinement Loops: Run PPTPilot with 0, 1, 3, and 5 refinement loops. Plot performance vs. cost (inference time/API calls) to find the point of diminishing returns
  3. Judge Consistency Check: Have a third VLM (different from the judges) and human annotators score a subset of cases. Measure the correlation between all three to validate the dual-judge protocol

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can PPT editing agents reliably handle long-horizon, document-scale tasks spanning 20+ slides with cross-slide dependencies?
- Basis in paper: [explicit] The abstract and conclusion state "existing agents still underperform on long-horizon, document-scale tasks" and "agents still fail on complex, long-horizon, multi-modal tasks indicating that robust PPT editing remains far from solved"
- Why unresolved: All evaluated systems (including PPTPilot) show significant performance degradation on tasks with 10+ slides and multi-step dependencies; current plan-edit-verify loops don't scale
- What evidence would resolve it: An agent achieving IF/VQ scores above 4.0 on Structure and Interactivity categories (currently ~2.3–2.5) without substantial latency increases

### Open Question 2
- Question: Can conversational, multi-turn dialogue improve editing accuracy for under-specified user intent (e.g., "make this less cluttered")?
- Basis in paper: [explicit] The Limitations section states: "Future work should explore conversational refinement, where agents are evaluated not just on the final edit, but on their ability to ask clarifying questions, propose options, and engage in multi-turn dialogue"
- Why unresolved: PPTArena assumes fully specified instructions; real-world requests are ambiguous, but no benchmark currently measures clarifying-question behavior
- What evidence would resolve it: A benchmark extension measuring edit success after multi-turn clarification dialogues, showing improved IF scores relative to single-turn baselines

### Open Question 3
- Question: How can agents maintain semantic consistency when editing across heterogeneous file formats (Excel charts, Word documents, PPT decks)?
- Basis in paper: [explicit] Future work section: "An exciting frontier is extending the benchmark to support cross-application manifests, testing an agent's ability to maintain semantic consistency as it shuttles content between diverse file formats"
- Why unresolved: PPTArena is closed-environment; professional workflows require embedding live Excel charts or synthesizing Word content, which introduces synchronization and format-translation challenges
- What evidence would resolve it: A cross-application benchmark where agents must propagate edits across linked Office documents while preserving data integrity and visual formatting

## Limitations

- Evaluation reliability depends on VLM judges that may exhibit systematic biases toward certain layouts or misinterpret complex text-image relationships
- All evaluated agents, including PPTPilot, underperform on long-horizon, document-scale tasks spanning 20+ slides with cross-slide dependencies
- The reliance on GPT-5 (not publicly available) for core components creates a reproducibility gap that could affect validation of the claimed performance gains

## Confidence

- High Confidence: The hybrid execution routing architecture is well-specified and technically sound. The dual-judge evaluation protocol is clearly defined with explicit scoring criteria
- Medium Confidence: Performance improvements over baseline agents are reported but depend on GPT-5 availability. The iterative refinement loop's effectiveness is demonstrated but could be sensitive to verifier quality
- Low Confidence: Generalization claims to other Office formats are speculative without implementation details. Long-horizon task limitations suggest fundamental scalability issues not yet resolved

## Next Checks

1. Router Classification Validation: Implement the Skill Router using an available VLM (GPT-4o) and measure its classification accuracy on a held-out set of edit instructions. Compare routing decisions against human expert judgment to quantify potential performance degradation
2. Judge Consistency Study: Have human experts score 20 sample cases from PPTArena using the same IF/VQ rubrics. Compute inter-annotator agreement (Cohen's kappa) and correlation with VLM judge scores to validate the evaluation protocol
3. Cross-Format Generalization Test: Adapt the dual-path architecture to handle Word document editing (using python-docx and OOXML). Evaluate on a small set of Word editing tasks to assess the framework's extensibility beyond PowerPoint