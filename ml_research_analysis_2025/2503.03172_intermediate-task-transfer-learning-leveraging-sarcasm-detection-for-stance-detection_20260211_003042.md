---
ver: rpa2
title: 'Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for Stance
  Detection'
arxiv_id: '2503.03172'
source_url: https://arxiv.org/abs/2503.03172
tags:
- sarcasm
- detection
- stance
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores sarcasm detection as an intermediate transfer
  learning task to enhance stance detection on social media. The authors propose a
  model combining BERT or RoBERTa with convolutional and BiLSTM layers, pre-trained
  on sarcasm detection before fine-tuning on stance detection.
---

# Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for Stance Detection

## Quick Facts
- arXiv ID: 2503.03172
- Source URL: https://arxiv.org/abs/2503.03172
- Reference count: 40
- Primary result: Sarcasm detection pre-training improves stance detection F1-scores by 0.038-0.053

## Executive Summary
This paper explores sarcasm detection as an intermediate transfer learning task to enhance stance detection on social media. The authors propose a model combining BERT or RoBERTa with convolutional and BiLSTM layers, pre-trained on sarcasm detection before fine-tuning on stance detection. Experiments on two datasets show the model outperforms state-of-the-art baselines, even without sarcasm pre-training, with average F1-score improvements of 0.038 and 0.053. Incorporating sarcasm knowledge boosts performance by correctly predicting 85% of previously misclassified sarcastic texts, highlighting the importance of intermediate-task transfer learning for nuanced language understanding in stance detection.

## Method Summary
The proposed framework uses BERT/RoBERTa as the base encoder, followed by convolutional and bidirectional LSTM layers before a dense classification layer. The model undergoes two-stage training: first pre-trained on sarcasm detection as an intermediate task, then fine-tuned on stance detection. The architecture employs dropout for regularization and class weights to handle imbalance. Experiments use macro-F1 for InFavor and Against classes only, evaluated across multiple targets in two datasets (SemEval 2016 Task 6A and MPCHI). Different sarcasm datasets are tested as intermediate tasks, with SARCTwitter (Twitter-sourced) showing optimal transfer performance.

## Key Results
- Sarcasm pre-training improves stance detection F1-scores by 0.038 (SemEval) and 0.053 (MPCHI) on average
- The model correctly predicts 85% of texts misclassified by models without sarcasm pre-training
- BERT+Conv+BiLSTM architecture outperforms BERT alone by 0.060 F1 on SemEval
- Dataset alignment (ST > SARC > SaV2C) matters more than dataset size for intermediate tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sarcasm pre-training corrects stance inversions where surface sentiment contradicts actual stance
- Mechanism: Sarcasm uses positive words to convey negative meaning. Models trained without this knowledge classify sarcastic "Against" texts as "InFavor" based on surface-level positive content. Pre-training on sarcasm detection teaches the model to recognize when literal meaning should be inverted
- Core assumption: Sarcasm in stance detection follows similar patterns to general sarcasm detection
- Evidence anchors: 85% correction rate for misclassified sarcastic samples; most sarcastic "Against" texts misclassified as "InFavor" due to positive content
- Break condition: If target task contains minimal sarcastic content, sarcasm pre-training provides negligible gains

### Mechanism 2
- Claim: Conv+BiLSTM layers on top of BERT/RoBERTa capture sequential stance patterns better than pooling alone
- Mechanism: Convolutional layers extract local n-gram patterns indicative of stance phrases; BiLSTM captures bidirectional context to model longer-range dependencies between stance-indicative tokens and the target
- Core assumption: Stance signals manifest in sequential word patterns, not just token-level semantics
- Evidence anchors: BERT+Conv+BiLSTM improves SemEval F1 from 0.665 to 0.725; related work shows similar architecture benefits for sarcasm detection
- Break condition: If stance is encoded primarily in individual tokens rather than sequences, simpler pooling may suffice

### Mechanism 3
- Claim: Intermediate-task transfer success depends on lexical and domain alignment between pre-training and target datasets
- Mechanism: Datasets sharing sentence length distributions, vocabulary, and source domain (e.g., both from Twitter) provide transferable linguistic patterns. Mismatched datasets introduce noise through irrelevant features
- Core assumption: Linguistic similarity correlates with transfer learning utility
- Evidence anchors: ST (Twitter-sourced) outperforms SARC and SaV2C for Twitter stance targets; sentence length similarity correlates with transfer success
- Break condition: If intermediate-task dataset is substantially larger or more diverse than target, domain mismatch may be overcome through scale

## Foundational Learning

- **Intermediate-Task Transfer Learning**: The core technique—pre-training on sarcasm detection before fine-tuning on stance detection. Requires understanding that knowledge from one task can improve performance on a related task through weight initialization
  - Quick check: Can you explain why pre-training on sentiment analysis might help sarcasm detection, and why this analogy extends to sarcasm→stance?

- **Stance Detection**: The target task being optimized. Stance (InFavor/Against/None) differs from sentiment; a negative tweet can be "InFavor" of a position if it criticizes the opposition
  - Quick check: Given "This politician's corruption is exactly why we need term limits"—what's the stance on "term limits"? What's the sentiment?

- **Sarcasm as Semantic Inversion**: Understanding that sarcasm detection works because it learns to identify when surface meaning contradicts intended meaning—the same mechanism that causes stance misclassification
  - Quick check: Why would "Great job, politicians!" be harder for a model to classify than "Politicians are doing great work"?

## Architecture Onboarding

- **Component map**: Input → BERT/RoBERTa tokenizer → BERT/RoBERTa (768-dim) → Conv1D(16 filters, kernel=3, ReLU) → BiLSTM(768) → Dropout(0.25) → Dense(3 outputs, softmax)

- **Critical path**:
  1. Select intermediate-task dataset: ST (Twitter-sourced) recommended for social media targets; verify sentence length distribution matches target
  2. Pre-train: Fine-tune full model on sarcasm detection binary classification
  3. Transfer: Initialize stance model with sarcasm-pretrained weights
  4. Fine-tune: Train on stance detection with class weights for imbalance

- **Design tradeoffs**:
  - ST dataset (350 sarcastic / 644 non-sarcastic) is small but well-aligned; SARC (1M+ samples) is large but introduces noise. Paper shows alignment > scale for this task
  - BERT vs. RoBERTa: BERT performed better overall (0.725 vs 0.712 avg F1 on SemEval without pre-training). RoBERTa slightly better on CC target only
  - Conv+BiLSTM vs. pooling: BiLSTM after Conv outperformed pooling in ablation—worth the added parameters

- **Failure signatures**:
  - Performance drop after sarcasm pre-training → Intermediate-task dataset mismatched (check domain, sentence length, vocabulary overlap)
  - High variance across runs → Increase dropout, reduce learning rate, add early stopping patience
  - "None" class dominates predictions → Verify class weights applied; check training set balance

- **First 3 experiments**:
  1. **Baseline**: Train BERT+Conv+BiLSTM on target stance task directly (no intermediate pre-training). Record per-class F1. Compare to Table II values
  2. **Ablation**: Remove Conv, remove BiLSTM, remove both. Quantify each component's contribution using Table IV as reference
  3. **Dataset alignment test**: Pre-train on ST vs. SARC vs. SaV2C. Measure transfer gap. Verify ST > others for Twitter-sourced stance targets. Check sentence length statistics for each pairing

## Open Questions the Paper Calls Out

- **Cross-target SD**: How does the framework perform on cross-target Stance Detection? [explicit] The authors list "cross-target SD for both tasks" as a specific focus for future research
- **Other intermediate tasks**: How does sarcasm detection compare to sentiment or emotion classification as an intermediate task? [explicit] The conclusion notes that future research will include "a more comprehensive examination of other intermediate tasks, including sentiment and emotion knowledge"
- **Domain-specific PLMs**: Can domain-specific pre-trained language models improve performance on specialized Stance Detection tasks like MPCHI? [explicit] The authors state, "Future investigations will assess variant BERT or RoBERTa embeddings tailored to health-related text data for the MPCHI task"

## Limitations

- SARCTwitter dataset contains only 994 samples, raising questions about scalability of transfer benefits
- Analysis focuses on macro-F1 for InFavor and Against classes only, leaving "None" class performance unmeasured
- Domain-specific effects unclear—benefits for non-social media domains like MPCHI health texts are less pronounced

## Confidence

- **High Confidence**: Sarcasm pre-training improves stance detection F1-scores (0.038-0.053); mechanism of semantic inversion correction is well-supported
- **Medium Confidence**: Conv+BiLSTM contributions are supported by ablation results but lack detailed error analysis; dataset alignment hypothesis is empirically validated but not theoretically grounded
- **Low Confidence**: Claims about general utility across domains are based on only two datasets; comparison to baselines doesn't establish whether simpler approaches would achieve similar results

## Next Checks

1. **Dataset Scaling Experiment**: Replicate using larger sarcasm detection datasets (e.g., full SARC corpus) to determine if transfer benefits scale with pre-training data size
2. **Component Isolation Test**: Conduct controlled ablation experiments that separately measure contributions of sarcasm pre-training, Conv layers, BiLSTM layers, and BERT initialization
3. **Domain Generalization Study**: Apply the same transfer learning pipeline to a non-social media stance detection dataset (e.g., parliamentary debates, news editorials) to test domain-specific effects