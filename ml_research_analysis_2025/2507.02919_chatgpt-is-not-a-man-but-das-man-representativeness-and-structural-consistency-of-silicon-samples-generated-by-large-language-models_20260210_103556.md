---
ver: rpa2
title: 'ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency
  of Silicon Samples Generated by Large Language Models'
arxiv_id: '2507.02919'
source_url: https://arxiv.org/abs/2507.02919
tags:
- llms
- accuracy
- these
- data
- abortion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  reliably simulate human opinions by examining structural consistency and homogenization
  in LLM-generated responses. The authors tested GPT-4 and Meta's Llama 3.1 series
  (8B, 70B, 405B) using questions on abortion and immigration from ANES 2020, comparing
  responses across demographic subgroups.
---

# ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models

## Quick Facts
- **arXiv ID:** 2507.02919
- **Source URL:** https://arxiv.org/abs/2507.02919
- **Authors:** Dai Li; Linzhuo Li; Huilian Sophie Qiu
- **Reference count:** 13
- **Key outcome:** LLMs fail to simulate human opinions reliably due to structural inconsistency (aggregation errors) and severe homogenization (underrepresentation of minority views).

## Executive Summary
This study reveals fundamental limitations in using large language models (LLMs) as substitutes for human survey data. Testing GPT-4 and Llama 3.1 models (8B, 70B, 405B parameters) against ANES 2020 data on abortion and immigration, the authors find that LLM responses exhibit structural inconsistency—response accuracy fails to hold across different levels of demographic aggregation—and severe homogenization where minority opinions are dramatically underrepresented. The authors propose an "accuracy-optimization hypothesis" suggesting these failures stem from models prioritizing modal responses over diverse representation, causing LLMs to function more like "Das Man" (the crowd) than individual voices. These findings challenge the validity of using LLMs to simulate human opinions and inform policy decisions.

## Method Summary
The authors evaluated GPT-4 and Llama 3.1 models using questions from ANES 2020 on abortion and immigration. They constructed 395 unique demographic personas from four variables (sex, race, education, religion) and queried each model for response probabilities using log-prob extraction. For GPT-4, they used system prompts with persona information and extracted `logprobs` for answer tokens. For Llama models, they employed forced-completion scoring with Together AI API. They computed accuracy by comparing model distributions to ANES ground truth and measured homogenization using variation ratio. Structural consistency was tested by comparing aggregated fine-grained responses against directly-queried coarse-grained responses at different demographic aggregation levels.

## Key Results
- **Structural inconsistency:** LLM response accuracy fails to hold across demographic aggregation levels—responses at fine-grained demographic levels do not reliably aggregate to match coarser-level responses (80% of subgroups for GPT-4 show VR<0.05).
- **Severe homogenization:** Minority opinions are dramatically underrepresented, with variation ratios indicating extreme compression of response diversity compared to ANES baseline.
- **Accuracy-optimization mechanism:** Homogenization stems from models prioritizing modal responses, where maximizing expected accuracy mathematically requires always predicting the most probable outcome.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs exhibit structural inconsistency—response distributions at fine-grained demographic levels do not aggregate reliably to coarser levels.
- **Mechanism:** Unlike human survey data, which maintains "closure under aggregation" because statistics derive from individual observations, LLMs generate each response from an independent conditional distribution. When prompted with a 4-variable persona, the model does not query an internal sample—it constructs a response based on the modal pattern associated with that specific prompt, without ensuring consistency with responses to related but differently-granular prompts.
- **Core assumption:** LLMs lack an internally coherent joint distribution over demographic attributes and opinions; they approximate conditional distributions independently at each query.
- **Evidence anchors:**
  - [abstract]: "failure in structural consistency, where response accuracy doesn't hold across demographic aggregation levels"
  - [section, Theoretical Framework]: "Achieving structural consistency implies that the model possesses a property akin to 'closure under aggregation'... This superficial accuracy at one level provides no guarantee about representativeness at finer granularities"
  - [corpus]: Related work on demographic-aligned LLMs (arxiv 2601.15755) evaluates marginal distributions but does not explicitly test cross-level aggregation consistency.
- **Break condition:** If LLMs maintained a consistent latent sample or joint distribution, aggregating fine-grained conditional outputs would match directly-queried coarser-grained outputs. Empirical results show systematic divergence (Figure 5: accuracy lines fail to overlap across aggregation levels).

### Mechanism 2
- **Claim:** Homogenization—the underrepresentation of minority opinions—arises from models' accuracy-optimization objective, not merely biased training data.
- **Mechanism:** The authors formalize this mathematically: when predicting a user's belief with unknown ground truth, expected accuracy is maximized by always outputting the modal belief (the most probable response). The utility function U(q) = Σpᵢ × qᵢ is maximized when q₁ = 1 (all probability mass on the mode). This creates a structural incentive toward homogenization independent of training data composition.
- **Core assumption:** Training objectives (next-token prediction, RLHF) implicitly reward predicting the most probable completion, which correlates with modal opinions in training corpora.
- **Evidence anchors:**
  - [abstract]: "homogenization, an underrepresentation of minority opinions... suggesting homogenization stems from prioritizing modal responses"
  - [section, Results]: "GPT4 (panel b) and Llama405B (panel c) exhibit much lower VRs across most subgroups, with P(VR<0.05) at 0.8 for both—indicating extreme homogenization in 80% of subgroups"
  - [corpus]: No direct corpus evidence testing the accuracy-optimization hypothesis specifically; related work (Boelaert et al. 2025, cited in paper) documents compressed variance but does not formalize the mechanism.
- **Break condition:** If homogenization were purely a data bias issue, adjusting training data distributions should restore variance. If accuracy-optimization is causal, variance suppression should persist even with balanced data unless the training objective changes.

### Mechanism 3
- **Claim:** LLMs function as "Das Man" (Heidegger's "the They")—generating stereotypical crowd-consensus responses rather than simulating diverse individuals.
- **Mechanism:** When conditioned on a demographic persona, the model infers the most probable utterance for that persona-type based on aggregated patterns in training text. This produces what "one says" (the anonymous, undifferentiated discourse of a group) rather than sampling from the genuine within-group diversity of actual individuals who hold idiosyncratic beliefs.
- **Core assumption:** LLMs encode correlational patterns between demographic descriptors and opinion expressions from training text; they do not maintain coherent individual-level samples.
- **Evidence anchors:**
  - [abstract]: "they function more like 'Das Man' (the crowd) than individual voices"
  - [section, Conclusion]: "LLM is thus more like an echo of the mass opinion... It articulates what 'one says' or what is generally considered plausible within a given context, rather than an opinion rooted in personal conviction"
  - [corpus]: Weak direct support; corpus papers focus on behavioral comparisons (Bayesian reasoning, conformity) rather than philosophical framing of representativeness.
- **Break condition:** If models maintained coherent individual-level latent variables, repeated queries with identical personas would show consistent but diverse responses across simulation instances. Instead, models show high homogenization across runs.

## Foundational Learning

- **Concept:** Next-token prediction with log-probability outputs
  - **Why needed here:** The study uses log probabilities (independent of sampling parameters like temperature) to recover the model's true conditional distribution, not just stochastic samples.
  - **Quick check question:** If you query an LLM with temperature=0 vs temperature=1, do the underlying log probabilities of tokens change?

- **Concept:** Aggregation consistency in survey sampling
  - **Why needed here:** Understanding why human survey statistics naturally aggregate consistently (same individuals observed at all levels) vs. LLM outputs (independently generated conditionals).
  - **Quick check question:** In a real survey, if 60% of women support policy X and 40% oppose, what must be true about the percentages within any subgroup of women (e.g., Black women)?

- **Concept:** Mode-seeking vs. distribution-matching objectives
  - **Why needed here:** The accuracy-optimization hypothesis rests on the mathematical result that maximizing expected accuracy = always predicting the mode, which differs from objectives that preserve distributional diversity.
  - **Quick check question:** If a classifier predicts class probabilities [0.55, 0.45] and ground truth is [0.55, 0.45], what is the expected accuracy under argmax prediction vs. probabilistic sampling?

## Architecture Onboarding

- **Component map:** Persona prompt construction -> Log-probability extraction -> Aggregation pipeline
- **Critical path:**
  1. Define demographic variable combinations (395 unique personas in this study).
  2. Query each persona × question combination; extract normalized probability distribution over response options.
  3. Compute accuracy (dot product with ANES ground truth) and variation ratio (1 – modal probability).
  4. Test structural consistency: aggregate 4-var → 3-var → 2-var → 1-var and compare to directly-queried distributions at each level.
- **Design tradeoffs:**
  - **Log-probability access vs. sampling:** Log-probs reveal the model's true distribution but require API support; sampling is universally available but introduces stochasticity.
  - **Persona granularity vs. subgroup size:** Finer personas enable intersectional analysis but reduce ANES sample sizes per subgroup, increasing ground-truth noise.
  - **Accuracy vs. diversity metrics:** Reporting accuracy alone can mask homogenization; always pair with variation ratio or similar diversity metrics.
- **Failure signatures:**
  - Aggregation mismatch: When aggregating fine-grained outputs to a coarser level produces different distributions than directly querying the coarser level.
  - Extreme homogenization: Variation ratio < 0.05 across >70% of subgroups (80% of subgroups for GPT-4 on abortion in this study).
  - Unweighted vs. weighted accuracy divergence: Large gaps indicate accuracy is uneven across population-proportional vs. equal-weighted subgroups.
- **First 3 experiments:**
  1. **Replicate structural consistency test:** Query model at 1-var, 2-var, 3-var, 4-var granularity for a single topic; aggregate and plot distributions. Expect divergence if mechanism holds.
  2. **Homogenization baseline:** Compare model variation ratio distribution to ANES baseline using identical subgroup definitions. Quantify P(VR < threshold).
  3. **Synthetic variation intervention:** For a subset of personas, query at multiple granularity levels and aggregate weighted responses; test whether this increases variation ratio compared to direct single-granularity queries (as suggested in "Synthetic variation" section).

## Open Questions the Paper Calls Out
None

## Limitations
- **Ground truth uncertainty:** ANES data represents ground truth without error, though small subgroup sizes create noisy estimates that could inflate perceived LLM inconsistencies.
- **Limited generalizability:** The study examines only two survey topics (abortion and immigration), limiting generalizability to other domains where opinion distributions or social desirability effects may differ.
- **Hypothesis validation:** The accuracy-optimization hypothesis remains mathematically compelling but lacks direct empirical testing against alternative explanations such as training data imbalances or architecture-level variance suppression.

## Confidence
- **High confidence** in empirical findings of structural inconsistency and homogenization across multiple models and aggregation levels, supported by clear statistical patterns (Figure 5, VR distributions).
- **Medium confidence** in the accuracy-optimization hypothesis as the primary driver of homogenization, given strong theoretical motivation but limited direct empirical validation against alternative mechanisms.
- **Medium confidence** in the "Das Man" philosophical framing, which provides useful interpretive context but extends beyond the empirical scope of the study.

## Next Checks
1. **Cross-topic generalization test:** Replicate the structural consistency and homogenization analyses using ANES questions from non-political domains (e.g., consumer preferences, health behaviors) to determine whether observed patterns generalize beyond the original two topics.

2. **Intervention experiment for accuracy-optimization hypothesis:** Create a synthetic dataset where the "true" distribution matches ANES but includes artificially inflated minority opinion frequencies. Test whether models still exhibit homogenization under these conditions, distinguishing between data bias and accuracy-maximization effects.

3. **Aggregation consistency robustness check:** For a subset of personas, generate responses at multiple granularity levels (fine-grained and coarse-grained), then aggregate the fine-grained responses using population weights and compare directly to coarse-grained responses. This tests whether weighted aggregation can partially restore structural consistency as suggested in the "Synthetic variation" discussion.