---
ver: rpa2
title: Fixed-Budget Change Point Identification in Piecewise Constant Bandits
arxiv_id: '2501.12957'
source_url: https://arxiv.org/abs/2501.12957
tags:
- change
- point
- budget
- then
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the problem of locating a single change point\
  \ in a piecewise constant reward function under bandit feedback, given a fixed budget\
  \ of queries. The authors establish lower bounds for both small and large budget\
  \ regimes, showing a separation in difficulty between them, and develop algorithms\u2014\
  Sequential Halving (SH) and Sequential Halving with Backtracking (SHB)\u2014that\
  \ are nearly minimax optimal in their respective regimes."
---

# Fixed-Budget Change Point Identification in Piecewise Constant Bandits

## Quick Facts
- arXiv ID: 2501.12957
- Source URL: https://arxiv.org/abs/2501.12957
- Reference count: 40
- Key outcome: Establishes lower bounds for fixed-budget change point identification and designs algorithms (SH, SHB, SHA) that are nearly minimax optimal in both small and large budget regimes

## Executive Summary
This paper studies the problem of locating a single change point in a piecewise constant reward function under bandit feedback, given a fixed budget of queries. The authors establish lower bounds for both small and large budget regimes, showing a separation in difficulty between them, and develop algorithms—Sequential Halving (SH) and Sequential Halving with Backtracking (SHB)—that are nearly minimax optimal in their respective regimes. SHB is better suited for large budgets and achieves an error probability of exp(-Δ²T/(600σ²) + 13log(1/2η)), while SH is better for small budgets with an error of exp(-TΔ²/(36σ²log²(1/2η))). A regime-adaptive algorithm, SHA, is proposed that performs near-optimally across both regimes simultaneously. The work is supported by theoretical analysis and experiments showing superior performance compared to existing methods, particularly when the change point is near the boundary of the action space.

## Method Summary
The paper proposes three algorithms for fixed-budget change point identification in piecewise constant bandits. Sequential Halving (SH) works by iteratively sampling three points and eliminating the half where the empirical mean difference is smaller, requiring correct decisions in all phases. Sequential Halving with Backtracking (SHB) extends this by sampling five points (including endpoints) and introducing a backtracking criterion that allows recovery from elimination mistakes, requiring correct decisions in only a majority of phases. The regime-adaptive algorithm SHA estimates the change magnitude from initial endpoint samples and chooses between SH and SHB based on a threshold. All algorithms aim to minimize the failure probability of returning an estimate within tolerance η of the true change point after T queries.

## Key Results
- SH achieves error probability O(exp(-TΔ²/(36σ²log²(1/2η)))) in the small-budget regime
- SHB achieves error probability O(exp(-Δ²T/(600σ²) + 13log(1/2η))) in the large-budget regime
- SHA achieves error probability matching the better of SH and SHB across both regimes
- SHB and SHA are robust to change points near boundaries, unlike existing methods

## Why This Works (Mechanism)

### Mechanism 1: Sequential Halving for Small Budgets (SH)
- Claim: In the small-budget regime, a simple sequential halving strategy without backtracking achieves near-optimal error probability.
- Mechanism: SH partitions the budget into J = ⌈log₂(1/2η)⌉ phases. In each phase, it samples three points (left, mid, right) of the remaining interval and eliminates the half where empirical mean difference is smaller, iteratively narrowing the interval containing the change point.
- Core assumption: The learner must make the correct elimination decision in every phase; one mistake propagates and causes failure. This makes the error probability scale multiplicatively with the number of phases.
- Evidence anchors:
  - [Section 5] "In SH, we split our budget up into J = ⌈log₂(1/2η)⌉ phases... we need to eliminate the correct half of the action space in every phase."
  - [Theorem 3] Error bound includes TΔ²/(36σ²log²(1/2η)), showing multiplicative dependence on η.
  - [corpus] Related work on fixed-budget best arm identification (e.g., Carpentier and Locatelli, 2016) uses similar elimination principles but in discrete action spaces.
- Break condition: If the budget is too small to allocate at least one sample per action per phase (T < 3⌈log₂(1/2η)⌉), the algorithm cannot proceed.

### Mechanism 2: Sequential Halving with Backtracking for Large Budgets (SHB)
- Claim: In the large-budget regime, backtracking enables near-optimal performance by allowing recovery from elimination mistakes.
- Mechanism: SHB also phases the budget but samples five points per phase (including endpoints 0 and 1) and introduces a backtracking criterion (EP,j) that detects when the change point lies outside the current interval. If triggered, it undoes the previous elimination, zooming out to re-expand the search space.
- Core assumption: The learner need only make correct decisions in a majority (≥3/4) of phases, not all phases. This reduces the multiplicative penalty of η to an additive one.
- Evidence anchors:
  - [Section 4] "If EP,j holds then we suspect that x* ∉ [aⱼ¹, aⱼ³) so we believe the change point is elsewhere... we 'zoom out' one step."
  - [Theorem 1] Error bound shows η appearing additively: exp(-Δ²T/(600σ²) + 13log(1/2η)).
  - [corpus] Similar backtracking ideas appear in Monotonic Thresholding Bandits (Cheshire et al., 2021), but there all observations are compared to a known threshold.
- Break condition: If the budget is insufficient for meaningful backtracking (T ≤ 60log(1/2η)), the algorithm may not have enough samples to make reliable decisions.

### Mechanism 3: Regime-Adaptive Sequential Halving (SHA)
- Claim: A simple adaptive strategy can achieve near-optimality across both small and large budgets without knowing the budget regime a priori.
- Mechanism: SHA spends a small fraction L of the budget sampling the endpoints (0 and 1) to estimate the change magnitude (Δ̂). It uses this estimate to compute a threshold τ. If the total budget T < τ, it proceeds with SH; otherwise, it uses SHB.
- Core assumption: A rough estimate of Δ from endpoint samples is sufficient to distinguish the budget regime. The adaptive switch happens at a threshold proportional to σ²/Δ² · log(1/η).
- Evidence anchors:
  - [Section 6] "We first sample the actions 0, 1 a total of L < T times and use these samples to estimate the change in mean Δ̂."
  - [Theorem 5] Provides unified bounds matching both regime-specific lower bounds.
  - [corpus] Adaptive algorithm design is common in bandits (e.g., UCB variants), but the regime-switching based on estimated problem difficulty is specific to this setting.
- Break condition: If the initial estimate Δ̂ is highly inaccurate (e.g., due to very noisy observations in the initial L samples), SHA may choose the wrong algorithm, potentially leading to suboptimal performance.

## Foundational Learning

- Concept: **Sub-Gaussian noise**
  - Why needed here: The error bounds in all theorems explicitly depend on the sub-Gaussian parameter σ². Understanding sub-Gaussian tails is essential to interpret how noise affects the probability of making incorrect elimination decisions.
  - Quick check question: Given i.i.d. noise ϵt with variance var(ϵt) = ν², what is the sub-Gaussian parameter σ² for bounded noise in [a,b]?

- Concept: **Fixed-budget pure exploration**
  - Why needed here: Unlike regret minimization, the goal here is to identify the change point accurately within a given budget T. This framing dictates the objective (minimize failure probability) and the algorithm design (non-anytime, budget-aware).
  - Quick check question: In fixed-budget best arm identification, what is the typical dependence of error probability on the budget T and the reward gap Δ?

- Concept: **Tolerance parameter η**
  - Why needed here: The change point is a point in a continuous space, so exact identification is impossible. η defines what "accurate enough" means and directly controls the number of phases (J) in the algorithms.
  - Quick check question: How does the error bound for SH change if η is made 10x smaller (i.e., higher precision required)?

## Architecture Onboarding

- **Component map**:
  - SHA -> Δ̂ estimator -> τ calculator -> (SH or SHB)
  - SH/SHB -> Phase loop -> Sampling points -> Empirical mean calculation -> Decision criteria -> Interval update
  - Decision Modules: Elimination criteria (ER,j, EL,j), backtracking criterion (EP,j), regime threshold calculator (τ = γσ²/Δ̂² · log(1/2η))

- **Critical path**:
  1. Input budget T, tolerance η, and optionally σ² (for SHA).
  2. If using SHA: Perform initial L pulls at 0 and 1 → estimate Δ̂ → compute τ → choose SH or SHB.
  3. Phase loop (for SH/SHB): Determine sampling points → pull each point tⱼ times → compute empirical means → apply decision criteria → update interval.
  4. Return midpoint of final interval as change point estimate.

- **Design tradeoffs**:
  - **SH vs. SHB**: SH is simpler and better for very small budgets but has multiplicative η penalty. SHB adds complexity (backtracking) and overhead (5 vs. 3 pulls per phase) but has additive η penalty, better for larger budgets.
  - **Choosing L and γ for SHA**: Larger L gives better Δ̂ estimates but wastes budget that could be used in the core algorithm. The paper suggests L=T/20 and γ=120 work well empirically (Appendix B.1). A smaller γ makes SHA more likely to choose SH (conservative for small budgets but may underperform for large budgets).

- **Failure signatures**:
  - **Near-boundary change points**: Existing methods like SRR and ACPD fail when x* is near 0 or 1 because they lack samples on one side to detect the change. SH/SHB/SHA are robust by design (SHB explicitly samples endpoints).
  - **Incorrect regime selection in SHA**: If Δ̂ is underestimated (overestimated), SHA may choose SHB (SH) when SH (SHB) would be better, leading to higher error than optimal.
  - **Budget too small**: For very small T, even SH may fail because tⱼ (samples per point per phase) becomes too small for reliable mean estimation.

- **First 3 experiments**:
  1. **Regime comparison**: Fix η, σ, Δ, x*. Vary T across a range spanning both regimes. Plot failure probability vs. T for SH, SHB, and SHA. Expect SH to win for small T, SHB for large T, and SHA to be competitive throughout. This replicates Figure 3a.
  2. **Boundary sensitivity**: Fix moderate budget T. Vary x* from near 0 to near 1. Compare failure probability of SH, SRR, and ACPD. Expect SH's performance to be stable while competitors degrade near boundaries. This replicates Figures 3b, 3c.
  3. **Robustness to noise**: Fix T, η, x*. Vary σ (noise level). Compare SHA with fixed hyperparameters (L=T/20, γ=120) against an oracle that knows the true regime. Expect SHA to closely match the oracle's performance, validating adaptivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Sequential Halving (SH) and Sequential Halting with Backtracking (SHB) algorithms be extended to settings with multiple change points?
- Basis in paper: [explicit] The conclusion states, "A natural extension of the piecewise constant bandit problem would be to allow for multiple abrupt changes in the reward function."
- Why unresolved: The current theoretical analysis and algorithmic design strictly assume exactly one change point exists in the action space.
- What evidence would resolve it: New algorithms and non-asymptotic bounds that handle k > 1 change points without relying on the single-discontinuity assumption.

### Open Question 2
- Question: How does the problem complexity change when the action space is multi-dimensional rather than one-dimensional?
- Basis in paper: [explicit] Section 8 lists allowing the action space to be "multi-dimensional" as a natural extension requiring significant innovation.
- Why unresolved: The current methods and minimax bounds are derived exclusively for the interval [0, 1].
- What evidence would resolve it: Algorithms adapted for [0,1]^d and theoretical analysis quantifying the sample complexity relative to dimensionality.

### Open Question 3
- Question: Is the 3/4 threshold constant used in the backtracking criterion (EP,j) optimal?
- Basis in paper: [explicit] Appendix G.2 notes the constant was chosen for simplicity and states, "It would be interesting to study exactly how this constant affects our upper bound for the failure probability."
- Why unresolved: The authors fixed the constant to simplify the proof for Theorem 1 rather than optimizing it.
- What evidence would resolve it: A theoretical analysis deriving the optimal strictness for the backtracking rule to minimize the upper bound on error probability.

## Limitations

- The theoretical analysis assumes known noise variance σ² for lower bounds, though the algorithms work with unknown σ²
- The bounds have constants that are not optimized, potentially leaving room for improvement
- The regime-adaptive algorithm SHA relies on initial endpoint samples to estimate the problem difficulty, which may fail if these samples are highly noisy

## Confidence

- **High confidence** in the lower bound derivations and the separation between small and large budget regimes
- **High confidence** in the algorithm designs and their near-optimality in their respective regimes
- **Medium confidence** in the practical performance of SHA, as it depends on the quality of the initial Δ̂ estimate
- **Medium confidence** in the numerical constants in the bounds, as they appear conservative

## Next Checks

1. **Sensitivity analysis**: Systematically vary the hyperparameters L and γ in SHA across a grid to understand their impact on performance and identify if the suggested values (L=T/20, γ=120) are truly robust.
2. **Unknown σ² case**: Extend the theoretical analysis to handle unknown noise variance, or empirically evaluate how the algorithms perform when the assumed σ² differs from the true noise level.
3. **More baselines**: Compare against additional change point detection methods, including non-bandit approaches like binary segmentation or wild binary segmentation, to better contextualize the results.