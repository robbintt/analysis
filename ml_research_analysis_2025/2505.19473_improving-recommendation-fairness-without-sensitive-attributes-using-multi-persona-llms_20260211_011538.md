---
ver: rpa2
title: Improving Recommendation Fairness without Sensitive Attributes Using Multi-Persona
  LLMs
arxiv_id: '2505.19473'
source_url: https://arxiv.org/abs/2505.19473
tags:
- sensitive
- fairness
- information
- user
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of improving recommendation fairness
  without access to sensitive user attributes. The authors propose LLMFOSA, a framework
  that leverages multi-persona LLM agents to infer sensitive attributes from user
  behavior, then uses a confusion-aware representation learning module to develop
  robust sensitive embeddings.
---

# Improving Recommendation Fairness without Sensitive Attributes Using Multi-Persona LLMs

## Quick Facts
- arXiv ID: 2505.19473
- Source URL: https://arxiv.org/abs/2505.19473
- Reference count: 40
- Primary result: LLMFOSA achieves 13.3% AUC fairness improvement without sensitive attribute access

## Executive Summary
This paper tackles the problem of improving recommendation fairness without access to sensitive user attributes. The authors propose LLMFOSA, a framework that leverages multi-persona LLM agents to infer sensitive attributes from user behavior, then uses a confusion-aware representation learning module to develop robust sensitive embeddings. The model is optimized via mutual information objectives. Extensive experiments on two public datasets show that LLMFOSA achieves significant fairness gains—up to 13.3% improvement in AUC metrics—while maintaining competitive recommendation accuracy compared to existing fairness methods.

## Method Summary
LLMFOSA addresses fair recommendation without sensitive attributes by first using multi-persona LLM inference to estimate sensitive attributes from behavioral data. It then employs a confusion-aware representation learning module with learnable annotator confusion matrices and consensus regularization. The framework uses a two-stage training process: first learning sensitive-aware embeddings, then optimizing mutual information objectives to disentangle sensitive and preference information. The final recommendations use sensitive-blind embeddings while achieving fairness through the learned sensitive-aware representations.

## Key Results
- LLMFOSA achieves up to 13.3% improvement in AUC fairness metrics on ML-1M and LFM-360K datasets
- The framework maintains competitive recommendation accuracy with NDCG@20 scores comparable to non-fairness baselines
- LLM-based inference (76.44% accuracy) significantly outperforms clustering-based approaches (58.18% accuracy) on ML-1M
- Two-stage training and mutual information objectives effectively disentangle sensitive and preference information

## Why This Works (Mechanism)

### Mechanism 1: Multi-Persona LLM Inference
Multi-persona LLM agents can infer latent sensitive attributes from behavioral signals more effectively than clustering-based proxies when ground-truth labels are unavailable. Diverse persona prompts induce varied reasoning perspectives (e.g., cultural background, professional field), reducing single-viewpoint bias. A meta summarizer aggregates rationales, distilling consensus signals for downstream embedding learning. The framework assumes LLMs encode sufficient world knowledge about demographic-behavior correlations from pretraining corpora to make plausible inferences.

### Mechanism 2: Confusion-Aware Representation Learning
Modeling annotator confusion and consensus corrects for noisy LLM inferences, enabling robust sensitive representation learning. Each annotator has a learnable confusion matrix modeling P(inferred label | true label). Consensus regularization aligns confusion matrices of similar personas. Contrastive learning incorporates rationale embeddings to refine sensitive-aware embeddings. The framework assumes annotator errors are conditionally independent given the true sensitive attribute; similar personas exhibit similar confusion patterns.

### Mechanism 3: Mutual Information Disentanglement
Mutual information objectives disentangle sensitive and preference information, achieving fairness while preserving recommendation accuracy. User embedding u → sensitive encoder S(·) → s_u (sensitive-aware); u → preference encoder P(·) → p_u (sensitive-blind). The framework optimizes: maximize I(S; A) to capture sensitivity, minimize I(S; P) to enforce independence, maximize I(P; R|S) to retain predictive power. This assumes the disentanglement objective can be sufficiently optimized with estimated A via LLM inference approximations.

## Foundational Learning

- **Mutual Information (MI) Bounds**: The framework uses variational bounds (InfoNCE, CLUB) to tractably optimize MI objectives for representation disentanglement. Quick check: Can you explain why directly computing I(S; A) is intractable and how contrastive bounds approximate it?

- **Confusion Matrix / Label Noise Modeling**: With no ground truth, the model must estimate and correct for systematic errors in LLM-based attribute inference. Quick check: How does the Dawid-Skene assumption (independent errors given true label) enable confusion estimation without ground truth?

- **Graph Neural Networks for Recommendation (GCN/LightGCN)**: The framework uses MF and LightGCN as backbone collaborative encoders; understanding embedding propagation is prerequisite. Quick check: What is the difference between how MF and GCN propagate user-item signals for embedding learning?

## Architecture Onboarding

- **Component map**: LLM Inference Layer (offline) → PersonaEditor → Annotators → MetaSummarizer → {ã_u^i}, e_u; Collaborative Encoder E(U,V,R) → u,v; Sensitive Encoder S(u) → s_u; Preference Encoder P(u) → p_u; MI Optimization with contrastive objectives; Two-Stage Training

- **Critical path**: 1) Run LLM inference offline to generate {ã_u^i}, e_u for all users; 2) Pre-train collaborative encoder with BPR loss; 3) Stage 1: Train sensitive encoder with L_sen; 4) Stage 2: Jointly optimize fairness via L_b

- **Design tradeoffs**: # annotators (N_o): More annotators increase inference cost quadratically; paper finds N_o=4-8 optimal. Consensus strength (λ_sim): Strong regularization reduces diversity, harming fairness. Accuracy vs. fairness: Inverse relationship observed; LLMFOSA trades ~2-5% accuracy for 8-13% fairness gain.

- **Failure signatures**: AUC not decreasing: Check if LLM inference accuracy > clustering baseline; if not, persona diversity may be insufficient. Accuracy collapse (>10% drop): λ_ub or λ_lb may be too large; reduce MI regularization strength. High variance across runs: Confusion matrices may be underdetermined; increase N_o or add label smoothing.

- **First 3 experiments**: 1) Ablation on inference source: Compare LLM-MV, LLM-Single, Hierarchical clustering, and Random assignment for attribute inference; measure inference accuracy (Acc/F1) and downstream fairness (AUC). 2) Sensitivity to annotator count: Sweep N_o ∈ {2, 4, 6, 8, 10} on validation set; plot fairness AUC and accuracy R@20 to identify elbow point. 3) Backbone comparison: Run LLMFOSA with MF vs. LightGCN encoder; verify that GCN backbone yields higher accuracy but similar fairness gains.

## Open Questions the Paper Calls Out

### Open Question 1: Instance-Dependent Confusion Modeling
Would instance-dependent transition matrices significantly improve confusion modeling accuracy compared to the current instance-independent assumption? The current formulation assumes annotator confusion depends only on ground-truth attributes, not on user-specific behavioral patterns that may affect inference reliability. Experiments comparing current confusion matrices against instance-dependent models that condition on user behavioral features would resolve this question.

### Open Question 2: Optimal Annotator Count Scaling
How does the optimal number of annotator agents (No) scale with dataset characteristics such as user population diversity and interaction sparsity? The non-monotonic relationship between No and fairness, combined with dataset-dependent turning points, suggests underlying factors are not well understood. Systematic experiments across datasets with varying density, user diversity, and behavioral complexity would help understand this relationship.

### Open Question 3: LLM Training Data Alignment
To what extent does LLMFOSA's effectiveness depend on the alignment between LLM training data demographics and the target recommendation platform's user population? If LLM training data lacks representation of certain demographic groups or cultural contexts, inference accuracy may systematically vary across user populations. Cross-platform evaluation comparing inference accuracy across user subgroups stratified by demographic characteristics would help answer this question.

## Limitations

- LLM Inference Reliability: Inference accuracy (76.44%) remains significantly below supervised performance (93.02%), potentially limiting fairness gains
- Confusion Matrix Identifiability: Framework assumes conditional independence of annotator errors, but this may not hold with shared systematic biases
- Cross-Dataset Generalization: Experiments only evaluate gender bias on two datasets, limiting conclusions about other sensitive attributes

## Confidence

- LLM Inference Reliability: **Medium** - Significant improvement over clustering but still below supervised performance
- Confusion Matrix Identifiability: **Low** - Assumes conditional independence unverified when ground truth unavailable
- Mutual Information Bounds: **Medium** - InfoNCE/CLUB bounds are known to be loose approximations
- Cross-Dataset Generalization: **Low** - Limited to gender bias on two datasets only

## Next Checks

1. **Ablation on Inference Source**: Compare LLM-MV, LLM-Single, Hierarchical clustering, and Random assignment for attribute inference; measure inference accuracy (Acc/F1) and downstream fairness (AUC) to quantify LLM contribution.

2. **Sensitivity to Annotator Count**: Sweep N_o ∈ {2, 4, 6, 8, 10} on validation set; plot fairness AUC and accuracy R@20 to identify optimal annotator count and verify the elbow point.

3. **Cross-Attribute Evaluation**: Run LLMFOSA on datasets with multiple sensitive attributes (e.g., using both gender and age); measure fairness degradation and verify that the framework generalizes beyond single-attribute settings.