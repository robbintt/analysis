---
ver: rpa2
title: 'BAMBI: Developing Baby Language Models for Italian'
arxiv_id: '2503.09481'
source_url: https://arxiv.org/abs/2503.09481
tags:
- language
- bambi
- linguistic
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BAMBI, a series of Baby Language Models (BabyLMs)
  trained on data designed to mimic the linguistic input received by a five-year-old
  Italian-speaking child. The models are evaluated using BaBIEs, a benchmark specifically
  designed for assessing BabyLMs in Italian.
---

# BAMBI: Developing Baby Language Models for Italian

## Quick Facts
- **arXiv ID**: 2503.09481
- **Source URL**: https://arxiv.org/abs/2503.09481
- **Reference count**: 7
- **Primary result**: BAMBI models achieve robust syntactic competence comparable to larger models despite training on significantly less data

## Executive Summary
This paper introduces BAMBI, a series of Baby Language Models trained on developmentally-plausible data designed to mimic the linguistic input of a five-year-old Italian-speaking child. The models are evaluated using BaBIEs, a benchmark specifically designed for assessing BabyLMs in Italian, comparing them against a large language model (Minerva) and a multimodal model (SmolVLM). The results show that BAMBI models achieve strong syntactic competence despite being trained on orders of magnitude less data than conventional LLMs, though they struggle with semantic tasks similar to English BabyLM findings.

## Method Summary
BAMBI uses ~25M words (presented twice for ~50M total exposure) from CHILDES transcriptions and child-directed speech/media, with custom 30K vocabulary tokenizer. Two architectures are trained: RoBERTa-base encoder (27M params) and GPT-2-base decoder (132M params) using HuggingFace Trainer with 2-epoch and unrestricted training regimes. Evaluation uses BaBIEs benchmark with perplexity-based selection for comprehension tasks and beam search for sentence completion, converting accuracy to age-equivalent scores using Italian pediatric assessment norms.

## Key Results
- BAMBI models achieve robust syntactic competence, outperforming larger models on negation structures and acceptability judgment tasks
- Despite syntactic strengths, BAMBI models struggle with semantic tasks (synonymy, hyponymy, idiom comprehension), aligning with English BabyLM findings
- Multimodal model (SmolVLM) does not outperform BAMBI on linguistic benchmarks, suggesting current VLMs' visual grounding is insufficient for child-like semantic development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Developmentally-plausible data curation enables robust syntactic acquisition with orders of magnitude less training data than conventional LLMs.
- Mechanism: Transcribed child-directed speech and age-appropriate content provide denser syntactic exemplars per token than web-scraped text, allowing distributional learning of grammatical patterns to converge faster on structural knowledge.
- Core assumption: The density of syntactically relevant exemplars in child-directed corpora exceeds that in arbitrary web text on a per-token basis.
- Evidence anchors:
  - [abstract] "BAMBI models achieve robust syntactic competence, comparable to larger models, despite being trained on significantly less data."
  - [section 4.4] "The accuracy obtained by BAMBI models is higher than those obtained by Minerva and SmolVLM" for multiple negative structures including Double Negation and Negative Passive clauses.
  - [corpus] No direct corpus evidence on data efficiency mechanisms; related work (BabyReasoningBench) addresses evaluation methodology rather than training dynamics.
- Break condition: If syntactic density in child-directed transcripts does not exceed web text baselines, the efficiency gain would require alternative explanation.

### Mechanism 2
- Claim: Semantic competence requires more than distributional learning from transcribed speech alone.
- Mechanism: Lexical and semantic tasks (synonymy, hyponymy, idiom comprehension) depend on grounding or world knowledge that cannot be extracted from pure text statistics at limited scale; the paper's lexical comprehension tasks show all models below age-appropriate thresholds.
- Core assumption: Semantic understanding draws on experiential or multimodal grounding that transcribed speech cannot provide.
- Evidence anchors:
  - [abstract] "While BAMBI models perform better on syntactic tasks, they struggle with semantic tasks, aligning with findings from English language models."
  - [section 4.4] "In the latter [Lexical Comprehension], the primary challenge lies in the semantic relationship between the stimulus and the target answer (e.g., synonymy, hyponymy, paraphrasis, or meronymy)."
  - [corpus] MAIA benchmark (Italian multimodal) suggests multimodal evaluation frameworks exist, but no corpus paper directly tests grounding mechanisms.
- Break condition: If semantic deficits disappear with different data mixing ratios or vocabulary frequency balancing, the mechanism may relate to corpus composition rather than grounding limits.

### Mechanism 3
- Claim: Multimodal pre-training alone does not systematically improve linguistic competence benchmarks designed for child acquisition stages.
- Mechanism: SmolVLM's visual grounding did not translate to superior BaBIEs performance; multimodal information in current VLMs may be insufficiently aligned with sensorimotor experiences relevant to early language acquisition.
- Core assumption: Current multimodal training objectives do not capture the specific grounding relationships that support child-like semantic development.
- Evidence anchors:
  - [abstract] The study examines "the contribution of extralinguistic information for language acquisition" through comparison with SmolVLM.
  - [section 4.4] "The results appear to confirm the findings of the BabyLM Challenges... which suggest that multimodal LMs does not outperform classical language modeling as a training strategy."
  - [corpus] No corpus papers directly address why multimodal training fails to transfer; this remains an open question.
- Break condition: If multimodal models with different training objectives or visual corpora show systematic gains, the failure may be training-protocol-specific rather than fundamental.

## Foundational Learning

- Concept: **BabyLM Challenge Constraints**
  - Why needed here: BAMBI follows the BabyLM paradigm of training on cognitively-plausible data quantities (~10M words/year × 5 years); understanding this constraint frame is essential for interpreting "age-equivalent scores" and why 2-epoch training is theoretically motivated.
  - Quick check question: Can you explain why training for 40 epochs on 25M words violates the cognitive-plausibility constraint, even if it improves accuracy?

- Concept: **Perplexity-Based Selection for Multiple-Choice Evaluation**
  - Why needed here: BaBIEs comprehension tasks use perplexity scoring over concatenated "cioè" (that is) sentences to select target answers; understanding this evaluation protocol is necessary for replicating results or debugging anomalous scores.
  - Quick check question: Given stimulus "Il cane è tirato dall'uomo" and four candidate completions, how would you determine which your model selects?

- Concept: **Age-Equivalent Scoring from Standardized Tests**
  - Why needed here: The paper maps model accuracy to age-equivalent scores using Italian pediatric assessment norms (BVL, TROG-2, TCGB-2, PPVT-R); this enables comparison between models and child development data but requires understanding how raw scores convert to developmental ages.
  - Quick check question: Why might two models with identical accuracy receive different age-equivalent scores if their assigned "model ages" differ?

## Architecture Onboarding

- Component map:
  - Training corpus (25M words × 2) -> Custom 30K vocab tokenizer -> RoBERTa-base encoder (6 layers, 256 hidden, 8 heads, ~27M params) OR GPT-2-base decoder (12 layers, 768 hidden, 12 heads, ~132M params) -> HuggingFace Trainer with specified hyperparameters -> BaBIEs evaluation pipeline

- Critical path:
  1. Assemble and validate corpus from CHILDES + transcription sources; verify word counts match 50M target
  2. Train custom tokenizer (30K vocab) on corpus
  3. Train encoder and decoder variants with 2-epoch (strict) and early-stopping (unrestricted) configurations
  4. Run BaBIEs evaluation pipeline: perplexity ranking for comprehension tasks, beam search for sentence completion
  5. Convert raw accuracy to age-equivalent scores using BVL/TROG-2/TCGB-2/PPVT-R norm tables

- Design tradeoffs:
  - **2-epoch vs. unrestricted training**: 2-epoch enforces cognitive plausibility but sacrifices potential performance; paper shows unrestricted training does not dramatically improve age-equivalent scores, suggesting diminishing returns
  - **Encoder vs. decoder architecture**: Decoder models show stronger sentence completion (Loose Scoring: 0.71-0.78 vs. 0.07-0.21 for encoders), but both struggle with strict semantic appropriateness; choice depends on generation vs. classification use case
  - **Custom tokenizer vs. reused vocabulary**: Custom 30K vocab trained on child-directed Italian may improve subword efficiency for age-appropriate vocabulary but limits transfer from pre-trained checkpoints

- Failure signatures:
  - **High Loose Scoring, low Strict Scoring on Sentence Completion**: Indicates morphosyntactic competence without semantic appropriateness (model produces grammatically correct but contextually wrong completions)
  - **Age-equivalent scores below -2 SD for minimum test age**: Model fundamentally underperforms developmental expectations; check for tokenizer issues or training instability
  - **Perplexity selection matches random baseline**: Verify "cioè" concatenation format matches training distribution; check for tokenization artifacts

- First 3 experiments:
  1. **Baseline replication**: Train both architectures for 2 epochs, evaluate on BaBIEs, confirm age-equivalent scores fall within reported ranges (syntactic tasks: 3;6-5;5 years; lexical tasks: below average).
  2. **Ablation on corpus composition**: Train decoder on (a) CHILDES-only subset, (b) media-transcripts-only subset, (c) full mixture; isolate which data source drives syntactic performance on negation structures.
  3. **Semantic probing experiment**: Add a simple word-prediction pre-training objective using synonyms/hyponyms from Italian WordNet as targets; test whether explicit semantic signals improve Lexical Comprehension without harming syntax.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a curriculum learning strategy based on the "starting small" hypothesis significantly improve semantic competence in Italian BabyLMs compared to standard uniform training?
- Basis in paper: [explicit] The authors state in the conclusion that these findings represent the "initial phase of a broader training curriculum" and explicitly reference the hypothesis that "starting small" with simpler data establishes stronger grounding.
- Why unresolved: The current study utilized standard training strategies without a progressive complexity curriculum.
- Evidence: A controlled experiment comparing BAMBI's performance against models trained with a curriculum of gradually increasing linguistic complexity on semantic tasks.

### Open Question 2
- Question: Does training a multimodal model from scratch on developmentally plausible visual and linguistic data enhance semantic grounding more effectively than pre-trained VLMs?
- Basis in paper: [inferred] The authors note that SmolVLM's multimodal training did not outperform standard models, suggesting that the "multimodal information embedded in current LMs is still not enough" to model sensorimotor experience in acquisition.
- Why unresolved: The study only evaluated an existing large VLM (SmolVLM), not a BabyLM specifically trained on child-like multimodal pairs.
- Evidence: Developing a BabyLM trained on aligned child-directed visual/text data and measuring performance on the BaBIEs Lexical Comprehension tasks.

### Open Question 3
- Question: To what extent does specific data curation (e.g., balancing transcribed speech vs. educational content) mitigate the semantic gap observed in low-resource training?
- Basis in paper: [explicit] The abstract suggests "data curation" is a crucial strategy beyond scaling, but the current work focuses on aggregating a plausible dataset rather than ablation studies on content types.
- Why unresolved: The paper establishes a single training set composition; it does not isolate the impact of different qualitative mixtures of input data on semantic performance.
- Evidence: Ablation studies comparing models trained on varying ratios of Child-Directed Speech vs. transcribed TV content to identify optimal input for semantic tasks.

## Limitations
- Corpus composition uncertainty: The exact sources and proportions of the non-CHILDES ~24M words are not detailed, making faithful reproduction difficult
- Semantic competence mechanistic uncertainty: It's unclear whether semantic deficits reflect fundamental grounding requirements or could be addressed through alternative training strategies
- Limited multimodal comparison: The study only evaluated a single pre-trained multimodal model without exploring alternative training objectives or visual corpora

## Confidence
- **Syntactic competence claims (High)**: Strong evidence through Acceptability Judgment task results showing BAMBI outperforming both Minerva and SmolVLM on multiple negative structure types
- **Semantic competence claims (Medium)**: Clear demonstration of semantic struggles, but mechanistic explanation (grounding requirements) is inferred rather than directly tested
- **Multimodal contribution claims (Low)**: Only a single multimodal comparison without exploring alternative training objectives or visual corpora

## Next Checks
1. **Corpus composition ablation**: Train BAMBI decoder model variants on (a) CHILDES-only subset, (b) media-transcripts-only subset, and (c) full mixture to isolate which data source drives the syntactic performance advantage, particularly on negation structures.

2. **Semantic intervention experiment**: Add a word-prediction pre-training objective using Italian WordNet synonyms and hyponyms as explicit semantic targets; test whether this improves Lexical Comprehension scores without degrading syntactic performance.

3. **Multimodal architecture variation**: Train BAMBI with alternative multimodal objectives (e.g., image-text matching, masked multimodal modeling) using different visual corpora to determine if the failure of visual grounding is protocol-specific or fundamental.