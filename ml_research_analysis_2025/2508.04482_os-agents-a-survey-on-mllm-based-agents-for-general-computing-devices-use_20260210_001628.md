---
ver: rpa2
title: 'OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use'
arxiv_id: '2508.04482'
source_url: https://arxiv.org/abs/2508.04482
tags:
- agents
- arxiv
- zhang
- wang
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of OS Agents, which
  are (M)LLM-based agents that use computing devices (e.g., computers and mobile phones)
  by operating within the environments and interfaces (e.g., Graphical User Interface
  (GUI)) provided by operating systems (OS) to automate tasks. The survey covers the
  fundamentals of OS Agents, including their key components (environment, observation
  space, and action space) and essential capabilities (understanding, planning, and
  grounding).
---

# OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use

## Quick Facts
- **arXiv ID**: 2508.04482
- **Source URL**: https://arxiv.org/abs/2508.04482
- **Reference count**: 40
- **Primary result**: Comprehensive survey of OS Agents - MLLM-based agents that operate within OS environments to automate computing tasks across devices

## Executive Summary
This paper presents a comprehensive survey of OS Agents, which are (M)LLM-based agents that use computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks. The survey covers the fundamentals of OS Agents, including their key components (environment, observation space, and action space) and essential capabilities (understanding, planning, and grounding). It then examines methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, the paper discusses current challenges and identifies promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development.

## Method Summary
The survey methodology involves a systematic review of existing literature on OS Agents, categorizing research according to fundamental components, methodologies, evaluation approaches, and future challenges. The authors analyze various aspects including environment modeling, observation and action spaces, agent capabilities, domain-specific foundation models, agent frameworks, evaluation benchmarks, and open challenges in the field.

## Key Results
- OS Agents represent a convergence of MLLM technology with operating system environments to automate general computing tasks
- Key components identified include environment modeling, observation spaces, and action spaces as foundational elements
- Essential capabilities for OS Agents encompass understanding, planning, and grounding within OS environments
- Evaluation protocols and benchmarks are crucial for assessing OS Agent performance across diverse tasks
- Future research directions include addressing safety/privacy concerns and enabling personalization/self-evolution capabilities

## Why This Works (Mechanism)
OS Agents work by leveraging MLLM capabilities to interpret and interact with OS environments through GUI interfaces. The mechanism involves processing visual and contextual observations from the computing environment, planning appropriate sequences of actions, and executing these actions through OS APIs or direct GUI manipulation. This enables automation of complex multi-step tasks that traditionally required human intervention.

## Foundational Learning

**GUI Interaction Understanding**: Why needed - OS Agents must interpret visual interfaces to navigate computing environments; Quick check - Can the agent identify UI elements and their functions from screenshots?

**Action Planning in OS Context**: Why needed - Complex tasks require multi-step reasoning within OS constraints; Quick check - Does the agent generate coherent action sequences for given objectives?

**Environment State Modeling**: Why needed - Accurate world representation is essential for reliable automation; Quick check - Can the agent track system state changes resulting from its actions?

**Cross-platform Compatibility**: Why needed - Agents should function across different OS and device types; Quick check - Does the agent adapt its approach when switching between Windows, macOS, and mobile environments?

## Architecture Onboarding

**Component Map**: MLLM Core -> Environment Perception -> Action Planning -> Execution Module -> Feedback Loop

**Critical Path**: Observation (GUI capture) -> Understanding (MLLM processing) -> Planning (action sequence generation) -> Execution (OS interaction) -> Evaluation (outcome assessment)

**Design Tradeoffs**: Accuracy vs. speed in GUI interpretation, generalization vs. specialization for specific OS environments, model complexity vs. deployment efficiency, safety vs. autonomy in decision-making

**Failure Signatures**: Misinterpretation of UI elements leading to incorrect actions, failure to maintain context across multi-step tasks, inability to handle unexpected system states or errors, performance degradation on non-standard or custom interfaces

**3 First Experiments**:
1. Basic GUI navigation test: Can the agent successfully open applications and navigate menus?
2. Multi-step task automation: Can the agent complete a simple workflow like document creation and file management?
3. Cross-platform functionality: Can the agent perform equivalent tasks across different operating systems?

## Open Questions the Paper Calls Out

The paper identifies several open questions in OS Agent research, including how to ensure safety and privacy when agents interact with sensitive user data, how to enable effective personalization and self-evolution capabilities, how to develop robust evaluation protocols that capture real-world complexity, and how to address the challenges of cross-platform compatibility and generalization across diverse computing environments.

## Limitations

- Evaluation bias exists as many benchmarks focus on English-language tasks and desktop environments, potentially underrepresenting mobile and cross-platform scenarios
- Safety and privacy considerations receive less detailed treatment compared to technical components
- The rapidly evolving nature of the field means some recently developed OS Agent approaches may not be included
- Coverage may not fully capture the complexity of deployment scenarios across diverse user populations and device types

## Confidence

**High**: Technical methodology and component categorization accurately describe established concepts in MLLM-based agent design and GUI interaction

**Medium**: Future research directions depend on uncertain technological developments and adoption patterns

**Medium**: Challenge identification is realistic but may not capture the full complexity of real-world deployment scenarios

## Next Checks

1. Cross-platform validation of benchmark results across different OS environments (Windows, macOS, Android, iOS)
2. Independent replication of key evaluation metrics using the described benchmarks
3. Privacy impact assessment of OS Agent interactions with sensitive user data across multiple use cases