---
ver: rpa2
title: 'Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model
  Approach'
arxiv_id: '2511.23311'
source_url: https://arxiv.org/abs/2511.23311
tags:
- driving
- qwen2
- safe
- video
- lvlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a vision language model (VLM) approach for
  generating safe driving instructions from synchronized driver-facing and road-facing
  videos. The authors construct a dataset of 2,145 video clips with Chain-of-Thought
  question-answer pairs for event detection and safe driving instruction.
---

# Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach

## Quick Facts
- **arXiv ID**: 2511.23311
- **Source URL**: https://arxiv.org/abs/2511.23311
- **Reference count**: 17
- **Primary result**: Fine-tuned Qwen2.5-VL models achieve BERTScore F1 of 0.897 for event detection and 0.894 for instruction generation on synchronized dual-view driving videos.

## Executive Summary
This study develops a vision language model approach for generating safe driving instructions from synchronized driver-facing and road-facing videos. The authors construct a dataset of 2,145 video clips with Chain-of-Thought question-answer pairs for event detection and safe driving instruction. After fine-tuning Qwen2.5-VL models on this dataset, performance improves significantly: BERTScore F1 reaches 0.897 for event detection and 0.894 for instruction generation. Fine-tuned models generate safety-aware driving instructions aligned with visual context, while pre-trained models produce generic guidance. However, challenges remain in detecting subtle events like excessive speed during turns, and some failure cases occur with good driving videos.

## Method Summary
The approach uses Qwen2.5-VL models with frozen vision encoders, fine-tuned on a dataset of 2,145 synchronized driver-facing and road-facing video clips. Videos are vertically stacked and processed at 2 FPS. The fine-tuning uses full-parameter supervised learning on the language model component only, with Chain-of-Thought prompting separating event detection from instruction generation. The dataset includes primary and sub-event annotations used to generate gold-standard answers via GPT-4o, which are reviewed by human experts. Training uses LLaMA-Factory with DeepSpeed ZeRO-2, batch size 8, 3 epochs, learning rate 1e-5, and BF16 precision.

## Key Results
- Fine-tuned models achieve BERTScore F1 of 0.897 for event detection and 0.894 for instruction generation, compared to 0.841 for pre-trained models
- Chain-of-Thought prompting successfully separates event detection from instruction generation, reducing hallucination
- Dual-view input enables detection of driver-specific events like mobile phone use that single-view approaches miss
- Error analysis reveals challenges with subtle events (excessive speed during turns) and good driving scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Chain-of-Thought prompting improves context-grounded instruction generation by separating event detection from action recommendation.
- **Mechanism**: The two-stage questioning ("What is happening?" → "What is recommended?") forces the model to first ground itself in observed visual events before generating safety instructions, reducing hallucination of non-existent hazards.
- **Core assumption**: VLMs can maintain reasoning consistency across multi-turn conversations where the second answer builds on the first.
- **Evidence anchors**: Fine-tuned LVLMs generate accurate and safety-aware driving instructions; CoT format guides reasoning from event identification to action inference.
- **Break condition**: If event detection accuracy drops below ~0.85 BERTScore F1, instructions become misaligned with actual video content.

### Mechanism 2
- **Claim**: Synchronized dual-view video input enables detection of events requiring both internal (driver) and external (road) context.
- **Mechanism**: Vertically stacking driver-facing and road-facing frames creates a unified visual token sequence that the vision encoder processes jointly, allowing cross-attention between driver behaviors and road conditions.
- **Core assumption**: The vision encoder can spatially distinguish between the two stacked views and associate them with correct semantic roles.
- **Evidence anchors**: Comprehensive safety requires monitoring driver-facing views to detect risky events like mobile phone use; each video presents synchronized views of both driver and road.
- **Break condition**: Temporal misalignment between views >500ms would likely cause incorrect event attribution.

### Mechanism 3
- **Claim**: Freezing the vision encoder while fine-tuning only the language model preserves general visual capabilities while adapting output style to domain-specific instruction format.
- **Mechanism**: The pre-trained vision encoder already encodes spatial and temporal features adequately; fine-tuning the LLM component learns the mapping from visual tokens to structured driving-specific language patterns.
- **Core assumption**: The pre-trained vision encoder's representations are sufficient for driving scene understanding without domain adaptation.
- **Evidence anchors**: Fine-tuning improves BERTScore F1 from ~0.84 to ~0.90 for both tasks; encoder-freezing approaches can inherit backbone capabilities effectively.
- **Break condition**: Subtle temporal events requiring fine-grained motion understanding remain undetected, suggesting encoder limitations.

## Foundational Learning

- **Concept: Vision-Language Model Architecture**
  - **Why needed here**: Understanding how visual tokens are generated and fused with language tokens explains why freezing the encoder works and what visual information is available to the LLM.
  - **Quick check question**: Can you explain how Qwen2.5-VL converts video frames into tokens the language model can process?

- **Concept: BERTScore vs. BLEU Evaluation**
  - **Why needed here**: The paper reports both metrics for different purposes—BERTScore captures semantic similarity (appropriate for instruction correctness), while BLEU captures n-gram overlap (appropriate for format consistency).
  - **Quick check question**: Why might a model score high on BERTScore but low on BLEU for instruction generation?

- **Concept: Language/Unimodal Bias in VLMs**
  - **Why needed here**: Error analysis reveals models sometimes mention objects not present in videos, indicating language priors override visual grounding.
  - **Quick check question**: What causes language bias in LVLMs, and how might fine-tuning mitigate vs. exacerbate it?

## Architecture Onboarding

- **Component map**: Dual-view video preprocessing (stacking, resizing, FPS reduction) → Vision token generation → LLM inference with CoT prompting → Post-processing of generated instructions

- **Critical path**: Video preprocessing (stacking, resizing, FPS reduction) → Vision token generation → LLM inference with CoT prompting → Post-processing of generated instructions

- **Design tradeoffs**:
  - 3B vs. 7B: 7B produces more diverse outputs pre-fine-tuning but outputs generic recommendations; 3B more consistent post-fine-tuning
  - 2 FPS vs. higher: Lower FPS reduces compute but may miss brief events
  - Frozen encoder vs. full fine-tuning: Freezing prevents overfitting to small dataset but limits adaptation to driving-specific visual patterns

- **Failure signatures**:
  - Good driving videos trigger unnecessary recommendations (33% of "difficult" subset)
  - Turn scenarios produce irrelevant suggestions (e.g., referencing non-existent stop signs)
  - Subtle temporal events (excessive speed during turns) go undetected
  - Pre-trained models produce generic safety lists regardless of video content (high self-BLEU = low diversity)

- **First 3 experiments**:
  1. **Baseline replication**: Load Qwen2.5-VL-3B/7B, run inference on 10 test videos with dual-view input, compare outputs to ground-truth using BERTScore to confirm pre-trained performance gap.
  2. **Fine-tuning ablation**: Fine-tune only the LLM component on the provided dataset for 1 epoch, evaluate on validation set, confirm vision encoder freeze does not degrade visual grounding.
  3. **Error categorization**: Manually inspect bottom 25% BERTScore samples to identify if failures cluster around specific event types (turns, good driving, mobile usage) as reported, then prioritize improvements accordingly.

## Open Questions the Paper Calls Out

- **Open Question 1**: Will performance improvements generalize to larger datasets? The current small dataset (1,719 training samples) may not reflect the model's potential ceiling. Resolution requires scaling experiments on larger synchronized driving video collections.

- **Open Question 2**: How can VLMs detect subtle dynamic events like excessive speed during turns without auxiliary sensors? The study excluded sensor data to simulate low-cost dashcams, leaving visual speed estimation as a challenge. Resolution requires integrating specialized temporal reasoning modules or optical flow inputs.

- **Open Question 3**: To what extent do language biases cause hallucinations in safety instructions? The paper identifies language priors as causing models to mention objects not in videos, but doesn't implement countermeasures. Resolution requires ablation studies testing bias-reduction techniques.

## Limitations

- Dataset availability is unclear as the 2,145 video clips are provided by Teatis Inc. without public release terms
- Small training dataset (1,719 samples) creates low-resource conditions that may limit generalization
- No comparison against domain-specific baselines like traditional computer vision + rule-based instruction generation

## Confidence

- **High confidence**: Core finding that fine-tuned VLMs generate more contextually relevant driving instructions than pre-trained models, supported by significant BERTScore improvements
- **Medium confidence**: Chain-of-Thought approach benefits, as conceptually sound but limited quantitative evidence beyond improved instruction quality metrics
- **Medium confidence**: Dual-view architecture advantages, as demonstrates capability but lacks ablation studies comparing single-view vs dual-view performance

## Next Checks

1. **Dataset Release Verification**: Confirm whether the dataset becomes publicly available and assess annotation quality by comparing a random sample of GPT-4o-generated answers against human expert reviews.

2. **Ablation on Vision Encoder Fine-tuning**: Conduct controlled experiments fine-tuning the vision encoder versus keeping it frozen to determine if driving-specific visual patterns require encoder adaptation.

3. **Cross-Dataset Generalization Test**: Evaluate the fine-tuned model on an independent driving dataset to assess whether instruction quality degrades when visual distribution shifts, particularly for challenging "good driving" and "turning" scenarios.