---
ver: rpa2
title: An Adaptive Method Stabilizing Activations for Enhanced Generalization
arxiv_id: '2506.08353'
source_url: https://arxiv.org/abs/2506.08353
tags:
- adaact
- activation
- generalization
- learning
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdaAct, an optimization algorithm that adapts
  learning rates based on activation variance to stabilize neuron outputs and improve
  generalization. The method scales gradient updates inversely proportional to the
  square root of activation variance, taking smaller steps with high variance and
  larger steps with low variance.
---

# An Adaptive Method Stabilizing Activations for Enhanced Generalization

## Quick Facts
- **arXiv ID:** 2506.08353
- **Source URL:** https://arxiv.org/abs/2506.08353
- **Reference count:** 40
- **Primary result:** AdaAct achieves strong generalization comparable to SGD and KFAC while maintaining Adam's fast convergence speed

## Executive Summary
This paper introduces AdaAct, an optimization algorithm that stabilizes neuron outputs by adapting learning rates based on activation variance. The method scales gradient updates inversely proportional to the square root of activation variance, taking smaller steps with high variance and larger steps with low variance. AdaAct combines the fast convergence of Adam with the strong generalization of SGD, outperforming other adaptive methods on standard image classification benchmarks while being faster than second-order methods like KFAC.

## Method Summary
AdaAct estimates per-neuron activation variance using exponential moving average of squared activations, then scales gradient updates inversely proportional to the square root of this variance. The algorithm shares learning rates across parameters connected to the same neuron, providing neuron-wise adaptivity without the over-adaptation issues of parameter-wise methods. Key hyperparameters include β₁=0.9, β₂=0.999, ε=10⁻⁸, and decoupled weight decay. The method approximates the full activation covariance matrix with its diagonal for computational efficiency.

## Key Results
- Achieves 92.49% accuracy on CIFAR-10 vs Adam's 91.37% and SGD's 92.63%
- Outperforms Adam, AdamW, and Adan on CIFAR-10/100 and ImageNet while being faster than FOOF and KFAC
- Particularly effective for vision transformers, matching LAMB's performance on ViT-S
- Maintains O(log T/√T) convergence rate with bounded generalization error via uniform stability

## Why This Works (Mechanism)

### Mechanism 1: Inverse Variance Scaling Stabilizes Neuron Outputs
Scaling gradient updates inversely proportional to √(activation variance) promotes stable neuron responses and improves generalization. AdaAct computes per-neuron activation variance via EMA of squared activations. Updates are divided by √(V̂_t + ε), taking smaller steps when activation variance is high (unstable neurons) and larger steps when variance is low (stable neurons). This contrasts with Adam, which adapts based on gradient variance.

### Mechanism 2: Neuron-wise Learning Rate Sharing Reduces Over-adaptation
Sharing learning rates across parameters that receive the same input (neuron-wise rather than parameter-wise) mitigates the generalization gap seen in Adam-like adaptive methods. Unlike Adam which maintains per-parameter learning rates, AdaAct applies the same learning rate to all weights connected to a given neuron.

### Mechanism 3: Bounded Generalization via Uniform Stability
AdaAct achieves bounded generalization error due to ε-uniform stability under bounded gradients and Lipschitz loss. Theorem 5.5 shows the expected parameter divergence E[‖θ_t - θ'_t‖_2] between models trained on datasets differing by one example is bounded.

## Foundational Learning

- **Concept: Exponential Moving Average (EMA)**
  - **Why needed here:** AdaAct uses EMA (β₂=0.999) to track activation variance stably over time.
  - **Quick check question:** If β₂=0.999 and current squared activation is 1.0 with previous EMA value 0.5, what is the new EMA value?

- **Concept: KFAC and Kronecker-Factored Approximation**
  - **Why needed here:** AdaAct is motivated as a diagonal approximation to KFAC's activation covariance preconditioning.
  - **Quick check question:** Why does using only the diagonal of the activation covariance matrix reduce computational cost compared to full KFAC?

- **Concept: Uniform Stability and Generalization Bounds**
  - **Why needed here:** The paper's theoretical contribution relies on proving ε-uniform stability to bound generalization error.
  - **Quick check question:** If an algorithm is ε-uniformly stable, what does Theorem 5.4 tell us about its generalization error?

## Architecture Onboarding

- **Component map:** Forward pass → Variance estimation (Lines 4-6) → Gradient EMA (Lines 8-10) → Variance adaptation (Line 12) → Update (Line 15)
- **Critical path:** Variance estimation → bias correction → gradient scaling → parameter update. Errors in variance computation propagate directly to learning rate scaling.
- **Design tradeoffs:**
  - **Diagonal vs. full covariance:** Diagonal avoids O(m³) inversion but ignores feature correlations. Paper shows p=0.5 with diagonal outperforms full covariance.
  - **Learning rate magnitude:** AdaAct uses high LR (~0.1-4.0) vs. Adam's typical 0.001. Higher LR is viable due to variance-based normalization.
  - **Weight decay:** Lower than AdamW default (0.01); higher values make AdaAct converge like SGD, lower values retain Adam-like speed.
- **Failure signatures:**
  - **Numerical instability:** If activation variance approaches zero without ε-clipping, division by near-zero causes explosion.
  - **No improvement over Adam:** Check if activation variance is actually varying; if constant, mechanism provides no adaptation benefit.
  - **Slow convergence vs. Adam:** May indicate weight decay too high or learning rate mismatch.
- **First 3 experiments:**
  1. **Baseline sanity check:** Train small CNN (e.g., ResNet-20) on CIFAR-10 with default hyperparameters (η=0.1, β₁=0.9, β₂=0.999, λ=2e-3). Compare final test accuracy and training curves to SGD and Adam baselines.
  2. **Ablation on scaling exponent p:** Test p ∈ {1/6, 1/4, 1/3, 1/2, 1} to confirm p=0.5 is optimal. Monitor both accuracy and activation variance trends.
  3. **Without batch normalization:** Train same model with BN removed to verify AdaAct's regularization effect is complementary to BN and provides robustness when BN is unavailable.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the performance gap between AdaAct and AdamW on Vision Transformers (ViT-S) be closed, or does the activation variance scaling mechanism conflict with the specific optimization landscape of attention layers?
- **Basis in paper:** [explicit] In Table 3 and Section 6.2, the authors note that while AdaAct matches LAMB, it "does not achieve the same accuracy with AdamW's leading 78.9%" on ViT-S, acknowledging a significant gap.

### Open Question 2
- **Question:** Does AdaAct maintain its convergence speed and generalization advantages in Natural Language Processing (NLP) tasks and non-image domains?
- **Basis in paper:** [inferred] The paper explicitly restricts its evaluation to "standard image classification benchmarks" (CIFAR and ImageNet) and vision architectures (ResNet, DenseNet, ViT).

### Open Question 3
- **Question:** Does the diagonal approximation of the activation covariance matrix limit AdaAct's ability to capture complex layerwise dependencies compared to full-covariance methods like KFAC or FOOF?
- **Basis in paper:** [inferred] Section 4 states that AdaAct "approximates [the covariance] as a diagonal matrix — this results in lower space complexity," explicitly ignoring off-diagonal correlations for efficiency.

## Limitations
- Performance on vision transformers (ViT-S) trails AdamW by a significant margin despite matching LAMB
- Core mechanism (inverse variance scaling) lacks direct empirical validation across diverse architectures beyond vision tasks
- Several hyperparameters are underspecified (ImageNet weight decay values, CIFAR augmentation details), potentially affecting reproducibility

## Confidence

**Major Uncertainties:**
The core mechanism is supported by theoretical analysis and CIFAR-10 experiments, but lacks direct empirical validation across diverse architectures. The generalization benefit appears most pronounced in vision transformers, while standard CNNs show more modest gains.

**Confidence Labels:**
- **High:** Convergence rate O(log T/√T) (Theorem 5.1), bounded activation variance during training (Section 5.1, Figure 4), diagonal approximation being computationally efficient
- **Medium:** Generalization bound via ε-uniform stability (Theorem 5.5) - relies on assumptions A1-A5 which may not hold in practice; superior performance on ViT vs. CNNs - architecture-specific effect not fully characterized
- **Low:** p=0.5 being universally optimal (Figure 3 shows CIFAR-10 specific; other datasets/architectures may prefer different values); the neuron-wise learning rate sharing being fundamentally superior to parameter-wise methods for all scenarios

## Next Checks
1. **Architecture Transfer Test:** Evaluate AdaAct on non-vision architectures (RNNs, MLPs on tabular data) to verify if activation variance stabilization generalizes beyond computer vision.
2. **Variance-Correlation Analysis:** Systematically measure correlation between activation variance and generalization gap across multiple random seeds to validate the core mechanism empirically.
3. **Hyperparameter Sensitivity:** Conduct grid search on p exponent (1/6 to 1) and weight decay across CIFAR-10/100 to determine if p=0.5 is indeed optimal or architecture-dependent.