---
ver: rpa2
title: 'ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with
  Reasoning-Enhanced Small Vision-Language Models'
arxiv_id: '2504.10757'
source_url: https://arxiv.org/abs/2504.10757
tags:
- driving
- reasoning
- autonomous
- fine-tuning
- vehicle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether explicitly modeling reasoning during
  fine-tuning enhances vision-language models for autonomous driving tasks. The authors
  use GPT-4o to generate structured reasoning chains for driving scenarios from the
  DriveLM benchmark with category-specific prompting strategies.
---

# ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models

## Quick Facts
- arXiv ID: 2504.10757
- Source URL: https://arxiv.org/abs/2504.10757
- Authors: Amirhosein Chahe; Lifeng Zhou
- Reference count: 32
- Models fine-tuned with reasoning chains achieve 0.55 final score vs. 0.26 for answer-only fine-tuning and 0.00 for instruction-tuned baselines

## Executive Summary
This paper investigates whether explicitly modeling reasoning during fine-tuning enhances vision-language models for autonomous driving tasks. The authors use GPT-4o to generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. They compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families. Results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions.

## Method Summary
The authors generate reasoning chains using GPT-4o with category-specific prompts (Perception, Prediction, Planning, Behavior) applied to the DriveLM-nuScenes subset (5,280 QA pairs, 800 frames, 6 camera views). Training data is formatted with system prompts and `<think/>` reasoning tags followed by `<answer/>` tags. Models are fine-tuned using rank-8 LoRA on language layers and MLP modules while freezing vision layers and attention. Training uses 4-bit quantization, 8-bit AdamW optimizer, and 3 epochs with early stopping. Three variants are compared: reason (with reasoning chains), simple (answers only), and instruct (baseline).

## Key Results
- Llama3.2-11B-reason achieves highest final score of 0.55
- Reasoning-based fine-tuning consistently outperforms answer-only (0.26) and instruction-tuned (0.00) baselines
- Accuracy scores range from 0.47 to 0.68 for reasoning models vs. 0.00 for instruction-tuned models
- ChatGPT evaluation scores inversely correlate with accuracy, suggesting current metrics misalign with task requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit reasoning chains during fine-tuning improve internal representations for driving decisions
- Mechanism: Training models to generate structured reasoning before answers creates intermediate supervision signals that guide gradient updates toward causally-ordered representations, rather than directly mapping inputs to outputs through opaque transformations
- Core assumption: The reasoning chains generated by GPT-4o accurately capture valid decision logic transferable to smaller models
- Evidence anchors:
  - [abstract] "Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions"
  - [section 5.1] "Models fine-tuned with reasoning (reason) consistently outperform their counterparts across nearly all metrics"
  - [corpus] Related work on knowledge distillation (arXiv:2502.00843) reports 21-32% gains on DriveLM, supporting transfer learning effectiveness
- Break condition: If reasoning chains contain systematic errors or hallucinations from GPT-4o, smaller models would learn incorrect causal patterns

### Mechanism 2
- Claim: Category-specific prompting aligns reasoning structure with task demands
- Mechanism: Different driving tasks (Perception, Prediction, Planning, Behavior) require different reasoning depths and foci; tailored prompts constrain reasoning generation to task-relevant patterns, reducing noise in training signals
- Core assumption: The four categories adequately partition driving reasoning with distinct cognitive requirements
- Evidence anchors:
  - [section 3.1] "Our prompting strategy was tailored to each driving task category as shown in Table 1"
  - [Table 1] Perception uses 1 concise sentence; Planning uses 2-3 sentences with trade-off evaluation
  - [corpus] No direct corpus comparison of category-specific vs. generic prompting found
- Break condition: If task boundaries are模糊 or reasoning patterns overlap significantly, category-specific prompts provide no advantage

### Mechanism 3
- Claim: Domain-specific fine-tuning is prerequisite for driving scenario understanding
- Mechanism: Pre-trained VLMs lack grounded representations for driving-specific concepts (ego vehicle, traffic interactions, safety constraints); fine-tuning on driving QA pairs aligns model distributions with target domain vocabulary and reasoning patterns
- Core assumption: The 5,280 QA pairs provide sufficient coverage of driving scenarios
- Evidence anchors:
  - [section 5.1] "Accuracy scores ranging from 0.47 to 0.68 compared to 0.00 for instruction-tuned models"
  - [section 5.4] "Non-fine-tuned instruction models score highest on ChatGPT evaluation but lack task accuracy"
  - [corpus] LAVQA (arXiv:2511.11840) similarly finds domain adaptation critical for shared autonomy in driving
- Break condition: If dataset contains distribution shift relative to deployment scenarios, fine-tuning may overfit to benchmark artifacts

## Foundational Learning

- Concept: **Chain-of-Thought Distillation**
  - Why needed here: Understanding how reasoning traces from teacher models can transfer to student models without requiring identical architecture
  - Quick check question: Can you explain why training on reasoning traces differs from training on answers alone in terms of gradient signal?

- Concept: **LoRA Fine-Tuning**
  - Why needed here: Paper uses rank-8 LoRA with selective layer targeting; understanding parameter-efficient tuning is essential for reproducing results
  - Quick check question: What layers were frozen vs. fine-tuned in the paper's configuration?

- Concept: **Multi-View Visual Fusion**
  - Why needed here: Input consists of 6 camera views (front/back/left/right variants); understanding how VLMs process multiple images is prerequisite
  - Quick check question: How does the paper structure multi-camera inputs during training?

## Architecture Onboarding

- Component map:
  - Data Pipeline: DriveLM subset → GPT-4o reasoning generation → Category-specific formatting → Training examples with 6 camera views + question + reasoning + answer
  - Model Backbones: Llama-3.2-Vision (11B), LLaVA-1.5 (7B), Qwen-2.5-VL (3B/7B) with shared LoRA configuration
  - Training: 4-bit quantization, 8-bit AdamW, rank-8 LoRA on language/MLP layers, frozen vision/attention
  - Evaluation: DriveLM benchmark metrics (Accuracy, BLEU, ROUGE-L, CIDEr, ChatGPT score, Final Score)

- Critical path:
  1. Generate reasoning chains with GPT-4o using category-specific prompts
  2. Format training data with system prompt + question + `<think/>` reasoning + `<answer/>` tags
  3. Apply LoRA fine-tuning for 3 epochs with early stopping
  4. Inference requires models to output reasoning before answers

- Design tradeoffs:
  - **Reason vs. Simple vs. Instruct**: Reason trades ChatGPT-evaluated fluency (0.56-0.62) for accuracy (0.47-0.68)
  - **Model size vs. efficiency**: Qwen-3B-reason underperforms 7B variants but may suit tighter constraints
  - **BLEU vs. semantic correctness**: LLaVA-1.5-simple achieves high BLEU (0.55) but lower accuracy than reason variants

- Failure signatures:
  - Accuracy = 0.00 indicates no domain fine-tuning applied
  - ChatGPT score high but accuracy low suggests plausible-but-incorrect responses
  - Missing or malformed `<think/>` tags during inference indicate training configuration issues

- First 3 experiments:
  1. **Baseline reproduction**: Train Llama-3.2-11B with reason/simple/instruct configurations on provided dataset; verify accuracy matches reported 0.68/0.26/0.00
  2. **Ablation on reasoning depth**: Test whether truncated reasoning (1 sentence across all categories) degrades performance vs. category-specific lengths
  3. **Cross-category generalization**: Evaluate whether Perception-trained models can answer Planning questions, probing reasoning transfer within domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does reasoning-based fine-tuning affect model robustness to out-of-distribution driving scenarios, adversarial inputs, and edge cases?
- Basis in paper: [explicit] The conclusion explicitly states: "Future work should focus on... investigating how explicit reasoning affects model robustness to out-of-distribution scenarios."
- Why unresolved: All experiments used the DriveLM-nuScenes benchmark under standard conditions. No evaluation was conducted on unseen weather conditions, unusual traffic situations, corrupted inputs, or adversarially perturbed images. The paper does not address whether the improved performance generalizes beyond the training distribution.
- What evidence would resolve it: Systematic evaluation on out-of-distribution test sets including unseen weather (fog, snow), sensor failures, adversarial image perturbations, and novel traffic scenarios, comparing degradation rates between reasoning-based and baseline models.

### Open Question 2
- Question: Does reasoning-based fine-tuning yield models that genuinely reason, or do they simply learn to generate plausible post-hoc rationalizations?
- Basis in paper: [inferred] The reasoning chains are generated by GPT-4o using ground truth answers as input (Section 3.1), meaning the reasoning is constructed to justify correct answers rather than derived independently. The paper does not validate whether generated reasoning is causally related to answer correctness or merely correlative text.
- Why unresolved: The evaluation metrics (BLEU, ROUGE, accuracy) measure output quality but do not assess reasoning fidelity. A model could generate coherent reasoning text that sounds correct while using spurious patterns to arrive at answers.
- What evidence would resolve it: Intervention studies where reasoning chains are systematically altered or ablated to test causal impact on final answers; correlation analysis between reasoning correctness and answer correctness on held-out examples with human-annotated reasoning validation.

### Open Question 3
- Question: What are the appropriate safety-oriented evaluation metrics for reasoning-enhanced driving VLMs, and how do current text-based metrics correlate with actual driving safety outcomes?
- Basis in paper: [explicit] The conclusion states: "Limitations of our work include... reliance on text-based evaluation metrics. Future work should focus on developing specialized safety-oriented metrics."
- Why unresolved: Current metrics (accuracy, BLEU, ROUGE, ChatGPT score) measure linguistic quality and answer matching but do not capture safety-critical distinctions. A response recommending "slow down slightly" vs. "brake immediately" may have high text similarity but vastly different safety implications. The inverse correlation between ChatGPT scores and accuracy further suggests current metrics misalign with task requirements.
- What evidence would resolve it: Development and validation of safety-weighted metrics that penalize dangerous recommendations disproportionately; correlation studies between model scores and simulated driving outcomes (collision rates, near-miss events) in closed-loop driving simulators.

## Limitations
- Limited dataset scale with only 5,280 QA pairs raises concerns about overfitting and generalization
- Single prompt template approach assumes clear boundaries between driving task categories that may not reflect real-world complexity
- Quality and correctness of GPT-4o-generated reasoning chains remain unverified and could propagate errors to smaller models

## Confidence
- **High confidence**: The experimental results showing reasoning-based fine-tuning consistently outperforms answer-only and instruction-tuned baselines across multiple model families with clear quantitative differences
- **Medium confidence**: The claim that explicit reasoning enhances internal representations is supported by performance improvements but relies on indirect evidence
- **Low confidence**: The assumption that GPT-4o-generated reasoning chains accurately capture valid decision logic transferable to smaller models lacks independent verification

## Next Checks
1. **Reasoning chain quality audit**: Sample 100 reasoning chains from GPT-4o and have domain experts evaluate their correctness and completeness. Compare hallucinated vs. accurate chains to quantify potential negative transfer.

2. **Cross-category transfer experiment**: Train Perception-only models and test them on Planning and Prediction questions. Measure whether category-specific reasoning is transferable or represents genuinely distinct reasoning patterns.

3. **Dataset scale sensitivity analysis**: Systematically reduce training set size (25%, 50%, 75%) and measure performance degradation rates for reason vs. simple vs. instruct configurations. This reveals whether improvements are robust to dataset size constraints.