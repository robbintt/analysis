---
ver: rpa2
title: 'PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs'
arxiv_id: '2312.15230'
source_url: https://arxiv.org/abs/2312.15230
tags:
- retraining
- pruning
- sparsity
- parameters
- magnitude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the traditional practice of full parameter\
  \ retraining after pruning neural networks, especially for large language models\
  \ (LLMs) where full retraining is computationally prohibitive. Instead, it demonstrates\
  \ that retraining only a small subset of highly expressive parameters\u2014such\
  \ as biases, layer normalization parameters, or using novel parameter-efficient\
  \ LoRA variants\u2014can restore or even improve performance after pruning."
---

# PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs

## Quick Facts
- arXiv ID: 2312.15230
- Source URL: https://arxiv.org/abs/2312.15230
- Authors: Max Zimmer; Megi Andoni; Christoph Spiegel; Sebastian Pokutta
- Reference count: 40
- Primary result: Retraining just 0.01%-0.05% of parameters (biases, LN) can match full retraining after pruning, enabling efficient LLM compression

## Executive Summary
This paper challenges the conventional wisdom that neural networks must be fully retrained after pruning, particularly for large language models where full retraining is computationally prohibitive. Through systematic experimentation, the authors demonstrate that retraining remarkably small parameter subsets—specifically biases and layer normalization parameters—can restore most of the lost performance after pruning. The approach enables retraining models up to 30 billion parameters on a single GPU in minutes rather than days. Additionally, the paper introduces two novel LoRA variants, ScaleLoRA and MaskLoRA, that preserve sparsity patterns when merged back into the model.

## Method Summary
The method applies unstructured magnitude pruning to LLM linear layers (excluding embeddings and final heads), then retrains only a small subset of parameters. The primary approaches are: (1) retraining biases and layer normalization parameters alone (0.01-0.05% of total parameters), (2) ScaleLoRA for scaling-factor-only updates, and (3) MaskLoRA which applies the pruning mask during forward pass to preserve sparsity when merged. Models are retrained for 1000 iterations using AdamW optimizer with linear schedule and weight decay, on a subset of the C4 dataset. The approach is validated across multiple architectures (OPT, LLaMA-2, Mistral, Mixtral) and sparsity levels (30-80%).

## Key Results
- Retraining biases alone (0.034% of parameters) recovers most performance at 30-50% sparsity, nearly matching full fine-tuning
- At 70% sparsity, MaskLoRA achieves 52.97% accuracy on OPT-30B vs 37.24 with no retraining
- Layer-wise reconstruction using MaskLoRA improves retraining-free methods like Wanda by up to 17% in zero-shot accuracy
- 30B parameter models can be retrained in minutes on a single GPU using parameter-efficient methods

## Why This Works (Mechanism)

### Mechanism 1: Feature Retention After Pruning
Pruning acts as a "disturbance" to learned features rather than destroying them. Since performance recovers in far fewer iterations than training from scratch would require, the underlying feature representations remain largely intact. Retraining adjusts affine transformations (biases, LN parameters) to realign distorted outputs. At very high sparsity (≥70%), feature degradation exceeds recoverable threshold and MaskLoRA or full retraining becomes necessary.

### Mechanism 2: Expressive Parameter Subsets
Biases and LayerNorm parameters provide sufficient expressivity to compensate for pruning-induced output shifts. Biases control per-layer translations while LN scaling/bias parameters adjust affine transformations post-normalization. These parameters can shift and rescale activations globally without modifying sparse weight patterns, compensating for removed connections. At high sparsity (70%+), bias/LN-only methods diverge and MaskLoRA becomes necessary.

### Mechanism 3: Sparsity-Preserving Low-Rank Adaptation
MaskLoRA enables LoRA-style updates that merge without destroying sparsity patterns. Standard LoRA produces dense update matrix BA; merging destroys sparse zeros. MaskLoRA applies the pruning mask M during forward pass: (W + M⊙BA)x. This trains BA to respect sparsity, allowing W←W+M⊙BA merge with zero performance loss. Very high sparsity (80%+) may exceed low-rank approximation capacity.

## Foundational Learning

- **Magnitude Pruning**: Baseline pruning method that removes smallest-magnitude weights. Understanding why it fails for LLMs without retraining motivates the entire approach. Quick check: Why does magnitude pruning cause "perplexity explosion" in LLMs at moderate sparsity?

- **Low-Rank Adaptation (LoRA)**: Foundation for MaskLoRA/ScaleLoRA variants. Must understand BA decomposition and merge operation to grasp why standard LoRA breaks sparsity. Quick check: What happens to sparsity when you merge a dense BA matrix into a sparse W?

- **Layer Normalization Parameters**: LN scaling (γ) and bias (β) parameters are the most parameter-efficient retraining option (0.01-0.005% of model). Understanding their expressive role is critical. Quick check: How do γ and β parameters affect the output distribution of a normalized layer?

## Architecture Onboarding

**Component map:**
Pruned Model → Parameter Subset Selection → Retraining Loop → Merge (if LoRA)

**Critical path:**
1. Apply magnitude pruning with uniform layerwise sparsity (exclude embeddings, head)
2. Select parameter subset: start with biases+LN (0.05% params)
3. If sparsity ≥60%, use MaskLoRA (rank=16, α=32)
4. Retrain on C4 subset (1000 iterations, linear LR decay)
5. For MaskLoRA: merge via W←W+M⊙BA

**Design tradeoffs:**
- Biases+LN only: Maximum memory efficiency (8100 tps), fails at high sparsity
- MaskLoRA: Best accuracy at high sparsity, slower (4700 tps optimized)
- Layer-wise reconstruction: Lowest memory (~0.35% of layer activations), but underperforms global retraining at high sparsity

**Failure signatures:**
- Perplexity plateauing above 100: learning rate too low or parameter subset insufficient for sparsity level
- Accuracy degradation after merge: using LoRA-Prune instead of MaskLoRA
- OOM at 30B model: not using gradient checkpointing or trying full fine-tuning

**First 3 experiments:**
1. **Baseline validation**: Magnitude-prune OPT-2.7B to 50% sparsity; retrain biases only (0.034% params, 1000 iters). Verify perplexity approaches full FT (~15.63 target).
2. **Sparsity boundary test**: Same setup at 70% sparsity; compare bias-only vs LN-only vs MaskLoRA. Confirm MaskLoRA closes the gap.
3. **Reconstruction enhancement**: Apply MaskLoRA reconstruction to Wanda-pruned OPT-6.7B at 50% sparsity. Target: improve zero-shot accuracy from 47.14% to ~49.81%.

## Open Questions the Paper Calls Out

### Open Question 1
Can PERP methods be successfully integrated into iterative pruning regimes like Iterative Magnitude Pruning (IMP), or are they restricted to one-shot settings? The authors focus on one-shot settings, and it's unclear if feature realignment accumulates effectively over multiple prune-retrain cycles or if errors compound.

### Open Question 2
Can the performance gap between simple parameter-efficient retraining (e.g., biases only) and full fine-tuning be closed at extreme sparsity levels (>70%) without LoRA-based adapters? While MaskLoRA bridges this gap, the limitations of updating only intrinsic parameters at high compression rates are not fully resolved.

### Open Question 3
Do the findings regarding parameter-efficient retraining transfer effectively to structured pruning, where entire neurons or attention heads are removed? The methodology is restricted to unstructured and semi-structured sparsity, and structured pruning may alter how biases and LayerNorm parameters interact with remaining features.

## Limitations
- Most experiments focus on OPT models in 2.7B-30B range at moderate sparsity (30-50%), with limited validation at extreme sparsity
- C4 dataset used for retraining is only a subset of original pretraining data, raising generalization questions
- Limited multi-seed validation for many experiments, making improvements potentially sensitive to initialization choices
- Cross-model generalization (Mistral, Mixtral, LLaMA-2) has limited experimental support

## Confidence

**High confidence**: Biases and LayerNorm retraining effectiveness at moderate sparsity (30-50%) - multiple ablation studies consistently show 80-90% recovery of full retraining performance with <0.05% parameters

**Medium confidence**: MaskLoRA performance at high sparsity (≥60%) - fewer ablation studies, though perplexity and accuracy gains are statistically significant within reported experiments

**Medium confidence**: Layer-wise reconstruction improvements to retraining-free methods - only tested on Wanda-pruned models, but improvements are consistent across sparsity levels

**Low confidence**: Cross-model generalization (Mistral, Mixtral, LLaMA-2) - limited experiments, though similar trends observed

## Next Checks

1. **Multi-seed validation at extreme sparsity**: Repeat MaskLoRA experiments at 70-80% sparsity with 5 different random seeds to establish confidence intervals for perplexity and accuracy improvements

2. **Cross-dataset generalization test**: Evaluate models retrained on C4 subset using the same 1000-iteration protocol on entirely different datasets (e.g., OpenWebText, RealNews) to verify domain transfer

3. **Scaling law verification**: Test the bias+LN retraining effectiveness on models smaller than 2.7B (500M-1B) and larger than 30B (70B) to establish parameter-scaling boundaries for the parameter-efficient approach