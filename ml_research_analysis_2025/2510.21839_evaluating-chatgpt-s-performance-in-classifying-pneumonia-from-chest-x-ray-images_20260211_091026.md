---
ver: rpa2
title: Evaluating ChatGPT's Performance in Classifying Pneumonia from Chest X-Ray
  Images
arxiv_id: '2510.21839'
source_url: https://arxiv.org/abs/2510.21839
tags:
- pneumonia
- image
- chest
- images
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated ChatGPT\u2019s (gpt-4o) zero-shot ability\
  \ to classify chest X-rays as NORMAL or PNEUMONIA using four prompt designs. The\
  \ best-performing concise, feature-focused prompt achieved 74% accuracy, while reasoning-oriented\
  \ prompts performed worse."
---

# Evaluating ChatGPT's Performance in Classifying Pneumonia from Chest X-Ray Images

## Quick Facts
- **arXiv ID**: 2510.21839
- **Source URL**: https://arxiv.org/abs/2510.21839
- **Reference count**: 20
- **Primary result**: ChatGPT (gpt-4o) achieved 74% accuracy in zero-shot pneumonia classification from chest X-rays using concise, feature-focused prompts

## Executive Summary
This study evaluates ChatGPT's (gpt-4o) zero-shot capability to classify chest X-ray images as NORMAL or PNEUMONIA. The research systematically tests four prompt designs and finds that concise, feature-focused prompts achieve the highest accuracy (74%), while reasoning-oriented prompts perform worse. The results demonstrate that shorter, direct prompts are more effective for image-based classification than longer reasoning chains. The model struggled to recognize subtle medical features, indicating that while promising, ChatGPT's diagnostic reliability remains limited for clinical use without further advances in visual reasoning and medical fine-tuning.

## Method Summary
The study uses gpt-4o multimodal model with four prompt designs: minimal output, with features, concise reasoning, and step-by-step reasoning. A balanced test set of 400 chest X-ray images (200 NORMAL, 200 PNEUMONIA) was created from the Kaggle Chest X-Ray Images (Pneumonia) dataset. Images were preprocessed by resizing to max 2048px on longest side and saving as high-quality JPEG. The model was prompted to return strict JSON with features, label, and confidence score. Classification accuracy was measured across all prompt variants.

## Key Results
- Concise, feature-focused prompts achieved 74% accuracy, outperforming all other prompt designs
- Reasoning-oriented prompts resulted in lower performance (70.75% for step-by-step reasoning)
- Feature extraction prompts improved NORMAL image identification compared to minimal output (64.50%)
- Model struggled with subtle feature recognition, limiting diagnostic reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concise, feature-focused prompts improve zero-shot medical image classification accuracy compared to reasoning-heavy prompts.
- Mechanism: Direct prompts reduce task ambiguity, focusing the model's visual-linguistic attention on identifying and reporting key features relevant to the classification label. Longer reasoning prompts may encourage the model to generate spurious associations or get "lost" in intermediate textual generation steps that do not aid visual discrimination.
- Core assumption: The model has sufficient pre-existing knowledge of visual representations of pneumonia to activate via focused instruction; extra reasoning steps add noise rather than signal for this specific visual task.
- Evidence anchors:
  - [abstract] "concise, feature-focused prompts achieved the highest classification accuracy of 74\%, whereas reasoning-oriented prompts resulted in lower performance."
  - [section 4.3 Summary of Results] Table 1 shows "With feature output" at 74.00% accuracy, outperforming "Features + step-by-step reasoning" at 70.75% and "Minimal Output (No features)" at 64.50%.
  - [corpus] Weak direct corpus evidence on *prompt design* specifically for zero-shot LLM-based X-ray classification. Related corpus papers focus on supervised CNN/ViT architectures and do not directly compare prompt designs.

### Mechanism 2
- Claim: Prompting for explicit feature extraction ("features":"cloudy lung areas") improves classification performance over minimal output prompts.
- Mechanism: Requiring the model to generate textual features forces it to first process the image for those features before making a classification decision. This acts as a form of implicit chain-of-thought tied directly to the visual evidence, aligning the model's output with grounded observations. The features serve as a justification that the model itself uses.
- Core assumption: The textual features generated by the model ("cloudy lung areas") are positively correlated with the ground-truth diagnosis and are not hallucinated or misinterpreted.
- Evidence anchors:
  - [section 4.3 Summary of Results] "Adding short visual features (Prompt 2) helped the model better identify NORMAL images." and Table 1 shows accuracy jump from 64.50% (Minimal) to 74.00% (With features).
  - [section 4.2.2 Prompt 2] Describes the prompt format requesting visual features.
  - [corpus] Weak corpus evidence. Neighboring papers on explainable deep learning (arXiv:2510.21823, 2601.09814) focus on visual explanations (CAMs, etc.) from supervised models, not textual feature generation from zero-shot LLMs.

### Mechanism 3
- Claim: Extending reasoning (step-by-step) degrades performance compared to concise reasoning or feature-only prompts.
- Mechanism: Long reasoning chains in multimodal models may decouple from the visual input. The model is a text generator, and once it starts generating textual reasoning, it may follow linguistic patterns rather than continuously re-attending to the image evidence. This leads to "reasoning drift" away from the specific visual cues.
- Core assumption: The model's visual-attention mechanism is not strongly integrated with its extended text-generation process.
- Evidence anchors:
  - [section 5 Conclusion] "The results also show that prompts requiring longer or step-by-step reasoning did not improve performance—in fact, they slightly reduced accuracy."
  - [section 5 Conclusion] "suggesting that reasoning-oriented large language models... are not yet optimized for visual diagnostic reasoning".
  - [corpus] No direct corpus support for this specific mechanism in zero-shot X-ray classification. One cited work [19] suggests chain-of-thought can hurt performance where thinking makes humans worse, but this is a distinct domain.

## Foundational Learning

- **Concept: Zero-Shot Learning (ZSL)**
  - Why needed here: This is the core paradigm of the study. The model is evaluated on its ability to classify images without any examples or fine-tuning on the target dataset.
  - Quick check question: Does the model need to have seen any pneumonia X-ray images during its training to perform zero-shot classification? (No, ZSL means no task-specific training examples).

- **Concept: Multimodal Large Language Models (MLLMs)**
  - Why needed here: The study uses gpt-4o, which is an MLLM capable of processing both text (prompts) and images (X-rays). Understanding this architecture is key to understanding why prompt design is the primary lever for performance.
  - Quick check question: How does an MLLM differ from a standard CNN used in medical imaging? (An MLLM is a general-purpose model with text and image capabilities, while a CNN is typically a specialized model trained for a specific image-based task).

- **Concept: Prompt Engineering**
  - Why needed here: The primary independent variable in the study is the design of the prompt (minimal, features, concise reasoning, step-by-step). The results show that prompt structure significantly impacts performance.
  - Quick check question: Why would asking a model to "explain its reasoning step-by-step" potentially yield a different result than asking for a simple classification? (It changes the model's inference process, potentially introducing linguistic biases or causing it to stray from direct visual evidence).

## Architecture Onboarding

- **Component map**: User provides image -> Preprocessing Pipeline -> Prompt Construction (using best-performing template) -> gpt-4o Model Inference -> JSON Parsing -> Classification Label ("NORMAL" | "PNEUMONIA") & Confidence Score.

- **Critical path**: User provides image -> Preprocessing Pipeline -> Prompt Construction (using best-performing template) -> gpt-4o Model Inference -> JSON Parsing -> Classification Label ("NORMAL" | "PNEUMONIA") & Confidence Score.

- **Design tradeoffs**:
  - **Accuracy vs. Explainability**: The study shows a tradeoff. The best-performing prompt (With Features, 74%) provides some explainability via features, but asking for *more* explanation (step-by-step reasoning) *reduces* accuracy (70.75%). Concise reasoning (71.5%) sits in the middle.
  - **Speed vs. Reasoning**: Generating step-by-step reasoning requires more output tokens, increasing latency and cost. The study shows this extra cost not only fails to improve accuracy but slightly degrades it.

- **Failure signatures**:
  - **Hallucinated Features**: The model may describe visual features that are not present in the image to support a predetermined label, or misinterpret normal anatomical structures as pathology.
  - **Spurious Reasoning**: In step-by-step prompts, the reasoning chain may become detached from the visual evidence, following plausible medical narratives that are incorrect for the specific case.
  - **Overconfidence**: The model provides a confidence score. A failure mode is high confidence with low accuracy, indicating a poorly calibrated model.
  - **Format Violation**: The model might fail to return strict JSON, especially with longer reasoning prompts, breaking automated parsing.

- **First 3 experiments**:
  1. **Reproduce the 4-Prompt Comparison**: Implement the exact four prompt templates from the paper (Minimal, With Features, Concise Reasoning, Step-by-Step) on a small held-out set (e.g., 50 images) to validate the performance hierarchy (Features > Concise Reasoning > Step-by-Step > Minimal).
  2. **Prompt Ablation Study**: Test variations of the best-performing "Features" prompt. For example, a prompt asking for "3 specific visual features" vs. "a single sentence description" to see if the structure of the feature request impacts accuracy.
  3. **Confidence Calibration Analysis**: Plot the model's reported confidence scores against its actual accuracy (calibration curve). This will reveal if the model is overconfident (high confidence on wrong answers) or underconfident, a critical metric for clinical decision support not detailed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning on domain-specific datasets elevate GPT-4o's diagnostic accuracy beyond the 74% zero-shot baseline?
- Basis in paper: [explicit] The authors conclude that "specialized medical fine-tuning" is required to achieve clinical reliability.
- Why unresolved: The study was restricted to a zero-shot setting to test out-of-the-box capabilities, leaving the potential benefits of training on the 5,216 available images unexplored.
- What evidence would resolve it: A comparative experiment measuring performance deltas after fine-tuning the model on the Kaggle training set versus the zero-shot results.

### Open Question 2
- Question: Why does step-by-step reasoning degrade classification performance in multimodal models?
- Basis in paper: [explicit] The authors note that "reasoning-oriented large language models... are not yet optimized for visual diagnostic reasoning" and that longer prompts reduced accuracy.
- Why unresolved: The paper identifies the counter-intuitive result—that reasoning lowers accuracy (70.75%) compared to feature-focused prompts (74%)—but does not investigate the internal mechanism causing this distraction.
- What evidence would resolve it: An ablation study analyzing attention maps to determine if linguistic reasoning tokens overshadow visual feature extraction.

### Open Question 3
- Question: Does the optimal prompt design transfer effectively to adult chest X-ray datasets?
- Basis in paper: [inferred] The study utilized a dataset composed entirely of pediatric patients (ages 1–5), creating an unstated assumption regarding generalizability.
- Why unresolved: Pediatric X-rays have different anatomical proportions and density patterns compared to adults; the "concise, feature-focused" prompt may rely on visual cues specific to pediatric physiology.
- What evidence would resolve it: Replicating the experiment with the four prompt designs on a standard adult dataset (e.g., NIH ChestX-ray14) to verify if the 74% accuracy holds.

## Limitations
- The 74% accuracy is significantly below the ~90% accuracy of specialized supervised models
- Pediatric-only dataset limits generalizability to adult populations
- No confidence calibration metrics reported, leaving reliability of high-confidence predictions unknown

## Confidence

- **High Confidence**: The performance hierarchy of prompt types (Features > Concise Reasoning > Step-by-Step > Minimal) is well-supported by the results table and conclusion. The finding that longer reasoning chains degrade performance is consistent across reported metrics.
- **Medium Confidence**: The claim that concise, feature-focused prompts improve zero-shot classification is supported by the data but lacks strong corpus validation in the medical imaging domain. The mechanism (reasoning drift) is plausible but not empirically proven within this study.
- **Low Confidence**: The assertion that gpt-4o's visual-attention mechanism is poorly integrated with extended text-generation is speculative and not directly tested. The break condition for prompt design (pre-training lacking relevant features) is inferred but not experimentally validated.

## Next Checks
1. **Confidence Calibration Analysis**: Plot the model's reported confidence scores against actual accuracy to assess whether high-confidence predictions are trustworthy—a critical metric for clinical applications.
2. **Failure Case Analysis**: Manually review a sample of misclassified images to categorize error types (hallucination, subtle feature miss, etc.) and determine if errors are systematic or random.
3. **Cross-Dataset Generalization**: Test the best-performing prompt on an adult chest X-ray dataset to assess whether the pediatric-only training data limits the model's real-world applicability.