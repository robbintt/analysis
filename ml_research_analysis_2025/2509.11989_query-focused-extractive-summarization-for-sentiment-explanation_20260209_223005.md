---
ver: rpa2
title: Query-Focused Extractive Summarization for Sentiment Explanation
arxiv_id: '2509.11989'
source_url: https://arxiv.org/abs/2509.11989
tags:
- query
- sentiment
- summarization
- task
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of explaining the causes of sentiment
  in feedback documents through a specialized Query-Focused Summarization (QFS) task
  called Explicative Sentiment Summarization (ESS). The main difficulty lies in the
  linguistic dissonance between user queries and source documents, which can vary
  in language register and information content.
---

# Query-Focused Extractive Summarization for Sentiment Explanation

## Quick Facts
- arXiv ID: 2509.11989
- Source URL: https://arxiv.org/abs/2509.11989
- Reference count: 17
- The paper proposes a multi-bias framework to explain the causes of sentiment in feedback documents through specialized Query-Focused Summarization.

## Executive Summary
This paper addresses the challenge of explaining the causes of sentiment in feedback documents through a specialized Query-Focused Summarization (QFS) task called Explicative Sentiment Summarization (ESS). The main difficulty lies in the linguistic dissonance between user queries and source documents, which can vary in language register and information content. To overcome this, the authors propose a multi-bias framework that combines multiple query formulations to better align user intent with source text. They implement this through a Multi-Bias TextRank (MBTR) model that incorporates several query biases, along with an Information Content Regularization component to guide specificity. For the ESS task, they introduce sentiment-based query expansion and use sentiment classifier outputs as an additional bias.

## Method Summary
The proposed Multi-Bias TextRank (MBTR) model extends Biased TextRank by incorporating multiple query formulations through a compound bias vector. The method uses three main components: semantic biases (query expansion with frequent reference words), sentiment bias (probabilities from a sentiment classifier), and Information Content Regularization (penalizing sentences based on their embedding norm difference from reference summaries). The model ranks sentences using PageRank with a custom bias vector that combines these elements, then extracts the top-ranked sentences as the summary.

## Key Results
- MBTR significantly outperforms baseline models including MMR, QuerySum, and Biased TextRank.
- The best configuration (MBTR with alpha=0.1, beta=0.2) achieves ROUGE-SU4 F1 scores of 25.64, compared to 16.69 for the best baseline.
- Results demonstrate the effectiveness of combining multiple query formulations and incorporating sentiment information for explaining sentiment causes in feedback analysis.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining multiple query formulations into a compound bias appears to bridge the "linguistic dissonance" (register and information content gaps) between user intent and source text better than single-query approaches.
- **Mechanism:** The Multi-Bias TextRank (MBTR) aggregates similarity scores from multiple query encodings (e.g., frequent reference words, expanded phrases) via summation. This compound bias vector replaces the single bias vector in the PageRank recursion, effectively allowing a sentence to be boosted if it matches *any* of the query formulations.
- **Core assumption:** The specific information need is distributed across different linguistic registers (colloquial vs. formal) and granularities; a single query cannot capture this variance.
- **Evidence anchors:**
  - [abstract] "propose a multi-bias framework to help bridge this gap... supporting multiple query formulations"
  - [section 3.1] "Intuitively, this is analogous to humans reformulating questions from multiple perspectives..."
  - [corpus] Related papers like *FG-RAG* and *DETQUS* focus on graph structures or decomposition for QFS, but do not explicitly validate the specific "multi-bias summation" approach, suggesting this is a distinct architectural choice.
- **Break condition:** If the query formulations are semantically redundant or if the aggregation method (summation) disproportionately amplifies noisy signals from weaker queries.

### Mechanism 2
- **Claim:** Penalizing candidate sentences based on the difference between their embedding norm and a target "reference" norm (Information Content Regularization) likely guides the summary toward a desired level of specificity.
- **Mechanism:** The model calculates a penalty term ($\beta \Delta IC$) derived from the absolute difference between a candidate sentence's vector norm and the average vector norm of a guide summary. This penalty is subtracted from the bias vector in the ranking equation, suppressing sentences that are either too generic or too specific relative to the reference.
- **Core assumption:** The vector norm of a sentence embedding correlates strongly with "Information Content" (specificity), as defined by Amigó et al. (2022), and the development set references represent the ideal specificity.
- **Evidence anchors:**
  - [section 3.3] "We leverage this feature to disfavor candidate sentences by their distance from the targeted level of specificity."
  - [section 5] "Dampening ICR performs best at $\beta=0.1$... suggests ERT's lesser regularization requirement as benefiting from its inherent proximity..."
  - [corpus] No direct validation of the "norm-as-information-content" hypothesis was found in the provided corpus signals; related work generally focuses on content retrieval rather than specificity regularization via norms.
- **Break condition:** If the underlying sentence encoder (e.g., SBERT) does not encode specificity monotonically with vector magnitude in the target domain, the regularization may arbitrarily suppress relevant sentences.

### Mechanism 3
- **Claim:** Incorporating a "Sentiment Bias" derived from classifier probabilities allows the extractive process to function as an explanation engine rather than just a retrieval engine.
- **Mechanism:** A sentiment classifier predicts the probability of the target sentiment (e.g., "negative") for each sentence. This probability forms a bias vector that is combined with the semantic query biases. This forces the centrality algorithm to prefer sentences that not only match the query keywords but also contain the targeted affective signal.
- **Core assumption:** The sentences containing the highest concentration of the target sentiment polarity are the ones that best "explain" the cause of that sentiment.
- **Evidence anchors:**
  - [abstract] "...use sentiment classifier outputs as an additional bias."
  - [section 3.4.2] "...we can utilize the probabilistic confidence in this sentiment for every input sentence to construct a sentiment bias vector."
  - [corpus] The corpus contains "Multilingual Sentiment Analysis of Summarized Texts," suggesting a known interplay between summarization and sentiment, but limited evidence for using classifier *confidence* as a ranking bias in graph-based summarization.
- **Break condition:** If the sentiment classifier is poorly calibrated or if the "cause" of sentiment is discussed in neutral terms (e.g., "The wait time was 4 hours" is a factual cause of negative sentiment but may be classified as neutral).

## Foundational Learning

- **Concept: Graph-based Centrality (TextRank/PageRank)**
  - **Why needed here:** The proposed MBTR model is built directly on top of TextRank. Understanding that nodes (sentences) are ranked based on their connections (similarity) to other important nodes is critical.
  - **Quick check question:** How does adding a "bias" vector modify the standard random surfer model in PageRank?

- **Concept: Vector Space Models & Norms**
  - **Why needed here:** The paper relies on cosine similarity for semantic matching and, critically, on the vector *norm* (length) for the Information Content Regularization mechanism.
  - **Quick check question:** Does normalizing vectors to unit length (common in semantic search) destroy the "Information Content" signal required for this specific architecture?

- **Concept: Extractive vs. Abstractive Summarization**
  - **Why needed here:** The authors explicitly frame their work as "extractive" to ensure "output traceability" and avoid hallucinations, contrasting it with generative models.
  - **Quick check question:** Why would an extractive approach be preferred over an abstractive one in high-stakes feedback analysis, even if the resulting summary is less fluent?

## Architecture Onboarding

- **Component map:**
  Input -> Preprocessing -> Bias Generators -> MBTR + ICR -> Output

- **Critical path:**
  1. **Encoder Selection:** You must use distinct encoders for specific tasks (asymmetric `msmarco-distilbert-base-v4` for short query-to-phrase matching vs. symmetric `xlm-r-distilroberta` for sentence-to-sentence similarity).
  2. **Parameter Tuning:** The paper highlights extreme sensitivity in alpha ($\alpha$) and beta ($\beta$). The standard TextRank recommendation ($\alpha=0.85$) failed here; $\alpha=0.1$ was required to prioritize query-focus over centrality.

- **Design tradeoffs:**
  - **Reference-dependence:** The best-performing configuration (ERT) requires a development set to extract "Frequent Reference-Words," which may not exist in zero-shot scenarios. The "Sentiment Biases" configuration is more general but performed slightly lower in experiments.
  - **Sentence vs. Phrase:** The system indexes sentences for the final summary but must index phrases (NP/VP) for the query expansion step, increasing preprocessing complexity.

- **Failure signatures:**
  - **Generic Summaries:** If $\beta$ is too low, the model may extract high-centrality but low-specificity sentences (e.g., "The product is great").
  - **Topic Drift:** If the sentiment classifier is inaccurate, the "Sentiment Bias" will pull the summary toward sentences that are sentiment-heavy but irrelevant to the specific entity query.

- **First 3 experiments:**
  1. **Baseline Validation:** Implement standard Biased TextRank (BTR) to verify the $\alpha=0.85$ vs $\alpha=0.1$ performance drop on your own data.
  2. **Ablation on Regularization:** Test MBTR with $\beta=0$ (off) vs. $\beta=0.2$ to confirm if "Information Content" norms actually correlate with summary quality in your domain.
  3. **Encoder Swap:** Swap the symmetric SBERT encoder for the asymmetric one in the main ranking loop to observe if query-document dissonance improves or if semantic coherence breaks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Explicative Sentiment Summarization (ESS) task and the Multi-Bias TextRank (MBTR) model be effectively adapted to publicly available Aspect-Based Sentiment Analysis (ABSA) datasets?
- Basis in paper: [explicit] The authors state in the Conclusion that "In future works, we plan on adapting ABSA datasets to the ESS task."
- Why unresolved: The current experiments rely exclusively on a proprietary dataset, leaving the model's efficacy on standard public benchmarks unverified.
- What evidence would resolve it: Successful application and evaluation of the MBTR model on standard ABSA datasets converted to the ESS format, demonstrating comparable performance to the proprietary results.

### Open Question 2
- Question: How does the MBTR framework compare to modern Large Language Models (LLMs) regarding factuality and hallucination rates in sentiment explanation?
- Basis in paper: [explicit] The paper notes the study was "completed before the advent of large language models (LLMs) like ChatGPT" and explicitly critiques abstractive models for "factuality and hallucination" issues.
- Why unresolved: The rapid evolution of generative AI introduces new paradigms for summarization that were not benchmarked against the proposed extractive method.
- What evidence would resolve it: A comparative study measuring ROUGE scores and factual consistency metrics (e.g., FACTSCORE) between MBTR and contemporary LLM-based summarizers on the same ESS dataset.

### Open Question 3
- Question: Can the Compound Bias-Focused Summarization (CBFS) framework be integrated into non-TextRank QFS models without performance degradation?
- Basis in paper: [explicit] The conclusion lists "integrating other QFS models into the CBFS framework" as a goal.
- Why unresolved: The paper concretizes the framework solely through the MBTR model; it is unproven whether the multi-bias reduction strategy is portable to other architectures (e.g., transformer-based attention models).
- What evidence would resolve it: Implementation of the CBFS multi-bias logic into alternative architectures (e.g., BERT-based extractive models) showing statistically significant improvements over their single-bias counterparts.

### Open Question 4
- Question: Is the Information Content Regularization (ICR) component transferable to datasets where the target specificity level is unknown or variable?
- Basis in paper: [inferred] The ICR mechanism depends on a guiding matrix $G$ derived from reference sentences in the development set to calculate a target Information Content norm. This relies on the assumption that a "desired level of specificity" can be pre-calculated from a static reference set.
- Why unresolved: It is unclear if the specific beta weights and target norms learned from the proprietary dataset generalize to domains with different linguistic registers or summary styles without manual recalibration.
- What evidence would resolve it: Ablation studies on out-of-domain datasets showing that the specific IC penalty ($\beta \Delta IC$) improves summary quality without requiring tuning based on a development set.

## Limitations
- The primary limitation is the proprietary nature of the evaluation dataset, which prevents independent verification of the claimed performance gains.
- The paper relies on a single performance metric (ROUGE-SU4) without reporting human evaluation of the generated summaries' explanatory quality.
- The Information Content Regularization mechanism assumes that vector norms correlate with specificity in a monotonic way, but this relationship is not empirically validated in the target domain.

## Confidence
- **High Confidence:** The MBTR architecture and its general approach of combining multiple query formulations is well-specified and follows established graph-based ranking principles. The improvement over baseline models (BTR, MMR, QuerySum) on the reported metric is clearly demonstrated.
- **Medium Confidence:** The specific contribution of the Information Content Regularization component is plausible but relies on an unvalidated assumption about the relationship between vector norms and specificity. The sensitivity to hyperparameters (α=0.1, β=0.2) is noted but the underlying reasons are not fully explained.
- **Low Confidence:** The quality and impact of the sentiment classifier on the final summary are uncertain, as its architecture, training data, and performance are not disclosed. The claim that MBTR effectively bridges "linguistic dissonance" is supported by the results but lacks a detailed linguistic analysis.

## Next Checks
1. **IC Regularization Ablation:** Reproduce the MBTR model with β=0 (no regularization) and β=0.2 on a public dataset to measure the specific impact of Information Content Regularization on summary quality and specificity.
2. **Sentiment Classifier Impact:** Replace the paper's unspecified sentiment classifier with a standard, pre-trained model (e.g., a BERT-based sentiment analyzer) and measure the variance in summary performance to quantify the classifier's contribution.
3. **Human Evaluation:** Conduct a small-scale human evaluation on a subset of generated summaries to assess whether the extractive summaries are truly explanatory of the sentiment cause, beyond their ROUGE scores.