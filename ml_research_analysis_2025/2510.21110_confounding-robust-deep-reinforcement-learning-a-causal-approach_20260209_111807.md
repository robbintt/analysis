---
ver: rpa2
title: 'Confounding Robust Deep Reinforcement Learning: A Causal Approach'
arxiv_id: '2510.21110'
source_url: https://arxiv.org/abs/2510.21110
tags:
- learning
- policy
- data
- demonstrator
- confounded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies off-policy reinforcement learning from confounded
  data, where unobserved confounders violate the standard Markov Decision Process
  assumption of no unmeasured confounders (NUC). Building on the Deep Q-Network (DQN),
  the authors introduce Causal DQN, a novel algorithm that learns robust abstractions
  from confounded data by leveraging causal inference techniques.
---

# Confounding Robust Deep Reinforcement Learning: A Causal Approach

## Quick Facts
- arXiv ID: 2510.21110
- Source URL: https://arxiv.org/abs/2510.21110
- Reference count: 40
- Primary result: Causal DQN learns robust policies from confounded demonstrations, outperforming standard DQN on 12 Atari games by handling unobserved confounders through causal inference.

## Executive Summary
This paper addresses the challenge of learning from confounded demonstrations in reinforcement learning, where unobserved variables create spurious correlations between actions and outcomes. The authors introduce Causal DQN, which extends the Deep Q-Network framework with causal inference techniques to learn robust abstractions despite confounding bias. By deriving a lower bound on the optimal value function using a Causal Bellman equation, the algorithm finds policies that perform well even in the worst-case environment compatible with observations. Empirically, Causal DQN demonstrates significant improvements over standard DQN baselines across twelve confounded Atari games, including seven games where it surpasses the demonstrator's performance.

## Method Summary
The method builds on DQN by introducing a causal Bellman optimality equation that accounts for confounding through pessimistic updates when observed actions differ from target actions. The algorithm maintains a Q-network that, for each state, computes targets for all possible actions: if an action matches the demonstrated action, it uses the observed reward; otherwise, it applies a lower bound on the reward plus the worst-case next state value. Training minimizes the mean squared error over all actions simultaneously. The approach requires bounded rewards and discrete action spaces, and relies on sampling to estimate worst-case next states.

## Key Results
- Causal DQN consistently outperforms standard DQN (CNN and LSTM variants) on 12 confounded Atari games
- Achieves higher returns than the demonstrator in 7 out of 12 games
- Demonstrates significant improvements in robustness and sample efficiency under confounded settings
- Ablation shows the causal updates (not just architecture) solve the confounding problem

## Why This Works (Mechanism)

### Mechanism 1: Causal Bellman Optimality Equation
The algorithm derives a valid lower bound on Q* even with unobserved confounders by splitting updates: matched actions use standard optimistic updates, while mismatched actions apply pessimistic penalties using bounded rewards and worst-case next states. This works under the assumption of bounded rewards [a, b], where the worst-case penalty becomes undefined if bounds are unknown or infinite.

### Mechanism 2: Robust Policy Optimization
The agent optimizes for worst-case compatible environments by assuming worst-case next states for actions not taken by the demonstrator. This pessimism counteracts spurious correlations from unobserved confounders, assuming safe policies perform well even under worst-case dynamics. Overly aggressive pessimism may lead to overly conservative policies that avoid profitable risks.

### Mechanism 3: Multi-action Q-network Training
Unlike standard DQN which updates only the taken action, Causal-DQN computes targets for every action simultaneously. This propagates safety information about counterfactual actions, with the assumption that state representations are shared across actions. In continuous action spaces, this becomes intractable.

## Foundational Learning

- **Concept: Off-Policy Evaluation (OPE)**
  - Why needed: The method relies on learning from data generated by a different policy (demonstrator).
  - Quick check: Can you explain why the standard Q-learning assumption P(s'|s,a) fails when data comes from a confounded demonstrator?

- **Concept: Partial Identification (Bounding)**
  - Why needed: The paper finds bounds on the value function rather than exact transition dynamics, which is impossible with confounders.
  - Quick check: Why is finding a lower bound often more useful than a point estimate in safety-critical RL?

- **Concept: MDP vs. CMDP**
  - Why needed: The environment is modeled as a Confounded MDP where unobserved variables affect actions, states, and rewards simultaneously.
  - Quick check: How does the graphical structure of a CMDP differ from a standard MDP regarding edges between Action and State?

## Architecture Onboarding

- **Component map:** Demonstrator (LSTM-based actor-critic) -> Replay Buffer (masked transitions) -> Learner (CNN-based Causal-DQN) -> Masked Atari environments
- **Critical path:** Step 9 calculation in Algorithm 1 - efficiently computing min over possible next states for conservative branch through random sampling
- **Design tradeoffs:** Robustness vs. optimality depends on bound tightness; simpler CNN learner chosen to prove algorithm (not architecture) handles confounding
- **Failure signatures:** Zero support when demonstrator never visits accessible regions; observational overfitting to spurious features
- **First 3 experiments:**
  1. Sanity Check (Pong): Causal-DQN vs. Standard DQN on "Confounded Pong" (opponent masked)
  2. Ablation on Architecture: CNN-based Causal-DQN vs. LSTM-based DQN on same confounded data
  3. Saliency Map Analysis: Visualize attention - successful Causal-DQN focuses on stable causal features (ball) vs. unstable confounded features (opponent/score)

## Open Questions the Paper Calls Out

- **Multi-step extension:** Can the Causal Bellman equation be extended to multi-step returns, eligibility traces, and advantage-based critics? Current derivation only handles single-step Q-values.
- **Zero support handling:** How to handle scenarios where demonstrator has zero support in regions relevant to learner's observations? Current approach fails when demonstrator never visits visible state-action regions.
- **Continuous control extension:** Can the framework extend to continuous action spaces and policy-gradient methods? Current formulation assumes discrete, finite action domains.

## Limitations

- The method relies on bounded reward assumptions, with unclear robustness when bounds are unknown or poorly estimated
- Empirical evaluation is limited to synthetic confounding scenarios in Atari environments rather than real-world applications
- Scalability to continuous state and action spaces remains an open challenge
- Performance depends critically on the demonstrator having support in unmasked regions

## Confidence

- **High Confidence:** Theoretical foundation of Causal Bellman Optimality Equation and its derivation as Q* lower bound
- **Medium Confidence:** Empirical results showing Causal DQN outperforming DQN variants on synthetic Atari benchmarks
- **Low Confidence:** Scalability to continuous spaces and sensitivity to reward bound specification

## Next Checks

1. **Reward Bound Sensitivity:** Systematically evaluate Causal DQN performance across varying reward bounds [a, b] to identify sensitivity to this critical parameter
2. **Continuous Action Extension:** Develop and test a continuous-action variant of Causal DQN to assess scalability beyond discrete action spaces
3. **Real-world Confounding:** Apply Causal DQN to a benchmark with naturally occurring confounding (e.g., confounded recommendation system data) rather than synthetically masked observations