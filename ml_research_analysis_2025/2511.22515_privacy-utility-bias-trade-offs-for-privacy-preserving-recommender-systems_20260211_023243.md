---
ver: rpa2
title: Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems
arxiv_id: '2511.22515'
source_url: https://arxiv.org/abs/2511.22515
tags:
- privacy
- items
- epsilon
- dpsgd
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates how two privacy mechanisms\u2014\
  DPSGD and LDP\u2014affect recommendation accuracy and bias across four recommender\
  \ models (NCF, BPR, SVD, VAE) on MovieLens-1M and Yelp datasets. DPSGD degrades\
  \ utility more sharply than LDP, especially for VAE and sparse datasets like Yelp."
---

# Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems

## Quick Facts
- arXiv ID: 2511.22515
- Source URL: https://arxiv.org/abs/2511.22515
- Reference count: 40
- Key outcome: This study systematically evaluates how two privacy mechanisms—DPSGD and LDP—affect recommendation accuracy and bias across four recommender models (NCF, BPR, SVD, VAE) on MovieLens-1M and Yelp datasets. DPSGD degrades utility more sharply than LDP, especially for VAE and sparse datasets like Yelp. NCF under DPSGD maintains high accuracy (within 10% of non-private) even at epsilon ≈ 1, while SVD and BPR suffer larger drops for niche users. LDP preserves existing bias patterns, whereas DPSGD generally reduces popularity bias (negative Popularity Lift) but at higher utility costs. VAE is highly sensitive, collapsing under sparsity. No single mechanism is universally superior; optimal trade-offs depend on model, dataset, and user/item subgroups.

## Executive Summary
This paper presents a systematic evaluation of privacy-utility-bias trade-offs in recommender systems using two privacy mechanisms: DPSGD and LDP. The study examines four recommender models (NCF, BPR, SVD, VAE) across two datasets (MovieLens-1M and Yelp). Results show that DPSGD causes sharper utility degradation than LDP, particularly affecting VAE and sparse datasets. NCF performs best under DPSGD, maintaining accuracy within 10% of non-private models even at low epsilon values. The study finds that while LDP preserves existing bias patterns, DPSGD tends to reduce popularity bias at the cost of higher utility loss.

## Method Summary
The authors evaluate privacy-utility-bias trade-offs by applying DPSGD and LDP mechanisms to four recommender models across two datasets. They measure recommendation accuracy using NDCG and examine bias through Popularity Lift metrics. The evaluation spans various epsilon values to assess privacy-utility relationships. Models are tested on both dense (MovieLens-1M) and sparse (Yelp) datasets to understand performance across different data characteristics. The study systematically compares how each privacy mechanism affects different user and item subgroups.

## Key Results
- DPSGD degrades utility more sharply than LDP, especially for VAE and sparse datasets like Yelp
- NCF under DPSGD maintains high accuracy (within 10% of non-private) even at epsilon ≈ 1
- SVD and BPR suffer larger drops for niche users under DPSGD
- LDP preserves existing bias patterns, while DPSGD generally reduces popularity bias (negative Popularity Lift)
- VAE is highly sensitive, collapsing under sparsity
- No single mechanism is universally superior; optimal trade-offs depend on model, dataset, and user/item subgroups

## Why This Works (Mechanism)
The effectiveness of privacy mechanisms depends on how they interact with model architecture and data characteristics. DPSGD adds noise during gradient updates, which affects models differently based on their sensitivity to gradient perturbations. NCF's implicit feedback handling makes it more robust to noise, while VAE's reconstruction-based approach amplifies noise effects, especially in sparse data. LDP adds noise at the data collection level, preserving model integrity but maintaining existing bias patterns. The differential impact on popularity bias occurs because DPSGD's gradient noise disrupts the amplification of popular items in the learning process.

## Foundational Learning
**Differential Privacy (DP)**: A framework ensuring individual data points cannot be distinguished in the output - needed to understand privacy guarantees; quick check: ε=1 provides reasonable privacy while ε=0.1 is very strict.
**Popularity Bias**: Systematic preference for popular items in recommendations - needed to evaluate fairness; quick check: measured via Popularity Lift comparing top-k vs random recommendations.
**NDCG (Normalized Discounted Cumulative Gain)**: Ranking quality metric giving higher weight to relevant items at top positions - needed to measure recommendation accuracy; quick check: ranges 0-1, higher is better.
**Recommender Models**: NCF (neural collaborative filtering), BPR (Bayesian personalized ranking), SVD (matrix factorization), VAE (variational autoencoder) - needed to understand model-specific behaviors; quick check: NCF handles implicit feedback, VAE reconstructs user-item interactions.
**Epsilon (ε)**: Privacy budget parameter - smaller ε means stronger privacy but more noise; quick check: ε < 1 is strict privacy, ε > 10 approaches non-private performance.

## Architecture Onboarding
**Component Map**: Data → Privacy Mechanism (DPSGD/LDP) → Recommender Model (NCF/BPR/SVD/VAE) → Evaluation (NDCG, Popularity Lift)
**Critical Path**: User interaction data → Privacy transformation → Model training → Recommendation generation → Accuracy and bias measurement
**Design Tradeoffs**: DPSGD offers stronger theoretical privacy but causes more utility loss; LDP preserves utility better but maintains existing biases; model choice significantly impacts privacy-utility balance
**Failure Signatures**: Sharp NDCG drops indicate model sensitivity to privacy noise; unchanged Popularity Lift suggests bias preservation; VAE collapse indicates sparsity sensitivity
**First Experiments**: 1) Compare NCF with and without DPSGD at ε=1 on MovieLens-1M; 2) Test LDP on sparse Yelp data across all models; 3) Measure Popularity Lift changes for SVD under varying ε values

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on only two privacy mechanisms (DPSGD and LDP) leaves alternative DP techniques unexplored
- Dataset choice (MovieLens-1M, Yelp) may not generalize to denser or categorical-only data
- Bias analysis relies solely on Popularity Lift, missing other forms of bias (e.g., gender, race)
- Hyperparameter sensitivity (e.g., learning rate under DP) is not explored
- No ablation of DP noise impact on convergence is provided

## Confidence
**High**: DPSGD vs. LDP utility comparisons across models and datasets
**Medium**: Bias-reduction claims using Popularity Lift metric
**Low**: Sparsity-specific VAE sensitivity based on one dataset pair

## Next Checks
1. Test PATE and amplification-by-shuffling mechanisms to compare trade-offs beyond DPSGD/LDP
2. Evaluate bias using multiple metrics (e.g., item exposure entropy, subgroup fairness) to verify LDP's "preservation" claim
3. Experiment with transformer-based recommenders to assess if VAE's sensitivity to sparsity is model-specific or architecture-general