---
ver: rpa2
title: 'Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering
  Attractors'
arxiv_id: '2510.00586'
source_url: https://arxiv.org/abs/2510.00586
tags:
- attention
- malicious
- attack
- attractor
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Eyes-on-Me, a modular poisoning attack against
  retrieval-augmented generation (RAG) systems. The core idea is to decompose an adversarial
  document into reusable Attention Attractors and swappable Focus Regions, enabling
  scalable and cost-effective attacks without repeated optimization.
---

# Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors

## Quick Facts
- **arXiv ID:** 2510.00586
- **Source URL:** https://arxiv.org/abs/2510.00586
- **Reference count:** 40
- **Primary result:** Modular RAG poisoning attack achieving 57.8% average attack success rate, 2.6× improvement over prior methods

## Executive Summary
Eyes-on-Me introduces a scalable approach to RAG poisoning by decomposing adversarial documents into reusable Attention Attractors and swappable Focus Regions. This modular design enables attackers to optimize attention concentration once and then insert different payloads without retraining, dramatically reducing attack costs. The method identifies influential attention heads through correlation analysis and optimizes attractors via HotFlip to maximize attention on designated regions. Experiments across 18 RAG configurations demonstrate 57.8% average attack success rate with strong cross-model transferability (96.6-97.8% black-box success), establishing a new threat model for RAG systems.

## Method Summary
The Eyes-on-Me attack optimizes adversarial documents through a three-stage process: (1) Head Selection identifies attention heads strongly correlated with downstream task performance, (2) Attractor Optimization uses HotFlip to iteratively refine tokens that concentrate attention on Focus Regions, and (3) Payload Insertion places semantic baits and malicious instructions into these regions. The modular decomposition decouples expensive optimization from payload insertion, enabling scalable attacks across different triggers and tasks. The method employs perplexity constraints to maintain document fluency and evade detection while achieving strong transfer across retrievers, generators, and triggers without retraining.

## Key Results
- Achieves 57.8% average attack success rate across 18 RAG configurations, a 2.6× improvement over baselines
- Demonstrates 96.6-97.8% black-box success rate across 5 retrievers and 5 generators without retraining
- Shows near-perfect transfer for semantically related triggers (e.g., 100% for "infection"→"amazon") but variable transfer for unrelated triggers (28% for "president"→"netflix")

## Why This Works (Mechanism)

### Mechanism 1: Modular Decoupling of Attack Components
The core innovation decomposes adversarial documents into reusable Attention Attractors and swappable Focus Regions. Attackers optimize attractor tokens once to concentrate model attention on a designated region, then insert any payload into that region. This decouples expensive optimization from payload insertion, enabling scalable attacks at near-zero cost for new targets. The mechanism assumes attention patterns induced by attractors generalize across different payloads and downstream content without requiring joint optimization.

### Mechanism 2: Attention-Steering via Influential Head Selection
Rather than optimizing all attention heads, Eyes-on-Me identifies and steers a small subset of heads with strong correlation (>0.9) to downstream metrics. By maximizing attention from these correlated heads to the Focus Region, the attack amplifies the influence of any payload placed there. The mechanism assumes these correlated heads are causal drivers of output behavior, making their manipulation sufficient to systematically increase task success.

### Mechanism 3: Cross-Model Transferability via Shared Attention Vulnerabilities
Optimized attractors transfer to unseen retrievers and generators because they exploit fundamental, architecture-agnostic attention mechanisms. Since dense retrievers and instruction-tuned LLMs share common attention processing patterns, the attractor generalizes without retraining. The mechanism assumes identified attention vulnerabilities (e.g., concentration on summary tokens) are consistent across model families and not unique to the source model.

## Foundational Learning

- **Transformer Attention Mechanisms (Multi-Head Self-Attention)**
  - Why needed here: The attack manipulates attention at the head level; understanding how attention scores are computed and aggregated is essential for following the steering mechanism.
  - Quick check question: Can you explain how attention scores are calculated from Q, K, V matrices and how multi-head attention differs from single-head attention?

- **RAG System Architecture (Retriever + Generator Pipeline)**
  - Why needed here: The attack targets both components; knowing how retrievers rank documents and how generators condition on context clarifies the dual optimization objective.
  - Quick check question: In a RAG system, how does the retriever influence what the generator sees, and what are common similarity metrics used for ranking?

- **Gradient-Based Adversarial Optimization (HotFlip)**
  - Why needed here: The attractor tokens are optimized via HotFlip; understanding discrete adversarial search with gradient approximations is key to implementing the attack.
  - Quick check question: How does HotFlip use directional derivatives to estimate the effect of token substitutions, and what are its limitations compared to continuous optimization?

## Architecture Onboarding

- **Component map:**
  - Trigger phrase t -> Head Selection (correlation analysis) -> Attractor Optimization (HotFlip) -> Payload Insertion -> Poisoned document d_m

- **Critical path:**
  1. Accurate head correlation analysis (must use held-out data to avoid overfitting)
  2. Balanced attention loss optimization (too few heads → weak steering; too many → noise)
  3. Perplexity filtering to ensure fluency and evade PPL-based defenses

- **Design tradeoffs:**
  - Attention correlation threshold (τ_corr): Higher values increase precision but may miss relevant heads (Table 4a shows peak at ~0.85)
  - Trigger frequency (α): Rare triggers (α < 0.05%) yield high R-ASR (>85%); common triggers (1-5%) drastically reduce success (<5%) (Table 4b)
  - Attractor length: Non-monotonic effect; 7-token attractors outperform shorter ones due to stronger steering vs. semantic disruption (Table 3b)

- **Failure signatures:**
  - Low transfer across triggers: Inconsistent semantic generalization (e.g., 28% for "president"→"netflix" vs. 100% for "infection"→"amazon"; Table 2c)
  - Detection by attention-based defenses: May evade current defenses (Table 9), but novel defenses targeting head-specific concentration could flag attractors
  - Task complexity bottleneck: Complex instructions (e.g., URL insertion) achieve <10% ASR vs. simple forced generation (>65%) (Table 3c)

- **First 3 experiments:**
  1. Replicate head correlation analysis: On a new RAG configuration (different retriever/generator), compute head correlations and verify that high-correlation heads exist and align with task performance.
  2. Test attractor transfer: Take a pre-optimized attractor from a source model (e.g., BCE retriever + Qwen2.5-0.5B generator) and evaluate ASR on a black-box target (e.g., GPT-4o-mini) without re-optimization.
  3. Evaluate against defenses: Apply PPL filtering, paraphrasing, and noise insertion to poisoned documents; measure G-ASR degradation and compare to baselines (per Table 5).

## Open Questions the Paper Calls Out

1. **Attention-based defenses:** Can defenses be designed to detect manipulation of the small subset of influential attention heads targeted by Eyes-on-Me? The authors note existing attention-based defenses fail because the attack perturbs only 10-15% of heads, avoiding broad anomalies.

2. **Reranker interaction:** How does Eyes-on-Me perform against RAG systems that use rerankers or retrievers with mean pooling aggregation? The paper acknowledges this interaction is unexplored and that influence may be weakened under mean pooling architectures.

3. **Semantic transfer determinants:** What determines successful semantic transfer of attractors across different trigger phrases? The paper demonstrates dramatic variance (100% to 28% transfer) but does not characterize what "trigger proximity" means.

4. **Larger model scaling:** How does attack efficacy scale with generator model size beyond the 0.5B-2B parameter range tested? The paper did not run optimization on larger open models or systematically test correlation patterns at scale.

## Limitations

- Limited evaluation scope to specific model families (dense retrievers, instruction-tuned LLMs) and relatively simple poisoning tasks
- Correlation-causation assumption in head selection mechanism not empirically validated
- Defense evaluation limited to existing methods; emerging attention-based defenses not tested

## Confidence

- **High Confidence:** Modular decomposition mechanism and scalability benefits (2.6× improvement, strong cross-model transfer)
- **Medium Confidence:** Attention-steering via influential head selection (plausible but relies on correlation-causation assumption)
- **Low Confidence:** Universality of cross-model transfer and defense against all attention-based defenses (overreaching claims)

## Next Checks

1. **Causal validation of head selection:** Design an ablation study to test whether removing selected influential heads degrades attack success more than removing non-selected heads, validating correlation implies causation.

2. **Cross-architecture transfer testing:** Evaluate Eyes-on-Me against RAG systems with rerankers, hybrid search, or different attention mechanisms (e.g., mean pooling) to test claimed universality.

3. **Defense against attention-based detectors:** Implement a defense that flags documents with abnormally high attention concentration on specific tokens, testing whether Eyes-on-Me evades this novel defense.