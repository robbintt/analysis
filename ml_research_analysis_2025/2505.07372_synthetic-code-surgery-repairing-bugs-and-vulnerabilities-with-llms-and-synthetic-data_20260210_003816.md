---
ver: rpa2
title: 'Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic
  Data'
arxiv_id: '2505.07372'
source_url: https://arxiv.org/abs/2505.07372
tags:
- data
- code
- quality
- synthetic
- vulrep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel methodology for enhancing Automated
  Program Repair (APR) through synthetic data generation utilizing Large Language
  Models (LL
---

# Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data

## Quick Facts
- arXiv ID: 2505.07372
- Source URL: https://arxiv.org/abs/2505.07372
- Authors: David de-Fitero-Dominguez; Antonio Garcia-Cabot; Eva Garcia-Lopez
- Reference count: 0
- Primary result: 19.82% Perfect Prediction rate on VulRepair test set using fine-tuned Qwen 2.5 Coder 7B with filtered synthetic data

## Executive Summary
This paper introduces a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs) and multi-model evaluation. The approach generates 30,000 synthetic bug-fix pairs across 12 programming languages and 13 vulnerability types, then employs a cross-model evaluation system where six different LLMs score each sample across five criteria. A filtering threshold of 8.5 on weighted average scores produces approximately 21,000 high-quality samples that, when used to fine-tune a Qwen 2.5 Coder 7B model, achieve a 19.82% Perfect Prediction rate on the challenging VulRepair test set—a 2.64× improvement over the baseline model trained only on real data.

## Method Summary
The methodology generates synthetic bug-fix pairs using six frontier LLMs (Llama-3.1, Qwen2.5, Mistral, Gemma) with structured XML prompts, then employs the same set of models as evaluators to score each sample across five criteria (correctness, code quality, length, security, performance, completeness) on a 0-10 scale. A weighted average with thresholds filters the data, retaining samples scoring above 8.5. The filtered dataset (~21k samples) fine-tunes a Qwen 2.5 Coder 7B model using LoRA (r=16, α=32, dropout=0.1) for three epochs. The approach demonstrates significant performance gains on the VulRepair benchmark while addressing the data scarcity challenge in APR research.

## Key Results
- 19.82% Perfect Prediction rate on VulRepair test set, representing a 2.64× improvement over baseline (7.50%)
- 17.18% Perfect Prediction rate with only the filtered synthetic dataset (vulrep_synt_85), outperforming baseline by 2.29×
- Statistical significance confirmed via ANOVA (p < 0.05) and Tukey's HSD post-hoc test
- Filtering threshold of 8.5 yields optimal performance, with lower thresholds degrading results

## Why This Works (Mechanism)
The methodology leverages cross-model evaluation to reduce individual LLM biases and generate higher-quality synthetic training data. By having multiple evaluators score each sample, the approach creates a more robust quality assessment that identifies truly valuable bug-fix pairs while filtering out noisy or incorrect examples. The weighted scoring system prioritizes correctness (30%) while balancing other important factors like code quality and security. This quality-over-quantity approach ensures the fine-tuned model learns from reliable examples rather than memorizing incorrect patterns, leading to superior generalization on the test set.

## Foundational Learning
- **Automated Program Repair (APR)**: The goal is auto-generating patches for buggy code. Understanding APR and its standard evaluation metric (Perfect Prediction/PP%) is fundamental since the entire paper improves upon this field.
  - Quick check: What does a Perfect Prediction rate of 17.18% signify in the context of the VulRepair test set?

- **Fine-Tuning with LoRA (Low-Rank Adaptation)**: The paper uses LoRA to efficiently fine-tune the Qwen 2.5 Coder 7B model instead of full fine-tuning.
  - Quick check: Why would the authors choose LoRA over full fine-tuning for a 7B parameter model?

- **LLM-as-a-Judge / Cross-Model Evaluation**: This is the core novel methodology—using LLMs not just to generate but to score and filter data.
  - Quick check: How does using multiple LLMs as evaluators potentially improve the quality assessment over using a single LLM?

## Architecture Onboarding
- **Component map**: Generators (6 LLMs) → Structured XML generation → Evaluators (same 6 LLMs) → Weighted scoring → Filter (threshold 8.5) → Filtered dataset (~21k) → Fine-tune Qwen 2.5 Coder 7B with LoRA → Evaluation on VulRepair test set → ANOVA + Tukey's HSD for significance

- **Critical path**: Prompt Generation → Model Invocation (Generators) → Validation of structured output (XML/JSON) → Scoring (Evaluators) → Weighted Score Aggregation → Filtering (Threshold 8.5) → Data Preprocessing → Model Fine-Tuning (LoRA) → Inference on VulRepair test set → Perfect Prediction calculation → Statistical Significance Testing

- **Design tradeoffs**: Quality vs. Quantity (smaller filtered dataset vs. larger unfiltered), Computational Cost vs. Rigor (expensive multi-model evaluation), Decoding Strategy (sampling vs. beam search)

- **Failure signatures**: Low Validity Rate (XML/JSON parsing failures), Score Skew (non-discriminative criteria), Statistical Insignificance (high p-value from ANOVA)

- **First 3 experiments**: Baseline Reproduction (train on VulRepair alone), Ablation on Filter Threshold (test different thresholds), Single vs. Multi-Evaluator (compare bias reduction)

## Open Questions the Paper Calls Out
None

## Limitations
- The cross-model evaluation approach is computationally expensive, requiring evaluation with models up to 72B parameters, raising scalability concerns for real-world deployment
- The 8.5 filtering threshold appears somewhat arbitrary without systematic sensitivity analysis across different threshold values
- Generalizability to programming languages and bug types beyond the 13 covered in VulRepair remains untested

## Confidence
- **High confidence** in the reported statistical significance (ANOVA p < 0.05) and overall methodology design
- **Medium confidence** in the absolute performance numbers due to potential undocumented implementation details in the evaluation pipeline
- **Low confidence** in the long-term practical utility without further testing on diverse, real-world codebases

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary the filtering threshold (7.0, 8.0, 8.5, 9.0) and measure the corresponding impact on downstream APR performance to validate the optimality of the 8.5 threshold

2. **Cross-Domain Generalization**: Apply the methodology to at least two additional vulnerability datasets (e.g., CWE-399, Common Weakness Enumeration) to assess robustness across different bug categories and codebases

3. **Real-World Deployment Simulation**: Conduct a pilot study where the fine-tuned model attempts to repair vulnerabilities discovered in recent open-source projects, measuring both success rate and the practical time/cost of the cross-model evaluation process