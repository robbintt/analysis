---
ver: rpa2
title: Memory- and Latency-Constrained Inference of Large Language Models via Adaptive
  Split Computing
arxiv_id: '2511.04002'
source_url: https://arxiv.org/abs/2511.04002
tags:
- edge
- quantization
- split
- compression
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an autoregressive-aware split computing framework
  for deploying large language models (LLMs) on memory- and latency-constrained edge
  devices. The framework addresses the challenges of iterative token generation and
  expanding KV caches in LLMs by employing one-point split compression (OPSC), a mixed-precision
  quantization scheme that partitions models into front-end and back-end segments
  with different precision levels.
---

# Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing

## Quick Facts
- arXiv ID: 2511.04002
- Source URL: https://arxiv.org/abs/2511.04002
- Reference count: 40
- Authors: Mingyu Sung; Vikas Palakonda; Suhwan Im; Sunghwan Moon; Il-Min Kim; Sangseok Yun; Jae-Mo Kang
- One-line primary result: Achieves 1.49× inference speedup and significant communication overhead reduction while maintaining or improving model accuracy compared to state-of-the-art quantization methods

## Executive Summary
This work introduces an autoregressive-aware split computing framework for deploying large language models (LLMs) on memory- and latency-constrained edge devices. The framework addresses the challenges of iterative token generation and expanding KV caches in LLMs by employing one-point split compression (OPSC), a mixed-precision quantization scheme that partitions models into front-end and back-end segments with different precision levels. A two-stage intermediate compression pipeline combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while reducing communication overhead. The framework also includes a unified optimization strategy that jointly selects optimal split points, quantization settings, and sequence lengths under strict memory and latency constraints. Extensive experiments demonstrate that the proposed framework achieves a 1.49× inference speedup and significant communication overhead reduction while maintaining or improving model accuracy compared to state-of-the-art quantization methods.

## Method Summary
The framework employs One-Point Split Compression (OPSC) to partition LLM weights between edge and cloud with different precision levels. The edge runs quantized front-end layers using Atom integration, while the cloud hosts higher-precision back-end layers. A two-stage compression pipeline processes intermediate outputs: Threshold Splitting (TS) isolates high-magnitude activations as sparse outliers, and Token-wise Adaptive Bit quantization (TAB-Q) compresses the remaining bulk tensor. The system optimizes split points, quantization settings, and sequence lengths under memory and latency constraints using discrete enumeration. An early exit mechanism monitors latency and can disable KV cache transmission or reduce token count to meet deadlines.

## Key Results
- Achieves 1.49× inference speedup compared to cloud-only inference
- Maintains or improves model accuracy compared to state-of-the-art quantization methods
- Reduces communication overhead by 70% compared to full-precision transmission
- Successfully prevents out-of-memory failures on Jetson Xavier NX for sequences up to 2048 tokens

## Why This Works (Mechanism)

### Mechanism 1: Outlier Isolation for Semantic Preservation
Separating high-magnitude activations from the bulk tensor before quantization preserves model accuracy better than uniform compression. Threshold Splitting (TS) identifies activations where $|T_{ij}| \geq \tau$, storing these "outliers" in sparse format without quantization loss while the remaining bulk tensor undergoes aggressive quantization. This works because a tiny fraction of activation values disproportionately dictate the model's reasoning output.

### Mechanism 2: Mixed-Precision Partitioning for Memory Constraints
One-Point Split Compression (OPSC) prevents Out-Of-Memory (OOM) errors by assigning lower bit-widths to the edge-resident segment while keeping the cloud segment precise. The model is split at layer $\ell_w$, with front-end weights quantized to fit edge RAM while back-end weights remain at higher precision on the server.

### Mechanism 3: Latency-Aware Adaptive Transmission
Joint optimization of split points and compression rates satisfies strict latency deadlines better than static configuration. The framework monitors total latency and triggers early exit strategies when approaching deadlines, such as disabling KV cache transmission or reducing token count.

## Foundational Learning

- **Concept: KV Cache in Autoregressive Models**
  - Why needed: The paper explicitly manages the expanding KV cache, which grows linearly with sequence length $w$ and consumes massive memory on edge devices during token generation
  - Quick check: How does disabling the KV cache transmission ($I_{kv}=0$) affect the server-side computation load versus communication bandwidth?

- **Concept: Asymmetric Integer Quantization (AIQ)**
  - Why needed: TAB-Q relies on mapping floating-point values to integers using scale $s$ and zero-point $z$ to compress intermediate outputs
  - Quick check: Why might symmetric quantization (centered at 0) fail for intermediate outputs in LLMs compared to the asymmetric approach used here?

- **Concept: Split Computing**
  - Why needed: This is the core architecture (Edge-Cloud partition) necessitating the OPSC and compression strategies to overcome the "memory wall" of edge devices
  - Quick check: What is the primary bottleneck that prevents a naive 50/50 layer split of an LLM between a mobile device and a cloud server?

## Architecture Onboarding

- **Component map:** Input Token -> Edge Forward Pass (Quantized) -> Threshold Splitting (Sparse Outliers + Dense Bulk) -> TAB-Q Compression -> Transmission -> Server Reconstruction -> Server Forward Pass
- **Critical path:** The pipeline processes each token sequentially, with edge computation followed by compression, transmission, and server-side reconstruction and inference
- **Design tradeoffs:** Higher threshold $\tau$ reduces sparse tensor size but risks quantizing important outliers; higher distortion tolerance $\Delta$ allows lower bit-widths (faster transmission) but increases accuracy loss; larger sequence lengths support longer context but exponentially increase KV cache memory pressure
- **Failure signatures:** OOM crashes when KV cache growth exceeds available RAM; accuracy collapse when HellaSwag/PIQA scores drop precipitously; latency violations when system triggers "Early Exit" too frequently
- **First 3 experiments:**
  1. Measure peak RAM usage on Jetson Xavier NX for different split layers and sequence lengths to validate memory model
  2. Sweep threshold value on validation set to find stability "knee" where communication cost drops but accuracy remains stable
  3. Simulate varying channel conditions to verify if Early Exit strategy maintains deadlines without excessive quality degradation

## Open Questions the Paper Calls Out

The paper explicitly notes that cloud server computation and queueing latencies fluctuate with the instantaneous number of active clients, making precise analytical modeling impractical. This uncertainty affects the reliability of the $\epsilon$-outage framework for latency prediction. Additionally, the paper states that models are assessed using pretrained weights and fixed prompt templates without task-specific fine-tuning or in-context demonstrations, leaving open questions about performance on instruction-tuned models or different task distributions.

## Limitations

- The unified optimization framework may miss global optima due to discrete enumeration over feasible configurations
- The impact of disabling KV cache transmission on server-side computation time is not quantified
- The $\epsilon$-outage framework assumes accurate SNR estimation, which may not hold in dynamic wireless environments
- Results are primarily validated on Llama-2 models, with uncertain generalization to other LLM architectures

## Confidence

- **High Confidence**: The memory footprint model and KV cache growth mechanism are well-defined and validated; the outlier isolation observation is empirically supported
- **Medium Confidence**: The two-stage compression pipeline is novel and mechanistically sound, but adaptive bit selection and distortion tolerance parameters are heuristic
- **Low Confidence**: The claim of "maximally preserving accuracy" under dual constraints is validated only on Llama-2 7B and 13B models

## Next Checks

1. **Memory Profiling Validation**: Measure peak RAM usage on Jetson Xavier NX for different split layers and sequence lengths to validate Equation (8c) and confirm the model does not crash when memory constraints are exceeded

2. **KV Cache Transmission Impact**: Run controlled experiments comparing inference with $I_{kv}=1$ (full KV cache transmission) versus $I_{kv}=0$ (server recomputation) to quantify server-side computation time changes and total latency

3. **Network Robustness Testing**: Simulate varying wireless channel conditions and measure early exit frequency and accuracy degradation to assess if the observed outage rate matches the modeled $\epsilon$ for latency prediction reliability