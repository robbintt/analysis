---
ver: rpa2
title: 'DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction
  from Language Models'
arxiv_id: '2602.00377'
source_url: https://arxiv.org/abs/2602.00377
tags:
- freq
- concept
- concepts
- quantization
- gptq-int4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DecompressionLM introduces a stateless, zero-shot concept graph
  extraction framework for language models that systematically samples output distributions
  using Van der Corput low-discrepancy sequences with arithmetic decoding. The method
  eliminates cross-sequence coupling and beam search bias by generating independent
  sequences in parallel without shared state, enabling efficient exploration of the
  model's knowledge space.
---

# DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models

## Quick Facts
- arXiv ID: 2602.00377
- Source URL: https://arxiv.org/abs/2602.00377
- Reference count: 21
- Primary result: Activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170% versus uniform quantization (GPTQ-Int4) which causes 71-86% coverage collapse

## Executive Summary
DecompressionLM introduces a zero-shot framework for extracting concept graphs from language models by systematically sampling output distributions using Van der Corput low-discrepancy sequences with arithmetic decoding. This approach enables deterministic, parallel generation without cross-sequence coupling, allowing efficient exploration of a model's knowledge space. The method reveals that quantization methods affect knowledge breadth differently—AWQ-4bit preserves long-tail concept access while uniform quantization degrades it, demonstrating that concept coverage serves as a complementary evaluation dimension beyond traditional perplexity metrics.

## Method Summary
DecompressionLM generates concept graphs by creating N parallel sequences through arithmetic decoding of Van der Corput codes. For each code in [0,1), the decoder recursively subdivides intervals according to token CDFs, producing sequences that follow model probabilities while ensuring systematic coverage. Concepts are extracted per line, normalized through NFKC, lowercasing, ASCII filtering, and fuzzy-merged via Levenshtein similarity. The framework evaluates quantization effects by comparing concept counts, graph connectivity, and hallucination rates across quantization variants on identical prompts.

## Key Results
- AWQ-4bit extracts 2.7× more concepts than BF16 baseline at sequence length 16, maintaining 96.5% connectivity in largest component
- GPTQ-Int4 causes 71-86% concept coverage collapse with graph fragmentation (40.8% largest component vs 86.6% BF16)
- 19.6-point hallucination gap exists between top- and bottom-ranked models on 21 MMLU-Pro Law models, validating concept coverage as evaluation metric

## Why This Works (Mechanism)

### Mechanism 1: Van der Corput Sampling for Stateless Coverage
Low-discrepancy sequences enable reproducible exploration of model output space without cross-sequence coupling. VdC codes generate in [0,1) that map to token sequences via arithmetic decoding, providing O(log N/N) discrepancy versus O(N^-1/2) for random sampling. This systematic coverage reveals distribution properties invisible to likelihood metrics alone.

### Mechanism 2: Arithmetic Decoding for Distribution-Guided Generation
Arithmetic coding provides bijection between continuous codes and discrete token sequences, mapping VdC codes to sequences that follow model probabilities while ensuring systematic output space coverage. The cumulative distribution function encodes knowledge structure that interval subdivision can systematically traverse.

### Mechanism 3: Activation-Aware Quantization Preserves Long-Tail Access
AWQ-4bit protects salient weights (top 1% by activation magnitude) that encode access to diverse, long-tail concepts. Uniform quantization applies equal compression across all weights, preserving high-frequency knowledge (stable perplexity) but degrading long-tail access (coverage collapse).

## Foundational Learning

- **Concept: Low-discrepancy sequences (quasi-random numbers)**
  - Why needed here: Enables reproducible, seed-free exploration of continuous probability space for benchmarking across models
  - Quick check question: Given first 4 Van der Corput codes (base 2): 0.5, 0.25, 0.75, 0.125—sketch their positions on [0,1) and compare uniformity to 4 hypothetical random samples

- **Concept: Arithmetic coding/decoding**
  - Why needed here: Maps continuous VdC codes to discrete token sequences while preserving probability structure
  - Quick check question: If CDF(token_A) = 0.4 and CDF(token_B) = 0.7, what token would code z = 0.5 decode to in the first position?

- **Concept: Cumulative Distribution Function (CDF) for language models**
  - Why needed here: Arithmetic decoding recursively subdivides intervals using CDF values
  - Quick check question: For vocabulary {A: 0.3, B: 0.5, C: 0.2} sorted alphabetically, compute CDF(A), CDF(B), and the interval [L, U) for decoding token B

## Architecture Onboarding

- **Component map**: VdC Generator -> Arithmetic Decoder -> Parallel Execution -> Concept Extractor -> Fuzzy Merger -> Graph Builder
- **Critical path**: Generate N VdC codes → Batch decode codes → Extract concepts per sequence → Merge via Levenshtein → Build directed graph → Validate against corpus
- **Design tradeoffs**: VdC vs. i.i.d. sampling (reproducibility vs. potential efficiency), sequence length (ℓ=16 vs 32) for attention cost vs. concept yield, stateless vs. sequential exploration (parallelism vs. adaptivity)
- **Failure signatures**: Low Jaccard + low core concept % (noise-dominated extraction), fragmented graph (knowledge structure disruption), negative frequency-verification correlation ("