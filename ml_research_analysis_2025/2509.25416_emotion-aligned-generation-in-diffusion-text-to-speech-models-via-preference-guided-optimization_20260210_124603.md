---
ver: rpa2
title: Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided
  Optimization
arxiv_id: '2509.25416'
source_url: https://arxiv.org/abs/2509.25416
tags:
- preference
- emotional
- speech
- emotion
- stepwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Emotion-Aware Stepwise Preference Optimization
  (EASPO), a diffusion-based text-to-speech framework that enables fine-grained emotional
  alignment by optimizing generation at intermediate denoising steps rather than only
  at the final output. The method uses an Emotion-Aware Stepwise Preference Model
  (EASPM) to score and compare multiple candidate samples at each denoising step,
  constructing win/lose preference pairs based on subtle emotional and prosodic differences.
---

# Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization

## Quick Facts
- **arXiv ID:** 2509.25416
- **Source URL:** https://arxiv.org/abs/2509.25416
- **Reference count:** 0
- **Primary result:** Introduces EASPO, a diffusion TTS framework optimizing at intermediate denoising steps, achieving 99.15% emotion similarity and 4.28 emotion MOS on ESD dataset

## Executive Summary
This paper introduces Emotion-Aware Stepwise Preference Optimization (EASPO), a diffusion-based text-to-speech framework that enables fine-grained emotional alignment by optimizing generation at intermediate denoising steps rather than only at the final output. The method uses an Emotion-Aware Stepwise Preference Model (EASPM) to score and compare multiple candidate samples at each denoising step, constructing win/lose preference pairs based on subtle emotional and prosodic differences. These preferences guide the diffusion policy to progressively align speech toward emotionally expressive targets while preserving naturalness.

Experiments on the ESD dataset show that EASPO outperforms seven emotion-controllable TTS baselines in emotion similarity (99.15%), prosody similarity (3.89), WER (3.74%), and subjective metrics including MOS (3.94), Emotion MOS (4.28), and Emotion Recall (85.84%). The framework achieves this through timestep-conditioned preference scoring, candidate pool-based exploration, and decoupled sampling from supervision.

## Method Summary
EASPO fine-tunes Grad-TTS by optimizing at intermediate denoising steps using stepwise preference supervision. The EASPM, adapted from CLEP with time-conditioning, scores k candidates at each timestep to construct win/lose pairs. The diffusion policy is updated to align its log-likelihood ratio difference with the reward difference from these pairs, with the next state randomly selected from the candidate pool to prevent trajectory collapse. Training uses MSP-Podcast for EASPM and ESD English split for RL fine-tuning, with key hyperparameters including k=4 candidates, κ=0.25T skip threshold, and decoder learning rate of 1e-5.

## Key Results
- Outperforms 7 emotion-controllable TTS baselines with 99.15% emotion similarity and 4.28 emotion MOS
- Achieves 3.74% WER and 3.89 prosody similarity while maintaining naturalness (MOS 3.94)
- Ablation confirms optimal candidate pool size k=4 and timestep range [0, 750] for robust emotional control

## Why This Works (Mechanism)

### Mechanism 1: Dense Stepwise Preference Supervision
Providing emotional preference signals at intermediate denoising steps yields better alignment than endpoint-only feedback for temporally evolving cues like prosody. At each reverse step t, EASPM scores k candidates from the shared latent x_t, capturing subtle prosodic differences at that timestep. The policy is updated to align its log-likelihood ratio difference with the reward difference, distributing supervision across the trajectory rather than concentrating it at x_0.

### Mechanism 2: Time-Conditioned Scoring of Noisy States
A preference model conditioned on denoising timestep can meaningfully score emotionally expressive content in partially-noised mel-spectrograms. EASPM adds time-aware normalization to CLEP's audio branch, learning to recover correct preferences at arbitrary noise levels. This enables scoring of intermediate states while preserving emotional information.

### Mechanism 3: Decoupled Sampling from Supervision
Randomly selecting the next state from the candidate pool prevents trajectory collapse and improves overall alignment. After EASPM ranks candidates and selects win/lose pairs for loss computation, the actual next state is uniformly sampled from the pool, ensuring diverse rollouts while still learning from strong contrastive pairs.

## Foundational Learning

- **Concept: Diffusion Denoising Trajectory**
  - Why needed here: EASPO operates at intermediate steps; understanding how x_T → x_0 unfolds is prerequisite
  - Quick check question: Can you explain why modifying gradients at step t=500 affects final output differently than t=100?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: EASPO extends DPO from endpoint comparisons to stepwise rewards
  - Quick check question: How does the log-likelihood ratio in Eq. 5 relate to the standard DPO objective?

- **Concept: Contrastive Audio-Language Encoding (CLAP/CLEP)**
  - Why needed here: EASPM inherits this architecture; understanding embedding alignment is critical
  - Quick check question: What does the cosine similarity in Eq. 1 measure, and why does it require ℓ2 normalization?

## Architecture Onboarding

- **Component map:** Grad-TTS encoder + duration predictor (frozen) -> Grad-TTS decoder/score network (fine-tuned) -> EASPM (CLEP audio encoder + time-conditioned projection + text encoder) -> Candidate sampler -> Loss module
- **Critical path:** 1. Sample text c, noise x_T, timestep τ ~ U[1, T-κ] 2. Generate k candidates via single-step denoising 3. Score each with EASPM 4. Select (x^w, x^l) by argmax/argmin scores 5. Compute log-likelihood ratios against frozen reference policy 6. Backprop Eq. 7 loss through decoder only
- **Design tradeoffs:** κ (skip threshold): larger skips more early steps, reducing noise in supervision but narrowing optimization range; k (candidate pool): higher increases contrast but raises compute and may introduce artifacts; Timestep range [0, 750] optimal; full range [0, 1000] degrades
- **Failure signatures:** Emo SIM drops below baseline → check EASPM timestep conditioning, verify frozen reference policy; WER increases → candidate pool too large (k>4) or timestep range includes late steps with weak structure; Monotone outputs → verify random next-state selection is active, not always selecting x^w
- **First 3 experiments:** 1. Sanity check EASPM scoring: Score pairs of known emotional samples at varying noise levels; verify preference ordering preserved with timestep conditioning 2. Ablate candidate pool size k: Run k∈{2,4,8} on held-out emotions; confirm k=4 yields best Emo SIM / WER tradeoff 3. Compare timestep ranges: Test [0,500], [0,750], [250,750]; verify mid-to-late range optimization outperforms full-range

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to one language (English) and dataset (ESD), raising questions about generalization across languages and emotional corpora
- Preference model trained on noisy intermediate states assumes preference orderings remain stable under diffusion, but no explicit validation of this assumption
- No analysis of how EASPO performs on out-of-domain text or emotions not present in the training set

## Confidence
- **High confidence:** Emotional alignment performance improvements (Emo SIM 99.15%, Emotion MOS 4.28) are well-supported by objective metrics and ablation studies
- **Medium confidence:** Claims about timestep conditioning and candidate pool size effects are supported by ablation studies but lack external validation
- **Low confidence:** Generalization claims to other languages, datasets, or emotion categories are unsupported by the current experimental scope

## Next Checks
1. **Cross-corpus validation:** Evaluate EASPO on an independent emotional speech dataset (e.g., RAVDESS or IEMOCAP) to test generalization beyond ESD
2. **Preference ordering stability:** Design a controlled experiment to verify that emotional preferences remain consistent when samples are diffused to various intermediate timesteps
3. **Out-of-distribution testing:** Generate speech for emotion-text combinations unseen during training to assess robustness and potential bias amplification