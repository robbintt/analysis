---
ver: rpa2
title: 'To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language
  Models'
arxiv_id: '2510.08510'
source_url: https://arxiv.org/abs/2510.08510
tags:
- tokens
- sink
- sinks
- visual
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses how visual tokens in large vision-language
  models (LVLMs) contribute to understanding and reasoning. While most prior work
  focuses on identifying low-semantic "attention sinks" within language models, this
  study instead investigates high-norm visual tokens from vision transformers (ViT),
  termed ViT attention sinks.
---

# To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models

## Quick Facts
- **arXiv ID**: 2510.08510
- **Source URL**: https://arxiv.org/abs/2510.08510
- **Reference count**: 40
- **Primary result**: High-norm visual tokens (ViT attention sinks) from vision transformers capture global semantic concepts and can be leveraged to improve visual reasoning in vision-language models through dynamic token selection and separate MLP projections.

## Executive Summary
This paper challenges the conventional view of attention sinks in vision-language models by showing that high-norm visual tokens from vision transformers (ViT sinks) encode high-level semantic concepts rather than being low-semantic noise. The authors demonstrate that these ViT sinks capture coarse-grained, global contextual features that benefit holistic image understanding. They propose DIYSink, a training-based framework using dual MLP projections and dynamic token selection, which consistently improves performance across four ViT-LLM combinations and various benchmarks, particularly for global reasoning tasks. The work reveals untapped potential in ViT attention sinks for enhancing visual reasoning capabilities.

## Method Summary
The method involves identifying high-norm visual tokens from ViT encoders (termed ViT sinks) and leveraging them through two main approaches. The training-free method repositions ViT sink tokens to the front of the visual sequence, while the training-based DIYSink framework uses separate MLP projections for sink and non-sink tokens, followed by dynamic token selection via either chain-of-thought routing or learned reweighting. The framework processes images through a ViT encoder, identifies sink tokens based on L2 norm threshold (τ≈100), projects sink and non-sink tokens through separate MLPs, and selects tokens dynamically based on task type before passing to the LLM decoder.

## Key Results
- ViT sinks capture coarse-grained, high-level contextual features aligned with attention head focus, showing strong associations with main objects versus non-sink tokens
- Dual-MLP projection alone improves performance (SQA: 57.76→60.63, MMMU: 30.30→30.80, MME: 1381→1439)
- DIYSink consistently improves performance across four ViT-LLM combinations, especially on global reasoning tasks
- Sink-only excels on global tasks while degrading on local tasks, confirming task-specific token utility
- Training-free repositioning and training-based DIYSink both show improvements, with DIYSink achieving superior results

## Why This Works (Mechanism)

### Mechanism 1: ViT Sinks Encode Global Semantic Context
- Claim: ViT attention sinks capture coarse-grained, high-level contextual features rather than being low-semantic noise
- Mechanism: High-norm tokens in ViT aggregate attention from across foreground/background regions (based on attention head specialization), encoding scene-level semantics like main objects and scene composition
- Core assumption: The norm-attention correlation reflects meaningful semantic aggregation rather than training artifacts
- Evidence anchors:
  - [abstract] "these ViT sinks encapsulate high-level semantic concepts from images"
  - [Section 3.2] "ViT sinks capture coarse-grained, high-level contextual features aligned with the specific focus of each attention head"
  - [Figure 4C] Word distributions show sink tokens strongly associated with main objects (cat, person) vs. non-sink tokens
  - [corpus] Related work "See What You Are Told" confirms visual attention sinks exist in LMMs but treats them as problematic

### Mechanism 2: Dual-MLP Projection Prevents Representational Interference
- Claim: Separating sink and non-sink token projections improves both token types' effectiveness
- Mechanism: Sink tokens have distinct activation patterns (high norm, specific hidden dimensions like 982, 2494, 3263) that differ from non-sinks; a single shared MLP cannot optimally project both distributions simultaneously
- Core assumption: The interference is primarily in the projection layer, not in the LLM's processing
- Evidence anchors:
  - [Section 3.1] "Visual sink tokens propagate into the LLM as distinct sink tokens, activating different hidden dimensions"
  - [Table 3] Dual-MLP alone improves SQA (57.76→60.63), MMMU (30.30→30.80), MME (1381→1439)

### Mechanism 3: Task-Adaptive Token Selection Matches Information Type to Task Demands
- Claim: Global reasoning tasks benefit from sink tokens; local/detail tasks benefit from non-sinks; mixed tasks need both
- Mechanism: CoT routing classifies (image type: symbolic/real) × (query type: global/local) → hard selection; Reweighting MLP learns soft weights from query embedding
- Core assumption: Task type correlates with optimal token type, and this can be inferred from input characteristics
- Evidence anchors:
  - [Figure 5B] Sink-only excels on global tasks, degrades on local; non-sink-only shows opposite pattern
  - [Section 4.2] Reweighting MLP assigns higher weights to sinks for algebraic/geometric reasoning, lower for artwork/posters

## Foundational Learning

- Concept: **Attention Sinks in Transformers**
  - Why needed here: The paper redefines attention sinks from "low-semantic noise" (LLM view) to "high-semantic aggregators" (ViT view) — this inversion is the core insight
  - Quick check question: In a standard LLM, what token typically acts as an attention sink, and why might ViT sinks behave differently?

- Concept: **Vision-Language Connector Architectures**
  - Why needed here: The dual-MLP design modifies the connector (ViT→LLM bridge); understanding Q-Former vs MLP projectors clarifies where the modification fits
  - Quick check question: What does an MLP projector do that a Q-Former doesn't, and why might separate MLPs help?

- Concept: **Token Norm as Proxy for Information Content**
  - Why needed here: The entire method hinges on using L2 norm (threshold τ≈100) to identify sink tokens
  - Quick check question: Why might high-norm tokens correlate with attention received, and what could break this correlation?

## Architecture Onboarding

- Component map:
```
[Image] → [ViT Encoder] → {Sink Tokens (norm>τ), Non-Sink Tokens}
                              ↓                        ↓
                    [MLP_sink (project)]    [MLP_non-sink (project)]
                              ↘                        ↙
                              [Concatenate + Position]
                                        ↓
                              [Dynamic Selection: CoT or ReW]
                                        ↓
                              [LLM Decoder] → [Output]
```

- Critical path:
  1. ViT forward pass → extract patch tokens
  2. Compute L2 norm per token → identify sinks (typically 3-5 per image with τ=100)
  3. Route through separate MLPs (trained separately in pretraining, jointly in fine-tuning)
  4. Apply selection: CoT (2-step classification) or ReW (learned weights [w_sink, w_non-sink])
  5. Concatenate with text tokens → LLM generation

- Design tradeoffs:
  - Training-free (sink-to-front) vs training-based (DIYSink): 0 cost vs +0.2% params, +1hr training
  - CoT routing vs ReW: +1.6s inference latency vs +0.01s latency + small MLP training
  - Hard selection vs soft weighting: Simpler but brittle vs flexible but requires training data

- Failure signatures:
  - Local task performance drops → sink weight too high, check ReW outputs
  - No improvement vs baseline → τ incorrectly set; verify norm distribution has clear separation
  - Inconsistent gains across similar tasks → CoT classification unreliable; inspect routing decisions

- First 3 experiments:
  1. **Token norm distribution analysis**: Plot L2 norms of all visual tokens across 100 images; verify bimodal distribution justifies τ=100 threshold
  2. **Sink-only vs non-sink-only probing**: On held-out tasks, run both configurations to confirm global/local task split aligns with Figure 5B patterns
  3. **Dual-MLP ablation without selection**: Train dual-MLP model without CoT/ReW module to isolate projection benefit from selection benefit (expect ~50-70% of total gain per Table 3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DIYSink framework and the concept of ViT attention sinks be effectively generalized to temporal video-language models or audio-visual architectures?
- Basis in paper: [explicit] The authors state in Appendix E.1 that this work focuses on image and language modalities, and that "Extending DIYSink to other modalities such as video and audio remains an open area for future research and exploration."
- Why unresolved: The paper's methodology and experiments were exclusively designed for static image inputs using ViT backbones like CLIP/SigLIP. It is unknown if the "sink" phenomenon persists or carries the same semantic weight in temporal tokens or spectrograms.
- What evidence would resolve it: An analysis of attention norms in video encoders (e.g., Video-LLaVA) to identify temporal sinks, followed by the application of the dual-MLP or repositioning strategies on video benchmarks.

### Open Question 2
- Question: What is the theoretical or mechanistic explanation for why propagated ViT sinks activate distinct hidden dimensions (e.g., {982, 2494}) compared to LLM-emerged sinks?
- Basis in paper: [inferred] Section 3.1 explicitly observes that propagated ViT sinks activate dimensions (e.g., 982, 2494) distinct from LLM sinks (e.g., 2533, 1415), noting that these dimensions emerge only after multimodal training. However, the paper does not explain *why* the model segregates these signals into specific dimensions.
- Why unresolved: The paper establishes the empirical existence of these distinct dimensions but leaves the functional role of these specific dimensions in the overall reasoning process as a "black box" observation.
- What evidence would resolve it: A mechanistic interpretability study (e.g., causal tracing or activation patching) determining if these specific dimensions store global visual primitives distinct from the syntactic biases stored in standard LLM sinks.

### Open Question 3
- Question: How does the DIYSink framework perform on architectures that use blended visual backbones (e.g., channel-wise concatenation of SAM and SigLIP) where token semantics are convoluted?
- Basis in paper: [inferred] Section 5 notes regarding Table 1 that for DeepSeek-VL, the "channel-wise concatenation of SAM and SigLIP features creates a blended features, complicating the original token behavior; as a result, we observe smaller gains."
- Why unresolved: The current method relies on identifying clear high-norm tokens from a single ViT source. When features are fused (blended), the definition of a "sink" becomes ambiguous, and the paper demonstrates reduced efficacy without offering a solution.
- What evidence would resolve it: A modification of the sink identification characteristic function (Eq. 1) to handle multi-source embeddings, or an evaluation of DIYSink on models using hybrid fusion strategies to see if separate projection paths are necessary for each visual backbone.

## Limitations

- **Threshold Sensitivity**: The method depends critically on the norm threshold τ=100 to identify sink tokens, with no theoretical justification for this specific value across different architectures
- **Task Distribution Bias**: Experimental validation focuses heavily on datasets where global reasoning is clearly separable from local tasks, potentially limiting generalizability to tasks requiring interleaved global-local reasoning
- **CoT Routing Reliability**: The two-step classification approach uses simple heuristic rules that can fail on ambiguous queries combining global and local reasoning or when symbolic content contains real-world objects

## Confidence

- **High Confidence**: The empirical observation that high-norm ViT tokens capture global semantic concepts is well-supported by qualitative visualizations and quantitative word distribution analysis
- **Medium Confidence**: The dual-MLP architecture consistently improves performance, but lacks direct evidence of representational interference in the shared-MLP baseline; CoT routing logic is reasonable but relies on assumptions about task structure
- **Low Confidence**: The specific threshold τ=100 has no theoretical justification and may be architecture-dependent; Reweighting MLP's learned weights are not interpretable beyond aggregate patterns

## Next Checks

1. **Cross-Architecture Threshold Validation**: Test τ=100 across three different ViT architectures (ViT, Swin, PVT) and two resolutions (224×224, 384×384) on the same image set. Plot norm distributions and verify bimodal separation persists. If thresholds vary significantly (>20% difference), implement an adaptive threshold based on percentile or distribution fitting rather than fixed value.

2. **Mixed-Reasoning Task Evaluation**: Design a benchmark with 50 tasks requiring both global and local reasoning in the same query (e.g., "What is the main object, and what text is written on it?"). Compare DIYSink's performance against a baseline using all tokens, and analyze routing decisions to identify failure modes where CoT hard selection is suboptimal versus where ReW soft weighting can adapt.

3. **Norm-to-Attention Correlation Analysis**: For 100 images, compute the Pearson correlation between token norm and average attention received across all heads. Additionally, measure correlation between norm and downstream task performance when individual tokens are ablated. This would validate whether norm truly proxies for semantic importance or merely reflects architectural artifacts.