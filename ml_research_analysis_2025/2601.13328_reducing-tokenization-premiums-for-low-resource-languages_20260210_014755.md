---
ver: rpa2
title: Reducing Tokenization Premiums for Low-Resource Languages
arxiv_id: '2601.13328'
source_url: https://arxiv.org/abs/2601.13328
tags:
- tokens
- language
- languages
- vocabulary
- tokenization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes tokenization premiums for low-resource languages
  across ten popular language models, finding that non-Latin scripts like Bangla,
  Hindi, and Urdu incur 3-5x higher tokenization costs than English. The authors propose
  a method to reduce these premiums by adding single-token encodings for multi-token
  characters in frozen models.
---

# Reducing Tokenization Premiums for Low-Resource Languages

## Quick Facts
- arXiv ID: 2601.13328
- Source URL: https://arxiv.org/abs/2601.13328
- Reference count: 4
- Low-resource languages incur 3-5x higher tokenization costs than English

## Executive Summary
This paper analyzes tokenization premiums for low-resource languages across ten popular language models, finding that non-Latin scripts like Bangla, Hindi, and Urdu incur 3-5x higher tokenization costs than English. The authors propose a method to reduce these premiums by adding single-token encodings for multi-token characters in frozen models. Using Llama 3.2 1B, they demonstrate that compressed inputs with new character tokens achieve high similarity (0.70-0.98 cosine similarity) to original inputs across 12 low-resource languages, with linear regression at layer 0 being the most effective embedding strategy.

## Method Summary
The authors propose a post-hoc method to reduce tokenization premiums in frozen models by identifying characters that require multiple tokens in the original vocabulary, then adding single-token encodings for these characters. For each multi-token character, they derive a single embedding by averaging the input embeddings of its constituent tokens (linear regression at layer 0). The augmented vocabulary is then used to re-tokenize text, reducing the number of tokens needed to represent the same content. The method is evaluated on Llama 3.2 1B using FLORES-200 dev data across 12 low-resource languages, measuring cosine similarity between compressed and uncompressed hidden states.

## Key Results
- Non-Latin scripts (Bangla, Hindi, Urdu) incur 3-5x higher tokenization costs than English
- Linear regression at layer 0 achieves 0.70-0.98 cosine similarity between compressed and original inputs
- 79% of tokens in Amharic can be replaced with single-token encodings
- Layer-0 averaging is the most effective embedding strategy on average

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BPE tokenizers trained on English-dominated corpora create 3-5x tokenization premiums for non-Latin scripts because common subwords in these languages never merge into single tokens.
- Mechanism: BPE iteratively merges the most frequent adjacent token pairs. When training corpora contain minimal text in a target language, character pairs in that language rarely appear together frequently enough to trigger merges, leaving entire scripts encoded as byte sequences or individual characters.
- Core assumption: The training corpus composition directly determines which token merges occur, and internet-scale corpora underrepresent many widely-spoken languages relative to their speaker populations.
- Evidence anchors:
  - [abstract] "non-Latin scripts like Bangla, Hindi, and Urdu incur 3-5x higher tokenization costs than English"
  - [section 1] "High-resource languages, especially English, typically dominate available training data, so the vocabularies of BPE-trained tokenizers largely comprise words and subwords from these languages"
  - [corpus] Related work "Explaining and Mitigating Crosslingual Tokenizer Inequities" and "The Token Tax: Systematic Bias in Multilingual Tokenization" confirm these disparities persist across models
- Break condition: If a tokenizer was explicitly trained on balanced multilingual data (e.g., BERT Multilingual shows lower premiums), this mechanism weakens.

### Mechanism 2
- Claim: Averaging input embeddings of multi-token character sequences produces embeddings that the frozen model processes similarly to the original tokenized form.
- Mechanism: When a character requires multiple tokens, each token has a learned embedding. The model has seen these token sequences during pre-training. Averaging these embeddings at layer 0 creates a single vector that occupies a similar region of embedding space to the original sequence, allowing the model to recognize the combined semantic unit.
- Core assumption: The model's pre-training exposure to the target language means it has learned to compose meaning from these multi-token sequences, and the averaged embedding approximates the "composed" representation.
- Evidence anchors:
  - [abstract] "linear regression at layer 0 being the most effective embedding strategy"
  - [section 4.2] "Linear regression at layer 0 is the best strategy on average. It is actually equivalent to simply averaging the input embeddings"
  - [corpus] Weak direct corpus evidence for this specific averaging mechanism; related papers focus on quantifying premiums rather than mitigation
- Break condition: If the model never saw the target language during pre-training, averaged embeddings may not map to meaningful representations.

### Mechanism 3
- Claim: Local methods (k-NN, local linear regression) at deeper layers can recover contextualized representations that simple averaging at layer 0 cannot.
- Mechanism: At later transformer layers, the model has already applied attention and composed context-aware representations. Finding neighbors in this space and mapping back to input embeddings leverages the model's learned transformations rather than raw embedding arithmetic.
- Core assumption: The nearest neighbors in hidden state space correspond to tokens with similar semantic/functional roles, and their input embeddings can be combined to approximate the desired representation.
- Evidence anchors:
  - [section 4.2] "The worst languages for local strategies at layer 0 (amh and shn) are better preserved by the same strategies at later layers"
  - [section 4.1] Describes the V_l matrices capturing hidden representations at each layer for mapping
  - [corpus] No direct corpus evidence for this layer-depth trade-off; this appears novel to this work
- Break condition: If the vocabulary lacks semantically similar tokens in the target language, k-NN methods will retrieve irrelevant neighbors.

## Foundational Learning

- Concept: **Byte-Pair Encoding (BPE) vocabulary construction**
  - Why needed here: Understanding how tokenizers create premiums requires knowing BPE starts from characters and merges frequent pairs—so rare languages stay un-merged.
  - Quick check question: Given corpus "a a b b a b", what is the first BPE merge?

- Concept: **Frozen model embedding augmentation**
  - Why needed here: The proposed method adds tokens without retraining, which requires understanding what can and cannot be modified post-hoc.
  - Quick check question: Can you add a new output token to a trained model without any retraining? What component must change?

- Concept: **Cosine similarity for representation comparison**
  - Why needed here: The paper evaluates success via cosine similarity between compressed and uncompressed hidden states—you need to know what this measures.
  - Quick check question: Two vectors with cosine similarity 0.98: are they nearly parallel or nearly orthogonal?

## Architecture Onboarding

- Component map:
  - Tokenizer (BPE) -> Embedding matrix (|V| × d model) -> Transformer layers (16 layers) -> V_l matrices (hidden representations at each layer)

- Critical path:
  1. Identify characters absent from vocabulary (multi-token encoded)
  2. For each missing character, collect its multi-token embedding sequence E_s
  3. Apply chosen strategy (layer-0 averaging recommended) to derive single embedding
  4. Augment vocabulary with new token, extend embedding matrix
  5. Re-tokenize target-language text using updated vocabulary

- Design tradeoffs:
  - Layer 0 vs. deeper layers: Layer 0 is simplest (just average embeddings) and works best on average; deeper layers may help for poorly-represented scripts
  - k-NN neighborhood size: Small k (1-2) works better; larger k dilutes signal
  - Augment vs. replace vocabulary: Augmenting preserves original tokens but creates "vote-splitting" at output

- Failure signatures:
  - Cosine similarity < 0.5: Embedding strategy not preserving semantics
  - Amharic pattern at layer 0 (similarity ~0.05 for k-NN): Local methods fail when no good neighbors exist
  - Vote-splitting at output: Model assigns probability mass to both new merged token and original sub-tokens

- First 3 experiments:
  1. Baseline premium measurement: Tokenize FLORES-200 dev set in target language and English, compute token ratios to establish premiums for your model
  2. Layer-0 averaging test: For 5-10 frequent multi-token characters in one language (e.g., Hindi), add single-token entries with averaged embeddings, measure cosine similarity on held-out sentences
  3. Cross-layer comparison: Repeat experiment 2 using k-NN at layers 1, 8, and 16 to identify if deeper representations help for your specific language

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed vocabulary compression maintain performance on downstream natural language processing tasks compared to uncompressed inputs?
- Basis in paper: [explicit] The authors state in the Limitations section that "Cosine similarity is not necessarily aligned with performance on real downstream tasks," and they identify the need for metrics beyond averaged last hidden states.
- Why unresolved: The study evaluates representational similarity (cosine similarity) but does not measure utility on tasks like translation, sentiment analysis, or question answering.
- What evidence would resolve it: Benchmarks on standard NLP tasks using the compressed inputs versus original inputs to verify functional equivalence.

### Open Question 2
- Question: To what extent does "vote-splitting" between new character tokens and original sub-character tokens degrade generation quality?
- Basis in paper: [explicit] The Limitations section highlights that augmenting the vocabulary causes the model to "split probability mass between the original sub-character tokens and the new combined tokens," potentially leading to incorrect output generation.
- Why unresolved: The paper focuses on input representation similarity and does not analyze the impact of the augmented vocabulary on the model's generative output distribution.
- What evidence would resolve it: Perplexity scores and token-level accuracy metrics for generated text, comparing standard decoding against proposed fixes like "winner-takes-all" logic.

### Open Question 3
- Question: Can a lightweight continuous "pseudo-translation" mechanism effectively map low-resource language embeddings to a frozen model's preferred embedding space?
- Basis in paper: [explicit] In the "Conclusions and Future Directions," the authors propose exploring "continuous/relaxed pseudo-translation" where a simple model maps sequences of low-resource embeddings to the model's preferred high-resource embeddings.
- Why unresolved: This is presented as a theoretical avenue for creating lightweight post-training adapters, but no implementation or experimental results are provided.
- What evidence would resolve it: Experiments showing that mapping low-resource embeddings to "preferred" embeddings yields better comprehension than the direct averaging of input embeddings.

## Limitations
- Implementation ambiguity between vocabulary augmentation vs replacement creates uncertainty in replication
- Limited language coverage (12 languages) may not generalize to all script families
- Unexplained layer-depth trade-offs for local strategies lack theoretical justification
- Vocabulary size sensitivity effects remain unexplored

## Confidence
- **High confidence**: The core observation that BPE tokenization creates 3-5x premiums for non-Latin scripts is well-established and reproducible
- **Medium confidence**: The claim that linear regression at layer 0 achieves 0.70-0.98 cosine similarity depends on ambiguous implementation choices
- **Low confidence**: The assertion that this provides a general solution for low-resource languages lacks validation across diverse language families and model architectures

## Next Checks
1. **Cross-vocabulary validation**: Test the method on models with different vocabulary sizes (e.g., 32K, 64K, 200K) to determine if the approach scales and identify any vocabulary-dependent failure modes.

2. **Script-family generalization**: Apply the method to languages from underrepresented script families (e.g., Southeast Asian abugidas, Arabic script variants, Cyrillic-based languages) to test whether the high performance on Indic scripts generalizes beyond this family.

3. **Long-range dependency preservation**: Design experiments to test whether the compressed representations preserve syntactic and semantic dependencies over longer sequences, not just sentence-level similarity. This would validate whether the method maintains functional equivalence for downstream tasks.