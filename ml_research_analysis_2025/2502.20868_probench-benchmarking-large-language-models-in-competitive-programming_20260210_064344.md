---
ver: rpa2
title: 'ProBench: Benchmarking Large Language Models in Competitive Programming'
arxiv_id: '2502.20868'
source_url: https://arxiv.org/abs/2502.20868
tags:
- reasoning
- code
- programming
- problem
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ProBench, a comprehensive benchmark for\
  \ evaluating large language models (LLMs) in competitive programming, addressing\
  \ the inadequacy of existing benchmarks for advanced reasoning models. ProBench\
  \ collects real competition problems from Codeforces, Luogu, and Nowcoder platforms\
  \ (July\u2013December 2024) and employs an online submission strategy to leverage\
  \ original platforms\u2019 robust test suites, ensuring fair and accurate code robustness\
  \ assessment."
---

# ProBench: Benchmarking Large Language Models in Competitive Programming

## Quick Facts
- **arXiv ID:** 2502.20868
- **Source URL:** https://arxiv.org/abs/2502.20868
- **Reference count:** 13
- **Key outcome:** ProBench introduces an online submission-based benchmark for LLMs in competitive programming, revealing that reasoning-oriented models significantly outperform general-purpose models, with QwQ-32B-Preview achieving the highest score of 20.93.

## Executive Summary
This paper introduces ProBench, a comprehensive benchmark for evaluating large language models (LLMs) in competitive programming, addressing the inadequacy of existing benchmarks for advanced reasoning models. ProBench collects real competition problems from Codeforces, Luogu, and Nowcoder platforms (July–December 2024) and employs an online submission strategy to leverage original platforms' robust test suites, ensuring fair and accurate code robustness assessment. The benchmark establishes a unified problem attribute system with difficulty grading and algorithm tagging. Experiments on 9 latest LLMs reveal that reasoning-oriented models significantly outperform general-purpose models, with QwQ-32B-Preview achieving the highest score of 20.93. Analysis identifies key areas for enhancement, including algorithm adaptability and reasoning sufficiency, providing insights for future reasoning model development.

## Method Summary
ProBench collects 790 competitive programming problems from Codeforces, Luogu, and Nowcoder platforms (July–December 2024), normalizes difficulty ratings and algorithm tags into unified categories, and evaluates LLM-generated code through automated online submissions to each platform's original judge system. For each problem, 8 solutions are generated using standardized prompts in C++ (primary), Java, and Python, with pass@k (k=1,2,4,8) computed from the verdicts. The approach leverages platform-specific test cases to comprehensively assess code robustness, eliminating potential false positive outcomes from synthetic test suites.

## Key Results
- QwQ-32B-Preview achieves the highest score of 20.93, followed by DeepSeek-V3 at 16.38, demonstrating that reasoning-trained models significantly outperform general-purpose models
- Models show strong performance on easy problems (scores 8.61–40.66) but performance approaches zero for medium and hard difficulty tiers
- Reasoning-oriented models exhibit significantly longer CoT sequences (10k characters on average) compared to non-reasoning models
- Effectiveness progressively diminishes with increasing reasoning demands, particularly in Search and DP algorithm categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Online submission to original platforms reduces false positive evaluations compared to synthetic offline test suites.
- **Mechanism:** Model-generated code is submitted directly to Codeforces, Luogu, and Nowcoder online judges, which employ comprehensive, expert-designed test cases covering edge cases that synthetic tests typically miss. This leverages platform-specific validation infrastructure that enforces time-space constraints rigorously.
- **Core assumption:** Original platform test suites are both more comprehensive and more representative of real-world correctness criteria than researcher-generated alternatives.
- **Evidence anchors:**
  - [abstract] "obtaining real test results through online submissions to ensure the fairness and accuracy of the evaluation"
  - [Section 3.3] "This approach leverages the platform's proprietary test cases to comprehensively assess code robustness, thereby eliminating potential false positive outcomes"
  - [corpus] OJBench (arXiv:2506.16395) similarly notes limitations in existing code benchmarks at competitive levels; corpus evidence supporting online submission specifically is weak beyond this paper.
- **Break condition:** If platforms change submission policies, rate-limit submissions, or if test case quality degrades over time, the fairness advantage diminishes.

### Mechanism 2
- **Claim:** Reasoning-oriented training improves competitive programming performance independent of model scale.
- **Mechanism:** Models trained with chain-of-thought prompting and reinforcement learning (e.g., QwQ-32B-Preview, DeepSeek-R1 variants) generate longer CoT sequences (averaging 10k characters vs. shorter for non-reasoning models), enabling deeper problem decomposition and algorithm selection before code generation.
- **Core assumption:** Extended CoT length causally contributes to better algorithmic reasoning rather than merely correlating with model architecture differences.
- **Evidence anchors:**
  - [abstract] "QwQ-32B-Preview achieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38, suggesting that models trained with specialized reasoning tasks significantly outperform general-purpose models"
  - [Section 5.1] "reasoning-oriented models exhibit significantly longer CoT sequences (10k of characters on average) compared to non-reasoning models"
  - [corpus] LiveOIBench (arXiv:2510.09595) similarly evaluates LLMs on competition-level problems but focuses on informatics olympiads; does not directly validate the CoT-length mechanism.
- **Break condition:** If longer CoT reflects verbosity without improved reasoning quality, or if inference costs outweigh accuracy gains, the mechanism's practical value collapses.

### Mechanism 3
- **Claim:** Difficulty-stratified evaluation reveals under-reasoning phenomena that aggregate metrics obscure.
- **Mechanism:** By normalizing problem difficulty across platforms into Easy/Medium/Hard tiers (aligned with ICPC award criteria), ProBench detects that models show only 30% CoT length growth from easy to medium and 15% from medium to hard—insufficient adaptation to problem complexity.
- **Core assumption:** Difficulty tiers across heterogeneous platforms can be meaningfully unified and human-defined difficulty correlates with reasoning demands.
- **Evidence anchors:**
  - [Section 4.2] "At the 'easy' difficulty level, examined LLMs demonstrates strong capability with scores ranging from 8.61 to 40.66, whereas their scores approach 0 for both 'medium' and 'hard' difficulty tiers"
  - [Section 5.1] "most models demonstrate relatively modest growth magnitudes in reasoning length, approximately 30% from easy to medium difficulty level, and merely 15% from medium to hard"
  - [corpus] LLM-ProS (arXiv:2502.04355) analyzes ICPC problems but does not quantify under-reasoning; corpus validation of this specific mechanism is weak.
- **Break condition:** If difficulty mappings across platforms prove inconsistent or if problem difficulty does not linearly correlate with reasoning depth required, stratified analysis becomes noisy.

## Foundational Learning

- **Concept: Online Judge Systems**
  - **Why needed here:** ProBench's core innovation depends on understanding how platforms like Codeforces evaluate submissions—test case execution, time/memory limits, and verdict types (WA, TLE, RE).
  - **Quick check question:** Can you explain why offline synthetic test suites produce more false positives than platform-based evaluation?

- **Concept: Pass@k Metric**
  - **Why needed here:** Evaluation uses pass@k (k=1,2,4,8) to measure the probability of finding at least one correct solution among k samples, critical for understanding model reliability vs. capability.
  - **Quick check question:** Why might pass@1 be more informative than pass@8 for assessing a model's practical deployment viability?

- **Concept: Chain-of-Thought Reasoning**
  - **Why needed here:** The paper attributes reasoning model success to extended CoT; understanding CoT length as a proxy for deliberation depth is essential for interpreting Section 5.1's under-reasoning analysis.
  - **Quick check question:** How would you distinguish between productive CoT elaboration and unproductive verbosity in model outputs?

## Architecture Onboarding

- **Component map:** Data Collection -> Attribute Integration -> Code Generation -> Online Submission -> Verdict Collection -> Multi-dimensional Analysis
- **Critical path:** Data collection → Attribute integration → Code generation → Online submission → Verdict collection → Multi-dimensional analysis. The online submission step is the bottleneck (rate limits, latency).
- **Design tradeoffs:**
  - Online vs. offline evaluation: Online ensures fairness but is slower and subject to platform availability.
  - Multi-platform vs. single-platform: Increases diversity (EN/CN, varied difficulty scales) but complicates attribute normalization.
  - C++ as primary language: Matches competitive programming norms but may underrepresent Python-centric code models.
- **Failure signatures:**
  - High early-test-case error rates (Case 0 failures >90%) indicate shallow reasoning.
  - Predominant "WRONG_ANSWER" vs. "COMPILATION_ERROR" distinguishes reasoning vs. code generation failures.
  - Flat CoT growth across difficulty levels signals under-reasoning.
- **First 3 experiments:**
  1. **Baseline pass@1 across all models on full ProBench:** Establishes ranking and validates that reasoning models (QwQ, Skywork-o1) outperform larger non-reasoning models.
  2. **Difficulty-stratified analysis:** Compute pass@1 separately for Easy/Medium/Hard to confirm performance collapse at higher tiers.
  3. **Error type distribution per model:** Categorize failures into code errors vs. reasoning errors to identify whether improvement should target syntax/execution or algorithmic logic.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can model architectures or training methodologies be modified to enable dynamic reasoning length scaling that prevents "under-reasoning" on hard problems and "over-reasoning" on easy problems?
- **Basis in paper:** [inferred] Section 5.1 analyzes Chain-of-Thought (CoT) length, finding that models show "potential under-reasoning phenomena" on high-difficulty tasks (growth in reasoning length is constrained) and "potential over-reasoning tendencies" on simple tasks.
- **Why unresolved:** The paper identifies the correlation between static reasoning lengths and performance caps but does not propose a mechanism for dynamic compute allocation based on problem complexity.
- **What evidence would resolve it:** A study showing a model that dynamically extends its CoT for "Hard" problems while shortening it for "Easy" problems, resulting in higher efficiency and pass@1 scores on ProBench.

### Open Question 2
- **Question:** How will the performance hierarchy on ProBench shift when evaluating the most computationally intensive reasoning models (e.g., DeepSeek-R1, OpenAI-o3) that were excluded from the initial study?
- **Basis in paper:** [explicit] The Limitation section states, "As of current testing phases, the number of evaluated reasoning language models remains limited... primarily stems from the substantial computational reasoning requirements of DeepSeek-R1... We plan to expedite the release of comprehensive evaluation results for these models."
- **Why unresolved:** The authors explicitly listed the evaluation of these specific advanced models as incomplete due to execution duration constraints.
- **What evidence would resolve it:** Publication of the authors' planned follow-up results or independent replication showing pass@1 scores for DeepSeek-R1 and OpenAI-o3 on the ProBench dataset.

### Open Question 3
- **Question:** What specific deficiencies in logical deduction or state-space search cause the significant performance gap between low-intensity reasoning tasks (e.g., String, Basic) and high-intensity ones (e.g., Search, DP), even at identical difficulty ratings?
- **Basis in paper:** [inferred] Section 5.4 establishes a "reasoning complexity hierarchy" and notes that effectiveness "progressively diminishes with increasing reasoning demands," suggesting a specific failure mode in advanced logical reasoning capabilities that requires further diagnosis.
- **Why unresolved:** The paper quantifies the performance drop across algorithm tags but leaves the root cause (e.g., lack of planning depth vs. lack of specific algorithmic knowledge) as an open area for enhancement.
- **What evidence would resolve it:** An error analysis isolating whether failures in Search/DP tasks stem from incorrect state definition, flawed transition logic, or inefficient pruning strategies.

## Limitations
- Platform dependency: The benchmark's validity hinges on continued access to Codeforces, Luogu, and Nowcoder APIs and submission mechanisms, with rate limits potentially restricting future reproducibility
- Difficulty normalization uncertainty: Unified difficulty mapping across platforms assumes linear comparability that may not hold, particularly for CN vs EN problems
- Sample size constraints: The 790-problem sample may not capture the full distribution of competitive programming problem types, especially rare but important algorithm categories

## Confidence
- **High Confidence:** The core observation that reasoning-oriented models outperform general-purpose models of comparable size, supported by clear score differentials and CoT length measurements
- **Medium Confidence:** The under-reasoning phenomenon across difficulty tiers, as the difficulty normalization across platforms introduces uncertainty in true problem complexity alignment
- **Low Confidence:** The mechanism by which extended CoT directly causes better reasoning rather than merely correlating with model architecture or training data differences

## Next Checks
1. **Cross-platform consistency test:** Randomly sample 50 problems and have independent human experts rate difficulty and algorithm complexity to validate the unified difficulty mapping across Codeforces, Luogu, and Nowcoder
2. **CoT quality audit:** Manually analyze 100 CoT sequences from top and bottom performing models to distinguish productive reasoning from mere verbosity, quantifying the relationship between CoT length and reasoning quality
3. **Synthetic vs. online comparison:** Run a subset of problems through both ProBench's online submission and a carefully constructed offline test suite to measure false positive rates and identify specific edge cases missed by synthetic tests