---
ver: rpa2
title: 'DINA: A Dual Defense Framework Against Internal Noise and External Attacks
  in Natural Language Processing'
arxiv_id: '2508.05671'
source_url: https://arxiv.org/abs/2508.05671
tags:
- adversarial
- label
- internal
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two simultaneous threats to NLP systems:
  internal label noise (caused by adversarial or erroneous annotators) and external
  adversarial attacks (such as character-level perturbations designed to evade detection).
  The proposed DINA framework integrates noisy-label learning with adversarial training.'
---

# DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing

## Quick Facts
- arXiv ID: 2508.05671
- Source URL: https://arxiv.org/abs/2508.05671
- Reference count: 30
- Primary result: DINA achieves 0.903 accuracy on a Chinese dataset without attacks and maintains high accuracy under adversarial attacks, outperforming baseline by 6-10% accuracy.

## Executive Summary
This paper introduces DINA, a dual defense framework that simultaneously addresses two major threats to NLP systems: internal label noise from adversarial or erroneous annotators and external adversarial text attacks. DINA integrates noisy-label learning with adversarial training by first using semi-supervised learning (DivideMix) to identify and correct corrupted training labels, then applying adversarial training with synthetic perturbations to improve robustness. Experiments on a real-world Chinese dataset from an online gaming service demonstrate that DINA significantly outperforms baseline models, maintaining high accuracy even under sophisticated adversarial attacks.

## Method Summary
DINA combines noisy-label learning with adversarial training to create a robust NLP defense system. The framework first applies semi-supervised learning techniques, specifically DivideMix, to detect and correct corrupted training labels in the presence of internal noise. It then incorporates adversarial training (A2T) using synthetic character-level perturbations generated during the training process. This dual approach allows the model to handle both mislabeled training data and external adversarial attacks simultaneously, creating a more resilient NLP system that maintains performance across both threat types.

## Key Results
- DINA achieves 0.903 accuracy on Chinese dataset without attacks, outperforming baseline (0.835)
- Under Random Attack, DINA accuracy drops to 0.901 versus baseline drop to 0.802
- Under BERT-Attack, DINA achieves 0.862 accuracy compared to baseline's 0.798
- Optimal performance achieved with approximately 200K adversarial examples during training

## Why This Works (Mechanism)
DINA's effectiveness stems from its dual-layer defense strategy that addresses both internal and external threats simultaneously. The semi-supervised noisy-label correction component (DivideMix) identifies and separates clean from noisy samples, allowing the model to focus on reliable training signals while avoiding the propagation of label errors. This internal defense creates a stronger foundation by ensuring the model learns from accurate data. The adversarial training component then builds on this clean foundation by exposing the model to synthetic adversarial examples during training, forcing it to develop robust representations that can withstand character-level perturbations and other attack vectors. The sequential integration ensures that the model first establishes strong learning from clean data before developing attack resistance, rather than attempting to solve both problems in isolation.

## Foundational Learning
- **Noisy-label learning**: Required because real-world datasets often contain mislabeled examples from adversarial annotators or human error; quick check: verify label corruption rate in your dataset
- **Semi-supervised learning**: Needed to leverage unlabeled or partially labeled data for improving model robustness; quick check: assess availability of unlabeled data for augmentation
- **Adversarial training**: Essential for building models resistant to intentional perturbations designed to fool classifiers; quick check: determine attack types most relevant to your application
- **Character-level perturbations**: Critical attack vector in NLP that modifies individual characters while preserving readability; quick check: evaluate whether your threat model includes such attacks
- **DivideMix algorithm**: Specialized technique for separating clean and noisy samples in training data; quick check: confirm DivideMix implementation compatibility with your framework
- **Synthetic adversarial example generation**: Used to create training data that mimics potential attack patterns; quick check: validate generated examples maintain semantic meaning

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Noisy-Label Correction (DivideMix) -> Adversarial Training (A2T) -> Robust Model

**Critical Path**: The most critical path flows from data preprocessing through noisy-label correction to adversarial training. Each stage builds upon the previous one: clean data from DivideMix enables effective adversarial training, which in turn produces the robust final model. Bypassing noisy-label correction would compromise the foundation for adversarial training.

**Design Tradeoffs**: The framework trades computational efficiency for robustness, as both DivideMix and adversarial training significantly increase training time. The choice of 200K adversarial examples represents a balance between sufficient coverage of attack space and computational feasibility. The sequential approach may miss synergies that parallel processing could offer but ensures stable convergence.

**Failure Signatures**: Poor performance may indicate inadequate noisy-label separation (if baseline accuracy remains low), insufficient adversarial example diversity (if attack resistance is weak), or incompatibility between the two defense mechanisms (if combined performance doesn't exceed individual components). Overfitting to synthetic attacks may also reduce generalization to unseen attack patterns.

**First 3 Experiments**:
1. Test noisy-label correction alone on corrupted datasets to establish baseline improvement from DivideMix
2. Evaluate adversarial training alone with clean data to measure attack resistance gains
3. Run combined DINA approach with varying numbers of adversarial examples (50K, 200K, 500K) to identify optimal training set size

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends on synthetic adversarial examples that may not reflect real-world attack diversity
- Limited testing to Chinese text in gaming domain, raising questions about cross-domain and multilingual applicability
- No comparison to more recent multi-threat defense methods in the literature
- Optimal adversarial example size (~200K) appears dataset-specific and may not generalize

## Confidence
- Dual defense strategy effectiveness: High
- Experimental results validity: Medium
- Generalization to other languages/domains: Low
- Comparison to state-of-the-art: Medium

## Next Checks
1. Test DINA on multilingual datasets with diverse noise distributions and attack vectors to assess generalization
2. Conduct broader ablation comparing DINA to emerging multi-threat defense frameworks in the literature
3. Evaluate performance under adaptive attacks that target the specific defenses employed by DINA to better understand its robustness limits