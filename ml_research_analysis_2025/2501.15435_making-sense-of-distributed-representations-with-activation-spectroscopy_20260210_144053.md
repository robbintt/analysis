---
ver: rpa2
title: Making Sense Of Distributed Representations With Activation Spectroscopy
arxiv_id: '2501.15435'
source_url: https://arxiv.org/abs/2501.15435
tags:
- fourier
- function
- network
- coefficients
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting distributed
  representations in neural networks by developing Activation Spectroscopy (ActSpec),
  which analyzes the pseudo-Boolean Fourier spectrum of activation patterns. The key
  insight is casting the sub-network from a layer to an output logit as a pseudo-Boolean
  function, where Fourier coefficients quantify the contributions of neuron subsets.
---

# Making Sense Of Distributed Representations With Activation Spectroscopy

## Quick Facts
- arXiv ID: 2501.15435
- Source URL: https://arxiv.org/abs/2501.15435
- Reference count: 22
- Key outcome: Activation Spectroscopy (ActSpec) analyzes pseudo-Boolean Fourier spectra to identify joint neuron influences in distributed representations, achieving perfect influence estimation in synthetic experiments versus systematic errors from attribution methods

## Executive Summary
This paper addresses the fundamental challenge of interpreting distributed representations in neural networks by developing Activation Spectroscopy (ActSpec), a method that quantifies joint influences of neuron subsets through pseudo-Boolean Fourier analysis. The key insight is casting the sub-network from a layer to an output logit as a pseudo-Boolean function, where Fourier coefficients directly measure the contributions of specific neuron subsets to output variance. ActSpec extends the Goldreich-Levin algorithm with problem-specific constraints to handle out-of-distribution samples and coefficient redundancy. In synthetic experiments, ActSpec achieved perfect influence estimation compared to ground truth, while attribution methods showed systematic errors. On real-world tasks, the method identified intuitive pixel sets for digit classification and revealed how dropout regularization changes information concentration patterns.

## Method Summary
Activation Spectroscopy casts the sub-network from a layer to an output logit as a pseudo-Boolean function f: {−1,1}ⁿ → ℝ, where Fourier coefficients quantify the contributions of neuron subsets to output variance. The method extends the Goldreich-Levin algorithm with two key modifications: (1) projection handling for out-of-distribution samples that zeros out OOD outputs while preserving Fourier coefficient estimation, and (2) redundancy filtering that prevents spurious high-order coefficients from being misinterpreted as meaningful distributed representations. ActSpec identifies high-valued, non-redundant subsets of neurons whose joint activation patterns significantly influence model predictions, then aggregates these coefficients to estimate per-variable importance scores. The approach is validated through synthetic multi-tiered Boolean functions with known ground truth, MNIST digit classification, and sentiment analysis with roBERTa.

## Key Results
- Synthetic experiments: ActSpec achieved 0.0 total variation distance from ground truth Influence values versus 0.1875 for attribution methods with 50 samples
- MNIST experiments: ActSpec identified intuitive pixel sets for distinguishing digits (e.g., horizontal top bar for digit 0 vs 8, lower-right pixels for 1 vs 7)
- roBERTa experiments: Interventions on ActSpec-identified neuron sets changed classification decisions in 32.4% of cases, revealing distributed representations that attribution methods miss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint neuron influences can be quantified through pseudo-Boolean Fourier coefficients, where each coefficient measures the contribution of a specific neuron subset to output variance.
- **Mechanism:** The sub-network from a layer to an output logit is cast as a pseudo-Boolean function f: {−1,1}ⁿ → ℝ. Fourier coefficients ĥ(S) are computed as scaled cosine distances between the parity of subset S and the function output. Higher-order coefficients (|S| > 1) directly capture distributed interactions that single-neuron attribution methods miss.
- **Core assumption:** Binarized activation patterns preserve sufficient information about the continuous representation. The authors note: "for every experiment we ran, we found that all joint activation patterns corresponded to a unique continuous representation vector" but acknowledge this may not generalize universally.
- **Evidence anchors:**
  - [abstract] "Fourier coefficients quantify the contributions of neuron subsets"
  - [Section 3.1] Defines Fourier expansion theorem and coefficient computation
  - [corpus] "Polysemy of Synthetic Neurons" discusses polysemantic neurons that respond to multiple features, supporting the distributed representation premise
- **Break condition:** If binarization creates many-to-one mappings from continuous activations, or if the function's Fourier spectrum is not concentrated (high entropy across all 2ⁿ coefficients), tractability is lost.

### Mechanism 2
- **Claim:** Standard Fourier analysis guarantees extend to in-distribution-only samples when treating the function as a pseudo-Boolean projection.
- **Mechanism:** Define projection P that zeros out all OOD sample outputs. The resulting function g(X) = P·f(X) satisfies: (1) Fourier coefficients match those computed only over in-distribution samples, and (2) OOD samples contribute nothing to coefficient estimation. This preserves Goldreich-Levin sample complexity guarantees with normalization constant adjustment.
- **Core assumption:** The projection is the unique function satisfying both conditions—similar to a maximum entropy principle where uncertainty is maximized outside satisfied constraints.
- **Evidence anchors:**
  - [Section 4.1] "uniform sampling over an In-distribution subspace is equivalent to uniform sampling over the entire space up to a constant factor"
  - [Section 4.1] Sketches proof that projection maintains orthonormal basis properties
  - [corpus] No direct corpus validation; this appears to be a novel theoretical contribution
- **Break condition:** If in-distribution samples do not adequately cover the relevant regions of the Boolean hypercube, coefficient estimates will be biased or incomplete.

### Mechanism 3
- **Claim:** Filtering redundant variables during Goldreich-Levin search prevents spurious high-order coefficients from being misinterpreted as meaningful distributed representations.
- **Mechanism:** When two variables have non-zero inner product over in-distribution samples (e.g., X₃ ≈ X₁·X₂·X₄·X₅ in the paper's example), they become interchangeable in Fourier coefficients. ActSpec adds an inner product check before accepting variables: if a candidate variable's parity correlates with existing subset parities above threshold γ, it is rejected as redundant rather than genuinely informative.
- **Core assumption:** Minimal descriptions (smaller subsets) are preferable explanations when multiple coefficient sets explain the same variance.
- **Evidence anchors:**
  - [Section 4.2, Figure 1] Illustrates how X₃ = X₄ in-distribution leads to θ₃ = θ₄
  - [Table 1] Shows attribution methods assign high importance to spurious constant variables; ActSpec correctly assigns 0.0 distance from ground truth
  - [corpus] Weak direct evidence; redundancy handling appears novel to this work
- **Break condition:** If genuinely distinct features happen to correlate strongly in the observed data distribution, ActSpec may incorrectly filter them as redundant.

## Foundational Learning

- **Concept: Pseudo-Boolean Fourier Analysis**
  - **Why needed here:** Understanding that any function f: {−1,1}ⁿ → ℝ has a unique multilinear polynomial representation via Fourier expansion is foundational to the entire method.
  - **Quick check question:** Can you explain why the parity functions xₛ = ∏ᵢ∈S xᵢ form an orthonormal basis for functions over the Boolean hypercube?

- **Concept: Influence (Average Sensitivity)**
  - **Why needed here:** The paper uses Influence—the probability that flipping a bit changes the output—as the ground truth importance measure that ActSpec estimates through squared Fourier coefficient sums.
  - **Quick check question:** Why does Infᵢ = Σ_{S∋ᵢ} ĥ(S)² hold, and what does this imply about how Fourier coefficients relate to variable importance?

- **Concept: Goldreich-Levin Algorithm**
  - **Why needed here:** ActSpec is explicitly framed as "an extension of the Goldreich-Levin algorithm which incorporates additional problem-specific constraints." Understanding GL's branch-and-bound structure is prerequisite.
  - **Quick check question:** How does Goldreich-Levin avoid exponential enumeration by estimating sums over collections of squared Fourier coefficients?

## Architecture Onboarding

- **Component map:** Binarization layer -> Projection constructor -> Modified GL search -> Influence aggregator
- **Critical path:**
  1. Collect in-distribution activation samples from target layer
  2. Binarize activations; verify unique mapping to continuous representations
  3. Run ActSpec with chosen thresholds (τ, γ) to identify high-valued, non-redundant subsets
  4. Aggregate coefficients to estimate per-variable Influence
  5. Validate via ablation/intervention experiments
- **Design tradeoffs:**
  - **τ (coefficient threshold):** Lower values capture more subsets but increase computational cost O(2ⁿS log 1/δ) where S = number of subsets above τ
  - **γ (redundancy threshold):** Higher values are more permissive but risk spurious interpretations
  - **Sample count:** Table 1 shows 1000 samples achieve 0.0424 TV distance vs. 0.1875 with 50 samples on noisy synthetic data
  - **Binarization:** May lose information in dense continuous encodings; authors note uncertainty about general applicability
- **Failure signatures:**
  - Many high-order coefficients (|S| ≥ 4) without corresponding predictive power suggests redundancy filtering is too weak
  - Interventions on identified subsets not changing predictions (unlike the 32.4% flip rate observed) indicates coefficients don't capture causal structure
  - Attribution results matching baseline methods on tasks requiring joint feature detection suggests the method isn't finding distributed representations
- **First 3 experiments:**
  1. **Synthetic validation:** Implement the multi-tiered Boolean function (f(X₁,X₂,X₃,X₄) = 1 iff X₃=X₄=1 or sorted order), verify ActSpec recovers ground truth Influence with TV distance < 0.05 in high-sample regime
  2. **Constant variable test:** Add a spurious constant input variable; confirm ActSpec assigns zero importance (unlike attribution methods that assigned it highest importance)
  3. **Intervention validation:** On a trained classifier, identify influential subsets via ActSpec, then ablate; measure classification flip rate—should exceed random subset ablation significantly (paper achieved 32.4% vs. ~0.96% individual-only mismatch rate)

## Open Questions the Paper Calls Out
- **Question:** Does the one-to-one mapping between binarized activation patterns and continuous representations hold across diverse network architectures, or does information loss occur in densely encoded layers?
  - **Basis in paper:** [explicit] "While this supports the one-to-one mapping in our experiments, we are unsure how generally applicable this is across all networks."
  - **Why unresolved:** The authors only verified this property empirically on their experimental networks, not systematically across architecture types, activation functions, or layer widths.
  - **What evidence would resolve it:** Systematic testing across varied architectures (CNNs, different transformer configurations, residual networks) measuring reconstruction error between binary and continuous representations.

- **Question:** Can principled methods be developed for selecting the thresholds τ and γ without requiring ground truth knowledge?
  - **Basis in paper:** [inferred] The paper introduces threshold parameters for the extended Goldreich-Levin algorithm but provides no guidance on principled selection; results depend on these choices.
  - **Why unresolved:** Threshold selection appears ad hoc in experiments, yet directly affects which subsets are identified as important.
  - **What evidence would resolve it:** Development of adaptive threshold selection methods with theoretical guarantees, validated against synthetic benchmarks with known ground truth.

- **Question:** What are the theoretical bounds on intervention effectiveness using ActSpec-identified subsets, and why do interventions fail in ~68% of cases?
  - **Basis in paper:** [inferred] The 32.4% intervention success rate is presented without explanation of failure modes or theoretical predictions for when interventions should succeed.
  - **Why unresolved:** The paper demonstrates feasibility but does not characterize conditions under which identified subsets are causally sufficient for changing model behavior.
  - **What evidence would resolve it:** Systematic analysis correlating intervention success with Fourier coefficient magnitude, subset size, and activation patterns, plus development of success probability estimators.

## Limitations
- The binarization assumption—that unique binary activation patterns map to unique continuous representations—lacks theoretical guarantees for general network architectures
- The method's computational complexity O(2ⁿS log 1/δ) becomes prohibitive for layers with many neurons, limiting scalability
- Key threshold values (τ for coefficient selection, γ for redundancy filtering) and architectural details are referenced to an unavailable Appendix, preventing faithful reproduction

## Confidence
- **High confidence:** The core mechanism of using Fourier coefficients to quantify joint neuron influences, and the synthetic experiment validation showing ActSpec recovers ground truth Influence values
- **Medium confidence:** The OOD sample handling through projection and redundancy filtering appear theoretically sound but lack extensive empirical validation beyond the synthetic case
- **Low confidence:** The generalizability of the binarization assumption across different network architectures and the method's scalability to deep layers with many neurons

## Next Checks
1. **Threshold sensitivity analysis:** Systematically vary τ and γ values on the synthetic multi-tiered Boolean function to identify robust parameter ranges that maintain low TV distance from ground truth
2. **Architecture generalization test:** Apply ActSpec to a 3-layer MLP on CIFAR-10 (instead of MNIST) to validate the binarization assumption holds for more complex visual features
3. **Scalability benchmark:** Run ActSpec on intermediate layers of a larger network (e.g., ResNet-18 on CIFAR-10) and measure computation time vs. number of neurons to establish practical limits