---
ver: rpa2
title: Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving
arxiv_id: '2503.23965'
source_url: https://arxiv.org/abs/2503.23965
tags:
- traffic
- light
- detection
- vitlr
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of real-time traffic light recognition
  for autonomous driving, specifically targeting the limitations of single-frame approaches
  that struggle with occlusions, adverse lighting, and complex urban scenarios. The
  proposed ViTLR is a video-based end-to-end neural network that processes multiple
  consecutive frames using a transformer-like architecture with convolutional self-attention
  modules, optimized for deployment on the Rockchip RV1126 embedded platform's NPU.
---

# Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving

## Quick Facts
- arXiv ID: 2503.23965
- Source URL: https://arxiv.org/abs/2503.23965
- Reference count: 30
- The paper addresses the challenge of real-time traffic light recognition for autonomous driving, specifically targeting the limitations of single-frame approaches that struggle with occlusions, adverse lighting, and complex urban scenarios. The proposed ViTLR is a video-based end-to-end neural network that processes multiple consecutive frames using a transformer-like architecture with convolutional self-attention modules, optimized for deployment on the Rockchip RV1126 embedded platform's NPU. The system achieves state-of-the-art performance with 79.6% mAP on the NTLD dataset while maintaining real-time processing capabilities (>25 FPS) on the RV1126 NPU. ViTLR demonstrates superior robustness compared to existing methods across temporal stability, varying target distances, and challenging environmental conditions, making it particularly effective for autonomous driving applications in urban environments.

## Executive Summary
This paper presents ViTLR, a video-based traffic light recognition system specifically designed for real-time deployment on the Rockchip RV1126 embedded platform. The system addresses key limitations of existing single-frame approaches by processing multiple consecutive frames through a transformer-like architecture that uses convolutional self-attention modules. By optimizing for the NPU's hardware constraints, ViTLR achieves state-of-the-art performance (79.6% mAP on NTLD) while maintaining real-time processing speeds (>25 FPS), making it suitable for autonomous driving applications in complex urban environments.

## Method Summary
The proposed ViTLR is a one-stage, end-to-end neural network that processes sequences of video frames to detect and classify traffic light states. The architecture leverages a YOLOX backbone for feature extraction, followed by a CNN-Encoder and CNN-Decoder that use convolutional self-attention instead of standard transformer attention to optimize for the RV1126's NPU constraints. The system takes $n$ consecutive frames as input, with 3 frames providing the optimal balance between accuracy and real-time performance. Training employs a combination of Focal Loss for classification and CIoU Loss for localization, with the model specifically designed to handle the Rockchip RKNN toolkit's operator limitations through the use of depthwise separable convolutions.

## Key Results
- Achieves 79.6% mAP on NTLD dataset with 3-frame input, outperforming single-frame approaches
- Maintains real-time processing speed (>25 FPS) on Rockchip RV1126 NPU
- Demonstrates superior robustness under occlusion, adverse lighting, and varying target distances
- ViTLR-5 achieves 85.75% mAP but drops to ~13 FPS, highlighting the accuracy-efficiency tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Processing multiple consecutive frames mitigates recognition failures caused by transient occlusion and adverse lighting, which are common failure modes in single-frame approaches.
- **Mechanism:** The model takes a sequence of frames ($X_t, \dots, X_n$) rather than a snapshot. By aggregating features across time using the CNN-Encoder, the system creates a temporal context that allows it to infer the presence and state of a traffic light even if it is obscured or blurred in the current specific frame ($t$).
- **Core assumption:** The state of a traffic light is temporally consistent over short intervals, and relevant visual features exist in at least some frames within the input window.
- **Evidence anchors:**
  - [abstract] "struggle with complex scenarios involving occlusions... ViTLR... processes multiple consecutive frames to achieve robust traffic light detection"
  - [section VI.C] "models utilizing multi-frame temporal features significantly outperform standard single-frame detection models, particularly in situations involving occlusion and incompleteness."
  - [corpus] Corpus papers (e.g., *The ATLAS of Traffic Lights*) support the general difficulty of occlusion in TLR, though specific evidence for this specific temporal mechanism is drawn primarily from the primary text.
- **Break condition:** Performance degrades if the traffic light is occluded or corrupted across *all* input frames (e.g., a persistent physical obstruction or camera failure lasting longer than the frame window).

### Mechanism 2
- **Claim:** Replacing standard Transformer self-attention with Convolutional Self-Attention (CSA) enables efficient deployment on the resource-constrained RV1126 NPU by aligning with hardware-preferred operations.
- **Mechanism:** Standard Transformer attention relies on matrix multiplications that may be sub-optimal or unsupported on the RV1126's NPU. ViTLR substitutes these with depthwise separable convolutions (specifically utilizing 3x3 and 9x9 kernels). This leverages the NPU's optimization for standard Conv2D operations, reducing computational overhead and latency.
- **Core assumption:** The RKNN toolkit and RV1126 hardware accelerate convolutional operations significantly better than the generalized matrix operations used in standard attention mechanisms.
- **Evidence anchors:**
  - [section IV.B] "Convolutional Projection module leverages depthwise separable convolution in place of the conventional multi-head attention mechanism... enhancing computational efficiency"
  - [section II.B] "RKNN model only supports a limited number of operators... kernel size of the Conv2D is preferably set by 3 Ã— 3... reducing additional computational consumption"
  - [corpus] Weak direct evidence in the provided corpus regarding RV1126 specifically; mechanism relies on the paper's internal hardware analysis.
- **Break condition:** If deployed on a different chipset (e.g., a standard GPU), this specific convolutional architecture might lose its efficiency advantage over standard attention or suffer from relative accuracy drops compared to full Transformer models.

### Mechanism 3
- **Claim:** A "CNN-Decoder" architecture using object queries allows for end-to-end detection without the computational bottleneck of post-processing modules like NMS (Non-Maximum Suppression) typically found in older one-stage detectors.
- **Mechanism:** The architecture utilizes a set of learned "object queries" processed by Self-Interaction (SIM) and Cross-Interaction (CIM) modules. This forces the model to predict a fixed set of bounding boxes ($m$) directly, rather than generating thousands of redundant proposals that require complex filtering.
- **Core assumption:** The number of traffic lights in a scene generally does not exceed the fixed maximum query count ($m$).
- **Evidence anchors:**
  - [section IV.B] "The CNN Decoder is composed of the Self-Interaction Module (SIM) and the Cross-Interaction Module (CIM)... [outputting] processed object query features"
  - [table II] Shows ViTLR-3 achieving 25.16 FPS, outperforming standard YOLO (6.94 FPS) on the same hardware, suggesting the architecture is better suited for the specific NPU pipeline than traditional proposal-based methods.
  - [corpus] No specific corpus evidence refutes this, but *TigAug* highlights the general fragility of detection models, making this efficiency gain critical for stability.
- **Break condition:** The model fails if the scene contains more traffic lights than the pre-defined maximum number of object queries ($m$).

## Foundational Learning

- **Concept:** **Depthwise Separable Convolution**
  - **Why needed here:** The paper relies on this operation to replace standard attention mechanisms. Understanding it is required to comprehend how the model fits on the RV1126 NPU.
  - **Quick check question:** How does a depthwise separable convolution reduce parameter count compared to a standard convolution?

- **Concept:** **Temporal Aggregation in Video Object Detection (VOD)**
  - **Why needed here:** ViTLR is fundamentally a video-based model. You must understand how features from Frame $t-2$ are combined with Frame $t$ to grasp the robustness claims.
  - **Quick check question:** Why would aggregating features over time help detect an object that is partially hidden in the current frame?

- **Concept:** **NPU Constraints & Operator Support**
  - **Why needed here:** The architecture is heavily dictated by the RV1126's limitations (preference for 3x3 kernels, lack of support for all PyTorch operators).
  - **Quick check question:** Why might a model that runs fast on an NVIDIA GPU run slowly or fail to compile on an embedded NPU like the RV1126?

## Architecture Onboarding

- **Component map:**
  Input frames -> YOLOX Backbone -> CNN-Encoder (ConvNeXt blocks) -> Convolutional Projection -> CNN-Decoder (SIM+CIM) -> Head (FFN + Polling)

- **Critical path:** The conversion of the PyTorch model to the **RKNN model**. This is where unsupported operators must be identified and replaced (e.g., replacing standard Self-Attention with the custom Convolutional Projection).

- **Design tradeoffs:**
  - **ViTLR-3 vs. ViTLR-5:** ViTLR-5 uses 5 frames and achieves higher mAP (85.75%) but drops to ~13 FPS. ViTLR-3 uses 3 frames, achieves lower mAP (79.6%), but maintains real-time speed (>25 FPS).
  - **Accuracy vs. Hardware Compatibility:** The authors traded the potential theoretical performance of full Transformers for the *deployability* of a convolution-heavy architecture.

- **Failure signatures:**
  - **Temporal Instability:** If the input video has a very low frame rate or extreme latency, the "consecutive" frames may be too different temporally for the aggregation mechanism to work (break condition noted in Section VI-A regarding frame intervals).
  - **NPU Compilation Error:** Standard Transformer attention blocks or non-3x3 convolutions may cause the RKNN conversion to fail or fall back to CPU, destroying performance.

- **First 3 experiments:**
  1. **Frame Ablation:** Run inference using ViTLR-1 (single frame) vs. ViTLR-3 on a video with heavy occlusion to verify the temporal stability claim.
  2. **Latency Profiling:** Measure the inference time of the "Convolutional Projection" layer vs. a standard Multi-Head Attention layer on the RV1126 to quantify the efficiency gain.
  3. **Distance Stress Test:** Evaluate mAP specifically for targets >50 meters away to verify if the feature aggregation actually improves long-range detection as claimed in Figure 6.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important areas remain unexplored based on the content and limitations identified.

## Limitations
- Hardware-specific optimization makes direct comparison to standard GPU-based models potentially misleading
- Dataset bias: NTLD is a proprietary dataset, limiting independent verification
- Real-world deployment gaps: No testing on moving vehicles or with varying weather conditions beyond what's in the datasets

## Confidence
- **High confidence:** Real-time performance claims on RV1126 (>25 FPS), architectural details of ViTLR
- **Medium confidence:** Robustness claims under occlusion and distance (supported by ablation studies but limited dataset scope)
- **Low confidence:** Generalization to other embedded platforms without architectural modifications

## Next Checks
1. **NPU Operator Verification**: Implement a minimal test to verify that Depthwise Separable Convolution runs faster than standard matrix multiplication on the actual RV1126 NPU, confirming the claimed efficiency advantage.

2. **Cross-Dataset Generalization**: Test ViTLR on the publicly available Bosch Small Traffic Lights dataset to assess performance when the model encounters different camera angles, lighting conditions, and traffic light designs.

3. **Temporal Stability Benchmark**: Create a controlled video sequence with systematic occlusion (e.g., using a mask that blocks the traffic light for 1-3 frames) and measure the model's ability to maintain consistent predictions across the occlusion period.