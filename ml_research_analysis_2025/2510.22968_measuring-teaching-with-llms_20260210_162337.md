---
ver: rpa2
title: Measuring Teaching with LLMs
arxiv_id: '2510.22968'
source_url: https://arxiv.org/abs/2510.22968
tags:
- human
- student
- teaching
- classroom
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether sentence-level embeddings can capture
  classroom discourse for automated instructional assessment. Five pretrained embedding
  models were used to train custom encoder architectures on predicting expert human
  ratings across 25 teaching dimensions.
---

# Measuring Teaching with LLMs
## Quick Facts
- arXiv ID: 2510.22968
- Source URL: https://arxiv.org/abs/2510.22968
- Reference count: 39
- Key outcome: Sentence-level embeddings achieve human-level and super-human performance predicting expert ratings of 25 teaching dimensions, with models showing stronger alignment to student learning at aggregate level but not individual rubric items.

## Executive Summary
This study evaluates whether sentence-level embeddings can capture classroom discourse for automated instructional assessment. Five pretrained embedding models were used to train custom encoder architectures on predicting expert human ratings across 25 teaching dimensions. Models achieved human-level and super-human performance, with correlations exceeding 0.65 for CLASS and surpassing human averages for MQI. Analysis showed mature models attribute more score variation to lesson-level features rather than isolated utterances, challenging single-turn evaluation paradigms. Validation using teacher value-added measures confirmed aggregate model scores align with student learning outcomes, though this did not hold at individual item levels, suggesting partial generalization.

## Method Summary
The study uses classroom transcripts from the National Center for Teacher Effectiveness (NCTE) dataset, applying five pretrained sentence embedding models (SimCSE, E5, GTE, RoBERTa) to train multi-task encoders that predict expert human ratings across 25 instructional dimensions. Transcripts are structured into 3-lesson phases with chapters, and models are trained for 5 epochs with strong regularization. Evaluation uses multilevel partial Spearman's correlation against human ratings and τ-canonical correlation with teacher value-added measures for external validity.

## Key Results
- Models achieved human-level correlations (ρ > 0.65) for CLASS items and surpassed human averages for MQI items
- Mature models shifted score variance from utterance-level to lesson-level features as training progressed
- Aggregate model scores aligned with teacher value-added measures, but this correlation did not hold at individual rubric item levels

## Why This Works (Mechanism)

### Mechanism 1: Sentence-Level Embeddings Preserve Pedagogical Semantics
Sentence-level embeddings provide more interpretable and stable representations for classroom discourse than subword tokenization. Sentences function as natural semantic units that align with pedagogical constructs, enabling direct mapping from teacher speech to rubric dimensions without losing contextual coherence.

### Mechanism 2: Context Accumulation Shifts Variance to Lesson-Level Features
More mature models attribute more score variation to lesson-level patterns rather than isolated utterances. As training progresses, the shared-weight architecture learns to integrate information across temporal context windows, reducing sensitivity to local fluctuations and capturing persistent pedagogical patterns.

### Mechanism 3: Multitask Shared Weights Capture General Teaching Quality Before Specializing
Shared-weight multitask architectures initially learn representations that align with student outcomes, but continued training can overfit to noisy human item-level annotations. Early training captures general teaching effectiveness signals; later training optimizes for specific rubric items that may not correlate with actual student learning gains.

## Foundational Learning

- **Sentence Embeddings (vs. Subword Tokenization)**
  - Why needed here: The architecture relies on precomputed sentence-level vectors; you must understand what semantic information is preserved vs. lost compared to token-level approaches
  - Quick check question: Can you explain why a sentence embedding might miss nuanced negation or sarcasm that subword attention could capture?

- **Generalizability Theory Variance Decomposition**
  - Why needed here: The paper uses G-theory to quantify how much score variation comes from different hierarchical levels (teacher, lesson, chapter, utterance, sentence)
  - Quick check question: If 60% of variance is at the lesson level and 10% at the utterance level, what does this imply for how often you should sample within a lesson?

- **Value-Added Measures (VAMs) as External Validity Criterion**
  - Why needed here: VAMs serve as the ground truth for whether model ratings capture features relevant to student learning, not just human rating agreement
  - Quick check question: Why might a model achieve high correlation with human raters but low correlation with VAMs?

## Architecture Onboarding

- **Component map**: Input Layer (Pretrained sentence embeddings) → Shared Encoder (Multi-task architecture) → Task-Specific Heads (25 rubric items) → Temporal Structure (3-phase lessons → chapters → utterances → sentences)

- **Critical path**: 1) Load and segment transcripts into 3-phase structure with chapter divisions 2) Equipartition text to align with human label timestamps 3) Compute sentence embeddings for each utterance 4) Apply sliding-window augmentation 5) Train multitask encoder for 5 epochs with strong regularization 6) Evaluate via multilevel partial Spearman's correlation and τ-canonical correlation

- **Design tradeoffs**: Interpretability vs. granularity (sentence embeddings more interpretable but may lose fine-grained signals); Human alignment vs. outcome alignment (early stopping may preserve VAM correlation); Data efficiency vs. model complexity (5-epoch regime prevents overfitting)

- **Failure signatures**: Utterance-level variance dominates (ρU high) indicates model has not learned lesson-level patterns; Human correlation high but VAM correlation low suggests overfitting to annotation noise; Large performance gap between embedding models indicates domain mismatch in pretraining data

- **First 3 experiments**: 1) Baseline correlation check: Train each embedding model for 5 epochs, report multilevel partial Spearman's correlation, target ρ > 0.6 for at least 2 models 2) Variance decomposition analysis: Apply G-theory at epoch 1 vs. epoch 5, expect ρL increases and ρU decreases 3) External validity probe: Compute τ-canonical correlation with VAMs at each epoch, identify peak before declining

## Open Questions the Paper Calls Out

- **Generalizability across subjects and grade levels**: Can models trained on elementary mathematics transcripts generalize to other academic subjects, grade levels, or cultural contexts? The study is limited to 4th and 5th-grade mathematics and lacks proven generalizability to other domains.

- **Item-level vs. aggregate VAM alignment**: Why does alignment with student value-added measures hold in aggregate but fail at the individual item level? This discrepancy exists but the paper does not determine if it's caused by noisy individual rubric items or a fundamental disconnect between granular instructional features and student outcomes.

- **Domain-specific embedding fine-tuning**: Can domain-specific fine-tuning of sentence embeddings improve pedagogical semantics capture? The study used off-the-shelf pre-trained embeddings and future work should explore fine-tuning on classroom transcripts to capture child speech patterns and pedagogical discourse.

## Limitations
- Models lack proven generalizability beyond 4th and 5th-grade mathematics
- VAM correlation exists only at aggregate level, not for individual rubric items
- 5-epoch training regime may leave models under-optimized for complex pedagogical distinctions

## Confidence
- **High confidence**: Sentence-level embeddings improve interpretability; models achieve human-level correlations; VAM alignment validates general teaching effectiveness capture
- **Medium confidence**: Variance decomposition showing lesson-level dominance; claim that single-turn annotation paradigms are insufficient; interpretation about early training capturing outcome-relevant signals
- **Low confidence**: Exact mechanism by which sentence embeddings outperform subword tokenization; specific thresholds for optimal epoch stopping; generalizability to non-math contexts

## Next Checks
1. **Epoch sensitivity analysis**: Train the best-performing model (E5) for 20 epochs instead of 5, evaluating VAM correlation at each epoch to identify the optimal stopping point where VAM correlation peaks before declining.

2. **Subword vs. sentence embedding ablation**: Implement an identical multi-task architecture using subword tokenization and compare multilevel partial Spearman's correlation and variance decomposition results to test whether sentence embeddings truly provide superior semantic coherence.

3. **Item-level VAM correlation mapping**: For each of the 25 rubric items, compute τ-canonical correlation with VAMs separately to create a heatmap showing which specific instructional dimensions correlate with student outcomes versus those that don't.