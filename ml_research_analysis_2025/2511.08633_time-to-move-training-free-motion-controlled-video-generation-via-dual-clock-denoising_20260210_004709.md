---
ver: rpa2
title: 'Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock
  Denoising'
arxiv_id: '2511.08633'
source_url: https://arxiv.org/abs/2511.08633
tags:
- motion
- video
- control
- diffusion
- appearance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Time-to-Move (TTM), a training-free framework\
  \ for precise motion and appearance control in video diffusion models. TTM uses\
  \ crude user-provided animations (e.g., cut-and-drag or depth-based reprojection)\
  \ as motion proxies and applies dual-clock denoising\u2014a region-dependent strategy\
  \ that enforces strong alignment in motion-specified areas while allowing flexibility\
  \ elsewhere."
---

# Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising

## Quick Facts
- arXiv ID: 2511.08633
- Source URL: https://arxiv.org/abs/2511.08633
- Reference count: 21
- Key outcome: Training-free framework achieving state-of-the-art motion control on MC-Bench and DL3DV benchmarks using dual-clock denoising with image-conditioned video models.

## Executive Summary
Time-to-Move (TTM) introduces a training-free approach for precise motion and appearance control in video diffusion models. The framework uses crude user-provided animations as motion proxies and applies dual-clock denoising—a region-dependent strategy that enforces strong alignment in motion-specified areas while allowing flexibility elsewhere. TTM achieves state-of-the-art results on motion benchmarks and introduces joint motion-appearance control not possible with text-only prompts. It is plug-and-play across multiple backbones without retraining.

## Method Summary
TTM uses dual-clock denoising with an image-conditioned video diffusion model. The method takes a single image, user-specified motion (trajectories or camera paths), and a binary mask indicating motion-guided regions. A warped reference video is generated from the image using forward warping based on the motion specification. The dual-clock process initializes at t_weak (higher noise, weaker constraint) and for t ∈ [t_strong, t_weak), overrides masked regions with the warped reference noised to t-1 at each step. Below t_strong, denoises uniformly. This forces masked areas to track the reference while giving background freedom to adapt. Appearance is preserved via image conditioning, anchoring generation to the clean first frame.

## Key Results
- Lowest CoTracker Distance on MC-Bench motion control benchmark
- 33% lower MSE than training-based baselines on DL3DV camera control task
- Matches or exceeds training-based methods on VBench suite while introducing joint motion-appearance control capability

## Why This Works (Mechanism)

### Mechanism 1: SDEdit-Style Motion Injection via Noise Anchoring
- Claim: Noising a crude reference video to the diffusion timestep where motion structure is established injects intended dynamics while allowing the model to refine appearance.
- Core assumption: Motion is determined at relatively early diffusion timesteps (higher noise levels).
- Evidence: Previous work shows coarse motion is determined early in denoising trajectory; noising V^w to t* injects dynamics at this stage.

### Mechanism 2: Dual-Clock Denoising for Spatially-Varying Control
- Claim: Assigning different effective noise levels to masked vs. unmasked regions balances motion adherence against natural background dynamics.
- Core assumption: Standard pretrained diffusion models cannot natively handle region-dependent noise.
- Evidence: Novel approach that overrides masked regions with different noise levels without retraining the model.

### Mechanism 3: Image Conditioning for Appearance Preservation
- Claim: I2V models that condition on a clean first frame anchor identity and appearance throughout generation.
- Core assumption: The I2V model's conditioning mechanism can maintain first-frame fidelity across the generated sequence.
- Evidence: Unified Text-Image-to-Video and Frame Guidance papers also leverage image/video conditioning for control.

## Foundational Learning

- **SDEdit (Stochastic Differential Editing)**: Essential for understanding why noising to a timestep then denoising preserves coarse structure. Quick check: If you noise an image to t=0.3T vs t=0.7T and denoise, which preserves more of the original structure, and why?

- **Diffusion Timestep Semantics**: Critical for understanding that early steps determine global structure/motion while late steps refine details. Quick check: At what approximate timestep range does motion typically get fixed in video diffusion models, based on the paper's citations?

- **Image-to-Video Conditioning Architectures**: Understanding how I2V models ingest first-frame conditioning informs why appearance is preserved. Quick check: What happens to appearance fidelity if you apply SDEdit-style noise injection to a text-only video model instead of an I2V model?

## Architecture Onboarding

- **Component map**: Input image I, motion specification, binary mask M → Motion signal generator (forward warping) → Dual-clock denoiser (I2V backbone) → Output video x_0

- **Critical path**:
  1. Generate warped reference V^w from I using user motion (cut-and-drag or depth reprojection)
  2. Initialize sampling: x_t_weak ~ q(x_t | V^w) at timestep t_weak
  3. For each t from t_weak down to t_strong+1: denoise normally, then override masked region with V^w noised to t-1
  4. For t from t_strong to 0: standard denoising (no override)
  5. Return x_0

- **Design tradeoffs**:
  - t_weak controls background flexibility vs. motion adherence (higher = more freedom, lower = more constrained)
  - t_strong controls how long the masked region is locked to the reference (higher = less refinement, lower = more model freedom on foreground)
  - Paper uses (t_weak, t_strong) = (36, 25) for SVD, (46, 41) for CogVideoX—requires backbone-specific tuning

- **Failure signatures**:
  - Frozen background: t_strong too low (over-constrains non-masked regions)
  - Motion drift: t_weak too high (reference signal lost)
  - Object duplication: Unconstrained background with t_weak = T causes model to hallucinate copies
  - Mask-edge artifacts: Sharp mask boundaries may need soft blending
  - Poor identity preservation: Using a text-only model instead of I2V

- **First 3 experiments**:
  1. Implement single-clock baseline (t_weak = t_strong) on a simple cut-and-drag example; observe tradeoff between motion adherence and background dynamics per Fig. 3.
  2. Ablate t_weak and t_strong independently using the protocol in Appendix A; reproduce the CoTracker distance vs. dynamic degree tradeoff curve.
  3. Test plug-and-play transfer: Apply the same dual-clock logic to a different I2V backbone (e.g., substitute SVD for CogVideoX or WAN) and identify what hyperparameter retuning is needed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the dual-clock denoising scheme be made adaptive to eliminate the need for manual tuning of the $t_{weak}$ and $t_{strong}$ hyperparameters across different I2V backbones?
- **Basis**: The authors explicitly state the dual-clock scheme requires tuning of $(t_{weak}, t_{strong})$, providing fixed values for SVD and CogVideoX but no general rule.
- **Why unresolved**: No heuristic or learning-based mechanism is proposed to predict these values based on model architecture or input complexity.
- **What evidence would resolve it**: Systematic evaluation showing an automated scheduling policy maintains performance parity with hand-tuned settings across three or more distinct video diffusion backbones.

### Open Question 2
- **Question**: Can the framework be extended to accept sparse user inputs (e.g., points or scribbles) instead of relying on full binary object masks for motion specification?
- **Basis**: The conclusion notes TTM requires full object masks when specifying motion, unlike some motion-prompting methods that operate from partial markings.
- **Why unresolved**: The current dual-clock logic relies on strict binary separation between strong and weak alignment regions, difficult to define with ambiguous or sparse inputs.
- **What evidence would resolve it**: Qualitative and quantitative results demonstrating successful motion control using only point trajectories or bounding boxes.

### Open Question 3
- **Question**: Does generalizing the binary dual-clock scheme to support multiple distinct regions or soft masks with continuous noise schedules improve control for complex, multi-object scenes?
- **Basis**: Authors suggest future work could explore multiple regions, soft masks, or smoother noise schedules for more fine-grained control.
- **Why unresolved**: Current implementation limits framework to a single "strong" region and a "weak" background, restricting ability to handle scenes requiring varying degrees of motion adherence across different layers.
- **What evidence would resolve it**: Ablation studies on scenes with multiple moving objects showing that a "multi-clock" approach yields better fidelity than binning regions into just two noise levels.

## Limitations
- Method constrained to appearance available in the first frame, unsuitable for novel object generation or mid-sequence object introduction
- Hyperparameter sensitivity requires backbone-specific tuning with no clear transferability across architectures
- Mask quality and forward warping accuracy directly impact motion adherence, with no robustness to inaccurate user inputs

## Confidence
- **High confidence**: The dual-clock mechanism's effectiveness in balancing motion adherence vs. background flexibility (validated across multiple benchmarks and backbones)
- **Medium confidence**: Claims about training-free generalization—while empirically supported, dependence on I2V model quality and need for hyperparameter tuning limit true "plug-and-play" status
- **Medium confidence**: The assertion that motion is fixed at early timesteps—this aligns with prior work but the exact timestep varies by model architecture and training regime

## Next Checks
1. **Cross-backbone hyperparameter transferability test**: Apply TTM's dual-clock parameters from SVD to CogVideoX/WAN without modification and quantify performance degradation
2. **Appearance preservation robustness test**: Evaluate TTM on videos requiring mid-sequence object appearance and measure identity preservation failure modes
3. **Mask sensitivity analysis**: Systematically degrade mask quality (blurred edges, incorrect regions) and measure impact on motion adherence metrics across different noise schedules