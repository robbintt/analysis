---
ver: rpa2
title: Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting
arxiv_id: '2512.07569'
source_url: https://arxiv.org/abs/2512.07569
tags:
- learning
- contrastive
- data
- normal
- weca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Weighted Contrastive Anomaly-Aware Adaptation (WECA) addresses
  the problem of maintaining forecasting accuracy under distribution shifts caused
  by anomalies in multivariate time series, particularly ATM transaction forecasting.
  The method uses a weighted contrastive objective that aligns normal and anomaly-augmented
  representations with variable strength based on perturbation severity, preserving
  anomaly-relevant information while maintaining consistency under benign variations.
---

# Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting

## Quick Facts
- arXiv ID: 2512.07569
- Source URL: https://arxiv.org/abs/2512.07569
- Authors: Joel Ekstrand; Tor Mattsson; Zahra Taghiyarrenani; Slawomir Nowaczyk; Jens Lundström; Mikael Lindén
- Reference count: 0
- Primary result: WECA improves SMAPE on anomaly-affected data by 6.13 percentage points with only 0.03 percentage points degradation on normal data

## Executive Summary
Weighted Contrastive Anomaly-Aware Adaptation (WECA) addresses forecasting accuracy degradation under distribution shifts caused by anomalies in multivariate time series. The method uses severity-weighted contrastive learning to align normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. WECA achieves this by computing alignment weights from input-space Euclidean distance, creating a soft boundary between normal and anomalous regimes that prevents catastrophic forgetting.

## Method Summary
WECA augments multivariate time series with synthetic anomalies using a parametric formula a(n)=A·n·e^(-Bn^C), then applies severity-weighted contrastive learning to align original and augmented representations. The alignment strength w(i,t) is computed from Euclidean distance between original and augmented inputs, ranging from ~1 for mild perturbations to reduced values for severe anomalies. The method jointly optimizes a forecasting loss (MAE) and a weighted InfoNCE contrastive loss, with coefficient λ=1. This enables the encoder to learn representations that are invariant to benign noise while remaining sensitive to meaningful distribution shifts.

## Key Results
- 6.13 percentage points improvement in SMAPE on anomaly-affected data compared to normally trained baseline
- Only 0.03 percentage points degradation on normal data (vs. 2.77 points for fine-tuning)
- Severity-weighted alignment preserves anomaly-relevant information while maintaining normal-regime knowledge
- Better trade-off between anomaly performance and normal operations than fine-tuning or standard contrastive learning

## Why This Works (Mechanism)

### Mechanism 1: Severity-Weighted Contrastive Alignment
Weighting contrastive alignment strength based on perturbation severity allows the encoder to learn representations invariant to benign noise while sensitive to meaningful distribution shifts. A similarity weight w(i,t) ∈ [0,1] scales each positive pair's contribution in InfoNCE-style objective. For mild perturbations, w ≈ 1 enforces strong alignment; for severe anomalies, reduced w preserves representation differences carrying predictive signal.

### Mechanism 2: Joint Forecasting-Contrastive Optimization
Combining MAE forecasting loss with weighted contrastive loss enables the encoder to learn representations serving both predictive accuracy and distribution-shift robustness. The total loss L = L_forecast + λ·L_WECA creates gradient signals simultaneously minimizing prediction error and shaping representation space. The contrastive term acts as regularizer encouraging consistent encoding of related inputs.

### Mechanism 3: Catastrophic Forgetting Prevention via Soft Boundaries
Weighted contrastive learning preserves normal-regime knowledge better than fine-tuning because it maintains original decision boundaries for normal inputs while allowing controlled adaptation. Unlike fine-tuning (which showed +2.77 SMAPE degradation on normal data), WECA's soft alignment doesn't force the model to fully commit to anomaly patterns. Normal inputs retain their original representation structure because w ≈ 1 pairs enforce consistency.

## Foundational Learning

**Concept: InfoNCE Contrastive Learning**
- Why needed here: WECA modifies standard InfoNCE objective with instance-wise weights. Understanding positive pairs, negative sampling, and temperature effects is essential for debugging.
- Quick check question: Why does reducing temperature parameter in InfoNCE create tighter clusters but risk mode collapse?

**Concept: Distribution Shift in Time Series**
- Why needed here: Core problem is forecasting under non-stationary conditions where anomalies cause sudden shifts. Understanding covariate shift vs. concept drift informs augmentation design.
- Quick check question: In ATM forecasting, would holiday spike be covariate shift or concept drift? How should augmentation strategy differ?

**Concept: Encoder-Decoder Forecasting Architecture**
- Why needed here: WECA applies to encoder output; decoder consumes these representations for prediction. Understanding what temporal patterns get encoded where helps identify failure points.
- Quick check question: If encoder produces representations at T' < T timesteps, what temporal information is necessarily lost?

## Architecture Onboarding

**Component map:**
Input x ∈ R^(T×C) -> Encoder g_φ (TimesNet) -> z ∈ R^(T'×D) -> Decoder h_ψ -> ŷ ∈ R^(H×C)
       |                                                      |
       v                                                      v
Anomaly Augmentation                                  Forecasting Loss (MAE)
       |
       v
Augmented x̃ -> Shared Encoder g_φ -> z̃ -> Weight Calculator w(x, x̃)
                                            |
                                            -> WECA Loss (weighted InfoNCE)

**Critical path:**
1. Batch of normal windows x_i and augmented versions x̃_i pass through shared encoder
2. Euclidean distance ||x_i - x̃_i|| determines weight w(i,t)
3. Weighted InfoNCE computes contrastive loss
4. Decoder produces forecasts; MAE computed against ground truth
5. Joint gradient update with λ controlling contrastive contribution

**Design tradeoffs:**
- λ=1 (paper default): Equal weighting; may need tuning per dataset
- Weight function: Current uses input-space Euclidean distance; could incorporate domain-specific severity metrics
- Augmentation parameters (A, B, C): Paper uses fitted distributions from historical events; domain knowledge critical

**Failure signatures:**
- Normal SMAPE increases >0.5%: λ too high or augmentations too aggressive
- Anomaly SMAPE unchanged: Weight function not distinguishing severity; check w distribution
- High variance across runs: Batch composition may have insufficient negative diversity

**First 3 experiments:**
1. **Baseline replication**: Train TimesNet alone vs. TimesNet+WECA (λ=1, paper augmentation) on 70/10/20 split; confirm ~6 pp SMAPE reduction on anomaly-augmented test set.
2. **Weight function ablation**: Compare (a) Euclidean-based w, (b) fixed w=0.7 for all pairs, (c) standard contrastive (w=1). Expect (a) > (b) > (c) on anomaly data.
3. **λ sensitivity sweep**: Test λ ∈ {0.1, 0.5, 1.0, 2.0} to map the trade-off frontier between normal and anomaly performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does WECA generalize to domains beyond ATM transaction forecasting, such as energy, healthcare, or traffic time series where anomaly characteristics differ?
- Basis in paper: [explicit] The evaluation is limited to "a nationwide ATM transaction dataset covering ~1.3k locations over two years" with no experiments on other domains.
- Why unresolved: Different domains exhibit distinct seasonality patterns, anomaly types, and inter-variable relationships that may affect the weighted contrastive mechanism differently.
- What evidence would resolve it: Evaluation on benchmark datasets from multiple domains (e.g., ETT, Traffic, Electricity) showing consistent improvements in the invariance–sensitivity trade-off.

### Open Question 2
- Question: How sensitive is WECA's performance to the choice of similarity metric for computing the alignment weights w(i,t), compared to the Euclidean distance used in this work?
- Basis in paper: [explicit] "The similarity weights w(i,t) are computed from the Euclidean distance between the original and augmented samples in input space, but the formulation is general and can incorporate other similarity metrics."
- Why unresolved: Euclidean distance may not capture semantic similarity in high-dimensional time series; alternative metrics (e.g., DTW, cosine) could yield different alignment behaviors.
- What evidence would resolve it: Ablation study comparing Euclidean, DTW-based, cosine-based, and learned similarity metrics for weight computation.

### Open Question 3
- Question: Does WECA maintain its favorable trade-off when evaluated on real (non-synthetic) anomalies rather than domain-informed synthetic injection?
- Basis in paper: [inferred] All anomaly experiments use synthetic augmentation via a parametric formula; no evaluation on naturally occurring anomalies is reported.
- Why unresolved: Real-world anomalies may have more complex temporal structures, multiple overlapping effects, or different signal-to-noise ratios than the injected spikes and dropouts.
- What evidence would resolve it: Evaluation on datasets with labeled real anomalies (e.g., server logs, sensor failures) where ground-truth anomaly timing is known.

## Limitations
- Weight function mapping from Euclidean distance to w(i,t) is not explicitly specified, creating uncertainty in reproduction
- All anomaly experiments use synthetic augmentation rather than naturally occurring anomalies
- Limited evaluation to single ATM transaction forecasting domain without cross-domain validation

## Confidence

**High Confidence**: The mechanism by which severity-weighted contrastive alignment enables better trade-off between normal and anomaly performance is well-supported by the 6.13 pp SMAPE improvement on anomalies with only 0.03 pp degradation on normal data.

**Medium Confidence**: The catastrophic forgetting prevention claim is supported by comparison to fine-tuning, but the mechanism depends heavily on the undisclosed weight function details.

**Low Confidence**: The generalization of these results to non-synthetic, real-world anomalies across different time-series domains remains unverified.

## Next Checks
1. **Weight Function Sensitivity**: Systematically test different distance-to-weight mappings (linear scaling, sigmoid, thresholded) to determine which preserves the most anomaly information while maintaining normal performance.
2. **Cross-Domain Validation**: Apply WECA to a different time-series forecasting task (e.g., electricity demand or traffic flow) with naturally occurring anomalies to test domain generalization.
3. **Ablation of Contrastive Component**: Train a version with contrastive loss removed (λ=0) and compare to standard fine-tuning to quantify the exact contribution of the weighted contrastive regularization beyond standard transfer learning approaches.