---
ver: rpa2
title: 'InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large
  Foundation Model'
arxiv_id: '2502.19026'
source_url: https://arxiv.org/abs/2502.19026
tags:
- video
- quality
- assessment
- distillation
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses compressed video quality assessment (VQA)
  by leveraging the rich feature representations of large video foundation models.
  The authors propose InternVQA, a knowledge distillation framework that transfers
  the powerful perceptual capabilities of InternVideo2-1B (the teacher) to a lightweight
  student model.
---

# InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large Foundation Model

## Quick Facts
- arXiv ID: 2502.19026
- Source URL: https://arxiv.org/abs/2502.19026
- Authors: Fengbin Guan; Zihao Yu; Yiting Lu; Xin Li; Zhibo Chen
- Reference count: 10
- Primary result: Distilled student models achieve comparable or superior performance to 1B-parameter teacher while being 50× smaller

## Executive Summary
This paper addresses compressed video quality assessment (VQA) by leveraging the rich feature representations of large video foundation models. The authors propose InternVQA, a knowledge distillation framework that transfers the powerful perceptual capabilities of InternVideo2-1B (the teacher) to a lightweight student model. They explore both homologous (InternVideo2-Small/Base) and heterogeneous (FastVQA) distillation strategies. Through homologous distillation, the student model not only surpasses existing VQA methods but also achieves comparable or even superior performance to the teacher model on two benchmark datasets (BVI-HD and Waterloo IVC 4K), while significantly reducing model size and computational requirements.

## Method Summary
InternVQA uses knowledge distillation to transfer quality assessment capabilities from InternVideo2-1B (1B parameters, 40 ViT blocks) to smaller student models. The framework employs a dual-loss design combining L2 prediction losses with Smooth L1 feature alignment between teacher and student features. Two homologous strategies use InternVideo2-Small (12 blocks, 384-dim) and InternVideo2-Base (12 blocks, 768-dim), while a heterogeneous approach uses FastVQA. The teacher backbone remains frozen during training, with only the final block and quality awareness head being trainable.

## Key Results
- Homologous distillation achieves +0.071 PLCC improvement over non-distilled baseline on BVI-HD dataset
- Student models (23M parameters) match or exceed 1B-parameter teacher performance on both BVI-HD and Waterloo IVC 4K datasets
- Distilled models significantly outperform existing VQA methods while reducing model size by ~50×

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Homologous distillation transfers compression quality priors more effectively than heterogeneous distillation.
- Mechanism: When student and teacher share the same ViT architecture family, feature alignment via Smooth L1 Loss enables direct mapping of learned representations. Architectural consistency preserves the hierarchical feature extraction pathway, allowing the student to inherit semantic, texture, and temporal motion sensitivities encoded in the teacher's final ViT block.
- Core assumption: The teacher's compression quality priors are sufficiently encoded in the final layer features for transfer.
- Evidence anchors:
  - [section] "Under this homologous distillation framework, the student model successfully inherits the compression quality priors from the teacher model."
  - [table] InternVQA_dist-Small gained +0.071 PLCC over non-distilled baseline on BVI-HD; heterogeneous FastVQA_dist gained only +0.059.
  - [corpus] Weak corpus support—no directly comparable homologous vs. heterogeneous distillation studies found in neighbors.
- Break condition: If student architecture diverges significantly (e.g., convolutional vs. attention-based), feature alignment degrades due to incompatible representation spaces.

### Mechanism 2
- Claim: Large video foundation models encode transferable quality-aware features through masked video modeling pretraining.
- Mechanism: InternVideo2-1B's pretraining on diverse video data with masked video modeling produces representations sensitive to semantic content, texture degradation, and temporal discontinuities. These features, originally learned for action recognition and video-text tasks, generalize to compression artifact detection without task-specific pretraining.
- Core assumption: Features learned for video understanding tasks correlate with perceptual quality indicators.
- Evidence anchors:
  - [abstract] "InternVideo2, has demonstrated strong potential in video understanding tasks due to its large parameter size and large-scale multimodal data pertaining."
  - [section] "These rich features effectively support various downstream video tasks."
  - [corpus] Neighbor papers on video tokenizers and generative compression suggest representation quality matters, but no direct evidence for transfer to VQA.
- Break condition: If downstream task requires quality dimensions absent from pretraining data (e.g., novel codec-specific artifacts), transfer may fail.

### Mechanism 3
- Claim: Dual-loss supervision grounds distilled features in ground truth quality scores.
- Mechanism: The combined loss (L_teacher_2 + L_student_2 + L_SmoothL1) provides three gradients: teacher prediction error, student prediction error, and feature alignment. This prevents the student from mimicking teacher features that don't correlate with actual quality scores.
- Core assumption: Teacher predictions are more accurate than student predictions; ground truth is reliable.
- Evidence anchors:
  - [section] "This dual-loss design enables the student model to learn from both the ground truth and the feature space of the teacher model."
  - [table] Distilled models consistently outperform non-distilled counterparts across both datasets.
  - [corpus] No corpus papers examine this specific loss combination for VQA distillation.
- Break condition: If ground truth annotations contain systematic bias, dual-loss may reinforce errors rather than correct them.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The core method transfers representations from a 1B-parameter teacher to ~23M-parameter students.
  - Quick check question: Can you explain why matching softmax distributions differs from matching penultimate layer features?

- Concept: Vision Transformer (ViT) Architectures
  - Why needed here: InternVideo2 family uses ViT blocks; understanding token processing and attention is essential for modifying the architecture.
  - Quick check question: How does the number of ViT blocks affect receptive field and computational cost?

- Concept: Video Quality Assessment Metrics (PLCC/SRCC)
  - Why needed here: Performance is evaluated using correlation coefficients; interpreting these metrics is critical for assessing model quality.
  - Quick check question: What does a 0.75 PLCC imply about prediction accuracy vs. ranking quality?

## Architecture Onboarding

- Component map:
  Teacher backbone (InternVideo2-1B, 40 ViT blocks, 1408-dim) -> Quality Awareness Head -> Predictions
  Student backbone (InternVQA-Small/Base, 12 ViT blocks) -> Quality Awareness Head -> Predictions
  Feature alignment layer (Smooth L1) connects teacher and student features

- Critical path:
  1. Preprocess: Random crop videos to 8×224×224 patches
  2. Forward pass through teacher (frozen backbone, trainable head) and student (fully trainable)
  3. Extract features from final ViT block (apply Norm)
  4. Compute Smooth L1 between teacher/student features
  5. Compute L2 between predictions and ground truth
  6. Backpropagate combined loss to student only

- Design tradeoffs:
  - Smaller student (Small vs. Base): ~4x fewer parameters, slightly lower performance but faster inference
  - Homologous vs. heterogeneous: Homologous yields +0.012-0.083 higher PLCC but constrains architecture choice
  - Freezing teacher backbone: Reduces memory/compute but limits adaptation to VQA-specific features

- Failure signatures:
  - PLCC not improving after distillation: Check feature dimension mismatch or incorrect loss weighting
  - Student outperforms teacher: Likely overfitting to small dataset; verify train/val split
  - Heterogeneous distillation underperforming: Architectural incompatibility may require projection layers

- First 3 experiments:
  1. Reproduce baseline: Train InternVQA-Small without distillation on BVI-HD; verify PLCC ≈ 0.686
  2. Ablate loss components: Remove Smooth L1, train with only L2 losses; quantify feature alignment contribution
  3. Test generalization: Train on BVI-HD, evaluate on Waterloo IVC 4K without fine-tuning; assess cross-dataset transfer

## Open Questions the Paper Calls Out
None

## Limitations

- The analysis lacks systematic ablation studies comparing different architectural similarity levels between student and teacher models
- Performance gains are demonstrated on only two datasets (BVI-HD and Waterloo IVC 4K) with limited sample sizes
- The exact contribution of each loss component to the observed improvements remains unclear without component-wise ablation

## Confidence

**High Confidence**: The claim that distilled student models achieve comparable or superior performance to the 1B-parameter teacher while being 50× smaller is well-supported by the experimental results showing consistent improvements across both datasets.

**Medium Confidence**: The assertion that homologous distillation transfers compression quality priors more effectively than heterogeneous distillation is supported by observed performance differences, but lacks theoretical justification or systematic exploration of intermediate architectural similarity levels.

**Low Confidence**: The claim that large video foundation models encode naturally transferable quality-aware features through masked video modeling pretraining lacks direct evidence, as there's no analysis of which specific features correlate with quality perception.

## Next Checks

1. **Ablation Study of Loss Components**: Systematically remove or weight the Smooth L1 feature alignment loss, teacher prediction loss, and student prediction loss independently to quantify each component's contribution to performance gains.

2. **Cross-Codec Generalization Test**: Evaluate the distilled models on videos compressed with codecs not represented in the training data (e.g., AV1, VVC) to assess whether the learned quality priors generalize beyond HEVC and H.264.

3. **Architectural Similarity Gradient**: Design experiments with intermediate architectures between homologous and heterogeneous to identify the threshold where feature alignment becomes ineffective.