---
ver: rpa2
title: 'Reasoning as State Transition: A Representational Analysis of Reasoning Evolution
  in Large Language Models'
arxiv_id: '2602.00770'
source_url: https://arxiv.org/abs/2602.00770
tags:
- reasoning
- generation
- representation
- quality
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how reasoning ability in large language
  models (LLMs) evolves during training, moving beyond traditional analysis of generation
  outcomes to examine internal representation dynamics. Using probing techniques to
  measure representation quality and tracking these representations during reasoning
  generation, the authors find that post-training yields minimal improvement in static
  initial representation quality (less than 5% increase) despite substantial gains
  in generation accuracy.
---

# Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models

## Quick Facts
- **arXiv ID:** 2602.00770
- **Source URL:** https://arxiv.org/abs/2602.00770
- **Reference count:** 40
- **Primary result:** Post-training yields minimal (<5%) improvement in initial representation quality despite substantial gains in generation accuracy

## Executive Summary
This study investigates how reasoning ability in large language models evolves during training by examining internal representation dynamics rather than just generation outcomes. Using probing techniques to measure representation quality, the authors find that pre-trained models encode significant latent reasoning capacity, with post-training providing minimal improvements to static initial representations. They reveal that reasoning tasks involve continuous distributional shifts in representations during generation, with post-training enhancing models' capacity to drive representations toward better states. Counterfactual experiments show that semantic content of generated tokens, rather than additional computation, drives these representation transitions.

## Method Summary
The authors freeze LLM backbone weights and train a linear probe (with LoRA adapter on embeddings) to classify task labels from final-layer last-token hidden states. They measure representation quality via V-probing accuracy on held-out sets, generation accuracy via rule-based answer extraction, and alignment via correlation coefficients. For dynamics analysis, they generate CoT responses, split by newlines, create progressive probing datasets by appending incremental CoT portions (truncated to 5,120 tokens), and track probing accuracy across generation steps. They also conduct counterfactual experiments using gibberish sequences, repeated prompts, and irrelevant CoT to isolate drivers of representation transitions.

## Key Results
- Pre-trained models encode significant latent reasoning capacity, with probing accuracy often exceeding generation accuracy on >60% of tasks
- Post-training yields less than 5% improvement in static initial representation quality despite substantial gains in generation accuracy
- Reasoning involves continuous distributional shifts in representations during CoT generation, driven primarily by semantic content of generated tokens
- Final representation quality correlates more strongly with generation correctness than initial representations, suggesting active solution construction

## Why This Works (Mechanism)

### Mechanism 1: Latent State Availability from Pre-training
Pre-trained models encode significant latent reasoning capacity in their initial representations before any generation or post-training. During pre-training, models learn high-dimensional representations of inputs that implicitly contain task-relevant information, extractable via linear probes. Probing accuracy on pre-trained models far exceeds random guessing and, on over 60% of tasks, exceeds the model's own generation accuracy. Post-training (RL, distillation) yields less than 5% improvement in this static initial representation quality.

### Mechanism 2: State Transition via Semantically Meaningful Generation
Reasoning involves a continuous, distributional shift in representations during CoT generation, driven primarily by the semantics of generated tokens. As the model generates CoT tokens, the representation evolves from an initial state to a higher-quality final state. Counterfactual experiments show that gibberish sequences, repeated prompts, or irrelevant CoT do not induce improvement, while high-quality CoT from strong models can improve representations even in base models.

### Mechanism 3: Alignment Gap Between Representation and Generation
Final representation quality often exceeds generation accuracy; generation correctness correlates more strongly with final representations than initial ones. The final representation after CoT encodes the correct answer with higher fidelity than the model explicitly generates. Generation correctness correlates weakly with initial representations, confirming active solution construction during generation. Post-training does not necessarily improve the conversion efficiency from representation to generation.

## Foundational Learning

- **Concept: Linear Probing and Representation Quality**
  - **Why needed here:** Understanding how to measure information in hidden states is critical to interpreting the paper's main metric. The probe is a linear classifier trained on frozen representations to predict task labels.
  - **Quick check question:** Can a linear probe trained on a randomly initialized model's representations achieve high accuracy on a complex reasoning task? (Expected: No; see Table 1.)

- **Concept: Auto-regressive Generation and Hidden State Evolution**
  - **Why needed here:** The state transition is tracked across generation steps. One must understand how each new token conditions the next hidden state.
  - **Quick check question:** At generation step $t$, what is the input to the model backbone $f_\theta$? (Expected: The original prompt plus all previously generated tokens $y_{1:t}$.)

- **Concept: Counterfactual Analysis**
  - **Why needed here:** The paper uses carefully controlled baselines to isolate the cause of representation transitions.
  - **Quick check question:** Why is a "gibberish" sequence of the same length as the CoT used as a baseline? (Expected: To control for computation and context length while removing semantic content.)

## Architecture Onboarding

- **Component map:** Backbone $f_\theta$ -> Decoding function $g_\theta$ -> Representation $c_t$ -> Linear probe $h$
- **Critical path:**
  1. Freeze backbone weights
  2. Extract $c_t$ for all samples in probe training set
  3. Train linear probe $h$ via cross-entropy loss
  4. Evaluate probe accuracy on held-out set
  5. For dynamics: construct progressive datasets by appending portions of CoT and repeat probing
- **Design tradeoffs:**
  - Using last-token representation vs. pooling simplifies extraction but may miss information in other tokens
  - LoRA rank for V-probing (r=4) limits probe capacity to prevent memorization, but very low rank may underfit
  - Truncating context to 5,120 tokens enables GPU feasibility but may miss long-range dependencies in long CoT
- **Failure signatures:**
  - Probe accuracy on random model not near majority guessing: suggests probe has excess capacity or data leakage
  - No correlation between generation correctness and final representations on reasoning tasks: may indicate probing setup issue or task too simple
  - Gibberish baseline improves representation quality: confounds semantic vs. computational depth
- **First 3 experiments:**
  1. Replicate Table 1: Compare pre-trained vs. random model probing on a subset of tasks to validate probe reliability
  2. Replicate Figure 8 (subset): Condition a base model on high-quality CoT from a strong model and track representation quality trajectory
  3. Implement alignment analysis: Compute ROC-AUC between generation correctness and probe probability for initial vs. final representations on one task

## Open Questions the Paper Calls Out

### Open Question 1
What mechanisms cause the disconnect where post-training substantially improves generation accuracy but yields minimal gains (<5%) in initial representation quality? The authors state this as a key finding but do not investigate the training dynamics or parameter-level changes that cause initial representations to remain relatively static while generation ability improves significantly.

### Open Question 2
Why do reasoning models (e.g., R1Distill) not achieve higher alignment between final representation quality and generation accuracy compared to non-reasoning models? The paper demonstrates this empirical finding but does not explain whether this is due to training objectives, architectural constraints, or the nature of the representation-to-generation mapping.

### Open Question 3
Under what task or representation conditions does reasoning cause representation quality degradation versus improvement? The paper notes that "reasoning process can be a double-edged sword" with downward trends on simple tasks (MuseD), but the factors determining this direction remain unclear.

### Open Question 4
Can representation-level loss functions during training improve both initial and dynamic representation quality? The authors suggest "designing loss functions at the representation level to enhance model representation quality" as a future direction, but no experiments implement or evaluate representation-level objectives.

## Limitations

- Probe reliability across tasks: Linear and V-probing may not fully capture reasoning-relevant information, and the special token wrapper approach introduces additional complexity
- Task scope and generalizability: Conclusions may not extend beyond the four specific reasoning tasks examined (ZebraPuzzle, MuseD, MATH, HaluEval)
- Counterfactual control specificity: Controls may not capture all potential confounding factors, particularly regarding token diversity and sequence length variations

## Confidence

**High confidence** (supported by multiple independent analyses):
- Pre-trained models encode significant latent reasoning capacity in initial representations
- Post-training yields minimal improvement (<5%) in static initial representation quality
- Final representation quality correlates more strongly with generation correctness than initial representations

**Medium confidence** (supported by primary experiments but with potential confounders):
- Reasoning involves continuous distributional shifts in representations during CoT generation
- Semantic content of generated tokens (not computation depth) drives representation transitions
- Post-training enhances models' capacity to drive representations toward better states

**Low confidence** (requires additional validation or faces methodological limitations):
- The semantic vs. computational depth distinction is fully resolved by current counterfactuals
- The specific LoRA rank (r=4) and special token configuration are optimal for V-probing
- Results generalize beyond the four specific tasks examined

## Next Checks

1. **Probe sensitivity validation:** Systematically vary LoRA rank and special token configurations across tasks to establish robustness of V-probing measurements. Test whether probe accuracy plateaus or continues improving with increased capacity.

2. **Extended task spectrum:** Apply the representation dynamics analysis to additional reasoning and non-reasoning tasks (e.g., commonsense QA, logical deduction, creative writing) to test generalizability of the semantic-driven transition hypothesis.

3. **Computational depth isolation:** Design counterfactual experiments that control for both semantic content and computation depth independently - for example, comparing semantically rich but shorter CoT sequences against semantically poor but longer sequences to disentangle the two effects.