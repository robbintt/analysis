---
ver: rpa2
title: 'An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism,
  and Potential Mitigations'
arxiv_id: '2505.15392'
source_url: https://arxiv.org/abs/2505.15392
tags:
- anchoring
- anchor
- question
- effect
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the anchoring effect in Large Language
  Models (LLMs), revealing their susceptibility to cognitive biases through a newly
  curated dataset and refined evaluation metrics. Findings indicate that anchoring
  biases are widespread across LLMs, with reasoning models showing mild mitigation,
  and that these effects operate at a shallow semantic level within model layers.
---

# An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations

## Quick Facts
- **arXiv ID**: 2505.15392
- **Source URL**: https://arxiv.org/abs/2505.15392
- **Reference count**: 40
- **Key outcome**: Investigation reveals widespread anchoring biases in LLMs, with reasoning models showing partial mitigation and effects operating at shallow semantic levels

## Executive Summary
This empirical study systematically investigates anchoring bias in Large Language Models through a newly curated dataset and refined evaluation metrics. The research demonstrates that anchoring effects are pervasive across various LLM architectures, with reasoning models like DeepSeek-R1 and QwQ-32B-Preview showing some mitigation capabilities. Through causal tracing and statistical analysis, the study reveals that anchoring influences operate primarily at shallow semantic levels within model layers. The work also introduces Anti-DP, a mitigation strategy that achieves partial success in reducing anchoring bias, highlighting the need for cognitive-bias-aware evaluations in LLM development.

## Method Summary
The study employed a multi-pronged approach to investigate anchoring effects in LLMs. Researchers curated a new dataset specifically designed to test anchoring bias across various negotiation scenarios. They developed refined evaluation metrics to quantify anchoring effects, moving beyond simple accuracy measures to capture bias patterns. The methodology included causal tracing techniques to identify where in the model architecture anchoring effects manifest, layer-wise analysis to understand semantic depth of influence, and statistical analysis to establish significance across different model families. A novel mitigation strategy called Anti-DP was proposed and tested against baseline approaches to evaluate effectiveness in reducing anchoring bias.

## Key Results
- Anchoring bias is widespread across multiple LLM families, with statistical significance (p < 0.05) demonstrated across diverse datasets
- Reasoning models (DeepSeek-R1, QwQ-32B-Preview) show mild but consistent mitigation of anchoring effects compared to non-reasoning counterparts
- Causal tracing reveals anchoring influences operate at shallow semantic levels within transformer layers rather than deep semantic representations
- Anti-DP mitigation strategy achieves 10-15% improvement in reducing anchoring bias, though effectiveness varies by task type

## Why This Works (Mechanism)
The anchoring effect in LLMs operates through the same cognitive mechanisms that affect human decision-making, where initial information disproportionately influences subsequent judgments. In transformer models, this manifests as early-layer activations that lock onto anchor values, creating a cascade effect that persists through later layers despite potentially contradictory information. The mechanism involves attention heads that become biased toward anchor-related tokens, effectively filtering subsequent reasoning through an anchor-tinted lens. This shallow semantic processing means the model hasn't fully contextualized the anchor within the broader problem space, making it vulnerable to manipulation through carefully crafted prompts that either avoid triggering early anchor sensitivity or provide alternative contextual frameworks.

## Foundational Learning

**Transformer Architecture**: Understanding self-attention mechanisms and layer-wise processing is essential because anchoring effects manifest differently across layers, with causal tracing revealing that shallow layers show the strongest anchor sensitivity. Quick check: Verify layer-wise attention patterns using attention rollout or similar visualization tools.

**Causal Inference in Neural Networks**: Knowledge of causal tracing methods is needed to distinguish correlation from causation when identifying anchoring-sensitive components. Quick check: Validate causal effects using ablation studies that remove identified components.

**Cognitive Bias Theory**: Understanding human cognitive biases provides the theoretical framework for identifying analogous patterns in LLM behavior. Quick check: Compare LLM bias patterns against established human bias taxonomies to ensure valid analogies.

**Statistical Significance Testing**: Proper application of p-values and effect size measures is crucial for establishing that observed anchoring effects are not random fluctuations. Quick check: Perform power analysis to ensure adequate sample sizes for detecting meaningful effects.

## Architecture Onboarding

**Component Map**: Input -> Tokenizer -> Embedding Layer -> Transformer Blocks (Multi-Head Attention + Feed-Forward) -> Output Layer -> Probability Distribution

**Critical Path**: The critical path for anchoring effects flows through early transformer blocks where initial token representations are established. Multi-head attention mechanisms in layers 1-4 show the strongest anchoring sensitivity, with effects propagating through residual connections to later layers.

**Design Tradeoffs**: The study reveals a fundamental tradeoff between model capacity (which enables complex reasoning) and susceptibility to anchoring bias (which increases with model size). Reasoning models trade some raw performance for improved bias resistance, suggesting that architectural choices can prioritize debiasing over pure capability.

**Failure Signatures**: Anchoring failures manifest as consistent directional bias toward initial values, with models showing reduced variance in outputs when anchors are present. The failures are characterized by early-layer activation patterns that remain stable despite contradictory evidence in later input tokens.

**First Experiments**:
1. Layer-wise activation analysis comparing anchored vs non-anchored inputs to identify critical layers
2. Attention head ablation study focusing on heads identified as anchoring-sensitive
3. Cross-model comparison of anchoring strength across different architectural families

## Open Questions the Paper Calls Out
The paper identifies several key open questions regarding the generalizability of anchoring effects across different task domains beyond negotiation scenarios, the long-term stability of mitigation strategies as models evolve through fine-tuning, and whether alternative architectural modifications could achieve better debiasing than prompt-based approaches. The authors also question whether the shallow semantic nature of anchoring effects represents a fundamental limitation of current transformer architectures or a training artifact that could be addressed through different optimization strategies.

## Limitations
- Synthetic task generation may not fully capture real-world negotiation complexity and diverse anchoring scenarios
- Anti-DP mitigation shows only partial effectiveness (10-15% improvement), suggesting limited generalizability across diverse prompting scenarios
- Causal tracing methodology cannot definitively prove causation versus correlation in layer-specific activations

## Confidence

**High Confidence**: The existence of anchoring bias across multiple LLM families and versions is well-supported by statistical significance (p < 0.05) across multiple datasets and evaluation metrics.

**Medium Confidence**: The characterization of anchoring effects as operating at a "shallow semantic level" within model layers is supported by causal tracing evidence but requires additional validation across different model architectures.

**Low Confidence**: The claim that reasoning-based approaches represent the most promising direction for debiasing requires further investigation, as the study only examined a limited set of reasoning models.

## Next Checks

1. **Cross-domain Generalization Test**: Apply the anchoring effect evaluation framework to non-negotiation domains (medical diagnosis, legal reasoning, creative writing) to assess whether observed biases transfer across task types and whether Anti-DP maintains effectiveness in diverse contexts.

2. **Architecture Ablation Study**: Systematically remove or modify specific transformer components (attention heads, feed-forward layers) identified as anchoring-sensitive in the causal tracing analysis to determine whether targeted architectural changes can achieve better mitigation than prompt-based approaches.

3. **Temporal Stability Analysis**: Conduct longitudinal testing across multiple model versions and fine-tuning iterations to determine whether anchoring effects strengthen, weaken, or remain stable over time, and whether mitigation strategies maintain effectiveness as models evolve.