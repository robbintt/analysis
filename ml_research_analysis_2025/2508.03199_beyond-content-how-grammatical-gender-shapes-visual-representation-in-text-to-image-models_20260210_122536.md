---
ver: rpa2
title: 'Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image
  Models'
arxiv_id: '2508.03199'
source_url: https://arxiv.org/abs/2508.03199
tags:
- gender
- grammatical
- language
- bias
- feminine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how grammatical gender in different languages
  influences visual representation in text-to-image models, addressing a gap in existing
  research that focuses primarily on demographic and stereotypical biases. The authors
  introduce GRAMVIS, a cross-linguistic benchmark using gender-divergent words where
  grammatical gender contradicts stereotypical associations, across five gendered
  languages (French, Spanish, German, Italian, Russian) and two gender-neutral controls
  (English, Chinese).
---

# Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models

## Quick Facts
- arXiv ID: 2508.03199
- Source URL: https://arxiv.org/abs/2508.03199
- Reference count: 29
- Primary result: Grammatical gender in prompts increases male representation to 73% (vs 22% in English), showing language structure shapes AI-generated visuals beyond semantic content

## Executive Summary
This study investigates how grammatical gender in different languages influences visual representation in text-to-image models, addressing a gap in existing research that focuses primarily on demographic and stereotypical biases. The authors introduce GRAMVIS, a cross-linguistic benchmark using gender-divergent words where grammatical gender contradicts stereotypical associations, across five gendered languages (French, Spanish, German, Italian, Russian) and two gender-neutral controls (English, Chinese). Results show that masculine grammatical markers significantly increase male representation to 73% on average compared to 22% with gender-neutral English, while feminine markers show more variable effects. These findings demonstrate that language structure itself, independent of content, shapes AI-generated visual outputs.

## Method Summary
The GRAMVIS benchmark uses 200 gender-divergent nouns (40 per language) across five gendered languages and two gender-neutral controls. Researchers generated 28,800 images using 800 prompts across three state-of-the-art models (DALL-E 3, Ideogram v3, Flux Pro 1.1). Prompts were translated with appropriate grammatical gender markers, and outputs were classified using BLIP2, LLaVA, and Qwen-VL via majority voting. The study compares gender representation percentages between gendered-language prompts and gender-neutral baselines using two-tailed t-tests.

## Key Results
- Masculine grammatical markers increase male representation to 73% on average (vs 22% with gender-neutral English)
- Feminine markers show variable effects, increasing female representation to 38% (vs 28% in English)
- High-resource languages (French, Spanish, German) show stronger grammatical gender effects than medium-resource languages (Italian, Russian)
- Flux Pro 1.1 shows the greatest sensitivity to grammatical gender (+75.5pp masculine effects)

## Why This Works (Mechanism)

### Mechanism 1: Grammatical Gender-to-Visual Association Learning
- T2I models learn statistical associations between grammatical gender markers and visual gender representations during multilingual training.
- When models ingest paired image-text data from gendered languages, grammatical gender markers co-occur with gendered visual content, creating learned correlations that persist even when semantic content contradicts these associations.
- Core assumption: Models encode grammatical gender as a latent feature that influences image generation, not merely as surface-level text processing.
- Evidence: Masculine markers show +51 percentage points vs English, statistically significant in 100% of comparisons.

### Mechanism 2: Stereotype-Grammar Tension Resolution
- When grammatical gender contradicts stereotypical associations, models exhibit asymmetric resolution—masculine markers override stereotypes more reliably than feminine markers.
- Masculine grammatical markers show consistent, strong effects (+51pp) while feminine markers show variable effects (+3pp vs English), suggesting differential training data distributions or targeted debiasing.
- Core assumption: The asymmetry reflects differential training data distributions or targeted debiasing rather than inherent linguistic asymmetry.
- Evidence: Feminine markers demonstrate significance in only 46.7% of English comparisons versus 80.0% for Chinese.

### Mechanism 3: Language Resource Availability Amplification
- Grammatical-visual associations strengthen with training data abundance for a given language.
- High-resource languages show stronger grammatical gender effects than medium-resource languages, suggesting models develop more robust feature-to-output mappings when exposed to more examples.
- Core assumption: Resource availability determines representation density, which determines how strongly latent features influence generation.
- Evidence: High-resource languages show strong masculine influences (Spanish: +75.5pp) while medium-resource languages show more varied patterns.

## Foundational Learning

- **Grammatical Gender Systems**: Languages assign gender to nouns via articles (le/la, el/la), inflections, or both; this is structural, not semantic.
  - Why needed here: To understand why "une sentinelle" (feminine grammar) produces different outputs than "a guard" despite identical semantic content.
  - Quick check: In German, would "die Brücke" (bridge, feminine) generate different visual gender associations than "el puente" (bridge, masculine in Spanish)?

- **Text-to-Image Cross-Modal Learning**: T2I models learn joint representations where text tokens condition visual generation through attention mechanisms.
  - Why needed here: To trace how grammatical markers (text) influence visual gender representation (images) during generation.
  - Quick check: If you mask grammatical articles in input prompts, would gender representation revert to gender-neutral baselines?

- **Gender-Divergent Word Design**: Words where grammatical gender contradicts stereotypical associations create natural experiments for isolating grammatical influence.
  - Why needed here: This experimental design isolates grammatical structure from semantic/cultural content.
  - Quick check: Why is "une sentinelle" (feminine grammar, masculine stereotype) better for testing grammatical influence than "un soldat" (masculine both grammatically and stereotypically)?

## Architecture Onboarding

- **Component map**: Text encoder → Cross-attention layers → Image decoder → VLM classifiers
- **Critical path**: Prompt → Text encoder (grammatical markers encoded) → Cross-attention (gender features influence generation) → Image output → VLM classification
- **Design tradeoffs**: Using gender-divergent words enables causal inference but limits vocabulary coverage; binary gender classification excludes non-binary representation analysis.
- **Failure signatures**: Counterintuitive feminine effects suggest aggressive debiasing may overcorrect; high "neither" classification rates suggest grammatical gender creates visual ambiguity.
- **First 3 experiments**:
  1. **Ablation study**: Mask grammatical articles in prompts to test whether effects persist without explicit markers
  2. **Cross-lingual transfer test**: Translate English prompts to gendered languages using grammatically feminine nouns for stereotypically masculine concepts
  3. **Fine-grained ambiguity analysis**: Manually review "neither" classifications to determine if they represent genuinely ambiguous presentations or classifier limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the reduced grammatical gender effect in English prompts (compared to Chinese) causally attributable to prior debiasing efforts in English T2I systems?
- Basis in paper: The study observed that feminine markers showed significance in only 46.7% of English comparisons versus 80.0% for Chinese, but could not establish whether this disparity stems from targeted English debiasing or other factors.
- Why unresolved: The study cannot definitively prove this causal relationship, highlighting it as an important area for future investigation.
- What evidence would resolve it: Controlled experiments with models before/after known debiasing interventions.

### Open Question 2
- Question: How do grammatical gender effects manifest in low-resource gendered languages not included in this study?
- Basis in paper: The study only examined high-resource and medium-resource languages, finding systematically stronger effects in high-resource languages.
- Why unresolved: Resource availability correlates with effect strength, but the pattern for languages with even less training data remains unknown.
- What evidence would resolve it: Extending GRAMVIS to low-resource gendered languages (e.g., Polish, Czech, Hebrew, Arabic).

### Open Question 3
- Question: What internal mechanisms cause Flux and DALL-E 3 to process grammatical gender cues so differently?
- Basis in paper: Flux shows +75.5pp masculine effects versus DALL-E 3's smaller +41.7pp and reversed feminine effects, but the authors cannot examine inner workings of closed-source models.
- Why unresolved: All tested models are closed-source; architectural and training differences responsible for divergent gender sensitivity cannot be directly examined.
- What evidence would resolve it: Controlled experiments with open-source models using controlled training variations.

## Limitations
- Binary gender classification approach excludes non-binary representation analysis and may miss nuanced visual presentations.
- Causal mechanisms linking grammatical gender to visual outputs are inferred rather than directly measured.
- Generalizability to languages beyond the five tested remains uncertain.

## Confidence

- **High confidence**: Core claim that language structure shapes AI-generated visual outputs; observed asymmetry between masculine and feminine marker effects.
- **Medium confidence**: Whether asymmetry reflects differential training data distributions or model-specific debiasing; interpretation of counterintuitive findings like German DALL-E 3's negative feminine effect.
- **Low confidence**: Generalizability to languages beyond the five tested.

## Next Checks

1. **Prompt Ablation Study**: Remove grammatical articles while preserving semantic content to test whether effects persist without explicit markers, isolating structural from surface-level influences.

2. **Cross-Lingual Transfer Test**: Translate English prompts containing stereotypically masculine concepts into gendered languages using grammatically feminine nouns - measure if translation alone introduces gender bias.

3. **Fine-Grained Ambiguity Analysis**: Manually review all "neither" classifications to determine whether they represent genuinely ambiguous visual presentations or classifier limitations.