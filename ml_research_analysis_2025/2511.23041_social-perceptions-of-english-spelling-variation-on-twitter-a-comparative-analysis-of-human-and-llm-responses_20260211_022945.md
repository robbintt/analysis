---
ver: rpa2
title: 'Social Perceptions of English Spelling Variation on Twitter: A Comparative
  Analysis of Human and LLM Responses'
arxiv_id: '2511.23041'
source_url: https://arxiv.org/abs/2511.23041
tags:
- variation
- language
- spelling
- social
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares human and LLM perceptions of English spelling
  variation in tweets across three social attributes: formality, carefulness, and
  age. Following sociolinguistic methodology, researchers collected human ratings
  for 70 tweet pairs containing conventional and unconventional spellings across seven
  variation types.'
---

# Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses

## Quick Facts
- arXiv ID: 2511.23041
- Source URL: https://arxiv.org/abs/2511.23041
- Authors: Dong Nguyen; Laura Rosseel
- Reference count: 20
- One-line primary result: LLM social perceptions of spelling variation correlate strongly with humans, particularly for larger models, but paired prompting improves alignment and difference-score correlations reveal weaker fine-grained matching

## Executive Summary
This study investigates whether large language models (LLMs) perceive social attributes of English spelling variation on Twitter similarly to humans. Following sociolinguistic methodology, researchers compared human ratings of 70 tweet pairs (conventional vs. unconventional spellings) with LLM ratings across three social attributes: formality, carefulness, and age. The study found strong correlations between human and LLM perceptions, with larger models like GPT-5 and Claude-4.5-sonnet showing the highest alignment. However, notable differences emerged in rating distributions and specific spelling variation types. Paired prompting consistently outperformed independent prompting, and correlations were substantially lower when comparing rating differences rather than raw ratings, suggesting LLMs capture general patterns but not fine-grained magnitude of perception shifts.

## Method Summary
The study collected human ratings for 70 tweet pairs containing conventional and unconventional spellings across seven variation types. Researchers then prompted 12 LLMs to rate the same tweets using two approaches: independent (rating each tweet separately) and paired (rating tweets side-by-side). Both humans and LLMs rated tweets on formality, carefulness, and age using visual analog sliders or integer responses. The researchers computed Spearman correlations between human and LLM ratings, analyzing both raw ratings and rating differences between conventional and unconventional variants. They also examined correlations by model size, prompting strategy, spelling type, and social attribute.

## Key Results
- Larger models (GPT-5, Claude-4.5-sonnet) showed strongest correlations with human ratings (raw correlations up to 0.901 in paired setup)
- Paired prompting consistently outperformed independent prompting across all models
- Raw rating correlations overstated alignment; difference-score correlations were substantially lower (e.g., GPT-5 dropped from 0.901 to 0.524)
- Letter swap and keyboard substitution showed smallest formality drops but largest carefulness drops compared to other variation types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paired prompting improves human-LLM alignment in social perception tasks.
- Mechanism: Presenting conventional and unconventional spelling variants side-by-side enables relative comparison, reducing the need for models to maintain consistent internal anchors across independent judgments. This mirrors the human experimental setup where participants see both versions simultaneously.
- Core assumption: The improvement stems from the comparison context rather than simply doubling the prompt length or exposure.
- Evidence anchors:
  - [abstract] "Paired prompting consistently outperformed independent prompting"
  - [section 5.3] "Consistently across all models, there is a substantial increase in correlation when tweets with conventional and non-conventional spellings are presented simultaneously rather than individually (Table 4, compare 'independent' vs 'paired'). For instance, GPT-5's correlation increases from 0.784 to 0.901"
  - [corpus] No direct corpus evidence for this specific mechanism; related work on prompt design sensitivity exists (e.g., Sclar et al. 2024 mentioned in paper) but not for paired comparisons.
- Break condition: If models were simply using prompt length as a signal, we would expect uniform improvement across all model sizes; however, smaller models show less dramatic gains.

### Mechanism 2
- Claim: Larger models better capture human-like social perceptions of spelling variation.
- Mechanism: Models with more parameters and larger training corpora have been exposed to more diverse examples of spelling variation in context, enabling them to learn the statistical associations between unconventional spellings and social attributes present in web-scale training data.
- Core assumption: The correlation improvements stem from training data exposure rather than architectural differences specific to certain model families.
- Evidence anchors:
  - [abstract] "Results showed strong correlations between human and LLM ratings, particularly with larger models like GPT-5 and Claude-4.5-sonnet"
  - [section 5.3] "GPT-5 and Claude-4.5-sonnet have the highest correlations with the human ratings; the smaller models (2â€“9B parameters) have the lowest correlations"
  - [corpus] Corpus shows related work on LLM stereotypes (paper 89827) and AI authorship perception (paper 79949), but no direct mechanism evidence for model size effects on sociolinguistic perception.
- Break condition: If correlation were purely about model size, Llama3.1-450b should outperform Llama3-70b, but it does not (Table 4 shows Llama3-70b has higher correlations).

### Mechanism 3
- Claim: Raw rating correlations overstate human-LLM alignment; difference-score correlations reveal weaker fine-grained matching.
- Mechanism: A model that merely distinguishes "conventional = high rating" from "unconventional = low rating" can achieve high raw correlations without capturing the magnitude of perception shifts that vary by spelling type and content. Difference scores require the model to predict the specific effect size for each spelling variation.
- Core assumption: The paper demonstrates this with a simple baseline model, but assumes this pattern generalizes across all models tested.
- Evidence anchors:
  - [abstract] "correlations were substantially lower when comparing rating differences rather than raw ratings"
  - [section 5.3] "As an example, GPT-5's correlation with human ratings decreases from 0.901 (raw ratings) to 0.524 (rating differences) in the paired setup... Thus, a model that merely distinguishes between non-conventional and conventional spellings can obtain high correlations when comparing raw ratings."
  - [corpus] No corpus evidence directly addresses this methodological distinction.
- Break condition: If LLMs had fully internalized human perception patterns, difference-score correlations would be similar to raw correlations; the gap indicates incomplete alignment.

## Foundational Learning

- Concept: Speaker evaluation paradigm (sociolinguistics)
  - Why needed here: This is the core experimental methodology. Participants rate speakers/writers on social attributes after exposure to controlled linguistic stimuli. The paper adapts this to LLMs by presenting tweet pairs and collecting ratings.
  - Quick check question: Can you explain why the "open guise" variant (explicitly highlighting spelling variation) was chosen over "matched guise" (hidden variation)?

- Concept: Spelling variation typology (g-dropping, lengthening, number substitution, etc.)
  - Why needed here: The seven variation types represent different orthographic phenomena with distinct social meanings. Some reflect spoken variation (g-dropping), some are intentional orthographic choices (number substitution), and some are likely accidental (letter swap, keyboard substitution).
  - Quick check question: Why might letter swap and keyboard substitution show smaller formality drops but larger carefulness drops compared to other variation types?

- Concept: Correlation interpretation (raw vs. difference scores)
  - Why needed here: Understanding that Spearman correlations on raw ratings vs. rating differences measure different things is critical. The former captures rank-ordering, the latter captures effect-size alignment.
  - Quick check question: If a model rated all conventional spellings as 100 and all unconventional as 0, what would happen to raw vs. difference correlations?

## Architecture Onboarding

- Component map:
  - Tweet dataset (70 controlled pairs) -> Human data collection (prolific, 217 participants) -> LLM prompting system (independent vs. paired) -> Correlation analysis (Spearman, raw and difference scores) -> Spelling type breakdown

- Critical path:
  1. Create minimal-pair tweets differing only in one spelling variant (conventional vs. unconventional)
  2. Collect human ratings on formality, carefulness, age (ground truth)
  3. Prompt LLMs with identical stimuli and instructions (matched setup)
  4. Compute correlations between LLM and human ratings (both raw and difference scores)
  5. Analyze by model size, prompting strategy, spelling type, and social attribute

- Design tradeoffs:
  - Controlled vs. ecological validity: Open guise with highlighted variants prioritizes construct validity over naturalistic perception
  - Dataset size: 70 pairs is small by NLP standards but follows sociolinguistic methodology with many ratings per item
  - Prompt fidelity: Matching human instructions closely (no prompt tuning) vs. potentially improving LLM performance through optimization
  - Rating modality: Humans used sliders (continuous), LLMs returned integers (both subject to different biases)

- Failure signatures:
  - Low raw correlations (<0.5): Model fails to distinguish conventional from unconventional spellings socially
  - High raw but near-zero difference correlations: Model captures direction but not magnitude of effects
  - Strong integer preferences (multiples of 5, 10): Model distribution doesn't match human variability
  - Uniform treatment across spelling types: Model lacks fine-grained sociolinguistic knowledge

- First 3 experiments:
  1. **Baseline replication with your target model**: Run both independent and paired prompting setups on the 70 tweet pairs, compute raw and difference correlations against the provided human ratings. Compare against GPT-4o and Claude-4.5-sonnet benchmarks.
  2. **Rating distribution analysis**: Generate density plots of your model's ratings (like Figure 1). Check for integer clustering and separation between conventional/unconventional spellings. Report standard deviations per item and compare to human variability (17.4 formality, 18.2 carefulness, 8.1 age).
  3. **Spelling type breakdown**: Compute average rating drops per spelling type (like Table 5). Verify whether your model shows the same pattern: letter swap/keyboard substitution have smallest formality drops but largest carefulness drops. This reveals whether the model differentiates intentional vs. accidental variation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' social perceptions compare to humans for other types of spelling variation (e.g., consonant substitution, first-syllable omission) or language variation (e.g., syntactic, lexical)?
- Basis in paper: [explicit] Section 6.1: "Future work could explore other types of spelling variation, such as consonant substitution... and omission of the first syllable... Future work could also explore other types of language variation, such as syntactic... and lexical variation."
- Why unresolved: Only seven specific spelling types were tested; other common variations remain unexplored.
- What evidence would resolve it: Replicate the study with new datasets covering these variation types, comparing human and LLM ratings.

### Open Question 2
- Question: Do human-LLM correlations in social perceptions of spelling variation hold for languages less represented in LLM training data?
- Basis in paper: [explicit] Section 6.1: "future studies should consider other languages to test the generalizability of our findings; since English is well represented in training data for LLMs, agreement between LLMs and humans may be lower for other languages."
- Why unresolved: Only English was tested; languages with less training data may show different alignment.
- What evidence would resolve it: Conduct parallel studies in multiple languages, comparing correlations between human and LLM ratings.

### Open Question 3
- Question: How does the format of highlighting target words (e.g., underlining vs. XML-style tags) affect human and LLM social perception ratings?
- Basis in paper: [explicit] Section 6.2: "future research could investigate how different types of cues influence ratings."
- Why unresolved: The study used different cue formats for humans and LLMs, potentially introducing confounds.
- What evidence would resolve it: Ablation study varying cue format while keeping other factors constant, measuring rating differences.

### Open Question 4
- Question: Does the format of rating input (numeric entry vs. visual slider) explain differences in rating distributions between humans and LLMs?
- Basis in paper: [inferred] Section 6.2 suggests: "an experiment where humans also provide numeric ratings instead of using a slider could be performed" to understand the effect on rating distributions.
- Why unresolved: LLMs gave ratings in multiples of 5/10, possibly due to numeric input; humans used sliders with continuous scales.
- What evidence would resolve it: Human experiments using numeric input directly, comparing resulting distributions to LLM outputs.

## Limitations

- Controlled experimental design using minimal pairs with highlighted variants may not generalize to naturalistic tweet perception
- The 70 tweet pairs, while methodologically rigorous, represent a limited sample of spelling variation types
- Raw rating correlations overstate alignment with human perceptions; difference-score correlations reveal substantially weaker fine-grained matching

## Confidence

- **High confidence**: Paired prompting consistently improves human-LLM alignment; larger models show better correlations with human ratings; independent vs. paired prompting effects are robust across all tested models
- **Medium confidence**: The specific ranking of spelling variation types by formality/carefulness impact; the mechanism by which larger models capture sociolinguistic patterns; the generalizability of findings to other social perception tasks
- **Low confidence**: The exact magnitude of effect sizes for individual spelling types; the stability of correlations across different model families; whether findings extend beyond English spelling to other linguistic variation

## Next Checks

1. **Replication with alternative spelling sets**: Test the independent vs. paired prompting effects using a different corpus of spelling variations (e.g., non-Twitter contexts) to assess generalizability
2. **Cross-linguistic validation**: Apply the methodology to other languages with different orthographic systems to determine if the model size correlation pattern holds universally
3. **Dynamic prompting analysis**: Systematically vary temperature and sampling parameters to determine optimal settings for capturing human-like social perceptions beyond the fixed temperature=1 condition used in this study