---
ver: rpa2
title: Extracting Social Connections from Finnish Karelian Refugee Interviews Using
  LLMs
arxiv_id: '2502.13566'
source_url: https://arxiv.org/abs/2502.13566
tags:
- data
- interviews
- prompt
- finnish
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for extracting
  social organizations and hobbies from Finnish-language refugee interviews. The task
  involves identifying relevant entities and associating them with individuals.
---

# Extracting Social Connections from Finnish Karelian Refugee Interviews Using LLMs

## Quick Facts
- **arXiv ID**: 2502.13566
- **Source URL**: https://arxiv.org/abs/2502.13566
- **Reference count**: 28
- **Primary result**: GPT-4 achieves 88.8% F-score on Finnish entity extraction, matching human performance

## Executive Summary
This study demonstrates that large language models can effectively extract social organizations and hobbies from Finnish-language refugee interviews, achieving human-level performance in a zero-shot setting. Using structured prompts, GPT-4 achieved an F-score of 88.8% on entity association tasks, while open models like Llama-3-70B-Instruct reached 87.7%. The research also shows that LLM-generated training data can effectively distill knowledge into smaller, computationally efficient models like FinBERT, which achieved 86.3% F-score after fine-tuning on 30,000 examples. These findings validate the use of both commercial and open LLMs for non-English information extraction tasks, with lightweight models offering a scalable alternative for processing large datasets.

## Method Summary
The method employs a two-path approach: (1) Zero-shot extraction using structured prompts with GPT-4 or Llama-3-70B-Instruct, followed by regex parsing of outputs, and (2) Supervised learning where GPT-4 generates training data that is aligned to text spans using fuzzy matching (Levenshtein distance), then used to fine-tune FinBERT. The task involves extracting four entity categories (P-HOB, P-ORG, S-HOB, S-ORG) from 89,339 OCR'd Finnish interviews, with evaluation on 400 doubly-annotated samples using micro F-score with fuzzy matching (threshold 0.75).

## Key Results
- GPT-4 achieved 88.8% F-score matching human performance on zero-shot entity extraction
- Llama-3-70B-Instruct reached 87.7% F-score, only 1.1% below GPT-4
- Supervised approach with GPT-4-generated training achieved 86.3% F-score after fine-tuning FinBERT on 30,000 examples
- Prompt language significantly impacts performance: +3pp for GPT-4 with Finnish prompts, but degradation for open models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large LLMs can resolve context-dependent entity attribution in zero-shot settings for non-English text using structural cues.
- **Mechanism**: The model uses cross-attention to identify entity boundaries and infers ownership based on interview structure (husband first) rather than gendered pronouns.
- **Core assumption**: Pre-training data included sufficient Finnish context to understand implicit family roles.
- **Evidence anchors**: GPT-4's 88.8% F-score matching human performance; flawless performance except for one complex interview.
- **Break condition**: Fails with significant text structure deviation or overcrowded context windows.

### Mechanism 2
- **Claim**: High-performing generative models can serve as effective annotators to distill knowledge into smaller encoder models.
- **Mechanism**: LLM generates entities aligned to text spans via fuzzy matching, creating training data for lightweight models like FinBERT.
- **Core assumption**: 60% similarity threshold successfully maps entities without excessive noise.
- **Evidence anchors**: 86.3% F-score achieved after fine-tuning on 30,000 GPT-4-generated examples.
- **Break condition**: If LLM generation cost exceeds budget or fuzzy alignment fails on OCR-degraded text.

### Mechanism 3
- **Claim**: Prompt language alignment significantly impacts performance non-linearly with model size and origin.
- **Mechanism**: Commercial models benefit from native-language prompts (+3pp), while open models relying on English-centric instruction tuning degrade with non-English prompts.
- **Core assumption**: Performance drop is due to instruction-following tied to English syntax rather than lack of Finnish knowledge.
- **Evidence anchors**: +3pp improvement for GPT-4 with Finnish prompt; open model degradation with Finnish prompts.
- **Break condition**: If Finnish terminology exceeds model vocabulary causing tokenization fragmentation.

## Foundational Learning

- **Concept: Named Entity Recognition (NER) with IOB Coding**
  - **Why needed here**: Required to convert unstructured LLM output into token-level training format for FinBERT.
  - **Quick check question**: Can you explain how IOB tags differentiate between continuation of a multi-word organization name and start of a new entity?

- **Concept: Fuzzy String Matching (Levenshtein Distance)**
  - **Why needed here**: Critical for evaluation pipeline to handle OCR errors and inflection differences (threshold 0.75).
  - **Quick check question**: Why would strict "exact match" unfairly penalize entity extraction from OCR'd historical text?

- **Concept: Zero-Shot Prompting**
  - **Why needed here**: Primary extraction method relying solely on natural language instructions without gradient updates.
  - **Quick check question**: What are specific failure modes of zero-shot prompting regarding output formatting (e.g., suggesting Python scripts)?

## Architecture Onboarding

- **Component map**: Raw OCR Text + Metadata -> Prompt Template (Finnish/English) -> LLM API (GPT-4/Llama-3) -> Regex Parser -> Structured Entities OR LLM Output -> Fuzzy Alignment Algorithm -> FinBERT Token Tagger -> Structured Entities -> Gold Standard <-> Prediction (Fuzzy Matcher) -> F-Score

- **Critical path**: Prompt engineering phase is highest leverage point; incorrect instructions cascade into failed parsing or noisy training data.

- **Design tradeoffs**:
  - GPT-4 vs. Llama-3-70B: Cost/convenience vs. data privacy/local control (~1.1% F-score difference)
  - Batching vs. Single-Interview: Throughput vs. accuracy (batching dropped GPT-4 by >5pp)
  - LLM vs. FinBERT: Accuracy (88.8% vs 86.3%) vs. massive cost reduction for inference

- **Failure signatures**:
  - Format Drift: Model generating conversational filler or Python code instead of required format
  - Language Confusion: Open models failing to follow Finnish instructions
  - Person Misattribution: Assigning hobbies to wrong spouse in complex interviews

- **First 3 experiments**:
  1. Prompt Language A/B Test: Run n=50 interviews through target model with both English and Finnish prompts
  2. Alignment Stress Test: Test Levenshtein alignment algorithm on LLM-generated entities vs. raw text
  3. Format Robustness Check: Implement Regex parser and run against dev set to identify spontaneous format switches

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can extracted entities be effectively normalized and grounded to resolve naming variations into unique organizations?
- **Basis in paper**: Future work involves "normalization and grounding of extracted entities, for instance identifying that 'Marttakerho' and 'Martat' refer essentially to the same organization."
- **Why unresolved**: Current study extracts raw text spans but doesn't link variant strings to canonical identifiers.
- **What evidence would resolve it**: Mapping system grouping text variants into distinct entity clusters with high accuracy.

### Open Question 2
- **Question**: What are most effective methods for categorizing extracted hobbies and organizations into general semantic types?
- **Basis in paper**: Plan to "categorize entities into smaller number of more general categories... enhancing usability for downstream analysis."
- **Why unresolved**: Current output consists of specific entity names without high-level semantic labels.
- **What evidence would resolve it**: Classified dataset where extracted entities are reliably sorted into predefined general categories.

### Open Question 3
- **Question**: Which specific aspects of social activity most significantly impact integration, reproduction, and lifespan of refugee populations?
- **Basis in paper**: Aim to use extracted data to "identify aspects of sociality that have greatest impact on migrants' lives, highly relevant in today's world."
- **Why unresolved**: Information extraction pipeline established but interdisciplinary analysis connecting social ties to life outcomes not yet conducted.
- **What evidence would resolve it**: Statistical correlations combining extracted social data with demographic registers.

## Limitations

- **Data Accessibility**: Evaluation relies on proprietary corpus of 89,339 OCR'd interviews; 400 annotated samples not publicly available, making replication difficult.
- **Model-Specific Behaviors**: Significant performance differences between GPT-4 and open models when prompted in Finnish may represent language-specific quirks rather than genuine capabilities.
- **Evaluation Methodology**: Fuzzy matching with 0.75 threshold introduces subjectivity; paper doesn't explore how different thresholds affect reported F-scores.

## Confidence

**High Confidence**: GPT-4 achieving human-level performance (88.8% F-score) on zero-shot Finnish entity extraction, supported by direct human comparison and consistent validation checks.

**Medium Confidence**: Effectiveness of LLM-as-annotator approach for distilling knowledge into smaller models (86.3% F-score), though performance drop from 88.8% and lack of detailed training hyperparameters introduce uncertainty.

**Low Confidence**: Generalizability of prompt language effects across different open models, as paper only shows clear differences between GPT-4 and Llama-3-70B-Instruct without testing other models or exploring underlying causes.

## Next Checks

1. **Prompt Language Transferability Test**: Select 3-4 open models of varying sizes and run parallel extraction experiments using both English and Finnish prompts to determine if language-dependent performance degradation is universal or model-specific.

2. **Threshold Sensitivity Analysis**: Systematically vary fuzzy matching threshold (0.6, 0.75, 0.85, 0.95) and re-evaluate F-scores for both LLM outputs and FinBERT predictions to assess sensitivity to this hyperparameter.

3. **Complex Interview Stress Test**: Manually curate 20-30 interviews with highest entity density and complex family structures, run both GPT-4 and Llama-3-70B-Instruct, and perform detailed error analysis focusing on person misattribution errors.