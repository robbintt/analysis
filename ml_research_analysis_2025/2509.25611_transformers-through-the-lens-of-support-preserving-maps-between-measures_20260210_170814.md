---
ver: rpa2
title: Transformers through the lens of support-preserving maps between measures
arxiv_id: '2509.25611'
source_url: https://arxiv.org/abs/2509.25611
tags:
- measures
- continuous
- such
- which
- derivative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper characterizes which mappings between measures can be
  universally approximated by measure-theoretic transformers. The key insight is that
  such maps must be support-preserving and satisfy a smoothness condition related
  to uniform continuity of their regular part of the derivative.
---

# Transformers through the lens of support-preserving maps between measures

## Quick Facts
- **arXiv ID**: 2509.25611
- **Source URL**: https://arxiv.org/abs/2509.25611
- **Reference count**: 40
- **Key outcome**: Characterizes which mappings between measures can be universally approximated by measure-theoretic transformers through support-preserving structure and uniform continuity of the regular part of the derivative.

## Executive Summary
This paper establishes a complete characterization of measure-theoretic transformers' expressive power through the lens of support-preserving maps between measures. The authors prove that transformers can approximate any map that preserves the support structure of measures while satisfying a smoothness condition related to uniform continuity of their regular part of the Fréchet derivative. They demonstrate this with a counterexample showing that continuity alone is insufficient, and apply their framework to establish that the solution map of the Vlasov equation for interacting particle systems satisfies these conditions. This bridges the mean-field regime of particle systems with measure-theoretic transformers and suggests implications for understanding large language models through this mathematical framework.

## Method Summary
The paper characterizes measure-theoretic transformers operating on probability measures through a push-forward construction f(μ) = G(μ)#μ, where G is an in-context map. The key theoretical results establish necessary and sufficient conditions for universal approximation: maps must be support-preserving (preserving weight structure when input points coincide) and have a regular part of their Fréchet derivative that exists and is uniformly continuous. The authors prove that infinitely deep transformers in the mean-field limit satisfy the Vlasov equation, connecting transformer dynamics to interacting particle systems. While the characterization is complete, the paper acknowledges a significant limitation: it provides no quantitative bounds on the depth or width required for achieving a given approximation error.

## Key Results
- Transformers can approximate any support-preserving map with uniformly continuous regular derivative arbitrarily well under 1-Wasserstein distance
- Continuity alone is insufficient: a counterexample demonstrates a continuous, support-preserving map that cannot be represented by any continuous in-context map G
- The solution map of the Vlasov equation satisfies the required conditions, establishing a connection between infinitely deep transformers and interacting particle systems
- The framework provides a mathematical foundation for understanding transformer expressivity in the infinite-context limit

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Support-preserving structure is both necessary and sufficient (with appropriate smoothness) for measure-theoretic transformer representability.
- **Mechanism:** A map f: M+(Ω) → M+(R^d') preserves support when for any discrete measure μ = Σaᵢδₓᵢ, the output f(μ) = Σaᵢδyᵢ maintains the same weight structure. This constraint arises from the push-forward operation f(μ) = G(μ)#μ, where G is the in-context map.
- **Core assumption:** The transformer operates via push-forward of measures through an in-context map G.
- **Evidence anchors:**
  - [abstract] "fully characterize the properties of maps between measures that enable these to be represented in terms of in-context maps via a push forward"
  - [section 2.1, Definition 1] Formal definition of support-preserving maps
  - [corpus] Related work on in-context learning mechanisms exists, but corpus lacks direct measure-theoretic treatment
- **Break condition:** If the target map merges or splits support points (e.g., y₁ ≠ y₂ despite x₁ = x₂ with different weights), it cannot be represented as a measure-theoretic transformer.

### Mechanism 2
- **Claim:** Uniform continuity of the regular part of the Fréchet derivative, not mere continuity, distinguishes transformer-representable maps.
- **Mechanism:** The regular derivative Df(μ, x, ψ) = lim_{k→∞} lim_{ε→0⁺} ⟨ψ, f(μₖ + εδₓ) - f(μₖ)⟩/ε captures how the map responds to point perturbations. Uniform continuity ensures stable interpolation between discrete token sequences and continuous measures.
- **Core assumption:** The limit Df exists and is independent of the approximating sequences μₖ and ψₖ.
- **Evidence anchors:**
  - [abstract] "satisfy a smoothness condition related to uniform continuity of their regular part of the derivative"
  - [section 3, Proposition 1] Explicit counterexample: continuous, support-preserving map f: P([−3,3]) → P([−3,3]) that cannot use continuous in-context G
  - [corpus] No corpus papers address this specific derivative condition
- **Break condition:** If Df(μ, x, ψ) oscillates or fails to converge uniformly (as in the Proposition 1 counterexample using Rₐ functions with frequency 1/ε), the map cannot be approximated by transformers.

### Mechanism 3
- **Claim:** Infinitely deep measure-theoretic transformers, in the mean-field limit, satisfy the Vlasov equation—a nonlocal transport PDE describing interacting particle systems.
- **Mechanism:** In the continuum limit, layer index τ scales to time t ∈ [0,1], and token evolution follows ∂ₜμₜ + div(Vₜ(μₜ)μₜ) = 0. The in-context map G^∞_{tran;t} evolves as ∂ₜG = Vₜ(μₜ)(G), analogous to particle trajectories in Vlasov dynamics.
- **Core assumption:** Assumption: The vector field V derived from attention + MLP compositions satisfies Piccoli-Rossi-Trélat well-posedness conditions (Lipschitz continuity, bounded growth).
- **Evidence anchors:**
  - [section 4.1, equation 10] Derivation of Vlasov equation from token dynamics
  - [section 4.2, Proposition 2] Proves solution map f_T satisfies conditions (B1) and (B2)
  - [corpus] Limited support; "From Memories to Maps" discusses episodic memory but not PDE connections
- **Break condition:** If the attention-derived velocity field V lacks the required regularity (e.g., develops singularities in finite time), the Vlasov correspondence breaks.

## Foundational Learning

- **Concept: Wasserstein Distance and Metric Structure on Measures**
  - Why needed here: The paper relies on W₁ topology to define continuity of maps between measures and to measure approximation quality.
  - Quick check question: Given two discrete measures μ₁ = (1/n)Σδₓᵢ and μ₂ = (1/n)Σδyᵢ, compute the W₁ distance via the optimal permutation.

- **Concept: Push-Forward of Measures**
  - Why needed here: The core theorem characterizes when f(μ) = G(μ)#μ holds; understanding how continuous maps transport measures is essential.
  - Quick check question: If μ = (1/2)δ₀ + (1/2)δ₁ and G(x) = x², what is G#μ?

- **Concept: Fréchet Derivatives on Infinite-Dimensional Spaces**
  - Why needed here: The "regular part of the derivative" is a novel extension of Fréchet differentiation tailored to measure-valued maps.
  - Quick check question: Why does the classical Fréchet derivative fail for support-preserving maps, necessitating the regular/irregular decomposition?

## Architecture Onboarding

- **Component map:** Input: probability measure μ ∈ P(Ω) → Multi-head self-attention Γ(μ, x) = x + Att(μ, x) → Optional MLP Fη → Composition f_{tran} = (G_{tran})#μ → Output: transformed measure.

- **Critical path:**
  1. Verify the target map f is support-preserving (check Definition 1 against your data).
  2. Estimate the regular derivative Df numerically using discrete approximations.
  3. Confirm uniform continuity by testing perturbation stability across measure space.
  4. If both conditions hold, Theorem 1 guarantees a transformer exists; construct via Corollary 1 using deep compositions of attention and MLP layers.

- **Design tradeoffs:**
  - Depth vs. expressivity: Approximation error ε requires sufficiently deep transformers (Corollary 1), but no explicit depth bound is provided—non-quantitative limitation acknowledged by authors.
  - Discrete vs. continuous: Classical transformers operate on token sequences; measure-theoretic formulation is necessary for infinite-context limiting behavior.

- **Failure signatures:**
  - Discontinuous in-context map G: Proposition 1 shows this occurs when the regular derivative condition is violated; manifests as instability under long prompts (see Appendix F toy example with "Alice" vs. "Elise").
  - Non-Lipschitz velocity fields: In the Vlasov correspondence, lack of regularity in V prevents well-posed solution maps.

- **First 3 experiments:**
  1. **Sanity check on discrete measures:** For a known support-preserving map (e.g., translation f(μ)(A) = μ(A - v)), verify that a small transformer recovers it and that W₁(f_{tran}(μ), f(μ)) decreases with depth.
  2. **Regular derivative estimation:** Numerically approximate Df(μ, x, ψ) for a candidate map using ε-perturbations; test uniform continuity by sampling μ across P(Ω) and checking if |Df(μ₁, x₁, ψ₁) - Df(μ₂, x₂, ψ₂)| ≤ C·D_X((μ₁, x₁, ψ₁), (μ₂, x₂, ψ₂)).
  3. **Vlasov flow validation:** Implement an infinitely deep (or very deep) transformer with attention-only layers; compare token trajectories xᵢ(t) against numerical solutions of ∂ₜμₜ + div(Vₜ(μₜ)μₜ) = 0 for a known Vlasov system (e.g., Cucker-Smale model cited in Appendix B.6).

## Open Questions the Paper Calls Out

- **Question:** Can measure-theoretic transformers approximate the BBGKY hierarchy for interacting particle systems, and if so, do the solution maps satisfy conditions (B1) and (B2)?
  - **Basis in paper:** [explicit] "Beyond the Vlasov equation, it will be interesting to study the BBGKY hierarchy describing the dynamics of a system of a large number of interacting particles (see, for example, [18]) with measure-theoretic transformers."
  - **Why unresolved:** The paper only establishes the connection for the Vlasov equation; BBGKY hierarchy involves higher-order correlation functions and more complex structure.
  - **What evidence would resolve it:** A proof that BBGKY solution maps satisfy support-preservation and the uniform continuity condition on the regular part of their derivative.

- **Question:** What quantitative approximation bounds can be established for the universal approximation result in Corollary 1?
  - **Basis in paper:** [explicit] "A limitation of our method is that it is not quantitative."
  - **Why unresolved:** The proof is constructive but relies on compactness arguments and density results without explicit convergence rates.
  - **What evidence would resolve it:** Derivation of explicit bounds on the depth and width of transformers needed to achieve ε-approximation error in Wasserstein distance.

- **Question:** Are there intermediate smoothness conditions between mere continuity and the uniform continuity condition (B2) that still guarantee representability by transformers?
  - **Basis in paper:** [inferred] The counterexample (Proposition 1) shows continuity alone is insufficient, while Theorem 1 shows (B1)-(B2) are sufficient, but no characterization of the gap between these conditions is provided.
  - **Why unresolved:** The paper provides necessary and sufficient conditions but does not explore whether weaker forms of uniform continuity might suffice.
  - **What evidence would resolve it:** Either a counterexample showing some form of weakened (B2) is still insufficient, or a proof that a broader class of maps can be represented.

## Limitations
- **Non-quantitative approximation bounds:** The paper establishes conditions for universal approximation but provides no explicit depth or width requirements for achieving a given approximation error ε.
- **Continuity counterexample subtlety:** The Proposition 1 counterexample relies on pathological behavior of Rₐ functions with frequency 1/ε as ε→0, raising questions about practical relevance.
- **Assumed well-posedness conditions:** The Vlasov equation connection assumes Piccoli-Rossi-Trélat well-posedness conditions that may not hold for finite-depth networks with practical attention mechanisms.

## Confidence
- **High confidence:** The support-preserving property (B1) is both necessary and sufficient for measure-theoretic transformer representability, assuming an appropriate in-context map G exists.
- **Medium confidence:** The regular derivative condition (B2) is necessary but the sufficiency argument relies on abstract universal approximation theory without constructive bounds.
- **Medium confidence:** The Vlasov equation connection provides an elegant theoretical framework but depends on strong regularity assumptions.

## Next Checks
1. **Empirical verification of counterexample:** Implement the specific continuous, support-preserving map from Proposition 1 and verify that no continuous in-context map G can represent it. Test this by attempting to train a transformer to approximate the map and observing the emergence of discontinuities in G as the regularization parameter varies.

2. **Regular derivative estimation pipeline:** Develop a numerical procedure to estimate Df(μ, x, ψ) for candidate maps using ε-perturbations of discrete measures. Validate the pipeline by testing it on known maps (e.g., translations, scalings) where the regular derivative can be computed analytically, then apply to real-world transformer outputs.

3. **Vlasov vs. transformer trajectories:** Implement a measure-theoretic transformer with attention-only layers and compare the evolution of token trajectories against numerical solutions of the Vlasov equation ∂ₜμₜ + div(Vₜ(μₜ)μₜ) = 0 for a known interacting particle system (e.g., Cucker-Smale model). Quantify the discrepancy as a function of depth to understand the finite-depth deviation from the continuum limit.