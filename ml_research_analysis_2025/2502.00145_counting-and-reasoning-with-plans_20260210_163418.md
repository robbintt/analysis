---
ver: rpa2
title: Counting and Reasoning with Plans
arxiv_id: '2502.00145'
source_url: https://arxiv.org/abs/2502.00145
tags:
- planning
- plans
- reasoning
- plan
- counting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first study on quantitative and qualitative
  reasoning in classical planning. The authors define a taxonomy of counting and reasoning
  problems for polynomially bounded plans and analyze their computational complexity,
  showing that probabilistic reasoning is CP=-complete while facet reasoning is NP-complete.
---

# Counting and Reasoning with Plans

## Quick Facts
- arXiv ID: 2502.00145
- Source URL: https://arxiv.org/abs/2502.00145
- Authors: David Speck; Markus Hecher; Daniel Gnad; Johannes K. Fichte; Augusto B. Corrêa
- Reference count: 30
- Introduces first study on quantitative and qualitative reasoning in classical planning with practical tool Planalyst

## Executive Summary
This paper introduces the first systematic study of quantitative and qualitative reasoning in classical planning. The authors define a taxonomy of counting and reasoning problems for polynomially bounded plans and analyze their computational complexity, showing that probabilistic reasoning is CP=-complete while facet reasoning is NP-complete. They propose Planalyst, a practical framework that transforms planning tasks into propositional formulas and uses knowledge compilation to count plans and answer reasoning queries. The tool demonstrates superior performance over state-of-the-art planners when the plan space is large, enabling reasoning over trillions of plans.

## Method Summary
The approach transforms a planning task into a propositional formula where satisfying assignments correspond one-to-one to plans. The system constructs a formula using state variables for each step and operator variables, enforcing constraints for initial state, goal state, and transition validity. It compiles this formula into deterministic Decomposable Negation Normal Form (d-DNNF), which enables polynomial-time counting and probability queries. The tool supports various reasoning modes including conditional probability, faceted reasoning, and uniform plan sampling through traversal of the compiled structure.

## Key Results
- Planalyst outperforms top-quality planners when plan space is large, handling trillions of plans
- Probabilistic reasoning is CP=-complete while facet reasoning is NP-complete, enabling efficient qualitative analysis
- The tool successfully counts and reasons over plans in domains where enumeration-based planners fail

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Planning tasks with polynomial length bounds can be reduced to SAT instances where valid models map 1-to-1 with executable plans.
- **Mechanism:** The system constructs a propositional formula $F_{plan}^{\leq \ell}$ using state variables $a_i$ (state at step $i$) and operator variables $o_i$ (action at step $i$). It enforces constraints for the initial state, goal state, and transition validity (preconditions/effects). It adds a "tail" constraint to ensure unused steps are empty, strictly preserving the bijection between models and plans.
- **Core assumption:** The plan length $\ell$ is polynomially bounded; otherwise, the encoding size explodes exponentially.
- **Evidence anchors:**
  - [abstract]: "...transform a planning task into a propositional formula... satisfying assignments correspond one-to-one to plans."
  - [section 2]: "We can construct a formula $F_{plan}^{\leq \ell}[\Pi]$ whose models are in one-to-one correspondence... Variable $a_i$ indicates the value of state variable $a$ at the $i$-th step."
  - [corpus]: Weak relevance. Neighbors focus on LLM reasoning, not SAT compilation.

### Mechanism 2
- **Claim:** Compiling the SAT representation into a deterministic Decomposable Negation Normal Form (d-DNNF) enables polynomial-time counting and probability queries.
- **Mechanism:** The SAT formula is compiled into a d-DNNF graph. This structure enforces "determinism" (disjoint children) and "decomposability" (independent children). This allows the system to compute the total model count by traversing the graph: summing counts at OR nodes and multiplying counts at AND nodes, rather than enumerating solutions.
- **Core assumption:** The computational cost of compiling to d-DNNF is amortized by the number of subsequent queries or the infeasibility of enumerating trillions of plans.
- **Evidence anchors:**
  - [section 6]: "...compiles (potentially large) formulas into a specialized normal form called d-DNNF... enabling fast reasoning."
  - [appendix A.1]: "Determinism and smoothness permit traversal operators on sd-DNNFs to count models of F in linear time..."
  - [corpus]: Not applicable; corpus does not cover knowledge compilation.

### Mechanism 3
- **Claim:** Facet reasoning provides a computationally cheaper (NP-complete) alternative to exact probability reasoning ($C_P_=$-complete) for identifying significant operators.
- **Mechanism:** Instead of calculating the exact probability $P_\ell[\Pi, o]$, the system identifies "facets"—operators present in *some* plans (brave) but not *all* (cautious). This classification is solved via SAT queries on the compiled structure, avoiding the harder counting requirement of exact probabilistic reasoning.
- **Core assumption:** The user needs qualitative guidance (e.g., "is this action flexible?") rather than precise statistical distribution metrics.
- **Evidence anchors:**
  - [section 4]: "...faceted reasoning... is NP-complete, and thus probably much simpler than counting the number of plans."
  - [section 1]: "We show that facet reasoning in planning is NP-complete... allows more efficient answers to complex reasoning queries."
  - [corpus]: Weak relevance. Neighbors mention planning but focus on LLM generation, not structural complexity analysis.

## Foundational Learning

- **Concept:** Knowledge Compilation (specifically d-DNNF)
  - **Why needed here:** This is the core engine allowing Planalyst to handle trillions of plans. Without understanding how compiled circuits factorize the solution space, the counting mechanism appears magical.
  - **Quick check question:** Can you explain why decomposability in a d-DNNF allows us to multiply model counts of sub-nodes to get the total count?

- **Concept:** Polynomial Hierarchy & Complexity Classes (#P vs NP vs CP)
  - **Why needed here:** The paper justifies the "Facet" mechanism by complexity class reductions. Understanding why counting (#P) is harder than decision (NP) explains why the tool uses approximations like facets.
  - **Quick check question:** Why is Poly-Probabilistic-Reason classified as $C_P_=$-complete, and why is that considered harder than the NP-complete Facet Reasoning?

- **Concept:** SAT Encoding of Planning (Planning-as-SAT)
  - **Why needed here:** This bridges the gap between the planning domain (states/operators) and the reasoning tools (SAT solvers/compilers). The "sequential encoding" details define what the system can actually represent.
  - **Quick check question:** How does the encoding enforce that state transitions follow operator preconditions and effects?

## Architecture Onboarding

- **Component map:** Madagascar -> SAT Formula Generator -> d4 Compiler -> d-DNNF Circuit -> ddnnife Query Engine
- **Critical path:** The **Compilation step (d4)**. This is the resource-intensive phase. If the SAT instance is too large or lacks structure, the pipeline stalls here.
- **Design tradeoffs:**
  - **Sequential vs. Parallel Encoding:** The paper uses sequential encoding (1 action per step) to ensure a 1-to-1 map with plans. This is larger but required for valid counting; parallel encodings might be smaller but lose the counting correspondence.
  - **Exact vs. Facet:** Using Facets (NP) is faster than Probabilistic Reasoning (CP=), but provides less granular insight.
- **Failure signatures:**
  - **Grounding Explosion:** If Madagascar cannot ground the task (e.g., "organic-synthesis" domain mentioned in text), the pipeline fails before SAT generation.
  - **Memory Overflow:** During d-DNNF compilation, memory usage spikes. If it exceeds 6 GiB (experiment limit), the task fails.
- **First 3 experiments:**
  1. **Scale Test (Counting):** Run Planalyst on the *blocks* domain with increasing plan length bounds. Verify that it counts $>10^{15}$ plans while K* or SymK fail/time out due to enumeration.
  2. **Validity Check (Enumeration):** Run the *Enum* configuration on a small instance. Sample 5 random plans from the output and simulate them to ensure they reach the goal (validates the SAT encoding integrity).
  3. **Facet Analysis:** Identify a "facet" operator in a task. Check manually: does it appear in at least one valid plan? Is there a valid plan that excludes it? (Validates the brave/cautious reasoning implementation).

## Open Questions the Paper Calls Out
None

## Limitations
- The d-DNNF compilation step can become computationally prohibitive for planning tasks with complex state-variable interactions or large numbers of parallelizable actions
- Results depend critically on the polynomial length bound assumption; if this bound grows too large, both SAT encoding and compilation may become intractable
- Corpus evidence is notably weak, with neighboring papers focusing on LLM-based reasoning rather than the knowledge compilation and complexity-theoretic foundations central to this work

## Confidence
- **High Confidence:** The SAT encoding correctly maps plans to models (Mechanism 1) - supported by explicit formula construction and clear 1-to-1 correspondence claims
- **Medium Confidence:** d-DNNF compilation enables polynomial-time counting (Mechanism 2) - theoretical justification is sound but empirical validation depends on compilation success
- **Medium Confidence:** Facet reasoning provides computationally efficient qualitative insights (Mechanism 3) - complexity classification is established but practical utility depends on user requirements

## Next Checks
1. **Compilation Stress Test:** Systematically vary problem size and structural complexity in domains like "organic-synthesis" to identify exact breaking points for the d-DNNF compilation step
2. **Facet Accuracy Validation:** For domains where exact probabilistic reasoning is tractable, compare facet-based recommendations against ground-truth probability distributions to quantify approximation error
3. **Memory Usage Profiling:** Instrument the pipeline to capture memory consumption at each stage (grounding, SAT generation, compilation) to identify bottlenecks and optimize resource allocation