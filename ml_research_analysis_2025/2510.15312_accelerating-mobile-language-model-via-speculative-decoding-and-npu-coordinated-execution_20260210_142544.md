---
ver: rpa2
title: Accelerating Mobile Language Model via Speculative Decoding and NPU-Coordinated
  Execution
arxiv_id: '2510.15312'
source_url: https://arxiv.org/abs/2510.15312
tags:
- decoding
- prefill
- graph
- mobile
- draft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling efficient on-device
  Retrieval-Augmented Generation (RAG) on mobile NPUs, which struggle with the dynamic
  workloads of RAG due to static graph constraints and underutilization during decoding.
  The authors propose sd.npu, a holistic acceleration framework that combines progressive
  graph scheduling and NPU-optimized speculative decoding.
---

# Accelerating Mobile Language Model via Speculative Decoding and NPU-Coordinated Execution

## Quick Facts
- **arXiv ID:** 2510.15312
- **Source URL:** https://arxiv.org/abs/2510.15312
- **Reference count:** 40
- **Primary result:** sd.npu achieves 1.06–3.81× speedup and 1.07–4.71× energy reduction for on-device RAG on mobile NPUs

## Executive Summary
This paper addresses the challenge of enabling efficient on-device Retrieval-Augmented Generation (RAG) on mobile NPUs, which struggle with the dynamic workloads of RAG due to static graph constraints and underutilization during decoding. The authors propose sd.npu, a holistic acceleration framework that combines progressive graph scheduling and NPU-optimized speculative decoding. Progressive graph scheduling overlaps graph switching overhead with prefill computation by partitioning the model into blocks and gradually transitioning to decoding-optimized graphs. NPU-optimized speculative decoding includes in-context distribution calibration, which aligns retrieved drafts with the model's output distribution using prefill logits, and draft extension, which reuses high-confidence rejected tokens to form longer, NPU-friendly workloads. Experiments on commercial smartphones demonstrate that sd.npu achieves 1.06–3.81× speedup and 1.07–4.71× energy reduction compared to baselines, outperforming existing methods by up to 2.53× in latency and 4.71× in energy.

## Method Summary
The sd.npu framework addresses mobile NPU inefficiencies in RAG workloads through two complementary optimizations. Progressive graph scheduling partitions the LLM into blocks and overlaps decoding graph loading with prefill computation, masking the expensive model reconfiguration overhead. NPU-optimized speculative decoding enhances draft quality and NPU utilization through in-context distribution calibration (using prefill logits to generate model-aligned drafts) and verification-aware draft extension (reusing high-confidence rejected tokens to reach NPU saturation lengths). The system is implemented on commercial Snapdragon devices using the mllm framework, targeting 0.5B-3B parameter models.

## Key Results
- Achieves 1.06–3.81× latency speedup and 1.07–4.71× energy reduction on commercial smartphones
- Progressive graph scheduling masks up to 29% model reconfiguration overhead
- In-context calibration improves draft acceptance rates by aligning retrieved text with model's output distribution
- Draft extension converts idle NPU cycles into valid token throughput by targeting minimum tile sizes

## Why This Works (Mechanism)

### Mechanism 1: Progressive Graph Scheduling
Reduces latency overhead of transitioning between prefill and decoding phases by overlapping I/O with computation. The system partitions the model into blocks and asynchronously loads the decoding graph for subsequent blocks while computing prefill for current blocks using chunked inputs. This works when the model can be partitioned such that I/O cost of loading a block's decoding graph is less than or equal to compute time of processing a prefill chunk on the current graph.

### Mechanism 2: In-Context Distribution Calibration
Increases acceptance rate of speculative drafts by aligning retrieved text with the model's predicted probability distribution. The system leverages prefill logits computed during the prefill phase to sample high-probability successors and construct a calibrated token tree, rather than naively copying text segments from retrieved context. This works when prefill logits contain sufficient signal to predict the model's preferred lexical choices better than raw retrieved text.

### Mechanism 3: Verification-Aware Draft Extension
Increases NPU compute density by artificially lengthening the verification workload. The system identifies "reusable" tokens from previously rejected drafts—tokens that failed due to lexical mismatch but are semantically valid—and appends them to current drafts. This extends sequence length to meet NPU's minimum tile size requirements (e.g., 32 tokens). This works when a significant portion of rejected tokens are "temporarily" wrong but become valid later, and the overhead of managing this recycling is negligible compared to NPU dispatch costs.

## Foundational Learning

- **NPU Static Graph Constraints:** Mobile NPUs require compiled, fixed-shape compute graphs and cannot dynamically change tensor shapes without reloading the graph, which is expensive. Quick check: Why can't we just use a single graph for both processing a 1000-token prompt and generating 1 token?

- **Speculative Decoding (Draft-then-Verify):** Generates candidate tokens cheaply (drafting) and verifies them in parallel with the main model to speed up memory-bound decoding. Quick check: In standard Speculative Decoding, if the draft model generates 5 tokens and the 3rd token is rejected, how many tokens are actually added to the sequence?

- **Arithmetic Intensity & NPU Saturation:** NPU efficiency relies on high arithmetic intensity (lots of math per byte of memory access). Processing 1 token at a time is memory-bound (low utilization), while processing 32+ tokens is compute-bound (high utilization). Quick check: Why does verifying a short draft (e.g., 4 tokens) leave an NPU "underutilized" compared to verifying a long draft (e.g., 32 tokens)?

## Architecture Onboarding

- **Component map:** Offline Profiler -> Prefill Engine -> Graph Scheduler -> Calibration Module -> Draft Engine -> Verification Unit

- **Critical path:**
  1. **Partitioning:** Model split into N blocks (e.g., N=2)
  2. **Prefill & Switch:** As Block 1 computes prefill on G1, Block 2's G2 is loaded into memory
  3. **Calibration:** Post-prefill, logits are scanned to build a "calibrated context"
  4. **Drafting:** A draft tree is built, padded with recycled tokens to reach length ≥ 32
  5. **Verify:** The tree is verified on the NPU

- **Design tradeoffs:**
  - **Partitioning Granularity (N):** More blocks allow finer-grained overlapping but increase scheduling complexity and kernel launch overhead
  - **Draft Length Threshold:** Increasing length improves NPU saturation but risks higher rejection rates if draft quality is poor
  - **Calibration Depth:** Searching deeper in logit tree improves draft quality but increases prefill overhead

- **Failure signatures:**
  - **High Rejection Rate:** If calibration fails or retrieval is irrelevant, system falls back to single-token generation, negating speedups
  - **Memory Pressure:** If G1 and G2 cannot coexist in RAM, progressive scheduling fails, forcing full reload stall
  - **Latency Spikes:** If "reusable" token buffer grows too large or lookup is slow, CPU overhead may consume NPU gains

- **First 3 experiments:**
  1. **Baseline Latency Breakdown:** Measure time spent in "Graph Switch" vs. "Compute" in vanilla RAG setup to confirm 29% overhead claim
  2. **Acceptance Rate Analysis:** Compare token acceptance rates of "Raw Retrieval" vs. "Calibrated Retrieval" to validate distribution calibration efficacy
  3. **Utilization Profiling:** Monitor NPU utilization during decoding with standard SD vs. Draft Extension to visualize saturation gap

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but several remain unresolved regarding generalization to non-systolic array NPU architectures, scalability to larger models, and performance with large retrieval corpora.

## Limitations
- Experimental evaluation restricted to Qualcomm Snapdragon devices with Hexagon NPUs, limiting generalizability to other mobile NPU architectures
- Fixed partitioning strategy (N=2) may not scale effectively to models larger than 3B parameters due to memory constraints
- The "confidence-based" threshold for reusing rejected tokens lacks detailed specification and robustness analysis

## Confidence
- **High Confidence:** The fundamental observation that NPU static graph constraints create inefficiency during RAG workloads is well-supported
- **Medium Confidence:** The in-context distribution calibration using prefill logits to improve draft acceptance rates is plausible but lacks comparative analysis against simpler alternatives
- **Low Confidence:** The NPU-optimized draft extension claims rest on the assumption that rejected tokens are often "temporarily wrong" and can be safely reused without thorough examination of potential quality degradation

## Next Checks
1. **Baseline Latency Breakdown:** Measure and report the time spent in "Graph Switch" versus "Compute" phases in a vanilla RAG setup to independently verify the claimed 29% overhead reduction across different context lengths and models.

2. **Acceptance Rate Analysis:** Conduct an ablation study comparing token acceptance rates of "Raw Retrieval" versus "Calibrated Retrieval" across multiple datasets, and measure how often the draft extension mechanism is actually triggered.

3. **Robustness Testing:** Evaluate sd.npu's performance when the retrieval system returns low-quality or irrelevant context, measuring whether calibration degrades gracefully and whether draft extension creates observable quality issues in edge cases.