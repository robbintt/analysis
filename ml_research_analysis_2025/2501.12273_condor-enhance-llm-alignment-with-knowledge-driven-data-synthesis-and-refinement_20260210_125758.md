---
ver: rpa2
title: 'Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement'
arxiv_id: '2501.12273'
source_url: https://arxiv.org/abs/2501.12273
tags:
- data
- condor
- arxiv
- qwen2
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Condor is a two-stage synthetic data generation framework for improving
  large language model alignment. It generates high-quality supervised fine-tuning
  data by first constructing a World Knowledge Tree to inspire diverse questions,
  then applying Self-Reflection Refinement to iteratively improve model responses.
---

# Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement

## Quick Facts
- arXiv ID: 2501.12273
- Source URL: https://arxiv.org/abs/2501.12273
- Reference count: 28
- Primary result: Two-stage synthetic data framework achieves RLHF-level performance using only 20K samples of SFT

## Executive Summary
Condor is a two-stage synthetic data generation framework that significantly improves LLM alignment through knowledge-driven data synthesis and self-reflection refinement. It generates high-quality supervised fine-tuning data by first constructing a World Knowledge Tree to inspire diverse questions, then applying Self-Reflection Refinement to iteratively improve model responses. Using only 20K synthetic samples, Condor fine-tuning significantly outperforms official RLHF models on human-preference benchmarks, achieving up to 75.07 average score compared to 71.03 for official models. The approach demonstrates strong scalability, enabling self-improvement across various model sizes up to 72B parameters, and shows that chat capabilities can be enhanced independently of knowledge capabilities.

## Method Summary
Condor employs a two-stage synthetic data generation process: Condor Void builds a World Knowledge Tree with 8,400+ hierarchical tags to generate diverse questions across 7 task types and 3 difficulty levels, producing initial responses; Condor Refine applies self-reflection to these responses, generating structured critiques and refined answers. The framework uses a single LLM (Qwen2.5-72B-Instruct) to synthesize all data, creating approximately 200K refined QA pairs. Fine-tuning uses xTuner with learning rate 2e-5 for 3 epochs. The method challenges the assumption that preference optimization requires reinforcement learning by achieving RLHF-level performance through high-quality synthetic SFT data alone.

## Key Results
- Condor achieves 75.07 average score on human-preference benchmarks, outperforming official RLHF models (71.03)
- Self-reflection refinement contributes 2-3% performance improvement over unrefined data
- Models fine-tuned on Condor data show strong scalability from 7B to 72B parameters with consistent gains
- Chat alignment and knowledge capabilities are largely independent, with SFT improving human preference scores while maintaining stable knowledge benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical knowledge structures stimulate more diverse question generation than seed prompts. The World Knowledge Tree organizes tags from coarse root concepts to fine-grained leaf tags, combined with 7 task types and 3 difficulty levels, creating a structured inspiration space rather than relying on random or existing-dataset prompts. The model's ability to generate diverse, high-quality questions depends on systematic coverage of conceptual space rather than stochastic exploration. If tag-to-question generation produces low-quality or repetitive outputs, the tree structure alone cannot compensate—quality depends on the underlying model's instruction-following capability.

### Mechanism 2
Self-generated critiques enable iterative response improvement without external reward models. The model generates structured critiques identifying strengths, weaknesses, and suggestions, then produces a refined response preserving effective elements while addressing weaknesses. This creates a self-contained improvement loop. Models can reliably identify weaknesses in their own outputs and the critique-to-improvement mapping is learnable from the same model's capabilities. If the model's self-critique is unreliable (e.g., identifies false weaknesses or misses real ones), refinement may degrade quality rather than improve it.

### Mechanism 3
Chat alignment and knowledge capability are largely independent, allowing targeted SFT improvement. SFT on Condor data improves human-preference scores significantly (+6-10% across models) while knowledge benchmarks remain stable (<1% variance), suggesting SFT primarily shapes response formatting/style rather than knowledge injection. Knowledge is primarily acquired during pretraining; SFT's role is teaching the model to effectively utilize existing knowledge. If synthetic SFT data contains factual errors or hallucinations, this may introduce knowledge degradation rather than neutral impact.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) vs. RLHF**: Why needed here: Condor achieves RLHF-level performance using only SFT, which challenges the assumption that preference optimization requires reinforcement learning. Quick check question: Can you explain why high-quality synthetic SFT data might reduce the need for explicit reward modeling?

- **Self-Consistency and Self-Improvement**: Why needed here: The Condor Refine stage relies on the model evaluating its own outputs, which requires understanding how models can serve as their own critics. Quick check question: What failure modes might occur when a model critiques its own responses?

- **Data Diversity Metrics**: Why needed here: The paper claims Condor achieves comparable/broader distribution than Magpie, requiring understanding of embedding-based coverage analysis. Quick check question: How would you measure whether synthetic data covers the same distribution as real user queries?

## Architecture Onboarding

- **Component map**: Tag selection → Question synthesis → Initial response → Critique generation → Refined response → SFT training
- **Critical path**: Tag selection → Question synthesis → Initial response → Critique generation → Refined response → SFT training. Weakness at any stage propagates; critique quality is the bottleneck.
- **Design tradeoffs**: Single-model vs. multi-model (Condor uses one model for all stages vs. using stronger models for critique); dataset size (20K samples suffice but scaling shows continued improvement); task coverage (all 7 tasks provide best results but partial sets still improve).
- **Failure signatures**: Low critique quality (refined responses show minimal improvement or introduce new errors); tag-constraint mismatch (questions don't meaningfully relate to assigned tags); knowledge drift (knowledge benchmark scores drop significantly); format collapse (all refined responses converge to similar structure).
- **First 3 experiments**: (1) Ablate refinement stage: train on D_V (unrefined) vs. D_R (refined) on same base model to isolate refinement contribution—expect 2-3% gap; (2) Scale tag coverage: train with 25%, 50%, 100% of tags to verify diversity hypothesis—expect monotonic improvement; (3) Self-iteration test: generate Condor data using the target model itself to validate self-improvement capability—7B should still improve.

## Open Questions the Paper Calls Out

- **Does multi-round iterative synthesis using the Condor framework lead to compounding performance gains or model collapse?** The authors state in the Limitations section that the "use of multi-round iterative synthetic data" requires further exploration to understand its effects. The current experiments utilize a single generation and refinement pass; the dynamics of feeding $D_R$ back into the model to generate $D_{R+1}$ are unknown.

- **To what extent does the Self-Reflection Refinement stage propagate factual hallucinations versus mitigating them?** The paper notes that "hallucinations produced by LLMs in synthetic data could also become a potential risk" that needs to be addressed. While the paper demonstrates improved human-preference scores, it does not isolate whether the refinement step corrects factual errors or merely improves stylistic quality while retaining or amplifying hallucinations.

- **How can the World Knowledge Tree be augmented to further maximize the semantic diversity of synthesized questions?** The authors identify "how to further enhance the diversity of the synthetic data" as a specific limitation requiring further improvement. The current method relies on trending topics and a static set of task types; it is unclear if this covers the "long tail" of potential user queries or if the model is constrained by its own initial knowledge boundaries.

## Limitations

- The use of multi-round iterative synthetic data requires further exploration to understand its effects on model performance and potential collapse.
- Hallucinations produced by LLMs in synthetic data could become a potential risk that needs to be addressed.
- How to further enhance the diversity of the synthetic data remains an open question requiring improvement.

## Confidence

- **High**: The paper provides comprehensive experimental results with multiple model sizes (7B, 14B, 32B, 72B) showing consistent improvements.
- **Medium**: Some key implementation details (exact root tags, example lists for prompts) are not fully specified in the paper.
- **Low**: The long-term effects of multi-round iterative synthesis and hallucination propagation remain unexplored.

## Next Checks

1. Reproduce the ablation study comparing D_V (unrefined) vs. D_R (refined) data on a 7B model to verify the 2-3% performance gap.
2. Test self-iteration capability by generating Condor data using the fine-tuned 7B model itself and measuring performance improvement.
3. Verify diversity claims by comparing t-SNE embeddings of Condor-generated questions against Magpie and real user query datasets.