---
ver: rpa2
title: The Imperfective Paradox in Large Language Models
arxiv_id: '2601.09373'
source_url: https://arxiv.org/abs/2601.09373
tags:
- group
- bias
- verbs
- aspectual
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We investigate whether large language models can logically reason\
  \ about the Imperfective Paradox, a linguistic phenomenon where past progressive\
  \ aspect entails event realization for activities (e.g., running \u2192 ran) but\
  \ not for goal-oriented accomplishments (e.g., building \u219B built). To probe\
  \ this, we introduce IMPERFECTIVENLI, a diagnostic NLI dataset constructed using\
  \ a minimal-pair design across four logical conditions (Interrupted/Ambiguous \xD7\
  \ Telic/Atelic), featuring 100 telic and 100 atelic verbs with fine-grained semantic\
  \ annotations (Creation, Consumption, Change of State, Motion to Goal)."
---

# The Imperfective Paradox in Large Language Models

## Quick Facts
- arXiv ID: 2601.09373
- Source URL: https://arxiv.org/abs/2601.09373
- Reference count: 40
- Models systematically hallucinate completion for telic verbs, overriding textual negation

## Executive Summary
This paper investigates whether large language models can logically reason about the Imperfective Paradox, a linguistic phenomenon where past progressive aspect entails event realization for activities but not for goal-oriented accomplishments. The authors introduce IMPERFECTIVENLI, a diagnostic NLI dataset constructed using minimal-pair design across four logical conditions, featuring 100 telic and 100 atelic verbs. Evaluating seven 7-9B open-weight models in zero-shot settings, they uncover a pervasive Teleological Bias where models systematically hallucinate completion for telic verbs, often overriding explicit textual negation. The study reveals that while embeddings distinguish process from result, inference is dominated by priors about goal attainment, and prompting interventions reduce hallucinated completions but cause over-correction for atelic verbs.

## Method Summary
The authors construct IMPERFECTIVENLI, a diagnostic NLI dataset using a minimal-pair design across four logical conditions (Interrupted/Ambiguous × Telic/Atelic). The dataset features 100 telic and 100 atelic verbs with fine-grained semantic annotations including Creation, Consumption, Change of State, and Motion to Goal. Seven 7-9B open-weight models are evaluated in zero-shot settings. Representational analyses examine how embeddings distinguish process from result states. The study tests various prompting interventions including Chain-of-Thought and Counterfactual approaches to mitigate the observed teleological bias.

## Key Results
- Models exhibit systematic teleological bias, hallucinating completion for telic verbs even when text explicitly negates it
- Embedding analyses show distinct process vs. result representations, but inference is dominated by prior beliefs about goal attainment
- Prompting interventions reduce hallucinated completions but over-correct, causing incorrect rejections of valid entailments for atelic verbs
- Aspectual reasoning improves with model scale and exhibits non-linear emergence
- Motion verbs are reasoned about more accurately than creation verbs

## Why This Works (Mechanism)
The mechanism underlying the observed behavior is that current LLMs operate as probabilistic narrative predictors rather than faithful logical reasoners. When encountering telic verbs, models rely on strong priors about goal attainment that override explicit textual information. The models' embeddings can distinguish between process and result states at the representational level, but the inference mechanism is dominated by teleological assumptions rather than careful logical analysis of aspectual markers. This explains why prompting interventions that encourage explicit reasoning can reduce hallucinated completions but often overcorrect, leading to incorrect rejections of valid entailments for atelic verbs.

## Foundational Learning

**Aspectual semantics**: Understanding the distinction between telic (goal-oriented) and atelic (non-goal-oriented) predicates is crucial for analyzing how models handle the Imperfective Paradox. This theoretical foundation enables the construction of diagnostic minimal pairs that test logical entailment.

Why needed: Provides the linguistic framework for constructing valid test cases and interpreting model behavior
Quick check: Can you explain why "John was building a house" does not necessarily entail "John built a house"?

**Natural Language Inference**: The task of determining whether one sentence logically follows from another forms the core evaluation framework for testing aspectual reasoning.

Why needed: Establishes the methodology for systematically testing logical entailment across different aspectual conditions
Quick check: Can you differentiate between neutral, entailment, and contradiction relationships in NLI?

**Semantic verb classes**: Categorizing verbs into Creation, Consumption, Change of State, and Motion to Goal allows fine-grained analysis of how different verb types affect model reasoning.

Why needed: Enables analysis of whether certain semantic classes are reasoned about more accurately than others
Quick check: Can you classify "destroy" as Creation, Consumption, Change of State, or Motion to Goal?

## Architecture Onboarding

**Component map**: Input text → Embedding layer → Transformer blocks → Attention mechanism → Output logits → Classification decision

**Critical path**: Input → Token embedding → Positional encoding → Multi-head attention → Feed-forward network → Residual connections → Layer normalization → Output projection

**Design tradeoffs**: The models balance expressivity (through deep architectures and attention mechanisms) against computational efficiency, but this design does not inherently encode aspectual structure, leading to reliance on statistical priors rather than logical reasoning.

**Failure signatures**: Systematic over-generation of completion entailments for telic verbs, incorrect rejection of valid entailments for atelic verbs under prompting interventions, and scale-dependent but non-linear improvements in aspectual reasoning.

**First experiments**: 1) Test model responses to minimal pairs with explicit negation of completion; 2) Compare zero-shot performance across different model scales; 3) Evaluate effect of Chain-of-Thought prompting on entailment judgments

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions for future research. However, the limitations section suggests several areas warranting investigation, including whether finetuning or domain adaptation might improve performance, whether similar biases appear in larger models, and whether the observed improvements with scale reflect genuine reasoning capacity or increased memorization of training patterns.

## Limitations

- Potential confounding effects of syntactic and lexical cues in constructed minimal pairs may obscure genuine aspectual reasoning
- Limited coverage to 200 targeted verb types may not generalize to broader linguistic phenomena
- Reliance on English-language stimuli restricts generalizability to other aspectual systems
- Zero-shot evaluations may reflect learned statistical associations rather than true aspectual reasoning

## Confidence

- High: Models exhibit systematic teleological bias, overriding textual negation for telic verbs
- Medium: Embedding analyses reveal distinct process vs. result representations, but inference is prior-dominated
- Medium: Prompting interventions reduce hallucinated completions but cause over-correction for atelic verbs
- Low: Non-linear emergence of aspectual reasoning with scale is conclusively established

## Next Checks

1. Conduct a targeted ablation study removing syntactic and lexical cues (e.g., tense markers, aspect particles) to isolate the contribution of genuine aspectual reasoning

2. Extend evaluation to multilingual corpora and models to test the cross-linguistic robustness of the observed teleological bias

3. Compare zero-shot performance to finetuned or prompted models using explicit aspectual training signals to determine whether observed reasoning is structural or associative