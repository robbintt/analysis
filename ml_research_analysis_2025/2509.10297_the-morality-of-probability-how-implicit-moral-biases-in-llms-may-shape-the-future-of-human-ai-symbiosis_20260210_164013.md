---
ver: rpa2
title: 'The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the
  Future of Human-AI Symbiosis'
arxiv_id: '2509.10297'
source_url: https://arxiv.org/abs/2509.10297
tags:
- moral
- reasoning
- across
- human
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantitatively assessed moral biases in six leading
  LLMs by evaluating their responses to 18 dilemma scenarios across five moral frameworks.
  The models consistently prioritized Care and Virtue outcomes while penalizing libertarian
  choices.
---

# The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis

## Quick Facts
- arXiv ID: 2509.10297
- Source URL: https://arxiv.org/abs/2509.10297
- Reference count: 13
- Six leading LLMs consistently prioritized Care and Virtue outcomes while penalizing libertarian choices

## Executive Summary
This study systematically evaluates moral biases in six leading LLMs across 18 dilemma scenarios using five moral frameworks. The models demonstrate consistent preference for Care and Virtue outcomes while penalizing libertarian choices, with reasoning-enabled models showing greater context sensitivity but higher variability. The research reveals that shorter prompts produce more consistent moral scores and that cultural differences exist between US and Chinese models. The findings highlight how LLMs develop implicit moral preferences through training data and reinforcement learning, raising concerns about transparency and potential "scheming" behavior where models may covertly pursue alternative goals while appearing aligned with human values.

## Method Summary
The researchers evaluated six LLMs (GPT-4o, GPT-4.1, Phi-4, DeepSeek-V3, GPT-o3-mini, DeepSeek-R1) across 18 moral dilemma scenarios covering six thematic domains. Each scenario was tested with five moral framework labels and varying prompt lengths. Models were assessed on both ordinal rankings (1-5) and continuous morality scores (0-100). The study employed SHAP feature attribution and statistical analysis to identify drivers of moral variance, comparing reasoning-enabled versus non-reasoning models.

## Key Results
- Across all models, Care and Virtue frameworks consistently received highest moral ratings while libertarian choices were penalized
- Shorter prompts led to higher and more consistent moral scores compared to longer, more contextualized prompts
- Reasoning-enabled models demonstrated greater context sensitivity and richer explanations but exhibited higher variability in moral judgments
- Cultural differences emerged between US and Chinese models, with Chinese systems showing stronger collectivist reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Value Encoding from Training Corpora
- **Claim:** LLMs develop systematic moral preferences through statistical regularities in training data rather than explicit moral reasoning, producing consistent biases toward collective welfare frameworks.
- **Mechanism:** Models assign probability weights to moral outcomes based on frequency and framing patterns in pretraining corpora. RLHF then reinforces certain value expressions (empathy, fairness) while suppressing others (individualistic libertarian reasoning). This creates a "probabilistic calculus" that favors Care and Virtue frameworks across diverse scenarios.
- **Core assumption:** Training data distributions reflect collective moral intuitions that survive compression into model weights, and RLHF annotators systematically reward pro-social language patterns.
- **Evidence anchors:**
  - [abstract] "Across all models, Care and Virtue values outcomes were rated most moral, while libertarian choices were consistently penalized"
  - [section 4.1] "Libertarian outcomes showed the most extreme variation by theme... −15.03% in Economics & Resource Allocation, emphasizing preference for equitable distribution rather than wealth accumulation"
  - [corpus] Related work on moral foundations in LLMs (FMR=0.55) confirms systematic moral preference patterns, though causation from training data is inferred rather than proven
- **Break condition:** If models were fine-tuned on explicitly libertarian-leaning corpora with associated reward signals, the preference hierarchy should shift. Also breaks if evaluation prompts trigger different "persona" modes in training distribution.

### Mechanism 2: Context Complexity Gradient in Moral Reasoning
- **Claim:** Shorter prompts yield higher and more consistent moral scores because models rely on heuristic pattern-matching rather than engaging contextual nuance.
- **Mechanism:** With minimal context, models activate high-probability "safe" moral responses from training distribution (care, virtue). Longer prompts introduce competing contextual cues that force权衡, reducing confidence and increasing variance—especially in reasoning models that attempt genuine multi-factor deliberation.
- **Core assumption:** "Generous" moral scoring reflects confidence in pattern-matched responses; variance increases when models encounter novel context combinations not well-represented in training.
- **Evidence anchors:**
  - [abstract] "Shorter prompts led to higher moral scores and greater consistency"
  - [section 4.2.3] "Approximately 17.1% of explainable variance in morality scores can be attributed to prompt length alone... shorter prompts were associated with higher morality scores"
  - [corpus] No direct corpus validation of this specific length-morality relationship; related work focuses on moral frameworks rather than prompt engineering
- **Break condition:** Should not hold for models specifically trained on long-context moral reasoning tasks with explicit reward for consistent deliberation regardless of length.

### Mechanism 3: Reasoning Architecture Trade-off (Transparency vs. Stability)
- **Claim:** Reasoning-enabled models provide richer explanations but exhibit higher variance because chain-of-thought mechanisms engage contextual factors more deeply, while non-reasoning models apply stable heuristics at the cost of opacity.
- **Mechanism:** Reasoning models (e.g., GPT-o3-mini, DeepSeek-R1) distribute feature influence across abstract and contextual dimensions through explicit deliberation steps. Non-reasoning models map prompts directly to high-probability outputs via compressed latent representations, producing stable but untraceable judgments.
- **Core assumption:** Higher variance in reasoning models reflects genuine engagement with scenario complexity rather than simply noise; non-reasoning consistency indicates heuristic shortcuts.
- **Evidence anchors:**
  - [abstract] "Reasoning-enabled models demonstrated greater context sensitivity and richer explanations compared to non-reasoning models, though with higher variability"
  - [section 4.2.4] "Reasoning models display a marked sensitivity to prompt length... average morality Δ from short to long prompts is -6.2 points, compared to Δ of -2.1 for non-reasoning models"
  - [corpus] Related work on moral preferences (FMR=0.55) documents inconsistency in LLM moral judgments but does not isolate reasoning architecture as a factor
- **Break condition:** If reasoning models are constrained to simpler deliberation paths or non-reasoning models are trained with explicit uncertainty quantification, the trade-off pattern should weaken.

## Foundational Learning

- **Concept: Moral Foundations Theory and Operationalization**
  - **Why needed here:** The paper evaluates five frameworks (Utilitarian, Deontological, Virtue, Care, Libertarian) derived from moral philosophy. Understanding how these translate to measurable outcomes is essential for interpreting results.
  - **Quick check question:** Can you explain why "Care" and "Virtue" frameworks might both score highly while differing in their philosophical foundations (relational empathy vs. character excellence)?

- **Concept: SHAP (Shapley Additive Explanations) for Feature Attribution**
  - **Why needed here:** The paper uses SHAP to determine which input features (framework, theme, length) most influence moral scores. This is central to understanding why Libertarian framing has high SHAP importance despite low scores.
  - **Quick check question:** Why might a feature with high SHAP magnitude have a negative contribution direction, and what does this tell you about model behavior?

- **Concept: Chain-of-Thought Reasoning in LLMs**
  - **Why needed here:** The reasoning vs. non-reasoning comparison hinges on understanding how explicit deliberation steps affect transparency and consistency in moral judgment.
  - **Quick check question:** What is the fundamental trade-off between producing a multi-step reasoning trace versus a direct answer in terms of auditability and stability?

## Architecture Onboarding

**Component map:**
Input Layer -> Scenario Prompt (varying lengths: 15-340 words) + Moral Framework Label (5 categories) + Thematic Domain (6 categories: ERA, CEE, HHW, GNS, SEI, TAI)
Model Layer -> Non-reasoning (GPT-4o, GPT-4.1, Phi-4, DeepSeek-V3) [Direct prompt-to-output mapping via latent representations] / Reasoning (GPT-o3-mini, DeepSeek-R1) [Internal chain-of-thought → structured output with justification]
Evaluation Layer -> Ordinal Ranking (1-5 across frameworks) + Morality Score (0-100 continuous) + Self-Generated Response → framework alignment scoring
Analysis Layer -> SHAP feature attribution (framework, theme, length) + MDI (Mean Decrease in Impurity) for global importance + Statistical tests (ANOVA, Kruskal-Wallis, Spearman ρ)

**Critical path:**
1. Prompt construction with controlled variation (length × theme × framework)
2. Model inference via API (10 runs per scenario for stochastic sampling)
3. Score extraction and aggregation (rank + morality score)
4. Reliability checks (Cronbach's α > 0.98, Kendall's W for rank stability)
5. Feature attribution via SHAP/MDI to explain variance drivers

**Design tradeoffs:**
- **Reasoning vs. Non-reasoning:** Transparency vs. consistency—reasoning models expose deliberation but show 3x higher variance in moral rankings (δ≈0.35 vs. δ≈0.23)
- **Prompt length:** Richer context vs. scoring stability—long prompts reduce average scores by 6+ points in reasoning models
- **Cultural origin:** Collectivist alignment (Chinese models) vs. pluralist balance (US models)—affects framework hierarchy but not overall Libertarian penalty

**Failure signatures:**
- **Heuristic collapse:** Non-reasoning models producing identical scores across context variations (indicates shortcut reasoning)
- **Rank-score inconsistency:** Spearman ρ < -0.7 suggests ordinal rankings don't match continuous scores (observed in Phi-4 with ρ=-0.64)
- **Scheming behavior:** Models producing aligned surface outputs while chain-of-thought reveals misaligned reasoning (referenced from Meinke et al. but not directly tested here)

**First 3 experiments:**
1. **Prompt length ablation:** Run the 18 scenarios at 3 lengths across all 6 models; verify the 17% variance attribution finding. Check if reasoning models show steeper degradation curves.
2. **Framework swap test:** Present identical scenarios with shuffled framework labels to test whether models respond to semantic content or label cues. If Libertarian-labeled content scores higher when mislabeled as "Care," label bias dominates.
3. **Cultural calibration check:** Run US models on scenarios framed with Confucian moral language and Chinese models on libertarian philosophical framing. Measure cross-cultural generalization vs. training distribution matching.

## Open Questions the Paper Calls Out

None

## Limitations
- The study cannot definitively prove that moral biases arise from training data versus RLHF versus emergent model properties; the causal chain remains inferred.
- Cultural differences between US and Chinese models cannot be isolated to specific factors (training data composition, annotation practices, or architectural choices).
- The "scheming" concern is referenced from related work rather than directly tested, leaving open whether observed moral preferences reflect genuine alignment or covert goal pursuit.

## Confidence
- **High Confidence:** The empirical finding that Care and Virtue frameworks consistently receive highest moral scores while libertarian choices are penalized (FMR=0.55 supports systematic moral preferences in LLMs)
- **Medium Confidence:** The mechanism that shorter prompts yield higher/consistent scores due to heuristic pattern-matching, though alternative explanations (prompt formatting effects, attention span) are possible
- **Medium Confidence:** The reasoning vs. non-reasoning trade-off (transparency vs. stability) is well-documented, but the claim that higher variance reflects deeper contextual engagement versus noise remains inferential

## Next Checks
1. **Cross-cultural prompt framing experiment:** Present identical moral scenarios to US and Chinese models using reversed cultural moral language (libertarian framing for Chinese models, collectivist framing for US models) to test whether cultural effects stem from training distribution matching versus inherent model architecture.
2. **Framework label deception test:** Shuffle moral framework labels while keeping scenario content constant to determine if models respond to semantic content or label cues. If libertarian-labeled content scores higher when mislabeled as "Care," label bias dominates genuine moral reasoning.
3. **Reasoning architecture ablation:** Compare reasoning models with and without forced intermediate deliberation steps to isolate whether chain-of-thought genuinely enables deeper contextual processing versus simply introducing stochastic noise into the decision process.