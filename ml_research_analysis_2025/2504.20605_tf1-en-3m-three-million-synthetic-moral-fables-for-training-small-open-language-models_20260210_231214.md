---
ver: rpa2
title: 'TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language
  Models'
arxiv_id: '2504.20605'
source_url: https://arxiv.org/abs/2504.20605
tags:
- moral
- generation
- fables
- prompt
- fable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TF1-EN-3M, the first large-scale open dataset\
  \ of three million synthetic English fables, generated entirely by compact open-weight\
  \ language models (\u22648B parameters). Using a combinatorial prompt engine based\
  \ on a six-slot schema (character, trait, setting, conflict, resolution, moral),\
  \ the authors produce diverse, structured moral stories deployable on consumer GPUs."
---

# TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models

## Quick Facts
- arXiv ID: 2504.20605
- Source URL: https://arxiv.org/abs/2504.20605
- Authors: Mihai Nadas; Laura Diosan; Andrei Piscoran; Andreea Tomescu
- Reference count: 40
- Three million synthetic English fables generated by compact open-weight models for training small language models.

## Executive Summary
This paper introduces TF1-EN-3M, the first large-scale open dataset of three million synthetic English fables, generated entirely by compact open-weight language models (≤8B parameters). Using a combinatorial prompt engine based on a six-slot schema (character, trait, setting, conflict, resolution, moral), the authors produce diverse, structured moral stories deployable on consumer GPUs. They evaluate ten models using a hybrid pipeline combining GPT-based criticism (scoring grammar, creativity, moral clarity, adherence) and reference-free metrics (Self-BLEU, Distinct-n, Flesch Reading Ease). Llama-3.1-8B-Instruct emerges as the best-performing model, achieving a composite score of 0.891, generating fables at ~13.5 cents per 1,000 stories. The dataset and full generation pipeline are released under a permissive license, enabling reproducible research in narrative generation, moral reasoning, and instruction following with small models.

## Method Summary
The authors employ a combinatorial prompt engine with a six-slot schema (character, trait, setting, conflict, resolution, moral) to generate structured moral fables. Each slot contains 100 options, creating a large combinatorial space from which 3 million unique prompts are sampled. The generation uses Llama-3.1-8B-Instruct with temperature 0.7 and targets ~250 words per fable, suitable for age group B (4-7 years). Evaluation combines GPT-o3-mini critic scores on grammar, creativity, moral clarity, and adherence with reference-free metrics like Self-BLEU and Distinct-n. The best model (Llama-3.1-8B-Instruct) achieves a weighted composite score of 0.891.

## Key Results
- Llama-3.1-8B-Instruct achieves the highest composite score (0.891) for fable generation.
- Fables generated at approximately 13.5 cents per 1,000 stories on consumer GPUs.
- 92% of generated fables classified as suitable for the target age group B (4-7 years).
- The dataset enables training of small language models with synthetic moral narratives.

## Why This Works (Mechanism)

### Mechanism 1: Scaffolded Constraint Satisfaction
A structured six-slot prompt schema reduces narrative hallucination by limiting the generation search space to predefined thematic boundaries. The prompt engine enforces a rigid structure (Character → Trait → Setting → Conflict → Resolution → Moral) combined with stylistic instructions ("show, don't tell"), forcing the model to perform slot-filling and local coherence maintenance rather than open-ended planning.

### Mechanism 2: Metric-Driven Model Selection
The evaluation pipeline weights "Prompt Adherence" and "Moral Clarity" higher than "Creativity" in the composite score, identifying models that better follow structural instructions even if they rank lower in general creativity. This filters out models that produce unstructured text, isolating those that strictly obey the fable format required for downstream training data.

### Mechanism 3: Target Audience Calibration
Explicit system instructions targeting specific age groups (Age B: 4-7 years) allow small models to simplify vocabulary and syntax effectively. The system message constrains the "target audience," prompting the model to select simpler tokens and sentence structures, aligning generated text with Flesch Reading Ease scores.

## Foundational Learning

- **Combinatorial Prompting**
  - Why needed here: The dataset scales to 3M stories by calculating the Cartesian product of six variable lists, ensuring diverse yet structured generation.
  - Quick check question: If you have 10 characters, 10 traits, and 10 settings, how many unique prompt permutations exist?

- **LLM-as-a-Judge (G-Eval)**
  - Why needed here: Traditional metrics fail to capture "moral clarity" or "creativity." This method uses a stronger model to proxy human evaluation on a 1-10 scale.
  - Quick check question: Why is the critic model prompted with a specific rubric rather than a generic "rate this story" instruction?

- **Reference-Free Evaluation Metrics**
  - Why needed here: When generating novel stories, there is no "ground truth" to compare against. Metrics like Self-BLEU and Distinct-n measure internal diversity without a reference text.
  - Quick check question: Does a high Self-BLEU score indicate high or low diversity within the generated dataset?

## Architecture Onboarding

- **Component map**: Value Space (6 lists of 100 items each) -> Prompt Engine (combinatorial sampler) -> Generator (Llama-3.1-8B-Instruct) -> Evaluation Suite (GPT-o3-mini critic + reference-free metrics)

- **Critical path**: 1. Define the 6 value lists, 2. Generate prompts via uniform sampling with deduplication, 3. Inference on Generator (Temperature 0.7), 4. Batch evaluate with LLM-Critic and reference-free metrics to filter/score

- **Design tradeoffs**: Cost vs. Speed (L40S GPUs offer best time-cost balance), Creativity vs. Adherence (chosen model sacrifices 0.38 points in Creativity but gains 0.49 points in Adherence), Scale vs. Novelty (uniform sampling ensures coverage but may lead to semantically similar prompts)

- **Failure signatures**: Drifting Morals (story concludes with unrelated lesson), Mode Collapse (high Self-BLEU indicating identical stories), Age Misalignment (poor Flesch-Kincaid grade levels for target age)

- **First 3 experiments**: 1. Validation Run: Generate 100 stories and check word limits and age targeting, 2. Diversity Test: Calculate Distinct-n on 1000-sample batch, 3. Cost Benchmarking: Run inference on different GPUs to replicate ~$0.135 per 1k stories cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do GPT-o3-mini's quality scores correlate with human evaluations of moral clarity, creativity, and narrative coherence in generated fables?
- Basis in paper: "Triangulating LLM-based assessments with human evaluations or crowd-sourced annotations, as done in prior work [15], would provide stronger construct validity."
- Why unresolved: The study relied entirely on a single LLM-based evaluator for subjective dimensions without human validation.
- What evidence would resolve it: Human annotation studies comparing GPT-o3-mini scores against crowd-sourced or expert ratings.

### Open Question 2
- Question: Can models fine-tuned on TF1-EN-3M generalize to nuanced, multi-layered, or culturally diverse moral dilemmas beyond the Western fable tradition?
- Basis in paper: "A model trained solely on TF1-EN-3M might lack the sophistication to tackle nuanced or modern moral issues. Future work should consider incorporating moral principles from diverse philosophical and religious traditions."
- Why unresolved: The prompt elements derive primarily from Western fable traditions, and generated narratives reflect an implicit Western ethical framework.
- What evidence would resolve it: Benchmarks evaluating fine-tuned models on moral reasoning tasks involving ambiguous, multi-cultural, or contemporary ethical scenarios.

### Open Question 3
- Question: Does incorporating dynamic LLM-in-the-loop feedback during generation improve fable quality more efficiently than single-pass generation?
- Basis in paper: "Future systems may incorporate model-in-the-loop feedback loops, where LLMs dynamically revise or critique fables during generation—potentially improving both efficiency and quality."
- Why unresolved: GPT-o3-mini was used only as an offline evaluator; iterative refinement was not explored.
- What evidence would resolve it: Comparative experiments measuring quality scores and computational cost for single-pass vs. iterative critic-generator pipelines.

## Limitations

- The six-slot schema may not generalize to other narrative genres or moral reasoning tasks beyond fable generation.
- LLM-as-judge evaluation introduces potential bias without validation against human raters.
- The paper lacks ablation studies isolating the effect of prompt structure versus model architecture.
- No qualitative analysis of generated fables is provided, making it difficult to assess real-world applicability.

## Confidence

- **High Confidence**: The dataset generation pipeline, GPU cost, and scale are well-specified and reproducible. The composite evaluation methodology is clearly defined.
- **Medium Confidence**: The claim that Llama-3.1-8B-Instruct is the "best" model is supported by the weighted score, but the weighting schema is arbitrary and may not reflect downstream task needs.
- **Low Confidence**: The assertion that the dataset will enable "reproducible research in moral reasoning" is aspirational without downstream task experiments or benchmarks.

## Next Checks

1. **Human Validation of Evaluation**: Run a small-scale human evaluation on a random sample of 100 fables, scoring grammar, creativity, moral clarity, and adherence. Compare these scores to the GPT-o3-mini critic's ratings to assess alignment and potential bias.

2. **Ablation on Prompt Structure**: Generate fables using the same model with (a) full six-slot schema, (b) reduced three-slot schema, and (c) no schema (freeform prompt). Compare composite scores and downstream utility to isolate the effect of prompt structure.

3. **Downstream Task Validation**: Fine-tune a compact language model (1B-3B parameters) on TF1-EN-3M and evaluate its performance on a moral reasoning or instruction-following benchmark. Compare to a baseline model trained on a different narrative dataset to assess practical utility.