---
ver: rpa2
title: 'Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates
  Hallucinations'
arxiv_id: '2511.03578'
source_url: https://arxiv.org/abs/2511.03578
tags:
- entropy
- conservation
- laws
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Constraint-Projected Learning (CPL), a training
  framework that eliminates both hard and soft violations of physical laws in neural
  PDE solvers. CPL projects network predictions onto the lawful manifold defined by
  conservation, Rankine-Hugoniot balance, entropy, and positivity after every gradient
  step, ensuring physical admissibility by construction.
---

# Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations

## Quick Facts
- arXiv ID: 2511.03578
- Source URL: https://arxiv.org/abs/2511.03578
- Authors: Mainak Singha
- Reference count: 13
- Key outcome: Constraint-Projected Learning (CPL) projects network predictions onto the lawful manifold after every gradient step, eliminating hard violations of physical laws (mass creation, entropy decrease, Rankine-Hugoniot imbalance) at machine precision while adding only ~10% computational overhead.

## Executive Summary
This paper introduces Constraint-Projected Learning (CPL), a training framework that eliminates both hard and soft violations of physical laws in neural PDE solvers. CPL projects network predictions onto the lawful manifold defined by conservation, Rankine-Hugoniot balance, entropy, and positivity after every gradient step, ensuring physical admissibility by construction. This projection is differentiable and adds only ~10% computational overhead, making it fully compatible with backpropagation. The method is extended with Total-Variation Damping (TVD) to suppress spurious oscillations and a rollout curriculum to enforce stability over multiple prediction steps. On Burgers and Euler systems, CPL with TVD reduces mass and entropy errors to machine precision, eliminates positive total-variation growth, and keeps long-horizon prediction errors bounded. The approach demonstrates that neural solvers can learn within the laws of physics rather than beside them, achieving stable, accurate, and law-consistent solutions.

## Method Summary
CPL operates by applying a differentiable projection step after each gradient update, enforcing physical constraints through a chain of projectors: Helmholtz (divergence-free), bounds (box constraints), entropy (second law), Rankine-Hugoniot (shock balance), and finite-volume (conservation). The projection is computed via alternating passes or Dykstra's method and is non-expansive. Total-Variation Damping (TVD) is added as a soft constraint to suppress oscillations by penalizing positive growth in total variation, with optional shock sensor masking. A rollout curriculum progressively increases prediction horizon during training to improve long-term stability. The method is validated on 1D Burgers and Euler equations, achieving machine-precision compliance with physical laws while maintaining competitive accuracy.

## Key Results
- CPL with TVD reduces mass and entropy errors to machine precision (~10⁻¹⁰) on Burgers and Euler systems
- Average positive total-variation growth eliminated (ΔTV⁺ = 0.0) while preserving sharp shocks
- 40-step rollout training maintains bounded errors (MSE ≈ 6.0×10⁻⁵) vs 2.8×10⁻⁴ for single-step training
- Computational overhead of projection is ~10%, fully compatible with backpropagation

## Why This Works (Mechanism)

### Mechanism 1: Geometric Projection onto Lawful Manifold
Post-hoc projection of predictions onto the intersection of physical constraint sets eliminates hard violations at machine precision. The projection is computed by composing individual projectors via alternating passes, and each constraint set is assumed convex or locally convex, ensuring non-expansiveness.

### Mechanism 2: Total-Variation Damping (TVD) as Soft Constraint Regularization
Penalizing positive growth in total variation suppresses spurious oscillations without blurring genuine discontinuities. The method adds a loss term max(0, TV(Uⁿ⁺¹) - TV(Uⁿ)) and optionally excludes cells flagged by a shock sensor from the TVD sum.

### Mechanism 3: Rollout Curriculum for Temporal Consistency
Training with progressively longer multi-step rollouts teaches the model to maintain bounded errors over extended horizons. During training, the model predicts R consecutive steps forward, with R increasing linearly from 1 to R_max (e.g., 8).

## Foundational Learning

- **Weak form of PDEs and finite-volume discretization**: CPL operates on discrete cell residuals derived from the weak (integral) form, not pointwise PDE residuals. Understanding how conservation translates to flux balances across cell faces is essential.
  - Quick check: Given a 1D conservation law ∂tU + ∂x f(U) = 0, can you derive the finite-volume residual Rᵢ for cell i with flux Fᵢ₊₁/₂ at faces?

- **Rankine-Hugoniot jump condition**: Shocks require special handling—the RH condition Jf(U)·nK = sₙ JUK ensures correct shock speeds. The paper enforces this as both a loss term and implicit constraint.
  - Quick check: For Burgers' equation with shock speed s, what is the RH condition relating left state U⁻, right state U⁺, and flux f(U) = U²/2?

- **Convex projection and Dykstra's algorithm**: CPL composes multiple projectors; understanding alternating projection methods is necessary for implementing the projection chain and diagnosing convergence issues.
  - Quick check: If projecting onto two convex sets A and B via alternating projections, under what conditions does the sequence converge to a point in A ∩ B?

## Architecture Onboarding

- Component map: Input (x, t) → Neural Network → Raw prediction U_raw → Loss computation (FV residual, entropy, RH, TVD, bounds) → Gradient step (standard backprop) → Projection chain: Π_Helmholtz ∘ Π_bounds ∘ Π_entropy ∘ Π_RH ∘ Π_FV → Constrained prediction U_CPL

- Critical path: Implement finite-volume residual Rᵢ for your PDE, implement individual projectors for each constraint, compose projectors via alternating passes (3-5 iterations typically suffice), add TVD loss with shock sensor masking, wrap in rollout curriculum if long-horizon stability is required.

- Design tradeoffs: Projection overhead (~10%) vs. soft penalty—projection guarantees admissibility but requires differentiable projectors; soft penalties are easier to implement but only encourage compliance. TVD strength vs. shock sharpness—higher w_TVD reduces oscillations more aggressively but may blur shocks; use shock sensor to protect discontinuities. Rollout length vs. training cost—longer rollouts improve temporal stability but increase memory and compute; start with R_max = 4-8.

- Failure signatures: Conservation errors > 10⁻⁶—check that flux computation is consistent across cell faces (telescoping property). Oscillations persist despite TVD—verify shock sensor is not over-masking; check w_TVD is not decayed to zero too early. Projection non-convergence—constraint sets may be conflicting; add relaxation or slack variables. Exploding gradients through projection—ensure each projector has a proper backward pass.

- First 3 experiments: Sanity check—Burgers with CPL only; train on 1D Burgers with FV residual and bounds projection, verify mass drift and RH error reach ~10⁻¹⁰. Add TVD—enable TVD loss with w_TVD = 0.10 and χ_thr = 0.2, confirm average positive TV growth drops to zero. Rollout curriculum—train with R increasing 1→4 over epochs, evaluate on 20-step held-out rollout, compare per-step MSE trajectory to single-step model.

## Open Questions the Paper Calls Out

### Open Question 1
Can the CPL framework be extended to higher-dimensional coupled systems like 3D Euler equations and magnetohydrodynamics (MHD) using only appropriate projectors? The authors state that future extensions will require differentiable projectors for higher-dimensional and coupled systems such as Euler and magnetohydrodynamics. This is unresolved because the paper currently validates the method primarily on 1-D Burgers and Euler systems; extending to multi-dimensions involves complex flux and divergence-free constraints not yet tested.

### Open Question 2
How can the Total-Variation Damping (TVD) mechanism be adapted to strictly preserve sharp discontinuities without introducing excess numerical diffusion? The conclusion identifies the need for more adaptive versions of TVD that preserve sharp discontinuities without excess diffusion. This is unresolved because while the current TVD penalty suppresses oscillations, the authors imply it may act too coarsely, potentially smoothing out genuine physical features that should remain sharp.

### Open Question 3
Do implicit projection solvers improve the efficiency of CPL when applied to stiff source terms or multiscale problems? The authors plan to explore longer rollout curricula and implicit projection solvers to improve efficiency on stiff or multiscale problems. This is unresolved because the current method uses explicit steps and an optional one-step repair, which may become computationally expensive or unstable for stiff chemical reactions or turbulence.

## Limitations
- No specification of neural network architecture or training hyperparameters makes exact reproduction impossible; performance claims depend on these choices
- Projector implementation details (closed-form vs iterative) are not disclosed, affecting both accuracy and computational overhead claims
- Limited ablation studies: only single constraint or design choice isolated in experiments; interactions between constraints not fully explored

## Confidence
- **High confidence**: CPL projection framework works as described; mass and entropy errors reaching machine precision is directly measurable and reproducible
- **Medium confidence**: TVD and rollout curriculum contribute as claimed; classical finite-volume literature supports TVD damping, but corpus lacks neural-specific precedents for explicit TVD losses
- **Low confidence**: Long-horizon stability claims (40-step rollout) without reporting on generalization to longer or different initial conditions; architecture-agnostic scalability not demonstrated

## Next Checks
1. **Architecture ablation**: Compare CPL performance across MLP, FNO, and U-Net backbones on identical problems; verify ~10% overhead claim is architecture-independent
2. **Constraint interaction stress test**: Design degenerate initial conditions where entropy, positivity, and FV constraints are mutually conflicting; measure projection convergence rate and identify failure modes
3. **Rollout generalization**: Evaluate 40-step trained model on 80-100 step inference sequences with unseen initial conditions; quantify error growth rate and detect any stability breakdown