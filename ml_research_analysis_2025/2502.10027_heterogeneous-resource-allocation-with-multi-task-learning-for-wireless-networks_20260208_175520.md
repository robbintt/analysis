---
ver: rpa2
title: Heterogeneous Resource Allocation with Multi-task Learning for Wireless Networks
arxiv_id: '2502.10027'
source_url: https://arxiv.org/abs/2502.10027
tags:
- multi-task
- tasks
- which
- task
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-task learning (MTL) framework to enable
  a single deep neural network (DNN) to jointly solve a range of diverse optimization
  problems in wireless networks. The framework treats optimization problems with varying
  dimensionality values, objectives, and constraints as distinct tasks.
---

# Heterogeneous Resource Allocation with Multi-task Learning for Wireless Networks

## Quick Facts
- arXiv ID: 2502.10027
- Source URL: https://arxiv.org/abs/2502.10027
- Reference count: 40
- Primary result: Multi-task DNN with routing outperforms single-task and naive multi-task approaches on diverse wireless optimization problems

## Executive Summary
This paper proposes a novel multi-task learning framework that enables a single deep neural network to solve diverse wireless optimization problems through conditional computation. The framework introduces a routing network that dynamically selects computational paths within a shared base network, allowing tasks to share parameters where beneficial while maintaining task-specific components to avoid interference. The approach demonstrates significant performance improvements over traditional single-task and naive multi-task methods across 14 different resource allocation problems with varying objectives, constraints, and dimensionalities.

## Method Summary
The proposed framework uses a two-component architecture consisting of a base DNN (bDNN) for extracting solutions and a routing DNN (rDNN) for managing task-specific computational paths. The rDNN takes one-hot task identifiers and outputs binary masks that gate the bDNN weights during forward propagation, creating unique pathways for each task. Training occurs in two phases: first jointly optimizing both networks, then freezing the routing structure and retraining the bDNN alone. This approach handles both supervised learning (for convex optimization problems) and unsupervised learning (for primal-dual constrained optimization) tasks, with applications to FDMA resource allocation problems.

## Key Results
- The proposed MTL framework achieves up to 20% lower MSE than single-task DNNs on supervised learning tasks
- For unsupervised learning tasks, the framework achieves up to 30% higher sum capacity while maintaining constraint satisfaction
- The routing mechanism reduces task interference, with the naive multi-task approach showing 2-10x worse performance on certain tasks

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Path Selection via Routing Network
The rDNN enables a single base DNN to serve multiple heterogeneous tasks by dynamically selecting which computational paths to activate for each task. The rDNN takes a one-hot task identifier vector and outputs binary masks H^{l,i} for each hidden layer. These masks are element-wise multiplied with bDNN weights W^l, effectively zeroing out weights that are not useful for that specific task. This creates unique computational paths through the shared network for each task. If tasks have no shared structure (completely orthogonal optimization landscapes), the routing mechanism would essentially create K independent networks within the bDNN, providing no benefit over separate DNNs.

### Mechanism 2: Hard Parameter Sharing Without Parameter Expansion
The binary masking approach achieves multi-task learning without increasing the total number of trainable parameters compared to a single-task DNN. Unlike soft parameter sharing which maintains separate parameter sets per task with regularization between them, the proposed hard sharing uses binary masks H^{l,i} ∈ {0,1}^{d_{l-1}×d_l} that gate existing bDNN weights. The training parameters for task i are given as {H^{l,i} ⊙ W^l, b^l}, where ⊙ denotes element-wise product. This allows tasks to share parameters when H values are 1 for both tasks, or use independent parameters when H values differ. If tasks require fundamentally different numbers of neurons per layer (not just different active subsets), the fixed bDNN architecture may be under- or over-parameterized for certain tasks.

### Mechanism 3: Two-Phase Training with Router Freezing
Joint training of both networks followed by router-freezing and bDNN retraining improves final task performance compared to single-phase joint training. Phase 1 jointly optimizes Ω = {Θ, Φ} (bDNN and rDNN parameters) using the multi-task loss function. After convergence, the rDNN outputs are binarized via Sign(ReLU(y_{r,i} - 0.5)) and frozen. Phase 2 retrains only the bDNN weights Θ while keeping the routing masks fixed, allowing the bDNN to specialize within the stabilized architecture. If the routing structure has not converged to a stable configuration in Phase 1, freezing it prematurely may lock in suboptimal task pathways.

## Foundational Learning

- **Concept**: Conditional Computation / Dynamic Networks
  - Why needed here: The paper's core innovation is conditional computation—activating different network paths based on task identity. Without understanding this paradigm, the rDNN mechanism will seem unnecessarily complex compared to standard multi-task approaches.
  - Quick check question: Can you explain how conditional computation differs from standard ensemble methods, and why it enables parameter sharing?

- **Concept**: Primal-Dual Optimization for Constrained Problems
  - Why needed here: The unsupervised learning component uses Lagrangian-based loss functions with dual variable updates to handle constraints. Understanding this optimization framework is essential for implementing UL-based tasks correctly.
  - Quick check question: What role do the dual variables λ play in ensuring constraint satisfaction during training, and how does their update differ from standard gradient descent?

- **Concept**: Multi-Task Learning Interference
  - Why needed here: The paper explicitly addresses the "interference" between tasks that share weights. The naive multi-task baseline fails because it cannot manage this interference. Understanding task interference is critical for appreciating why the routing mechanism matters.
  - Quick check question: Why would jointly training two optimization tasks with conflicting objectives (e.g., delay minimization vs. capacity maximization) using shared weights potentially hurt both tasks' performance?

## Architecture Onboarding

- **Component map**:
  - Input -> bDNN (Base DNN) -> Output (sliced to N_i)
  - Input (one-hot task) -> rDNN (Routing DNN) -> Binary masks H^{l,i}
  - bDNN weights ⊙ rDNN masks -> Task-specific subnet

- **Critical path**:
  1. **Data preparation**: Generate datasets D_i for each task (input-output pairs for SL, inputs only for UL). Ensure N_i dimensionalities are known.
  2. **Phase 1 training**: Jointly train bDNN and rDNN using multi-task loss (Eq. 16) with primal-dual updates for UL tasks; sampling tasks uniformly per batch.
  3. **Router finalization**: Apply binarization (Eq. 15) to convert soft masks to hard {0,1} masks. Save routing matrices H^{l,i} for all tasks.
  4. **Phase 2 training**: Reinitialize bDNN weights (Xavier) and retrain with frozen masks using Eq. 19-20.
  5. **Inference**: For task i, forward pass through masked bDNN using precomputed H^{l,i} masks. Input uses first N_i neurons; output reads first N_i neurons.

- **Design tradeoffs**:
  - **bDNN width vs. task coverage**: Larger hidden layers (d_l) allow more task-specific pathways but increase parameters. Paper uses d_2-d_5 = 32 with max input dimension 20.
  - **Number of tasks K vs. interference**: More tasks increase routing complexity and potential interference. Paper tests K=14 tasks (2 problem types × 7 dimensionalities).
  - **γ parameter in tanh activation**: Higher γ better approximates step function but causes vanishing gradients. Paper uses γ=5; Section III-B-1 explains the gradient-zero tradeoff.
  - **Phase 1 vs. Phase 2 iteration split**: Paper uses t_1 = t_2 = 5000; the optimal ratio is not empirically validated.

- **Failure signatures**:
  - **Naive multi-task failure (no rDNN)**: Training loss plateaus early; testing performance shows overfitting-like behavior with poor generalization. Constraint violation remains high for UL tasks.
  - **Zero-padding failure**: Low-dimensional tasks (small N) converge poorly and generalize worse than high-dimensional tasks.
  - **rDNN gradient collapse**: If initialization causes rDNN outputs near 0 or ≥2, gradients vanish. Symptoms: rDNN outputs stuck at all-zeros or all-ones across all tasks.
  - **Routing not converged**: If Phase 1 ends prematurely, masks may not represent meaningful task structure. Look for high variance in performance across random seeds.

- **First 3 experiments**:
  1. **Single-task baseline**: Train separate DNNs for each task using the architecture from Section II. This establishes upper-bound performance that the multi-task approach should approach (within ~5-10% per Table II-III).
  2. **Ablation without routing**: Train the "naive multi-task DNN" where all masks H^{l,i} = 1 (all weights shared). This validates that routing is necessary by showing interference degrades performance significantly—expect 2-10x worse MSE for SL tasks and ~15% lower capacity for UL tasks.
  3. **Routing visualization**: After training, visualize the sparsity pattern of H^{l,i} masks across tasks. Check that tasks with similar objectives (e.g., same problem type, different dimensionality) share more active weights than tasks with different objectives. If all masks look similar, the rDNN is not differentiating tasks properly.

## Open Questions the Paper Calls Out

- **Question**: How does the robustness of the proposed multi-task framework vary when subjected to datasets with missing or corrupted data samples?
  - Basis in paper: [explicit] Page 3 states that while the evaluation assumes a perfect dataset, "The robustness of the proposed framework under missing or corrupted data is left as a future work."
  - Why unresolved: The current study evaluates the architecture only under ideal conditions with clean, complete datasets, which is often not reflective of real-world wireless channel data.
  - What evidence would resolve it: Performance metrics (e.g., MSE, constraint violation) comparing the proposed MTL scheme against benchmarks when trained on datasets with induced noise or missing values.

- **Question**: Can the conditional computation MTL framework be successfully integrated with Graph Neural Networks (GNNs) to improve scalability?
  - Basis in paper: [explicit] Page 12 concludes that "Future directions will aim to integrate the proposed MTL scheme with GNNs for a scalable heterogeneous resource allocation scheme..."
  - Why unresolved: The current paper focuses on fully-connected DNNs, while GNNs are identified as a superior architecture for scalability in related works, but have not yet been combined with the proposed routing mechanism.
  - What evidence would resolve it: A demonstration of a hybrid GNN-MTL architecture handling large-scale network topologies with lower latency or higher throughput than the proposed bDNN/rDNN structure.

- **Question**: Does the use of dynamic task weighting strategies provide performance benefits over the static uniform weighting (β_i = 1/K) utilized in the current loss function?
  - Basis in paper: [inferred] Page 7 sets β_i = 1/K in equation (16). Page 5 identifies "select[ing] the multi-task loss function to regulate the trade-off" as a main challenge, yet the paper employs a fixed weighting scheme without analyzing dynamic alternatives.
  - Why unresolved: Uniform weighting assumes all tasks are equally difficult and important, which may not hold true for heterogeneous optimization problems with varying dimensionality and conflicting objectives.
  - What evidence would resolve it: Comparative results showing convergence rates and final optimality gaps when using adaptive weighting algorithms (e.g., GradNorm) versus the static uniform approach.

## Limitations

- The paper does not explore the sensitivity of the routing mechanism to initialization parameters, particularly γ in the rDNN activation function, which can cause gradient collapse if poorly tuned
- The two-phase training procedure is presented as empirically beneficial but lacks systematic comparison with alternative training schedules or convergence criteria
- The framework's scalability to larger task sets (beyond the 14 tasks tested) is not demonstrated, raising questions about routing complexity and interference management

## Confidence

- **Routing-based parameter sharing**: Medium - Results show clear performance improvements over baselines, but lack extensive ablation studies on architectural choices
- **Two-phase training effectiveness**: Low - The paper reports empirical improvements without systematic exploration of alternative training schedules
- **Optimal architecture design**: Medium - The 7-layer architecture with specific widths appears effective, but sensitivity analysis is limited

## Next Checks

1. **Routing stability analysis**: Test the sensitivity of the routing masks to different random seeds and γ values. Verify that task-specific pathways emerge consistently across multiple training runs.
2. **Single-phase vs. two-phase comparison**: Systematically compare the two-phase training approach against various single-phase alternatives (e.g., warm-starting from Phase 1, alternative binarization schedules) to validate the claimed benefits.
3. **Interference measurement**: Quantify the parameter sharing efficiency by measuring how many weights are truly shared across tasks versus task-specific, and analyze whether this aligns with expected task similarity.