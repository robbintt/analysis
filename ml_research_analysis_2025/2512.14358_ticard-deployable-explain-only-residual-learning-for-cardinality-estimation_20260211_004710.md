---
ver: rpa2
title: 'TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation'
arxiv_id: '2512.14358'
source_url: https://arxiv.org/abs/2512.14358
tags:
- plan
- correction
- ticard
- cardinality
- optimizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TiCard is a correction-based cardinality estimation framework that
  learns multiplicative residual corrections on top of a database's native estimator
  using only EXPLAIN plan features. It improves tail accuracy without replacing the
  optimizer, enabling low-intrusion deployment.
---

# TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation

## Quick Facts
- arXiv ID: 2512.14358
- Source URL: https://arxiv.org/abs/2512.14358
- Reference count: 38
- TiCard reduces P90 Q-error from 312.85 to 13.69 and P99 from 37,974.37 to 3,416.50 using only 157 executions

## Executive Summary
TiCard introduces a correction-based cardinality estimation framework that learns multiplicative residual corrections using only EXPLAIN plan features, avoiding invasive access to database internals. It improves tail accuracy (P90/P99 Q-error) without replacing the native optimizer, making it deployable in production environments with minimal overhead. The system focuses on join operators for conservative deployment while preserving median accuracy, and demonstrates strong results with both Gradient Boosting and TabPFN models on TPC-H and JOB benchmarks.

## Method Summary
TiCard learns to predict log-space multiplicative corrections to a database's native cardinality estimates using only features extracted from EXPLAIN plans. The target is a residual correction factor ŷ(q) ≈ log((1 + C(q)) / (1 + E(q))), where C is actual cardinality and E is the native estimate. Features include plan structure (depth, operator type), native estimates (estRows), and operator attributes, with one-hot encoding for categoricals and standard scaling for numerics. The framework uses query-level splitting to prevent data leakage and selects features via SelectKBest. Two models are evaluated: Gradient Boosting Regressor for inference speed and TabPFN for adaptive accuracy without retraining.

## Key Results
- Reduces P90 Q-error from 312.85 to 13.69 (Gradient Boosting) and P99 from 37,974.37 to 3,416.50 (TabPFN)
- Preserves near-perfect median behavior (1.00 → 1.0078 with join-only correction vs 1.3158 with full correction)
- Achieves quality-band improvements: Excellent+Good rises from 41.86% to 91.09% (TabPFN, join-only)

## Why This Works (Mechanism)

### Mechanism 1
Learning multiplicative residuals in log-space conditionally improves tail accuracy more effectively than direct cardinality prediction. The model predicts a correction factor ŷ(q) ≈ log((1 + C(q)) / (1 + E(q))) rather than raw cardinality C, using the native estimator E as a strong prior and focusing model capacity on systematic correlation errors. This works when the native optimizer provides a structurally sound but biased baseline and the error distribution is correlated with features available in the plan.

### Mechanism 2
An EXPLAIN-only feature pipeline allows for deployable, leakage-free learning without accessing internal database statistics or base tables. Features are extracted solely from query plan structure and the optimizer's own metadata, preventing the model from memorizing specific plan shapes that only appear in the test set. This works when systematic errors are correlated with plan structure and operator types and do not require direct data distribution features to correct.

### Mechanism 3
Conservative "join-only" correction policies preserve median accuracy while reducing tail errors, preventing regression on well-estimated operators. The system applies learned corrections only to join operators where correlation errors are most acute and falls back to native estimates for scans/aggregations. This works when native estimators are generally competent for single-table scans but fail on cross-table correlations.

## Foundational Learning

### Concept: Q-Error (Scale-invariant error metric)
Why needed: Standard absolute error is misleading for cardinality. Q-error (max(est/act, act/est)) measures the multiplicative miss.
Quick check: If a model predicts 100 rows and the actual is 10, what is the Q-error? (Answer: 10)

### Concept: Leakage-free splitting
Why needed: Operators in the same query plan are not independent. Splitting by operator artificially inflates performance.
Quick check: Should you split your dataset by query ID or by operator node? (Answer: By query ID)

### Concept: In-Context Learning (TabPFN)
Why needed: TiCard uses TabPFN to adapt without gradient descent. The model "learns" by loading reference examples into context at inference time.
Quick check: Does updating TabPFN's "setup" data require backpropagation? (Answer: No)

## Architecture Onboarding

### Component map:
Collector -> Featurizer -> Model (GBR/TabPFN) -> Safe Injection Layer

### Critical path:
The Featurizer must generate identical features for both offline training (from logs) and online inference (from live planner) to avoid training-serving skew.

### Design tradeoffs:
- GBR: Sub-millisecond inference but requires retraining on drift
- TabPFN: Higher inference latency (~100ms) but refreshes instantly by swapping reference set
- Scope: Join-only = safer/stable; Full-plan = better tails but higher regression risk

### Failure signatures:
- Semantic Violation: Corrected filter cardinality > input cardinality (must enforce Ĉ_out ≤ Ĉ_in)
- Plan Instability: Small correction changes flip join orders unpredictably
- Zero-divergence: Native estimate is 0, causing instability in log-ratio target

### First 3 experiments:
1. **Offline Validation**: Train on 157 executions. Verify that "Join-only" GBR improves P90 Q-error without degrading Median Q-error compared to Native.
2. **Ablation on Zero-handling**: Check if the "Two-stage" zero classifier reduces false positives on the test set.
3. **Shadow Mode**: Run the model alongside the optimizer. Log the difference between Native estRows and Corrected estRows for production queries to measure potential impact before enabling.

## Open Questions the Paper Calls Out

### Open Question 1
Does TiCard's reduction in operator-level Q-error translate to measurable improvements in end-to-end query latency and plan stability? The paper intentionally excludes end-to-end measurements because they require invasive in-planner integration outside the paper's scope. Evidence would require a shadow-mode deployment study comparing execution times and join-order choices between plans generated with native CE versus TiCard-corrected CE.

### Open Question 2
Can modeling inter-operator dependencies and error propagation improve accuracy compared to TiCard's independent node-level predictions? The current approach predicts per-operator corrections independently, ignoring hierarchical plan structure. Evidence would come from comparing TiCard against structural models (e.g., GNN or TreeLSTM) that encode parent-child relationships.

### Open Question 3
How can the framework be extended with uncertainty estimation to safely handle out-of-distribution (OOD) queries or unstable zero-cardinality predictions? The current framework lacks mechanisms to detect when query patterns deviate significantly from training or when zero-cardinality classifications are uncertain. Evidence would come from ablation studies showing confidence-thresholded fallback mechanisms reduce tail errors on unseen query templates.

## Limitations
- Operator independence: Current approach predicts per-operator corrections independently without modeling error propagation through the plan tree
- Out-of-distribution patterns: Performance degrades on query patterns significantly different from the low-trace training set
- Zero-cardinality handling: Two-stage zero classifier ablations failed to improve accuracy and add complexity

## Confidence
- Core claim (EXPLAIN-only residual learning improves tail accuracy): **High confidence** - directly supported by reported results and prior work
- Distinction from prior approaches (low-intrusion deployment): **High confidence** - well-grounded though novelty could be strengthened
- TabPFN adaptability claims: **Medium confidence** - strong P99 improvement but inference latency uncharacterized
- Zero-handling policy: **Medium confidence** - two-stage classifier adds complexity without clear benefit

## Next Checks
1. **Ablation on Zero-handling**: Verify the "two-stage" zero classifier's impact on false positives/negatives using your production data
2. **Shadow Mode Validation**: Deploy the model in shadow mode to log the delta between native and corrected estimates on live queries before enabling live injection
3. **Feature Completeness Audit**: Audit the feature extraction pipeline to ensure all 10 selected features are consistently available and correctly encoded across both training and inference plan logs