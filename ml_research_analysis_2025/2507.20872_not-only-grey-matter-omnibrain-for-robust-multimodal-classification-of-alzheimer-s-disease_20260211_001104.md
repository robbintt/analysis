---
ver: rpa2
title: 'Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer''s
  Disease'
arxiv_id: '2507.20872'
source_url: https://arxiv.org/abs/2507.20872
tags:
- data
- alzheimer
- disease
- adni
- anmerge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the challenge of accurately diagnosing Alzheimer\u2019\
  s disease using multimodal clinical data, addressing issues of limited generalizability,\
  \ handling missing data, and model interpretability. The proposed OmniBrain framework\
  \ integrates brain MRI, radiomics, gene expression, and clinical data through a\
  \ unified model with cross-attention and modality dropout, allowing effective handling\
  \ of incomplete inputs."
---

# Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease

## Quick Facts
- arXiv ID: 2507.20872
- Source URL: https://arxiv.org/abs/2507.20872
- Reference count: 40
- OmniBrain achieves 92.2±2.4% accuracy on ANMerge, generalizes to ADNI with 70.4±2.7%

## Executive Summary
OmniBrain addresses the challenge of diagnosing Alzheimer's disease using multimodal clinical data while handling missing data and maintaining interpretability. The framework integrates brain MRI, radiomics, gene expression, and clinical data through a unified model with cross-attention and modality dropout. Using foundation model pretraining (AnatCL and y-Aware InfoNCE) for MRI encoding and FT-Transformer for tabular features, OmniBrain achieves state-of-the-art performance and generalizes well across datasets. The approach offers a robust, interpretable solution for real-world Alzheimer's diagnosis.

## Method Summary
OmniBrain is a multimodal classification framework that integrates brain MRI (encoded via AnatCL/y-Aware InfoNCE), radiomics, gene expression, and clinical metadata through cross-attention fusion. The model handles missing data via modality dropout during training, forcing attention mechanisms to redistribute diagnostic reasoning across available modalities. The architecture uses a 3D MRI backbone (AnatCL) and FT-Transformer for tabular features, fused via a 4-head cross-attention layer, with weighted focal loss for imbalanced classes. Training employs 5-fold group cross-validation on ANMerge with systematic modality masking.

## Key Results
- Achieves 92.2±2.4% accuracy on ANMerge 3-class classification (CTL/MCI/AD)
- Generalizes to MRI-only ADNI with 70.4±2.7% accuracy (zero-shot transfer)
- Demonstrates robustness: only 2-3% accuracy drop when modalities are missing at inference
- Attention visualizations highlight neuropathologically relevant brain regions and genes

## Why This Works (Mechanism)

### Mechanism 1: Cross-modal attention fusion
Cross-attention enables dynamic weighting of modalities based on contextual importance rather than treating all inputs equally. Each encoded modality enters a shared attention layer that learns query-key-value projections across modalities, computing attention weights that emphasize diagnostically relevant signals while suppressing noisy channels. When modalities are absent, their attention weights are masked rather than imputed.

### Mechanism 2: Modality dropout training
Random modality masking during training creates a model that maintains performance when subsets of modalities are unavailable at inference. This forces the attention mechanism to learn redundant predictive pathways rather than depending on any single modality, making the model resilient to missing inputs.

### Mechanism 3: Foundation model pretraining
AnatCL and y-Aware InfoNCE pretraining on healthy T1-weighted MRIs provides anatomically meaningful representations that generalize across scanner protocols and patient populations. Contrastive pretraining captures subtle anatomical variations relevant to neurodegeneration, reducing the need for large labeled disease cohorts.

## Foundational Learning

- **Cross-Attention for Multimodal Fusion**: OmniBrain uses cross-attention to integrate MRI and tabular embeddings. Understanding query-key-value projections across modalities is essential for debugging fusion behavior.
  - Quick check: Can you explain why cross-attention differs from simply concatenating modality embeddings?

- **Contrastive Learning Objectives (InfoNCE)**: AnatCL and y-Aware InfoNCE use contrastive pretraining. You need to understand how negative sampling and temperature scaling affect representation quality.
  - Quick check: How does y-Aware InfoNCE incorporate continuous metadata (like age) into its contrastive objective?

- **Handling Missing Data in Neural Networks**: The modality dropout strategy requires understanding how masking differs from imputation, and why reconstruction-free approaches may be preferred.
  - Quick check: Why might learning to redistribute attention be preferable to hallucinating missing modalities?

## Architecture Onboarding

- **Component map**: MRI input → AnatCL encoder → 768-dim embedding; Tabular input → FT-Transformer → embedding; Cross-attention fusion (4 heads) → 3-way classifier
- **Critical path**: MRI preprocessing → SynthSeg segmentation → GM map extraction → AnatCL encoding → FT-Transformer embedding → Cross-attention fusion → Classification head
- **Design tradeoffs**: AnatCL vs. y-Aware (AnatCL slightly outperforms); FT-Transformer vs. XGBoost (FT-Transformer captures interactions natively); Modality dropout rate (too high → underutilizes modalities; too low → brittle)
- **Failure signatures**: Accuracy collapses to ~50% when gene expression is missing (dropout not applied); Grad-CAM shows attention on ventricles (encoder not learning disease features); Cross-dataset performance drops >20% (foundation model not transferring)
- **First 3 experiments**:
  1. Ablation by modality: Train on ANMerge, test with each modality systematically removed. Verify 2-3% accuracy degradation.
  2. Encoder swap: Replace AnatCL with y-Aware InfoNCE. Compare accuracy on ANMerge and ADNI.
  3. Attention visualization: Extract attention weights from fusion layer. Confirm modality weights shift appropriately when inputs are missing.

## Open Questions the Paper Calls Out

### Open Question 1
Do LSM3 and CCDC72 genes have distinct biological roles in Alzheimer's pathology or are they spurious correlations? The discussion notes these genes "have not been previously linked to Alzheimer's disease... and need further investigation." Statistical importance from SHAP scores cannot confirm biological causality without wet-lab validation.

### Open Question 2
Can causal inference methods be integrated to determine if identified biomarkers drive disease progression or merely result from it? The current architecture optimizes for classification accuracy rather than modeling causal directionality. Incorporating causal methods would require architectural modifications or post-hoc analyses.

### Open Question 3
Does OmniBrain maintain state-of-the-art accuracy on cohorts with significantly different demographics and comorbidities than ADNI and ANMerge? The authors acknowledge reliance on these datasets "may not adequately represent the full spectrum of patient demographics... in real-world settings." External validation on diverse datasets is needed.

## Limitations

- Modality dropout specifics not fully specified (exact probability/schedule unclear)
- Foundation model checkpoint details and hyperparameter settings not provided
- Generalizability across diverse populations and scanner protocols remains unproven
- Attention mechanism interpretability not systematically validated against clinical knowledge

## Confidence

- OmniBrain's state-of-the-art accuracy on ANMerge: Medium confidence (5-fold CV but dropout parameters unclear)
- Robustness to missing modalities: Medium confidence (ablation studies show 2-3% degradation but specific dropout parameters unclear)
- Foundation model transfer capabilities: Medium confidence (zero-shot ADNI transfer demonstrated but checkpoint details missing)
- Clinical interpretability of attention maps: Low confidence (visualizations mentioned but not systematically evaluated)

## Next Checks

1. **Modality dropout ablation study**: Systematically vary dropout probability (p=0.1, 0.3, 0.5, 0.7) during training and measure accuracy degradation when each modality is removed at inference to validate the chosen dropout rate.

2. **Cross-dataset generalization stress test**: Evaluate OmniBrain on datasets with different scanner protocols (1.5T vs 3T), preprocessing pipelines, or demographic distributions to quantify true generalization beyond controlled ADNI transfer.

3. **Attention mechanism interpretability validation**: Extract and visualize attention weights from the fusion layer across multiple patients to verify consistent focus on disease-relevant brain regions (hippocampus, entorhinal cortex) rather than technical artifacts.