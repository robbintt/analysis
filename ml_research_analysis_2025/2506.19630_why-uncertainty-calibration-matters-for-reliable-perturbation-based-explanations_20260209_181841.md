---
ver: rpa2
title: Why Uncertainty Calibration Matters for Reliable Perturbation-based Explanations
arxiv_id: '2506.19630'
source_url: https://arxiv.org/abs/2506.19630
tags:
- calibration
- explanations
- perturbation
- explanation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical link between uncertainty calibration
  and perturbation-based explanations, showing that poor calibration under explainability-specific
  perturbations directly undermines explanation quality. The authors introduce ReCalX,
  a recalibration approach that adapts temperature scaling to maintain model performance
  while improving reliability under perturbations.
---

# Why Uncertainty Calibration Matters for Reliable Perturbation-based Explanations

## Quick Facts
- arXiv ID: 2506.19630
- Source URL: https://arxiv.org/abs/2506.19630
- Reference count: 14
- Key outcome: ReCalX reduces calibration error by up to 96% and improves explanation quality, achieving up to 46% better human alignment and better object localization

## Executive Summary
This paper establishes a theoretical link between uncertainty calibration and perturbation-based explanations, showing that poor calibration under explainability-specific perturbations directly undermines explanation quality. The authors introduce ReCalX, a recalibration approach that adapts temperature scaling to maintain model performance while improving reliability under perturbations. Through theoretical analysis and experiments on ImageNet using DenseNet, Vision Transformer, and SigLip models, ReCalX reduces calibration error by up to 96% and improves explanation quality as measured by human alignment (up to 46% improvement) and object localization metrics. The findings demonstrate that recalibration under perturbations produces explanations more aligned with human perception and actual object positions.

## Method Summary
The authors propose ReCalX, a recalibration method that learns bin-specific temperatures for different perturbation levels to reduce calibration error during explanation generation. ReCalX partitions perturbation levels into bins and optimizes temperature scaling per bin on perturbed validation samples, maintaining model performance while improving reliability under perturbations. The method is evaluated on ImageNet using three architectures (DenseNet, ViT, SigLip) with perturbation-based explanations (Shapley Values, LIME) using zero-masking as the baseline perturbation function.

## Key Results
- ReCalX reduces maximum calibration error by up to 96% across all architectures and perturbation levels
- Explanation quality improves with up to 46% better human alignment (DenseNet Shapley) and consistent improvements in object localization
- Models frequently produce unreliable probability estimates when subjected to explainability-specific perturbations, directly undermining explanation quality
- Calibration error tends to increase with higher perturbation strength for uncalibrated models across all architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Calibration error under perturbations directly degrades both global and local explanation quality.
- **Mechanism:** When models face perturbed inputs (feature occlusion/masking), their confidence estimates become misaligned with actual accuracy. These miscalibrated predictions are then aggregated by explanation methods, propagating error into the final attribution scores.
- **Core assumption:** Explanation methods faithfully aggregate model outputs—if the outputs are unreliable, the explanation inherits this unreliability.
- **Evidence anchors:**
  - [abstract] "We show that models frequently produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines explanation quality."
  - [section 3.1, Theorem 3.2] Decomposes predictive power as: perturbation baseline bias + mutual information − calibration error, showing calibration error directly subtracts from explanation fidelity.
  - [section 3.2, Theorem 3.4] Bounds local explanation deviation from ideal by maximum calibration error across perturbation subsets.
  - [corpus] Corpus paper "Uncertainty Propagation in XAI" supports the general principle that uncertainty propagates through explanation functions.

### Mechanism 2
- **Claim:** Miscalibration worsens as perturbation intensity increases.
- **Mechanism:** Perturbations create out-of-distribution inputs. Models, trained on natural images, extrapolate poorly, producing overconfident or underconfident predictions. Higher perturbation levels (more features replaced) increase distribution shift, amplifying calibration error.
- **Core assumption:** Models are trained on unperturbed data and lack exposure to systematic feature ablation during training.
- **Evidence anchors:**
  - [section 5.1, Figure 1] "For uncalibrated models, the calibration error tends to increase with higher perturbation strength across all architectures."
  - [section 1] References prior work showing perturbation-based methods "operate by generating inputs that differ substantially from the training distribution."
  - [corpus] Limited direct corpus evidence on perturbation intensity specifically; primarily anchored in paper's empirical results.

### Mechanism 3
- **Claim:** Adaptive temperature scaling per perturbation level reduces calibration error and improves explanation quality.
- **Mechanism:** Standard temperature scaling uses a single global temperature. ReCalX partitions perturbation levels into bins and learns bin-specific temperatures, optimizing cross-entropy on perturbed validation samples. This local adaptation calibrates confidence across the full perturbation spectrum without changing predicted class rankings.
- **Core assumption:** Temperature scaling is sufficient to correct miscalibration under perturbations; more complex recalibration methods may not be necessary.
- **Evidence anchors:**
  - [section 4] "ReCalXTS aims to reduce the calibration error under all perturbations faced during the explanation process by scaling logits using an adaptive temperature that depends on the perturbation level."
  - [section 5.1] "ReCalXTS substantially reduces the maximal calibration errors across all perturbation levels and architectures. The maximum calibration error drops to 0.014 for DenseNet (96% ↓)."
  - [section 5.2, Tables 1-2] Human alignment improved up to 46% (DenseNet Shapley); localization improved across all models.
  - [corpus] Corpus paper "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration" appears to be a related/identical work (same first author), reinforcing the mechanism.

## Foundational Learning

- **Concept: Uncertainty Calibration (KL-divergence based)**
  - **Why needed here:** The paper uses KL-divergence calibration error (not Expected Calibration Error) because it directly connects to cross-entropy loss and information-theoretic decomposition.
  - **Quick check question:** Given a classifier that outputs [0.9, 0.1] on 100 samples, how many should be correct for the model to be calibrated on that confidence level?

- **Concept: Perturbation-based Explanations (LIME, Shapley Values)**
  - **Why needed here:** These methods aggregate model predictions under feature ablations; understanding their aggregation (linear summary via matrix A) is essential to see how miscalibration propagates.
  - **Quick check question:** Why does replacing features with zeros create out-of-distribution inputs for an ImageNet classifier?

- **Concept: Temperature Scaling**
  - **Why needed here:** ReCalX builds directly on temperature scaling; you must understand how dividing logits by T > 0 flattens/sharpens distributions without changing argmax.
  - **Quick check question:** If original temperature T=1 produces softmax [0.7, 0.3], what happens to these probabilities when T=2?

## Architecture Onboarding

- **Component map:**
  ```
  Input Image → Feature Perturbation (π) → Perturbed Image
                                              ↓
                                    Model Backbone (frozen)
                                              ↓
                                    Logits z(x)
                                              ↓
                           ReCalX: Bin Selection ← Perturbation Level λ(S)
                                              ↓
                           Temperature Tb applied → Calibrated Softmax
                                              ↓
                           Explanation Aggregator (LIME/Shapley) → Attribution Map
  ```

- **Critical path:**
  1. Implement perturbation function (zero-masking baseline)
  2. Compute perturbation level λ(S) = (d − |S|) / d for each subset
  3. Bin perturbation levels (B=10 bins by default)
  4. For each bin, sample M perturbed validation instances and optimize Tb via L-BFGS on cross-entropy
  5. During explanation generation, select Tb based on perturbation level and apply before softmax

- **Design tradeoffs:**
  - **Bins (B):** More bins → finer calibration but more optimization overhead and potential overfitting with sparse samples per bin. Paper uses B=10.
  - **Samples per bin (M):** More samples → robust temperature estimates but longer calibration time. Paper uses M=10.
  - **Validation set size:** Paper uses 200 instances; larger sets improve reliability but increase compute.

- **Failure signatures:**
  - Calibration error remains high after ReCalX → likely insufficient samples per bin or bin boundaries misaligned with perturbation distribution
  - Explanation quality degrades → temperature may be altering relative rankings (should not happen if T>0); verify argmax preservation
  - Localization/human alignment does not improve → perturbation function may not match explanation method's baseline strategy

- **First 3 experiments:**
  1. **Calibration curve validation:** Plot calibration error vs. perturbation level before/after ReCalX on a held-out test set (replicate Figure 1) to verify bin-wise temperature is learning correctly.
  2. **Ablation on bins:** Compare B ∈ {5, 10, 20} to assess sensitivity to bin granularity; monitor if per-bin sample counts drop below ~50.
  3. **Explanation metric baseline:** Compute human alignment (Spearman correlation with attention maps) and localization (bounding box overlap) for uncalibrated vs. ReCalX models on 50-100 images to confirm improvement magnitude matches paper (10-46% relative gains).

## Open Questions the Paper Calls Out

- **Open Question 1:** Does ReCalX yield similar improvements in explanation quality when applied to data modalities other than computer vision, such as natural language or tabular data?
  - **Basis in paper:** [explicit] The conclusion explicitly states, "Future work could... explore other data modalities such as natural language or tabular data."
  - **Why unresolved:** The current experiments are restricted to the ImageNet dataset and image classification architectures (DenseNet, ViT, SigLip).
  - **What evidence would resolve it:** Empirical results from applying ReCalX to text classifiers or tabular models, measuring changes in explanation alignment and localization.

- **Open Question 2:** Can more advanced calibration techniques further enhance explanation reliability compared to the bin-wise temperature scaling utilized in ReCalX?
  - **Basis in paper:** [explicit] The authors suggest that "Future work could investigate if ReCalX benefits from more advanced calibration strategies beyond temperature scaling."
  - **Why unresolved:** The proposed method specifically augments temperature scaling, leaving the potential benefits of non-linear or ensemble calibration methods untested.
  - **What evidence would resolve it:** A comparative study evaluating ReCalX implemented with methods like Platt scaling or beta calibration against the current temperature-based approach.

- **Open Question 3:** Is the efficacy of ReCalX robust to different perturbation strategies (e.g., blurring or noise injection) used during the explanation process?
  - **Basis in paper:** [inferred] The experiments rely exclusively on zero-value baseline replacement, whereas the introduction notes various perturbation strategies exist.
  - **Why unresolved:** It is unclear if recalibrating specifically for zero-masking transfers to other distribution shifts like Gaussian noise or inpainting used in other explanation methods.
  - **What evidence would resolve it:** Experiments measuring calibration error reduction and explanation quality when ReCalX is trained and evaluated using non-zero masking perturbation functions.

## Limitations

- **Core mechanism:** The theoretical link between calibration error and explanation quality is derived under idealized assumptions (Theorem 3.2 assumes calibration error is the dominant source of bias; real-world models may have other confounding factors like feature correlation).
- **Generalizability:** Experiments focus on image classification (ImageNet) and two perturbation-based methods (Shapley, LIME). The paper does not validate ReCalX on NLP or tabular data, nor on other explanation methods like gradient-based approaches.
- **Sample efficiency:** ReCalX requires additional calibration data (200 images, 10 samples per bin per image) and optimization per bin. For very large models or datasets, this overhead could be prohibitive.

## Confidence

- **High confidence:** The empirical results showing calibration error reduction (up to 96%) and improvement in explanation quality (up to 46% human alignment, consistent localization gains) are robust given the controlled experimental setup and multiple model architectures.
- **Medium confidence:** The theoretical derivation that calibration error directly subtracts from explanation fidelity (Theorem 3.2) is sound, but the assumption that perturbation-based explanations are purely linear aggregators of model outputs may not hold in all practical implementations.
- **Low confidence:** The claim that calibration error "always" degrades explanation quality under all perturbation-based methods is overstated; the paper only validates on Shapley and LIME, and does not test gradient-based or attention-based explanation methods.

## Next Checks

1. **ReCalX on a different explanation method:** Apply ReCalX to a gradient-based explanation method (e.g., Integrated Gradients) and measure if calibration error reduction translates to improved explanation metrics. This tests the generalizability of the mechanism beyond Shapley and LIME.

2. **Cross-dataset generalization:** Validate ReCalX on a non-ImageNet dataset (e.g., CIFAR-10 or a tabular dataset) to check if calibration error reduction and explanation quality improvements are consistent across domains.

3. **Calibration error vs. perturbation function:** Systematically vary the perturbation function (e.g., Gaussian noise vs. zero masking) and measure how calibration error and explanation quality change. This tests whether the mechanism is robust to the choice of perturbation strategy.