---
ver: rpa2
title: Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal
  Reasoning
arxiv_id: '2602.01983'
source_url: https://arxiv.org/abs/2602.01983
tags:
- tool
- reasoning
- tools
- arxiv
- loop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UCT, a training-free framework that transforms
  agents from tool users to tool creators by harvesting reasoning experiences and
  distilling them into reusable assets. The method enables adaptive tool creation
  and self-updating during inference through an online task loop, online build loop,
  and offline memory consolidation module.
---

# Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning

## Quick Facts
- arXiv ID: 2602.01983
- Source URL: https://arxiv.org/abs/2602.01983
- Reference count: 40
- Primary result: +20.86% and +23.04% improvements on benchmarks for mathematical and scientific reasoning tasks

## Executive Summary
This paper introduces UCT, a training-free framework that transforms agents from tool users to tool creators by harvesting reasoning experiences and distilling them into reusable assets. The method enables adaptive tool creation and self-updating during inference through an online task loop, online build loop, and offline memory consolidation module. Extensive experiments demonstrate state-of-the-art performance across multiple domains, achieving significant improvements on mathematical and scientific reasoning benchmarks.

## Method Summary
UCT operates through three integrated modules: (1) Online Task Loop - a ReAct-based agent handling queries, retrieving tools, and triggering creation when needed; (2) Online Build Loop - isolated code generation with sandbox testing and critic model review for iterative refinement; (3) Offline Memory Consolidation - asynchronous process reading usage logs to merge, deduplicate, and prune tools. The framework uses Qwen3-VL-235B-Thinking as the base model and operates on TRBench, a curated dataset of 959 instances across math, science, and visual question answering tasks.

## Key Results
- Achieves +20.86% and +23.04% improvements on mathematical and scientific reasoning benchmarks
- Demonstrates high tool reuse rates: 93.1% of tools used ≥1 time, 77.1% used ≥10 times
- Outperforms existing methods across multiple domains including VQA and multimodal reasoning

## Why This Works (Mechanism)

### Mechanism 1
Isolated tool creation with dual verification improves tool quality and system stability. When a missing tool is requested, a separate Online Build Loop generates code that undergoes sandbox testing AND critic-model review. Failures feed back into the ReAct model for iterative refinement until both checks pass.

### Mechanism 2
Offline memory consolidation prevents retrieval degradation as the tool library grows. The offline module merges similar tools, removes duplicates, and deprecates low-utility tools based on usage logs, maintaining retrieval efficiency without adding latency to online inference.

### Mechanism 3
Reusing successful tool creation experiences improves downstream task accuracy. Successfully created tools are registered to a persistent Tool Library, and high reuse rates suggest tools generalize beyond their originating task.

## Foundational Learning

- **ReAct Paradigm (Reasoning + Acting)**
  - Why needed here: The entire Online Task Loop is built on ReAct, where the model alternates between generating thoughts (reasoning traces) and executing actions (tool calls).
  - Quick check question: Can you explain how a ReAct agent decides when to think versus when to act?

- **Tool-Integrated Reasoning (TIR)**
  - Why needed here: UCT extends TIR by making the tool set dynamic rather than fixed; understanding TIR basics clarifies what UCT adds.
  - Quick check question: What limitation of fixed toolsets does TIR address, and what limitation does UCT address beyond TIR?

- **Sandbox Execution Environments**
  - Why needed here: The Build Loop relies on sandboxed code execution to safely test generated tools before registration.
  - Quick check question: Why must tool testing occur in isolation from the main inference environment?

## Architecture Onboarding

- **Component map:**
  - Query → Task Loop detects missing tool → Build Ticket issued → Build Loop generates/tests/reviews → Tool registered → Task Loop resumes with new tool

- **Critical path:** Query → Task Loop detects missing tool → Build Ticket issued → Build Loop generates/tests/reviews → Tool registered → Task Loop resumes with new tool

- **Design tradeoffs:**
  - Isolation vs. context: Build Loop is isolated to prevent context bloat, but cannot access full task context
  - Online speed vs. library quality: Consolidation is offline to avoid latency, but library may temporarily accumulate redundancy
  - Generality vs. specificity: Tools must be abstract enough to reuse but specific enough to be useful

- **Failure signatures:**
  - Tool creation loops without convergence (check iteration caps)
  - Retrieval returns irrelevant tools (consolidation may be overdue)
  - Newly created tools pass tests but fail on edge cases in production (sandbox coverage gaps)

- **First 3 experiments:**
  1. Replicate the ablation in Table 2: run with Build Loop only (no critic), then add critic, then add consolidation—confirm each contributes to the 60.17 → 83.21 improvement trajectory.
  2. Measure retrieval latency as library size grows (simulate 50, 100, 200 tools) to validate that offline consolidation maintains sub-linear retrieval cost.
  3. Test tool reuse on held-out tasks: train/create tools on Math subset, evaluate reuse rate and accuracy on Science subset to assess cross-domain generalization.

## Open Questions the Paper Calls Out

### Open Question 1
How can the self-evolution process be sustained beyond the plateau observed when problem-type diversity is exhausted in a given dataset? The paper demonstrates plateauing but does not propose mechanisms to continuously generate novel tool requirements or transfer learning across domains.

### Open Question 2
What is the optimal scheduling strategy for offline memory consolidation to balance retrieval efficiency against integration latency? The paper relegates consolidation to an offline phase but does not quantify the trade-off between consolidation frequency and system performance.

### Open Question 3
How robust is the critic model verification against adversarial tool generation or subtle semantic errors over long-term evolution? The paper does not analyze failure modes where tools pass verification but introduce subtle errors into the tool library.

## Limitations

- Critic model implementation details are unspecified, making faithful reproduction challenging
- No explicit iteration caps or fallback strategies for build loop convergence
- Offline memory consolidation algorithm lacks implementation details for clustering and deprecation

## Confidence

**High Confidence**: The core ReAct-based Online Task Loop mechanism is well-specified with clear equations and component descriptions.

**Medium Confidence**: The improvement claims (+20.86% and +23.04% on benchmarks) are credible given the comprehensive experimental setup and comparison against strong baselines.

**Low Confidence**: Claims about cross-domain tool generalization (77.1% reuse@10) are supported by statistics but lack detailed analysis of transfer success conditions.

## Next Checks

1. **Ablation Validation**: Replicate the component ablation study by running experiments with Build Loop only (no critic), then add critic, then add consolidation to verify the improvement trajectory.

2. **Library Growth Analysis**: Simulate tool library expansion by running on increasing subsets (50, 100, 200 tools) and measure retrieval latency to confirm offline consolidation maintains sub-linear retrieval cost.

3. **Cross-Domain Generalization Test**: Create tools on the Math subset of TRBench, then evaluate their reuse rate and accuracy on the Science subset to measure whether the 77.1% reuse@10 statistic holds.