---
ver: rpa2
title: Scalable Offline ASR for Command-Style Dictation in Courtrooms
arxiv_id: '2507.01021'
source_url: https://arxiv.org/abs/2507.01021
tags:
- processing
- audio
- segments
- system
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an open-source ASR framework for command-style
  dictation in courtrooms, addressing the challenge of achieving low-latency transcription
  at scale. The core method decouples Voice Activity Detection (VAD) from transcription
  and employs parallel, multiplexed inference across concurrent users.
---

# Scalable Offline ASR for Command-Style Dictation in Courtrooms

## Quick Facts
- arXiv ID: 2507.01021
- Source URL: https://arxiv.org/abs/2507.01021
- Authors: Kumarmanas Nethil; Vaibhav Mishra; Kriti Anandan; Kavya Manohar
- Reference count: 0
- One-line primary result: Multiplexed batch processing reduces p90 latency by up to 26% compared to sequential processing at 20 concurrent users

## Executive Summary
This paper presents an open-source ASR framework for command-style dictation in courtrooms, addressing the challenge of achieving low-latency transcription at scale. The core method decouples Voice Activity Detection (VAD) from transcription and employs parallel, multiplexed inference across concurrent users. By segmenting audio into self-contained speech units and processing them in shared batches, the system maximizes GPU utilization without per-user overhead. Evaluations using live courtroom recordings show that multiplexed processing consistently outperforms sequential batch processing. At 20 concurrent users, p90 latency improved by up to 26% (from ~13.5s to ~10s) for long audio clips (105–120s), with benefits scaling with concurrency. The system is deployed in ~15% of India's courtrooms and supports both Whisper and CTC-based ASR models. Future work will refine batching, VAD alignment, and dynamic resource allocation. A live demo at Interspeech will showcase real-time interaction and performance under load.

## Method Summary
The system segments audio using Silero VAD into 3-30 second chunks with 300ms silence padding, creating independent linguistic units. A centralized priority queue aggregates these segments from multiple users, enabling shared batch inference. Segments are stacked for parallel feature extraction and encoding, then decoded in parallel using Whisper models with disabled timestamps and prompt tokens for efficiency. The approach is evaluated on 100 authentic courtroom recordings at concurrency levels of 5, 10, and 20 users on a single NVIDIA T4 GPU.

## Key Results
- Multiplexed processing consistently outperforms sequential batch processing
- At 20 concurrent users, p90 latency improved by up to 26% (from ~13.5s to ~10s) for long audio clips (105–120s)
- Benefits scale with concurrency, with improvements increasing from 14% at C=5 to 26% at C=20
- System supports both Whisper and CTC-based ASR models

## Why This Works (Mechanism)

### Mechanism 1: VAD-Based Segmentation Creates Processable Units
- Claim: Segmenting audio via VAD enables independent processing of speech chunks that would otherwise require sequential handling.
- Mechanism: Silero VAD produces segments between 3-30 seconds with 300ms silence padding, creating self-contained linguistic units. These segments can be processed without waiting for preceding audio to complete.
- Core assumption: Properly segmented commands do not require inter-segment context for accurate transcription.
- Evidence anchors:
  - [abstract]: "Our approach uses Voice Activity Detection (VAD) to segment audio and transcribes these segments in parallel"
  - [section 2.1]: "Proper VAD segmentation is crucial as it creates self-contained linguistic units that can be processed independently, directly enabling the parallel inference that our system depends on."
  - [corpus]: Limited direct support; WhisperKit addresses real-time ASR but focuses on on-device optimization rather than VAD-driven segmentation strategies.
- Break condition: When speech contains strong cross-segment contextual dependencies (e.g., coreference chains, incomplete syntactic structures spanning segment boundaries), transcription quality may degrade.

### Mechanism 2: Multiplexing Aggregates Idle Compute Across Users
- Claim: Centralized queue-based batching improves GPU utilization by filling compute gaps from individual users' silence periods.
- Mechanism: A priority queue aggregates segments from all active users. Dynamic or continuous batching collects segments into shared inference batches, overlapping computation across users rather than processing each user's audio sequentially.
- Core assumption: Individual users have irregular dictation patterns with natural pauses; aggregated demand smooths utilization.
- Evidence anchors:
  - [abstract]: "multiplexing strategy that aggregates segments from multiple users into shared inference batches"
  - [section 2.2]: "This multiplexing significantly improves efficiency in multi-user environments by maintaining high GPU utilization even when individual users have irregular dictation patterns."
  - [corpus]: Semantic Multiplexing paper addresses parallel task offloading in wireless edge contexts but is not directly comparable to ASR batching.
- Break condition: Under synchronized burst patterns (all users speaking simultaneously), queue depth may exceed capacity, increasing p90 latency.

### Mechanism 3: Parallel Encoder Pass Reduces Per-Segment Overhead
- Claim: Stacking feature extraction and batch encoding reduces per-segment fixed costs compared to sequential processing.
- Mechanism: Audio chunks are padded/trimmed, features are stacked via `torch.stack`, encoder processes all segments in a single pass, then decoder generates transcriptions with shared prompt tokens. Timestamp generation is disabled for faster inference.
- Core assumption: Disabling timestamps and using consistent prompt tokens does not materially degrade transcription accuracy for command-style dictation.
- Evidence anchors:
  - [abstract]: "transcribes these segments in parallel using Whisper models"
  - [section 2.3]: Code listing shows `torch.stack` for features and batch `model.encode(features)` followed by parallel `model.model.generate()`; text notes "sidestepping the sequential dependencies in autoregressive models."
  - [corpus]: Whisfusion paper explicitly notes that "truly parallel ASR decoding remains challenging due to the sequential nature of autoregressive (AR) decoders," confirming the architectural constraint this design works around.
- Break condition: If downstream applications require word-level timestamps or cross-segment context is needed for accuracy, this optimization becomes unsuitable.

## Foundational Learning

- Concept: Voice Activity Detection (VAD) and Segment Boundary Effects
  - Why needed here: The entire multiplexing strategy depends on VAD creating clean, processable segments. Mis-segmentation propagates errors downstream.
  - Quick check question: If VAD cuts mid-word due to insufficient padding, what happens to transcription accuracy at segment edges?

- Concept: Batch vs. Streaming ASR Tradeoffs
  - Why needed here: This system positions itself between high-latency batch processing and resource-intensive streaming ASR.
  - Quick check question: Why does sequential batch processing create queuing delays when multiple users submit audio simultaneously?

- Concept: GPU Utilization and Batch Efficiency
  - Why needed here: Multiplexing aims to maximize GPU utilization by aggregating segments; understanding this requires knowing how batch size affects throughput.
  - Quick check question: What happens to GPU occupancy when processing a single 3-second segment versus a batch of 20 segments?

## Architecture Onboarding

- Component map:
  - Silero VAD → Segment extraction (3-30s, 300ms padding)
  - Priority Queue → Aggregates segments across users
  - Batching Engine → Dynamic or continuous batching
  - Whisper/CTC Encoder → Parallel feature extraction + encoding
  - Decoder → Parallel transcription generation (timestamps disabled)
  - Output Router → Returns results to appropriate user streams

- Critical path:
  1. Audio input arrives per-user
  2. VAD segments audio into discrete chunks
  3. Segments enter shared priority queue
  4. Batching engine collects segments into inference batch
  5. Encoder processes batch in parallel
  6. Decoder generates transcriptions
  7. Results routed back to user-specific output streams

- Design tradeoffs:
  - Segment length vs. latency: Longer segments improve model accuracy but increase wait time before processing begins.
  - Dynamic vs. continuous batching: Dynamic adapts to segment length and wait times; continuous maintains minimum batch size for stable GPU utilization.
  - Timestamps disabled: Faster inference but unsuitable for alignment-sensitive applications.

- Failure signatures:
  - Queue overflow: Sustained high concurrency with few natural pauses; p90 latency spikes.
  - VAD edge errors: Transcription artifacts at segment boundaries indicate insufficient padding.
  - GPU underutilization: Batch sizes too small or segments too short to saturate compute.
  - Memory pressure: Long segments from many concurrent users exceed VRAM.

- First 3 experiments:
  1. Reproduce latency curve: Run 100 courtroom recordings at C=5, 10, 20 concurrency levels; measure p90 latency against sequential baseline to validate reported 14-26% improvements.
  2. VAD padding sweep: Test padding values (0ms, 150ms, 300ms, 500ms) and measure WER at segment boundaries to quantify quality-latency tradeoff.
  3. Batch size threshold test: Vary minimum batch size in continuous batching mode and measure GPU utilization vs. per-user latency to identify optimal operating point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What batching heuristics (e.g., segment length uniformity, user priority aging, predicted decode time) can further reduce p90 latency beyond the current dynamic/continuous batching approach?
- Basis in paper: [explicit] Authors state future work will focus on "smarter batching strategies in the multiplexing queue."
- Why unresolved: The paper evaluates dynamic and continuous batching but does not explore optimization criteria within the priority queue.
- What evidence would resolve it: Ablation experiments comparing alternative batching policies (length-binned, wait-time-weighted, hybrid) on the same 100 courtroom recordings with latency and GPU utilization metrics.

### Open Question 2
- Question: How can VAD-postprocessing algorithms improve segment boundary alignment with linguistic units (e.g., clause/sentence boundaries) to reduce truncation artifacts?
- Basis in paper: [explicit] Future directions include "improved VAD-postprocessing to better align segment boundaries."
- Why unresolved: The current Silero VAD uses fixed duration constraints (3–30s) and 300ms padding without explicit linguistic boundary detection; the paper does not evaluate boundary accuracy or its impact on WER.
- What evidence would resolve it: Comparison of WER and semantic completeness between baseline VAD segments and linguistically-informed segmentation, along with latency impact.

### Open Question 3
- Question: To what extent does the absence of inter-segment context affect transcription accuracy for utterances spanning VAD boundaries?
- Basis in paper: [inferred] The paper claims "properly segmented commands don't require inter-segment context" but provides no quantitative validation of this assumption.
- Why unresolved: The evaluation focuses on latency, not on whether critical context (e.g., carried-over topics, partial words at boundaries) degrades accuracy.
- What evidence would resolve it: Paired WER analysis on segments artificially split at natural boundaries vs. forced mid-utterance splits, controlling for segment length.

### Open Question 4
- Question: How does multiplexing efficiency scale across heterogeneous hardware (multi-GPU, edge devices) and under bursty traffic patterns beyond C=20?
- Basis in paper: [inferred] All experiments use a single T4 GPU; future work mentions "dynamic resource allocation for large-scale deployments with bursty user patterns."
- Why unresolved: The latency improvements are hardware-specific and tested only up to 20 concurrent users with steady load; generalizability is unknown.
- What evidence would resolve it: Benchmarks on multi-GPU servers and lower-power edge hardware under synthetic burst traffic (e.g., Poisson arrivals with varying concurrency peaks).

## Limitations
- Evaluation limited to single GPU type (NVIDIA T4) and specific model variant (Whisper) without reported accuracy metrics (WER/CER)
- Scalability claims based on concurrency up to 20 users without testing higher loads or bursty traffic patterns
- No detailed analysis of VAD segmentation quality or its impact on transcription accuracy at segment boundaries

## Confidence

- **High confidence** in the core architectural claim that VAD-based segmentation combined with multiplexed batch inference improves GPU utilization and reduces p90 latency compared to sequential processing.
- **Medium confidence** in the system's real-world applicability and robustness, given deployment in ~15% of India's courtrooms but limited evaluation scope.
- **Low confidence** in claims regarding optimal batching strategies and the impact of disabling timestamps due to lack of empirical evidence.

## Next Checks

1. **Reproduce Latency Curve with Quality Metrics**: Conduct a controlled experiment using the same Whisper model (or a specified variant) and the exact VAD parameters (3-30s segments, 300ms padding) on the reported courtroom recordings. Measure not only p90 latency at concurrency levels C=5, 10, 20 but also WER and CER to validate both the speed and accuracy claims. Compare against a sequential baseline with timestamps enabled to assess the quality-latency tradeoff.

2. **VAD Segmentation Quality Analysis**: Perform a detailed analysis of VAD segmentation quality by testing different padding values (0ms, 150ms, 300ms, 500ms) on a subset of recordings. Measure WER at segment boundaries for each padding value to identify the optimal tradeoff between minimizing latency and preserving transcription quality. Include a case study of recordings with high cross-segment dependencies to assess the impact of segmentation on accuracy.

3. **Batch Strategy and Resource Utilization Study**: Conduct a systematic study varying minimum batch size in continuous batching mode and the dynamic batching thresholds. Measure GPU utilization, per-user latency, and memory usage across different configurations. Identify the optimal operating point that maximizes throughput while maintaining acceptable latency. Additionally, test the system on a different GPU type (e.g., A100) to assess scalability and resource efficiency.