---
ver: rpa2
title: Massively Parallel Expectation Maximization For Approximate Posteriors
arxiv_id: '2503.08264'
source_url: https://arxiv.org/abs/2503.08264
tags:
- posterior
- approximate
- massively
- parallel
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QEM, a gradient-free Bayesian inference method
  that uses massively parallel importance weighting (MPIW) to learn approximate posteriors
  for hierarchical models. The key innovation is using MPIW posterior moment estimates
  to directly update exponential family approximate posteriors through an EM-style
  algorithm, avoiding gradient-based optimization entirely.
---

# Massively Parallel Expectation Maximization For Approximate Posteriors

## Quick Facts
- **arXiv ID:** 2503.08264
- **Source URL:** https://arxiv.org/abs/2503.08264
- **Authors:** Thomas Heap; Sam Bowyer; Laurence Aitchison
- **Reference count:** 40
- **Primary result:** QEM achieves faster convergence and higher ELBOs than massively parallel VI and RWS on five benchmark datasets while being invariant to model reparameterizations.

## Executive Summary
This paper introduces QEM (Quasi-EM), a gradient-free Bayesian inference method that uses massively parallel importance weighting (MPIW) to learn approximate posteriors for hierarchical models. The key innovation is using MPIW posterior moment estimates to directly update exponential family approximate posterizations through an EM-style algorithm, avoiding gradient-based optimization entirely. The authors demonstrate that QEM converges faster than state-of-the-art massively parallel VI and RWS on five datasets (Bus Breakdown, MovieLens, Occupancy, Radon, and Covid), achieving higher ELBOs and predictive log-likelihoods. Critically, QEM is invariant to model reparameterizations that dramatically slow down gradient-based methods.

## Method Summary
QEM is a gradient-free Bayesian inference algorithm that combines importance sampling with Expectation-Maximization updates for exponential family approximations. The method generates samples from the prior and computes importance weights using the joint probability of latent variables and observations. These weights are then used to compute posterior moment estimates via massively parallel importance weighting (MPIW). The algorithm iteratively updates the parameters of the approximate posterior by matching these MPIW-estimated moments to the exponential family's natural parameters. This approach avoids the need for gradients or backpropagation, instead relying on direct moment matching that is invariant to model reparameterizations.

## Key Results
- QEM converges faster than massively parallel VI and RWS on five benchmark datasets
- QEM achieves higher ELBOs and predictive log-likelihoods compared to competing methods
- QEM demonstrates reparameterization invariance, maintaining performance when models are transformed
- MPIW provides superior moment estimates compared to standard VI approaches
- QEM matches RWS performance while avoiding gradient-based optimization entirely

## Why This Works (Mechanism)
QEM works by leveraging the strengths of importance sampling for posterior estimation while using the EM framework for efficient parameter updates. The massively parallel importance weighting allows for accurate estimation of posterior moments across all latent variables simultaneously, without requiring gradient information. By directly matching these moments to the exponential family's natural parameters, the method achieves stable and efficient updates that are not affected by model reparameterizations. The gradient-free nature eliminates sensitivity to learning rates and gradient noise that plague VI methods.

## Foundational Learning

**Importance Sampling**: A Monte Carlo method for estimating expectations under complex distributions by reweighting samples from a proposal distribution. *Why needed*: Provides the foundation for computing posterior expectations without requiring the true posterior. *Quick check*: Verify that weights sum to the marginal likelihood estimate.

**Exponential Family Distributions**: A class of distributions with natural parameters and sufficient statistics that enable conjugate updates. *Why needed*: Allows for closed-form moment matching in the M-step of QEM. *Quick check*: Confirm the approximate posterior belongs to the exponential family.

**EM Algorithm**: An iterative optimization method that alternates between computing expectations (E-step) and maximizing parameters (M-step). *Why needed*: Provides the framework for updating approximate posterior parameters using MPIW estimates. *Quick check*: Ensure E and M steps are correctly implemented.

**Posterior Moment Matching**: The process of updating parameters to match empirical moments from samples. *Why needed*: Enables direct parameter updates without gradient information. *Quick check*: Verify moments computed from samples match theoretical expectations.

**Massively Parallel Computation**: Distributing computations across multiple processors or devices. *Why needed*: Makes MPIW feasible by parallelizing weight computation and moment estimation. *Quick check*: Confirm parallel implementation scales with available resources.

## Architecture Onboarding

**Component Map**: Prior sampling -> Importance weight computation -> MPIW moment estimation -> Exponential family parameter update -> Repeat

**Critical Path**: The algorithm follows a clear iterative loop: sample from prior → compute importance weights → estimate posterior moments via MPIW → update approximate posterior parameters → repeat until convergence.

**Design Tradeoffs**: Memory vs. accuracy tradeoff where storing more samples improves moment estimates but increases memory consumption exponentially with latent dimensions. The method trades computational simplicity (no gradients) for memory efficiency.

**Failure Signatures**: Poor convergence when importance weights have high variance, indicating the prior is a poor proposal distribution. Memory overflow when latent dimension is large due to exponential scaling. Suboptimal performance when the true posterior is far from the chosen exponential family.

**First Experiments**: 1) Run QEM on a simple Gaussian model with known posterior to verify correctness. 2) Compare QEM against VI on a reparameterized version of the same model to test invariance. 3) Scale up to a hierarchical model with 5-10 latent variables to test memory scaling.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Exponential memory scaling with number of latent variables limits applicability to high-dimensional latent spaces
- Performance depends heavily on the quality of importance weights, which can have high variance
- Limited validation across diverse model architectures beyond the five benchmark datasets
- No solution proposed for handling the memory bottleneck in models with many latent variables

## Confidence

**High confidence**: QEM's reparameterization invariance property and superior moment estimates compared to VI are well-supported by empirical results and mathematical derivation.

**Medium confidence**: Faster convergence claims compared to massively parallel VI and RWS are supported by experimental results but limited to five datasets.

**Low confidence**: Assertion of "comparable" performance to RWS across all scenarios needs more extensive validation across diverse model architectures.

## Next Checks
1. Test QEM on hierarchical models with more than 10 latent variables to quantify memory scaling limitations and identify breaking points
2. Compare QEM against gradient-based VI methods on reparameterized versions of the same models to verify claimed invariance advantage
3. Evaluate QEM's performance on time-series or spatial models where latent dimension naturally grows with data size