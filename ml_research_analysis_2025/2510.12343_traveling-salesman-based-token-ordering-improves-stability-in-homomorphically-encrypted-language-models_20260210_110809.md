---
ver: rpa2
title: Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically
  Encrypted Language Models
arxiv_id: '2510.12343'
source_url: https://arxiv.org/abs/2510.12343
tags:
- text
- token
- vector
- language
- encrypted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of stable text generation under
  homomorphic encryption, where inexact sampling leads to corrupted outputs. The authors
  propose a TSP-based token reordering strategy that places semantically similar tokens
  adjacently, combined with a post-processing step that enhances one-hotness of sampling
  results.
---

# Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models

## Quick Facts
- arXiv ID: 2510.12343
- Source URL: https://arxiv.org/abs/2510.12343
- Reference count: 40
- Primary result: TSP-based token reordering + post-processing reduces corruption scores from 1.5 to 0.5 and corruption ratios from 19% to near 0% with 60 steps of domain fine-tuning

## Executive Summary
This paper addresses the challenge of stable text generation under homomorphic encryption, where inexact sampling leads to corrupted outputs. The authors propose a TSP-based token reordering strategy that places semantically similar tokens adjacently, combined with a post-processing step that enhances one-hotness of sampling results. Theoretical analysis bounds the approximation errors, while experiments show that their method reduces corruption scores from 1.5 to 0.5 and corruption ratios from 19% to near 0% when fine-tuned with 60 steps. The approach enables privacy-preserving LLM inference with text quality approaching unencrypted baselines.

## Method Summary
The method combines three key components: (1) TSP-based token reordering that permutes the embedding matrix so semantically similar tokens are adjacent, reducing semantic corruption when sampling errors produce weighted combinations of nearby embeddings; (2) a post-processing polynomial PP(x) = -2x³ + 3x² that sharpens approximate one-hot vectors toward true one-hotness with improved error bounds from O(ε) to O(ε²); and (3) domain-specific fine-tuning with LoRA that narrows token probability distributions, reducing the effective error surface for approximate sampling. The approach is implemented as a CKKS-compatible inverse transform sampling algorithm with polynomial Heaviside approximation.

## Key Results
- Corruption score reduced from 1.5 to 0.5 when combining TSP reordering, post-processing, and 60 steps of fine-tuning
- Corruption ratio decreased from 19% to near 0% with 60+ steps of fine-tuning and TSP+PP enabled
- Average cosine similarity between adjacent tokens increased from 0.024 to 0.271 after TSP reordering on Llama-2-7b-hf

## Why This Works (Mechanism)

### Mechanism 1
TSP-based token reordering reduces semantic corruption when approximate one-hot vectors multiply embedding matrices. The algorithm solves a Traveling Salesman Problem over the vocabulary where edge weights are cosine distances between token embeddings. This permutes the embedding matrix rows so adjacent indices contain semantically similar tokens. When sampling errors cause a weighted combination of nearby embeddings (indices i-m to i+m), the result remains semantically coherent because those neighbors share meaning.

### Mechanism 2
Post-processing polynomial PP(x) = -2x³ + 3x² sharpens approximate one-hot vectors toward true one-hotness. The cubic polynomial has fixed points at x = 0 and x = 1 with derivative zero at both endpoints. Values near 0 are pushed closer to 0; values near 1 are pushed closer to 1. Theoretical analysis shows error bound improves from O(ε) to O(ε²) after post-processing.

### Mechanism 3
Domain-specific fine-tuning narrows token probability distributions, reducing the effective error surface for approximate sampling. Fine-tuning with LoRA (rank 2, 60 steps) causes probability mass to concentrate on fewer tokens, increasing zero-probability token ratios. This mimics top-p/k thresholding without requiring explicit comparisons under HE.

## Foundational Learning

- Concept: CKKS Homomorphic Encryption Scheme
  - Why needed here: CKKS enables approximate arithmetic on encrypted real/complex numbers via SIMD operations (addition, multiplication, rotation). Understanding these primitives explains why comparison-based sampling is problematic.
  - Quick check question: Can you explain why argmax requires polynomial approximation under CKKS and why this introduces bounded errors?

- Concept: Inverse Transform Sampling
  - Why needed here: The sampling algorithm uses CDF inversion with polynomial Heaviside approximation. Understanding ITS connects the probability vector to the index selection mechanism.
  - Quick check question: Given a probability vector [0.2, 0.3, 0.5] and uniform sample r = 0.6, which index does exact ITS select? What about r = 0.55?

- Concept: Approximation Error Bounds (Theorem 5)
  - Why needed here: The paper proves bounded errors under "good events" (r not near CDF discontinuities). This theoretical grounding validates practical deployment.
  - Quick check question: What happens to error bounds when the uniform sample r falls within δ of a CDF threshold?

## Architecture Onboarding

- Component map: TSP Preprocessor -> Inverse Transform Sampler -> Post-Processor -> Embedding Matrix Multiplication -> Next Layer Input
- Critical path: TSP reordering → embedding matrix permutation → inverse transform sampling → post-processing → embedding matrix multiplication → next layer input
- Design tradeoffs:
  - TSP approximation quality vs. preprocessing time (nearest neighbor vs. optimal solver)
  - Heaviside polynomial degree vs. multiplicative depth consumption vs. approximation accuracy (ε, δ parameters)
  - Fine-tuning steps vs. domain specificity vs. generalization
- Failure signatures:
  - "MS" token repetition (corruption score 4) indicates embedding vector collapse
  - High corruption ratio (>10%) suggests TSP ordering not applied or fine-tuning insufficient
  - Post-processing sum far from 1.0 indicates Heaviside approximation degraded
- First 3 experiments:
  1. Replicate corruption score reduction: Generate 100 texts with/without TSP ordering, compare average corruption scores against Table 5 baseline
  2. Ablate post-processing: Run Algorithm 1 with step 9 disabled, measure ‖Ĩ - e_i(r)‖∞ distribution
  3. Validate error bounds: Monte Carlo sample r values, measure frequency of "bad events" (r within δ of CDF thresholds) against Theorem 5 predictions

## Open Questions the Paper Calls Out

- Can threshold-based sampling methods (top-k or top-p) be implemented efficiently under CKKS using the proposed framework?
- Does the TSP-based token reordering strategy effectively stabilize text generation for diffusion-based language models under encryption?
- Do the theoretical error bounds and stability guarantees hold when inference is performed on actual encrypted data with bootstrapping noise?
- Is the nearest neighbor heuristic sufficient to minimize embedding interference, or do optimal TSP solvers offer significantly lower corruption rates?

## Limitations

- All reported results were obtained in plaintext simulation rather than actual homomorphic encryption execution
- Heaviside approximation parameters (ε, δ) and polynomial degree are not specified, making exact reproduction impossible
- Fine-tuning dataset composition and domain characteristics are unspecified, raising questions about generalizability

## Confidence

**High Confidence Claims:**
- TSP-based token reordering increases average cosine similarity between adjacent tokens (0.024 → 0.271 on Llama-2-7b-hf)
- Post-processing polynomial PP(x) = -2x³ + 3x² effectively sharpens approximate one-hot vectors toward 0/1 values
- Domain-specific fine-tuning with LoRA can reduce corruption scores from 1.5 to 0.5 with sufficient steps

**Medium Confidence Claims:**
- Error bounds improve from O(ε) to O(ε²) after post-processing under Theorem 5 conditions
- The combination of TSP reordering, post-processing, and fine-tuning achieves near-zero corruption ratios at 60+ steps
- CKKS-compatible inverse transform sampling with Heaviside approximation introduces bounded errors

**Low Confidence Claims:**
- Translation of simulation results to actual HE performance under real-world computational constraints
- Generalization of corruption score improvements across different model architectures and domains
- Long-term stability of fine-tuned models under extended inference sequences

## Next Checks

1. **Heaviside Approximation Sensitivity Analysis**: Systematically vary ε and δ parameters while measuring corruption scores and error bounds. Validate whether Theorem 5 predictions hold empirically across different uniform sample distributions near CDF thresholds.

2. **Actual HE Implementation Benchmark**: Implement the complete pipeline (TSP reordering + sampling + post-processing) on a small CKKS-HE instance using a reduced vocabulary. Measure real execution time, multiplicative depth consumption, and compare corruption metrics against plaintext simulation.

3. **Domain Transferability Test**: Fine-tune on multiple distinct datasets (technical documentation, conversational dialogue, creative writing) and measure whether the 60-step convergence and corruption ratio improvements generalize or are domain-specific artifacts.