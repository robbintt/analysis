---
ver: rpa2
title: 'MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting
  Minutes'
arxiv_id: '2602.00316'
source_url: https://arxiv.org/abs/2602.00316
tags:
- metadata
- minutes
- extraction
- municipal
- meeting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiNER, a two-stage pipeline for metadata
  extraction from municipal meeting minutes. The approach combines question answering-based
  boundary detection with transformer-based named entity recognition, enhanced by
  deslexicalization for cross-municipality generalization.
---

# MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes

## Quick Facts
- arXiv ID: 2602.00316
- Source URL: https://arxiv.org/abs/2602.00316
- Reference count: 25
- MiNER achieves F1 up to 0.96 on in-domain metadata extraction, outperforming general LLMs by 400x in efficiency

## Executive Summary
This paper introduces MiNER, a two-stage pipeline for metadata extraction from municipal meeting minutes. The approach combines question answering-based boundary detection with transformer-based named entity recognition, enhanced by deslexicalization for cross-municipality generalization. Evaluated on a new dataset of 120 Portuguese municipal minutes with eight metadata categories, MiNER achieves strong in-domain performance (F1 up to 0.96) and outperforms both larger general-purpose LLMs and unsupervised baselines in accuracy, inference time, and carbon footprint. Cross-municipality evaluation reveals performance degradation, highlighting challenges posed by linguistic variability. MiNER establishes the first benchmark for this domain and demonstrates the value of task-specific fine-tuning over general-purpose models for structured metadata extraction.

## Method Summary
MiNER employs a two-stage pipeline: (1) QA-based boundary detection to identify opening and closing text segments containing metadata using XLM-RoBERTa-large-squad2, and (2) transformer-based NER on the reduced segments using BERTimbau-large or XLM-RoBERTa-large with optional CRF layer. The approach includes deslexicalization preprocessing that replaces specific entity values with synthetic placeholders to improve cross-municipality generalization. The model is trained on 120 Portuguese municipal minutes from 6 municipalities, achieving strong in-domain performance while demonstrating significant computational efficiency gains over general-purpose LLMs.

## Key Results
- Achieves F1 up to 0.96 on in-domain metadata extraction from municipal minutes
- Outperforms general-purpose LLMs (Gemini, Phi) by 400x in computational efficiency and 1800x in inference time
- Cross-municipality evaluation shows performance degradation (F1 drops to 0.72-0.80), indicating sensitivity to linguistic variability
- Boundary detection stage provides 50x training time reduction with only marginal accuracy gains (0.965 vs 0.945 F1)

## Why This Works (Mechanism)

### Mechanism 1: QA-based Boundary Detection Reduces Search Space
- Claim: Treating metadata localization as an extractive QA task improves both efficiency and precision by restricting NER to relevant text spans.
- Mechanism: A QA model (XLM-RoBERTa-large-squad2) receives document-context plus prompts asking for opening/closing segments, predicting span boundaries that define where metadata resides. This reduced region R is then passed to NER.
- Core assumption: Metadata concentrates in predictable document regions (opening/closing), not distributed throughout.
- Evidence anchors: [abstract] "First, a question answering (QA) model identifies the opening and closing text segments containing metadata." [Section 3] "For each document D, we employ two prompts pt, one for each segment t ∈ {opening,closing}, which output the positions ât and b̂t of the last and the first sentences." [Section 5] "When the NER component was applied directly to the full documents, without the first stage... decreasing the training time from approximately 50 minutes to only 1 minute."

### Mechanism 2: Deslexicalization Promotes Structural Pattern Learning
- Claim: Replacing specific entity values with synthetic placeholders can improve cross-municipality generalization by forcing models to learn structural cues over lexical memorization.
- Mechanism: The deslexicalization function φ(·) replaces participants/locations with Faker-generated values (60% probability), varies dates/times (30%), and substitutes municipality names with @MUNICIPIO placeholders before NER training.
- Core assumption: Metadata extraction depends more on document structure and contextual patterns than specific lexical tokens.
- Evidence anchors: [abstract] "enhanced by deslexicalization for cross-municipality generalization" [Section 3] "participants and location names are replaced with synthetic values (60% probability) using Faker library... Municipality mentions are also replaced with a placeholder token (@MUNICIPIO)." [Section 5, Table 3] Leave-one-out results show mixed deslexicalization effects: Portuguese BERTimbau Deslex (0.78 F1) slightly underperforms Base (0.80).

### Mechanism 3: Task-Specific Fine-Tuning Outperforms General LLMs for Structured Extraction
- Claim: Domain-adapted transformer models achieve higher accuracy and dramatically lower computational cost than general-purpose LLMs for structured metadata extraction.
- Mechanism: Fine-tuned models learn domain-specific entity categories and boundary patterns directly from annotated data, producing consistent structured outputs. General LLMs produce fragmented/inconsistent outputs without task-specific training.
- Core assumption: Structured output requirements and domain-specific categories benefit from targeted supervision.
- Evidence anchors: [abstract] "MiNER achieves strong in-domain performance (F1 up to 0.96) and outperforms both larger general-purpose LLMs and unsupervised baselines in accuracy, inference time, and carbon footprint." [Section 5] "Gemini model achieved a modest F1-score (0.27), characterized by high precision (0.83) but low recall (0.16)... The Phi model could not be reliably evaluated because of inconsistent JSON outputs." [Section 5] "requiring over 1,800 times less inference time (approximately 0.4s vs. 737s) and emitting nearly 400 times lower carbon emissions."

## Foundational Learning

- **Concept: Extractive Question Answering (SQuAD-style)**
  - Why needed here: Stage 1 frames boundary detection as span prediction, requiring understanding of how QA models output answer positions from context passages.
  - Quick check question: Given a context paragraph and question "Where does the meeting metadata begin?", can you identify the model's output format (start/end token indices vs. text span)?

- **Concept: Named Entity Recognition with BIO/CRF Tagging**
  - Why needed here: Stage 2 uses token-level classification with BIO or CRF layers for sequence labeling; understanding tag schemes is essential for interpreting output and debugging boundary errors.
  - Quick check question: In BIO tagging, what does the "B-" prefix indicate versus "I-", and how would a CRF layer improve predictions over independent token classification?

- **Concept: Data Augmentation via Deslexicalization**
  - Why needed here: The pipeline applies entity replacement to improve generalization; understanding this tradeoff is critical for deciding when to apply it.
  - Quick check question: If you replace all person names with synthetic values during training, what specific generalization problem does this address, and what information might be lost?

## Architecture Onboarding

- **Component map:** raw minutes -> SQuAD v2 format conversion -> QA model (XLM-RoBERTa-squad2) -> boundary spans (ât, b̂t) -> reduced region R -> deslexicalization (optional) -> NER model (BERTimbau/XLM-RoBERTa) -> entity outputs

- **Critical path:**
  1. Convert raw minutes to SQuAD v2 format (full text = context, prompts = questions, annotated spans = answers)
  2. Train/fine-tune QA model on boundary detection task
  3. Extract R = S_opening ∪ S_closing from each document
  4. Apply deslexicalization to R (if using)
  5. Train NER model on R segments with 8 entity categories
  6. Inference: QA → extract R → NER → output entities

- **Design tradeoffs:**
  - **With vs. without CRF layer:** CRF can improve sequence consistency but added complexity; paper shows mixed results (BERTimbau+CRF 0.95 vs. BERTimbau 0.96 F1)
  - **Deslexicalization:** May improve cross-municipality generalization but can reduce in-domain performance (Portuguese leave-one-out: 0.80 → 0.78)
  - **Pipeline vs. end-to-end:** Ablation shows MBD only improves F1 by ~0.02 (0.945 → 0.965) but reduces training time 50x

- **Failure signatures:**
  - **Cross-municipality drop:** Leave-one-out F1 drops from 0.96 (global) to 0.72-0.80 → indicates stylistic variability limits transfer
  - **Boundary errors:** Paper notes most errors are boundary-related (correct type, wrong span delimitation)
  - **Rare entity underperformance:** "Extraordinary meetings" (6 instances) underperform vs. "ordinary meetings" (F1 0.95-1.0)
  - **Presence state confusion:** Absent/substituted councilors range 0.28-0.70 F1 due to category confusion

- **First 3 experiments:**
  1. **Reproduce Stage 1 QA boundary detection:** Train XLM-RoBERTa-squad2 on provided SQuAD-format data; verify EM and F1 against reported 0.826 (Portuguese) / 0.714 (English)
  2. **Ablate deslexicalization:** Train NER with and without φ(·) on same split; measure gap between global and leave-one-out performance to quantify generalization benefit
  3. **Establish inference baseline:** Compare end-to-end pipeline latency against Gemini/Phi on same test set using Code Carbon; verify ~1800x speedup claim with structured output validation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive fine-tuning or domain-agnostic representations be optimized to mitigate the performance degradation observed in cross-municipality generalization?
- Basis in paper: [explicit] The conclusion states that generalization remains limited by stylistic variability and lists "adaptive fine-tuning strategies" and "domain-agnostic representations" as future work.
- Why unresolved: The leave-one-out evaluation shows a performance drop from F1 0.96 (in-domain) to 0.80 (cross-municipality), indicating current transfer learning approaches are insufficient.
- What evidence would resolve it: Experiments demonstrating that specific adaptive fine-tuning methods achieve statistically similar F1-scores in leave-one-out settings compared to the global split.

### Open Question 2
- Question: Does the QA-based boundary detection stage (Stage 1) yield significant accuracy improvements over end-to-end NER in noisier or more heterogeneous document collections?
- Basis in paper: [inferred] The conclusion notes the segmentation stage offers "only marginal gains in end-to-end accuracy" (0.965 to 0.945), suggesting its utility might be primarily computational efficiency rather than predictive power.
- Why unresolved: It is unclear if the two-stage complexity is justified solely by speed, or if there exists a threshold of document noise where the accuracy gap widens significantly.
- What evidence would resolve it: An ablation study on a dataset with higher noise or unstructured metadata placement showing a substantial F1 delta between the pipeline and a single-stage NER model.

### Open Question 3
- Question: Can structured decoding techniques allow general-purpose LLMs to match the inference efficiency and accuracy of the task-specific MiNER pipeline?
- Basis in paper: [inferred] The authors note the Phi model failed due to "inconsistent JSON outputs," suggesting the evaluation was limited by output formatting issues rather than raw model capability.
- Why unresolved: Without constrained decoding, it remains unknown if LLMs failed due to fundamental understanding limits or merely formatting unreliability.
- What evidence would resolve it: A benchmark comparison where LLMs utilize grammar-constrained decoding to ensure valid JSON, compared against MiNER's accuracy and carbon footprint.

## Limitations

- **Cross-municipality generalization is limited:** Performance drops significantly (F1 0.96→0.72-0.80) when applying to documents from unseen municipalities, indicating sensitivity to stylistic variations
- **Category ambiguity affects rare entities:** Presence/absence status of councilors and rare meeting types show poor performance due to limited training examples and semantic ambiguity
- **Evaluation may not capture full LLM capabilities:** The comparison with general-purpose LLMs focuses on structured extraction accuracy, potentially overlooking nuanced understanding they might provide for ambiguous or context-dependent metadata interpretation

## Confidence

**High Confidence (90%+):** The core architectural innovation of QA-based boundary detection followed by NER on reduced segments is well-supported by ablation studies showing 50x training time reduction. The computational efficiency claims (1800x faster inference, 400x lower emissions) are specific and verifiable. The superiority over LLMs for structured extraction is demonstrated through concrete metrics.

**Medium Confidence (70-89%):** The deslexicalization mechanism's effectiveness for cross-municipality generalization is supported but shows mixed results in leave-one-out experiments. The claim that task-specific fine-tuning universally outperforms general LLMs may not hold for all municipal document formats or when labeled data is extremely limited.

**Low Confidence (Below 70%):** The assumption that opening/closing segments consistently contain all relevant metadata may not hold for all municipal minutes formats, particularly those with embedded metadata or non-standard structures. The long-term effectiveness of synthetic value replacement for participant names and locations requires further validation across diverse linguistic contexts.

## Next Checks

1. **Cross-Domain Generalization Test:** Apply MiNER to municipal minutes from structurally different regions (e.g., North American or Asian formats) to assess whether the QA-boundary detection approach remains effective when opening/closing segment conventions differ.

2. **Edge Case and Rare Entity Analysis:** Conduct detailed error analysis on documents containing rare meeting types, ambiguous presence/absence scenarios, or non-standard date/time formats to identify specific failure modes and assess whether architectural modifications could address these limitations.

3. **Real-World Deployment Monitoring:** Implement MiNER in an active municipal transparency system and track performance degradation over time as document formats evolve, particularly monitoring the boundary detection component's sensitivity to structural variations and the NER component's handling of emerging entity types.