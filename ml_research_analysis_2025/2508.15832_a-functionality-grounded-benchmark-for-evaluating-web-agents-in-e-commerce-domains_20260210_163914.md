---
ver: rpa2
title: A Functionality-Grounded Benchmark for Evaluating Web Agents in E-commerce
  Domains
arxiv_id: '2508.15832'
source_url: https://arxiv.org/abs/2508.15832
tags:
- agent
- user
- page
- agents
- amazon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Amazon-Bench, a new benchmark for evaluating
  web agents in e-commerce domains that addresses two key gaps in existing benchmarks:
  lack of task diversity beyond product search and absence of safety evaluation. To
  address these issues, the authors propose a functionality-grounded user query generation
  pipeline that leverages real webpage content and interactive elements to create
  diverse user queries covering tasks like address management, wishlist management,
  and store interaction.'
---

# A Functionality-Grounded Benchmark for Evaluating Web Agents in E-commerce Domains

## Quick Facts
- arXiv ID: 2508.15832
- Source URL: https://arxiv.org/abs/2508.15832
- Reference count: 40
- Current web agents achieve 42-60% success rates on diverse e-commerce tasks, with harmful failure rates of 4-9%

## Executive Summary
Amazon-Bench addresses critical gaps in web agent evaluation by introducing functionality-grounded user query generation and safety-focused assessment. The benchmark moves beyond traditional product search tasks to include account management, wishlist operations, and store interactions, using a pipeline that leverages real webpage content to generate diverse, realistic queries. A novel automated evaluation framework classifies agent outcomes as success, benign failure, or harmful failure, enabling safety-aware assessment. Experiments with multiple agents reveal significant performance limitations and safety risks, with harmful failure rates ranging from 4% to 9% across different tasks.

## Method Summary
The benchmark employs a functionality-grounded user query generation pipeline that uses accessibility tree (AXTree) representations of real webpages to create diverse e-commerce tasks. The process begins with BFS exploration of Amazon.com, categorizing pages by URL patterns, followed by diversity-based sampling using BERT embeddings of interactive elements. LLM-generated queries are refined and validated by humans. Agents are evaluated using BrowserGym with full AXTree observations, and an LLM-as-Judge classifies outcomes as success, benign failure, or harmful failure based on query intent, action history, and screenshots. The benchmark includes 400 queries across 7 task types and evaluates safety through automated assessment of negative user impact.

## Key Results
- Current web agents show success rates between 42% and 60% on diverse e-commerce tasks
- Harmful failure rates range from 4% to 9%, with product interaction and account management tasks showing highest risks
- Account & Profile category shows highest functional diversity (0.624), requiring 20 pages sampled vs. 5 for low-diversity categories
- Human evaluation shows 92.5% agreement rate with GPT-4o judge for safety classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding user query generation in actual webpage content produces more diverse and realistic tasks than attribute-based augmentation alone.
- Mechanism: The pipeline feeds real webpage accessibility trees to LLMs, which generate queries based on visible interactive elements (buttons, forms, dropdowns) rather than abstract task categories. This ensures queries reflect actual platform capabilities.
- Core assumption: LLMs can reliably identify actionable elements from AXTree representations and generate natural language queries that exercise those functionalities.
- Evidence anchors:
  - [abstract] "data generation pipeline that leverages webpage content and interactive elements (e.g., buttons, check boxes) to create diverse, functionality-grounded user queries"
  - [section 3.3] "LLM M generates the user query q = M(w, pgen)" where w is the webpage AXTree
  - [corpus] WebMall and REAL similarly use grounded environments for evaluation, but Amazon-Bench extends this to account management tasks specifically
- Break condition: If webpage structures change significantly or AXTree extraction fails, query relevance degrades. Limited to functionalities present at crawl time.

### Mechanism 2
- Claim: Functional diversity sampling prevents over-representation of structurally similar pages (e.g., product detail pages).
- Mechanism: The diversity score D(c) measures average dissimilarity of interactive element embeddings within each category, using BERT embeddings of visible widget text. Categories with higher functional variation are sampled more heavily via logarithmic proportional sampling.
- Core assumption: Widget text similarity correlates with functional similarity across pages, and embedding-based comparison captures this adequately.
- Evidence anchors:
  - [section 3.2] "Diversity(c) = 2/(n(n-1)) × Σ(1 - sim(ei, ej))" where ei is the textual embedding of page i
  - [section 3.2] Account & Profile pages had highest diversity score (0.624), resulting in 20 pages sampled vs. 5 for low-diversity categories
  - [corpus] No direct corpus comparison found for this specific sampling approach
- Break condition: If new page types emerge with novel interactive patterns not captured by embedding similarity, diversity estimates become inaccurate.

### Mechanism 3
- Claim: Distinguishing benign from harmful failures enables safety-aware agent evaluation beyond binary success metrics.
- Mechanism: The evaluation framework uses an LLM-as-Judge that considers query, action history, and screenshots to classify outcomes. Harmful failures are defined as unintended state changes with negative user impact (wrong purchases, incorrect settings, duplicate cart additions).
- Core assumption: The judge LLM can reliably infer user impact from action sequences and screenshots, and the three-category classification (success/benign/harmful) captures the relevant failure modes.
- Evidence anchors:
  - [section 3.4] "harmful failure rate across different tasks... product interaction and account management tasks show higher harmful failure rates"
  - [section 4.5] Human evaluation showed 92.5% agreement rate with GPT-4o judge
  - [corpus] AgentRewardBench also evaluates trajectory assessment but focuses on task completion rather than safety categorization
- Break condition: If agents produce novel failure modes not covered by the judge prompt, or if screenshots miss relevant context, classification accuracy degrades.

## Foundational Learning

- Concept: **Accessibility Tree (AXTree) representation**
  - Why needed here: The entire observation space relies on simplified HTML representations that retain visible structure and interactive elements while removing noise. Without understanding AXTrees, you cannot interpret agent inputs or debug observation issues.
  - Quick check question: Can you explain what information is preserved vs. discarded when converting raw HTML to an AXTree?

- Concept: **LLM-as-Judge evaluation paradigm**
  - Why needed here: The safety evaluation depends on an LLM classifying trajectories from screenshots and action histories. Understanding prompt design and failure modes of judge models is essential for interpreting benchmark results.
  - Quick check question: What are two potential biases an LLM judge might exhibit when evaluating agent trajectories?

- Concept: **E-commerce task taxonomy beyond product search**
  - Why needed here: The benchmark specifically addresses the gap in existing benchmarks by including account management, wishlist operations, store following, and media interactions. Understanding these task types is prerequisite to interpreting performance differentials.
  - Quick check question: Why might account management tasks show higher harmful failure rates than product search tasks?

## Architecture Onboarding

- Component map:
  Exploration module (BFS crawler with depth ≤3) → Diversity scorer (BERT embeddings + cosine similarity) → Query generator (LLM + AXTree input + refinement step) → Human validation layer → Evaluation harness (BrowserGym-based) → Judge module (LLM + query + actions + screenshots)

- Critical path:
  1. Crawl and categorize URLs → 2. Score diversity and sample pages → 3. Extract AXTrees and generate queries → 4. Human review → 5. Run agents online → 6. Judge trajectories → 7. Compute success/benign/harmful rates

- Design tradeoffs:
  - Live website evaluation (realism) vs. reproducibility risk (page content changes)
  - Full-page AXTree input (complete context) vs. visible-window-only (WebVoyager/Nova-Act approach, more challenging)
  - Single-turn queries (controlled evaluation) vs. multi-turn conversations (not supported, noted as limitation)

- Failure signatures:
  - **Scroll loops without progress** (Nova-Act case): agent lacks task-specific page knowledge, searches for non-existent elements
  - **Redundant state-changing actions** (Claude-3.7 case): agent doesn't track cart state, adds same item twice
  - **High token usage with low efficiency** (Deepseek-R1): reasoning models generate many tokens but don't improve success rates
  - **Store interaction failures**: difficulty navigating to brand stores before completing tasks

- First 3 experiments:
  1. Reproduce the diversity scoring analysis on a different e-commerce platform to validate that account/profile pages consistently show highest functional diversity.
  2. Run ablation comparing full-page AXTree input vs. visible-window-only input on a subset of queries to quantify observation-space impact.
  3. Test judge reliability by having multiple human annotators label the same 40 trajectories and measuring inter-annotator agreement alongside LLM-human agreement.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does extending Amazon-Bench to multi-turn, conversational interactions affect agent performance and safety outcomes compared to single-turn queries?
  - Basis in paper: [explicit] "Our benchmark is designed for single-turn queries rather than multi-turn or conversational interactions. Extending the benchmark to support multi-step dialogues would enable the evaluation of agents' ability to handle task clarification and follow-up instructions."
  - Why unresolved: The current benchmark only evaluates atomic tasks, but real users often engage in iterative, clarifying dialogues that may surface different failure modes.
  - What evidence would resolve it: A modified benchmark with multi-turn task scenarios, comparing success rates, harmful failure rates, and efficiency between single-turn and conversational settings.

- **Open Question 2**: Can incorporating user history and personalized context into task design improve agent safety by reducing unintended state changes?
  - Basis in paper: [explicit] "The current evaluation assumes tasks are independent of a specific user's past interactions. Incorporating context (e.g., user history) into task design could help design a more personalized agent."
  - Why unresolved: Current tasks assume no prior user state, but real agents operate on accounts with existing addresses, cart contents, and preferences that may influence action appropriateness.
  - What evidence would resolve it: Experiments comparing agent performance on context-aware tasks versus context-free tasks, measuring harmful failure rate reductions when agents have access to user history.

- **Open Question 3**: Does the functionality diversity score based on interactive element text embeddings reliably capture task novelty across different e-commerce platforms beyond Amazon?
  - Basis in paper: [inferred] The diversity score uses BERT embeddings of widget text from interactive elements to measure functional variation, but this approach was only validated on Amazon's specific page structure.
  - Why unresolved: Different e-commerce platforms may have different UI patterns, naming conventions, and functional organization that affect whether text-based similarity correlates with actual task diversity.
  - What evidence would resolve it: Cross-platform experiments applying the same diversity scoring and sampling methodology to other e-commerce sites (e.g., eBay, Walmart), comparing correlation between diversity scores and actual query diversity generated.

## Limitations

- Benchmark relies on single e-commerce platform (Amazon.com), limiting generalizability despite claims of broad applicability
- LLM-as-Judge evaluation may exhibit systematic biases in classifying harmful failures for novel failure modes
- Full AXTree observation space differs significantly from viewport-only approaches, complicating direct performance comparisons
- Diversity scoring mechanism assumes widget text embedding similarity correlates with functional diversity across platforms

## Confidence

- **High Confidence**: The benchmark successfully addresses documented gaps in existing web agent evaluation by including diverse e-commerce tasks beyond product search and introducing safety evaluation through benign/harmful failure classification.
- **Medium Confidence**: The functionality-grounded query generation mechanism produces realistic and diverse tasks, though the effectiveness depends on LLM reliability in interpreting AXTree structures.
- **Medium Confidence**: The automated evaluation framework provides practical scalability for safety assessment, though human evaluation remains necessary for validation.

## Next Checks

1. Apply the diversity scoring and query generation pipeline to two additional e-commerce platforms (e.g., eBay, Walmart) to verify generalizability of functional diversity findings and query relevance.

2. Conduct blind human evaluation of 100 trajectories across all task types, comparing human classifications with LLM-as-Judge outputs to quantify agreement rates and identify systematic biases.

3. Run controlled experiment comparing agent performance using full AXTree vs. viewport-only observation on identical query sets to measure impact on success rates and failure modes.