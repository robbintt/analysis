---
ver: rpa2
title: Gauge-invariant representation holonomy
arxiv_id: '2601.21653'
source_url: https://arxiv.org/abs/2601.21653
tags:
- holonomy
- loop
- representation
- mnist
- procrustes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Representation holonomy is a gauge-invariant statistic that quantifies
  path-dependent "twist" in neural network feature fields. The method measures how
  much local feature alignments accumulate when features are parallel-transported
  around a closed loop in input space, using shared-neighborhood whitening and rotation-only
  Procrustes transport in low-dimensional subspaces.
---

# Gauge-invariant representation holonomy

## Quick Facts
- arXiv ID: 2601.21653
- Source URL: https://arxiv.org/abs/2601.21653
- Authors: Vasileios Sevetlidis; George Pavlidis
- Reference count: 40
- Primary result: Representation holonomy quantifies path-dependent "twist" in neural network feature fields and correlates with robustness across training regimes.

## Executive Summary
Representation holonomy is a gauge-invariant statistic that measures how much local feature alignments accumulate when features are parallel-transported around a closed loop in input space. The method uses shared-neighborhood whitening and rotation-only Procrustes transport in low-dimensional subspaces to compute a net rotation that deviates from identity when the representation field has curvature. On MNIST and CIFAR-10, holonomy increases with loop radius, correlates with adversarial and corruption robustness across training regimes (ERM, label smoothing, mixup, PGD), and distinguishes models that appear similar under CKA. The estimator is invariant to orthogonal reparameterizations, has a linear null for affine layers, and vanishes at small radii as predicted by theory.

## Method Summary
The method computes holonomy by first fixing a gauge through global whitening (z-score or ZCA-correlation), then constructing loops around test points using local PCA planes. For each edge, shared-midpoint k-NN selection ensures consistent alignment targets, and joint q-subspace extraction via thin SVD captures relevant local feature structure. Rotation-only Procrustes alignments are composed around the loop to yield a net rotation H, whose deviation from identity quantifies path-dependent curvature. The estimator uses default parameters (k,q)=(128,64) for MNIST and (192,96) for CIFAR layer2, with SO(q) enforcement and embedding to SO(p).

## Key Results
- Holonomy increases with loop radius on MNIST and CIFAR-10
- Holonomy correlates with FGSM/PGD-10 and corruption robustness across training regimes
- Holonomy distinguishes models with similar CKA similarity scores
- Linear null: affine layers yield holonomy near 10⁻⁸ floor
- Small-radius scaling follows O(r) behavior as predicted by theory

## Why This Works (Mechanism)

### Mechanism 1
Global whitening fixes a consistent feature basis across the network, making subsequent transport computations comparable. ZCA-correlation whitening removes second-order anisotropy, ensuring the same gauge is used for all feature vectors so rotations measured at different points are meaningfully composable. Core assumption: pool statistics adequately characterize the feature distribution. Evidence: Proposition A.2 proves affine invariance post-whitening. Break condition: per-neighborhood whitening instead of global causes stepwise gauge drift.

### Mechanism 2
Shared-midpoint k-NN selection ensures consistent alignment targets across loop edges, reducing index-mismatch noise. For edge (xᵢ, xᵢ₊₁), neighbors are drawn from a pool using the midpoint mᵢ as query, ensuring the same indices for both endpoints cancel translations and capture true local orientation change. Core assumption: k is large enough that shared neighbors overlap with high probability as radius → 0. Evidence: Ablation shows disjoint k-NNs inflate holonomy by +0.222. Break condition: using separate k-NNs per endpoint catastrophically inflates holonomy.

### Mechanism 3
Composing rotation-only (SO) Procrustes alignments around a closed loop yields a net rotation whose deviation from identity quantifies path-dependent curvature. Each edge's transport Rᵢ ∈ SO(q) is computed in a shared q-dimensional subspace, then embedded to SO(p). Composition H = R_{L−1}···R₀ measures accumulated "twist." Nonzero H−I indicates non-integrable transport. Core assumption: the q-dimensional subspace captures relevant local feature structure. Evidence: Linear null test yields H = I (holonomy ~10⁻⁸). Break condition: allowing reflections (O vs SO) introduces π-flips; using O(p) raised h by 5.37×10⁻⁷ on average.

## Foundational Learning

- **Gauge freedom and gauge fixing**: Neural representations admit many equivalent parameterizations (permutations, rotations, scalings). Without fixing a gauge, comparing feature orientations is meaningless. Quick check: If you apply a random orthogonal matrix Q to all features at a layer, should your measured holonomy change?

- **Parallel transport on vector bundles**: Holonomy is the net rotation after parallel-transporting a vector around a closed loop. Understanding this differential-geometric intuition clarifies why composition of local alignments matters. Quick check: On a flat plane, what is the holonomy of a vector parallel-transported around any closed loop?

- **Orthogonal Procrustes alignment**: The method computes optimal rotation-only alignments between neighboring feature clouds. Understanding Procrustes (and why det=+1 matters) is essential for implementation. Quick check: Given centered matrices X, Y, what does the orthogonal Procrustes solution R* = UV⊤ (from SVD of X⊤Y) minimize?

## Architecture Onboarding

- **Component map**: Feature extraction layer -> Global whitening (pool-based) -> Loop construction (PCA plane around test point) -> Per-edge shared k-NN selection -> Joint q-subspace extraction (thin SVD) -> SO(q) Procrustes -> Embed to SO(p) -> Compose holonomy H -> Compute h_norm = ‖H−I‖_F/(2√p)

- **Critical path**: The shared-midpoint k-NN and SO(q) enforcement are the most fragile; ablations show these cause catastrophic bias if mishandled.

- **Design tradeoffs**: k vs q: k ≫ q recommended (e.g., k∈[128,192], q∈[64,96]) for stability vs compute; Radius: small radii give O(r) scaling but hit numerical floor (~10⁻⁸); larger radii increase signal but risk semantic off-manifold paths; Pool size: N_pool = 2048 works well; sensitivity analysis shows ~10⁻⁹ variation across 10³–8×10³.

- **Failure signatures**: Self-loop h_norm >> 10⁻⁷ → check whitening/global gauge; h_norm varies wildly with n_points → likely using disjoint neighbors; h_norm doesn't vanish for linear network → check SO vs O, shared neighbors.

- **First 3 experiments**: 1) Linear null test: Replace all nonlinearities with identity; verify h_norm collapses to ~10⁻⁸–10⁻⁹ floor; 2) Gauge invariance test: Apply random orthogonal Q to features; verify ‖H'−I‖_F = ‖H−I‖_F and eigen-angles unchanged; 3) Self-loop bias test: Run with radius ≈ 10⁻⁴ (repeated same point); characterize numerical floor before running real experiments.

## Open Questions the Paper Calls Out
None

## Limitations
- The gauge-fixing assumption (global whitening) may not hold if feature distributions are strongly class-conditional or non-stationary during training
- The method's sensitivity to subspace dimension q and neighbor count k is demonstrated empirically but lacks theoretical bounds on optimal selection
- The relationship between holonomy and robustness appears correlational; causal mechanisms remain speculative

## Confidence
- **High confidence**: Gauge invariance properties, linear null test results, small-radius O(r) scaling behavior
- **Medium confidence**: Robustness correlations across training regimes, CKA comparison findings, spectral analysis patterns
- **Low confidence**: Generalization of findings to architectures beyond MLP/ResNet-18, interpretation of holonomy as a "curvature" measure

## Next Checks
1. Test gauge invariance explicitly by applying random orthogonal transformations to features and verifying holonomy remains unchanged
2. Conduct ablation studies varying q and k systematically to establish stability thresholds and optimal ranges
3. Extend validation to additional architectures (ViT, ConvNeXt) and datasets to assess generalizability