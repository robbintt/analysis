---
ver: rpa2
title: 'Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With
  Them'
arxiv_id: '2510.19634'
source_url: https://arxiv.org/abs/2510.19634
tags:
- constraint
- least-squares
- null-space
- learning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that least squares solvers can be made differentiable
  and used as general building blocks in machine learning pipelines. By deriving custom
  gradients for adaptive least-squares solvers, the authors enable seamless integration
  into modern deep learning frameworks.
---

# Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them

## Quick Facts
- **arXiv ID:** 2510.19634
- **Source URL:** https://arxiv.org/abs/2510.19634
- **Reference count:** 29
- **Primary result:** Demonstrates differentiable least-squares solvers for ML pipelines, enabling efficient constraint enforcement (e.g., sparsity, equivariance, conservativeness) with 5-10x speedup over AD unrolling.

## Executive Summary
This paper presents a method to make least-squares solvers differentiable and applicable as general building blocks in machine learning. The authors derive custom gradients for adaptive least-squares solvers, enabling seamless integration into deep learning frameworks. By reformulating the null-space method as a least-squares problem, they make it applicable to deep learning for enforcing hard equality constraints. Experiments demonstrate the approach is efficient, effective, and versatile across various tasks including enforcing weight sparsity, imposing conservativeness constraints on generative models, and hyperparameter tuning of Gaussian processes.

## Method Summary
The method makes least-squares solvers differentiable by deriving custom adjoint gradients using the implicit function theorem, avoiding the need to store iterative solver traces for backpropagation. The core innovation reformulates the null-space method as a least-squares problem, enabling hard constraint enforcement on neural networks. The approach uses matrix-free LSMR solvers that interact with linear operators only via matrix-vector products, with transpose operations automated through reverse-mode autodiff. This is implemented in JAX with custom `LstSq` and `nuox` libraries, allowing constrained optimization via projected gradient descent.

## Key Results
- Custom adjoint gradients for least-squares solvers achieve 5-10x speedup over automatic differentiation unrolling
- Null-space method reformulation enables hard constraint enforcement on deep networks while maintaining performance
- Matrix-free implementation scales to large operators that cannot fit in memory
- Successfully applied to enforce weight sparsity on 50M-parameter models, conservativeness constraints on score-based generative models, and hyperparameter tuning of Gaussian processes

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Custom adjoint gradients for least-squares solvers significantly reduce memory and compute costs compared to automatic differentiation unrolling.

**Mechanism:** Instead of storing the execution trace of the iterative solver for backpropagation, the method applies the adjoint state method. It derives an analytical backward pass that requires solving only two additional least-squares problems to compute the vector-Jacobian product, independent of the number of iterations run in the forward pass.

**Core assumption:** The linear operator $A(\theta)$ is full-rank, and the solver has converged sufficiently to a precise solution $x^\star$.

**Evidence anchors:** [abstract] "deriving custom gradients... 5-10x faster than automatic differentiation"; [section 2.2] Theorem 1 defines gradient computation via additional least-squares calls; [section 3.1] Figure 3 shows runtime improvements scaling with problem size.

**Break condition:** If $A(\theta)$ is rank-deficient, the derived gradients may not hold as the proof relies on invertibility of specific submatrices.

### Mechanism 2
**Claim:** The null-space method allows standard unconstrained optimizers to enforce hard equality constraints $c(\theta)=0$ by projecting gradient steps onto the constraint manifold.

**Mechanism:** The method linearizes the constraint $c(\theta)$ at each step and solves a least-squares problem to find a step direction $\delta$ that minimizes the task loss while strictly satisfying the linearized constraint ($J_c \delta = -c(\theta)$).

**Core assumption:** Constraints are continuously differentiable and the Jacobian $J_c$ is accessible via autodiff.

**Evidence anchors:** [abstract] "reformulating the null-space method as a least-squares problem... applicable to deep learning"; [section 2.3] Equation 10b casts constrained update as `LstSq` operation.

**Break condition:** If learning rate is too high relative to constraint manifold curvature, the linear approximation may fail, leading to oscillation or divergence.

### Mechanism 3
**Claim:** Matrix-free solvers enable least-squares operations on operators too large to fit in memory.

**Mechanism:** The solver interacts with the linear operator $A$ only via matrix-vector products. The transpose operation required by LSMR is automated via reverse-mode autodiff, avoiding explicit storage of $A$.

**Core assumption:** The system allows efficient computation of vector-Jacobian products for the operator definition.

**Evidence anchors:** [section 2.1] "transposition of the linear operator... emerges from automatic differentiation"; [section 2.1] Table 1 comparing Direct vs. Matrix-free memory costs.

**Break condition:** If $A$ is severely ill-conditioned, iterative matrix-free methods may converge too slowly without preconditioning.

## Foundational Learning

- **Concept:** Adjoint State Method (Implicit Differentiation)
  - **Why needed here:** Essential to understand how to derive gradients through an iterative process without unrolling the loop.
  - **Quick check question:** Can you explain why backpropagating through 10,000 iterations of an iterative solver is slower than solving two linear systems?

- **Concept:** Constrained Optimization & KKT Conditions
  - **Why needed here:** The paper frames the null-space method as a solution to equality-constrained optimization.
  - **Quick check question:** Why does the null-space method project the gradient onto the tangent space of the constraint manifold?

- **Concept:** Krylov Subspace Methods (LSQR/LSMR)
  - **Why needed here:** The implementation relies on LSMR for iterative least-squares solutions.
  - **Quick check question:** Why is solving the normal equations ($A^\top A x = A^\top b$) numerically less stable than using a bidiagonalization-based solver like LSMR?

## Architecture Onboarding

- **Component map:** matfree library -> LstSq Operator -> nuox library -> Optax optimizer

- **Critical path:**
  1. Define the physics/constraint model as a linear operator (matvec)
  2. Construct the `LstSq` solver call within the forward pass
  3. Define the constraint function $c(\theta)$
  4. Instantiate the `nuox.projection` transform and chain it with an optimizer
  5. Run the training loop; the optimizer now performs projected gradient descent

- **Design tradeoffs:**
  - **Speed vs. Accuracy:** LSMR tolerance must balance accuracy and speed
  - **Constraint Satisfaction vs. Task Loss:** $\gamma$ parameter balances constraint violation against primary loss
  - **Memory vs. Compute:** Matrix-free methods trade high memory usage for increased compute

- **Failure signatures:**
  - **Divergence to NaN:** Often caused by rank-deficiency in $A$ or the constraint Jacobian $J_c$
  - **Non-convergence of Constraints:** If constraint function is too nonlinear or learning rate too high
  - **Slow Convergence:** If the operator $A$ is ill-conditioned, LSMR may require many iterations

- **First 3 experiments:**
  1. **Gradient Check:** Verify custom VJP of `LstSq` against finite differences on a small, dense matrix
  2. **Toy Constraint:** Implement a simple unit-norm constraint on a small MLP weight matrix
  3. **Solver Benchmark:** Compare runtime and memory usage of `matfree.lsmr` vs. `jax.numpy.linalg.lstsq` on a $10,000 \times 1,000$ matrix

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not fully specify exact LeNet architecture details or O(3) sampling strategy for equivariance constraint
- The assumption of full-rank operators $A(\theta)$ is crucial but not always verified in practice
- Method's sensitivity to learning rate and constraint nonlinearity is not fully explored

## Confidence
- **High Confidence:** Core mechanism of custom adjoint gradients for least-squares solvers (Mechanism 1) is well-supported by theoretical derivation and experimental benchmarks
- **Medium Confidence:** Effectiveness of null-space method for enforcing hard constraints (Mechanism 2) is demonstrated but sensitivity to hyperparameters needs more exploration
- **Medium Confidence:** Matrix-free approach (Mechanism 3) is validated on large-scale problems but convergence behavior for severely ill-conditioned operators is not discussed

## Next Checks
1. **Gradient Verification:** Implement finite-difference check comparing custom VJP of `LstSq` against numerical gradients on a small, dense matrix
2. **Constraint Feasibility Test:** Train a small MLP on synthetic data with simple norm constraint to verify `nuox` projection maintains feasibility
3. **Solver Robustness Test:** Evaluate LSMR solver's convergence on a severely ill-conditioned operator to assess method's limits and need for preconditioning