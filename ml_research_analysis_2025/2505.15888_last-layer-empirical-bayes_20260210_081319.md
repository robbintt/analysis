---
ver: rpa2
title: Last Layer Empirical Bayes
arxiv_id: '2505.15888'
source_url: https://arxiv.org/abs/2505.15888
tags:
- learning
- lleb
- ensembles
- neural
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses uncertainty quantification in neural networks,
  comparing Bayesian neural networks (BNNs) and deep ensembles. It proposes Last Layer
  Empirical Bayes (LLEB), which learns a prior over the last layer weights using a
  normalizing flow to maximize the evidence lower bound, aiming to combine the strengths
  of BNNs and ensembles while maintaining tractability.
---

# Last Layer Empirical Bayes

## Quick Facts
- arXiv ID: 2505.15888
- Source URL: https://arxiv.org/abs/2505.15888
- Reference count: 9
- Primary result: LLEB performs on par with existing uncertainty quantification methods but does not significantly outperform them

## Executive Summary
This paper proposes Last Layer Empirical Bayes (LLEB) for uncertainty quantification in neural networks, learning a data-dependent prior over the last layer weights using a normalizing flow. The approach aims to combine the strengths of Bayesian neural networks and deep ensembles while maintaining computational tractability. LLEB trains the normalizing flow to maximize the evidence lower bound, creating a flexible prior that can concentrate mass around high-performing weights while maintaining diversity. While the method shows promise, empirical results demonstrate performance on par with existing approaches like Laplace approximation and Monte Carlo dropout, but without significant improvements, likely due to tractability constraints in the current implementation.

## Method Summary
LLEB applies empirical Bayes to neural network uncertainty quantification by learning a prior distribution over the last layer weights using a normalizing flow. The method either trains end-to-end for small networks or uses a two-step procedure for larger networks: first training the backbone to convergence, then training the flow on frozen features. The normalizing flow acts as a flexible distribution family that can concentrate mass near optimal weights while maintaining diversity through its invertibility property. During inference, predictions are averaged over samples from the learned prior distribution, providing both point estimates and uncertainty quantification.

## Key Results
- On MNIST and Fashion-MNIST, LLEB achieves accuracies around 97-98% with calibration errors near zero, similar to ensemble methods
- On CIFAR-10 and SVHN, LLEB achieves accuracies around 92-95% with low calibration errors
- LLEB performs on par with existing approaches like last-layer Laplace approximation and Monte Carlo dropout but does not significantly outperform them

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning a data-dependent prior through normalizing flows can concentrate mass around high-performing weights while maintaining diversity.
- Mechanism: Normalizing flows parameterize the prior as an invertible transformation of a simple base distribution. This invertibility acts as implicit regularization—preventing collapse to point masses while allowing the distribution to flexibly place mass near Θ* (the set of maximum-likelihood weights). The learned prior q* is then used as both prior and approximate posterior.
- Core assumption: The objective Eθ∼q[log p(D|θ)] is maximized when q concentrates on Θ* while remaining diverse; invertible flows provide sufficient inductive bias to achieve this balance.
- Evidence anchors:
  - [abstract] "LLEB instantiates a learnable prior as a normalizing flow, which is then trained to maximize the evidence lower bound"
  - [section 3] "NFs are very flexible, yet their invertibility acts as an implicit regularizer which prevents collapse onto a point mass and promotes some diversity"
  - [corpus] Related work (Loaiza-Ganem et al., 2025) shows ensembles implicitly perform empirical Bayes with strong data-dependent priors, but corpus does not contain direct evidence on normalizing flow priors for this purpose.
- Break condition: If the last layer dimension is too large, the flow becomes computationally intractable; if too small, insufficient expressivity limits uncertainty quantification.

### Mechanism 2
- Claim: Restricting Bayesian treatment to the last layer preserves tractability while capturing a substantial portion of epistemic uncertainty.
- Mechanism: The network is partitioned into θ_QU (uncertainty-quantified parameters, i.e., last layer) and θ_NU (non-uncertainty parameters). Only θ_QU receives a learned prior via normalizing flow. The forward pass through earlier layers produces features; uncertainty is then marginalized over the last layer weights during prediction.
- Core assumption: Most epistemic uncertainty relevant to prediction is captured by the last layer; earlier layers can be treated as deterministic feature extractors.
- Evidence anchors:
  - [section 3] "to retain tractability we use the flow only on the last layer"
  - [section 3] "we follow the recent trend in BNNs of being Bayesian only over a subset of parameters such as those in the last layer"
  - [corpus] "From Deep Addlicative Kernel Learning to Last-Layer Bayesian Neural Networks" (arXiv:2502.10540) similarly uses last-layer Bayesian treatment for tractability, suggesting this is a recognized approximation strategy.
- Break condition: For tasks where feature extraction itself has high uncertainty (e.g., domain shift affecting early layers), last-layer-only Bayesian treatment may undercapture epistemic uncertainty.

### Mechanism 3
- Claim: Two-step training (pretrain backbone, then train flow) stabilizes optimization for larger networks.
- Mechanism: First, maximize log p(D|θ) to obtain θ*. Then discard θ*_QU and freeze θ*_NU. Finally, maximize Eθ_QU∼qη[log p(D|θ_QU, θ*_NU)] over η (flow parameters). This avoids joint optimization instabilities.
- Core assumption: The backbone features from maximum-likelihood training are sufficiently good that the flow can learn a meaningful distribution over last-layer weights conditioned on them.
- Evidence anchors:
  - [section 3] "we found this strategy to be faster and much more stable for larger classifiers"
  - [appendix C] For CIFAR-10/SVHN with ResNet18, "we use the two-step training procedure and use the frozen weights from the default network and train the flow for 100 epochs"
  - [corpus] No direct corpus evidence on two-step vs. end-to-end training for this specific architecture.
- Break condition: If the pretrained backbone is severely miscalibrated or overconfident, the flow may inherit and amplify these issues rather than correcting them.

## Foundational Learning

- Concept: **Normalizing Flows**
  - Why needed here: LLEB uses neural spline flows to parameterize the learnable prior; understanding invertible transformations, log-density computation, and the reparameterization trick is essential.
  - Quick check question: Given a base distribution Z ~ N(0,I) and invertible function f, can you write the log-density of q = f(Z)?

- Concept: **Evidence Lower Bound (ELBO)**
  - Why needed here: The training objective is derived from variational inference; understanding the trade-off between likelihood and KL divergence explains why the prior matters.
  - Quick check question: In ELBO(q,π) = Eθ∼q[log p(D|θ)] - KL(q||π), what happens to each term when q collapses to a point mass?

- Concept: **Empirical Bayes**
  - Why needed here: The paper's central claim is that learning the prior from data (rather than fixing it) bridges BNNs and ensembles; this paradigm shift is key to interpreting results.
  - Quick check question: How does empirical Bayes differ from hierarchical Bayes in where the prior parameters come from?

## Architecture Onboarding

- Component map: Backbone network (CNN/ResNet) -> Features -> Last layer with normalizing flow output -> Predictions
- Critical path:
  1. Train base network with maximum likelihood → obtain θ*
  2. Freeze backbone weights θ*_NU
  3. Initialize normalizing flow with Gaussian base distribution
  4. Optimize Eθ_QU∼qη[log p(D|θ_QU, θ*_NU)] over η using reparameterization trick (10 samples per step)
  5. At inference: sample θ_QU ~ q* (10 samples), average predictions
- Design tradeoffs:
  - End-to-end vs. two-step: End-to-end is more principled but unstable for large networks; two-step is stable but may yield suboptimal joint solutions.
  - Flow capacity vs. tractability: More coupling layers increase expressivity but add computation; the paper uses 2 layers.
  - Last-layer-only vs. full BNN: Tractability vs. comprehensive uncertainty—paper acknowledges this as a limitation.
- Failure signatures:
  - Flow collapses to point mass: Check if entropy H(qη) → 0 during training; indicates need for stronger implicit regularization or different flow architecture.
  - Instability on large networks with end-to-end training: Switch to two-step training.
  - Poor OOD detection despite good accuracy: May indicate backbone features are overconfident; consider joint fine-tuning or alternative uncertainty targets.
- First 3 experiments:
  1. Reproduce MNIST results with end-to-end training: Verify accuracy ~97.7%, ECE ~0.00, AUC ~0.95 on Fashion-MNIST OOD. Use Table 5 hyperparameters exactly.
  2. Ablate flow architecture: Reduce to 1 coupling layer and observe if diversity degrades (check entropy estimates if available, or variance of predictions).
  3. Test two-step vs. end-to-end on CIFAR-10: Compare stability (loss curves) and final metrics; expect two-step to be stable but potentially lower accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an empirical Bayes approach be designed to strictly outperform deep ensembles in uncertainty quantification while retaining a similar computational cost?
- Basis in paper: [explicit] The conclusion states the authors hope "future research will manage to improve upon LLEB by better leveraging empirical Bayes... [to] outperform existing UQ approaches."
- Why unresolved: LLEB performs "on par" with baselines but does not exceed them, which the authors suggest may be due to approximations made for tractability.
- What evidence would resolve it: A modified LLEB implementation demonstrating statistically significant improvements in calibration (ECE) or OOD detection (AUC) over deep ensembles.

### Open Question 2
- Question: Does applying the empirical Bayes framework to all layers of a neural network, rather than just the last layer, yield significantly better uncertainty estimates?
- Basis in paper: [inferred] The paper attributes LLEB's lack of superior performance to "concessions we made in LLEB for tractability," specifically restricting the normalizing flow to the last layer to avoid high-dimensional instability.
- Why unresolved: The authors limited the scope to the last layer because scaling normalizing flows to the full parameter space results in "intractable" optimization.
- What evidence would resolve it: A tractable method for learning a full-network prior that remains stable during training and improves upon last-layer results.

### Open Question 3
- Question: Are there alternative distribution families or regularization terms that can more effectively concentrate prior mass around optimal weights without collapsing into a point mass?
- Basis in paper: [inferred] Appendix B notes that the authors tried alternative distributions and entropy regularization but found them ineffective or prone to collapse, concluding that "using different distributions... is not trivial."
- Why unresolved: The invertibility of normalizing flows provides implicit regularization against collapse, but the authors imply better options might exist if diversity can be maintained more effectively.
- What evidence would resolve it: Identifying a distribution class that maximizes the ELBO without collapsing and outperforms the Neural Spline Flow implementation.

## Limitations
- LLEB performs on par with existing methods but does not demonstrate clear superiority, suggesting the normalizing flow prior may not provide sufficient inductive bias advantage
- The paper does not provide entropy or diversity metrics for the learned prior, making it difficult to verify whether the flow achieves the desired balance between concentration and diversity
- Analysis focuses on classification tasks where last-layer uncertainty may suffice, without exploring regression or structured prediction tasks where early-layer uncertainty could be more critical

## Confidence

- High confidence: The mechanism of using normalizing flows for learnable priors is technically sound and well-established in the literature.
- Medium confidence: The empirical results showing LLEB performs comparably to baselines are reliable, but the interpretation that this validates the empirical Bayes approach is somewhat speculative given the lack of clear performance gains.
- Low confidence: The claim that the flow's invertibility provides sufficient implicit regularization to prevent collapse is not directly validated in the paper with empirical measurements.

## Next Checks

1. Measure and report the entropy of the learned prior q* during training to verify it maintains sufficient diversity and does not collapse to a point mass.
2. Conduct ablation studies varying the number of coupling layers in the normalizing flow (1 vs 2 vs 3) to quantify the trade-off between expressivity and performance.
3. Test LLEB on a regression task (e.g., UCI datasets) where early-layer uncertainty is known to matter, to evaluate whether last-layer-only treatment is sufficient beyond classification.