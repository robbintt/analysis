---
ver: rpa2
title: Training-Free Reward-Guided Image Editing via Trajectory Optimal Control
arxiv_id: '2509.25845'
source_url: https://arxiv.org/abs/2509.25845
tags:
- image
- inversion
- reward
- editing
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free framework for reward-guided
  image editing by formulating it as a trajectory optimal control problem. The method
  treats the reverse diffusion process as a controllable trajectory starting from
  the source image and iteratively updates adjoint states to steer the editing process.
---

# Training-Free Reward-Guided Image Editing via Trajectory Optimal Control

## Quick Facts
- arXiv ID: 2509.25845
- Source URL: https://arxiv.org/abs/2509.25845
- Reference count: 40
- Training-free reward-guided image editing framework using trajectory optimal control significantly outperforms existing baselines across multiple editing tasks

## Executive Summary
This paper introduces a training-free framework for reward-guided image editing by formulating it as a trajectory optimal control problem. The method treats the reverse diffusion process as a controllable trajectory starting from the source image and iteratively updates adjoint states to steer the editing process. Through extensive experiments across four distinct editing tasks (human preference, style transfer, counterfactual generation, and text-guided editing), the approach significantly outperforms existing inversion-based training-free guidance baselines, achieving superior balance between reward maximization and fidelity to the source image without reward hacking.

## Method Summary
The method generates an initial trajectory from the source image using DDIM inversion or time-reversed ODE, then iteratively optimizes this trajectory to maximize a target reward while preserving source identity. The optimization uses Pontryagin's Maximum Principle to compute adjoint states that guide the control signal at each denoising step. The algorithm runs for N iterations (typically 25), updating control signals and simulating new trajectories until convergence. Hyperparameters include trajectory depth T (0.5-0.7), reward weight w (50-1000), and learning rate λ, with specific values task-dependent.

## Key Results
- Achieves state-of-the-art performance on human preference editing, outperforming DPS and Gradient Ascent baselines
- Successfully transfers style between images while maintaining source identity with lower LPIPS scores
- Generates realistic counterfactuals (young/Asian versions) with high CLIP scores and good preservation
- Enables text-guided editing through CLIP-based rewards with superior FID scores compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Global Trajectory Optimization via Control Theory
The method formulates image editing as a trajectory optimal control problem, allowing global optimization of the entire denoising path rather than greedy step-wise updates. This overcomes local optima by finding valid latent paths connecting source identity to target reward through a cost functional balancing control effort against terminal reward.

### Mechanism 2: Adjoint State Backpropagation for Latent Guidance
Using Pontryagin's Maximum Principle, adjoint states propagate precise reward gradients backward from the clean image to noisy latents. This provides globally optimal control signals at every step by accounting for the model's drift dynamics, ensuring the trajectory remains on the image manifold.

### Mechanism 3: Iterative Constraint Satisfaction (Coordinate Descent)
The algorithm decomposes the complex joint optimization into sequential updates: computing adjoint states, updating control signals, and simulating new trajectories. This iterative approach enables convergence to feasible solutions by refining the path incrementally rather than solving it in one shot.

## Foundational Learning

- **Concept: Stochastic Differential Equations (SDEs) in Diffusion/Flow Matching**
  - Why needed: The method treats generative models as dynamical systems defined by dx_t = b(x_t, t)dt + σ_t dB_t, requiring understanding of drift terms for both diffusion and flow-matching
  - Quick check: Can you derive the drift term b for standard DDIM sampler versus Flow-Matching ODE?

- **Concept: Pontryagin's Maximum Principle (PMP)**
  - Why needed: PMP provides the theoretical foundation for computing optimal control signals through the adjoint equation dp_t/dt = -∇_H
  - Quick check: Why is the optimal control u*_t directly proportional to the negative adjoint state -p*_t?

- **Concept: Vector-Jacobian Products (VJP)**
  - Why needed: Computing adjoint evolution requires ∇_x_t b(x_t, t) without explicitly forming Jacobian matrices in high dimensions
  - Quick check: How would you implement the backward pass for adjoint state p_t without materializing the Jacobian matrix?

## Architecture Onboarding

- **Component map:** Source Image → Inversion Module → Base Model (drift b) → Adjoint Solver → Control Update → Optimized Trajectory → Edited Image

- **Critical path:** Source Image → Initial Trajectory → Adjoint States → Optimized Trajectory → Edited Image

- **Design tradeoffs:**
  - Depth T: Deeper (T→0) allows larger edits but risks source fidelity; shallower (T→1) preserves structure but limits edit magnitude
  - Trajectory Type: Deterministic stable but single result; Markovian allows diversity but introduces noise artifacts
  - Iterations N: Higher N ensures PMP conditions but increases latency linearly (~60s/image)

- **Failure signatures:**
  - Artifacts/Grid patterns: Low N or high learning rate
  - Reward Hacking: Gray/adversarial images from excessive reward weight
  - Structure Loss: Poor inversion fails to capture high-frequency details

- **First 3 experiments:**
  1. Ablation on Iterations (N=1 vs N=25): Expect N=1 yields high-reward broken structure; N=25 yields coherent images
  2. Human Preference Task: Compare LPIPS vs Reward Score against DPS/Gradient Ascent baselines
  3. Trajectory Style Comparison: Test Deterministic vs Markovian on Flow-Matching models for stability

## Open Questions the Paper Calls Out
None

## Limitations
- Performance relies on Pontryagin's Maximum Principle providing globally optimal solutions, but real image manifolds may contain local optima
- Iterative approach convergence guarantees are limited to local convexity assumptions
- Global trajectory optimization claims are difficult to verify given black-box diffusion model complexity

## Confidence
- **High confidence**: Adjoint state backpropagation mechanism is theoretically sound and empirically validated
- **Medium confidence**: Iterative coordinate descent shows promising results but convergence guarantees are limited
- **Low confidence**: Global trajectory optimization effectiveness is difficult to verify across diverse scenarios

## Next Checks
1. **Convergence Analysis**: Test trajectory optimization across broader initial conditions and reward functions to quantify global vs local optima convergence rates
2. **Generalization Testing**: Evaluate robustness on out-of-distribution images and adversarial reward functions to test reward hacking resistance
3. **Computational Complexity Scaling**: Measure performance and quality trade-offs at higher resolutions (1024x1024) focusing on adjoint computation time and memory requirements