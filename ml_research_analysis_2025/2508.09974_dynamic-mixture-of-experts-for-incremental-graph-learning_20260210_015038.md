---
ver: rpa2
title: Dynamic Mixture-of-Experts for Incremental Graph Learning
arxiv_id: '2508.09974'
source_url: https://arxiv.org/abs/2508.09974
tags:
- data
- learning
- graph
- experts
- blocks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in graph incremental
  learning by proposing a Dynamic Mixture-of-Experts (DyMoE) framework. The key idea
  is to dynamically add expert networks specialized for each incoming data block while
  maintaining existing experts, with a gating mechanism to route nodes to the most
  relevant experts.
---

# Dynamic Mixture-of-Experts for Incremental Graph Learning

## Quick Facts
- arXiv ID: 2508.09974
- Source URL: https://arxiv.org/abs/2508.09974
- Reference count: 40
- This paper proposes Dynamic Mixture-of-Experts (DyMoE) for graph incremental learning, achieving 4.92% relative accuracy increase over baselines while maintaining computational efficiency.

## Executive Summary
This paper addresses catastrophic forgetting in graph incremental learning by proposing a Dynamic Mixture-of-Experts (DyMoE) framework. The key idea is to dynamically add expert networks specialized for each incoming data block while maintaining existing experts, with a gating mechanism to route nodes to the most relevant experts. The method includes a block-guided loss to ensure proper expert assignment and a graph block-guided loss to handle topology changes in incremental graph learning. A sparse variant uses only the top-k experts for efficiency. Experiments show DyMoE achieves a 4.92% relative accuracy increase compared to best baselines on class incremental learning tasks while maintaining similar computational efficiency.

## Method Summary
DyMoE is a dynamic Mixture-of-Experts framework for graph incremental learning that addresses catastrophic forgetting by adding a new expert network for each incoming data block. Each expert contains separate parameters for attention (W_Q, W_K, W_V) and MLP components. When training on a new block, only the new expert and all gating vectors are optimized while existing experts are frozen. The framework uses two specialized losses: block-guided loss (L_BL) for correct expert assignment using known block indices, and graph block-guided loss (L_GBL) to handle topology changes by filtering future node neighbors. A sparse variant selects only the top-k most relevant experts for efficiency.

## Key Results
- DyMoE achieves 4.92% relative accuracy increase compared to best baseline methods on class incremental learning tasks
- The framework maintains similar computational efficiency to non-incremental baselines while avoiding catastrophic forgetting
- Experiments demonstrate strong performance across multiple graph datasets including CoraFull, Reddit, Arxiv, and DBLP with up to 49 data blocks
- DyMoE-L (larger expert model) shows 4.92% higher AA than best baseline, while DyMoE-S (smaller model) achieves 3.88% higher AA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Specialized expert networks with dynamic routing mitigate catastrophic forgetting.
- **Mechanism:** A new expert network is added for each incoming data block. A gating mechanism routes input nodes to the most relevant experts based on learned similarity. During training on a new block, only the *new* expert's parameters and *all* gating vectors are updated. Existing expert weights are frozen, preserving their learned representations. This allows selective activation of relevant past knowledge.
- **Core assumption:** Knowledge from different data blocks is best represented by separate, specialized parameters rather than a monolithic, shared representation.
- **Evidence anchors:**
  - [abstract] "...DyMoE GNN layer adds new expert networks specialized in modeling the incoming data blocks."
  - [section 3.1] "When training on a new data block $t$, we only optimize the new expert and all the gating vectors... this training scheme completely preserves the knowledge obtained from previous data blocks."
  - [corpus] Corpus contains related work on MoE for incremental learning (e.g., MoTE, Unified Class and Domain Incremental Learning), suggesting this is a recognized architectural pattern, though specific implementations vary.
- **Break condition:** This mechanism fails if the gating vectors cannot learn to accurately route inputs to the correct expert (poor specialization) or if the frozen experts' knowledge becomes fundamentally incompatible with significant distribution shifts.

### Mechanism 2
- **Claim:** Block-guided and graph block-guided losses enforce expert specialization and handle graph topology changes.
- **Mechanism:** A `block-guided loss` ($L_{BL}$) uses the known data block index of a node as a supervisory signal. It applies cross-entropy loss between the gating values and a one-hot vector indicating the node's originating block, forcing correct routing. The `graph block-guided loss` ($L_{GBL}$) addresses the problem where new nodes become neighbors of old nodes. It learns a gating value ($\beta_{u,t}$) to predict if a node existed *before* an expert was created, using this to mask attention from future nodes in old experts' message-passing.
- **Core assumption:** The data block index is a strong proxy for the optimal expert assignment, and suppressing information from future nodes helps recover the original graph topology known by old experts.
- **Evidence anchors:**
  - [section 3.1] "We propose a block-guided loss to train the gating vectors for correct expert assignment... The loss forces an expertâ€™s corresponding data and gating vector to have large similarities."
  - [section 3.2] "The loss encourages the gating value to be 1 if the experts are added after the node, and 0 otherwise... This loss ensures that... the expert gets the input that it recognizes from training."
  - [corpus] No direct corpus evidence found for these specific loss functions for graph incremental learning.
- **Break condition:** These losses may fail if the block index is a poor proxy for feature similarity or if aggressively masking future neighbors removes critical information needed for accurate prediction on old nodes.

### Mechanism 3
- **Claim:** Sparse expert selection maintains computational efficiency as the model scales.
- **Mechanism:** The `Sparse DyMoE` variant modifies the gating mechanism to compute output using only the top-$k$ most relevant experts. This avoids the $O(t)$ cost of evaluating all $t$ experts, reducing it to $O(k)$. Load balancing techniques are used during training to ensure all experts, including new ones, get sufficient training data.
- **Core assumption:** Only a small subset of experts ($k$) is needed to make accurate predictions for any given input.
- **Evidence anchors:**
  - [section 3.3] "...only the experts with the top-k importance score are used to generate predictions... significantly reducing the training and inference cost."
  - [table 3] Shows DyMoE training and inference times are comparable to baselines and significantly lower than the retrain baseline.
  - [corpus] Sparse MoE is a known technique, cited in the paper.
- **Break condition:** Efficiency gains are lost if `k` must be set high to maintain accuracy, or if load balancing noise significantly slows convergence.

## Foundational Learning

- **Concept:** **Catastrophic Forgetting in Graph Neural Networks**
  - **Why needed here:** This is the core problem DyMoE solves. In incremental graph learning, new data blocks connect to the old graph. Standard fine-tuning alters the model to favor the new distribution, degrading performance on old nodes whose predictions may now depend on new neighbors.
  - **Quick check question:** If you fine-tune a GNN on a new subgraph that connects to an old subgraph, what happens to the model's accuracy on the old subgraph's nodes and why?

- **Concept:** **Mixture-of-Experts (MoE) Architecture**
  - **Why needed here:** DyMoE is an MoE variant. It's critical to understand the basic components: multiple sub-networks ("experts"), a trainable "gating network" (router), and the weighted combination of expert outputs. The novelty here is the *dynamic* addition of experts.
  - **Quick check question:** In a standard MoE layer for a single task, how does the gating network decide which experts to use for a given input?

- **Concept:** **Stability-Plasticity Dilemma**
  - **Why needed here:** The paper frames its contribution in terms of this fundamental continual learning trade-off. Stability (retaining old knowledge) is achieved by freezing old experts; plasticity (learning new knowledge) is achieved by training a new expert and the router.
  - **Quick check question:** How does freezing the parameters of old experts affect "stability," and how does adding a new expert address "plasticity"?

## Architecture Onboarding

- **Component map:** The core DyMoE layer consists of a GNN layer (e.g., Transformer Graph Convolution) where key components (MLPs, Attention weights) are replicated for each expert. Two sets of gating vectors exist: one set ($\boldsymbol{g}_i$) for expert selection and another ($\boldsymbol{p}_i$) for determining node existence relative to expert creation time.

- **Critical path:**
    1.  **Input:** A target node and its sampled subgraph (neighbors from different blocks).
    2.  **Routing:** For each node, the gating mechanism computes relevance scores against all gating vectors.
    3.  **Sparse Selection:** The top-$k$ experts are selected.
    4.  **Message Passing:** Active experts process assigned nodes. Graph block-guided gating ($\beta$) modifies attention scores, down-weighting neighbors from future blocks.
    5.  **Output Combination:** Outputs from top-$k$ experts are combined via a weighted sum.
    6.  **Loss:** Total loss = $L_{cls} + \gamma L_{BL} + \delta L_{GBL}$.

- **Design tradeoffs:**
    - **Memory size (`p`):** A larger memory set helps train the gating network but increases footprint and time. The paper uses small `p` (0.01-0.05).
    - **Active experts (`k`):** Higher `k` improves accuracy but can increase forgetting and computation. A trade-off point exists.
    - **Expert Capacity:** A larger expert model (DyMoE-L) gives better performance but more parameters and slower inference.

- **Failure signatures:**
    - **Expert Collapse:** Gating network always selects the same few experts, causing undertraining of others. Mitigated by load balancing and block-guided loss.
    - **Topology Mismatch:** Performance degrades on old nodes despite correct routing. Suggests graph block-guided loss is failing to mask future neighbors, disrupting old experts' learned attention patterns.

- **First 3 experiments:**
    1.  **Sanity Check:** Replicate the "Expert Specialization" result (Figure 7). Train DyMoE with and without the block-guided loss (`gamma=0`). Plot expert accuracy per data block to confirm specialization.
    2.  **Ablation of Graph-Specific Loss:** Run an ablation on the graph block-guided loss (`delta=0`) using a dataset with significant temporal topology change (e.g., Arxiv). Measure AA and AF to quantify the benefit of masking future neighbors.
    3.  **Scaling Analysis:** Measure training time and memory usage while increasing data blocks. Compare full DyMoE vs. Sparse DyMoE (`k` fixed) to validate efficiency claims.

## Open Questions the Paper Calls Out

- **Question:** How does DyMoE's performance degrade when scaling to extremely long data sequences (e.g., over 1000 data blocks), and what architectural modifications are needed to maintain effective expert routing?
  - **Basis in paper:** [explicit] The authors state: "we plan to extend our work to handle extremely long data sequences (over 1000 data blocks) in future work" and acknowledge "our model may have trouble locating the correct experts when there are too many data blocks, resulting in compromised performance."
  - **Why unresolved:** All experiments used at most 49 data blocks (Elliptic dataset). The current sparse gating mechanism with top-k selection may struggle to differentiate among hundreds of experts, and computational overhead from maintaining expert-specific gating vectors scales linearly.
  - **What evidence would resolve it:** Empirical evaluation on streaming graph datasets with 100+ blocks, analysis of gating accuracy as expert count increases, and proposed mechanisms (e.g., expert merging, hierarchical routing) that preserve performance at scale.

## Limitations

- The paper acknowledges that DyMoE may struggle with extremely long data sequences (over 1000 blocks) due to difficulties in expert routing and maintaining performance
- Implementation details for memory set representativeness scoring and class-proportion allocation are not fully specified, requiring access to code for faithful reproduction
- The relationship between gating accuracy and forgetting is assumed but not directly measured, leaving uncertainty about how routing errors impact overall performance

## Confidence

- **High Confidence:** The core mechanism of dynamic expert addition with frozen parameters for stability is well-specified and theoretically sound
- **Medium Confidence:** The effectiveness of block-guided and graph block-guided losses is supported by ablation results, though the latter's specific impact on topology handling needs independent verification
- **Medium Confidence:** Computational efficiency claims are supported by runtime tables, but scaling behavior beyond the tested block counts remains unverified

## Next Checks

1. **Expert Specialization Verification:** Reproduce the expert accuracy per block plot (Figure 7) to confirm that each expert specializes in its corresponding data block, particularly when block-guided loss is disabled
2. **Graph Block-Guided Loss Ablation:** Test DyMoE without the graph block-guided loss on Arxiv to quantify its specific benefit for handling topology changes between blocks
3. **Sparse Scaling Analysis:** Measure training time and memory usage as the number of data blocks increases, comparing full DyMoE against Sparse DyMoE with fixed k=3 to validate efficiency claims across scales