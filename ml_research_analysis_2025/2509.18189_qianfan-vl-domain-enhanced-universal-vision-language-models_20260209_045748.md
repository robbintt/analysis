---
ver: rpa2
title: 'Qianfan-VL: Domain-Enhanced Universal Vision-Language Models'
arxiv_id: '2509.18189'
source_url: https://arxiv.org/abs/2509.18189
tags:
- data
- understanding
- arxiv
- reasoning
- qianfan-vl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Qianfan-VL is a series of vision-language models (3B to 70B parameters)\
  \ developed to address the trade-off between general multimodal capabilities and\
  \ domain-specific expertise. The models employ a four-stage progressive training\
  \ pipeline\u2014cross-modal alignment, general knowledge injection, domain enhancement,\
  \ and instruction tuning\u2014combined with high-precision data synthesis for six\
  \ major task categories including document OCR, mathematics, charts, tables, formulas,\
  \ and natural scene OCR."
---

# Qianfan-VL: Domain-Enhanced Universal Vision-Language Models

## Quick Facts
- **arXiv ID**: 2509.18189
- **Source URL**: https://arxiv.org/abs/2509.18189
- **Reference count**: 40
- **Primary result**: Domain-enhanced vision-language models achieving state-of-the-art performance on document OCR, mathematics, and chart understanding while maintaining general multimodal capabilities

## Executive Summary
Qianfan-VL is a series of vision-language models (3B to 70B parameters) designed to address the trade-off between general multimodal capabilities and domain-specific expertise. The models employ a four-stage progressive training pipeline—cross-modal alignment, general knowledge injection, domain enhancement, and instruction tuning—combined with high-precision data synthesis for six major task categories. Training is performed entirely on Baidu's Kunlun P800 chips with over 90% scaling efficiency on 5000+ chips. Qianfan-VL achieves state-of-the-art performance on domain-specific benchmarks including DocVQA (94.75%), OCRBench (873), and MathVista (78.60%), while maintaining competitive performance on general multimodal benchmarks.

## Method Summary
Qianfan-VL uses a modular architecture combining InternViT vision encoder, cross-modal MLP adapter, and Llama/Qwen LLM backbone. The training pipeline consists of four progressive stages: (1) cross-modal alignment with frozen backbones, (2) general knowledge injection with full parameter training, (3) domain enhancement with 70% domain-specific and 30% general data sampling, and (4) instruction tuning. Domain expertise is added through high-quality synthetic data generation pipelines with multi-layer quality control for document OCR, mathematics, charts, tables, formulas, and scene OCR. Token-activated chain-of-thought reasoning with process supervision enhances mathematical problem-solving capabilities.

## Key Results
- **DocVQA**: 94.75% (state-of-the-art, +4.03% over previous best)
- **OCRBench**: 873 score (state-of-the-art, +12 points over previous best)
- **MathVista-mini**: 78.60% (70B variant, state-of-the-art)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Four-stage progressive training enables domain enhancement without catastrophic forgetting of general capabilities.
- Mechanism: Stage 1 (100B tokens, adapter-only) establishes cross-modal alignment with frozen vision and language backbones. Stage 2 (2.66T tokens, full parameters) injects general knowledge. Stage 3 (0.32T tokens, 70% domain + 30% general sampling) adds domain expertise while maintaining broad capabilities. Stage 4 (1B tokens) refines instruction following.
- Core assumption: Curriculum-style progression and general data replay during domain training preserves previously learned representations.
- Evidence anchors:
  - [section 3.1] "Without [Stage 1], we observe unstable loss curves during the early phase of Stage 2, which negatively impacts final model performance."
  - [section 5.4, Table 8-9] Ablation shows Stage 3 improves all 16 evaluated tasks with no regressions; handwritten text +8.20%, Mathvision +6.02%
  - [corpus] Parameter Importance-Driven Continual Learning paper addresses similar forgetting challenges but uses different methods; no direct test of this specific 4-stage approach
- Break condition: If domain-specific data ratio in Stage 3 exceeds ~70% or general sampling is removed, expect general benchmark degradation.

### Mechanism 2
- Claim: Data synthesis pipelines with multi-stage quality control enable high-quality domain-specific training at scale.
- Mechanism: Each of six pipelines (document OCR, math, charts, tables, formulas, scene OCR) combines: (1) diverse data sources, (2) traditional CV models + programmatic generation, (3) LLM/VLM-based generation, (4) multi-layer quality control (rule-based + model-based + human validation). Document OCR uses multi-VLM cross-validation; math uses rejection sampling with 5-10 solutions per problem.
- Core assumption: Synthetic data with rigorous quality filtering approximates real-world task distributions well enough for transfer.
- Evidence anchors:
  - [section 3.2] Quality assurance employs "multi-VLM cross-validation with agreement thresholds" and "rejection sampling to generate 5-10 solutions per problem"
  - [section 5.2] DocVQA 94.75%, OCRBench 873, ChartQA 89.60% suggest synthesis quality
  - [corpus] V-Zero and Simple Vision-Language Math Reasoning use synthesis but with different validation approaches; limited cross-paper validation of this specific pipeline design
- Break condition: If quality control layers are reduced (e.g., single-model validation only), expect increased hallucination on edge cases.

### Mechanism 3
- Claim: Token-activated chain-of-thought with special delimiter tokens enhances multi-step reasoning in mathematical and logical tasks.
- Mechanism: Special tokens (`` and ``) delineate reasoning processes. Training corpus includes ~200K mathematical problems with long CoT traces generated by models like DeepSeek-R1. Process supervision verifies intermediate steps, not just final answers.
- Core assumption: Explicit reasoning traces with structured delimiters improve model's ability to decompose complex problems.
- Evidence anchors:
  - [section 3.4] "Token-Activated Reasoning: We introduce special tokens (  and ) to delineate reasoning processes"
  - [section 5.3, Table 7] Mathvista-mini 78.60% (70B), Mathvision 50.29%, Mathverse 61.04% with CoT enabled
  - [corpus] Reason-RFT and X-Reasoner papers explore CoT for visual reasoning but use different training strategies; mechanism not independently validated
- Break condition: If CoT training data lacks process supervision or intermediate step verification, reasoning quality may degrade on out-of-distribution problems.

## Foundational Learning

- Concept: Vision-Language Model Architecture Components
  - Why needed here: Qianfan-VL combines three modular components (vision encoder, adapter, LLM backbone); understanding their interaction is essential for debugging and customization.
  - Quick check question: Can you explain why Stage 1 freezes the vision encoder and LLM while only training the MLP adapter?

- Concept: Catastrophic Forgetting and Replay Strategies
  - Why needed here: The core challenge is adding domain expertise without losing general capabilities; Stage 3's 70/30 domain/general split is the mitigation strategy.
  - Quick check question: What would happen to general benchmark performance if Stage 3 used 100% domain-specific data?

- Concept: Chain-of-Thought Reasoning with Process Supervision
  - Why needed here: Mathematical and logical reasoning gains come from explicit reasoning traces with intermediate verification, not just final-answer supervision.
  - Quick check question: How does process supervision differ from outcome-only supervision in CoT training?

## Architecture Onboarding

- Component map:
  Image → dynamic tiling → InternViT encoding → MLP adapter → LLM embedding space → autoregressive generation

- Critical path: Image → dynamic tiling → InternViT encoding → MLP adapter → LLM embedding space → autoregressive generation. For CoT: special tokens trigger reasoning mode with intermediate steps.

- Design tradeoffs:
  - 3B variant: Edge deployment, no CoT, lower math performance but efficient
  - 8B/70B variants: CoT enabled, superior reasoning (Mathvista 78.60% for 70B), higher compute cost
  - 32K context limit: Constrains long-document processing; 128K planned via sparse attention

- Failure signatures:
  - Unstable loss in Stage 2 early phase → Stage 1 alignment was skipped or insufficient
  - General benchmark regression after domain training → General data sampling in Stage 3 reduced below 30%
  - Hallucination on OCR edge cases → Quality control layers in synthesis pipeline reduced
  - Poor math reasoning despite CoT → Process supervision missing or intermediate step validation weak

- First 3 experiments:
  1. Reproduce Stage 1 ablation: Train with and without cross-modal alignment on small scale (~10B tokens) to confirm loss stability effect.
  2. Test domain/general ratio sensitivity: Train Stage 3 variants with 60/40, 70/30, 80/20 splits; measure general benchmark degradation.
  3. Validate synthesis quality: Train on synthetic vs. real document data subsets; compare DocVQA and OCRBench scores to estimate synthesis-to-real transfer gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the context window be effectively extended to 128K tokens or beyond to support lengthy document processing and extended multi-turn conversations?
- Basis in paper: [explicit] The authors state in Section 6 that the current 32K limit restricts applications and that they are "actively working on extending the context window to 128K tokens and beyond through techniques like sparse attention."
- Why unresolved: The paper does not detail the specific architectural modifications required to handle the memory and computational demands of 128K context in a multimodal setting.
- What evidence would resolve it: A technical report or ablation study demonstrating successful training and inference at 128K context lengths using the proposed sparse attention mechanisms.

### Open Question 2
- Question: What specific data composition or interleaving strategies are required to close the performance gap on general knowledge benchmarks like MMMU and MMVet without diluting domain expertise?
- Basis in paper: [inferred] Section 5.1 identifies lower performance on MMMU and MMVet, attributing it to "insufficient coverage of general knowledge questions." The authors suggest "incorporating more interleaved image-text data" as a future solution but have not yet validated this approach.
- Why unresolved: The current four-stage training pipeline optimizes for domain tasks, and the exact "recipe" for balancing general world knowledge with specialized OCR/math skills remains an open optimization problem.
- What evidence would resolve it: Evaluation results from a model variant trained with the proposed interleaved general knowledge data, showing improved MMMU scores while maintaining current DocVQA/MathVista performance.

### Open Question 3
- Question: Can Native Resolution ViT (NaViT) techniques be successfully integrated to reduce computational overhead while maintaining the high accuracy required for document understanding?
- Basis in paper: [explicit] Section 6 notes that current high-resolution processing requires substantial resources and lists the "integration of NaViT... to process images at their native resolutions" as a planned technical solution for reducing overhead.
- Why unresolved: The paper currently relies on a dynamic tiling strategy for 4K resolution. It is unclear if NaViT can handle the dense text found in documents as effectively as the current patch-based method without sacrificing accuracy.
- What evidence would resolve it: A comparative study of inference latency and FLOPs between the current tiling approach and the NaViT approach on standard document understanding benchmarks.

## Limitations

- Heavy reliance on synthetic data with limited independent verification of real-world generalization beyond reported benchmarks
- Training infrastructure requirements (5000+ Kunlun chips) create significant reproducibility barriers
- 32K context window constraint may limit performance on extremely long documents

## Confidence

**High Confidence**: The four-stage progressive training pipeline effectively achieves domain-specific performance gains without catastrophic forgetting of general capabilities. This is strongly supported by the ablation studies showing consistent improvements across all 16 evaluated tasks when Stage 3 is included, with no regressions in general benchmarks.

**Medium Confidence**: The synthetic data generation pipelines with multi-layer quality control produce high-quality training data that generalizes to real-world benchmarks. While performance metrics are strong (DocVQA 94.75%, OCRBench 873, ChartQA 89.60%), the lack of direct comparison between synthetic-only and mixed synthetic/real training makes it difficult to quantify the exact contribution of the synthesis approach versus the training methodology.

**Medium Confidence**: Token-activated chain-of-thought reasoning with process supervision significantly enhances mathematical and logical reasoning capabilities. The improvements on MathVista (78.60%), MathVision (50.29%), and MathVerse (61.04%) are substantial, but the evaluation focuses primarily on structured mathematical problems rather than testing the robustness of this reasoning approach on more diverse or ambiguous visual reasoning tasks.

## Next Checks

1. **Synthetic-to-Real Transfer Gap Analysis**: Train identical model variants using only synthetic data versus mixed synthetic/real data on the same domain tasks, then compare performance on held-out real-world benchmarks to quantify the quality gap and identify failure modes specific to synthetic data.

2. **Domain Generalization Robustness Test**: Evaluate domain-enhanced models on out-of-distribution variations within each domain (e.g., document OCR on unseen fonts, math problems with novel problem types) to assess whether performance gains transfer beyond the specific synthetic distributions used during training.

3. **Stage 3 Sampling Ratio Sensitivity**: Systematically vary the domain-to-general data ratio in Stage 3 (50/50, 60/40, 70/30, 80/20, 90/10) while monitoring both domain-specific performance gains and general capability retention to identify the optimal balance point and potential breaking points for catastrophic forgetting.