---
ver: rpa2
title: 'scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction'
arxiv_id: '2505.05612'
source_url: https://arxiv.org/abs/2505.05612
tags:
- drug
- cancer
- data
- cell
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: scDrugMap is a framework for benchmarking foundation models on
  single-cell drug response prediction, using 326,751 primary cells and 18,856 validation
  cells across 36 datasets. It evaluates eight single-cell models and two large language
  models under pooled-data and cross-data settings with layer freezing and LoRA fine-tuning.
---

# scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction

## Quick Facts
- arXiv ID: 2505.05612
- Source URL: https://arxiv.org/abs/2505.05612
- Reference count: 40
- Primary result: First large-scale benchmark of foundation models for single-cell drug response prediction

## Executive Summary
scDrugMap introduces a comprehensive benchmarking framework for evaluating foundation models on single-cell drug response prediction. The study assesses eight single-cell models and two large language models across 36 datasets containing 326,751 primary cells and 18,856 validation cells. Using both pooled-data and cross-data evaluation settings with layer freezing and LoRA fine-tuning, the framework identifies scFoundation as the top performer in pooled-data evaluation (F1 scores of 0.971 with freezing and 0.947 with fine-tuning), outperforming the weakest model by 54-57%. In cross-data evaluation, UCE achieved the highest F1 score of 0.774 after fine-tuning, while scGPT excelled in zero-shot learning with F1 0.858. The framework also provides an accessible web tool for drug discovery research.

## Method Summary
The scDrugMap framework evaluates foundation models through two distinct settings: pooled-data evaluation where models are trained and tested on the same dataset, and cross-data evaluation testing zero-shot and fine-tuning capabilities across different datasets. The study employs layer freezing and LoRA fine-tuning techniques to optimize model performance. Eight single-cell models (scFoundation, scGPT, scBERT, scFormer, scT, scBERT-PT, scVAE, scGNN) and two large language models (UCE, Geneformer) are assessed using F1 scores as the primary metric. The evaluation encompasses 36 datasets with 326,751 primary cells for training and 18,856 validation cells for testing.

## Key Results
- scFoundation achieved F1 scores of 0.971 (freezing) and 0.947 (fine-tuning) in pooled-data evaluation
- UCE achieved highest F1 score of 0.774 in cross-data evaluation after fine-tuning
- scGPT achieved F1 score of 0.858 in zero-shot learning cross-data evaluation
- scFoundation outperformed weakest model by 54-57% in pooled-data evaluation

## Why This Works (Mechanism)
None

## Foundational Learning
None

## Architecture Onboarding
**Component Map:**
Preprocessing -> Model Selection -> Training (Freezing/Fine-tuning) -> Evaluation -> Web Interface

**Critical Path:**
Data preprocessing -> Model fine-tuning -> Cross-data validation -> Performance benchmarking

**Design Tradeoffs:**
- Freezing vs. fine-tuning: Freezing preserves pre-trained knowledge but limits adaptation, while fine-tuning allows dataset-specific learning but risks overfitting
- Single-cell vs. large language models: Single-cell models specialize in cellular features, while LLMs leverage broader biological context
- Pooled vs. cross-data evaluation: Pooled tests in-distribution performance, cross-data tests generalization

**Failure Signatures:**
- Poor cross-data performance indicating overfitting to training distributions
- Inconsistent results between freezing and fine-tuning suggesting model instability
- Low F1 scores across all models indicating potential dataset quality issues

**3 First Experiments:**
1. Compare scFoundation performance with and without LoRA fine-tuning on pooled data
2. Evaluate zero-shot learning capability of UCE across different cancer types
3. Test model robustness by varying the fraction of validation cells from 10% to 50%

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation metrics focus primarily on F1 scores, potentially overlooking other clinically relevant measures
- Validation set represents only 5.7% of total dataset, which may limit generalizability
- Cross-data evaluation uses a specific subset of 12 datasets that may not represent full diversity of drug responses

## Confidence
- High Confidence: Pooled-data evaluation results showing scFoundation's superior performance with F1 scores of 0.971 (freezing) and 0.947 (fine-tuning)
- Medium Confidence: Cross-data evaluation results showing UCE's lead after fine-tuning (F1 0.774) and scGPT's zero-shot performance (F1 0.858)
- Low Confidence: Claim of being the "first large-scale benchmark" requires verification

## Next Checks
1. Replicate cross-data evaluation using an expanded set of datasets to verify robustness of UCE and scGPT performance
2. Conduct additional validation using alternative metrics such as AUPRC and concordance index
3. Perform external validation using an independent, held-out test set from a different source