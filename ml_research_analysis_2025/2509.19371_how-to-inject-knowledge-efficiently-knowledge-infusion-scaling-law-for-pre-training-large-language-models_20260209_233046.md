---
ver: rpa2
title: How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training
  Large Language Models
arxiv_id: '2509.19371'
source_url: https://arxiv.org/abs/2509.19371
tags:
- knowledge
- subject
- object
- infusion
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing domain knowledge
  infusion during large language model pretraining. The authors systematically vary
  knowledge injection frequency, model scale (137M-3B parameters), and training tokens
  (up to 100B) to investigate memorization dynamics.
---

# How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models

## Quick Facts
- **arXiv ID:** 2509.19371
- **Source URL:** https://arxiv.org/abs/2509.19371
- **Reference count:** 25
- **Primary result:** Knowledge infusion scaling law predicts optimal injection frequency based on model size and training tokens to avoid memory collapse.

## Executive Summary
This paper systematically investigates how knowledge infusion frequency affects memorization during LLM pretraining. Through experiments varying model scale (137M-3B parameters) and training tokens (up to 100B), the authors identify a "Memory Collapse Phenomenon" where excessive knowledge injection degrades retention. They propose a parametric model P(F) = a·Fᵇ·exp(-c·F) that captures the optimal balance between knowledge accumulation and oversaturation. The resulting scaling law F(C) = A/C^α + E predicts that larger models reach collapse points earlier and require proportionally less infusion, providing actionable guidelines for efficient domain-specialized LLM development.

## Method Summary
The method involves filtering FineWeb-Edu corpus to remove evaluation triples, converting Wikidata triples to natural language questions using templates, and injecting knowledge at controlled frequencies during pretraining of Llama2-style models (137M-3B parameters). Training uses consistent hyperparameters across scales. Memorization rate is evaluated via 4-way multiple choice perplexity. The parametric model P(F) is fit per model to extract optimal frequencies, which are then used to derive the scaling law F(C) = A/C^α + E where C represents compute (6ND).

## Key Results
- Memory collapse phenomenon observed across all model sizes: performance peaks then degrades with excessive knowledge injection
- Larger models reach collapse points earlier and require proportionally less knowledge infusion
- Optimal injection frequency scales super-linearly with training token count
- Scaling law F(C) = A/C^α + E fits experimental data with R² = 0.9685

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excessive knowledge repetition triggers memory collapse where retention degrades sharply past a model-specific threshold
- Mechanism: P(F) = a·Fᵇ·exp(-c·F) combines power-law accumulation (Fᵇ) with exponential saturation decay (exp(-c·F)); analytic optimum at F* = b/c predicts collapse point
- Core assumption: Knowledge acquisition follows smooth unimodal curve transitioning from accumulation to degradation
- Evidence anchors: Figure 2 shows unimodal curves across all model sizes; abstract notes larger models collapse earlier
- Break condition: Non-unimodal performance curves (multi-peak or monotonic) invalidate parametric form

### Mechanism 2
- Claim: Optimal injection frequency inversely correlates with model size—larger models saturate at lower frequencies
- Mechanism: Larger models have greater parametric efficiency, achieving equivalent memorization with fewer exposures; scaling law F(C) = A/C^α + E maps compute to optimal frequency
- Core assumption: Model efficiency gains are consistent across scales, following power-law analogous to Chinchilla scaling
- Evidence anchors: Figure 5 shows R² = 0.9685 fit; collapse points decrease monotonically with model size
- Break condition: If larger models don't reach collapse earlier, scaling law fails to generalize

### Mechanism 3
- Claim: Increasing training token budgets delays collapse thresholds, requiring proportionally higher injection frequencies
- Mechanism: Larger corpora dilute infused knowledge, requiring super-linear frequency increases to maintain exposure density
- Core assumption: Knowledge retention depends on effective exposure frequency per parameter, not absolute token count
- Evidence anchors: Figure 6 shows collapse points shift rightward with larger D; experiments at 58B, 75B, 100B tokens confirm pattern
- Break condition: If collapse thresholds don't shift predictably with D, law cannot extrapolate across token budgets

## Foundational Learning

- **Concept: Catastrophic forgetting**
  - Why needed here: The core trade-off between under-infusion and over-infusion directly references forgetting of previously acquired knowledge
  - Quick check question: Explain why fine-tuning on narrow domains sometimes degrades general capabilities and how this relates to memory collapse

- **Concept: Scaling laws (Kaplan/Chinchilla)**
  - Why needed here: Knowledge infusion scaling law builds on Chinchilla's compute-optimal framework, extending it to predict infusion thresholds
  - Quick check question: What is Chinchilla's scaling law functional form, and how does F(C) = A/C^α + E analogously map compute to a different optimal quantity?

- **Concept: Power-law vs. exponential decay**
  - Why needed here: The parametric equation combines Fᵇ (power-law growth) with exp(-c·F) (exponential decay); understanding both is essential to interpret the curve's rise and collapse
  - Quick check question: If b = 0.5 and c = 0.02, at what frequency F does P(F) peak, and what does this imply about the balance between accumulation and saturation?

## Architecture Onboarding

- **Component map:** Corpus preparation (FineWeb-Edu → entity/relation filtering → controlled triple injection) → Training (Llama2-style Transformers 137M-3B) → Evaluation (Triple→question→4-option multiple choice→PPL-based selection) → Scaling law fitting (L-BFGS-B optimization)

- **Critical path:** 1) Filter corpus to remove evaluation triple entities 2) Inject knowledge at controlled frequencies 3) Train models and evaluate PPL-based memorization 4) Fit P(F) per model and extract F* = b/c 5) Fit cross-model law F(C) = A/C^α + E

- **Design tradeoffs:** Lexical filtering (efficient but misses paraphrases) vs semantic filtering (tighter but costly); single template suffices for 3B+ (saves effort); PPL-based evaluation avoids instruction-tuning confounds but may not reflect generative recall

- **Failure signatures:** Non-unimodal P(F) curves invalidate parametric form; collapse points not monotonically decreasing with model size make scaling law unreliable; large prediction error when extrapolating >3B suggests law doesn't hold at production scales

- **First 3 experiments:** 1) Replicate 137M-562M sweep at fixed D with frequencies {10, 50, 100, 200, 500, 1000} to validate unimodal P(F) shape 2) Test corpus scaling: train 300M at D ∈ {58B, 75B, 100B} to verify rightward collapse shift 3) Out-of-sample prediction: use only 137M-1.05B data to predict optimal F for 3B, then validate with targeted experiment

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the knowledge infusion scaling law extrapolate reliably to models significantly larger than 3B parameters (7B, 70B, 175B+)? Basis: Paper notes exploration of larger scales as future work; scaling law validated only on 137M-3B models.

- **Open Question 2:** How does semantic interference from paraphrased knowledge affect scaling law predictions? Basis: Current lexical filtering fails to address semantically equivalent paraphrases that may implicitly reinforce target knowledge.

- **Open Question 3:** What mechanistic dynamics in the model drive the memory collapse phenomenon? Basis: Parametric form was initially derived from empirical fit with post-hoc theoretical motivation; internal cause of collapse not explained.

- **Open Question 4:** Does the scaling law generalize to complex knowledge structures beyond simple (subject, relation, object) triples? Basis: Evaluation uses only six basic relation types; real-world domain knowledge often involves relational chains, temporal facts, or conflicting updates.

## Limitations
- Parametric form assumes unimodal smooth knowledge retention dynamics that may not hold for all pretraining scenarios
- Inverse correlation between model size and optimal injection frequency lacks validation beyond 3B parameters
- Single template generation assumption may not generalize to domains requiring diverse knowledge representation
- PPL-based evaluation may not fully capture generative recall capabilities

## Confidence

- **High confidence:** Memory collapse phenomenon itself is well-supported by unimodal P(F) curves across all tested models and token budgets; fitting methodology is sound
- **Medium confidence:** Knowledge Infusion Scaling Law fits experimental data well (R² = 0.9685) within tested range but extrapolation to larger models remains unproven
- **Low confidence:** Single template assumption for larger models may not generalize; PPL-based evaluation may not reflect real-world performance

## Next Checks

1. **Multi-peak validation:** Train 300M model at frequencies spanning 10-5000 with finer granularity (F ∈ {10, 50, 100, 200, 500, 1000, 2000, 5000}) to verify P(F) remains unimodal and doesn't exhibit multiple peaks that would invalidate parametric fitting

2. **Large-scale extrapolation test:** Use only 137M-1.05B data to fit scaling law, predict optimal F for 7B model, and run targeted experiment at ±20% of predicted value to quantify extrapolation error and assess inverse size-frequency relationship beyond 3B

3. **Corpus diversity stress test:** Repeat 562M model experiment at 58B tokens using heterogeneous corpus (scientific papers, code, dialogue) to verify super-linear frequency-token relationship F(D) remains robust when knowledge dilution effects become more complex