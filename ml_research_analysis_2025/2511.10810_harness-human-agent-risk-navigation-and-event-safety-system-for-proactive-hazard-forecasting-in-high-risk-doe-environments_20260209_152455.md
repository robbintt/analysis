---
ver: rpa2
title: 'HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive
  Hazard Forecasting in High-Risk DOE Environments'
arxiv_id: '2511.10810'
source_url: https://arxiv.org/abs/2511.10810
tags:
- retrieval
- risk
- safety
- system
- ridge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HARNESS is a multi-agent AI system that integrates LLMs with structured
  work data and historical event retrieval to forecast hazards in DOE environments.
  It employs a modular framework combining smart retrieval, failure mode analysis,
  policy matching, and human-in-the-loop refinement.
---

# HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments

## Quick Facts
- arXiv ID: 2511.10810
- Source URL: https://arxiv.org/abs/2511.10810
- Reference count: 29
- Key outcome: Multi-agent AI system integrating LLMs with structured work data and historical event retrieval to forecast hazards in DOE environments

## Executive Summary
HARNESS is a multi-agent AI system that integrates Large Language Models with structured work data and historical event retrieval to forecast hazards in Department of Energy environments. The system employs a modular framework combining smart retrieval, failure mode analysis, policy matching, and human-in-the-loop refinement. Evaluation demonstrated Qwen3-Embedding-8B achieved 75.3% answer correctness with 2.0s query time, while the hybrid retrieval system attained an F1@5 score of 0.384. Generated vulnerability reports scored 5.0 for accuracy and 4.0 for clarity and usefulness, but 3.0 for completeness and specificity.

## Method Summary
HARNESS processes work plans through a multi-agent pipeline: first summarizing documents using GPT-4o, then retrieving relevant historical incidents via hybrid semantic search with query expansion and cross-encoder reranking, followed by failure mode analysis, hazard-control extraction, safety policy integration, and final report generation. The system operates on 65,107 DOE incident documents using 1024-dimensional embeddings, with Qwen3-Embedding-8B selected for optimal balance of correctness (75.3%) and latency (2.0s). A human-in-the-loop mechanism enables SME refinement, though the adaptation mechanism remains underspecified.

## Key Results
- Qwen3-Embedding-8B achieved 75.3% answer correctness with 2.0s query time
- Hybrid retrieval system attained F1@5 score of 0.384
- Generated vulnerability reports scored 5.0 for accuracy, 4.0 for clarity and usefulness, but only 3.0 for completeness and specificity

## Why This Works (Mechanism)

### Mechanism 1
Multi-stage retrieval with semantic expansion and cross-encoder reranking improves relevant incident retrieval over pure similarity search. The Smart Retrieval Agent decomposes queries into subqueries, expands them via paraphrasing (maintaining cosine similarity ≥ 0.8), retrieves top-K chunks via vector search, filters low-similarity candidates (< 0.5), and reranks with a cross-encoder before final selection. Core assumption: Historical incident descriptions contain semantically similar patterns to future work plans, even when surface vocabulary differs.

### Mechanism 2
Specialized agent decomposition with orchestrated handoffs produces more traceable hazard analyses than single-prompt LLM approaches. The Orchestrator Agent coordinates six specialized agents—Summarization, Smart Retrieval, Failure Mode Analysis (FEMA), Hazard-Control Extraction, Safety Policy Integration, and Report Generation—each performing discrete functions with explicit inputs/outputs. Core assumption: Hazard analysis can be decomposed into separable subtasks (retrieval → failure mode identification → policy matching → report synthesis) without losing critical cross-agent context.

### Mechanism 3
SME feedback integration creates an adaptive learning loop that refines predictions over time. Human-in-the-loop mechanism allows SMEs to review and modify generated vulnerability reports, with corrections fed back to improve future retrieval and analysis. Core assumption: SME corrections can be systematically captured and incorporated into system behavior.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: HARNESS grounds LLM outputs in 65,107 historical documents; without RAG, hallucination risk increases in safety-critical contexts
  - Quick check question: Can you explain why RAG reduces hallucination compared to pure LLM generation?

- **Cross-Encoder Reranking**
  - Why needed here: Initial vector retrieval uses approximate similarity; reranking with cross-encoders improves precision for top-k results
  - Quick check question: What is the computational tradeoff between bi-encoder retrieval and cross-encoder reranking?

- **Failure Mode and Effects Analysis (FMEA)**
  - Why needed here: The FEMA engine decomposes hazards into failure modes, causes, and effects—domain knowledge required to interpret agent outputs
  - Quick check question: How does FMEA differ from a simple hazard checklist?

## Architecture Onboarding

- **Component map**: Work plan → Summarization Agent → Smart Retrieval Agent → Failure Mode Analysis Agent → Hazard-Control Extraction Agent → Safety Policy Integration Agent → Report Generation Agent → SME review
- **Critical path**: 1. Work plan input → Summarization Agent → scope/components 2. Summary → Smart Retrieval Agent → ranked historical events 3. Retrieved events + work plan → Failure Mode Analysis Agent → critical failures list 4. Failures → Hazard-Control Extraction Agent → hazard-control pairs 5. Failures + controls → Safety Policy Integration Agent → matched policies 6. All outputs → Report Generation Agent → Final Vulnerability Report 7. Report → SME review → feedback (adaptation mechanism underspecified)
- **Design tradeoffs**: Qwen3-Embedding-8B vs. faster models: Selected for 75.3% correctness vs. SFR's 67.1%, accepting 0.1s latency increase; Hybrid retrieval vs. pure RAG: F1@5 improved from 0.281 to 0.384 at cost of added complexity; LLM-as-Judge evaluation: GPT-4 rated reports 5.0 accuracy but only 3.0 completeness—tradeoff between grounding and coverage
- **Failure signatures**: Low retrieval precision: If F1@5 drops below 0.3, check query expansion thresholds or cross-encoder calibration; Incomplete reports: Specificity/completeness scores of 3.0 indicate missed facility-specific hazards; inspect retrieval coverage for niche terms; SME feedback not incorporated: If reports do not improve after SME review, verify adaptation pipeline implementation
- **First 3 experiments**: 1. Retrieval ablation: Run pure RAG vs. hybrid retrieval on held-out work plans; measure F1@5 and latency to validate Table 2 tradeoffs 2. Embedding model swap: Test OpenAI text-embedding-3-large vs. Qwen3-Embedding-8B on domain-specific queries; verify 60.1% vs. 75.3% correctness gap 3. Report completeness audit: Have SMEs manually score 10 generated reports on 5 dimensions; compare to LLM-as-Judge scores to calibrate evaluation reliability

## Open Questions the Paper Calls Out

- To what extent does HARNESS reduce decision latency compared to manual baseline processes in live operational settings? The abstract lists "decision latency reduction" as a primary focus for future work, but current evaluation focuses on technical metrics rather than end-to-end operational timing.
- Do human Subject Matter Expert (SME) assessments align with the GPT-4-based "LLM-as-Judge" ratings, particularly regarding the identified gaps in report completeness and specificity? The abstract cites "SME agreement" as future work, but Section 4.3 relies entirely on automated LLM evaluation.
- Can modifications to the retrieval or generation agents improve the "completeness" and "specificity" of reports to match the high "accuracy" scores? Section 4.3 highlights that "completeness" and "specificity" scores (3.0) lag significantly behind "accuracy" (5.0), but the paper does not propose specific architectural solutions.

## Limitations

- Performance depends on proprietary DOE incident data (65,107 documents) and safety policy documents that are not publicly accessible, preventing independent validation
- Human-in-the-loop adaptation mechanism is conceptually described but lacks implementation details for how SME feedback systematically improves future outputs
- Report completeness scores of 3.0/5.0 indicate systematic gaps in capturing facility-specific hazards, suggesting the retrieval-augmented generation approach may miss nuanced local contexts

## Confidence

- **High confidence**: Multi-agent architecture decomposition and modular agent responsibilities are well-specified with clear data flows and defined agent roles
- **Medium confidence**: Retrieval performance metrics (F1@5=0.384, 75.3% answer correctness) are internally validated but dependent on proprietary datasets
- **Low confidence**: Adaptation/learning loop from SME feedback lacks concrete implementation details and systematic validation procedures

## Next Checks

1. **Cross-domain transferability test**: Deploy HARNESS on non-DOE high-risk domains (e.g., chemical plants, aviation maintenance) using their incident databases; measure F1@5 degradation and report completeness variation
2. **Feedback loop validation**: Implement systematic SME feedback capture with explicit update rules (e.g., retrieval reindexing, embedding fine-tuning); track report quality improvement over 5+ SME review cycles
3. **Component ablation study**: Compare pure semantic search vs. hybrid retrieval vs. full system on same query sets; measure precision-recall tradeoffs and identify minimum viable retrieval configuration for target accuracy threshold