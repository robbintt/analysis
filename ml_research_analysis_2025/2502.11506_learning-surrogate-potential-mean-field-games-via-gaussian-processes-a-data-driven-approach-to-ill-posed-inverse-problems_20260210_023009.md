---
ver: rpa2
title: 'Learning Surrogate Potential Mean Field Games via Gaussian Processes: A Data-Driven
  Approach to Ill-Posed Inverse Problems'
arxiv_id: '2502.11506'
source_url: https://arxiv.org/abs/2502.11506
tags:
- problem
- points
- recovered
- potential
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the ill-posed inverse problem in potential
  mean field games (MFGs), where the goal is to recover population, momentum, and
  environmental parameters from limited, noisy observations. The authors propose two
  Gaussian process (GP)-based frameworks: an inf-sup formulation and a bilevel approach.'
---

# Learning Surrogate Potential Mean Field Games via Gaussian Processes: A Data-Driven Approach to Ill-Posed Inverse Problems

## Quick Facts
- arXiv ID: 2502.11506
- Source URL: https://arxiv.org/abs/2502.11506
- Reference count: 40
- Key outcome: Gaussian process-based frameworks recover population, momentum, and environmental parameters from limited noisy observations in potential mean field games, yielding data-consistent surrogate models even when the inverse problem is ill-posed.

## Executive Summary
This paper addresses the ill-posed inverse problem in potential mean field games, where the goal is to recover population distribution, momentum, and environmental parameters from limited, noisy observations. The authors propose two Gaussian process-based frameworks: an inf-sup formulation suitable for concave unknowns and a more general bilevel approach. Both methods leverage GP linearity to preserve the convexity structure of the problem. Numerical experiments demonstrate that sufficient prior information enables accurate parameter recovery, while limited priors result in ill-posedness but still yield surrogate MFG models closely matching observed data. The adjoint-based gradient computation offers robustness and independence from the inner solver.

## Method Summary
The method employs two GP-based frameworks for inverse problems in potential MFGs. The inf-sup framework reformulates the problem as a convex-concave min-max optimization when unknowns appear concavely in the objective, using GP linearity to preserve structure. The bilevel approach separates the inner MFG dynamics enforcement from outer data misfit minimization with regularization. GP parameterization maps latent variables to functions like spatial cost V and coupling F, with the representer theorem enabling dimensionality reduction. Gradients are computed via automatic differentiation or adjoint methods, with Monte Carlo convexity penalization enforcing F's convexity when unknown.

## Key Results
- GP-based frameworks successfully recover unknown parameters from partial noisy observations in potential MFGs
- Inf-sup formulation preserves convexity-concavity structure when unknowns appear concavely in the objective
- Bilevel optimization with adjoint gradients produces data-consistent surrogate MFGs even when inverse problem is ill-posed
- Recovered parameters may differ from ground truth but still generate MFG solutions matching observed data

## Why This Works (Mechanism)

### Mechanism 1
Gaussian process parameterization preserves convexity-concavity structure in potential MFG inverse problems when unknowns appear concavely in the objective. GPs are linear function approximators, so when the objective depends linearly on a GP-parameterized unknown (e.g., V), the GP's linearity preserves the original convex structure of the potential MFG variational formulation. This enables reformulation as an inf-sup (min-max) problem solvable via primal-dual convex optimization methods.

### Mechanism 2
Bilevel optimization with adjoint-based gradient computation produces data-consistent surrogate MFGs even when the inverse problem is ill-posed. The bilevel framework separates concerns: the inner level enforces MFG dynamics (ensuring physical consistency), while the outer level minimizes data misfit with regularization. The adjoint method computes exact gradients of the outer objective with respect to parameters by solving a linear adjoint system, avoiding differentiation through the iterative inner solver. This yields gradients that are solver-agnostic and numerically exact.

### Mechanism 3
Monte Carlo convexity penalization enables recovery of general convex coupling functions without parametric assumptions. For unknown F with only convexity assumed, a penalization term -E[⟨F'(ξ)-F'(ζ), ξ-ζ⟩] enforces monotonicity of F' (equivalent to convexity). The expectation is approximated via Monte Carlo sampling. Combined with GP or basis expansion parameterization, this constrains the search space to convex functions while maintaining flexibility.

## Foundational Learning

- **Potential Mean Field Games (variational formulation)**: The entire approach hinges on reformulating MFGs as convex optimization problems with linear PDE constraints. Understanding that potential MFGs minimize ∫[b(m,w) + F(m) + Vm] dx subject to continuity equations is essential for grasping the inf-sup and bilevel frameworks.
  - Quick check question: Can you explain why the Hamilton-Jacobi-Bellman / Fokker-Planck system (1.1) is equivalent to the optimization problem (1.2)?

- **Gaussian Process regression and RKHS**: GP parameterization provides the linear structure that preserves convexity. Understanding posterior mean formulas, kernel functions, and the representer theorem is essential for implementing the latent variable reduction in Sections 3.3 and 4.3.
  - Quick check question: Given observations at points x₁,...,xₙ, what is the closed-form posterior mean of a GP with kernel K?

- **Primal-dual methods for convex optimization**: The inf-sup formulation (3.5) is solved via Chambolle-Pock algorithm. Understanding proximal operators, duality, and convergence is necessary to implement and debug the inner MFG solver.
  - Quick check question: In the primal-dual algorithm, what role does the proximal operator of the convex conjugate play?

## Architecture Onboarding

- **Component map**: Forward MFG solver -> GP parameterization layer -> Outer optimizer -> Convexity enforcement
- **Critical path**: 
  1. Implement forward MFG solver and validate on known solutions
  2. Implement GP parameterization with chosen kernels
  3. Choose framework: inf-sup (if unknown is concave) or bilevel (general case)
  4. Implement gradient computation: start with auto-diff, then add adjoint for robustness
  5. Add convexity penalization if F is unknown
- **Design tradeoffs**:
  - Auto-diff vs Adjoint: Auto-diff is easier to implement but requires differentiating through inner solver iterations; accuracy depends on solver convergence. Adjoint is solver-agnostic and exact but requires deriving and implementing adjoint equations.
  - One-step vs Two-step method: One-step uses all Monte Carlo points as latent variables (high dimension). Two-step uses pseudo-points for F (lower dimension, but approximation error).
  - Parametric vs GP for F: Power function (one parameter) is well-identified; polynomial library (multiple parameters) is less identified; GP is most flexible but most ill-posed.
- **Failure signatures**:
  - Non-converging inner solver: Check that F remains convex during optimization (penalization weight α_fp too low)
  - Gradient explosion with auto-diff: Inner solver may need more iterations or better initialization; switch to adjoint method
  - Recovered parameters don't match ground truth but fit data: This is expected ill-posedness. Check if data misfit is low
  - High sensitivity to regularization weights: Grid search over α parameters; start with α_mo ≫ others
- **First 3 experiments**:
  1. Inf-sup recovery of V only (known F, Λ, q, ν; recover V and m from partial noisy observations)
  2. Bilevel recovery of V and F (power function) (assume F(m) = m^α/α, recover α alongside V)
  3. Full recovery with adjoint method (recover all unknowns from partial data, accept parameter mismatch if data consistent)

## Open Questions the Paper Calls Out

### Open Question 1
Can sufficient conditions for unique identifiability of MFG parameters (particularly the coupling function F and metric matrix Λ) be established, and how does observation density affect uniqueness guarantees? The authors state "the inverse problem is ill-posed, as multiple parameter sets can fit the data without necessarily matching the true parameters" and note that "multiple combinations of F and V may yield identical MFGs that generate the observed data." Numerical experiments show recovered F and Λ consistently differ from ground truth even when data is fit accurately, but no theoretical identifiability analysis is provided.

### Open Question 2
How can the GP-based frameworks be scaled to high-dimensional spatial domains and large populations while maintaining computational tractability? The conclusion states: "Potential extensions include integrating scalable techniques (e.g., Random Fourier Features, sparse GPs, and mini-batch methods) to handle large datasets more efficiently." Current experiments are limited to 2D stationary or 1D time-dependent problems; the GP kernel matrix inversion scales cubically with observation count.

### Open Question 3
Can a fully GP-based potential MFG solver be developed that eliminates the need for finite-difference discretization and existing primal-dual algorithms? The paper states: "While a purely GP-based method for solving potential MFGs is a promising direction for future research, our primary focus is on inverse problems" and "we defer the design of such a solver to future research." The current approach uses GPs only for parameterizing unknowns, relying on finite-difference discretization and the Chambolle-Pock algorithm for solving forward MFG problems.

### Open Question 4
What adaptive loss weighting and prior selection strategies can mitigate ill-posedness when recovering multiple unknown parameters simultaneously? Section 7.2.2 states: "Incorporating tailored priors, adaptive loss weighting, and advanced optimization strategies could help address these challenges in both data-rich and data-scarce settings. We leave the exploration of these directions to future work." Current experiments use fixed regularization parameters chosen manually; simultaneous recovery of all unknowns shows significant deviation in recovered Λ, q, and ν from ground truth.

## Limitations

- The approach fundamentally relies on convexity-preserving properties of GP parameterization, which breaks down when unknown parameters appear non-concavity in the MFG objective
- The ill-posedness of the inverse problem is acknowledged but not fully resolved—recovered parameters may differ significantly from ground truth while still producing valid surrogate models
- The adjoint method derivation is conceptually sound but lacks complete implementation details for the specific MFG discretization used
- The effectiveness of Monte Carlo convexity penalization depends heavily on sample quality and coverage, with limited empirical validation of robustness

## Confidence

- **High confidence**: The preservation of convexity structure through GP linearization when unknowns appear concavely in the objective. The basic bilevel optimization framework and its theoretical justification. Numerical results showing data-consistent surrogate models even with parameter mismatch.
- **Medium confidence**: The adjoint-based gradient computation method's robustness and independence from inner solver details. The Monte Carlo convexity penalization approach for general convex couplings. Implementation details for convergence criteria and kernel hyperparameters.
- **Low confidence**: The method's performance on high-dimensional MFGs beyond the 2D torus examples. Sensitivity to noise levels beyond the γ=10⁻³ tested. Scalability of the two-step GP parameterization approach for very large observation sets.

## Next Checks

1. **Adjoint Method Implementation Validation**: Implement the adjoint gradient computation for the 2D torus MFG case and verify numerical gradients match finite-difference approximations across a grid of parameter values. Compare convergence speed and final objective values against auto-differentiation through the inner solver.

2. **Convexity Enforcement Robustness**: Systematically vary the Monte Carlo sample size and coverage for the convexity penalization term. Test on F functions with different convexity regimes (linear, quadratic, exponential growth) to quantify how sample quality affects recovered coupling functions and MFG solution accuracy.

3. **Parameter Recovery vs Data Fidelity Trade-off**: Conduct a systematic study where ground truth parameters are intentionally mismatched but data fidelity is maintained. Quantify how changes in regularization weights (α_mo, α_vo, α_fp) affect the balance between parameter recovery accuracy and data consistency, particularly for ill-posed cases where multiple parameter sets fit the observations equally well.