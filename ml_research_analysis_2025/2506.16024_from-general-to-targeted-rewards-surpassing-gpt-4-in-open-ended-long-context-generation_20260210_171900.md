---
ver: rpa2
title: 'From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context
  Generation'
arxiv_id: '2506.16024'
source_url: https://arxiv.org/abs/2506.16024
tags:
- proxyreward
- arxiv
- reward
- generation
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of open-ended long text generation
  (Open-LTG), where traditional approaches struggle due to the lack of gold-standard
  reference data and inadequate reward signals. The authors propose ProxyReward, a
  reinforcement learning framework that automatically generates training data through
  LLM-generated meta-questions and proxy question-answer pairs, transforming subjective
  evaluation into objective reading comprehension tasks.
---

# From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation

## Quick Facts
- arXiv ID: 2506.16024
- Source URL: https://arxiv.org/abs/2506.16024
- Reference count: 22
- Surpasses GPT-4-Turbo in open-ended long-context generation accuracy by 20% using ProxyReward framework

## Executive Summary
This paper addresses the challenge of open-ended long text generation (Open-LTG), where traditional approaches struggle due to the lack of gold-standard reference data and inadequate reward signals. The authors propose ProxyReward, a reinforcement learning framework that automatically generates training data through LLM-generated meta-questions and proxy question-answer pairs, transforming subjective evaluation into objective reading comprehension tasks. The framework then uses these proxy QA pairs to create targeted reward signals that assess information comprehensiveness and accuracy. Experimental results show ProxyReward significantly improves performance on the ProxyQA benchmark by 20% when applied to widely used open-source models, surpassing both the LLM-as-a-Judge approach and even GPT-4-Turbo, achieving state-of-the-art results while using substantially fewer parameters.

## Method Summary
ProxyReward is a reinforcement learning framework that addresses open-ended long text generation by transforming subjective evaluation into objective reading comprehension tasks. The framework generates a dataset of meta-questions and corresponding proxy question-answer pairs, then uses these to create targeted reward signals. For each meta-question, the generation model produces multiple diverse responses, which are evaluated by a reward model that attempts to answer the proxy questions based solely on the generated text. The reward score is the proportion of correctly answered proxy questions, directly measuring information comprehensiveness and accuracy. Direct Preference Optimization (DPO) is then applied using preference pairs constructed from responses with the highest and lowest scores, with data selection prioritizing meta-questions showing higher score variance.

## Key Results
- Surpasses GPT-4-Turbo (25.73 → 35.07 accuracy) on ProxyQA benchmark using Qwen2.5-7B model
- Improves widely used open-source models by >20% compared to baselines
- Shows consistent improvement across model sizes (1.5B to 7B parameters)
- Achieves state-of-the-art results while using substantially fewer parameters than GPT-4

## Why This Works (Mechanism)

### Mechanism 1: Proxy Question-Answer Transformation
- Claim: Transforming subjective long-form evaluation into objective reading comprehension tasks provides more precise reward signals than holistic quality judgments.
- Mechanism: For each meta-question requiring a long-form answer, the framework generates ~15 boolean proxy questions with predetermined answers. A reward model reads the generated response and attempts to answer these proxy questions. The reward score (Equation 4) is the proportion of correctly answered proxy questions, directly measuring information comprehensiveness and accuracy.
- Core assumption: If a long-form response enables accurate answers to specific questions about its content, it contains the necessary information and coherence for high-quality generation.
- Evidence anchors:
  - [abstract] "...transforming subjective evaluation into objective reading comprehension tasks."
  - [section 4.1] "The main idea is to transform subjective expert evaluations of long-form text quality into objective reading comprehension questions that can be automatically assessed by LLMs."
  - [corpus] Neighboring work "From Verifiable Dot to Reward Chain" similarly extends verifiable rewards to open-ended generation by decomposing evaluation into verifiable components.
- Break condition: If proxy questions fail to cover critical aspects of the meta-question, or if proxy answers contain errors, the reward signal decouples from actual response quality.

### Mechanism 2: Active Exploration with Variance-Based Data Selection
- Claim: Generating multiple diverse responses per meta-question and selecting preference pairs based on score variance improves training efficiency by prioritizing informative training examples.
- Mechanism: The generation model produces k diverse responses per meta-question (temperature=0.8). The framework prioritizes meta-questions showing higher score variance—indicating meaningful quality differences between responses—and filters cases where multiple preference indicators conflict. Highest and lowest scoring responses form preference pairs for DPO training.
- Core assumption: Meta-questions with higher response score variance represent more valuable training signals because they reveal meaningful distinctions between good and poor responses.
- Evidence anchors:
  - [section 4.2] "we extract a meta-question m from the ProxyReward Dataset and input it to our generation LLM, which produces k different responses"
  - [section 5.1 Data Selection] "we prioritize meta-questions that exhibit higher variance in scores for the same response. This approach prevents our dataset from being dominated by simple questions"
  - [corpus] Related work on hierarchical synthetic data generation addresses similar data curation challenges for long-context training.
- Break condition: If score variance stems from reward model noise rather than genuine quality differences, training optimizes for spurious signals.

### Mechanism 3: Iterative Refinement with Scale-Dependent Returns
- Claim: Two-iteration training improves performance for larger models (≥3B parameters), but smaller models may degrade with additional iterations.
- Mechanism: After initial DPO training, the updated model undergoes a second round of response generation and preference pair creation. The ablation shows Iter 2 improves Qwen2.5-3B from 27.58 to 28.54 and Llama-3.1-8B from 29.38 to 30.11, but Qwen2.5-1.5B degrades from 22.10 to 21.54.
- Core assumption: The reward signal quality remains stable as the model improves, creating positive feedback without collapsing into reward hacking.
- Evidence anchors:
  - [section 6.1] "Across all tested models, multiple iterations of ProxyReward consistently outperform single iterations"
  - [table 4] Shows Iter 2 accuracy outperforms Iter 1 for 3B and 7B models, but not for 1.5B
  - [corpus] LongPO similarly uses iterative preference optimization for long-context alignment.
- Break condition: If the model overfits to proxy question patterns rather than learning general generation quality, subsequent iterations amplify this bias.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: The framework uses DPO to avoid training a separate reward model. DPO directly optimizes policy using implicit rewards defined by the policy itself (Equation 3: rθ(x,y) = β log πθ(y|x)/πref(y|x)), improving computational efficiency for iterative training.
  - Quick check question: Why does DPO eliminate the need for a separately trained reward model while still optimizing for preferences?

- Concept: Reading Comprehension as Proxy Evaluation
  - Why needed here: The core innovation reframes quality assessment as comprehension testing. Understanding how boolean QA pairs proxy for holistic evaluation explains why ProxyReward outperforms LLM-as-a-Judge (35.07 vs 25.73 on Qwen2.5-7B).
  - Quick check question: Why would checking whether specific facts can be extracted from generated text be more reliable than asking for an overall quality score?

- Concept: Preference Pair Construction from Continuous Scores
  - Why needed here: The framework converts continuous reward scores into binary preference pairs for DPO. Understanding variance-based data selection helps diagnose training failures.
  - Quick check question: What problem arises when constructing preference pairs from meta-questions where all responses receive nearly identical scores?

## Architecture Onboarding

- Component map: ProxyReward Dataset Generator (GPT-4o-mini) -> Generation Model (Gθ) -> Reward Model (Rref) -> DPO Training Loop

- Critical path:
  1. Generate meta-questions and proxy Q&A pairs (one-time setup)
  2. Per training iteration: Sample meta-question → Generation model produces k responses → Reward model evaluates all responses → Apply data selection → DPO training (lr=5e-7, batch_size=2, grad_accum=8, 5 epochs, max_length=2048)

- Design tradeoffs:
  - **Accuracy vs Precision metrics**: Table 4 shows accuracy-based rewards substantially outperform precision (35.07 vs 25.67 for 7B model)
  - **Iteration count**: 2 iterations optimal for 3B+ models; 1 iteration better for <2B models
  - **Temperature for exploration**: 0.8 balances diversity vs noise
  - **Assumption**: Value of k (responses per meta-question) not explicitly stated; must be sufficient to reveal quality variance

- Failure signatures:
  1. **LLM-as-a-Judge underperforms baseline**: Table 2 shows LLM-as-Judge (12.88, 16.69) below base models (18.30, 27.00), indicating general assessment can be counterproductive
  2. **Small model iteration degradation**: 1.5B model drops from 22.10 (Iter 1) to 21.54 (Iter 2)
  3. **Low reward variance**: Table 5 shows ProxyReward-Accuracy variance (1.34-2.13) lower than Precision (4.07-5.75); insufficient variance breaks data selection

- First 3 experiments:
  1. **Validate data generation pipeline**: Implement meta-question and proxy Q&A generation using Appendix A prompts. Test on 10 examples—verify proxy questions are relevant and most answers are "True" as paper specifies.
  2. **Single-iteration baseline comparison**: Train Qwen2.5-3B with ProxyReward vs LLM-as-a-Judge using identical hyperparameters. Expect ~11-point gap (27.58 vs 16.69). If smaller, debug reward computation first.
  3. **Ablate data selection**: Compare variance-based selection vs random pair selection. If no improvement, verify selected meta-questions actually have higher variance using Table 5-style statistics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the compounding error from LLM-generated proxy question-answer pairs affect the long-term stability and "truthfulness" of models trained with ProxyReward?
- Basis in paper: [explicit] The limitations section states that reliance on LLM-generated pairs "introduces potential biases and errors inherent to the underlying models, which may affect the objectivity and coverage of the reward signals."
- Why unresolved: While the authors demonstrate performance gains, they do not analyze how hallucinations or biases in the "ground truth" proxy answers (generated by GPT-4o-mini) might propagate to the student model, a common issue in RL from AI Feedback (RLAIF).
- What evidence would resolve it: An error analysis quantifying the correlation between incorrect proxy answers in the training set and specific hallucinations or biases observed in the fine-tuned model's outputs.

### Open Question 2
- Question: Can the ProxyReward framework maintain its efficacy if the expensive "reward model" (GPT-4o-mini) is replaced by a smaller, open-source model?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that the framework "still requires access to high-performing LLMs for reward computation, which may pose computational and cost challenges for some practitioners."
- Why unresolved: The paper relies exclusively on GPT-4o-mini for generating the reward signals; it is unknown if weaker, cheaper models can accurately enough distinguish between "comprehensive" and "incomplete" long-form text to provide a useful training signal.
- What evidence would resolve it: Comparative experiments evaluating the ProxyQA scores of models trained using rewards generated by open-source models (e.g., Llama-3-70B or smaller) versus the proprietary baseline.

### Open Question 3
- Question: What specific mechanisms can mitigate performance degradation in smaller models during multi-iteration ProxyReward training?
- Basis in paper: [inferred] In Table 4, the Qwen2.5-1.5B model shows a performance drop in Iteration 2 (21.54%) compared to Iteration 1 (22.10%), whereas larger models consistently improve. The authors hypothesize this is due to noise but offer no solution.
- Why unresolved: The paper confirms the negative impact of additional iterations on small models but does not investigate if this is an inherent capacity limitation or a failure of the data selection/metric strategy for smaller parameter scales.
- What evidence would resolve it: An ablation study applying different data filtering thresholds or learning rate schedules specifically for sub-3B models to determine if the iterative degradation is preventable.

## Limitations

- Dataset construction transparency: The paper doesn't provide exact prompt formats or domain distribution, making reproduction challenging
- Evaluation circularity: ProxyQA benchmark uses similar methodology to training approach, potentially inflating performance claims
- Hyperparameter sensitivity: Critical parameters like k (responses per meta-question) and variance thresholds are unspecified

## Confidence

**High confidence**: The core mechanism of transforming subjective evaluation into objective reading comprehension tasks is well-supported. The empirical results showing LLM-as-a-Judge underperforms baseline models (12.88, 16.69 < 18.30, 27.00) and that ProxyReward consistently improves performance across model sizes (≥20% gain) are robust and clearly presented.

**Medium confidence**: The iteration-dependent effects (2 iterations optimal for 3B+ models, 1 iteration better for <2B) are supported by Table 4, but the explanation for why smaller models degrade with additional iterations is speculative. The variance-based data selection shows promise (Table 5), but the paper doesn't explore alternative selection strategies or their impact on convergence.

**Low confidence**: The claim of "surpassing GPT-4" is based on a single benchmark (ProxyQA) using a methodology similar to the training approach. Without evaluation on independent open-ended generation tasks or human assessment of generated text quality, the practical significance of the improvement remains uncertain.

## Next Checks

1. **Cross-task generalization**: Evaluate ProxyReward-trained models on independent open-ended generation benchmarks (e.g., creative writing, summarization, or instruction following) that don't use proxy QA evaluation to verify the approach generalizes beyond its training signal.

2. **Human evaluation study**: Conduct blind human assessments comparing GPT-4-Turbo, base models, and ProxyReward-trained models on the same meta-questions to validate that proxy QA accuracy correlates with human-perceived quality and that the 20% improvement reflects actual quality gains.

3. **Reward signal ablation**: Systematically vary the reward formulation (accuracy vs precision, True/False balance, proxy question count) while holding other factors constant to isolate which components drive the performance improvements and test whether the approach is robust to these variations.