---
ver: rpa2
title: 'Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial
  Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology'
arxiv_id: '2507.05716'
source_url: https://arxiv.org/abs/2507.05716
tags:
- human
- plans
- clinical
- experts
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared treatment plans generated by human dermatologists,
  a generalist AI (GPT-4o), and a reasoning AI (o3), evaluating them through both
  human expert and AI judge assessments. Human experts scored peer-generated plans
  significantly higher than AI-generated plans (mean 7.62 vs.
---

# Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology

## Quick Facts
- **arXiv ID**: 2507.05716
- **Source URL**: https://arxiv.org/abs/2507.05716
- **Reference count**: 13
- **Primary result**: Human experts scored peer-generated plans significantly higher than AI-generated plans (mean 7.62 vs. 7.16; p=0.0313), while the AI judge showed the opposite trend, rating AI plans significantly higher than human ones (mean 7.75 vs. 6.79; p=0.0313).

## Executive Summary
This study investigated a critical divergence in medical AI evaluation by comparing treatment plans generated by human dermatologists, a generalist AI (GPT-4o), and a reasoning AI (o3), using both human expert and AI judge assessments. The key finding revealed a fundamental "evaluator effect" where human experts and the AI judge (Gemini 2.5 Pro) showed opposite preferences, with humans favoring peer-generated plans and the AI judge favoring AI-generated ones. This divergence was particularly stark for the reasoning model o3, which ranked 11th by human experts but 1st by the AI judge. The study demonstrates that neither humans nor AI are independently sufficient for optimal clinical planning, suggesting the future of medical AI depends on synergistic, explainable systems that bridge this reasoning gap to enhance clinical care.

## Method Summary
The study compared treatment plans for 5 complex dermatology cases generated by three sources: human dermatologists (10 board-certified experts), a generalist AI (GPT-4o), and a reasoning AI (o3). All plans underwent a two-step normalization process to standardize format and length. In Phase 1, blinded human experts evaluated anonymized plans using a holistic 0-10 rubric across five criteria. In Phase 2, a superior AI (Gemini 2.5 Pro) evaluated all plans using the identical rubric via API. Statistical analysis (Wilcoxon signed-rank test, ICC) compared scores and rankings across both phases to identify evaluator effects.

## Key Results
- Human experts scored peer-generated plans significantly higher than AI-generated plans (mean 7.62 vs. 7.16; p=0.0313)
- The AI judge rated AI plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313)
- The reasoning model o3 ranked 11th by human experts but 1st by the AI judge
- This evaluator effect demonstrates fundamental divergence between human clinical heuristics and AI algorithmic reasoning

## Why This Works (Mechanism)
The study reveals that human experts and AI judges evaluate treatment plans through fundamentally different frameworks. Human experts rely on clinical heuristics, experience-based judgment, and local practice norms, while the AI judge applies data-driven algorithmic optimality criteria. This creates systematic disagreement about plan quality, with neither evaluation method necessarily correlating with actual patient outcomes.

## Foundational Learning
- **Evaluator Effect**: When different evaluators use different reasoning frameworks, they systematically disagree on quality assessments. Why needed: Reveals that AI "better" plans may not align with human clinical priorities. Quick check: Compare evaluation scores across evaluator types.
- **Plan Normalization**: Standardizing format/length before evaluation reduces stylistic bias but risks removing clinically relevant information. Why needed: Ensures fair comparison across generation methods. Quick check: Blinded clinician review of pre/post normalization content.
- **Holistic vs. Checklist Scoring**: Holistic rubrics capture clinical gestalt but increase subjectivity; checklist rubrics force objectivity but may miss plan coherence. Why needed: Tradeoff between efficiency and objective alignment. Quick check: Re-run study with checklist rubric to measure effect on evaluator divergence.

## Architecture Onboarding
- **Component map**: Clinical Cases -> Generation Agents (Human Experts, GPT-4o, o3) -> Normalization Layer -> Evaluation Interface (Phase 1: Human Experts, Phase 2: AI Judge) -> Analysis Engine
- **Critical path**: Present cases to all generation agents → Normalize and anonymize plans → Phase 1: Human experts score plans → Phase 2: AI judge scores plans → Compare scores and rankings to identify divergences
- **Design tradeoffs**: Holistic rubric increases efficiency and captures clinical gestalt but introduces subjectivity; AI persona aims for fair comparison but doesn't guarantee human-equivalent interpretation
- **Failure signatures**: Homogeneous plans prevent evaluator effect manifestation; collapsed scores indicate system failure to discriminate quality; normalization artifacts remove key clinical details
- **First 3 experiments**: 1) Repeat with granular checklist-based rubric to test if evaluator effect diminishes; 2) Add mandatory "rationale" section to AI plans and re-evaluate; 3) Fine-tune reasoning AI on local practice guidelines and compare evaluation scores

## Open Questions the Paper Calls Out
1. **Do AI-preferred or human-preferred plans correlate with superior actual patient outcomes?** The study found the "better" plan is relative to the evaluator, but lacked real-world clinical data to determine which framework predicts success.
2. **Is the evaluator effect consistent across medical specialties with different paradigms?** The authors note findings may differ in fields with different diagnostic and therapeutic approaches.
3. **Can XAI mechanisms bridge the reasoning gap and increase human acceptance of AI plans?** The study used direct plans without explanation; authors hypothesized humans rejected "unconventional" AI plans due to poor understanding.

## Limitations
- Small sample size (10 human experts, 5 clinical cases) limits generalizability
- Reliance on hypothetical vignettes rather than real-world patient outcomes
- Normalization process may have removed clinically relevant information
- AI judge scoring may have been influenced by subtle stylistic patterns

## Confidence
- **High Confidence**: Human experts scored peer-generated plans significantly higher than AI-generated plans (p=0.0313); documented evaluator effect showing opposite scoring trends
- **Medium Confidence**: Interpretation that divergence represents different reasoning frameworks is plausible but requires validation; claim that neither humans nor AI are independently sufficient extrapolates beyond current scope
- **Low Confidence**: Assertion that o3 was "mis-ranked" by human experts assumes AI judge evaluation is superior, which cannot be determined without outcome data

## Next Checks
1. **Outcome Correlation Study**: Track actual patient outcomes when treatments from top-ranked AI plans versus top-ranked human plans are implemented to measure which evaluation framework better predicts real-world success
2. **Cross-Cultural Validation**: Replicate the study with dermatologists from different regions and healthcare systems to determine if the evaluator effect persists across varying clinical practice norms
3. **Explainable AI Integration**: Develop a hybrid evaluation system where the AI judge provides specific, interpretable criteria for its rankings, then test whether this transparency helps human experts better understand and potentially accept AI-generated plans