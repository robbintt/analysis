---
ver: rpa2
title: 'Seeing is Believing: Belief-Space Planning with Foundation Models as Uncertainty
  Estimators'
arxiv_id: '2504.03245'
source_url: https://arxiv.org/abs/2504.03245
tags:
- robot
- object
- planning
- state
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a belief-space planning framework that uses
  vision-language models (VLMs) as uncertainty estimators to handle partially observable
  robotic mobile manipulation tasks. The approach constructs symbolic belief representations
  with three-valued predicates (known-true, known-false, unknown) and employs a belief-space
  planner to generate plans that incorporate strategic information gathering.
---

# Seeing is Believing: Belief-Space Planning with Foundation Models as Uncertainty Estimators

## Quick Facts
- arXiv ID: 2504.03245
- Source URL: https://arxiv.org/abs/2504.03245
- Reference count: 40
- One-line primary result: VLMs as uncertainty estimators enable strategic information gathering for partially observable robotic mobile manipulation tasks.

## Executive Summary
This paper presents a belief-space planning framework that uses vision-language models (VLMs) to handle partially observable robotic manipulation tasks. The approach constructs symbolic belief representations using three-valued predicates (known-true, known-false, unknown) and employs a belief-space planner to generate plans that incorporate strategic information gathering. VLMs are used both for grounding natural language goals into formal specifications and for evaluating predicates over observed objects. The system was evaluated on synthetic tasks with real images and a real Spot robot, demonstrating that it outperforms end-to-end VLM planning and VLM-based state estimation baselines by effectively planning for and executing strategic information gathering.

## Method Summary
The method translates natural language goals into symbolic specifications using an LLM, then evaluates predicates over images as belief classifiers (true/false/unknown) using GPT-4o. A Fast Downward symbolic planner generates plans using determinized belief-space PDDL operators, with optimistic assumptions about observation outcomes. The system executes the first action, updates the belief state, and replans when unexpected outcomes occur. The framework assumes deterministic world dynamics and perfectly accurate perception, with knowledge monotonically increasing over time.

## Key Results
- BKLV A framework outperforms end-to-end VLM planning and VLM-based state estimation baselines on synthetic and real-robot tasks
- Successfully handles long-horizon tasks requiring reasoning about partial observability and property uncertainty
- Demonstrates strategic information gathering through explicit predicate-based reasoning and belief maintenance
- Shows strong performance on tasks like Cup Pick-Place, Drawer Cleaning, Sort Weight, and Empty Cup Removal

## Why This Works (Mechanism)

### Mechanism 1
Representing uncertainty through three-valued predicates (known-true, known-false, unknown) enables symbolic planners to reason about when information gathering is necessary. Each predicate P is encoded as two binary K-fluents: KP (known-true) and K¬P (known-false). When both are false, the value is unknown. Information-gathering actions have preconditions requiring ¬KP ∧ ¬K¬P, forcing the planner to resolve uncertainty before acting on uncertain properties. Core assumption: Uncertainty is discrete and can be captured without probabilistic distributions; knowledge monotonically increases.

### Mechanism 2
Optimistic determinization with replanning approximates optimal POMDP solutions for information-gathering tasks while maintaining computational tractability. Nondeterministic observation actions are split into two deterministic variants (ObserveEmptiness+, ObserveEmptiness-). The planner optimistically "chooses" the favorable outcome. If execution yields an unexpected result, the system replans from the updated belief state. Core assumption: Replanning after each execution step is sufficient; observation outcomes are correct.

### Mechanism 3
VLMs can serve as belief-space classifiers, determining whether predicates are known-true, known-false, or unknown from visual observations. VLMs are queried with structured prompts asking about predicate values over detected objects. The VLM evaluates observability—if insufficient visual evidence exists, the predicate remains unknown; otherwise it returns true/false. Core assumption: VLMs can accurately judge both predicate truth and their own epistemic uncertainty.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The paper frames the problem as belief-space planning, which is the standard solution approach for POMDPs. Understanding belief states vs world states is essential.
  - Quick check question: Can you explain why maintaining a distribution over possible world states (belief) is necessary when the agent cannot directly observe the true state?

- **Concept: PDDL (Planning Domain Definition Language)**
  - Why needed here: The system encodes operators, preconditions, and effects in PDDL-style syntax. The planner (Fast Downward) consumes these representations.
  - Quick check question: Given an action with preconditions (A, B) and effects (¬A, C), what is the resulting state when executed in state {A, B, D}?

- **Concept: Three-Valued (Kleene) Logic**
  - Why needed here: The belief representation extends classical binary logic with "unknown" as a third truth value. This is the formal basis for K-fluents.
  - Quick check question: In three-valued logic, what is the truth value of (Unknown ∧ True)? Of (Unknown ∨ True)?

## Architecture Onboarding

- **Component map**: Perception Layer (MOLMO → SAM → depth fusion → spatial memory; GPT-4o) -> Belief Layer (object list + three-valued predicates + robot state) -> Planning Layer (Fast Downward with determinized operators) -> Execution Layer (parameterized skills with replanning)
- **Critical path**: VLM predicate evaluation accuracy → correct belief state → planner generates valid information-gathering sequence → skill execution reliability → observations match expected effects (else replan)
- **Design tradeoffs**: Three-valued predicates vs probabilistic beliefs (simpler planning but loses uncertainty magnitude information); Optimistic determinization vs full POMDP solution (faster but may generate suboptimal policies); Monolithic VLM query vs per-predicate queries (efficiency vs granularity)
- **Failure signatures**: VLM returns "known-false" when evidence is merely insufficient; Replanning loops triggered repeatedly; Planner fails to find plan; Objects not detected → never added to belief state
- **First 3 experiments**:
  1. Validate VLM predicate evaluation on held-out images with ground-truth labels
  2. Ablate replanning on synthetic tasks to measure impact on success rate
  3. Measure replanning frequency under controlled uncertainty conditions

## Open Questions the Paper Calls Out

### Open Question 1
Can belief-space operators be automatically learned or extracted from pretrained foundation models, eliminating the need for human-defined operator-level transitions? The current framework assumes a priori, human-defined PDDL operators for belief-space transitions, which limits adaptability to new environments or tasks without manual engineering.

### Open Question 2
How can active object search for objects in unknown locations be integrated into the belief-space planning framework? The current incidental object discovery mechanism only adds objects when revealed through manipulation, but cannot actively search for objects whose locations are entirely unknown.

### Open Question 3
Can the assumption of deterministic world dynamics and perfectly accurate perception be relaxed to handle noisy observations and information loss over time? The paper assumes monotonic knowledge acquisition, but real-world robotic systems face sensor noise and perceptual errors.

### Open Question 4
How can high-level belief-space planning be tightly coupled with real-time skill execution under physical constraints and control uncertainty? The current separation between symbolic planning and parameterized skills may fail when low-level execution cannot satisfy high-level assumptions.

## Limitations
- Framework assumes VLMs can reliably estimate their own uncertainty, which corpus evidence challenges
- Determinized planning approximation only guarantees optimality under specific replanning assumptions that may not hold in real-world latency conditions
- Three-valued predicate representation discards uncertainty magnitude information that could be critical for nuanced decision-making

## Confidence
- VLM as belief classifier mechanism: Medium - Strong implementation details but corpus shows VLMs struggle with uncertainty estimation
- Optimistic determinization effectiveness: Medium - Theoretical backing exists but real-world replanning delays could break guarantees
- Overall task success claims: High - Demonstrated empirically on synthetic and real-robot tasks with clear baselines

## Next Checks
1. **VLM uncertainty calibration test**: Evaluate VLM's ability to distinguish "unknown" from "false" on a dataset with ground-truth uncertainty annotations. Measure false confidence rates.
2. **Replanning latency impact study**: Measure how replanning time affects overall task success when observation noise is introduced. Identify the threshold where the determinization approximation breaks.
3. **Three-valued vs probabilistic comparison**: Implement the same tasks using probabilistic belief representations and compare information-gathering efficiency and success rates.