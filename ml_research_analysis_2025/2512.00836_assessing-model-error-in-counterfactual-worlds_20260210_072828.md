---
ver: rpa2
title: Assessing model error in counterfactual worlds
arxiv_id: '2512.00836'
source_url: https://arxiv.org/abs/2512.00836
tags:
- scenario
- error
- each
- projections
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the fundamental challenge of evaluating counterfactual
  scenario projections, where models predict outcomes under specific hypothetical
  conditions that are never observed in reality. The core problem is that error in
  these projections has two sources: model miscalibration and scenario deviation,
  making traditional evaluation methods inapplicable.'
---

# Assessing model error in counterfactual worlds

## Quick Facts
- arXiv ID: 2512.00836
- Source URL: https://arxiv.org/abs/2512.00836
- Reference count: 40
- Key outcome: Proposes three methods to evaluate model miscalibration error in counterfactual scenarios by decomposing total error into model calibration and scenario specification components

## Executive Summary
This paper addresses the fundamental challenge of evaluating counterfactual scenario projections, where models predict outcomes under hypothetical conditions that are never observed. The core problem is that error in these projections has two sources: model miscalibration and scenario deviation, making traditional evaluation methods inapplicable. The authors propose three approaches to estimate model miscalibration error: (1) evaluate only plausible scenarios close to reality, (2) infer error distribution by modeling errors across locations with varying realized scenarios, and (3) estimate observations in modeled scenarios using statistical models.

## Method Summary
The paper proposes three approaches to estimate model miscalibration error in counterfactual worlds: Approach 1 filters projections to plausible scenarios near realized values; Approach 2 leverages natural variation across units to infer error distributions at counterfactual values; and Approach 3 estimates counterfactual observations via statistical modeling. A simulation experiment using SIR epidemic models demonstrates that Approaches 2 and 3 outperform Approach 1 in accurately estimating true error distributions. The methods decompose observed deviation into model calibration error and scenario specification error, which may partially cancel each other out. Performance depends heavily on scenario design, requiring measurable scenario axes with sufficient variation across groups.

## Key Results
- Approaches 2 and 3 outperform Approach 1 in accurately estimating true error distributions in simulation
- Approach 3, which estimates observations in counterfactual scenarios using statistical models with covariates, shows particularly strong performance
- Observed differences between projections and reality decompose linearly into model calibration error and scenario specification error
- Model miscalibration error is the most important component for assessing model value in decision-making

## Why This Works (Mechanism)

### Mechanism 1: Error Decomposition into Orthogonal Components
- **Claim:** Observed deviation between projections and reality decomposes linearly into model calibration error and scenario specification error.
- **Mechanism:** By separating `Pm(y|xi) - P*(y|x*)` into `[Pm(y|xi) - P*(y|xi)] + [P*(y|xi) - P*(y|x*)]`, evaluators can isolate the component relevant to model performance (miscalibration) from the component driven by scenario assumptions not matching reality.
- **Core assumption:** The two error sources are functionally independent; model miscalibration at the counterfactual scenario is the relevant metric for decision-making value.
- **Evidence anchors:**
  - [abstract]: "Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making."
  - [section 4]: "Observed deviation = model calibration error + scenario specification error... the two sources of error can act in different ways... there is also the possibility for the less intuitive case where the errors are in opposing directions."
  - [corpus]: Weak direct evidence; neighboring papers focus on simulation/counterfactual generation rather than error decomposition frameworks.
- **Break condition:** When errors are non-additive (e.g., interaction between scenario choice and model structure), the decomposition may underestimate total error.

### Mechanism 2: Leveraging Natural Variation to Infer Counterfactual Error Distributions
- **Claim:** Variation in realized scenario values across units (e.g., locations) enables statistical inference of error distributions at unobserved counterfactual values.
- **Mechanism:** Calculate error at realized values `em(x*)`, fit a statistical model `em(x) = g(x) + ε` (e.g., GAM with splines), then predict errors at modeled scenario values `xi`. The approach exploits that different locations realize different values along the scenario axis.
- **Core assumption:** Realized scenario values are sampled independently along the scenario axis; models capture system properties that statistical models may miss (e.g., non-linearities like herd immunity).
- **Evidence anchors:**
  - [section 3.2]: "When naturally occurring variation exists in the realized scenario and projected outcome (e.g., across locations), this variation can be leveraged to model the errors in scenarios that were not realized."
  - [section 5.3]: "Approach 2... better capturing locations with a higher R0 which have the strongest non-linear effects."
  - [corpus]: Neighbor paper "Leveraging a Simulator for Learning Causal Representations" supports using simulation structure to improve counterfactual estimation.
- **Break condition:** When honest retrospective reprojection is infeasible or when location-specific covariates needed for error prediction are unavailable.

### Mechanism 3: Estimating Counterfactual Observations via Causal Inference
- **Claim:** Observations at counterfactual scenario values can be estimated by fitting a model to observed `(x*, P*(y|x*))` pairs and extrapolating.
- **Mechanism:** Fit `P*(y|x) = f(x) + ε` using GAMs or similar, include relevant covariates (e.g., R0), then predict observations at `xi`. Error is then `Pm(y|xi) - estimated P*(y|xi)`. Causal inference methods provide theoretical grounding for required assumptions.
- **Core assumption:** The relationship between scenario axis and outcome is correctly specified; important confounders/covariates are identified and measured.
- **Evidence anchors:**
  - [section 3.3]: "Estimating the 'observed' value for each scenario requires a range of assumptions... there are well-established methods for making such inferences from the field of causal inference."
  - [section 5.3]: "Approach 3 performs best when covariates are included... Without covariates, the variance of the inferred error distribution is too large."
  - [corpus]: C-kNN-LSH paper addresses sequential counterfactual inference, suggesting active research in nearest-neighbor approaches to similar problems.
- **Break condition:** When covariates are unmeasured, when functional form `f(x)` is misspecified, or when extrapolation extends far beyond observed `x*` range.

## Foundational Learning

- **Concept: Counterfactual vs. Forecast Evaluation**
  - **Why needed here:** The paper's central distinction; forecasts predict "what will happen" (can be directly evaluated), while scenario projections predict "what would happen if" (counterfactual, cannot be directly observed).
  - **Quick check question:** If a model projected hospitalizations under 70% vaccination but actual uptake was 55%, can you directly compare projection to observation? Why or why not?

- **Concept: Generalized Additive Models (GAMs) with Splines**
  - **Why needed here:** Both Approaches 2 and 3 use GAMs with cubic spline terms to model non-linear relationships between scenario variables and outcomes/errors.
  - **Quick check question:** Why might a spline-based GAM outperform linear regression for modeling epidemic final size as a function of vaccination coverage?

- **Concept: Honest Retrospective Reprojection**
  - **Why needed here:** Approach 2 requires re-running models at realized scenario values without using post-hoc knowledge, which creates operational and incentive challenges.
  - **Quick check question:** What could go wrong if a modeling team uses data from the projection period to "improve" their retrospective reprojection?

## Architecture Onboarding

- **Component map:**
  - **Approach 1 (Plausible Scenarios):** Selector → filters projections by `|xi - x*| < τ` → direct comparison to observation → error estimate for subset
  - **Approach 2 (Infer Error Distribution):** Reprojection engine → compute `em(x*)` for all units → GAM fitter `em(x) = g(x) + ε` → predict at `xi` → error distribution
  - **Approach 3 (Estimate Observations):** Observation fitter `P*(y|x) = f(x) + ε` → predict at `xi` → compare to `Pm(y|xi)` → error estimate

- **Critical path:**
  1. During scenario design: ensure scenario axes are measurable, continuous, and have sufficient cross-unit variation
  2. At projection time: document model structure and parameters to enable honest reprojection
  3. Post-realization: collect `x*` and `y*` for all units
  4. Apply Approach 3 (preferred) or Approach 2; validate with sensitivity analyses

- **Design tradeoffs:**
  - **Approach 1 vs. {2,3}:** Simplicity vs. interpretability; Approach 1 is easy but conflates error sources
  - **Approach 2 vs. 3:** Mechanistic model non-linearities vs. consistent observation estimates; Approach 2 implies different observations per model, Approach 3 keeps observations fixed
  - **With vs. without covariates:** Approaches 2 and 3 both require covariates for location-specific estimates; without them, only aggregate distributions are possible

- **Failure signatures:**
  - Approach 1 shows systematic bias correlated with `|xi - x*|` (see Figure S1)
  - Approach 3 without covariates shows inflated error variance
  - Both approaches fail when realized `x*` values cluster narrowly, limiting extrapolation validity

- **First 3 experiments:**
  1. **Baseline sanity check:** Run all three approaches on simulated data where ground truth is known; verify Approaches 2/3 recover true error distribution better than Approach 1
  2. **Covariate sensitivity:** Implement Approach 3 with and without location-specific covariates; quantify variance reduction from covariate inclusion
  3. **Cross-validation of observation model:** Hold out subset of units when fitting `P*(y|x)`, predict their observations, compare to actual; assess extrapolation reliability before applying to counterfactual scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can these evaluation approaches be extended to probabilistic scenario projections, which include uncertainty quantification rather than only point predictions?
- **Basis in paper:** [explicit] "Third, we only assess the error distribution of point predictions, whereas most scenario projections are probabilistic in nature. This is a key area for future work, but theoretical grounding of the approaches should remain the same."
- **Why unresolved:** The simulation experiment only evaluated point estimates; most real scenario projections provide predictive intervals or probability distributions that require different evaluation metrics.
- **What evidence would resolve it:** A modified simulation experiment incorporating probabilistic forecasts, evaluated using proper scoring rules adapted for counterfactual worlds.

### Open Question 2
- **Question:** How can modelers effectively communicate scenario projection performance to decision-makers when model miscalibration and scenario deviation errors cancel, producing deceptively accurate-seeming projections?
- **Basis in paper:** [explicit] "This latter example highlights both the essential nature of rigorous evaluation methods and the need for research into how we effectively communicate the performance of scenario projection models and build trust in their results."
- **Why unresolved:** Cases where errors cancel create a "right answer, wrong reason" problem that undermines intuitive trust-building; effective communication strategies remain undefined.
- **What evidence would resolve it:** User studies testing different visualization and framing approaches for presenting decomposed error components to decision-makers.

### Open Question 3
- **Question:** How robust are Approaches 2 and 3 when observation error affects covariates and outcomes, rather than the generous "perfect observation" conditions assumed in the simulation?
- **Basis in paper:** [inferred] From limitation: "Finally, we assume a very generous data environment for assessing models, including perfect observation of covariates, measurements of outcomes, etc. Observation error will impact real world performance."
- **Why unresolved:** The simulation assumed covariates like R₀ were known perfectly, but real-world applications involve measurement error that could propagate through the causal inference and error distribution fitting steps.
- **What evidence would resolve it:** Simulation experiments with varying levels of covariate and outcome measurement error to quantify sensitivity of each approach.

### Open Question 4
- **Question:** How can these methods be adapted for multi-axis scenarios where multiple intervention or assumption dimensions vary simultaneously?
- **Basis in paper:** [inferred] From discussion: "Third, careful thought is needed in the definition of multi-axis scenarios to ensure that it is possible to model the simultaneous effect of both scenario axes on outcomes (or error)."
- **Why unresolved:** The simulation only examined a single scenario axis (vaccination coverage); multi-dimensional scenario spaces require modeling interactions between axes and may reduce the natural variation available for statistical inference.
- **What evidence would resolve it:** Extension of the simulation framework to include two or more scenario axes, comparing performance of approaches under different correlation structures between axes.

## Limitations
- The methods rely on critical assumptions about error independence and correct model specification that require validation
- The simulation uses a simple SIR model with only vaccination coverage as the scenario axis, limiting generalizability to complex real-world scenarios
- Approach 2 requires honest retrospective reprojection of models at realized scenario values, creating operational challenges and perverse incentives
- The paper assumes perfect observation of covariates and outcomes, which is unrealistic in real-world applications

## Confidence
- **High confidence:** The error decomposition framework and the intuition that scenario deviation can partially cancel model miscalibration (based on mathematical derivation and simulation results)
- **Medium confidence:** The superiority of Approaches 2 and 3 over Approach 1 in the simulation experiment (supported by quantitative comparisons but limited to one model type)
- **Low confidence:** The generalizability of the three proposed approaches to scenarios with multiple axes, non-continuous scenarios, or when honest reprojection is infeasible (extrapolated from single-axis SIR example)

## Next Checks
1. **Cross-model validation:** Apply the proposed approaches to multiple SIR models with different structural assumptions (e.g., SEIR, age-structured) to test robustness of error decomposition across model families.
2. **Multi-axis scenario test:** Extend the simulation to include a second scenario axis (e.g., both vaccination coverage and contact reduction) to evaluate whether the approaches scale to more complex scenarios.
3. **Real-world application:** Apply Approach 3 to a real epidemiological scenario with multiple locations (e.g., COVID-19 projections across US states) where true observations are known, comparing estimated vs. actual model miscalibration errors.