---
ver: rpa2
title: 'OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution'
arxiv_id: '2601.20380'
source_url: https://arxiv.org/abs/2601.20380
tags:
- arxiv
- grounding
- agent
- omegause
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OmegaUse is a general-purpose GUI agent model for autonomous task
  execution across mobile and desktop platforms, supporting both computer-use and
  phone-use scenarios. It is built on a Mixture-of-Experts (MoE) backbone and employs
  a decoupled two-stage training paradigm: supervised fine-tuning (SFT) to establish
  foundational interaction syntax, followed by Group Relative Policy Optimization
  (GRPO) to refine spatial grounding and sequential planning.'
---

# OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution

## Quick Facts
- arXiv ID: 2601.20380
- Source URL: https://arxiv.org/abs/2601.20380
- Reference count: 40
- Primary result: 96.3% ScreenSpot-V2 score

## Executive Summary
OmegaUse is a general-purpose GUI agent model for autonomous task execution across mobile and desktop platforms. It employs a Mixture-of-Experts (MoE) backbone with a decoupled two-stage training paradigm: supervised fine-tuning (SFT) establishes foundational interaction syntax, followed by Group Relative Policy Optimization (GRPO) to refine spatial grounding and sequential planning. The model achieves state-of-the-art performance on multiple benchmarks, notably 96.3% on ScreenSpot-V2 and 79.1% on AndroidControl.

## Method Summary
OmegaUse uses a 30B-A3B MoE VL backbone trained in two stages. First, SFT on curated datasets establishes basic interaction syntax. Second, GRPO refines spatial precision using format and coordinate-based rewards. The data pipeline combines open-source datasets (aggregated and filtered to 111K samples) with automated synthesis using bottom-up DFS exploration and top-down taxonomy-guided generation. The unified action space covers Click, Drag, Scroll, Type, Wait, and Hotkey actions across platforms.

## Key Results
- 96.3% score on ScreenSpot-V2 benchmark
- 79.1% step success rate on AndroidControl
- 74.24% on ChiM-Nav and 55.9% on Ubu-Nav (OS-Nav benchmarks)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling SFT followed by GRPO improves spatial grounding stability compared to end-to-end reinforcement learning.
- **Mechanism:** SFT initializes the policy with basic interaction syntax, preventing the RL phase from needing to explore fundamental formatting from scratch. GRPO estimates baselines using group-relative rewards rather than a separate critic model, reducing computational overhead and variance.
- **Core assumption:** Assumes interaction syntax and spatial reasoning can be effectively isolated into sequential phases without catastrophic forgetting of syntax during the RL phase.
- **Evidence anchors:** [abstract] "...decoupled two-stage training paradigm: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO)..." [section 3.1.2] Describes advantage calculation normalization within groups and clipped objective.
- **Break condition:** If SFT fails to converge on unified action space syntax, subsequent GRPO phase will diverge due to invalid rollouts triggering zero format rewards.

### Mechanism 2
- **Claim:** Integrating bottom-up autonomous exploration with top-down taxonomy-guided generation creates higher-fidelity navigation data than manual collection alone.
- **Mechanism:** DFS agent explores UI to build state transition graph (bottom-up). This graph is compressed via semantic clustering. Separately, taxonomy defines complex tasks, and expert agent executes them (top-down). Combines coverage of diverse UI states with logical goal-oriented trajectories.
- **Core assumption:** Assumes MLLM-based state clustering can reliably identify functionally identical UI states to prevent graph explosion.
- **Evidence anchors:** [abstract] "...integrates bottom-up autonomous exploration with top-down taxonomy-guided generation..." [section 3.2.2] Details "Exploration-driven Synthesis" using triples and "Taxonomy-guided Generation" via hierarchical task design.
- **Break condition:** If "cycle-avoidance strategy" fails during DFS, resulting trajectories will contain unproductive loops, degrading training signal.

### Mechanism 3
- **Claim:** MoE backbone preserves reasoning capacity of larger dense models while significantly reducing inference latency.
- **Mechanism:** By activating only subset of parameters (3B active out of 30B total) per token, model maintains high parameter count for knowledge storage while processing inputs efficiently.
- **Core assumption:** Assumes GUI grounding and navigation tasks rely on diverse sub-domains of knowledge that can be routed to specific experts without requiring full dense computation every step.
- **Evidence anchors:** [abstract] "...balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone." [section 5.1.1] Confirms use of "30B-A3B VL model".
- **Break condition:** If router network collapses (load balancing failure), model effectively functions as smaller dense model, losing capacity advantage.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Core algorithm refining agent's spatial precision. Uses group statistics for baselines, removing need to train separate value function.
  - **Quick check question:** How does GRPO estimate advantage $\hat{A}_i$ differently from standard PPO? (Answer: Normalizes rewards within group of rollouts rather than comparing against learned value function).

- **Concept: Unified Action Space**
  - **Why needed here:** Model must output consistent schema across mobile, desktop, and web platforms to function as "General-Purpose" agent.
  - **Quick check question:** What is specific output format for coordinate-based action? (Answer: `Click(box=(x, y))` using normalized coordinates).

- **Concept: Semantic State Clustering**
  - **Why needed here:** Essential for bottom-up data pipeline to reduce redundancy. System must recognize two screenshots are same logical "page" despite minor pixel differences.
  - **Quick check question:** In exploration phase, how does system decide if two UI states are equivalent? (Answer: Uses MLLMs to judge functional similarity and merges redundant nodes in state transition graph).

## Architecture Onboarding

- **Component map:** Aggregator (Open Source) -> Filter (Manual/Synthetic) -> Synthesis (DFS Explorer + Taxonomy Generator) -> SFT Stage (Syntax) -> GRPO Stage (Reward Engine: Format + Inside-Box/Coordinate Precision) -> Inference (Screenshot Input -> (O -> T -> A) Chain -> Action Output)

- **Critical path:** Data filtering pipeline is bottleneck. Section 3.1.1 notes 40% of raw instances contain noise. If "manual inspection and correction pipeline" is skipped or "Inside-of-Bounding-Box" reward is misconfigured, grounding model fails to converge.

- **Design tradeoffs:**
  - **MoE vs. Dense:** Uses MoE for speed/efficiency but admits slight performance gap on extremely complex benchmarks compared to massive dense models.
  - **Decoupled Models:** Separates Grounding (OmegaUse-G) and Navigation. Optimizes individual performance but requires maintaining two distinct training pipelines.

- **Failure signatures:**
  - **Stalling:** Agent repeats actions (e.g., Click -> Back -> Click). Suggests "cycle-avoidance" logic in training data generation failed or reward model penalizes stopping.
  - **Format Drift:** Model outputs natural language descriptions instead of valid `Click(box=(x,y))` code. Indicates GRPO format reward weight was too low relative to spatial reward.

- **First 3 experiments:**
  1. **Reward Ablation:** Run GRPO with only R_fmt vs. only R_pos to verify paper's implicit claim both are required for stable convergence.
  2. **Synthetic Data Validation:** Train smaller baseline on raw open-source data vs. "OmegaUse" synthetic pipeline to quantify performance delta from DFS exploration method.
  3. **Expert Load Analysis:** Visualize expert activation patterns in MoE layer when processing mobile screenshots vs. desktop screenshots to validate cross-platform generalization mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- Work relies heavily on proprietary model weights (30B-A3B MoE) without disclosing architecture details or initialization sources, creating significant reproducibility barriers.
- Automated data synthesis pipeline depends on MLLM-based semantic clustering whose reliability across diverse UI designs remains unverified.
- Decoupling assumption that SFT can perfectly establish syntax before GRPO refines spatial reasoning has no empirical validation for potential catastrophic forgetting.

## Confidence
- **High confidence**: Architectural design (MoE backbone, decoupled training) and benchmark results (96.3% ScreenSpot-V2, 79.1% AndroidControl) are well-documented and verifiable through reproduction.
- **Medium confidence**: Data synthesis methodology (bottom-up DFS + top-down taxonomy) is described but lacks comparative validation against alternative synthesis approaches.
- **Low confidence**: Specific GRPO implementation details, including reward weight tuning and KL penalty calibration, are insufficiently specified for independent replication.

## Next Checks
1. **Reward ablation study**: Train OmegaUse variants with only format rewards, only spatial rewards, and combined rewards to quantify each component's contribution to the 96.3% ScreenSpot-V2 score.
2. **Data pipeline validation**: Compare model performance trained on raw open-source datasets versus the proposed filtered/synthesized pipeline to isolate contribution of automated exploration method.
3. **Cross-platform generalization test**: Evaluate model's ability to transfer learned navigation strategies between mobile and desktop environments by testing on held-out cross-platform tasks not seen during training.