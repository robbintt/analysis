---
ver: rpa2
title: 'MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents'
arxiv_id: '2503.10659'
source_url: https://arxiv.org/abs/2503.10659
tags:
- legal
- rhetorical
- documents
- embeddings
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARRO, a family of models for rhetorical
  role labeling in legal documents that combines transformer-inspired multi-headed
  attention with BiLSTM-CRF architecture. The authors address the challenge of labeling
  sentences in lengthy legal documents by using multi-headed attention over sentence
  embeddings to capture contextual relationships.
---

# MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents

## Quick Facts
- arXiv ID: 2503.10659
- Source URL: https://arxiv.org/abs/2503.10659
- Reference count: 40
- Key outcome: MARRO achieves state-of-the-art F1 scores of 0.724 on Indian and 0.617 on UK Supreme Court legal documents using multi-headed attention with BiLSTM-CRF architecture

## Executive Summary
This paper introduces MARRO, a family of models for rhetorical role labeling in legal documents that combines transformer-inspired multi-headed attention with BiLSTM-CRF architecture. The authors address the challenge of labeling sentences in lengthy legal documents by using multi-headed attention over sentence embeddings to capture contextual relationships. They propose four variants: MARRO base using sent2vec embeddings, TF-MARRO using LEGAL-BERT embeddings, and two multi-task learning versions (MTL-MARRO and MTL-TF-MARRO) that incorporate label shift prediction as an auxiliary task. The models are evaluated on two datasets from Indian and UK Supreme Courts, achieving state-of-the-art results with an F1 score of 0.724 on the Indian dataset and 0.617 on the UK dataset. The paper also contributes a new dataset of 150 annotated Indian Supreme Court documents with 30,729 sentences.

## Method Summary
MARRO uses a BiLSTM-CRF architecture enhanced with transformer-style multi-headed attention over sentence embeddings within legal documents. The approach processes each sentence independently through sent2vec or LEGAL-BERT to obtain embeddings, then applies multi-headed attention across sentences within a document. The model includes four variants: MARRO base with sent2vec embeddings, TF-MARRO with LEGAL-BERT embeddings, and two multi-task learning versions that jointly train label shift prediction. The MTL variants add a parallel branch predicting whether consecutive sentences have different rhetorical roles, with both branches sharing a CRF layer. Training uses 5-fold cross-validation with macro-averaged F1 score as the primary metric.

## Key Results
- MARRO variants achieve state-of-the-art performance on legal document rhetorical role labeling
- MTL-MARRO achieves F1 score of 0.724 on the Indian Supreme Court dataset (DIN)
- MTL-TF-MARRO achieves F1 score of 0.617 on the UK Supreme Court dataset (DUK)
- The multi-task learning approach with label shift prediction consistently improves performance across both datasets

## Why This Works (Mechanism)
The multi-headed attention mechanism captures long-range dependencies between sentences within legal documents, addressing the challenge of context-dependent rhetorical roles. Legal documents exhibit complex rhetorical structures where the same sentence type can appear in different contexts with different functions. The attention mechanism allows the model to weigh the importance of different sentences when determining the role of any given sentence. The multi-task learning approach with label shift prediction helps the model learn smoother transitions between rhetorical roles, which is particularly important in legal documents where roles often change gradually.

## Foundational Learning
- **Multi-headed attention**: Why needed: Captures complex relationships between sentences in legal documents; Quick check: Verify attention weights show higher values for semantically related sentences
- **BiLSTM-CRF**: Why needed: Models sequential dependencies and enforces valid label sequences; Quick check: Ensure Viterbi decoding produces coherent label sequences
- **Multi-task learning**: Why needed: Auxiliary label shift task improves main task performance; Quick check: Compare single-task vs multi-task performance on label pairs that show confusion
- **Sentence embeddings**: Why needed: Transforms variable-length text into fixed-dimensional representations; Quick check: Validate embedding quality using similarity tasks
- **Legal document structure**: Why needed: Understanding typical rhetorical flow patterns in court decisions; Quick check: Visualize attention patterns to confirm they align with expected legal document structure

## Architecture Onboarding
- **Component map**: Document -> Sentence embeddings (sent2vec/LEGAL-BERT) -> Multi-headed attention (2 blocks) -> BiLSTM -> CRF (or BiLSTM outputs concatenated -> CRF for MTL)
- **Critical path**: Input sentences → embeddings → multi-headed attention → BiLSTM → CRF → rhetorical role prediction
- **Design tradeoffs**: sent2vec vs LEGAL-BERT embeddings (vocabulary match vs. semantic richness), number of attention heads (computational cost vs. expressiveness), single-task vs. multi-task learning (simplicity vs. performance)
- **Failure signatures**: Poor performance on DUK with sent2vec (vocabulary mismatch expected), slow training with LEGAL-BERT (embedding generation bottleneck), confusion between ARG and RATIO labels (expected due to interleaving)
- **First experiments**: 1) Train MARRO base on DIN dataset with sent2vec embeddings, 2) Implement multi-headed attention and visualize attention patterns, 3) Test label shift prediction auxiliary task separately

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- The DIN dataset's newly annotated documents are marked as "to be released," creating uncertainty for immediate reproduction
- Critical hyperparameters for BiLSTM, CRF, and sent2vec components are not fully specified
- The label shift module's exact embedding concatenation formula is described ambiguously
- Runtime embedding generation with LEGAL-BERT-SMALL is identified as a bottleneck

## Confidence
- High confidence: The architectural framework combining multi-headed attention with BiLSTM-CRF is clearly described and represents a valid approach for sentence-level classification in legal documents
- Medium confidence: The reported performance improvements over baseline methods, though exact comparison conditions are not fully specified
- Medium confidence: The auxiliary task of label shift prediction provides consistent benefits, as evidenced by MTL variants outperforming single-task models

## Next Checks
1. Verify dataset availability and annotation consistency by requesting access to the newly annotated DIN documents and comparing with the Bhattacharya et al. dataset portion
2. Implement the multi-headed attention mechanism with the specified configurations (5 heads for sent2vec, 8 heads for LEGAL-BERT-SMALL) and validate attention pattern visualization matches expected legal document structure
3. Reproduce the label shift prediction auxiliary task by implementing the binary classification branch and testing whether it improves performance on the ARG and RATIO label pairs that show the most confusion in ablation studies