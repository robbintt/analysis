---
ver: rpa2
title: Strategies for training point distributions in physics-informed neural networks
arxiv_id: '2508.13216'
source_url: https://arxiv.org/abs/2508.13216
tags:
- grid
- training
- points
- random
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically investigates the impact of training\
  \ point distributions on the accuracy of physics-informed neural networks (PINNs)\
  \ for solving differential equations. Five distribution strategies\u2014equidistant,\
  \ random, random sorted, Chebyshev, and a newly proposed sine-based grid\u2014are\
  \ evaluated on two ordinary and two partial differential equations using shallow\
  \ network architectures with fixed-seed and random weight initialisations."
---

# Strategies for training point distributions in physics-informed neural networks

## Quick Facts
- **arXiv ID:** 2508.13216
- **Source URL:** https://arxiv.org/abs/2508.13216
- **Reference count:** 31
- **Primary result:** PINN accuracy depends significantly on training point distribution, with Chebyshev nodes optimal for rapid decay problems and sine-based grids for oscillatory problems.

## Executive Summary
This paper systematically investigates how training point distributions affect the accuracy of physics-informed neural networks (PINNs) for solving differential equations. Five distribution strategies—equidistant, random, random sorted, Chebyshev, and a newly proposed sine-based grid—are evaluated on ordinary and partial differential equations using shallow network architectures. The results demonstrate that optimal training point distributions depend strongly on the characteristics of the differential equation, with Chebyshev nodes excelling for rapid-decay problems and sine-based grids performing best for oscillatory problems. The study also reveals high variability in accuracy across random weight initializations, emphasizing the importance of both distribution strategy and initialization.

## Method Summary
The study evaluates five training point distribution strategies (equidistant, random, random sorted, Chebyshev, and sine-based) on PINNs solving four differential equations: radioactive decay, simple harmonic oscillator, Laplace, and Poisson equations. Experiments use shallow networks (one or two hidden layers) with both fixed-seed and random weight initializations. Performance is measured using mean absolute error on separate test grids, with 50,000-100,000 Adam optimization epochs. The proposed sine-based distribution uses arc-length parameterization of the solution domain to concentrate points where solution gradients are large.

## Key Results
- Chebyshev nodes achieve best accuracy for rapid-decay problems like radioactive decay
- Sine-based grids perform optimally for oscillatory problems like simple harmonic oscillator
- Equidistant and random grids tend to yield superior performance for Laplace and Poisson equations
- Increasing grid resolution does not guarantee monotonic accuracy improvement due to architecture-distribution interactions
- High standard deviation across random initializations indicates significant optimization sensitivity

## Why This Works (Mechanism)

### Mechanism 1
Aligning training point distribution with solution characteristics improves approximation accuracy. Non-uniform distributions concentrate collocation points in regions where the solution changes rapidly or exhibits characteristic behavior, improving the network's ability to fit those regions. Chebyshev clustering at boundaries captures rapid decay dynamics; sine-based arc-length sampling aligns with oscillatory crests and troughs. Core assumption: The solution's important features are known a priori or can be inferred from problem structure. Evidence: Chebyshev grid achieves best results for radioactive decay; sine-based grid yields lowest average MAE for simple harmonic oscillator.

### Mechanism 2
Increasing grid resolution does not guarantee monotonic accuracy improvement; architecture-distribution interactions affect outcomes. More training points increase residual constraints but also increase optimization complexity. Network depth changes parameter count and optimization landscape. The interaction between grid density and architecture depth creates non-monotonic error trends. Core assumption: Adam optimizer with fixed hyperparameters can reach adequate minima regardless of grid size and architecture. Evidence: Poisson equation shows MAE decreases from 20×20 to 40×40 but increases at 80×80 for most distributions; two-layer FNN with random grid shows worse MAE at 400 points than 200 points.

### Mechanism 3
Random weight initialization causes high variance in PINN accuracy; deterministic seeds improve reproducibility but may lock into suboptimal minima. Initial weights determine starting position in loss landscape. PINNs use first-order optimizers that converge to local minima. Different seeds place optimization in different basins of attraction, yielding different final solutions. Core assumption: The loss landscape has multiple local minima with varying quality; fixed seeds provide consistent but potentially suboptimal convergence. Evidence: MAE varies significantly across seeds for both harmonic oscillator and Poisson equation; variance quantified through 20 random seed experiments.

## Foundational Learning

- **Concept: Physics-Informed Neural Networks (PINNs)**
  - Why needed here: The entire framework depends on embedding PDE residuals into the loss function rather than relying on ground-truth data pairs.
  - Quick check question: Can you explain how automatic differentiation enables gradient computation through the PDE residual term?

- **Concept: Collocation/Training Points**
  - Why needed here: This paper's central question—how point distribution affects accuracy—requires understanding that these points constrain the solution space, not provide labels.
  - Quick check question: What is the difference between collocation points for the residual loss and boundary/initial condition points?

- **Concept: Chebyshev Nodes and Interpolation Theory**
  - Why needed here: The Chebyshev distribution strategy is motivated by polynomial interpolation properties (Runge phenomenon avoidance) that transfer to neural approximation.
  - Quick check question: Why do Chebyshev nodes cluster near boundaries, and how does this relate to polynomial interpolation error bounds?

## Architecture Onboarding

- **Component map:**
  Input Layer (t, x, y coords) -> Hidden Layer(s) [tanh activation] -> Output Layer (u_θ, linear) -> Automatic Differentiation → ∂u/∂t, ∂²u/∂x², etc. -> Loss Composition: L_R (residual) + L_IC (initial) + L_BC (boundary) -> Adam Optimizer → θ update

- **Critical path:**
  1. Generate training points using chosen distribution
  2. Initialize network weights (fixed seed or random)
  3. Forward pass to compute u_θ at collocation points
  4. Compute derivatives via automatic differentiation
  5. Assemble total loss (residual + conditions)
  6. Backpropagate and update with Adam (50k–100k epochs)
  7. Evaluate on separate equidistant test grid

- **Design tradeoffs:**
  - One hidden layer (100 neurons) vs. two hidden layers (50 each): Problem-dependent; two layers increase parameters but may not improve accuracy if optimizer capacity is fixed
  - Grid resolution vs. optimization epochs: Higher resolution requires more epochs to saturate loss; 50k epochs insufficient for some configurations
  - Deterministic vs. random seeds: Fixed seed enables reproducibility; multiple random seeds reveal variance and expected performance

- **Failure signatures:**
  - Non-monotonic MAE with increasing grid size (optimizer not converging; under-optimized for density)
  - High standard deviation across seeds (> mean MAE) indicates optimization instability
  - Chebyshev performing worst on problems without boundary layers (Poisson) suggests distribution mismatch

- **First 3 experiments:**
  1. Reproduce radioactive decay with Chebyshev vs. equidistant distribution at 200 points, single hidden layer, fixed seed. Expect lower MAE with Chebyshev.
  2. Test simple harmonic oscillator with sine-based vs. Chebyshev at 400 points across 20 random seeds. Measure mean MAE and standard deviation to verify sine-based consistency advantage.
  3. Run Poisson equation at 40×40 and 80×80 with equidistant points, comparing one vs. two hidden layers. Verify non-monotonic trend and architecture interaction.

## Open Questions the Paper Calls Out

### Open Question 1
Do mixed training point distributions, such as combining equidistant points with densely packed points at boundaries, yield superior accuracy compared to single-strategy distributions? The "Discussion and future work" section explicitly states that future work will include "mixed distributions of, e.g., equidistant points with densely packed points at the boundaries." This remains unresolved as the current study only evaluated five distinct, static distribution strategies in isolation.

### Open Question 2
Do the observed correlations between specific distribution strategies and equation characteristics persist when using deeper, non-shallow neural network architectures? The authors note that "investigating larger FNN architectures... may provide more information about preferable distribution strategy choices." This is unresolved because experiments were limited to shallow architectures (1 or 2 hidden layers).

### Open Question 3
Can adaptive sampling strategies, which adjust point placement based on residual gradients, outperform the static strategies evaluated in this study? The conclusion lists "approaches towards more adaptive training point strategies" as a specific direction for future work. The paper focused on fixed training sets generated prior to training and did not test dynamic redistribution methods.

### Open Question 4
Can a heuristic be developed to automatically map differential equation characteristics (e.g., rapid decay vs. oscillation) to the optimal training point distribution? The paper concludes that optimal choices are "problem-dependent" and connected to DE characteristics, but currently require manual selection based on experimental evidence. The study provides evidence for specific cases but offers no generalized rule-set or automation for selecting a grid for a new, arbitrary equation.

## Limitations
- Results limited to shallow network architectures and specific differential equations
- Performance differences may be partly attributable to optimization artifacts rather than fundamental distribution advantages
- Computational cost analysis only considers training point generation, not full training process
- Sine-based distribution requires careful parameter tuning that may not generalize

## Confidence

- **High confidence:** PINNs are sensitive to training point distribution; Chebyshev and sine-based distributions show consistent advantages for problems with characteristic solution features
- **Medium confidence:** Non-monotonic accuracy trends with grid resolution; performance variance across random seeds
- **Low confidence:** Proposed sine-based distribution outperforms established methods in general; universal guidelines for distribution selection

## Next Checks
1. Test the proposed sine-based distribution on a broader set of differential equations (including nonlinear and multi-dimensional problems) to verify its general applicability beyond simple harmonic oscillators
2. Investigate the impact of training point distribution when using deeper networks (3+ layers) and modern optimization techniques to assess whether shallow network findings scale
3. Conduct ablation studies isolating the effects of point distribution from weight initialization by comparing fixed vs. random seeds across all tested distributions and equations to quantify optimization stability contributions