---
ver: rpa2
title: Anatomy-Aware Lymphoma Lesion Detection in Whole-Body PET/CT
arxiv_id: '2511.07047'
source_url: https://arxiv.org/abs/2511.07047
tags:
- detection
- swin
- lesion
- lymphoma
- nndetection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether anatomical context improves lymphoma
  lesion detection in whole-body PET/CT. Anatomical segmentation masks from TotalSegmentator
  were provided as auxiliary input to nnDetection (CNN-based) and Swin RetinaUNeTR
  (transformer-based).
---

# Anatomy-Aware Lymphoma Lesion Detection in Whole-Body PET/CT

## Quick Facts
- arXiv ID: 2511.07047
- Source URL: https://arxiv.org/abs/2511.07047
- Authors: Simone Bendazzoli; Antonios Tzortzakakis; Andreas Abrahamsson; Björn Engelbrekt Wahlin; Örjan Smedby; Maria Holstensson; Rodrigo Moreno
- Reference count: 40
- Primary result: Anatomical priors improved CNN-based lymphoma lesion detection (FROC@0.1: 0.284→0.343) but had negligible effect on transformer-based approaches

## Executive Summary
This study investigated whether anatomical context improves lymphoma lesion detection in whole-body PET/CT imaging. The researchers provided anatomical segmentation masks from TotalSegmentator as auxiliary input to both nnDetection (CNN-based) and Swin RetinaUNeTR (transformer-based) detection frameworks. Anatomical priors significantly improved CNN performance by reducing false positives in physiologically high-uptake regions, while transformers showed minimal benefit from the additional anatomical information. The CNN-based approach outperformed the transformer across all evaluation metrics, highlighting the importance of anatomical context for certain detection architectures.

## Method Summary
The study utilized 872 annotated PET/CT volumes from AutoPET for self-supervised pretraining and 228 lymphoma PET/CT studies from Karolinska for detection training. TotalSegmentator generated 104-organ anatomical masks from CT as a third input channel alongside PET and CT. Two architectures were compared: nnDetection (Retina U-Net CNN encoder) and Swin RetinaUNeTR (Swin-T transformer encoder). The Swin encoder underwent self-supervised pretraining via masked reconstruction before detection fine-tuning. Both models were trained with Dice+CE segmentation loss, GIoU box regression, and focal classification loss using nnDetection framework.

## Key Results
- Anatomical priors improved nnDetection FROC@0.1 from 0.284 to 0.343 and mAP from 0.288 to 0.335
- Swin RetinaUNeTR showed negligible improvement with anatomical masks (FROC@0.1: 0.243→0.244; mAP: 0.237→0.234)
- nnDetection outperformed Swin RetinaUNeTR across all evaluation metrics
- Anatomical information helped reduce false positives in high-uptake regions like liver, thyroid, brain, and bladder

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Anatomical segmentation masks improve CNN-based detection by reducing false positives in physiologically high-uptake regions.
- **Mechanism:** The network learns to correlate metabolic uptake with valid anatomical locations, filtering out "hot" regions inconsistent with lymphoma involvement.
- **Core assumption:** TotalSegmentator masks are accurate and convolutional filters can effectively utilize explicit spatial constraints.
- **Evidence:** Abstract states anatomical information helped reduce false positives; discussion confirms learning to ignore physiologically high-uptake regions.

### Mechanism 2
- **Claim:** Vision Transformers derive negligible benefit from explicit anatomical masks due to global attention mechanisms.
- **Mechanism:** Swin Transformer's self-attention across shifted windows may learn organ-context relationships implicitly, making explicit masks redundant.
- **Core assumption:** Self-attention layers can infer anatomical boundaries from raw CT data alone.
- **Evidence:** Abstract shows minimal performance change with masks; discussion suggests global attention may already capture contextual dependencies.

### Mechanism 3
- **Claim:** Self-supervised pretraining via masked reconstruction initializes Transformers to recognize complex 3D textures but doesn't guarantee superiority over CNNs.
- **Mechanism:** Reconstruction of corrupted patches forces the model to understand spatial structure across PET/CT modalities before detection fine-tuning.
- **Core assumption:** Features learned from reconstruction are transferable to localization tasks.
- **Evidence:** Abstract mentions self-supervised pretraining; discussion describes reconstruction as forcing understanding of spatial structure.

## Foundational Learning

- **Concept:** FROC (Free-Response Receiver Operating Characteristic)
  - **Why needed here:** Primary evaluation metric for localization accuracy in lesion detection
  - **Quick check question:** Can you explain why Average Precision (AP) alone might be insufficient for evaluating a detector where location matters as much as classification?

- **Concept:** nnDetection / Auto-configuration
  - **Why needed here:** Baseline performance relies on nnDetection's ability to "fingerprint" datasets and automatically set hyperparameters
  - **Quick check question:** If you change the resolution of input PET/CT volumes, how would nnDetection likely respond compared to a fixed-parameter Transformer pipeline?

- **Concept:** Retina U-Net Architecture
  - **Why needed here:** Winning approach combines object detection with segmentation; auxiliary segmentation head improves feature pyramid
  - **Quick check question:** Why does attaching an auxiliary segmentation head to the highest-resolution FPN output improve bounding box regression?

## Architecture Onboarding

- **Component map:** Inputs (PET + CT + TotalSegmentator Mask) -> Encoder (ResNet/U-Net or Swin Transformer) -> Neck (FPN) -> Heads (Classification, Regression, Auxiliary Segmentation) -> Trainer (nnDetection Framework)

- **Critical path:** nnDetection fingerprinting determines patch size, batch size, and resampling strategy; must re-run if dataset properties change

- **Design tradeoffs:**
  - CNN: High performance via auto-configuration, requires explicit masks for FP reduction, local attention only
  - Transformer: Global attention theoretically better for multi-focal disease, massive computational overhead, didn't respond to anatomical masks

- **Failure signatures:**
  - High FPs in Brain/Bladder: Missing anatomical context; add TotalSegmentator masks
  - Poor Localization: Check IoU thresholds or FPN anchor sizes
  - Transformer Divergence: Verify pre-training weights loaded; check patch size compatibility

- **First 3 experiments:**
  1. Run vanilla nnDetection (CNN) on KUH dataset without masks to establish FROC@0.1 ~0.28 baseline
  2. Add TotalSegmentator masks as 3rd channel to nnDetection to verify ~20% relative performance gain
  3. Run pre-trained Swin RetinaUNeTR inference to confirm performance gap (~0.24 FROC) and visualize attention maps

## Open Questions the Paper Calls Out
- Can alternative autoconfiguration mechanisms tailored for Vision Transformers improve Swin RetinaUNeTR performance?
- Does integration of anatomical priors lead to tangible clinical benefits like reduced reporting time or improved staging accuracy?
- Is the effectiveness of anatomical priors generalizable to other cancer types or imaging modalities beyond lymphoma PET/CT?
- Can attenuation correction CT replace contrast-enhanced diagnostic CT in anatomy-aware detection models without accuracy loss?

## Limitations
- Self-supervised pretraining approach shows promise but transfer learning benefits remain unproven across different anatomical sites
- Confidence is low that transformers inherently capture anatomical context through attention mechanisms due to limited validation
- Effectiveness of anatomical priors on localized or structurally different tumors remains unverified

## Confidence
- **High:** nnDetection CNN baseline performance improvements with anatomical masks (FROC@0.1: 0.284→0.343)
- **Medium:** Anatomical masks reduce false positives in high-uptake regions
- **Low:** Transformers inherently capture anatomical context through attention mechanisms

## Next Checks
1. Perform organ-specific ablation by removing individual organ masks to identify which anatomical regions contribute most to FP reduction in CNN-based detection
2. Test additional transformer architectures (BEiT, MAE) with explicit anatomical masks to determine if Swin-specific design choices explain lack of performance gain
3. Evaluate both nnDetection and Swin RetinaUNeTR on a third lymphoma dataset with different acquisition protocols to assess generalization of anatomical priors across heterogeneous clinical populations