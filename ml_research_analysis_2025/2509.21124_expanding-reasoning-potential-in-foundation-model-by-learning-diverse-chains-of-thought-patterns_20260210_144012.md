---
ver: rpa2
title: Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains
  of Thought Patterns
arxiv_id: '2509.21124'
source_url: https://arxiv.org/abs/2509.21124
tags:
- reasoning
- uni00000013
- data
- patterns
- pattern
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to expand the reasoning potential
  of foundation models by leveraging high-value chain-of-thought data. It defines
  reasoning potential as the inverse of the number of attempts needed to answer correctly,
  then constructs a core reference set of valuable reasoning patterns extracted from
  CoT sequences.
---

# Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns

## Quick Facts
- arXiv ID: 2509.21124
- Source URL: https://arxiv.org/abs/2509.21124
- Reference count: 40
- Primary result: 10B tokens of high-value CoT data improves 85A6B MoE math performance by 9.58% on AIME 2024/2025 and 7.81% RL upper bound.

## Executive Summary
This paper proposes a framework to expand the reasoning potential of foundation models by leveraging high-value chain-of-thought data. It defines reasoning potential as the inverse of the number of attempts needed to answer correctly, then constructs a core reference set of valuable reasoning patterns extracted from CoT sequences. A dual-granularity selection algorithm based on pattern chains and token entropy is used to curate high-value reasoning data (CoTP) from a larger pool. Mid-training on just 10B tokens of CoTP data significantly improves the 85A6B MoE model's performance on challenging math benchmarks, raising AIME 2024/2025 accuracy by 9.58% and the upper bound of RL performance by 7.81%.

## Method Summary
The framework constructs a core set of high-value reasoning patterns through manual curation and LLM annotation, then uses dual-granularity matching (pattern chains + entropy) via weighted DTW to retrieve similar sequences from a larger CoT pool. The selected data is used for mid-training (10-60B tokens), followed by SFT on long-CoT and RL (GSPO). The key innovation is the reasoning potential framework, which claims that expanding a model's ability to solve problems in fewer attempts before RL will reduce downstream RL effort.

## Key Results
- 9.58% improvement in AIME 2024/2025 accuracy over base model
- 7.81% improvement in RL upper bound performance
- Maintains general capability on MMLU/CMMLU while improving math reasoning
- Ablation studies show importance of both pattern chain and entropy features

## Why This Works (Mechanism)

### Mechanism 1
If reasoning potential (inverse of expected attempts to solve correctly) governs RL ceiling, then expanding this potential during mid-training may reduce downstream RL effort. A theoretical formulation equates model potential Φ(M,qi) with 1/E[Ki], where Ki is the first-passage time. By selecting CoT data that minimizes expected reasoning attempts on a target distribution, the framework claims to expand the foundation model's reasoning potential before RL. Core assumption: Sampling-based pass@k correlates with "reasoning potential," and lowering expected attempts on a core set transfers to broader benchmarks. Evidence: [abstract] defines reasoning potential explicitly and claims 7.81% RL upper bound improvement. Break condition: If pass@k on held-out benchmarks does not monotonically improve with CoTP scaling, or if RL gains disappear with different RL algorithms, the potential-expansion claim weakens.

### Mechanism 2
If high-value reasoning patterns can be abstracted and aligned via dual-granularity matching, then selecting CoT data with similar pattern chains and entropy profiles yields higher effective training density than random or difficulty-only selection. Atomic reasoning patterns are annotated by an LLM (DeepSeek-V3). A core set is manually curated with TF-IDF importance weights. Source CoT sequences are matched using weighted DTW on pattern chains + absolute entropy distance, optimized via Hungarian assignment. Core assumption: LLM-annotated patterns capture transferable reasoning primitives; TF-IDF weighting meaningfully distinguishes "important" patterns. Evidence: [Table 3] ablation shows performance drops without entropy or importance weighting. Break condition: If ablations show minimal impact from pattern-chain distance vs. entropy-only, or if pattern annotations prove inconsistent across annotator models, the dual-granularity justification collapses.

### Mechanism 3
If a manually curated core set approximates an "oracle" reasoning distribution, then retrieving similar CoT sequences from a larger pool should yield training data with comparable pattern density at scale. Core set examples (high-value, manually verified patterns) serve as anchors. Source pool is matched via weighted DTW; Hungarian algorithm ensures optimal 1:o assignment with capacity constraints. Core assumption: The core set is sufficiently representative of "high-value" reasoning; similarity in pattern/entropy space correlates with downstream utility. Evidence: [Figure 6b–c] shows CoTP distribution aligning more closely with core set than LongCoTPool. Break condition: If core set size or composition changes significantly alter CoTP performance, or if retrieved data fails to improve pass@k beyond random long-CoT sampling, the approximation assumption fails.

## Foundational Learning

- Concept: Dynamic Time Warping (DTW)
  - Why needed here: Core to the dual-granularity distance computation between reasoning pattern sequences of varying lengths.
  - Quick check question: Can you implement DTW with a custom distance metric and explain why it handles sequence alignment better than Euclidean distance?

- Concept: TF-IDF Weighting
  - Why needed here: Used to quantify pattern importance within and across questions for weighted DTW.
  - Quick check question: Given a set of reasoning pattern sequences per question, how would you compute TF-IDF scores to identify distinctive patterns?

- Concept: Chain-of-Thought Reasoning Patterns
  - Why needed here: The entire framework relies on abstracting atomic reasoning operations (e.g., "parameter substitution," "inequality parsing") from CoT sequences.
  - Quick check question: Given a math CoT solution, can you manually extract a sequence of reasoning patterns that would generalize across problem types?

## Architecture Onboarding

- Component map:
  1. Core Set Construction: Manual curation → LLM annotation → TF-IDF weighting → Core set C_core.
  2. Source Pool Preparation: Collect long-CoT data → Filter/annotate pattern chains + entropy → D_pool.
  3. Dual-Granularity Matching: For each core example, compute weighted DTW distance to all source examples → Build cost matrix → Hungarian assignment → D_select.
  4. Training Pipeline: Mid-training on D_select (10–60B tokens) → SFT on long-CoT → RL (GSPO).

- Critical path: Core set quality → Annotation consistency → DTW distance calibration → Assignment optimization → Mid-training data blend ratio. Errors in pattern annotation or distance weighting cascade directly to data quality.

- Design tradeoffs:
  - Core set size vs. representativeness: Too small may miss key patterns; too large increases manual effort.
  - λ in distance computation: High λ emphasizes patterns; low λ emphasizes entropy. Ablation suggests λ=0.8 works, but domain-specific tuning may be needed.
  - Assignment ratio o: More assignments per core example increase diversity but may dilute pattern density.

- Failure signatures:
  - CoTP performance no better than random long-CoT sampling → Pattern/entropy features not discriminative.
  - High variance across RL seeds → Mid-training instability or data contamination.
  - Pass@k plateaus early → Core set may not capture transferable patterns.

- First 3 experiments:
  1. Reproduce ablations: Train with w/o entropy, n-gram n=2, and w/o importance on a small scale (1B tokens) to validate component contributions.
  2. Sensitivity analysis: Vary core set size (100, 500, 1000 examples) and measure alignment (KL divergence, token length distribution) and downstream pass@1.
  3. Cross-domain test: Apply CoTP framework to a non-math reasoning domain (e.g., logical puzzles) using the same annotation prompt to assess generalizability of pattern abstraction.

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework's specific taxonomy of atomic reasoning patterns be effectively generalized to non-mathematical domains like coding or common-sense reasoning without substantial manual redesign? Basis: [Explicit] The Introduction and Abstract focus exclusively on "challenging mathematical reasoning," and Section 3.1 evaluates only mathematical benchmarks (AIME, MATH500). Why unresolved: The reasoning patterns defined (e.g., "Formula Substitution") are domain-specific; it is unclear if the current dual-granularity selection algorithm captures high-value patterns in domains with different structural constraints. What evidence would resolve it: Successful application of the CoTP selection method to code generation (e.g., MBPP) or logical deduction tasks (e.g., BBH) showing consistent performance gains over non-selected data.

### Open Question 2
Is manual screening necessary for constructing the "Core Set," or can this process be automated to scale the framework to trillions of tokens? Basis: [Explicit] Page 3 states that questions are "manually screened for quality" and patterns are selected based on "manual evaluation and importance scores." Why unresolved: Manual verification acts as a bottleneck for industrial-scale pre-training, making it difficult to apply this method to the massive, noisy datasets typically used for foundation models. What evidence would resolve it: Experiments demonstrating that a fully automated core set construction method (e.g., using model-based filtering alone) achieves comparable improvements in reasoning potential to the manually curated set.

### Open Question 3
To what extent does the choice of the annotation model (DeepSeek-V3) bias the extracted reasoning patterns and limit the student model's ability to surpass the teacher's capabilities? Basis: [Inferred] Section 2.2 notes the use of Deepseek-V3 for annotating reasoning pattern chains, and Section 4 compares the student's patterns to DeepSeek-R1. Why unresolved: Defining "high-value" patterns through a specific teacher model may propagate that model's specific reasoning blind spots or stylistic inefficiencies, potentially capping the student's potential. What evidence would resolve it: A comparative analysis where models are trained on CoTP data generated by different frontier models (e.g., Gemini vs. DeepSeek) to see if the "reasoning potential" is constrained by the annotator's upper bound.

## Limitations
- Manual core set construction represents a significant bottleneck for scaling the framework to industrial datasets
- The theoretical framework of "reasoning potential" lacks external validation and may be specific to the mathematical reasoning domain
- The 85A6B MoE model architecture details are not fully specified, making exact reproduction challenging

## Confidence
- High Confidence: The empirical observation that mid-training on carefully selected CoT data improves pass@k on math benchmarks. The ablation studies showing the importance of dual-granularity features (pattern chains + entropy) are methodologically sound.
- Medium Confidence: The theoretical framing of reasoning potential and its relationship to RL performance. While internally consistent and showing correlation, the causal mechanism is not independently validated.
- Low Confidence: The generalizability of the pattern annotation scheme and core set construction methodology across different domains or annotation models. The paper does not test whether different annotators or domains yield consistent pattern abstractions.

## Next Checks
1. **Ablation of Theoretical Framework:** Train an otherwise identical model using random long-CoT sampling (matched for token count and difficulty) instead of CoTP, then apply identical RL. Compare not just final pass@k but the learning curves and RL sample efficiency to test whether the claimed reasoning potential expansion actually reduces RL effort.

2. **Core Set Robustness Test:** Systematically vary core set size (e.g., 100, 500, 1000 examples) and composition (random vs. high-pass@k vs. diverse reasoning types) while keeping all other parameters constant. Measure both the alignment metrics (KL divergence to core set) and downstream performance to determine whether the manual curation step is truly necessary or whether a different selection criterion could achieve similar results.

3. **Cross-Domain Pattern Transferability:** Apply the exact same pattern annotation prompt and dual-granularity selection framework to a non-mathematical reasoning domain (such as logical puzzles or commonsense reasoning). Compare whether the same pattern abstraction quality and performance gains transfer, or whether the framework is specialized to mathematical reasoning patterns.