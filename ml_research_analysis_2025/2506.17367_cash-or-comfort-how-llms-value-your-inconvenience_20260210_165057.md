---
ver: rpa2
title: Cash or Comfort? How LLMs Value Your Inconvenience
arxiv_id: '2506.17367'
source_url: https://arxiv.org/abs/2506.17367
tags:
- llms
- user
- they
- decisions
- inconvenience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors develop a framework to quantify how large language\
  \ models (LLMs) value human inconveniences\u2014like waiting, walking, hunger, or\
  \ pain\u2014against monetary compensation. Using prompts that ask six leading LLMs\
  \ to act as AI assistants making trade-offs for users, they systematically vary\
  \ reward levels and discomfort magnitudes."
---

# Cash or Comfort? How LLMs Value Your Inconvenience

## Quick Facts
- **arXiv ID:** 2506.17367
- **Source URL:** https://arxiv.org/abs/2506.17367
- **Reference count:** 35
- **Primary result:** LLMs exhibit highly variable and sometimes erratic valuations of human inconveniences when compared to monetary rewards, with prompt changes causing large shifts in responses.

## Executive Summary
This study develops a framework to quantify how large language models (LLMs) value human inconveniences—like waiting, walking, hunger, or pain—against monetary compensation. Using systematic prompts across six leading LLMs, the authors fit logistic regression models to estimate each model's "price of inconvenience" (the reward at which the model accepts a trade-off with 50% probability). The research reveals high variability in valuations across models and scenarios, including some models accepting unreasonably low rewards for major inconveniences or rejecting free offers. Additionally, minor prompt changes can cause large, sometimes erratic shifts in responses, questioning the reliability of current LLMs for autonomous personal assistance in cash-versus-comfort decisions.

## Method Summary
The authors prompt six LLMs (GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash, DeepSeek-V3, Llama 3.3-70B, Mixtral 8x22B) with binary trade-off decisions between monetary rewards (€0.10–€1,000) and four inconvenience types (time, distance, hunger, pain). For each scenario, they collect 5 repetitions per reward level at temperature T=1.0, fit logistic regression to the binary responses, and extract the 50% acceptance threshold as the "price of inconvenience." They also test robustness across 10 prompt variations including language changes, perspective shifts, and chain-of-thought prompting.

## Key Results
- LLMs show highly variable valuations of inconveniences, with some accepting €1 for 10 hours of waiting or rejecting free money
- Minor prompt changes (language, person, context) can cause valuation shifts up to two orders of magnitude
- Chain-of-thought prompting reduces freebie rejection but introduces noisier decision boundaries
- Different models exhibit distinct failure patterns, from extreme greed to excessive caution

## Why This Works (Mechanism)

### Mechanism 1: Logistic Regression for Valuation Thresholds
The method quantifies LLM valuations by fitting logistic regression to binary accept/reject responses across a logarithmic range of rewards. The 50% acceptance threshold (where P(acceptance)=0.5) is extracted as the "price of inconvenience." This assumes monotonic increase in acceptance probabilities as rewards increase.

### Mechanism 2: Prompt Sensitivity in LLMs
Small prompt variations—especially language changes—can dramatically shift LLM trade-off valuations. Changing from third-person to first-person, or translating prompts to Dutch/French/Chinese, alters implicit cost-of-living inferences, cultural norms, or safety priors, leading to valuation shifts up to two orders of magnitude.

### Mechanism 3: Chain-of-Thought Prompting Effects
Chain-of-thought prompting reduces some anomalous behaviors (freebie rejection, power-of-ten discontinuities) but introduces noisier decision boundaries. CoT forces explicit reasoning steps, potentially overriding heuristic responses, but also introduces variance as intermediate reasoning paths differ across runs.

## Foundational Learning

- **Concept: Logistic Regression for Binary Decision Thresholds**
  - **Why needed here:** The paper's core quantification method relies on fitting logits to binary accept/reject responses. Understanding how decision boundaries map to probability thresholds (P=0.5) is essential to interpret "price of inconvenience" values.
  - **Quick check question:** If an LLM accepts €5 with P=0.2 and €50 with P=0.8, where would the logistic decision boundary likely fall?

- **Concept: Prompt Sensitivity in LLMs**
  - **Why needed here:** The paper demonstrates that minor prompt changes (person, language, context) cause large valuation shifts. Engineers must understand this fragility before deploying LLMs in decision-making roles.
  - **Quick check question:** Name two prompt dimensions the paper identifies as causing large valuation shifts.

- **Concept: Freebie Dilemma**
  - **Why needed here:** LLMs sometimes reject strictly dominant offers (e.g., free money), mirroring human suspicion of "too good" deals. This is a failure mode for autonomous agents.
  - **Quick check question:** In the paper, what explanation did an LLM give when rejecting a free monetary offer?

## Architecture Onboarding

- **Component map:** Prompt Generator -> LLM Interface -> Response Aggregator -> Logit Fitter -> Robustness Tester
- **Critical path:** Prompt design → LLM query (repeated) → Probability computation → Logit fitting → Threshold extraction → Robustness comparison
- **Design tradeoffs:** Temperature T=1.0 introduces variability; T=0 would be more deterministic but less representative. Binary responses simplify fitting but discard nuance; open-ended responses would be harder to quantify.
- **Failure signatures:** Non-monotonic acceptance (higher reward rejected), extreme thresholds (€0.10 or >€1,000), high variance across prompt variations
- **First 3 experiments:**
  1. Replicate baseline Time scenario for one model, fit logit, verify 50% threshold matches paper's reported value (±10%)
  2. Test prompt variation sensitivity: run first-person vs. third-person variation, quantify threshold shift magnitude
  3. Apply chain-of-thought prompting to a model showing freebie rejection; confirm whether noise increases and anomalies decrease

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the "prices of inconvenience" derived from LLMs compare to actual human willingness-to-accept thresholds, and does this alignment vary across different cultural or socioeconomic groups?
- **Basis in paper:** The authors state that "on the user side, we need to better understand what people want from AI systems" and emphasize the need to ensure models "align with human values."
- **Why unresolved:** The study quantifies model behavior in monetary terms but does not include a human baseline to determine if the valuations are misaligned with actual user preferences.
- **What evidence would resolve it:** A comparative study measuring human participants' trade-off thresholds using the exact same scenarios and prompts used for the LLMs.

### Open Question 2
- **Question:** What underlying mechanisms drive the drastic instability in valuations when switching prompt languages, and do these shifts reflect cultural norms or training artifacts?
- **Basis in paper:** The authors observe that language changes cause valuations to shift by orders of magnitude but note the pattern "does not clearly support [cost-of-living] interpretation."
- **Why unresolved:** The results show massive sensitivity to language, but the paper does not determine if this is due to cultural alignment, data imbalance, or tokenization issues.
- **What evidence would resolve it:** A causal analysis of LLM internal states across languages, or experiments controlling for cost-of-living vs. linguistic structure.

### Open Question 3
- **Question:** Can prompt engineering strategies like chain-of-thought (CoT) be refined to eliminate specific irrational behaviors (such as the "freebie dilemma") without introducing the decision noise currently observed?
- **Basis in paper:** The authors find that CoT prompting mitigates the "freebie dilemma" but simultaneously notes that "a more noisy decision boundary emerges for all models."
- **Why unresolved:** There is a trade-off between using CoT to smooth decision boundaries and the introduction of variance, making it unclear if reliable mitigation is possible via prompting alone.
- **What evidence would resolve it:** Testing hybrid prompting strategies or fine-tuning approaches to assess if "freebie" rejection can be removed while maintaining low noise levels.

## Limitations

- The logistic regression framework may misrepresent model behavior when decision boundaries are unstable or multimodal
- Binary "Yes"/"No" forcing may artificially constrain nuanced reasoning
- Temperature T=1.0 introduces variability that could mask systematic patterns
- The study does not include human baselines for comparison

## Confidence

- **High confidence:** The observation that minor prompt changes cause large valuation shifts is well-supported by systematic testing across 10 variations and multiple languages
- **Medium confidence:** The logistic regression method for estimating "price of inconvenience" is technically sound but limited by non-monotonic response patterns
- **Low confidence:** The generalizability of specific threshold values across real-world deployment contexts, given the extreme fragility to prompt framing demonstrated

## Next Checks

1. Test logistic regression stability by varying the number of reward points (50 vs 200) and checking if 50% thresholds remain consistent within 10%
2. Systematically vary prompt dimensions identified as sensitive (person, language, context) on a single model-scenario pair to quantify baseline variance
3. Compare chain-of-thought vs direct response patterns across models to determine if CoT consistently reduces freebie dilemmas while controlling for increased noise