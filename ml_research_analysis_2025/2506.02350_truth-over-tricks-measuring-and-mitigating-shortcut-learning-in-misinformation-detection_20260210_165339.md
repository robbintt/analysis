---
ver: rpa2
title: 'Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation
  Detection'
arxiv_id: '2506.02350'
source_url: https://arxiv.org/abs/2506.02350
tags:
- uni00000013
- uni00000011
- uni00000015
- uni00000048
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRUTH OVERTRICKS, a unified evaluation paradigm
  for diagnosing shortcut learning in misinformation detection systems. It distinguishes
  between intrinsic shortcut induction, which captures spurious correlations naturally
  present in existing benchmarks, and extrinsic shortcut injection, which introduces
  adversarially crafted variations using LLMs.
---

# Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection

## Quick Facts
- **arXiv ID:** 2506.02350
- **Source URL:** https://arxiv.org/abs/2506.02350
- **Reference count:** 40
- **Primary result:** Current detectors suffer accuracy drops to 0% under shortcut exposure; SMF augmentation mitigates this with up to 838.4% gains

## Executive Summary
This paper introduces TRUTH OVERTRICKS, a unified evaluation paradigm for diagnosing shortcut learning in misinformation detection systems. It distinguishes between intrinsic shortcuts (naturally present spurious correlations) and extrinsic shortcuts (adversarially crafted variations via LLMs). The study reveals that seven evaluated detectors across 16 benchmarks suffer substantial performance degradation when exposed to both shortcut types, with accuracy dropping to 0% in extreme cases. To address this, the authors propose SMF, an LLM-augmented data augmentation framework that mitigates shortcut reliance through paraphrasing, factual summarization, and sentiment normalization. SMF consistently improves robustness, with performance gains reaching up to 838.4% under explicit injection attacks, demonstrating the effectiveness of data-centric mitigation strategies.

## Method Summary
The study creates a unified evaluation paradigm with 16 misinformation detection benchmarks, including 14 existing datasets and 2 new factual datasets. For intrinsic shortcuts, they construct distribution-shifted "Shortcut" splits where the joint distribution of shortcut features and labels differs between training and test sets. For extrinsic shortcuts, they use Meta-Llama-3-8B-Instruct to inject adversarially crafted variations through three strategies: Vanilla (general rewriting), Explicit (attribute-specific rewriting with clear prompts), and Implicit (subtle attribute manipulation). Seven detectors are evaluated: LLM-based (MistralV3/Llama3), LM-based (BERT/DeBERTA), and debiasing methods (CMTR/DISC/CATCH). The SMF framework mitigates shortcuts through LLM-based paraphrasing, summarization, and neutralization of training data.

## Key Results
- Current detectors suffer accuracy drops to 0% when exposed to shortcut correlations
- Intrinsic shortcuts cause performance degradation ranging from 2.5% to 98.5% across detectors
- Explicit injection attacks reduce accuracy by up to 99.9% in worst cases
- SMF consistently improves robustness, with gains up to 838.4% under explicit injection attacks
- Data augmentation (SMF) outperforms model-centric debiasing approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Detectors fail because they internalize spurious correlations between non-causal surface features (shortcuts) and labels during training, rather than learning semantic authenticity.
- **Mechanism:** Standard Empirical Risk Minimization (ERM) favors features that are predictive in the training set but non-causal. If a dataset contains predominantly negative-toned misinformation, the model learns "negative tone → fake" instead of analyzing the factual content.
- **Core assumption:** The joint distribution of the shortcut feature and the label differs between training and test sets.
- **Evidence anchors:**
  - [abstract] "Misinformation detection models often rely on superficial cues (i.e., shortcuts) that correlate with misinformation in training data but fail to generalize..."
  - [section 3.1] "detectors are prone to associating auxiliary features with authenticity..."
  - [corpus] "Bot Meets Shortcut" confirms that ERM models rely on spurious correlations in social bot detection.
- **Break condition:** If the training data is perfectly balanced such that P(Shortcut|Real) = P(Shortcut|Fake).

### Mechanism 2
- **Claim:** LLMs can effectively generate adversarial inputs by manipulating surface attributes (sentiment, tone, perplexity) without altering the underlying truthfulness.
- **Mechanism:** An LLM is prompted to rewrite text with a specific attribute (e.g., "make it formal"). It modifies the distribution of tokens while preserving the semantic meaning (authenticity).
- **Core assumption:** The LLM possesses sufficient instruction-following capability to decouple writing style from factual content.
- **Evidence anchors:**
  - [section 2.2] "Taking Word Choice as an example... The clear distributional shift indicates that LLMs can effectively modify word usage patterns."
  - [table 5] Shows explicit prompts used for injection.
  - [corpus] "MiMu" discusses mitigating multiple shortcut behaviors in Transformers.
- **Break condition:** If the LLM hallucinates or refuses to rewrite, or if the rewriting process inadvertently corrects the misinformation.

### Mechanism 3
- **Claim:** Data augmentation via LLM-based rewriting (SMF) mitigates shortcuts by normalizing or diversifying surface attributes, forcing the model to rely on semantic content.
- **Mechanism:** The framework applies three strategies: (1) Paraphrasing to vary token patterns, (2) Summarization to remove non-factual stylistic noise, and (3) Neutralization to balance attribute distributions.
- **Core assumption:** The LLM performing the augmentation does not introduce new artifacts or biases that become shortcuts themselves.
- **Evidence anchors:**
  - [abstract] "SMF consistently improves robustness... encouraging models to rely on deeper semantic understanding rather than shortcut cues."
  - [table 3] Shows performance recovery (e.g., BERT improves from 7.0% to 63.4% under Word Choice attacks).
  - [corpus] "ClearGCD" proposes mitigating shortcut learning in Generalized Category Discovery via contrastive learning.
- **Break condition:** If the augmentation pipeline creates unnatural text or fails to sufficiently diversify the shortcut attribute.

## Foundational Learning

- **Concept: Spurious Correlation / Shortcut Learning**
  - **Why needed here:** This is the central problem definition. You must understand that models optimize for statistical correlation, not logical causation.
  - **Quick check question:** If a model trained on red circles and blue squares sees a blue circle, does it predict "square" or "circle"?

- **Concept: Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** The paper evaluates detectors under distribution shifts where the correlation between shortcuts and labels is inverted or altered from the training set.
  - **Quick check question:** Why does test accuracy often drop if the test set statistics differ from the training set statistics?

- **Concept: Data-Centric AI**
  - **Why needed here:** The proposed solution (SMF) focuses on modifying the training data distribution rather than changing the loss function or architecture.
  - **Quick check question:** Can you fix a model's bias solely by adding more data, without changing the model architecture?

## Architecture Onboarding

- **Component map:** 16 datasets -> Indicator classifiers -> Distribution-shifted splits -> LLM injector (Meta-Llama-3-8B) -> 7 detectors (LLM-based/LM-based/debiasing) -> SMF augmentor -> Performance evaluation
- **Critical path:**
  1. **Dataset Splitting:** Constructing the "Shortcut" split is critical. You must partition data so that P(S, Y)train ≠ P(S, Y)test.
  2. **Injection Validation:** Before running expensive experiments, validate that the LLM injector actually changed the target attribute using a classifier.
- **Design tradeoffs:**
  - **Model-centric vs. Data-centric:** The paper argues model-side debiasing (CATCH, DISC) is insufficient compared to data augmentation (SMF).
  - **Intrinsic vs. Extrinsic:** Intrinsic shortcuts are "free" (found in data) but limited; Extrinsic shortcuts require LLM generation costs but offer controlled adversarial testing.
- **Failure signatures:**
  - **Accuracy Inversion:** The model performs worse than random guessing (e.g., 0% accuracy) when the shortcut correlation is perfectly inverted.
  - **Cross-task Transfer:** A detector trained for misinformation unexpectedly performs well on sentiment classification, proving it learned the wrong feature.
- **First 3 experiments:**
  1. **Intrinsic Probe:** Train a standard classifier (e.g., DeBERTa) on the standard split, then evaluate on the "Shortcut" split to confirm the vulnerability.
  2. **Explicit Injection Attack:** Use the LLM to rewrite test data with "Simple" vs. "Complex" word choices. Measure the performance gap.
  3. **SMF Ablation:** Apply only the "Summary" augmentation to the training data of the failed detector from Experiment 1. Retrain and check if robustness improves.

## Open Questions the Paper Calls Out

- **Question:** How does shortcut learning manifest in misinformation detectors that rely on social context (e.g., user profiles, propagation networks) compared to the text-based detectors evaluated in this study?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that they focused on textual content and excluded detectors employing additional information like social context.
  - **Why unresolved:** The current study restricts the TRUTH OVERTRICKS evaluation paradigm to text-based analysis, leaving the vulnerability of graph-based or context-aware detectors unexplored.
  - **What evidence would resolve it:** An extension of the evaluation framework to include social context features, measuring performance degradation in graph neural networks when social structures are manipulated or biased.

- **Question:** Can LLM-based strategies be refined to reliably simulate implicit author attributes (e.g., age, gender) for robustness testing without triggering safety refusals or failing to capture distinct styles?
  - **Basis in paper:** [explicit] The authors report that implicit injection strategies resulted in a very low Cohen's Kappa (0.035) and frequent model refusals.
  - **Why unresolved:** Current RLHF safety alignment prevents LLMs from easily impersonating specific demographics or rewriting misinformation.
  - **What evidence would resolve it:** Successful generation of implicit shortcuts with high human annotator agreement (Kappa > 0.8) and low refusal rates.

- **Question:** Does the Shortcut Mitigation Framework (SMF) introduce unintended performance degradation on authentic or "clean" data that does not contain adversarial shortcuts?
  - **Basis in paper:** [inferred] The authors note that performance decreases slightly under implicit injection when using SMF.
  - **Why unresolved:** While SMF is shown to improve robustness against attacks, its potential trade-off on standard, non-adversarial detection tasks is not comprehensively quantified.
  - **What evidence would resolve it:** A comparative evaluation of detector accuracy on original, unmodified benchmarks versus SMF-processed versions.

## Limitations

- The LLM-based shortcut injection mechanism relies heavily on the capability of the chosen LLM to preserve truthfulness while manipulating surface features.
- The SMF augmentation framework assumes that LLM-generated paraphrases, summaries, and neutralizations do not introduce new spurious correlations.
- The intrinsic shortcut analysis depends on finding naturally occurring distributional shifts in existing benchmarks, which may be limited.
- Performance gains reported for SMF are impressive but evaluated primarily on a limited set of detector architectures.

## Confidence

- **High confidence:** The existence of shortcut learning in misinformation detection systems and the methodology for creating distribution-shifted evaluation splits.
- **Medium confidence:** The effectiveness of SMF as a mitigation strategy, though long-term stability requires further validation.
- **Low confidence:** The claim that model-centric debiasing approaches are universally inferior to data augmentation, as the comparison is limited to specific architectures.

## Next Checks

1. Conduct ablation studies on the SMF framework to identify which augmentation strategy contributes most to performance gains and whether combinations create diminishing returns or new shortcuts.
2. Test the robustness of LLM-generated shortcuts by having human annotators verify that truthfulness labels remain unchanged after rewriting, and measure inter-annotator agreement rates.
3. Evaluate whether detectors that benefit from SMF on misinformation detection benchmarks also show improved performance on out-of-domain tasks like sentiment analysis or topic classification.