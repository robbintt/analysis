---
ver: rpa2
title: 'Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large
  Reasoning Models'
arxiv_id: '2510.21978'
source_url: https://arxiv.org/abs/2510.21978
tags:
- arxiv
- reasoning
- answer
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reasoning-focused reinforcement learning with verifiable rewards
  improves performance on targeted tasks but degrades general capabilities such as
  perception and robustness. We show that open-source reasoning models consistently
  underperform their base models on non-reasoning benchmarks after RLVR finetuning.
---

# Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models

## Quick Facts
- arXiv ID: 2510.21978
- Source URL: https://arxiv.org/abs/2510.21978
- Reference count: 40
- One-line primary result: RECAP preserves or improves general capabilities while matching or exceeding reasoning performance compared to reasoning-only or uniform baselines.

## Executive Summary
Reasoning-focused reinforcement learning with verifiable rewards improves performance on targeted tasks but degrades general capabilities such as perception and robustness. This paper addresses catastrophic forgetting in large vision-language models (VLMs) by proposing RECAP—a replay strategy with dynamic objective reweighting that integrates general-capability data into RLVR and adapts loss coefficients based on convergence rates and instability signals. Experiments show RECAP recovers over 2% segmentation accuracy on perception tasks while achieving up to 6% gains on reasoning benchmarks, without sacrificing correctness or efficiency.

## Method Summary
RECAP mitigates catastrophic forgetting during RLVR finetuning by combining experience replay of general-capability data with dynamic loss reweighting. The method tracks per-objective convergence rates and instability using sliding window statistics, then computes priority scores to adaptively weight heterogeneous losses (reasoning accuracy, format, IoU, NTP). During training, batches are sampled uniformly across reasoning and general domains, but losses are weighted by the dynamic coefficients before applying GRPO updates. This approach steers learning away from saturated objectives while preserving foundational skills learned during pretraining.

## Key Results
- On perception tasks, RECAP recovers over 2% segmentation accuracy compared to reasoning-only baselines
- On reasoning benchmarks, RECAP achieves gains of up to 6% accuracy while preserving general capabilities
- The method yields more concise rationales without sacrificing correctness, improving inference efficiency

## Why This Works (Mechanism)

### Mechanism 1: Gradient Interference Mitigation via Experience Replay
Replaying general capability data preserves foundational skills by maintaining gradient alignment with the pretraining distribution. By mixing general domain data (detection, segmentation) alongside reasoning data, gradient updates are constrained by general capability loss, preventing weights from moving too far into a reasoning-only local minimum.

### Mechanism 2: Dynamic Loss Reweighting via Convergence Signals
Adaptive reweighting prevents over-optimization of easy rewards by calculating priority scores combining convergence rate and instability. This down-weights "saturated" objectives and up-weights "volatile" or "underperforming" ones, reallocating optimization capacity dynamically based on short-horizon sliding window statistics.

### Mechanism 3: Inference Efficiency via Implicit Pruning
Dynamic weighting leads to more concise rationales by implicitly learning which tasks require extensive reasoning vs. direct perception. The method converges to shorter reasoning traces (~27 words vs ~67 words) on reasoning tasks without losing accuracy, suggesting the dynamic focus prevents "thought rambling."

## Foundational Learning

**Concept: Catastrophic Forgetting**
- Why needed here: RLVR is a form of sequential fine-tuning that risks erasing pretraining knowledge, justifying the "replay" component
- Quick check question: Does the model lose performance on held-out general tasks after training only on math data?

**Concept: GRPO (Group Relative Policy Optimization)**
- Why needed here: The paper uses GRPO as the base RL algorithm, which replaces the value network with group-relative advantages
- Quick check question: How is the advantage calculated for a group of rollouts in GRPO?

**Concept: Multi-objective Optimization**
- Why needed here: RECAP balances heterogeneous losses (RL rewards + SFT losses), requiring understanding of trade-offs
- Quick check question: How does the temperature parameter T affect the sharpness of the priority distribution over objectives?

## Architecture Onboarding

**Component map:**
Data Loader -> Objective Calculator -> Signal Tracker -> Scheduler -> Optimizer

**Critical path:** The Signal Tracker and Scheduler must be lightweight and run on-CPU or as a low-overhead hook to avoid stalling GPU training. The dependency is: Batch → Loss → Stats → Weights → Step.

**Design tradeoffs:**
- Window Size (W): Small W reacts fast to noise; large W smooths but lags. Paper uses W implicitly relative to total steps (e.g., 50 steps in ablation)
- Temperature (T): T=5 is default. Lower T aggressively focuses on one objective, risking collapse; higher T approaches uniform sampling

**Failure signatures:**
- Reward Collapse: Instability signal i_k stays high indefinitely, causing weights to oscillate
- Format Forgetting: If general data is too sparse, format rewards saturate, and the model loses instruction-following ability
- Stagnation: If c_k ≈ 1 for all objectives early, weights flatten and training stalls

**First 3 experiments:**
1. Baseline Verification: Train Qwen2.5-VL-3B/7B on reasoning-only data (no replay) and confirm >2% drop on LISA/segmentation benchmarks
2. Ablation on Reweighting: Compare RECAP vs. Uniform weights on the Hybrid Setting (Table 2), tracking if RECAP down-weights format rewards over time
3. Hyperparameter Sensitivity: Sweep window size W and temperature T, plotting final accuracy vs. IoU to assess trade-off frontier robustness

## Open Questions the Paper Calls Out

### Open Question 1
Does the RECAP dynamic reweighting mechanism effectively transfer to non-RL objectives, such as preference-based alignment (e.g., DPO, KTO) or process reward models (PRMs)? The authors state in the Limitations section that they leave comprehensive evaluation across non-RL objectives to future work.

### Open Question 2
Can a learned or weighted combination of convergence rate and instability signals outperform the simple additive heuristic (s_k = c_k + i_k) used in the study? Section 4 notes that finetuning the trade-off between those two terms offers finer-grained control and potentially improves performance.

### Open Question 3
Does RECAP provide orthogonal benefits to KL-divergence regularization, or does it render the KL penalty redundant? Section 5.1 states they disable the reference-KL penalty to disentangle the effectiveness of regularization approaches and their replay mechanism.

## Limitations
- The method's effectiveness is demonstrated primarily on Qwen2.5-VL models and specific datasets, with limited testing on different architectures or problem domains
- The connection between concise rationales and genuine reasoning efficiency is inferred rather than directly validated
- The specific reward formulations for accuracy, IoU, and format are not detailed, requiring assumptions about their scale and combination

## Confidence

**High Confidence:** The observation that reasoning-only RLVR degrades general capabilities is well-supported by baseline experiments showing >2% drops on LISA segmentation and other benchmarks.

**Medium Confidence:** The specific RECAP implementation details and the exact mechanism by which dynamic reweighting improves both reasoning and general capabilities are reasonable but require careful implementation matching.

**Low Confidence:** The assumption that short-horizon convergence signals provide reliable long-term guidance in non-stationary RL settings is plausible but untested.

## Next Checks

1. **Long-term Stability Validation:** Extend training beyond 500 steps and monitor whether RECAP maintains its advantage in preserving general capabilities over longer horizons, particularly as convergence signals may change character.

2. **Alternative Architecture Testing:** Apply RECAP to a different VLM architecture (e.g., LLaVA or MiniGPT-4) to verify that the forgetting phenomenon and mitigation strategy generalize beyond Qwen2.5-VL models.

3. **Objective Signal Robustness:** Systematically vary the sliding window size W and temperature T parameters across a wider range than reported, and test whether the method remains effective under different reward formulations (e.g., continuous vs. binary rewards) to assess sensitivity to implementation details.