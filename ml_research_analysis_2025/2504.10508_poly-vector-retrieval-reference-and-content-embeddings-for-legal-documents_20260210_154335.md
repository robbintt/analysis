---
ver: rpa2
title: 'Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents'
arxiv_id: '2504.10508'
source_url: https://arxiv.org/abs/2504.10508
tags:
- crfb
- federal
- constituicao
- caput
- inciso
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving legal documents
  when queries reference provisions by labels (e.g., "Article 5") or identifiers (URNs)
  rather than semantic content. Standard embedding-based retrieval methods struggle
  with such referential queries.
---

# Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents

## Quick Facts
- arXiv ID: 2504.10508
- Source URL: https://arxiv.org/abs/2504.10508
- Reference count: 40
- Primary result: Poly-Vector Retrieval significantly improves retrieval accuracy for legal queries that reference document provisions by labels or identifiers (e.g., "Article 5") without degrading semantic search performance.

## Executive Summary
This paper addresses the challenge of retrieving legal documents when queries reference provisions by labels (e.g., "Article 5") or identifiers (URNs) rather than semantic content. Standard embedding-based retrieval methods struggle with such referential queries. The proposed Poly-Vector Retrieval method assigns multiple distinct embeddings to each legal provision: one for its content and separate ones for its label, URN, and a combined label+URN. This approach is inspired by Frege's distinction between sense and reference. Experiments on the Brazilian Federal Constitution show that Poly-Vector significantly improves retrieval accuracy for label-centric and URN-based queries, while maintaining strong performance on semantic queries. The method also shows potential for resolving internal and external cross-references.

## Method Summary
The Poly-Vector Retrieval method assigns four distinct embeddings per legal provision: content embedding (DISP), label embedding (LBL), URN embedding, and combined label+URN embedding (I+L). The approach is implemented in a RAG pipeline where documents are parsed hierarchically (Titles, Chapters, Articles, etc.), labels and URNs are generated according to specific formats, and all embeddings are stored in a unified vector index. During retrieval, queries are embedded and compared against all embeddings via k-NN search, with results deduplicated and ranked by maximum similarity. The method supports hierarchical multi-layer indexing and optional query normalization.

## Key Results
- Poly-Vector achieved 0.7982–0.8349 similarity on purely referential query "Explain Art. 69" versus <0.64 for non-Poly methods
- URN-based queries reached 0.9191 similarity with Poly-Vector + Multi-layer + Query Normalization
- Higher-level label queries (e.g., "Theme of Chapter VI of Title VIII") achieved 0.76–0.78 similarity with Poly-Vector versus 0.65–0.71 for non-Poly methods
- Poly-Vector + Multi-layer + Query Normalization maintained top or near-top performance across all query types

## Why This Works (Mechanism)

### Mechanism 1: Reference-Content Embedding Separation
Treating document labels as distinct vector representations from content embeddings improves retrieval of referential queries (e.g., "Article 5") without degrading semantic search. Labels like "Art. 5º" or URN identifiers are embedded separately from the provision's full text. During retrieval, label embeddings function as "rigid designators" that match query mentions of identifiers directly, while content embeddings capture semantic substance. Both embedding types map to the same underlying text chunk.

### Mechanism 2: Unified Search Space with Multi-Vector Deduplication
A single similarity search across all embedding types (content + labels + URNs) retrieves relevant chunks whether queries are semantic or referential. All embeddings populate one index E. Query embedding Eq is compared against E via k-NN. Retrieved embeddings may be content or label type; both resolve to underlying chunks. Duplicates (same chunk retrieved via multiple embeddings) are deduplicated, retaining the highest similarity score.

### Mechanism 3: Hierarchical Chunking Amplifies Poly-Vector Gains
Combining multi-layer hierarchical indexing with Poly-Vector label embeddings yields the most consistent performance across query types. Multi-layer indexing creates embeddings at multiple granularities (title, chapter, article, paragraph, clause). Poly-Vector adds label embeddings at each granularity. This enables referential queries at any structural level (e.g., "Chapter VI of Title VIII") while semantic queries match at appropriate granularity.

## Foundational Learning

- **Concept: Dense Embedding Retrieval & Cosine Similarity**
  - **Why needed here**: Poly-Vector builds on standard embedding-based retrieval; understanding how vectors represent text and similarity is measured is prerequisite.
  - **Quick check question**: Given two text chunks embedded as vectors [0.8, 0.6] and [0.6, 0.8], what is their cosine similarity?

- **Concept: Chunking Strategies (Fixed vs. Structural)**
  - **Why needed here**: The paper compares blind fixed-size chunking against hierarchical structural chunking; results show structural awareness matters for legal texts.
  - **Quick check question**: Why might splitting a legal provision mid-sentence alter its legal meaning?

- **Concept: RAG Pipeline Components**
  - **Why needed here**: Poly-Vector modifies the indexing and retrieval stages of RAG; understanding baseline RAG architecture clarifies where changes apply.
  - **Quick check question**: In a standard RAG pipeline, what happens between document retrieval and LLM generation?

## Architecture Onboarding

- **Component map**: Document Parser -> Label Generator -> Embedding Encoder -> Unified Vector Index -> Query Processor -> Retrieval Engine -> Context Assembler

- **Critical path**: Label extraction accuracy → embedding quality → unified index integrity → deduplication correctness. If labels are malformed or URNs missing, referential queries fail silently.

- **Design tradeoffs**:
  - Index size: Poly-Vector + Multi-layer increased index from 284 (blind) to 11,892 embeddings (4× content embeddings)
  - I+L embedding: Paper hypothesized combined identifier+label would outperform, but "did not consistently outperform simpler LBL or URN vectors"—consider omitting to reduce index size
  - Query normalization: Helps verbose queries (Q2) but marginal for concise ones

- **Failure signatures**:
  - Label queries returning wrong article: Check label string construction (prepend full norm name?)
  - URN queries failing: Verify URN format matches query input exactly
  - Semantic queries degrading: Ensure content embeddings aren't drowned by label embeddings in ranking
  - Multi-label queries (Q8) returning only one article: Check deduplication logic preserves distinct chunk sources

- **First 3 experiments**:
  1. **Baseline comparison**: Implement content-only flat indexing on your corpus; measure retrieval accuracy on a held-out set mixing semantic and label-based queries
  2. **Label embedding ablation**: Add label embeddings only (no URNs, no I+L); isolate contribution of label indexing to referential query performance
  3. **Granularity sweep**: Test at article-level only vs. full hierarchical (paragraphs, clauses) to quantify where gains plateau for your document structure

## Open Questions the Paper Calls Out

- **Open Question 1**: Does Poly-Vector Retrieval improve performance in non-legal domains with explicit reference identifiers, such as financial regulations or medical guidelines?
  - **Basis in paper**: Section 8 proposes validating the approach in "other domains characterized by explicit reference identifiers."
  - **Why unresolved**: The study was restricted to the Brazilian Federal Constitution, leaving the transferability of the "sense vs. reference" embedding strategy to other structured corpora unproven.
  - **What evidence would resolve it**: Successful replication of retrieval improvements on corpora from diverse domains (e.g., medical, engineering) with distinct structural schemas.

- **Open Question 2**: How does Poly-Vector Retrieval impact standard Information Retrieval metrics and end-to-end LLM generation quality?
  - **Basis in paper**: Section 8 states the evaluation relied on cosine similarity, which "does not equate to a full end-to-end assessment using classical IR metrics (precision, recall, F1) nor... final LLM output quality."
  - **Why unresolved**: Higher similarity scores do not guarantee better ranking or more accurate generated answers in a live RAG pipeline.
  - **What evidence would resolve it**: Experiments measuring NDCG/MRR for retrieval and faithfulness/accuracy scores for the final LLM outputs.

- **Open Question 3**: Can Poly-Vector be utilized to automatically detect and resolve internal cross-references (e.g., "pursuant to Article 34") within retrieved text?
  - **Basis in paper**: Section 8 suggests "Automated Cross-Reference Resolution" as a future direction to reduce context omissions.
  - **Why unresolved**: While the method shows potential, the current study did not implement a mechanism to proactively expand context by following pointers found in the text.
  - **What evidence would resolve it**: A system architecture that recursively retrieves referenced provisions and demonstrates higher context completeness scores.

## Limitations
- Evaluation focused exclusively on Brazilian Federal Constitution, limiting generalizability to other legal systems or languages
- Computational overhead and index size impact not fully analyzed beyond reporting increased embeddings from 284 to 11,892
- I+L combined embedding variant underperformed simpler LBL/URN variants without clear explanation
- Query normalization contribution remains unclear with limited implementation details

## Confidence

**High Confidence**: The core claim that separating label embeddings from content embeddings improves retrieval of structural/referential queries (e.g., "Article 5") is well-supported by direct experimental comparisons showing 0.7982-0.8349 similarity versus <0.64 for non-Poly methods on label-centric queries.

**Medium Confidence**: The assertion that Poly-Vector maintains semantic retrieval performance while improving referential retrieval is supported, but the paper doesn't analyze whether semantic queries experience any degradation or if gains come at the cost of index efficiency.

**Low Confidence**: The claim about Poly-Vector's effectiveness for cross-reference resolution is speculative, mentioned only in passing without empirical validation or concrete examples of how the method resolves internal versus external references.

## Next Checks

1. **Index Efficiency Analysis**: Measure retrieval latency and memory usage for Poly-Vector vs. content-only approaches on the same corpus to quantify the computational overhead of maintaining 4× embeddings per provision.

2. **Cross-Reference Resolution Test**: Design a benchmark task where queries contain internal cross-references (e.g., "What does Article 5 say about Article 7?") to empirically validate the paper's claim about Poly-Vector's utility for reference resolution.

3. **Generalization Study**: Implement Poly-Vector on a non-legal corpus with hierarchical structure (e.g., technical documentation or academic papers) to test whether the reference-content separation mechanism transfers beyond legal documents.