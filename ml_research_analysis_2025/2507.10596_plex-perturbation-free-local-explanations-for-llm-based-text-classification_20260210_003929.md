---
ver: rpa2
title: 'PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification'
arxiv_id: '2507.10596'
source_url: https://arxiv.org/abs/2507.10596
tags:
- lime
- shap
- plex
- classification
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PLEX, a perturbation-free method for explaining
  LLM-based text classification. PLEX leverages contextual embeddings from LLMs and
  a Siamese network trained to map these embeddings to feature importance scores,
  eliminating the need for computationally expensive perturbations used by traditional
  methods like LIME and SHAP.
---

# PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification

## Quick Facts
- arXiv ID: 2507.10596
- Source URL: https://arxiv.org/abs/2507.10596
- Authors: Yogachandran Rahulamathavan; Misbah Farooq; Varuna De Silva
- Reference count: 40
- Key outcome: PLEX achieves over 92% agreement with LIME and SHAP while reducing computational overhead by 2-4 orders of magnitude

## Executive Summary
This paper introduces PLEX, a perturbation-free method for explaining LLM-based text classification that leverages contextual embeddings and a Siamese network to map embeddings to feature importance scores. By eliminating the need for computationally expensive perturbations used by traditional methods like LIME and SHAP, PLEX achieves similar explanation quality with dramatically reduced computational overhead. The method demonstrates over 92% agreement with baseline explanation methods across four classification tasks while being two to four orders of magnitude faster.

## Method Summary
PLEX uses contextual embeddings extracted from LLMs and employs a Siamese network architecture to map these embeddings to feature importance scores. The Siamese network is trained to learn the relationship between input embeddings and their corresponding importance scores without requiring perturbations of the input text. This approach fundamentally differs from traditional methods like LIME and SHAP, which rely on generating multiple perturbed versions of the input to estimate feature importance. PLEX directly learns to predict importance scores from embeddings, making the explanation generation process significantly more efficient while maintaining comparable explanation quality.

## Key Results
- PLEX achieves over 92% agreement with LIME and SHAP explanations across four classification tasks
- Computational overhead reduced by two to four orders of magnitude compared to perturbation-based methods
- PLEX shows similar accuracy decline when removing important words, indicating faithful explanations
- Successfully applied to sentiment analysis, fake news detection, COVID-19 fake news detection, and depression detection tasks

## Why This Works (Mechanism)
PLEX works by leveraging the rich contextual information captured in LLM embeddings and training a Siamese network to directly map these embeddings to feature importance scores. The Siamese architecture allows the model to learn a similarity function that can effectively identify which features (words or phrases) are most influential for the classification decision. By avoiding perturbations entirely, PLEX eliminates the computational bottleneck while maintaining explanation quality through its learned mapping function.

## Foundational Learning
- Siamese Networks: Used for learning similarity between embeddings and importance scores; needed for feature importance mapping without perturbations; quick check: verify twin network weights are shared
- Contextual Embeddings: LLM-generated representations capturing word meaning in context; needed as input features for importance scoring; quick check: confirm embeddings capture task-relevant information
- Feature Importance Scoring: Mapping from embeddings to importance values; needed to generate explanations; quick check: validate scores correlate with actual feature influence

## Architecture Onboarding

Component Map:
Input Text -> LLM Embeddings -> Siamese Network -> Feature Importance Scores -> Explanation

Critical Path:
Input Text -> LLM Embeddings -> Siamese Network (trained weights) -> Feature Importance Scores

Design Tradeoffs:
- Computational efficiency vs. explanation fidelity (plex favors efficiency)
- Direct learning vs. perturbation-based estimation (plex uses direct learning)
- Generalizability vs. task-specific optimization (plex balances both)

Failure Signatures:
- Low agreement with baseline methods indicates poor Siamese network training
- Inconsistent importance scores across similar inputs suggests embedding quality issues
- Computational overhead similar to baselines indicates architectural problems

First Experiments:
1. Verify embedding extraction produces consistent vectors across model runs
2. Test Siamese network convergence with synthetic importance labels
3. Validate feature importance scores on simple, interpretable classification tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope of evaluated classification tasks may limit generalizability
- Agreement with baseline methods doesn't validate explanation correctness
- Evaluation focuses on agreement metrics rather than human validation or downstream task performance

## Confidence
- Computational efficiency claims: High confidence
- Explanation quality through agreement metrics: Medium confidence
- Faithful explanations through word removal experiments: Medium confidence

## Next Checks
1. Conduct ablation studies removing multiple important phrases or sentences rather than individual words to test explanation robustness at different granularities
2. Perform human evaluation studies where domain experts assess whether PLEX explanations align with their understanding of feature importance in classification tasks
3. Test PLEX on a broader range of classification tasks including multi-label classification and sequence labeling tasks to evaluate generalizability beyond the four tasks presented