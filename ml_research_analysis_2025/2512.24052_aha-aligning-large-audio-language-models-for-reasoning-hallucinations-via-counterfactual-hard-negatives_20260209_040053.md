---
ver: rpa2
title: 'AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via
  Counterfactual Hard Negatives'
arxiv_id: '2512.24052'
source_url: https://arxiv.org/abs/2512.24052
tags:
- audio
- event
- temporal
- these
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles hallucinations in Large Audio-Language Models
  (LALMs), where models generate text not grounded in the actual audio input. These
  hallucinations are particularly problematic in audio because sound events are temporal
  and often overlapping, requiring precise tracking of event order, duration, and
  frequency.
---

# AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives

## Quick Facts
- **arXiv ID**: 2512.24052
- **Source URL**: https://arxiv.org/abs/2512.24052
- **Reference count**: 6
- **Primary result**: Reduces hallucination error rates by up to 17% on AHA-Eval while improving generalization to public benchmarks

## Executive Summary
This paper addresses hallucinations in Large Audio-Language Models (LALMs), where models generate text not grounded in actual audio input. The authors systematically categorize hallucinations into four types: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. They introduce the AHA framework, which uses counterfactual hard negative mining to create a preference dataset that contrasts correct, audio-grounded responses with acoustically incorrect alternatives. Applying this to Qwen2.5-Omni via Direct Preference Optimization yields Qwen-Audio-AHA, which reduces hallucination error rates by up to 17% while showing positive transfer to public benchmarks.

## Method Summary
The AHA framework constructs a preference dataset using counterfactual hard negative mining. It generates rejected responses that are linguistically fluent but violate temporal logic, then contrasts these with grounded responses during Direct Preference Optimization (DPO). The method uses Qwen2.5-Omni-7B with LoRA adapters (r=16, α=32) trained on an AudioTime corpus subset. The preference pairs are validated by human annotators who select the hardest negatives. DPO optimizes the policy against a frozen reference model to increase likelihood of grounded responses while suppressing specific hallucination patterns.

## Key Results
- Qwen-Audio-AHA reduces hallucination error rates by up to 17% on AHA-Eval
- Shows positive transfer to public benchmarks: MMAU-Test (+1.3%) and MMAR (+1.6%)
- Outperforms previous state-of-the-art methods in both hallucination reduction and general multimodal understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual hard negatives teach models to prioritize acoustic evidence over linguistic priors
- Mechanism: The pipeline generates rejected responses that are linguistically fluent but deliberately violate temporal logic, teaching models to penalize outputs relying on language priors
- Core assumption: Models default to linguistic plausibility when acoustic parsing is difficult
- Evidence: "By leveraging counterfactual hard negative mining, our pipeline constructs a high-quality preference dataset that forces models to distinguish strict acoustic evidence from linguistically plausible fabrications"

### Mechanism 2
- Claim: DPO reduces hallucination rates by directly increasing likelihood of grounded responses
- Mechanism: DPO optimizes the policy against a frozen reference model using preference pairs, explicitly increasing log probability of preferred responses
- Core assumption: Preference signal from contrastive pairs generalizes beyond training distribution
- Evidence: "This objective increases the likelihood of grounded responses while explicitly suppressing the specific hallucination in r−"

### Mechanism 3
- Claim: Targeted fine-grained temporal reasoning alignment yields positive transfer to broader multimodal tasks
- Mechanism: Training on hallucination-sensitive questions requiring precise event ordering develops robust temporal representations that transfer to general audio benchmarks
- Core assumption: Temporal grounding is foundational for diverse downstream reasoning tasks
- Evidence: "Qwen-Audio-AHA defies this trend, exhibiting positive transfer across the board" with Complex Temporal/Event Reasoning accuracy improving +8.3% on MMAU-Test

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Core training method replacing RLHF; requires understanding how preference pairs directly optimize policy without reward model. Quick check: Can you explain why DPO uses a frozen reference model and what the β hyperparameter controls?

- **Hard Negative Mining**: The method's effectiveness depends on generating "hard" negatives that are superficially plausible but systematically incorrect. Quick check: What makes a negative "hard" versus "easy," and why does hardness matter for contrastive learning?

- **Audio-Language Model Architecture**: Understanding the encoder-decoder structure clarifies where hallucinations originate and where interventions apply. Quick check: In a typical LALM, which component is responsible for grounding text in acoustic signals—the audio encoder or the language decoder?

## Architecture Onboarding

- **Component map**: AudioTime corpus subset → Question generation → Ground-truth caption → Chosen response → LLM prompting → Rejected response candidates → Human validation → Preference pairs → DPO training → Aligned model

- **Critical path**: 1) Audio-caption pairs → Question generation via templates 2) Ground-truth caption → Chosen response (r+) 3) LLM prompting with hallucination instructions → Rejected response candidates (R−) 4) Human validation → Final preference pairs (r+, r−) 5) DPO training on preference dataset → Aligned model

- **Design tradeoffs**: Caption-level vs. raw audio supervision (AHA uses caption-derived text for model-facing supervision but requires human verification against actual audio); LoRA vs. full fine-tuning (LoRA reduces compute cost but may limit capacity); Automated vs. human validation (human selection ensures quality but doesn't scale)

- **Failure signatures**: "Semantic Equivalence" trap where LLM judges penalize valid synonyms (e.g., "laughing" vs. "chuckle"); Granularity mismatch between AHA's fine-grained temporal reasoning and public benchmarks' coarse perception; Emotion/category drift with ESS performance dropping -0.5%

- **First 3 experiments**: 1) Reproduce baseline hallucination rates on AHA-Eval to verify ~70% omission rate and ~30% temporal relation error rate 2) Ablate hard negative generation by training with random negatives vs. counterfactual hard negatives 3) Cross-model transfer test by applying AHA preference dataset to a different LALM (e.g., SALMONN) to validate model-agnostic claims

## Open Questions the Paper Calls Out

- **How can automated evaluation benchmarks be redesigned to recognize semantic equivalence and valid reasoning depth?** Current LLM-judges unfairly penalize models for using synonyms or higher-level inferences, causing inflated false positive rates. Resolution requires benchmarks incorporating dynamic scoring that accepts verified taxonomies of valid semantic descriptors.

- **Does targeted alignment for temporal reasoning hallucinations inevitably incur performance costs on subjective tasks like emotion recognition?** While general reasoning improves, ESS performance dropped -0.5% because alignment data excluded emotion-specific examples. Ablation studies measuring emotional recognition accuracy variance as temporal hard negative weight increases would resolve this.

- **To what extent does relying on text-only LLMs to synthesize counterfactual negatives limit detection of hallucinations derived from acoustic features invisible in text?** Text-based synthesis may fail to generate "hard negatives" for acoustic hallucinations that have no distinct textual linguistic prior. Comparative analysis of text-synthesized versus audio signal-manipulated negatives would provide evidence.

## Limitations
- Evaluation protocol's reliance on LLM-as-a-judge creates vulnerabilities, including the semantic equivalence trap that penalizes valid synonyms
- Human validation step for hard negative mining lacks specification in coordination and criteria definition
- Generalizability is constrained with AHA-Eval covering only temporal reasoning while ESS performance declined -0.5%

## Confidence
- **High confidence**: Preference alignment mechanism reduces hallucination error rates on AHA-Eval (up to 17% reduction) and shows positive transfer to public benchmarks
- **Medium confidence**: Counterfactual hard negatives teach models to prioritize acoustic evidence over linguistic priors, though evidence is primarily indirect through benchmark performance
- **Low confidence**: Generalizability claim across all audio reasoning categories due to ESS performance decline and narrow focus on temporal reasoning

## Next Checks
1. Conduct ablation studies comparing DPO with counterfactual hard negatives versus DPO with random negative sampling
2. Re-run AHA-Eval evaluation using human annotators instead of GPT-4o to control for semantic equivalence trap artifacts
3. Test the AHA preference dataset on a different LALM architecture (e.g., SALMONN or Audio Flamingo) to validate model-agnostic claims