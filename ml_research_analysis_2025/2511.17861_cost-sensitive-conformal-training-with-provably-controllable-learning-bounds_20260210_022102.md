---
ver: rpa2
title: Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds
arxiv_id: '2511.17861'
source_url: https://arxiv.org/abs/2511.17861
tags:
- prediction
- training
- rwce
- size
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of minimizing prediction set
  sizes in conformal prediction while maintaining valid coverage. Existing conformal
  training methods rely on surrogate functions to approximate the discrete prediction
  set size, but these surrogates introduce approximation errors that can misalign
  the training objective with the true goal.
---

# Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds

## Quick Facts
- arXiv ID: 2511.17861
- Source URL: https://arxiv.org/abs/2511.17861
- Reference count: 40
- Primary result: 21.38% average reduction in prediction set size while maintaining valid coverage

## Executive Summary
This paper addresses the challenge of minimizing prediction set sizes in conformal prediction while maintaining valid coverage. Existing conformal training methods rely on surrogate functions to approximate the discrete prediction set size, but these surrogates introduce approximation errors that can misalign the training objective with the true goal. The authors propose a rank-weighted cross-entropy (RWCE) loss that avoids surrogate approximations by directly incorporating the true-label rank into the training objective. They prove that minimizing this loss provides a tight upper bound on the expected prediction set size and establish generalization bounds. Experiments on multiple benchmark datasets demonstrate that RWCE consistently produces smaller prediction sets with an average 21.38% reduction compared to baselines while preserving valid coverage.

## Method Summary
The method introduces a Rank-Weighted Cross-Entropy (RWCE) loss that directly optimizes the expected rank of the true label as a surrogate for minimizing prediction set size. During training, the model computes softmax probabilities, determines the rank of the true label (highest probability = rank 1), and multiplies the standard cross-entropy loss by this rank value. Critically, the rank is treated as a non-differentiable weight - gradients flow through the CE loss but not through the rank calculation itself. This approach eliminates the approximation gap present in prior methods that use smooth surrogates (like Sigmoid) to approximate discrete set sizes. The theoretical contribution shows that minimizing expected rank provides an upper bound on expected prediction set size, and the method includes generalization bounds for the proposed training procedure.

## Key Results
- 21.38% average reduction in Average Prediction Set Size (APSS) compared to baseline methods
- Maintains ≥90% marginal coverage across all tested datasets (CIFAR-100, Caltech-101, iNaturalist, SST-5)
- Tight tracking between training loss and actual prediction set size, eliminating the soft-hard gap seen in baseline methods
- Strong performance on highly imbalanced datasets like iNaturalist with 341 classes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing the expected rank of the true label serves as a tight upper bound for minimizing the expected prediction set size.
- **Mechanism:** The paper theoretically proves (Theorem 1) that the expected size of a conformal prediction set is bounded by the expected rank of the correct class plus a calibration slack term. By training the model to rank the correct class higher (lower rank value), the number of classes included in the thresholded set is implicitly reduced.
- **Core assumption:** Nonconformity scores are distinct (no ties), ensuring a strict ordering.
- **Evidence anchors:**
  - [abstract] "theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels."
  - [section 4.2] Theorem 1 and Remark 1 confirm the bound $E[|C(X)|] \leq E[R(X, Y)] + \text{slack}$.
  - [corpus] Weak support for this specific theoretical bound; related work generally focuses on efficiency but not this specific rank-based inequality.
- **Break condition:** If ties in nonconformity scores are frequent, the mapping between rank and set size becomes ambiguous, potentially loosening the bound.

### Mechanism 2
- **Claim:** A Rank-Weighted Cross-Entropy (RWCE) loss aligns the differentiable training objective with the discrete rank minimization goal.
- **Mechanism:** Instead of smoothing the indicator function (e.g., Sigmoid), RWCE weights the standard cross-entropy loss by the discrete rank of the true label. The rank acts as a cost signal: mis-ranked examples (hard samples) incur higher penalties. Crucially, gradients flow through the CE loss, not the rank, maintaining differentiability while optimizing the rank-weighted objective.
- **Core assumption:** Assumption: The covariance between rank and loss is non-negative (i.e., rank and loss generally increase together), as stated in Theorem 2.
- **Evidence anchors:**
  - [section 4.1] "R f(X, Y) is treated as a fixed, non-differentiable weight: it serves purely as a cost signal, and we do not backpropagate through the rank itself."
  - [section 5.2] Figure 3 validates the covariance assumption empirically on CIFAR-100.
- **Break condition:** If the model learns to suppress the cross-entropy loss for low-ranking classes without improving the rank of the true label (gradient hacking), the mechanism fails.

### Mechanism 3
- **Claim:** Eliminating surrogate smoothing functions (Sigmoid/Erf) prevents the "approximation gap" that destabilizes learning bounds.
- **Mechanism:** Prior methods (ConfTr) approximate discrete set sizes with smooth functions. These lack uniform error bounds, causing the training objective (soft set size) to deviate significantly from the actual set size (hard set size). RWCE bypasses this by directly optimizing a bound on the hard metric (rank), closing the gap between training dynamics and evaluation metrics.
- **Core assumption:** The discrete rank can be utilized as a weight without causing gradient instability or divergence during stochastic optimization.
- **Evidence anchors:**
  - [section 3] Figure 1 demonstrates the persistent gap between soft and hard set sizes in baselines.
  - [section 5.2] Figure 2(b) shows RWCE loss tracks Actual Prediction Set Size (APSS) tightly, unlike smoothed approaches.
  - [corpus] Consistent with "Non-Asymptotic Analysis of Efficiency in Conformalized Regression" which analyzes efficiency bounds.
- **Break condition:** If the discrete weights cause excessive oscillation in the loss landscape, SGD may fail to converge.

## Foundational Learning

- **Concept:** **Conformal Prediction (CP) Calibration**
  - **Why needed here:** You must understand how a calibration set determines a quantile threshold $\hat{Q}$ to form prediction sets. RWCE relies on this process being the evaluation target it optimizes for.
  - **Quick check question:** If I add a new data point to the calibration set, could the prediction set size change?

- **Concept:** **Nonconformity Scores (APS/HPS)**
  - **Why needed here:** The rank is calculated based on these scores. You need to distinguish between simple scores (HPS: $1-p$) and cumulative scores (APS).
  - **Quick check question:** Does a higher nonconformity score mean the model is more or less confident in that class?

- **Concept:** **Surrogate Loss Functions**
  - **Why needed here:** Understanding why one would use a Sigmoid to approximate a step function (indicator) helps in understanding exactly what "approximation error" RWCE avoids.
  - **Quick check question:** Why is the indicator function $\mathbb{1}[\cdot]$ incompatible with standard backpropagation?

## Architecture Onboarding

- **Component map:** Backbone (ResNet/DenseNet/Transformer) -> Softmax Head -> Rank Module -> RWCE Loss
- **Critical path:**
  1. Forward pass computes logits → Softmax probabilities
  2. Sort probabilities to find the rank of the ground truth label (Stop Gradient here)
  3. Compute standard Cross-Entropy loss
  4. Multiply CE loss by the computed Rank
  5. Backward pass updates model weights

- **Design tradeoffs:**
  - **Surrogate vs. Rank Weighting:** Avoiding surrogates closes the theoretical gap but introduces a cost-sensitive weight that may cause early-training instability (seen as oscillation in Figure 2)
  - **Complexity:** Computing rank requires O(K log K) sorting per batch, slightly more overhead than a simple Sigmoid mask but cheaper than bilevel optimization

- **Failure signatures:**
  - **Coverage Collapse:** Marginal coverage drops below $1-\alpha$. This implies the model learned to rank the true label lower to minimize the weighted loss on other classes
  - **Stagnation:** Loss remains high. This occurs if the rank becomes a "constant" scalar (e.g., always 1) effectively turning the loss into standard CE without efficiency gains
  - **Divergence:** Exploding gradients if ranks are not normalized or clamped in large-class settings

- **First 3 experiments:**
  1. **Baseline Validation:** Reproduce Figure 1 on CIFAR-100 to verify the "Soft vs. Hard" gap exists in standard ConfTr implementations
  2. **Ablation:** Run RWCE while accidentally backpropagating through the rank calculation (treating rank as differentiable). Verify if the bound/efficiency fails to confirm the "fixed weight" design choice
  3. **Generalization:** Train on a high-class count dataset (e.g., iNaturalist with 341 classes) to ensure rank weights do not destabilize training compared to low-class datasets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Rank Weighted Cross-Entropy (RWCE) framework be extended to regression tasks?
- **Basis in paper:** [inferred] The proposed loss function (Eq. 4) relies entirely on the discrete rank of the true label $R_f(X,Y)$, which is undefined for continuous target variables in regression.
- **Why unresolved:** The core mechanism of importance weighting via rank assumes a finite set of classes to sort; continuous spaces require a different measure of "conformity" to generate weights.
- **What evidence would resolve it:** A formulation of RWCE that utilizes quantile statistics or density-based ordering to weight the loss for continuous outputs.

### Open Question 2
- **Question:** Does the theoretical upper bound remain informative for datasets with a very large number of classes (Large $K$)?
- **Basis in paper:** [explicit] Remark 1 notes that the calibration slack term $K(1-\alpha + \frac{1}{n+1})$ in Theorem 1 "may be numerically large" and scales with the number of classes.
- **Why unresolved:** If the slack term dominates the bound, the theoretical guarantee may become too loose to provide meaningful assurances about prediction set size for complex datasets (e.g., ImageNet-1k or language modeling).
- **What evidence would resolve it:** Empirical validation on datasets with $K > 1000$ showing the gap between the bound and empirical set size remains manageable, or a tighter theoretical analysis.

### Open Question 3
- **Question:** How does RWCE impact class-conditional coverage in imbalanced settings?
- **Basis in paper:** [inferred] The method optimizes for marginal efficiency (smaller average set size), and experiments on iNaturalist show strong performance, but the paper does not report per-class coverage metrics.
- **Why unresolved:** Minimizing the average rank might implicitly encourage the model to produce small sets for frequent classes while potentially under-covering rare classes where the rank is inherently less stable.
- **What evidence would resolve it:** Evaluation of class-conditional coverage (e.g., worst-case coverage over classes) on highly imbalanced benchmarks compared to methods like RAPS.

## Limitations

- **Theoretical Tightness:** The slack term in the upper bound scales with the number of classes, potentially making the bound loose for datasets with many classes
- **Early Training Instability:** The rank-weighting mechanism can cause oscillation in early training phases, suggesting sensitivity to hyperparameter tuning
- **No Class-Conditional Coverage Analysis:** The paper does not evaluate how the method performs on rare classes in imbalanced datasets, leaving open questions about worst-case coverage

## Confidence

**High Confidence:** The 21.38% reduction in prediction set size is well-supported by extensive experimental results across multiple datasets with consistent coverage validity.

**Medium Confidence:** The theoretical claims about rank minimization providing tight bounds are mathematically proven but the practical tightness depends on assumptions about nonconformity score distributions.

**Low Confidence:** The assertion that eliminating surrogate functions completely closes the approximation gap lacks full theoretical characterization of the approximation error bounds.

## Next Checks

1. **Robustness to Calibration Set Size:** Systematically vary the calibration set size (from 1% to 20% of the training data) to quantify how sensitive the coverage and efficiency guarantees are to calibration data availability.

2. **Cross-Domain Generalization Test:** Evaluate RWCE on out-of-distribution datasets (e.g., CIFAR-10 models tested on ImageNet-10) to verify whether the rank-based training objective maintains efficiency and validity when the test distribution differs from training.

3. **Gradient Flow Analysis:** Instrument the training process to measure the actual gradient contribution from rank-weighted samples versus standard CE samples to quantify whether the rank weighting is effectively prioritizing hard examples.