---
ver: rpa2
title: Representation Decomposition for Learning Similarity and Contrastness Across
  Modalities for Affective Computing
arxiv_id: '2506.07086'
source_url: https://arxiv.org/abs/2506.07086
tags:
- information
- representation
- arxiv
- shared
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-modal affective computing,
  where the goal is to recognize and interpret human emotions from diverse data sources
  like images and text. The authors propose a novel approach that explicitly decomposes
  visual and textual representations into shared (modality-invariant) and modality-specific
  components.
---

# Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing

## Quick Facts
- arXiv ID: 2506.07086
- Source URL: https://arxiv.org/abs/2506.07086
- Reference count: 25
- This paper proposes a novel approach that explicitly decomposes visual and textual representations into shared and modality-specific components for multi-modal affective computing.

## Executive Summary
This paper addresses the challenge of multi-modal affective computing, where the goal is to recognize and interpret human emotions from diverse data sources like images and text. The authors propose a novel approach that explicitly decomposes visual and textual representations into shared (modality-invariant) and modality-specific components. Their method involves encoding and aligning input modalities using pre-trained multi-modal encoders, then employing a representation decomposition framework to separate common emotional content from unique cues. These decomposed signals are integrated via an attention mechanism to form a dynamic soft prompt for a multi-modal LLM. The approach is evaluated on three representative tasks: multi-modal aspect-based sentiment analysis, multi-modal emotion analysis, and hateful meme detection. The results demonstrate that the proposed method consistently outperforms strong baselines and state-of-the-art models across all tasks, achieving higher accuracy and F1-scores.

## Method Summary
The proposed method follows a pipeline: first, aligned visual and textual representations are obtained from a pre-trained multi-modal encoder (CLIP). These representations are then jointly decomposed into a shared low-rank matrix (capturing common emotional content) and two sparse matrices (capturing modality-specific information) using an iterative low-rank matrix recovery algorithm. The decomposed components are then integrated via an attention mechanism that learns to weight shared versus contrastive information based on the specific input pair. This weighted combination is used as a dynamic soft prompt for a multi-modal LLM, which predicts the final affective label. The decomposition algorithm is fixed and non-differentiable, while the attention weights and LLM parameters are learned via task-specific loss.

## Key Results
- The proposed method consistently outperforms strong baselines and state-of-the-art models across all three tasks: MABSA, MEA, and HMD.
- Empirical results show higher accuracy and F1-scores compared to vanilla LLM approaches and variants without the attention-weighted soft prompt.
- Case studies demonstrate that the attention mechanism appropriately emphasizes shared content when modalities align and sparse components when modalities contrast.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint low-rank matrix recovery separates shared semantic content from modality-specific cues, enabling the model to handle both aligned and conflicting cross-modal evidence.
- Mechanism: Given aligned visual I and textual T matrices from CLIP, the algorithm solves for a shared low-rank matrix L (capturing common emotional content) and two sparse matrices S_I, S_T (capturing unique/contradictory information per modality) via iterative optimization of an augmented Lagrangian. The constraint that both modalities share the same L (unlike standard LMR) forces the decomposition to isolate genuinely shared semantics.
- Core assumption: Emotional semantics that are truly cross-modal exhibit low-rank structure in the joint embedding space, while modality-specific or contradictory information appears as sparse deviations.
- Evidence anchors:
  - [abstract]: "explicitly deconstructs visual and textual representations into shared (modality-invariant) and modality-specific components"
  - [section 2.2, Eq. 6]: "I = L + S_I and T = L + S_T ... The key difference between our approach and the standard LMR is that the standard approach does not require the low-rank matrix to be the same for different modalities, whereas our approach does."
  - [corpus]: Related work on cross-modal contrastive representation distillation (CMCRD) supports the value of explicit contrast modeling, though does not validate the specific LMR-based decomposition.
- Break condition: If the aligned representations from the encoder are not semantically comparable (e.g., poor cross-modal alignment), the shared L will fail to capture meaningful common content, and the sparse matrices will contain redundant rather than complementary information.

### Mechanism 2
- Claim: Attention-weighted fusion of decomposed components enables adaptive emphasis on shared vs. contrastive information based on input-specific image-text relations.
- Mechanism: The flattened L, S_I, S_T vectors are scored through a shared linear projection (W, b), then softmax-normalized to produce α_L, α_I, α_T. The weighted sum R = α_L·L + α_I·S_I + α_T·S_T becomes the soft prompt, allowing the model to upweight shared content when modalities align or upweight sparse components when contrast drives affective meaning.
- Core assumption: The scalar attention weights learned via task supervision will generalize to appropriately weight shared vs. contrastive information across diverse image-text configurations.
- Evidence anchors:
  - [abstract]: "integrates these decomposed signals via an attention mechanism to form a dynamic soft prompt"
  - [section 4.4, Figure 4]: Case study shows α_L dominant when modalities are consistent (positive sentiment from both), while α_I, α_T increase when modalities contrast.
  - [corpus]: Corpus evidence for this specific attention design is weak; no directly comparable mechanism found in neighbors.
- Break condition: If the attention mechanism overfits to training distribution (e.g., always preferring shared or always preferring sparse), it will fail to adapt to novel image-text configurations, particularly rare conflict patterns.

### Mechanism 3
- Claim: Using the fused representation R as a soft prompt provides task-relevant cross-modal context that guides the LLM's prediction without modifying its parameters.
- Mechanism: The aggregated R is concatenated with image I and text T as input to the frozen or fine-tuned LLM: ŷ = f_LLM(I, T, R). During training, only LLM parameters are optimized via cross-entropy loss; the encoder and decomposition modules remain fixed. The soft prompt effectively instructs the LLM about what aspects of cross-modal relation matter for the task.
- Core assumption: The LLM can interpret the soft prompt representation as meaningful instruction about cross-modal relations, even though it was not explicitly trained on such prompts.
- Evidence anchors:
  - [section 2.3, Eq. 11]: "R is employed as a soft prompt to instruct the LLM f_LLM in predicting final label"
  - [Table 2]: "LLM + RD + Att (Ours)" consistently outperforms "LLM + RD" (uniform weighting) and vanilla LLM across all tasks and model sizes, suggesting the attention-weighted soft prompt adds value.
  - [corpus]: Related work on multi-modal LLM prompting for hateful memes (Cao et al. 2023) provides precedent for prompt-based approaches, though not with decomposed representations.
- Break condition: If the soft prompt dimensionality or representation space is incompatible with the LLM's input expectations, or if the frozen encoder produces representations the LLM cannot interpret, the prompt will add noise rather than signal.

## Foundational Learning

- Concept: **Low-Rank Matrix Recovery (Robust PCA)**
  - Why needed here: The core decomposition assumes familiarity with nuclear norm minimization for low-rank structure and ℓ1 norm for sparsity. Understanding why minimizing ∥L∥_* promotes low-rankness and ∥S∥_1 promotes sparsity is essential for debugging convergence issues.
  - Quick check question: Given a matrix X that is the sum of a rank-3 matrix and a sparse noise matrix, would standard RPCA recover both? What if the "noise" was actually structured signal you wanted to preserve?

- Concept: **Cross-Modal Alignment via Contrastive Pre-training**
  - Why needed here: The decomposition quality depends entirely on CLIP producing semantically aligned I and T. If image and text embeddings occupy different subspaces or use incompatible similarity metrics, the shared L will not capture meaningful commonalities.
  - Quick check question: CLIP is trained with contrastive loss on image-text pairs. What failure mode would occur if you used a vision encoder trained on ImageNet classification paired with a text encoder trained on masked language modeling, without alignment training?

- Concept: **Soft Prompting vs. Hard Prompting for LLMs**
  - Why needed here: The approach uses continuous representation R as a soft prompt rather than discrete tokens. Understanding the difference—soft prompts are differentiable and can encode information not easily expressed linguistically—is critical for interpreting why this works and when it might fail.
  - Quick check question: If you wanted to inspect what instruction the soft prompt R is giving the LLM, could you decode it back to natural language? What are the tradeoffs of soft vs. hard prompting for interpretability?

## Architecture Onboarding

- Component map:
  - Input Processing: Image I → patch embeddings → f_vis → I (visual matrix); Text T → tokens → f_text → T (textual matrix). Both via CLIP encoders.
  - Representation Decomposition: Joint LMR module takes I, T; outputs L (shared low-rank), S_I (image-specific sparse), S_T (text-specific sparse). Runs 3000 iterations with μ=10, λ=1.
  - Attention-based Fusion: Flatten L, S_I, S_T; compute scores via shared W, b; softmax → α_L, α_I, α_T; aggregate R = α_L·L + α_I·S_I + α_T·S_T.
  - LLM Prediction: Concatenate I, T, R as input; f_LLM produces ŷ; train with cross-entropy loss.
  - Fixed Components: CLIP encoder, decomposition algorithm (not differentiable).
  - Learned Components: Attention weights W, b; LLM parameters (optional fine-tuning).

- Critical path:
  1. Verify CLIP alignment quality on sample image-text pairs (check cosine similarity distribution).
  2. Run decomposition on held-out samples; visually inspect L, S_I, S_T norms to confirm meaningful separation.
  3. Monitor attention weight distributions during training; check for collapse (α values near 0 or 1 constantly).
  4. Compare predictions with vs. without attention module to isolate its contribution.

- Design tradeoffs:
  - **Fixed vs. end-to-end decomposition**: Current design fixes decomposition (non-differentiable iterative algorithm). Assumption: LMR provides useful inductive bias. Tradeoff: Cannot adapt decomposition to task-specific needs; error propagation if decomposition is suboptimal.
  - **Shared projection W, b for attention**: Uses same W, b for all three components because L, S_I, S_T share the same embedding space. Assumption: This ensures comparability of scores. Tradeoff: May limit expressiveness; separate projections could capture different aspects of each component.
  - **Iteration count (3000)**: Determined empirically. Too few iterations → incomplete separation; too many → diminishing returns (Figure 3 shows plateau after 3000).

- Failure signatures:
  - **α_L always dominant**: Model may be ignoring contrastive information; check if training data has few conflict cases.
  - **α_I or α_T always near zero**: One modality's sparse component is not contributing; check encoder alignment or sparse matrix norms.
  - **Decomposition divergence**: If ∥I - L - S_I∥_F or ∥T - L - S_T∥_F does not converge, check λ/μ balance or input matrix conditioning.
  - **Performance same as vanilla LLM**: Soft prompt may not be providing useful signal; verify R is being passed correctly and attention weights are updating.

- First 3 experiments:
  1. **Ablation on decomposition iterations**: Run with 500, 1000, 2000, 3000, 4000 iterations on validation set; plot performance vs. iterations to confirm 3000 is optimal for your data distribution (paper shows this in Figure 3).
  2. **Conflict vs. alignment stratified analysis**: Manually annotate 100 test samples as "aligned" or "conflicting"; compare attention weight distributions (α_L should be higher for aligned, α_I/α_T higher for conflicting) to validate mechanism 2.
  3. **Encoder swap test**: Replace CLIP with a different multi-modal encoder (e.g., BLIP, FLAVA); if performance drops significantly, decomposition quality depends on encoder alignment quality—investigate whether encoder choice or decomposition parameters need co-optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:**
  Can the iterative decomposition algorithm be replaced with a differentiable approximation to enable end-to-end training and reduce the computational latency of the pre-processing step?
- **Basis in paper:**
  [explicit] Page 6, Section 3.3 states the decomposition runs for 3000 iterations as a pre-processing step, taking an average of 1.3 seconds per pair on a CPU, which operates outside the LLM's gradient descent.
- **Why unresolved:**
  The current method relies on an iterative optimization solver (Augmented Lagrangian) which is computationally expensive and decoupled from the downstream task's loss function, preventing the encoder from being fine-tuned based on the quality of the decomposition.
- **What evidence would resolve it:**
  An experiment comparing a neural-approximation of the low-rank recovery (e.g., using a shallow neural network) against the iterative solver in terms of both inference speed and performance on the MABSA/MEA tasks.

### Open Question 2
- **Question:**
  How can the framework be improved to detect hateful content that relies on implicit analogies or metaphorical connections between the image and text, which current decomposition fails to fully interpret?
- **Basis in paper:**
  [explicit] Page 8, Section 4.4 discusses a failure case (Figure 4c) where the model identifies a conflict but fails to predict "hateful" because it cannot infer the hidden analogy (e.g., comparing a daughter to a vegetable).
- **Why unresolved:**
  The current method identifies "contrast" (differences in representation) but lacks a reasoning module to synthesize these contrasts into complex semantic implications (e.g., metaphors) required for implicit hate speech detection.
- **What evidence would resolve it:**
  Performance improvements on a dataset of implicit hateful memes (e.g., Facebook's Hateful Memes Challenge test set) using an enhanced reasoning module or chain-of-thought prompting on the decomposed components.

### Open Question 3
- **Question:**
  Does the sparse component $S$ in the decomposition capture semantically meaningful "contrast" evidence, or does it primarily encode uncorrelated noise and background details?
- **Basis in paper:**
  [inferred] Section 2.2 defines the sparse matrices as containing "unique information" or "contradictory information," but the optimization objective (Equation 7) minimizes the L1 norm (sparsity) without explicitly enforcing semantic contradiction or relevance to the target label.
- **Why unresolved:**
  The paper attributes performance gains to capturing "contrast," but lacks a quantitative analysis verifying that the sparse matrices specifically isolate the contrastive sentiment features rather than just non-aligned visual/textual residuals.
- **What evidence would resolve it:**
  An ablation study that visualizes or masks the top-k features of the sparse matrices to confirm their correlation with the sentiment/affect labels, compared to a random noise baseline.

### Open Question 4
- **Question:**
  How does the bi-modal decomposition formulation generalize to scenarios involving three or more modalities (e.g., audio, video, and text)?
- **Basis in paper:**
  [inferred] The mathematical formulation in Section 2.2 is strictly defined for two modalities ($I = L + S_I$ and $T = L + S_T$), sharing a single low-rank matrix $L$, but does not address the geometric constraints or computational complexity of adding a third modality to the shared space.
- **Why unresolved:**
  The current algorithm relies on the pairwise relationship between image and text; adding a third modality may require a tensor decomposition approach or a different shared constraint to prevent the optimization from becoming unstable or trivial.
- **What evidence would resolve it:**
  Application of the method to a tri-modal dataset (e.g., CMU-MOSEI) with an extended objective function, reporting the convergence rate and classification accuracy.

## Limitations
- The non-differentiable decomposition creates a hard pipeline boundary, preventing task-specific adaptation of the shared vs. sparse separation.
- The approach's success critically depends on CLIP producing semantically aligned representations, with no analysis of performance degradation with imperfect alignment.
- Evaluation focuses on accuracy/F1 metrics without analyzing failure modes or identifying when the model relies on spurious correlations.

## Confidence
**High Confidence:**
- The proposed decomposition framework is technically sound and the iterative algorithm will converge under standard conditions (low-rank + sparse structure assumption holds).
- The empirical results showing consistent improvement over strong baselines across three distinct tasks are likely reproducible with the same experimental setup.

**Medium Confidence:**
- The mechanism claim that attention-weighted fusion enables adaptive emphasis based on input-specific relations is supported by case studies but lacks rigorous statistical validation across diverse conflict/alignment patterns.
- The claim that the soft prompt provides meaningful cross-modal context to the LLM is plausible given the ablation results, but the exact mechanism of how the LLM interprets the prompt remains unclear.

**Low Confidence:**
- The claim that low-rank matrix recovery reliably separates shared emotional semantics from modality-specific cues assumes emotional content exhibits the required low-rank structure, which is not empirically validated beyond convergence metrics.

## Next Checks
1. **Ablation on decomposition iterations**: Run with 500, 1000, 2000, 3000, 4000 iterations on validation set; plot performance vs. iterations to confirm 3000 is optimal for your data distribution.
2. **Conflict vs. alignment stratified analysis**: Manually annotate 100 test samples as "aligned" or "conflicting"; compare attention weight distributions (α_L should be higher for aligned, α_I/α_T higher for conflicting) to validate adaptive fusion mechanism.
3. **Encoder swap test**: Replace CLIP with a different multi-modal encoder (e.g., BLIP, FLAVA); if performance drops significantly, decomposition quality depends on encoder alignment quality—investigate whether encoder choice or decomposition parameters need co-optimization.