---
ver: rpa2
title: 'Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing'
arxiv_id: '2505.15195'
source_url: https://arxiv.org/abs/2505.15195
tags:
- retraining
- state
- evolution
- given
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of optimally combining model predictions
  with noisy labels in iterative retraining. The authors develop a principled framework
  based on Approximate Message Passing (AMP) to analyze iterative retraining procedures
  for binary classification under Gaussian mixture and generalized linear models.
---

# Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing

## Quick Facts
- **arXiv ID**: 2505.15195
- **Source URL**: https://arxiv.org/abs/2505.15195
- **Reference count**: 40
- **Primary result**: Derives Bayes optimal aggregator for combining model predictions with noisy labels in iterative retraining, outperforming baseline methods in high noise regimes (p=0.45) by up to 18.47% accuracy improvement over 10 iterations.

## Executive Summary
This paper addresses the problem of optimally combining model predictions with noisy labels in iterative retraining for binary classification. The authors develop a principled framework based on Approximate Message Passing (AMP) to analyze iterative retraining procedures under Gaussian mixture and generalized linear models. Their main contribution is deriving the Bayes optimal aggregator function that minimizes prediction error when combining current model predictions with given labels. The theoretical analysis shows that retraining can either improve or hurt performance depending on the initial model quality, with optimal aggregators outperforming full and consensus-based retraining approaches. Experiments on MedMNIST Pneumonia and Food-101 datasets demonstrate significant improvements in high label noise regimes.

## Method Summary
The method involves fitting a bimodal Gaussian Mixture Model (GMM) to the distribution of training logits from an initial linear probe trained on noisy labels. Using this fitted distribution, the Bayes optimal aggregator computes soft labels by combining the model's predictions with the noisy labels through a posterior probability calculation. These soft labels replace the original noisy labels in iterative retraining rounds. The optimal aggregator function is derived as: `g(z,ŷ) = 2/(1 + (p/(1-p))^ŷ * exp((z-μ₊)²/(2σ₊²) - (z-μ₋)²/(2σ₋²)) * (π₋/π₊)) - 1`, where z is the unnormalized logit and parameters come from the GMM fit. The process repeats for 10 iterations, with each round retraining a linear layer on ResNet-50 (ImageNet pretrained) using BCE loss with soft labels as targets.

## Key Results
- BayesMix RT achieves up to 18.47% improvement in test accuracy over 10 iterations compared to baseline retraining methods in high noise regimes (p=0.45)
- Optimal aggregator outperforms both full retraining (using only predicted labels) and consensus-based retraining (using samples where predictions match noisy labels)
- State evolution analysis shows retraining can hurt performance when initial model quality exceeds a threshold (η_1 > η*)
- BayesMix RT initially lags behind consensus-based RT but overtakes around iteration 6-7 in experiments

## Why This Works (Mechanism)

### Mechanism 1: Bayes-Optimal Label Aggregation
Combining model predictions with noisy labels using the posterior probability P(Y|prediction, noisy_label) minimizes test error by maximizing the signal-to-noise ratio η_t = m_t/σ_t in state evolution. The aggregator adaptively weighs model vs. noisy labels based on current model quality.

### Mechanism 2: AMP with Onsager Memory Correction
Memory correction terms (-C_tθ_t and -g(y_t,ŷ)d/n) compensate for correlation between iterates and the data matrix X, ensuring Gaussian asymptotics and valid state evolution. This makes iterates behave "as if" X were resampled each iteration.

### Mechanism 3: State Evolution Governs Convergence
Test error is a deterministic function of the single-parameter state evolution sequence η_t, enabling exact prediction of retraining dynamics. The mapping F is non-decreasing with fixed points - starting below the smallest fixed point η* guarantees monotonic improvement.

## Foundational Learning

- **Approximate Message Passing (AMP)**: The theoretical framework uses AMP iterations to analyze retraining. Without understanding Onsager corrections and state evolution, the analysis is opaque.
  - Quick check: Why does AMP add correction terms proportional to ∂g/∂y?

- **Gaussian Mixture Models (GMM) and Generalized Linear Models (GLM)**: The theory is derived for two specific ground truth models; practical adaptation assumes GMM-like structure in logits.
  - Quick check: In GMM, how does the flip probability p enter the observed label distribution?

- **Bayes Optimal Estimation**: The core contribution is deriving the Bayes-optimal aggregator - the posterior mean of the true label given observations.
  - Quick check: Why does the optimal aggregator depend on the current iteration's η_t?

## Architecture Onboarding

- **Component map**: Collect logits from initial model -> Fit bimodal GMM to logit distribution -> Compute soft labels via optimal aggregator -> Retrain linear layer with soft labels -> Repeat for 10 iterations

- **Critical path**: 1) Train initial linear probe, collect logits on training set 2) Fit GMM to logit distribution 3) For each retraining round: compute g(z,ŷ) for all samples → use as soft labels → retrain 4) Monitor validation accuracy; stop when gains plateau

- **Design tradeoffs**: Number of iterations (10 shown, more may not help), noise estimation (requires knowing or estimating p), class prior estimation (needs π₊/π₋; empirical class frequencies suffice)

- **Failure signatures**: Test accuracy decreases over iterations (η_1 likely above fixed point), no improvement from retraining (p may be too low), high variance across runs (learning rate instability)

- **First 3 experiments**: 1) Synthetic GMM validation: Generate data with known p, γ, α; verify state evolution predictions match Opt-AMP performance 2) Noise level sweep: On MedMNIST/Food-101, vary p ∈ {0.2, 0.3, 0.35, 0.4, 0.45}; confirm BayesMix RT wins at high p 3) Ablation on iterations: Track per-iteration accuracy; verify crossover where BayesMix RT surpasses consensus

## Open Questions the Paper Calls Out

- **Multi-class extension**: How can the Bayes optimal aggregator framework be extended to multi-class classification settings beyond binary classification? The conclusion notes this is future work, as multi-class settings involve more complex data structures and aggregator functions that may not have closed-form solutions.

- **Non-uniform noise**: How does the optimal retraining strategy change under non-uniform (instance-dependent or class-conditional) label noise models? The current framework assumes uniform flipping probability p for all samples, which may not reflect real-world noise patterns.

- **Deep learning generalization**: Can the optimal aggregator framework be generalized to non-linear deep learning models while preserving theoretical guarantees? The AMP-based analysis relies on Gaussian properties and linear structure that do not directly transfer to deep networks.

## Limitations
- The theoretical framework is limited to binary classification and linear models, with extensions to multi-class and deep learning noted as future work
- Performance degrades when the initial model quality exceeds a threshold, potentially causing harm rather than improvement
- The method requires knowledge or accurate estimation of the label noise probability p, which may not be available in practice

## Confidence
- **AMP framework and state evolution**: High - well-established theory with clear derivation
- **Bayes optimal aggregator derivation**: High - follows directly from posterior probability calculation
- **Experimental results**: Medium - limited to two datasets with specific noise levels
- **Generalizability to other settings**: Low - explicit statements about limitations to binary classification and linear models

## Next Checks
1. Verify GMM fitting produces consistent component ordering (μ₊ > μ₋) to prevent inverted signals
2. Check that BayesMix RT performance improves over iterations on both MedMNIST and Food-101 datasets as shown in Tables 3-4
3. Confirm that consensus-based RT outperforms BayesMix RT in early iterations but is overtaken by iteration 6-10 as predicted by theory