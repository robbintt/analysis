---
ver: rpa2
title: 'It takes a village to write a book: Mapping anonymous contributions in Stephen
  Langton''s Quaestiones Theologiae'
arxiv_id: '2508.12830'
source_url: https://arxiv.org/abs/2508.12830
tags:
- langton
- quaestiones
- collection
- reportationes
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes applying stylometric techniques to Stephen
  Langton's Quaestiones Theologiae to identify layers of editorial work from anonymous
  reportationes. The research tests whether function words, POS n-grams, and pseudo-affixes
  can cluster quaestiones by editorial origin, validating earlier findings from collaborative
  medieval Latin writing.
---

# It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae

## Quick Facts
- arXiv ID: 2508.12830
- Source URL: https://arxiv.org/abs/2508.12830
- Reference count: 0
- Primary result: Study proposes using stylometric techniques to identify editorial layers in Stephen Langton's Quaestiones Theologiae through function words, POS n-grams, and pseudo-affixes

## Executive Summary
This registered report protocol outlines a study to apply stylometric techniques to Stephen Langton's *Quaestiones Theologiae* to identify layers of editorial work from anonymous reportationes. The research tests whether function words, POS n-grams, and pseudo-affixes can cluster quaestiones by editorial origin, validating earlier findings from collaborative medieval Latin writing. The study will compare manually edited and HTR-extracted data to establish reliable minimal sample lengths for clustering, aiming to identify distinct editorial contributions and potentially trace individual reportatores.

## Method Summary
The study proposes applying stylometric techniques including function word frequencies (200 MFW), POS 3-grams, and pseudo-affixes to Langton's corpus to identify editorial layers. The methodology involves using Kraken and TrOCR for automated transcription, PASSIM for alignment, and stylo, LatinPipe/UDPipe, and custom scripts for feature extraction. The analysis will employ PCA and unsupervised clustering, with particular attention to establishing minimum sample length thresholds using the Moisl (2011) test. The research will validate findings against stemmatic groupings and compare HTR-extracted data with critical edition transcriptions.

## Key Results
- Exploratory analysis shows that 200 most frequent words can distinguish Langton's corpus from contemporaries
- Preliminary PCA reveals stylistic clusters among subcollections, with Ca and H/K classes appearing as outliers
- Proposed methodology establishes framework for identifying anonymous editorial contributions through stylometric clustering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Function word frequencies can distinguish authorial signals even in highly formulaic scholastic Latin texts.
- Mechanism: The 200 most frequent words capture unconscious stylistic patterns that persist across technical theological writing. PCA on these distributions clusters texts by authorial origin.
- Core assumption: Function word usage reflects cognitive habits rather than topic/content; this holds even in formulaic genres.
- Evidence anchors:
  - [abstract]: "Exploratory analysis shows that 200 most frequent words can distinguish Langton's corpus from contemporaries"
  - [section: Exploratory stylometric analysis]: Figure 2 shows clear separation between Aquinas, Courson, and Langton samples using PCA on 200 MFW
  - [corpus]: Weak direct corpus support—neighbor papers on stylometry absent; relies on cited works (Kestemont et al. 2013, De Gussem 2017)
- Break condition: If function word distributions converge across authors in ultra-formulaic sub-genres, clustering fails.

### Mechanism 2
- Claim: POS 3-grams and pseudo-affixes extend reliable stylometric analysis to shorter texts (<2000 words).
- Mechanism: POS n-grams capture syntactic structure ignored by bag-of-words approaches; pseudo-affixes encode morphological patterns. These provide additional stylistic dimensions, enabling richer sample representations without increasing sample length.
- Core assumption: Syntactic and morphological habits are as diagnostic as lexical habits, and remain detectable in noisy data.
- Evidence anchors:
  - [section: Extended features]: "POS 3-grams are especially promising as a simple representation of syntactic structures mostly ignored in the bag-of-words approach"
  - [section: Extended features]: Cites Sapkota et al. (2015) for pseudo-affix effectiveness; Chen et al. (2024) for POS n-grams
  - [corpus]: No direct corpus validation; mechanism remains proposed, not yet tested on Langton data
- Break condition: If POS taggers error rates increase substantially on scholastic Latin variants, syntactic features become unreliable.

### Mechanism 3
- Claim: Transformer-based HTR (TrOCR) produces transcriptions where stylometric features (POS, affixes) have higher task-specific accuracy than raw Character Error Rate suggests.
- Mechanism: TrOCR's BERT-type decoder prefers regular forms, partially normalizing orthography and enabling correct POS assignment even with transcription errors. Critical features survive noise that would invalidate word-level comparisons.
- Core assumption: The model's regularization bias aligns with correct morphological analysis; abbreviation handling doesn't introduce systematic distortion.
- Evidence anchors:
  - [section: Data preparation]: "Even where the transcription is inaccurate, the produced form can be sufficiently close to ground truth to enable correct assignment of POS tags and prefixes"
  - [abstract]: "test the validity of transformer-based OCR and automated transcription alignment for workflows applied to scholastic Latin corpora"
  - [corpus]: No corpus evidence yet—this is a proposed validation, not a completed finding
- Break condition: If systematic abbreviation expansion errors create consistent bias across documents, spurious stylistic clusters may emerge.

## Foundational Learning

- Concept: **Reportationes as transmission vehicles**
  - Why needed here: The entire study assumes quaestiones originated as oral teaching records, later edited by unknown hands. Without understanding reportatio practices, you can't interpret what clusters might represent.
  - Quick check question: Can you explain why multiple versions of a single quaestio might exist, and what editorial layers would look like?

- Concept: **Minimum sample length for stylometric reliability**
  - Why needed here: The paper's central constraint is that most quaestiones (~1400 words avg) fall below typical thresholds (2000-5000 words). The Moisl (2011) test determines which texts are analyzable.
  - Quick check question: Why can't you simply concatenate texts when seeking to identify individual contributions?

- Concept: **Unsupervised vs. supervised stylometry**
  - Why needed here: This study uses unsupervised clustering because no labeled training data exists for individual reportatores. Word-embedding methods requiring labeled samples are inapplicable.
  - Quick check question: Why does the absence of securely attributed samples rule out classifier-based approaches?

## Architecture Onboarding

- Component map:
  - Manuscript images -> Kraken blla segmentation -> TrOCR transcription OR pre-existing critical edition transcriptions
  - PASSIM script for automated transcription-to-ground-truth alignment
  - (1) 200 MFW frequencies via stylo package, (2) POS 3-grams via LatinPipe/UDPipe 2, (3) Pseudo-affixes via character n-gram extraction
  - Moisl (2011) test per feature type to establish minimum reliable sample length
  - PCA for visualization; unsupervised clustering for group identification
  - Cluster assignments mapped to stemmatic classes; potential editorial/reportatorial attribution

- Critical path:
  1. Establish minimum sample length thresholds for each feature type
  2. Filter corpus to quaestiones meeting all thresholds
  3. Extract features on qualifying texts
  4. Run unsupervised clustering
  5. Map clusters against stemmatic groupings (Ca, Cb, H/K, etc.) and version relationships (short vs. long quaestio versions)

- Design tradeoffs:
  - **Feature richness vs. corpus coverage**: More features may require longer samples, excluding more texts
  - **HTR automation vs. editorial noise**: Automated extraction enables unedited material but introduces transcription noise
  - **Cluster granularity vs. reliability**: More clusters may capture real contributors but risk overfitting to noise

- Failure signatures:
  - All quaestiones cluster together regardless of known stemmatic divisions -> features insufficiently diagnostic
  - Clusters map perfectly to manuscript source but not to content -> scribe signal dominates reportator signal
  - Short and long versions of same quaestio never cluster together -> systematic stylistic gap between reportationes and literary forms (not failure, but meaningful negative result)

- First 3 experiments:
  1. Replicate Figure 2 PCA on the masked corpus (200 MFW) to validate exploratory findings against peer-review data
  2. Run Moisl threshold test on POS 3-grams using existing transcriptions to determine minimum viable quaestio length
  3. Train provisional Kraken model on 20 manually aligned pages; compare feature extraction accuracy against critical edition ground truth

## Open Questions the Paper Calls Out

- **Open Question 1**: Can distinct stylistic clusters be discerned among longer quaestiones that correspond to the activity of different anonymous editors or Langton himself?
- **Open Question 2**: Do short reportationes and their corresponding long literary versions tend to cluster together, or do they show systematic stylistic divergence?
- **Open Question 3**: Can individual reportatores (note-takers) be identified through stylometric clusters within the anonymous short quaestiones?
- **Open Question 4**: Does transformer-based HTR (TrOCR) extract stylometric features (function words, POS n-grams, pseudo-affixes) with reliability comparable to manually transcribed text?

## Limitations
- Corpus's short average quaestio length (1400 words) falls below typical stylometric thresholds, requiring untested Moisl threshold methodology
- Absence of securely attributed reportatores for supervised validation of clustering results
- Masking of non-MFW tokens in public data prevents full reproducibility of proposed analyses
- Proposed HTR workflow for scholastic Latin remains unverified against critical edition ground truth

## Confidence

- Function word clustering mechanism: Medium confidence - exploratory results promising but lack corpus validation
- POS n-grams and pseudo-affixes extension: Low confidence - theoretical support but no direct evidence on Langton's corpus
- HTR feature extraction reliability: Low confidence - plausible assumptions but unverified on this dataset

## Next Checks

1. Run Moisl threshold tests on the critical edition transcriptions to determine minimum viable sample lengths for each feature type (MFW, POS 3-grams, pseudo-affixes) before applying methods to HTR data.

2. Conduct controlled experiments comparing feature extraction accuracy from TrOCR output versus critical edition ground truth on a manually transcribed sample of 20 pages, measuring POS tagging accuracy and morphological feature preservation.

3. Perform cluster analysis on the full corpus using only the validated minimum sample lengths, mapping results against known stemmatic divisions to assess whether unsupervised groupings align with manuscript relationships rather than random noise.