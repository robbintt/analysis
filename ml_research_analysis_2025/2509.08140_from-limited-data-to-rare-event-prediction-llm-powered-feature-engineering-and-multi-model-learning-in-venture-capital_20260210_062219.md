---
ver: rpa2
title: 'From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering
  and Multi-model Learning in Venture Capital'
arxiv_id: '2509.08140'
source_url: https://arxiv.org/abs/2509.08140
tags:
- feature
- precision
- success
- features
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting rare, high-impact
  outcomes (startup success) in venture capital using limited and noisy early-stage
  data. The authors propose a framework that combines large language model (LLM)-powered
  feature engineering with a multi-model machine learning architecture.
---

# From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital

## Quick Facts
- **arXiv ID**: 2509.08140
- **Source URL**: https://arxiv.org/abs/2509.08140
- **Reference count**: 7
- **Primary result**: 10.3× baseline precision with 36% recall in rare-event startup success prediction

## Executive Summary
This paper addresses the challenge of predicting rare, high-impact outcomes (startup success) in venture capital using limited and noisy early-stage data. The authors propose a framework that combines large language model (LLM)-powered feature engineering with a multi-model machine learning architecture. LLMs are used to extract and synthesize complex signals from unstructured founder and startup data, creating a rich set of 63 features organized into categorical, textual, continuous, and boolean groups. These features are then processed by an ensemble of models (XGBoost, Random Forest, and Linear Regression) to produce a continuous funding prediction, which is thresholded to generate binary success outcomes.

The model achieves precision between 9.8× and 11.1× the random classifier baseline across three test subsets, with overall precision 10.3× higher than baseline. Recall is maintained above 30%. Feature sensitivity analysis reveals that startup category list accounts for 15.6% of predictive influence, followed by number of founders. Ablation studies show that removing LLM-engineered features reduces precision from 10.4× to 4.6×, highlighting their critical importance.

## Method Summary
The method uses LLM-powered feature engineering to create 63 structured features from unstructured founder and startup data, including semantic variables like "Skill Relevance" and "Domain Expertise" that capture founder-idea fit relationships. These features are processed by an ensemble of XGBoost, Random Forest, and Linear Regression models that first predict continuous funding amounts (MAPE <4%), then apply a high threshold (0.8) to convert to binary success outcomes. The approach addresses class imbalance by prioritizing precision over recall, achieving 10.3× baseline precision at 36% recall across three test subsets.

## Key Results
- Precision between 9.8× and 11.1× baseline random classifier across three test subsets
- Overall precision 10.3× higher than baseline with recall maintained above 36%
- Startup category list accounts for 15.6% of predictive influence, followed by number of founders
- Removing LLM-engineered features reduces precision from 10.4× to 4.6×

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-powered feature engineering extracts predictive signal from unstructured data that traditional ML cannot encode.
- Mechanism: LLMs process founder profiles and startup descriptions to generate 63 structured features including semantic variables like "Skill Relevance" (0-4 scale) and "Domain Expertise" (4-level alignment). These capture founder-idea fit relationships requiring natural language reasoning.
- Core assumption: Semantic patterns in founder backgrounds and startup descriptions correlate with funding outcomes; LLMs can reliably encode subjective assessments.
- Evidence anchors: [abstract] "LLMs are used to extract and synthesize complex signals from unstructured data"; [section 3.2] "Skill Relevance variable, encoded on a scale from 0 to 4... features of this kind are difficult to generate without LLMs"; [section 4.4.1] Ablation shows removing LLM features drops precision from 10.4× to 4.6×
- Break condition: If LLM-derived features contain systematic misclassification (e.g., skill relevance scoring inconsistencies), precision gains may not generalize to new founder profiles.

### Mechanism 2
- Claim: Layered ensemble with continuous intermediate prediction improves rare-event classification precision.
- Mechanism: Rather than direct binary classification, the architecture predicts continuous funding amounts first (MAPE <4%), then applies logistic regression with a high threshold (0.8) to convert to binary success. This two-stage approach allows the model to learn funding patterns before making sparse positive predictions.
- Core assumption: Funding amount is a reliable proxy for startup success; threshold tuning at 0.8 generalizes across data subsets.
- Evidence anchors: [abstract] "ensemble first produces a continuous estimate of success likelihood, which is then thresholded"; [section 3.3] "funding prediction then serves two purposes... improves precision when converted into a binary outcome"; [section 4.2] Precision peaks between 0.75-0.85 threshold; 0.8 selected as representative
- Break condition: If error propagation from funding prediction (even at 4% MAPE) compounds in the logistic regression step, binary classification may inherit systematic biases.

### Mechanism 3
- Claim: Categorical features (particularly startup category list and team composition) drive the majority of predictive influence.
- Mechanism: Feature sensitivity analysis using model parameter weights combined with embedding contributions reveals startup category list accounts for 15.6% of predictive weight, followed by number of founders. Education level and domain expertise contribute smaller but consistent effects.
- Core assumption: Feature importance rankings from ensemble weights reflect true causal relationships rather than model-specific artifacts.
- Evidence anchors: [abstract] "startup's category list accounts for 15.6% of predictive influence, followed by number of founders"; [section 4.3] Figure 3 shows relative importance hierarchy; categorical data ablation causes largest decline (-3.9X precision); [section 4.4.4] "categorical data produced the largest decline in model performance"
- Break condition: If categorical features encode historical biases (e.g., certain sectors funded more regardless of merit), the model may amplify rather than predict success patterns.

## Foundational Learning

- Concept: **Class imbalance in rare-event prediction**
  - Why needed here: Only 8.5% of founders are labeled successful; conventional accuracy metrics are misleading. Understanding precision vs. recall tradeoffs is essential for interpreting 10.3× baseline precision with 36% recall.
  - Quick check question: If a random classifier achieves 8.5% precision on this dataset, what does 10.3× improvement mean in absolute terms?

- Concept: **LLM embeddings for structured prediction**
  - Why needed here: Startup descriptions are converted to text embeddings and integrated with tabular features. Understanding how embeddings represent semantic similarity helps explain why removing textual features reduces precision by 1.7×.
  - Quick check question: Why would `text-embedding-ada-002` outperform `all-MiniLM-L6-v2` for startup descriptions?

- Concept: **Ensemble meta-learning (stacking)**
  - Why needed here: XGBoost and Random Forest outputs feed into a Linear Regression meta-model. Understanding stacking helps explain why removing XGBoost causes the largest performance drop (-3.2×).
  - Quick check question: What is the difference between bagging (Random Forest), boosting (XGBoost), and stacking (this architecture's meta-model)?

## Architecture Onboarding

- Component map: Raw Data (10,825 founders) → LLM Feature Engineering → 63 features (categorical, textual, continuous, boolean) → Preprocessing (categorical to integer, textual to embeddings, continuous to Z-score, boolean pass-through) → Layer 1 (XGBoost + Random Forest + Text embeddings) → Meta-Model (Linear Regression) → continuous funding prediction → Logistic Regression (threshold=0.8) → binary success classification → Feature Sensitivity Analysis (parameter weights + embedding contributions)

- Critical path: LLM feature engineering → categorical encoding → XGBoost/Random Forest ensemble → funding prediction → thresholded binary classification. Ablation shows XGBoost removal causes -3.2× precision drop; LLM feature removal causes -5.8× drop.

- Design tradeoffs:
  - Precision vs. recall: High threshold (0.8) maximizes precision (10.3×) at cost of recall (36%); lowering threshold increases coverage but reduces confidence
  - Interpretability vs. performance: Neural network meta-model achieved +0.4× precision but introduced overfitting and unstable feature sensitivity; Linear Regression chosen for reliability
  - LLM dependency: All 63 features are LLM-derived; paper acknowledges misclassification risk for subjective variables

- Failure signatures:
  - Precision drops below 5× baseline: Check LLM feature quality, particularly categorical encodings
  - Unstable feature importance rankings across subsets: Random Forest or Linear Regression may be degraded
  - Funding MAPE exceeds 5%: Error propagation will compound in binary classification
  - High variance across test subsets: Possible overfitting to training distribution

- First 3 experiments:
  1. **Ablate embedding source**: Compare `text-embedding-ada-002`, `all-MiniLM-L6-v2`, and no embeddings on held-out set. Expected: ada-002 achieves ~10.4×, MiniLM ~10.2×, none ~8.7× (per Table 8).
  2. **Threshold sensitivity analysis**: Test precision/recall at thresholds 0.5, 0.6, 0.7, 0.8, 0.9 on same test split. Expected: precision peaks 0.75-0.85 with recall declining as threshold increases.
  3. **Feature category removal**: Retrain removing categorical, continuous, boolean, textual features in turn. Expected: categorical removal causes largest decline (-3.9×), confirming LLM-derived categorical features drive performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating SHAP-based explanations with LLM outputs systematically quantify feature contributions while exposing potential inconsistencies in LLM-derived features?
- Basis in paper: [explicit] Authors state in Future Directions: "integrating SHAP-based explanations with LLM outputs could provide a systematic way to quantify feature contributions while exposing potential inconsistencies."
- Why unresolved: The current feature sensitivity analysis uses parameter weights, but LLM-generated features may contain inconsistencies or hallucinations that remain undetected without dedicated explanation methods.
- What evidence would resolve it: A comparative study showing SHAP-LLM integration improves detection of misclassified features (e.g., skill relevance with subjective boundaries) versus current parameter weight analysis.

### Open Question 2
- Question: How can hallucinations in LLM-driven feature generation be detected, measured, and reduced to improve reliability?
- Basis in paper: [explicit] Authors identify this as future work: "developing methods to detect, measure, and reduce hallucinations in LLM-driven feature generation will be important to improve reliability."
- Why unresolved: All 63 features are LLM-derived and subject to misclassification, particularly for variables with subjective boundaries such as skill relevance (0-4 scale).
- What evidence would resolve it: Development and validation of a hallucination detection metric that correlates with downstream prediction accuracy drops when applied to held-out LLM-generated features.

### Open Question 3
- Question: Does the layered architecture design (funding prediction → logistic regression) introduce systematic error propagation that degrades final success classification?
- Basis in paper: [explicit] Authors state in Limitations: "The layered design of the continuous funding predictor followed by logistic regression introduces error propagation: inaccuracies in the funding estimate, though small (below 4% MAPE), may still influence the calibrated probabilities used for success classification."
- Why unresolved: While MAPE is low (3.02-3.89%), the relationship between funding prediction errors and success classification errors remains unquantified.
- What evidence would resolve it: Sensitivity analysis showing how simulated funding prediction errors of varying magnitudes systematically affect final precision and recall metrics.

### Open Question 4
- Question: Does reliance on publicly available founder profiles bias the model toward individuals with greater online presence, limiting generalizability across different ecosystems?
- Basis in paper: [explicit] Authors state in Limitations: "Founder-centric profiles constructed from publicly available sources with information provided by founders themselves may embed biases toward individuals with greater online presence, limiting the generalizability of results across different ecosystems."
- Why unresolved: The dataset construction methodology may systematically underrepresent founders from regions or backgrounds with lower digital footprint density.
- What evidence would resolve it: Cross-ecosystem validation showing model performance stratified by founder online presence metrics (e.g., LinkedIn completeness, Crunchbase profile depth) or geographic region.

## Limitations
- LLM-generated features may contain hallucinations and subjective misclassification errors
- Categorical features may encode historical funding biases rather than true predictive power
- Layered architecture introduces potential error propagation from funding prediction to binary classification
- Model may be biased toward founders with greater online presence, limiting ecosystem generalizability

## Confidence
- **High confidence**: 10.3× baseline precision improvement due to systematic ablation study showing 5.8× drop when LLM features removed
- **Medium confidence**: Multi-model ensemble effectiveness due to documented precision-recall tradeoff but limited threshold sensitivity analysis
- **Medium confidence**: Categorical feature dominance (15.6% predictive influence) but may reflect historical funding biases

## Next Checks
1. **Threshold sensitivity analysis**: Test precision-recall tradeoff across thresholds 0.5-0.9 to confirm optimal point and generalization
2. **Feature category ablation**: Systematically remove each feature group (categorical, continuous, boolean, textual) to quantify individual contributions
3. **Cross-dataset validation**: Apply the trained model to a different VC dataset or timeframe to test generalizability of categorical feature dominance