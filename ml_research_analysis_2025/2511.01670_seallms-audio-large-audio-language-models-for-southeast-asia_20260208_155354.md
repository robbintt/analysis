---
ver: rpa2
title: 'SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia'
arxiv_id: '2511.01670'
source_url: https://arxiv.org/abs/2511.01670
tags:
- audio
- text
- speech
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SeaLLMs-Audio, the first large audio-language
  model (LALM) tailored for multiple Southeast Asian languages, including Indonesian,
  Thai, Vietnamese, English, and Chinese. The model is trained on a large-scale audio
  corpus spanning diverse tasks such as automatic speech recognition, audio captioning,
  speech-to-text translation, and voice-based dialogue.
---

# SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia

## Quick Facts
- arXiv ID: 2511.01670
- Source URL: https://arxiv.org/abs/2511.01670
- Authors: Chaoqun Liu; Mahani Aljunied; Guizhen Chen; Hou Pong Chan; Weiwen Xu; Yu Rong; Wenxuan Zhang
- Reference count: 12
- Primary result: First large audio-language model for multiple Southeast Asian languages, outperforming comparable LALMs on SEA languages

## Executive Summary
SeaLLMs-Audio introduces the first large audio-language model specifically designed for multiple Southeast Asian languages including Indonesian, Thai, Vietnamese, English, and Chinese. The model achieves strong performance across 14 tasks spanning automatic speech recognition, audio captioning, speech-to-text translation, and voice-based dialogue. A comprehensive benchmark (SeaBench-Audio) with 580 questions across SEA languages enables systematic evaluation. The LLM-as-a-judge framework shows high correlation with human judgments (0.8 Pearson), validating the evaluation approach for these underrepresented languages.

## Method Summary
SeaLLMs-Audio replaces the LLM module in Qwen2-Audio-7B with Qwen2.5-7B-Instruct, requiring reinitialization of the audio adapter due to embedding mismatch. The model undergoes full-parameter fine-tuning on a large-scale audio dataset (1.58M conversations across 10 task types and 6 languages) for 1 epoch over 6 days on 32 A800 GPUs. Synthetic data augmentation via LLMs and TTS expands task coverage for low-resource SEA languages. The model is evaluated on SeaBench-Audio using Gemini-2.5-flash as judge, achieving high correlation with human judgments.

## Key Results
- SeaLLMs-Audio achieves strong performance across 14 tasks in SEA languages, outperforming comparable LALMs
- LLM-as-a-judge shows high correlation with human judgments (average Pearson correlation of 0.8)
- Model handles multiple SEA languages effectively while maintaining strong performance on English and Chinese
- Synthetic data generation enables task coverage expansion for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
Reinitializing the audio adapter enables cross-component integration when embedding dimensions differ between pretrained modules. SeaLLMs-Audio replaces the LLM module in Qwen2-Audio-7B with Qwen2.5-7B-Instruct, requiring adapter reinitialization due to embedding mismatch. Full-parameter fine-tuning then aligns the adapter to bridge the audio encoder's representations with the new LLM's embedding space.

### Mechanism 2
Synthetic data augmentation via LLMs and TTS expands task coverage for low-resource SEA languages. The curation pipeline generates training data where none exists: unpunctuated ASR transcripts receive punctuation via LLM post-processing, S2TT data is created by translating ASR transcripts, and QA pairs are synthesized by prompting Gemini-2.0-Flash to generate questions from audio content.

### Mechanism 3
Task-specific rubrics combined with LLM-as-a-judge enable scalable evaluation that correlates highly with human judgment. SeaBench-Audio uses Gemini-2.5-flash to score responses on a 1-5 scale using task-specific evaluation criteria, with each task having dedicated rubrics reflecting its characteristics.

## Foundational Learning

- **Audio-Language Model (LALM) Architecture**
  - Why needed here: SeaLLMs-Audio follows the encoder-adapter-LLM paradigm where audio signals must be encoded into representations the LLM can process
  - Quick check question: Can you explain how audio features flow from the encoder through the adapter to the LLM's token space?

- **Embedding Alignment in Multimodal Models**
  - Why needed here: The paper explicitly addresses embedding mismatch when swapping LLM backbones, requiring adapter reinitialization
  - Quick check question: Why can't you directly connect a pretrained audio encoder to a different LLM without retraining the adapter?

- **LLM-as-a-Judge Evaluation**
  - Why needed here: The paper relies on automated evaluation with Gemini, validated against human annotations
  - Quick check question: What conditions must hold for an LLM judge to produce reliable scores (hint: consider rubrics, language coverage, and correlation validation)?

## Architecture Onboarding

- Component map: Audio input → Audio Encoder → Adapter → LLM → Text output
- Critical path: Audio input flows through the audio encoder, adapter, and LLM to produce text output
- Design tradeoffs: Swapping LLM backbones trades proven audio-text alignment for stronger multilingual capabilities; full-parameter fine-tuning improves performance but increases computational cost
- Failure signatures: Language mixing in outputs, performance drops on fine-grained paralinguistic tasks, low correlation between LLM judge and human evaluators
- First 3 experiments:
  1. Validate adapter initialization: Train with frozen encoder/LLM (adapter-only) vs. full fine-tuning
  2. Ablate synthetic data quality: Train on filtered vs. unfiltered synthetic S2TT data
  3. Cross-task transfer analysis: Evaluate performance on held-out tasks to assess generalization

## Open Questions the Paper Calls Out

- Can reinforcement learning effectively mitigate the issue of language mixing (code-switching) observed in SeaLLMs-Audio outputs?
- How does the reliance on synthetic data generation affect the model's robustness to natural, noisy, or colloquial speech compared to human-curated datasets?
- Does the use of Gemini-2.5-Flash as an "LLM-as-a-judge" introduce systematic bias favoring models trained on data generated by Gemini models?
- Does the strategy of "newly initializing" the audio adapter result in loss of pre-trained audio knowledge compared to using original adapter weights?

## Limitations

- Model still exhibits language mixing/code-switching despite strong performance, indicating incomplete language separation
- Heavy reliance on synthetic data generation may introduce domain gaps between synthetic and natural audio
- Training data includes private sources, raising questions about reproducibility and true performance on public data alone

## Confidence

- **High Confidence**: Architectural claims regarding adapter reinitialization due to embedding mismatch are well-supported by methodology
- **Medium Confidence**: Performance claims on SEA languages are supported by benchmark results, but LLM-as-a-judge reliability for these specific languages has limited independent validation
- **Low Confidence**: Claims about handling fine-grained paralinguistic tasks and nuanced cultural contexts are weakly supported

## Next Checks

1. Conduct independent human evaluations on a subset of SeaBench-Audio tasks across all target SEA languages to verify the 0.8 Pearson correlation and 93% agreement rates
2. Quantitatively measure language mixing rates in model outputs across different task types using automated language identification tools
3. Train ablated versions of SeaLLMs-Audio using only public data versus full dataset to isolate contribution of private data to performance improvements