---
ver: rpa2
title: 'OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized
  Conversational Agents'
arxiv_id: '2601.13722'
source_url: https://arxiv.org/abs/2601.13722
tags:
- user
- uni00000013
- memory
- uni00000011
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces OP-Bench, the first benchmark to systematically\
  \ evaluate over-personalization in memory-augmented conversational agents. The benchmark\
  \ defines three categories of over-personalization\u2014irrelevance, sycophancy,\
  \ and repetition\u2014and includes 1,700 human-verified instances derived from long-horizon\
  \ dialogue histories."
---

# OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents

## Quick Facts
- arXiv ID: 2601.13722
- Source URL: https://arxiv.org/abs/2601.13722
- Reference count: 40
- Memory-augmented methods suffer 26.2%-61.1% relative performance drops on OP-Bench compared to memory-free baselines

## Executive Summary
This paper introduces OP-Bench, the first systematic benchmark for evaluating over-personalization in memory-augmented conversational agents. The benchmark defines three categories of over-personalization—irrelevance, sycophancy, and repetition—and includes 1,700 human-verified instances derived from long-horizon dialogue histories. Evaluations across 36 configurations show that memory-augmented methods suffer significant performance degradation on OP-Bench compared to memory-free baselines, with more sophisticated memory systems exhibiting greater drops. The paper proposes Self-ReCheck, a lightweight filtering module that reduces over-personalization by 29% while preserving personalization performance.

## Method Summary
OP-Bench evaluates over-personalization using three LLM-as-judge metrics: Irrelevance and Sycophancy (scored 0-1, higher = better) and Repetition (embedding-based diversity score). The benchmark uses LoCoMo dataset with 20 users and 1,700 test instances across six subcategories. Self-ReCheck filters retrieved memories via LLM prompt that outputs either filtered context or "NO RELEVANT CONTEXT" when memories are deemed irrelevant to the query. The evaluation compares memory-augmented agents against memory-free baselines across multiple model configurations.

## Key Results
- Memory-augmented methods show 26.2%-61.1% relative performance drops on OP-Bench compared to memory-free baselines
- Self-ReCheck reduces over-personalization by 29% while maintaining personalization performance (+3% improvement)
- Models exhibit attention asymmetry, allocating 2× more attention to retrieved memories than user queries on average

## Why This Works (Mechanism)

### Mechanism 1: Attention Asymmetry (Memory Dominance)
When retrieved memory is present, models allocate disproportionately more attention to memory tokens than to the user query. Retrieved memories are treated as privileged signals rather than conditional cues, with memory-to-query attention ratios exceeding 2× across tasks. This "memory hijacking" shifts focus away from the user's immediate intent.

### Mechanism 2: Aggressive Retrieval Without Abstention
Memory systems retrieve entries even when queries are semantically unrelated to stored memories, rather than returning nothing. Retrieval optimizes for recall over precision and returns "potentially related" memories even when similarity is low, leading to low-value or misleading context that models often over-integrate.

### Mechanism 3: Memory-Induced Response Collapse
Memory conditioning reduces response diversity across semantically distinct queries, leading to repetitive, stereotyped outputs. Retrieved memories anchor the generation space, constraining embeddings to tighter clusters and reducing exposure diversity.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) vs. Persistent Agent Memory
  - Why needed here: OP-Bench evaluates memory-augmented agents that maintain structured, long-term user profiles—not just single-session retrieval
  - Quick check question: Does the system store and update user-specific memories across sessions, or only retrieve from a static corpus?

- Concept: Attention as a Proxy for Influence
  - Why needed here: The paper's causal analysis relies on token attention ratios to explain why memory dominates responses
  - Quick check question: If a model attends 2× more to memory tokens than query tokens, what behavior would you expect in the output?

- Concept: Precision vs. Recall Trade-offs in Retrieval
  - Why needed here: Over-personalization stems partly from high-recall, low-precision retrieval that prioritizes not missing relevant memories over avoiding irrelevant ones
  - Quick check question: A retrieval system always returns top-5 memories. What failure mode does this introduce when all 5 have low similarity to the query?

## Architecture Onboarding

- Component map: Query → Retrieval Module → Self-ReCheck Filter → Generation Model → Response
- Critical path: 1. Query received → 2. Retrieval fetches top-k memories → 3. Self-ReCheck filters relevance → 4. Filtered context + query → 5. Generation → 6. Response scored by OP-Bench metrics
- Design tradeoffs: Higher retrieval k improves personalization recall but increases over-personalization risk; stricter Self-ReCheck reduces OP-Bench failures but may drop useful memories
- Failure signatures: Irrelevance (response discusses user's hobbies when query is unrelated), Sycophancy (response affirms false facts), Repetition (distinct queries yield identical responses), Attention ratio >2×
- First 3 experiments: 1) Baseline measurement on OP-Bench and LoCoMo to establish over-personalization severity, 2) Ablate Self-ReCheck to quantify trade-off between personalization and over-personalization, 3) Threshold sweep varying retrieval k to identify Pareto frontier

## Open Questions the Paper Calls Out
- How does over-personalization manifest and compound across multi-turn dialogue interactions?
- Does Self-ReCheck's effectiveness generalize across model scales, architectures, and computational budgets?
- Can training-time interventions complement or outperform inference-time filtering for mitigating over-personalization?
- How do real users perceive over-personalized responses, and does the taxonomy align with human preferences?

## Limitations
- Relies on LLM-based scoring which introduces potential subjectivity and consistency concerns
- Self-ReCheck adds inference latency and computational overhead not fully characterized
- Attention analysis assumes attention weights directly translate to generation influence without empirical validation

## Confidence
- High confidence: Memory-augmented methods show 26.2%-61.1% relative performance drops on OP-Bench (directly measurable)
- Medium confidence: Attention asymmetry mechanism explanation (causal link requires additional validation)
- Medium confidence: Self-ReCheck effectiveness claims (+29% improvement depends on LLM judge consistency)

## Next Checks
1. Conduct ablation experiments where attention weights to memory tokens are artificially reduced to test direct mitigation of over-personalization
2. Implement and evaluate an abstention mechanism in the retrieval system to compare against post-hoc Self-ReCheck filtering
3. Perform inter-annotator reliability testing on OP-Bench scoring using human evaluators to establish consistency of LLM-based judgment framework