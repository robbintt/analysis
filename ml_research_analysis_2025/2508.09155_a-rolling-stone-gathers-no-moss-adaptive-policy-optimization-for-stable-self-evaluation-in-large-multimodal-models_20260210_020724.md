---
ver: rpa2
title: 'A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation
  in Large Multimodal Models'
arxiv_id: '2508.09155'
source_url: https://arxiv.org/abs/2508.09155
tags:
- reward
- training
- arxiv
- reasoning
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaPO is a reinforcement learning framework designed to enhance
  self-evaluation in large multimodal models. It addresses the problem of reward hacking
  and training instability when optimizing multiple objectives by dynamically adjusting
  training focus in real time.
---

# A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models

## Quick Facts
- arXiv ID: 2508.09155
- Source URL: https://arxiv.org/abs/2508.09155
- Reference count: 40
- Primary result: AdaPO improves self-evaluation in LMMs with up to 25.5% relative improvement across 8 benchmarks.

## Executive Summary
AdaPO introduces an adaptive reinforcement learning framework for training large multimodal models (LMMs) to perform stable self-evaluation. The method addresses reward hacking and training instability that occur when optimizing multiple objectives (direct response and self-correction) by dynamically adjusting training focus in real time. Through an Adaptive Reward Model that assesses task proficiency and a reward-aware dynamic KL regularization, AdaPO enables automated, single-stage training that significantly improves both direct reasoning and self-evaluation capabilities without manual intervention.

## Method Summary
AdaPO builds on GRPO to train LMMs for two-turn self-evaluation: generating an initial response, then self-correcting it. The framework samples multiple trajectories per query, uses a rule-based verifier to classify response correctness, and computes proficiency metrics to dynamically adjust rewards. The Adaptive Reward Model scales rewards for error correction or consolidation based on current proficiency, while the reward-aware dynamic KL regularization selectively penalizes the generation step most at risk of drifting. The entire process operates within a single training stage with automated filtering of zero-advantage batches and uniform-response queries.

## Key Results
- Achieves up to 25.5% relative improvement in self-evaluation accuracy (acc@t2) across 8 benchmarks
- Improves both direct reasoning (acc@t1) and self-evaluation (acc@t2) capabilities simultaneously
- Outperforms all baselines on 93.75% of tasks while maintaining training stability
- Eliminates need for manual multi-stage training through automated curriculum learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic reward modulation prevents reward hacking by decoupling the incentive to correct errors from the incentive to preserve correct answers.
- **Mechanism:** The Adaptive Reward Model (ARM) calculates proficiency P_{0→*} from initial response correctness and scales rewards for correction vs. consolidation trajectories based on this metric.
- **Core assumption:** Sample-based error rate accurately reflects true proficiency on specific query types.
- **Evidence anchors:** [abstract] ARM assesses training state; [method] Eq. 4 defines dynamic reward switch; [corpus] SR-GRPO highlights reward hacking vulnerability.
- **Break condition:** Small sample sizes N lead to noisy error rate estimates and erratic reward scaling.

### Mechanism 2
- **Claim:** Reward-aware KL constraints stabilize policy by selectively penalizing generation steps most at risk of drifting.
- **Mechanism:** Separate KL coefficients β₁ (initial response) and β₂ (corrected response) scale based on reward gaps between trajectory types.
- **Core assumption:** Reward gap magnitude reliably proxies instability/drift risk in policy updates.
- **Evidence anchors:** [method] Eq. 7-8 define β₁/β₂ as functions of reward gap; [motivation] Figure 2 illustrates collapse from fixed rewards; [corpus] GRPO variance reduction discussions support tighter controls.
- **Break condition:** Misspecified rescaling factor λ causes KL penalty to become negligible relative to advantage.

### Mechanism 3
- **Claim:** Instance-level curriculum learning replaces manual multi-stage training with automated, query-specific focus adjustment.
- **Mechanism:** AdaPO acts as curriculum learner, optimizing for error correction on hard queries and consolidation on mastered queries within single batch.
- **Core assumption:** Optimization landscape allows simultaneous gradient updates from conflicting objectives without destructive interference.
- **Evidence anchors:** [method] "adaptive curriculum learning... without manual intervention"; [method] Eq. 11 shows objective dependency on current policy state; [corpus] MPO uses evolving models but not per-query adaptation.
- **Break condition:** Highly skewed data distribution causes gradient updates from dominant trajectory type to overwhelm others.

## Foundational Learning

- **Concept: Reward Hacking in Multi-Objective RL**
  - **Why needed here:** Paper explicitly targets "reward hacking" where models exploit static rewards by intentionally failing to maximize correction bonuses.
  - **Quick check question:** If a model receives +1 for correct answer and +2 for correcting wrong answer, why might accuracy drop during training?

- **Concept: KL Divergence as Trust Region Constraint**
  - **Why needed here:** AdaPO introduces dynamic KL mechanism; learners need to understand standard KL penalties to appreciate innovation of adjusting β based on reward gaps.
  - **Quick check question:** What happens to policy update if KL coefficient β is set to 0, and why does AdaPO propose making it variable?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** AdaPO builds upon GRPO; learners need to know GRPO estimates advantage by normalizing rewards within sample groups rather than using critic model.
  - **Quick check question:** How does AdaPO reuse group samples generated for GRPO's advantage estimation to calculate its own proficiency metric P_{0→*}?

## Architecture Onboarding

- **Component map:**
  Policy Model → Verifier → ARM Module → Dynamic KL Calculator → Loss Engine

- **Critical path:**
  1. Sample N trajectories per query
  2. Verifier classifies correctness of y₁ and y₂
  3. Compute proficiency P_{0→*} from y₁ stats
  4. **ARM** adjusts rewards based on P_{0→*}
  5. **Dynamic KL** sets penalties β₁, β₂ based on reward gaps
  6. Optimize loss

- **Design tradeoffs:**
  - **Stability vs. Speed:** Filtering "Zero-Advantage" batches stabilizes training but discards data, potentially slowing convergence
  - **Automation vs. Control:** Automated threshold θ removes manual staging but requires careful validation for diverse benchmarks

- **Failure signatures:**
  - **Collapse:** Sudden drop in acc@t1 suggests β₁ too weak or correction rewards too high
  - **Stagnation:** No improvement in acc@t2 suggests model too conservative (β₂ too high) or θ preventing error correction learning
  - **Gradient Vanishing:** High incidence of "Zero-Advantage" filtering indicates reward signal too sparse or model over-fitting to single response type

- **First 3 experiments:**
  1. **Baseline Reproduction:** Train Qwen2-VL with fixed rewards to reproduce "Collapse" curve
  2. **Ablation on Threshold θ:** Vary State Change Threshold (0.4, 0.6, 0.8) to observe impact on error correction vs. consolidation balance
  3. **Component Isolation:** Run AdaPO without Dynamic KL component to quantify stability contribution

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Framework relies on rule-based ground-truth verifiers, limiting application to tasks with deterministic answers
- Fixed State Change Threshold θ requires grid search and may not generalize across heterogeneous datasets
- Computational overhead of multiple rollouts may become prohibitive for frontier-sized (>50B parameter) models
- Limited ablation studies on hyperparameter sensitivity, particularly for filtering mechanisms and threshold selection

## Confidence

**High Confidence**: Core claim that AdaPO improves both direct reasoning and self-evaluation capabilities is well-supported by consistent improvements across 8 benchmarks with up to 25.5% relative improvement.

**Medium Confidence**: Claim that AdaPO achieves stability without manual intervention is supported by results, but hyperparameter sensitivity analysis is limited and the assertion about instance-level curriculum learning is conceptually valid but not rigorously tested.

**Low Confidence**: The claim that AdaPO "outperforms all baselines on 93.75% of tasks" aggregates across very different task types without showing per-task variance or statistical significance testing.

## Next Checks

1. **Sample Size Sensitivity Analysis**: Systematically vary the number of samples N used to estimate proficiency P_{0→*} and measure variance in reward scaling and final performance to determine minimum reliable sample size.

2. **Threshold Robustness Test**: Evaluate AdaPO across a broader range of θ values on held-out validation sets to quantify sensitivity of error correction vs. consolidation balance to this hyperparameter.

3. **Open-Ended Task Generalization**: Adapt correctness function to handle partial credit or fuzzy matching for open-ended reasoning tasks and test whether AdaPO maintains stability and performance improvements on datasets where exact matching is insufficient.