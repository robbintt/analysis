---
ver: rpa2
title: Improving Context Fidelity via Native Retrieval-Augmented Reasoning
arxiv_id: '2509.13683'
source_url: https://arxiv.org/abs/2509.13683
tags:
- reasoning
- retrieval
- context
- arxiv
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses context fidelity issues in large language
  models by proposing CARE, a native retrieval-augmented reasoning framework. CARE
  teaches models to dynamically identify and integrate relevant evidence from input
  context within their reasoning process using in-context retrieval capabilities,
  rather than relying on external retrieval modules.
---

# Improving Context Fidelity via Native Retrieval-Augmented Reasoning

## Quick Facts
- arXiv ID: 2509.13683
- Source URL: https://arxiv.org/abs/2509.13683
- Reference count: 30
- Primary result: CARE improves context fidelity in LLMs by teaching native retrieval-augmented reasoning, achieving up to 15.29% average F1 improvement over vanilla models

## Executive Summary
This paper addresses the critical challenge of context fidelity in large language models, where models often generate answers based on parametric knowledge rather than provided context. The authors propose CARE (Context-Augmented Retrieval-augmented rEasoning), a native retrieval-augmented reasoning framework that teaches models to dynamically identify and integrate relevant evidence from input context within their reasoning process. By using special tokens to demarcate evidence spans during reasoning, CARE grounds model outputs in actual context rather than relying on external retrieval modules.

The framework employs a two-phase training approach: supervised fine-tuning with evidence-annotated data followed by reinforcement learning with curriculum-based training. Experiments demonstrate CARE consistently outperforms baselines across multiple QA benchmarks, achieving up to 15.29% average F1 improvement over vanilla models. The method effectively improves context fidelity by grounding reasoning in contextual evidence while requiring limited labeled data, making it practical for real-world deployment.

## Method Summary
CARE employs a two-phase training approach to teach LLMs native retrieval-augmented reasoning. In the first phase, supervised fine-tuning (SFT) trains the model to use special tokens (`<retrieval>` and `</retrieval>`) to demarcate evidence spans within reasoning chains. This phase uses a synthetically generated dataset of 7,739 instances derived from HotpotQA, where a reasoning model creates chains-of-thought and a fact injection model identifies supporting evidence. The second phase applies reinforcement learning with GRPO (Group Relative Policy Optimization) to refine retrieval behavior using curriculum learning, transitioning from simpler DROP dataset to more complex MS MARCO. The composite reward function combines accuracy (70%), format adherence (10%), and retrieval quality (20%) to optimize both reasoning quality and evidence integration.

## Key Results
- CARE achieves up to 15.29% average F1 improvement over vanilla models across QA benchmarks
- Outperforms state-of-the-art retrieval-augmented models like E5-Mistral-7B-Instruct (13.54% average F1 gain on HotpotQA)
- Demonstrates superior performance on counterfactual QA scenarios, improving accuracy by 6.33% over vanilla models
- Effectively grounds reasoning in contextual evidence while requiring only limited labeled data (7,739 SFT instances)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit evidence tagging in reasoning chains grounds model outputs in provided context, reducing reliance on parametric knowledge.
- Mechanism: Special tokens (`<retrieval></retrieval>`) demarcate evidence spans within `<think/>` reasoning, forcing the model to attend to and reproduce context segments before drawing conclusions.
- Core assumption: Models can learn to associate retrieval-tagged spans with higher reward when they match context verbatim and support correct answers.
- Evidence anchors:
  - [abstract]: "teaches LLMs to explicitly integrate in-context evidence within their reasoning process"
  - [section 3.3]: "The retrieval reward allows the model to make better use of the context in reasoning without ground-truth retrieval data"
  - [corpus]: Related work on in-context retrieval (Arxiv 2501.08248) suggests LCLMs can perform retrieval directly; however, CARE-specific mechanism of token-mediated grounding is not directly validated in corpus.
- Break condition: If context contains no relevant evidence or is adversarially misleading, tagged retrieval may amplify noise without improving fidelity.

### Mechanism 2
- Claim: Two-phase training (SFT→RL) efficiently bootstraps evidence-aware reasoning with limited labeled data.
- Mechanism: SFT establishes output format and evidence-integration patterns using ~7.7K examples; RL with GRPO refines retrieval behavior via reward shaping, requiring only QA pairs without golden evidence.
- Core assumption: SFT provides sufficient initialization for RL to explore retrieval strategies without catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation"
  - [section 3.4]: "This phase further develops the model's ability to identify and integrate supporting facts... using only question-answer pairs without golden supporting facts"
  - [corpus]: RL-based retrieval approaches (Arxiv 2501.08282, 2509.23071) show RL can optimize search/retrieval; CARE's specific SFT→RL curriculum is not directly compared in corpus.
- Break condition: If SFT data distribution is too narrow, RL may fail to generalize to out-of-distribution reasoning patterns.

### Mechanism 3
- Claim: Curriculum-based RL improves generalization from simple to complex QA without additional labeling.
- Mechanism: Training transitions from short-context/short-answer tasks (DROP) to long-context/multihop tasks (MS MARCO), adjusting sampling ratio α linearly; retrieval and accuracy rewards jointly optimize evidence selection.
- Core assumption: Progressive difficulty increase prevents overfitting to simple patterns and enables transfer to complex reasoning.
- Evidence anchors:
  - [section 3.4]: "This structured progression mitigates catastrophic forgetting while enhancing retrieval capabilities in increasing complexity"
  - [section 5.3]: Ablation shows curriculum improves balance across diverse QA types, particularly long-form and counterfactual scenarios
  - [corpus]: Curriculum learning is a known strategy, but specific application to retrieval-augmented reasoning lacks direct corpus validation.
- Break condition: If curriculum pace is too fast, model may fail to consolidate retrieval skills; if too slow, compute is wasted.

## Foundational Learning

- **Chain-of-Thought Reasoning with External Anchors**
  - Why needed here: CARE interleaves retrieval tags within reasoning; understanding how CoT structures thought is prerequisite.
  - Quick check question: Can you trace how intermediate reasoning steps in a CoT connect to final answers?

- **Reinforcement Learning from Verifiable Rewards**
  - Why needed here: GRPO uses accuracy, format, and retrieval rewards; understanding reward shaping is essential for debugging training.
  - Quick check question: Can you explain how a verifiable reward (e.g., F1 score) differs from a learned reward model?

- **Curriculum Learning**
  - Why needed here: CARE transitions from easy to hard datasets; misconfiguring curriculum pacing can stall training.
  - Quick check question: Can you identify when a model has overfit to a curriculum stage and needs harder examples?

## Architecture Onboarding

- **Component map**: (Query Q, Context C) → Model → Reasoning with `<retrieval>` tags → Answer A
- **Critical path**:
  1. Generate SFT data using reasoning model M_R and fact injection model M_I (Section 3.3)
  2. Run LoRA-based SFT for 3 epochs (Appendix B.1)
  3. Configure GRPO with retrieval-aware rewards; set curriculum mixing ratio α (Section 3.4)
  4. Validate on held-out QA before deploying
- **Design tradeoffs**:
  - **SFT data size vs. quality**: Only 7,739 instances used; filtering for correct answers and evidence inclusion is critical
  - **Reward weighting**: λ₁=0.7 (accuracy), λ₂=0.1 (format), λ₃=0.2 (retrieval); tuning may be needed per domain
  - **Token overhead**: CARE generates longer outputs (500–900 tokens vs. <30 for baselines), but eliminates external retrieval API calls
- **Failure signatures**:
  - **Empty or hallucinated retrieval tags**: Model fails to ground in context → check retrieval reward signal
  - **Correct answer but wrong reasoning**: Model overfits to answer prediction → increase format/retrieval reward weights
  - **Degradation on counterfactual QA**: Model ignores context in favor of parametric knowledge → strengthen R_ret or extend curriculum
- **First 3 experiments**:
  1. **Reproduce SFT data generation**: Use HotpotQA + DeepSeek models to create ~1K examples; validate evidence-integration quality manually
  2. **Ablate retrieval reward**: Train with R_acc + R_fmt only; compare F1 on HotpotQA vs. full CARE (expect ~5–10% drop per Table 3)
  3. **Test on counterfactual benchmark**: Run CARE on CofCA subset; if F1 < baseline, inspect retrieval tags for parametric leakage

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CARE perform on abstract reasoning, numerical computation, and creative generation tasks beyond multi-hop QA?
  - Basis in paper: [explicit] "The effectiveness of CARE for more abstract reasoning, numerical computation, creative generation tasks, or domain-specific tasks remains to be thoroughly investigated."
  - Why unresolved: Evaluation focused primarily on multi-hop general-domain reasoning (HotpotQA, 2WikiMQA, MuSiQue, MFQA) and counterfactual QA, leaving other task categories untested.
  - What evidence would resolve it: Benchmarking CARE on datasets requiring numerical reasoning (e.g., GSM8K, AQuA), creative writing tasks, and domain-specific corpora (legal, medical, scientific).

- **Open Question 2**: How does CARE handle contexts containing ambiguous or contradictory information?
  - Basis in paper: [explicit] "Although our method improves context fidelity, it does not completely eliminate the possibility of hallucinations, especially when the input contains ambiguous or contradictory information."
  - Why unresolved: Experiments used coherent contexts; no evaluation tested scenarios with conflicting evidence spans or deliberately noisy/contradictory passages.
  - What evidence would resolve it: Constructing adversarial test sets with contradictory facts and measuring hallucination rates and answer consistency.

- **Open Question 3**: Can CARE be effectively combined with external retrieval systems for scenarios requiring information beyond the provided context?
  - Basis in paper: [explicit] "For scenarios requiring information not present in the input, our approach would need to be combined with external retrieval systems like RAG, potentially complicating the overall architecture."
  - Why unresolved: CARE was designed as a standalone native retrieval method; no experiments tested hybrid architectures combining native and external retrieval.
  - What evidence would resolve it: Implementing a combined system that falls back to external RAG when native retrieval confidence is low, and comparing performance against both standalone approaches.

## Limitations

- **Generalization across domains**: CARE's performance improvements are primarily validated on QA datasets. The framework's effectiveness for other tasks requiring context fidelity (e.g., summarization, multi-hop reasoning in scientific domains) remains untested.

- **Sensitivity to SFT data quality**: The approach depends heavily on the quality of synthetically generated SFT data. If the reasoning model produces incorrect chains or the fact injection model misidentifies evidence, CARE may learn to ground reasoning in hallucinated rather than actual context.

- **Long-context handling**: While CARE operates natively without external retrieval, the approach's effectiveness for extremely long contexts (>32K tokens) is untested. The paper does not address potential performance degradation when evidence is deeply embedded in lengthy documents.

## Confidence

- **High confidence**: CARE's two-phase training approach (SFT→RL) is sound and well-documented. The performance improvements on standard QA benchmarks are statistically significant and reproducible based on the methodology described.
- **Medium confidence**: The mechanism by which CARE improves context fidelity is theoretically plausible but not exhaustively validated. The paper demonstrates correlation between retrieval tag usage and performance gains, but does not conclusively prove causation or rule out alternative explanations.
- **Low confidence**: Claims about CARE's ability to handle counterfactual QA scenarios are based on a single dataset (CofCA) with limited ablation studies. The framework's robustness to adversarial contexts or noisy inputs is not systematically evaluated.

## Next Checks

1. **Cross-domain transfer evaluation**: Apply CARE-trained models to non-QA tasks requiring context fidelity (e.g., document summarization with factual accuracy constraints) and measure performance degradation compared to domain-specific training.

2. **Ablation on SFT data quality**: Systematically vary the accuracy of the SFT data generation pipeline (e.g., using different reasoning models or fact injection models) and measure the correlation between data quality and final CARE performance.

3. **Reward function sensitivity analysis**: Conduct a grid search over reward weights (λ₁, λ₂, λ₃) and plot performance contours to identify regions where CARE degrades, helping establish robust weight ranges for different application domains.