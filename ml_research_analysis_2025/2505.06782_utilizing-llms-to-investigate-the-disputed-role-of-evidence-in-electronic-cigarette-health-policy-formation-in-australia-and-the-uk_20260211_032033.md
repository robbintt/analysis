---
ver: rpa2
title: Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette
  Health Policy Formation in Australia and the UK
arxiv_id: '2505.06782'
source_url: https://arxiv.org/abs/2505.06782
tags:
- evidence
- policy
- e-cigarettes
- health
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper used a GPT-4-based classifier to analyze 109 policy\
  \ documents on e-cigarettes from Australia and the UK, identifying sentences that\
  \ made evidence-based claims about the products\u2019 health impacts. After annotating\
  \ 200 sentences for whether they portrayed e-cigarettes as helpful, harmful, or\
  \ neither, the classifier achieved an F-score of 0.9 and was applied to a larger\
  \ corpus of 2,152 sentences."
---

# Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK

## Quick Facts
- **arXiv ID:** 2505.06782
- **Source URL:** https://arxiv.org/abs/2505.06782
- **Authors:** Damian Curran; Brian Chapman; Mike Conway
- **Reference count:** 7
- **Primary result:** GPT-4-based classifier achieved F-score of 0.9 for labeling e-cigarette policy sentences as helpful, harmful, or neither

## Executive Summary
This study demonstrates the feasibility of using large language models (LLMs) to analyze electronic cigarette (ENDS) health policy documents from Australia and the UK. The researchers employed GPT-4 to classify sentences containing both ENDS-related and evidence-related terms into three categories: helpful, harmful, or neither. After validating the classifier on 200 manually annotated sentences (F-score = 0.9), they applied it to a larger corpus of 2,152 sentences. The results revealed significant differences in how evidence is framed across jurisdictions: Australian documents contained more harmful claims and fewer helpful claims than expected, while UK documents showed the opposite pattern, suggesting that despite using similar evidence, policymakers in each country emphasize different aspects in their regulatory approaches.

## Method Summary
The researchers analyzed 109 policy documents from Australia and the UK, identifying sentences that contained both ENDS-related terms (e.g., e-cig, vape, ENDS) and evidence-related terms (e.g., evidence, studies, research). They sampled 200 sentences for manual annotation using a three-category scheme (helpful, harmful, neither), achieving high inter-annotator agreement (kappa = 0.84). A GPT-4-based classifier was then trained using a detailed prompt template that included the full annotation scheme and chain-of-thought reasoning requirements. The classifier achieved an F-score of 0.9 on the annotated sample and was applied to the full corpus of 2,152 evidence sentences. Cross-jurisdiction differences were analyzed using chi-square tests to compare observed vs. expected label distributions.

## Key Results
- GPT-4 classifier achieved F-score of 0.9 for labeling policy sentences
- Australian documents contained significantly more harmful claims and fewer helpful claims than expected
- UK documents showed the opposite pattern with more helpful claims and fewer harmful claims than expected
- The differences were statistically significant (p < 0.0001) using chi-square testing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4 can classify policy sentences into helpful/harmful/neither categories with high accuracy using detailed prompt engineering
- **Mechanism:** The prompt template embeds the full annotation scheme (category definitions, inclusion/exclusion criteria) and requests explicit reasoning before classification. Chain-of-thought prompting appears to improve classification by forcing intermediate articulation
- **Core assumption:** The annotation scheme categories are sufficiently well-defined that both humans and LLMs can apply them consistently
- **Evidence anchors:** "Our LLM-based classifier achieved an F-score of 0.9"; "Inter-annotator agreement between the two annotators was high (kappa = 0.84)"
- **Break condition:** If annotation categories are ambiguous or domain-specific jargon exceeds the LLM's training distribution, classification accuracy may degrade

### Mechanism 2
- **Claim:** Pre-filtering sentences for both ENDS-related terms AND evidence-related terms isolates claims most relevant to policy framing analysis
- **Mechanism:** Boolean keyword matching narrows 109 documents to 2,152 evidence sentences. This reduces noise and focuses analysis on sentences making evidentiary claims rather than procedural or administrative content
- **Core assumption:** Sentences containing both term types are where substantive policy positions are articulated
- **Evidence anchors:** "only sentences that contained both a term related to ENDS... and a term related to evidence... were retained, resulting in a final dataset of 2,152 evidence sentences"
- **Break condition:** If key policy claims use indirect phrasing (e.g., "these products" without explicit ENDS terms), they will be excluded

### Mechanism 3
- **Claim:** Observed vs. expected frequency comparison using chi-square reveals systematic cross-jurisdictional differences in evidence framing
- **Mechanism:** After classification, count helpful/harmful labels per country and compare to expected counts assuming independence. The chi-square test quantifies whether the distribution differs significantly from random
- **Core assumption:** The sentence sample is representative of broader policy discourse in each jurisdiction
- **Evidence anchors:** "Australian legislative documents show a much higher proportion of harmful statements... with the opposite holding for the UK"; "measured using the chi-squared test, p < 0.0001"
- **Break condition:** If corpus composition differs systematically (e.g., different document types per country), observed differences may reflect corpus artifacts rather than genuine policy orientation

## Foundational Learning

- **Concept: Cohen's Kappa (Inter-annotator Agreement)**
  - Why needed here: Validates that the three-category scheme is reliably interpretable before trusting LLM classification
  - Quick check question: If two annotators label the same 100 sentences, what kappa value indicates "substantial agreement"?

- **Concept: F-score / F1 (Harmonic Mean of Precision and Recall)**
  - Why needed here: Primary metric for evaluating classifier performance against human annotations
  - Quick check question: If precision = 0.9 and recall = 0.9, what is the F1 score?

- **Concept: Chi-square Test for Independence**
  - Why needed here: Determines whether label distributions differ significantly between Australia and UK beyond chance
  - Quick check question: What does p < 0.0001 imply about the null hypothesis of no country-label association?

## Architecture Onboarding

- **Component map:** Raw documents (109) -> Plaintext conversion -> SpaCy sentence splitting -> Dual-term filtering (ENDS + evidence) -> GPT-4 classification (prompt + reasoning) -> Label aggregation by country -> Chi-square comparison -> Interpretation

- **Critical path:** The prompt template is the highest-leverage component. It must encode the annotation scheme precisely; any ambiguity propagates to all 2,152 classifications

- **Design tradeoffs:**
  - Manual annotation (200 sentences) vs. full automation: Small labeled set limits evaluation robustness but enables rapid scaling
  - Strict term filtering vs. recall: Reduces false positives but may miss implicit claims
  - Single LLM (GPT-4) vs. ensemble: No baseline comparison; results are model-dependent

- **Failure signatures:**
  - Low inter-annotator agreement (kappa < 0.6) signals unclear annotation scheme
  - Classifier F1 < 0.7 suggests prompt or scheme mismatch with LLM capabilities
  - Non-significant chi-square despite large N may indicate balanced/neutral corpus composition
  - Reproducibility drift across runs due to LLM nondeterminism

- **First 3 experiments:**
  1. **Prompt ablation:** Remove chain-of-thought instruction ("Give reasons") and compare F1 to assess reasoning contribution
  2. **Baseline comparison:** Run a simple keyword-based classifier (e.g., presence of "benefit" vs. "risk" terms) to quantify LLM value-add
  3. **Cross-validation:** Split the 200 annotated sentences into train/test folds multiple times to estimate classifier stability with limited labeled data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What factors explain why Australian and UK policymakers frame identical evidence bases differently when formulating ENDS regulations?
- **Basis in paper:** Limitations section states "while we have shown that there are clear differences in the way that ENDS-related evidence is presented across the two jurisdictions, our results do not explain the underlying reason for these differences"
- **Why unresolved:** The study quantifies the divergence but does not investigate causal mechanisms such as institutional culture, stakeholder influence, or historical precedent
- **What evidence would resolve it:** Interviews with policymakers, longitudinal analysis of document drafting processes, or comparative studies of policy advisory networks in both jurisdictions

### Open Question 2
- **Question:** Can the LLM-based classification framework be adapted to extract other policy-relevant dimensions such as target populations, commercial interests, rhetorical tone, or appeals to values?
- **Basis in paper:** Discussion section states "our experimental framework could be modified to extract insights on, for example, target populations, commercial interests, tone, or appeals to underlying values within policy documents more generally"
- **Why unresolved:** The current study only validates the framework for helpful/harmful classification; extending to other analytical dimensions requires new annotation schemes and validation
- **What evidence would resolve it:** Applying the framework with modified prompts to the same corpus and evaluating classifier performance against human annotations for each new dimension

### Open Question 3
- **Question:** How does LLM-based policy text classification compare to traditional supervised NLP methods in accuracy, efficiency, and training data requirements?
- **Basis in paper:** Limitations states "we have not experimented with alternative or baseline classification methods," despite claiming LLMs reduce need for training data
- **Why unresolved:** Without comparison to supervised approaches (e.g., fine-tuned BERT, SVM), the relative advantages of LLMs for policy analysis remain unquantified
- **What evidence would resolve it:** A comparative study applying both LLM-based and traditional supervised classifiers to the same annotated corpus, measuring F-scores, annotation burden, and computational costs

### Open Question 4
- **Question:** Can the keyword-filtering approach for identifying evidence-related sentences be improved to capture implicit or indirectly-referenced evidence claims?
- **Basis in paper:** Limitations notes "our method for identifying ENDS-related evidence sentences may not have captured the full spectrum of ENDS-related evidence claims present in our dataset"
- **Why unresolved:** Requiring both ENDS terms and explicit evidence terms may exclude sentences discussing evidence through inference, pronouns, or implicit references
- **What evidence would resolve it:** Manual review of excluded sentences to assess false negative rates, followed by development of expanded filtering criteria or LLM-based sentence retrieval

## Limitations
- The narrow annotation sample (200 sentences) relative to the 2,152-sentence corpus limits evaluation robustness and generalizability
- Corpus composition differences across countries could confound interpretation of jurisdictional differences
- The methodology demonstrates feasibility but requires further validation before drawing definitive policy conclusions
- No comparison to traditional supervised NLP methods to quantify LLM advantages

## Confidence

**Confidence Labels:**
- **High:** LLM classification achieves F-score of 0.9 on held-out annotations
- **Medium:** Systematic differences exist between Australian and UK policy framing of e-cigarette evidence
- **Low:** The observed framing differences directly explain regulatory divergence between jurisdictions

## Next Checks
1. Replicate the full pipeline with an independent random sample of 200 additional annotated sentences to assess classifier stability and potential overfitting to the initial sample
2. Apply the classification pipeline to policy documents from a third jurisdiction (e.g., New Zealand) to test whether the helpful/harmful framing pattern generalizes beyond the Australia-UK comparison
3. Conduct a document-level analysis correlating the aggregate helpful/harmful sentence counts with explicit regulatory outcomes to test whether framing differences predict policy stringency