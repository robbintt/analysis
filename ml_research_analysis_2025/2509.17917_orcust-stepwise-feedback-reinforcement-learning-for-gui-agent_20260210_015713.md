---
ver: rpa2
title: 'Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent'
arxiv_id: '2509.17917'
source_url: https://arxiv.org/abs/2509.17917
tags:
- reward
- agent
- action
- uni0000015c
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Orcust, a reinforcement learning framework
  that improves GUI agent performance by integrating Principle-Constrained Reward
  Modeling (PCRM) with Online VM-Grounded Trajectory Construction (OVTC). PCRM combines
  environment-verifiable rules and LLM-derived critiques to provide stepwise, interpretable
  rewards, while OVTC uses lightweight virtual machines to autonomously generate high-fidelity
  interaction trajectories.
---

# Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent

## Quick Facts
- arXiv ID: 2509.17917
- Source URL: https://arxiv.org/abs/2509.17917
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on eight GUI agent benchmarks, improving 22.2% on ScreenSpot and 23.9% on ScreenSpot-Pro over the base model

## Executive Summary
Orcust introduces a reinforcement learning framework that improves GUI agent performance by integrating Principle-Constrained Reward Modeling (PCRM) with Online VM-Grounded Trajectory Construction (OVTC). PCRM combines environment-verifiable rules and LLM-derived critiques to provide stepwise, interpretable rewards, while OVTC uses lightweight virtual machines to autonomously generate high-fidelity interaction trajectories. Evaluated on eight benchmarks, Orcust achieves state-of-the-art performance, demonstrating robust reasoning, adaptability, and data efficiency across diverse GUI environments.

## Method Summary
Orcust employs a two-phase training approach: first a brief supervised fine-tuning (SFT) warm-up on curated trajectories, then reinforcement fine-tuning (RFT) using PCRM via Group Relative Policy Optimization (GRPO). The policy model (Qwen2.5-VL-3B/7B) receives screenshots, instructions, and principles as input, producing chain-of-thought reasoning and JSON action coordinates. The PCRM reward combines Environment-Verifiable Principles (EVP) - deterministic checks for safety and validity - with LLM-Derived Principles (LDP) - generative critiques from Qwen2.5-72B scoring reasoning quality 1-10. OVTC continuously generates trajectories in QEMU/KVM virtual machines, capturing screenshots, DOM snapshots, and input events for precise verification. Rewards are aggregated over 4-step windows with self-labeled milestone tokens providing intermediate feedback.

## Key Results
- Achieves 22.2% improvement on ScreenSpot and 23.9% on ScreenSpot-Pro over the base model
- Demonstrates 72.32% success rate on AndroidControl-Low, outperforming both EVP-only (69.12%) and LDP-only (70.31%) approaches
- Shows 4-step reward aggregation intervals outperform 1-step or 8-step intervals for long-horizon tasks
- Validates trajectory diversity is more critical than quantity alone for performance gains

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Reward Composition (EVP + LDP)
Combining deterministic environment checks with generative LLM critiques provides denser, interpretable feedback than sparse success/failure signals alone. The hybrid approach decomposes rewards into EVP (hard constraints like cursor bounds) and LDP (LLM scoring of reasoning quality), with EVP dominating early training and LDP refining strategy later.

### Mechanism 2: Subgoal-Driven Credit Assignment
Self-labeled milestones within the Chain-of-Thought mitigate credit assignment problems in long-horizon tasks by converting sparse rewards into dense stepwise feedback. The agent emits `[MILESTONE: ...]` tokens that trigger immediate intermediate rewards, chunking long trajectories into verifiable segments.

### Mechanism 3: VM-Grounded Data Synthesis (OVTC)
Generating interaction traces in controlled VM environments allows high-fidelity, diverse training data without manual labeling. OVTC spins up QEMU/KVM instances to run real apps while capturing Screen RGB, Input events, and DOM snapshots simultaneously, enabling automated verification of structural objectives.

## Foundational Learning

- **Concept: Reinforcement Fine-Tuning (RFT) / GRPO**
  - Why needed: RFT allows the model to explore and refine reasoning chains based on PCRM rewards, unlike SFT which only mimics ground truth
  - Quick check: How does GRPO differ from standard PPO in terms of how it estimates advantages (hint: group baselines)?

- **Concept: Vision-Language Models (VLMs) as Agents**
  - Why needed: The policy is a VLM (Qwen2.5-VL) that must map visual pixels directly to action coordinates (JSON)
  - Quick check: How does a VLM represent screen coordinatesâ€”is it via discrete tokens or regression heads?

- **Concept: Process Reward Models (PRM)**
  - Why needed: PCRM rewards the reasoning process and intermediate actions, unlike Outcome Reward Models that only reward final outcomes
  - Quick check: Why might an ORM lead to "reward hacking" in long-horizon GUI tasks compared to a PRM?

## Architecture Onboarding

- **Component map:** Policy Model (Qwen2.5-VL) -> PCRM (EVP + LDP) -> GRPO Update -> OVTC Generator -> VM Environment -> (Screenshots, DOMs, Events)
- **Critical path:** 1) SFT Warm-up on curated trajectories, 2) OVTC Loop: agent acts in VM, OVTC captures trace, 3) PCRM Evaluation: trace scored by EVP and LDP, 4) RFT Update: combined reward updates Policy via GRPO
- **Design tradeoffs:** EVP vs LDP weighting (EVP dominates early, LDP later); VM Overhead vs Scale (QEMU is lightweight but scaling requires significant infrastructure)
- **Failure signatures:** "Context-Lacking" behavior (clicks correctly but at wrong time), "Reward Drift" (circular reasoning to maximize LDP scores), Format Violations (outputs free text instead of JSON)
- **First 3 experiments:** 1) Reward Ablation (EVP-only vs LDP-only vs Hybrid), 2) Milestone Sensitivity (with vs without milestone tokens), 3) Data Diversity Study (low vs high diversity datasets)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Orcust maintain performance when deployed on physical devices without VM instrumentation?
  - Basis: The Limitations section states reliance on simulated VM environments may not capture full real-world GUI variability
  - Resolution: Evaluation results from non-virtualized environments benchmarking against simulated environment

- **Open Question 2:** Is it possible to reduce the computational overhead of the LLM-based critique model?
  - Basis: Authors note significant computational overhead from running VM simulations and computing dense stepwise rewards
  - Resolution: Performance comparison using smaller, distilled reward models (7B or 14B) versus 72B model

- **Open Question 3:** What architectural safeguards are required to prevent unauthorized actions on sensitive interfaces?
  - Basis: Limitations section raises ethical and security concerns about manipulating financial or personal data
  - Resolution: Experiments demonstrating adherence to "privacy-preserving" principles in red-teaming evaluation

## Limitations
- High computational overhead from VM-based simulations and LLM critiques limits scalability
- Performance may degrade when deployed on physical devices lacking VM instrumentation
- Limited analysis of failure case distributions and long-term safety implications in real-world deployment

## Confidence
- **High Confidence:** 22.2% improvement on ScreenSpot and 23.9% on ScreenSpot-Pro clearly demonstrated through controlled experiments
- **Medium Confidence:** Generalizability across eight benchmarks supported but could benefit from more diverse failure analysis
- **Low Confidence:** Scalability analysis for OVTC data generation is minimal, lacking concrete computational overhead quantification

## Next Checks
1. **LDP Robustness Testing:** Conduct adversarial testing to measure how often the Qwen2.5-72B critic correctly identifies superficially complete but logically flawed reasoning chains versus being fooled.

2. **Sim-to-Real Transfer Gap Analysis:** Compare agent performance on identical tasks across VM environment versus real devices, quantifying performance drop and identifying specific environmental factors contributing to the gap.

3. **Reward Attribution Study:** Analyze reward distribution across EVP and LDP components for successful and failed trajectories to determine whether success correlates more with EVP compliance or LDP reasoning quality, and how this varies by task complexity.