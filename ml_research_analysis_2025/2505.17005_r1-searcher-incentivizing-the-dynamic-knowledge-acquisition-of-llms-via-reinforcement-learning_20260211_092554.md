---
ver: rpa2
title: 'R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via
  Reinforcement Learning'
arxiv_id: '2505.17005'
source_url: https://arxiv.org/abs/2505.17005
tags:
- knowledge
- internal
- external
- answer
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "R1-Searcher++ introduces a two-stage training strategy to teach\
  \ large language models to dynamically leverage both internal and external knowledge\
  \ sources. The first stage employs supervised fine-tuning to standardize the model\u2019\
  s format for using internal reasoning and external retrieval."
---

# R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.17005
- **Source URL:** https://arxiv.org/abs/2505.17005
- **Reference count:** 17
- **Primary result:** Outperforms existing retrieval-augmented and reasoning methods by up to 4.3% in accuracy while reducing retrieval count by 42.9%

## Executive Summary
R1-Searcher++ introduces a two-stage training strategy to teach large language models to dynamically leverage both internal and external knowledge sources. The first stage employs supervised fine-tuning to standardize the model's format for using internal reasoning and external retrieval. The second stage uses reinforcement learning with a carefully designed reward mechanism that encourages internal knowledge utilization and a memory mechanism to assimilate retrieved information, enabling continuous enrichment of the model's internal knowledge. Experimental results demonstrate that R1-Searcher++ outperforms existing retrieval-augmented and reasoning methods, achieving up to 4.3% higher accuracy while reducing retrieval count by 42.9% compared to vanilla RL-based approaches. The model also generalizes well to out-of-domain datasets and real-world online search scenarios, effectively balancing internal reasoning with external retrieval.

## Method Summary
The method employs a two-stage training pipeline: (1) a supervised fine-tuning (SFT) cold-start phase that teaches the model to format responses using special tokens for internal knowledge (`<internal>`) and external retrieval (`<external>`), and (2) a reinforcement learning (RL) phase that optimizes the model's policy for when to use each knowledge source. The RL stage incorporates a composite reward function that encourages correct answers while penalizing excessive retrieval through a group reward mechanism. Additionally, a memorization component converts retrieved information into internal knowledge representations, allowing the model to continuously enrich its parametric knowledge base during training.

## Key Results
- Achieves up to 4.3% higher accuracy than baseline retrieval-augmented and reasoning methods
- Reduces retrieval count by 42.9% compared to vanilla RL approaches
- Demonstrates strong generalization to out-of-domain datasets and real-world online search scenarios

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Training for Knowledge Modulation
The sequential SFT-then-RL pipeline stabilizes the acquisition of dynamic knowledge-switching behavior. The SFT "Cold-Start" phase first teaches the model the correct format for distinguishing internal knowledge from external retrieval actions via special tokens (`<internal>`, `<external>`). Once the format is stable, the RL stage uses outcome-based rewards to optimize the policy for when to switch, encouraging exploration of more effective actions beyond the SFT data. The model requires a stable, format-compliant behavioral baseline before it can effectively explore and optimize its reasoning paths via reinforcement learning.

### Mechanism 2: Reward-Shaping for Efficient Retrieval and Internal Utilization
A composite reward function explicitly incentivizes a preference for internal knowledge, reducing unnecessary retrieval. The reward function combines a standard answer reward and a format reward with a novel "group reward" (`Rgroup`). `Rgroup` provides a bonus to correct responses that achieve the minimal retrieval count within a sampled group of outputs for the same question. This creates a gradient that favors solving the problem with fewer external calls, pushing the model to first exhaust its internal parametric knowledge. The variance in retrieval count across correct reasoning paths is a reliable proxy for the problem's solvability via internal knowledge, and the model can learn to find this minimal path.

### Mechanism 3: External-to-Internal Knowledge Memorization
Explicitly training the model to convert retrieved documents into internal knowledge representations during training leads to continuous knowledge enrichment. During the RL process, correct reasoning paths that involved retrieval are selected. A separate "rewriter" model (fine-tuned on SFT data) takes the question and the retrieved document and generates a new, correct reasoning path using only internal reasoning tokens (`<internal>`). This synthesized data is then used to fine-tune the policy model with a supervised loss (`LM(θ)`) alongside the RL loss. This internalizes the external fact, allowing the model to enrich its internal knowledge without losing previously learned information.

## Foundational Learning

- **Concept: Reinforcement Learning from Verifiable Rewards (RLVR) / Outcome-Based RL**
  - **Why needed here:** The core of the second training stage. Instead of providing a step-by-step "process reward model" (PRM), the model learns from a final, verifiable outcome (e.g., is the final answer correct?).
  - **Quick check question:** Can you explain why an outcome-based reward (e.g., +1 for a correct final answer, 0 otherwise) is considered more scalable than a process-based reward that grades each reasoning step?

- **Concept: Special Tokens for Tool Use**
  - **Why needed here:** The entire framework depends on the model generating specific tokens (`<external>`, `<internal>`) to interface with its environment. This is a form of structured tool-calling embedded directly in the generation stream.
  - **Quick check question:** How does a model learn to trigger a Python function call or an API request solely by generating a special string of characters like `<|begin_external_search|>`?

- **Concept: Catastrophic Forgetting vs. Knowledge Assimilation**
  - **Why needed here:** A central goal is *enriching* the model's internal knowledge. This requires updating the model's weights without erasing previously learned information, a delicate balance in continual learning.
  - **Quick check question:** When fine-tuning a pre-trained LLM on new, fact-heavy data, what are the risks to its general reasoning abilities, and how might a low learning rate or regularization (like KL divergence) help?

## Architecture Onboarding

- **Component map:**
  Policy Model (LLM) -> Environment (Retriever) -> Tokenizer/Autoregressive Loop -> Reward Calculator -> Rewriter Model

- **Critical path:**
  1. SFT Data Curation: Use rejection sampling to generate data with correct answers and valid format (both internal and external tags)
  2. Stage 1 (SFT): Fine-tune the policy model on curated data to learn the format
  3. Stage 2 (RL):
     - Rollout: Policy generates a response. If it calls `<external>`, the environment provides a document
     - Evaluation: Calculate `Rformat`, `Ranswer`, `Rgroup`. Sum to get total reward `R`
     - Rewriting: For correct rollouts with retrieval, use the rewriter model to create a path with only `<internal>` tokens
     - Policy Update: Calculate RL loss (`JMask`) and memorization loss (`LM`). Combine them with a weight (`µ`) and backpropagate

- **Design tradeoffs:**
  - **On-Policy RL:** The paper uses on-policy sampling (REINFORCE++), which is more stable but can be slower than off-policy methods. The trade is stability for efficiency
  - **Rewriting vs. Direct Memorization:** Using a separate rewriter model is an added complexity but likely provides higher-quality training data than simply hoping the policy model learns the fact from seeing the document once in its context window
  - **Efficiency vs. Accuracy:** The `Rgroup` reward is explicitly designed to trade off some potential accuracy (by penalizing retrieval) for massive gains in efficiency (42.9% reduction in retrieval count). This is a feature, not a bug, for many real-world applications

- **Failure signatures:**
  - **Over-reliance on retrieval:** If `Rgroup` is not implemented or weighted correctly, the model will call the retriever for every sub-step, mirroring the "vanilla RL" baseline
  - **Format collapse:** If the RL rewards are too strong or noisy, the model may forget the special token format learned in SFT, breaking the tool-use mechanism
  - **Memorization of Noise:** If the rewriter model hallucinates, the policy model will internalize false facts, leading to a degradation in performance on questions it previously could have answered correctly

- **First 3 experiments:**
  1. Replicate SFT Stage: Take a small set of multi-hop QA data (e.g., from HotpotQA). Use a capable model to generate chain-of-thought responses with simulated retrieval calls. Filter for correct answers and valid format. Fine-tune a base model and verify it can generate the `<internal>` and `<external>` tags correctly
  2. Implement Basic RL Reward: Set up the RL loop. First, implement and test only the `Rformat` and `Ranswer` rewards. Confirm the model learns to get the correct answer but becomes heavily reliant on retrieval (the "vanilla RL" problem described in the paper)
  3. Integrate `Rgroup` Reward: Add the group reward mechanism. For a given prompt, sample multiple responses. Identify correct ones. Find the one with the lowest retrieval count. Implement the reward bonus. Observe if the model's average retrieval count per question begins to decrease over training steps

## Open Questions the Paper Calls Out

- **Open Question 1:** Does integrating a real-time online search engine during the RL training phase improve performance and robustness compared to using a static local corpus?
  - **Basis in paper:** The authors note in the Limitations section that they relied on a local dense retrieval corpus during training due to resource constraints, suggesting that "Aligning the training process with real-world conditions by integrating a real search engine may lead to improved performance."
  - **Why unresolved:** The current model was trained on static Wikipedia data; the distribution shift between static training data and dynamic real-world inference (handled only during evaluation) remains unaddressed in the training pipeline.
  - **What evidence would resolve it:** A training run utilizing a live search API (e.g., Google/Bing) compared against the static corpus baseline on temporal benchmarks.

- **Open Question 2:** Does the efficacy of the "dynamic knowledge acquisition" reward mechanism hold for significantly larger models (e.g., 70B+ parameters)?
  - **Basis in paper:** The paper states, "our current experiments are limited to a 7B-parameter model," and explicitly lists plans to "train and evaluate our framework on larger-scale models" as future work.
  - **Why unresolved:** Larger models possess stronger internal knowledge caches, which might reduce the necessity or alter the optimal frequency of external retrieval, potentially changing the balance of the reward design.
  - **What evidence would resolve it:** Scaling experiments applying R1-Searcher++ to 70B or 100B+ parameter backbones to observe changes in retrieval count and answer accuracy.

- **Open Question 3:** Does the external knowledge memorization mechanism (converting retrieved text to internal reasoning) cause catastrophic forgetting of pre-trained general capabilities?
  - **Basis in paper:** The paper claims the model "continuously enriches its internal knowledge" via a memorization loss ($L_M$), but evaluates only on downstream QA accuracy, not on the stability of the model's original pre-trained competencies.
  - **Why unresolved:** Adding auxiliary losses to assimilate specific facts during RL can interfere with the model's existing weights, potentially degrading performance on tasks unrelated to the retrieval domain.
  - **What evidence would resolve it:** Evaluating the model on general reasoning or language understanding benchmarks (e.g., MMLU, GSM8K) before and after the RL stage to measure capability drift.

## Limitations

- The paper relies on a static local dense retrieval corpus during training due to resource constraints, limiting real-world applicability
- Current experiments are limited to a 7B-parameter model, leaving questions about scalability to larger models
- The memorization mechanism's effectiveness in truly enriching internal knowledge versus pattern matching is not directly validated through knowledge retention tests

## Confidence

- **High Confidence:** The two-stage training pipeline (SFT followed by RL) demonstrably improves performance over single-stage approaches. The ablation showing degradation when removing either stage provides strong evidence for this architectural choice.
- **Medium Confidence:** The reward-shaping mechanism effectively reduces retrieval count while maintaining accuracy. The group reward design is novel and theoretically sound, but its long-term effectiveness across diverse problem types requires further validation.
- **Medium Confidence:** The memorization mechanism contributes positively to performance and efficiency. However, the evidence for true knowledge assimilation versus pattern matching from the rewriting data is less direct.

## Next Checks

1. **Knowledge retention test:** After training with the memorization mechanism, evaluate the model on questions that could be answered by facts originally retrieved during training but not present in the original pre-training data. Compare against a control model trained without memorization to measure actual knowledge enrichment.

2. **Robustness to retrieval path complexity:** Design test cases where the minimal retrieval path (according to `Rgroup`) is objectively insufficient to answer the question correctly. Measure whether the model can still identify when multiple retrievals are necessary despite the efficiency incentive.

3. **Rewriter quality analysis:** Systematically evaluate the auxiliary rewriter model's outputs for factual accuracy, logical consistency, and format compliance. Measure hallucination rates and analyze whether errors in rewriting propagate to the policy model's internalized knowledge.