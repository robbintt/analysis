---
ver: rpa2
title: 'CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language
  Models'
arxiv_id: '2509.22737'
source_url: https://arxiv.org/abs/2509.22737
tags:
- reasoning
- comparison
- comparebench
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CompareBench, a new benchmark designed to
  evaluate visual comparison reasoning in vision-language models (VLMs). The benchmark
  includes 1,000 QA pairs across four fundamental tasks: quantity comparison, temporal
  ordering, geometric property comparison, and spatial relation reasoning.'
---

# CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models

## Quick Facts
- arXiv ID: 2509.22737
- Source URL: https://arxiv.org/abs/2509.22737
- Reference count: 21
- Primary result: Current VLMs show systematic failures in visual comparison tasks including counting, geometric, spatial, and temporal reasoning, with even top models scoring 60-80% on basic comparison tasks

## Executive Summary
This paper introduces CompareBench, a new benchmark designed to evaluate visual comparison reasoning in vision-language models (VLMs). The benchmark includes 1,000 QA pairs across four fundamental tasks: quantity comparison, temporal ordering, geometric property comparison, and spatial relation reasoning. These tasks are derived from two auxiliary datasets: TallyBench (2,000 counting images with QA) and HistCaps (515 historical images with bilingual captions). The authors evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show that even the strongest models struggle with temporal ordering and spatial relations, and make frequent errors in basic counting and geometric comparisons. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. CompareBench establishes a foundation for advancing more reliable multimodal reasoning by providing controlled, diverse, and diagnostic evaluation of this underexplored capability.

## Method Summary
CompareBench is constructed from TallyBench (2,000 counting images) and HistCaps (515 historical images) to create 1,000 QA pairs across four tasks: CompareTallyBench (600 samples), CompareTemporalBench (100), CompareGeometryBench (200), and CompareSpatialBench (100). Models are evaluated using standardized instruction prompts requiring single-letter (A-D) or integer outputs. The benchmark tests both closed-source APIs and open-source models through inference-only evaluation, measuring exact-match accuracy. The evaluation pipeline includes 4-image grids for comparison tasks and single images with labeled objects for geometric and spatial reasoning.

## Key Results
- Even strongest models (GPT-5, Gemini 2.5 Pro) score only 85.40% overall, with significant drops on geometric (60-70%) and spatial (70-80%) tasks
- Open-source models lag significantly behind closed-source APIs, with Qwen3-VL-235B-A22B achieving only 65.40% accuracy
- Temporal ordering performance strongly suggests knowledge contamination, with humans scoring 30% while GPT-5 scores 74%
- Basic counting and geometric comparisons that are trivial for humans remain systematic failure points for VLMs

## Why This Works (Mechanism)

### Mechanism 1: Diagnostic Isolation of Comparative Relations
Standard benchmarks may overestimate VLM reasoning capability because they conflate recognition with relational processing; isolating "comparison" exposes a distinct deficit. By forcing models to select between options based on relative attributes rather than absolute labels, the benchmark bypasses simple object recognition. Success requires models to construct joint embedding spaces where relative spatial, temporal, or quantitative differences are linearly separable, not just semantic categories.

### Mechanism 2: Knowledge-Perception Decoupling in Temporal Reasoning
High performance on temporal ordering relies significantly on parametric world knowledge rather than visual evidence extraction. In CompareTemporalBench, models leverage pre-training associations to order images. This allows large models to outperform humans who rely solely on visual cues if they possess specific historical knowledge. The vision encoder is sufficient to identify semantic keys which the LLM then maps to dates.

### Mechanism 3: Compression Artifacts in Fine-Grained Geometric Reasoning
Failures in geometric and spatial comparison arise from the vision encoder's loss of precise pixel-level metric information during patch embedding. Vision Transformers tokenize images into patches, losing precise sub-pixel geometric details. When asked "which is thicker," the model lacks the high-resolution feature map required for metric comparison, defaulting to semantic guessing.

## Foundational Learning

- **Concept: Multimodal Grounding vs. Hallucination**
  - Why needed here: The paper demonstrates that models often ignore strict visual rules in favor of statistical priors
  - Quick check question: Can you distinguish between a model generating a caption based on visual features vs. retrieving a memorized caption from training data?

- **Concept: Vision Transformer (ViT) Patch Mechanics**
  - Why needed here: Understanding why models fail at "thickness" or "counting" requires knowing how images are tokenized into patches, which destroys fine-grained density information
  - Quick check question: How does reducing an image to a sequence of 256 patches affect the model's ability to judge precise spatial distances?

- **Concept: Benchmark Contamination**
  - Why needed here: The authors note that high performance on temporal tasks might result from memorizing historical image-caption pairs from pre-training data rather than reasoning
  - Quick check question: How would you design a test to ensure a model is reasoning about a novel image rather than recalling a memorized label?

## Architecture Onboarding

- **Component map:**
  - TallyBench: 2,000 counting images (Real/Synthetic mix) → Tests basic numerosity
  - HistCaps: 515 historical images (Curated historical) → Provides temporal anchors (Tags + Bilingual Captions)
  - CompareBench:
    - CompareGeometry/Spatial: Single Image → 4 marked points
    - CompareTally/Temporal: 4-Image Grid → Relative comparison

- **Critical path:**
  1. Input Processing: Ensure model accepts multi-image grids and high-res single images
  2. Prompt Adherence: Enforce strict instruction templates to prevent parsing errors
  3. Evaluation: Exact match accuracy on (A-D) or Integer

- **Design tradeoffs:**
  - Grid vs. Single Image: 4-image grid tests relative comparison but forces model to process 4x visual tokens
  - Knowledge vs. Vision: Temporal tasks are "contaminated" by world knowledge; use them to test cultural knowledge, not pure visual temporal reasoning

- **Failure signatures:**
  - Counting Hallucination: Consistent under-counting of occluded objects or over-counting of textured patterns
  - Spatial Inversion: Confusing "closer to camera" with "higher above ground" due to 2D-to-3D projection failures
  - Temporal Bias: Defaulting to "most famous event" as the answer regardless of options

- **First 3 experiments:**
  1. Zero-Shot Baseline: Run strongest available model on CompareBench to establish baseline
  2. Visual-Ablation on Temporal: Crop/blur faces and landmarks to force reliance on visual style
  3. Resolution Sensitivity: Evaluate CompareGeometry at varying input resolutions to test geometric precision scaling

## Open Questions the Paper Calls Out

### Open Question 1
What architectural or training modifications are required to close the gap between VLM and human performance on basic counting and geometric comparison tasks that are currently "trivial for humans"? The paper benchmarks failures but does not investigate whether improvements require fine-tuning, architectural changes, or different training objectives.

### Open Question 2
Does CompareTemporalBench primarily measure visual comparison reasoning or leveraged world knowledge from pretraining corpora? The paper notes high model performance but does not disentangle whether this reflects genuine visual temporal reasoning versus recognition plus retrieval of historical facts.

### Open Question 3
Can open-source VLMs match closed-source API performance on visual comparison tasks through scaling alone, or is there a persistent capability gap? The paper documents the gap but does not determine whether it stems from model scale, training data quality, proprietary techniques, or architectural differences.

### Open Question 4
What specific visual features or image characteristics predict VLM failures on counting tasks (e.g., occlusion, stacked objects, visually similar instances)? The paper provides anecdotal hard cases without quantifying which factors most strongly correlate with errors.

## Limitations

- Benchmark design may inadvertently conflate visual comparison reasoning with knowledge retrieval, particularly in temporal tasks where world knowledge can dominate performance
- Resolution sensitivity analysis is incomplete - the hypothesis about ViT patch tokenization destroying fine-grained information requires systematic validation
- The paper documents failures but does not provide concrete architectural solutions or training modifications to address identified gaps

## Confidence

- **High Confidence**: Visual comparison reasoning remains systematically difficult for VLMs across all four task types; counting, geometric, and spatial tasks show consistent performance gaps (60-80% accuracy) even for state-of-the-art models
- **Medium Confidence**: Temporal ordering task contamination by pre-training knowledge is the primary explanation for large model performance advantages; this requires controlled ablation studies to confirm
- **Medium Confidence**: Geometric and spatial comparison failures are fundamentally due to vision encoder resolution limitations rather than reasoning deficits; this needs empirical validation through resolution scaling experiments

## Next Checks

1. **Knowledge-Ablation Temporal Test**: Apply consistent image degradation (blurring, cropping, removing identifying features) to HistCaps images and measure performance drop to isolate visual vs. knowledge-based temporal reasoning

2. **Resolution Scaling Experiment**: Evaluate CompareGeometryBench across multiple input resolutions (224px, 512px, 1024px) to determine if geometric precision scales with pixel density, confirming or refuting the ViT patch limitation hypothesis

3. **Novel Temporal Reasoning Task**: Create a new temporal dataset using artificially aged/modified images of common objects (not historical events) where temporal ordering must be inferred from visual cues alone, eliminating pre-training knowledge contamination