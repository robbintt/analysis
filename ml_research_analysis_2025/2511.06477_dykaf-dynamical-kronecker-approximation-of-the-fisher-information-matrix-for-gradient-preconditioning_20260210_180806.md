---
ver: rpa2
title: 'DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix
  for Gradient Preconditioning'
arxiv_id: '2511.06477'
source_url: https://arxiv.org/abs/2511.06477
tags:
- matrix
- approximation
- fisher
- dykaf
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DyKAF improves Kronecker-factored Fisher matrix approximations
  in deep learning optimizers by applying projector-splitting integrators to maintain
  dynamic low-rank updates. This method enhances the accuracy of curvature estimation
  while keeping memory and computational costs low.
---

# DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning

## Quick Facts
- arXiv ID: 2511.06477
- Source URL: https://arxiv.org/abs/2511.06477
- Reference count: 40
- Primary result: DyKAF outperforms existing optimizers like SOAP and Shampoo on language model fine-tuning and pretraining tasks

## Executive Summary
DyKAF improves Kronecker-factored Fisher matrix approximations in deep learning optimizers by applying projector-splitting integrators to maintain dynamic low-rank updates. This method enhances the accuracy of curvature estimation while keeping memory and computational costs low. Empirical results show DyKAF outperforms existing optimizers like SOAP and Shampoo on language model fine-tuning and pretraining tasks, achieving better validation losses and downstream task performance across a range of benchmarks. The approach is particularly effective in settings where gradient structure varies, offering both robustness and efficiency without extensive hyperparameter tuning.

## Method Summary
DyKAF is an optimizer that enhances SOAP by using projector-splitting integrators for Kronecker factor updates and SVD-based initialization from the first gradient. It maintains dynamic low-rank approximations of the Fisher matrix through alternating updates of left and right Kronecker factors, followed by normalization. The method optionally uses rank-1 second moment estimation for memory efficiency in fine-tuning scenarios. DyKAF inherits SOAP's rotation-to-diagonal strategy but achieves more accurate Kronecker factors, leading to better diagonal preconditioning in the rotated space.

## Key Results
- DyKAF achieves better validation losses than SOAP and Shampoo on language model fine-tuning and pretraining tasks
- Rank-1 second moment setting improves fine-tuning performance but degrades pretraining results
- SVD initialization from the first gradient provides better starting points than identity matrices
- Projector-splitting updates track optimal Kronecker factors more accurately than cumulative gradient summation

## Why This Works (Mechanism)

### Mechanism 1: Projector-Splitting Integrator for Kronecker Factor Updates
DyKAF maintains more accurate Kronecker-factored Fisher approximations than Shampoo/SOAP's cumulative gradient summation by applying projector-splitting integrators from dynamical low-rank approximation. The Fisher matrix rearrangement R(F_t) maps the Kronecker approximation L⊗R to a rank-1 problem. Projector-splitting alternately updates L and R using the current gradient, then normalizes. This tracks the optimal Kronecker factors dynamically rather than accumulating heuristically.

### Mechanism 2: Improved Eigenspace Rotation Enables Better Diagonal Preconditioning
More accurate Kronecker factors yield better eigenvector matrices Q_L, Q_R, making the rotated Fisher matrix "more diagonal" and thus more amenable to diagonal preconditioning. SOAP rotates gradients via (Q_L ⊗ Q_R)^T before diagonal scaling. If F ≈ L⊗R, then the rotated Fisher approaches diagonal structure, making Adam-style second-moment estimation more effective in rotated space.

### Mechanism 3: SVD-Based Initialization from First Gradient
Initializing Kronecker factors from the first gradient's SVD provides a better starting point than scaled identity matrices. For F_1 = vec(G_1)vec(G_1)^T, the optimal rank-1 Kronecker approximation is L=σ_1(G_1)u_1u_1^T, R=σ_1(G_1)v_1v_1^T. This is computed via power iteration on G_1, avoiding the cold-start problem of identity initialization.

## Foundational Learning

- **Kronecker Products and Rearrangement Operator**
  - Why needed here: Understanding how A⊗B maps to vec(A)vec(B)^T via rearrangement is essential for seeing why Kronecker approximation is a rank-1 problem
  - Quick check question: Given L∈R^{m×m}, R∈R^{n×n}, what is the shape of R(L⊗R) and what is its rank?

- **Empirical Fisher Information Matrix**
  - Why needed here: DyKAF approximates the empirical Fisher F_t = Σ vec(G_i)vec(G_i)^T, which serves as a curvature proxy in natural gradient descent
  - Quick check question: How does the empirical Fisher differ from the true Fisher, and when are they equivalent?

- **Dynamical Low-Rank Approximation / Projector-Splitting**
  - Why needed here: The core algorithmic innovation applies integrators from numerical analysis to maintain low-rank approximations of time-varying matrices
  - Quick check question: In Algorithm 1, why does the K-step update L before S, and why does this avoid dense matrix operations?

## Architecture Onboarding

- **Component map:**
  DyKAF = SOAP + three modifications:
  ├── Initialization: SVD-based vs identity
  ├── Kronecker updates: kron_proj_split vs L += GG^T
  └── Optional: rank-1 second moment vs full V_t

- **Critical path:**
  1. Implement `kron_proj_split` for dynamical Kronecker factor updates
  2. Replace SOAP's L += GG^T, R += G^T G with projector-splitting updates
  3. Add SVD initialization on first step via power iteration
  4. (Optional) Implement rank-1 second moment with `proj_split`

- **Design tradeoffs:**
  - `rank1_second moment=True`: Saves memory, better for fine-tuning where gradients are low-rank. Wall-clock overhead ~25%
  - `rank1_second moment=False`: Better for pretraining where full curvature information matters
  - Preconditioning frequency f: Same as SOAP (default 10); higher f = more accurate but slower

- **Failure signatures:**
  - Loss divergence with rank-1 second moment in pretraining → disable rank-1 mode
  - NaN in Kronecker factors → check gradient norm explosion; ensure normalization
  - No improvement over SOAP → verify SVD initialization is actually running
  - Slower convergence than Adam → f too high or learning rate needs retuning

- **First 3 experiments:**
  1. Validation on small-scale classification comparing ||F - approximation||_F for DyKAF vs SOAP
  2. Ablation on rank-1 second moment: run GLUE fine-tuning with both settings
  3. Hyperparameter transfer test: run Shakespeare-char pretraining using SOAP hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed tensor generalization of DyKAF (Algorithm 5) maintain computational efficiency and approximation accuracy in convolutional neural networks? The paper derives a higher-order projector-splitting algorithm for tensor-shaped parameters but only evaluates matrix-shaped weights in language models.

### Open Question 2
What specific properties of the gradient structure in pretraining cause the rank-1 second moment approximation to fail? The paper observes this discrepancy empirically but doesn't provide theoretical explanation for why gradient second moments in pretraining violate the low-rank assumption.

### Open Question 3
Does the approximation error of the projector-splitting integrator accumulate unboundedly over long optimization trajectories? While single-step error is bounded, the paper doesn't analyze if errors compound over thousands of iterations, potentially leading to drift in Fisher approximation.

## Limitations
- The ~25% computational overhead may be prohibitive for very large-scale training
- The superiority of projector-splitting depends on gradient structure that may vary across architectures
- The bounded error assumption may not hold in early training phases with high gradient variance

## Confidence

**Key Uncertainties**
- The bounded error assumption (||E_i|| ≤ ε||F_i||) may not hold in early training phases
- The ~25% computational overhead may be prohibitive for very large-scale training
- The superiority of projector-splitting over Shampoo's accumulation strategy depends on gradient structure

**Major Claim Confidence**
- Mechanism 1 (projector-splitting): High confidence - theoretically grounded with error bounds
- Mechanism 2 (eigenspace rotation): Medium confidence - logical but dependent on Kronecker structure assumption
- Mechanism 3 (SVD initialization): Medium confidence - theoretically optimal but practical benefit unproven at scale

## Next Checks

1. Test SVD initialization's impact by comparing against random initialization on the same tasks
2. Evaluate performance degradation when Kronecker structure assumption is violated (e.g., using highly correlated gradient layers)
3. Measure convergence sensitivity to update frequency f across different model scales and tasks