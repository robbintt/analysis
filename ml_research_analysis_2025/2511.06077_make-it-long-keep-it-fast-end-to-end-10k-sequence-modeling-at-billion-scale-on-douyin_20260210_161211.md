---
ver: rpa2
title: 'Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale
  on Douyin'
arxiv_id: '2511.06077'
source_url: https://arxiv.org/abs/2511.06077
tags:
- train
- training
- user
- length
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of efficiently modeling extremely
  long user interaction sequences in short-video recommendation systems like Douyin,
  where long histories can improve ranking but introduce high computational costs.
  The authors propose three key innovations: (1) Stacked Target-to-History Cross Attention
  (STCA), which reduces attention complexity from quadratic to linear in sequence
  length by focusing on cross-attention from the target to the history and stacking
  multiple layers; (2) Request Level Batching (RLB), which amortizes user-side encoding
  across multiple targets within a request to significantly reduce memory, communication,
  and compute overhead; and (3) a length-extrapolative training strategy that trains
  on shorter sequences but infers on much longer ones, enabling deployment on 10k-length
  histories without increasing training compute.'
---

# Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin

## Quick Facts
- arXiv ID: 2511.06077
- Source URL: https://arxiv.org/abs/2511.06077
- Authors: Lin Guan; Jia-Qi Yang; Zhishan Zhao; Beichuan Zhang; Bo Sun; Xuanyuan Luo; Jinan Ni; Xiaowen Li; Yuhang Qi; Zhifang Fan; Hangyu Wang; Qiwei Chen; Yi Cheng; Feng Zhang; Xiao Yang
- Reference count: 40
- Primary result: 3.3-6.3% lift in key engagement metrics while modeling 10k user interaction sequences at Douyin scale

## Executive Summary
This work tackles the fundamental challenge of efficiently modeling extremely long user interaction sequences in short-video recommendation systems. The authors propose three key innovations: (1) Stacked Target-to-History Cross Attention (STCA), which reduces attention complexity from quadratic to linear in sequence length; (2) Request Level Batching (RLB), which amortizes user-side encoding across multiple targets within a request; and (3) a length-extrapolative training strategy that trains on shorter sequences but infers on much longer ones. Deployed at Douyin's billion-user scale, the system delivers significant improvements in key engagement metrics while meeting strict production latency requirements.

## Method Summary
The core innovation is STCA, which replaces standard self-attention with stacked cross-attention from the target to the history, reducing computational complexity from O(L²) to O(L). RLB groups multiple targets per user request to share the expensive user history encoding. The length-extrapolative training strategy uses stochastic length sampling during training (average 2k sequences) to enable inference on much longer sequences (10k). The model is trained with a curriculum starting at 512 sequence length, then continuing at 2048, using Beta-distributed sampling and temporal suffix selection.

## Key Results
- 3.3-6.3% lift in finish rates on key online metrics at Douyin scale
- Linear complexity O(L) achieved through STCA, enabling 10k sequence modeling
- Effective generalization from 2k training sequences to 10k inference sequences
- Significant reduction in memory, communication, and compute overhead through RLB

## Why This Works (Mechanism)

### Mechanism 1: Linear Complexity via Single-Query Target-to-History Cross Attention
STCA reduces per-layer computational complexity from quadratic O(L²) to linear O(L) by using the target as the sole query against the history. Instead of computing all pairwise interactions, it uses a single query vector (derived from the target) to attend to the full history. The per-layer cost is the dot-product of a d-dimensional query with L d-dimensional keys (O(Ld)), making it linear in L.

### Mechanism 2: Amortized Computation via Request-Level Batching (RLB)
RLB groups multiple target items per user request, computing the shared user history encoding once and reusing it for all associated candidates. This reduces per-target I/O and encoding cost from O(L) to O(L/m) where m is the number of targets per request, substantially lowering sequence-related storage, communication, and compute overhead.

### Mechanism 3: Length Extrapolation via "Train Sparsely, Infer Densely"
The model generalizes to extremely long inference sequences (10k) by training on a distribution of shorter sequences (average 2k) using stochastic length sampling. Each training sample's history is randomly truncated to a length drawn from a Beta distribution, exposing the model to various sequence lengths and enabling it to scale computation to longer inputs at inference time.

## Foundational Learning

**Concept: Attention Mechanisms (Self vs. Cross)**
- Why needed: STCA fundamentally restructures standard Transformer attention; understanding self-attention vs. cross-attention is crucial
- Quick check: In a single attention layer of STCA, how many query vectors are there, and what do they represent?

**Concept: Stochastic Training & Extrapolation**
- Why needed: The "train sparsely, infer densely" strategy is counter-intuitive; understanding random length sampling is critical
- Quick check: Why does training on an average sequence length of 2k allow the model to perform well on 10k sequences during inference? What is the role of the Beta distribution?

**Concept: Batch Efficiency in Distributed Systems**
- Why needed: RLB's value is primarily in systems efficiency (I/O, memory, communication)
- Quick check: How does grouping samples by user reduce host-to-device I/O bandwidth, and what is the key assumption about the data that enables this?

## Architecture Onboarding

**Component map:** STCA encoder (input embedding -> SwiGLU FFN -> LayerNorm -> Multi-head Target Cross Attention -> stacking layers) -> RankMixer head

**Critical path:** Single target query flows from target embedding through FFN/LN to cross-attention against history Keys/Values, then undergoes "target-conditioned fusion" and passes to next stacked layer

**Design tradeoffs:** Expressiveness vs. efficiency - removing history self-attention loses ability to model history-history relationships directly, betting that target-history relevance is sufficient

**Failure signatures:**
- Quadratic cost: If you inadvertently reintroduce history-to-history operation, compute will explode at 10k sequence lengths
- Extrapolation failure: If model performs well on 2k but degrades sharply at 10k, training distribution may be misconfigured
- Training instability: If gradients explode at L=2048, reduce learning rate or add gradient clipping

**First 3 experiments:**
1. Implement single-layer STCA and compare offline AUC and training throughput against standard single-layer target attention
2. Implement Request Level Batching and measure end-to-end training samples/second and CPU-GPU bandwidth with RLB enabled vs. disabled
3. Train models with stochastic lengths at different L_avg (1k, 2k) and evaluate AUC at various inference lengths (2k, 5k, 10k)

## Open Questions the Paper Calls Out

**Open Question 1:** Does removing history self-attention in STCA create an accuracy ceiling for complex temporal dependencies compared to quadratic models?
- Basis: Section 3.1 states authors "de-emphasize explicit history–history interactions" to achieve linear complexity
- Why unresolved: Validated only on short-video data; tasks requiring strict sequential reasoning may suffer
- What evidence would resolve it: Direct comparison of STCA versus full self-attention on synthetic datasets requiring high-order item-item dependency modeling

**Open Question 2:** What is the maximum extrapolation ratio the "train sparsely, infer densely" strategy can sustain before generalization fails?
- Basis: Section 3.3 demonstrates success at 5x ratio (2k to 10k) but provides no theoretical upper bound
- Why unresolved: Unclear if strategy implicitly teaches positional curriculum or relies on local context window overlap
- What evidence would resolve it: Evaluating performance degradation as inference lengths extend to 20k, 50k, and 100k while keeping training at 2k

**Open Question 3:** Does the observed monotonic scaling with sequence length and capacity persist indefinitely, or does it saturate?
- Basis: Abstract and Figure 1 suggest "scaling law behavior," but experiments limited to 133M parameters and 10k length
- Why unresolved: Scaling law claim lacks theoretical backing found in LLM literature
- What evidence would resolve it: Training significantly larger STCA modules (>500M params) and tracking AUC lift per unit of added compute

## Limitations

- Heavy dependence on proprietary data and production environments at Douyin scale, making full reproducibility challenging
- Length-extrapolation strategy may not generalize to domains with fundamentally different temporal dynamics or interaction patterns
- Upper bounds of approach untested - unclear whether performance would degrade attempting to extrapolate from 2k to 50k sequences

## Confidence

**High confidence:** STCA's linear complexity claim (verified by computational analysis), RLB's systems efficiency gains (clear mechanism), 3.3-6.3% online metric improvements (reported with specific numbers)

**Medium confidence:** Length-extrapolation strategy's effectiveness (relies on stochastic sampling without extensive ablation), generalization without retraining at 10k sequences (only one inference length tested)

**Low confidence:** Exact implementation details of RankMixer and precise impact of query fusion parameters (referenced to external work)

## Next Checks

1. **Ablation Study on Sequence Length Distribution:** Systematically vary Beta distribution parameters (α, β) and L_avg during training to quantify trade-off between training efficiency and inference performance at 10k sequences

2. **Stress Test RLB Efficiency Gains:** Measure actual CPU-GPU bandwidth savings and memory footprint reduction when increasing m (targets per user) from 1 to 32 to determine point of diminishing returns

3. **Cross-Domain Generalization:** Implement STCA + RLB on non-video recommendation dataset (e.g., e-commerce with longer purchase histories) to test whether length-extrapolation strategy generalizes beyond Douyin's specific user behavior patterns