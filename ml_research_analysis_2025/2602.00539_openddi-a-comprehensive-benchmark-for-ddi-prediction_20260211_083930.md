---
ver: rpa2
title: 'OpenDDI: A Comprehensive Benchmark for DDI Prediction'
arxiv_id: '2602.00539'
source_url: https://arxiv.org/abs/2602.00539
tags:
- drug
- prediction
- datasets
- interaction
- openddi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenDDI, a comprehensive benchmark for Drug-Drug
  Interaction (DDI) prediction. The primary challenges addressed include the lack
  of high-quality data and standardized evaluation protocols in existing DDI prediction
  studies.
---

# OpenDDI: A Comprehensive Benchmark for DDI Prediction

## Quick Facts
- arXiv ID: 2602.00539
- Source URL: https://arxiv.org/abs/2602.00539
- Reference count: 40
- Primary result: OpenDDI unifies 6 widely used DDI datasets and 2 existing drug representations, contributes 3 new large-scale LLM-augmented datasets and a multimodal drug representation covering 5 modalities, and establishes standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency in DDI prediction.

## Executive Summary
This paper introduces OpenDDI, a comprehensive benchmark for Drug-Drug Interaction (DDI) prediction that addresses critical gaps in the field including lack of high-quality data and standardized evaluation protocols. OpenDDI unifies existing datasets and drug representations while introducing new LLM-augmented datasets and multimodal representations. The benchmark integrates 20 state-of-the-art algorithms across 3 downstream tasks, establishing standardized protocols for evaluation across multiple dimensions. Comprehensive evaluation using OpenDDI reveals 11 valuable insights for DDI prediction and demonstrates significant expansion in scale and diversity of available resources.

## Method Summary
OpenDDI was developed by unifying 6 widely used DDI datasets and 2 existing drug representations, while contributing 3 new large-scale LLM-augmented datasets and a multimodal drug representation covering 5 modalities. The benchmark integrates 20 state-of-the-art algorithms across 3 downstream tasks, establishing standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. The comprehensive evaluation framework was designed to assess DDI prediction methods across these standardized protocols, revealing valuable insights about current limitations and performance characteristics of existing approaches.

## Key Results
- Unified 6 widely used DDI datasets and 2 existing drug representations into a standardized benchmark
- Contributed 3 new large-scale LLM-augmented datasets and multimodal drug representation covering 5 modalities
- Conducted comprehensive evaluation revealing 11 valuable insights for DDI prediction and exposing current limitations

## Why This Works (Mechanism)
OpenDDI addresses fundamental challenges in DDI prediction by creating a unified, standardized evaluation framework that overcomes data quality inconsistencies and protocol fragmentation. By integrating multiple datasets and representation methods, it provides a more robust and comprehensive assessment environment that better reflects real-world complexity. The inclusion of LLM-augmented data and multimodal representations expands the scope of features available for prediction, while standardized protocols ensure fair comparison across different algorithmic approaches.

## Foundational Learning
- **Data standardization**: Why needed - inconsistent data quality across sources affects model performance; Quick check - verify annotation consistency across unified datasets
- **Multimodal representation**: Why needed - single-modality approaches miss important drug characteristics; Quick check - assess contribution of each modality to prediction accuracy
- **Benchmark protocols**: Why needed - lack of standardization hinders progress comparison; Quick check - validate protocols across diverse algorithm types
- **Generalization assessment**: Why needed - models must perform well beyond training data; Quick check - test performance on independent clinical datasets
- **Robustness evaluation**: Why needed - real-world data contains noise and variations; Quick check - test under adversarial conditions and varying data quality
- **Efficiency metrics**: Why needed - practical deployment requires resource consideration; Quick check - measure computational requirements across algorithms

## Architecture Onboarding
- **Component map**: Datasets (6 unified + 3 LLM-augmented) -> Drug Representations (2 existing + 1 multimodal) -> Algorithms (20 SOTA) -> Evaluation Tasks (3 downstream) -> Protocols (quality, effectiveness, generalization, robustness, efficiency)
- **Critical path**: Data preparation -> Feature extraction -> Model training -> Protocol-based evaluation -> Insight generation
- **Design tradeoffs**: Balance between dataset diversity and consistency, between representation richness and computational efficiency, between protocol comprehensiveness and practical usability
- **Failure signatures**: Inconsistent performance across datasets indicates data quality issues; poor generalization suggests overfitting; high resource requirements limit practical deployment
- **First experiments**: 1) Validate unified dataset consistency across sources; 2) Test multimodal representation contribution to accuracy; 3) Assess LLM-augmented data reliability and bias characteristics

## Open Questions the Paper Calls Out
None

## Limitations
- Quality and consistency concerns across the 6 unified datasets due to varying original annotation standards
- Uncertainty about reliability and generalizability of LLM-augmented datasets due to potential model hallucinations
- Insufficient exploration of relative importance and contribution of each modality in multimodal representation

## Confidence
- Data quality standardization protocols: Medium confidence
- LLM-augmented dataset reliability: Low confidence
- Multimodal representation effectiveness: Medium confidence
- Algorithm integration comprehensiveness: High confidence
- Protocol standardization robustness: Medium confidence
- Clinical translation potential: Low confidence

## Next Checks
1. Conduct external validation using independent clinical datasets to verify the generalizability of OpenDDI-based models beyond the benchmark datasets
2. Perform systematic analysis of the LLM-augmented datasets to quantify and characterize any biases or inconsistencies introduced by the generation process
3. Test the robustness of OpenDDI protocols under adversarial conditions and varying data quality scenarios to ensure reliability in practical applications