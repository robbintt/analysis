---
ver: rpa2
title: 'Don''t Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense
  Retrieval'
arxiv_id: '2504.21015'
source_url: https://arxiv.org/abs/2504.21015
tags:
- hard
- negatives
- retrieval
- bm25
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes generating synthetic hard negatives for training
  dense retrieval models using large language models (LLMs), eliminating the need
  for corpus access. The authors fine-tune DistilBERT with synthetic negatives generated
  by four LLMs ranging from 4B to 30B parameters and evaluate performance across 10
  BEIR benchmark datasets.
---

# Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval

## Quick Facts
- arXiv ID: 2504.21015
- Source URL: https://arxiv.org/abs/2504.21015
- Reference count: 14
- Primary result: Corpus-free LLM-generated hard negatives underperform traditional BM25/cross-encoder mining by 12-18% relative nDCG@10

## Executive Summary
This study investigates whether large language models can generate synthetic hard negatives for training dense retrieval models without requiring corpus access. The author fine-tunes DistilBERT on synthetic negatives produced by four LLMs (4B-30B parameters) and evaluates performance across 10 BEIR benchmark datasets. Contrary to expectations, the generative pipeline consistently underperforms traditional corpus-based mining strategies, with the best standalone LLM (Phi4-14B) achieving an average nDCG@10 of 0.314 compared to BM25's 0.359 and cross-encoder's 0.366. Notably, scaling the generator model does not improve retrieval performance monotonically—the 14B model outperforms the 30B model in some settings. The study concludes that while corpus-free synthetic data generation is possible, it currently underperforms corpus-mined negatives and requires advanced filtering or integration strategies for practical use.

## Method Summary
The study fine-tunes DistilBERT for dense retrieval using hard negatives from three sources: BM25 indexing, cross-encoder re-ranking, and LLM generation. Four LLMs (Qwen3-4B, Qwen3-30B, LLaMA3-8B, Phi4-14B) generate 5 hard negatives per query-positive pair with specific sampling parameters (temp=0.6, top-p=0.95, top-k=20, max_tokens=1024). The training uses Multiple-Negative Ranking Loss with a 90/10 train-validation split, early stopping after 3 consecutive non-improving evaluations, and batch size of 16. Evaluation occurs on 10 BEIR datasets using nDCG@10 as the primary metric. The study compares standalone LLM-generated negatives, combinations with BM25, combinations with cross-encoder, and aggregated datasets.

## Key Results
- Standalone LLM-generated negatives underperform BM25 (0.314 vs 0.359 nDCG@10) and cross-encoder (0.366) baselines
- Generator model scale does not improve performance monotonically—14B model outperforms 30B model
- Naive concatenation of synthetic and retrieved negatives fails to improve performance consistently
- The best standalone LLM (Phi4-14B) achieves average nDCG@10 of 0.314 across 10 BEIR datasets

## Why This Works (Mechanism)

### Mechanism 1: Hard Negative Contrastive Learning
Hard negatives—passages semantically similar to queries but irrelevant—provide stronger training signals than random negatives for dense retrievers. Contrastive objectives pull query-positive pairs closer in embedding space while pushing negative passages away, creating tighter decision boundaries. The informativeness of the gradient signal depends on negative difficulty; trivial negatives provide weak supervision.

### Mechanism 2: Corpus-Grounded Negative Mining
Hard negatives mined from real document corpora capture the actual distribution of confusable passages better than synthetically generated ones. BM25 and cross-encoders retrieve passages that exist in the target retrieval space—they share vocabulary, style, length, and topical distributions with real corpus documents, grounding training in realistic "distractors."

### Mechanism 3: LLM Generation Quality-Difficulty Mismatch
LLM-generated hard negatives underperform because synthetic passages do not match the difficulty distribution of corpus-mined negatives, despite scaling model size. LLMs may produce negatives that are either too easy (obviously irrelevant) or effectively false positives (actually relevant), misaligning with the "hard" requirement. The 14B model outperforming 30B suggests generation quality doesn't scale with parameter count for this task.

## Foundational Learning

- **Concept: Contrastive Learning / Triplet Loss**
  - Why needed here: The entire training pipeline uses Multiple-Negative Ranking Loss to learn query-passage embeddings. Understanding why hard negatives matter requires grasping the push-pull dynamics of contrastive objectives.
  - Quick check question: Given a query about "photosynthesis," which is a better hard negative: a passage about car engines, or a passage about plant respiration that doesn't mention sunlight?

- **Concept: Dense Retrieval vs. Sparse Retrieval**
  - Why needed here: The paper fine-tunes DistilBERT as a dense encoder. You need to distinguish between BM25's lexical matching and embedding-based semantic similarity to interpret why corpus-mined negatives might transfer better.
  - Quick check question: Why would BM25 hard negatives help train a dense retriever if they use different similarity functions?

- **Concept: nDCG@10 and BEIR Benchmark**
  - Why needed here: All performance claims use nDCG@10 across 10 BEIR datasets. Understanding this metric is essential to interpret the 0.314 vs. 0.359 gap as meaningful.
  - Quick check question: If a model retrieves 10 documents and the relevant one is at position 10, how does nDCG penalize this compared to having it at position 1?

## Architecture Onboarding

- **Component map:**
  ```
  [Input: Query + Positive Passage]
           ↓
  ┌──────────────────────────────────────┐
  │  Hard Negative Generator (3 options)  │
  │  1. BM25 → Corpus Index              │
  │  2. Cross-Encoder → Corpus Re-rank   │
  │  3. LLM → Synthetic Generation       │
  └──────────────────────────────────────┘
           ↓
  [Training Triplets: (Query, Positive, Negatives)]
           ↓
  [DistilBERT Fine-tuning with MNRL Loss]
           ↓
  [Evaluation: 10 BEIR datasets → nDCG@10]
  ```

- **Critical path:** The hard negative quality directly determines downstream retrieval performance. The paper isolates this by keeping the encoder (DistilBERT), loss function (MNRL), and evaluation (BEIR) constant while varying only the negative source.

- **Design tradeoffs:**
  - Corpus access vs. portability: BM25/CE require index construction; LLM approach is corpus-free but underperforms by 12-18% relative nDCG
  - Scale vs. quality: 30B LLM doesn't beat 14B; diminishing returns suggest prompt/task alignment matters more than raw capacity
  - Combination strategy: Naive concatenation can degrade performance (e.g., "All LLMs + Cross-Encoder" at 0.307 < "All LLMs" at 0.292 is anomalous; overall mixed results)

- **Failure signatures:**
  - Standalone LLM negatives: nDCG 0.247–0.314 (all below BM25's 0.359)
  - 30B model worst-performing in some configurations
  - Concatenation producing inconsistent gains—"fine-tuning on the full combination... results in worse retrieval performance than fine-tuning on standalone LLM-generated hard negatives" for most models except LLaMA3-8B

- **First 3 experiments:**
  1. Reproduce the baseline gap: Fine-tune DistilBERT with BM25 negatives on MS-MARCO sample, evaluate on 2-3 BEIR datasets (e.g., FEVER, NFCorpus) to verify you can achieve ~0.35 range before testing LLM approaches
  2. Ablate LLM negative quality: Manually inspect 50 LLM-generated negatives from Phi4-14B vs. Qwen3-30B. Label each as "too easy," "appropriately hard," or "false positive" to test the difficulty-mismatch hypothesis
  3. Test filtered combination: Instead of naive concatenation, implement a cross-encoder filter on LLM negatives before mixing with BM25. Compare against the paper's "All LLMs + BM25" result of 0.343 to see if curation helps

## Open Questions the Paper Calls Out

- What specific filtering or integration strategies can successfully combine synthetic LLM negatives with corpus-mined negatives to outperform traditional baselines?
- Why does generator model scale fail to correlate monotonically with synthetic data quality, causing larger models (30B) to underperform smaller ones (14B)?
- To what extent does the phrasing of the generation prompt influence the efficacy of synthetic hard negatives compared to the choice of the generator model?

## Limitations

- Dataset bias: All experiments use MS-MARCO passages and BEIR evaluation; performance on proprietary or non-English corpora may differ substantially
- DistilBERT capacity ceiling: The encoder architecture may not fully leverage high-quality synthetic negatives; scaling the encoder alongside generators could yield different results
- Single epoch fine-tuning: While standard practice, longer training or curriculum learning with synthetic negatives might improve outcomes

## Confidence

**High Confidence**: The empirical finding that BM25/CE-mined hard negatives outperform LLM-generated ones across all tested configurations (directly observable from nDCG@10 results: 0.359/0.366 vs. 0.247-0.314 range)

**Medium Confidence**: The mechanism explanation—that LLM-generated negatives don't match corpus-mined negatives' difficulty distribution (data shows performance doesn't scale with model size, but doesn't definitively prove this is due to quality mismatch)

**Low Confidence**: That corpus access is always required for effective dense retrieval training (paper shows current approaches fail without corpus access, but future prompt engineering or filtering strategies might overcome this limitation)

## Next Checks

1. **Manual quality audit**: Randomly sample 100 LLM-generated negatives from each generator and 100 BM25 negatives. Have three annotators label each as "too easy," "appropriately hard," "false positive," or "unclear." This directly tests whether the quality distribution differs systematically.

2. **Cross-encoder filtering validation**: Implement a post-generation filter using the same cross-encoder from the baseline. Only keep LLM-generated negatives that score below a threshold (e.g., 0.4 similarity). Compare the filtered set's nDCG@10 against the paper's "All LLMs + Cross-Encoder" result of 0.307 to verify if filtering helps.

3. **Encoder capacity stress test**: Repeat the best-performing configuration (Phi4-14B standalone) but with a larger encoder (e.g., MiniLM-L6-v3 from the cross-encoder baseline). If the gap to BM25 narrows, this suggests DistilBERT is the bottleneck rather than synthetic negatives being inherently inferior.