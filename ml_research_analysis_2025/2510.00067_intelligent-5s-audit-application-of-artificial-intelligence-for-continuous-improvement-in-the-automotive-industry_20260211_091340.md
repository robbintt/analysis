---
ver: rpa2
title: 'Intelligent 5S Audit: Application of Artificial Intelligence for Continuous
  Improvement in the Automotive Industry'
arxiv_id: '2510.00067'
source_url: https://arxiv.org/abs/2510.00067
tags:
- system
- audit
- automated
- automotive
- audits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an AI-driven automated 5S audit system for\
  \ automotive manufacturing environments, addressing the challenge of time-intensive\
  \ and subjective manual audits. The method employs a multimodal large language model\
  \ (GPT-4 Turbo) with structured prompt engineering to evaluate the five 5S components\u2014\
  Seiri, Seiton, Seiso, Seiketsu, and Shitsuke\u2014via intelligent image analysis."
---

# Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry

## Quick Facts
- arXiv ID: 2510.00067
- Source URL: https://arxiv.org/abs/2510.00067
- Reference count: 7
- System achieved Cohen's kappa of 0.75 agreement with human auditors

## Executive Summary
This paper presents an AI-driven automated 5S audit system for automotive manufacturing environments, addressing the challenge of time-intensive and subjective manual audits. The method employs a multimodal large language model (GPT-4 Turbo) with structured prompt engineering to evaluate the five 5S components—Seiri, Seiton, Seiso, Seiketsu, and Shitsuke—via intelligent image analysis. Validation against human auditors achieved a Cohen's kappa of 0.75, indicating substantial agreement. The system reduced audit time by 50% (20 minutes vs. 60 minutes), increased frequency capacity (from 20 to over 100 audits/month), and lowered operating costs by 99.8% (R$75.00 to R$0.17 per audit). It also achieved ROI within one month, demonstrating technical reliability and strong economic feasibility.

## Method Summary
The system captures daily images from fixed camera positions in automotive manufacturing sectors, encodes them in base64 format, and transmits them to GPT-4 Turbo API with structured prompts defining 5S evaluation criteria. The LLM processes visual data to identify non-conformities and scores each of the five senses on a 1-5 scale. Scores are automatically extracted via regex parsing of structured text responses. The Python-based system includes GUI (Tkinter), PDF report generation (ReportLab), and visualizations (Matplotlib). Validation compared AI-generated scores against human auditor assessments using Cohen's kappa coefficient across 75 images collected over 5 working days.

## Key Results
- Achieved Cohen's kappa of 0.75 agreement with human auditors (substantial agreement)
- Reduced audit time by 50% (20 minutes vs. 60 minutes per audit)
- Lowered operating costs by 99.8% (R$75.00 to R$0.17 per audit)
- Enabled audit frequency increase from 20 to over 100 audits per month

## Why This Works (Mechanism)

### Mechanism 1: Multimodal LLM Visual Analysis for 5S Compliance Detection
- Claim: A multimodal large language model can evaluate industrial workspace compliance against 5S criteria through image analysis with substantial agreement to human auditors.
- Mechanism: Images encoded in base64 format are transmitted to LLM API. The model processes visual data to identify non-conformities based on learned representations of industrial environments. Scores are extracted via regex parsing of structured text responses.
- Core assumption: The model's pre-training includes sufficient visual examples of organized industrial environments to recognize deviations without fine-tuning.
- Evidence anchors: System demonstrated high consistency with minimal variation between repeated evaluations (standard deviation < 0.3 points). Performance degrades significantly in low-light conditions (κ=0.72 for Seiketsu affected by poor lighting).
- Break condition: Performance degrades significantly in low-light conditions or when contextual knowledge outside the prompt is required (8% disagreement from contextual ambiguity).

### Mechanism 2: Structured Prompt Engineering for Standardized Scoring
- Claim: Domain-specific prompt architecture enables consistent, repeatable 5S assessments by constraining model outputs to a fixed 1-5 scoring rubric per sense.
- Mechanism: A specialized prompt establishes the AI as a "5S audit specialist for automotive manufacturing" and defines explicit evaluation criteria for each sense with numerical scoring bounds. This reduces output variability and enables automated parsing.
- Core assumption: Prompt-based constraints produce sufficiently consistent outputs that regex extraction reliably captures scores without human verification.
- Evidence anchors: The system demonstrated high consistency, with minimal variation between repeated evaluations of the same image (standard deviation < 0.3 points on a scale of 1-5).
- Break condition: Environmental variability causes score instability, particularly for Seiton (κ=0.65, lowest performing component).

### Mechanism 3: Temporal Inference for Abstract Discipline Assessment
- Claim: The abstract concept of Shitsuke (discipline) can be inferred through temporal consistency patterns across the other four 5S components.
- Mechanism: Rather than directly observing discipline, the system compiles historical compliance data across Seiri, Seiton, Seiso, and Seiketsu. Consistent high scores over time indicate disciplined adherence to standards.
- Core assumption: Sustained compliance across measurable senses is a valid proxy for the cultural/behavioral construct of discipline.
- Evidence anchors: Shitsuke (Discipline)-κ=0.71: The system inferred discipline by analyzing temporal consistency across other senses. Although this proved effective, it was inherently limited by the duration of observation.
- Break condition: Short observation windows reduce inference accuracy; discipline scores become unreliable without sufficient longitudinal data.

## Foundational Learning

- **Cohen's Kappa Coefficient (κ)**
  - Why needed here: Primary validation metric quantifying agreement between AI and human auditors beyond chance. Understanding κ=0.75 as "substantial agreement" (Landis-Koch scale: 0.61-0.80) is essential for interpreting system reliability claims.
  - Quick check question: If two evaluators agree 90% of the time but chance agreement is 80%, what is κ? (Answer: 0.50)

- **5S Methodology Hierarchy**
  - Why needed here: Each S has distinct visual indicators and inference difficulty. Seiri (tangible items) is easier to detect than Shitsuke (behavioral pattern). This explains performance variance (κ=0.83 to κ=0.65).
  - Quick check question: Which 5S component would be hardest to assess from a single static image and why? (Answer: Shitsuke—requires temporal/behavioral inference)

- **Base64 Image Encoding for API Transmission**
  - Why needed here: Understanding the data pipeline—images must be encoded, transmitted to external API, and responses parsed. This creates dependencies (connectivity, rate limits, cost per request) that shape system architecture.
  - Quick check question: What happens if the API rate limit is exceeded mid-batch? (Answer: Retry system activates with 3-second delays; 98.7% success rate achieved)

## Architecture Onboarding

- **Component map:**
Camera (fixed position) → Image Capture (daily, end-of-shift) → Base64 Encoding (PIL library) → API Request (GPT-4 Turbo) + Structured Prompt → Response Parsing (Regex extraction) → Score Aggregation (5 senses × 5 points each) → PDF Report Generation (ReportLab) + Visualizations (Matplotlib)

- **Critical path:** Image quality → Prompt clarity → API response completeness → Regex extraction accuracy. A failure at regex parsing triggers default value assignment (score=0), which can skew results.

- **Design tradeoffs:**
  - External API dependency (R$0.17/audit) vs. self-hosted model (higher upfront, lower marginal cost, more maintenance)
  - Generalist model (no fine-tuning required) vs. potential accuracy gains from domain-specific training
  - Fixed camera positions (consistent framing) vs. mobile capture (more coverage, higher variability)

- **Failure signatures:**
  - Inadequate lighting: 12% of disagreements—manifests as missed stains/disorganization in shadows
  - Contextual ambiguity: 8% of disagreements—AI lacks local knowledge about acceptable temporary arrangements
  - Elements out of frame: 3% of disagreements—fixed camera cannot capture all relevant zones
  - Regex parsing failure: Incomplete scores default to 0; check logs for extraction errors

- **First 3 experiments:**
  1. **Lighting calibration test:** Capture identical scenes under varying light conditions (bright, shadowed, mixed). Measure κ degradation per condition to establish minimum lighting thresholds for deployment.
  2. **Prompt sensitivity analysis:** Systematically vary prompt wording (e.g., add specific examples of non-conformities) and measure impact on κ scores. Target: identify prompt changes that raise Seiton κ above 0.70.
  3. **Temporal window validation:** Run 30-day extended study with daily captures. Compare Shitsuke inference accuracy against human behavioral observations to validate the temporal consistency proxy mechanism.

## Open Questions the Paper Calls Out

- **Sector Expansion and Validation:** How does the system's reliability (kappa score) vary when applied to visually distinct automotive sectors, such as painting or stamping, compared to the general assembly environment tested?
- **Personalization and Contextual Adaptation:** Can fine-tuning the AI model on factory-specific data reduce the 8% error rate attributed to "contextual ambiguity"?
- **Predictive Models and Preventive Maintenance:** To what extent can multimodal sensor data (vibration, air quality) be integrated with visual analysis to predict 5S degradation before it becomes visually apparent?
- **Technical Challenges and Improvements:** What specific image pre-processing or hardware modifications are required to mitigate the 12% disagreement rate caused by inadequate lighting conditions?

## Limitations

- System's reliance on external LLM APIs introduces potential variability in scoring that is difficult to audit or debug
- Performance degrades under non-ideal imaging conditions (lighting, shadows, partial views)
- Fixed camera setup may not capture all critical workspace zones
- Abstract assessment of Shitsuke through temporal inference remains a heuristic rather than direct observation

## Confidence

- **High confidence**: Time/cost savings metrics (50% time reduction, 99.8% cost reduction, ROI within one month)
- **Medium confidence**: Cohen's kappa agreement (κ=0.75) - substantial but subject to variance across senses and environmental factors
- **Medium confidence**: Shitsuke inference validity - demonstrated effectiveness but limited by observation window length

## Next Checks

1. **Extended temporal validation:** Deploy for 90+ days with daily captures to validate Shitsuke inference accuracy against longitudinal human behavioral observations, establishing minimum observation windows for reliable discipline assessment.
2. **Environmental robustness testing:** Systematically vary lighting conditions during image capture to quantify κ degradation thresholds and establish minimum illumination requirements for deployment.
3. **Prompt optimization study:** Conduct A/B testing with alternative prompt formulations to determine if Seiton κ can be raised above 0.70 and overall system consistency improved.