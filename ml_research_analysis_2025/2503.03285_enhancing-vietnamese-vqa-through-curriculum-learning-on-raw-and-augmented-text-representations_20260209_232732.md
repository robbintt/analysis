---
ver: rpa2
title: Enhancing Vietnamese VQA through Curriculum Learning on Raw and Augmented Text
  Representations
arxiv_id: '2503.03285'
source_url: https://arxiv.org/abs/2503.03285
tags:
- training
- samples
- learning
- question
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving Visual Question
  Answering (VQA) performance for Vietnamese, a low-resource language, by proposing
  a training framework that combines paraphrase-based feature augmentation with dynamic
  curriculum learning. The key idea is to treat augmented samples (created via paraphrasing)
  as "easy" and original samples as "hard", and to dynamically adjust the ratio of
  these samples during training.
---

# Enhancing Vietnamese VQA through Curriculum Learning on Raw and Augmented Text Representations

## Quick Facts
- arXiv ID: 2503.03285
- Source URL: https://arxiv.org/abs/2503.03285
- Reference count: 10
- Primary result: Improved VQA performance on Vietnamese using paraphrase-based augmentation with dynamic curriculum learning

## Executive Summary
This paper addresses the challenge of improving Visual Question Answering (VQA) performance for Vietnamese, a low-resource language, by proposing a training framework that combines paraphrase-based feature augmentation with dynamic curriculum learning. The key idea is to treat augmented samples (created via paraphrasing) as "easy" and original samples as "hard", and to dynamically adjust the ratio of these samples during training. This allows the model to gradually adapt to increasing task complexity. Experiments on the OpenViVQA dataset show consistent improvements across different model backbones, with notable gains in CIDEr scores and reduced standard deviation. Results on ViVQA are mixed, likely due to dataset limitations. The approach demonstrates effectiveness in leveraging external knowledge without additional annotated data and offers a promising direction for VQA in low-resource languages.

## Method Summary
The proposed framework trains a dual-stream VQA model (text encoder + image encoder) using a curriculum that gradually shifts from paraphrase-augmented "easy" samples to raw "hard" samples. A paraphrase pool is generated offline using mT5 fine-tuned for Vietnamese paraphrasing. During training, the model randomly selects between using the original question embedding or a weighted sum of original and paraphrased embeddings, with the probability of selecting augmented samples decaying linearly from 1.0 to 0.8 over 40 epochs. The augmentation module uses learned projection layers to fuse embeddings, while the curriculum scheduler updates the selection threshold based on the current epoch.

## Key Results
- OpenViVQA CIDEr score improves from 0.5656 to 0.5874 using BARTpho backbone
- ViVQA accuracy improves from 0.5432 to 0.5554 using BARTpho backbone
- Standard deviation across 5 runs decreases from 0.0113 to 0.0094 on OpenViVQA
- Ablation shows curriculum learning with threshold decay (1.0→0.8) outperforms fixed thresholds

## Why This Works (Mechanism)

### Mechanism 1: Semantic Smoothing via Paraphrase Fusion
- **Claim:** Fusing original question embeddings with paraphrased variants creates "easy" samples that stabilize early training by providing a denser semantic signal.
- **Mechanism:** The model generates a pool of paraphrases offline. During the forward pass, it randomly samples k paraphrases, embeds them, and sums their weighted representations with the original question embedding.
- **Evidence anchors:** Ablation study shows accuracy increasing from 0.5432 to 0.5538 as paraphrase count increases from 0 to 2.
- **Break condition:** If paraphrase model generates low-quality or noisy candidates, the "richness" becomes "noise," likely degrading the baseline.

### Mechanism 2: Progressive Distribution Alignment via Curriculum Learning
- **Claim:** Dynamically shifting the ratio of "easy" (augmented) to "hard" (raw) samples prevents the model from overfitting to synthetic data while ensuring it eventually adapts to the raw test distribution.
- **Mechanism:** A stochastic threshold t_thresh governs whether the model uses the augmented or raw embedding for a given batch. This threshold decays linearly over epochs, systematically reducing the probability of seeing "easy" augmented samples as training concludes.
- **Evidence anchors:** Fixed threshold of 1.0 (all augmented) performs worst (0.5283), validating the need to eventually expose the model to raw data.
- **Break condition:** If the decay schedule ends too abruptly or the minimum threshold is too low (e.g., 0.0), the model may lose the generalization benefits of the augmented data.

### Mechanism 3: Multimodal Consistency Regularization
- **Claim:** Training on paraphrased variants forces the visual encoder to align with a broader set of linguistic descriptors, implicitly regularizing the visual-semantic space.
- **Mechanism:** By keeping the image input fixed while varying the text input (via paraphrase fusion), the model must map multiple linguistic representations to the same visual features.
- **Evidence anchors:** Improved CIDEr scores (0.5874 vs 0.5656) suggest the generated answers align better with reference consensus, potentially due to this regularization.

## Foundational Learning

- **Concept: Curriculum Learning**
  - **Why needed here:** This is the core training paradigm. You must understand that "easy" vs. "hard" is defined here by data augmentation status (augmented vs. raw), not output length or syntax.
  - **Quick check question:** If t_thresh = 0.8 at epoch 1 and decays to 0.4 at epoch 40, is the model seeing *more* or *fewer* raw samples at the end of training? (Answer: More).

- **Concept: Dual-Stream VQA Architectures**
  - **Why needed here:** The method modifies the text stream specifically. You need to distinguish between the text encoder (PhoBERT/BARTpho) and image encoder (ResNet/BEiTv2) to implement the augmentation module correctly.
  - **Quick check question:** Where is the augmentation module inserted—before the text encoder or after it? (Answer: After, as it fuses pre-computed embeddings).

- **Concept: Stochastic Training Layers**
  - **Why needed here:** The framework uses a random variable x ~ U(0, 1) to decide which embedding to use per forward pass. Understanding this stochastic depth-like behavior is crucial for debugging reproducibility.
  - **Quick check question:** During inference, do we still use the random selector x? (Answer: No, only the non-augmented branch is active).

## Architecture Onboarding

- **Component map:** Paraphrase Pool (Offline) -> Text Encoder (PhoBERT/BARTpho) -> Augmentation Module (Linear layers W_P, W_O) -> Stochastic Gate (random x~U(0,1)) -> Curriculum Scheduler (updates t_thresh) -> Multimodal Fusion (Concatenation + Classifier)

- **Critical path:** The data pipeline depends on the Paraphrase Pool being pre-built. If paraphrases are missing, the Augmentation Module will crash. The Scheduler must be strictly synchronized with the epoch counter to ensure the "easy-to-hard" transition.

- **Design tradeoffs:**
  - **Paraphrase Count (n):** Increasing n improves accuracy (0.5432 → 0.5538) but significantly increases VRAM usage due to larger batch computations. The paper settles on n=2.
  - **Decay Strategy:** Linear decay (1.0 → 0.8) yields best results (0.5583 Acc). Dropping threshold too low (e.g., 0.0) hurts performance.

- **Failure signatures:**
  - **Low Test Accuracy + High Train Accuracy:** Likely caused by setting minimum threshold too high (model overfits to augmented styles not present in test set).
  - **Training Instability:** Observed if the decay is too aggressive or if the paraphrase pool contains noisy/contradictory data.
  - **VRAM OOM:** Occurs if batch size is large and paraphrase count n > 2.

- **First 3 experiments:**
  1. **Sanity Check (Baseline):** Run model with t_thresh = 0.0 (no augmentation) to establish a baseline accuracy (e.g., 0.5432 on ViVQA).
  2. **Fixed Augmentation:** Set a fixed threshold (e.g., t_thresh = 0.8) to verify the augmentation module is functioning and adding value *during* training.
  3. **Curriculum Validation:** Implement the Linear Decay scheduler (1.0 → 0.8). Compare final accuracy against the fixed threshold run to confirm the curriculum effect.

## Open Questions the Paper Calls Out

- **Does augmenting visual features alongside textual features further enhance the curriculum learning framework's convergence and robustness?**
  - **Basis in paper:** [explicit] The Conclusion states that future work "could explore augmentation techniques for the image channel and design efficient strategies to train with both augmented image and text modalities."
  - **Why unresolved:** The current study strictly augments the text modality (using paraphrasing) while keeping image features static, limiting the exploration of multi-modal synergy.
  - **What evidence would resolve it:** Experimental results comparing the current text-only augmentation against a dual-modality augmentation pipeline on the OpenViVQA dataset.

- **Can a learnable, adaptive mechanism replace the manually defined curriculum schedule to optimize the ratio of easy to hard samples?**
  - **Basis in paper:** [explicit] The Conclusion proposes "developing a learnable approach to optimize hyperparameters, rather than relying on manually determined constants."
  - **Why unresolved:** The current linear decay strategy depends on fixed threshold constants (t_max, t_min), which may not be optimal for all training stages or datasets.
  - **What evidence would resolve it:** Implementation of a meta-learning or reinforcement learning agent that dynamically adjusts thresholds based on real-time validation performance.

- **Does the "easy-to-hard" curriculum assumption hold for languages with morphological structures significantly different from Vietnamese?**
  - **Basis in paper:** [explicit] The Conclusion suggests "extending the proposed framework to additional languages would validate its effectiveness across diverse linguistic contexts."
  - **Why unresolved:** The method is tailored for Vietnamese; it is unclear if paraphrase-based augmentation consistently creates "easy" samples in languages with different syntactic properties.
  - **What evidence would resolve it:** Successful replication of the training improvements on low-resource datasets from non-isolating language families (e.g., agglutinative or inflected languages).

## Limitations

- Performance improvements rely heavily on the quality of mT5-generated paraphrases, which are not provided in the repository and may vary significantly across different fine-tuning data or random seeds.
- Results on ViVQA are mixed (Accuracy: 0.5554), suggesting the approach may not generalize uniformly across datasets with different question distributions.
- The fixed curriculum schedule (1.0→0.8) was not optimized across the full parameter space of decay functions, minimum thresholds, or paraphrase counts, leaving open the possibility of better configurations.

## Confidence

- **High confidence** in the methodology description and reproducibility of the augmentation + curriculum framework. The mathematical formulation (Eq. 8-11) and implementation details (layer dimensions, optimizer settings) are sufficiently specified.
- **Medium confidence** in the attribution of performance gains specifically to the curriculum learning mechanism rather than the augmentation alone, since the paper doesn't fully isolate these effects in ablation studies.
- **Low confidence** in the generalization of results to other low-resource languages or VQA datasets, given the mixed performance across the two Vietnamese datasets tested.

## Next Checks

1. **Paraphrase Quality Audit**: Generate paraphrases using the same mT5 fine-tuning procedure (or closest available Vietnamese paraphrase model) and measure semantic similarity between original and paraphrased questions using metrics like BERTScore or SBERT similarity. This validates whether the "easy" samples genuinely preserve meaning.

2. **Curriculum Schedule Sensitivity**: Run experiments with alternative decay functions (exponential, step-wise) and minimum thresholds (0.4, 0.6) to determine whether the linear 1.0→0.8 schedule is optimal or merely sufficient.

3. **Cross-Dataset Generalization Test**: Apply the exact same framework to a non-Vietnamese VQA dataset (e.g., VQA v2.0) with language-appropriate paraphrase generation to test whether the curriculum learning benefits transfer beyond the specific Vietnamese context.