---
ver: rpa2
title: 'SonicBench: Dissecting the Physical Perception Bottleneck in Large Audio Language
  Models'
arxiv_id: '2601.11039'
source_url: https://arxiv.org/abs/2601.11039
tags:
- audio
- attributes
- tasks
- recognition
- comparison
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SonicBench, a psychophysically grounded benchmark
  to evaluate large audio language models' (LALMs) physical perception capabilities
  across 12 core audio attributes spanning five perceptual dimensions. SonicBench
  employs a controllable generation toolbox to construct stimuli for recognition (absolute
  judgment) and comparison (relative judgment) tasks, ensuring attribute differences
  are well above human perceptual thresholds.
---

# SonicBench: Dissecting the Physical Perception Bottleneck in Large Audio Language Models

## Quick Facts
- **arXiv ID:** 2601.11039
- **Source URL:** https://arxiv.org/abs/2601.11039
- **Reference count:** 40
- **Primary result:** Most LALMs perform near chance on physical perception tasks, with the bottleneck in alignment/decoding rather than audio encoding.

## Executive Summary
This work introduces SonicBench, a psychophysically grounded benchmark to evaluate large audio language models' (LALMs) physical perception capabilities across 12 core audio attributes spanning five perceptual dimensions. SonicBench employs a controllable generation toolbox to construct stimuli for recognition (absolute judgment) and comparison (relative judgment) tasks, ensuring attribute differences are well above human perceptual thresholds. Evaluation of 36 models reveals that most LALMs perform near chance accuracy on physical perception tasks, show no systematic advantage on comparison tasks unlike humans, and exhibit minimal gains from explicit reasoning. Crucially, linear probing demonstrates that frozen audio encoders already capture these physical cues (accuracy ≥60%), indicating the primary bottleneck lies in alignment and decoding stages rather than perception. This highlights the need to optimize how LALMs leverage encoded physical signals for robust real-world audio understanding.

## Method Summary
SonicBench evaluates LALMs on 12 physical audio attributes through recognition (absolute judgment on 4s clips) and comparison (relative judgment on 8.5s concatenated clips) tasks. The dataset contains 2,400 question-audio pairs (1,200 per task type) with attribute differences set above human perceptual thresholds. Models are evaluated zero-shot with greedy decoding (input ≤2048 tokens, output ≤1024 tokens). A linear probing study isolates the representation capability of frozen audio encoders from the utilization capability of LLMs, using a two-layer lightweight linear probe trained with AdamW (LR=3e-3, cosine decay) on a 50/50 train-eval split.

## Key Results
- Most LALMs achieve near-chance accuracy (≈50%) on physical perception tasks despite frozen encoders capturing physical cues (≥60% accuracy via linear probing)
- Models fail to show the human advantage on comparison tasks, performing worse or equal to recognition tasks
- Explicit reasoning yields minimal gains, with inference-time scaling providing only marginal improvements
- The primary bottleneck is identified as alignment and decoding stages rather than audio perception capability

## Why This Works (Mechanism)

### Mechanism 1: Alignment-Induced Information Loss
The failure of LALMs to solve physical perception tasks stems primarily from the alignment and decoding stages, not the audio encoder's ability to represent physical features. Pre-trained audio encoders successfully encode acoustic properties like pitch and loudness into latent states, but the projection layer or LLM backbone fails to preserve or route these features to the final output token. Linear probes on frozen encoders achieve ≥60% accuracy while end-to-end models cluster around 50%, confirming information loss occurs after encoding.

### Mechanism 2: Absence of Relational Processing
LALMs do not exhibit the "comparison advantage" seen in humans because they lack mechanisms to explicitly compare discrete audio segments. Human psychophysics shows relative judgment is easier than absolute estimation due to contrastive processing. LALMs likely treat comparison tasks as sequence classification rather than differential judgment, failing to leverage the 0.5s gap between clips to segment and contrast features.

### Mechanism 3: Reasoning-Ungrounded Hallucination
Explicit reasoning (Chain-of-Thought) fails to improve physical perception because the LLM generates reasoning traces based on linguistic priors rather than grounding them in acoustic evidence. When upstream perception is weak (near-chance), the reasoning module fabricates plausible acoustic logic based on text training data rather than correcting perceptual errors, resulting in confident but incorrect rationales.

## Foundational Learning

- **Concept: Linear Probing**
  - **Why needed here:** To isolate the representation capability of the encoder from the utilization capability of the LLM, identifying where the bottleneck occurs
  - **Quick check question:** If a linear classifier on frozen encoder outputs achieves 80% accuracy but the full model achieves 50%, where is the bottleneck? (Answer: Alignment/Decoding)

- **Concept: Psychophysical Just-Noticeable Difference (JND)**
  - **Why needed here:** SonicBench constructs stimuli based on human perceptual thresholds to ensure tasks are "trivially easy" for humans, guaranteeing that model errors are representational failures, not ambiguous data
  - **Quick check question:** Why does SonicBench ensure attribute differences are "well above human JNDs"? (Answer: To rule out sensory ambiguity and confirm model failure is due to lack of physical grounding)

- **Concept: Modality Alignment Gap**
  - **Why needed here:** The "Alignment" phase (Audio Encoder -> LLM) is identified as the specific point of failure, critical for understanding how to optimize LALMs
  - **Quick check question:** Does freezing the audio encoder during LLM training help or hurt physical perception according to the paper? (Answer: Preserving encoder quality is vital, but current alignment strategies lose the information)

## Architecture Onboarding

- **Component map:** Audio Waveform -> Encoder (e.g., Whisper, BEATs) -> Projector/Adaptor (e.g., Linear Layer, Q-Former) -> LLM Backbone (e.g., Qwen, Llama) -> Text Token (A/B)
- **Critical path:** The Encoder Output -> Projector -> LLM Input path is where signal degradation occurs. Linear probing confirms the encoder is functional; E2E failure confirms the right side is broken
- **Design tradeoffs:**
  - Frozen vs. Unfrozen Encoder: Freezing preserves physical attributes but may limit task-specific adaptation; E2E training with unfrozen encoders shows modest gains but risks catastrophic forgetting
  - Task Complexity: Designing for semantic understanding (current SOTA focus) seems to negatively impact physical perception
- **Failure signatures:**
  - The "Comparison Inversion": If accuracy on Comparison tasks ≤ Recognition accuracy, the model lacks relational reasoning
  - The "Probe Gap": If Probe Accuracy >> E2E Accuracy, the alignment layer is the bottleneck
- **First 3 experiments:**
  1. Replicate the Probe: Train a linear classifier on your specific audio encoder's outputs using SonicBench subset; confirm ≥60% accuracy to verify representation quality
  2. Ablate the Projector: Replace standard linear projector with deeper or attention-based adaptor to see if preserving more detail improves E2E physical perception
  3. Inference-Time Scaling Check: Toggle "Think" mode on/off for specific attributes like Pitch vs. Counting to verify if reasoning actively harms performance (hallucination) or helps

## Open Questions the Paper Calls Out

### Open Question 1
What specific alignment strategies or architectural modifications are required to effectively transfer physical perceptual cues from the audio encoder to the LLM decoder? The paper identifies the adaptor/LLM interface as the bottleneck but doesn't propose a specific mechanism to solve representational degradation. Evidence would be a new training objective or adaptor architecture achieving accuracy comparable to linear probe baseline.

### Open Question 2
How can audio encoders be modified to support relational reasoning, allowing models to exploit the comparative structure of audio signals? Current encoders lack mechanisms to explicitly compare distinct temporal segments, unlike humans who excel at comparison tasks. Evidence would be an architectural change (e.g., native cross-segment attention) resulting in systematic accuracy advantage on comparison tasks over recognition tasks.

### Open Question 3
Does inference-time reasoning (CoT) reliably improve physical perception, or does it primarily risk "reasoning-induced errors" when upstream perception is uncertain? The paper shows reasoning can be logically coherent but acoustically ungrounded. Evidence would be a "grounding" mechanism that constrains reasoning steps to verified acoustic features, resulting in consistent performance improvements in "think" mode.

## Limitations
- The paper identifies the alignment bottleneck but doesn't definitively prove whether it stems from projector architecture, LLM token-level processing, or insufficient multimodal training objectives
- Linear probe architecture's simplicity (two-layer MLP) may underestimate the complexity needed for full task performance
- Evaluation relies on zero-shot settings, leaving open whether few-shot prompting or fine-tuning could partially overcome the identified bottleneck

## Confidence
- **High confidence:** Frozen audio encoders capture physical cues (probe accuracy ≥60%) while end-to-end models perform near chance
- **Medium confidence:** Comparison tasks should be easier than recognition tasks for models as they are for humans
- **Medium confidence:** Explicit reasoning minimally improves performance and doesn't compensate for weak perception

## Next Checks
1. Test whether deeper or attention-based linear probes achieve higher accuracy than simple two-layer MLP, establishing minimal representation quality needed for task success
2. Implement modified model with explicit cross-attention between two audio segments in comparison tasks to test whether relational processing can recover human comparison advantage
3. Train reasoning module to attend to specific encoder attention maps or spectral features, then test whether this grounded reasoning can correct perceptual errors rather than hallucinate explanations