---
ver: rpa2
title: 'Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments
  of Language Models'
arxiv_id: '2506.11068'
source_url: https://arxiv.org/abs/2506.11068
tags:
- obligation
- modal
- expressions
- llms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models exhibit a systematic bias toward judging
  non-obligatory contexts as obligations when prompts include modal expressions such
  as must or ought to. This phenomenon, termed Deontological Keyword Bias (DKB), leads
  models to assign obligation judgments to over 90% of commonsense scenarios when
  such expressions are present.
---

# Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models

## Quick Facts
- arXiv ID: 2506.11068
- Source URL: https://arxiv.org/abs/2506.11068
- Authors: Bumjin Park; Jinsil Lee; Jaesik Choi
- Reference count: 39
- Large language models systematically over-predict obligations when prompts include modal expressions

## Executive Summary
This study identifies a systematic bias in large language models called Deontological Keyword Bias (DKB), where the presence of modal expressions like "must" or "ought to" leads models to judge non-obligatory contexts as obligations at rates exceeding 90%. The bias persists across various model families, question types, and answer formats, revealing a fundamental vulnerability in how language models process normative judgments. To address this issue, the authors propose a judgment strategy combining few-shot examples with reasoning prompts that effectively reduces the tendency to over-predict obligations.

## Method Summary
The study employs a systematic evaluation framework using commonsense scenarios where models are prompted with both obligation and non-obligation contexts, with and without modal expressions. Multiple model families are tested across different question types and answer formats to establish the pervasiveness of DKB. The proposed mitigation strategy integrates few-shot examples demonstrating correct judgment patterns with reasoning prompts that guide models toward more nuanced analysis. Results are quantified through comparative analysis of obligation prediction rates across experimental conditions.

## Key Results
- Modal expressions cause models to assign obligation judgments to over 90% of commonsense scenarios
- DKB persists across various model families, question types, and answer formats
- Proposed mitigation strategy effectively reduces over-prediction of obligations

## Why This Works (Mechanism)
The mechanism behind DKB appears to stem from language models' training on large corpora where modal expressions frequently co-occur with obligation-related content. When encountering phrases like "must" or "ought to," models may activate learned associations between these expressions and normative content, leading to systematic over-prediction of obligations regardless of the actual context. This reflects a form of linguistic framing effect where the presence of certain keywords overrides contextual understanding.

## Foundational Learning
- **Deontological ethics**: Understanding duty-based moral frameworks provides context for why obligation judgments matter in normative reasoning
- **Linguistic framing effects**: Knowledge of how language structure influences interpretation helps explain DKB's mechanism
- **Few-shot learning**: Understanding this technique is crucial for evaluating the proposed mitigation strategy
- **Normative judgment evaluation**: Framework for assessing moral reasoning quality in AI systems

Why needed: These concepts establish the theoretical foundation for understanding both the problem (DKB) and the solution approach.
Quick check: Can you explain how modal expressions might bias moral judgments in both humans and AI systems?

## Architecture Onboarding
Component map: Input prompts -> Language model processing -> Output generation -> Evaluation metrics
Critical path: Prompt formulation → Model inference → Judgment classification → Bias measurement
Design tradeoffs: Balancing prompt specificity against generalizability; few-shot examples versus reasoning prompts
Failure signatures: Systematic over-prediction of obligations when modal expressions are present
First experiments:
1. Test baseline obligation prediction rates without modal expressions
2. Measure DKB magnitude across different model families
3. Evaluate mitigation strategy effectiveness with varying numbers of few-shot examples

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation focuses primarily on commonsense scenarios without exploring complex ethical dilemmas
- Mitigation strategy analysis doesn't explore trade-offs in accuracy for non-obligation judgments
- Cross-cultural and multilingual variations in DKB are not investigated

## Confidence
- High: Primary claims about DKB's existence and pervasiveness across models
- Medium: General applicability of proposed mitigation strategy
- Low: Transferability of findings to real-world applications involving nuanced moral reasoning

## Next Checks
1. Test the mitigation strategy across diverse ethical domains (e.g., medical ethics, legal reasoning, professional codes of conduct) to assess generalizability
2. Conduct human evaluation studies comparing model judgments with human normative reasoning in identical scenarios to establish ground truth alignment
3. Investigate the impact of linguistic framing effects across different languages and cultural contexts to identify potential cross-cultural variations in DKB