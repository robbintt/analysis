---
ver: rpa2
title: A New Benchmark for the Appropriate Evaluation of RTL Code Optimization
arxiv_id: '2601.01765'
source_url: https://arxiv.org/abs/2601.01765
tags:
- optimization
- design
- designs
- synthesis
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RTL-OPT, a new benchmark designed to evaluate
  the optimization capabilities of large language models (LLMs) for Register Transfer
  Level (RTL) code. Unlike existing benchmarks that focus only on syntactic correctness,
  RTL-OPT provides 36 handcrafted digital design tasks with realistic suboptimal and
  optimized RTL code pairs, derived from industry-proven optimization patterns.
---

# A New Benchmark for the Appropriate Evaluation of RTL Code Optimization

## Quick Facts
- arXiv ID: 2601.01765
- Source URL: https://arxiv.org/abs/2601.01765
- Reference count: 40
- Primary result: Introduces RTL-OPT, a benchmark showing existing RTL optimization evaluations are misleading due to synthesis tool dependencies, and demonstrates LLMs still struggle to match human-optimized PPA improvements

## Executive Summary
This paper introduces RTL-OPT, a new benchmark designed to evaluate the optimization capabilities of large language models (LLMs) for Register Transfer Level (RTL) code. Unlike existing benchmarks that focus only on syntactic correctness, RTL-OPT provides 36 handcrafted digital design tasks with realistic suboptimal and optimized RTL code pairs, derived from industry-proven optimization patterns. An integrated evaluation framework verifies functional correctness and quantifies power, performance, and area (PPA) improvements using standard EDA tools. Experimental results show that while LLMs like DeepSeek R1 achieve some PPA gains, many still fail to match human-optimized designs, highlighting the benchmark's effectiveness in exposing real optimization challenges.

## Method Summary
RTL-OPT provides 36 RTL design pairs with suboptimal and optimized versions based on six categories of industry-validated optimization patterns. The evaluation framework uses commercial synthesis (Design Compiler compile_ultra) with 1ns timing constraints, followed by formal equivalence checking (Formality) against the optimized reference, then dynamic simulation (VCS) for timing-dependent behavior. PPA metrics are extracted and compared against both suboptimal and optimized references. The benchmark also tests with weaker synthesis (Yosys) to verify RTL-level differences persist through optimization.

## Key Results
- Prior benchmarks like QAMP show optimized designs often achieve no PPA improvement or worse under industrial synthesis settings (only 13/43 better under compile_ultra)
- RTL-OPT shows 35/36 designs achieving better PPA than suboptimal versions under the same conditions
- DeepSeek R1 achieves best PPA improvements but has highest functional error rate (38.9% failure), demonstrating the trade-off between optimization aggressiveness and correctness

## Why This Works (Mechanism)

### Mechanism 1
RTL-OPT's benchmark tasks retain genuine optimization opportunities that persist through aggressive industrial synthesis, unlike prior benchmarks that conflate superficial code changes with true optimization. The benchmark embeds six categories of industry-validated optimization patterns (bit-width optimization, precomputation/LUT conversion, operator strength reduction, control simplification, resource sharing, state encoding optimization) that represent structural transformations synthesis tools cannot automatically infer. These patterns create inefficiencies at the architectural level rather than local algebraic redundancies. Core assumption: Human expert-optimized RTL implementing these patterns represents achievable ground truth for LLM optimization targets. Evidence anchors: [abstract] states each task provides "human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools," [section 3.2] details pattern categories and synthesis-resistant properties. Break condition: If synthesis tools evolve to automatically infer these patterns, the benchmark's optimization gap would diminish.

### Mechanism 2
The evaluation of RTL optimization quality is fundamentally dependent on synthesis tool, optimization mode, and timing constraints, creating a configuration space where misleading conclusions are easily produced. Synthesis tools apply varying optimization aggressiveness. Commercial tools like Design Compiler with compile_ultra aggressively restructure logic, potentially eliminating differences between suboptimal and optimized RTL. Open-source tools like Yosys provide weaker optimization, retaining more differences but poorly reflecting industrial outcomes. Tighter timing constraints force timing-driven optimizations that may trade area/power for performance. Core assumption: Industrial synthesis with aggressive settings (DC compile_ultra) is the appropriate reference standard for evaluating RTL optimization quality. Evidence anchors: [section 2.1] notes "Different tools adopt distinct optimization strategies... Commercial tools such as Synopsys Design Compiler (DC) typically perform more aggressive and sophisticated optimizations," [table 1] shows prior benchmark's "optimized" designs often show same or worse PPA than suboptimal under compile_ultra. Break condition: If synthesis tools converge in optimization capability, this mechanism's importance would decrease.

### Mechanism 3
LLMs exhibit a trade-off between optimization aggressiveness and functional correctness, where models achieving better PPA improvements also tend to introduce more functional errors. Models employing aggressive transformation strategies (code restructuring, resource sharing, pipelining) explore larger solution spaces but risk misinterpreting subtle design semantics. Conservative models preserve functionality but achieve fewer meaningful optimizations. Core assumption: The observed correlation between optimization aggressiveness and functional errors is inherent to current LLM architectures/training. Evidence anchors: [section 4.1] shows "Deepseek R1 generally outperforms the other models in terms of PPA. However, Deepseek R1 also exhibits a higher rate of functional discrepancies," [section 4.2] identifies three failure modes: control logic inconsistencies, overly aggressive pipelining, and improper resource sharing. Break condition: If future LLMs incorporate formal verification in their generation loop, this trade-off could shift.

## Foundational Learning

### RTL (Register Transfer Level) and PPA Metrics
Why needed: The entire benchmark operates at RTL abstraction and measures success via Power, Performance (WNS/TNS), and Area. Understanding that RTL describes hardware at the cycle-accurate register-to-register transfer level is essential to interpreting the benchmark's evaluation criteria. Quick check: Given a simple combinational adder RTL module, if you double the bit-width, how would you expect area and power to change? What synthesis metrics would capture these changes?

### Logic Synthesis and Technology Mapping
Why needed: The paper's central critique is that synthesis tool behavior determines whether RTL differences manifest in final PPA. Understanding that synthesis transforms behavioral RTL into technology-mapped gate-level netlists, and that tools vary in optimization aggressiveness (Yosys vs. DC compile vs. DC compile_ultra), is essential to interpreting the benchmark results. Quick check: If two RTL implementations produce identical gate-level netlists after synthesis with compile_ultra, what does this imply about the "optimization" in the RTL source?

### Functional Equivalence Verification
Why needed: RTL-OPT requires that optimized designs preserve exact functional equivalence to the suboptimal input. Understanding that formal equivalence checking (e.g., Synopsys Formality) compares combinational logic cones and sequential behavior, while dynamic simulation (VCS) validates timing-dependent behavior, is critical to understanding why "optimization" can fail verification. Quick check: If an LLM re-pipelines a datapath, reducing latency from 3 cycles to 2 cycles, would formal equivalence checking pass? What about the benchmark's functional verification?

## Architecture Onboarding
- Component map: [Suboptimal RTL Input] → [LLM Agent] → [LLM-Optimized RTL] → [Synthesis Tool (DC/Yosys)] → [Gate-Level Netlist] → [Formal Equivalence Checking vs. Reference] → [Pass/Fail] → [PPA Extraction] → [Comparison vs. Suboptimal & Optimized References]
- Critical path: The functional equivalence check is the primary gate. If verification fails, PPA comparison is meaningless. The benchmark correctly treats functional correctness as a prerequisite for optimization evaluation.
- Design tradeoffs: Synthesis tool choice (DC compile_ultra for industrial relevance vs. Yosys for accessibility), timing constraint selection (tight vs. relaxed), pattern coverage vs. benchmark size (36 designs cover 6 optimization pattern categories but remain limited)
- Failure signatures: Control logic inconsistencies (incorrect Boolean conditions), overly aggressive pipelining (violating latency requirements), improper resource sharing (register reuse causing stale data)
- First 3 experiments: 1) Baseline calibration: Run all 36 suboptimal RTL files through synthesis flow and verify PPA extraction matches Table 5 values, 2) Synthesis sensitivity probe: Take 3-5 design pairs, synthesize with Yosys, DC compile, and DC compile_ultra, quantify PPA gap variation, 3) LLM optimization pilot with functional gate: Run a single LLM (GPT-4o-mini) on 5 suboptimal RTL inputs, synthesize, run equivalence check against reference optimized RTL, extract PPA for passed designs

## Open Questions the Paper Calls Out

### Open Question 1
How can LLMs maintain functional correctness while aggressively optimizing RTL code to achieve better PPA metrics? Basis in paper: [inferred] Section 4.2 notes DeepSeek R1 achieves superior PPA but suffers higher functional discrepancy rates compared to more conservative models like GPT-4o-mini. Why unresolved: Current models often conflate aggressive logic restructuring with semantic-altering changes, failing to preserve exact behavior during optimization. What evidence would resolve it: A new LLM agent that consistently passes formal equivalence checking while surpassing the golden reference PPA on all 36 tasks.

### Open Question 2
Do the PPA improvements reported at the synthesis level persist after physical implementation (place and route)? Basis in paper: [inferred] The methodology (Section 3.3) evaluates PPA strictly at the logic synthesis stage using Design Compiler, without accounting for physical effects like routing congestion or wire delays. Why unresolved: Logic restructuring that benefits synthesis may degrade post-route timing or power due to actual wire loads versus statistical wire-load models. What evidence would resolve it: A study correlating RTL-OPT synthesis gains with final post-place-and-route results using a complete physical design flow.

### Open Question 3
Can optimization patterns derived from the 45nm technology library generalize effectively to advanced process nodes? Basis in paper: [inferred] Section 3.3.1 acknowledges the use of the Nangate45 library and states that "the choice of library significantly impacts the RTL optimization process." Why unresolved: Optimization heuristics (e.g., operator strength reduction) may yield different PPA trade-offs in advanced nodes (e.g., 5nm) where interconnect delay dominates cell delay. What evidence would resolve it: Successful replication of the RTL-OPT optimization margins using advanced commercial standard cell libraries.

## Limitations
- The benchmark uses a single technology library (Nangate45) and may not generalize to advanced process nodes
- Only 36 design tasks are provided, which may not capture the full diversity of industrial RTL optimization challenges
- The evaluation relies on specific synthesis tool settings that may not represent all industrial design flows

## Confidence
- High confidence: The synthesis-tool-dependency mechanism is strongly supported by empirical evidence showing prior benchmarks produce misleading results under industrial synthesis settings
- Medium confidence: The industry pattern coverage provides genuine optimization opportunities that synthesis cannot automatically capture, though the completeness of the 6-pattern taxonomy remains unverified
- Medium confidence: The LLM trade-off between optimization aggressiveness and functional correctness is demonstrated but may be specific to the benchmark's design set and evaluation conditions

## Next Checks
1. Test the same 3-5 design pairs from the benchmark across multiple synthesis tools (Yosys, DC compile, DC compile_ultra) to quantify how PPA gaps vary and verify the synthesis-dependency mechanism independently
2. Evaluate additional LLM models beyond those tested in the paper to determine if the observed trade-off between PPA improvement and functional correctness persists across different architectures and training approaches
3. Verify that the 6 optimization pattern categories are indeed synthesis-resistant by attempting to synthesize hand-optimized examples with aggressive compile_ultra settings and measuring PPA improvements relative to suboptimal versions