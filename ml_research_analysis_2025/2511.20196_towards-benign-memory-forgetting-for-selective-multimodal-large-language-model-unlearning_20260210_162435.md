---
ver: rpa2
title: Towards Benign Memory Forgetting for Selective Multimodal Large Language Model
  Unlearning
arxiv_id: '2511.20196'
source_url: https://arxiv.org/abs/2511.20196
tags:
- unlearning
- image
- forgetting
- score
- smfa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of unlearning sensitive information
  from multimodal large language models (MLLMs) without degrading their general image
  understanding abilities. It proposes a novel approach called Sculpted Memory Forgetting
  Adapter (SMFA), which combines refusal label-based fine-tuning with a retaining
  anchor-guided masking mechanism to selectively forget targeted knowledge while preserving
  unrelated capabilities.
---

# Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning

## Quick Facts
- **arXiv ID**: 2511.20196
- **Source URL**: https://arxiv.org/abs/2511.20196
- **Reference count**: 40
- **Primary result**: Proposes SMFA, achieving superior selective unlearning in MLLMs while preserving general image understanding on a novel benchmark.

## Executive Summary
This paper tackles the challenge of unlearning sensitive information from multimodal large language models (MLLMs) without degrading their general image understanding capabilities. The authors propose Sculpted Memory Forgetting Adapter (SMFA), a novel approach that combines refusal label-based fine-tuning with a retaining anchor-guided masking mechanism. SMFA generates a modular forgetting adapter that can be selectively sculpted to avoid interference with unrelated knowledge. To evaluate this, the authors introduce S-MLLMUn Bench, the first benchmark designed to assess both forgetting effectiveness and image understanding retention in MLLM unlearning. Experiments demonstrate that SMFA outperforms existing methods by maintaining coherent outputs and robust image understanding while effectively removing sensitive information.

## Method Summary
SMFA addresses selective unlearning by first fine-tuning an MLLM on sensitive data with refusal responses to generate a Memory Forgetting Adapter (MFA). It then trains a Retaining Anchor (RA) on a small set of non-sensitive data to define parameters for preserving general knowledge. A masking mechanism combines directional conflict and relative magnitude criteria to sculpt the MFA, removing parameters that conflict with the RA. The final unlearned model is produced by an additive merge of the base model with the sculpted adapter. The method is evaluated on S-MLLMUn Bench, which includes synthetic image-text profiles, forget sets, retain sets, and image understanding tasks, using metrics like ROUGE-L, Fact Score, and Meaningful Score.

## Key Results
- SMFA achieves superior selective unlearning compared to baselines like GA Difference and IDK Tuning
- Maintains coherent outputs and robust image understanding while effectively removing sensitive information
- Demonstrates effective trade-off between forgetting sensitive data and preserving general capabilities

## Why This Works (Mechanism)

### Mechanism 1: Refusal Label-Based Fine-Tuning (MFA Generation)
Fine-tuning an MLLM on sensitive data with refusal responses (e.g., "I don't know") generates a Memory Forgetting Adapter (MFA) that encodes the shift towards refusal behavior. This modular parameter update isolates the forgetting signal but can cause over-generalization to non-target queries.

### Mechanism 2: Retaining Anchor-Guided Masking (SMFA Sculpting)
A retaining anchor trained on non-sensitive data defines parameters for preserving general knowledge. Masking parameters in the MFA that conflict with this anchor (via directional conflict and relative magnitude criteria) selectively prunes the forgetting update, preventing degradation of unrelated capabilities.

### Mechanism 3: Modularity via Additive Weight Merging
Treating the forgetting update as a modular adapter allows it to be sculpted in isolation before being cleanly merged back with the base model. This preserves the base model's fundamental capabilities while applying only the curated forgetting update.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: The core problem SMFA solves is preventing catastrophic forgetting of general image understanding while unlearning specific facts.
  - Quick check question: If a model is fine-tuned on a new task (A) using all its parameters, what typically happens to its performance on its original task (B)?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper implements SMFA and baselines using LoRA, which confines updates to low-rank adapter matrices.
  - Quick check question: In LoRA, are the pre-trained model weights ($W_o$) frozen or updated during fine-tuning? Where is the learned change stored?

- **Concept: Task Arithmetic / Model Merging**
  - Why needed here: SMFA conceptualizes forgetting as a task vector ($\Delta W_f$) that can be added to ($W_o$), applying task arithmetic principles.
  - Quick check question: If you have a model fine-tuned for task A and another for task B, how might "model merging" or "task arithmetic" combine them into a single model?

## Architecture Onboarding

- **Component map**: Base MLLM ($f_\theta$) -> LoRA Layers -> Forget Set ($D_f$) -> Retain Set ($D_r^{few}$) -> S-MLLMUn Bench
- **Critical path**:
  1. Prepare Data: Construct $D_f$, $D_r^{few}$, and refusal label set
  2. Train MFA: Fine-tune base model on $D^{idk}_f \cup D_r^{few}$, extract adapter weights $\Delta W_f$
  3. Train RA: Fine-tune a separate copy on $D_r^{few}$, extract adapter weights $\Delta W_a$
  4. Sculpt MFA: Compute masks $C$ and $R$ from $\Delta W_f$ and $\Delta W_a$, apply mask $M$ to $\Delta W_f$ to get $\Delta W'_f$
  5. Final Model: Merge $\Delta W'_f$ with base model weights $W_o$
- **Design tradeoffs**:
  - Forgetting vs. Retention (controlled by $k$): Higher $k$ = more masking = less forgetting but better retention
  - Anchor Size ($D_r^{few}$): Small set is efficient but may not cover all knowledge to be preserved
  - Refusal Label Diversity: Diverse pool promotes more natural and coherent refusals
- **Failure signatures**:
  - Over-forgetting: Model refuses almost all queries, even simple ones
  - Under-forgetting: Model still answers sensitive queries correctly
  - Degenerate Outputs: Model outputs nonsense (e.g., "is is is is")
- **First 3 experiments**:
  1. Baseline Check: Run GA Difference and IDK Tuning to confirm their trade-off
  2. Ablation Study: Run SMFA variants with only directional conflict or only relative magnitude masking
  3. Hyperparameter Scan: Sweep on masking parameter $k$ to visualize trade-off curve

## Open Questions the Paper Calls Out

- **Open Question 1**: How does SMFA's performance degrade as unlearned data volume significantly exceeds the 15% ratio tested? The paper notes performance degradation becomes more severe with increased forgetting data, but scalability remains unexplored.

- **Open Question 2**: Can the retaining anchor-guided masking strategy generalize to real-world data with complex semantic entanglements? The current benchmark uses synthetic data, bypassing the noisy nature of real-world pre-training data.

- **Open Question 3**: To what extent is the stability of unlearning dependent on the domain diversity of the few-shot retaining anchor set? The paper assumes MLLM generalization allows limited anchor signals to propagate but doesn't test if poorly chosen anchors fail to prevent over-generalization.

## Limitations

- Key implementation details like LoRA configuration (rank, alpha, learning rate, epochs, batch size) are not specified
- Exact templates and prompts for fine-tuning data and evaluation metrics are not provided
- The assumption that weight-level conflicts directly translate to task-level interference is not rigorously validated
- Reliance on a single masking threshold $k$ may not handle diverse MLLM forgetting tasks optimally

## Confidence

- **High Confidence**: The modular architecture of SMFA is clearly defined and the conceptual framework is sound, supported by experimental results on S-MLLMUn Bench
- **Medium Confidence**: Experimental results showing SMFA's superiority are compelling, but missing implementation details introduce uncertainty about exact conditions
- **Low Confidence**: The core assumption that weight-level directional conflicts directly correlate with task-level interference lacks rigorous proof

## Next Checks

1. **Mechanistic Validation of Masking**: Conduct an ablation study isolating the contribution of each masking criterion (directional conflict vs. relative magnitude) to validate both components are necessary for performance gains.

2. **Hyperparameter Sensitivity Analysis**: Perform a systematic sweep of the masking parameter $k$ (e.g., $k \in \{0.1, 0.5, 1, 5, 10, 50\}$) and plot the trade-off curve between forgetting rate and retention.

3. **Task-Level Interference Validation**: Design a controlled experiment to directly test whether parameter-level conflicts correlate with task-level interference by artificially manipulating the conflict between MFA and RA.