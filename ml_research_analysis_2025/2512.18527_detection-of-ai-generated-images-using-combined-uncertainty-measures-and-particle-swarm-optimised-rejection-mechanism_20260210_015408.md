---
ver: rpa2
title: Detection of AI Generated Images Using Combined Uncertainty Measures and Particle
  Swarm Optimised Rejection Mechanism
arxiv_id: '2512.18527'
source_url: https://arxiv.org/abs/2512.18527
tags:
- uncertainty
- images
- fisher
- rejection
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a detection framework that leverages multiple\
  \ uncertainty measures\u2014Fisher Information, Monte Carlo Dropout entropy, and\
  \ Gaussian Process predictive variance\u2014to distinguish AI-generated images from\
  \ natural ones. These measures are combined using Particle Swarm Optimisation to\
  \ learn optimal weightings and an adaptive rejection threshold."
---

# Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism

## Quick Facts
- arXiv ID: 2512.18527
- Source URL: https://arxiv.org/abs/2512.18527
- Reference count: 39
- Primary result: Combined uncertainty measures with PSO-optimized rejection achieve ~70% incorrect rejection rate on unseen generative models and 61% rejection of adversarial attacks

## Executive Summary
This paper proposes a framework for detecting AI-generated images by combining three uncertainty measures—Fisher Information, Monte Carlo Dropout entropy, and Gaussian Process predictive variance—using Particle Swarm Optimisation to learn optimal weights and a rejection threshold. The system is trained on Stable Diffusion-generated images and evaluated under distribution shifts from other generators like Midjourney and GLIDE, as well as adversarial attacks. While standard classification metrics degrade under shift, the combined uncertainty measure consistently filters misclassified AI samples with around 70% incorrect rejection rate. Under adversarial attacks, it rejects 61% of successful attacks, outperforming individual uncertainty measures.

## Method Summary
The method fine-tunes a ResNet50 backbone (and ViT) with a custom classification head on Stable Diffusion images versus real images. Three uncertainty measures are computed per image: Fisher Information on classification head parameters, MC Dropout entropy with 20 forward passes, and GP predictive variance via Deep Kernel Learning with 1000 inducing points. These are z-score normalized, combined with PSO-learned weights and threshold to maximize correct prediction acceptance and incorrect prediction rejection across both classes. The rejection mechanism is evaluated on OOD generators and under FGSM/PGD adversarial attacks.

## Key Results
- Combined uncertainty achieves ~70% incorrect rejection rate on unseen generative models under distribution shift
- Rejects 61% of adversarial attacks (FGSM and PGD) while maintaining high acceptance of correct predictions
- Standard classification accuracy degrades under distribution shift, but uncertainty-based rejection maintains robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fusing orthogonal uncertainty signals using PSO creates a more robust boundary for detecting OOD AI images than any single metric.
- **Mechanism:** The framework calculates three distinct uncertainty types and uses PSO to learn optimal weights and a rejection threshold. This forces the system to reject samples that exhibit non-conformity across multiple dimensions of uncertainty.
- **Core assumption:** Individual uncertainty metrics fail under different conditions; their combined "consensus" on uncertainty is a stronger signal for anomaly detection.
- **Evidence anchors:**
  - [abstract] "...integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings..."
  - [section] Page 11, Eq. (40): PSO minimizes the negative score of CPA% + IPR% to find $(w^*, \tau^*)$.
- **Break condition:** If the uncertainty measures are highly correlated, the fusion offers diminishing returns without added robustness.

### Mechanism 2
- **Claim:** Fisher Information acts as a "local sensitivity" detector, identifying samples that would significantly shift model parameters if included in training.
- **Mechanism:** By computing the Fisher Information Matrix on the classification head per image, the method measures the curvature of the loss landscape. High Fisher Information suggests the image is "influential" or potentially OOD.
- **Core assumption:** Natural images and in-domain AI images produce consistent parameter gradient responses, while images from unseen generators create divergent gradient signals.
- **Evidence anchors:**
  - [abstract] "Fisher Information... captures the sensitivity of model parameters to input variations..."
  - [section] Page 6, Eq. (1)-(3): Definition of per-instance Fisher Information and its accumulation via gradients.
- **Break condition:** If the backbone feature extractor is invariant to the artifacts of a new generator, the gradients will be small regardless of the image's "fakeness," resulting in false negatives.

### Mechanism 3
- **Claim:** Gaussian Process variance provides a calibrated uncertainty score that rises for inputs far from the training data manifold, offering superior defense against adversarial attacks.
- **Mechanism:** A Deep Kernel Learning framework maps image features to a latent space where a GP classifier assigns predictive variance. Samples falling into low-density regions yield high variance, triggering rejection.
- **Core assumption:** Adversarial perturbations push images off the learned data manifold into regions of high predictive variance.
- **Evidence anchors:**
  - [abstract] "...predictive variance from a Deep Kernel Learning framework... Under adversarial attacks... GP-based uncertainty alone achieves up to 80%."
  - [section] Page 23: "Gaussian Process uncertainty achieves the highest IPR (80.45%)... effective at rejecting incorrect predictions."
- **Break condition:** If the adversarial attack constrains the perturbation to stay on the data manifold, GP variance may not rise, and the attack will go undetected.

## Foundational Learning

- **Concept: Gaussian Process (GP) Inducing Points**
  - **Why needed here:** Exact GP inference is computationally impossible for image datasets. Understanding inducing points is required to grasp how the authors approximate the posterior distribution to make the uncertainty calculation feasible for large-scale image data.
  - **Quick check question:** How does reducing the number of inducing points likely affect the model's ability to estimate uncertainty for highly novel AI generators?

- **Concept: Fisher Information Matrix (FIM)**
  - **Why needed here:** This is the statistical backbone of one of the primary uncertainty signals. You must understand that the FIM approximates the local curvature of the loss landscape to see why "high curvature" implies the model is sensitive to (and uncertain about) a specific image.
  - **Quick check question:** Why is the FIM calculated only on the custom classification head rather than the entire ResNet50 backbone?

- **Concept: Particle Swarm Optimisation (PSO)**
  - **Why needed here:** The system does not use fixed weights. PSO is the "meta-brain" that dynamically searches for the best combination of weights to balance the trade-off between accepting correct predictions and rejecting incorrect ones.
  - **Quick check question:** If the PSO is optimizing purely for "Incorrect Prediction Rejection" (IPR), what unwanted side effect might occur regarding "Correct Prediction Acceptance" (CPA)?

## Architecture Onboarding

- **Component map:**
  - Input: Image (Real/AI)
  - Backbone: ResNet50 (frozen) → Custom Classification Head
  - Parallel Uncertainty Heads:
    1. **Fisher Head:** Computes gradients of loss w.r.t. parameters → FIM → Trace/Frobenius/Entropy
    2. **GP Head:** Deep Kernel Learning (DKL) → Sparse GP → Predictive Variance
    3. **MC Dropout Head:** Stochastic forward passes → Entropy of Expected
  - Fusion Layer: PSO-optimized weighted sum of normalized uncertainty scores
  - Decision: Compare Combined Score vs. Threshold τ → Accept or Reject

- **Critical path:** The PSO optimization loop is the most critical failure point. The entire system relies on the assumption that the weights derived from the training set generalize to unseen distributions.

- **Design tradeoffs:**
  - ResNet50 is the primary focus; ViT showed slightly lower baseline performance but was tested for generalizability
  - The combined uncertainty metric is tuned to be conservative (high rejection rate)
  - The "Combined" approach outperforms individual metrics on generalization but underperforms single metrics on specific adversarial attacks

- **Failure signatures:**
  - **High False Negative Rate:** The model rejects valid "Nature" images. This implies the rejection threshold τ is too aggressive or the GP/Fisher metrics are miscalibrated for the natural image distribution.
  - **Adversarial Collapse:** If PGD attacks drop accuracy to near-zero without a spike in rejection rates, the uncertainty metrics are not capturing the perturbation.
  - **Metric Drift:** If Fisher Information is near zero for all inputs, check gradient propagation.

- **First 3 experiments:**
  1. **Verify Backbone Separability:** Train the ResNet50 custom head on Stable Diffusion vs. Nature. If accuracy is <95%, do not proceed to uncertainty fusion; the base features are insufficient.
  2. **PSO Weight Validation:** Run the PSO loop on a validation set of Stable Diffusion. Verify that the weights do not collapse to a single metric, which would indicate the other metrics are dead.
  3. **OOD Stress Test:** Apply the optimized threshold τ to the BigGAN dataset (unseen during training). Confirm that the "Combined Uncertainty" IPR is significantly higher (>50%) than the standard "Probability" metric IPR.

## Open Questions the Paper Calls Out

- **Question:** Can an automated online learning framework be successfully integrated to dynamically retrain the model using the rejected high-uncertainty samples?
  - **Basis in paper:** [explicit] The Conclusion states that "retraining the model or adopting advanced domain adaptation strategies may be necessary to maintain stringent performance standards" and that "rejected synthetic samples serve as valuable input for retraining."
  - **Why unresolved:** The current framework identifies and rejects uncertain samples but leaves the actual mechanism of incorporating these samples back into the training loop for future iterations as a procedural next step rather than an implemented feature.
  - **What evidence would resolve it:** A demonstration of an active learning pipeline where rejected samples are automatically labeled and added to the training set, showing improved detection rates on subsequent, previously unseen generative models.

- **Question:** How robust is the Combined Uncertainty rejection mechanism against black-box adversarial attacks designed to spoof the classifier or the uncertainty metrics specifically?
  - **Basis in paper:** [inferred] The "Adversarial Robustness Evaluation" section is limited to white-box attacks (FGSM and PGD), leaving the system's resilience against attacks where the adversary does not have full access to model gradients or uncertainty parameters untested.
  - **Why unresolved:** While the framework resists gradient-based perturbations, it is unclear if an attacker could exploit the specific combination of Fisher Information, GP variance, and MC Dropout to generate inputs that appear certain but are misclassified.
  - **What evidence would resolve it:** Evaluation of the Incorrect Prediction Rejection (IPR) rate under black-box attacks (e.g., Square Attack, boundary attacks) or attacks specifically crafted to minimize the combined uncertainty score while causing misclassification.

- **Question:** Do the PSO-derived weighting schemes generalize effectively across diverse backbone architectures, or must they be re-optimized for specific feature extractors like Vision Transformers?
  - **Basis in paper:** [inferred] The paper compares ResNet50 and ViT but relies on weights optimized for ResNet50; the authors note the experiments with ViT show "potential," yet Tables 4 and 5 show a performance gap, raising the question of whether the uncertainty integration strategy is architecture-agnostic.
  - **Why unresolved:** It is not established if the relative importance of Fisher Information versus GP variance (learned via PSO) remains consistent when the underlying feature representations change from convolutional (ResNet) to attention-based (ViT) mechanisms.
  - **What evidence would resolve it:** A cross-validation study applying the ResNet-optimized weights directly to a ViT model versus re-optimizing weights specifically for ViT, comparing the resulting Correctly Predicted Acceptance (CPA) rates.

## Limitations
- Computational tractability of FIM: Computing per-instance Fisher Information for each test image is highly expensive and may not scale to large datasets without approximations.
- PSO generalizability: The PSO-derived weights and threshold are optimized on Stable Diffusion; their performance on truly novel generators without any retraining remains unproven.
- Evaluation scope: Results are shown primarily on face images; performance on non-face AI-generated content (e.g., landscapes, objects) is not demonstrated.

## Confidence
- **High confidence:** The baseline detection performance of ResNet50 and ViT on in-distribution Stable Diffusion images.
- **Medium confidence:** The effectiveness of the combined uncertainty measure under distribution shift, given the observed ~70% IPR on unseen generators.
- **Low confidence:** The robustness of the system against sophisticated adversarial attacks, as the reported 61% rejection rate leaves significant room for successful evasion.

## Next Checks
1. **Compute Cost Audit:** Profile the runtime and memory usage of the Fisher Information calculation per image. If it exceeds 1 second per image, explore diagonal FIM approximations or remove this component.
2. **OOD Generalization Test:** Apply the PSO-optimized threshold to a completely unseen generator (e.g., DALL-E 3) not in the BigGAN/Midjourney/GLIDE/VQDM set. Measure if the ~70% IPR holds.
3. **Adversarial Stress Test:** Generate stronger attacks (e.g., PGD with random restarts) and verify if the combined uncertainty measure maintains >50% rejection rate, or if it degrades catastrophically.