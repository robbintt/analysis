---
ver: rpa2
title: One-shot In-context Part Segmentation
arxiv_id: '2503.01144'
source_url: https://arxiv.org/abs/2503.01144
tags:
- part
- segmentation
- in-context
- features
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free One-shot In-context Part
  Segmentation (OIParts) framework that leverages visual foundation models (VFMs)
  to segment objects into their constituent parts using only a single example. The
  core innovation lies in combining features from DINOv2 and Stable Diffusion with
  an adaptive channel selection approach that minimizes intra-class distance, enabling
  more discriminative and generalizable part representations.
---

# One-shot In-context Part Segmentation

## Quick Facts
- arXiv ID: 2503.01144
- Source URL: https://arxiv.org/abs/2503.01144
- Authors: Zhenqi Dai; Ting Liu; Xingxing Zhang; Yunchao Wei; Yanning Zhang
- Reference count: 40
- Primary result: OIParts achieves mIoU scores of 72.0% on face segmentation, 65.2% on car segmentation, and 66.9% on horse segmentation in one-shot settings

## Executive Summary
This paper introduces a training-free One-shot In-context Part Segmentation (OIParts) framework that leverages visual foundation models (VFMs) to segment objects into their constituent parts using only a single example. The core innovation lies in combining features from DINOv2 and Stable Diffusion with an adaptive channel selection approach that minimizes intra-class distance, enabling more discriminative and generalizable part representations. Experiments on three benchmark datasets demonstrate that OIParts achieves superior segmentation performance compared to existing one-shot methods, particularly excelling in challenging scenarios with significant appearance/perspective variations and partially visible objects.

## Method Summary
The OIParts framework operates by first extracting features from both DINOv2 and Stable Diffusion for the reference object and target image. An adaptive channel selection mechanism then identifies the most discriminative feature channels by minimizing intra-class distance within the reference object. The selected features are combined to create a unified representation that captures both appearance and semantic information. This representation is used to guide the segmentation of corresponding parts in the target image through a similarity-based matching process. The entire pipeline is training-free, relying solely on the pre-trained VFMs and the single reference example.

## Key Results
- Achieves 72.0% mIoU on face part segmentation in one-shot setting
- Achieves 65.2% mIoU on car part segmentation in one-shot setting
- Achieves 66.9% mIoU on horse part segmentation in one-shot setting
- Outperforms existing one-shot methods on all three benchmark datasets
- Demonstrates superior performance on challenging scenarios with appearance variations and partial visibility

## Why This Works (Mechanism)
The method works by leveraging the rich, pre-trained representations from visual foundation models to capture both low-level appearance features and high-level semantic information about object parts. The adaptive channel selection mechanism identifies the most discriminative features by minimizing intra-class distance, which helps focus on the most relevant visual cues for each part type. The combination of DINOv2's self-supervised features and Stable Diffusion's generative features provides complementary information that enhances the robustness and generalization of the part representations across different objects and viewpoints.

## Foundational Learning
- Visual Foundation Models (VFMs): Pre-trained deep learning models that learn general visual representations from large-scale data. Needed because they provide rich, transferable features that capture both appearance and semantics. Quick check: Verify the specific versions and training datasets of DINOv2 and Stable Diffusion used.
- Self-supervised Learning: Training approach where models learn from unlabeled data by solving pretext tasks. Needed because it enables DINOv2 to learn robust features without requiring manual annotations. Quick check: Confirm the specific self-supervised objectives used in DINOv2 training.
- Channel Selection: Process of identifying and using only the most relevant feature channels from a deep network. Needed because it reduces noise and focuses computation on discriminative features. Quick check: Validate the channel selection threshold and its sensitivity to different reference examples.
- Intra-class Distance: Measure of similarity between samples of the same class. Needed as the criterion for selecting discriminative features that best characterize each object part. Quick check: Verify the distance metric (e.g., Euclidean, cosine) used for channel selection.
- One-shot Learning: Machine learning paradigm where models learn from a single example per class. Needed because it enables the framework to generalize to new objects without extensive training. Quick check: Confirm the exact definition of "one-shot" used (e.g., single reference image, single annotated example).

## Architecture Onboarding

Component Map:
Reference Object -> Feature Extraction (DINOv2 + Stable Diffusion) -> Adaptive Channel Selection -> Unified Feature Representation -> Similarity Matching -> Part Segmentation

Critical Path:
The critical path flows from feature extraction through adaptive channel selection to the unified representation. The quality of this representation directly determines segmentation accuracy, making the combination of VFMs and channel selection the most crucial components. Any degradation in feature quality or channel selection accuracy will propagate through the entire pipeline.

Design Tradeoffs:
The framework trades computational efficiency for performance by using two separate VFMs and a complex channel selection mechanism. This approach sacrifices speed for improved accuracy and generalization. An alternative design using a single VFM or simpler feature combination could reduce computation but might compromise segmentation quality, particularly for challenging cases with appearance variations.

Failure Signatures:
The method is likely to fail when: (1) reference and target objects have significantly different appearances due to lighting or occlusion, (2) the single reference example doesn't capture the full variability of the part being segmented, or (3) the VFMs' pre-trained representations don't adequately capture the specific object category or part types. Performance degradation will be most noticeable on objects with high intra-class variation or unusual viewpoints.

First Experiments:
1. Ablation study removing either DINOv2 or Stable Diffusion features to quantify their individual contributions
2. Test with reference examples showing only partial parts to evaluate robustness to incomplete information
3. Evaluate performance with multiple reference examples (few-shot setting) to establish upper performance bounds

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on the quality and generalization of pre-trained VFMs, which may not capture all object categories or part types effectively
- The adaptive channel selection mechanism lacks thorough analysis of its robustness across diverse datasets and potential sensitivity to hyperparameter choices
- Evaluation is limited to three object categories (faces, cars, horses), raising questions about scalability to more complex scenes or rare object types

## Confidence

Major claim confidence assessment:
- **High confidence**: The reported mIoU scores (72.0% for faces, 65.2% for cars, 66.9% for horses) are specific and methodologically verifiable through standard benchmarks
- **Medium confidence**: Claims of superior performance over existing one-shot methods require independent replication, particularly for challenging scenarios with appearance variations
- **Medium confidence**: The assertion that the method excels with partially visible objects needs more rigorous testing with controlled occlusion studies

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of DINOv2 features, Stable Diffusion features, and the adaptive channel selection mechanism to overall performance
2. Test the framework's robustness across a broader range of object categories and part types, including fine-grained distinctions like distinguishing individual furniture components
3. Evaluate performance degradation when using VFMs trained on different datasets or with different architectures to assess dependency on specific model choices