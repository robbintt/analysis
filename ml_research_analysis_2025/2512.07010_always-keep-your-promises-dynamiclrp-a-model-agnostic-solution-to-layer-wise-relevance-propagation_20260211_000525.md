---
ver: rpa2
title: 'Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise
  Relevance Propagation'
arxiv_id: '2512.07010'
source_url: https://arxiv.org/abs/2512.07010
tags:
- promise
- relevance
- propagation
- node
- computation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of existing Layer-wise Relevance
  Propagation (LRP) implementations, which operate at the module level and require
  architecture-specific propagation rules. This restricts their applicability to evolving
  architectures.
---

# Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation

## Quick Facts
- arXiv ID: 2512.07010
- Source URL: https://arxiv.org/abs/2512.07010
- Authors: Kevin Lee; Pablo Millan Arias
- Reference count: 40
- Primary result: Achieves 99.92% node coverage across 15 diverse architectures with operation-level LRP rules

## Executive Summary
This paper addresses the fundamental limitation of existing Layer-wise Relevance Propagation (LRP) implementations, which operate at the module level and require architecture-specific propagation rules. This restricts their applicability to evolving neural architectures. The authors propose DynamicLRP, a model-agnostic LLRP framework that operates at the tensor operation level, decomposing attribution into individual operations within computation graphs. By introducing the Promise System for deferred activation resolution, DynamicLRP achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. The method operates independently of backpropagation machinery, requiring no model modification and enabling side-by-side execution with gradient backpropagation.

## Method Summary
DynamicLRP implements LRP by decomposing neural network computation graphs into fundamental tensor operations (47 types) and defining conservation-preserving propagation rules for each primitive. The Promise System enables deferred activation resolution by creating mutable metadata objects that traverse the graph to find nodes with cached forward activations, allowing relevance propagation without graph modification. The framework operates independently of backpropagation, requiring no model modification, and achieves architecture agnosticity through operation-level decomposition rather than module-specific rules.

## Key Results
- Achieves 99.92% node coverage across 31,465 computation graph nodes from 15 diverse architectures
- Matches or exceeds specialized implementations: 1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT
- Achieves 93.70% and 95.06% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2
- Maintains LRP theoretical guarantees while operating independently of backpropagation machinery

## Why This Works (Mechanism)

### Mechanism 1: Operation-Level Relevance Decomposition
Defining LRP propagation rules for fundamental tensor operations (~47 types) rather than neural network modules enables architecture-agnostic attribution without loss of faithfulness. By decomposing every module into primitive operations and defining conservation-preserving rules for each primitive, any architecture reduces to this bounded operation set, achieving universal coverage.

### Mechanism 2: Promise System for Deferred Activation Resolution
Deferred relevance propagation via "Promises" enables operation-level LRP without requiring graph modification or forced activation caching. When backward traversal reaches a node lacking cached forward activations, a Promise object is created with branches that continue traversing to find "Arg Nodes" that store activations. Forward closures reconstruct missing values, backward closures propagate relevance once resolved.

### Mechanism 3: Conservation-Preserving Propagation at Operation Granularity
Maintaining LRP conservation property (∑R_in = ∑R_out) at operation-level rather than layer-level preserves theoretical guarantees while enabling model-agnosticity. Each operation type has a propagation rule that redistributes output relevance to inputs proportionally based on contribution magnitudes, ensuring conservation is maintained throughout the computation graph.

## Foundational Learning

- **Computation Graphs in Auto-Differentiation**: DynamicLRP repurposes PyTorch's autograd graph structure; understanding DAG construction, gradient caching behavior, and node metadata is essential for implementing Promises. Quick check: For `c = a + b`, does PyTorch's autograd cache the values of `a` and `b`? Why or why not?

- **Layer-wise Relevance Propagation (LRP) Theory**: The method maintains LRP's theoretical foundations; familiarity with Taylor decomposition, epsilon/gamma rules, and conservation properties is prerequisite to understanding why operation-level rules work. Quick check: Derive the epsilon rule (Eq. 4) from the Taylor expansion (Eq. 1) and conservation constraint (Eq. 2).

- **Topological Graph Traversal**: The Promise System is fundamentally a graph algorithm; understanding DFS-based topological sorting, in-degree tracking, and deadlock detection is necessary for implementing the traversal heuristic. Quick check: Given a DAG with residual connections, what happens if you traverse strictly by topological order without handling nodes with partial inputs?

## Architecture Onboarding

- **Component map**: Forward pass → standard model execution, autograd graph construction → Auxiliary graph construction (Algorithm 1) → Promise-augmented backward traversal (Algorithm 2) → Promise resolution → Input aggregation
- **Critical path**: The Promise System is the most complex component; specifically, handling Promise Trees with nesting and Pre-Promises for deadlock resolution. Start by tracing the example in Appendix D.2.
- **Design tradeoffs**: Memory vs. computation (caching Promise paths vs. recomputing), Coverage vs. specialization (47 operations cover 99.92%), VRAM vs. speed (2x VRAM overhead vs. no model modification)
- **Failure signatures**: Uncovered operation error (extend rule dictionary), Promise deadlock (verify Pre-Promise mechanism), Memory overflow on large models (>1B parameters)
- **First 3 experiments**: 
  1. Run LRP on VGG16 on CIFAR-10 and verify ABPC matches reported values (~1.77)
  2. Run on ResNet-50 to stress-test Promise System with residual connections
  3. Run on ViT-b-16 and compare attributions qualitatively with AttnLRP

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- Operation-level decomposition assumes the 47 tensor operations remain stable across future architectures
- Promise System reliance on Arg Nodes may depend on PyTorch's gradient-tracking behavior consistency
- 2x VRAM overhead compared to specialized implementations may limit applicability to very large models (>1B parameters)

## Confidence
- **High Confidence**: Operation-level decomposition principle - extensively validated across 15 diverse architectures
- **High Confidence**: Promise System correctness - formally proven with example tracing
- **Medium Confidence**: Conservation equivalence between operation-level and module-level LRP - theoretically sound but lacks corpus validation
- **Medium Confidence**: Faithfulness metrics - matches or exceeds specialized implementations on reported architectures

## Next Checks
1. Test conservation properties on extremely deep architectures to detect potential numerical drift from operation-level decomposition accumulation
2. Implement and validate LRP rules for currently uncovered operations to achieve 100% coverage
3. Profile VRAM usage and execution time on a range of model scales to quantify the practical impact of the 2x memory overhead