---
ver: rpa2
title: 'The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language
  Models'
arxiv_id: '2511.02589'
source_url: https://arxiv.org/abs/2511.02589
tags:
- reasoning
- arxiv
- error
- language
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The ORCA Benchmark introduces a novel framework for evaluating\
  \ large language models (LLMs) on real-world quantitative reasoning using 500 natural-language\
  \ tasks from finance, physics, health, and statistics. Unlike standard academic\
  \ math benchmarks, ORCA tests models against verified calculator outputs from Omni\u2019\
  s engines, focusing on everyday applied calculations rather than abstract or competition-style\
  \ problems."
---

# The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models

## Quick Facts
- **arXiv ID:** 2511.02589
- **Source URL:** https://arxiv.org/abs/2511.02589
- **Reference count:** 0
- **Primary result:** Leading LLMs achieve only 45-63% accuracy on real-world quantitative tasks, with calculation and rounding errors accounting for 68% of failures.

## Executive Summary
The ORCA Benchmark introduces a novel framework for evaluating large language models (LLMs) on real-world quantitative reasoning using 500 natural-language tasks from finance, physics, health, and statistics. Unlike standard academic math benchmarks, ORCA tests models against verified calculator outputs from Omni's engines, focusing on everyday applied calculations rather than abstract or competition-style problems. The five leading LLMs—ChatGPT-5, Gemini 2.5 Flash, Claude Sonnet 4.5, Grok 4, and DeepSeek V3.2—achieved only 45–63% accuracy overall, with errors primarily stemming from rounding (35%) and calculation mistakes (33%). Accuracy varied widely by domain, with highest performance in mathematics and conversions, and lowest in physics and health. Correlation analysis showed moderate overlap in error patterns (r ≈ 0.40–0.65), indicating complementary failure profiles across models. The benchmark highlights the gap between linguistic reasoning and precise numerical computation, suggesting that hybrid architectures coupling LLMs with dedicated computational backends may offer the most promising path forward.

## Method Summary
The ORCA Benchmark evaluates LLMs on 500 real-world quantitative reasoning tasks drawn from natural-language queries corresponding to Omni calculator engine problems. Each prompt has a verified ground truth answer, and models are scored strictly (0/1) based on exact numerical match including rounding. The framework tests models across four domains—finance, physics, health, and statistics—with tasks ranging from compound interest calculations to BMI computations. Five leading LLMs are evaluated, and error types are categorized into calculation errors, rounding/precision issues, wrong parameter selection, and reasoning failures.

## Key Results
- Five leading LLMs achieved only 45–63% overall accuracy on real-world quantitative tasks
- Calculation errors (33%) and rounding/precision issues (35%) account for 68% of all failures
- Accuracy varies dramatically by domain: highest in mathematics and conversions, lowest in physics and health
- Moderate correlation in error patterns (r ≈ 0.40–0.65) suggests complementary failure profiles across models

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Verification Isolation
- **Claim:** The benchmark isolates failures in numerical execution from failures in logical reasoning by comparing probabilistic model outputs against a deterministic ground truth.
- **Mechanism:** By grounding evaluation in 500 natural-language tasks with pre-verified outputs from Omni's calculator engines, the framework attributes errors specifically to the model's calculation or rounding behavior rather than ambiguity in the problem statement.
- **Core assumption:** The Omni calculator engines provide mathematically absolute ground truth, and discrepancies are solely attributable to LLM failure.
- **Evidence anchors:**
  - [Abstract]: "...evaluates large language models (LLMs) on multi-domain, real-life quantitative reasoning using verified outputs from Omni's calculator engine."
  - [Section 3.1]: "Each prompt corresponds directly to an existing Omni calculator, ensuring that every problem has a single deterministic ground-truth answer..."
  - [Corpus]: The corpus contains related benchmarks (e.g., PhysReason) but lacks direct validation of the Omni engine's specific accuracy, limiting external corroboration.
- **Break condition:** This mechanism fails if the "verified" calculator outputs themselves contain edge-case bugs or if the natural language prompts introduce ambiguity that the calculator engine does not account for.

### Mechanism 2: Complementary Failure Profile Detection
- **Claim:** Moderate correlation in error patterns ($r \approx 0.40-0.65$) suggests that models fail non-identically, enabling ensemble strategies to outperform single models.
- **Mechanism:** Distinct architectural biases and training data lead models to make different types of mistakes (e.g., one model fails on rounding while another fails on formula selection). An ensemble or "router" architecture can exploit this lack of perfect correlation.
- **Core assumption:** Error patterns observed in the benchmark generalize to broader, unseen quantitative tasks.
- **Evidence anchors:**
  - [Abstract]: "Correlation analysis showed moderate overlap in error patterns (r ≈ 0.40–0.65), indicating complementary failure profiles across models."
  - [Section 4.4]: "...no pair exhibited near-perfect alignment, suggesting that their reasoning failures are only partially shared."
  - [Corpus]: Weak/missing; corpus neighbors do not provide specific correlation coefficients for these specific model pairs in other contexts.
- **Break condition:** This breaks if the moderate correlation rises to near-perfect levels on specific high-stakes sub-domains (e.g., all models fail simultaneously on specific physics constants), removing the complementarity benefit.

### Mechanism 3: Tool-Use Decoupling
- **Claim:** Hybrid architectures may resolve the accuracy gap by decoupling semantic planning from symbolic execution.
- **Mechanism:** The LLM acts as a semantic parser (translating natural language to structured inputs/parameters) while an external computational backend handles the arithmetic, eliminating the "calculation error" class (33% of errors).
- **Core assumption:** LLMs are sufficiently capable at parameter extraction and tool invocation to prevent "Wrong parameter" errors (4.7%) from negating the gains in calculation accuracy.
- **Evidence anchors:**
  - [Section 5.7]: "...the most promising path forward lies in hybrid architectures—systems that use language models to decompose and explain problems while delegating numerical precision to dedicated computational backends."
  - [Section 5.2]: "...coupling a model with an external computational module... substantially mitigates these mechanical slips."
  - [Corpus]: Corpus papers (e.g., ORCA for VLMs) discuss agentic reasoning, which aligns with tool-use concepts but does not quantify the specific gain for arithmetic tasks in this dataset.
- **Break condition:** This mechanism fails if the LLM frequently hallucinates input parameters (wrong constants/units) for the external tool, shifting the error mode from "calculation" to "parametrization."

## Foundational Learning

- **Concept:** **Data Contamination vs. Generalization**
  - **Why needed here:** The paper argues that high scores on standard benchmarks (GSM8K, MATH) may be inflated by data contamination (models seeing test questions during training). Understanding this distinction is critical for valuing the ORCA benchmark's use of "fresh" calculator-based tasks.
  - **Quick check question:** How does the use of dynamic, tool-generated prompts in ORCA differ from static academic datasets in terms of contamination risk?

- **Concept:** **Precision vs. Accuracy in Floating-Point Arithmetic**
  - **Why needed here:** A dominant error type (35%) was rounding/precision. Learners must distinguish between a model getting the "concept" right but failing the specific display precision required by the benchmark.
  - **Quick check question:** If a model outputs 31.874 for a BMI calculation but the ground truth rounds to 31.9, is this a reasoning failure or an alignment failure?

- **Concept:** **Mixture-of-Experts (MoE) vs. Dense Architectures**
  - **Why needed here:** The paper evaluates DeepSeek V3.2 (MoE) against dense models like GPT-5. Understanding how MoE activates subsets of parameters helps explain why DeepSeek might show different domain specializations (strong in Math, weak in Biology).
  - **Quick check question:** Why might an MoE architecture exhibit higher variance across distinct domains compared to a dense model?

## Architecture Onboarding

- **Component map:** Frontend (LLM) -> Evaluator (ORCA Framework) -> Backend (Compute Engine)

- **Critical path:**
  1. **Prompt Ingestion:** LLM receives natural language query.
  2. **Decomposition:** LLM identifies variables and required formula.
  3. **Execution:** (In current LLMs) Internal token prediction generates the answer; (In proposed Hybrid) LLM generates code/API call to backend.
  4. **Verification:** Output is compared against Omni engine.

- **Design tradeoffs:**
  - **Strictness vs. Nuance:** The benchmark uses strict rounding rules (matching Omni's display). This flags valid intermediate reasoning as "incorrect" if the final formatting is wrong.
  - **Ensemble Latency:** utilizing complementary profiles (Mechanism 2) requires running multiple models or complex routing, increasing inference cost/latency compared to a single model.

- **Failure signatures:**
  - **Mechanical Drift:** Models achieve correct logic but output `$53,908.65` instead of `$53,892.27` (Calculation Error).
  - **Unit Slippage:** Correct numerical value but wrong magnitude (e.g., mW vs W), though rare (0.0%) in this dataset, "Wrong Parameter" errors (4.7%) often look like this.
  - **Domain Collapse:** Specialized models (e.g., DeepSeek) dropping to ~10% accuracy in non-core domains (Biology/Chemistry).

- **First 3 experiments:**
  1. **Tool-Augmentation Run:** Re-run the ORCA benchmark on ChatGPT-5/Gemini using a "Calculator Tool" (Python interpreter) to quantify the reduction in "Calculation Errors" (33% baseline).
  2. **Router Accuracy Test:** Build a simple domain-classifier router that sends Physics/Health questions to Grok 4 and Math/Stats to Gemini 2.5 Flash based on Figure 5 domain strengths. Measure the lift over random assignment.
  3. **Rounding Tolerance Analysis:** Re-score the dataset with a ±0.1% tolerance to determine how much of the "failure" is strictly numerical precision versus logical reasoning collapse.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's ground truth validity depends entirely on the accuracy of Omni calculator outputs, with no external validation provided
- Strict rounding comparison may conflate precision alignment with reasoning failure, potentially inflating calculation error categorization
- Tool-use recommendations assume LLM parameter extraction capabilities are reliable enough to prevent "wrong parameter" errors from becoming dominant

## Confidence

**High Confidence (★★★):** The overall performance gap between LLMs and deterministic calculators is real and substantial (45-63% accuracy). The domain-specific performance patterns (Math/Conversions > Physics/Health) are robustly observed across all five models tested.

**Medium Confidence (★★☆):** The error type categorization (33% calculation, 35% rounding, 4.7% wrong parameters) is methodologically sound but may be sensitive to the strict rounding threshold. The moderate correlation coefficients (r ≈ 0.40-0.65) between models are accurately computed but may not generalize to non-calculator quantitative tasks.

**Low Confidence (★☆☆):** The recommendation for hybrid architectures assumes that LLM tool-use capabilities (parameter extraction, API calling) are sufficiently reliable to prevent "wrong parameter" errors from becoming the dominant failure mode. The paper provides minimal empirical evidence for this assumption.

## Next Checks
1. **Ground-Truth Cross-Validation:** Run 50 ORCA prompts through multiple independent calculation engines (Python, Wolfram Alpha, Omni) and compute inter-engine agreement rates. Any significant discrepancies would invalidate the benchmark's error attribution framework.

2. **Tool-Augmentation Efficacy Test:** Implement a "calculator tool" callback in ChatGPT-5 and Gemini 2.5 Flash using Python's eval() on extracted parameters. Measure the reduction in "calculation errors" (33% baseline) and determine whether "wrong parameter" errors (4.7%) become the new bottleneck.

3. **Rounding Tolerance Sensitivity Analysis:** Re-run the ORCA benchmark with three rounding thresholds: strict (±0.0), moderate (±0.1%), and loose (±1%). Track how each error category's percentage changes across thresholds to quantify how much of the "failure" is strictly numerical precision versus logical reasoning collapse.