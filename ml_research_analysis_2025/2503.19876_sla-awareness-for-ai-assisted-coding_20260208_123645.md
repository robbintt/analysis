---
ver: rpa2
title: SLA-Awareness for AI-assisted coding
arxiv_id: '2503.19876'
source_url: https://arxiv.org/abs/2503.19876
tags:
- code
- coding
- tasks
- latency
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CATO addresses the challenge of serving diverse AI-assisted coding
  tasks with varying latency requirements (TTFT vs. E2E critical) by introducing SLA-aware
  scheduling and scaling algorithms.
---

# SLA-Awareness for AI-assisted coding

## Quick Facts
- arXiv ID: 2503.19876
- Source URL: https://arxiv.org/abs/2503.19876
- Authors: Kishanthan Thangarajah; Arthur Leung; Boyuan Chen; Ahmed E. Hassan
- Reference count: 40
- Key outcome: CATO improves Goodput by up to 10%, resource utilization by 41.1%, P95 E2E latency by 18% for code summarization, and P95 TTFT by 14% for code generation compared to state-of-the-art systems.

## Executive Summary
CATO addresses the challenge of serving diverse AI-assisted coding tasks with varying latency requirements (TTFT vs. E2E critical) by introducing SLA-aware scheduling and scaling algorithms. The core idea is to decompose coding tasks into nodes, measure baseline latencies, and assign slack time to each node to ensure end-to-end SLA compliance. The scheduling algorithm routes requests to replicas based on slack time and queue length, while the scaling algorithm provisions replicas proportionally to slack violations. Experimental results show that CATO improves Goodput by up to 10%, resource utilization by 41.1%, P95 E2E latency by 18% for code summarization, and P95 TTFT by 14% for code generation compared to state-of-the-art systems.

## Method Summary
CATO is a distributed system for serving diverse AI-assisted coding tasks with varying latency requirements. It uses a profiling component to measure baseline latencies and assign slack time to each node in the task graph. The replica router schedules requests using slack-aware priority queuing, while the resource provisioner scales replicas proportionally to slack violations. The system was evaluated on a cluster of 2× Atlas 800 with 12× Ascend 910B4 GPUs, using CodeXGLUE datasets and CodeLlama2-7B/StarCoder2-7B models. Load tests varied Poisson arrival rates (0.67-3.33 req/s) and compared Goodput, latency metrics, and utilization against Ray Serve and Round Robin baselines.

## Key Results
- Goodput improvement up to 10% over state-of-the-art systems
- Resource utilization increased by 41.1%
- P95 E2E latency reduced by 18% for code summarization
- P95 TTFT improved by 14% for code generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Slack-based request scheduling improves Goodput by prioritizing requests nearing SLA violation.
- **Mechanism**: CATO calculates "slack time" (remaining time before SLA breach) for each request. Requests with violated/negative slack get priority 0 (highest), at-risk requests get priority 1, and safe requests get priority 2. The scheduler selects replicas based on both slack compliance and shortest queue wait time.
- **Core assumption**: Historical execution times from profiling reasonably predict future node completion times.
- **Evidence anchors**:
  - [abstract] "scheduling algorithm routes requests to replicas based on slack time and queue length"
  - [section 4.3, Algorithm 1] Lines 11-29 show priority assignment: priority=0 for violated slack, priority=1 for no replicas meeting slack, priority=2 for slack-compliant replicas
  - [corpus] No direct corpus evidence for slack-based scheduling mechanisms
- **Break condition**: If profiling data becomes stale due to workload distribution shift, slack estimates will be inaccurate, degrading scheduling decisions.

### Mechanism 2
- **Claim**: Slack-violation-proportional autoscaling balances Goodput and utilization better than queue-length-based scaling.
- **Mechanism**: The Resource Provisioner counts slack violations per time window. When violations exceed a threshold AND exceed idle replica count, it provisions new replicas proportional to the violation count. This triggers scaling before queue buildup causes cascading SLA failures.
- **Core assumption**: Violation counts are a leading indicator of future SLA breaches under sustained load.
- **Evidence anchors**:
  - [abstract] "scaling algorithm provisions replicas proportionally to slack violations"
  - [section 4.4, Algorithm 2] Lines 20-32 show violation counting and proportional replica creation
  - [section 5.7.2] "replicas are provisioned early before request at the queues of each replica has a chance to grow"
  - [corpus] No corpus evidence for slack-violation-driven autoscaling
- **Break condition**: Cold start latency for new replicas may itself cause SLA violations if the violation threshold is set too low or workloads are extremely bursty.

### Mechanism 3
- **Claim**: Task decomposition with per-node slack allocation enables end-to-end SLA awareness across multi-step coding workflows.
- **Mechanism**: Coding tasks are decomposed into nodes (CodeLLM calls, DB fetches, validation steps). The Profiler measures baseline latencies and assigns slack proportionally (e.g., if nodes A,B,C take 5s,15s,5s and SLA is 50s, allotted slack becomes 10s,30s,10s). This propagates SLA awareness through the workflow.
- **Core assumption**: Node execution times are relatively stable and the critical path can be identified at profiling time.
- **Evidence anchors**:
  - [section 4.1] "Slack time is the time allotted for each node to execute, such that the sum of all slack time on the critical path does not exceed SLA"
  - [section 4.2, Profiler] "if a coding task's nodes A,B,C take 5s, 15s, 5s to execute E2E, and the user provided SLA was 50s, then the Profiler may determine allotted slack proportionally"
  - [corpus] Weak corpus connection; Agint [arxiv:2511.19635] mentions graph compilation for agents but not SLA-aware slack allocation
- **Break condition**: If nodes have highly variable execution times (e.g., RAG retrieval with unpredictable DB query complexity), fixed slack allocation will misestimate.

## Foundational Learning

- **Concept**: **Time-To-First-Token (TTFT) vs End-to-End (E2E) Latency**
  - **Why needed here**: CATO optimizes differently for TTFT-critical tasks (code completion, generation) vs E2E-critical tasks (translation, summarization). Confusing these leads to wrong SLA targets.
  - **Quick check question**: For a streaming code suggestion that appears character-by-character, which metric matters more for perceived responsiveness?

- **Concept**: **Goodput as SLA Compliance Rate**
  - **Why needed here**: CATO's primary optimization target is Goodput (fraction of requests meeting SLA), not raw throughput. This is the metric used in all experimental comparisons.
  - **Quick check question**: If a system serves 100 requests/second but 30% violate latency SLAs, what is the Goodput rate?

- **Concept**: **Slack Time in Scheduling Theory**
  - **Why needed here**: The entire scheduling algorithm hinges on slack (remaining time before deadline). Without this concept, Algorithm 1 appears arbitrary.
  - **Quick check question**: A request started 5 seconds ago with a 15-second SLA and expected remaining execution of 6 seconds—what is its slack? Should it be prioritized?

## Architecture Onboarding

- **Component map**: Request arrives → Profiler measures baseline latencies → Replica Router decomposes tasks into nodes → Scheduler routes CodeLLM nodes to replicas using slack-aware priority queuing → Resource Provisioner monitors violations and scales replicas proportionally

- **Critical path**:
  1. Request arrives → Replica Router decomposes into nodes
  2. For each CodeLLM node: calculate remaining slack (SLA - elapsed time)
  3. Scheduler selects replica: iterate available replicas, compute expected wait + completion time, pick shortest queue that meets slack
  4. If slack violations exceed threshold → Resource Provisioner scales up replicas
  5. Monitor utilization and Goodput; adjust profiling thresholds periodically

- **Design tradeoffs**:
  - **Goodput vs Utilization**: CATO's early provisioning improves Goodput (up to 10%) but may temporarily lower utilization vs queue-length-based scaling (Table 4 shows CATO at 0.43 utilization vs Ray Serve at 0.47 under moderate load)
  - **Scheduling overhead**: Computing expected wait times by iterating queue elements adds latency vs simple power-of-2 or round-robin
  - **Profiling accuracy vs adaptability**: Fixed slack allocation from profiling is simple but may not adapt to distribution shifts without re-profiling

- **Failure signatures**:
  - **Cascading SLA violations**: Cold starts for new replicas (model weight loading from NVMe) can take seconds; if load spikes faster than provisioning, Goodput drops sharply
  - **Priority inversion**: If all requests are high-priority (slack violated), the priority queue degenerates to FIFO
  - **Profile drift**: Baseline latencies measured at light load may underestimate queueing delay at high load

- **First 3 experiments**:
  1. **Reproduce RQ1** (long-output tasks): Deploy code generation + code translation with fixed 6 replicas per model; verify P95 TTFT improvement over Ray Serve baseline at 2.0-3.33 req/s
  2. **Ablate slack calculation**: Replace slack-aware scheduling with power-of-2-choices while keeping SLA-aware scaling; measure Goodput degradation to isolate scheduling contribution
  3. **Stress test cold start**: Send burst of requests from idle state (1 replica) and measure time-to-steady-state Goodput; tune `max_exceeded_times` threshold in Algorithm 2 to minimize violation during scale-up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CATO’s SLA-aware scheduling and scaling algorithms be effectively generalized to serve non-coding AI workflows?
- Basis in paper: [explicit] The conclusion states the authors plan to explore serving "other workflows and AI applications" in future research.
- Why unresolved: The current evaluation is restricted to four specific coding tasks (generation, completion, summarization, translation) defined by CodeLLMs.
- What evidence would resolve it: An evaluation of CATO running diverse generic AI workflows (e.g., RAG-based chat, multi-modal analysis) showing maintained Goodput and utilization.

### Open Question 2
- Question: What performance gains can be achieved by combining CATO with prefill and decoding separation techniques?
- Basis in paper: [explicit] Section 5.4 suggests future research could explore combining CATO with "prefill and decoding separations" to improve performance for long-prompt tasks like code translation.
- Why unresolved: The current implementation treats the CodeLLM inference as a monolithic node in the workflow graph, potentially struggling with the distinct compute characteristics of prefill vs. decoding phases.
- What evidence would resolve it: A comparative study of CATO’s latency metrics when integrated with disaggregated serving systems (e.g., DistServe) on workloads with long input contexts.

### Open Question 3
- Question: How does CATO perform in large-scale production clusters exceeding 100 machines?
- Basis in paper: [inferred] Section 6 notes the experiments were limited to 2 machines and 100-200 requests due to resource constraints, posing a threat to external validity.
- Why unresolved: The scheduler's decision logic iterates over available replicas to calculate wait times, which may introduce latency overhead or scalability bottlenecks in large-scale deployments with thousands of replicas.
- What evidence would resolve it: A load test on a distributed cluster with 100+ nodes measuring scheduling overhead and Goodput stability under high concurrency.

## Limitations

- **Implementation details**: Key components (Profiler slack distribution, priority queue reordering logic, scaling hyperparameters) are not specified, making exact reproduction difficult.
- **Hardware specificity**: Results depend on Ascend 910B4 GPUs and specific vLLM integration; NVIDIA GPU deployment may require substantial adaptation.
- **Cold start impact**: The effect of model weight loading latency (from NVMe) on SLA violations during scaling events is not quantified.

## Confidence

- **High confidence**: The slack-based scheduling mechanism and its priority assignment logic are clearly specified in Algorithm 1 and supported by the Goodput improvements shown in Table 3.
- **Medium confidence**: The proportional autoscaling approach is well-defined in Algorithm 2, but the specific threshold values (max_exceeded_proportion, max_exceeded_times) that would be needed for reproduction are unspecified.
- **Medium confidence**: The task decomposition and per-node slack allocation concept is sound, but the fixed slack assignment from profiling may not adapt well to highly variable workloads.

## Next Checks

1. **Reproduce core SLA-awareness benefit**: Implement CATO's slack-aware scheduling and proportional autoscaling on a simpler platform (e.g., CPU-based CodeLlama2-7B) and verify Goodput improvement over queue-length-based scaling under moderate load.
2. **Test adaptive profiling**: Add a mechanism to periodically re-profile baseline latencies and adjust slack allocation when violation rates increase, then measure robustness to workload distribution shifts.
3. **Cold start sensitivity analysis**: Vary the replica provisioning threshold and measure Goodput impact during burst arrivals from idle state to find optimal trade-off between utilization and SLA compliance.