---
ver: rpa2
title: 'BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification
  and Localization in AI Tutor Responses'
arxiv_id: '2506.01817'
source_url: https://arxiv.org/abs/2506.01817
tags:
- tutor
- mistake
- track
- some
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Team BD\u2019s BEA 2025 submission tackles automated assessment\
  \ of AI tutor responses using MPNet ensembles for three-class classification of\
  \ mistake identification and location in educational dialogues. By fine-tuning MPNet\
  \ with grouped cross-validation and class-weighted loss to address data imbalance,\
  \ then ensembling the best models per fold via hard voting, the system achieved\
  \ macro-F1 scores of 0.7110 (Mistake Identification) and 0.5543 (Mistake Location)\
  \ on the official test set."
---

# BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses

## Quick Facts
- arXiv ID: 2506.01817
- Source URL: https://arxiv.org/abs/2506.01817
- Reference count: 8
- Primary result: MPNet ensemble achieves macro-F1 0.7110 (Mistake Identification) and 0.5543 (Mistake Location) on BEA 2025 official test set

## Executive Summary
Team BD's BEA 2025 submission tackles automated assessment of AI tutor responses using MPNet ensembles for three-class classification of mistake identification and location in educational dialogues. By fine-tuning MPNet with grouped cross-validation and class-weighted loss to address data imbalance, then ensembling the best models per fold via hard voting, the system achieved macro-F1 scores of 0.7110 (Mistake Identification) and 0.5543 (Mistake Location) on the official test set. MPNet outperformed other Transformers in cross-validation, and ensemble voting yielded 2–3 point gains over individual models. Error analysis revealed systematic confusion between partial and full recognition, with t-SNE visualizations and confidence distributions confirming ambiguity in the "To some extent" class. Limitations include calibration issues, label subjectivity, and computational overhead. The approach offers a strong baseline for pedagogical dialogue evaluation and highlights the value of ensembles and careful cross-validation in small, imbalanced educational datasets.

## Method Summary
The system fine-tunes sentence-transformers/all-mpnet-base-v2 with 10-fold grouped cross-validation (grouped by dialogue to prevent context leakage) using class-weighted cross-entropy loss for class imbalance. Each fold trains independently with AdamW (lr=2e-5), dropout 0.1, and early stopping (patience=2), saving the best checkpoint. The final ensemble combines predictions from all 10 fold-specific models via hard voting, breaking ties using averaged softmax confidence. Input preprocessing includes lowercasing, metadata removal, code abstraction, and truncation to 300 tokens. Class weights: Track 1 [1.0, 3.0, 0.5], Track 2 [0.8, 2.2, 0.9] for [No, To some extent, Yes].

## Key Results
- MPNet ensemble achieves macro-F1 0.7110 on Mistake Identification test set
- Ensemble voting yields 2–3 point macro-F1 gains over individual models
- Class-weighted loss improves minority class recall for "To some extent" label
- Confusion matrix shows systematic errors between "To some extent" and other classes

## Why This Works (Mechanism)

### Mechanism 1: Hard Voting Ensemble Reduces Variance
- Claim: Aggregating predictions from fold-specific MPNet models via majority voting yields 2–3 point macro-F1 gains over single models.
- Mechanism: Each fold trains on different data subsets, producing models with uncorrelated errors. Hard voting cancels individual misclassifications when the majority is correct; tie-breaking uses averaged softmax confidence.
- Core assumption: Fold-specific models capture complementary error patterns rather than identical biases.
- Evidence anchors:
  - [abstract] "ensemble voting yielded 2–3 point gains over individual models"
  - [section 4.4] "Ensembling helps to reduce variance and correct individual model biases, leading to more robust predictions"
  - [corpus] Limited direct corpus corroboration; neighboring papers (NeuralNexus, MSA) also use ensemble or multi-task approaches but report varied gains.

### Mechanism 2: Class-Weighted Loss Improves Minority Class Recall
- Claim: Inverse-frequency weighting in cross-entropy boosts recall for underrepresented "To some extent" class without catastrophic performance loss on majority classes.
- Mechanism: Weight formula wc = N/(K·nc) amplifies gradient signal for rare classes, shifting decision boundaries toward majority-class regions.
- Core assumption: The label distribution reflects true class rarity rather than annotation artifact; misclassification costs are asymmetric.
- Evidence anchors:
  - [abstract] "class-weighted cross-entropy loss to handle class imbalance"
  - [section 5.1] Track 1 weights [1.0, 3.0, 0.5] for [No, Some, Yes]; "improved macro-F1 by reducing systematic underprediction of minority classes"
  - [corpus] No explicit comparison to focal loss or oversampling in neighbors; mechanism appears effective but not uniquely validated.

### Mechanism 3: Grouped Cross-Validation Prevents Context Leakage
- Claim: Grouping by dialogue ID ensures no conversation spans train/validation splits, preserving generalization estimates.
- Mechanism: Each dialogue contains 8–9 tutor responses; random splitting would leak context. Grouped splits force models to generalize across conversations, not memorize dialogue-specific patterns.
- Core assumption: Dialogue-level independence is appropriate; test distribution contains unseen conversations.
- Evidence anchors:
  - [abstract] "grouped cross-validation (10 folds) to maximize the use of limited data while avoiding dialogue overlap"
  - [section 4.3] "each dialogue (or group of dialogues) is entirely assigned to either the training or validation set"
  - [corpus] Standard practice in dialogue evaluation papers; not explicitly contested.

## Foundational Learning

- Concept: **Class Imbalance in Multi-Class Classification**
  - Why needed here: The "To some extent" class is ~7× smaller than "Yes" in Track 1, causing systematic underprediction without intervention.
  - Quick check question: If a model predicts only the majority class, what would macro-F1 be for a 3-class problem with class proportions [0.78, 0.15, 0.07]?

- Concept: **Cross-Validation for Small Datasets**
  - Why needed here: With only ~2,480 labeled examples, a held-out validation set wastes training data; k-fold CV enables full-data utilization while preserving evaluation integrity.
  - Quick check question: Why does grouped CV matter specifically for dialogue data versus standard k-fold?

- Concept: **Ensemble Diversity and Voting**
  - Why needed here: Individual models may overfit to fold-specific patterns; combining diverse classifiers smooths predictions.
  - Quick check question: When would soft voting (averaging probabilities) outperform hard voting (majority vote)?

## Architecture Onboarding

- Component map:
  Preprocessed input -> MPNet-base encoder -> [CLS] embedding -> Dropout(0.1) -> Linear(768, 3) -> Softmax -> Class-weighted cross-entropy loss

- Critical path:
  1. Map each response to its dialogue_id for group-aware splitting
  2. Compute class weights from training fold distribution (not global)
  3. Train 10 independent MPNet models, saving best checkpoint per fold
  4. Inference: collect all 10 predictions -> majority vote -> confidence tie-break

- Design tradeoffs:
  - Ensemble (10 models) vs. single model: +2–3 F1 vs. 10× inference cost
  - MPNet vs. larger LLMs: Lower compute, but struggles with implicit pedagogical cues
  - 3-class exact vs. 2-class lenient: Higher granularity vs. lower ceiling

- Failure signatures:
  - "To some extent" confusion: Model collapses partial recognition to "Yes" or "No" (113 of 174 misclassified in Track 1 dev)
  - Calibration drift: Confidence scores cluster in mid-range (1.5–3.0) with poor reliability
  - Template bias: Surface phrases like "Great work!" trigger false positives
  - Hedged language miss: Polite indirectness ("Maybe check...") under-recognized

- First 3 experiments:
  1. Baseline replication: Single MPNet, no class weights, standard random split. Expect ~0.68 macro-F1 (Table 1).
  2. Ablation: class weights: Add weighted loss to baseline. Measure per-class F1 delta, especially "To some extent."
  3. Ablation: ensemble size: Compare 5-fold vs. 10-fold vs. 15-fold ensembles. Plot macro-F1 vs. inference latency to find diminishing returns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would multi-task learning across all four BEA 2025 evaluation tracks improve Mistake Identification and Location performance?
- Basis in paper: [explicit] "In future work, we aim to explore multi-task learning across the different evaluation dimensions (e.g., training a single model to predict all four labels simultaneously). This approach could enable the model to leverage signals from one aspect (like providing guidance) to inform another (like mistake identification)."
- Why unresolved: The authors trained separate models per track, leaving potential cross-task signal transfer unexplored.
- What evidence would resolve it: Train a unified multi-task model on all four tracks and compare macro-F1 against single-task baselines on the official test set.

### Open Question 2
- Question: Would temperature scaling or Bayesian ensembling improve calibration, particularly for the ambiguous "To some extent" class?
- Basis in paper: [explicit] "Future work could explore temperature scaling or Bayesian ensembling to better calibrate prediction confidence, particularly for interpretability in high-stakes educational settings."
- Why unresolved: Confidence distributions show overconfidence on incorrect labels; calibration techniques were not applied due to time constraints.
- What evidence would resolve it: Apply Adaptive Temperature Scaling and report Expected Calibration Error and reliability diagrams before and after.

### Open Question 3
- Question: Would ordinal classification better capture the continuum between "Yes," "To some extent," and "No" than nominal 3-way classification?
- Basis in paper: [explicit] "Modeling the task as ordinal or probabilistic, rather than categorical, could better capture this continuum."
- Why unresolved: Error analysis reveals systematic partial-full confusion; the inherently ordered labels were treated as nominal categories.
- What evidence would resolve it: Implement ordinal classification (e.g., cumulative logit loss) and compare confusion patterns and macro-F1 against the categorical baseline.

## Limitations

- Data access and preprocessing variability: BEA 2025 dataset requires coordination with organizers; preprocessing steps may impact results
- Class weight implementation ambiguity: Two different formulations provided, creating uncertainty about actual implementation
- Calibration limitations: Confidence scores show poor reliability with clustering in mid-range, suggesting ensemble tie-breaking may not be well-founded

## Confidence

**High confidence** (Multiple independent evidence points, no contradictions):
- MPNet outperformed other Transformers in cross-validation
- Grouped cross-validation prevents context leakage
- Ensemble voting yields 2-3 point macro-F1 gains over individual models
- Systematic confusion between "To some extent" and other classes

**Medium confidence** (Single evidence point or minor contradictions):
- Class-weighted loss improves minority class recall
- 10-fold CV maximizes data usage while avoiding overlap
- Confidence-based tie-breaking improves ensemble decisions

**Low confidence** (Contradictory evidence or missing implementation details):
- Specific class weight values used during training
- Reason for Track 2 using 7 vs 10 ensemble models
- Whether code abstraction or other preprocessing steps significantly impact performance

## Next Checks

1. **Class weight ablation study**: Train the same MPNet model with three variants: (a) no class weights, (b) Track 1 weights [1.0, 3.0, 0.5], (c) focal loss with γ=2.0. Compare macro-F1 and per-class F1 scores to verify that class-weighted cross-entropy provides the claimed 2-3 point improvement specifically for the "To some extent" class.

2. **Ensemble size sensitivity analysis**: Train MPNet models using 5, 10, and 15 folds with identical hyperparameters. Plot macro-F1 vs. inference latency to identify the point of diminishing returns. Verify whether the 2-3 point gain from 10-fold ensemble is statistically significant compared to 5-fold.

3. **Calibration verification**: Apply temperature scaling to the ensemble's softmax outputs using a held-out calibration set. Measure expected calibration error (ECE) before and after scaling, and verify whether post-hoc calibration improves the reliability of confidence-based tie-breaking decisions.