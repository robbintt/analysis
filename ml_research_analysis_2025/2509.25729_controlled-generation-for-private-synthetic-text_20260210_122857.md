---
ver: rpa2
title: Controlled Generation for Private Synthetic Text
arxiv_id: '2509.25729'
source_url: https://arxiv.org/abs/2509.25729
tags:
- privacy
- private
- text
- generation
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for generating synthetic text that
  preserves utility while protecting privacy by leveraging de-identification and Hiding
  In Plain Sight theory. The approach uses entity-aware control codes to guide controllable
  generation via either in-context learning (ICL) or prefix tuning, ensuring that
  real sensitive information is not reproduced.
---

# Controlled Generation for Private Synthetic Text

## Quick Facts
- arXiv ID: 2509.25729
- Source URL: https://arxiv.org/abs/2509.25729
- Authors: Zihao Zhao; Anjalie Field
- Reference count: 17
- Primary result: Entity-aware control codes enable synthetic text generation with PIPP leakage rates of 0.77–3.25% while maintaining MAUVE utility scores of 0.66–0.82

## Executive Summary
This paper proposes a method for generating privacy-preserving synthetic text that prevents leakage of sensitive identifiers while maintaining utility. The approach leverages entity-aware control codes to guide controllable generation via either in-context learning (ICL) or prefix tuning, ensuring real sensitive information is not reproduced. Experiments on legal and clinical datasets show the method achieves low leakage rates while maintaining high text utility, with the ICL variant with privacy enhancement offering the strongest privacy guarantees.

## Method Summary
The method extracts entities from documents and formats them as control codes (e.g., `PERSON: Alice Jones`). During generation, "fictional" codes containing fake values are substituted to force the model to generate text grounded in synthetic data. Two variants are proposed: (1) ICL with privacy enhancement that uses "bad words" lists to hard-block sensitive tokens during decoding, and (2) prefix tuning with masking that uses a custom loss function combining contrastive divergence and KL regularization to "unlearn" private spans while preserving general utility. The approach is evaluated on TAB (legal) and MIMIC-III (clinical) datasets using Sheared-LLaMA 1.3B.

## Key Results
- PIPP leakage rates: 0.77–3.25% in known entity settings, 2.26–3.25% in unknown entity settings
- ELP scores: 0.36–0.67% across datasets
- MAUVE utility scores: 0.66–0.82 indicating strong language quality preservation
- ICL with privacy enhancement achieves zero leakage but higher ROUGE similarity to source
- Prefix tuning with masking provides best balance of privacy and utility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring sensitive information as explicit control codes allows a language model to treat identifiers as mutable variables rather than fixed context.
- **Mechanism:** The method extracts entities and formats them as control codes. During training or in-context learning, the model conditions on these codes to generate text. At inference, "fictional" codes containing fake values are substituted. This forces the model to generate text grounded in the synthetic code, effectively overwriting the real data associations.
- **Core assumption:** The model can learn the syntactic relationship between the control code and the corresponding span in the text well enough to replicate the pattern with new, unseen values.
- **Evidence anchors:** [abstract] "Our approach introduces entity-aware control codes to guide controllable generation..."; [section 2.1] "control codes have been used to guide models to produce outputs with desired properties... fictional control codes direct the model to generate fake sensitive information..."

### Mechanism 2
- **Claim:** Hard-blocking specific tokens during decoding provides a verifiable privacy floor by eliminating the possibility of generating exact matches from the prompt.
- **Mechanism:** In the ICL variant, the method constructs a "bad words" list containing the specific private tokens present in the few-shot examples. During generation, the probability of these tokens is set to zero. This ensures that even if the model "wants" to copy the example, it physically cannot output the sensitive token.
- **Core assumption:** The "bad words" list covers all variations (case, sub-words) of the sensitive entity.
- **Evidence anchors:** [section 2.2] "...explicitly designate these values as a set of 'bad' tokens... assigned low or zero probability..."; [section 4.1] "...forcing leakage to zero is still possible by repeating each generation..."

### Mechanism 3
- **Claim:** A custom loss function combining contrastive divergence and KL regularization allows the model to "unlearn" specific private spans while preserving general linguistic utility.
- **Mechanism:** The prefix tuning variant uses masking. It maximizes divergence of the fine-tuned model from the base model on private tokens (Contrastive Loss) to prevent memorization, while minimizing divergence on non-private tokens (KL Loss) to maintain fluency.
- **Core assumption:** Private tokens can be mathematically isolated in the loss landscape without degrading the surrounding context understanding.
- **Evidence anchors:** [section 2.3] "...encourages the fine-tuned model Pθ to deviate from the base model Pbase specifically on private tokens..."; [section 4.3] "Fine-tuning with masking strikes the best balance between utility and privacy..."

## Foundational Learning

- **Concept: Conditional Generation (P(x|c))**
  - **Why needed here:** The entire architecture relies on the premise that text generation can be conditioned on specific variables (control codes). Without understanding that the model predicts the next token based on both the prior text *and* the control code, the data swapping mechanism is opaque.
  - **Quick check question:** If I change the control code from `LOC: London` to `LOC: Paris` but keep the prompt text identical, does the model's output distribution change?

- **Concept: Hiding In Plain Sight (HIPS)**
  - **Why needed here:** This is the theoretical defense. It posits that realistic fake data is safer than redaction because it obscures residual identifiers.
  - **Quick check question:** Why might a document with realistic fake names be harder to de-anonymize than one with all names replaced by `[REDACTED]`?

- **Concept: Memorization in LLMs**
  - **Why needed here:** The paper explicitly counters the tendency of LLMs to memorize and regurgitate training data (canonical examples like Carlini et al. 2021). Understanding this risk explains why simple fine-tuning is insufficient and why active counter-measures (bad tokens, contrastive loss) are required.
  - **Quick check question:** If an exact sentence from the training set appears in the output, is that a failure of generation or a failure of memorization defense?

## Architecture Onboarding

- **Component map:** Entity Extractor -> Control Code Builder -> Synthetic Code Generator -> Generator (LLM) -> Validator
- **Critical path:** The accuracy of the **Entity Extractor**. If the extractor misses a quasi-identifier, it will not be flagged for masking or swapping, leading to potential leakage in the "private entity unknown" setting.
- **Design tradeoffs:**
  - ICL with Privacy Enhancement: Strongest privacy (hard blocks), lowest latency (no training), but *high* ROUGE scores (text looks similar to source)
  - Prefix Tuning with Masking: Best balance of utility and privacy, lower ROUGE (better synthesis), but requires training and has non-zero leakage risk
- **Failure signatures:**
  - High ELP in ICL: Check tokenization. The paper notes "Apple" vs "apple" issues. Ensure the bad word list includes all sub-word/case variants
  - Low MAUVE in Prefix Tuning: Lambda weights (λ_contrastive) may be too high, causing the model to diverge too far from natural language on non-private tokens
- **First 3 experiments:**
  1. **Sanity Check (ICL):** Run the ICL pipeline with "bad tokens" enabled. Attempt to generate text containing a blocked name. If successful, debug the tokenizer/blocking list implementation
  2. **Hyperparameter Sweep (Prefix Tuning):** Vary the λ_contrastive and λ_KL weights. Plot the curve of PIPP (Privacy) vs. Perplexity (Utility) to find the optimal operating point
  3. **Robustness Test (Unknown Entity):** Run the pipeline using the automatic "Presidio" entity recognizer instead of ground-truth tags. Measure the degradation in PIPP to estimate real-world performance

## Open Questions the Paper Calls Out

- **Question:** How does the privacy-utility balance hold up across a broader range of domains and dataset scales beyond the legal and clinical datasets tested?
  - **Basis:** [explicit] "we evaluate our methods on only two datasets," and suggest future work should "include a broader range of datasets across different domains and scales"
  - **Why unresolved:** The study is currently restricted to the TAB (legal) and MIMIC-III (clinical) datasets
  - **What evidence would resolve it:** Experimental results on synthetic data generation for datasets from diverse domains (e.g., financial records, social media) and of varying sizes

- **Question:** To what extent can the privacy-utility trade-off be optimized through hyperparameter tuning?
  - **Basis:** [explicit] "we did not perform hyperparameter tuning; it is possible that further tuning could improve the method’s performance," specifically regarding the loss weighting coefficients (λ) and the number of in-context examples
  - **Why unresolved:** The experiments relied on fixed default values and "assumed equal importance" for loss components
  - **What evidence would resolve it:** An ablation study showing the sensitivity of PIPP, ELP, and MAUVE scores to changes in λ_LM, λ_contrastive, λ_KL, and the number of ICL shots

- **Question:** How does the performance of this control code methodology vary across a wider range of language model architectures and parameter counts?
  - **Basis:** [explicit] "all experiments are conducted using the Sheared LLaMA 1.3B model" and suggest that "evaluating our methods on larger range of language models would help determine how performance varies"
  - **Why unresolved:** While the authors performed a small ablation on a larger model (Llama 3.1-8B), the primary validation is confined to a single 1.3B parameter model architecture
  - **What evidence would resolve it:** Benchmarks of the ICL and prefix tuning methods across different model families (e.g., GPT, Mistral) and a spectrum of model sizes

## Limitations

- The method's performance heavily depends on accurate entity recognition, with automatic tools like Presidio achieving significantly higher leakage rates (2.26–3.25% PIPP) compared to ground-truth tagging (0.77–0.67% PIPP)
- The paper doesn't evaluate against other synthetic data generation approaches like GANs or VAEs, limiting the claim that prefix tuning with masking provides the "best balance"
- Fictional control code generation relies on public lists that aren't fully specified, introducing variability in reproduction attempts
- The connection between ROUGE scores, MAUVE metrics, and actual downstream task performance remains unclear

## Confidence

- **High Confidence:** The core mechanism of using control codes to guide generation is well-specified and theoretically sound
- **Medium Confidence:** The claim that ICL with privacy enhancement offers "strongest privacy guarantees" is supported by zero-leakage results but may be brittle in practice
- **Low Confidence:** The assertion that this approach generalizes well to "unseen domains" is primarily theoretical, as experiments only cover legal and clinical domains

## Next Checks

1. **Entity Extraction Robustness Test:** Evaluate the pipeline using multiple entity extractors (Presidio, Stanza, SpaCy) on the same datasets to quantify variance in PIPP scores and identify which entity types are most frequently missed

2. **Downstream Task Performance:** Apply the generated synthetic text to actual downstream tasks (e.g., legal document classification, clinical named entity recognition) to verify that MAUVE scores correlate with practical utility beyond just language modeling metrics

3. **Cross-Domain Generalization:** Test the approach on a third domain (e.g., financial reports or social media text) to validate the claim about performance on "unseen domains" and identify any domain-specific failure patterns in entity recognition or control code generation