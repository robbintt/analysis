---
ver: rpa2
title: LLM Sensitivity Evaluation Framework for Clinical Diagnosis
arxiv_id: '2504.13475'
source_url: https://arxiv.org/abs/2504.13475
tags:
- llms
- sensitivity
- medical
- information
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LLMSenEval, a framework for evaluating the\
  \ sensitivity of large language models (LLMs) to key medical information in clinical\
  \ diagnosis. The authors propose four types of key medical information\u2014gender,\
  \ age, symptoms, and checkup results\u2014and design two perturbation strategies:\
  \ change and removal."
---

# LLM Sensitivity Evaluation Framework for Clinical Diagnosis

## Quick Facts
- arXiv ID: 2504.13475
- Source URL: https://arxiv.org/abs/2504.13475
- Reference count: 6
- Primary result: LLMs show significant sensitivity gaps to key medical information perturbations, with GPT-4 achieving only 5.28% accuracy on questions where correct answers should change

## Executive Summary
This paper introduces LLMSenEval, a framework for evaluating how sensitive large language models are to changes in key medical information during clinical diagnosis. The authors construct the DiagnosisQA dataset from MedQA, containing 4,603 case-based questions, and create eight perturbed datasets by modifying gender, age, symptoms, and checkup results. Testing five state-of-the-art LLMs reveals that while models perform well on standard benchmarks (GPT-4 achieves 78.95% accuracy), they show alarming insensitivity to clinically meaningful changes—correctly answering only 5.28% of questions where perturbations should change the diagnosis.

## Method Summary
The framework extracts four key medical information types (gender, age, symptoms, checkup results) from case-based questions using regex/keyword patterns, applies two perturbation strategies (change and removal), and splits perturbed questions into Same Answer Subset (SAS) and Different Answer Subset (DAS). Four physicians annotate ground truth answers for perturbed datasets, confirming whether perturbations change correct diagnoses. Five LLMs are evaluated using accuracy, precision, recall, F1-score, response rate, and instruction-following rate, with prompts requiring JSON-formatted outputs. The evaluation distinguishes between appropriate stability (SAS) and dangerous insensitivity (DAS).

## Key Results
- GPT-4 achieves highest accuracy (78.95%) on original dataset but only 5.28% on DAS questions where answers should change
- LLaMA2-7b achieves 0% followed-instruction rate despite 97.84% response rate, indicating format compliance failure
- GPT-3.5-Turbo-0613 shows 4.9% accuracy drop on SAS questions, indicating over-sensitivity to irrelevant changes
- All tested LLMs demonstrate significant gaps in detecting and responding to critical changes in key medical information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted perturbation of clinically-relevant information reveals sensitivity gaps that standard benchmarks miss.
- Mechanism: The framework extracts four key medical information types (gender, age, symptoms, checkup results) from case-based questions, applies two perturbation strategies (change and removal), and measures whether the model correctly adjusts its diagnosis. By splitting perturbed questions into Same Answer Subset (SAS) and Different Answer Subset (DAS), the evaluation distinguishes between appropriate stability and dangerous insensitivity.
- Core assumption: If LLMs truly "think like physicians," they should maintain accuracy when perturbations don't affect diagnosis (SAS) and correctly change answers when perturbations do affect diagnosis (DAS).
- Evidence anchors: [abstract] "GPT-4 correctly answers only 345 out of 6,528 questions (5.28% accuracy)" on DAS questions; [section 6] "The first category is called Same Answer Subset (SAS)... The second category is called Different Answer Subset (DAS)"

### Mechanism 2
- Claim: Multi-metric evaluation captures both capability and reliability dimensions.
- Mechanism: Beyond accuracy, the framework measures response rate (RR) and followed-instruction rate (FIR) to ensure models produce valid, structured outputs. This prevents rewarding models that avoid difficult cases or ignore format requirements.
- Core assumption: A clinically useful LLM must both answer correctly AND follow output specifications reliably.
- Evidence anchors: [section 4.3] "RR = #validR / N" and "FIR = #followedInstructionR / #validR"; [Table 3] LLaMA2-7b achieves 0% FIR despite 97.84% RR

### Mechanism 3
- Claim: Physician-annotated ground truth ensures clinical validity of sensitivity measurements.
- Mechanism: Four professional physicians reviewed and corrected answers for perturbed datasets, confirming whether each perturbation actually changes the correct diagnosis. This prevents evaluation noise from incorrectly labeled examples.
- Core assumption: Physician consensus on "correct answer after perturbation" reflects ground truth for clinical reasoning.
- Evidence anchors: [section 4.1] "we annotate the derived datasets with the help of four professional physicians... confirming that the answers... have been reviewed and corrected by human doctors"

## Foundational Learning

- Concept: **Perturbation-based robustness testing**
  - Why needed here: Understanding how to systematically test model sensitivity requires knowing how controlled input modifications reveal brittleness versus appropriate stability.
  - Quick check question: If you swap "fever" to "no fever" in a pneumonia case, should accuracy change? How would you design a test to distinguish between models that ignore the change versus models that correctly adjust?

- Concept: **Clinical reasoning vs. pattern matching**
  - Why needed here: The paper's core contribution is showing that benchmark performance (78.95% accuracy) doesn't guarantee sensitivity to clinically-meaningful information changes.
  - Quick check question: A model achieves 90% on MedQA. What additional test would reveal whether it understands disease mechanisms versus surface correlations?

- Concept: **Evaluation dataset construction from existing resources**
  - Why needed here: DiagnosisQA was built by filtering MedQA for case-based questions, then generating perturbations. Understanding this pipeline is essential for extending the framework.
  - Quick check question: Given a medical QA dataset, what filtering criteria would you apply to isolate questions where specific information (e.g., age, gender) is diagnostically relevant?

## Architecture Onboarding

- Component map: MedQA → Filter → DiagnosisQA (4,603 questions) → Extract Key Info (regex/keyword) → Apply Perturbations (8 strategies) → Physician Annotation → LLM Testing (structured prompts) → Response Parsing & Metric Computation

- Critical path: The DAS evaluation is the core signal—SAS measures stability (lower Δ accuracy = better), DAS measures sensitivity (higher accuracy = better). GPT-4's 5.28% DAS accuracy is the headline reliability gap.

- Design tradeoffs:
  - Regex/keyword extraction vs. NER model: Authors chose simpler method (Chapman 2001 approach), which may miss complex expressions but is transparent
  - QA format vs. EMR format: Simpler but less realistic (acknowledged in limitations)
  - Four info types only: Excludes family history, medications, social factors

- Failure signatures:
  - Model gives same answer after perturbation that should change diagnosis → dangerous insensitivity
  - Model's accuracy drops significantly on SAS questions → over-sensitive to irrelevant changes (Gemini showed this)
  - Model refuses to answer or ignores format → low RR/FIR (LLaMA2 had 0% FIR)

- First 3 experiments:
  1. **Baseline replication**: Run DiagnosisQA original dataset through target LLM with paper's exact prompt format ("You are a medical expert..."), measure accuracy/RR/FIR. Compare to Table 3 benchmarks.
  2. **Single perturbation test**: Apply only symptom-change perturbation to 100 questions, manually verify DAS classification, measure model's accuracy change. This isolates one variable before full pipeline.
  3. **Error analysis on DAS failures**: For questions where model fails to detect answer change, categorize failure modes (ignored perturbation? wrong alternative diagnosis? format error?). This guides targeted improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does LLM sensitivity to key medical information generalize from curated MedQA-derived questions to real-world Electronic Medical Records (EMRs)?
- Basis in paper: [explicit] The limitations section notes clinical EMRs are "generally longer and more comprehensive" and states "there remains a gap compared to our investigated results" when applying LLMs to clinical practice.
- Why unresolved: DiagnosisQA uses expert-validated, case-based questions with highly relevant information, whereas real EMRs contain longer, noisier, and less curated data that may affect sensitivity differently.
- What evidence would resolve it: Replication of LLMSenEval evaluation on de-identified real EMR data with similar perturbation strategies, comparing sensitivity metrics to DiagnosisQA benchmarks.

### Open Question 2
- Question: How does LLM sensitivity performance change when the evaluation framework is extended to include additional clinically relevant factors such as family medical history, medication history, and social determinants of health?
- Basis in paper: [explicit] The limitations section states "there are other aspects not covered, such as family medical history. All of this information plays a pivotal role in guiding doctors to make accurate diagnosis."
- Why unresolved: LLMSenEval only tested four key information types (gender, age, symptoms, checkup results), leaving other critical diagnostic factors unexamined.
- What evidence would resolve it: Extension of the framework with additional key information categories, construction of new derived datasets with corresponding perturbations, and comparative evaluation of the same LLMs.

### Open Question 3
- Question: What training approaches or architectural modifications can reduce LLM sensitivity to diagnostically irrelevant changes while enhancing sensitivity to clinically significant perturbations?
- Basis in paper: [explicit] The conclusion states "there is still a long way to go in reducing the sensitivity of LLMs to unimportant perturbations while enhancing their sensitivity to crucial ones."
- Why unresolved: The paper identifies the problem (low DAS accuracy of 5.28% for GPT-4) but does not propose or test improvement methods.
- What evidence would resolve it: Comparative evaluation of LLMs fine-tuned with sensitivity-aware loss functions or specialized medical attention mechanisms, demonstrating improved DAS accuracy while maintaining SAS stability.

## Limitations

- The framework relies on case-based QA format rather than real-world EMRs, limiting generalizability to actual clinical practice
- Perturbation strategies may not capture all clinically relevant information types, excluding factors like family history and medications
- Physician annotation process lacks detailed inter-rater agreement metrics, raising questions about annotation reliability

## Confidence

- **High confidence**: The framework's core design (perturbation-based sensitivity testing) is well-justified and the measurement methodology is sound. The observation that GPT-4 achieves only 5.28% accuracy on DAS questions despite 78.95% baseline accuracy is robust.
- **Medium confidence**: The generalizability of findings to other medical domains and real clinical settings is uncertain due to the QA format limitation. The specific perturbation strategies may not capture all clinically relevant information types.
- **Low confidence**: The framework's ability to distinguish between appropriate stability and dangerous insensitivity in edge cases where perturbations create ambiguous clinical scenarios.

## Next Checks

1. **Clinical workflow validation**: Test the framework using actual EMR extracts rather than case-based QA questions to assess whether sensitivity patterns hold in more realistic clinical contexts.

2. **Multi-factor perturbation study**: Design experiments that simultaneously perturb multiple key information types to capture interaction effects that single-factor perturbations miss.

3. **Annotation reliability assessment**: Conduct inter-rater reliability analysis on physician annotations, measuring Cohen's kappa for SAS/DAS classifications and identifying sources of disagreement to improve annotation protocols.