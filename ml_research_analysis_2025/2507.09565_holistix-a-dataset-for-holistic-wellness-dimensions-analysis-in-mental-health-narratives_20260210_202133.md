---
ver: rpa2
title: 'Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health
  Narratives'
arxiv_id: '2507.09565'
source_url: https://arxiv.org/abs/2507.09565
tags:
- mental
- health
- wellness
- social
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Holistix, a dataset for classifying wellness\
  \ dimensions in mental health narratives from social media, specifically from Australia\u2019\
  s Beyond Blue forum. The dataset captures six wellness dimensions\u2014physical,\
  \ emotional, social, intellectual, spiritual, and vocational\u2014using an annotation\
  \ framework developed with domain experts."
---

# Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives

## Quick Facts
- arXiv ID: 2507.09565
- Source URL: https://arxiv.org/abs/2507.09565
- Authors: Heba Shakeel; Tanvir Ahmad; Chandni Saxena
- Reference count: 34
- Primary Result: MentalBERT achieves 0.74 accuracy on 6-class wellness dimension classification from Australian mental health forum posts

## Executive Summary
This paper introduces Holistix, a dataset for classifying wellness dimensions in mental health narratives from social media, specifically from Australia's Beyond Blue forum. The dataset captures six wellness dimensions—physical, emotional, social, intellectual, spiritual, and vocational—using an annotation framework developed with domain experts. The authors evaluate traditional ML models (SVM, Logistic Regression, Naive Bayes) and transformer models (BERT, DistilBERT, MentalBERT, Flan-T5, XLNet, GPT-2.0) for multi-class classification, with MentalBERT achieving the highest accuracy of 0.74 and strong performance across dimensions. LIME is used for explainability, showing MentalBERT also provides better interpretable results. The dataset contributes to region-specific wellness assessments and supports personalized well-being evaluation and early mental health intervention strategies.

## Method Summary
The Holistix dataset contains 1,420 annotated posts from Australia's Beyond Blue mental health forum, processed through BeautifulSoup scraping and preprocessing to remove duplicates and empty posts. Domain experts annotated posts using a six-dimension wellness framework (Physical, Emotional, Social, Intellectual, Spiritual, Vocational) with structured guidelines including dominant dimension resolution and perplexity rules. Models were evaluated using 10-fold cross-validation with precision, recall, F1, and accuracy metrics. MentalBERT was fine-tuned with learning rate 1e-3, batch size 16, and 10 epochs, while other transformers used similar hyperparameters. LIME provided post-hoc explainability with ROUGE/BLEU comparison to human-annotated text spans.

## Key Results
- MentalBERT achieves highest accuracy of 0.74 on 6-class wellness classification
- Traditional ML models underperform on semantic understanding, particularly for Intellectual and Vocational aspects
- LIME explanations from MentalBERT show better alignment with human annotations (F1~0.45) than Logistic Regression
- Emotional and Spiritual dimensions show consistently lower performance across all models due to subjective nature

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Pre-training Transfers to Wellness Classification
MentalBERT achieves higher accuracy (0.74) than general-purpose BERT (0.65) because pre-training on mental health corpora creates representations that better capture wellness-related language patterns before fine-tuning. Domain-specific pre-training enables the model to learn specialized vocabulary, discourse patterns, and semantic relationships specific to mental health narratives, which transfer to the downstream six-class wellness classification task. The paper acknowledges this is an Australian-specific dataset; cross-cultural validation is not provided.

### Mechanism 2: Structured Annotation Framework with Dominant Dimension Resolution
The six-dimension wellness model combined with explicit class indicators and perplexity guidelines enables consistent classification of ambiguous text. Clear annotation rules (prioritize dominant dimension, use context clues, break compound sentences) reduce annotator disagreement and provide models with cleaner training signals. The paper explicitly notes this breaks for "emotional and spiritual dimensions due to their inherently subjective nature," with examples like "I don't belong anywhere" being ambiguous between Social and Emotional aspects.

### Mechanism 3: Post-hoc Explanation Alignment Correlates with Model Performance
Better-performing models produce LIME explanations that more closely match human-annotated text spans, suggesting improved semantic understanding rather than spurious correlations. Models that learn more accurate wellness dimension representations naturally highlight more relevant text spans when probed, creating interpretable outputs aligned with expert reasoning. The gap between model explanations and human annotations suggests room for improvement.

## Foundational Learning

- Concept: **Transfer Learning for Domain-Specific NLP**
  - Why needed here: Understanding why MentalBERT outperforms general BERT requires knowledge of how pre-training on domain corpora affects downstream task performance through feature reuse.
  - Quick check question: Why would a model pre-trained on mental health text better classify "I feel exhausted all the time" as Physical Aspect than one pre-trained on Wikipedia?

- Concept: **Multi-class vs Multi-label Classification**
  - Why needed here: The paper uses single-label classification (one dominant dimension), but acknowledges overlapping dimensions. Future work proposes multi-label classification.
  - Quick check question: Given a post like "My job drains me and I have no friends," would single-label or multi-label classification better capture its wellness dimensions?

- Concept: **Inter-annotator Agreement (Fleiss' Kappa)**
  - Why needed here: Understanding κ = 75.92% requires knowing this metric measures agreement beyond chance, and what constitutes acceptable reliability for subjective tasks.
  - Quick check question: If two annotators label "I don't belong anywhere" differently (Social vs Emotional), how does this affect both κ and model training?

## Architecture Onboarding

- Component map:
  Data Pipeline: Beyond Blue Forum -> BeautifulSoup scraping -> Preprocessing -> Expert annotation -> 1,420 labeled posts
  Model Architecture: Transformer encoder (MentalBERT/BERT-family) -> Classification head (6-class softmax)
  Explainability Layer: LIME post-hoc explanations -> ROUGE/BLEU comparison to human spans
  Evaluation: 10-fold cross-validation with precision/recall/F1 per class

- Critical path:
  1. Data collection and preprocessing (filter to 1,420 posts from 2,000 raw)
  2. Expert annotation with structured guidelines (κ = 75.92%)
  3. Fine-tune MentalBERT (LR=1e-3, batch=16, epochs=10)
  4. 10-fold cross-validation evaluation (target accuracy ~0.74)
  5. LIME explainability analysis (target F1 ~0.45 vs human spans)

- Design tradeoffs:
  - Single-label vs multi-label: Single dominant dimension simplifies classification but loses nuance; future work proposes multi-label
  - Dataset size vs model complexity: 1,420 posts limits model capacity; traditional ML fails on semantic understanding
  - Cultural specificity: Australian-only data may not generalize; cross-cultural differences noted
  - Explainability depth: LIME provides post-hoc explanations but moderate alignment (F1=0.45) suggests gap between model attention and human reasoning

- Failure signatures:
  - Emotional Aspect consistently underperforms across all models (MentalBERT F1=0.48, GPT-2.0 F1=0.36)
  - Traditional ML fails on Intellectual/Vocational aspects (LR IA F1=0.25 vs MentalBERT 0.72)
  - Low agreement for subjective dimensions—explicitly noted in limitations for emotional/spiritual aspects

- First 3 experiments:
  1. Reproduce MentalBERT baseline: Clone repository, fine-tune with specified hyperparameters, evaluate via 10-fold CV, verify ~0.74 accuracy and LIME F1~0.45
  2. Multi-label classification pilot: Re-annotate 200 posts allowing multiple dimensions, train binary relevance classifier, compare utility to single-label approach
  3. Cross-platform generalization test: Evaluate Holistix-trained MentalBERT on Reddit mental health data, measure performance drop and analyze cultural/platform shift patterns

## Open Questions the Paper Calls Out

### Open Question 1
Can a multi-label classification approach improve performance on posts containing overlapping wellness dimensions compared to the current single-label "dominant dimension" framework? The authors state in the Conclusion, "Future work will explore multi-label classification to better handle overlapping wellness dimensions." The current annotation guidelines force annotators to prioritize a single "Dominant Dimension" for multi-faceted text, potentially losing granular data.

### Open Question 2
How do state-of-the-art Large Language Models (LLMs) compare to the fine-tuned MentalBERT baseline in classifying wellness dimensions and generating explanatory spans? The Conclusion notes, "we plan to incorporate large language models... to further enhance model explainability and improve overall classification accuracy." The current study only benchmarks traditional ML models and smaller transformer models, leaving modern LLMs untested.

### Open Question 3
Can annotation guidelines or model architectures be refined to effectively disambiguate the subjective boundaries between Emotional (EA) and Spiritual (SpiA) dimensions? The Limitations section highlights the "difficulty in accurately identifying emotional and spiritual dimensions due to their inherently subjective nature," noting low model performance (MentalBERT F1=0.48 for EA). Phrases like "I feel lost" are interpreted variably as Spiritual or Emotional, leading to lower inter-annotator agreement and model confusion.

## Limitations

- Dataset size of 1,420 posts limits model capacity and generalizability
- Australian-specific forum data may not transfer to other cultural contexts
- Emotional and spiritual dimensions show consistently poor performance (F1 scores below 0.5) due to inherent subjectivity and overlap with other classes
- Single-label approach discards multi-dimensional nuance that mental health narratives often contain

## Confidence

- High: MentalBERT outperforms general BERT (0.74 vs 0.65 accuracy) on the Australian dataset
- Medium: LIME explanations moderately align with human annotations (F1~0.45), suggesting reasonable but imperfect interpretability
- Low: Cross-cultural validity of wellness dimension classification from Australian data to other regions

## Next Checks

1. **Multi-label Classification Validation**: Re-annotate 200 posts allowing multiple wellness dimensions, train binary relevance classifier, and compare performance and utility to the single-label approach. This tests whether the dominant-dimension assumption limits practical usefulness.

2. **Cross-Platform Generalization Test**: Evaluate the Holistix-trained MentalBERT model on mental health data from Reddit or other international forums. Measure performance drop and analyze whether cultural or platform differences significantly impact wellness classification accuracy.

3. **Explainability Gap Analysis**: Conduct qualitative review of 50 LIME explanations comparing model-highlighted text spans to human-annotated dimensions. Identify systematic patterns where model attention diverges from expert reasoning, particularly for emotional and spiritual aspects where alignment is poorest.