---
ver: rpa2
title: Exploring Database Normalization Effects on SQL Generation
arxiv_id: '2510.01989'
source_url: https://arxiv.org/abs/2510.01989
tags:
- schema
- data
- normalization
- queries
- nl2sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first systematic study of how database
  normalization affects natural language to SQL (NL2SQL) systems. The authors evaluate
  eight leading large language models across three experimental settings: synthetic
  datasets with formal normalization levels (1NF-3NF), simulated real-world scenarios,
  and practical real-world academic publication data.'
---

# Exploring Database Normalization Effects on SQL Generation

## Quick Facts
- arXiv ID: 2510.01989
- Source URL: https://arxiv.org/abs/2510.01989
- Reference count: 40
- This paper presents the first systematic study of how database normalization affects natural language to SQL (NL2SQL) systems.

## Executive Summary
This paper investigates how database normalization impacts the performance of natural language to SQL (NL2SQL) systems. Through systematic evaluation of eight leading large language models across synthetic and real-world datasets with varying normalization levels (1NF-3NF), the authors demonstrate that denormalized schemas excel at simple retrieval queries while normalized schemas perform better for aggregation queries. The study reveals that schema design significantly affects NL2SQL performance, with optimal normalization levels depending on query types. These findings suggest that NL2SQL systems should consider adaptive schema selection based on workload characteristics.

## Method Summary
The authors evaluate eight LLMs (GPT-4o/4o-mini/4.1/4.1-mini, Gemini 1.5 Pro/2.0 Flash, Claude 3.5/3.7 Sonnet) across three experimental settings: synthetic datasets with formal normalization levels (1NF-3NF), simulated real-world scenarios, and practical real-world academic publication data. The evaluation uses zero-shot and few-shot (5 demonstration pairs) prompting on query templates covering retrieval and aggregation tasks. The study employs standardized prompt engineering without advanced workflow optimizations, measuring execution accuracy with 60-second timeouts.

## Key Results
- Denormalized schemas achieve superior performance on simple retrieval queries, even with cost-effective models in zero-shot settings
- Normalized schemas (2NF/3NF) perform better for aggregation queries due to robustness against data duplication and NULL value issues
- Optimal normalization levels depend on query types, with potential benefits from adaptive schema selection based on workload characteristics

## Why This Works (Mechanism)
The effectiveness of different normalization levels depends on the inherent trade-offs between data redundancy and query complexity. Denormalized schemas simplify query construction by eliminating the need for complex JOIN operations, making them ideal for straightforward retrieval tasks. However, they introduce data redundancy that complicates aggregation queries through duplicate counting. Normalized schemas, while requiring more complex query construction with JOINs and handling of NULL values, provide cleaner data structures that prevent overcounting in aggregation scenarios. This fundamental database principle explains why the optimal schema design varies by query type.

## Foundational Learning
- Database normalization concepts (1NF, 2NF, 3NF): Understanding normal forms is essential to grasp why different schemas perform differently on various query types. Quick check: Can you explain the key difference between 2NF and 3NF?
- NL2SQL task fundamentals: Knowledge of how natural language queries map to SQL is crucial for understanding the evaluation context. Quick check: What are the main challenges in converting natural language to SQL?
- Query optimization principles: Understanding how JOINs, DISTINCT, and aggregation functions affect query results is vital for interpreting performance differences. Quick check: How does using INNER JOIN vs LEFT JOIN affect results with NULL values?

## Architecture Onboarding
Component map: LLMs -> Prompt templates -> Schema normalization levels -> Query templates -> Execution accuracy
Critical path: Prompt generation → SQL generation → Query execution → Result validation
Design tradeoffs: Zero-shot vs few-shot prompting, prompt engineering simplicity vs effectiveness
Failure signatures: Incorrect JOIN types in normalized schemas causing row omission; missing DISTINCT/COUNT(DISTINCT) in denormalized schemas causing duplicate overcounting
First experiments:
1. Test Formal-Basic synthetic data with FDT tables at different normalization levels to verify expected performance gaps
2. Validate Practical-Real dataset with LOW/MID/HIGH schemas on retrieval vs aggregation queries
3. Evaluate prompt engineering variations on baseline models to establish sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt templates and few-shot example selection criteria are not disclosed
- Synthetic data generation process and probability models are underspecified
- Ground truth SQL construction methodology for practical real-world dataset is not detailed
- The study does not evaluate hybrid approaches or practical feasibility of dynamic normalization selection

## Confidence
High confidence: General finding that denormalized schemas perform better on simple retrieval queries and normalized schemas on aggregation queries
Medium confidence: Specific performance gaps and LLM rankings due to undisclosed prompt engineering choices
Low confidence: Recommendation for adaptive schema selection without evaluation of hybrid approaches

## Next Checks
1. Reproduce Formal-Basic results with synthetic data to verify performance gaps between 1NF and 2NF/3NF in zero-shot mode
2. Test Practical-Real dataset subset with both normalized and denormalized schemas to verify aggregation query performance advantage
3. Evaluate impact of different prompt engineering approaches on baseline models to establish sensitivity to prompt variations