---
ver: rpa2
title: Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability
  in Mind for Better Reasoning
arxiv_id: '2505.14582'
source_url: https://arxiv.org/abs/2505.14582
tags:
- pruning
- reasoning
- verification
- logic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pruning long chain-of-thought (Long-CoT)
  reasoning can improve the reasoning performance of small language models (SLMs).
  The authors propose Prune-on-Logic, a framework that transforms Long-CoT into logic
  graphs and selectively prunes low-utility reasoning steps based on semantic contribution
  scores.
---

# Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning

## Quick Facts
- **arXiv ID:** 2505.14582
- **Source URL:** https://arxiv.org/abs/2505.14582
- **Reference count:** 40
- **Primary result:** Pruning verification steps in Long-CoT reasoning improves SLM accuracy by up to 6.0% while reducing tokens by 9.51%

## Executive Summary
This paper investigates whether pruning long chain-of-thought (Long-CoT) reasoning can improve the reasoning performance of small language models (SLMs). The authors propose Prune-on-Logic, a framework that transforms Long-CoT into logic graphs and selectively prunes low-utility reasoning steps based on semantic contribution scores. Experiments across three pruning strategies show that pruning verification steps consistently improves accuracy while reducing token usage, with DeepSeek-R1-Distill-Qwen-7B achieving 63.0% accuracy (+6.0% over baseline) with 9.51% fewer tokens. The benefits hold across tasks, model scales, and CoT capability, with larger models benefiting more due to redundant reasoning.

## Method Summary
The authors propose Prune-on-Logic, a framework that transforms Long-CoT into logic graphs and selectively prunes low-utility reasoning steps. The method involves four key steps: (1) Logic Graph Construction using a parser LLM to segment CoT into nodes and connections, (2) Importance Scoring using a frozen pre-SFT SLM to compute semantic utility scores based on perplexity differences, (3) Pruning of top-k lowest-scoring nodes based on specific strategies, and (4) SFT fine-tuning using LoRA on the pruned dataset. The framework was evaluated across three pruning strategies (all-chain, reasoning-only, verification-only) using the Bespoke-Stratos-17k dataset and tested on AMC23, AIME, MATH500, GSM8K, and BBH benchmarks.

## Key Results
- Pruning verification steps improves DeepSeek-R1-Distill-Qwen-7B accuracy to 63.0% (+6.0% over baseline) with 9.51% fewer tokens
- Larger models benefit more from pruning due to redundant reasoning steps
- Pruning connections instead of nodes drops accuracy to ~30%, highlighting the importance of targeting logic nodes
- Benefits hold across tasks, model scales, and CoT capability

## Why This Works (Mechanism)
The paper demonstrates that effective pruning aligns supervision with model capacity rather than merely shortening inputs. By identifying and removing verification steps—which are redundant for smaller models with limited reasoning capacity—the framework provides more focused supervision that matches the model's actual needs. The semantic utility scoring captures the actual contribution of each reasoning step, allowing targeted removal of steps that don't add value for the specific model scale.

## Foundational Learning
- **Logic Graph Construction:** Converting CoT into DAGs with nodes (calculations, logic) and connections (rhetoric) is needed to systematically analyze reasoning structure. Quick check: Verify the parser correctly identifies dependencies between reasoning steps.
- **Semantic Utility Scoring:** Using perplexity differences to measure node importance captures the actual contribution to reasoning. Quick check: Compare scores across different model states (base vs. instruction-tuned).
- **Verification vs Reasoning Distinction:** Identifying verification steps (redundant checks) versus reasoning steps (core logic) enables targeted pruning. Quick check: Validate classification accuracy using human annotations.
- **Scale-Aware Pruning:** Recognizing that larger models have more redundant reasoning steps allows for model-specific pruning strategies. Quick check: Measure redundancy levels across different model scales.

## Architecture Onboarding
- **Component Map:** Long-CoT -> Parser LLM -> DAG Construction -> Utility Scoring -> Pruning -> SFT Training
- **Critical Path:** The most critical path is from DAG construction through utility scoring to pruning decisions, as errors here directly impact model performance.
- **Design Tradeoffs:** The framework trades computational overhead of logic graph construction and scoring for improved accuracy and efficiency, with the tradeoff becoming more favorable for larger models.
- **Failure Signatures:** Pruning connections instead of nodes causes accuracy to drop to ~30%; random pruning destroys critical reasoning chains; incorrect verification classification eliminates essential reasoning steps.
- **3 First Experiments:** 1) Test verification node classification using multiple heuristics (position-based, content-based, hybrid), 2) Compare semantic utility scoring using different model states, 3) Apply framework to non-mathematical reasoning tasks for cross-domain validation.

## Open Questions the Paper Calls Out
None

## Limitations
- The specific criteria for distinguishing verification from reasoning nodes remain underspecified in the segmentation prompt
- The semantic utility scoring methodology lacks clarity on whether the scoring model should be base foundation or instruction-tuned
- The framework's effectiveness for non-mathematical reasoning domains remains untested

## Confidence
- **High:** Experimental results showing accuracy improvements through verification pruning are robust and consistent
- **Medium:** The theoretical framework explaining why verification pruning improves reasoning has strong empirical support but needs more causal mechanism detail
- **Low:** Generalizability to other domains and model architectures is limited by exclusive focus on mathematical reasoning

## Next Checks
1. Implement and test multiple heuristics for identifying verification nodes to determine which most reliably reproduces performance gains
2. Conduct controlled experiments comparing semantic utility scoring using different model states (base foundation vs. instruction-tuned)
3. Apply the Prune-on-Logic framework to a non-mathematical reasoning dataset to assess cross-domain effectiveness