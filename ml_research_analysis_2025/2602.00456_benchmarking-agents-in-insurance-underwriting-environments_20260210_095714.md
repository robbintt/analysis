---
ver: rpa2
title: Benchmarking Agents in Insurance Underwriting Environments
arxiv_id: '2602.00456'
source_url: https://arxiv.org/abs/2602.00456
tags:
- tool
- task
- answer
- user
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UNDERWRITE is an expert-first, multi-turn insurance underwriting
  benchmark designed to reflect the complexity of real-world enterprise workflows.
  Unlike existing benchmarks that focus on open-domain tasks and narrow accuracy metrics,
  UNDERWRITE incorporates proprietary business knowledge, noisy tool interfaces, and
  imperfect simulated users requiring careful information gathering.
---

# Benchmarking Agents in Insurance Underwriting Environments

## Quick Facts
- arXiv ID: 2602.00456
- Source URL: https://arxiv.org/abs/2602.00456
- Authors: Amanda Dsouza; Ramya Ramakrishnan; Charles Dickens; Bhavishya Pohani; Christopher M Glaze
- Reference count: 4
- Primary result: Expert-first, multi-turn insurance underwriting benchmark (UNDERWRITE) reveals significant hallucination and reliability gaps in frontier models when evaluated on proprietary business knowledge and imperfect tool interfaces.

## Executive Summary
UNDERWRITE is an expert-first, multi-turn insurance underwriting benchmark designed to reflect the complexity of real-world enterprise workflows. Unlike existing benchmarks that focus on open-domain tasks and narrow accuracy metrics, UNDERWRITE incorporates proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. We evaluated 13 frontier models on 300 tasks and found that models frequently hallucinate domain-specific information despite tool access, with smaller models like GPT-5-Mini showing up to 19% hallucination rates in product recommendation tasks. Pass^4 results revealed up to a 20% drop in answer correctness, indicating significant reliability gaps. While the most accurate models achieved 88% correctness, they were not the most efficient and exhibited higher tool error rates. These findings highlight that expert involvement in benchmark design is essential for realistic evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains requires compositional approaches.

## Method Summary
UNDERWRITE evaluates multi-turn insurance underwriting agents using ReAct architecture in LangGraph with 3-node graphs (agent, simulated user, error detector). The benchmark uses a SQLite database with 9 tables containing proprietary business rules, NAICS codes, and underwriting guidelines. Agents interact with simulated users constrained to provide at most two pieces of information per turn, requiring strategic question-asking. 300 tasks across 6 types were created with expert review, and performance is measured via answer correctness (LLM-as-Judge with GPT-4.1-mini), tool errors, uncertain answers, and pass^k reliability metrics.

## Key Results
- Models hallucinate domain-specific insurance products despite tool access, with smaller models like GPT-5-Mini showing up to 19% hallucination rates
- Pass^4 results revealed up to a 20% drop in answer correctness, indicating significant reliability gaps
- The most accurate models achieved 88% correctness but were not the most efficient and exhibited higher tool error rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert involvement in holistic system design—not just individual task creation—improves benchmark realism and exposes genuine model limitations.
- Mechanism: Domain experts (CPCUs) reviewed both tasks and the overall system architecture, including business rules and data resources. Feedback loops increased task acceptance from 55% to 88% and identified task-applicant dependencies not anticipated by researchers.
- Core assumption: Expert-identified realism gaps correlate with deployment-time failure modes.
- Evidence anchors:
  - [abstract] "expert involvement in benchmark design is essential for realistic agent evaluation"
  - [section] "CPCUs pointed out that the same task would be relevant to some applicants but not others, suggesting a dependency we had not anticipated"
  - [corpus] INSEva and CUFEInse v1.0 similarly emphasize expert-driven evaluation frameworks for insurance domains, though neither explicitly addresses system-level co-design
- Break condition: If experts lack recent operational experience or if the domain has shifted significantly since their expertise was current, realism signals may degrade.

### Mechanism 2
- Claim: Constrained information disclosure by simulated users reveals agent deficits in strategic question-asking.
- Mechanism: Simulated users are prompted to provide at most two pieces of information per response, forcing agents to iteratively elicit relevant details. The "uncertain answers" reward metric captures how often agents ask inappropriate questions.
- Core assumption: Effective information gathering correlates with downstream task success.
- Evidence anchors:
  - [abstract] "imperfect simulated users requiring careful information gathering"
  - [section] "one of the goals of the benchmark is to evaluate how effective models are at asking the right questions to solve tasks"
  - [corpus] Related work (τ-bench, τ²-Bench) uses dual-control environments but does not systematically constrain information disclosure per turn
- Break condition: If task complexity does not require multi-turn information gathering, this mechanism provides limited signal.

### Mechanism 3
- Claim: Proprietary knowledge requirements expose reliance on pretrained domain knowledge, manifesting as hallucinations.
- Mechanism: The benchmark includes fictional but plausible business rules and products not found in training data. Models hallucinate common insurance products (e.g., Inland Marine, Equipment Breakdown) absent from the guidelines but prevalent in pretraining corpora. Accuracy drops sharply as reference answer "surprise" increases.
- Core assumption: Hallucinated entities originate from pretrained knowledge rather than stochastic generation.
- Evidence anchors:
  - [abstract] "models frequently hallucinate domain-specific insurance products despite tool access"
  - [section] "All hallucinated products were common ones in the insurance industry... strongly suggesting that they come from pretrained domain knowledge"
  - [corpus] No direct corpus evidence on compositional hallucination detection in specialized domains; related insurance benchmarks (INSEva, InsQABench) do not report hallucination analysis
- Break condition: If proprietary knowledge overlaps significantly with public domain knowledge, hallucination signal diminishes.

## Foundational Learning

- Concept: ReAct agent architecture
  - Why needed here: UNDERWRITE implements agents as ReAct loops (reasoning → action → observation) in LangGraph. Understanding this pattern is prerequisite to interpreting failure modes like premature user responses vs. multi-step tool chaining.
  - Quick check question: Can you trace the state transitions in Figure 6 and explain why Claude-Sonnet-4.5 makes more tool→tool transitions than Gemini-2.5-Pro?

- Concept: LLM-as-a-Judge evaluation
  - Why needed here: Correctness evaluation uses GPT-4.1-mini as a judge with >95% agreement on manual annotations. Understanding judge design and calibration is essential for interpreting benchmark results and avoiding evaluator bias.
  - Quick check question: What failure modes might arise if the judge model shares pretrained knowledge with evaluated agents?

- Concept: pass^k reliability metrics
  - Why needed here: The benchmark reports up to 20% drop from pass¹ to pass⁴, revealing reliability gaps invisible to single-run accuracy. This metric is critical for enterprise deployment decisions.
  - Quick check question: Why does pass^k provide different signal than average accuracy across k independent runs?

## Architecture Onboarding

- Component map: ReAct agent node (copilot model) → tool calls via MCP server → SQLite database (9 tables including NAICS versions, appetite matrices) → Simulated user node (GPT-4.1) → constrained information disclosure (max 2 facts/turn) → Error detection node → terminates on code blocks, malformed responses

- Critical path: Agent receives task → queries metadata → chains SQL queries → gathers user information → applies guidelines → produces final decision. Tasks average 3-7 reasoning steps across 10-20 conversational turns.

- Design tradeoffs:
  - Proprietary vs. public knowledge: Fictional guidelines increase realism but require expert validation
  - Constrained vs. cooperative user simulation: Constrained disclosure tests information gathering but may underrepresent cooperative users
  - Conservative hallucination detection: Focused on product hallucinations only; general detection showed high recall/low precision

- Failure signatures:
  - High tool errors + high recovery → self-correcting agent (moderate correlation with accuracy)
  - Fewer steps + higher tokens → verbose reasoning without tool use (correlated with incorrect outcomes)
  - High "user cannot answer" rate → ineffective question-asking strategy

- First 3 experiments:
  1. Run baseline evaluation on 3 models (e.g., Claude-Sonnet-4.5, GPT-5, DeepSeek-V3.1) across all 6 task types; compare answer correctness vs. tool error recovery rates.
  2. Ablate the constrained user simulation (allow full information disclosure) to quantify the information-gathering difficulty contribution to overall accuracy.
  3. Implement task-level "surprise" scoring on a held-out subset; correlate with accuracy drop to validate the proprietary knowledge mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning strategies reduce hallucination rates on proprietary knowledge without sacrificing general reasoning capabilities?
- Basis in paper: [explicit] The conclusion states future work could explore this specific trade-off.
- Why unresolved: The study observes high hallucination rates (up to 19%) even with tool access, but does not test mitigation via training.
- What evidence would resolve it: Evaluation of fine-tuned models on UNDERWRITE showing reduced hallucinations while maintaining performance on general reasoning benchmarks.

### Open Question 2
- Question: How can evaluation frameworks rigorously disentangle model reasoning failures from agentic framework brittleness?
- Basis in paper: [explicit] The authors identify disentangling these factors as a "key technical challenge" and source of inaccurate performance reporting.
- Why unresolved: Current frameworks (e.g., LangGraph) introduce artifacts that may terminate tasks or skew results independently of the model's ability.
- What evidence would resolve it: An evaluation harness that quantifies and subtracts framework-induced errors from overall failure rates.

### Open Question 3
- Question: What compositional methods effectively detect hallucinations in complex domains beyond simple entity extraction?
- Basis in paper: [inferred] The authors used a conservative detection model focused only on product names because general LLM-as-Judge approaches yielded low precision.
- Why unresolved: Current detection methods struggle to distinguish incorrect reasoning or complex logic errors from valid rationales without high false positive rates.
- What evidence would resolve it: A detection framework that maintains high precision while identifying nuanced logical inconsistencies in domain-specific outputs.

## Limitations
- Proprietary knowledge and database schemas are not released, making independent verification difficult
- Evaluation relies on LLM-as-judge which may share knowledge with evaluated models
- Framework brittleness introduces artifacts that may terminate tasks or skew results independently of model ability

## Confidence
- Expert involvement improves benchmark realism: Medium confidence
- Constrained information disclosure reveals information-gathering deficits: High confidence
- Proprietary knowledge requirements expose hallucination from pretrained domain knowledge: Medium confidence
- Framework brittleness skews performance reporting: High confidence
- pass^k reliability gaps indicate deployment risks: High confidence

## Next Checks
1. Reproduce hallucination detection: Implement the product-focused hallucination detector on a subset of tasks with ground truth annotations to measure precision and recall independently of the original benchmark.
2. Ablate user simulation constraints: Run a controlled experiment comparing unconstrained vs. constrained user information disclosure to quantify the information-gathering difficulty contribution to overall accuracy.
3. Cross-domain transferability: Apply the proprietary knowledge hallucination detection mechanism to another specialized domain (e.g., legal or medical) with fictional guidelines to test generalizability of the compositional approach.