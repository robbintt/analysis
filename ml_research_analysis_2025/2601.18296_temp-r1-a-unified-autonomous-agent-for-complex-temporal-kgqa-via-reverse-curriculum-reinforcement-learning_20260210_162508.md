---
ver: rpa2
title: 'Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse
  Curriculum Reinforcement Learning'
arxiv_id: '2601.18296'
source_url: https://arxiv.org/abs/2601.18296
tags:
- uni00000013
- title
- reasoning
- temporal
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Temp-R1 introduces an autonomous agent for Temporal Knowledge\
  \ Graph Question Answering (TKGQA) that overcomes the limitations of fixed workflows\
  \ by using reinforcement learning and an expanded action space. The agent features\
  \ internal reasoning actions\u2014plan, filter, and rank\u2014alongside external\
  \ search actions to reduce cognitive overload and prevent hallucinations."
---

# Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.18296
- **Source URL:** https://arxiv.org/abs/2601.18296
- **Reference count:** 30
- **Primary result:** 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions

## Executive Summary
Temp-R1 introduces an autonomous agent for Temporal Knowledge Graph Question Answering (TKGQA) that overcomes the limitations of fixed workflows by using reinforcement learning and an expanded action space. The agent features internal reasoning actions—plan, filter, and rank—alongside external search actions to reduce cognitive overload and prevent hallucinations. A reverse curriculum learning strategy trains on difficult questions first, avoiding shortcut learning and improving generalization. The 8B-parameter model achieves state-of-the-art performance on MultiTQ and TimelineKGQA, notably improving 19.8% over strong baselines on complex questions, and demonstrates strong efficiency by surpassing larger, closed-source models.

## Method Summary
Temp-R1 formulates TKGQA as a Markov Decision Process where an autonomous agent interacts with a temporal knowledge graph through a sequence of actions. The agent uses an expanded action space with internal reasoning actions (plan, filter, rank) and external search actions. Training begins with a cold-start supervised fine-tuning phase using ~1,000 high-quality trajectories generated by GPT-4o, followed by Group Relative Policy Optimization (GRPO) with a reverse curriculum that prioritizes difficult multi-hop questions. The model employs LLaMA3.1-8B-Instruct as the base architecture with an E5 retriever for external knowledge retrieval.

## Key Results
- Achieves 0.780 overall accuracy on MultiTQ, surpassing GPT-4o and DeepSeek-Coder
- Demonstrates 19.8% improvement over baselines on complex "Multiple" questions
- Shows strong efficiency by outperforming larger closed-source models like GPT-4o and DeepSeek-Coder
- Maintains 0.969 accuracy on time-type questions, indicating robust temporal reasoning

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Load Reduction via Action Space Decomposition
- **Claim**: Decomposing reasoning into explicit internal actions (`<plan>`, `<filter>`, `<rank>`) reduces cognitive overload and improves reasoning accuracy compared to single-token thinking.
- **Mechanism**: The paper argues that "mixing distinct cognitive demands within a single reasoning tag often leads to inadequate reinforcement learning and logical reasoning" (Page 2). By decoupling search, planning, filtering, and ranking into separate actions, each operation can be optimized independently. The `<filter>` action applies temporal constraints, while `<rank>` handles chronological ordering—tasks that "often lead to reasoning errors or even hallucinated conclusions" when conflated (Page 7).
- **Core assumption**: Explicit action boundaries reduce interference between reasoning subtasks during policy gradient updates.
- **Evidence anchors**:
  - [abstract]: "The agent features internal reasoning actions—plan, filter, and rank—alongside external search actions to reduce cognitive overload and prevent hallucinations."
  - [section]: Ablation study (Table 4) shows removing internal actions drops overall accuracy from 0.780 to 0.620, with "Multiple" performance declining from 0.550 to 0.388.
  - [corpus]: Limited corpus support; one neighbor paper (EPERM) mentions "evidence path enhanced reasoning" but doesn't address action space decomposition directly.

### Mechanism 2: Reverse Curriculum Learning Prevents Shortcut Exploitation
- **Claim**: Training on difficult multi-hop questions first prevents the agent from overfitting to simple shortcut patterns that fail on complex reasoning.
- **Mechanism**: Standard curriculum learning (easy→hard) causes models to "overfit to simpler questions and stop exploring harder reasoning paths once achieving high rewards on easy samples" (Page 2). Reverse curriculum forces the model to "survive in high-difficulty environments first, compelling it to acquire sophisticated tool-chain logic before transferring to simpler environments" (Page 7). This produces a "dimensional reduction" effect where learned complex reasoning generalizes downward.
- **Core assumption**: Skills learned on hard questions transfer to easier ones more reliably than vice versa.
- **Evidence anchors**:
  - [abstract]: "A reverse curriculum learning strategy trains on difficult questions first, avoiding shortcut learning and improving generalization."
  - [section]: Figure 8 shows "Easy First" approach plateaus with low validation accuracy, while "Hard First" achieves higher accuracy through more extensive exploration. Table 4 shows removing reverse curriculum drops "Multiple" accuracy from 0.550 to 0.143.
  - [corpus]: No direct corpus evidence on reverse curriculum for TKGQA; this appears to be a novel application.

### Mechanism 3: Cold-Start SFT Stabilizes RL Exploration
- **Claim**: Brief supervised fine-tuning on high-quality trajectories is necessary to prevent RL training collapse and ensure stable policy optimization.
- **Mechanism**: The paper notes that "a pretrained base model without task-specific alignment struggles to produce valid structured tags and coherent reasoning in the early stages of training" (Page 4). SFT provides "essential prior knowledge of temporal patterns and formatting, serving as a bridge between static KG understanding and dynamic policy training" (Page 7). Figure 7 shows without SFT, the model suffers "early training collapse and uncontrollable KL loss spikes."
- **Core assumption**: The SFT trajectories, though limited (~1,000 samples), cover the structural diversity needed for RL to begin meaningful exploration.
- **Evidence anchors**:
  - [section]: Section 4.3 describes the SFT cold-start process with masked cross-entropy loss on agent-generated tokens only.
  - [section]: Table 4 shows removing SFT reduces overall performance to 0.582 and time-type accuracy from 0.969 to 0.713.
  - [corpus]: Neighbor papers on KGQA (e.g., "Plan Then Retrieve") use supervised pre-training but don't analyze the SFT→RL transition specifically.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation for language agents**
  - **Why needed here**: Temp-R1 formulates TKGQA as an MDP (Section 3), where states include question+history, actions are internal/external operations, and rewards are binary answer correctness. Understanding MDPs is essential to grasp how policy gradients optimize reasoning trajectories.
  - **Quick check question**: Can you explain why the transition function differs between `<search>` actions (which produce external observations) and `<filter>` actions (which do not)?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: Temp-R1 uses GRPO (Equation 3) instead of standard PPO. GRPO samples multiple trajectories per question and computes group-relative advantages (normalized rewards within each group). This provides stable baselines for sparse, binary rewards common in QA tasks.
  - **Quick check question**: How does group-relative advantage computation (`r_i - mean({r_k}) / std({r_k}) + η`) differ from traditional advantage estimation, and why might it help with binary rewards?

- **Concept: Temporal Knowledge Graph representation**
  - **Why needed here**: TKGs use quadruples `(subject, predicate, object, timestamp)` rather than triples. Queries involve temporal constraints ("before 2015-03-01"), temporal ordering ("first," "last"), and multi-granularity time (year/month/day). Understanding this structure is critical for interpreting the `<filter>` and `<rank>` action designs.
  - **Quick check question**: Why would a query asking "Who was the last visitor before X?" require both a temporal filter and a ranking operation?

## Architecture Onboarding

- **Component map**:
  - Base model: LLaMA3.1-8B-Instruct (or Qwen2.5 variants)
  - Retriever: E5 embedding model (external, frozen)
  - Rollout loop: Interleaved actions (`<plan>` → `<search>` → `<filter>` → `<rank>` → `<answer>`)
  - Training pipeline: SFT cold-start (2 epochs, ~1K samples) → GRPO with reverse curriculum (3×A800 GPUs, ~35K rollouts)
  - Reward model: Rule-based terminal reward (binary: exact match on final answer)

- **Critical path**:
  1. **SFT data quality**: ~1,000 GPT-4o-generated trajectories must be filtered for structural correctness (valid tags) and semantic correctness (right answers). Poor SFT data leads to unstable RL.
  2. **Curriculum threshold T₀**: The point at which simple questions are introduced must balance hard-question mastery with overall coverage. Too early → shortcuts return; too late → wasted compute.
  3. **Action masking in loss**: Only agent-generated tokens receive gradient updates; system prompts and retrieved content are masked. Implementation errors here will cause the model to memorize retrieval artifacts.

- **Design tradeoffs**:
  - **Model size vs. efficiency**: Figure 5 shows 7B models outperform 1.5B substantially (0.790 vs. 0.532 accuracy). Paper limited to 8B due to compute constraints; scaling to 14B+ is untested.
  - **Action granularity**: More internal actions improve interpretability but increase sequence length and credit assignment difficulty. Current 3 internal actions (`plan`, `filter`, `rank`) appear sufficient for TKGQA but may not generalize to other domains.
  - **Retriever integration**: Paper uses frozen E5 retriever; jointly training retriever and agent could improve performance but adds complexity.

- **Failure signatures**:
  - **Cognitive overload**: Model produces incoherent `<think...>` blocks, misorders timestamps, or hallucinates entities not in retrieval results. → Check action space configuration and SFT coverage.
  - **Shortcut exploitation**: Model achieves high accuracy on "Single" questions but near-zero on "Multiple" (Table 4: 0.143 without reverse curriculum). → Verify curriculum implementation and difficulty labeling.
  - **Training collapse**: KL loss spikes, validation accuracy flatlines, or model generates malformed action sequences. → Verify SFT initialization quality and GRPO hyperparameters (ε=0.2, β=0.01).

- **First 3 experiments**:
  1. **Action space ablation**: Train with only external `<search>` actions (no `<plan>`, `<filter>`, `<rank>`). Compare accuracy on "Multiple" questions to full model. This isolates the cognitive load reduction mechanism.
  2. **Curriculum direction comparison**: Run identical GRPO training with (a) standard curriculum (easy→hard), (b) reverse curriculum (hard→easy), (c) random shuffle. Plot training reward, policy entropy, and validation accuracy over time. This validates the shortcut-prevention claim.
  3. **SFT scale sensitivity**: Train with (a) 100 SFT samples, (b) 1,000 samples (paper default), (c) 5,000 samples. Measure KL stability and final accuracy. This determines whether the SFT cold-start is strictly necessary or if direct RL is viable with better hyperparameters.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the performance of Temp-R1's reverse curriculum strategy scale effectively to larger backbone models (14B+ parameters), or does the "shortcut trap" diminish naturally with increased model capacity?
  - **Basis in paper**: [explicit] The authors state, "Due to computational constraints, our experiments are limited to models with parameter scales up to 8B... the scalability of Temp-R1 to higher-capacity models remains to be empirically verified."
  - **Why unresolved**: The authors did not conduct fine-tuning or RL training on larger models due to resource limits.
  - **What evidence would resolve it**: Evaluation of the Temp-R1 training pipeline on 14B or 70B parameter backbones to observe if the 19.8% improvement over baselines is maintained or enhanced.

- **Open Question 2**: Can the reverse curriculum learning approach be successfully transferred to non-temporal reasoning domains (e.g., static KGQA or mathematical reasoning), or is it specifically tuned for temporal constraint distributions?
  - **Basis in paper**: [explicit] The authors note, "Our findings primarily demonstrate its efficacy within the TKGQA task, and its generalizability to broader, non-temporal domains requires further investigation."
  - **Why unresolved**: The current study restricts validation to temporal benchmarks (MultiTQ, TimelineKGQA) which have specific uneven difficulty distributions.
  - **What evidence would resolve it**: Experiments applying the "hard-first" curriculum strategy to standard static KGQA or logical reasoning datasets to compare against standard curriculum learning.

## Limitations

- **Generalizability beyond TKGQA**: The internal action design (`<plan>`, `<filter>`, `<rank>`) is tailored to temporal reasoning patterns. Whether this action space generalizes to other reasoning domains remains untested.
- **SFT data dependency**: The 1,000-sample cold-start is a critical bottleneck. If SFT trajectories are noisy or unrepresentative, RL training may amplify systematic errors rather than recover from them.
- **Reverse curriculum threshold sensitivity**: The switch point T₀ (when to introduce simple questions) is a hyperparameter with unknown sensitivity that could significantly impact performance.

## Confidence

- **High confidence**: Action space decomposition improves accuracy (supported by ablation: 0.780→0.620 without internal actions). Cold-start SFT prevents training collapse (supported by KL loss spikes in Figure 7).
- **Medium confidence**: Reverse curriculum prevents shortcut learning (supported by training curves and Table 4, but lacks ablation on curriculum threshold). 8B model efficiency claims (Figure 5) are internally consistent but untested against larger models.
- **Low confidence**: Generalization claims to "any structured reasoning task" (Section 1) and assertions about "dimensional reduction" in skill transfer (Page 7) are speculative without empirical validation on non-temporal tasks.

## Next Checks

1. **Action space ablation with time-only queries**: Remove `<filter>` and `<rank>` actions and train on TimelineKGQA-CRON (time-only answers). If accuracy drops significantly on "Multiple" questions, this validates that temporal-specific actions are essential rather than generic reasoning improvements.

2. **Curriculum ablation with synthetic difficulty**: Create synthetic questions with controlled reasoning depth (1-hop vs. 3-hop). Train with (a) standard curriculum, (b) reverse curriculum, (c) no curriculum. Measure final accuracy and policy entropy to test whether reverse curriculum specifically addresses shortcut learning or just increases exploration.

3. **SFT scale sensitivity with synthetic trajectories**: Generate SFT trajectories using both GPT-4o and a weaker model (e.g., GPT-3.5). Train with (a) 100, (b) 1,000, (c) 5,000 samples from each. Compare final accuracy and KL stability to determine whether SFT quality or quantity is the limiting factor.