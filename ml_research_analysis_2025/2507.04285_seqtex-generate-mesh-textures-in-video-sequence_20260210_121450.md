---
ver: rpa2
title: 'SeqTex: Generate Mesh Textures in Video Sequence'
arxiv_id: '2507.04285'
source_url: https://arxiv.org/abs/2507.04285
tags:
- texture
- video
- generation
- arxiv
- multi-view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SeqTex, a novel end-to-end framework that
  leverages pretrained video diffusion models to directly generate UV texture maps
  for 3D meshes. Unlike prior approaches that require multi-stage pipelines and post-processing,
  SeqTex reformulates texture generation as a sequence prediction problem, jointly
  modeling multi-view images and UV textures to transfer image-space priors into the
  UV domain.
---

# SeqTex: Generate Mesh Textures in Video Sequence
## Quick Facts
- arXiv ID: 2507.04285
- Source URL: https://arxiv.org/abs/2507.04285
- Reference count: 40
- Generates UV texture maps for 3D meshes using pretrained video diffusion models in an end-to-end framework

## Executive Summary
SeqTex introduces a novel end-to-end framework for generating UV texture maps for 3D meshes by reformulating texture generation as a sequence prediction problem. The method leverages pretrained video diffusion models and jointly models multi-view images and UV textures to transfer image-space priors into the UV domain. Key innovations include a decoupled multi-view and UV processing architecture, geometry-informed attention for cross-domain alignment, and adaptive token resolution to preserve fine texture details. Trained on both 3D texture datasets and multi-view-only geometry data, SeqTex achieves state-of-the-art performance on image- and text-conditioned texture generation tasks with superior 3D consistency and texture-geometry alignment.

## Method Summary
SeqTex is an end-to-end framework that generates UV texture maps for 3D meshes by treating texture generation as a sequence prediction problem. The method decouples multi-view and UV processing while maintaining cross-domain alignment through geometry-informed attention mechanisms. A pretrained video diffusion model serves as the foundation, with adaptations including adaptive token resolution to preserve fine texture details. The framework is trained on both 3D texture datasets and multi-view-only geometry data, enabling it to learn robust texture generation capabilities. The approach eliminates the need for multi-stage pipelines and post-processing typically required by prior methods.

## Key Results
- Achieves FID of 30.27 and KID of 1.21 on image-conditioned texture generation tasks
- Demonstrates highest user preference and MLLM scores on text-conditioned tasks
- Shows superior 3D consistency and texture-geometry alignment compared to existing methods

## Why This Works (Mechanism)
The framework's success stems from reformulating texture generation as a sequence prediction problem, which allows leveraging powerful video diffusion priors. By jointly modeling multi-view images and UV textures, the method effectively transfers image-space priors into the UV domain. The decoupled architecture separates multi-view and UV processing while maintaining alignment through geometry-informed attention, enabling efficient learning. Adaptive token resolution preserves fine texture details that would otherwise be lost in standard fixed-resolution approaches.

## Foundational Learning
- **Video diffusion models**: Pretrained models that generate video sequences - needed because they provide strong image-space priors that can be transferred to texture generation; quick check: verify the specific video diffusion model used and its capabilities
- **UV texture mapping**: Process of mapping 2D textures onto 3D mesh surfaces - needed as the target output format for 3D rendering; quick check: confirm UV parameterization conventions used
- **Geometry-informed attention**: Attention mechanisms that incorporate geometric information - needed for cross-domain alignment between multi-view images and UV textures; quick check: examine how geometric features are encoded and used
- **Token resolution adaptation**: Dynamic adjustment of token count based on content complexity - needed to preserve fine texture details; quick check: verify how resolution adapts to different texture regions
- **Sequence prediction**: Framing generation as predicting sequences rather than individual samples - needed to leverage temporal/spatial coherence from video models; quick check: confirm sequence formulation and training objectives
- **Multi-view consistency**: Ensuring generated textures are consistent across different viewing angles - needed for realistic 3D rendering; quick check: examine consistency metrics and evaluation methods

## Architecture Onboarding
**Component Map**: Input Views -> Multi-view Encoder -> Geometry-informed Attention -> UV Decoder -> Texture Output
**Critical Path**: Multi-view image processing → Geometry-informed cross-attention → UV texture generation → Adaptive resolution refinement
**Design Tradeoffs**: Decoupled architecture improves efficiency but requires careful attention mechanisms for alignment; adaptive resolution preserves details but adds complexity to training
**Failure Signatures**: Inconsistent textures across views, loss of fine details, poor geometry-texture alignment, artifacts at mesh seams
**Three First Experiments**: 1) Test multi-view consistency by rendering generated textures from different viewpoints, 2) Evaluate texture quality on meshes with varying topology complexity, 3) Compare inference speed against staged pipeline approaches

## Open Questions the Paper Calls Out
The paper acknowledges that evaluation relies heavily on image-space metrics (FID/KID) which may not fully capture 3D consistency or texture-geometry alignment. The authors recognize this limitation but don't provide alternative 3D-specific metrics or comprehensive ablation studies demonstrating the impact of their geometry-informed attention mechanism.

## Limitations
- Evaluation relies primarily on image-space metrics that may not fully capture 3D-specific quality measures
- User study methodology lacks detail on participant demographics and task design
- Framework's generalization to diverse mesh topologies and UV parameterizations beyond standard datasets is unclear

## Confidence
- Quantitative performance claims (FID/KID): High confidence in reported numbers, Medium confidence in their relevance to 3D texture quality
- End-to-end architecture advantage: Medium confidence - claims are supported but lack direct comparisons to staged approaches under identical conditions
- User study results: Low-Medium confidence - methodology details are insufficient for full assessment

## Next Checks
1. Conduct ablation studies removing the geometry-informed attention and adaptive token resolution to quantify their individual contributions to performance
2. Implement 3D-specific evaluation metrics (e.g., texture continuity across mesh seams, geometric consistency under viewpoint changes) to complement image-space metrics
3. Test the framework on meshes with complex UV layouts and irregular topology to assess robustness beyond standard benchmark datasets