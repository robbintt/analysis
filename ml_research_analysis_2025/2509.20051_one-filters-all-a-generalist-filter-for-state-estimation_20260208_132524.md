---
ver: rpa2
title: 'One Filters All: A Generalist Filter for State Estimation'
arxiv_id: '2509.20051'
source_url: https://arxiv.org/abs/2509.20051
tags:
- llm-filter
- systems
- state
- estimation
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-Filter, a generalist filter that leverages
  large language models (LLMs) for state estimation in dynamical systems. By embedding
  noisy observations as text prototypes and using a System-as-Prompt (SaP) approach,
  LLM-Filter aligns modalities between observations and LLMs, enabling effective state
  estimation across various systems.
---

# One Filters All: A Generalist Filter for State Estimation

## Quick Facts
- **arXiv ID**: 2509.20051
- **Source URL**: https://arxiv.org/abs/2509.20051
- **Reference count**: 40
- **Primary result**: A generalist filter using LLMs outperforms learning-based filters by up to 32% lower RMSE and generalizes across systems without retraining

## Executive Summary
This paper introduces LLM-Filter, a generalist state estimation filter that leverages pre-trained Large Language Models (LLMs) to perform filtering across multiple dynamical systems without task-specific training. By embedding noisy observations as text prototypes and using a System-as-Prompt (SaP) approach, LLM-Filter aligns modalities between observations and LLMs, enabling effective state estimation. Experiments demonstrate that LLM-Filter consistently outperforms state-of-the-art learning-based filters, achieving up to 32% lower RMSE and exceptional generalization in changed or unseen environments without retraining. The approach shows scaling-law behavior, with accuracy improving with larger model sizes and longer training times.

## Method Summary
LLM-Filter embeds noisy observations into token-like representations using trainable MLPs (ObsEmbedding and StateProjection) while keeping a frozen LLM backbone (default LLaMA-7B). Observations are segmented and normalized before embedding into the LLM's hidden dimension. The System-as-Prompt (SaP) provides task instructions and examples to condition the LLM for different dynamical systems. Only the MLPs are trained using MSE loss against ground truth states, while the LLM remains frozen. This enables a single model to generalize across multiple systems through prompt engineering rather than weight updates.

## Key Results
- LLM-Filter achieves up to 32% lower RMSE compared to state-of-the-art learning-based filters across 7 dynamical systems
- Generalizes to unseen environments without retraining, maintaining performance through System-as-Prompt adaptation
- Demonstrates scaling-law behavior where accuracy monotonically increases with model size (tested on 1B, 7B, 70B parameter models)
- Outperforms KalmanNet on high-dimensional systems (Lorenz96, VL20) where KalmanNet diverges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained LLMs provide transferable reasoning patterns for sequential state estimation.
- Mechanism: The paper hypothesizes alignment between LLM next-token prediction and Bayesian filtering—both estimate distributions conditioned on sequential history. By embedding observations as token-like inputs, the frozen LLM's learned transition patterns are repurposed for state inference without updating core weights.
- Core assumption: The sequential reasoning encoded in language pre-training transfers to numerical dynamical systems through proper modality alignment.
- Evidence anchors:
  - [abstract]: "state estimation can significantly benefit from the reasoning knowledge embedded in pre-trained LLMs"
  - [Section 5.3, Table 3]: Ablation shows LLM-Filter outperforms MLP/RNN/Transformer replacements by 30-32% average RMSE reduction
  - [corpus]: Related work on neural state estimation (arxiv 2601.21266) confirms neural architectures can behave as principled filters but does not address LLM-specific transfer
- Break condition: If LLM pre-training has no sequential structure relevant to dynamics (e.g., random weights), performance should collapse to MLP baseline.

### Mechanism 2
- Claim: In-context prompting with System-as-Prompt (SaP) enables cross-system generalization without retraining.
- Mechanism: SaP provides task instructions and input-output examples that condition the LLM on system-specific behavior. This allows a single frozen model to adapt its inference pattern to different dynamical systems via prompt engineering rather than weight updates.
- Core assumption: LLM in-context learning capabilities extend to numerical estimation tasks when examples are properly formatted.
- Evidence anchors:
  - [Section 3.2]: SaP includes Task Instruction and Task Examples; Figure 3 shows example format
  - [Section 5.2, Figure 6]: LLM-Filter with SaP maintains low RMSE in cross-system scenarios (e.g., Oscillator→Hopf: 0.66 vs. baselines >2.5), while LLM-Filter-O (no SaP) degrades significantly
  - [corpus]: No direct corpus evidence on in-context learning for state estimation
- Break condition: Removing SaP (LLM-Filter-O) should reduce generalization; if performance remains identical, SaP is not the active mechanism.

### Mechanism 3
- Claim: Segment-based observation embedding preserves multi-dimensional correlations better than flattened sequences.
- Mechanism: Instead of flattening all observation dimensions into a single series, the paper segments Y_t while preserving dimensional structure (segment length L, all N dimensions retained per segment). Each segment is normalized and embedded independently via ObsEmbedding MLP into the LLM's D-dimensional latent space.
- Core assumption: Preserving variable relationships (e.g., position-velocity coupling) within segments improves estimation over sequence-flattening approaches.
- Evidence anchors:
  - [Section 3.1]: "this approach can disrupt the inherent correlations between variables, such as the crucial relationship between position and speed"
  - [Section 3.1]: Formal specification of segmentation with floor function and padding
  - [corpus]: Related work on flow-based Bayesian filtering (arxiv 2502.16232) addresses high-dimensional systems but uses different approach; no direct comparison to segmentation strategies
- Break condition: If flattened single-series embedding matches or exceeds segment-based RMSE, correlation preservation claim is weakened.

## Foundational Learning

- Concept: **Bayesian Filtering (Prediction/Update Cycle)**
  - Why needed here: LLM-Filter replaces traditional recursive Bayes updates with learned inference; understanding p(x_t|y_{1:t}) computation clarifies what the model approximates.
  - Quick check question: Can you explain why equation (3b) requires normalization by the marginal likelihood?

- Concept: **Modality Alignment in Multimodal Models**
  - Why needed here: The core contribution repurposes a frozen text model for numerical data via learned projection layers.
  - Quick check question: Why must ObsEmbedding output dimension D match the LLM's hidden dimension rather than the state dimension M?

- Concept: **In-Context Learning**
  - Why needed here: SaP enables zero-shot adaptation; understanding prompt conditioning is essential for debugging generalization failures.
  - Quick check question: If SaP examples use different noise levels than the target system, what degradation would you expect?

## Architecture Onboarding

- Component map: Observations Y_t → [Normalize] → [Segment (L×N)] → [ObsEmbedding MLP] → Token embeddings e_{1:N_s} + SaP tokens c_{1:N_c} → [Frozen LLM Core] → Output embeddings ê_{1:N_s} → [StateProjection MLP] → Estimates X̂_t
- Critical path: Segmentation logic → ObsEmbedding initialization → SaP formatting → loss backprop through projection layers only. Incorrect segment handling or mismatched embedding dimensions will cause silent failures.
- Design tradeoffs:
  - Window length T: Longer windows capture more history but increase latency; paper uses T=40 default, notes T=20-40 optimal (Appendix E.4)
  - Segment length L: Controls granularity of observation-to-token mapping; Table 11 shows modest sensitivity
  - LoRA vs. full fine-tuning: Table 2 shows LoRA degrades performance, full fine-tuning helps high-dimensional systems but hurts simpler ones (overfitting risk)
- Failure signatures:
  - NaN loss: Check for division-by-zero in normalization or exploding gradients in projection layers
  - No generalization improvement with SaP: Verify token concatenation order matches LLM expected format
  - KalmanNet-style divergence on high-dimensional systems: Paper notes this baseline fails on Lorenz96/VL20 (Table 1)
- First 3 experiments:
  1. **Sanity check**: Train LLM-Filter-O (no SaP) on Selkov system; RMSE should approach ~0.64 (Table 1). If significantly worse, check embedding/projection layer initialization.
  2. **Ablation**: Replace LLaMA-7B with randomly initialized Transformer of same architecture; expect substantial RMSE increase (Table 3 shows ~0.86 vs. 0.58 on Hopf). Confirms pre-training contribution.
  3. **Cross-system test**: Train on Oscillator, evaluate on Hopf with SaP from Hopf. Target RMSE ~0.66 (Table 9). If >2.0, verify SaP tokenization and example formatting match inference-time structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-Filter be adapted to generalize across systems with varying state and observation dimensions without retraining?
- Basis in paper: [explicit] The Conclusion states: "The generalization ability of LLM-Filter is currently restricted to systems with the same dimensionality... Future work will focus on... enabling generalization across systems with varying dimensions."
- Why unresolved: The current architecture utilizes embedding and projection layers (MLPs) that are structurally dependent on fixed input ($N$) and output ($M$) dimensions, preventing a single trained model from handling systems of different sizes.
- What evidence would resolve it: A modified LLM-Filter architecture or input encoding scheme that successfully estimates states for systems with dimensions unseen during training (e.g., training on 2D systems, testing on 4D systems) without parameter updates.

### Open Question 2
- Question: What is the rigorous theoretical explanation for why LLM pre-training knowledge aligns with and improves state estimation in dynamical systems?
- Basis in paper: [inferred] Appendix C acknowledges that "theoretical foundations of LLMs themselves remain an open research problem," and the authors state that for their own framework, "it is challenging to provide [a detailed theoretical explanation] at this stage."
- Why unresolved: The paper demonstrates empirical success through alignment but lacks a mathematical justification for why the reasoning patterns in natural language text (or the features learned by LLMs) should correspond to the dynamics of filtering numerical time-series data.
- What evidence would resolve it: A formal analysis mapping the transformer attention mechanism to Bayesian update rules, or proof of convergence guarantees for LLM-Filter in specific system classes (e.g., linear Gaussian).

### Open Question 3
- Question: Why does Low-Rank Adaptation (LoRA) degrade the performance of LLM-Filter, and what parameter-efficient fine-tuning methods are suitable for this task?
- Basis in paper: [inferred] Section 5.3 and Table 2 show that adding LoRA leads to higher RMSE compared to the frozen backbone. The authors hypothesize this is due to "limitations of low-rank matrix adjustments" but do not provide a definitive cause.
- Why unresolved: LoRA is generally effective for LLM adaptation, so its failure here suggests a specific conflict with the high-precision numerical requirements of state estimation that current experiments have not isolated.
- What evidence would resolve it: An ablation study comparing LoRA against other efficient tuning methods (e.g., adapter layers, prompt tuning) on the same systems, or an analysis of the rank-error trade-off to determine if the low-rank constraint is the sole bottleneck.

### Open Question 4
- Question: How can tokenization inconsistencies be resolved to enable in-context generalization when scaling the model across different LLM backbones?
- Basis in paper: [inferred] In Section 5.3 ("Scaling Behavior"), the authors note they "did not use in-context prompting in this experiment" to avoid "inconsistencies caused by varying tokenizers across models."
- Why unresolved: It is unclear if the System-as-Prompt (SaP) mechanism—the key to generalization—is robust enough to function across different model architectures (e.g., OPT vs. LLaMA) which process text tokens differently.
- What evidence would resolve it: Experiments demonstrating that a single SaP prompt structure yields consistent performance improvements across multiple distinct LLM architectures (GPT, LLaMA, OPT) without system-specific re-tokenization or calibration.

## Limitations
- The theoretical mechanism linking LLM pre-training to sequential state estimation remains informal and speculative
- Generalist capabilities rely heavily on prompt engineering that may not generalize to arbitrary systems
- Performance on radically different dynamical systems (discrete vs. continuous, deterministic vs. stochastic) remains untested

## Confidence
- **High Confidence**: State estimation performance improvements over classical filters (Kalman variants) are well-established through multiple experiments
- **Medium Confidence**: Generalist capabilities (single model across systems) are demonstrated but rely on prompt engineering that may not generalize to arbitrary systems
- **Low Confidence**: The theoretical mechanism linking LLM pre-training to sequential state estimation remains speculative and requires formal investigation

## Next Checks
1. **Random Weights Control**: Replace LLaMA-7B with a randomly initialized Transformer of identical architecture and train on the same tasks. Compare RMSE performance to determine whether pre-training provides essential inductive biases versus architectural advantages alone.
2. **Prompt Ablation Study**: Systematically vary SaP components (instruction specificity, number of examples, example quality) across multiple systems to quantify the contribution of prompt engineering to generalization performance.
3. **Structural Generalization Test**: Evaluate LLM-Filter on dynamical systems with fundamentally different structures (e.g., discrete-time systems, systems with discontinuous dynamics, or purely stochastic processes) to test the limits of LLM-based reasoning transfer.