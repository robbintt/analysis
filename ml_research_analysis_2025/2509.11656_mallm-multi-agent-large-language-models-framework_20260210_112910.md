---
ver: rpa2
title: 'MALLM: Multi-Agent Large Language Models Framework'
arxiv_id: '2509.11656'
source_url: https://arxiv.org/abs/2509.11656
tags:
- agent
- mallm
- solution
- decision
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MALLM is an open-source framework for multi-agent debate (MAD)
  with large language models. It enables systematic experimentation with over 144
  configurations across agent personas, response generators, discussion paradigms,
  and decision protocols.
---

# MALLM: Multi-Agent Large Language Models Framework

## Quick Facts
- arXiv ID: 2509.11656
- Source URL: https://arxiv.org/abs/2509.11656
- Reference count: 40
- MALLM is an open-source framework for systematic experimentation with multi-agent debate (MAD) configurations

## Executive Summary
MALLM is an open-source framework enabling systematic experimentation with over 144 unique configurations of Multi-Agent Debate (MAD) systems. The framework decouples MAD components—agent personas, response generators, discussion paradigms, and decision protocols—allowing researchers to study how individual variations impact task performance. MALLM supports easy integration of any text-based Hugging Face dataset and provides an evaluation pipeline with automatic chart generation and statistical variance analysis.

The framework addresses limitations of existing MAD frameworks that tightly couple components, making systematic investigation difficult. MALLM's modular architecture enables controlled ablation studies to understand when and why MAD is successful, exploring the balance between test-time compute efficiency and emergent capabilities. The framework has been validated on reasoning and knowledge tasks, demonstrating how different configurations affect accuracy, convergence speed, and computational efficiency.

## Method Summary
MALLM orchestrates multi-agent debates through modular components: PersonaGenerators create agent system prompts (Expert, IPIP, None), ResponseGenerators format agent outputs (Simple, Critical, Reasoning), DiscussionParadigms control turn-taking and visibility (Memory, Relay, Report, Debate), and DecisionProtocols aggregate final answers (Voting, Consensus, Judge). The framework uses JSON configuration files to define parameter combinations without code changes, supporting any text-based Hugging Face dataset. Inference uses OpenAI-style APIs with temperature=1.0, and evaluation includes automatic metric computation (accuracy, BERTScore, BLEU, ROUGE) with statistical variance analysis across repeated runs.

## Key Results
- MALLM supports over 144 unique MAD configurations across four independent component categories
- Memory paradigm achieves faster convergence (avg. 1.75 turns) without sacrificing accuracy vs Relay (avg. 2.61 turns)
- Decision protocol effectiveness depends on task type: Consensus for knowledge tasks (+2.8%), Voting for reasoning tasks (+13.2%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling MAD components enables causal attribution of performance changes to specific configuration choices.
- Mechanism: The framework separates agents, discussion paradigms, and decision protocols into independent modules with abstract base classes. Each module can be varied while holding others constant, allowing controlled ablation studies. Configuration files define parameter combinations without code changes.
- Core assumption: Performance differences between configurations can be attributed to the varied component rather than implementation artifacts or random variance.
- Evidence anchors:
  - [abstract] "MALLM offers more than 144 unique configurations of MAD, including (1) agent personas... (2) response generators... (3) discussion paradigms... (4) decision protocols"
  - [section 1] "Adjusting them individually is particularly important for systematic investigations."
  - [corpus] Related work (e.g., "Stop Overvaluing Multi-Agent Debate") critiques MAD evaluation practices, suggesting current research lacks systematic component isolation—supporting MALLM's modular approach.
- Break condition: If components have hidden interdependencies not exposed by the framework, causal claims become unreliable.

### Mechanism 2
- Claim: Information transparency in discussion paradigms affects convergence speed without necessarily degrading task performance.
- Mechanism: The Memory paradigm provides full visibility into all agent messages, enabling faster consensus (avg. 1.75 turns). The Relay paradigm restricts visibility to sequential message passing, slowing convergence (avg. 2.61 turns). Both achieve comparable accuracy on StrategyQA (60.8% vs 62.9%).
- Core assumption: Turn count is a valid proxy for convergence efficiency and correlates with computational cost.
- Evidence anchors:
  - [section 4.2] "Information transparency can lead to quicker convergence in MAD without sacrificing task performance."
  - [section 4.2, Table 2] Memory: 60.8±2.6, Relay: 62.9±1.6 on StrategyQA with overlapping confidence intervals.
  - [corpus] Evidence is limited—corpus papers focus on accuracy, not convergence dynamics explicitly.
- Break condition: If faster convergence introduces systematic biases (e.g., groupthink), task performance may degrade on harder datasets.

### Mechanism 3
- Claim: Decision protocol effectiveness is task-type dependent: Consensus favors knowledge tasks; Voting favors reasoning-intensive tasks.
- Mechanism: Consensus protocols require repeated verification and agreement, improving knowledge retrieval accuracy by ~2.8% on MMLU-Pro and GPQA. Voting protocols aggregate diverse reasoning paths, improving SQuAD 2.0 and MuSR by ~13.2%.
- Core assumption: Task categories (knowledge vs. reasoning) are the primary moderator of protocol effectiveness, not other dataset characteristics.
- Evidence anchors:
  - [section 4.2, Table 3] Knowledge tasks: Consensus consistently higher (MMLU-Pro: 36.0% vs 31.1%). Reasoning tasks: Voting higher (SQuAD 2.0: 56.7% vs 43.6%).
  - [section 4.2] "The selection of the decision protocol depends on the specific task."
  - [corpus] "Debate or Vote" paper similarly disentangles MAD into voting and debate components, finding task-dependent effects—providing converging evidence.
- Break condition: If dataset-specific confounds (e.g., question format, answer space size) drive the effect, protocol recommendations may not generalize.

## Foundational Learning

- Concept: **Multi-Agent Debate (MAD) architecture**
  - Why needed here: MALLM is fundamentally a coordination framework for multiple LLM agents. Understanding the three-component decomposition (agents, paradigms, protocols) is prerequisite to effective use.
  - Quick check question: Can you explain why tightly coupling agents with discussion paradigms (as in AutoGen) hinders systematic study?

- Concept: **Statistical variance in LLM outputs**
  - Why needed here: The paper emphasizes that many MAD studies ignore variance, yet standard deviations in Table 1-3 often exceed 2 percentage points. Proper experimental design requires repeated runs.
  - Quick check question: Why does temperature=1.0 (MALLM default) necessitate multiple experimental runs?

- Concept: **Test-time compute scaling**
  - Why needed here: MAD is hypothesized as an alternative to training-time scaling by using more inference compute. Understanding this helps contextualize why convergence speed matters.
  - Quick check question: How does the Memory paradigm's faster convergence relate to computational efficiency?

## Architecture Onboarding

- Component map:
  - PersonaGenerator (abstract base) → Expert/IPIP/None implementations create agent system prompts
  - ResponseGenerator (abstract base) → Simple/Critical/Reasoning implementations format agent outputs
  - DiscussionParadigm (abstract base) → Memory/Relay/Report/Debate implementations control turn-taking and visibility
  - DecisionProtocol (abstract base) → Voting/Consensus/Judge implementations aggregate final answers
  - Evaluation pipeline → Loads datasets, computes metrics (accuracy, BLEU, BERTScore), generates charts

- Critical path:
  1. Define experiment via JSON config file (see Section H for template)
  2. Dataset loader converts to unified JSON format
  3. Orchestrator initializes agents with persona/response generators
  4. Discussion paradigm manages message flow for max_turns iterations
  5. Decision protocol extracts final answer
  6. Evaluation pipeline computes metrics with standard deviation across repeats

- Design tradeoffs:
  - Modularity vs. optimization: Abstract base classes enable extension but add indirection layers
  - Dataset generality vs. task-specific tuning: Unified format supports any HuggingFace dataset but may lose domain-specific preprocessing
  - Configuration simplicity vs. expressiveness: JSON configs cover common cases; custom components require Python subclassing

- Failure signatures:
  - Tie loops in voting protocols without resolution (handled by additional debate round)
  - Consensus never reaching threshold within max_turns (defaults to final vote)
  - Problem drift (agents diverge from original task)—addressed in related work Becker et al. 2025
  - High variance across runs requiring more repetitions for statistical power

- First 3 experiments:
  1. **Baseline replication**: Run single-agent Chain-of-Thought vs. 3-agent Memory+Simple+MajorityConsensus on StrategyQA (10 samples, 3 repeats) to verify MAD benefit.
  2. **Protocol ablation**: Fix agents (Expert+Simple) and paradigm (Memory), compare all 7 decision protocols on MMLU-Pro subset to replicate Table 3 patterns.
  3. **Custom component test**: Implement a custom ResponseGenerator (e.g., "Skeptical" style) by subclassing the abstract base, run against Simple/Critical baselines on a reasoning dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms determine the success of Multi-Agent Debate (MAD) systems?
- Basis in paper: [explicit] The introduction states, "Yet, we have not understood the exact mechanisms of when and why MAD is successful," noting competing hypotheses regarding test-time compute versus emergent capabilities.
- Why unresolved: Existing frameworks tightly couple components, making it difficult to isolate variables and measure individual effects on performance.
- What evidence would resolve it: Systematic ablation studies using MALLM to independently vary agents, paradigms, and protocols while controlling for compute.

### Open Question 2
- Question: Does multi-agent debate improve resilience against jailbreak attacks compared to single-agent systems?
- Basis in paper: [explicit] Section 4.1 identifies a use case involving JailbreakBench to measure "the comparison of multi-agent safety with a single-agent setup."
- Why unresolved: It is unclear if collective scrutiny acts as a defense or if the complexity of interaction introduces new vulnerabilities.
- What evidence would resolve it: Benchmarking MALLM configurations against single-agent baselines on the JailbreakBench dataset to compare attack success rates.

### Open Question 3
- Question: How does dynamic, adaptive moderation affect debate convergence and task performance?
- Basis in paper: [explicit] Section 4.1 suggests a "promising direction" involving a "dynamic moderator" that adjusts speaking order based on previous contributions.
- Why unresolved: Current discussion paradigms (e.g., Memory, Relay) use static rules; the impact of real-time, LLM-based orchestration remains unquantified.
- What evidence would resolve it: Implementing a `DiscussionParadigm` subclass for adaptive moderation and comparing turn counts and accuracy against static paradigms.

## Limitations

- The framework's modular architecture may not fully eliminate hidden interdependencies between components, potentially confounding causal attribution of performance differences
- Statistical significance is limited by the high variance in LLM outputs (temperature=1.0), with standard deviations often exceeding 2 percentage points
- Task categorization (knowledge vs. reasoning) driving protocol effectiveness may not generalize to datasets with mixed characteristics or novel task formats

## Confidence

- **High**: MALLM successfully provides a modular framework with >144 configurations and reproducible evaluation pipeline
- **Medium**: Observed performance differences across configurations are real and meaningful
- **Low**: Causal claims about individual component effects and universal protocol recommendations

## Next Checks

1. **Interaction effect validation**: Systematically vary two components simultaneously (e.g., Expert persona × Memory paradigm) while holding others constant to quantify interaction effects beyond individual main effects.

2. **Statistical power analysis**: Conduct experiments with varying sample sizes and statistical tests to determine minimum repetitions needed for reliable detection of performance differences at different effect sizes.

3. **Cross-dataset generalization**: Test the knowledge-vs-reasoning protocol distinction on datasets that blend both characteristics (e.g., multi-step reasoning with factual retrieval) to validate or refine the proposed categorization framework.