---
ver: rpa2
title: 'FEABench: Evaluating Language Models on Multiphysics Reasoning Ability'
arxiv_id: '2504.06260'
source_url: https://arxiv.org/abs/2504.06260
tags:
- gid00001
- gid00068
- gid00083
- code
- gid00078
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FEABench introduces a benchmark for evaluating large language\
  \ models (LLMs) and LLM agents on their ability to solve multiphysics problems using\
  \ finite element analysis (FEA) software. The core method involves providing LLMs\
  \ with natural language problem descriptions and requiring them to generate executable\
  \ API calls to COMSOL Multiphysics\xAE to compute target values."
---

# FEABench: Evaluating Language Models on Multiphysics Reasoning Ability

## Quick Facts
- arXiv ID: 2504.06260
- Source URL: https://arxiv.org/abs/2504.06260
- Authors: Nayantara Mudur; Hao Cui; Subhashini Venugopalan; Paul Raccuglia; Michael P. Brenner; Peter Norgaard
- Reference count: 40
- Primary result: Multi-Turn Agent achieves 88% executability in generating COMSOL FEA code from natural language

## Executive Summary
FEABench introduces a benchmark for evaluating large language models (LLMs) and LLM agents on their ability to solve multiphysics problems using finite element analysis (FEA) software. The core method involves providing LLMs with natural language problem descriptions and requiring them to generate executable API calls to COMSOL Multiphysics® to compute target values. The study introduces a comprehensive evaluation strategy with intermediate metrics to assess different facets of "distance to a correct solution," including executability, model tree score, and physics-specific metrics.

## Method Summary
The benchmark requires LLMs to generate COMSOL Multiphysics® Java API code from natural language problem descriptions. Two prompting strategies are evaluated: One-Shot (baseline) and PhyDoc In-Context (includes valid physics interfaces/features list). The Multi-Turn Agent implements a 20-sample initial population plus 20 correction iterations using execution feedback. A hybrid evaluator provides line-by-line API execution feedback and queries a VerifierLLM when executability exceeds 90%. Tools include RetrieveAnnotatedSnippets (768 annotated code snippets), QueryModelTreeProperties, QueryPhysicsInterfaces, and QueryPhysicsFeatures. The fitness function combines executability and export success to guide iteration selection.

## Key Results
- Multi-Turn Agent achieves 88% executability in generating executable API calls
- PhyDoc In-Context prompting eliminates interface hallucinations (0.54→1.0 factuality)
- Physics-specific blocks show largest executability gap (0.71 vs 0.84 for geometry)
- Valid targets within 10% error achieved for only 1/15 problems despite high executability

## Why This Works (Mechanism)

### Mechanism 1
Iterative execution feedback improves code executability in low-resource domain-specific languages. The agent executes generated COMSOL API calls, receives line-by-line error messages, and a CorrectorSubAgent revises code based on specific failure modes. Executability increased from 0.62 to 0.88 in the multi-turn agent condition.

### Mechanism 2
Providing valid API documentation in-context reduces interface hallucination. The "PhyDoc In-Context" prompt includes a list of 140 valid physics interfaces and features, constraining the model's generation space. Interface factuality improved from 0.54 to 1.0 on ModelSpecs.

### Mechanism 3
Retrieval-augmented code search helps translate natural language steps to syntactically correct API calls. The RetrieveAnnotatedSnippets tool queries a library of 768 annotated code snippets using semantic similarity, returning examples that demonstrate how to express specific modeling steps in COMSOL syntax.

## Foundational Learning

- **Finite Element Analysis workflow (Geometry → Material → Physics → Mesh → Solver → Results)**: Understanding the sequence is prerequisite to generating coherent API calls. *Quick check: Can you explain why physics interface selection must precede boundary condition definition?*

- **API tree navigation patterns (model.component().physics().feature().set())**: Errors often stem from incorrect node paths; the agent must understand parent-child relationships to diagnose "Unknown feature" errors. *Quick check: Given `model.component("comp1").physics("ht").feature("temp1")`, what happens if "ht" was never created?*

- **Hybrid evaluation signals (syntactic executability vs. semantic correctness)**: 88% executability does not imply correct solutions; the VerifierLLM provides alignment feedback only when executability exceeds 90%. *Quick check: Why might a solution with 100% executability still compute an incorrect target value?*

## Architecture Onboarding

- **Component map**: ControllerAgent -> Evaluator -> CorrectorSubAgent -> ToolLookupAgent -> RetrieveAnnotatedSnippets/QueryModelTreeProperties/QueryPhysicsInterfaces/QueryPhysicsFeatures -> Python-COMSOL Bridge

- **Critical path**: 
  1. Sample initial population (20 solutions) using PhyDoc In-Context prompt
  2. Execute each solution; compute fitness = Executability + ExportSuccessful
  3. For 20 correction steps: select solution via MCMC-inspired criterion → call tools → CorrectorSubAgent revises → evaluate
  4. Select best solution by: Executability + bool(ComputedValue) + [(1 - RelativeError) if valid]

- **Design tradeoffs**: Fitness function prioritizes executability over correctness; valid targets within 10% error were achieved for only 1/15 problems. VerifierLLM called only above 90% executability threshold—balances cost vs. coverage.

- **Failure signatures**: 
  - "Unknown Interface" → hallucinated physics name; check against valid list
  - "Unknown feature. Tag: X" → parent node X never created due to earlier error
  - "Feature cannot be created in dimension N" → geometry dimension mismatch with feature type
  - Solution exports default value (e.g., 293.15 K) → model partially built but solver didn't run correctly

- **First 3 experiments**:
  1. Run baseline ModelSpecs task with One-Shot prompt on a single heat transfer problem; inspect block-wise executability to identify which stage fails most.
  2. Compare One-Shot vs. PhyDoc In-Context on the same problem; quantify reduction in interface hallucinations.
  3. Enable the ToolLookupAgent for one correction iteration; examine whether retrieved snippets address the specific error encountered.

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs effectively solve multiphysics problems by operating simulation software through a graphical user interface (GUI) rather than a text-based API? The current study relies exclusively on generating API calls, which bypasses the visual-spatial reasoning required to identify geometric features typical in human engineering workflows.

### Open Question 2
How can agentic feedback loops be modified to correct physics reasoning errors rather than just syntax errors? The Multi-Turn Agent significantly improved code "executability" (to 88%) but rarely produced a correct "Valid Target"; the difficulty lies in "making correct physics reasoning decisions."

### Open Question 3
Does the specific retrieval-augmented generation strategy used here generalize to other low-resource domain-specific languages (DSLs) in engineering? The conclusion hypothesizes that "Using an LLM-annotated corpus to boost code executability might facilitate code generation in other low-resource domain-specific language contexts."

## Limitations

- Absence of gold-standard target values for majority of FEABench Large (200 problems), relying instead on LLMs to validate targets
- Arbitrary VerifierLLM threshold (90% executability) may exclude valid solutions with cascading errors
- MCMC-inspired selection criterion for correction iterations is not fully specified
- Physics-specific blocks show largest executability gap but paper doesn't fully explain why physics reasoning is more challenging

## Confidence

- **Multi-Turn Agent Performance (88% executability)**: High confidence - directly measured from code execution on FEABench Gold problems
- **PhyDoc In-Context Improvement (0.54→1.0 factuality)**: High confidence - clear before/after comparison on documented interface hallucination rates
- **Retrieval-augmented Search Effectiveness**: Medium confidence - demonstrated on specific problems but lacks ablation studies
- **Physics-specific Block Reasoning**: Low confidence - physics blocks show largest gap but paper doesn't fully explain why

## Next Checks

1. **Ablation Study on Correction Iterations**: Run the Multi-Turn Agent with 0, 5, 10, 15, and 20 correction steps on FEABench Gold to determine whether the 20-step budget is necessary or if performance plateaus earlier.

2. **Ground Truth Target Validation**: Manually verify target values for 10 problems from FEABench Large using COMSOL to establish whether LLM-verified targets introduce systematic bias in the evaluation.

3. **Tool Usage Analysis**: Instrument the agent to log which tools are called, when, and with what success rate. Analyze whether RetrieveAnnotatedSnippets is actually being used effectively or if most corrections come from direct feedback processing.