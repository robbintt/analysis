---
ver: rpa2
title: Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question
  Answering?
arxiv_id: '2507.09638'
source_url: https://arxiv.org/abs/2507.09638
tags:
- reward
- grpo
- citation
- legal
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces GRPO for Thai legal question answering to
  improve citation accuracy and response quality. The approach uses a multi-component
  reward function (format, non-hallucination, citation F1) and compares GRPO against
  instruction tuning.
---

# Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?

## Quick Facts
- **arXiv ID**: 2507.09638
- **Source URL**: https://arxiv.org/abs/2507.09638
- **Reference count**: 40
- **Primary result**: GRPO achieves up to 90% citation-F1 gains and 31% improvement in joint quality metrics over instruction tuning for Thai legal question answering.

## Executive Summary
This work introduces GRPO for Thai legal question answering to improve citation accuracy and response quality. The approach uses a multi-component reward function (format, non-hallucination, citation F1) and compares GRPO against instruction tuning. Experiments on NitiBench show GRPO achieves up to 90% citation-F1 gains and 31% improvement in joint quality metrics over instruction tuning, with enhanced robustness on complex reasoning tasks. Using semantic similarity (BGE-M3) as a reward proxy reduces computational cost by 2.5x compared to LLM judges while maintaining alignment quality. Thai-specific pretraining further improves alignment effectiveness.

## Method Summary
The method employs GRPO with LoRA adapters (r=256) to optimize Thai legal question answering models. The reward function consists of four components: Format (XML structure validation), Non-Hallucination (citation presence in retrieved context), Citation F1 (ground truth overlap), and Answer Quality (semantic similarity via BGE-M3 or coverage/consistency via LLM judge). Training uses 1 epoch with LR=5e-6, rollout=10, and max_grad_norm=0.2 on Unsloth with bfloat16. The approach compares GRPO against SFT baselines on WangchanX-Legal-ThaiCCL-RAG training data and evaluates on NitiBench (CCL/Tax splits).

## Key Results
- GRPO achieves up to 90% citation-F1 gains over SFT baselines
- GRPO improves joint quality metrics by 31% compared to instruction tuning
- Semantic similarity rewards reduce computational cost by 2.5x versus LLM judges while maintaining alignment quality
- Thai-specific pretraining models show superior GRPO effectiveness compared to language-agnostic models

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical reward decomposition likely improves citation fidelity better than standard instruction tuning by enforcing structure before semantic evaluation. The system applies a three-stage reward filter: Format (XML structure) → Non-Hallucination (citation presence in context) → Citation F1 (ground truth overlap). This prevents the optimizer from rewarding high semantic similarity for answers that cite non-existent laws.

### Mechanism 2
Semantic similarity embeddings (BGE-M3) serve as a cost-efficient proxy for answer quality rewards, though efficacy varies by task complexity. Instead of using an expensive LLM judge to score coverage, the system computes vector similarity between generated answer and ground truth, providing a dense gradient signal at 2.5x lower compute cost.

### Mechanism 3
GRPO likely enhances sampling efficiency for domain-aligned base models rather than expanding fundamental reasoning boundaries. GRPO optimizes the policy to bias output generation toward correct pathways already latent in the base model, effective primarily when the base model has strong domain priors (Thai-CPT).

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the core RL algorithm replacing or supplementing Supervised Fine-Tuning (SFT). Unlike standard PPO, it calculates advantages relative to group mean rewards rather than a separate value function, critical for reported efficiency gains.
  - **Quick check question:** How does GRPO calculate the advantage function differently than PPO, and why does this remove the need for a critic model?

- **Concept: Reward Shaping / Decomposition**
  - **Why needed here:** The paper's success relies on breaking down "legal QA" into verifiable sub-goals (format, non-hallucination, F1). Understanding this is necessary to debug why a model might achieve high semantic scores but low citation accuracy.
  - **Quick check question:** In the proposed reward function, why is the Non-Hallucination reward contingent on the Format reward?

- **Concept: In-Domain vs. Out-of-Distribution (OOD) Generalization**
  - **Why needed here:** Results show stark contrast between Nitibench-CCL (in-domain) and Nitibench-Tax (OOD). Mechanisms that work for CCL (semantic proxy) fail for Tax, highlighting limits of proposed alignment strategy.
  - **Quick check question:** Why does the semantic similarity reward correlate with coverage on CCL but not on Tax?

## Architecture Onboarding

- **Component map:** User Query + Retrieved Context → Policy Model → Reward Server (Semantic OR Judge) → GRPO Trainer
- **Critical path:** The connection between the Reward Server and the GRPO Trainer. If the reward signal is noisy (as seen in Tax/Semantic correlation) or delayed, the policy update will destabilize.
- **Design tradeoffs:**
  - Semantic vs. Judge Reward: Semantic rewards are 2.5x cheaper and align well for simple tasks (CCL). Judge rewards are expensive but maintain robustness on complex tasks (Tax).
  - Citation vs. Answer Position: Placing citations after the answer (default) outperforms placing them before (Table 3), suggesting the model uses answer generation to consolidate citations.
- **Failure signatures:**
  - Reward Hacking: High Citation F1 but low Coverage (model cites correctly but answers wrongly)
  - Generalization Collapse: Performance drops below baseline on OOD data (seen with SFT on Tax)
  - Format Drift: Model generates free text instead of XML if Format Reward is weighted too low
- **First 3 experiments:**
  1. Sanity Check (Reward Correlation): Scatter plot of BGE-M3 similarity scores vs. Ground Truth Coverage on a validation set to verify the proxy works for your specific domain before training.
  2. Ablation (Citation Only): Train with only Citation Rewards (w/o answer reward) to confirm if answer quality degrades (as per Section 6.2), establishing the need for multi-component rewards.
  3. Efficiency Baseline: Compare GPU hours and Joint Score of "GRPO + Semantic" vs. "SFT" to validate the cost-benefit claim in Section 6.5.

## Open Questions the Paper Calls Out

### Open Question 1
Can non-naive reward combination strategies (such as weighted normalization or learned weighting) outperform individual reward signals for GRPO in Thai legal QA? The authors state in the Limitations section that "A well-tuned combination may offer synergistic benefits, but this remains unexplored" due to resource constraints preventing exploration beyond naive summation.

### Open Question 2
How does GRPO alignment effectiveness differ when applied directly to base models compared to pre-instruction-tuned models? The Limitations section notes, "Investigating its effect from different model initialization states may yield further insights, but it was beyond our current scope."

### Open Question 3
Does the "Dr. GRPO" variant improve token efficiency and alignment performance compared to standard GRPO in this legal domain context? The authors acknowledge potential biases in the standard GRPO formulation regarding response length normalization and state, "employing Dr. GRPO might yield different results... but it was beyond our current scope."

### Open Question 4
Can alternative efficient reward proxies be developed that maintain correlation with coverage and consistency on complex, multi-label reasoning tasks like NitiBench-Tax? The paper concludes that "Future work should focus on developing reward mechanisms that balance accuracy for complex reasoning with practical training efficiency," noting that semantic similarity fails on complex tasks.

## Limitations
- Reward function reliability is domain-task dependent, with semantic similarity working for simple tasks but failing on complex multi-label reasoning
- Approach strongly depends on domain-aligned base models, limiting applicability when such models are unavailable
- Out-of-distribution brittleness causes SFT to show negative gains on complex tasks and GRPO benefits to disappear on language-agnostic models

## Confidence
- **High confidence**: GRPO achieves measurable citation accuracy improvements (90% F1 gain) on in-domain Thai legal tasks when using domain-aligned base models with multi-component rewards
- **Medium confidence**: Semantic similarity (BGE-M3) provides 2.5x cost reduction while maintaining alignment quality, but only for simple, single-answer tasks
- **Low confidence**: The approach generalizes to complex legal reasoning or non-Thai legal domains without substantial modifications to reward functions and base model selection

## Next Checks
1. **Reward correlation validation**: Before full training, compute and plot BGE-M3 similarity scores against ground truth coverage on a validation subset to verify the semantic proxy correlates with factual accuracy for your specific task domain
2. **Ablation for reward balance**: Train with only citation rewards (no answer quality component) to confirm whether answer quality degrades, establishing the necessity of the multi-component reward structure
3. **Base model dependency test**: Compare GRPO performance between a domain-aligned model (Thai-CPT) and a language-agnostic model (Qwen2.5) on both simple and complex tasks to quantify the extent to which improvements depend on pre-existing domain knowledge rather than learned reasoning capabilities