---
ver: rpa2
title: 'AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision
  LLMs'
arxiv_id: '2510.10467'
source_url: https://arxiv.org/abs/2510.10467
tags:
- anybcq
- precision
- speed
- alpaper
- binary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AnyBCQ, a hardware-efficient quantization
  framework that extends Binary-Coded Quantization (BCQ) to support multi-precision
  large language models (LLMs). AnyBCQ addresses the challenge of efficiently deploying
  LLMs across diverse service-level objectives by enabling flexible bit-width inference
  with minimal memory overhead.
---

# AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs

## Quick Facts
- arXiv ID: 2510.10467
- Source URL: https://arxiv.org/abs/2510.10467
- Reference count: 21
- Enables flexible bit-width LLM inference with minimal memory overhead through BCQ-based progressive precision expansion

## Executive Summary
This paper introduces AnyBCQ, a hardware-efficient quantization framework that extends Binary-Coded Quantization (BCQ) to support multi-precision large language models (LLMs). AnyBCQ addresses the challenge of efficiently deploying LLMs across diverse service-level objectives by enabling flexible bit-width inference with minimal memory overhead. The core innovation lies in progressively expanding model precision while reusing binary bit-planes and introducing new scale factors, allowing direct bit-plane operations without costly centroid lookups. Experiments on Llama-3.1-8B demonstrate that AnyBCQ significantly narrows accuracy degradation in low-bit regimes (e.g., 2-bit), remains competitive at higher precisions, and achieves throughput gains of up to 3.0× over half precision and 1.2× over state-of-the-art multi-precision methods.

## Method Summary
AnyBCQ implements a progressive precision expansion framework that builds on binary-coded quantization (BCQ) representation. The method first quantizes models to a base precision (typically 2-bit) using greedy initialization followed by alternating refinement cycles. For each subsequent precision level, existing binary codes remain frozen while new residual-derived bit-planes are optimized with updated scaling factors. A specialized CUDA kernel exploits this BCQ structure to enable dynamic per-request precision selection by directly operating on bit-planes without bit-transposition or centroid table lookups. The approach includes group-wise scale optimization and reconstruction error minimization calibrated on 512 sequences from the C4 dataset.

## Key Results
- AnyBCQ achieves 2.94× higher throughput than cuBLAS FP16 and 1.19× over state-of-the-art Any-Precision LLM
- Outperforms fixed-precision baselines at 2-bit with only 0.5% accuracy degradation on MMLU
- Maintains competitive accuracy at 3-bit and 4-bit while enabling flexible per-request precision switching

## Why This Works (Mechanism)

### Mechanism 1
Binary-coded quantization (BCQ) enables direct bit-plane computation that avoids the memory and compute overhead of centroid table lookups required by non-uniform quantization. Weights are decomposed as Ŵ = ΣᵢαᵢBᵢ where Bᵢ ∈ {-1, +1} and αᵢ ∈ ℝ. During inference, each bit-plane operation reduces to simple addition/subtraction of activations scaled by αᵢ, eliminating bit-transposition and table-lookup phases. The core assumption is that the binary basis representation is sufficiently expressive to approximate weight distributions at target precisions (2–4 bits).

### Mechanism 2
Progressive precision expansion with frozen binary codes enables multi-precision models with minimal memory overhead. The model is first quantized to base precision pL. For each subsequent precision p+1, existing binary codes B₁...Bₚ remain frozen; only a new residual-derived bit-plane Bₚ₊₁ and updated scaling factors {α⁽ᵖ⁾} are optimized. This allows sharing binary representations across precisions. The core assumption is that the residual structure captured by frozen lower-bit planes remains valid when higher precision is enabled.

### Mechanism 3
Specialized CUDA kernels exploiting BCQ structure achieve throughput gains by eliminating bit-transposition and table-lookup overhead while fetching only required bit-planes. The kernel fetches p bit-planes for p-bit inference. Each bit-plane operation uses LUT-based GEMM where binary × activation reduces to precomputed partial sums indexed by binary patterns. Outputs are scaled by αᵢ and accumulated. The core assumption is that memory bandwidth is the primary bottleneck in LLM inference.

## Foundational Learning

**Binary-Coded Quantization (BCQ)**
- Why needed here: AnyBCQ builds directly on BCQ representation; understanding how Ŵ = ΣᵢαᵢBᵢ decomposes weights is essential.
- Quick check question: Given a 2-bit BCQ with α₁=0.5, α₂=0.25 and binary codes B₁=[-1,+1], B₂=[+1,+1], what is the reconstructed weight vector?

**Post-Training Quantization (PTQ) and Calibration**
- Why needed here: AnyBCQ uses reconstruction error minimization over calibration data to optimize scaling factors.
- Quick check question: Why does PTQ for LLMs typically use reconstruction error (MSE) rather than task-specific loss?

**Bit-Plane Storage and Memory Layout**
- Why needed here: Multi-precision models store weights as bit-planes (M×K×p) rather than packed integers to enable selective fetching.
- Quick check question: For a 4096×4096 weight matrix at 4-bit precision, what is the memory footprint difference between bit-plane storage and packed 4-bit integer storage?

## Architecture Onboarding

**Component map**:
Quantization module (greedy init + alternating refinement) -> Progressive precision expansion (frozen binaries + new bit-planes) -> CUDA kernel (bit-plane fetch → LUT-GEMM → scaling → accumulation) -> Memory layout (binary tensor + per-precision scales)

**Critical path**:
1. Calibration data sampling (512 sequences from C4)
2. Base-precision quantization (pL-bit) with T=20 refinement cycles
3. Progressive expansion: for p = pL+1 to pH, initialize new bit-plane from residual, refine scales
4. Per-precision scale set optimization via reconstruction error minimization

**Design tradeoffs**:
- Shared binary codes: Reduces memory 49% vs. multi-model baseline but limits optimization space at higher precisions
- Group size g=128: Larger groups reduce scale memory but may hurt accuracy; smaller groups increase scale overhead
- BCQ vs. non-uniform quantization: BCQ trades some representational flexibility for hardware efficiency

**Failure signatures**:
- 2-bit accuracy collapse: Wiki perplexity >100 or MMLU <25% indicates base precision optimization failed
- No throughput gain vs. FP16: Kernel not memory-bound or bit-plane fetch not optimized
- Multi-precision model worse than fixed-precision at same bits: Shared-binary constraint too restrictive
- Generation loops/repetition at low average precision: Indicates 2-bit quality insufficient for mixed-precision decoding

**First 3 experiments**:
1. Baseline validation: Quantize Llama-3.1-8B to 2/3/4-bit fixed precision; verify Table 2 accuracy numbers within ±0.5% on MMLU and CSR average
2. Ablation on base precision: Compare pL=2 vs. pL=3 initialization for pH=4 target; measure accuracy gap to fixed-precision baselines and memory overhead
3. Kernel profiling: Profile GEMV latency breakdown (bit-fetch, compute, accumulation) on A100 for shapes in Table 3; verify bit-transposition overhead is eliminated

## Open Questions the Paper Calls Out

### Open Question 1
Can formal theoretical bounds be established for the reconstruction error and convergence properties of the progressive precision expansion procedure? The present work remains largely empirical and lacks theoretical guarantees, particularly for the progressive precision expansion procedure.

### Open Question 2
What throughput and energy efficiency gains can AnyBCQ achieve on dedicated BCQ-supporting accelerators (e.g., iFPU, FIGLUT) compared to the observed GPU improvements? Because AnyBCQ is also BCQ-based and supports dynamic bit-width selection, it can be naturally deployed on such architectures.

### Open Question 3
Can the shared-binary constraint be selectively relaxed for higher precision levels to narrow the accuracy gap with non-uniform methods while preserving memory efficiency? The inherent representational capacity of BCQ, together with the shared-binary constraint, can reduce peak accuracy at higher bit widths relative to non-uniform schemes.

### Open Question 4
How does AnyBCQ's accuracy–throughput trade-off scale when applied to models with 70B+ parameters? Experiments are limited to 8B–14B models; kernel latency tests use 70B layer shapes but lack end-to-end evaluation.

## Limitations
- Representational flexibility trade-off: BCQ-based methods may face representational limitations at very low precisions compared to non-uniform quantization schemes
- Hardware-specific optimizations: Throughput gains rely heavily on specialized CUDA kernels that may not translate to other hardware architectures
- Calibration data requirements: Limited or unrepresentative calibration sequences can cascade poor performance from base to higher precisions

## Confidence
- High confidence: BCQ-based bit-plane computation and memory efficiency benefits are well-established with eliminated bit-transposition overhead demonstrated
- Medium confidence: 2-bit precision results show promise but some degradation compared to fixed-precision baselines; multi-precision benefits may be workload-dependent
- Low confidence: Generalizability of throughput gains across different hardware platforms and long-term stability under varying deployment conditions remain unverified

## Next Checks
1. Ablation study on base precision initialization: Systematically compare pL=2 vs. pL=3 initialization strategies for pH=4 target precision to quantify impact of base precision quality on final multi-precision accuracy
2. Cross-platform kernel performance validation: Port the BCQ kernel to alternative hardware (e.g., AMD GPUs or CPU-based implementations) and measure whether memory bandwidth savings translate to similar throughput improvements
3. Long-term generation stability test: Evaluate the multi-precision model on extended generation tasks (e.g., 1000+ tokens) to verify that 2-bit precision quality is sufficient to prevent degradation phenomena like repetition loops or semantic drift