---
ver: rpa2
title: Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget
arxiv_id: '2506.02386'
source_url: https://arxiv.org/abs/2506.02386
tags:
- lemma
- algorithm
- then
- best
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the best feasible arm identification problem
  under fixed budget in linear bandits, where arms are constrained by linear inequalities
  and the goal is to identify the optimal arm within the feasible set. The authors
  propose BLFAIPS, a posterior sampling-based algorithm that integrates a min-learner
  (posterior sampling with constraint-aware sampling) and a max-learner (AdaHedge
  with a novel loss function) in a game-theoretic framework.
---

# Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget

## Quick Facts
- arXiv ID: 2506.02386
- Source URL: https://arxiv.org/abs/2506.02386
- Reference count: 40
- One-line primary result: Achieves asymptotic optimality by matching theoretical lower bound on exponential decay rate of error probability

## Executive Summary
This paper addresses the best feasible arm identification problem in linear bandits with fixed budget, where arms must satisfy linear constraints. The authors propose BLFAIPS, a novel algorithm combining posterior sampling and AdaHedge in a game-theoretic framework. The key contribution is achieving asymptotic optimality by matching the theoretical lower bound on the exponential decay rate of error probability, making it the first algorithm to do so in this setting. The method demonstrates superior empirical performance on both synthetic and real-world datasets compared to existing baselines.

## Method Summary
BLFAIPS operates through a game-theoretic sampling rule where a "max-learner" (AdaHedge) and "min-learner" (posterior sampling with constraint-aware sampling) play against each other. The algorithm estimates linear reward and cost functions using Ridge Regression, then uses posterior sampling restricted to the set of parameters that would make the current empirical best arm not the true best. AdaHedge automatically adapts the learning rate to eliminate the need for manual tuning while maintaining sublinear regret. The algorithm mixes the AdaHedge distribution with a G-optimal design to ensure proper exploration and guarantee convergence of the covariance matrix.

## Key Results
- Achieves asymptotic optimality by matching theoretical lower bound on exponential decay rate $\Gamma$
- First algorithm to achieve this bound in the linear best feasible arm identification setting
- Superior performance on synthetic and MovieLens datasets compared to constrained linear Thompson Sampling and Linear Top-Two Thompson Sampling
- Faster convergence and higher accuracy under varying dimensions and budget constraints

## Why This Works (Mechanism)

### Mechanism 1: Game-Theoretic Sampling Rule
The algorithm achieves asymptotic optimality by framing the sampling process as a zero-sum game between max-learner (algorithm) and min-learner (adversary), ensuring error probability decays at optimal exponential rate $\Gamma$. The max-learner selects sampling distribution to maximize divergence between true and alternative models, while min-learner selects alternative parameters to minimize this divergence. Theorem 4.3 proves this saddle-point strategy matches the information-theoretic lower bound.

### Mechanism 2: Adversarial Posterior Sampling (Min-Learner)
Instead of solving computationally expensive minimization over all alternative parameters, the algorithm uses posterior sampling restricted to the "alternative" set $\Theta_{\hat{z}_t}$. This probabilistically biases the search toward parameters that are plausible given data but confuse the identity of the best feasible arm, forcing the max-learner to gather evidence to rule out these confusing cases.

### Mechanism 3: AdaHedge for Tuning-Free Adaptation
Replacing standard exponential weights with AdaHedge eliminates need for manual learning rate scheduling while maintaining sublinear regret. AdaHedge adapts learning rate automatically based on the mixability gap, allowing the algorithm to handle varying scales of the loss function without requiring foreknowledge of total budget $T$ or specific gap parameters.

## Foundational Learning

- **Concept: Linear Bandits & Ridge Regression**
  - Why needed here: Algorithm models rewards and costs as linear functions $y = \langle \theta, x \rangle + \epsilon$. Understanding Ridge Regression ($\hat{\theta} = V^{-1}S$) is essential for posterior sampling and "good event" definition.
  - Quick check question: How does matrix $V_t$ (Line 18) influence variance of posterior distribution from which adversarial parameters are sampled?

- **Concept: Thompson Sampling (Posterior Sampling)**
  - Why needed here: "Min-learner" is modified Thompson Sampler. Standard Thompson Sampling samples parameters proportional to probability they are optimal, whereas here it samples parameters proportional to probability they represent a confusing alternative.
  - Quick check question: In Line 12, why is sample $(\theta_r^t, \theta_c^t)$ restricted to set $\Theta_{\hat{z}_t}$ instead of global parameter space $\Theta$?

- **Concept: Minimax Theorem & Sion's Theorem**
  - Why needed here: Theoretical justification relies on swapping max and min operators in lower bound definition. Understanding saddle-point problem helps explain why there are two "learners" playing against each other.
  - Quick check question: How does "min-learner" represent "inf" operator and "max-learner" represent "sup" operator in optimization problem $\Gamma$?

## Architecture Onboarding

- **Component map:**
  Input -> Ridge Regression -> Min-Learner (Adversary) -> Max-Learner (Strategist) -> Explorer -> Output

- **Critical path:**
  1. Estimation Accuracy: Ridge estimates must converge fast enough for empirical best arm $\hat{z}_t$ to equal true best arm $z^*$ (Lemma 5.4)
  2. Loss Calculation: Loss $l_{t,x}$ must be computed using adversarial samples $(\theta_r^t, \theta_c^t)$, not just empirical means (Line 16)
  3. Distribution Update: AdaHedge must successfully track best fixed distribution in hindsight (Lemma F.4)

- **Design tradeoffs:**
  - Exploration Mixing ($\gamma_t$): Algorithm mixes in G-optimal design to ensure $V_t$ grows linearly. Decay rate $\alpha=1/4$ balances pure exploitation vs coverage. Too little mixing risks singular covariance matrix; too much wastes budget on uninformative arms.
  - AdaHedge vs Exponential Weights: AdaHedge removes need to tune $\beta$ but adds computational overhead for tracking cumulative mixability gap $\Delta_t$.

- **Failure signatures:**
  - Stuck Recommendation: If constraint threshold $\tau$ is tight and noise $\gamma$ is high, empirical feasible set $\hat{F}$ might be empty, forcing uniform random choice. Indicates initial exploration budget or mixing parameter $\gamma_t$ is insufficient.
  - Oscillating Empirical Best Arm: If $\hat{z}_t$ oscillates wildly, restriction set $\Theta_{\hat{z}_t}$ changes constantly, turning min-max game into moving target and potentially invalidating regret bounds.

- **First 3 experiments:**
  1. End of Optimism Validation: Replicate scenario with $\alpha=0.1$ to verify BLFAIPS identifies arm $[1,0]^\top$ correctly while handling super-optimal infeasible arm $[1.2, 1.2]^\top$
  2. Constraint Sensitivity: Run sweep on threshold $\tau$ to find phase transition point where algorithm switches between identifying constrained optimal arm vs unconstrained optimal arm
  3. Ablation on AdaHedge: Replace AdaHedge with standard fixed-learning-rate Hedge algorithm to validate adaptive rate is necessary for stability when $T$ is unknown

## Open Questions the Paper Calls Out

- **Open Question 1:** Can theoretical guarantees be extended to non-convex or high-dimensional parameter spaces?
  - Basis in paper: [explicit] Conclusion states extending results to "non-convex or high-dimensional parameter spaces, remains valuable"
  - Why unresolved: Current proofs rely on Assumption 2.1 (bounded, closed parameter spaces) and Laplace approximations that may not hold without convexity
  - What evidence would resolve it: Derivation of error probability bounds that hold for non-convex $\Theta$ or in high-dimensional settings ($d \gg T$)

- **Open Question 2:** How does algorithm perform under non-Gaussian or general noise distributions?
  - Basis in paper: [explicit] Conclusion suggests "refining guarantees under weaker assumptions, especially regarding noise," could broaden applicability
  - Why unresolved: Theoretical analysis explicitly utilizes Gaussian properties of noise and conjugate priors to derive posterior ratios
  - What evidence would resolve it: Proof of asymptotic optimality for BLFAIPS under sub-Gaussian or heavy-tailed noise distributions

- **Open Question 3:** Can uniqueness assumption on best feasible arm be relaxed?
  - Basis in paper: [inferred] Assumption 2.3 explicitly requires best feasible arm $z^*$ to be unique
  - Why unresolved: Hardness parameter $\Gamma$ and error exponent derivation depend on minimum gap $\Delta_{\min}$ being non-zero
  - What evidence would resolve it: Algorithmic modification or analysis demonstrating consistent identification of optimal set of arms when gaps are zero

## Limitations

- Theoretical guarantees rely on assumptions (bounded parameters, unique best arm, well-specified noise variances) that may not hold in practice
- Game-theoretic formulation introduces complexity that could obscure failure modes
- Empirical evaluation focuses on relatively low-dimensional problems (up to $d=50$) and uses synthetic MovieLens subset rather than full-scale datasets

## Confidence

- **High:** Asymptotic optimality proof structure and AdaHedge mechanism are well-established in literature
- **Medium:** Game-theoretic sampling rule and connection to information-theoretic lower bound are theoretically sound but rely on assumptions about "good event" $E_{4,\delta}$
- **Low:** Practical performance on real-world data is promising but limited to small subset of movies, and feature construction method is not fully specified

## Next Checks

1. **Robustness to Misspecification:** Test BLFAIPS with varying levels of noise variance ($\sigma^2, \gamma^2$) relative to algorithm's internal parameters ($\eta_r, \eta_c$) to identify threshold where calibration of loss function degrades and convergence slows

2. **High-Dimensional Scaling:** Evaluate algorithm's performance on synthetic data with dimensions $d > 50$ (e.g., $d = 100, 200$) to empirically validate claim that information-theoretic lower bound on exponential decay rate $\Gamma$ is achievable in practice

3. **Oracle Baseline Comparison:** Implement true Oracle baseline that pulls arms according to optimal allocation rate derived from known parameters $(\theta_r, \theta_c)$ to provide tighter comparison for empirical performance curves, especially in early stages of budget