---
ver: rpa2
title: The Impact of Model Zoo Size and Composition on Weight Space Learning
arxiv_id: '2504.10141'
source_url: https://arxiv.org/abs/2504.10141
tags:
- sane
- weight
- training
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the diversity of model zoos affects
  weight space learning, particularly for generating unseen neural network weights
  for zero-shot knowledge transfer. The authors extend the SANE encoder-decoder backbone
  to handle heterogeneous model zoos, introducing a novel per-token loss normalization
  to accommodate varying architectures and datasets.
---

# The Impact of Model Zoo Size and Composition on Weight Space Learning

## Quick Facts
- arXiv ID: 2504.10141
- Source URL: https://arxiv.org/abs/2504.10141
- Authors: Damian Falk; Konstantin Schürholt; Damian Borth
- Reference count: 15
- Primary result: Training on diverse model zoos significantly improves zero-shot knowledge transfer for generating unseen neural network weights

## Executive Summary
This paper investigates how the diversity of model zoos affects weight space learning, particularly for generating unseen neural network weights for zero-shot knowledge transfer. The authors extend the SANE encoder-decoder backbone to handle heterogeneous model zoos, introducing a novel per-token loss normalization to accommodate varying architectures and datasets. They systematically evaluate the impact of model diversity across different model sizes (CNNs and ResNets) and datasets. Results show that training on diverse model zoos significantly improves performance and generalization compared to single-zoo training, with up to 42.8% improvement on ResNet models for out-of-distribution tasks. The study also demonstrates that model zoo size impacts performance, with multi-zoo training outperforming single-zoo baselines even with equal sample sizes.

## Method Summary
The method extends the SANE weight space learning framework by introducing masked per-token loss normalization. Neural network weights are reshaped into fixed-size tokens with binary masks indicating valid data vs. padding. A transformer encoder maps tokens to latent embeddings, and a decoder maps them back. The key innovation is normalizing reconstruction loss per-token at runtime, using only non-padded elements for mean and standard deviation calculations. This allows the model to handle inhomogeneous zoos with varying architectures and datasets. The system is trained with a combined loss of normalized reconstruction and contrastive loss, enabling zero-shot generation of neural network weights for unseen tasks.

## Key Results
- Multi-zoo training significantly outperforms single-zoo baselines, with up to 42.8% improvement on ResNet models for out-of-distribution tasks
- Including models with varying underlying image datasets has high impact on performance and generalization for both in- and out-of-distribution settings
- Model zoo size impacts performance, with multi-zoo training outperforming single-zoo baselines even with equal sample sizes
- Generated models initialized with multi-zoo trained SANE achieve faster training and better final performance than HyperNetwork baselines

## Why This Works (Mechanism)

### Mechanism 1
Training weight space learning models on heterogeneous, multi-source model zoos improves generalization for generating out-of-distribution (OOD) model weights. A traditional model trained on a homogeneous zoo overfits to that specific domain. By learning from a diverse population of models, the system captures a more universal representation of neural network weights. When sampling new models, this generalized representation transfers knowledge more effectively to unseen tasks.

### Mechanism 2
Masked per-token loss normalization enables stable and accurate training on inhomogeneous zoos with varying weight scales. Neural network layers have vastly different weight distributions. Standard reconstruction loss is dominated by high-magnitude weights. The proposed normalization standardizes each token at runtime using only its valid elements, forcing the model to learn the structure of all layers equally.

### Mechanism 3
Weights generated by a multi-zoo trained encoder provide a superior, more adaptable initialization for downstream fine-tuning than HyperNetworks. SANE is trained purely on weight structure, while HyperNetworks generate weights based on image data and can overfit to it. Multi-zoo training teaches SANE a more robust structural prior, resulting in well-formed "blank slates" that are highly amenable to fine-tuning on new data.

## Foundational Learning

- **Concept: Weight Space Learning**
  - Why needed here: This is the paradigm of treating trained model parameters as the data modality, rather than images or text. All methods in the paper operate on this principle.
  - Quick check question: In this paper, what serves as the "data" for training the primary encoder-decoder model?

- **Concept: Model Zoo**
  - Why needed here: A "model zoo" is the dataset for weight space learning. Understanding the difference between a homogeneous zoo and a heterogeneous one is the core problem this paper addresses.
  - Quick check question: What are the two primary characteristics that define a model zoo's composition?

- **Concept: Encoder-Decoder Architecture (SANE)**
  - Why needed here: The SANE model is the backbone being modified. It tokenizes weights, encodes them into a latent space, and decodes them back. The paper's modification happens inside its loss function.
  - Quick check question: What is the purpose of the "projection head" in the SANE architecture?

## Architecture Onboarding

- **Component map:**
  Input Processing -> Encoder (g_θ) -> Latent Embeddings (z) -> Decoder (h_ψ) -> Output Tokens
  Mask generation -> Per-token normalization -> Reconstruction + Contrastive loss

- **Critical path:**
  1. Prepare Zoos: Aggregate models from diverse datasets
  2. Tokenize with Mask: Convert weights to tokens, ensuring the padding mask M is correctly generated
  3. Apply Masked Normalization: In the loss loop, compute per-token mean/std ignoring padding. Normalize target and prediction
  4. Train Encoder-Decoder: Optimize using the combined loss
  5. Sample & Transfer: Sample from the latent space and decode to generate weights for a new task

- **Design tradeoffs:**
  - Normalization Method: Per-layer normalization is incompatible with mixed architectures. Per-token normalization solves this but adds per-batch computational overhead
  - Zoo Composition: A larger, more diverse zoo improves OOD generalization but requires more models to avoid undersampling

- **Failure signatures:**
  - Batch Norm Collapse: If the mask is not applied during normalization, statistics will be skewed by padding, causing reconstruction failure
  - OOD Stagnation: If the training zoo is not diverse enough, performance on OOD tasks will not improve
  - Slow Convergence: The new loss normalization may require learning rate adjustments to ensure stable training

- **First 3 experiments:**
  1. Ablation on Normalization: Train on a single zoo with no normalization, unmasked per-token normalization, and masked per-token normalization. Compare reconstructed weight distributions
  2. In-Distribution Scaling: Train separate SANE models on zoos of increasing diversity while keeping total models constant. Evaluate accuracy on held-out in-distribution task
  3. Zero-Shot OOD Evaluation: Take best-performing multi-zoo model and single-zoo baseline. Generate models for completely unseen dataset. Report zero-shot accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed masked per-token normalization effectively scale to training on zoos containing models with fundamentally different architectures (e.g., mixing CNNs and Transformers) rather than just different datasets? The paper does not demonstrate if per-token normalization is sufficient to align the weight distributions of structurally distinct networks in a shared latent space.

### Open Question 2
What are the precise scaling laws regarding model zoo size and diversity, and does the performance saturation observed in single-zoo training recur in multi-zoo setups at larger scales? It is unclear if the improvements from diversity are linear, logarithmic, or if they encounter a new plateau once the zoo size exceeds the capacity of the SANE backbone.

### Open Question 3
To what extent do the performance gains stem from the novel normalization technique versus the intrinsic diversity of the multi-zoo data? Without a direct comparison of single-zoo baselines using the new normalization, it is difficult to determine if the methodological change independently boosted generalization.

## Limitations
- The study focuses exclusively on image classification datasets and CNNs/ResNets, limiting generalizability to other domains or architectures
- The per-token normalization technique is evaluated primarily through reconstruction quality and downstream accuracy, but lacks independent validation through ablation studies
- The subsampling method for generating final models uses fixed parameters (5 prompt examples, 200 candidates) without exploring sensitivity to these choices

## Confidence

- **High confidence:** The core finding that diverse model zoos improve weight space learning generalization is well-supported by systematic experiments across multiple datasets and architectures
- **Medium confidence:** The masked per-token normalization mechanism is plausible and shown to improve performance, but lacks independent validation beyond this work
- **Low confidence:** Claims about the generality of these findings to non-image domains or different model architectures are speculative

## Next Checks

1. Replicate the key experiments (Fig. 3, 4) on a held-out dataset not used in any training zoo to verify OOD generalization claims independently
2. Conduct ablation studies on the subsampling method: vary the number of prompt examples (1, 5, 10) and candidate samples (50, 200, 500) to identify sensitivity
3. Test the model zoo diversity hypothesis on a different domain: apply the method to weight space learning for NLP models (e.g., BERT variants on different text tasks) to assess cross-domain applicability