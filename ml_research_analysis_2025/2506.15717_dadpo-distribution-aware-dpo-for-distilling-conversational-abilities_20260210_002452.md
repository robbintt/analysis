---
ver: rpa2
title: 'daDPO: Distribution-Aware DPO for Distilling Conversational Abilities'
arxiv_id: '2506.15717'
source_url: https://arxiv.org/abs/2506.15717
tags:
- teacher
- dadpo
- ddpo
- distillation
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces daDPO, a method that enhances Direct Preference\
  \ Optimization (DPO) by incorporating the teacher model\u2019s probability distribution\
  \ alongside its responses. This approach aims to improve the conversational abilities\
  \ of smaller language models (LLMs) distilled from larger ones."
---

# daDPO: Distribution-Aware DPO for Distilling Conversational Abilities

## Quick Facts
- **arXiv ID**: 2506.15717
- **Source URL**: https://arxiv.org/abs/2506.15717
- **Reference count**: 38
- **Primary result**: daDPO significantly outperforms standard DPO on pruned and smaller LLMs across MT-Bench, AlpacaEval, and in-domain benchmarks, with a 20% pruned Vicuna-1.5B model nearly matching its full-sized teacher and Qwen2.5-1.5B occasionally surpassing its 7B teacher.

## Executive Summary
daDPO enhances Direct Preference Optimization by incorporating the teacher model's probability distribution alongside its responses, creating a dual-constraint objective that balances stability with richer distillation signals. The method introduces two Kullback-Leibler divergence terms—one referencing the student model and one referencing the teacher model—to create a balanced optimization objective. Empirically, daDPO demonstrates significant performance improvements over standard DPO on both pruned and smaller LLM models across multiple benchmarks, while preserving task-specific capabilities without introducing significant alignment tax. The approach also enables cross-architecture distillation by using sentence-level KL divergence rather than token-level, making it applicable across different tokenizer families.

## Method Summary
daDPO modifies the standard DPO objective by adding a second KL divergence term that references the teacher model's output distribution. The loss function combines two Bradley-Terry preference terms weighted by coefficients β₁ and β₂, where β₁ controls the reference policy constraint and β₂ controls the teacher distribution constraint. The training follows a two-stage process: first running dSFT on teacher responses to create a reference policy, then optimizing the student model with daDPO while freezing both the teacher and reference models. The method uses sentence-level KL divergence rather than token-level, enabling cross-architecture distillation across models with different tokenizers. Key hyperparameters include β₁ ∈ [0.01, 0.1, 1.0], β₂ ∈ [0.001, 0.01, 0.1, 1.0], batch_size=32, and lr=5e-7, implemented using the OpenRLHF framework.

## Key Results
- 20% pruned Vicuna-1.5B model using daDPO nearly matches the performance of its full-sized teacher
- Qwen2.5-1.5B model occasionally surpasses its 7B teacher when using daDPO
- daDPO maintains task-specific capabilities without significant alignment tax on academic benchmarks
- Cross-family distillation from Qwen2.5-7B to Llama3.2-1B succeeds with daDPO but fails with rDPO variant

## Why This Works (Mechanism)

### Mechanism 1: Dual KL-divergence constraints balance stability and knowledge transfer
The daDPO objective simultaneously constrains the student model against both the reference policy (for training stability) and the teacher distribution (for richer distillation signal). The modified RL objective introduces two KL terms with separate coefficients β₁ and β₂. Theorem 1 proves the optimal policy becomes a geometric interpolation: π*(y|x) ∝ π_ref^(β₁/(β₁+β₂)) × π_te^(β₂/(β₁+β₂)) × exp(r/(β₁+β₂)). This means the student learns to navigate between staying close to its SFT initialization while also aligning with the teacher's distributional preferences. The core assumption is that the student model has sufficient capacity to approximate a policy that satisfies both constraints simultaneously; the teacher and reference distributions are not so divergent that interpolation becomes infeasible.

### Mechanism 2: Teacher preference magnitude amplifies gradient signal beyond binary labels
The gradient of daDPO incorporates how strongly the teacher prefers the winning response, providing richer signal than binary win/loss labels alone. The gradient includes a term β₂δ_te where δ_te = log(π_te(y_t)/π_ref(y_t)) - log(π_te(y_s)/π_ref(y_s)). When the teacher strongly prefers y_t over y_s, this coefficient increases, amplifying the gradient push toward the winning response. This allows nuanced learning—responses the teacher is confident about receive stronger reinforcement. The core assumption is that the teacher's probability ratios meaningfully capture preference strength that correlates with actual response quality.

### Mechanism 3: Sentence-level formulation enables cross-architecture distillation
Using sentence-level rather than token-level KL divergence allows daDPO to work across models with different tokenizers, unlike traditional white-box KD. The KL terms compute π(y|x) over complete sequences rather than per-token distributions. This means only the joint probability of the full response matters, not the alignment of individual token probabilities. Since sentence probability can be computed via product of token probabilities regardless of vocabulary, different tokenizers are compatible. The core assumption is that sentence-level optimization provides sufficient signal for conversational ability transfer; no critical information is lost by ignoring token-level distributional nuances.

## Foundational Learning

- **Concept: KL Divergence as a Regularizer**
  - Why needed here: daDPO's core innovation is adding a second KL constraint. You must understand why KL divergence penalizes distributional drift and how it acts as a regularizer before grasping why dual KL terms create a tradeoff between stability and distillation quality.
  - Quick check question: If π_θ(y|x) = 0.9 but π_ref(y|x) = 0.1 for some response y, what is the KL penalty contribution? Would increasing β make training more conservative or more aggressive?

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: DPO's theoretical guarantees derive from this model—preferences are modeled as p(y₁≻y₂) = σ(r(x,y₁)-r(x,y₂)). daDPO modifies this to include teacher distribution, and understanding the original formulation is prerequisite to seeing what changes.
  - Quick check question: Under Bradley-Terry, if r(x,y₁) = 5 and r(x,y₂) = 2, what is the probability y₁ is preferred? How does this relate to the DPO loss formulation?

- **Concept: Reference Policy Role in DPO**
  - Why needed here: The paper's key finding is that removing the reference policy (rDPO) causes instability when student-teacher gap is large. Understanding why the reference matters—preventing excessive policy updates during optimization—is essential.
  - Quick check question: In standard DPO, what is π_ref typically set to? What would happen if π_ref = π_θ at every step?

## Architecture Onboarding

- **Component map**: [Prompt Dataset] → [Teacher Model π_te] → Teacher responses y_t → [dSFT Stage] → π_dSFT (becomes π_ref) → [daDPO Stage] → π_θ (optimized student)

- **Critical path**:
  1. Generate preference dataset: sample y_t ∼ π_te(x) and y_s ∼ π_st(x) for each prompt
  2. Run dSFT: train student on teacher responses to get π_ref
  3. Initialize π_θ = π_ref
  4. For each batch: compute log-probabilities under all three models, compute daDPO loss, update π_θ
  5. Key: teacher model only needs forward pass (gradient-free), keeping memory overhead manageable

- **Design tradeoffs**:
  - **β₁ vs β₂ ratio**: Higher β₂ (teacher KL weight) improves distillation but reduces stability. Paper shows β₂ up to 1.0 helps, but excessive values hurt. Start with β₁=0.1, β₂=0.1-0.5.
  - **Teacher size selection**: Larger teacher doesn't always mean better distillation due to distribution gap. Match teacher capability to student capacity.
  - **Sentence-level vs token-level**: Sentence-level enables cross-family distillation but may lose fine-grained signal. Choose based on whether tokenizer compatibility exists.

- **Failure signatures**:
  - Training instability with large teacher-student gap: If using rDPO (β₁=0) with heavily pruned models (50%), performance degrades vs dDPO. Fix: ensure β₁ > 0 to maintain reference constraint.
  - No improvement over dDPO: If β₂ is too small (<0.001), teacher distribution has negligible effect. Fix: increase β₂ incrementally.
  - Degraded non-conversational performance: GSM8K performance drops with both dDPO and daDPO (alignment tax). Monitor academic benchmarks during training.
  - Cross-family distillation failure with rDPO: rDPO fails for Qwen→Llama distillation. Always use full daDPO with β₁>0 for cross-family.

- **First 3 experiments**:
  1. Reproduce pruned model recovery: Take Vicuna-7B, prune 20% using LLM-Pruner, run dSFT→daDPO pipeline. Target: achieve <10% preference rate drop vs teacher.
  2. Ablate reference vs teacher constraint: Compare dDPO (β₂=0), rDPO (β₁=0), and daDPO (both>0) on 20% vs 50% pruned models. Verify that rDPO works at 20% but fails at 50%.
  3. Cross-family distillation test: Distill Qwen2.5-7B → Llama3.2-1B. Compare dDPO vs daDPO. Verify daDPO maintains advantage (target: MT-Bench improvement of ~0.2-0.3 points).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can daDPO effectively transfer complex reasoning capabilities (e.g., mathematical logic) or is it restricted to stylistic conversational abilities?
- **Basis in paper**: The authors list "Extended Experimentation on Reasoning Tasks" as future work, noting the current scope is limited to general chat ability and reasoning generalization is unknown.
- **Why unresolved**: Current evaluations focus on MT-Bench and AlpacaEval, which prioritize conversational flow and helpfulness over complex logical deduction or coding accuracy.
- **What evidence would resolve it**: Experiments applying daDPO to distill reasoning-specific models (e.g., DeepSeek-R1) onto smaller students using reasoning benchmarks like MATH or HumanEval.

### Open Question 2
- **Question**: How does the daDPO objective integrate with alternative preference optimization frameworks like KTO or f-DPO?
- **Basis in paper**: The authors suggest "Integration with Alternative Preference Learning Frameworks" as future research, specifically mentioning KTO and f-DPO.
- **Why unresolved**: The current theoretical derivation modifies the standard DPO Bradley-Terry objective; compatibility with non-Bradley-Terry models or different divergence constraints remains unanalyzed.
- **What evidence would resolve it**: Theoretical derivations of the daDPO loss function within the KTO or f-DPO formulations, followed by empirical comparisons.

### Open Question 3
- **Question**: Does utilizing multiple diverse teacher models improve daDPO performance compared to a single teacher?
- **Basis in paper**: The authors propose "Expansion to Diverse Teacher LLMs" to understand the impact of teacher diversity and ensemble effects.
- **Why unresolved**: Current experiments utilize a single teacher model; it is unclear how the loss function handles potentially conflicting distributions from multiple high-quality teachers.
- **What evidence would resolve it**: Implementation of a multi-teacher daDPO variant (e.g., averaging teacher log-probabilities) and evaluation against single-teacher baselines.

## Limitations

- The empirical claims are built on a constrained experimental framework focusing primarily on general chat ability, while acknowledging that mathematical and task-specific capabilities may suffer alignment tax
- The teacher-student distribution gap assumption is critical but not systematically characterized across different architectural families or capabilities
- The β hyperparameter sensitivity analysis shows reasonable stability but doesn't explore the full space of possible β₁/β₂ ratios or their interaction with model scale and pruning level

## Confidence

- **High Confidence**: The core daDPO formulation (dual KL-divergence objective) and its theoretical derivation from modified Bradley-Terry preference modeling
- **Medium Confidence**: Empirical improvements on MT-Bench and AlpacaEval benchmarks, though evaluation relies on GPT-4-based scoring with potential variance
- **Low Confidence**: Claims about daDPO's superiority for mathematical and task-specific capabilities, as GSM8K results actually show degradation

## Next Checks

1. **Dataset Robustness Test**: Evaluate daDPO performance across multiple preference datasets (not just ShareGPT) including synthetically generated preference pairs with known ground truth quality differences to validate whether improvements are dataset-specific or generalize to different preference distributions.

2. **Architectural Transfer Analysis**: Systematically test cross-family distillation across diverse model capabilities (code generation, mathematical reasoning, structured output) to identify which capabilities transfer well versus those that suffer alignment tax.

3. **Distribution Gap Characterization**: Quantify teacher-student distribution divergence (e.g., using KL divergence or other distributional metrics) across different pruning levels and model families, then correlate this with daDPO's performance improvements to validate whether the dual-constraint mechanism is most beneficial precisely when distribution gaps are large.