---
ver: rpa2
title: 'X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based
  Machine Learning'
arxiv_id: '2505.14273'
source_url: https://arxiv.org/abs/2505.14273
tags:
- x-kan
- rule
- xcsf
- function
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces X-KAN, a novel function approximation method
  that combines Kolmogorov-Arnold Networks (KANs) with evolutionary rule-based machine
  learning. X-KAN addresses the limitation of existing neural networks that struggle
  with locally complex or discontinuous functions by implementing multiple local KAN
  models through the XCSF framework.
---

# X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning

## Quick Facts
- **arXiv ID**: 2505.14273
- **Source URL**: https://arxiv.org/abs/2505.14273
- **Reference count**: 15
- **Key outcome**: X-KAN significantly outperforms conventional methods (XCSF, MLP, KAN) in function approximation accuracy while using an average of only 7.2 ± 2.3 rules.

## Executive Summary
X-KAN introduces a novel function approximation method that combines Kolmogorov-Arnold Networks (KANs) with evolutionary rule-based machine learning through the XCSF framework. The method addresses the limitation of existing neural networks that struggle with locally complex or discontinuous functions by implementing multiple local KAN models, each specialized for a specific region of the input space. Each rule in X-KAN defines a local region via hyperrectangular antecedents and uses a KAN model as the consequent, enabling adaptive partitioning of the input space. The approach was evaluated on eight function approximation problems including both artificial test functions and real-world datasets, achieving statistically significant improvements across all problems.

## Method Summary
X-KAN integrates KANs as local function approximators within the XCSF (eXtended Classifier System for Function approximation) framework. The method partitions the input space into hyperrectangular regions using evolutionary optimization of rule antecedents, with each region assigned a dedicated KAN model optimized via backpropagation. The system employs dual optimization: evolutionary algorithms adjust antecedent bounds to explore optimal region boundaries, while gradient descent optimizes KAN parameters within each region. Fitness evaluation incorporates both accuracy and generality to promote compact, generalizable rulesets. During training, the system forms match sets, covers uncovered regions, updates rule fitness, applies evolutionary operators, performs subsumption, and compacts the final ruleset for inference.

## Key Results
- X-KAN achieved statistically significant improvements in test MAE compared to XCSF, MLP, and KAN across all eight benchmark problems
- The method used an average of only 7.2 ± 2.3 rules while maintaining superior performance
- X-KAN particularly excelled at handling functions with locally complex or discontinuous structures that challenge conventional KAN models
- The inclusion of generality in fitness evaluation reduced rule proliferation compared to accuracy-only variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dividing input space into local regions with specialized KAN models improves approximation of functions with local complexity or discontinuities.
- Mechanism: X-KAN partitions the input space via hyperrectangular antecedents and assigns a dedicated KAN model to each region. Each local KAN learns only the function behavior within its region, reducing the burden on any single model.
- Core assumption: Complex or discontinuous functions can be more efficiently approximated by piecewise local models than by a single global model.
- Evidence anchors:
  - [abstract] "X-KAN addresses the limitation of existing neural networks that struggle with locally complex or discontinuous functions by implementing multiple local KAN models through the XCSF framework."
  - [Section 5.1] On a discontinuous function, "KAN fails to detect discontinuities, instead producing a smooth continuous function approximation" while "X-KAN successfully identifies discontinuities and performs piecewise function approximation using three rules."
  - [corpus] Free-RBF-KAN and BEKAN similarly address efficiency in KAN variants but do not implement adaptive input-space partitioning.
- Break condition: If the function is globally smooth and continuous, the overhead of partitioning may not yield meaningful gains over a single global KAN.

### Mechanism 2
- Claim: Dual optimization of rule antecedents (via evolutionary algorithm) and rule consequents (via backpropagation) enables simultaneous adaptation of region boundaries and local function approximators.
- Mechanism: The EA adjusts antecedent bounds to explore where to place region boundaries, while gradient descent optimizes KAN parameters within each region. This enables co-adaptation of partitioning and local modeling.
- Core assumption: Neither fixed partitions nor fixed model parameters alone are optimal; joint optimization improves both region placement and local fit.
- Evidence anchors:
  - [Section 3] "The rule antecedents are optimized through the EA, while the rule consequents (local KAN models) are optimized via backpropagation."
  - [Section 3.2] Offspring rules with modified antecedents trigger retraining of KAN consequents using Eq. (13)-(15).
  - [corpus] GA-KAN applies genetic algorithms to KAN structure optimization, but does not couple this with adaptive input-space partitioning.
- Break condition: If evolutionary search is underpowered or local KANs are undertrained, joint optimization may fail to converge.

### Mechanism 3
- Claim: Fitness evaluation based on both accuracy and generality promotes compact, generalizable rulesets.
- Mechanism: Rule fitness F^k is updated via Widrow-Hoff learning incorporating κ_k (accuracy) and num_k (generality/number of aggregated rules). This pressures the system toward fewer, more general rules that still meet error thresholds.
- Core assumption: Rules with high generality but acceptable accuracy generalize better than highly specialized rules.
- Evidence anchors:
  - [Section 3.2, Eq. (16)] Fitness update explicitly includes both accuracy and generality terms.
  - [Section 5.2, Table 2] X-KAN_κ (accuracy-only fitness) produces more rules and worse testing MAE on some problems compared to X-KAN with both accuracy and generality.
  - [corpus] No direct corpus evidence on fitness formulation in XCSF variants; this mechanism is specific to the XCSF lineage.
- Break condition: If generality pressure is too strong, rules may become overly broad and underfit; if too weak, rules may overfit and proliferate.

## Foundational Learning

- Concept: Kolmogorov-Arnold Networks (KANs)
  - Why needed here: X-KAN uses KANs as local function approximators; understanding spline-based learnable activations is essential.
  - Quick check question: Can you explain how KAN differs from MLP in where learnable parameters reside (edges vs. nodes)?

- Concept: XCSF (Learning Classifier Systems for Function Approximation)
  - Why needed here: X-KAN inherits the XCSF framework for rule evolution, matching, and fitness.
  - Quick check question: What does XCSF optimize in rule antecedents vs. consequents?

- Concept: B-spline basis functions
  - Why needed here: KAN activations are parameterized as combinations of SiLU and B-splines (Eq. 8).
  - Quick check question: How does increasing G (grid count) or K (degree) affect KAN expressivity and computational cost?

## Architecture Onboarding

- Component map:
  - Input data point → Match set M (rules whose antecedents contain input)
  - Covering mechanism → Generates new rule when M is empty
  - Fitness update (Eq. 16) → Incorporates accuracy κ_k and generality num_k
  - Evolutionary algorithm → Tournament selection, crossover, mutation on antecedents
  - Subsumption → Aggregates offspring into more general accurate parents
  - Rule compaction → Produces compacted ruleset P_C via single-winner selection
  - Inference (Eq. 18) → Select single winner rule by max fitness in M_te

- Critical path:
  1. Sample data point → form match set M
  2. If M empty → covering generates new rule
  3. Update fitness for all rules in M (Eq. 16)
  4. Periodically apply EA to M → generate offspring
  5. Subsume offspring if conditions met
  6. After training, apply rule compaction to obtain P_C
  7. Inference: select single winner rule by max fitness in M_te

- Design tradeoffs:
  - More rules → better local fit but higher runtime and potential overfitting
  - Higher P# → more "Don't Care" dimensions → more general rules but coarser partitioning
  - Lower ε_0 → stricter accuracy threshold → potentially more rules
  - Using KAN vs MLP as consequent: KAN more expressive per parameter, but slower due to B-spline computation

- Failure signatures:
  - Too many rules with low generality: generality pressure insufficient (check fitness formulation)
  - Poor test performance despite low training error: overfitting from accuracy-only fitness
  - Runtime blowup: too many rules or excessive KAN training epochs per rule
  - Smooth predictions on discontinuous functions: partitioning failed to detect boundaries (increase EA exploration)

- First 3 experiments:
  1. Replicate X-KAN vs KAN on f2 (Sine-in-Sine) to verify partitioning benefit on high-curvature functions.
  2. Ablate generality in fitness (use X-KAN_κ) to confirm compactness and generalization effects from Table 2.
  3. Test on the discontinuous function from Section 5.1 to validate piecewise approximation capability.

## Open Questions the Paper Calls Out
None

## Limitations
- The experiments were limited to 1D and 2D problems without systematic testing on higher dimensions
- Specific training parameters for local KAN models (epochs, optimizer settings) were not fully specified
- The scalability of rule count and runtime to high-dimensional problems remains uncharacterized
- The stability of rule evolution over time and sensitivity to hyperparameters was not thoroughly examined

## Confidence
- **High**: Empirical performance improvements with statistically significant MAE reductions across all eight tested problems
- **Medium**: Generality mechanism effectiveness supported by comparison between X-KAN and X-KAN_κ
- **Low**: Scalability claims to high-dimensional problems not validated beyond 2D experiments

## Next Checks
1. **Scale-up test**: Evaluate X-KAN on 10D+ synthetic functions (e.g., rotated Styblinski-Tang) to assess rule count and runtime scaling.
2. **Fitness ablation**: Systematically vary the weight of accuracy vs. generality in fitness (Eq. 16) to quantify their individual contributions to rule compaction.
3. **Rule stability analysis**: Track rule evolution over time in XCSF to measure convergence, rule birth/death rates, and sensitivity to initial conditions.