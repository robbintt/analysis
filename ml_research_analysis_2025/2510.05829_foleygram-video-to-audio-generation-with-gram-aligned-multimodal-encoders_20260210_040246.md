---
ver: rpa2
title: 'FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders'
arxiv_id: '2510.05829'
source_url: https://arxiv.org/abs/2510.05829
tags:
- audio
- video
- semantic
- modalities
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FoleyGRAM, a novel video-to-audio generation
  model that addresses the challenge of creating semantically rich and temporally
  aligned audio from video inputs. The core innovation lies in using Gramian Representation
  Alignment Measure (GRAM) to align embeddings across video, text, and audio modalities,
  enabling precise semantic control over the audio generation process.
---

# FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders

## Quick Facts
- arXiv ID: 2510.05829
- Source URL: https://arxiv.org/abs/2510.05829
- Authors: Riccardo Fosco Gramaccioni; Christian Marinoni; Eleonora Grassucci; Giordano Cicchetti; Aurelio Uncini; Danilo Comminiello
- Reference count: 40
- Primary result: State-of-the-art V2A generation on Greatest Hits with FAD-C 0.072, FAD-LC 0.8912, CLAP-score 0.7083, FAVD 0.8912

## Executive Summary
FoleyGRAM introduces a novel video-to-audio generation model that achieves state-of-the-art performance through GRAM (Gramian Representation Alignment Measure) alignment of multimodal embeddings. The approach simultaneously aligns video, text, and audio embeddings in a unified latent space using parallelotope volume minimization, rather than pairwise alignment. Combined with temporal envelope conditioning via a ControlNet, FoleyGRAM generates semantically rich and temporally synchronized audio from video inputs.

The model demonstrates significant improvements over baseline methods on the Greatest Hits dataset, achieving best scores across all objective metrics including Fr\'echet Audio Distance (FAD), CLAP-score, and Fr\'echet Audio-Visual Distance (FAVD). The ablation studies confirm the benefits of multimodal semantic alignment when using all three modalities simultaneously for conditioning.

## Method Summary
FoleyGRAM uses three pretrained encoders (EVA-CLIP-ViT-G for video, BERT-B for text, BEATs for audio) fine-tuned on Greatest Hits with GRAM loss. These GRAM-aligned embeddings are projected to the diffusion conditioning dimension and injected via cross-attention. Temporal synchronization is achieved through a ControlNet that processes RMS envelopes extracted from ground truth audio. The diffusion backbone (Stable Audio Open) remains frozen while only conditioning adapters are trained. The dual conditioning mechanism ensures semantic fidelity through GRAM and temporal synchronization through envelopes.

## Key Results
- Achieves state-of-the-art FAD-C of 0.072 on Greatest Hits dataset
- Attains best CLAP-score of 0.7083, demonstrating strong semantic alignment
- Shows superior performance in ablation studies when using all three modalities (video, audio, text) simultaneously
- Maintains FAVD of 0.8912, indicating good audiovisual coherence

## Why This Works (Mechanism)

### Mechanism 1: Joint Geometric Alignment via Parallelotope Volume Minimization
- **Claim:** Aligning video, text, and audio embeddings simultaneously via the volume of a high-dimensional parallelotope (GRAM) yields more semantically consistent audio generation than pairwise cosine similarity alignment.
- **Mechanism:** Standard contrastive losses align pairs (e.g., audio-text) often relative to a single anchor modality. FoleyGRAM utilizes the Gramian Representation Alignment Measure (GRAM), which computes the determinant of the Gram matrix formed by video, audio, and text embeddings. This determinant represents the volume of the parallelotope defined by these vectors. By minimizing this volume during training, the model forces all three modalities into a tighter unified latent space, reducing the risk of "anchor bias" where non-anchor modalities remain misaligned.
- **Core assumption:** The semantic content of a video, its textual description, and its audio are sufficiently correlated such that forcing their embeddings to occupy a minimal geometric volume improves semantic fidelity without collapsing distinct but related concepts.
- **Evidence anchors:**
  - [Section III.A]: "The GRAM loss function is based on the intuition that modalities embedding vectors... act as the edges of a high-dimensional parallelotope... the volume... provides direct information about the alignment."
  - [Abstract]: "Unlike previous approaches that rely on pairwise alignment... GRAM ensures true geometric alignment of all three modalities simultaneously."
  - [Corpus]: Related work like *ImageBind* and *LanguageBind* are cited in the paper as relying on anchor-based alignment which lacks these geometric guarantees.
- **Break condition:** If the audio contains semantic information not present in the video or text (or vice versa), minimizing the joint volume may force the model to ignore unique features, potentially degrading performance on complex, multi-layered scenes.

### Mechanism 2: Dual Conditioning for Semantics and Temporal Structure
- **Claim:** Decoupling semantic control (via embeddings) from temporal synchronization (via envelopes) allows the diffusion backbone to maintain high-fidelity audio generation while adhering to specific video timings.
- **Mechanism:** The model uses a "forked" conditioning strategy. Semantic embeddings (from GRAM) are injected via cross-attention to define *what* sound plays. Simultaneously, a temporal envelope (RMS energy extracted from audio) is encoded and injected via a ControlNet architecture to dictate *when* and *how loudly* it plays. This separates the high-level semantic latent space from the low-level temporal modulation.
- **Core assumption:** The RMS envelope is a sufficient proxy for the temporal dynamics of the video (assuming visual motion correlates with audio energy), and the pre-trained Stable Audio Open backbone can accept this external temporal control without disrupting its internal prior.
- **Evidence anchors:**
  - [Section III.B]: "This dual conditioning mechanism ensures semantic fidelity through GRAM and also temporal synchronization... by means of the envelope."
  - [Section III.B.2]: "The envelope serves as a coarse temporal guide... processed through a ControlNet-inspired architecture."
  - [Corpus]: The corpus paper "Hear What Matters!" suggests controlling specific audio events is crucial, supporting the need for precise conditioning mechanisms.
- **Break condition:** If the video involves "silent" actions (visual motion without sound) or asynchronous audio (e.g., reverb, delays), the direct RMS envelope coupling may cause the model to hallucinate noise or misalign the onset of sounds.

### Mechanism 3: Frozen Prior Adaptation
- **Claim:** Freezing the core diffusion model weights and training only the conditioning adapters (ControlNet + Linear Projections) preserves the audio generation quality of the foundation model while adapting it to multimodal inputs.
- **Mechanism:** The authors use Stable Audio Open as a "frozen brain." They do not fine-tune the main U-Net weights. Instead, they train a separate ControlNet (for temporal features) and simple linear layers (to project GRAM embeddings into the diffusion model's conditioning dimension). This prevents "catastrophic forgetting" of high-fidelity audio textures that the base model learned from large datasets.
- **Core assumption:** The pre-trained latent space of Stable Audio Open is sufficiently expressive to map the new GRAM-aligned embeddings to high-quality audio without needing to adjust the underlying noise estimation network.
- **Evidence anchors:**
  - [Section III.B.3]: "We freeze the pre-trained weights of the diffusion model and only train the ControlNet layers... and the linear projections."
  - [Section V.B]: "...integration of the multimodal-aligned encoder GRAM for conditioning the state-of-the-art audio generation model, Stable Audio."
- **Break condition:** If the GRAM embedding space has a drastically different geometric structure than the text-embedding space Stable Audio was originally trained on, the linear projection may fail to map semantics effectively, resulting in high-fidelity but irrelevant audio.

## Foundational Learning

- **Concept: InfoNCE & Contrastive Learning**
  - **Why needed here:** The GRAM loss modifies standard contrastive learning. You must understand how standard models (like CLIP) align positive pairs and negative pairs to grasp why FoleyGRAM introduces a "volume" term to align triplets (Video, Audio, Text) simultaneously.
  - **Quick check question:** Can you explain why minimizing pairwise cosine similarity between A-V and A-T does *not* guarantee that V and T are aligned with each other?

- **Concept: Latent Diffusion Models (LDMs) & Cross-Attention**
  - **Why needed here:** The architecture relies on injecting external signals (embeddings) into a denoising network. Understanding how cross-attention layers mix the conditioning signal into the generation process is vital for debugging why the model might be ignoring semantic prompts.
  - **Quick check question:** In a Latent Diffusion Model, does the cross-attention mechanism modify the weights of the U-Net, or does it modify the feature maps during the forward pass?

- **Concept: Gram Matrix & Determinants (Linear Algebra)**
  - **Why needed here:** The core novelty is using the determinant of the Gram matrix to measure alignment.
  - **Quick check question:** If three vectors are perfectly aligned (collinear) in 3D space, what is the volume of the parallelotope they form? (Answer: Zero).

## Architecture Onboarding

- **Component map:**
  1. Encoders (Frozen/GRAM-trained): EVAClip-ViT-G (Video), BERT-B (Text), BEATs (Audio)
  2. Projectors (Trainable): Linear layers mapping embeddings to the diffusion conditioning dimension
  3. Temporal Processor (Trainable): Librosa RMS extraction -> Pre-trained VAE -> ControlNet (mirrors Diffusion U-Net structure)
  4. Generator (Frozen): Stable Audio Open (VAE + DiT/U-Net)

- **Critical path:**
  Input Video -> GRAM Video Encoder -> Projection -> **Cross-Attention** -> Generated Latent
  Input Audio (Ground Truth) -> RMS Envelope -> ControlNet -> **Zero-Conv Addition** -> Generated Latent

- **Design tradeoffs:**
  - **Envelope vs. Onset tracks:** The paper uses RMS envelopes for temporal control. While effective for continuous sounds (rubbing), it may be less precise for percussive events than the onset tracks used in models like SyncFusion (baselines).
  - **Triplet vs. Pairwise:** Training with 3 modalities (AVT) improves metrics (Table II) but increases data requirements and pipeline complexity compared to simple Video-Audio models.

- **Failure signatures:**
  - **Insertion Hallucination:** The corpus paper "Detecting and Mitigating Insertion Hallucination" highlights a risk in V2A models where audio is generated for silent objects. If FoleyGRAM's GRAM embeddings are too strong, they might force sound generation even when the visual envelope is flat.
  - **Temporal Smoothing:** If the ControlNet weight is too low, the audio will sound generic and ignore the specific timing of the video hits.

- **First 3 experiments:**
  1. **Ablation on Conditioning:** Run inference using only Text (T) embeddings vs. only Video (V) embeddings on the same clip to verify that the linear projections correctly map disparate modalities to the same audio concept.
  2. **Envelope Perturbation:** Input a "shifted" or randomized envelope to the ControlNet to verify that the temporal alignment degrades predictably (proving the ControlNet is actually doing the work, not the semantic encoder).
  3. **Volume Visualization:** Extract embeddings for a batch of data and compute the "volume" (det G) for matched vs. mismatched triplets (e.g., Video A + Text B + Audio C) to confirm the GRAM loss has successfully separated unaligned data.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the temporal envelope be accurately predicted directly from video features rather than extracted from ground truth audio?
- **Basis in paper:** [explicit] The authors state: "the temporal alignment is provided using directly the envelope extracted with librosa library from the ground truth audio, such as the main scope of this work is focusing on the semantic alignment and not introducing novel methods for temporal synchrony."
- **Why unresolved:** The current approach requires access to ground truth audio for envelope extraction, which is unavailable in true video-to-audio inference scenarios. This limits practical deployment where only video input exists.
- **What evidence would resolve it:** A study showing comparable FAD, CLAP, and FAVD scores when using video-predicted envelopes versus ground truth envelopes on the same test set.

### Open Question 2
- **Question:** Does GRAM-based alignment provide measurable benefits over anchor-based multimodal alignment methods (e.g., ImageBind, LanguageBind) in video-to-audio generation?
- **Basis in paper:** [inferred] The paper critiques anchor-based methods for lacking "geometrical guarantees that the other modalities are aligned with each other" but does not include ImageBind or LanguageBind in the baseline comparisons (Table I).
- **Why unresolved:** The theoretical advantage of GRAM's parallelotope volume-based alignment over cosine similarity-based anchor methods remains untested against these specific models in the V2A context.
- **What evidence would resolve it:** Direct comparison experiments on Greatest Hits including ImageBind and LanguageBind as conditioning encoders, reporting FAD-C, FAD-LC, CLAP-score, and FAVD metrics.

### Open Question 3
- **Question:** How do human listeners rate FoleyGRAM outputs compared to baseline methods on semantic appropriateness and temporal synchronization?
- **Basis in paper:** [inferred] All reported metrics (FAD, CLAP-score, FAVD) are objective measures. The paper notes that FAD "aligns with human perception" variably depending on the encoder, suggesting potential disconnect between metrics and perceptual quality.
- **Why unresolved:** Objective metrics may not fully capture perceived semantic coherence or temporal naturalness that matters for practical sound design applications in cinema and video games.
- **What evidence would resolve it:** A subjective listening study with human participants rating generated audio samples on semantic match, temporal alignment, and overall quality using Mean Opinion Score (MOS) or pairwise comparisons.

### Open Question 4
- **Question:** Does FoleyGRAM generalize to video content beyond the drumstick-strike scenarios in Greatest Hits?
- **Basis in paper:** [inferred] The paper acknowledges Greatest Hits contains only "videos of people using a drumstick to strike or rub different surfaces" and that "real-world video datasets often lack both the audiovisual alignment and the quality required."
- **Why unresolved:** The model's performance on diverse real-world videos with complex sound sources (speech, music, ambient environments, multiple simultaneous actions) is unknown.
- **What evidence would resolve it:** Evaluation on additional V2A benchmarks such as AudioSet or VGGSound, reporting the same objective metrics to enable comparison with Greatest Hits performance.

## Limitations
- Evaluation relies entirely on objective metrics without user studies to validate perceived audio quality or semantic alignment
- Model's performance on complex, multi-layered audio scenes with multiple concurrent sound sources remains unclear
- GRAM embedding space compatibility with Stable Audio's original training distribution is assumed but not empirically validated

## Confidence

- **High Confidence**: The technical implementation details of the GRAM loss function and its mathematical formulation are well-specified and verifiable. The dual conditioning architecture combining semantic embeddings with temporal envelopes is clearly described.
- **Medium Confidence**: The claim that GRAM alignment produces superior results compared to pairwise methods is supported by quantitative metrics but lacks qualitative or perceptual validation. The ablation study showing improved performance with three modalities is convincing but limited in scope.
- **Low Confidence**: The assertion that freezing the diffusion backbone preserves audio quality while adapting to multimodal inputs assumes the GRAM embedding space is compatible with the original Stable Audio training distribution. This compatibility is not empirically validated.

## Next Checks

1. **Perceptual Study**: Conduct a user study comparing FoleyGRAM outputs against baseline models using paired comparison tasks focusing on semantic accuracy, temporal alignment, and overall audio quality.

2. **Cross-Modal Compatibility Test**: Evaluate model performance when GRAM embeddings are projected through random vs. learned linear projections to quantify how much of the performance gain depends on the specific embedding alignment learned during training.

3. **Complex Scene Analysis**: Test the model on videos containing multiple concurrent sound sources (e.g., dialogue + background noise + Foley effects) to assess whether the GRAM alignment mechanism can handle audio scenes more complex than the training data distribution.