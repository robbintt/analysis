---
ver: rpa2
title: Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement
arxiv_id: '2509.16221'
source_url: https://arxiv.org/abs/2509.16221
tags:
- ensemble
- methods
- dataset
- base
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates Ensemble Learning techniques for improving
  handwritten OCR accuracy. Three OCR models (TrOCR, AttentionHTR, SimpleHTR) were
  combined using ensemble methods at three design levels: dataset (CompleteData, Bagging,
  KFOLD, Partitioning), base learner (homogeneous/heterogeneous models), and output
  (WordVote, CharVote, WeightedWordVote, WeightedCharVote, MaxProb, AvgProb).'
---

# Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement

## Quick Facts
- arXiv ID: 2509.16221
- Source URL: https://arxiv.org/abs/2509.16221
- Reference count: 0
- Three OCR models (TrOCR, AttentionHTR, SimpleHTR) combined using ensemble methods significantly improved accuracy compared to single models

## Executive Summary
This work evaluates Ensemble Learning techniques for improving handwritten OCR accuracy. Three OCR models were combined using ensemble methods at three design levels: dataset (CompleteData, Bagging, KFOLD, Partitioning), base learner (homogeneous/heterogeneous models), and output (WordVote, CharVote, WeightedWordVote, WeightedCharVote, MaxProb, AvgProb). Experiments on two datasets (Duke: 6,287 images; IAM: 115,320 images) showed that ensemble learning significantly improved accuracy compared to single models. The best methods were CompleteData and KFOLD at the dataset level, TrOCR for Duke and AttentionHTR for IAM at the base learner level, and WordVote and MaxProb at the output level. Ensemble learning provided consistent accuracy gains regardless of dataset size.

## Method Summary
The study combined three OCR models (TrOCR, AttentionHTR, SimpleHTR) using ensemble methods at three levels: dataset (CompleteData, Bagging, KFOLD, Partitioning), base learner (homogeneous ensembles with k=5 same architecture or heterogeneous with k=9 mixed), and output (WordVote, CharVote, WeightedWordVote, WeightedCharVote, MaxProb, AvgProb). Experiments used Duke dataset (6,287 medical record images) and IAM dataset (115,320 English word images) with 70/15/15 train/val/test splits. TrOCR and AttentionHTR used pretrained weights while SimpleHTR was trained from scratch with specified hyperparameters.

## Key Results
- Ensemble learning significantly improved accuracy compared to single models on both datasets
- Best dataset-level methods were CompleteData and KFOLD
- TrOCR performed best on Duke dataset while AttentionHTR excelled on IAM
- WordVote and MaxProb were the most effective output-level methods
- Ensemble learning provided consistent accuracy gains regardless of dataset size

## Why This Works (Mechanism)

### Mechanism 1
- Aggregating predictions from multiple diverse base learners reduces single-model failure modes through statistical averaging and voting
- Core assumption: Base learners must exhibit diversity in their error patterns while maintaining individual accuracy above random
- Evidence: Ensemble methods significantly improved accuracy; diversity is essential for ensemble success
- Break condition: If base learners are too similar or all perform poorly, voting provides no benefit

### Mechanism 2
- Selecting prediction with highest model confidence (MaxProb) can outperform voting when some base learners are systematically more reliable
- Core assumption: Confidence scores correlate with actual prediction correctness, and different models are confident on different input types
- Evidence: MaxProb performed significantly better for SimpleHTR and generally best for homogeneous ensembles on IAM dataset
- Break condition: If confidence scores are poorly calibrated, MaxProb will select errors

### Mechanism 3
- Creating diversity through training data variation (KFOLD) yields more consistent gains than varying model architectures alone
- Core assumption: Dataset is large enough that each fold provides sufficient training signal
- Evidence: KFOLD had highest average accuracy, followed by CompleteData
- Break condition: On very small datasets, KFOLD reduces training data below viable thresholds

## Foundational Learning

- Concept: **OCR Confidence Score Calibration**
  - Why needed here: MaxProb and AvgProb methods rely on decoder confidence scores
  - Quick check question: Can you explain why multiplying character-level probabilities to get word confidence tends to produce very small numbers?

- Concept: **Diversity-Accuracy Tradeoff in Ensembles**
  - Why needed here: Paper explicitly states both diversity and accuracy principles must be satisfied
  - Quick check question: If you have three models with 70%, 65%, and 55% accuracy, would you expect their ensemble to outperform the 70% model?

- Concept: **Sequence Labeling vs. Classification Aggregation**
  - Why needed here: OCR outputs are sequences, constraining aggregation methods
  - Quick check question: Why can't you simply average predictions of two sequence models like regression predictions?

## Architecture Onboarding

- Component map:
  - Dataset Level: CompleteData -> Bagging -> KFOLD -> Partitioning
  - Base Learner Level: Architecture selection (TrOCR, AttentionHTR, SimpleHTR) -> Count (k=5 homogeneous, k=9 heterogeneous) -> Initialization (pretrained vs scratch)
  - Output Level: WordVote/CharVote -> WeightedWordVote/WeightedCharVote -> MaxProb/AvgProb

- Critical path:
  1. Initialize base learners with appropriate pretrained weights
  2. Apply dataset-level method (recommend CompleteData or KFOLD)
  3. Train all base learners to convergence
  4. For Weighted methods, evaluate base learners on validation set
  5. Apply output-level aggregation and evaluate on held-out test set

- Design tradeoffs:
  - CompleteData vs. KFOLD: CompleteData simpler but may produce correlated learners; KFOLD adds diversity but requires more training runs
  - Homogeneous vs. Heterogeneous: Heterogeneous adds architectural diversity but complicates deployment
  - WordVote vs. MaxProb: WordVote robust to miscalibrated confidence; MaxProb leverages well-calibrated models

- Failure signatures:
  - Partitioning + small data → near-zero accuracy
  - CharVote underperforming WordVote → character-level errors may compound
  - Ensemble worse than best base learner → likely low diversity or poorly calibrated confidence scores
  - Heterogeneous ensemble matching homogeneous → diversity gain minimal

- First 3 experiments:
  1. Baseline single-model benchmark: Train TrOCR, AttentionHTR, and SimpleHTR independently
  2. CompleteData + WordVote homogeneous ensemble: Train k=5 instances of best architecture
  3. KFOLD + MaxProb: Train k=5 instances using k-fold data splits

## Open Questions the Paper Calls Out

- Question: What is the optimal number of base learners ($k$) to balance accuracy improvements against computational costs?
- Question: Can advanced ensemble techniques like Stacking, Boosting, or Snapshot Ensembling outperform standard voting methods?
- Question: Do character-level voting methods provide significant accuracy gains when applied to longer text sequences compared to the short labels used in this study?

## Limitations
- Partitioning method's catastrophic performance on small datasets shows ensemble benefits are not guaranteed
- Analysis doesn't investigate whether specific architectures chosen are optimal base learners
- Doesn't explore when ensembles fail or identify minimum viable dataset sizes

## Confidence
- High confidence: Ensemble learning improves accuracy compared to single models
- Medium confidence: CompleteData and KFOLD are best dataset-level methods
- Medium confidence: WordVote and MaxProb are optimal output-level methods
- Low confidence: Heterogeneous ensembles provide significant benefits

## Next Checks
1. Small dataset stress test: Replicate results with progressively smaller training subsets to identify minimum viable data size
2. Confidence calibration analysis: Systematically evaluate whether MaxProb's performance correlates with well-calibrated confidence scores
3. Architectural diversity quantification: Measure actual diversity in error patterns between base learners to determine if improvements stem from true diversity