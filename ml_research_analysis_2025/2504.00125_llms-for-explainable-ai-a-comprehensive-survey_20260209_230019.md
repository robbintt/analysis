---
ver: rpa2
title: 'LLMs for Explainable AI: A Comprehensive Survey'
arxiv_id: '2504.00125'
source_url: https://arxiv.org/abs/2504.00125
tags:
- arxiv
- explanations
- llms
- such
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey examines how large language models (LLMs) can enhance
  Explainable AI (XAI) by transforming complex machine learning outputs into natural
  language narratives. The paper categorizes XAI techniques into post-hoc explanations,
  intrinsic interpretability, and human-centered explanations, detailing their mechanisms
  and evaluation methods.
---

# LLMs for Explainable AI: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2504.00125
- Source URL: https://arxiv.org/abs/2504.00125
- Reference count: 40
- Primary result: Survey examining how LLMs enhance XAI by transforming ML outputs into natural language narratives, identifying key challenges and applications

## Executive Summary
This survey comprehensively examines how large language models (LLMs) can enhance Explainable AI (XAI) by converting complex machine learning outputs into natural language explanations. The paper systematically categorizes XAI techniques into post-hoc explanations, intrinsic interpretability, and human-centered explanations, providing detailed analysis of their mechanisms and evaluation methods. The survey also identifies benchmark datasets and explores real-world applications across healthcare, finance, and other domains, while highlighting critical challenges including data sensitivity, societal diversity, and model complexity.

## Method Summary
The survey employs a structured literature review methodology, examining existing XAI techniques through the lens of LLM capabilities. The authors categorize approaches based on their fundamental mechanisms, analyze evaluation frameworks, and synthesize findings across multiple domains. The methodology includes systematic analysis of benchmark datasets, identification of application areas, and critical assessment of current limitations and future directions in the field.

## Key Results
- LLMs can effectively transform complex ML outputs into natural language narratives across multiple XAI categories
- Critical challenges include data sensitivity, societal diversity, and managing model complexity
- Applications span healthcare, finance, and other domains requiring interpretable AI systems
- Feature importance analysis across LLMs using saliency maps reveals important patterns for explanation quality

## Why This Works (Mechanism)
LLMs excel at natural language generation and understanding, making them uniquely suited to bridge the gap between complex model outputs and human comprehension. Their ability to synthesize information, contextualize explanations, and adapt to different user needs enables more effective communication of AI decision-making processes. The integration of visual-textual information and automated feedback mechanisms further enhances the quality and relevance of generated explanations.

## Foundational Learning
- XAI Categories (Post-hoc, Intrinsic, Human-centered) - why needed: Provides framework for understanding different explanation approaches; quick check: Can categorize any XAI method correctly
- Evaluation Metrics - why needed: Ensures explanations are both technically sound and user-friendly; quick check: Can identify appropriate metrics for different XAI scenarios
- Benchmark Datasets - why needed: Enables standardized comparison of XAI techniques; quick check: Can access and understand dataset characteristics
- Saliency Maps - why needed: Visualizes feature importance for model interpretation; quick check: Can interpret and explain saliency map outputs

## Architecture Onboarding
Component map: Input Data -> ML Model -> LLM Processing -> Natural Language Output -> User Interface
Critical path: Data preprocessing → Model inference → Explanation generation → Quality evaluation
Design tradeoffs: Accuracy vs. interpretability, computational cost vs. explanation quality, technical precision vs. user comprehension
Failure signatures: Overly simplified explanations, misleading interpretations, computational bottlenecks, context misalignment
First experiments: 1) Generate explanations for simple classification tasks; 2) Compare LLM vs human-generated explanations; 3) Test explanation effectiveness with target user groups

## Open Questions the Paper Calls Out
- How to effectively integrate automated feedback mechanisms into LLM-based XAI systems
- What are the optimal methods for combining visual and textual information in explanations
- How to advance cross-disciplinary research to improve LLM-driven explainability

## Limitations
- Primary focus on English-language literature may miss important non-English research and cultural contexts
- Limited empirical validation across diverse real-world scenarios for XAI evaluation
- Potential for LLMs to generate misleading or overly simplified explanations that could harm user understanding

## Confidence
High: Categorization of XAI techniques (post-hoc, intrinsic, human-centered)
Medium: Discussion of evaluation metrics and benchmark datasets
Low: Real-world application examples without detailed case studies or quantitative performance metrics

## Next Checks
1. Conduct a multilingual literature review to identify and incorporate non-English research on LLM-driven XAI
2. Perform empirical studies comparing LLM-generated explanations with human-annotated explanations to assess accuracy and user comprehension
3. Develop and test a standardized evaluation framework that measures both the technical quality and user-centric effectiveness of LLM-based XAI systems