---
ver: rpa2
title: Mitigating the Participation Bias by Balancing Extreme Ratings
arxiv_id: '2502.03737'
source_url: https://arxiv.org/abs/2502.03737
tags:
- ratings
- rating
- aggregator
- participation
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of rating aggregation under participation
  bias, where some raters do not report their ratings with probabilities depending
  on the rating values. The authors propose two novel aggregators: the Balanced Extremes
  Aggregator (BEA) for known sample sizes, which estimates unobserved ratings using
  a convex combination of extreme ratings, and the Polarizing-Averaging Aggregator
  (PAA) for unknown sample sizes, which averages two polarized histograms.'
---

# Mitigating the Participation Bias by Balancing Extreme Ratings

## Quick Facts
- arXiv ID: 2502.03737
- Source URL: https://arxiv.org/abs/2502.03737
- Reference count: 40
- Primary result: BEA nearly matches theoretical lower bounds; PAA achieves near-optimal performance as sample size grows

## Executive Summary
This paper addresses the problem of rating aggregation under participation bias, where some raters do not report their ratings with probabilities depending on the rating values. The authors propose two novel aggregators: the Balanced Extremes Aggregator (BEA) for known sample sizes, which estimates unobserved ratings using a convex combination of extreme ratings, and the Polarizing-Averaging Aggregator (PAA) for unknown sample sizes, which averages two polarized histograms. Numerical results show that BEA nearly matches theoretical lower bounds across a wide range of participation probabilities, while PAA achieves near-optimal performance as sample size grows. Experiments on real-world hotel rating data demonstrate that PAA outperforms simple averaging and the spectral method in mitigating participation bias.

## Method Summary
The paper proposes two methods to mitigate participation bias in rating systems. The Balanced Extremes Aggregator (BEA) is designed for known sample sizes and estimates unobserved ratings as a convex combination of the most extreme observed values, with weights determined by the difference between counts of lowest and highest ratings. The Polarizing-Averaging Aggregator (PAA) handles unknown sample sizes by constructing two hypothetical scenarios (maximizing and minimizing means) through threshold-based histogram manipulation, then averaging the resulting estimates. Both methods frame the problem as minimizing worst-case regret relative to an ideal aggregator with complete data, using minimax optimization against an adversary that controls participation probabilities.

## Key Results
- BEA achieves regret within 20% of theoretical lower bounds across q∈[0.3,0.8] and n∈{10,20}
- PAA converges to optimal performance as sample size increases, outperforming simple averaging in real hotel rating data
- BEA degrades at high q (>0.8) while PAA maintains robust performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BEA reduces estimation error by imputing missing ratings as a convex combination of the most extreme observed values.
- **Mechanism:** BEA assumes that if extreme ratings (1 or m) are missing, they likely would have fallen into the opposite extreme or followed a specific distribution. It calculates a weight α based on the discrepancy between the counts of the lowest (n₁) and highest (nₘ) ratings. It then estimates the mean of unobserved ratings as μ̂ᵤ = α × 1 + (1 - α) × m, blending this with the observed mean.
- **Core assumption:** The sample size n is known, and raters are homogeneous in their participation probabilities.
- **Evidence anchors:** [abstract] "estimates unobserved ratings using a convex combination of extreme ratings"; [section 3] Definition 3.1 defines μ̂ᵤ as a function of counts n₁ - nₘ and parameter q.
- **Break condition:** The mechanism fails if the sample size n is unknown or if the participation probability q is not a valid lower bound.

### Mechanism 2
- **Claim:** PAA achieves near-optimal performance by averaging the estimated means of two adversarially constructed histograms.
- **Mechanism:** PAA addresses unknown sample sizes by generating two hypothetical scenarios: one where low ratings are suppressed (maximizing the mean) and one where high ratings are suppressed (minimizing the mean). It identifies thresholds k₁ and k₂ to effectively "cut off" portions of the histogram and averages the resulting empirical means to center the estimate.
- **Core assumption:** The sample size n is sufficiently large (asymptotic regime) to justify the midpoint strategy, and ratings are independent.
- **Evidence anchors:** [abstract] "averaging two polarized histograms... becomes optimal as the sample size grows to infinity"; [section 4.1.1] Theorem 4.2 proves that in the asymptotic case, the optimal aggregator is the midpoint of the lower and upper bounds of the expectation.
- **Break condition:** Performance degrades in finite sample scenarios where the empirical distribution is a poor approximation of the true distribution, violating the "large n" assumption.

### Mechanism 3
- **Claim:** Framing the problem as a zero-sum game against "Nature" (an adversary) allows the aggregator to minimize worst-case regret rather than absolute error.
- **Mechanism:** Instead of trying to guess the true distribution p, the methods optimize for the worst-case scenario where an adversary chooses the participation probabilities g to maximize error. By balancing the extremes (BEA) or averaging bounds (PAA), the aggregator positions itself at a point that minimizes the maximum possible deviation from the true mean.
- **Core assumption:** There exists a known lower bound q for the participation probability, and the "ideal aggregator" (with full data) is the correct baseline for regret.
- **Evidence anchors:** [abstract] "minimize the expected squared loss... in the worst-case scenario"; [section 1.1] The objective function is defined as minimizing regret relative to the ideal aggregator.
- **Break condition:** If the adversary (bias mechanism) does not conform to the bounded [q, 1] probability structure assumed, the theoretical guarantees may not hold.

## Foundational Learning

- **Concept: Regret Minimization**
  - **Why needed here:** The paper optimizes for "regret" (performance relative to an omniscient agent) rather than absolute error, which is standard in robust statistics.
  - **Quick check question:** Can you explain why minimizing worst-case regret is preferable to minimizing absolute error when the underlying data distribution is unknown?

- **Concept: Selection / Participation Bias**
  - **Why needed here:** The core problem is that ratings are "Missing Not At Random" (MNAR)—people with neutral experiences participate less.
  - **Quick check question:** How does the J-shaped distribution of reviews (common in online ratings) physically manifest the participation bias described in the paper?

- **Concept: Asymptotic Analysis**
  - **Why needed here:** Theoretical guarantees for PAA rely on n → ∞, while BEA is designed for finite n.
  - **Quick check question:** Why does the PAA mechanism become "optimal" only as the sample size grows, and what risks arise when applying it to small datasets?

## Architecture Onboarding

- **Component map:** Observed Histograms (n₁, ..., nₘ) + Optional Sample Size (n) -> BEA/PAA Estimator -> Aggregator -> Final Estimate
- **Critical path:**
  1. Verify if total sample size n is known.
  2. If Known → Run BEA logic (compute α, combine observed/imputed).
  3. If Unknown → Run PAA logic (compute thresholds, average two means).
- **Design tradeoffs:**
  - **BEA vs. PAA:** Use BEA for controlled environments (e.g., teaching evaluations where class size is known) and finite samples. Use PAA for open platforms (e.g., hotel reviews) where total viewership is unknown but traffic is high.
  - **Robustness vs. Accuracy:** These methods are conservative; in non-adversarial settings (smooth distributions), they may underperform compared to simple averaging (as seen with BEA in the real-world experiment).
- **Failure signatures:**
  - **High Variance in small n:** PAA may produce unstable threshold estimates on low-traffic items.
  - **Correlated Ratings:** The assumption of independence breaks down, potentially invalidating the theoretical lower bounds.
- **First 3 experiments:**
  1. **Synthetic Bias Injection:** Generate a dataset with a known ground truth, artificially remove ratings based on value (e.g., drop 50% of neutral ratings), and compare BEA/PAA recovery against simple averaging.
  2. **Threshold Sensitivity Analysis:** For PAA, vary the lower bound parameter q (e.g., 0.1 to 0.9) to visualize how the "cut points" shift and impact the final aggregated score.
  3. **Ablation on Sample Size:** Run PAA on increasing subsets of a large dataset to plot the "regret" curve and verify if it converges to the theoretical optimum as n increases.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the Balanced Extremes Aggregator (BEA) and Polarizing-Averaging Aggregator (PAA) be generalized to aggregate multi-dimensional ratings?
- **Basis in paper:** [explicit] The Conclusion identifies extending aggregators to handle "multi-dimensional ratings," where users rate multiple attributes like service, quality, and price, as a "promising direction."
- **Why unresolved:** The current theoretical framework in Section 1.1 restricts the rating xᵢ to a single discrete value in [m], modeling a one-dimensional score.
- **Evidence that would resolve it:** A formal extension of the minimax regret objective and aggregation rules to vector-valued ratings, validated on datasets containing multi-attribute reviews.

### Open Question 2
- **Question:** Can adaptive algorithms be developed to maintain robustness when participation probabilities vary dynamically over time?
- **Basis in paper:** [explicit] The Conclusion suggests that exploring "adaptive algorithms that dynamically adjust to varying participation probabilities over time could enhance robustness."
- **Why unresolved:** The current methods assume a static lower bound q for participation probability, whereas real-world environments like social media exhibit rapidly changing participation patterns.
- **Evidence that would resolve it:** An online learning algorithm that updates participation estimates in real-time, along with theoretical bounds or empirical results showing reduced regret in non-stationary environments.

### Open Question 3
- **Question:** How can the aggregators be modified to account for systematic biases arising from user demographics or past behavior?
- **Basis in paper:** [explicit] The Conclusion proposes "incorporating user behavior analysis" to adjust for systematic biases; [inferred] Section 1.1 explicitly assumes raters are "homogeneous" and have the same participation bias.
- **Why unresolved:** The assumption of homogeneity implies that all users participate with identical probability distributions, which likely does not hold in diverse populations.
- **Evidence that would resolve it:** A generalized model where participation probability gᵣ varies by user group, and a modified aggregator that leverages demographic metadata to lower worst-case regret.

## Limitations

- The methods assume homogeneous raters with identical participation probabilities, which may not hold in diverse populations.
- BEA's performance degrades at high participation probabilities (q > 0.8), making it less effective in scenarios with minimal selection bias.
- Theoretical guarantees for PAA rely on asymptotic analysis, leaving finite-sample performance uncertain.

## Confidence

**High Confidence:** The theoretical lower bound derivation and the basic BEA mechanism for known sample sizes. The numerical results demonstrating BEA's near-optimal performance across various participation probabilities are well-supported.

**Medium Confidence:** The PAA mechanism's asymptotic optimality claims and its practical performance on the hotel dataset. While theoretically sound, the finite-sample behavior and generalizability across domains require further validation.

**Low Confidence:** The optimization procedure for parameter a* in BEA when the gap n₁-nₘ is large, as the paper only mentions using "Monte Carlo method" without specifying implementation details. The exact remapping procedure from 10-point to 7-point scales in the real-world experiment also lacks specificity.

## Next Checks

1. **Finite-Sample Performance Analysis:** Conduct controlled experiments varying sample sizes from n=10 to n=1000 for both BEA and PAA, measuring regret against the theoretical bounds. This would empirically validate the asymptotic claims for PAA and identify sample size thresholds where BEA outperforms PAA.

2. **Domain Transferability Test:** Apply both methods to rating datasets from different domains (e.g., product reviews, academic course evaluations, app store ratings) with varying rating scales and participation patterns. This would test the generalizability of the theoretical guarantees beyond the hotel dataset.

3. **Sensitivity Analysis for Participation Probability Bounds:** Systematically vary the lower bound parameter q from 0.1 to 0.9 in increments of 0.1, measuring the impact on both BEA and PAA performance. This would clarify the operational ranges where each method is most effective and identify potential crossover points.