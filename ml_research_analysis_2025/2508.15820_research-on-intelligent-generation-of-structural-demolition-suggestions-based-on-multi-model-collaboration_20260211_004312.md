---
ver: rpa2
title: Research on intelligent generation of structural demolition suggestions based
  on multi-model collaboration
arxiv_id: '2508.15820'
source_url: https://arxiv.org/abs/2508.15820
tags:
- demolition
- generation
- fine-tuning
- structural
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency and low automation in generating
  structural demolition suggestions, which typically require extensive manual effort
  and domain expertise. To solve this, the authors propose a multi-model collaborative
  framework enhanced with Retrieval-Augmented Generation (RAG) and Low-Rank Adaptation
  (LoRA) fine-tuning of large language models (LLMs).
---

# Research on intelligent generation of structural demolition suggestions based on multi-model collaboration

## Quick Facts
- **arXiv ID:** 2508.15820
- **Source URL:** https://arxiv.org/abs/2508.15820
- **Reference count:** 0
- **Primary result:** Proposes multi-model collaborative framework with RAG and LoRA fine-tuning to automate structural demolition suggestion generation, achieving up to 100% accuracy on multiple-choice questions and 86.67% average accuracy across tasks.

## Executive Summary
This paper addresses the inefficiency and low automation in generating structural demolition suggestions, which typically require extensive manual effort and domain expertise. To solve this, the authors propose a multi-model collaborative framework enhanced with Retrieval-Augmented Generation (RAG) and Low-Rank Adaptation (LoRA) fine-tuning of large language models (LLMs). The framework integrates specialized knowledge retrieval with multi-role expert models to generate targeted, context-aware demolition suggestions. Experiments show that the proposed approach significantly improves LLM performance, with Qwen2.5-LoRA-RAG achieving up to 100% accuracy on multiple-choice questions and 86.67% average accuracy across tasks. Compared to CivilGPT, the framework provides more targeted and structure-specific suggestions, demonstrating its effectiveness in automating and enhancing the quality of structural demolition planning.

## Method Summary
The method involves constructing a domain knowledge base from structural engineering standards, demolition schemes, and research papers, then generating 841 synthetic instruction-tuning samples using a large teacher model. Qwen2.5-7B-Instruct is fine-tuned using LoRA with rank=8 for 30 epochs, then integrated with LightRAG using BGE-M3 embeddings for knowledge retrieval. The multi-model collaborative framework employs five specialized models (analysis, demolition, inspection, integration, response) with role-specific prompts to process engineering contexts and generate targeted demolition suggestions. The system is evaluated on BLEU-4, ROUGE scores, and accuracy on multiple-choice and judgment questions.

## Key Results
- Qwen2.5-LoRA achieves 96.67% accuracy on multiple-choice questions and 73.33% on judgment questions
- Qwen2.5-LoRA-RAG achieves 86.67% average accuracy across tasks, outperforming LoRA-only approach (83.33%)
- The multi-model framework provides more targeted and structure-specific suggestions compared to single-model approaches like CivilGPT
- BLEU-4 score of 83.27 and ROUGE-1 score of 85.33 achieved on test set

## Why This Works (Mechanism)

### Mechanism 1
LoRA fine-tuning improves LLM domain-specific text comprehension and generation in structural demolition tasks by decomposing weight updates into low-rank matrices A×B, injecting domain knowledge with minimal parameter changes while preserving base model capabilities.

### Mechanism 2
RAG with knowledge graph integration enhances retrieval quality and context-awareness beyond flat vector similarity by constructing entity-relationship graphs from domain documents, enabling multi-hop reasoning and comprehensive retrieval.

### Mechanism 3
Multi-model collaboration with role-specialized prompts produces more targeted, structure-specific demolition suggestions by decomposing complex reasoning into specialized subtasks through five distinct model roles.

## Foundational Learning

- **Concept:** Parameter-Efficient Fine-Tuning (PEFT) and LoRA
  - **Why needed here:** Understanding how LoRA achieves domain adaptation with ~0.1% of trainable parameters versus full fine-tuning.
  - **Quick check question:** Can you explain why LoRA initializes matrix B to zeros and matrix A with Gaussian distribution, and what this means for training dynamics?

- **Concept:** Retrieval-Augmented Generation (RAG) with Vector Databases
  - **Why needed here:** The framework relies on semantic search over domain documents.
  - **Quick check question:** What is the difference between flat vector retrieval and graph-enhanced retrieval, and when would each approach fail?

- **Concept:** Multi-Agent / Multi-Model Orchestration
  - **Why needed here:** The collaboration framework requires coordinating multiple LLM calls with different prompts.
  - **Quick check question:** How would you design a prompt to make a model act as a "safety inspector" versus a "demolition planner," and what failure modes would you expect?

## Architecture Onboarding

- **Component map:** Knowledge Base (3 standards + 10 schemes + 11 papers + 5 patents) → Embedding Layer (BGE-M3 → Vector database) → Fine-tuning Layer (LoRA adapters on Qwen2.5-7B) → Retrieval Layer (LightRAG with hybrid retrieval) → Collaboration Layer (5 specialized models with role-specific prompts) → Output Layer (Aggregated demolition suggestions)

- **Critical path:** Dataset construction → LoRA fine-tuning (30 epochs, lr=5e-5) → RAG integration → Multi-model orchestration → Response generation

- **Design tradeoffs:** Model size vs. deployment cost (7B models chosen for A10 24GB GPU constraints), LoRA rank vs. adaptation capacity (Rank=8 balances efficiency with knowledge capture), RAG vs. training cost (RAG provides updatable knowledge without retraining but adds latency)

- **Failure signatures:** Hallucination on unseen scenarios if knowledge base lacks relevant precedents, role collapse in multi-model setup if prompts are insufficiently differentiated, judgment question underperformance indicating LoRA alone doesn't enhance reasoning

- **First 3 experiments:**
  1. Baseline reproduction: Fine-tune Qwen2.5-7B-Instruct with provided hyperparameters; verify loss convergence and BLEU-4/ROUGE scores match reported values (±2%).
  2. Ablation study: Test Qwen2.5-LoRA-RAG with RAG disabled to isolate retrieval contribution; expect ~3-5% accuracy drop per Table 4 patterns.
  3. Collaboration framework validation: Run same structural precondition through single-model and multi-model framework; qualitatively compare specificity and targetiveness of suggestions against CivilGPT outputs in Figure 10 as reference.

## Open Questions the Paper Calls Out

### Open Question 1
How can the logical reasoning capability of the framework be improved for judgment tasks where LoRA fine-tuning was ineffective? The authors note judgment questions focus on logical reasoning, and "LoRA fine-tuning did not improve it significantly."

### Open Question 2
How do human domain experts quantitatively rate the safety and constructability of the generated proposals compared to human-written plans? Evaluation relies on text metrics (BLEU/ROUGE) and qualitative examples without structured expert validation of the final output's engineering safety.

### Open Question 3
Can the framework support direct integration with Finite Element Analysis (FEA) software to automate the ingestion of structural data? The introduction emphasizes reliance on finite element model updates, but the input data appears to be manually summarized text rather than raw FEA output.

## Limitations
- No quantitative comparison of actual demolition suggestions against expert human outputs or real-world outcomes
- Knowledge base construction relies on synthetic data generation using a 72B model, potentially introducing domain-specific biases
- Evaluation is limited to synthetic questions without validation on actual engineering cases

## Confidence
- **High Confidence:** LoRA fine-tuning effectiveness on objective questions (96.67% accuracy achieved)
- **Medium Confidence:** RAG contribution to accuracy improvement (86.67% vs 83.33%)
- **Medium Confidence:** Multi-model collaboration superiority over single-model approaches
- **Low Confidence:** Generalization to real-world structural demolition scenarios

## Next Checks
1. Ablation study on knowledge base quality: Test the framework with progressively smaller/shallower knowledge bases to quantify minimum viable domain knowledge required for acceptable performance.

2. Real-world validation pilot: Apply the framework to 3-5 actual structural demolition cases with documented expert suggestions, comparing automated outputs against human expert recommendations using domain-specific metrics.

3. Prompt sensitivity analysis: Systematically vary the role definitions and prompt structures across the 5 specialized models to determine if collaboration benefits are robust to prompt engineering variations.