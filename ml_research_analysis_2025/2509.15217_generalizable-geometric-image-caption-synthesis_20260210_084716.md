---
ver: rpa2
title: Generalizable Geometric Image Caption Synthesis
arxiv_id: '2509.15217'
source_url: https://arxiv.org/abs/2509.15217
tags:
- reasoning
- geometric
- dataset
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeoReasoning-10K, a 10,000-image caption
  dataset for geometric reasoning built via symbolic synthesis and refined using reinforcement
  learning with verifiable rewards (RLVR). The authors sample geometric relations,
  generate aligned image-caption pairs, and employ a RAFT-based framework to iteratively
  optimize captions and models using reasoning and caption rewards.
---

# Generalizable Geometric Image Caption Synthesis

## Quick Facts
- arXiv ID: 2509.15217
- Source URL: https://arxiv.org/abs/2509.15217
- Reference count: 40
- Key outcome: GeoReasoning-10K dataset + RLVR training improves geometric reasoning (+2.8% MathVista, +4.8% MathVerse) and generalizes to structured non-geometric tasks (+2.4%-3.9% MMMU)

## Executive Summary
This paper introduces GeoReasoning-10K, a 10,000-image geometric caption dataset built through symbolic synthesis and refined using reinforcement learning with verifiable rewards (RLVR). The authors sample geometric relations, generate aligned image-caption pairs with explicit visual markers, and employ a RAFT-based framework to iteratively optimize captions and models. Experimental results show that models trained on this dataset outperform baselines on both geometric and non-geometric reasoning tasks, with particular gains on structured visual domains like engineering diagrams and artistic drawings.

## Method Summary
The method involves two phases: (1) Cold-start SFT training on the initial GeoReasoning-10K dataset using Gemma3-4B, and (2) RLVR with RAFT for 5 epochs. The RAFT loop generates 8 candidate captions per image, scores them using a composite reward (70% reasoning reward from Qwen2.5-7B-Instruct, 30% caption reward from ROUGE+BLEU), selects the top-1, and retrains. The dataset is created by symbolically synthesizing geometric problems from 50 basic relations, rendering them with visual augmentations (ticks, arcs, annotations), and generating aligned captions via rule-based templates.

## Key Results
- Models trained on GeoReasoning-10K outperform baselines on MathVista (+2.8%) and MathVerse (+4.8%) for in-domain geometric tasks
- The same models generalize to non-geometric tasks on MMMU, showing 2.4%-3.9% gains in Art & Design and Tech & Engineering domains
- Ablation studies confirm the effectiveness of both the cold-start phase and RLVR training process
- The dataset is released under MIT license and is safe for research use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based visual augmentation creates provably aligned image-text pairs.
- Mechanism: Symbolic geometric relations are converted to clauses, then rendered as images with explicit visual markers (ticks for equal segments, arcs for equal angles, angle value annotations). These markers are simultaneously translated into captions using rule-based templates, ensuring every visual element has textual correspondence.
- Core assumption: Explicit visual-textual correspondence is necessary and sufficient for cross-modal reasoning transfer.
- Evidence anchors: [abstract] states the method "ensures full alignment between visual and textual content"; [section 3.1.2] describes visual augmentation strategies that encode semantic relationships.

### Mechanism 2
- Claim: RLVR with composite rewards iteratively improves both dataset quality and model capability.
- Mechanism: The RAFT loop generates N candidate captions per image, scores them using R(c,I) = λr·Rreasoning + (1-λr)·Rcaption, selects top-K, and retrains. Reasoning reward uses a frozen LLM to solve questions based on each caption; caption reward measures ROUGE/BLEU against ground truth.
- Core assumption: Better captions for question-solving correlate with better cross-modal representations; reward sparsity can be mitigated by combining two reward types.
- Evidence anchors: [abstract] notes the pipeline "successfully captures the key features of geometry problem-solving"; [section 3.3.3] details the composite reward function.

### Mechanism 3
- Claim: Geometric alignment training transfers to non-geometric reasoning domains.
- Mechanism: Training forces models to identify and extract key visual elements relevant to problem-solving. This "key-element capturing ability" generalizes to other line-art and spatial reasoning tasks where identifying salient features determines success.
- Core assumption: Reasoning skills are partially modality-agnostic; visual attention patterns learned in geometry transfer to other structured visual domains.
- Evidence anchors: [abstract] states models "generalize well to both geometric and non-geometric reasoning tasks"; [section 4.3] attributes MMMU improvements to improved "key-element capturing ability."

## Foundational Learning

- **Concept: Cross-modal alignment**
  - Why needed here: The core problem is asymmetrical alignment—captions omit visual details, images lack textual correspondences. Understanding that "two lines of equal length" must be visually indicated is prerequisite to understanding the contribution.
  - Quick check question: Given a triangle image with no annotations and the caption "Triangle ABC is isosceles with AB=BC," what information is missing from the image to achieve full alignment?

- **Concept: Reinforcement learning with verifiable rewards (RLVR)**
  - Why needed here: The RAFT method relies on reward signals that can be objectively computed (answer correctness, format validity), unlike RLHF which uses human preferences.
  - Quick check question: Why is answer correctness a "verifiable" reward while human preference scores are not?

- **Concept: Symbolic-to-rendering pipelines**
  - Why needed here: The paper builds on AlphaGeometry's approach—relations → clauses → graph representation → rendering. Understanding this cascade explains why the pipeline can generate infinite problem types.
  - Quick check question: If you add a new geometric relation "spiral_center" to the relation library, what three components must be defined for it to work in this pipeline?

## Architecture Onboarding

- **Component map:**
Relation Library (50 relations) → Clause Generator → Graph-based Representation → Rule-based Renderer + Caption Templates → Initial Image-Caption Pairs → LLM-based QA Generator → Cold-Start SFT → RLVR Loop (RAFT, 5 epochs)

- **Critical path:** The reward function design is the most failure-prone component. If Rreasoning is too sparse, learning stalls. The caption reward (ROUGE+BLEU) acts as a dense auxiliary signal to bootstrap early training.

- **Design tradeoffs:**
  - Rule-based vs. LLM-based caption generation: Rule-based ensures alignment but lacks diversity; the paper chooses rule-based for initial data, LLM for QA only.
  - N=8 candidates vs. more: More candidates increase compute linearly but may yield diminishing returns; ablation not provided.
  - λr=0.7: Higher reasoning weight improves MathVista but not MathVerse, suggesting task-specific tuning may help.

- **Failure signatures:**
  - If accuracy plateaus after epoch 2-3 of RAFT: Reward signal may be saturated; candidates are indistinguishable. Check reward distribution variance.
  - If non-geometric tasks don't improve: Model may be overfitting to geometric primitives. Reduce training epochs or add diversity to relation sampling.
  - If generated captions contain hallucinated measurements: Rule-based templates may have bugs; verify renderer-caption template parity.

- **First 3 experiments:**
  1. Replicate the ablation on a smaller model (e.g., 1B parameters) to verify cold-start necessity before full training runs.
  2. Visualize reward distributions across RAFT epochs—if Rreasoning mean increases but variance collapses, the model may be converging to safe-but-suboptimal captions.
  3. Test a single domain transfer (geometry → Tech & Engineering only) with intermediate checkpoints to identify when transfer capability emerges.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section identifies areas for future work including exploring the impact of different reward models and extending the approach to more diverse visual domains.

## Limitations
- Transfer to non-geometric tasks is limited to structured domains (engineering diagrams, charts, artistic drawings) where visual primitives resemble geometric elements
- The 50 basic geometric relations form a closed vocabulary requiring manual engineering for new relations
- The RAFT procedure assumes a frozen LLM judge with stable reasoning capabilities; performance degrades if the judge fails on out-of-distribution captions

## Confidence
**High Confidence**: Claims about cold-start effectiveness, RAFT improving MathVista/MathVerse accuracy, and dataset licensing/safety are directly measurable from reported experiments.

**Medium Confidence**: Claims about RLVR iteratively improving captions and geometric training transferring to non-geometric tasks are supported by experimental results but lack independent validation of underlying assumptions.

**Low Confidence**: Claims about cross-modal alignment being "necessary and sufficient" for reasoning transfer are strong theoretical assertions not directly tested.

## Next Checks
1. **Reward distribution analysis**: Plot R_reasoning and R_caption distributions across all RAFT epochs. If R_reasoning mean increases but variance collapses, the model may have converged to safe-but-suboptimal captions.

2. **Domain transfer ablation**: Test intermediate checkpoints (after 1, 3, 5 RAFT epochs) on Tech & Engineering only, comparing gains to final model. This isolates when transfer capability emerges.

3. **Relation library stress test**: Systematically remove 10-15 relations from the 50-relation library and retrain. If performance drops proportionally to coverage loss, results depend heavily on relation diversity.