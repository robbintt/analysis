---
ver: rpa2
title: 'MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation
  with Med-LVLMs'
arxiv_id: '2505.18530'
source_url: https://arxiv.org/abs/2505.18530
tags:
- mrgagents
- medical
- report
- reports
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRGAgents introduces a multi-agent framework for medical report
  generation that addresses the limitations of existing Med-LVLM models, which tend
  to exhibit bias toward normal findings and produce incomplete reports. The approach
  decomposes report generation into disease-specific tasks, training specialized agents
  on curated subsets of IU X-ray and MIMIC-CXR datasets.
---

# MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs

## Quick Facts
- arXiv ID: 2505.18530
- Source URL: https://arxiv.org/abs/2505.18530
- Authors: Pengyu Wang; Shuchang Ye; Usman Naseem; Jinman Kim
- Reference count: 19
- Primary result: MRGAgents improves diagnostic recall from 0.354 to 0.376 on IU X-ray and from 0.314 to 0.382 on MIMIC-CXR while increasing CIDEr scores from 0.401 to 0.426 on IU X-ray

## Executive Summary
MRGAgents introduces a multi-agent framework that addresses the bias toward normal findings and incomplete reports in existing Med-LVLM models. The approach decomposes report generation into disease-specific tasks, training 13 specialized agents on curated subsets of IU X-ray and MIMIC-CXR datasets. Each agent focuses on a distinct disease category, enabling more precise and comprehensive report generation. Experiments demonstrate significant improvements in diagnostic utility and disease-specific classification accuracy, particularly for conditions like pleural effusion and consolidation.

## Method Summary
MRGAgents fine-tunes 13 specialized BioMedGPT agents, each trained on disease-specific sentence subsets extracted from IU X-ray and MIMIC-CXR datasets. The framework uses CheXbert for sentence-level labeling into 13 disease categories, then fine-tunes each agent independently using positive and negative finding sentences. At inference, each agent generates one sentence per image, and the system selects the 6 most unique sentences based on lowest average CIDEr scores to form the final report.

## Key Results
- Diagnostic recall improved from 0.354 to 0.376 on IU X-ray and from 0.314 to 0.382 on MIMIC-CXR
- CIDEr scores increased from 0.401 to 0.426 on IU X-ray
- Disease-specific classification accuracy significantly improved, particularly for pleural effusion (0.288→0.979 on IU X-ray) and consolidation
- Framework effectively balances normal and abnormal findings while ensuring comprehensive clinical descriptions

## Why This Works (Mechanism)

### Mechanism 1: Task decomposition reduces class imbalance effects
- **Claim:** Decomposing into disease-specific subtasks reduces class imbalance effects and improves abnormality detection
- **Mechanism:** Curating disease-specific training subsets and assigning each of 13 agents to a distinct CheXbert category enables each model to learn more balanced representations for its target condition rather than being dominated by majority "normal" samples
- **Core assumption:** Disease categories are sufficiently separable at the sentence level; errors from one agent do not cascade catastrophically into others
- **Evidence anchors:** "By curating subsets of the IU X-ray and MIMIC-CXR datasets to train disease-specific agents, MRGAgents generates reports that more effectively balance normal and abnormal findings"
- **Break condition:** If disease categories have high semantic overlap at sentence level, specialized agents may produce redundant or contradictory outputs, degrading aggregation quality

### Mechanism 2: Sentence-level labeling via CheXbert enables fine-grained supervision
- **Claim:** Sentence-level labeling via CheXbert enables fine-grained supervision that preserves clinically relevant detail
- **Mechanism:** CheXbert assigns positive/negative/uncertain labels per observation to individual sentences. Training on these labeled sentences—rather than full reports—provides cleaner supervision signals, reducing noise from unrelated content within multi-sentence reports
- **Core assumption:** CheXbert labeling accuracy is sufficiently high; sentence boundaries align with clinical semantic units
- **Evidence anchors:** "We utilize CheXbert... a BERT-based classifier designed for multi-label medical text classification... This classifier allows us to construct subsets of training data tailored for different agent specializations"
- **Break condition:** Systematic CheXbert mislabeling would propagate errors into agent training data, potentially reinforcing rather than reducing bias

### Mechanism 3: CIDEr-based sentence selection at inference reduces redundancy
- **Claim:** CIDEr-based sentence selection at inference reduces redundancy while preserving diversity
- **Mechanism:** Each agent generates one sentence; to assemble a coherent report, the framework computes pairwise CIDEr scores and selects the 6 most unique sentences (lowest average CIDEr)
- **Core assumption:** Six sentences approximate optimal report length; low CIDEr correlates with clinical diversity rather than merely lexical variation
- **Evidence anchors:** "We compute each sentence's average CIDEr score against all others, selecting the six most unique sentences with the lowest scores"
- **Break condition:** If multiple agents generate clinically important but lexically similar sentences, CIDEr-based filtering may incorrectly discard clinically relevant content

## Foundational Learning

- **Concept: Multi-label medical text classification (CheXbert)**
  - **Why needed here:** Understanding how reports are decomposed into 14 observation categories with positive/negative/uncertain labels is essential for debugging agent training data quality
  - **Quick check question:** Given a radiology sentence, can you predict which CheXbert label(s) it should receive?

- **Concept: Vision-language model fine-tuning**
  - **Why needed here:** MRGAgents builds on BioMedGPT; practitioners must understand how to fine-tune Med-LVLMs on curated subsets without catastrophic forgetting
  - **Quick check question:** What hyperparameters from BioMedGPT's original training does MRGAgents preserve, and why?

- **Concept: CIDEr metric semantics**
  - **Why needed here:** The inference-time sentence selection relies on CIDEr as a diversity proxy; misunderstanding its TF-IDF weighting could lead to suboptimal aggregation
  - **Quick check question:** Does a low CIDEr score between two sentences indicate they share rare words or common words?

## Architecture Onboarding

- **Component map:** CheXbert labeler -> 13 Specialized agents -> Sentence selector -> Report aggregator
- **Critical path:**
  1. Preprocess datasets (IU X-ray, MIMIC-CXR) via sentence splitting + CheXbert labeling
  2. Partition labeled sentences into 13 disease-specific training subsets
  3. Fine-tune each agent independently using BioMedGPT defaults
  4. At inference, pass image through all 13 agents → generate 13 candidate sentences
  5. Compute CIDEr matrix, select 6 lowest-average-score sentences, concatenate
- **Design tradeoffs:**
  - **Precision vs. recall per disease:** Specialization improves per-disease recall but may increase false positives for rare conditions
  - **Report length vs. redundancy:** Selecting 6 sentences balances comprehensiveness against repetition; this threshold is not rigorously validated
  - **Modularity vs. coherence:** Independent agents may generate sentences with inconsistent terminology; no cross-agent coordination during generation
- **Failure signatures:**
  - **Silent omission:** Critical findings missing if responsible agent's sentence is filtered out by CIDEr selection
  - **Category collapse:** Agents for under-represented diseases (e.g., Fracture with 38 IU X-ray training samples) may fail to learn meaningful patterns
  - **Support device blindness:** Paper explicitly notes failure to describe support devices, as these don't map cleanly to disease categories
- **First 3 experiments:**
  1. **Ablate sentence selection:** Compare 6-sentence CIDEr selection against (a) all 13 sentences, (b) random selection, (c) rule-based filtering to quantify selection contribution
  2. **Per-agent data audit:** For each disease category, analyze training sample counts and label quality; flag categories with <100 samples for potential merging or augmentation
  3. **Cross-dataset generalization:** Train agents on IU X-ray subsets, evaluate on MIMIC-CXR (and vice versa) to assess whether disease-specific specialization transfers across imaging protocols

## Open Questions the Paper Calls Out

- **How can the framework be refined to effectively detect and describe non-disease-related findings, such as support devices and medical implants?**
  - **Basis in paper:** The authors state in Section 4.3 that MRGAgents "fails to describe support devices present in the images" and explicitly highlight the "need for further refinement to improve coverage of non-disease-related findings"
  - **Why unresolved:** The current architecture relies on 13 agents strictly aligned with specific disease labels, leaving no specialized agent to handle medical hardware or non-pathological observations
  - **What evidence would resolve it:** Successful integration of an additional agent trained on hardware annotations, resulting in the accurate generation of support device descriptions in the final report

- **Does the heuristic of selecting exactly six sentences based on average CIDEr scores optimize diagnostic utility, or does it introduce arbitrary truncation?**
  - **Basis in paper:** Section 2.3 describes selecting the "six most unique sentences" to reduce redundancy, but provides no justification for the specific number six or the superiority of CIDEr for this filtering step
  - **Why unresolved:** A fixed sentence count may fail to reflect the variable complexity of clinical cases; simple cases might include redundant sentences while complex ones might omit necessary findings
  - **What evidence would resolve it:** An ablation study analyzing performance variances when the selection threshold $k$ is dynamically adjusted or set to different fixed values

- **Is the multi-agent decomposition strategy effective when applied to other Med-LVLM backbones besides BioMedGPT?**
  - **Basis in paper:** The authors note in Footnote 1: "Our framework is flexible and BioMedGPT can be replaced with any Med-LVLMs in future"
  - **Why unresolved:** The experiments are limited to the BioMedGPT architecture; it remains unconfirmed whether other vision-language models (e.g., LLaVA-Med) respond similarly to the proposed fine-tuning and sentence-level decomposition
  - **What evidence would resolve it:** Benchmark results reproducing the MRGAgents framework using alternative foundational models as the agent backbone

## Limitations
- CIDEr-based sentence selection mechanism lacks validation—it's unclear whether selecting sentences with lowest pairwise CIDEr scores actually improves clinical coherence or simply filters for lexical diversity
- Per-disease agent performance varies dramatically, with some categories (Fracture) having insufficient training samples (38 for IU X-ray) to learn meaningful patterns
- Support devices and non-disease findings are not captured by the framework, limiting report completeness

## Confidence
- **High confidence:** Task decomposition reduces class imbalance effects (supported by substantial performance improvements for specific diseases like pleural effusion and consolidation)
- **Medium confidence:** CIDEr-based sentence selection improves report quality (mechanism described but not rigorously validated)
- **Medium confidence:** CheXbert-in-the-loop supervision provides cleaner learning signals (mechanism assumed but no direct evidence)

## Next Checks
1. Ablate the sentence selection mechanism: Compare 6-sentence CIDEr selection against all 13 sentences, random selection, and rule-based filtering to quantify selection contribution
2. Analyze per-agent data quality: Audit training sample counts and label accuracy for each disease category; flag categories with <100 samples for potential merging or augmentation
3. Cross-dataset generalization test: Train agents on IU X-ray subsets and evaluate on MIMIC-CXR (and vice versa) to assess whether disease-specific specialization transfers across imaging protocols