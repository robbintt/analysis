---
ver: rpa2
title: 'DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval'
arxiv_id: '2509.04193'
source_url: https://arxiv.org/abs/2509.04193
tags:
- domain
- image
- retrieval
- dude
- cross-domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUDE tackles the domain gap challenge in unsupervised cross-domain
  image retrieval by disentangling domain-invariant object features from domain-specific
  styles. It leverages a text-to-image diffusion model with style-specific prompts
  to extract clean object semantics before performing cross-domain alignment.
---

# DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval

## Quick Facts
- arXiv ID: 2509.04193
- Source URL: https://arxiv.org/abs/2509.04193
- Reference count: 40
- Primary result: State-of-the-art performance across PACS, Office-Home, and DomainNet with 18.71-20.83% average improvements in precision@50/100/200 over best baselines.

## Executive Summary
DUDE tackles the domain gap challenge in unsupervised cross-domain image retrieval by disentangling domain-invariant object features from domain-specific styles. It leverages a text-to-image diffusion model with style-specific prompts to extract clean object semantics before performing cross-domain alignment. A progressive alignment strategy then gradually expands positive contrastive pairs from instances to in-domain neighbors and finally to cross-domain neighbors. The method achieves state-of-the-art performance across three standard benchmarks (PACS, Office-Home, DomainNet) over 13 domains.

## Method Summary
DUDE operates through two main modules: Object Disentanglement and Progressive Alignment. The Object Disentanglement module uses a frozen Stable Diffusion model conditioned on style-specific prompts ("a {domain} of a [z]") to extract domain-invariant object features from source domain images. The Progressive Alignment module then aligns these features through three stages: instance-to-augmentation, in-domain mutual neighbor, and cross-domain mutual neighbor alignment, using InfoNCE contrastive loss at each stage. Training proceeds in warm-up (50 epochs disentanglement only), in-domain alignment (30 epochs), and cross-domain alignment (20 epochs) phases.

## Key Results
- Achieves state-of-the-art P@200 scores of 61.78 on PACS, 48.04 on Office-Home, and 74.90 on DomainNet
- Demonstrates 18.71-20.83% average improvements in precision@50/100/200 compared to best baselines
- Outperforms existing methods across all 13 domain pairs in three standard benchmarks
- Ablation studies confirm the effectiveness of both the disentanglement module and progressive alignment strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-invariant object features can be extracted by inverting a frozen text-to-image diffusion model through style-specific prompting.
- Mechanism: A frozen Stable Diffusion model is conditioned on prompts of the form "a {domain} of a [z]", where {domain} is a fixed style token (e.g., "sketch") and [z] is a learnable token produced by the feature extractor. By optimizing the feature extractor to minimize the diffusion reconstruction loss while keeping SD frozen, gradients force [z] to encode only the object semantics, since the style is already specified by the {domain} token.
- Core assumption: The diffusion model's learned text-to-image generation capability implies a factorized latent space where style and object semantics are separable in the token embedding dimension.
- Evidence anchors:
  - [abstract] "DUDE leverages a text-to-image generative model to disentangle object features from domain-specific styles"
  - [section 3.2] "Rather than optimizing the image generation capability of SD itself, we freeze all parameters of SD... and train the feature extractor fθ"
  - [corpus] Limited direct corpus support for diffusion-based disentanglement in retrieval; neighboring work focuses on cross-domain adaptation via optimal transport or feature alignment, not generative inversion.

### Mechanism 2
- Claim: Progressive expansion of contrastive positive pairs improves alignment stability and reduces noise in cross-domain matching.
- Mechanism: Training proceeds in three stages: (1) instance-to-augmentation alignment, (2) in-domain mutual neighbor alignment, (3) cross-domain mutual neighbor alignment. Each stage uses the stable representations learned in the previous stage to define increasingly ambitious positive pairs, with momentum encoders providing a consistent feature bank.
- Core assumption: Early-stage alignment on easier positive pairs yields representations that are sufficiently reliable to identify meaningful neighbors in later stages.
- Evidence anchors:
  - [abstract] "DUDE aligns mutual neighbors from within domains to across domains in a progressive manner"
  - [section 3.3] "By progressively constructing positive contrastive pairs from single instances, in-domain mutual neighbors, to cross-domain mutual neighbors, DUDE achieves reliable and accurate UCIR"
  - [corpus] "Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval" (arXiv 2505.13907) similarly employs progressive alignment for adaptive retrieval, suggesting this strategy generalizes beyond this specific method.

### Mechanism 3
- Claim: Mutual nearest neighbor filtering removes spurious alignments by requiring bidirectional agreement in feature similarity.
- Mechanism: For both in-domain and cross-domain alignment, positive pairs are only formed when sample i is in j's top-k neighbors and j is in i's top-k neighbors. This symmetric adjacency matrix removes asymmetric or coincidental similarities.
- Core assumption: True semantic matches are more likely to exhibit mutual proximity in feature space, while noisy matches tend to be unidirectional.
- Evidence anchors:
  - [section 3.3] "This adjacency matrix captures only those sample pairs that mutually recognize each other as top-k neighbors, filtering noisy or incidental neighbors"
  - [section 4.6.2] Ablation shows that removing progressive stages or disentanglement hurts performance, though direct ablation of mutual filtering is not separately reported.
  - [corpus] Weak direct corpus evidence for mutual neighbor filtering specifically; related cross-domain works use optimal transport or prototype-based alignment rather than bidirectional neighbor selection.

## Foundational Learning

- **Concept: Contrastive Learning with InfoNCE**
  - Why needed here: The progressive alignment module relies on InfoNCE loss to pull positive pairs together and push negatives apart in the embedding space. Understanding how temperature, batch size, and memory banks affect representation quality is essential for debugging alignment failures.
  - Quick check question: Can you explain why a smaller temperature in InfoNCE makes the model more sensitive to hard negatives, and how a memory bank (as in MoCo) affects the diversity of negatives?

- **Concept: Diffusion Models as Conditional Generators**
  - Why needed here: The Object Disentanglement Module uses Stable Diffusion not for generation but as a frozen conditioner. Understanding how diffusion models are trained to denoise latents conditioned on text embeddings helps clarify why gradients through the frozen network can shape the learnable object token.
  - Quick check question: In a text-conditional diffusion model, what role does the text encoder play, and why can we backpropagate through a frozen U-Net to optimize a conditioning token?

- **Concept: Domain Adaptation vs. Domain Generalization in Retrieval**
  - Why needed here: UCIR is distinct from standard domain adaptation: there is no labeled source domain, and the goal is retrieval accuracy, not classification. Recognizing this distinction prevents misapplying techniques that assume source labels or supervised alignment.
  - Quick check question: How does UCIR differ from unsupervised domain adaptation (UDA), and why might methods that rely on pseudo-labels from a labeled source domain fail in the UCIR setting?

## Architecture Onboarding

- **Component map:**
  - Images -> Feature Extractor (fθ) -> Embeddings (z_i)
  - Embeddings + Style Tokens -> Text Encoder -> Prompt Embeddings
  - Prompt Embeddings + Noisy Latents -> Frozen SD U-Net -> Diffusion Loss (L_OD)
  - Momentum Encoder (f_m) -> Memory Banks (Q_A, Q_B) -> Mutual Neighbor Graphs
  - Loss Aggregation -> InfoNCE Contrastive Losses (L_aug, L_in, L_cross)

- **Critical path:**
  1. Images from two domains are passed through fθ to produce embeddings.
  2. Embeddings are inserted into domain-specific prompts and encoded by the frozen text encoder.
  3. Frozen SD receives noisy latents (from VAE) and prompt embeddings; L_OD is computed and backpropagated to update fθ only.
  4. After warm-up, momentum encoder is initialized, and progressive alignment begins: L_aug → L_in → L_cross in stages.
  5. At test time, fθ embeddings from query and database are compared via cosine similarity for retrieval.

- **Design tradeoffs:**
  - Frozen vs. Fine-tuned Diffusion: Freezing SD preserves its generative priors and reduces compute, but may limit adaptability to domains not well-covered by its training data. The paper assumes SD v1.5's style tokens (e.g., "sketch", "infograph") are sufficiently expressive.
  - Prompt Template Choice: Using domain-specific {domain} tokens enables disentanglement but requires manual specification per dataset. A fixed "photo" prompt underperforms (Table 5), suggesting style token quality is critical.
  - Mutual Neighbor Threshold (k): Small k yields fewer but cleaner positives; large k adds signal but risks noise. The paper sets k=50 based on a sweep (Fig. 4a), but this may not generalize to datasets with different class or sample densities.

- **Failure signatures:**
  - Retrieval precision degrades sharply when switching from L_OD warm-up directly to cross-domain alignment (Table 6, lines 3–4 vs. 7), indicating unstable representations.
  - Large performance gaps between domains (e.g., Photo→Sketch vs. Sketch→Photo in Table 1) may indicate asymmetric domain gaps that mutual neighbor filtering cannot fully resolve.
  - If generated images from disentangled features (Fig. 3) retain style artifacts from the source domain, the disentanglement is incomplete.

- **First 3 experiments:**
  1. Sanity check of disentanglement: For a subset of images, extract [z] embeddings with varying {domain} prompts, then use these embeddings to generate images via SD. Visually confirm that the same object appears across different style prompts without source style leakage.
  2. Mutual neighbor ablation: Run the full pipeline with mutual filtering disabled (i.e., use unidirectional top-k neighbors). Compare retrieval precision to the full method to quantify the contribution of bidirectional filtering.
  3. Prompt template sensitivity: Test alternative prompt templates (e.g., "a {domain} style image of a [z]" or using CLIP text embeddings instead of hand-specified {domain} strings). Measure performance changes on a held-out domain pair to assess robustness to prompt design.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the handcrafted domain-style tokens be replaced with learnable embeddings or automatically discovered prompts to accommodate domains with ambiguous or fine-grained stylistic variations?
- **Basis in paper:** [inferred] Section 3.2 states that DUDE constructs prompts using a "handcrafted domain-style token (e.g., painting, infograph, sketch)." The ablation study (Table 5) compares "varying" vs. "fixed" prompts but does not address the sensitivity of retrieval performance to the specific choice of the handcrafted token (e.g., synonyms) or the feasibility of automating this selection for diverse datasets.
- **Why unresolved:** The current reliance on manual prompt engineering may limit scalability to datasets where domain characteristics are difficult to describe concisely in natural language, or where the "domain" definition is subjective.
- **What evidence would resolve it:** An ablation study testing the sensitivity of L_{OD} to synonymous handcrafted tokens, or the implementation of an optimization loop (e.g., using textual inversion) to learn domain tokens without manual labeling, showing comparable or improved retrieval accuracy.

### Open Question 2
- **Question:** How does DUDE perform on target domains that are poorly represented in the pre-trained Stable Diffusion model's latent space?
- **Basis in paper:** [inferred] Section 3.2 leverages the frozen Stable Diffusion model under the assumption that it can synthesize images reflecting the style and object specified. The method relies on the diffusion model's existing generative capability to separate style and object via the prompt mechanism.
- **Why unresolved:** If the "domain-specific style" is novel or significantly distinct from the data distribution used to train Stable Diffusion (e.g., specialized medical imaging, non-visual sensory data), the frozen denoising network ε_φ may fail to accurately reconstruct or disentangle features, potentially degrading the object feature extraction.
- **What evidence would resolve it:** Experimental results on cross-domain retrieval tasks involving domains stylistically distant from LAION-5B (the SD training set), or an analysis correlating the "novelty" of a domain style (measured by FID of generation) with the retrieval performance drop.

### Open Question 3
- **Question:** What is the computational overhead of the Object Disentanglement Module during training compared to standard discriminative UCIR methods, and does it scale linearly with image resolution?
- **Basis in paper:** [inferred] Section 3.2 involves backpropagating gradients from the frozen Stable Diffusion U-Net noise predictor ε_φ to the feature extractor f_θ. While inference uses standard ResNet features, the training phase (Eq. 4) requires processing images through a large generative model (SD v1.5), which is significantly more computationally intensive than standard CNN backbones used in baselines like ProtoNCE or DD.
- **Why unresolved:** The paper focuses on retrieval accuracy (Precision@k) and does not report training efficiency metrics (e.g., FLOPs, memory footprint, training time per epoch), leaving the practical trade-off between disentanglement quality and computational cost unexplored.
- **What evidence would resolve it:** A comparative analysis of training time and GPU memory consumption against baseline methods (e.g., ProtoOT, DD), specifically quantifying the overhead introduced by the diffusion-based denoising loss.

## Limitations
- The method requires manual specification of domain-style tokens, limiting scalability to datasets with ambiguous or numerous domain categories.
- Performance may degrade when target domains are stylistically distant from the pre-trained diffusion model's training distribution.
- Computational overhead during training is significantly higher than standard discriminative methods due to the frozen diffusion model processing.

## Confidence
- **High:** State-of-the-art performance on standard benchmarks, clear ablation of disentanglement and progressive alignment, reproducibility of experimental setup.
- **Medium:** Theoretical justification for diffusion-based disentanglement, generalizability of mutual neighbor filtering across diverse domain gaps, computational efficiency.
- **Low:** Long-term robustness to out-of-distribution styles, sensitivity to prompt engineering beyond the tested templates, behavior when domains share no mutual neighbors.

## Next Checks
1. **Disentanglement validation:** Generate images from the same object embedding with different style prompts (e.g., "a sketch of a [z]" vs. "a photo of a [z]"). Visually confirm that the object remains consistent while only the style changes.
2. **Mutual neighbor sparsity analysis:** For each domain pair, report the fraction of samples that have mutual neighbors at k=50. Correlate this with performance drops to identify when the method fails due to lack of cross-domain matches.
3. **Computational overhead measurement:** Profile GPU memory and wall-clock time per training epoch with and without the frozen diffusion model. Estimate scalability limits for larger datasets or higher-resolution images.