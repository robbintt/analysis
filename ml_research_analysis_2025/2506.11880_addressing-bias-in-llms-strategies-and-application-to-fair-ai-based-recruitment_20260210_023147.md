---
ver: rpa2
title: 'Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment'
arxiv_id: '2506.11880'
source_url: https://arxiv.org/abs/2506.11880
tags:
- gender
- bias
- information
- systems
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates gender bias in Transformer-based language
  models (BERT, RoBERTa) applied to AI-based recruitment tools. When trained with
  gender-biased scores, both models learned to discriminate against women, producing
  male-dominated shortlists (69% male, 31% female) and violating fairness metrics
  (demographic ratio 0.441-0.493, below the 0.8 threshold).
---

# Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment

## Quick Facts
- **arXiv ID:** 2506.11880
- **Source URL:** https://arxiv.org/abs/2506.11880
- **Reference count:** 22
- **Primary result:** Privacy-enhancing methods (explainability-based token masking and adversarial learning) successfully reduced gender bias in LLM-based recruitment tools, achieving near-equal gender representation and improved fairness metrics.

## Executive Summary
This paper investigates gender bias in Transformer-based language models (BERT, RoBERTa) when applied to AI-based recruitment. When trained with gender-biased scores, both models learned to discriminate against women, producing male-dominated shortlists (69% male, 31% female) and violating fairness metrics. The authors propose a bias mitigation framework using two methods: Integrated Gradients-based token removal from biographies and adversarial learning to remove gender information from latent representations. Both methods successfully reduced bias, achieving near-equal gender representation (49-51%) and demographic ratios above 0.84.

## Method Summary
The paper proposes a multimodal MLP architecture combining frozen BERT/RoBERTa text embeddings with structured competency features to predict candidate suitability scores. Two bias mitigation strategies are evaluated: (1) Explainability-based token removal using Integrated Gradients to identify and mask gender-correlated tokens in biographies, and (2) Adversarial learning where an auxiliary classifier attempts to predict gender from the latent representation while the main model learns to prevent this. Both methods were tested on the FairCVdb dataset of 24,000 synthetic resumes, showing significant improvements in fairness metrics while maintaining predictive performance.

## Key Results
- Baseline models trained on gender-biased scores produced male-dominated shortlists (69% male, 31% female)
- Demographic ratios fell below 0.8 threshold (0.441-0.493), indicating unfair bias
- Both mitigation methods achieved near-equal gender representation (49-51%)
- Method 1 (explainability) showed slightly better bias reduction (DKL 0.0267-0.0168 vs 0.0422-0.0487)
- Method 2 (adversarial) achieved higher overall recall (83-80%)

## Why This Works (Mechanism)
The paper demonstrates that language models can learn and amplify gender bias present in training labels, but this bias can be mitigated through targeted interventions. The explainability method works by identifying and removing tokens most correlated with gender from the input text, forcing the model to make decisions based on job-relevant information rather than demographic cues. The adversarial method creates a min-max game where the main model tries to predict scores while preventing gender prediction, effectively removing gender information from the learned representations.

## Foundational Learning
- **Demographic Parity**: A fairness metric requiring equal acceptance rates across groups (why needed: primary fairness evaluation criterion; quick check: shortlist gender ratio)
- **Integrated Gradients**: Attribution method to identify input features most influencing model predictions (why needed: token selection for masking; quick check: top-20 tokens per resume)
- **Adversarial Learning**: Training framework where two models compete, one trying to predict sensitive attributes while the other prevents this (why needed: removing gender information from representations; quick check: auxiliary classifier accuracy ~50%)
- **Kullback-Leibler Divergence**: Measure of difference between probability distributions (why needed: quantifying bias reduction; quick check: DKL closer to 0 indicates less bias)
- **Gradient Reversal Layer**: Technique to implement adversarial training by reversing gradients during backpropagation (why needed: implementing method 2; quick check: stable training without collapse)

## Architecture Onboarding

**Component Map:** Biographies + Competencies -> BERT (frozen) -> MLP (3 layers) -> Score Prediction; Auxiliary Gender Classifier (method 2)

**Critical Path:** Input text and features -> BERT embedding -> MLP layers -> Output score; Adversarial branch (method 2) attaches to first MLP layer

**Design Tradeoffs:** Frozen BERT preserves pre-trained knowledge but limits adaptation to domain-specific bias patterns; token masking may remove relevant context; adversarial training can cause instability

**Failure Signatures:** Baseline shows male-skewed shortlists (<0.5 gender ratio) and low demographic ratio (<0.8); adversarial collapse shows auxiliary accuracy at 0% or 100%; explainability failure shows persistent gender-correlated tokens

**First Experiments:**
1. Train baseline multimodal model on synthetic biased labels; verify shortlist ratio <0.5
2. Implement auxiliary gender classifier on first MLP layer; train with gradient reversal (λ=0.1); monitor auxiliary accuracy
3. Apply Integrated Gradients to identify top-20 tokens per resume; mask top-30 gender-correlated tokens per sector; compare shortlist balance

## Open Questions the Paper Calls Out
- How do the proposed bias mitigation strategies perform when applied to real-world, non-synthetic recruitment datasets with natural noise and bias distributions? (The experiments rely entirely on controlled synthetic data)
- How can the adversarial learning framework be adapted to mitigate bias for non-binary gender identities and intersectional demographic attributes? (Current formulation models gender as binary)
- Does updating the Transformer's weights during training compromise the bias mitigation capabilities of the explainability or adversarial approaches? (Transformers are frozen in current experiments)

## Limitations
- FairCVdb dataset is synthetic and may not capture real-world complexity and noise
- Adversarial method implementation details are underspecified (exact optimizer settings, stability mechanisms)
- Both methods assume binary gender classification, limiting applicability to non-binary identities
- Token masking may inadvertently remove job-relevant information along with biased terms

## Confidence
- **High confidence**: Observed bias direction and baseline results (biased labels consistently produce male-skewed shortlists)
- **Medium confidence**: Mitigation method efficacy (results internally consistent but depend on undocumented hyperparameters)
- **Low confidence**: Exact reproduction of reported numbers without FairCVdb access and full adversarial loss details

## Next Checks
1. Verify baseline bias: Train multimodal BERT+MLP on synthetic biased labels; confirm shortlist ratio <0.5 and demographic ratio <0.8
2. Implement adversarial module: Add auxiliary gender classifier to first MLP layer; train with gradient reversal and λ=0.1; check auxiliary accuracy stays near 50%
3. Apply explainability masking: Run Integrated Gradients, filter by POS tags, mask top-30 gender-correlated tokens per sector; compare shortlist gender balance to baseline