---
ver: rpa2
title: Fairness in Multi-modal Medical Diagnosis with Demonstration Selection
arxiv_id: '2511.15986'
source_url: https://arxiv.org/abs/2511.15986
tags:
- fairness
- medical
- fads
- selection
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fairness in medical AI is a critical concern, especially for multimodal
  large language models (MLLMs) that analyze medical images. Existing fairness improvement
  methods are often impractical for foundation-scale MLLMs due to their reliance on
  large labeled datasets or fine-tuning.
---

# Fairness in Multi-modal Medical Diagnosis with Demonstration Selection

## Quick Facts
- **arXiv ID**: 2511.15986
- **Source URL**: https://arxiv.org/abs/2511.15986
- **Reference count**: 40
- **Key outcome**: Fairness in medical AI is a critical concern, especially for multimodal large language models (MLLMs) that analyze medical images. Existing fairness improvement methods are often impractical for foundation-scale MLLMs due to their reliance on large labeled datasets or fine-tuning. This paper investigates In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. The authors systematically analyze how different demonstration selection (DS) strategies affect fairness and find that conventional methods fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, they propose Fairness-Aware Demonstration Selection (FADS), which constructs demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning.

## Executive Summary
This paper addresses fairness challenges in multimodal medical diagnosis by investigating in-context learning (ICL) as a tuning-free approach for improving demographic equity. The authors systematically analyze how demonstration selection strategies affect fairness outcomes and identify that conventional methods fail due to demographic imbalance in selected exemplars. They propose Fairness-Aware Demonstration Selection (FADS), which uses clustering-based sampling to create demographically balanced and semantically relevant demonstrations. Experiments on multiple medical imaging benchmarks demonstrate that FADS consistently reduces demographic disparities while maintaining diagnostic accuracy, offering an efficient and scalable solution for fair medical image reasoning.

## Method Summary
The authors propose Fairness-Aware Demonstration Selection (FADS) as a lightweight, tuning-free method to improve fairness in multimodal medical diagnosis. FADS addresses the limitations of conventional demonstration selection strategies that fail to ensure fairness due to demographic imbalance in selected exemplars. The method employs clustering-based sampling to construct demographically balanced and semantically relevant demonstrations. This approach leverages in-context learning capabilities of foundation models without requiring extensive labeled datasets or fine-tuning, making it practical for foundation-scale multimodal large language models (MLLMs). The framework systematically analyzes how different demonstration selection strategies affect fairness outcomes and demonstrates consistent reduction in gender-, race-, and ethnicity-related disparities across multiple medical imaging benchmarks.

## Key Results
- FADS consistently reduces gender-, race-, and ethnicity-related disparities in medical diagnosis
- The method maintains strong diagnostic accuracy while improving fairness metrics
- FADS outperforms conventional demonstration selection strategies across multiple medical imaging benchmarks

## Why This Works (Mechanism)
The effectiveness of FADS stems from its ability to create demographically balanced demonstrations through clustering-based sampling. By ensuring representation across different demographic groups in the in-context examples, the model receives more equitable guidance during inference. The clustering approach identifies semantically relevant samples while maintaining demographic diversity, addressing the fundamental issue that conventional demonstration selection strategies introduce bias through demographic imbalance. This mechanism allows the foundation model to learn fairer decision boundaries without requiring model retraining or extensive labeled data.

## Foundational Learning

**In-Context Learning (ICL)**: Why needed - Enables foundation models to adapt to new tasks without fine-tuning; Quick check - Verify model can follow task instructions using only prompt demonstrations.

**Demonstration Selection (DS)**: Why needed - Critical for ICL success as examples guide model behavior; Quick check - Ensure selected demonstrations are relevant and representative of target task.

**Demographic Balance**: Why needed - Prevents model from learning biased patterns from imbalanced training data; Quick check - Verify demographic distribution in selected demonstrations matches target population.

**Clustering-based Sampling**: Why needed - Enables identification of semantically relevant yet diverse samples; Quick check - Validate clusters capture meaningful task-relevant variations.

**Fairness Metrics**: Why needed - Quantifies demographic disparities in model predictions; Quick check - Ensure metrics capture relevant fairness dimensions for medical applications.

## Architecture Onboarding

**Component Map**: Input data -> Preprocessing -> Clustering module -> Demonstration selection -> In-context prompt construction -> Foundation MLLM inference -> Fairness evaluation

**Critical Path**: The most critical components are the clustering module and demonstration selection algorithm, as they directly determine the demographic balance and semantic relevance of in-context examples that guide the foundation model's reasoning.

**Design Tradeoffs**: FADS trades some computational overhead for clustering against the benefit of improved fairness. The approach sacrifices the simplicity of random demonstration selection for more sophisticated, fairness-aware sampling that ensures demographic representation.

**Failure Signatures**: Key failure modes include: clustering that fails to capture demographic dimensions, selection bias that reintroduces imbalance, or semantic relevance that is sacrificed for demographic balance. Performance degradation on minority subgroups or unexpected accuracy drops would indicate issues.

**First Experiments**: 
1. Test FADS on a simple binary classification task with known demographic biases to verify fairness improvements.
2. Compare clustering quality metrics (silhouette score, demographic entropy) across different clustering approaches.
3. Evaluate FADS on a small medical imaging dataset with balanced demographics to establish baseline performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the limitations section suggests areas for future investigation including real-world deployment validation and cross-modal generalization.

## Limitations
- Evaluation scope may not fully represent real-world medical population diversity
- Performance on rare demographic subgroups remains unclear
- Focus on image-based diagnosis may not translate to other medical modalities or complex clinical scenarios

## Confidence

**High confidence**: Core observation that conventional demonstration selection strategies fail to ensure fairness due to demographic imbalance in selected exemplars. The systematic analysis of ICL fairness provides robust evidence for this claim.

**Medium confidence**: Effectiveness of the FADS method, as results are promising across multiple benchmarks but long-term stability and generalizability require further validation.

**Low confidence**: Scalability claims for foundation-scale MLLMs, as the study does not directly test the approach on the largest available models or in production settings.

## Next Checks

1. **Real-world deployment validation**: Test FADS in clinical settings with diverse patient populations to assess performance across rare demographic subgroups and evaluate potential biases that may emerge in practical use.

2. **Cross-modal generalization**: Evaluate the approach's effectiveness beyond image-based diagnosis, including text, audio, and sensor data, to determine its applicability across different medical modalities.

3. **Long-term stability assessment**: Conduct extended testing over time to identify potential performance drift or emerging biases as the system encounters new data patterns and demographic distributions.