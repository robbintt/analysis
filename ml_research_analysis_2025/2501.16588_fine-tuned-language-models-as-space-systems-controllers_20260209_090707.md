---
ver: rpa2
title: Fine-Tuned Language Models as Space Systems Controllers
arxiv_id: '2501.16588'
source_url: https://arxiv.org/abs/2501.16588
tags:
- trajectories
- llms
- problem
- data
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs), when
  fine-tuned with task-specific data, can effectively control simplified space systems
  across multiple problem domains, including a 3D spring-mass system, low-thrust orbit
  transfers, cislunar trajectory planning, and 3-degree-of-freedom powered descent
  guidance. The study uses relatively small, open-source LLMs (7-13 billion parameters)
  and shows that fine-tuning requires less data than traditional deep neural networks
  while maintaining strong generalization capabilities, even for inputs outside the
  training distribution.
---

# Fine-Tuned Language Models as Space Systems Controllers

## Quick Facts
- arXiv ID: 2501.16588
- Source URL: https://arxiv.org/abs/2501.16588
- Reference count: 21
- This paper demonstrates that fine-tuned LLMs can effectively control simplified space systems across multiple domains using relatively small, open-source models.

## Executive Summary
This paper investigates the use of large language models (LLMs) as controllers for space systems, demonstrating that fine-tuning with task-specific data enables effective control across multiple problem domains. The study shows that relatively small, open-source LLMs (7-13 billion parameters) can generate precise multi-dimensional control outputs with up to 10 significant digits when fine-tuned using LoRA. The approach achieves strong generalization capabilities, even for inputs outside the training distribution, and can handle multiple space control problems with minimal performance degradation. The work highlights LLMs' pattern recognition strengths and robustness, particularly in nonlinear control scenarios, positioning them as promising tools for autonomous spacecraft operations.

## Method Summary
The approach uses fine-tuning with LoRA to adapt pretrained LLMs (Llama-2-7B, Llama-2-13B, Llama3-8B) to control space systems. Numerical state vectors are serialized into text prompts with explicit formatting, and the LLM predicts tokens autoregressively to generate control outputs. Training data is generated from optimal control solutions across four problem domains: 3D spring-mass system, low-thrust orbit transfers, cislunar trajectory planning, and 3-DoF powered descent guidance. The method uses small datasets (3-3000 trajectories) and achieves closed-loop control with precision up to 10-11 significant digits.

## Key Results
- Fine-tuned LLMs achieve position RMSE of 0.05-0.10 DU in orbit transfers and 0.01-0.02 DU in spring-mass systems
- A single LLM can control multiple space systems with minimal performance degradation compared to specialist models
- LLMs show better robustness than traditional optimizers for nonlinear control problems with perturbed initial conditions
- Training requires less data than traditional deep neural networks while maintaining strong generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained transformers transfer general pattern recognition capabilities to numerical control domains unrelated to their training data.
- Mechanism: The attention mechanism learns to identify structural patterns in sequences regardless of whether those sequences represent language or numerical state-control mappings. During fine-tuning, these pattern recognition circuits are repurposed rather than built from scratch.
- Core assumption: Pattern recognition learned from text corpora generalizes to control sequences without requiring domain-specific physics knowledge in the pretraining data.
- Evidence anchors: [abstract] "researchers have shown that large pretrained models excel at pattern recognition, even when such pattern has likely nothing in common with pretraining data" [section] Page 2 references Mirchandani et al. showing "LLMs as general pattern machines"

### Mechanism 2
- Claim: Low-Rank Adaptation (LoRA) enables sample-efficient fine-tuning by constraining weight updates to low-dimensional subspaces.
- Mechanism: Rather than updating all parameters, LoRA decomposes weight changes into low-rank matrices (B × A), dramatically reducing the number of trainable parameters while preserving the pretrained knowledge base.
- Core assumption: The control policy can be expressed as a low-rank perturbation to the pretrained model; higher-rank modifications are unnecessary.
- Evidence anchors: [abstract] "amount of data required to perform fine-tuning is smaller than what is generally required of traditional deep neural networks" [section] Page 4 defines LoRA: "∆W = BA, where B ∈ ℝ^(nd×nr), A ∈ ℝ^(nr×nd), with nr ≪ nd the rank of the update matrix"

### Mechanism 3
- Claim: Structured text prompts with numerical state encoding enable precise multi-dimensional control output generation through autoregressive token prediction.
- Mechanism: State vectors are serialized into text prompts with explicit formatting. The LLM predicts tokens autoregressively, and numerical values are parsed from the structured output string. Precision emerges from the model's learned numerical representation.
- Core assumption: Tokenization and embedding preserve sufficient numerical fidelity for control applications.
- Evidence anchors: [abstract] "fine-tuned LLMs are capable of controlling systems by generating sufficiently accurate outputs that are multi-dimensional vectors with up to 10 significant digits" [section] Page 5 shows prompt structure with 7-significant-digit inputs

## Foundational Learning

- Concept: **Transformer Attention Mechanism**
  - Why needed here: The entire approach relies on attention-based pattern matching between state sequences and control outputs. Understanding multi-head attention (Q, K, V matrices) is essential for debugging why the model attends to certain state components.
  - Quick check question: Can you explain why attention allows the model to relate state components that are far apart in the input sequence?

- Concept: **Fine-Tuning vs. In-Context Learning**
  - Why needed here: This paper uses fine-tuning (parameter updates via LoRA), not in-context learning. Understanding the distinction clarifies why data efficiency is achieved through weight modification rather than prompt engineering alone.
  - Quick check question: What is the difference between providing examples in a prompt (in-context) versus updating model weights (fine-tuning)?

- Concept: **Optimal Control Fundamentals (LQR, Primer Vector Theory)**
  - Why needed here: Training data is generated from optimal control solutions (LQR for linear systems, indirect methods with primer vectors for orbit transfer). You must understand these to interpret the "ground truth" labels and why certain control sequences are optimal.
  - Quick check question: For a minimum-energy orbit transfer, what does the primer vector theory tell us about the optimal thrust direction?

## Architecture Onboarding

- Component map: Numerical State → Prompt Generator (text template) → Tokenizer → Embedding Layer → Stacked Attention Blocks (96 heads for GPT-3 scale) → Output Token Distribution → Autoregressive Sampling → Text Output → Numerical Parser → Control Vector → Environment
- Critical path:
  1. **Prompt engineering** (Page 5): State serialization format directly impacts the model's ability to parse numerical relationships. The template must include constraints, current state, and target state explicitly.
  2. **Numerical precision extraction** (Page 4): Output parsing must handle the specific decimal format the model was trained to produce.
  3. **Control frequency**: LLM is queried at discrete timesteps (20 Hz for spring, 0.02 TU for orbit transfer). Inference latency must be bounded.
- Design tradeoffs:
  - **Model size vs. sample efficiency**: Llama-2-13B learns faster with fewer trajectories than Llama-2-7B (Fig. 11), but requires more memory.
  - **Accuracy vs. robustness**: For nonlinear problems, LLM guidance trades accuracy for robustness—LLMs succeeded where the optimizer failed 17% of the time on biased initial conditions (Fig. 5).
  - **Single-task vs. multi-task training**: LOT-LLM (landing + orbit transfer) shows minor performance degradation vs. specialist models but enables unified control across mission phases.
- Failure signatures:
  - **Bang-bang control instability** (Page 18): Landing problem shows larger errors than orbit transfer due to the discontinuous nature of optimal thrust profiles; perturbations destabilize the system near constraint boundaries.
  - **End-of-trajectory degradation** (Page 18): Thrust constraint violations increase toward trajectory end due to sparse training data for late-stage perturbations.
  - **Out-of-distribution drift**: While LLMs generalize better than DNNs (Fig. 6), large biases (≥2σ) cause cost and RMSE to increase substantially.
- First 3 experiments:
  1. **Replicate the 3D spring problem** with 3 trajectories for fine-tuning. Verify the LLM stabilizes the unstable system (cost ratio ~1.59× optimal per Table 1). This is the lowest-complexity validation of the core mechanism.
  2. **Ablation on prompt informativeness**: Train identical models with minimal prompts (state only) vs. full prompts (state + constraints + objective). Quantify the impact of pretraining leverage on data efficiency.
  3. **Out-of-distribution stress test**: Train on orbit transfer with σ=0.05 perturbations, then test with σ=0.10 and σ=0.15. Compare LLM failure rate against the optimizer's 17% failure rate at 0.5σ bias to establish the robustness boundary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs effectively control spacecraft using only high-level natural language goals rather than explicit numeric state vectors?
- Basis in paper: [explicit] The Introduction states, "The long-term end of this line of research would be that it would only require the desired high-level goal in text form as input," whereas the current implementation relies on numeric prompts.
- Why unresolved: The current study inputs numeric state vectors (e.g., position [0.703740...]) into the text prompt; the models have not been tested on purely qualitative or semantic instructions.
- What evidence would resolve it: Successful closed-loop control simulations where the input prompt contains mission goals (e.g., "land near the crater rim") without numerical state data.

### Open Question 2
- Question: How can control stability be guaranteed for safety-critical, bang-bang control problems where LLM inaccuracies risk destabilization?
- Basis in paper: [inferred] The Conclusion notes that for the convex landing problem (which is bang-bang), "perturbations, and the inaccuracies introduced by the FMs, can destabilize the system," highlighting a lack of robustness compared to convex optimizers.
- Why unresolved: The paper demonstrates successful stabilization in linear/non-linear cases but identifies specific instability risks in the discrete bang-bang regime without offering a solution.
- What evidence would resolve it: Development of a hybrid controller or verification method that mathematically bounds LLM output errors to prevent state divergence in bang-bang scenarios.

### Open Question 3
- Question: Does the data efficiency and generalization of fine-tuned LLMs persist when scaling to high-fidelity, 6-degree-of-freedom (6-DoF) environments?
- Basis in paper: [inferred] The paper focuses on "simplified space systems" and "3 degrees-of-freedom" (3-DoF) problems, explicitly excluding attitude dynamics and full coupling found in real-world 6-DoF operations.
- Why unresolved: It is unclear if the pattern recognition capabilities observed in 3-DoF state-space will hold when the input dimensionality and dynamic complexity increase significantly for 6-DoF models.
- What evidence would resolve it: A comparative study showing fine-tuning convergence rates and control accuracy when applying the method to a 6-DoF simulation with coupled attitude and translational dynamics.

## Limitations

- The control problems tested are relatively low-dimensional (up to 6D state spaces) compared to real spacecraft systems
- Training datasets are small (3-3000 trajectories) and may not capture full complexity of real-world perturbations
- Numerical precision achieved (10-11 significant digits) is not validated against actual hardware requirements
- Bang-bang control problems show stability risks where LLM inaccuracies can destabilize the system

## Confidence

- **High Confidence**: LLMs can be fine-tuned to generate precise multi-dimensional control outputs for space systems with up to 10 significant digits of precision
- **Medium Confidence**: A single LLM can effectively control multiple space systems simultaneously with minimal performance degradation
- **Medium Confidence**: LLMs show better robustness than traditional optimizers for nonlinear control problems with perturbed initial conditions
- **Low Confidence**: The pattern recognition generalization mechanism extends broadly beyond the tested problems to arbitrary control domains

## Next Checks

1. **Cross-Domain Transfer Test**: Fine-tune a single LLM on the spring-mass problem, then test it (without additional fine-tuning) on the orbit transfer problem. Measure performance degradation to validate the single-controller hypothesis beyond the LOT-LLM case.

2. **High-Dimensional Control Test**: Scale up to a 12-15 dimensional spacecraft control problem (e.g., full 6-DoF dynamics with attitude control) using the same methodology. Compare sample efficiency and precision against a DNN baseline to test the scaling limits.

3. **Hardware-In-The-Loop Validation**: Implement the fine-tuned LLM controller on a spacecraft simulator or hardware testbed. Measure actual control precision, latency, and robustness to sensor noise and actuator delays that cannot be captured in simulation.