---
ver: rpa2
title: 'HADA: Human-AI Agent Decision Alignment Architecture'
arxiv_id: '2506.04253'
source_url: https://arxiv.org/abs/2506.04253
tags:
- agents
- hada
- agent
- tools
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HADA provides a protocol-agnostic architecture for aligning AI
  agents and legacy algorithms with organizational goals and values. It wraps decision
  tools in stakeholder-specific agents (business, data-science, audit, ethics, customer)
  that expose conversational APIs for steering, auditing, and contesting decisions.
---

# HADA: Human-AI Agent Decision Alignment Architecture

## Quick Facts
- arXiv ID: 2506.04253
- Source URL: https://arxiv.org/abs/2506.04253
- Reference count: 3
- Primary result: HADA provides a protocol-agnostic architecture for aligning AI agents and legacy algorithms with organizational goals and values.

## Executive Summary
HADA introduces a novel architecture that wraps decision tools in stakeholder-specific conversational agents to achieve human-AI alignment. The system maps each human role (business, data-science, audit, ethics, customer) to a dedicated interaction agent, enabling natural-language steering, auditing, and contesting of decisions. A cloud-native prototype deployed on a credit-scoring model demonstrated end-to-end alignment flows across five scripted scenarios, achieving 100% coverage of design objectives including conversational control, KPI/value alignment, and auditability without dependence on specific LLM or framework.

## Method Summary
The method involves building a credit-scoring AI tool using a Kaggle loan dataset with synthetically added ZIP_Code features to demonstrate bias detection. Two decision-tree versions are trained (v1.0 without ZIP_Code, v1.1 with ZIP_Code) and exposed via REST API. Stakeholder agents are containerized with LLM cores, MCP tool adapters, and A2A endpoints. A Controller Agent orchestrates role resolution and policy enforcement. The system is deployed on Kubernetes with standard tools (catalogues, ledger) and evaluated through five scripted dialogues covering KPI changes, model retraining, deployment approval, loan applications, and ethics complaints.

## Key Results
- 100% coverage of six design objectives across all stakeholder scenarios
- Demonstrated ZIP-code bias detection and remediation through conversational workflows
- Achieved framework-agnostic design through MCP and A2A protocol layering
- Provided full decision lineage and audit trails without specific LLM dependence

## Why This Works (Mechanism)

### Mechanism 1: Stakeholder-to-Agent One-to-One Mapping
- Claim: Assigning each human role a dedicated interaction agent creates traceable alignment chains from intent to action.
- Mechanism: A HADA Controller Agent receives natural-language prompts, performs role resolution, routes tasks to stakeholder-specific agents, and logs every tool invocation.
- Core assumption: Stakeholders will express intent accurately through natural language, and agents can correctly parse and propagate that intent.
- Evidence anchors: [abstract] "HADA wraps any algorithm or LLM in role-specific stakeholder agents"; [section 3.6] "each human role is mirrored by a dedicated interaction agent"

### Mechanism 2: Catalogue-Driven Alignment Propagation
- Claim: Centralized catalogues for Business Targets, KPIs, Values, and AI Metadata enable policy changes to cascade across thousands of agents without code changes.
- Mechanism: Alignment objectives are stored in versioned catalogues and propagate to affected AI tools and agents through the orchestration layer.
- Core assumption: Catalogue updates are correctly linked to downstream agents and tools.
- Evidence anchors: [abstract] "Alignment objectives, KPIs, and value constraints are expressed in natural language and are continuously propagated"; [section 5.3] "ZIP-code was removed in subsequent versions"

### Mechanism 3: Protocol-Agnostic Tool Layering (Tools Pattern)
- Claim: Separating the Agent Layer from the Tools Layer via protocol adapters enables vendor-neutral, hot-swappable AI systems.
- Mechanism: AI Tools and Standard Tools expose HTTP/GRPC or MCP endpoints; agents communicate via A2A protocol; sidecars translate between protocols.
- Core assumption: MCP and A2A provide sufficient semantic richness for alignment metadata.
- Evidence anchors: [abstract] "without dependence on specific LLM or framework"; [section 3.5] "The layers can communicate via either (1) direct API calls when schemas are known a-priori, or (2) the emerging Model Context Protocol (MCP)"

## Foundational Learning

- **Concept: LLM Agent Architecture (Memory, Tools, Planning)**
  - Why needed here: HADA builds on canonical LLM agent patterns; understanding memory, tool invocation, and planning is prerequisite to grasping how stakeholder agents operate.
  - Quick check question: Can you explain how a ReAct loop differs from a Reflexion-based self-correction cycle?

- **Concept: MCP and A2A Protocols**
  - Why needed here: HADA explicitly uses Model Context Protocol for tool integration and Agent-to-Agent protocol for multi-agent coordination.
  - Quick check question: What is the difference between an MCP Server exposing tools and an A2A Agent Card for discovery?

- **Concept: RACI Governance Matrices**
  - Why needed here: HADA operationalizes alignment through a RACI matrix mapping each lifecycle activity to responsible/accountable/consulted/informed roles.
  - Quick check question: In a RACI matrix, who is ultimately answerable for a decision—the Responsible or Accountable party?

## Architecture Onboarding

- **Component map:**
  - Agent Layer: Stakeholder agents (BM, DS, Audit, Ethics, Customer) + HADA Controller Agent
  - Tools Layer: AI Tools (getLoanDecision decision tree, LLMs), Standard Tools (Business-Target Catalogue, Values Catalogue, Model Registry, Decision Ledger)
  - Protocols: MCP (agent↔tools), A2A (agent↔agent), HTTP/GRPC façades
  - Infrastructure: Docker containers, Kubernetes clusters, service mesh (zero-trust)

- **Critical path:**
  1. Containerize AI Tool (e.g., getLoanDecision) with OpenAPI/MCP endpoint
  2. Register tool in Model Catalogue with version, KPI links, and value constraints
  3. Deploy stakeholder agents with A2A Agent Cards; configure HADA Controller for role resolution
  4. Run scripted scenario (e.g., KPI change → model retrain → ethics trigger) and verify log completeness

- **Design tradeoffs:**
  - One-to-one stakeholder-agent mapping vs. shared agents: one-to-one maximizes audit clarity but increases container sprawl
  - MCP vs. direct API: MCP offers schema-free flexibility but adds sidecar complexity; direct API is faster for known schemas
  - Centralized Controller vs. sharded sub-controllers: centralized simplifies governance; sharded improves scalability for large deployments

- **Failure signatures:**
  - Alignment drift: Stakeholder updates KPI, but downstream model continues using old objective—check catalogue version binding
  - Protocol breakage: Agent cannot discover tool—verify MCP server registration and Agent Card endpoints
  - Audit gap: Decision lineage missing features or policy version—check immutable ledger logging hooks

- **First 3 experiments:**
  1. Deploy single AI Tool (e.g., credit decision tree) with MCP endpoint; verify natural-language query returns correct decision and logs lineage
  2. Trigger KPI change via Business Manager agent; confirm Data Scientist agent receives ticket and model version increments
  3. Simulate ethics complaint (e.g., flag ZIP Code); verify Values Catalogue updates, model retraining ticket created, and audit log captures full remediation chain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM agents automate the alignment of business objectives with AI tool configurations?
- Basis in paper: [explicit] The authors state, "Future research should explore the automated alignment of business objective changes with AI tools."
- Why unresolved: The current evaluation relied on scripted role-playing and manual verification rather than autonomous, agent-driven objective propagation.
- What evidence would resolve it: Accuracy metrics measuring how correctly agents update AI tool parameters following a change in strategic OKRs.

### Open Question 2
- Question: Can LLM agents automatically enforce value alignment during AI tool updates?
- Basis in paper: [explicit] The conclusion proposes exploring "the automatic value alignment of AI tool changes using LLM agents."
- Why unresolved: The prototype demonstrated bias remediation (ZIP code) only after a human stakeholder triggered the workflow via dialogue.
- What evidence would resolve it: Experiments showing agents autonomously detecting ethical violations during model retraining and executing corrective actions.

### Open Question 3
- Question: Does the architecture maintain performance and auditability under production load?
- Basis in paper: [explicit] The limitations section notes that "Scalability requires stress testing" and validation under "real-world load" remains to be done.
- Why unresolved: Evaluation was limited to walkthrough observations and log inspections with a single credit-scoring tool.
- What evidence would resolve it: Stress-test results from a system with thousands of concurrent agents showing latency, stability, and logging integrity.

## Limitations
- LLM Provider Dependence: Performance depends heavily on underlying LLM's reasoning and tool-calling capabilities.
- Natural Language Parsing Ambiguity: Real-world stakeholder instructions may be ambiguous or contradictory.
- Protocol Implementation Gaps: Specific implementation details for MCP and A2A are not provided.

## Confidence
- **High Confidence**: Protocol-agnostic architecture design, stakeholder-agent mapping mechanism, and catalogue-driven alignment propagation.
- **Medium Confidence**: Empirical evaluation based on five scripted scenarios with a single decision-tree model on synthetic dataset.
- **Low Confidence**: Claim of framework-agnostic design not fully validated due to undisclosed specific LLM models.

## Next Checks
1. **Multi-LLM Performance Benchmark**: Deploy HADA agents using three different LLM providers and evaluate alignment accuracy and conversation quality across identical scenarios.
2. **Real-World Bias Detection Test**: Apply HADA to real-world credit dataset with known demographic correlations and evaluate subtle bias detection.
3. **Scale and Latency Analysis**: Deploy scaled HADA instance with 50+ concurrent stakeholder agents and 100+ AI tools to measure protocol overhead and alignment propagation delay.